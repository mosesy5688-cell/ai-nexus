/* LOGS:
Downloading image for github-AMAI-GmbH-AI-Expert-Roadmap from https://github.com/AMAI-GmbH.png
Image converted to WebP: data/images/github-AMAI-GmbH-AI-Expert-Roadmap.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-iperov-DeepFaceLive', 'github--iperov--deepfacelive', 'DeepFaceLive', 'iperov', '<table align="center" border="0"> <tr><td colspan=2 align="center"> </td></tr> </table> <table align="center" border="0"> <tr><td colspan=2 align="center"> You can swap your face from a webcam or the face in the video using trained face models. Here is a list of available ready-to-use public face models. These persons do not exists. Similarities with real people are accidental. Except Keanu Reeves. He exists, and he''s breathtaking! </td></tr> <tr><td colspan=2 align="center"> <table align="ce...', '["deepfake","faceswap","machine-learning","real-time","streaming","videocall","webcam","python"]', 'other', 30200, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/iperov/DeepFaceLive","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<table align="center" border="0">\n\n<tr><td colspan=2 align="center">\n\n![](doc/deepfacelive_intro.png)\n\n![](doc/logo_onnx.png)![](doc/logo_directx.png)![](doc/logo_python.png)\n\n</td></tr>\n</table>\n<table align="center" border="0">\n\n<tr><td colspan=2 align="center">\n\n## Face Swap (DFM)\n\nYou can swap your face from a webcam or the face in the video using trained face models.\n\nHere is a list of available ready-to-use public face models.\n\nThese persons do not exists. Similarities with real people are accidental. Except Keanu Reeves. He exists, and he''s breathtaking!\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n<table align="center" border="0">\n<tr><td align="center">\nKeanu Reeves\n\n<img src="doc/celebs/Keanu_Reeves/Keanu_Reeves.png" width=128></img>\n\n<a href="doc/celebs/Keanu_Reeves/examples.md">examples</a>\n</td><td align="center">\nIrina Arty\n\n<img src="doc/celebs/Irina_Arty/Irina_Arty.png" width=128></img>\n\nexamples\n</td><td align="center">\nMillie Park\n\n<img src="doc/celebs/Millie_Park/Millie_Park.png" width=128></img>\n\nexamples\n</td><td align="center">\nRob Doe\n\n<img src="doc/celebs/Rob_Doe/Rob_Doe.png" width=128></img>\n\n<a href="doc/celebs/Rob_Doe/examples.md">examples</a>\n</td><td align="center">\nJesse Stat\n\n<img src="doc/celebs/Jesse_Stat/Jesse_Stat.png" width=128></img>\n\nexamples\n</td></tr>\n\n</table>\n\n<table align="center" border="0">\n<tr><td align="center">\nBryan Greynolds\n\n<img src="doc/celebs/Bryan_Greynolds/Bryan_Greynolds.png" width=128></img>\n\n<a href="doc/celebs/Bryan_Greynolds/examples.md">examples</a>\n</td><td align="center">\nMr. Bean\n\n<img src="doc/celebs/Mr_Bean/Mr_Bean.png" width=128></img>\n\nexamples\n</td><td align="center">\nEwon Spice\n\n<img src="doc/celebs/Ewon_Spice/Ewon_Spice.png" width=128></img>\n\n<a href="doc/celebs/Ewon_Spice/examples.md">examples</a>\n\n</td><td align="center">\nNatasha Former\n\n<img src="doc/celebs/Natasha_Former/Natasha_Former.png" width=128></img>\n\n<a href="doc/celebs/Natasha_Former/examples.md">examples</a>\n\n</td><td align="center">\nEmily Winston\n\n<img src="doc/celebs/Emily_Winston/Emily_Winston.png" width=128></img>\n\n<a href="doc/celebs/Emily_Winston/examples.md">examples</a>\n\n</td></tr></table>\n<table align="center" border="0">\n<tr><td align="center">\nAva de Addario\n\n<img src="doc/celebs/Ava_de_Addario/Ava_de_Addario.png" width=128></img>\n\n<a href="doc/celebs/Ava_de_Addario/examples.md">examples</a>\n</td><td align="center">\nDilraba Dilmurat\n\n<img src="doc/celebs/Dilraba_Dilmurat/Dilraba_Dilmurat.png" width=128></img>\n\nexamples\n</td><td align="center">\nMatilda Bobbie\n\n<img src="doc/celebs/Matilda_Bobbie/Matilda_Bobbie.png" width=128></img>\n\n<a href="doc/celebs/Matilda_Bobbie/examples.md">examples</a>\n</td><td align="center">\nYohanna Coralson\n\n<img src="doc/celebs/Yohanna_Coralson/Yohanna_Coralson.png" width=128></img>\n\n<a href="doc/celebs/Yohanna_Coralson/examples.md">examples</a>\n\n</td><td align="center">\nAmber Song\n\n<img src="doc/celebs/Amber_Song/Amber_Song.png" width=128></img>\n\nexamples\n\n</td></tr></table>\n<table align="center" border="0">\n<tr align="center"><td align="center">\nKim Jarrey\n\n<img src="doc/celebs/Kim_Jarrey/Kim_Jarrey.png" width=128></img>\n\n<a href="doc/celebs/Kim_Jarrey/examples.md">examples</a>\n</td><td align="center">\nDavid Kovalniy\n\n<img src="doc/celebs/David_Kovalniy/David_Kovalniy.png" width=128></img>\n\n<a href="doc/celebs/David_Kovalniy/examples.md">examples</a>\n</td><td align="center">\nJackie Chan\n\n<img src="doc/celebs/Jackie_Chan/Jackie_Chan.png" width=128></img>\n\nexamples\n</td><td align="center">\nNicola Badge\n\n<img src="doc/celebs/Nicola_Badge/Nicola_Badge.png" width=128></img>\n\n<a href="doc/celebs/Nicola_Badge/examples.md">examples</a>\n</td><td align="center">\nJoker\n\n<img src="doc/celebs/Joker/Joker.png" width=128></img>\n\nexamples\n</td></tr></table>\n<table align="center" border="0">\n<tr align="center"><td>\nDean Wiesel\n\n<img src="doc/celebs/Dean_Wiesel/Dean_Wiesel.png" width=128></img>\n\n<a href="doc/celebs/Dean_Wiesel/examples.md">examples</a>\n</td><td align="center">\nSilwan Stillwone\n\n<img src="doc/celebs/Silwan_Stillwone/Silwan_Stillwone.png" width=128></img>\n\n<a href="doc/celebs/Silwan_Stillwone/examples.md">examples</a>\n</td><td align="center">\nTim Chrys\n\n<img src="doc/celebs/Tim_Chrys/Tim_Chrys.png" width=128></img>\n\n<a href="doc/celebs/Tim_Chrys/examples.md">examples</a>\n\n</td><td align="center">\nZahar Lupin\n\n<img src="doc/celebs/Zahar_Lupin/Zahar_Lupin.png" width=128></img>\n\n<a href="doc/celebs/Zahar_Lupin/examples.md">examples</a>\n</td><td align="center">\nTim Norland\n\n<img src="doc/celebs/Tim_Norland/Tim_Norland.png" width=128></img>\n\n<a href="doc/celebs/Tim_Norland/examples.md">examples</a>\n</td></tr></table>\n\n\n<table align="center" border="0">\n<tr align="center"><td>\nNatalie Fatman\n\n<img src="doc/celebs/Natalie_Fatman/Natalie_Fatman.png" width=128></img>\n\n<a href="doc/celebs/Natalie_Fatman/examples.md">examples</a>\n</td><td align="center">\nLiu Lice\n\n<img src="doc/celebs/Liu_Lice/Liu_Lice.png" width=128></img>\n\n<a href="doc/celebs/Liu_Lice/examples.md">examples</a>\n</td><td align="center">\nAlbica Johns\n\n<img src="doc/celebs/Albica_Johns/Albica_Johns.png" width=128></img>\n\n<a href="doc/celebs/Albica_Johns/examples.md">examples</a>\n\n</td><td align="center">\nMeggie Merkel\n\n<img src="doc/celebs/Meggie_Merkel/Meggie_Merkel.png" width=128></img>\n\n<a href="doc/celebs/Meggie_Merkel/examples.md">examples</a>\n</td><td align="center">\nTina Shift\n\n<img src="doc/celebs/Tina_Shift/Tina_Shift.png" width=128></img>\n\n<a href="doc/celebs/Tina_Shift/examples.md">examples</a>\n</td></tr></table>\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\nIf you want a higher quality or better face match, you can train your own face model using <a href="https://github.com/iperov/DeepFaceLab">DeepFaceLab</a>\n\nHere is an <a href="https://www.tiktok.com/@arnoldschwarzneggar/video/6995538782204300545">example</a> of Arnold Schwarzneggar trained on a particular face and used in a video call. Read the FAQ for more information.\n\n</td></tr>\n\n</table>\n<table align="center" border="0">\n\n<tr><td colspan=2 align="center">\n\n## Face Swap (Insight)\n\nYou can swap your face from a webcam or the face in the video using your own single photo.\n\n<img src="doc/lukashenko.png" width=128></img>\n\n<img src="doc/insight_faceswap_example.gif"></img>\n\n</td></tr>\n\n</table>\n<table align="center" border="0">\n\n<tr><td colspan=2 align="center">\n\n## Face Animator\n\nThere is also a Face Animator module in DeepFaceLive app. You can control a static face picture using video or your own face from the camera. The quality is not the best, and requires fine face matching and tuning parameters for every face pair, but enough for funny videos and memes or real-time streaming at 25 fps using 35 TFLOPS GPU.\n\n<img src="doc/face_animator_example.gif"></img>\n\n[![Stranger Things theme intro acapella](doc/Ng1C78Ceyxg_screenshot.png)](https://www.youtube.com/watch?v=Ng1C78Ceyxg)\n\nHere is a [mini video](doc/FaceAnimator_tutor.webm?raw=true) showing the process of setting up the Face Animator for Obama controlling Kim Chen''s face.\n\n</td></tr>\n\n</table>\n\n<table align="center" border="0">\n\n<tr><td colspan=2 align="center">\n\n## System requirements\n\nany DirectX12 compatible graphics card\n\n(Recommended RTX 2070+ / Radeon RX 5700 XT+ )\n\nModern CPU with AVX instructions\n\n4GB RAM, 32GB+ paging file\n\nWindows 10\n\n</td></tr>\n<tr><td colspan=2 align="center">\n\n## Documentation\n\n</td></tr>\n<tr><td align="right">\nWindows\n</td><td align="left">\n\n<a href="doc/windows/main_setup.md">Main setup</a>\n\n- <a href="doc/windows/for_streaming.md">additional setup for streaming</a>\n\n- <a href="doc/windows/for_video_calls.md">additional setup for video calls</a>       \n\n<a href="doc/windows/using_android_phone_camera.md">Using Android phone camera</a>  \n\n</td></tr>\n<tr><td align="right">\nLinux\n</td><td align="left">\n<a href="build/linux">Build info</a>\n</td></tr>\n<tr><td align="right">\nFrequently asked questions\n</td><td align="left">\n<a href="doc/user_faq/user_faq.md">for User</a>\n\n<a href="doc/developer_faq/developer_faq.md">for Developer</a>\n</td></tr>\n<tr><td colspan=2 align="center">\n\n## Releases\n\n</td></tr>\n<tr><td align="right">\n\n<a href="https://disk.yandex.ru/d/7i5XTKIKVg5UUg">Windows 10 x64 (yandex.ru)</a>\n\n<a href="https://mega.nz/folder/m10iELBK#Y0H6BflF9C4k_clYofC7yA">Windows 10 x64 (mega.nz)</a>\n\n\n</td><td align="left">\nContains stand-alone zero-dependency all-in-one ready-to-use portable self-extracting folder! You don''t need to install anything other than video drivers.\n<br><br>\nDirectX12 build : NVIDIA, AMD, Intel videocards.\n<br><br>\nNVIDIA build : NVIDIA cards only, GT730 and higher. Works faster than DX12. FaceMerger can work also on AMD/Intel.\n</td></tr>\n<tr><td colspan=2 align="center">\n\n## Communication groups\n\n<tr><td align="right">\n<a href="https://discord.gg/rxa7h9M6rH">Discord</a>\n</td><td align="left">Official discord channel. English / Russian.</td></tr>\n\n<tr><td align="right">\nQQç¾¤124500433\n</td><td align="left">ä¸­æ–‡äº¤æµQQç¾¤ï¼Œå•†åŠ¡åˆä½œæ‰¾ç¾¤ä¸»</td></tr>\n\n</td></tr>\n<tr><td colspan=2 align="center">\n\n## How can I help the project?\n\n</td></tr>\n<tr><td colspan=2 align="center">\nTrain your own face model by following the recommendations in the FAQ section and share it on Discord. If the model fits the quality, it will be added to the public library.\n</td></tr>\n<tr><td colspan=2 align="center">\nRegister github account and push "Star" button.\n</td></tr>\n<!--<tr><td colspan=2 align="center">\n<a href="https://www.paypal.com/paypalme/DeepFaceLab">Donate via Paypal</a>\n</td></tr>-->\n<tr><td colspan=2 align="center">\n<a href="https://yoomoney.ru/to/41001142318065">Donate via Yoomoney</a>\n</td></tr>\n<tr><td colspan=2 align="center">\nbitcoin:bc1qewl062v70rszulml3f0mjdjrys8uxdydw3v6rq\n</td></tr>\n<tr><td colspan=2 align="center">\n\n\n<!--\n    <a href="https://br-stone.online"><img src="doc/logo_barclay_stone.png"></img></a><a href="https://exmo.com"><img src="doc/logo_exmo.png"></img></a>\n\n    presents\n\n    <tr><td align="right">\n\n\n    <a href="">Windows (magnet link)</a>\n    </td><td align="center">Latest release. Use torrent client to download.</td></tr>\n    </tr>\n-->\n\n</table>\n\n\n\n', '{"language":"Python","stars":30200,"forks":1019,"watchers":30200,"open_issues":1,"topics":["deepfake","faceswap","machine-learning","real-time","streaming","videocall","webcam"],"default_branch":"master","size_kb":1540184,"archived":true,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:iperov:DeepFaceLab\">DeepFaceLab<","source_url":"https://github.com/iperov/DeepFaceLab\">DeepFaceLab<"}]', NULL, 'GPL-3.0', 'approved', 80, '132e8c6be66745b1d9e6c0ed9a1f2dcf', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-iperov-DeepFaceLive from https://github.com/iperov.png
Image converted to WebP: data/images/github-iperov-DeepFaceLive.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-eriklindernoren-ML-From-Scratch', 'github--eriklindernoren--ml-from-scratch', 'ML-From-Scratch', 'eriklindernoren', 'Python implementations of some of the fundamental Machine Learning models and algorithms from scratch. The purpose of this project is not to produce as optimized and computationally efficient algorithms as possible but rather to present the inner workings of them in a transparent and accessible way. - Machine Learning From Scratch * About * Table of Contents * Installation * Examples + Polynomial Regression + Classification With CNN + Density-Based Clustering + Generating Handwritten Digits +...', '["data-mining","data-science","deep-learning","deep-reinforcement-learning","genetic-algorithm","machine-learning","machine-learning-from-scratch","python"]', 'other', 29764, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/eriklindernoren/ML-From-Scratch","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# Machine Learning From Scratch\n\n## About\nPython implementations of some of the fundamental Machine Learning models and algorithms from scratch.\n\nThe purpose of this project is not to produce as optimized and computationally efficient algorithms as possible\nbut rather to present the inner workings of them in a transparent and accessible way.\n\n## Table of Contents\n- [Machine Learning From Scratch](#machine-learning-from-scratch)\n  * [About](#about)\n  * [Table of Contents](#table-of-contents)\n  * [Installation](#installation)\n  * [Examples](#examples)\n    + [Polynomial Regression](#polynomial-regression)\n    + [Classification With CNN](#classification-with-cnn)\n    + [Density-Based Clustering](#density-based-clustering)\n    + [Generating Handwritten Digits](#generating-handwritten-digits)\n    + [Deep Reinforcement Learning](#deep-reinforcement-learning)\n    + [Image Reconstruction With RBM](#image-reconstruction-with-rbm)\n    + [Evolutionary Evolved Neural Network](#evolutionary-evolved-neural-network)\n    + [Genetic Algorithm](#genetic-algorithm)\n    + [Association Analysis](#association-analysis)\n  * [Implementations](#implementations)\n    + [Supervised Learning](#supervised-learning)\n    + [Unsupervised Learning](#unsupervised-learning)\n    + [Reinforcement Learning](#reinforcement-learning)\n    + [Deep Learning](#deep-learning)\n  * [Contact](#contact)\n\n## Installation\n    $ git clone https://github.com/eriklindernoren/ML-From-Scratch\n    $ cd ML-From-Scratch\n    $ python setup.py install\n\n## Examples\n### Polynomial Regression\n    $ python mlfromscratch/examples/polynomial_regression.py\n\n<p align="center">\n    <img src="http://eriklindernoren.se/images/p_reg.gif" width="640"\>\n</p>\n<p align="center">\n    Figure: Training progress of a regularized polynomial regression model fitting <br>\n    temperature data measured in LinkÃ¶ping, Sweden 2016.\n</p>\n\n### Classification With CNN\n    $ python mlfromscratch/examples/convolutional_neural_network.py\n\n    +---------+\n    | ConvNet |\n    +---------+\n    Input Shape: (1, 8, 8)\n    +----------------------+------------+--------------+\n    | Layer Type           | Parameters | Output Shape |\n    +----------------------+------------+--------------+\n    | Conv2D               | 160        | (16, 8, 8)   |\n    | Activation (ReLU)    | 0          | (16, 8, 8)   |\n    | Dropout              | 0          | (16, 8, 8)   |\n    | BatchNormalization   | 2048       | (16, 8, 8)   |\n    | Conv2D               | 4640       | (32, 8, 8)   |\n    | Activation (ReLU)    | 0          | (32, 8, 8)   |\n    | Dropout              | 0          | (32, 8, 8)   |\n    | BatchNormalization   | 4096       | (32, 8, 8)   |\n    | Flatten              | 0          | (2048,)      |\n    | Dense                | 524544     | (256,)       |\n    | Activation (ReLU)    | 0          | (256,)       |\n    | Dropout              | 0          | (256,)       |\n    | BatchNormalization   | 512        | (256,)       |\n    | Dense                | 2570       | (10,)        |\n    | Activation (Softmax) | 0          | (10,)        |\n    +----------------------+------------+--------------+\n    Total Parameters: 538570\n\n    Training: 100% [------------------------------------------------------------------------] Time: 0:01:55\n    Accuracy: 0.987465181058\n\n<p align="center">\n    <img src="http://eriklindernoren.se/images/mlfs_cnn1.png" width="640">\n</p>\n<p align="center">\n    Figure: Classification of the digit dataset using CNN.\n</p>\n\n### Density-Based Clustering\n    $ python mlfromscratch/examples/dbscan.py\n\n<p align="center">\n    <img src="http://eriklindernoren.se/images/mlfs_dbscan.png" width="640">\n</p>\n<p align="center">\n    Figure: Clustering of the moons dataset using DBSCAN.\n</p>\n\n### Generating Handwritten Digits\n    $ python mlfromscratch/unsupervised_learning/generative_adversarial_network.py\n\n    +-----------+\n    | Generator |\n    +-----------+\n    Input Shape: (100,)\n    +------------------------+------------+--------------+\n    | Layer Type             | Parameters | Output Shape |\n    +------------------------+------------+--------------+\n    | Dense                  | 25856      | (256,)       |\n    | Activation (LeakyReLU) | 0          | (256,)       |\n    | BatchNormalization     | 512        | (256,)       |\n    | Dense                  | 131584     | (512,)       |\n    | Activation (LeakyReLU) | 0          | (512,)       |\n    | BatchNormalization     | 1024       | (512,)       |\n    | Dense                  | 525312     | (1024,)      |\n    | Activation (LeakyReLU) | 0          | (1024,)      |\n    | BatchNormalization     | 2048       | (1024,)      |\n    | Dense                  | 803600     | (784,)       |\n    | Activation (TanH)      | 0          | (784,)       |\n    +------------------------+------------+--------------+\n    Total Parameters: 1489936\n\n    +---------------+\n    | Discriminator |\n    +---------------+\n    Input Shape: (784,)\n    +------------------------+------------+--------------+\n    | Layer Type             | Parameters | Output Shape |\n    +------------------------+------------+--------------+\n    | Dense                  | 401920     | (512,)       |\n    | Activation (LeakyReLU) | 0          | (512,)       |\n    | Dropout                | 0          | (512,)       |\n    | Dense                  | 131328     | (256,)       |\n    | Activation (LeakyReLU) | 0          | (256,)       |\n    | Dropout                | 0          | (256,)       |\n    | Dense                  | 514        | (2,)         |\n    | Activation (Softmax)   | 0          | (2,)         |\n    +------------------------+------------+--------------+\n    Total Parameters: 533762\n\n\n<p align="center">\n    <img src="http://eriklindernoren.se/images/gan_mnist5.gif" width="640">\n</p>\n<p align="center">\n    Figure: Training progress of a Generative Adversarial Network generating <br>\n    handwritten digits.\n</p>\n\n### Deep Reinforcement Learning\n    $ python mlfromscratch/examples/deep_q_network.py\n\n    +----------------+\n    | Deep Q-Network |\n    +----------------+\n    Input Shape: (4,)\n    +-------------------+------------+--------------+\n    | Layer Type        | Parameters | Output Shape |\n    +-------------------+------------+--------------+\n    | Dense             | 320        | (64,)        |\n    | Activation (ReLU) | 0          | (64,)        |\n    | Dense             | 130        | (2,)         |\n    +-------------------+------------+--------------+\n    Total Parameters: 450\n\n<p align="center">\n    <img src="http://eriklindernoren.se/images/mlfs_dql1.gif" width="640">\n</p>\n<p align="center">\n    Figure: Deep Q-Network solution to the CartPole-v1 environment in OpenAI gym.\n</p>\n\n### Image Reconstruction With RBM\n    $ python mlfromscratch/examples/restricted_boltzmann_machine.py\n\n<p align="center">\n    <img src="http://eriklindernoren.se/images/rbm_digits1.gif" width="640">\n</p>\n<p align="center">\n    Figure: Shows how the network gets better during training at reconstructing <br>\n    the digit 2 in the MNIST dataset.\n</p>\n\n### Evolutionary Evolved Neural Network\n    $ python mlfromscratch/examples/neuroevolution.py\n\n    +---------------+\n    | Model Summary |\n    +---------------+\n    Input Shape: (64,)\n    +----------------------+------------+--------------+\n    | Layer Type           | Parameters | Output Shape |\n    +----------------------+------------+--------------+\n    | Dense                | 1040       | (16,)        |\n    | Activation (ReLU)    | 0          | (16,)        |\n    | Dense                | 170        | (10,)        |\n    | Activation (Softmax) | 0          | (10,)        |\n    +----------------------+------------+--------------+\n    Total Parameters: 1210\n\n    Population Size: 100\n    Generations: 3000\n    Mutation Rate: 0.01\n\n    [0 Best Individual - Fitness: 3.08301, Accuracy: 10.5%]\n    [1 Best Individual - Fitness: 3.08746, Accuracy: 12.0%]\n    ...\n    [2999 Best Individual - Fitness: 94.08513, Accuracy: 98.5%]\n    Test set accuracy: 96.7%\n\n<p align="center">\n    <img src="http://eriklindernoren.se/images/evo_nn4.png" width="640">\n</p>\n<p align="center">\n    Figure: Classification of the digit dataset by a neural network which has<br>\n    been evolutionary evolved.\n</p>\n\n### Genetic Algorithm\n    $ python mlfromscratch/examples/genetic_algorithm.py\n\n    +--------+\n    |   GA   |\n    +--------+\n    Description: Implementation of a Genetic Algorithm which aims to produce\n    the user specified target string. This implementation calculates each\n    candidate''s fitness based on the alphabetical distance between the candidate\n    and the target. A candidate is selected as a parent with probabilities proportional\n    to the candidate''s fitness. Reproduction is implemented as a single-point\n    crossover between pairs of parents. Mutation is done by randomly assigning\n    new characters with uniform probability.\n\n    Parameters\n    ----------\n    Target String: ''Genetic Algorithm''\n    Population Size: 100\n    Mutation Rate: 0.05\n\n    [0 Closest Candidate: ''CJqlJguPlqzvpoJmb'', Fitness: 0.00]\n    [1 Closest Candidate: ''MCxZxdr nlfiwwGEk'', Fitness: 0.01]\n    [2 Closest Candidate: ''MCxZxdm nlfiwwGcx'', Fitness: 0.01]\n    [3 Closest Candidate: ''SmdsAklMHn kBIwKn'', Fitness: 0.01]\n    [4 Closest Candidate: ''  lotneaJOasWfu Z'', Fitness: 0.01]\n    ...\n    [292 Closest Candidate: ''GeneticaAlgorithm'', Fitness: 1.00]\n    [293 Closest Candidate: ''GeneticaAlgorithm'', Fitness: 1.00]\n    [294 Answer: ''Genetic Algorithm'']\n\n### Association Analysis\n    $ python mlfromscratch/examples/apriori.py\n    +-------------+\n    |   Apriori   |\n    +-------------+\n    Minimum Support: 0.25\n    Minimum Confidence: 0.8\n    Transactions:\n        [1, 2, 3, 4]\n        [1, 2, 4]\n        [1, 2]\n        [2, 3, 4]\n        [2, 3]\n        [3, 4]\n        [2, 4]\n    Frequent Itemsets:\n        [1, 2, 3, 4, [1, 2], [1, 4], [2, 3], [2, 4], [3, 4], [1, 2, 4], [2, 3, 4]]\n    Rules:\n        1 -> 2 (support: 0.43, confidence: 1.0)\n        4 -> 2 (support: 0.57, confidence: 0.8)\n        [1, 4] -> 2 (support: 0.29, confidence: 1.0)\n\n\n## Implementations\n### Supervised Learning\n- [Adaboost](mlfromscratch/supervised_learning/adaboost.py)\n- [Bayesian Regression](mlfromscratch/supervised_learning/bayesian_regression.py)\n- [Decision Tree](mlfromscratch/supervised_learning/decision_tree.py)\n- [Elastic Net](mlfromscratch/supervised_learning/regression.py)\n- [Gradient Boosting](mlfromscratch/supervised_learning/gradient_boosting.py)\n- [K Nearest Neighbors](mlfromscratch/supervised_learning/k_nearest_neighbors.py)\n- [Lasso Regression](mlfromscratch/supervised_learning/regression.py)\n- [Linear Discriminant Analysis](mlfromscratch/supervised_learning/linear_discriminant_analysis.py)\n- [Linear Regression](mlfromscratch/supervised_learning/regression.py)\n- [Logistic Regression](mlfromscratch/supervised_learning/logistic_regression.py)\n- [Multi-class Linear Discriminant Analysis](mlfromscratch/supervised_learning/multi_class_lda.py)\n- [Multilayer Perceptron](mlfromscratch/supervised_learning/multilayer_perceptron.py)\n- [Naive Bayes](mlfromscratch/supervised_learning/naive_bayes.py)\n- [Neuroevolution](mlfromscratch/supervised_learning/neuroevolution.py)\n- [Particle Swarm Optimization of Neural Network](mlfromscratch/supervised_learning/particle_swarm_optimization.py)\n- [Perceptron](mlfromscratch/supervised_learning/perceptron.py)\n- [Polynomial Regression](mlfromscratch/supervised_learning/regression.py)\n- [Random Forest](mlfromscratch/supervised_learning/random_forest.py)\n- [Ridge Regression](mlfromscratch/supervised_learning/regression.py)\n- [Support Vector Machine](mlfromscratch/supervised_learning/support_vector_machine.py)\n- [XGBoost](mlfromscratch/supervised_learning/xgboost.py)\n\n### Unsupervised Learning\n- [Apriori](mlfromscratch/unsupervised_learning/apriori.py)\n- [Autoencoder](mlfromscratch/unsupervised_learning/autoencoder.py)\n- [DBSCAN](mlfromscratch/unsupervised_learning/dbscan.py)\n- [FP-Growth](mlfromscratch/unsupervised_learning/fp_growth.py)\n- [Gaussian Mixture Model](mlfromscratch/unsupervised_learning/gaussian_mixture_model.py)\n- [Generative Adversarial Network](mlfromscratch/unsupervised_learning/generative_adversarial_network.py)\n- [Genetic Algorithm](mlfromscratch/unsupervised_learning/genetic_algorithm.py)\n- [K-Means](mlfromscratch/unsupervised_learning/k_means.py)\n- [Partitioning Around Medoids](mlfromscratch/unsupervised_learning/partitioning_around_medoids.py)\n- [Principal Component Analysis](mlfromscratch/unsupervised_learning/principal_component_analysis.py)\n- [Restricted Boltzmann Machine](mlfromscratch/unsupervised_learning/restricted_boltzmann_machine.py)\n\n### Reinforcement Learning\n- [Deep Q-Network](mlfromscratch/reinforcement_learning/deep_q_network.py)\n\n### Deep Learning\n  + [Neural Network](mlfromscratch/deep_learning/neural_network.py)\n  + [Layers](mlfromscratch/deep_learning/layers.py)\n    * Activation Layer\n    * Average Pooling Layer\n    * Batch Normalization Layer\n    * Constant Padding Layer\n    * Convolutional Layer\n    * Dropout Layer\n    * Flatten Layer\n    * Fully-Connected (Dense) Layer\n    * Fully-Connected RNN Layer\n    * Max Pooling Layer\n    * Reshape Layer\n    * Up Sampling Layer\n    * Zero Padding Layer\n  + Model Types\n    * [Convolutional Neural Network](mlfromscratch/examples/convolutional_neural_network.py)\n    * [Multilayer Perceptron](mlfromscratch/examples/multilayer_perceptron.py)\n    * [Recurrent Neural Network](mlfromscratch/examples/recurrent_neural_network.py)\n\n## Contact\nIf there''s some implementation you would like to see here or if you''re just feeling social,\nfeel free to [email](mailto:eriklindernoren@gmail.com) me or connect with me on [LinkedIn](https://www.linkedin.com/in/eriklindernoren/).\n', '{"language":"Python","stars":29764,"forks":5055,"watchers":29764,"open_issues":68,"topics":["data-mining","data-science","deep-learning","deep-reinforcement-learning","genetic-algorithm","machine-learning","machine-learning-from-scratch"],"default_branch":"master","size_kb":553,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:eriklindernoren:ML-From-Scratch","source_url":"https://github.com/eriklindernoren/ML-From-Scratch"}]', NULL, 'MIT', 'approved', 80, 'cbc0ae71030004da1e92540e114f9703', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-eriklindernoren-ML-From-Scratch from https://github.com/eriklindernoren.png
Image converted to WebP: data/images/github-eriklindernoren-ML-From-Scratch.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ashishpatel26-500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code', 'github--ashishpatel26--500-ai-machine-learning-deep-learning-computer-vision-nlp-projects-with-code', '500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code', 'ashishpatel26', '***500 AI Machine learning Deep learning Computer vision NLP Projects with code* !!!** Follow me on LinkedIn : ***This list is continuously updated.*** - You can take pull requests and contribute. All Links are tested and working fine. Please ping if any link doesn''t work | Sr No | Name | Link | | ----- | ------------------------------------------------------------ | ------------------------------------------------------------ | | 1 | 365 Days Computer Vision Learning | ğŸ‘† | | 2 | 125+ NLP La...', '["artificial-intelligence","artificial-intelligence-projects","awesome","computer-vision","computer-vision-project","data-science","deep-learning","deep-learning-project","machine-learning","machine-learning-projects","nlp","nlp-projects","python"]', 'other', 29464, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ashishpatel26/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '## 500 + ğ—”ğ—¿ğ˜ğ—¶ğ—³ğ—¶ğ—°ğ—¶ğ—®ğ—¹ ğ—œğ—»ğ˜ğ—²ğ—¹ğ—¹ğ—¶ğ—´ğ—²ğ—»ğ—°ğ—² ğ—£ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜ ğ—Ÿğ—¶ğ˜€ğ˜ ğ˜„ğ—¶ğ˜ğ—µ ğ—°ğ—¼ğ—±ğ—²\n\n***500 AI Machine learning Deep learning Computer vision NLP Projects with code* !!!**\n\n![](https://raw.githubusercontent.com/ashishpatel26/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code/main/images/Colorful%20Futuristic%20Technology%20Poster.gif)\n\nFollow me on LinkedIn : [![](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/ashishpatel2604/)\n\n***This list is continuously updated.*** - You can take pull requests and contribute. All Links are tested and working fine. Please ping if any link doesn''t work\n\n| Sr No | Name                                                         | Link                                                         |\n| ----- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| 1     | 365 Days Computer Vision Learning                            | [ğŸ‘†](https://github.com/ashishpatel26/365-Days-Computer-Vision-Learning-Linkedin-Post) |\n| 2     | 125+ NLP Language Models Treasure of Transformers            | [ğŸ‘†](https://github.com/ashishpatel26/Treasure-of-Transformers) |\n| 3     | Andrew NG ML notes                                           | [ğŸ‘†](https://github.com/ashishpatel26/Andrew-NG-Notes)        |\n| 4     | 10 Machine Learning Projects on Time Series Forecasting      | [ğŸ‘†](https://medium.com/coders-camp/10-machine-learning-projects-on-time-seri%20es-forecasting-ee0368420ccd) |\n| 5     | 20 Deep Learning Projects Solved and Explained with Python   | [ğŸ‘†](https://thecleverprogrammer.com/2020/11/22/deep-learning-projects-with-python/) |\n| 6     | 20 Machine learning Project                                  | [ğŸ‘†](https://amankharwal.medium.com/20-machine-learning-projects-for-portfolio-81e3dbd167b1) |\n| 7     | 30 Python Project Solved and Explained                       | [ğŸ‘†](https://amankharwal.medium.com/30-python-projects-solved-and-explained-563fd7473003) |\n| 8     | Machine learning Course for Free                             | [ğŸ‘†](https://thecleverprogrammer.com/2020/09/24/machine-learning-course/) |\n| 9     | 5 Web Scraping Projects with Python                          | [ğŸ‘†](https://amankharwal.medium.com/5-web-scraping-projects-with-python-4bcc25ff039) |\n| 10    | 20 Machine Learning Projects on Future Prediction with Python | [ğŸ‘†](https://amankharwal.medium.com/20-machine-learning-projects-on-future-prediction-with-python-93932d9a7f7f) |\n| 11    | 4 Chatbot Project With Python                                | [ğŸ‘†](https://amankharwal.medium.com/4-chatbot-projects-with-python-5b32fd84af37) |\n| 12    | 7 Python Gui project                                         | [ğŸ‘†](https://amankharwal.medium.com/7-python-gui-projects-for-beginners-87ae2c695d78) |\n| 13    | All Unsupervised learning Projects                           | [ğŸ‘†](https://amankharwal.medium.com/all-unsupervised-machine-learning-algorithms-explained-aecf1ba95d8b) |\n| 14    | 10 Machine learning Projects for Regression Analysis         | [ğŸ‘†](https://amankharwal.medium.com/10-machine-learning-projects-on-regression-with-python-e5494615a0d0) |\n| 15    | 10 Machine learning Project for Classification with Python   | [ğŸ‘†](https://medium.datadriveninvestor.com/10-machine-learning-projects-on-classification-with-python-9261add2e8a7) |\n| 16    | 6 Sentimental Analysis Projects with python                  | [ğŸ‘†](https://amankharwal.medium.com/6-sentiment-analysis-projects-with-python-1fdd3d43d90f) |\n| 17    | 4 Recommendations Projects with Python                       | [ğŸ‘†](https://medium.com/coders-camp/4-recommendation-system-projects-with-python-5934de32ba7d) |\n| 18    | 20 Deep learning Project with python                         | [ğŸ‘†](https://medium.com/coders-camp/20-deep-learning-projects-with-python-3c56f7e6a721) |\n| 19    | 5 COVID19 Projects with Python                               | [ğŸ‘†](https://amankharwal.medium.com/5-covid-19-projects-with-python-and-machine-learning-63d51cde96e2) |\n| 20    | 9 Computer Vision Project with python                        | [ğŸ‘†](https://becominghuman.ai/computer-vision-projects-with-python-ecfac58ded18) |\n| 21    | 8 Neural Network Project with python                         | [ğŸ‘†](https://medium.datadriveninvestor.com/8-neural-networks-projects-solved-and-explained-a4f142bc10c) |\n| 22    | 5 Machine learning Project for healthcare                    | [ğŸ‘†](https://medium.datadriveninvestor.com/5-machine-learning-projects-for-healthcare-bbd0eac57b4a) |\n| 23    | 5 NLP Project with Python                                    | [ğŸ‘†](https://medium.datadriveninvestor.com/5-nlp-projects-for-machine-learning-72d3234381d4) |\n| 24    | 47 Machine Learning Projects for 2021                        | [ğŸ‘†](https://data-flair.training/blogs/machine-learning-project-ideas/) |\n| 25    | 19 Artificial Intelligence Projects for 2021                 | [ğŸ‘†](https://data-flair.training/blogs/artificial-intelligence-project-ideas/) |\n| 26    | 28 Machine learning Projects for 2021                        | [ğŸ‘†](https://data-flair.training/blogs/machine-learning-project-ideas/) |\n| 27    | 16 Data Science Projects with Source Code for 2021           | [ğŸ‘†](https://data-flair.training/blogs/data-science-project-ideas/) |\n| 28    | 23 Deep learning Projects with Source Code for 2021          | [ğŸ‘†](https://data-flair.training/blogs/deep-learning-project-ideas/) |\n| 29    | 25 Computer Vision Projects with Source Code for 2021        | [ğŸ‘†](https://data-flair.training/blogs/computer-vision-project-ideas/) |\n| 30    | 23 Iot Projects with Source Code for 2021                    | [ğŸ‘†](https://data-flair.training/blogs/iot-project-ideas/)    |\n| 31    | 27 Django Projects with Source Code for 2021                 | [ğŸ‘†](https://data-flair.training/blogs/django-project-ideas/) |\n| 32    | 37 Python Fun Projects with Code for 2021                    | [ğŸ‘†](https://data-flair.training/blogs/python-project-ideas/) |\n| 33    | 500 + Top Deep learning Codes                                | [ğŸ‘†](https://github.com/aymericdamien/TopDeepLearning)        |\n| 34    | 500 + Machine learning Codes                                 | [ğŸ‘†](https://github.com/josephmisiti/awesome-machine-learning) |\n| 35    | 20+ Machine Learning Datasets & Project Ideas                | [ğŸ‘†](https://www.kdnuggets.com/2020/03/20-machine-learning-datasets-project-ideas.html) |\n| 36    | 1000+ Computer vision codes                                  | [ğŸ‘†](https://github.com/jbhuang0604/awesome-computer-vision)  |\n| 37    | 300 + Industry wise Real world projects with code            | [ğŸ‘†](https://github.com/ashishpatel26/Real-time-ML-Project)   |\n| 38    | 1000 + Python Project Codes                                  | [ğŸ‘†](https://github.com/vinta/awesome-python)                 |\n| 39    | 363 + NLP Project with Code                                  | [ğŸ‘†](https://github.com/fighting41love/funNLP)                |\n| 40    | 50 + Code ML Models (For iOS 11) Projects                    | [ğŸ‘†](https://github.com/likedan/Awesome-CoreML-Models)        |\n| 41    | 360+ Pretrained Model Projects for Image, text, Audio and Video | [ğŸ‘†](https://github.com/PaddlePaddle/PaddleHub)               |\n| 42    | 50 + Graph Classification Project List                       | [ğŸ‘†](https://github.com/benedekrozemberczki/awesome-graph-classification) |\n| 43    | 100 + Sentence Embedding(NLP Resources)                      | [ğŸ‘†](https://github.com/Separius/awesome-sentence-embedding)  |\n| 44    | 100 + Production Machine learning Projects                   | [ğŸ‘†](https://github.com/EthicalML/awesome-production-machine-learning) |\n| 45    | 300 + Machine Learning Resources Collection                  | [ğŸ‘†](https://github.com/ujjwalkarn/Machine-Learning-Tutorials) |\n| 46    | 70 + Awesome AI                                              | [ğŸ‘†](https://github.com/NirantK/awesome-project-ideas)        |\n| 47    | 150 + Machine learning Project Ideas with code               | [ğŸ‘†](https://github.com/src-d/awesome-machine-learning-on-source-code) |\n| 48    | 100 + AutoML Projects with code                              | [ğŸ‘†](https://github.com/hibayesian/awesome-automl-papers)     |\n| 49    | 100 + Machine Learning Model Interpretability Code Frameworks | [ğŸ‘†](https://github.com/jphall663/awesome-machine-learning-interpretability) |\n| 50    | 120 + Multi Model Machine learning Code Projects             | [ğŸ‘†](https://github.com/pliang279/awesome-multimodal-ml)      |\n| 51    | Awesome Chatbot Projects                                     | [ğŸ‘†](https://github.com/fendouai/Awesome-Chatbot)             |\n| 52    | Awesome ML Demo Project with iOS                             | [ğŸ‘†](https://github.com/tucan9389/awesome-ml-demos-with-ios)  |\n| 53    | 100 + Python based Machine learning Application Projects     | [ğŸ‘†](https://github.com/prateekiiest/Code-Sleep-Python)       |\n| 54    | 100 + Reproducible Research Projects of ML and DL            | [ğŸ‘†](https://github.com/leipzig/awesome-reproducible-research) |\n| 55    | 25 + Python Projects                                         | [ğŸ‘†](https://github.com/saadhaxxan/Awesome-Python-Projects)   |\n| 56    | 8 + OpenCV Projects                                          | [ğŸ‘†](https://github.com/moadmmh/Awesome-OpenCV)               |\n| 57    | 1000 + Awesome Deep learning Collection                      | [ğŸ‘†](https://github.com/ChristosChristofidis/awesome-deep-learning) |\n| 58    | 200 + Awesome NLP learning Collection                        | [ğŸ‘†](https://github.com/keon/awesome-nlp)                     |\n| 59    | 200 + The Super Duper NLP Repo                               | [ğŸ‘†](https://notebooks.quantumstat.com/)                      |\n| 60    | 100 + NLP dataset for your Projects                          | [ğŸ‘†](https://index.quantumstat.com/#dataset)                  |\n| 61    | 364 + Machine Learning Projects definition                   | [ğŸ‘†](https://projectworlds.in/free-projects/machine-learning-projects-with-source-code/) |\n| 62    | 300+ Google Earth Engine Jupyter Notebooks to Analyze Geospatial Data | [ğŸ‘†](https://github.com/giswqs/earthengine-py-notebooks)      |\n| 63    | 1000 + Machine learning Projects Information                 | [ğŸ‘†](https://1000projects.org/projects/machine-learning-projects) |\n| 64.   | 11 Computer Vision Projects with code                        | [ğŸ‘†](https://github.com/akshaybhatia10/ComputerVision-Projects) |\n| 65.   | 13 Computer Vision Projects with Code                        | [ğŸ‘†](https://github.com/anuragreddygv323/computer-vision-projects) |\n| 66.   | 13 Cool Computer Vision GitHub Projects To Inspire You       | [ğŸ‘†](https://machinelearningknowledge.ai/cool-computer-vision-github-projects-to-inspire-you/) |\n| 67.   | Open-Source Computer Vision Projects (With Tutorials)        | [ğŸ‘†](https://www.theclickreader.com/open-source-computer-vision-projects-with-tutorials/) |\n| 68.   | OpenCV Computer Vision Projects with Python                  | [ğŸ‘†](https://github.com/PacktPublishing/OpenCV-Computer-Vision-Projects-with-Python) |\n| 69.   | 100 + Computer vision Algorithm Implementation               | [ğŸ‘†](https://github.com/gmalivenko/awesome-computer-vision-models) |\n| 70.   | 80 + Computer vision Learning code                           | [ğŸ‘†](https://github.com/spmallick/learnopencv)                |\n| 71.   | Deep learning Treasure                                       | [ğŸ‘†](https://github.com/kmario23/deep-learning-drizzle)       |\n| 72    | Data Analysis and Machine learning Projects                  | [ğŸ‘†](https://github.com/rhiever/Data-Analysis-and-Machine-Learning-Projects) |\n| 73    | AI Projects                                                  | [ğŸ‘†](https://github.com/StevenLei2017/AI_projects)            |\n| 74    | Kaggle projects collection                                   | [ğŸ‘†](https://github.com/alexattia/Data-Science-Projects)      |\n| 75    | Unique AI projects                                           | [ğŸ‘†](https://github.com/robsalgado/personal_data_science_projects) |\n| 76    | Data Science Project Collection                              | [ğŸ‘†](https://github.com/tuangauss/DataScienceProjects)        |\n| 77    | Advance Data Science Projects                                | [ğŸ‘†](https://github.com/TheCodex-Me/Projects)                 |\n| 78    | Deep and Machine learning Projects                           | [ğŸ‘†](https://github.com/nitinkaushik01/Deep_and_Machine_Learning_Projects) |\n| 79    | Data Science Projects kaggle                                 | [ğŸ‘†](https://github.com/alexattia/Data-Science-Projects)      |\n| 80    | Auto Deep learning Project                                   | [ğŸ‘†](https://github.com/D-X-Y/AutoDL-Projects)                |\n| 81    | 180 Machine learning Project                                 | [ğŸ‘†](https://medium.com/coders-camp/180-data-science-and-machine-learning-projects-with-python-6191bc7b9db9) |\n| 82    | Amazing Hackthon Project Collection                          | [ğŸ‘†](https://github.com/analyticsindiamagazine/MachineHack/tree/master/Hackathon_Solutions) |\n| 83    | Awesome NLP Project Ideas                                    | [ğŸ‘†](https://nirantk.com/awesome-project-ideas/)              |\n| 84    | 12 NLP Projects                                              | [ğŸ‘†](https://github.com/gaoisbest/NLP-Projects)               |\n| 85    | Advance NLP Projects                                         | [ğŸ‘†](https://github.com/PacktPublishing/Advanced-NLP-Projects-with-TensorFlow-2.0) |\n| 86    | 6 Amazing NLP Projects                                       | [ğŸ‘†](https://github.com/anujvyas/Natural-Language-Processing-Projects) |\n| 87    | NLP Beginner Projects                                        | [ğŸ‘†](https://github.com/positivepeng/nlp-beginner-projects)   |\n| 88    | Paper with Code by PwC Collection                            | [ğŸ‘†](https://github.com/zziz/pwc)                             |\n| 89    | SOTA Models(State of the Art Results)                        | [ğŸ‘†](https://github.com/RedditSota/state-of-the-art-result-for-machine-learning-problems) |\n| 90    | Best AI Papers                                               | [ğŸ‘†](https://github.com/louisfb01/Best_AI_paper_2020)         |\n| 91    | Generative Adversarial nets                                  | [ğŸ‘†](https://github.com/zhangqianhui/AdversarialNetsPapers)   |\n| 92    | Computer Vision Paper with Code                              | [ğŸ‘†](https://github.com/DWCTOD/CVPR2022-Papers-with-Code-Demo) |\n| 93    | NILMS Paper with code                                        | [ğŸ‘†](https://github.com/klemenjak/nilm-papers-with-code)      |\n| 94    | 3D Computer Vision Research Projects                         | [ğŸ‘†](https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-vision) |\n| 95    | NLP and Computer Vision Project Collection                   | [ğŸ‘†](https://github.com/NELSONZHAO/zhihu)                     |\n| 96    | Udacity Collection of Computer Vision Projects               | [ğŸ‘†](https://github.com/Bjarten/computer-vision-ND)           |\n| 97    | Zero to Hero Tensorflow Tutorial                             | [ğŸ‘†](https://github.com/mrdbourke/tensorflow-deep-learning)   |\n| 98    | Deep learning in Production                                  | [ğŸ‘†](https://github.com/The-AI-Summer/Deep-Learning-In-Production) |\n| 99    | GANs Collection                                              | [ğŸ‘†](https://github.com/The-AI-Summer/GANs-in-Computer-Vision) |\n| 100   | Time Series Projects Code                                    | [ğŸ‘†](https://github.com/deshpandenu/Time-Series-Forecasting-of-Amazon-Stock-Prices-using-Neural-Networks-LSTM-and-GAN-) |\n| 101   | 12 Machine learning Object Detection                         | [ğŸ‘†](https://amankharwal.medium.com/12-machine-learning-projects-on-object-detection-46b32adc3c37) |\n| 102   | 20 NLP Project with Python                                   | [ğŸ‘†](https://medium.com/coders-camp/20-machine-learning-projects-on-nlp-582effe73b9c) |\n| 103   | Learning Material for Deep Learning, ML, Computer Vision and NLP   | [ğŸ‘†](https://github.com/kmario23/deep-learning-drizzle) |\n***More Projects list is coming...!!!***\n\n---\n\n', '{"language":null,"stars":29464,"forks":6527,"watchers":29464,"open_issues":59,"topics":["artificial-intelligence","artificial-intelligence-projects","awesome","computer-vision","computer-vision-project","data-science","deep-learning","deep-learning-project","machine-learning","machine-learning-projects","nlp","nlp-projects","python"],"default_branch":"main","size_kb":879,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:ashishpatel26:365-Days-Computer-Vision-Learning-Linkedin-Post","source_url":"https://github.com/ashishpatel26/365-Days-Computer-Vision-Learning-Linkedin-Post"},{"type":"has_code","target_id":"github:ashishpatel26:Treasure-of-Transformers","source_url":"https://github.com/ashishpatel26/Treasure-of-Transformers"},{"type":"has_code","target_id":"github:ashishpatel26:Andrew-NG-Notes","source_url":"https://github.com/ashishpatel26/Andrew-NG-Notes"},{"type":"has_code","target_id":"github:aymericdamien:TopDeepLearning","source_url":"https://github.com/aymericdamien/TopDeepLearning"},{"type":"has_code","target_id":"github:josephmisiti:awesome-machine-learning","source_url":"https://github.com/josephmisiti/awesome-machine-learning"},{"type":"has_code","target_id":"github:jbhuang0604:awesome-computer-vision","source_url":"https://github.com/jbhuang0604/awesome-computer-vision"},{"type":"has_code","target_id":"github:ashishpatel26:Real-time-ML-Project","source_url":"https://github.com/ashishpatel26/Real-time-ML-Project"},{"type":"has_code","target_id":"github:vinta:awesome-python","source_url":"https://github.com/vinta/awesome-python"},{"type":"has_code","target_id":"github:fighting41love:funNLP","source_url":"https://github.com/fighting41love/funNLP"},{"type":"has_code","target_id":"github:likedan:Awesome-CoreML-Models","source_url":"https://github.com/likedan/Awesome-CoreML-Models"},{"type":"has_code","target_id":"github:PaddlePaddle:PaddleHub","source_url":"https://github.com/PaddlePaddle/PaddleHub"},{"type":"has_code","target_id":"github:benedekrozemberczki:awesome-graph-classification","source_url":"https://github.com/benedekrozemberczki/awesome-graph-classification"},{"type":"has_code","target_id":"github:Separius:awesome-sentence-embedding","source_url":"https://github.com/Separius/awesome-sentence-embedding"},{"type":"has_code","target_id":"github:EthicalML:awesome-production-machine-learning","source_url":"https://github.com/EthicalML/awesome-production-machine-learning"},{"type":"has_code","target_id":"github:ujjwalkarn:Machine-Learning-Tutorials","source_url":"https://github.com/ujjwalkarn/Machine-Learning-Tutorials"},{"type":"has_code","target_id":"github:NirantK:awesome-project-ideas","source_url":"https://github.com/NirantK/awesome-project-ideas"},{"type":"has_code","target_id":"github:src-d:awesome-machine-learning-on-source-code","source_url":"https://github.com/src-d/awesome-machine-learning-on-source-code"},{"type":"has_code","target_id":"github:hibayesian:awesome-automl-papers","source_url":"https://github.com/hibayesian/awesome-automl-papers"},{"type":"has_code","target_id":"github:jphall663:awesome-machine-learning-interpretability","source_url":"https://github.com/jphall663/awesome-machine-learning-interpretability"},{"type":"has_code","target_id":"github:pliang279:awesome-multimodal-ml","source_url":"https://github.com/pliang279/awesome-multimodal-ml"},{"type":"has_code","target_id":"github:fendouai:Awesome-Chatbot","source_url":"https://github.com/fendouai/Awesome-Chatbot"},{"type":"has_code","target_id":"github:tucan9389:awesome-ml-demos-with-ios","source_url":"https://github.com/tucan9389/awesome-ml-demos-with-ios"},{"type":"has_code","target_id":"github:prateekiiest:Code-Sleep-Python","source_url":"https://github.com/prateekiiest/Code-Sleep-Python"},{"type":"has_code","target_id":"github:leipzig:awesome-reproducible-research","source_url":"https://github.com/leipzig/awesome-reproducible-research"},{"type":"has_code","target_id":"github:saadhaxxan:Awesome-Python-Projects","source_url":"https://github.com/saadhaxxan/Awesome-Python-Projects"},{"type":"has_code","target_id":"github:moadmmh:Awesome-OpenCV","source_url":"https://github.com/moadmmh/Awesome-OpenCV"},{"type":"has_code","target_id":"github:ChristosChristofidis:awesome-deep-learning","source_url":"https://github.com/ChristosChristofidis/awesome-deep-learning"},{"type":"has_code","target_id":"github:keon:awesome-nlp","source_url":"https://github.com/keon/awesome-nlp"},{"type":"has_code","target_id":"github:giswqs:earthengine-py-notebooks","source_url":"https://github.com/giswqs/earthengine-py-notebooks"},{"type":"has_code","target_id":"github:akshaybhatia10:ComputerVision-Projects","source_url":"https://github.com/akshaybhatia10/ComputerVision-Projects"},{"type":"has_code","target_id":"github:anuragreddygv323:computer-vision-projects","source_url":"https://github.com/anuragreddygv323/computer-vision-projects"},{"type":"has_code","target_id":"github:PacktPublishing:OpenCV-Computer-Vision-Projects-with-Python","source_url":"https://github.com/PacktPublishing/OpenCV-Computer-Vision-Projects-with-Python"},{"type":"has_code","target_id":"github:gmalivenko:awesome-computer-vision-models","source_url":"https://github.com/gmalivenko/awesome-computer-vision-models"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:kmario23:deep-learning-drizzle","source_url":"https://github.com/kmario23/deep-learning-drizzle"},{"type":"has_code","target_id":"github:rhiever:Data-Analysis-and-Machine-Learning-Projects","source_url":"https://github.com/rhiever/Data-Analysis-and-Machine-Learning-Projects"},{"type":"has_code","target_id":"github:StevenLei2017:AI_projects","source_url":"https://github.com/StevenLei2017/AI_projects"},{"type":"has_code","target_id":"github:alexattia:Data-Science-Projects","source_url":"https://github.com/alexattia/Data-Science-Projects"},{"type":"has_code","target_id":"github:robsalgado:personal_data_science_projects","source_url":"https://github.com/robsalgado/personal_data_science_projects"},{"type":"has_code","target_id":"github:tuangauss:DataScienceProjects","source_url":"https://github.com/tuangauss/DataScienceProjects"},{"type":"has_code","target_id":"github:TheCodex-Me:Projects","source_url":"https://github.com/TheCodex-Me/Projects"},{"type":"has_code","target_id":"github:nitinkaushik01:Deep_and_Machine_Learning_Projects","source_url":"https://github.com/nitinkaushik01/Deep_and_Machine_Learning_Projects"},{"type":"has_code","target_id":"github:alexattia:Data-Science-Projects","source_url":"https://github.com/alexattia/Data-Science-Projects"},{"type":"has_code","target_id":"github:D-X-Y:AutoDL-Projects","source_url":"https://github.com/D-X-Y/AutoDL-Projects"},{"type":"has_code","target_id":"github:analyticsindiamagazine:MachineHack","source_url":"https://github.com/analyticsindiamagazine/MachineHack"},{"type":"has_code","target_id":"github:gaoisbest:NLP-Projects","source_url":"https://github.com/gaoisbest/NLP-Projects"},{"type":"has_code","target_id":"github:PacktPublishing:Advanced-NLP-Projects-with-TensorFlow-2.0","source_url":"https://github.com/PacktPublishing/Advanced-NLP-Projects-with-TensorFlow-2.0"},{"type":"has_code","target_id":"github:anujvyas:Natural-Language-Processing-Projects","source_url":"https://github.com/anujvyas/Natural-Language-Processing-Projects"},{"type":"has_code","target_id":"github:positivepeng:nlp-beginner-projects","source_url":"https://github.com/positivepeng/nlp-beginner-projects"},{"type":"has_code","target_id":"github:zziz:pwc","source_url":"https://github.com/zziz/pwc"},{"type":"has_code","target_id":"github:RedditSota:state-of-the-art-result-for-machine-learning-problems","source_url":"https://github.com/RedditSota/state-of-the-art-result-for-machine-learning-problems"},{"type":"has_code","target_id":"github:louisfb01:Best_AI_paper_2020","source_url":"https://github.com/louisfb01/Best_AI_paper_2020"},{"type":"has_code","target_id":"github:zhangqianhui:AdversarialNetsPapers","source_url":"https://github.com/zhangqianhui/AdversarialNetsPapers"},{"type":"has_code","target_id":"github:DWCTOD:CVPR2022-Papers-with-Code-Demo","source_url":"https://github.com/DWCTOD/CVPR2022-Papers-with-Code-Demo"},{"type":"has_code","target_id":"github:klemenjak:nilm-papers-with-code","source_url":"https://github.com/klemenjak/nilm-papers-with-code"},{"type":"has_code","target_id":"github:Tom-Hardy-3D-Vision-Workshop:awesome-3D-vision","source_url":"https://github.com/Tom-Hardy-3D-Vision-Workshop/awesome-3D-vision"},{"type":"has_code","target_id":"github:NELSONZHAO:zhihu","source_url":"https://github.com/NELSONZHAO/zhihu"},{"type":"has_code","target_id":"github:Bjarten:computer-vision-ND","source_url":"https://github.com/Bjarten/computer-vision-ND"},{"type":"has_code","target_id":"github:mrdbourke:tensorflow-deep-learning","source_url":"https://github.com/mrdbourke/tensorflow-deep-learning"},{"type":"has_code","target_id":"github:The-AI-Summer:Deep-Learning-In-Production","source_url":"https://github.com/The-AI-Summer/Deep-Learning-In-Production"},{"type":"has_code","target_id":"github:The-AI-Summer:GANs-in-Computer-Vision","source_url":"https://github.com/The-AI-Summer/GANs-in-Computer-Vision"},{"type":"has_code","target_id":"github:deshpandenu:Time-Series-Forecasting-of-Amazon-Stock-Prices-using-Neural-Networks-LSTM-and-GAN-","source_url":"https://github.com/deshpandenu/Time-Series-Forecasting-of-Amazon-Stock-Prices-using-Neural-Networks-LSTM-and-GAN-"},{"type":"has_code","target_id":"github:kmario23:deep-learning-drizzle","source_url":"https://github.com/kmario23/deep-learning-drizzle"}]', NULL, NULL, 'pending', 70, '98811fcd8b39ade4742dddc3e16be02d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ashishpatel26-500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code from https://github.com/ashishpatel26.png
Image converted to WebP: data/images/github-ashishpatel26-500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-donnemartin-data-science-ipython-notebooks', 'github--donnemartin--data-science-ipython-notebooks', 'data-science-ipython-notebooks', 'donnemartin', '<br/> <p align="center"> <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/README_1200x800.gif"> </p> <p align="center"> <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/coversmall_alt.png"> <br/> </p> * deep-learning * tensorflow * theano * keras * caffe * scikit-learn * statistical-inference-scipy * pandas * matplotlib * numpy * python-data * kaggle-and-business-analyses * spark * mapreduc...', '["aws","big-data","caffe","data-science","deep-learning","hadoop","kaggle","keras","machine-learning","mapreduce","matplotlib","numpy","pandas","python","scikit-learn","scipy","spark","tensorflow","theano","python"]', 'other', 28737, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/donnemartin/data-science-ipython-notebooks","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/README_1200x800.gif">\n</p>\n\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/coversmall_alt.png">\n  <br/>\n</p>\n\n# data-science-ipython-notebooks\n\n## Index\n\n* [deep-learning](#deep-learning)\n    * [tensorflow](#tensor-flow-tutorials)\n    * [theano](#theano-tutorials)\n    * [keras](#keras-tutorials)\n    * [caffe](#deep-learning-misc)\n* [scikit-learn](#scikit-learn)\n* [statistical-inference-scipy](#statistical-inference-scipy)\n* [pandas](#pandas)\n* [matplotlib](#matplotlib)\n* [numpy](#numpy)\n* [python-data](#python-data)\n* [kaggle-and-business-analyses](#kaggle-and-business-analyses)\n* [spark](#spark)\n* [mapreduce-python](#mapreduce-python)\n* [amazon web services](#aws)\n* [command lines](#commands)\n* [misc](#misc)\n* [notebook-installation](#notebook-installation)\n* [credits](#credits)\n* [contributing](#contributing)\n* [contact-info](#contact-info)\n* [license](#license)\n\n<br/>\n<p align="center">\n  <img src="http://i.imgur.com/ZhKXrKZ.png">\n</p>\n\n## deep-learning\n\nIPython Notebook(s) demonstrating deep learning functionality.\n\n<br/>\n<p align="center">\n  <img src="https://avatars0.githubusercontent.com/u/15658638?v=3&s=100">\n</p>\n\n### tensor-flow-tutorials\n\nAdditional TensorFlow tutorials:\n\n* [pkmital/tensorflow_tutorials](https://github.com/pkmital/tensorflow_tutorials)\n* [nlintz/TensorFlow-Tutorials](https://github.com/nlintz/TensorFlow-Tutorials)\n* [alrojo/tensorflow-tutorial](https://github.com/alrojo/tensorflow-tutorial)\n* [BinRoot/TensorFlow-Book](https://github.com/BinRoot/TensorFlow-Book)\n* [tuanavu/tensorflow-basic-tutorials](https://github.com/tuanavu/tensorflow-basic-tutorials)\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [tsf-basics](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/1_intro/basic_operations.ipynb) | Learn basic operations in TensorFlow, a library for various kinds of perceptual and language understanding tasks from Google. |\n| [tsf-linear](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/2_basic_classifiers/linear_regression.ipynb) | Implement linear regression in TensorFlow. |\n| [tsf-logistic](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/2_basic_classifiers/logistic_regression.ipynb) | Implement logistic regression in TensorFlow. |\n| [tsf-nn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/2_basic_classifiers/nearest_neighbor.ipynb) | Implement nearest neighboars in TensorFlow. |\n| [tsf-alex](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/alexnet.ipynb) | Implement AlexNet in TensorFlow. |\n| [tsf-cnn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/convolutional_network.ipynb) | Implement convolutional neural networks in TensorFlow. |\n| [tsf-mlp](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/multilayer_perceptron.ipynb) | Implement multilayer perceptrons in TensorFlow. |\n| [tsf-rnn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/recurrent_network.ipynb) | Implement recurrent neural networks in TensorFlow. |\n| [tsf-gpu](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/4_multi_gpu/multigpu_basics.ipynb) | Learn about basic multi-GPU computation in TensorFlow. |\n| [tsf-gviz](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/5_ui/graph_visualization.ipynb) | Learn about graph visualization in TensorFlow. |\n| [tsf-lviz](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-examples/notebooks/5_ui/loss_visualization.ipynb) | Learn about loss visualization in TensorFlow. |\n\n### tensor-flow-exercises\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [tsf-not-mnist](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/1_notmnist.ipynb) | Learn simple data curation by creating a pickle with formatted datasets for training, development and testing in TensorFlow. |\n| [tsf-fully-connected](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/2_fullyconnected.ipynb) | Progressively train deeper and more accurate models using logistic regression and neural networks in TensorFlow. |\n| [tsf-regularization](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/3_regularization.ipynb) | Explore regularization techniques by training fully connected networks to classify notMNIST characters in TensorFlow. |\n| [tsf-convolutions](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/4_convolutions.ipynb) | Create convolutional neural networks in TensorFlow. |\n| [tsf-word2vec](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/5_word2vec.ipynb) | Train a skip-gram model over Text8 data in TensorFlow. |\n| [tsf-lstm](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/tensor-flow-exercises/6_lstm.ipynb) | Train a LSTM character model over Text8 data in TensorFlow. |\n\n<br/>\n<p align="center">\n  <img src="http://www.deeplearning.net/software/theano/_static/theano_logo_allblue_200x46.png">\n</p>\n\n### theano-tutorials\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [theano-intro](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/intro_theano/intro_theano.ipynb) | Intro to Theano, which allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation. |\n| [theano-scan](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/scan_tutorial/scan_tutorial.ipynb) | Learn scans, a mechanism to perform loops in a Theano graph. |\n| [theano-logistic](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/intro_theano/logistic_regression.ipynb) | Implement logistic regression in Theano. |\n| [theano-rnn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/rnn_tutorial/simple_rnn.ipynb) | Implement recurrent neural networks in Theano. |\n| [theano-mlp](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/theano-tutorial/theano_mlp/theano_mlp.ipynb) | Implement multilayer perceptrons in Theano. |\n\n<br/>\n<p align="center">\n  <img src="http://i.imgur.com/L45Q8c2.jpg">\n</p>\n\n### keras-tutorials\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| keras | Keras is an open source neural network library written in Python. It is capable of running on top of either Tensorflow or Theano. |\n| [setup](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/0.%20Preamble.ipynb) | Learn about the tutorial goals and how to set up your Keras environment. |\n| [intro-deep-learning-ann](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/1.1%20Introduction%20-%20Deep%20Learning%20and%20ANN.ipynb) | Get an intro to deep learning with Keras and Artificial Neural Networks (ANN). |\n| [theano](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/1.2%20Introduction%20-%20Theano.ipynb) | Learn about Theano by working with weights matrices and gradients. |\n| [keras-otto](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/1.3%20Introduction%20-%20Keras.ipynb) | Learn about Keras by looking at the Kaggle Otto challenge. |\n| [ann-mnist](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/1.4%20%28Extra%29%20A%20Simple%20Implementation%20of%20ANN%20for%20MNIST.ipynb) | Review a simple implementation of ANN for MNIST using Keras. |\n| [conv-nets](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/2.1%20Supervised%20Learning%20-%20ConvNets.ipynb) | Learn about Convolutional Neural Networks (CNNs) with Keras. |\n| [conv-net-1](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/2.2.1%20Supervised%20Learning%20-%20ConvNet%20HandsOn%20Part%20I.ipynb) | Recognize handwritten digits from MNIST using Keras - Part 1. |\n| [conv-net-2](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/2.2.2%20Supervised%20Learning%20-%20ConvNet%20HandsOn%20Part%20II.ipynb) | Recognize handwritten digits from MNIST using Keras - Part 2. |\n| [keras-models](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/2.3%20Supervised%20Learning%20-%20Famous%20Models%20with%20Keras.ipynb) | Use pre-trained models such as VGG16, VGG19, ResNet50, and Inception v3 with Keras. |\n| [auto-encoders](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/3.1%20Unsupervised%20Learning%20-%20AutoEncoders%20and%20Embeddings.ipynb) | Learn about Autoencoders with Keras. |\n| [rnn-lstm](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/3.2%20RNN%20and%20LSTM.ipynb) | Learn about Recurrent Neural Networks (RNNs) with Keras. |\n| [lstm-sentence-gen](https://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/keras-tutorial/3.3%20%28Extra%29%20LSTM%20for%20Sentence%20Generation.ipynb) |  Learn about RNNs using Long Short Term Memory (LSTM) networks with Keras. |\n\n### deep-learning-misc\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [deep-dream](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/deep-learning/deep-dream/dream.ipynb) | Caffe-based computer vision program which uses a convolutional neural network to find and enhance patterns in images. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/scikitlearn.png">\n</p>\n\n## scikit-learn\n\nIPython Notebook(s) demonstrating scikit-learn functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [intro](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-intro.ipynb) | Intro notebook to scikit-learn.  Scikit-learn adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays. |\n| [knn](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-intro.ipynb#K-Nearest-Neighbors-Classifier) | Implement k-nearest neighbors in scikit-learn. |\n| [linear-reg](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-linear-reg.ipynb) | Implement linear regression in scikit-learn. |\n| [svm](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-svm.ipynb) | Implement support vector machine classifiers with and without kernels in scikit-learn. |\n| [random-forest](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-random-forest.ipynb) | Implement random forest classifiers and regressors in scikit-learn. |\n| [k-means](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-k-means.ipynb) | Implement k-means clustering in scikit-learn. |\n| [pca](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-pca.ipynb) | Implement principal component analysis in scikit-learn. |\n| [gmm](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-gmm.ipynb) | Implement Gaussian mixture models in scikit-learn. |\n| [validation](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scikit-learn/scikit-learn-validation.ipynb) | Implement validation and model selection in scikit-learn. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/scipy.png">\n</p>\n\n## statistical-inference-scipy\n\nIPython Notebook(s) demonstrating statistical inference with SciPy functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| scipy | SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data. |\n| [effect-size](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scipy/effect_size.ipynb) | Explore statistics that quantify effect size by analyzing the difference in height between men and women.  Uses data from the Behavioral Risk Factor Surveillance System (BRFSS) to estimate the mean and standard deviation of height for adult women and men in the United States. |\n| [sampling](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scipy/sampling.ipynb) | Explore random sampling by analyzing the average weight of men and women in the United States using BRFSS data. |\n| [hypothesis](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/scipy/hypothesis.ipynb) | Explore hypothesis testing by analyzing the difference of first-born babies compared with others. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/pandas.png">\n</p>\n\n## pandas\n\nIPython Notebook(s) demonstrating pandas functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [pandas](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/pandas.ipynb) | Software library written for data manipulation and analysis in Python. Offers data structures and operations for manipulating numerical tables and time series. |\n| [github-data-wrangling](https://github.com/donnemartin/viz/blob/master/githubstats/data_wrangling.ipynb) | Learn how to load, clean, merge, and feature engineer by analyzing GitHub data from the [`Viz`](https://github.com/donnemartin/viz) repo. |\n| [Introduction-to-Pandas](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.00-Introduction-to-Pandas.ipynb) | Introduction to Pandas. |\n| [Introducing-Pandas-Objects](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.01-Introducing-Pandas-Objects.ipynb) | Learn about Pandas objects. |\n| [Data Indexing and Selection](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.02-Data-Indexing-and-Selection.ipynb) | Learn about data indexing and selection in Pandas. |\n| [Operations-in-Pandas](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.03-Operations-in-Pandas.ipynb) | Learn about operating on data in Pandas. |\n| [Missing-Values](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.04-Missing-Values.ipynb) | Learn about handling missing data in Pandas. |\n| [Hierarchical-Indexing](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.05-Hierarchical-Indexing.ipynb) | Learn about hierarchical indexing in Pandas. |\n| [Concat-And-Append](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.06-Concat-And-Append.ipynb) | Learn about combining datasets: concat and append in Pandas. |\n| [Merge-and-Join](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.07-Merge-and-Join.ipynb) | Learn about combining datasets: merge and join in Pandas. |\n| [Aggregation-and-Grouping](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.08-Aggregation-and-Grouping.ipynb) | Learn about aggregation and grouping in Pandas. |\n| [Pivot-Tables](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.09-Pivot-Tables.ipynb) | Learn about pivot tables in Pandas. |\n| [Working-With-Strings](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.10-Working-With-Strings.ipynb) | Learn about vectorized string operations in Pandas. |\n| [Working-with-Time-Series](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.11-Working-with-Time-Series.ipynb) | Learn about working with time series in pandas. |\n| [Performance-Eval-and-Query](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/pandas/03.12-Performance-Eval-and-Query.ipynb) | Learn about high-performance Pandas: eval() and query() in Pandas. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/matplotlib.png">\n</p>\n\n## matplotlib\n\nIPython Notebook(s) demonstrating matplotlib functionality.\n\n| Notebook | Description |\n|-----------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [matplotlib](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/matplotlib.ipynb) | Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. |\n| [matplotlib-applied](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/matplotlib-applied.ipynb) | Apply matplotlib visualizations to Kaggle competitions for exploratory data analysis.  Learn how to create bar plots, histograms, subplot2grid, normalized plots, scatter plots, subplots, and kernel density estimation plots. |\n| [Introduction-To-Matplotlib](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.00-Introduction-To-Matplotlib.ipynb) | Introduction to Matplotlib. |\n| [Simple-Line-Plots](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.01-Simple-Line-Plots.ipynb) | Learn about simple line plots in Matplotlib. |\n| [Simple-Scatter-Plots](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.02-Simple-Scatter-Plots.ipynb) | Learn about simple scatter plots in Matplotlib. |\n| [Errorbars.ipynb](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.03-Errorbars.ipynb) | Learn about visualizing errors in Matplotlib. |\n| [Density-and-Contour-Plots](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.04-Density-and-Contour-Plots.ipynb) | Learn about density and contour plots in Matplotlib. |\n| [Histograms-and-Binnings](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.05-Histograms-and-Binnings.ipynb) | Learn about histograms, binnings, and density in Matplotlib. |\n| [Customizing-Legends](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.06-Customizing-Legends.ipynb) | Learn about customizing plot legends in Matplotlib. |\n| [Customizing-Colorbars](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.07-Customizing-Colorbars.ipynb) | Learn about customizing colorbars in Matplotlib. |\n| [Multiple-Subplots](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.08-Multiple-Subplots.ipynb) | Learn about multiple subplots in Matplotlib. |\n| [Text-and-Annotation](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.09-Text-and-Annotation.ipynb) | Learn about text and annotation in Matplotlib. |\n| [Customizing-Ticks](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.10-Customizing-Ticks.ipynb) | Learn about customizing ticks in Matplotlib. |\n| [Settings-and-Stylesheets](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.11-Settings-and-Stylesheets.ipynb) | Learn about customizing Matplotlib: configurations and stylesheets. |\n| [Three-Dimensional-Plotting](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.12-Three-Dimensional-Plotting.ipynb) | Learn about three-dimensional plotting in Matplotlib. |\n| [Geographic-Data-With-Basemap](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.13-Geographic-Data-With-Basemap.ipynb) | Learn about geographic data with basemap in Matplotlib. |\n| [Visualization-With-Seaborn](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/matplotlib/04.14-Visualization-With-Seaborn.ipynb) | Learn about visualization with Seaborn. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/numpy.png">\n</p>\n\n## numpy\n\nIPython Notebook(s) demonstrating NumPy functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [numpy](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/numpy.ipynb) | Adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays. |\n| [Introduction-to-NumPy](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.00-Introduction-to-NumPy.ipynb) | Introduction to NumPy. |\n| [Understanding-Data-Types](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.01-Understanding-Data-Types.ipynb) | Learn about data types in Python. |\n| [The-Basics-Of-NumPy-Arrays](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.02-The-Basics-Of-NumPy-Arrays.ipynb) | Learn about the basics of NumPy arrays. |\n| [Computation-on-arrays-ufuncs](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.03-Computation-on-arrays-ufuncs.ipynb) | Learn about computations on NumPy arrays: universal functions. |\n| [Computation-on-arrays-aggregates](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.04-Computation-on-arrays-aggregates.ipynb) | Learn about aggregations: min, max, and everything in between in NumPy. |\n| [Computation-on-arrays-broadcasting](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.05-Computation-on-arrays-broadcasting.ipynb) | Learn about computation on arrays: broadcasting in NumPy. |\n| [Boolean-Arrays-and-Masks](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.06-Boolean-Arrays-and-Masks.ipynb) | Learn about comparisons, masks, and boolean logic in NumPy. |\n| [Fancy-Indexing](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.07-Fancy-Indexing.ipynb) | Learn about fancy indexing in NumPy. |\n| [Sorting](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.08-Sorting.ipynb) | Learn about sorting arrays in NumPy. |\n| [Structured-Data-NumPy](http://nbviewer.jupyter.org/github/donnemartin/data-science-ipython-notebooks/blob/master/numpy/02.09-Structured-Data-NumPy.ipynb) | Learn about structured data: NumPy''s structured arrays. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/python.png">\n</p>\n\n## python-data\n\nIPython Notebook(s) demonstrating Python functionality geared towards data analysis.\n\n| Notebook | Description |\n|-----------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|\n| [data structures](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/structs.ipynb) | Learn Python basics with tuples, lists, dicts, sets. |\n| [data structure utilities](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/structs_utils.ipynb) | Learn Python operations such as slice, range, xrange, bisect, sort, sorted, reversed, enumerate, zip, list comprehensions. |\n| [functions](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/functions.ipynb) | Learn about more advanced Python features: Functions as objects, lambda functions, closures, *args, **kwargs currying, generators, generator expressions, itertools. |\n| [datetime](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/datetime.ipynb) | Learn how to work with Python dates and times: datetime, strftime, strptime, timedelta. |\n| [logging](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/logs.ipynb) | Learn about Python logging with RotatingFileHandler and TimedRotatingFileHandler. |\n| [pdb](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/pdb.ipynb) | Learn how to debug in Python with the interactive source code debugger. |\n| [unit tests](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/python-data/unit_tests.ipynb) | Learn how to test in Python with Nose unit tests. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/kaggle.png">\n</p>\n\n## kaggle-and-business-analyses\n\nIPython Notebook(s) used in [kaggle](https://www.kaggle.com/) competitions and business analyses.\n\n| Notebook | Description |\n|-------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n| [titanic](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/kaggle/titanic.ipynb) | Predict survival on the Titanic.  Learn data cleaning, exploratory data analysis, and machine learning. |\n| [churn-analysis](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/analyses/churn.ipynb) | Predict customer churn.  Exercise logistic regression, gradient boosting classifers, support vector machines, random forests, and k-nearest-neighbors.  Includes discussions of confusion matrices, ROC plots, feature importances, prediction probabilities, and calibration/descrimination.|\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/spark.png">\n</p>\n\n## spark\n\nIPython Notebook(s) demonstrating spark and HDFS functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|\n| [spark](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/spark/spark.ipynb) | In-memory cluster computing framework, up to 100 times faster for certain applications and is well suited for machine learning algorithms. |\n| [hdfs](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/spark/hdfs.ipynb) | Reliably stores very large files across machines in a large cluster. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/mrjob.png">\n</p>\n\n## mapreduce-python\n\nIPython Notebook(s) demonstrating Hadoop MapReduce with mrjob functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|\n| [mapreduce-python](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/mapreduce/mapreduce-python.ipynb) | Runs MapReduce jobs in Python, executing jobs locally or on Hadoop clusters. Demonstrates Hadoop Streaming in Python code with unit test and [mrjob](https://github.com/Yelp/mrjob) config file to analyze Amazon S3 bucket logs on Elastic MapReduce.  [Disco](https://github.com/discoproject/disco/) is another python-based alternative.|\n\n<br/>\n\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/aws.png">\n</p>\n\n## aws\n\nIPython Notebook(s) demonstrating Amazon Web Services (AWS) and AWS tools functionality.\n\n\nAlso check out:\n\n* [SAWS](https://github.com/donnemartin/saws): A Supercharged AWS command line interface (CLI).\n* [Awesome AWS](https://github.com/donnemartin/awesome-aws): A curated list of libraries, open source repos, guides, blogs, and other resources.\n\n| Notebook | Description |\n|------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [boto](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#Boto) | Official AWS SDK for Python. |\n| [s3cmd](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#s3cmd) | Interacts with S3 through the command line. |\n| [s3distcp](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#s3distcp) | Combines smaller files and aggregates them together by taking in a pattern and target file.  S3DistCp can also be used to transfer large volumes of data from S3 to your Hadoop cluster. |\n| [s3-parallel-put](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#s3-parallel-put) | Uploads multiple files to S3 in parallel. |\n| [redshift](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#redshift) | Acts as a fast data warehouse built on top of technology from massive parallel processing (MPP). |\n| [kinesis](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#kinesis) | Streams data in real time with the ability to process thousands of data streams per second. |\n| [lambda](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/aws/aws.ipynb#lambda) | Runs code in response to events, automatically managing compute resources. |\n\n<br/>\n<p align="center">\n  <img src="https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/commands.png">\n</p>\n\n## commands\n\nIPython Notebook(s) demonstrating various command lines for Linux, Git, etc.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [linux](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/linux.ipynb) | Unix-like and mostly POSIX-compliant computer operating system.  Disk usage, splitting files, grep, sed, curl, viewing running processes, terminal syntax highlighting, and Vim.|\n| [anaconda](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#anaconda) | Distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing, that aims to simplify package management and deployment. |\n| [ipython notebook](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#ipython-notebook) | Web-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document. |\n| [git](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#git) | Distributed revision control system with an emphasis on speed, data integrity, and support for distributed, non-linear workflows. |\n| [ruby](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#ruby) | Used to interact with the AWS command line and for Jekyll, a blog framework that can be hosted on GitHub Pages. |\n| [jekyll](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#jekyll) | Simple, blog-aware, static site generator for personal, project, or organization sites.  Renders Markdown or Textile and Liquid templates, and produces a complete, static website ready to be served by Apache HTTP Server, Nginx or another web server. |\n| [pelican](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#pelican) | Python-based alternative to Jekyll. |\n| [django](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/commands/misc.ipynb#django) | High-level Python Web framework that encourages rapid development and clean, pragmatic design. It can be useful to share reports/analyses and for blogging. Lighter-weight alternatives include [Pyramid](https://github.com/Pylons/pyramid), [Flask](https://github.com/pallets/flask), [Tornado](https://github.com/tornadoweb/tornado), and [Bottle](https://github.com/bottlepy/bottle).\n\n## misc\n\nIPython Notebook(s) demonstrating miscellaneous functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [regex](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/misc/regex.ipynb) | Regular expression cheat sheet useful in data wrangling.|\n[algorithmia](http://nbviewer.ipython.org/github/donnemartin/data-science-ipython-notebooks/blob/master/misc/Algorithmia.ipynb) | Algorithmia is a marketplace for algorithms. This notebook showcases 4 different algorithms: Face Detection, Content Summarizer, Latent Dirichlet Allocation and Optical Character Recognition.|\n\n## notebook-installation\n\n### anaconda\n\nAnaconda is a free distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing that aims to simplify package management and deployment.\n\nFollow instructions to install [Anaconda](https://docs.continuum.io/anaconda/install) or the more lightweight [miniconda](http://conda.pydata.org/miniconda.html).\n\n### dev-setup\n\nFor detailed instructions, scripts, and tools to set up your development environment for data analysis, check out the [dev-setup](https://github.com/donnemartin/dev-setup) repo.\n\n### running-notebooks\n\nTo view interactive content or to modify elements within the IPython notebooks, you must first clone or download the repository then run the notebook.  More information on IPython Notebooks can be found [here.](http://ipython.org/notebook.html)\n\n    $ git clone https://github.com/donnemartin/data-science-ipython-notebooks.git\n    $ cd data-science-ipython-notebooks\n    $ jupyter notebook\n\nNotebooks tested with Python 2.7.x.\n\n## credits\n\n* [Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython](http://www.amazon.com/Python-Data-Analysis-Wrangling-IPython/dp/1449319793) by Wes McKinney\n* [PyCon 2015 Scikit-learn Tutorial](https://github.com/jakevdp/sklearn_pycon2015) by Jake VanderPlas\n* [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook) by Jake VanderPlas\n* [Parallel Machine Learning with scikit-learn and IPython](https://github.com/ogrisel/parallel_ml_tutorial) by Olivier Grisel\n* [Statistical Interference Using Computational Methods in Python](https://github.com/AllenDowney/CompStats) by Allen Downey\n* [TensorFlow Examples](https://github.com/aymericdamien/TensorFlow-Examples) by Aymeric Damien\n* [TensorFlow Tutorials](https://github.com/pkmital/tensorflow_tutorials) by Parag K Mital\n* [TensorFlow Tutorials](https://github.com/nlintz/TensorFlow-Tutorials) by Nathan Lintz\n* [TensorFlow Tutorials](https://github.com/alrojo/tensorflow-tutorial) by Alexander R Johansen\n* [TensorFlow Book](https://github.com/BinRoot/TensorFlow-Book) by Nishant Shukla\n* [Summer School 2015](https://github.com/mila-udem/summerschool2015) by mila-udem\n* [Keras tutorials](https://github.com/leriomaggio/deep-learning-keras-tensorflow) by Valerio Maggio\n* [Kaggle](https://www.kaggle.com/)\n* [Yhat Blog](http://blog.yhat.com/)\n\n## contributing\n\nContributions are welcome!  For bug reports or requests please [submit an issue](https://github.com/donnemartin/data-science-ipython-notebooks/issues).\n\n## contact-info\n\nFeel free to contact me to discuss any issues, questions, or comments.\n\n* Email: [donne.martin@gmail.com](mailto:donne.martin@gmail.com)\n* Twitter: [@donne_martin](https://twitter.com/donne_martin)\n* GitHub: [donnemartin](https://github.com/donnemartin)\n* LinkedIn: [donnemartin](https://www.linkedin.com/in/donnemartin)\n* Website: [donnemartin.com](http://donnemartin.com)\n\n## license\n\nThis repository contains a variety of content; some developed by Donne Martin, and some from third-parties.  The third-party content is distributed under the license provided by those parties.\n\nThe content developed by Donne Martin is distributed under the following license:\n\n*I am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).*\n\n    Copyright 2015 Donne Martin\n\n    Licensed under the Apache License, Version 2.0 (the "License");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an "AS IS" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n', '{"language":"Python","stars":28737,"forks":8042,"watchers":28737,"open_issues":40,"topics":["aws","big-data","caffe","data-science","deep-learning","hadoop","kaggle","keras","machine-learning","mapreduce","matplotlib","numpy","pandas","python","scikit-learn","scipy","spark","tensorflow","theano"],"default_branch":"master","size_kb":49025,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:pkmital:tensorflow_tutorials","source_url":"https://github.com/pkmital/tensorflow_tutorials"},{"type":"has_code","target_id":"github:nlintz:TensorFlow-Tutorials","source_url":"https://github.com/nlintz/TensorFlow-Tutorials"},{"type":"has_code","target_id":"github:alrojo:tensorflow-tutorial","source_url":"https://github.com/alrojo/tensorflow-tutorial"},{"type":"has_code","target_id":"github:BinRoot:TensorFlow-Book","source_url":"https://github.com/BinRoot/TensorFlow-Book"},{"type":"has_code","target_id":"github:tuanavu:tensorflow-basic-tutorials","source_url":"https://github.com/tuanavu/tensorflow-basic-tutorials"},{"type":"has_code","target_id":"github:donnemartin:viz","source_url":"https://github.com/donnemartin/viz"},{"type":"has_code","target_id":"github:donnemartin:viz","source_url":"https://github.com/donnemartin/viz"},{"type":"has_code","target_id":"github:Yelp:mrjob","source_url":"https://github.com/Yelp/mrjob"},{"type":"has_code","target_id":"github:discoproject:disco","source_url":"https://github.com/discoproject/disco"},{"type":"has_code","target_id":"github:donnemartin:saws","source_url":"https://github.com/donnemartin/saws"},{"type":"has_code","target_id":"github:donnemartin:awesome-aws","source_url":"https://github.com/donnemartin/awesome-aws"},{"type":"has_code","target_id":"github:Pylons:pyramid","source_url":"https://github.com/Pylons/pyramid"},{"type":"has_code","target_id":"github:pallets:flask","source_url":"https://github.com/pallets/flask"},{"type":"has_code","target_id":"github:tornadoweb:tornado","source_url":"https://github.com/tornadoweb/tornado"},{"type":"has_code","target_id":"github:bottlepy:bottle","source_url":"https://github.com/bottlepy/bottle"},{"type":"has_code","target_id":"github:donnemartin:dev-setup","source_url":"https://github.com/donnemartin/dev-setup"},{"type":"has_code","target_id":"github:donnemartin:data-science-ipython-notebooks.git","source_url":"https://github.com/donnemartin/data-science-ipython-notebooks.git"},{"type":"has_code","target_id":"github:jakevdp:sklearn_pycon2015","source_url":"https://github.com/jakevdp/sklearn_pycon2015"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:ogrisel:parallel_ml_tutorial","source_url":"https://github.com/ogrisel/parallel_ml_tutorial"},{"type":"has_code","target_id":"github:AllenDowney:CompStats","source_url":"https://github.com/AllenDowney/CompStats"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:pkmital:tensorflow_tutorials","source_url":"https://github.com/pkmital/tensorflow_tutorials"},{"type":"has_code","target_id":"github:nlintz:TensorFlow-Tutorials","source_url":"https://github.com/nlintz/TensorFlow-Tutorials"},{"type":"has_code","target_id":"github:alrojo:tensorflow-tutorial","source_url":"https://github.com/alrojo/tensorflow-tutorial"},{"type":"has_code","target_id":"github:BinRoot:TensorFlow-Book","source_url":"https://github.com/BinRoot/TensorFlow-Book"},{"type":"has_code","target_id":"github:mila-udem:summerschool2015","source_url":"https://github.com/mila-udem/summerschool2015"},{"type":"has_code","target_id":"github:leriomaggio:deep-learning-keras-tensorflow","source_url":"https://github.com/leriomaggio/deep-learning-keras-tensorflow"},{"type":"has_code","target_id":"github:donnemartin:data-science-ipython-notebooks","source_url":"https://github.com/donnemartin/data-science-ipython-notebooks"}]', NULL, 'NOASSERTION', 'approved', 80, '37dba8dda7f80f6172b4d2987fcc6473', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-donnemartin-data-science-ipython-notebooks from https://github.com/donnemartin.png
WebP conversion failed: The decoder for WebP does not support the color type `Rgb16`

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ZuzooVn-machine-learning-for-software-engineers', 'github--zuzoovn--machine-learning-for-software-engineers', 'machine-learning-for-software-engineers', 'ZuzooVn', '<p align="center"> <a href="https://github.com/ZuzooVn/machine-learning-for-software-engineers"> <img alt="Top-down learning path: Machine Learning for Software Engineers" src="https://img.shields.io/badge/Machine%20Learning-Software%20Engineers-blue.svg"> </a> <a href="https://github.com/ZuzooVn/machine-learning-for-software-engineers/stargazers"> <img alt="GitHub stars" src="https://img.shields.io/github/stars/ZuzooVn/machine-learning-for-software-engineers.svg"> </a> <a href="https://githu...', '["artificial-intelligence","deep-learning","machine-learning","machine-learning-algorithms","software-engineer"]', 'other', 28668, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ZuzooVn/machine-learning-for-software-engineers","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Top-down learning path: Machine Learning for Software Engineers\n\n<p align="center">\n  <a href="https://github.com/ZuzooVn/machine-learning-for-software-engineers">\n    <img alt="Top-down learning path: Machine Learning for Software Engineers" src="https://img.shields.io/badge/Machine%20Learning-Software%20Engineers-blue.svg">\n  </a>\n  <a href="https://github.com/ZuzooVn/machine-learning-for-software-engineers/stargazers">\n    <img alt="GitHub stars" src="https://img.shields.io/github/stars/ZuzooVn/machine-learning-for-software-engineers.svg">\n  </a>\n  <a href="https://github.com/ZuzooVn/machine-learning-for-software-engineers/network">\n    <img alt="GitHub forks" src="https://img.shields.io/github/forks/ZuzooVn/machine-learning-for-software-engineers.svg">\n  </a>\n</p>\n\nInspired by [Coding Interview University](https://github.com/jwasham/coding-interview-university).\n\nTranslations: [Brazilian Portuguese](https://github.com/ZuzooVn/machine-learning-for-software-engineers/blob/master/README-pt-BR.md) | [ä¸­æ–‡ç‰ˆæœ¬](https://github.com/ZuzooVn/machine-learning-for-software-engineers/blob/master/README-zh-CN.md) | [FranÃ§ais](https://github.com/ZuzooVn/machine-learning-for-software-engineers/blob/master/README-fr-FR.md) | [è‡ºç£è¯èªç‰ˆæœ¬](https://github.com/ZuzooVn/machine-learning-for-software-engineers/blob/master/README-zh-TW.md)\n\n[How I (Nam Vu) plan to become a machine learning engineer](https://www.codementor.io/zuzoovn/how-i-plan-to-become-a-machine-learning-engineer-a4metbcuk)\n\n## What is it?\n\nThis is my multi-month study plan for going from mobile developer (self-taught, no CS degree) to machine learning engineer.\n\nMy main goal was to find an approach to studying Machine Learning that is mainly hands-on and abstracts most of the Math for the beginner.\nThis approach is unconventional because itâ€™s the top-down and results-first approach designed for software engineers.\n\nPlease, feel free to make any contributions you feel will make it better.\n\n---\n\n## Table of Contents\n\n- [What is it?](#what-is-it)\n- [Why use it?](#why-use-it)\n- [How to use it](#how-to-use-it)\n- [Follow me](#follow-me)\n- [Don''t feel you aren''t smart enough](#dont-feel-you-arent-smart-enough)\n- [About Video Resources](#about-video-resources)\n- [Prerequisite Knowledge](#prerequisite-knowledge)\n- [The Daily Plan](#the-daily-plan)\n- [Motivation](#motivation)\n- [Machine learning overview](#machine-learning-overview)\n- [Machine learning mastery](#machine-learning-mastery)\n- [Machine learning is fun](#machine-learning-is-fun)\n- [Inky Machine Learning](#inky-machine-learning)\n- [Machine Learning: An In-Depth Guide](#machine-learning-an-in-depth-guide)\n- [Stories and experiences](#stories-and-experiences)\n- [Machine Learning Algorithms](#machine-learning-algorithms)\n- [Beginner Books](#beginner-books)\n- [Practical Books](#practical-books)\n- [Kaggle knowledge competitions](#kaggle-knowledge-competitions)\n- [Video Series](#video-series)\n- [MOOC](#mooc)\n- [Resources](#resources)\n- [Becoming an Open Source Contributor](#becoming-an-open-source-contributor)\n- [Games](#games)\n- [Podcasts](#podcasts)\n- [Communities](#communities)\n- [Conferences](#conferences)\n- [Interview Questions](#interview-questions)\n- [My admired companies](#my-admired-companies)\n\n---\n\n## Why use it?\n\nI''m following this plan to prepare for my near-future job: Machine learning engineer. I''ve been building native mobile applications (Android/iOS/Blackberry) since 2011. I have a Software Engineering degree, not a Computer Science degree. I have an itty-bitty amount of basic knowledge about: Calculus, Linear Algebra, Discrete Mathematics, Probability & Statistics from university.\nThink about my interest in machine learning:\n- [Can I learn and get a job in Machine Learning without studying CS Master and PhD?](https://www.quora.com/Can-I-learn-and-get-a-job-in-Machine-Learning-without-studying-CS-Master-and-PhD)\n    - *"You can, but it is far more difficult than when I got into the field."* [Drac Smith](https://www.quora.com/Can-I-learn-and-get-a-job-in-Machine-Learning-without-studying-CS-Master-and-PhD/answer/Drac-Smith?srid=oT0p)\n- [How do I get a job in Machine Learning as a software programmer who self-studies Machine Learning, but  never has a chance to use it at work?](https://www.quora.com/How-do-I-get-a-job-in-Machine-Learning-as-a-software-programmer-who-self-studies-Machine-Learning-but-never-has-a-chance-to-use-it-at-work)\n    - *"I''m hiring machine learning experts for my team and your MOOC will not get you the job (there is better news below). In fact, many people with a master''s in machine learning will not get the job because they (and most who have taken MOOCs) do not have a deep understanding that will help me solve my problems."* [Ross C. Taylor](https://www.quora.com/How-do-I-get-a-job-in-Machine-Learning-as-a-software-programmer-who-self-studies-Machine-Learning-but-never-has-a-chance-to-use-it-at-work/answer/Ross-C-Taylor?srid=oT0p)\n- [What skills are needed for machine learning jobs?](http://programmers.stackexchange.com/questions/79476/what-skills-are-needed-for-machine-learning-jobs)\n    - *"First, you need to have a decent CS/Math background. ML is an advanced topic so most textbooks assume that you have that background. Second, machine learning is a very general topic with many sub-specialties requiring unique skills. You may want to browse the curriculum of an MS program in Machine Learning to see the course, curriculum and textbook."* [Uri](http://softwareengineering.stackexchange.com/a/79717)\n    - *"Probability, distributed computing, and Statistics."* [Hydrangea](http://softwareengineering.stackexchange.com/a/79575)\n\nI find myself in times of trouble.\n\nAFAIK, [There are two sides to machine learning](http://machinelearningmastery.com/programmers-can-get-into-machine-learning/):\n- Practical Machine Learning: This is about querying databases, cleaning data, writing scripts to transform data and gluing algorithm and libraries together and writing custom code to squeeze reliable answers from data to satisfy difficult and ill-defined questions. Itâ€™s the mess of reality.\n- Theoretical Machine Learning: This is about math and abstraction and idealized scenarios and limits and beauty and informing what is possible. It is a whole lot neater and cleaner and removed from the mess of reality.\n\nI think the best way for practice-focused methodology is something like [''practice â€” learning â€” practice''](http://machinelearningmastery.com/machine-learning-for-programmers/#comment-358985), that means where students first come with some existing projects with problems and solutions (practice) to get familiar with traditional methods in the area and perhaps also with their methodology. After practicing with some elementary experiences, they can go into the books and study the underlying theory, which serves to guide their future advanced practice and will enhance their toolbox of solving practical problems. Studying theory also further improves their understanding on the elementary experiences, and will help them acquire advanced experiences more quickly.\n\n It''s a long plan. It''s going to take me years. If you are familiar with a lot of this already it will take you a lot less time.\n\n## How to use it\nEverything below is an outline, and you should tackle the items in order from top to bottom.\n\nI''m using Github''s special markdown flavor, including tasks lists to check progress.\n\n- [x] Create a new branch so you can check items like this, just put an x in the brackets: [x]\n\n[More about Github-flavored markdown](https://guides.github.com/features/mastering-markdown/#GitHub-flavored-markdown)\n\n## Follow me\nI''m a Vietnamese Software Engineer who is really passionate and wants to work in the USA.\n\nHow much did I work during this plan? Roughly 4 hours/night after a long, hard day at work.\n\nI''m on the journey.\n\n- Twitter: [@Nam Vu](https://twitter.com/zuzoovn)\n\n| ![Nam Vu - Top-down learning path: machine learning for software engineers](http://sv1.upsieutoc.com/2016/10/08/331f241c8da44d0c43e9324d55440db6.md.jpg)|\n|:---:|\n| USA as heck |\n\n## Don''t feel you aren''t smart enough\nI get discouraged from books and courses that tell me as soon as I open them that multivariate calculus, inferential statistics and linear algebra are prerequisites. I still donâ€™t know how to get startedâ€¦\n\n- [What if Iâ€™m Not Good at Mathematics](http://machinelearningmastery.com/what-if-im-not-good-at-mathematics/)\n- [5 Techniques To Understand Machine Learning Algorithms Without the Background in Mathematics](http://machinelearningmastery.com/techniques-to-understand-machine-learning-algorithms-without-the-background-in-mathematics/)\n- [How do I learn machine learning?](https://www.quora.com/Machine-Learning/How-do-I-learn-machine-learning-1)\n\n## About Video Resources\n\nSome videos are available only by enrolling in a Coursera or EdX class. It is free to do so, but sometimes the classes\nare no longer in session so you have to wait a couple of months, so you have no access. I''m going to be adding more videos\nfrom public sources and replacing the online course videos over time. I like using university lectures.\n\n## Prerequisite Knowledge\n\nThis short section consists of prerequisites/interesting info I wanted to learn before getting started on the daily plan.\n\n- [ ] [What is the difference between Data Analytics, Data Analysis, Data Mining, Data Science, Machine Learning, and Big Data?](https://www.quora.com/What-is-the-difference-between-Data-Analytics-Data-Analysis-Data-Mining-Data-Science-Machine-Learning-and-Big-Data-1)\n- [ ] [Learning How to Learn](https://www.coursera.org/learn/learning-how-to-learn)\n- [ ] [Donâ€™t Break The Chain](http://lifehacker.com/281626/jerry-seinfelds-productivity-secret)\n- [ ] [How to learn on your own](https://metacademy.org/roadmaps/rgrosse/learn_on_your_own)\n\n## The Daily Plan\n\nEach subject does not require a whole day to be able to understand it fully, and you can do multiple of these in a day.\n\nEach day I take one subject from the list below, read it cover to cover, take notes, do the exercises and write an implementation in Python or R.\n\n# Motivation\n- [ ] [Dream](https://www.youtube.com/watch?v=g-jwWYX7Jlo)\n\n## Machine learning overview\n- [ ] [A Visual Introduction to Machine Learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n- [ ] [Gentle Guide to Machine Learning](https://blog.monkeylearn.com/gentle-guide-to-machine-learning/)\n- [ ] [Introduction to Machine Learning for Developers](http://blog.algorithmia.com/introduction-machine-learning-developers/)\n- [ ] [Machine Learning basics for a newbie](https://www.analyticsvidhya.com/blog/2015/06/machine-learning-basics/)\n- [ ] [How do you explain Machine Learning and Data Mining to non Computer Science people?](https://www.quora.com/How-do-you-explain-Machine-Learning-and-Data-Mining-to-non-Computer-Science-people)\n- [ ] [Machine Learning: Under the hood. Blog post explains the principles of machine learning in layman terms. Simple and clear](https://georgemdallas.wordpress.com/2013/06/11/big-data-data-mining-and-machine-learning-under-the-hood/)\n- [ ] [What is machine learning, and how does it work?](https://www.youtube.com/watch?v=elojMnjn4kk&list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A&index=1)\n- [ ] [How to Become a Machine Learning Engineer?](https://www.scaler.com/blog/how-to-become-a-machine-learning-engineer/)\n- ~~[] [Deep Learning - A Non-Technical Introduction](http://www.slideshare.net/AlfredPong1/deep-learning-a-nontechnical-introduction-69385936)~~[removed]\n\n## Machine learning mastery\n- [ ] [The Machine Learning Mastery Method](http://machinelearningmastery.com/machine-learning-mastery-method/)\n- [ ] [Machine Learning for Programmers](http://machinelearningmastery.com/machine-learning-for-programmers/)\n- [ ] [Applied Machine Learning with Machine Learning Mastery](http://machinelearningmastery.com/start-here/)\n- [ ] [Python Machine Learning Mini-Course](http://machinelearningmastery.com/python-machine-learning-mini-course/)\n- [ ] [Machine Learning Algorithms Mini-Course](http://machinelearningmastery.com/machine-learning-algorithms-mini-course/)\n\n## Machine learning is fun\n- [ ] [Machine Learning is Fun!](https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471#.37ue6caww)\n- [ ] [Part 2: Using Machine Learning to generate Super Mario Maker levels](https://medium.com/@ageitgey/machine-learning-is-fun-part-2-a26a10b68df3#.kh7qgvp1b)\n- [ ] [Part 3: Deep Learning and Convolutional Neural Networks](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.44rhxy637)\n- [ ] [Part 4: Modern Face Recognition with Deep Learning](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78#.3rwmq0ddc)\n- [ ] [Part 5: Language Translation with Deep Learning and the Magic of Sequences](https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa#.wyfthap4c)\n- [ ] [Part 6: How to do Speech Recognition with Deep Learning](https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a#.lhr1nnpcy)\n- [ ] [Part 7: Abusing Generative Adversarial Networks to Make 8-bit Pixel Art](https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7)\n- [ ] [Part 8: How to Intentionally Trick Neural Networks](https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196)\n\n## [Inky Machine Learning](https://triskell.github.io/2016/11/15/Inky-Machine-Learning.html)\n- [ ] [Part 1: What is Machine Learning ?](https://triskell.github.io/2016/10/23/What-is-Machine-Learning.html)\n- [ ] [Part 2: Supervised Learning and Unsupervised Learning](https://triskell.github.io/2016/11/13/Supervised-Learning-and-Unsupervised-Learning.html)\n\n## Machine Learning: An In-Depth Guide\n- [ ] [Overview, goals, learning types, and algorithms](http://www.innoarchitech.com/machine-learning-an-in-depth-non-technical-guide/)\n- [ ] [Data selection, preparation, and modeling](http://www.innoarchitech.com/machine-learning-an-in-depth-non-technical-guide-part-2/)\n- [ ] [Model evaluation, validation, complexity, and improvement](http://www.innoarchitech.com/machine-learning-an-in-depth-non-technical-guide-part-3/)\n- [ ] [Model performance and error analysis](http://www.innoarchitech.com/machine-learning-an-in-depth-non-technical-guide-part-4/)\n- [ ] [Unsupervised learning, related fields, and machine learning in practice](http://www.innoarchitech.com/machine-learning-an-in-depth-non-technical-guide-part-5/)\n\n## Stories and experiences\n- [ ] [Machine Learning in a Week](https://medium.com/learning-new-stuff/machine-learning-in-a-week-a0da25d59850#.tk6ft2kcg)\n- [ ] [Machine Learning in a Year](https://medium.com/learning-new-stuff/machine-learning-in-a-year-cdb0b0ebd29c#.hhcb9fxk1)\n- [ ] [How I wrote my first Machine Learning program in 3 days](http://blog.adnansiddiqi.me/how-i-wrote-my-first-machine-learning-program-in-3-days/)\n- [ ] [Learning Path : Your mentor to become a machine learning expert](https://www.analyticsvidhya.com/learning-path-learn-machine-learning/)\n- [ ] [You Too Can Become a Machine Learning Rock Star! No PhD](https://backchannel.com/you-too-can-become-a-machine-learning-rock-star-no-phd-necessary-107a1624d96b#.g9p16ldp7)\n- [ ] How to become a Data Scientist in 6 months: A hackerâ€™s approach to career planning\n    - [Video](https://www.youtube.com/watch?v=rIofV14c0tc)\n    - [Slide](http://www.slideshare.net/TetianaIvanova2/how-to-become-a-data-scientist-in-6-months)\n- [ ] [5 Skills You Need to Become a Machine Learning Engineer](http://blog.udacity.com/2016/04/5-skills-you-need-to-become-a-machine-learning-engineer.html)\n- [ ] [Are you a self-taught machine learning engineer? If yes, how did you do it & how long did it take you?](https://www.quora.com/Are-you-a-self-taught-machine-learning-engineer-If-yes-how-did-you-do-it-how-long-did-it-take-you)\n- [ ] [How can one become a good machine learning engineer?](https://www.quora.com/How-can-one-become-a-good-machine-learning-engineer)\n- [ ] [A Learning Sabbatical focused on Machine Learning](http://karlrosaen.com/ml/)\n\n## Machine Learning Algorithms\n- [ ] [10 Machine Learning Algorithms Explained to an â€˜Army Soldierâ€™](https://www.analyticsvidhya.com/blog/2015/12/10-machine-learning-algorithms-explained-army-soldier/)\n- [ ] [Top 10 data mining algorithms in plain English](https://rayli.net/blog/data/top-10-data-mining-algorithms-in-plain-english/)\n- [ ] [10 Machine Learning Terms Explained in Simple English](http://blog.aylien.com/10-machine-learning-terms-explained-in-simple/)\n- [ ] [A Tour of Machine Learning Algorithms](http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/)\n- [ ] [The 10 Algorithms Machine Learning Engineers Need to Know](https://gab41.lab41.org/the-10-algorithms-machine-learning-engineers-need-to-know-f4bb63f5b2fa#.ofc7t2965)\n- [ ] [Comparing supervised learning algorithms](http://www.dataschool.io/comparing-supervised-learning-algorithms/)\n- [ ] [Machine Learning Algorithms: A collection of minimal and clean implementations of machine learning algorithms](https://github.com/rushter/MLAlgorithms)\n- [ ] [KNN Algorithm in Machine Learning](https://www.scaler.com/topics/what-is-knn-algorithm-in-machine-learning/)\n\n## Beginner Books\n- [ ] [Data Smart: Using Data Science to Transform Information into Insight 1st Edition](https://www.amazon.com/Data-Smart-Science-Transform-Information/dp/111866146X)\n- [ ] [Data Science for Business: What you need to know about data mining and dataÂ­ analytic-thinking](https://www.amazon.com/Data-Science-Business-Data-Analytic-Thinking/dp/1449361323/)\n- [ ] [Predictive Analytics: The Power to Predict Who Will Click, Buy, Lie, or Die](https://www.amazon.com/Predictive-Analytics-Power-Predict-Click/dp/1118356853)\n\n## Practical Books\n- [ ] [Machine Learning for Hackers](https://www.amazon.com/Machine-Learning-Hackers-Drew-Conway/dp/1449303714)\n    - [GitHub repository(R)](https://github.com/johnmyleswhite/ML_for_Hackers)\n    - [GitHub repository(Python)](https://github.com/carljv/Will_it_Python)\n- [ ] [Python Machine Learning](https://www.amazon.com/Python-Machine-Learning-Sebastian-Raschka-ebook/dp/B00YSILNL0)\n    - [GitHub repository](https://github.com/rasbt/python-machine-learning-book)\n- [ ] [Programming Collective Intelligence: Building Smart Web 2.0 Applications](https://www.amazon.com/Programming-Collective-Intelligence-Building-Applications-ebook/dp/B00F8QDZWG)\n- [ ] [Machine Learning: An Algorithmic Perspective, Second Edition](https://www.amazon.com/Machine-Learning-Algorithmic-Perspective-Recognition/dp/1466583282)\n    - [GitHub repository](https://github.com/alexsosn/MarslandMLAlgo)\n    - [Resource repository](http://seat.massey.ac.nz/personal/s.r.marsland/MLbook.html)\n- [ ] [Introduction to Machine Learning with Python: A Guide for Data Scientists](http://shop.oreilly.com/product/0636920030515.do)\n    - [GitHub repository](https://github.com/amueller/introduction_to_ml_with_python)\n- [ ] [Data Mining: Practical Machine Learning Tools and Techniques, Third Edition](https://www.amazon.com/Data-Mining-Practical-Techniques-Management/dp/0123748569)\n    - Teaching material\n        - [Slides for Chapters 1-5 (zip)](http://www.cs.waikato.ac.nz/ml/weka/Slides3rdEd_Ch1-5.zip)\n        - [Slides for Chapters 6-8 (zip)](http://www.cs.waikato.ac.nz/ml/weka/Slides3rdEd_Ch6-8.zip)\n- [ ] [Machine Learning in Action](https://www.amazon.com/Machine-Learning-Action-Peter-Harrington/dp/1617290181/)\n    - [GitHub repository](https://github.com/pbharrin/machinelearninginaction)\n- [ ] [Reactive Machine Learning Systems(MEAP)](https://www.manning.com/books/reactive-machine-learning-systems)\n    - [GitHub repository](https://github.com/jeffreyksmithjr/reactive-machine-learning-systems)\n- [ ] [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)\n    - [GitHub repository(R)](http://www-bcf.usc.edu/~gareth/ISL/code.html)\n    - [GitHub repository(Python)](https://github.com/JWarmenhoven/ISLR-python)\n    - [Videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/)\n- [ ] [Building Machine Learning Systems with Python](https://www.packtpub.com/big-data-and-business-intelligence/building-machine-learning-systems-python)\n    - [GitHub repository](https://github.com/luispedro/BuildingMachineLearningSystemsWithPython)\n- [ ] [Learning scikit-learn: Machine Learning in Python](https://www.packtpub.com/big-data-and-business-intelligence/learning-scikit-learn-machine-learning-python)\n    - [GitHub repository](https://github.com/gmonce/scikit-learn-book)\n- [ ] [Probabilistic Programming & Bayesian Methods for Hackers](https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)\n- [ ] [Probabilistic Graphical Models: Principles and Techniques](https://www.amazon.com/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193)\n- [ ] [Machine Learning: Hands-On for Developers and Technical Professionals](https://www.amazon.com/Machine-Learning-Hands-Developers-Professionals/dp/1118889061)\n    - [Machine Learning Hands-On for Developers and Technical Professionals review](https://blogs.msdn.microsoft.com/querysimon/2015/01/01/book-review-machine-learning-hands-on-for-developers-and-technical-professionals/)\n    - [GitHub repository](https://github.com/jasebell/mlbook)\n- [ ] [Learning from Data](https://www.amazon.com/Learning-Data-Yaser-S-Abu-Mostafa/dp/1600490069)\n    - [Online tutorials](https://work.caltech.edu/telecourse.html)\n- [ ] [Reinforcement Learning: An Introduction (2nd Edition)](https://webdocs.cs.ualberta.ca/~sutton/book/the-book-2nd.html)\n    - [GitHub repository](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction)\n- [ ] [Machine Learning with TensorFlow(MEAP)](https://www.manning.com/books/machine-learning-with-tensorflow)\n    - [GitHub repository](https://github.com/BinRoot/TensorFlow-Book)\n- [ ] [How Machine Learning Works (MEAP)](https://www.manning.com/books/how-machine-learning-works)\n    - [GitHub repository](https://github.com/Mostafa-Samir/How-Machine-Learning-Works)\n- [ ] [Succeeding with AI](https://www.manning.com/books/succeeding-with-ai)\n\n## Kaggle knowledge competitions\n- [ ] [Kaggle Competitions: How and where to begin?](https://www.analyticsvidhya.com/blog/2015/06/start-journey-kaggle/)\n- [ ] [How a Beginner Used Small Projects To Get Started in Machine Learning and Compete on Kaggle](http://machinelearningmastery.com/how-a-beginner-used-small-projects-to-get-started-in-machine-learning-and-compete-on-kaggle)\n- [ ] [Master Kaggle By Competing Consistently](http://machinelearningmastery.com/master-kaggle-by-competing-consistently/)\n\n## Video Series\n- [ ] [Machine Learning for Hackers](https://www.youtube.com/playlist?list=PL2-dafEMk2A4ut2pyv0fSIXqOzXtBGkLj)\n- [ ] [Fresh Machine Learning](https://www.youtube.com/playlist?list=PL2-dafEMk2A6Kc7pV6gHH-apBFxwFjKeY)\n- [ ] [Machine Learning Recipes with Josh Gordon](https://www.youtube.com/playlist?list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal)\n- [ ] [Everything You Need to know about Machine Learning in 30 Minutes or Less](https://vimeo.com/43547079)\n- [ ] [A Friendly Introduction to Machine Learning](https://www.youtube.com/watch?v=IpGxLWOIZy4)\n- [ ] [Nuts and Bolts of Applying Deep Learning - Andrew Ng](https://www.youtube.com/watch?v=F1ka6a13S9I)\n- [ ] BigML Webinar\n    - [Video](https://www.youtube.com/watch?list=PL1bKyu9GtNYHcjGa6ulrvRVcm1lAB8he3&v=W62ehrnOVqo)\n    - [Resources](https://bigml.com/releases)\n- [ ] [mathematicalmonk''s Machine Learning tutorials](https://www.youtube.com/playlist?list=PLD0F06AA0D2E8FFBA)\n- [ ] [Machine learning in Python with scikit-learn](https://www.youtube.com/playlist?list=PL5-da3qGB5ICeMbQuqbbCOQWcS6OYBr5A)\n    - [GitHub repository](https://github.com/justmarkham/scikit-learn-videos)\n    - [Blog](http://blog.kaggle.com/author/kevin-markham/)\n- [ ] [My playlist â€“ Top YouTube Videos on Machine Learning, Neural Network & Deep Learning](https://www.analyticsvidhya.com/blog/2015/07/top-youtube-videos-machine-learning-neural-network-deep-learning/)\n- [ ] [16 New Must Watch Tutorials, Courses on Machine Learning](https://www.analyticsvidhya.com/blog/2016/10/16-new-must-watch-tutorials-courses-on-machine-learning/)\n- [ ] [DeepLearning.TV](https://www.youtube.com/channel/UC9OeZkIwhzfv-_Cb7fCikLQ)\n- [ ] [Learning To See](https://www.youtube.com/playlist?list=PLiaHhY2iBX9ihLasvE8BKnS2Xg8AhY6iV)\n- [ ] [Neural networks class - UniversitÃ© de Sherbrooke](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH)\n- [ ] [21 Deep Learning Videos, Tutorials & Courses on Youtube from 2016](https://www.analyticsvidhya.com/blog/2016/12/21-deep-learning-videos-tutorials-courses-on-youtube-from-2016/)\n- [ ] [30 Top Videos, Tutorials & Courses on Machine Learning & Artificial Intelligence from 2016](https://www.analyticsvidhya.com/blog/2016/12/30-top-videos-tutorials-courses-on-machine-learning-artificial-intelligence-from-2016/)\n- [ ] [Practical Deep Learning For Coders](http://course.fast.ai/index.html)\n- [ ]  [Practical Deep Learning For Coders Version 2 (PyTorch)](http://forums.fast.ai/t/welcome-to-part-1-v2/5787)\n\n## MOOC\n- [ ] [Courseraâ€™s AI For Everyone](https://www.coursera.org/learn/ai-for-everyone)\n- [ ] [edX''s Introduction to Artificial Intelligence (AI)](https://www.edx.org/course/introduction-artificial-intelligence-ai-microsoft-dat263x)\n- [ ] [Udacityâ€™s Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120)\n    - [Udacity Intro to Machine Learning Review](http://hamelg.blogspot.com/2014/12/udacity-intro-to-machine-learning-review.html)\n- [ ] [Udacityâ€™s Supervised, Unsupervised & Reinforcement](https://www.udacity.com/course/machine-learning--ud262)\n- [ ] [Machine Learning Foundations: A Case Study Approach](https://www.coursera.org/learn/ml-foundations)\n- [ ] [Machine Learning & AI Foundations: Value Estimations](https://www.lynda.com/Data-Science-tutorials/Machine-Learning-Essential-Training-Value-Estimations/548594-2.html)\n- [ ] [Kaggle''s Hands-On Data Science Education](https://www.kaggle.com/learn/overview)\n- [ ] [Microsoft Professional Program for Artificial Intelligence](https://academy.microsoft.com/en-us/professional-program/tracks/artificial-intelligence/)\n- [ ] [Courseraâ€™s Machine Learning](https://www.coursera.org/learn/machine-learning)\n    - [Video only](https://www.youtube.com/playlist?list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW)\n    - [Coursera Machine Learning review](https://rayli.net/blog/data/coursera-machine-learning-review/)\n    - [Coursera: Machine Learning Roadmap](https://metacademy.org/roadmaps/cjrd/coursera_ml_supplement)\n- [ ] [Machine Learning Distilled](https://code.tutsplus.com/courses/machine-learning-distilled)\n- [ ] [BigML training](https://bigml.com/training)\n- [ ] [Courseraâ€™s Neural Networks for Machine Learning](https://www.coursera.org/learn/neural-networks)\n    - Taught by Geoffrey Hinton, a pioneer in the field of neural networks\n- [ ] [Machine Learningâ€Š-â€ŠCSâ€Š-â€ŠOxford University](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/)\n- [ ] [Creative Applications of Deep Learning with TensorFlow](https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow/info)\n- [ ] [Intro to Descriptive Statistics](https://www.udacity.com/course/intro-to-descriptive-statistics--ud827)\n- [ ] [Intro to Inferential Statistics](https://www.udacity.com/course/intro-to-inferential-statistics--ud201)\n- [ ] [6.S094: Deep Learning for Self-Driving Cars](http://selfdrivingcars.mit.edu/)\n- [ ] [6.S191: Introduction to Deep Learning](http://introtodeeplearning.com/index.html)\n- [ ] [Courseraâ€™s Deep Learning](https://www.coursera.org/specializations/deep-learning)\n\n## Resources\n- [ ] [Absolute Beginning into Machine Learning](https://hackernoon.com/absolute-beginning-into-machine-learning-e90ceda5a4bc)\n- [ ] [Learn Machine Learning in a Single Month](https://elitedatascience.com/machine-learning-masterclass)\n- [ ] [The Non-Technical Guide to Machine Learning & Artificial Intelligence](https://medium.com/@samdebrule/a-humans-guide-to-machine-learning-e179f43b67a0#.cpzf3a5c0)\n- [ ] [Programming Community Curated Resources for learning Machine Learning](https://hackr.io/tutorials/learn-machine-learning-ml)\n- [ ] [Best practices rule book for Machine Learning engineering from Google](http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf)\n- [ ] [Machine Learning for Software Engineers on Hacker News](https://news.ycombinator.com/item?id=12898718)\n- [ ] [Machine Learning for Developers](https://xyclade.github.io/MachineLearning/)\n- [ ] [Machine Learning for HumansğŸ¤–ğŸ‘¶](https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12)\n- [ ] [Machine Learning Advice for Developers](https://dev.to/thealexlavin/machine-learning-advice-for-developers)\n- [ ] [Machine Learning For Complete Beginners](http://pythonforengineers.com/machine-learning-for-complete-beginners/)\n- [ ] [Getting Started with Machine Learning: For absolute beginners and fifth graders](https://medium.com/@suffiyanz/getting-started-with-machine-learning-f15df1c283ea#.yjtiy7ei9)\n- [ ] [How to Learn Machine Learning: The Self-Starter Way](https://elitedatascience.com/learn-machine-learning)\n- [ ] [Machine Learning Self-study Resources](https://ragle.sanukcode.net/articles/machine-learning-self-study-resources/)\n- [ ] [Level-Up Your Machine Learning](https://metacademy.org/roadmaps/cjrd/level-up-your-ml)\n- [ ] [An Honest Guide to Machine Learning](https://medium.com/axiomzenteam/an-honest-guide-to-machine-learning-2f6d7a6df60e#.ib12a1yw5)\n- [ ] Enough Machine Learning to Make Hacker News Readable Again\n    - [Video](https://www.youtube.com/watch?v=O7IezJT9uSI)\n    - [Slide](https://speakerdeck.com/pycon2014/enough-machine-learning-to-make-hacker-news-readable-again-by-ned-jackson-lovely)\n- [ ] [Dive into Machine Learning](https://github.com/hangtwenty/dive-into-machine-learning)\n- [ ] [{Machine, Deep} Learning for software engineers](https://speakerdeck.com/pmigdal/machine-deep-learning-for-software-engineers)\n- [ ] [Deep Learning For Beginners](https://deeplearning4j.org/deeplearningforbeginners.html)\n- [ ] [Foundations for deep learning](https://github.com/pauli-space/foundations_for_deep_learning)\n- [ ] [Machine Learning Mindmap / Cheatsheet](https://github.com/dformoso/machine-learning-mindmap)\n- Machine Learning courses in Universities\n    - [ ] [Stanford](http://ai.stanford.edu/courses/)\n    - [ ] [Machine Learning Summer Schools](http://mlss.cc/)\n    - [ ] [Oxford](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/)\n    - [ ] [Cambridge](http://mlg.eng.cam.ac.uk/)\n- Flipboard Topics\n    - [Machine learning](https://flipboard.com/topic/machinelearning)\n    - [Deep learning](https://flipboard.com/topic/deeplearning)\n    - [Artificial Intelligence](https://flipboard.com/topic/artificialintelligence)\n- Medium Topics\n    - [Machine learning](https://medium.com/tag/machine-learning/latest)\n    - [Deep learning](https://medium.com/tag/deep-learning)\n    - [Artificial Intelligence](https://medium.com/tag/artificial-intelligence)\n- Monthly top 10 articles\n    - [Machine Learning](https://medium.mybridge.co/search?q=%22Machine%20Learning%22)\n    - [Algorithms](https://medium.mybridge.co/search?q=Algorithms)\n- [Comprehensive list of data science resources](http://www.datasciencecentral.com/group/resources/forum/topics/comprehensive-list-of-data-science-resources)\n- [DigitalMind''s Artificial Intelligence resources](http://blog.digitalmind.io/post/artificial-intelligence-resources)\n- [Awesome Machine Learning](https://github.com/josephmisiti/awesome-machine-learning)\n- [Awesome Graph Classification](https://github.com/benedekrozemberczki/awesome-graph-classification)\n- [Awesome Community Detection](https://github.com/benedekrozemberczki/awesome-community-detection)\n- [CreativeAi''s Machine Learning](http://www.creativeai.net/?cat%5B0%5D=machine-learning)\n- [Roadmap of Machine Learning](https://www.scaler.com/blog/machine-learning-roadmap/)\n- [Machine Learning Online Courses](https://classpert.com/machine-learning)\n\n## Games\n- [Halite: A.I. Coding Game](https://halite.io/)\n- [Vindinium: A.I. Programming Challenge](http://vindinium.org/)\n- [General Video Game AI Competition](http://www.gvgai.net/)\n- [Angry Birds AI Competition](https://aibirds.org/)\n- [The AI Games](http://theaigames.com/)\n- [Fighting Game AI Competition](http://www.ice.ci.ritsumei.ac.jp/~ftgaic/)\n- [CodeCup](http://www.codecup.nl/intro.php)\n- [Student StarCraft AI Tournament](http://sscaitournament.com/)\n- [AIIDE StarCraft AI Competition](http://www.cs.mun.ca/~dchurchill/starcraftaicomp/)\n- [CIG StarCraft AI Competition](https://sites.google.com/site/starcraftaic/)\n- [CodinGame - AI Bot Games](https://www.codingame.com/training/machine-learning)\n\n## Becoming an Open Source Contributor\n- [ ] [tensorflow/magenta: Magenta: Music and Art Generation with Machine Intelligence](https://github.com/tensorflow/magenta)\n- [ ] [tensorflow/tensorflow: Computation using data flow graphs for scalable machine learning](https://github.com/tensorflow/tensorflow)\n- [ ] [cmusatyalab/openface: Face recognition with deep neural networks.](https://github.com/cmusatyalab/openface)\n- [ ] [tensorflow/models/syntaxnet: Neural Models of Syntax.](https://github.com/tensorflow/models/tree/master/syntaxnet)\n\n## Podcasts\n- ### Podcasts for Beginners:\n    - [Talking Machines](http://www.thetalkingmachines.com/)\n    - [Linear Digressions](http://lineardigressions.com/)\n    - [Data Skeptic](http://dataskeptic.com/)\n    - [This Week in Machine Learning & AI](https://twimlai.com/)\n    - [Machine Learning Guide](http://ocdevel.com/podcasts/machine-learning)\n    \n- ### Interviews with ML Practitioners, Researchers and Kagglers about their Joureny\n    - [Chai Time Data Science](https://www.youtube.com/playlist?list=PLLvvXm0q8zUbiNdoIazGzlENMXvZ9bd3x), [Audio](http://anchor.fm/chaitimedatascience), [Writeups](https://sanyambhutani.com/tag/chaitimedatascience/)\n    - [Machine Learning for Beginners - Interviews](https://www.youtube.com/channel/UCdZ0GX-F3ULMKfxtyzSFbaw), [Audio](https://jayshah.buzzsprout.com/)\n\n- ### "More" advanced podcasts\n    - [Partially Derivative](http://partiallyderivative.com/)\n    - [Oâ€™Reilly Data Show](http://radar.oreilly.com/tag/oreilly-data-show-podcast)\n    - [Not So Standard Deviation](https://soundcloud.com/nssd-podcast)\n\n- ### Podcasts to think outside the box:\n    - [Data Stories](http://datastori.es/)\n    \n## Communities\n- Quora\n    - [Machine Learning](https://www.quora.com/topic/Machine-Learning)\n    - [Statistics](https://www.quora.com/topic/Statistics-academic-discipline)\n    - [Data Mining](https://www.quora.com/topic/Data-Mining)\n\n- Reddit\n    - [Machine Learning](https://www.reddit.com/r/machinelearning)\n    - [Computer Vision](https://www.reddit.com/r/computervision)\n    - [Natural Language](https://www.reddit.com/r/languagetechnology)\n    - [Data Science](https://www.reddit.com/r/datascience)\n    - [Big Data](https://www.reddit.com/r/bigdata)\n    - [Statistics](https://www.reddit.com/r/statistics)\n\n- [Data Tau](http://www.datatau.com/)\n\n- [Deep Learning News](http://news.startup.ml/)\n\n- [KDnuggets](http://www.kdnuggets.com/)\n\n## Conferences\n- Neural Information Processing Systems ([NIPS](https://nips.cc/))\n- International Conference on Learning Representations ([ICLR](http://www.iclr.cc/doku.php?id=ICLR2017:main&redirect=1))\n- Association for the Advancement of Artificial Intelligence ([AAAI](http://www.aaai.org/Conferences/AAAI/aaai17.php))\n- IEEE Conference on Computational Intelligence and Games ([CIG](http://www.ieee-cig.org/))\n- IEEE International Conference on Machine Learning and Applications ([ICMLA](http://www.icmla-conference.org/))\n- International Conference on Machine Learning ([ICML](https://2017.icml.cc/))\n- International Joint Conferences on Artificial Intelligence ([IJCAI](http://www.ijcai.org/))\n- Association for Computational Linguistics ([ACL](http://acl2017.org/))\n\n## Interview Questions\n- [ ] [How To Prepare For A Machine Learning Interview](http://blog.udacity.com/2016/05/prepare-machine-learning-interview.html)\n- [ ] [40 Interview Questions asked at Startups in Machine Learning / Data Science](https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science)\n- [ ] [21 Must-Know Data Science Interview Questions and Answers](http://www.kdnuggets.com/2016/02/21-data-science-interview-questions-answers.html)\n- [ ] [Top 50 Machine learning Interview questions & Answers](http://career.guru99.com/top-50-interview-questions-on-machine-learning/)\n- [ ] [Machine Learning Engineer interview questions](https://resources.workable.com/machine-learning-engineer-interview-questions)\n- [ ] [Popular Machine Learning Interview Questions](http://www.learn4master.com/machine-learning/popular-machine-learning-interview-questions)\n- [ ] [What are some common Machine Learning interview questions?](https://www.quora.com/What-are-some-common-Machine-Learning-interview-questions)\n- [ ] [What are the best interview questions to evaluate a machine learning researcher?](https://www.quora.com/What-are-the-best-interview-questions-to-evaluate-a-machine-learning-researcher)\n- [ ] [Collection of Machine Learning Interview Questions](http://analyticscosm.com/machine-learning-interview-questions-for-data-scientist-interview/)\n- [ ] [121 Essential Machine Learning Questions & Answers](https://elitedatascience.com/mlqa-reading-list)\n- [ ] [Minimum Viable Study Plan for Machine Learning Interviews](https://github.com/khangich/machine-learning-interview)\n\n## My admired companies\n- [ ] [ELSA - Your virtual pronunciation coach](https://www.elsanow.io/home)\n', '{"language":null,"stars":28668,"forks":6218,"watchers":28668,"open_issues":29,"topics":["artificial-intelligence","deep-learning","machine-learning","machine-learning-algorithms","software-engineer"],"default_branch":"master","size_kb":471,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:ZuzooVn:machine-learning-for-software-engineers\">","source_url":"https://github.com/ZuzooVn/machine-learning-for-software-engineers\">"},{"type":"has_code","target_id":"github:ZuzooVn:machine-learning-for-software-engineers","source_url":"https://github.com/ZuzooVn/machine-learning-for-software-engineers"},{"type":"has_code","target_id":"github:ZuzooVn:machine-learning-for-software-engineers","source_url":"https://github.com/ZuzooVn/machine-learning-for-software-engineers"},{"type":"has_code","target_id":"github:jwasham:coding-interview-university","source_url":"https://github.com/jwasham/coding-interview-university"},{"type":"has_code","target_id":"github:ZuzooVn:machine-learning-for-software-engineers","source_url":"https://github.com/ZuzooVn/machine-learning-for-software-engineers"},{"type":"has_code","target_id":"github:ZuzooVn:machine-learning-for-software-engineers","source_url":"https://github.com/ZuzooVn/machine-learning-for-software-engineers"},{"type":"has_code","target_id":"github:ZuzooVn:machine-learning-for-software-engineers","source_url":"https://github.com/ZuzooVn/machine-learning-for-software-engineers"},{"type":"has_code","target_id":"github:ZuzooVn:machine-learning-for-software-engineers","source_url":"https://github.com/ZuzooVn/machine-learning-for-software-engineers"},{"type":"has_code","target_id":"github:rushter:MLAlgorithms","source_url":"https://github.com/rushter/MLAlgorithms"},{"type":"has_code","target_id":"github:johnmyleswhite:ML_for_Hackers","source_url":"https://github.com/johnmyleswhite/ML_for_Hackers"},{"type":"has_code","target_id":"github:carljv:Will_it_Python","source_url":"https://github.com/carljv/Will_it_Python"},{"type":"has_code","target_id":"github:rasbt:python-machine-learning-book","source_url":"https://github.com/rasbt/python-machine-learning-book"},{"type":"has_code","target_id":"github:alexsosn:MarslandMLAlgo","source_url":"https://github.com/alexsosn/MarslandMLAlgo"},{"type":"has_code","target_id":"github:amueller:introduction_to_ml_with_python","source_url":"https://github.com/amueller/introduction_to_ml_with_python"},{"type":"has_code","target_id":"github:pbharrin:machinelearninginaction","source_url":"https://github.com/pbharrin/machinelearninginaction"},{"type":"has_code","target_id":"github:jeffreyksmithjr:reactive-machine-learning-systems","source_url":"https://github.com/jeffreyksmithjr/reactive-machine-learning-systems"},{"type":"has_code","target_id":"github:JWarmenhoven:ISLR-python","source_url":"https://github.com/JWarmenhoven/ISLR-python"},{"type":"has_code","target_id":"github:luispedro:BuildingMachineLearningSystemsWithPython","source_url":"https://github.com/luispedro/BuildingMachineLearningSystemsWithPython"},{"type":"has_code","target_id":"github:gmonce:scikit-learn-book","source_url":"https://github.com/gmonce/scikit-learn-book"},{"type":"has_code","target_id":"github:jasebell:mlbook","source_url":"https://github.com/jasebell/mlbook"},{"type":"has_code","target_id":"github:ShangtongZhang:reinforcement-learning-an-introduction","source_url":"https://github.com/ShangtongZhang/reinforcement-learning-an-introduction"},{"type":"has_code","target_id":"github:BinRoot:TensorFlow-Book","source_url":"https://github.com/BinRoot/TensorFlow-Book"},{"type":"has_code","target_id":"github:Mostafa-Samir:How-Machine-Learning-Works","source_url":"https://github.com/Mostafa-Samir/How-Machine-Learning-Works"},{"type":"has_code","target_id":"github:justmarkham:scikit-learn-videos","source_url":"https://github.com/justmarkham/scikit-learn-videos"},{"type":"has_code","target_id":"github:hangtwenty:dive-into-machine-learning","source_url":"https://github.com/hangtwenty/dive-into-machine-learning"},{"type":"has_code","target_id":"github:pauli-space:foundations_for_deep_learning","source_url":"https://github.com/pauli-space/foundations_for_deep_learning"},{"type":"has_code","target_id":"github:dformoso:machine-learning-mindmap","source_url":"https://github.com/dformoso/machine-learning-mindmap"},{"type":"has_code","target_id":"github:josephmisiti:awesome-machine-learning","source_url":"https://github.com/josephmisiti/awesome-machine-learning"},{"type":"has_code","target_id":"github:benedekrozemberczki:awesome-graph-classification","source_url":"https://github.com/benedekrozemberczki/awesome-graph-classification"},{"type":"has_code","target_id":"github:benedekrozemberczki:awesome-community-detection","source_url":"https://github.com/benedekrozemberczki/awesome-community-detection"},{"type":"has_code","target_id":"github:tensorflow:magenta","source_url":"https://github.com/tensorflow/magenta"},{"type":"has_code","target_id":"github:tensorflow:tensorflow","source_url":"https://github.com/tensorflow/tensorflow"},{"type":"has_code","target_id":"github:cmusatyalab:openface","source_url":"https://github.com/cmusatyalab/openface"},{"type":"has_code","target_id":"github:tensorflow:models","source_url":"https://github.com/tensorflow/models"},{"type":"has_code","target_id":"github:khangich:machine-learning-interview","source_url":"https://github.com/khangich/machine-learning-interview"}]', NULL, 'CC-BY-SA-4.0', 'approved', 80, '151b8eaf28270dd190eb17e1ff0fab5e', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ZuzooVn-machine-learning-for-software-engineers from https://github.com/ZuzooVn.png
Image converted to WebP: data/images/github-ZuzooVn-machine-learning-for-software-engineers.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-eugeneyan-applied-ml', 'github--eugeneyan--applied-ml', 'applied-ml', 'eugeneyan', 'Curated papers, articles, and blogs on **data science & machine learning in production**. âš™ï¸ !HitCount Figuring out how to implement your ML project? Learn how other organizations did it: - **How** the problem is framed ğŸ”(e.g., personalization as recsys vs. search vs. sequences) - **What** machine learning techniques worked âœ… (and sometimes, what didn''t âŒ) - **Why** it works, the science behind it with research, literature, and references ğŸ“‚ - **What** real-world results were achieved (so yo...', '["applied-data-science","applied-machine-learning","computer-vision","data-discovery","data-engineering","data-quality","data-science","deep-learning","machine-learning","natural-language-processing","production","recsys","reinforcement-learning","search"]', 'other', 28541, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/eugeneyan/applied-ml","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# applied-ml\nCurated papers, articles, and blogs on **data science & machine learning in production**. âš™ï¸\n\n[![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](./CONTRIBUTING.md) [![Summaries](https://img.shields.io/badge/summaries-in%20tweets-%2300acee.svg?style=flat)](https://twitter.com/eugeneyan/status/1350509546133811200) ![HitCount](http://hits.dwyl.com/eugeneyan/applied-ml.svg)\n\nFiguring out how to implement your ML project? Learn how other organizations did it:\n\n- **How** the problem is framed ğŸ”(e.g., personalization as recsys vs. search vs. sequences)\n- **What** machine learning techniques worked âœ… (and sometimes, what didn''t âŒ)\n- **Why** it works, the science behind it with research, literature, and references ğŸ“‚\n- **What** real-world results were achieved (so you can better assess ROI â°ğŸ’°ğŸ“ˆ)\n\nP.S., Want a summary of ML advancements? ğŸ‘‰[`ml-surveys`](https://github.com/eugeneyan/ml-surveys)\n\nP.P.S, Looking for guides and interviews on applying ML? ğŸ‘‰[`applyingML`](https://applyingml.com)\n\n**Table of Contents**\n\n1. [Data Quality](#data-quality)\n2. [Data Engineering](#data-engineering)\n3. [Data Discovery](#data-discovery)\n4. [Feature Stores](#feature-stores)\n5. [Classification](#classification)\n6. [Regression](#regression)\n7. [Forecasting](#forecasting)\n8. [Recommendation](#recommendation)\n9. [Search & Ranking](#search--ranking)\n10. [Embeddings](#embeddings)\n11. [Natural Language Processing](#natural-language-processing)\n12. [Sequence Modelling](#sequence-modelling)\n13. [Computer Vision](#computer-vision)\n14. [Reinforcement Learning](#reinforcement-learning)\n15. [Anomaly Detection](#anomaly-detection)\n16. [Graph](#graph)\n17. [Optimization](#optimization)\n18. [Information Extraction](#information-extraction)\n19. [Weak Supervision](#weak-supervision)\n20. [Generation](#generation)\n21. [Audio](#audio)\n22. [Privacy-Preserving Machine Learning](#privacy-preserving-machine-learning)\n23. [Validation and A/B Testing](#validation-and-ab-testing)\n24. [Model Management](#model-management)\n25. [Efficiency](#efficiency)\n26. [Ethics](#ethics)\n27. [Infra](#infra)\n28. [MLOps Platforms](#mlops-platforms)\n29. [Practices](#practices)\n30. [Team Structure](#team-structure)\n31. [Fails](#fails)\n\n## Data Quality\n1. [Reliable and Scalable Data Ingestion at Airbnb](https://www.slideshare.net/HadoopSummit/reliable-and-scalable-data-ingestion-at-airbnb-63920989) `Airbnb` `2016`\n2. [Monitoring Data Quality at Scale with Statistical Modeling](https://eng.uber.com/monitoring-data-quality-at-scale/) `Uber` `2017`\n3. [Data Management Challenges in Production Machine Learning](https://research.google/pubs/pub46178/) ([Paper](https://thodrek.github.io/CS839_spring18/papers/p1723-polyzotis.pdf)) `Google` `2017`\n4. [Automating Large-Scale Data Quality Verification](https://www.amazon.science/publications/automating-large-scale-data-quality-verification) ([Paper](https://assets.amazon.science/a6/88/ad858ee240c38c6e9dce128250c0/automating-large-scale-data-quality-verification.pdf))`Amazon` `2018`\n5. [Meet Hodor â€” Gojekâ€™s Upstream Data Quality Tool](https://www.gojek.io/blog/meet-hodor-gojeks-upstream-data-quality-tool) `Gojek` `2019`\n6. [Data Validation for Machine Learning](https://research.google/pubs/pub47967/) ([Paper](https://mlsys.org/Conferences/2019/doc/2019/167.pdf)) `Google` `2019`\n6. [An Approach to Data Quality for Netflix Personalization Systems](https://www.youtube.com/watch?v=t7vHpA39TXM) `Netflix` `2020`\n7. [Improving Accuracy By Certainty Estimation of Human Decisions, Labels, and Raters](https://research.fb.com/blog/2020/08/improving-the-accuracy-of-community-standards-enforcement-by-certainty-estimation-of-human-decisions/) ([Paper](https://research.fb.com/wp-content/uploads/2020/08/CLARA-Confidence-of-Labels-and-Raters.pdf)) `Facebook` `2020`\n\n## Data Engineering\n1. [Zipline: Airbnbâ€™s Machine Learning Data Management Platform](https://www.youtube.com/watch?v=Tg5VEMEsC-0) `Airbnb` `2018`\n2. [Sputnik: Airbnbâ€™s Apache Spark Framework for Data Engineering](https://www.youtube.com/watch?v=BQumogSBsUw) `Airbnb` `2020`\n3. [Unbundling Data Science Workflows with Metaflow and AWS Step Functions](https://netflixtechblog.com/unbundling-data-science-workflows-with-metaflow-and-aws-step-functions-d454780c6280) `Netflix` `2020`\n4. [How DoorDash is Scaling its Data Platform to Delight Customers and Meet Growing Demand](https://doordash.engineering/2020/09/25/how-doordash-is-scaling-its-data-platform/) `DoorDash` `2020`\n5. [Revolutionizing Money Movements at Scale with Strong Data Consistency](https://eng.uber.com/money-scale-strong-data/) `Uber` `2020`\n6. [Zipline - A Declarative Feature Engineering Framework](https://www.youtube.com/watch?v=LjcKCm0G_OY) `Airbnb` `2020`\n7. [Automating Data Protection at Scale, Part 1](https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-1-c74909328e08) ([Part 2](https://medium.com/airbnb-engineering/automating-data-protection-at-scale-part-2-c2b8d2068216)) `Airbnb` `2021`\n8. [Real-time Data Infrastructure at Uber](https://arxiv.org/pdf/2104.00087.pdf) `Uber` `2021`\n9. [Introducing Fabricator: A Declarative Feature Engineering Framework](https://doordash.engineering/2022/01/11/introducing-fabricator-a-declarative-feature-engineering-framework/) `DoorDash` `2022`\n10. [Functions & DAGs: introducing Hamilton, a microframework for dataframe generation](https://multithreaded.stitchfix.com/blog/2021/10/14/functions-dags-hamilton/) `Stitch Fix` `2021`\n11. [Optimizing Pinterestâ€™s Data Ingestion Stack: Findings and Learnings](https://medium.com/@Pinterest_Engineering/optimizing-pinterests-data-ingestion-stack-findings-and-learnings-994fddb063bf) `Pinterest` `2022`\n12. [Lessons Learned From Running Apache Airflow at Scale](https://shopifyengineering.myshopify.com/blogs/engineering/lessons-learned-apache-airflow-scale) `Shopify` `2022`\n13. [Understanding Data Storage and Ingestion for Large-Scale Deep Recommendation Model Training](https://arxiv.org/abs/2108.09373v4) `Meta` `2022`\n14. [Data Mesh â€” A Data Movement and Processing Platform @ Netflix](https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873) `Netflix` `2022`\n15. [Building Scalable Real Time Event Processing with Kafka and Flink](https://doordash.engineering/2022/08/02/building-scalable-real-time-event-processing-with-kafka-and-flink/) `DoorDash` `2022`\n\n## Data Discovery\n1. [Apache Atlas: Data Goverance and Metadata Framework for Hadoop](https://atlas.apache.org/#/) ([Code](https://github.com/apache/atlas)) `Apache`\n2. [Collect, Aggregate, and Visualize a Data Ecosystem''s Metadata](https://marquezproject.github.io/marquez/) ([Code](https://github.com/MarquezProject/marquez)) `WeWork`\n3. [Discovery and Consumption of Analytics Data at Twitter](https://blog.twitter.com/engineering/en_us/topics/insights/2016/discovery-and-consumption-of-analytics-data-at-twitter.html) `Twitter` `2016`\n4. [Democratizing Data at Airbnb](https://medium.com/airbnb-engineering/democratizing-data-at-airbnb-852d76c51770) `Airbnb` `2017`\n5. [Databook: Turning Big Data into Knowledge with Metadata at Uber](https://eng.uber.com/databook/) `Uber` `2018`\n6. [Metacat: Making Big Data Discoverable and Meaningful at Netflix](https://netflixtechblog.com/metacat-making-big-data-discoverable-and-meaningful-at-netflix-56fb36a53520) ([Code](https://github.com/Netflix/metacat)) `Netflix` `2018`\n7. [Amundsen â€” Lyftâ€™s Data Discovery & Metadata Engine](https://eng.lyft.com/amundsen-lyfts-data-discovery-metadata-engine-62d27254fbb9) `Lyft` `2019`\n8. [Open Sourcing Amundsen: A Data Discovery And Metadata Platform](https://eng.lyft.com/open-sourcing-amundsen-a-data-discovery-and-metadata-platform-2282bb436234) ([Code](https://github.com/lyft/amundsen)) `Lyft` `2019`\n9. [DataHub: A Generalized Metadata Search & Discovery Tool](https://engineering.linkedin.com/blog/2019/data-hub) ([Code](https://github.com/linkedin/datahub)) `LinkedIn` `2019`\n10. [Amundsen: One Year Later](https://eng.lyft.com/amundsen-1-year-later-7b60bf28602) `Lyft` `2020`\n11. [Using Amundsen to Support User Privacy via Metadata Collection at Square](https://developer.squareup.com/blog/using-amundsen-to-support-user-privacy-via-metadata-collection-at-square/) `Square` `2020`\n12. [Turning Metadata Into Insights with Databook](https://eng.uber.com/metadata-insights-databook/) `Uber` `2020`\n13. [DataHub: Popular Metadata Architectures Explained](https://engineering.linkedin.com/blog/2020/datahub-popular-metadata-architectures-explained) `LinkedIn` `2020`\n14. [How We Improved Data Discovery for Data Scientists at Spotify](https://engineering.atspotify.com/2020/02/27/how-we-improved-data-discovery-for-data-scientists-at-spotify/) `Spotify` `2020` \n15. [How Weâ€™re Solving Data Discovery Challenges at Shopify](https://engineering.shopify.com/blogs/engineering/solving-data-discovery-challenges-shopify) `Shopify` `2020`\n16. [Nemo: Data discovery at Facebook](https://engineering.fb.com/data-infrastructure/nemo/) `Facebook` `2020`\n17. [Exploring Data @ Netflix](https://netflixtechblog.com/exploring-data-netflix-9d87e20072e3) ([Code](https://github.com/Netflix/nf-data-explorer)) `Netflix` `2021`\n\n## Feature Stores\n1. [Distributed Time Travel for Feature Generation](https://netflixtechblog.com/distributed-time-travel-for-feature-generation-389cccdd3907) `Netflix` `2016`\n2. [Building the Activity Graph, Part 2 (Feature Storage Section)](https://engineering.linkedin.com/blog/2017/07/building-the-activity-graph--part-2) `LinkedIn` `2017`\n3. [Fact Store at Scale for Netflix Recommendations](https://www.youtube.com/watch?v=DiwKg8KynVU) `Netflix` `2018`\n4. [Zipline: Airbnbâ€™s Machine Learning Data Management Platform](https://www.youtube.com/watch?v=Tg5VEMEsC-0) `Airbnb` `2018`\n5. [Feature Store: The missing data layer for Machine Learning pipelines?](https://www.hopsworks.ai/post/feature-store-the-missing-data-layer-in-ml-pipelines) `Hopsworks` `2018`\n6. [Introducing Feast: An Open Source Feature Store for Machine Learning](https://cloud.google.com/blog/products/ai-machine-learning/introducing-feast-an-open-source-feature-store-for-machine-learning) ([Code](https://github.com/feast-dev/feast)) `Gojek` `2019`\n7. [Michelangelo Palette: A Feature Engineering Platform at Uber](https://www.infoq.com/presentations/michelangelo-palette-uber/) `Uber` `2019`\n8. [The Architecture That Powers Twitter''s Feature Store](https://www.youtube.com/watch?v=UNailXoiIrY) `Twitter` `2019`\n9. [Accelerating Machine Learning with the Feature Store Service](https://technology.condenast.com/story/accelerating-machine-learning-with-the-feature-store-service) `CondÃ© Nast` `2019` \n10. [Feast: Bridging ML Models and Data](https://www.gojek.io/blog/feast-bridging-ml-models-and-data) `Gojek` `2020`\n11. [Building a Scalable ML Feature Store with Redis, Binary Serialization, and Compression](https://doordash.engineering/2020/11/19/building-a-gigascale-ml-feature-store-with-redis/) `DoorDash` `2020`\n12. [Rapid Experimentation Through Standardization: Typed AI features for LinkedInâ€™s Feed](https://engineering.linkedin.com/blog/2020/feed-typed-ai-features) `LinkedIn` `2020`\n13. [Building a Feature Store](https://nlathia.github.io/2020/12/Building-a-feature-store.html) `Monzo Bank` `2020`\n14. [Butterfree: A Spark-based Framework for Feature Store Building](https://medium.com/quintoandar-tech-blog/butterfree-a-spark-based-framework-for-feature-store-building-48c3640522c7) ([Code](https://github.com/quintoandar/butterfree)) `QuintoAndar` `2020`\n15. [Building Riviera: A Declarative Real-Time Feature Engineering Framework](https://doordash.engineering/2021/03/04/building-a-declarative-real-time-feature-engineering-framework/) `DoorDash` `2021`\n16. [Optimal Feature Discovery: Better, Leaner Machine Learning Models Through Information Theory](https://eng.uber.com/optimal-feature-discovery-ml/) `Uber` `2021`\n17. [ML Feature Serving Infrastructure at Lyft](https://eng.lyft.com/ml-feature-serving-infrastructure-at-lyft-d30bf2d3c32a) `Lyft` `2021`\n18. [Near real-time features for near real-time personalization](https://engineering.linkedin.com/blog/2022/near-real-time-features-for-near-real-time-personalization) `LinkedIn` `2022`\n19. [Building the Model Behind DoorDashâ€™s Expansive Merchant Selection](https://doordash.engineering/2022/04/19/building-merchant-selection/) `DoorDash` `2022`\n20. [Open sourcing Feathr â€“ LinkedInâ€™s feature store for productive machine learning](https://engineering.linkedin.com/blog/2022/open-sourcing-feathr---linkedin-s-feature-store-for-productive-m) `LinkedIn` `2022`\n21. [Evolution of ML Fact Store](https://netflixtechblog.com/evolution-of-ml-fact-store-5941d3231762) `Netflix` `2022`\n22. [Developing scalable feature engineering DAGs](https://outerbounds.com/blog/developing-scalable-feature-engineering-dags) `Metaflow + Hamilton` via `Outerbounds` `2022`\n23. [Feature Store Design at Constructor](https://medium.com/constructor-engineering/feature-store-design-at-constructor-330b65f64b18) `Constructor.io` `2023`\n\n\n## Classification\n1. [Prediction of Advertiser Churn for Google AdWords](https://research.google/pubs/pub36678/) ([Paper](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/36678.pdf)) `Google` `2010`\n2. [High-Precision Phrase-Based Document Classification on a Modern Scale](https://engineering.linkedin.com/research/2011/high-precision-phrase-based-document-classification-on-a-modern-scale) ([Paper](http://web.stanford.edu/~gavish/documents/phrase_based.pdf)) `LinkedIn` `2011`\n3. [Chimera: Large-scale Classification using Machine Learning, Rules, and Crowdsourcing](https://dl.acm.org/doi/10.14778/2733004.2733024) ([Paper](http://pages.cs.wisc.edu/%7Eanhai/papers/chimera-vldb14.pdf)) `Walmart` `2014`\n4. [Large-scale Item Categorization in e-Commerce Using Multiple Recurrent Neural Networks](https://www.kdd.org/kdd2016/subtopic/view/large-scale-item-categorization-in-e-commerce-using-multiple-recurrent-neur/) ([Paper](https://www.kdd.org/kdd2016/papers/files/adf0392-haAemb.pdf)) `NAVER` `2016`\n5. [Learning to Diagnose with LSTM Recurrent Neural Networks](https://arxiv.org/abs/1511.03677) ([Paper](https://arxiv.org/pdf/1511.03677.pdf)) `Google` `2017`\n6. [Discovering and Classifying In-app Message Intent at Airbnb](https://medium.com/airbnb-engineering/discovering-and-classifying-in-app-message-intent-at-airbnb-6a55f5400a0c) `Airbnb` `2019`\n7. [Teaching Machines to Triage Firefox Bugs](https://hacks.mozilla.org/2019/04/teaching-machines-to-triage-firefox-bugs/) `Mozilla` `2019`\n8. [Categorizing Products at Scale](https://engineering.shopify.com/blogs/engineering/categorizing-products-at-scale) `Shopify` `2020`\n9. [How We Built the Good First Issues Feature](https://github.blog/2020-01-22-how-we-built-good-first-issues/) `GitHub` `2020`\n10. [Testing Firefox More Efficiently with Machine Learning](https://hacks.mozilla.org/2020/07/testing-firefox-more-efficiently-with-machine-learning/) `Mozilla` `2020`\n11. [Using ML to Subtype Patients Receiving Digital Mental Health Interventions](https://www.microsoft.com/en-us/research/blog/a-path-to-personalization-using-ml-to-subtype-patients-receiving-digital-mental-health-interventions/) ([Paper](https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2768347)) `Microsoft` `2020`\n12. [Scalable Data Classification for Security and Privacy](https://engineering.fb.com/security/data-classification-system/) ([Paper](https://arxiv.org/pdf/2006.14109.pdf)) `Facebook` `2020`\n13. [Uncovering Online Delivery Menu Best Practices with Machine Learning](https://doordash.engineering/2020/11/10/uncovering-online-delivery-menu-best-practices-with-machine-learning/) `DoorDash` `2020`\n14. [Using a Human-in-the-Loop to Overcome the Cold Start Problem in Menu Item Tagging](https://doordash.engineering/2020/08/28/overcome-the-cold-start-problem-in-menu-item-tagging/) `DoorDash` `2020`\n15. [Deep Learning: Product Categorization and Shelving](https://medium.com/walmartglobaltech/deep-learning-product-categorization-and-shelving-630571e81e96) `Walmart` `2021`\n16. [Large-scale Item Categorization for e-Commerce](https://dl.acm.org/doi/10.1145/2396761.2396838) ([Paper](https://www.researchgate.net/profile/Jean_David_Ruvini/publication/262270957_Large-scale_item_categorization_for_e-commerce/links/5512dc3d0cf270fd7e33a0d5/Large-scale-item-categorization-for-e-commerce.pdf)) `DianPing`, `eBay` `2012`\n17. [Semantic Label Representation with an Application on Multimodal Product Categorization](https://medium.com/walmartglobaltech/semantic-label-representation-with-an-application-on-multimodal-product-categorization-63d668b943b7) `Walmart` `2022`\n18. [Building Airbnb Categories with ML and Human-in-the-Loop](https://medium.com/airbnb-engineering/building-airbnb-categories-with-ml-and-human-in-the-loop-e97988e70ebb) `Airbnb` `2022`\n\n\n## Regression\n1. [Using Machine Learning to Predict Value of Homes On Airbnb](https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d) `Airbnb` `2017`\n2. [Using Machine Learning to Predict the Value of Ad Requests](https://blog.twitter.com/engineering/en_us/topics/insights/2020/using-machine-learning-to-predict-the-value-of-ad-requests.html) `Twitter` `2020`\n3. [Open-Sourcing Riskquant, a Library for Quantifying Risk](https://netflixtechblog.com/open-sourcing-riskquant-a-library-for-quantifying-risk-6720cc1e4968) ([Code](https://github.com/Netflix-Skunkworks/riskquant)) `Netflix` `2020`\n4. [Solving for Unobserved Data in a Regression Model Using a Simple Data Adjustment](https://doordash.engineering/2020/10/14/solving-for-unobserved-data-in-a-regression-model/) `DoorDash` `2020`\n\n## Forecasting\n1. [Engineering Extreme Event Forecasting at Uber with RNN](https://eng.uber.com/neural-networks/) `Uber` `2017`\n2. [Forecasting at Uber: An Introduction](https://eng.uber.com/forecasting-introduction/) `Uber` `2018`\n3. [Transforming Financial Forecasting with Data Science and Machine Learning at Uber](https://eng.uber.com/transforming-financial-forecasting-machine-learning/) `Uber` `2018`\n4. [Under the Hood of Gojekâ€™s Automated Forecasting Tool](https://www.gojek.io/blog/under-the-hood-of-gojeks-automated-forecasting-tool) `Gojek` `2019`\n5. [BusTr: Predicting Bus Travel Times from Real-Time Traffic](https://dl.acm.org/doi/abs/10.1145/3394486.3403376) ([Paper](https://dl.acm.org/doi/pdf/10.1145/3394486.3403376), [Video](https://crossminds.ai/video/5f3369790576dd25aef288db/)) `Google` `2020`\n6. [Retraining Machine Learning Models in the Wake of COVID-19](https://doordash.engineering/2020/09/15/retraining-ml-models-covid-19/) `DoorDash` `2020`\n7. [Automatic Forecasting using Prophet, Databricks, Delta Lake and MLflow](https://www.youtube.com/watch?v=TkcpjnLh690) ([Paper](https://peerj.com/preprints/3190.pdf), [Code](https://github.com/facebook/prophet)) `Atlassian` `2020`\n8. [Introducing Orbit, An Open Source Package for Time Series Inference and Forecasting](https://eng.uber.com/orbit/) ([Paper](https://arxiv.org/abs/2004.08492), [Video](https://youtu.be/LXDpq_iwcWY), [Code](https://github.com/uber/orbit)) `Uber` `2021`\n9. [Managing Supply and Demand Balance Through Machine Learning](https://doordash.engineering/2021/06/29/managing-supply-and-demand-balance-through-machine-learning/) `DoorDash` `2021`\n10. [Greykite: A flexible, intuitive, and fast forecasting library](https://engineering.linkedin.com/blog/2021/greykite--a-flexible--intuitive--and-fast-forecasting-library) `LinkedIn` `2021`\n11. [The history of Amazonâ€™s forecasting algorithm](https://www.amazon.science/latest-news/the-history-of-amazons-forecasting-algorithm) `Amazon` `2021`\n11. [DeepETA: How Uber Predicts Arrival Times Using Deep Learning](https://eng.uber.com/deepeta-how-uber-predicts-arrival-times/) `Uber` `2022`\n12. [Forecasting Grubhub Order Volume At Scale](https://bytes.grubhub.com/forecasting-grubhub-order-volume-at-scale-a966c2f901d2) `Grubhub` `2022`\n13. [Causal Forecasting at Lyft (Part 1)](https://eng.lyft.com/causal-forecasting-at-lyft-part-1-14cca6ff3d6d) `Lyft` `2022`\n\n## Recommendation\n1. [Amazon.com Recommendations: Item-to-Item Collaborative Filtering](https://ieeexplore.ieee.org/document/1167344) ([Paper](https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf)) `Amazon` `2003`\n2. [Netflix Recommendations: Beyond the 5 stars (Part 1](https://netflixtechblog.com/netflix-recommendations-beyond-the-5-stars-part-1-55838468f429) ([Part 2](https://netflixtechblog.com/netflix-recommendations-beyond-the-5-stars-part-2-d9b96aa399f5)) `Netflix` `2012`\n3. [How Music Recommendation Works â€” And Doesnâ€™t Work](https://notes.variogram.com/2012/12/11/how-music-recommendation-works-and-doesnt-work/) `Spotify` `2012`\n4. [Learning to Rank Recommendations with the k -Order Statistic Loss](https://dl.acm.org/doi/10.1145/2507157.2507210) ([Paper](https://dl.acm.org/doi/pdf/10.1145/2507157.2507210)) `Google` `2013`\n5. [Recommending Music on Spotify with Deep Learning](https://benanne.github.io/2014/08/05/spotify-cnns.html) `Spotify` `2014`\n6. [Learning a Personalized Homepage](https://netflixtechblog.com/learning-a-personalized-homepage-aa8ec670359a) `Netflix` `2015`\n7. [The Netflix Recommender System: Algorithms, Business Value, and Innovation](https://dl.acm.org/doi/10.1145/2843948) ([Paper](https://dl.acm.org/doi/pdf/10.1145/2843948)) `Netflix` `2015`\n7. [Session-based Recommendations with Recurrent Neural Networks](https://arxiv.org/abs/1511.06939) ([Paper](https://arxiv.org/pdf/1511.06939.pdf)) `Telefonica` `2016`\n8. [Deep Neural Networks for YouTube Recommendations](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf) `YouTube` `2016`\n9. [E-commerce in Your Inbox: Product Recommendations at Scale](https://arxiv.org/abs/1606.07154) ([Paper](https://arxiv.org/pdf/1606.07154.pdf)) `Yahoo` `2016`\n10. [To Be Continued: Helping you find shows to continue watching on Netflix](https://netflixtechblog.com/to-be-continued-helping-you-find-shows-to-continue-watching-on-7c0d8ee4dab6) `Netflix` `2016`\n11. [Personalized Recommendations in LinkedIn Learning](https://engineering.linkedin.com/blog/2016/12/personalized-recommendations-in-linkedin-learning) `LinkedIn` `2016`\n12. [Personalized Channel Recommendations in Slack](https://slack.engineering/personalized-channel-recommendations-in-slack/) `Slack` `2016`\n13. [Recommending Complementary Products in E-Commerce Push Notifications](https://arxiv.org/abs/1707.08113) ([Paper](https://arxiv.org/pdf/1707.08113.pdf)) `Alibaba` `2017`\n14. [Artwork Personalization at Netflix](https://netflixtechblog.com/artwork-personalization-c589f074ad76) `Netflix` `2017`\n15. [A Meta-Learning Perspective on Cold-Start Recommendations for Items](https://papers.nips.cc/paper/7266-a-meta-learning-perspective-on-cold-start-recommendations-for-items) ([Paper](https://papers.nips.cc/paper/7266-a-meta-learning-perspective-on-cold-start-recommendations-for-items.pdf)) `Twitter` `2017`\n16. [Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time](https://arxiv.org/abs/1711.07601) ([Paper](https://arxiv.org/pdf/1711.07601.pdf)) `Pinterest` `2017`\n17. [Powering Search & Recommendations at DoorDash](https://doordash.news/company/powering-search-recommendations-at-doordash/) `DoorDash` `2017`\n17. [How 20th Century Fox uses ML to predict a movie audience](https://cloud.google.com/blog/products/ai-machine-learning/how-20th-century-fox-uses-ml-to-predict-a-movie-audience) ([Paper](https://arxiv.org/abs/1810.08189)) `20th Century Fox` `2018`\n18. [Calibrated Recommendations](https://dl.acm.org/doi/10.1145/3240323.3240372) ([Paper](https://dl.acm.org/doi/pdf/10.1145/3240323.3240372)) `Netflix` `2018`\n19. [Food Discovery with Uber Eats: Recommending for the Marketplace](https://eng.uber.com/uber-eats-recommending-marketplace/) `Uber` `2018`\n20. [Explore, Exploit, and Explain: Personalizing Explainable Recommendations with Bandits](https://dl.acm.org/doi/10.1145/3240323.3240354) ([Paper](https://static1.squarespace.com/static/5ae0d0b48ab7227d232c2bea/t/5ba849e3c83025fa56814f45/1537755637453/BartRecSys.pdf)) `Spotify` `2018`\n21. [Talent Search and Recommendation Systems at LinkedIn: Practical Challenges and Lessons Learned](https://arxiv.org/abs/1809.06481) ([Paper](https://arxiv.org/pdf/1809.06481.pdf)) `LinkedIn` `2018`\n21. [Behavior Sequence Transformer for E-commerce Recommendation in Alibaba](https://arxiv.org/abs/1905.06874) ([Paper](https://arxiv.org/pdf/1905.06874.pdf)) `Alibaba` `2019`\n22. [SDM: Sequential Deep Matching Model for Online Large-scale Recommender System](https://arxiv.org/abs/1909.00385) ([Paper](https://arxiv.org/pdf/1909.00385.pdf)) `Alibaba` `2019`\n23. [Multi-Interest Network with Dynamic Routing for Recommendation at Tmall](https://arxiv.org/abs/1904.08030) ([Paper](https://arxiv.org/pdf/1904.08030.pdf)) `Alibaba` `2019`\n24. [Personalized Recommendations for Experiences Using Deep Learning](https://www.tripadvisor.com/engineering/personalized-recommendations-for-experiences-using-deep-learning/) `TripAdvisor` `2019`\n25. [Powered by AI: Instagramâ€™s Explore recommender system](https://ai.facebook.com/blog/powered-by-ai-instagrams-explore-recommender-system/) `Facebook` `2019`\n26. [Marginal Posterior Sampling for Slate Bandits](https://www.ijcai.org/proceedings/2019/308) ([Paper](https://www.ijcai.org/proceedings/2019/0308.pdf)) `Netflix` `2019`\n27. [Food Discovery with Uber Eats: Using Graph Learning to Power Recommendations](https://eng.uber.com/uber-eats-graph-learning/) `Uber` `2019`\n28. [Music recommendation at Spotify](http://sigir.org/afirm2019/slides/16.%20Friday%20-%20Music%20Recommendation%20at%20Spotify%20-%20Ben%20Carterette.pdf) `Spotify` `2019`\n29. [Using Machine Learning to Predict what File you Need Next (Part 1)](https://dropbox.tech/machine-learning/content-suggestions-machine-learning) `Dropbox` `2019`\n30. [Using Machine Learning to Predict what File you Need Next (Part 2)](https://dropbox.tech/machine-learning/using-machine-learning-to-predict-what-file-you-need-next-part-2) `Dropbox` `2019`\n31. [Learning to be Relevant: Evolution of a Course Recommendation System](https://dl.acm.org/doi/pdf/10.1145/3357384.3357817) (**PAPER NEEDED**)`LinkedIn` `2019`\n32. [Temporal-Contextual Recommendation in Real-Time](https://www.amazon.science/publications/temporal-contextual-recommendation-in-real-time) ([Paper](https://assets.amazon.science/96/71/d1f25754497681133c7aa2b7eb05/temporal-contextual-recommendation-in-real-time.pdf)) `Amazon` `2020`\n33. [P-Companion: A Framework for Diversified Complementary Product Recommendation](https://www.amazon.science/publications/p-companion-a-principled-framework-for-diversified-complementary-product-recommendation) ([Paper](https://assets.amazon.science/d5/16/3f7809974a899a11bacdadefdf24/p-companion-a-principled-framework-for-diversified-complementary-product-recommendation.pdf)) `Amazon` `2020`\n34. [Deep Interest with Hierarchical Attention Network for Click-Through Rate Prediction](https://arxiv.org/abs/2005.12981) ([Paper](https://arxiv.org/pdf/2005.12981.pdf)) `Alibaba` `2020`\n35. [TPG-DNN: A Method for User Intent Prediction with Multi-task Learning](https://arxiv.org/abs/2008.02122) ([Paper](https://arxiv.org/pdf/2008.02122.pdf)) `Alibaba` `2020`\n36. [PURS: Personalized Unexpected Recommender System for Improving User Satisfaction](https://dl.acm.org/doi/10.1145/3383313.3412238) ([Paper](https://dl.acm.org/doi/pdf/10.1145/3383313.3412238)) `Alibaba` `2020`\n37. [Controllable Multi-Interest Framework for Recommendation](https://arxiv.org/abs/2005.09347) ([Paper](https://arxiv.org/pdf/2005.09347)) `Alibaba` `2020`\n38. [MiNet: Mixed Interest Network for Cross-Domain Click-Through Rate Prediction](https://arxiv.org/abs/2008.02974) ([Paper](https://arxiv.org/pdf/2008.02974.pdf)) `Alibaba` `2020`\n39. [ATBRG: Adaptive Target-Behavior Relational Graph Network for Effective Recommendation](https://arxiv.org/abs/2005.12002) ([Paper](https://arxiv.org/pdf/2005.12002.pdf)) `Alibaba` `2020`\n40. [For Your Ears Only: Personalizing Spotify Home with Machine Learning](https://engineering.atspotify.com/2020/01/16/for-your-ears-only-personalizing-spotify-home-with-machine-learning/) `Spotify` `2020`\n41. [Reach for the Top: How Spotify Built Shortcuts in Just Six Months](https://engineering.atspotify.com/2020/04/15/reach-for-the-top-how-spotify-built-shortcuts-in-just-six-months/) `Spotify` `2020`\n42. [Contextual and Sequential User Embeddings for Large-Scale Music Recommendation](https://dl.acm.org/doi/10.1145/3383313.3412248) ([Paper](https://dl.acm.org/doi/pdf/10.1145/3383313.3412248)) `Spotify` `2020`\n43. [The Evolution of Kit: Automating Marketing Using Machine Learning](https://engineering.shopify.com/blogs/engineering/evolution-kit-automating-marketing-machine-learning) `Shopify` `2020`\n44. [A Closer Look at the AI Behind Course Recommendations on LinkedIn Learning (Part 1)](https://engineering.linkedin.com/blog/2020/course-recommendations-ai-part-one) `LinkedIn` `2020`\n45. [A Closer Look at the AI Behind Course Recommendations on LinkedIn Learning (Part 2)](https://engineering.linkedin.com/blog/2020/course-recommendations-ai-part-two) `LinkedIn` `2020`\n46. [Building a Heterogeneous Social Network Recommendation System](https://engineering.linkedin.com/blog/2020/building-a-heterogeneous-social-network-recommendation-system) `LinkedIn` `2020`\n47. [How TikTok recommends videos #ForYou](https://newsroom.tiktok.com/en-us/how-tiktok-recommends-videos-for-you) `ByteDance` `2020`\n48. [Zero-Shot Heterogeneous Transfer Learning from RecSys to Cold-Start Search Retrieval](https://arxiv.org/abs/2008.02930) ([Paper](https://arxiv.org/pdf/2008.02930.pdf)) `Google` `2020`\n49. [Improved Deep & Cross Network for Feature Cross Learning in Web-scale LTR Systems](https://arxiv.org/abs/2008.13535) ([Paper](https://arxiv.org/pdf/2008.13535.pdf)) `Google` `2020`\n50. [Mixed Negative Sampling for Learning Two-tower Neural Networks in Recommendations](https://research.google/pubs/pub50257/) ([Paper](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b9f4e78a8830fe5afcf2f0452862fb3c0d6584ea.pdf)) `Google` `2020`\n51. [Future Data Helps Training: Modeling Future Contexts for Session-based Recommendation](https://arxiv.org/pdf/1906.04473.pdf) ([Paper](https://arxiv.org/pdf/1906.04473.pdf)) `Tencent` `2020`\n52. [A Case Study of Session-based Recommendations in the Home-improvement Domain](https://dl.acm.org/doi/10.1145/3383313.3412235) ([Paper](https://dl.acm.org/doi/pdf/10.1145/3383313.3412235)) `Home Depot` `2020`\n53. [Balancing Relevance and Discovery to Inspire Customers in the IKEA App](https://dl.acm.org/doi/10.1145/3383313.3411550) ([Paper](https://dl.acm.org/doi/pdf/10.1145/3383313.3411550)) `Ikea` `2020`\n54. [How we use AutoML, Multi-task learning and Multi-tower models for Pinterest Ads](https://medium.com/pinterest-engineering/how-we-use-automl-multi-task-learning-and-multi-tower-models-for-pinterest-ads-db966c3dc99e) `Pinterest` `2020`\n55. [Multi-task Learning for Related Products Recommendations at Pinterest](https://medium.com/pinterest-engineering/multi-task-learning-for-related-products-recommendations-at-pinterest-62684f631c12) `Pinterest` `2020`\n56. [Improving the Quality of Recommended Pins with Lightweight Ranking](https://medium.com/pinterest-engineering/improving-the-quality-of-recommended-pins-with-lightweight-ranking-8ff5477b20e3) `Pinterest` `2020`\n57. [Multi-task Learning and Calibration for Utility-based Home Feed Ranking](https://medium.com/pinterest-engineering/multi-task-learning-and-calibration-for-utility-based-home-feed-ranking-64087a7bcbad) `Pinterest` `2020`\n57. [Personalized Cuisine Filter Based on Customer Preference and Local Popularity](https://doordash.engineering/2020/01/27/personalized-cuisine-filter/) `DoorDash` `2020`\n58. [How We Built a Matchmaking Algorithm to Cross-Sell Products](https://www.gojek.io/blog/how-we-built-a-matchmaking-algorithm-to-cross-sell-products) `Gojek` `2020`\n59. [Lessons Learned Addressing Dataset Bias in Model-Based Candidate Generation](https://arxiv.org/abs/2105.09293) ([Paper](https://arxiv.org/pdf/2105.09293.pdf)) `Twitter` `2021`\n60. [Self-supervised Learning for Large-scale Item Recommendations](https://arxiv.org/abs/2007.12865) ([Paper](https://arxiv.org/pdf/2007.12865.pdf)) `Google` `2021`\n61. [Deep Retrieval: End-to-End Learnable Structure Model for Large-Scale Recommendations](https://arxiv.org/abs/2007.07203) ([Paper](https://arxiv.org/pdf/2007.07203.pdf)) `ByteDance` `2021`\n62. [Using AI to Help Health Experts Address the COVID-19 Pandemic](https://ai.facebook.com/blog/using-ai-to-help-health-experts-address-the-covid-19-pandemic/) `Facebook` `2021`\n63. [Advertiser Recommendation Systems at Pinterest](https://medium.com/pinterest-engineering/advertiser-recommendation-systems-at-pinterest-ccb255fbde20) `Pinterest` `2021`\n64. [On YouTube''s Recommendation System](https://blog.youtube/inside-youtube/on-youtubes-recommendation-system/) `YouTube` `2021`\n65. ["Are you sure?": Preliminary Insights from Scaling Product Comparisons to Multiple Shops](https://arxiv.org/abs/2107.03256) `Coveo` `2021`\n66. [Mozrt, a Deep Learning Recommendation System Empowering Walmart Store Associates](https://medium.com/walmartglobaltech/mozrt-a-deep-learning-recommendation-system-empowering-walmart-store-associates-with-a-5d42c08d88da) `Walmart` `2021`\n67. [Understanding Data Storage and Ingestion for Large-Scale Deep Recommendation Model Training](https://arxiv.org/abs/2108.09373) ([Paper](https://arxiv.org/pdf/2108.09373.pdf)) `Meta` `2021`\n67. [The Amazon Music conversational recommender is hitting the right notes](https://www.amazon.science/latest-news/how-amazon-music-uses-recommendation-system-machine-learning) `Amazon` `2022`\n68. [Personalized complementary product recommendation](https://www.amazon.science/publications/personalized-complementary-product-recommendation) ([Paper](https://assets.amazon.science/6c/d9/a0ec3eda4f0fb4312ce0ada41771/personalized-complementary-product-recommendation.pdf)) `Amazon` `2022`\n69. [Building a Deep Learning Based Retrieval System for Personalized Recommendations](https://tech.ebayinc.com/engineering/building-a-deep-learning-based-retrieval-system-for-personalized-recommendations/) `eBay` `2022`\n70. [How We Built: An Early-Stage Machine Learning Model for Recommendations](https://www.onepeloton.com/press/articles/how-we-built-machine-learning) `Peloton` `2022`\n71. [Lessons Learned from Building out Context-Aware Recommender Systems](https://www.onepeloton.com/press/articles/lessons-learned-from-building-context-aware-recommender-systems) `Peloton` `2022`\n72. [Beyond Matrix Factorization: Using hybrid features for user-business recommendations](https://engineeringblog.yelp.com/2022/04/beyond-matrix-factorization-using-hybrid-features-for-user-business-recommendations.html) `Yelp` `2022`\n73. [Improving job matching with machine-learned activity features](https://engineering.linkedin.com/blog/2022/improving-job-matching-with-machine-learned-activity-features-) `LinkedIn` `2022`\n74. [Understanding Data Storage and Ingestion for Large-Scale Deep Recommendation Model Training](https://arxiv.org/abs/2108.09373v4) `Meta` `2022`\n75. [Blueprints for recommender system architectures: 10th anniversary edition](https://amatriain.net/blog/RecsysArchitectures) `Xavier Amatriain` `2022`\n76. [How Pinterest Leverages Realtime User Actions in Recommendation to Boost Homefeed Engagement Volume](https://medium.com/pinterest-engineering/how-pinterest-leverages-realtime-user-actions-in-recommendation-to-boost-homefeed-engagement-volume-165ae2e8cde8) `Pinterest` `2022`\n77. [RecSysOps: Best Practices for Operating a Large-Scale Recommender System](https://netflixtechblog.medium.com/recsysops-best-practices-for-operating-a-large-scale-recommender-system-95bbe195a841) `Netflix` `2022`\n78. [Recommend API: Unified end-to-end machine learning infrastructure to generate recommendations](https://slack.engineering/recommend-api/) `Slack` `2022`\n79. [Evolving DoorDashâ€™s Substitution Recommendations Algorithm](https://doordash.engineering/2022/09/08/evolving-doordashs-substitution-recommendations-algorithm/) `DoorDash` `2022`\n80. [Homepage Recommendation with Exploitation and Exploration](https://doordash.engineering/2022/10/05/homepage-recommendation-with-exploitation-and-exploration/) `DoorDash` `2022`\n81. [GPU-accelerated ML Inference at Pinterest](https://medium.com/@Pinterest_Engineering/gpu-accelerated-ml-inference-at-pinterest-ad1b6a03a16d) `Pinterest` `2022`\n82. [Addressing Confounding Feature Issue for Causal Recommendation](https://arxiv.org/abs/2205.06532) ([Paper](https://arxiv.org/pdf/2205.06532.pdf)) `Tencent` `2022`\n\n\n## Search & Ranking\n1. [Amazon Search: The Joy of Ranking Products](https://www.amazon.science/publications/amazon-search-the-joy-of-ranking-products) ([Paper](https://assets.amazon.science/89/cd/34289f1f4d25b5857d776bdf04d5/amazon-search-the-joy-of-ranking-products.pdf), [Video](https://www.youtube.com/watch?v=NLrhmn-EZ88), [Code](https://github.com/dariasor/TreeExtra)) `Amazon` `2016`\n2. [How Lazada Ranks Products to Improve Customer Experience and Conversion](https://www.slideshare.net/eugeneyan/how-lazada-ranks-products-to-improve-customer-experience-and-conversion) `Lazada` `2016`\n3. [Ranking Relevance in Yahoo Search](https://www.kdd.org/kdd2016/subtopic/view/ranking-relevance-in-yahoo-search) ([Paper](https://www.kdd.org/kdd2016/papers/files/adf0361-yinA.pdf)) `Yahoo` `2016`\n4. [Learning to Rank Personalized Search Results in Professional Networks](https://arxiv.org/abs/1605.04624) ([Paper](https://arxiv.org/pdf/1605.04624.pdf)) `LinkedIn` `2016`\n5. [Using Deep Learning at Scale in Twitterâ€™s Timelines](https://blog.twitter.com/engineering/en_us/topics/insights/2017/using-deep-learning-at-scale-in-twitters-timelines.html) `Twitter` `2017`\n6. [An Ensemble-based Approach to Click-Through Rate Prediction for Promoted Listings at Etsy](https://arxiv.org/abs/1711.01377) ([Paper](https://arxiv.org/pdf/1711.01377.pdf)) `Etsy` `2017`\n7. [Powering Search & Recommendations at DoorDash](https://doordash.engineering/2017/07/06/powering-search-recommendations-at-doordash/) `DoorDash` `2017`\n8. [Applying Deep Learning To Airbnb Search](https://arxiv.org/abs/1810.09591) ([Paper](https://arxiv.org/pdf/1810.09591.pdf)) `Airbnb` `2018`\n9. [In-session Personalization for Talent Search](https://arxiv.org/abs/1809.06488) ([Paper](https://arxiv.org/pdf/1809.06488.pdf)) `LinkedIn` `2018`\n10. [Talent Search and Recommendation Systems at LinkedIn](https://arxiv.org/abs/1809.06481) ([Paper](https://arxiv.org/pdf/1809.06481.pdf)) `LinkedIn` `2018`\n11. [Food Discovery with Uber Eats: Building a Query Understanding Engine](https://eng.uber.com/uber-eats-query-understanding/) `Uber` `2018`\n12. [Globally Optimized Mutual Influence Aware Ranking in E-Commerce Search](https://arxiv.org/abs/1805.08524) ([Paper](https://arxiv.org/pdf/1805.08524.pdf)) `Alibaba` `2018`\n13. [Reinforcement Learning to Rank in E-Commerce Search Engine](https://arxiv.org/abs/1803.00710) ([Paper](https://arxiv.org/pdf/1803.00710.pdf)) `Alibaba` `2018`\n14. [Semantic Product Search](https://arxiv.org/abs/1907.00937) ([Paper](https://arxiv.org/pdf/1907.00937.pdf)) `Amazon` `2019`\n15. [Machine Learning-Powered Search Ranking of Airbnb Experiences](https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789) `Airbnb` `2019`\n16. [Entity Personalized Talent Search Models with Tree Interaction Features](https://arxiv.org/abs/1902.09041) ([Paper](https://arxiv.org/pdf/1902.09041.pdf)) `LinkedIn` `2019`\n17. [The AI Behind LinkedIn Recruiter Search and recommendation systems](https://engineering.linkedin.com/blog/2019/04/ai-behind-linkedin-recruiter-search-and-recommendation-systems) `LinkedIn` `2019`\n18. [Learning Hiring Preferences: The AI Behind LinkedIn Jobs](https://engineering.linkedin.com/blog/2019/02/learning-hiring-preferences--the-ai-behind-linkedin-jobs) `LinkedIn` `2019`\n19. [The Secret Sauce Behind Search Personalisation](https://www.gojek.io/blog/the-secret-sauce-behind-search-personalisation) `Gojek` `2019`\n20. [Neural Code Search: ML-based Code Search Using Natural Language Queries](https://ai.facebook.com/blog/neural-code-search-ml-based-code-search-using-natural-language-queries/) `Facebook` `2019`\n21. [Aggregating Search Results from Heterogeneous Sources via Reinforcement Learning](https://arxiv.org/abs/1902.08882) ([Paper](https://arxiv.org/pdf/1902.08882.pdf)) `Alibaba` `2019`\n22. [Cross-domain Attention Network with Wasserstein Regularizers for E-commerce Search](https://dl.acm.org/doi/10.1145/3357384.3357809) `Alibaba` `2019`\n23. [Understanding Searches Better Than Ever Before](https://www.blog.google/products/search/search-language-understanding-bert/) ([Paper](https://arxiv.org/pdf/1810.04805.pdf)) `Google` `2019`\n24. [How We Used Semantic Search to Make Our Search 10x Smarter](https://medium.com/tokopedia-engineering/how-we-used-semantic-search-to-make-our-search-10x-smarter-bd9c7f601821) `Tokopedia` `2019`\n25. [Query2vec: Search query expansion with query embeddings](https://bytes.grubhub.com/search-query-embeddings-using-query2vec-f5931df27d79) `GrubHub` `2019`\n26. [MOBIUS: Towards the Next Generation of Query-Ad Matching in Baiduâ€™s Sponsored Search](http://research.baidu.com/Public/uploads/5d12eca098d40.pdf) `Baidu` `2019`\n27. [Why Do People Buy Seemingly Irrelevant Items in Voice Product Search?](https://www.amazon.science/publications/why-do-people-buy-irrelevant-items-in-voice-product-search) ([Paper](https://assets.amazon.science/f7/48/0562b2c14338a0b76ccf4f523fa5/why-do-people-buy-irrelevant-items-in-voice-product-search.pdf)) `Amazon` `2020`\n28. [Managing Diversity in Airbnb Search](https://arxiv.org/abs/2004.02621) ([Paper](https://arxiv.org/pdf/2004.02621.pdf)) `Airbnb` `2020`\n29. [Improving Deep Learning for Airbnb Search](https://arxiv.org/abs/2002.05515) ([Paper](https://arxiv.org/pdf/2002.05515.pdf)) `Airbnb` `2020`\n30. [Quality Matches Via Personalized AI for Hirer and Seeker Preferences](https://engineering.linkedin.com/blog/2020/quality-matches-via-personalized-ai) `LinkedIn` `2020`\n31. [Understanding Dwell Time to Improve LinkedIn Feed Ranking](https://engineering.linkedin.com/blog/2020/understanding-feed-dwell-time) `LinkedIn` `2020`\n32. [Ads Allocation in Feed via Constrained Optimization](https://dl.acm.org/doi/abs/10.1145/3394486.3403391) ([Paper](https://dl.acm.org/doi/pdf/10.1145/3394486.3403391), [Video](https://crossminds.ai/video/5f33697a0576dd25aef288ea/)) `LinkedIn` `2020`\n33. [Understanding Dwell Time to Improve LinkedIn Feed Ranking](https://engineering.linkedin.com/blog/2020/understanding-feed-dwell-time) `LinkedIn` `2020`\n34. [AI at Scale in Bing](https://blogs.bing.com/search/2020_05/AI-at-Scale-in-Bing) `Microsoft` `2020`\n35. [Query Understanding Engine in Traveloka Universal Search](https://medium.com/traveloka-engineering/query-understanding-engine-in-traveloka-universal-search-410ad3895db7) `Traveloka` `2020`\n36. [Bayesian Product Ranking at Wayfair](https://tech.wayfair.com/data-science/2020/01/bayesian-product-ranking-at-wayfair) `Wayfair` `2020`\n37. [COLD: Towards the Next Generation of Pre-Ranking System](https://arxiv.org/abs/2007.16122) ([Paper](https://arxiv.org/pdf/2007.16122.pdf)) `Alibaba` `2020`\n38. [Shop The Look: Building a Large Scale Visual Shopping System at Pinterest](https://dl.acm.org/doi/abs/10.1145/3394486.3403372) ([Paper](https://dl.acm.org/doi/pdf/10.1145/3394486.3403372), [Video](https://crossminds.ai/video/5f3369790576dd25aef288d7/)) `Pinterest` `2020`\n39. [Driving Shopping Upsells from Pinterest Search](https://medium.com/pinterest-engineering/driving-shopping-upsells-from-pinterest-search-d06329255402) `Pinterest` `2020`\n40. [GDMix: A Deep Ranking Personalization Framework](https://engineering.linkedin.com/blog/2020/gdmix--a-deep-ranking-personalization-framework) ([Code](https://github.com/linkedin/gdmix)) `LinkedIn` `2020`\n41. [Bringing Personalized Search to Etsy](https://codeascraft.com/2020/10/29/bringing-personalized-search-to-etsy/) `Etsy` `2020`\n42. [Building a Better Search Engine for Semantic Scholar](https://medium.com/ai2-blog/building-a-better-search-engine-for-semantic-scholar-ea23a0b661e7) `Allen Institute for AI` `2020`\n43. [Query Understanding for Natural Language Enterprise Search](https://arxiv.org/abs/2012.06238) ([Paper](https://arxiv.org/pdf/2012.06238.pdf)) `Salesforce` `2020`\n44. [Things Not Strings: Understanding Search Intent with Better Recall](https://doordash.engineering/2020/12/15/understanding-search-intent-with-better-recall/) `DoorDash` `2020`\n45. [Query Understanding for Surfacing Under-served Music Content](https://research.atspotify.com/publications/query-understanding-for-surfacing-under-served-music-content/) ([Paper](https://labtomarket.files.wordpress.com/2020/08/cikm2020.pdf)) `Spotify` `2020`\n46. [Embedding-based Retrieval in Facebook Search](https://arxiv.org/abs/2006.11632) ([Paper](https://arxiv.org/pdf/2006.11632.pdf)) `Facebook` `2020`\n47. [Towards Personalized and Semantic Retrieval for E-commerce Search via Embedding Learning](https://arxiv.org/abs/2006.02282) ([Paper](https://arxiv.org/pdf/2006.02282.pdf)) `JD` `2020`\n48. [QUEEN: Neural query rewriting in e-commerce](https://www.amazon.science/publications/queen-neural-query-rewriting-in-e-commerce) ([Paper](https://assets.amazon.science/f9/78/dda8f1e143dba8ca96e43ec487c6/queen-neural-query-rewriting-in-ecommerce.pdf)) `Amazon` `2021`\n49. [Using Learning-to-rank to Precisely Locate Where to Deliver Packages](https://www.amazon.science/blog/using-learning-to-rank-to-precisely-locate-where-to-deliver-packages) ([Paper](https://www.amazon.science/publications/getting-your-package-to-the-right-place-supervised-machine-learning-for-geolocation)) `Amazon` `2021`\n50. [Seasonal relevance in e-commerce search](https://www.amazon.science/publications/seasonal-relevance-in-e-commerce-search) ([Paper](https://assets.amazon.science/ac/5e/d47612a846d6bec15738d7c8ab40/seasonal-relevance-in-ecommerce-search.pdf)) `Amazon` `2021`\n51. [Graph Intention Network for Click-through Rate Prediction in Sponsored Search](https://arxiv.org/abs/2103.16164) ([Paper](https://arxiv.org/pdf/2103.16164.pdf)) `Alibaba` `2021`\n52. [How We Built A Context-Specific Bidding System for Etsy Ads](https://codeascraft.com/2021/03/23/how-we-built-a-context-specific-bidding-system-for-etsy-ads/) `Etsy` `2021`\n53. [Pre-trained Language Model based Ranking in Baidu Search](https://arxiv.org/abs/2105.11108) ([Paper](https://arxiv.org/pdf/2105.11108.pdf)) `Baidu` `2021`\n54. [Stitching together spaces for query-based recommendations](https://multithreaded.stitchfix.com/blog/2021/08/13/stitching-together-spaces-for-query-based-recommendations/) `Stitch Fix` `2021`\n55. [Deep Natural Language Processing for LinkedIn Search Systems](https://arxiv.org/abs/2108.08252) ([Paper](https://arxiv.org/pdf/2108.08252.pdf)) `LinkedIn` `2021`\n56. [Siamese BERT-based Model for Web Search Relevance Ranking](https://arxiv.org/abs/2112.01810) ([Paper](https://arxiv.org/pdf/2112.01810.pdf), [Code](https://github.com/seznam/DaReCzech)) `Seznam` `2021`\n57. [SearchSage: Learning Search Query Representations at Pinterest](https://medium.com/pinterest-engineering/searchsage-learning-search-query-representations-at-pinterest-654f2bb887fc) `Pinterest` `2021`\n58. [Query2Prod2Vec: Grounded Word Embeddings for eCommerce](https://aclanthology.org/2021.naacl-industry.20/) `Coveo` `2021`\n59. [3 Changes to Expand DoorDashâ€™s Product Search Beyond Delivery](https://doordash.engineering/2022/05/10/3-changes-to-expand-doordashs-product-search/) `DoorDash` `2022`\n60. [Learning To Rank Diversely](https://medium.com/airbnb-engineering/learning-to-rank-diversely-add6b1929621) `Airbnb` `2022`\n61. [How to Optimise Rankings with Cascade Bandits](https://medium.com/expedia-group-tech/how-to-optimise-rankings-with-cascade-bandits-5d92dfa0f16b) `Expedia` `2022`\n62. [A Guide to Google Search Ranking Systems](https://developers.google.com/search/docs/appearance/ranking-systems-guide) `Google` `2022` \n63. [Deep Learning for Search Ranking at Etsy](https://www.etsy.com/codeascraft/deep-learning-for-search-ranking-at-etsy) `Etsy` `2022`\n64. [Search at Calm](https://eng.calm.com/posts/search-at-calm) `Calm` `2022`\n\n## Embeddings\n1. [Vector Representation Of Items, Customer And Cart To Build A Recommendation System](https://arxiv.org/abs/1705.06338) ([Paper](https://arxiv.org/pdf/1705.06338.pdf)) `Sears` `2017`\n2. [Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba](https://arxiv.org/abs/1803.02349) ([Paper](https://arxiv.org/pdf/1803.02349.pdf)) `Alibaba` `2018`\n3. [Embeddings@Twitter](https://blog.twitter.com/engineering/en_us/topics/insights/2018/embeddingsattwitter.html) `Twitter` `2018`\n4. [Listing Embeddings in Search Ranking](https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e) ([Paper](https://www.kdd.org/kdd2018/accepted-papers/view/real-time-personalization-using-embeddings-for-search-ranking-at-airbnb)) `Airbnb` `2018`\n5. [Understanding Latent Style](https://multithreaded.stitchfix.com/blog/2018/06/28/latent-style/) `Stitch Fix` `2018`\n6. [Towards Deep and Representation Learning for Talent Search at LinkedIn](https://arxiv.org/abs/1809.06473) ([Paper](https://arxiv.org/pdf/1809.06473.pdf)) `LinkedIn` `2018`\n7. [Personalized Store Feed with Vector Embeddings](https://doordash.engineering/2018/04/02/personalized-store-feed-with-vector-embeddings/) `DoorDash` `2018`\n8. [Should we Embed? A Study on Performance of Embeddings for Real-Time Recommendations](https://arxiv.org/abs/1907.06556)([Paper](https://arxiv.org/pdf/1907.06556.pdf)) `Moshbit` `2019`\n9. [Machine Learning for a Better Developer Experience](https://netflixtechblog.com/machine-learning-for-a-better-developer-experience-1e600c69f36c) `Netflix` `2020`\n10. [Announcing ScaNN: Efficient Vector Similarity Search](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html) ([Paper](https://arxiv.org/pdf/1908.10396.pdf), [Code](https://github.com/google-research/google-research/tree/master/scann)) `Google` `2020`\n11. [BERT Goes Shopping: Comparing Distributional Models for Product Representations](https://aclanthology.org/2021.ecnlp-1.1/) `Coveo` `2021`\n12. [The Embeddings That Came in From the Cold: Improving Vectors for New and Rare Products with Content-Based Inference](https://dl.acm.org/doi/10.1145/3383313.3411477) `Coveo` `2022`\n13. [Embedding-based Retrieval at Scribd](https://tech.scribd.com/blog/2021/embedding-based-retrieval-scribd.html) `Scribd` `2021`\n14. [Multi-objective Hyper-parameter Optimization of Behavioral Song Embeddings](https://arxiv.org/abs/2208.12724) ([Paper](https://arxiv.org/pdf/2208.12724.pdf)) `Apple` `2022`\n15. [Embeddings at Spotify''s Scale - How Hard Could It Be?](https://arize.com/resource/embeddings-at-scale-spotify-recsys/) `Spotify` `2023`\n\n## Natural Language Processing\n1. [Abusive Language Detection in Online User Content](https://dl.acm.org/doi/10.1145/2872427.2883062) ([Paper](http://www.yichang-cs.com/yahoo/WWW16_Abusivedetection.pdf)) `Yahoo` `2016`\n2. [Smart Reply: Automated Response Suggestion for Email](https://research.google/pubs/pub45189/) ([Paper](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45189.pdf)) `Google` `2016` \n3. [Building Smart Replies for Member Messages](https://engineering.linkedin.com/blog/2017/10/building-smart-replies-for-member-messages) `LinkedIn` `2017`\n4. [How Natural Language Processing Helps LinkedIn Members Get Support Easily](https://engineering.linkedin.com/blog/2019/04/how-natural-language-processing-help-support) `LinkedIn` `2019`\n5. [Gmail Smart Compose: Real-Time Assisted Writing](https://arxiv.org/abs/1906.00080) ([Paper](https://arxiv.org/pdf/1906.00080.pdf)) `Google` `2019`\n6. [Goal-Oriented End-to-End Conversational Models with Profile Features in a Real-World Setting](https://www.amazon.science/publications/goal-oriented-end-to-end-chatbots-with-profile-features-in-a-real-world-setting) ([Paper](https://assets.amazon.science/47/03/e0d14dc34d3eb6e0d4ec282067bd/goal-oriented-end-to-end-chatbots-with-profile-features-in-a-real-world-setting.pdf)) `Amazon` `2019`\n7. [Give Me Jeans not Shoes: How BERT Helps Us Deliver What Clients Want](https://multithreaded.stitchfix.com/blog/2019/07/15/give-me-jeans/) `Stitch Fix` `2019`\n8. [DeText: A deep NLP Framework for Intelligent Text Understanding](https://engineering.linkedin.com/blog/2020/open-sourcing-detext) ([Code](https://github.com/linkedin/detext)) `LinkedIn` `2020`\n9. [SmartReply for YouTube Creators](https://ai.googleblog.com/2020/07/smartreply-for-youtube-creators.html) `Google` `2020`\n10. [Using Neural Networks to Find Answers in Tables](https://ai.googleblog.com/2020/04/using-neural-networks-to-find-answers.html) ([Paper](https://arxiv.org/pdf/2004.02349.pdf)) `Google` `2020`\n11. [A Scalable Approach to Reducing Gender Bias in Google Translate](https://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html) `Google` `2020`\n12. [Assistive AI Makes Replying Easier](https://www.microsoft.com/en-us/research/group/msai/articles/assistive-ai-makes-replying-easier-2/) `Microsoft` `2020`\n13. [AI Advances to Better Detect Hate Speech](https://ai.facebook.com/blog/ai-advances-to-better-detect-hate-speech/) `Facebook` `2020`\n14. [A State-of-the-Art Open Source Chatbot](https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot) ([Paper](https://arxiv.org/pdf/2004.13637.pdf)) `Facebook` `2020`\n15. [A Highly Efficient, Real-Time Text-to-Speech System Deployed on CPUs](https://ai.facebook.com/blog/a-highly-efficient-real-time-text-to-speech-system-deployed-on-cpus/) `Facebook` `2020`\n16. [Deep Learning to Translate Between Programming Languages](https://ai.facebook.com/blog/deep-learning-to-translate-between-programming-languages/) ([Paper](https://arxiv.org/abs/2006.03511), [Code](https://github.com/facebookresearch/TransCoder)) `Facebook` `2020`\n17. [Deploying Lifelong Open-Domain Dialogue Learning](https://arxiv.org/abs/2008.08076) ([Paper](https://arxiv.org/pdf/2008.08076.pdf)) `Facebook` `2020`\n18. [Introducing Dynabench: Rethinking the way we benchmark AI](https://ai.facebook.com/blog/dynabench-rethinking-ai-benchmarking/) `Facebook` `2020`\n19. [How Gojek Uses NLP to Name Pickup Locations at Scale](https://www.gojek.io/blog/nlp-cartobert) `Gojek` `2020`\n20. [The State-of-the-art Open-Domain Chatbot in Chinese and English](http://research.baidu.com/Blog/index-view?id=142) ([Paper](https://arxiv.org/pdf/2006.16779.pdf)) `Baidu` `2020`\n21. [PEGASUS: A State-of-the-Art Model for Abstractive Text Summarization](https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html) ([Paper](https://arxiv.org/pdf/1912.08777.pdf), [Code](https://github.com/google-research/pegasus)) `Google` `2020`\n22. [Photon: A Robust Cross-Domain Text-to-SQL System](https://www.aclweb.org/anthology/2020.acl-demos.24/) ([Paper](https://www.aclweb.org/anthology/2020.acl-demos.24.pdf)) ([Demo](http://naturalsql.com)) `Salesforce`	`2020`\n23. [GeDi: A Powerful New Method for Controlling Language Models](https://blog.einstein.ai/gedi/) ([Paper](https://arxiv.org/abs/2009.06367), [Code](https://github.com/salesforce/GeDi)) `Salesforce` `2020`\n24. [Applying Topic Modeling to Improve Call Center Operations](https://www.youtube.com/watch?v=kzRR8OjF_eI&t=2s) `RICOH` `2020`\n25. [WIDeText: A Multimodal Deep Learning Framework](https://medium.com/airbnb-engineering/widetext-a-multimodal-deep-learning-framework-31ce2565880c) `Airbnb` `2020`\n26. [Dynaboard: Moving Beyond Accuracy to Holistic Model Evaluation in NLP](https://ai.facebook.com/blog/dynaboard-moving-beyond-accuracy-to-holistic-model-evaluation-in-nlp) ([Code](https://github.com/facebookresearch/dynalab?fbclid=IwAR3qcV7QK2uXm4s4M0XUoQQo4i2DEsDy0LZFKxSQCHhP-3hF6fr2-NDFWX8)) `Facebook`  `2021`\n27. [How we reduced our text similarity runtime by 99.96%](https://medium.com/data-science-at-microsoft/how-we-reduced-our-text-similarity-runtime-by-99-96-e8e4b4426b35) `Microsoft` `2021`\n28. [Textless NLP: Generating expressive speech from raw audio](https://ai.facebook.com/blog/textless-nlp-generating-expressive-speech-from-raw-audio/) [(Part 1)](https://arxiv.org/abs/2102.01192) [(Part 2)](https://arxiv.org/abs/2104.00355) [(Part 3)](https://arxiv.org/abs/2109.03264) [(Code and Pretrained Models)](https://github.com/pytorch/fairseq/tree/master/examples/textless_nlp) `Facebook` `2021`\n29. [Grammar Correction as You Type, on Pixel 6](https://ai.googleblog.com/2021/10/grammar-correction-as-you-type-on-pixel.html) `Google` `2021`\n30. [Auto-generated Summaries in Google Docs](https://ai.googleblog.com/2022/03/auto-generated-summaries-in-google-docs.html) `Google` `2022`\n31. [ML-Enhanced Code Completion Improves Developer Productivity](https://ai.googleblog.com/2022/07/ml-enhanced-code-completion-improves.html) `Google` `2022`\n32. [Words All the Way Down â€” Conversational Sentiment Analysis](https://medium.com/paypal-tech/words-all-the-way-down-conversational-sentiment-analysis-afe0165b84db) `PayPal` `2022`\n\n## Sequence Modelling\n1. [Doctor AI: Predicting Clinical Events via Recurrent Neural Networks](https://arxiv.org/abs/1511.05942) ([Paper](https://arxiv.org/pdf/1511.05942.pdf)) `Sutter Health` `2015`\n2. [Deep Learning for Understanding Consumer Histories](https://engineering.zalando.com/posts/2016/10/deep-learning-for-understanding-consumer-histories.html) ([Paper](https://doogkong.github.io/2017/papers/paper2.pdf)) `Zalando` `2016`\n3. [Using Recurrent Neural Network Models for Early Detection of Heart Failure Onset](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5391725/) ([Paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5391725/pdf/ocw112.pdf)) `Sutter Health` `2016`\n4. [Continual Prediction of Notification Attendance with Classical and Deep Networks](https://arxiv.org/abs/1712.07120) ([Paper](https://arxiv.org/pdf/1712.07120.pdf)) `Telefonica` `2017` \n5. [Deep Learning for Electronic Health Records](https://ai.googleblog.com/2018/05/deep-learning-for-electronic-health.html) ([Paper](https://www.nature.com/articles/s41746-018-0029-1.pdf)) `Google` `2018`\n6. [Practice on Long Sequential User Behavior Modeling for Click-Through Rate Prediction](https://arxiv.org/abs/1905.09248) ([Paper](https://arxiv.org/pdf/1905.09248.pdf))`Alibaba` `2019`\n7. [Search-based User Interest Modeling with Sequential Behavior Data for CTR Prediction](https://arxiv.org/abs/2006.05639) ([Paper](https://arxiv.org/pdf/2006.05639.pdf)) `Alibaba` `2020`\n8. [How Duolingo uses AI in every part of its app](https://venturebeat.com/2020/08/18/how-duolingo-uses-ai-in-every-part-of-its-app/) `Duolingo` `2020`\n9. [Leveraging Online Social Interactions For Enhancing Integrity at Facebook](https://research.fb.com/blog/2020/08/leveraging-online-social-interactions-for-enhancing-integrity-at-facebook/) ([Paper](https://research.fb.com/wp-content/uploads/2020/08/TIES-Temporal-Interaction-Embeddings-For-Enhancing-Social-Media-Integrity-At-Facebook.pdf), [Video](https://crossminds.ai/video/5f3369780576dd25aef288cf/)) `Facebook` `2020`\n10. [Using deep learning to detect abusive sequences of member activity](https://engineering.linkedin.com/blog/2021/using-deep-learning-to-detect-abusive-sequences-of-member-activi) ([Video](https://exchange.scale.com/public/videos/using-deep-learning-to-detect-abusive-sequences-of-member-activity-on-linkedin)) `LinkedIn` `2021`\n\n## Computer Vision\n1. [Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning](https://dropbox.tech/machine-learning/creating-a-modern-ocr-pipeline-using-computer-vision-and-deep-learning) `Dropbox` `2017`\n2. [Categorizing Listing Photos at Airbnb](https://medium.com/airbnb-engineering/categorizing-listing-photos-at-airbnb-f9483f3ab7e3) `Airbnb` `2018`\n3. [Amenity Detection and Beyond â€” New Frontiers of Computer Vision at Airbnb](https://medium.com/airbnb-engineering/amenity-detection-and-beyond-new-frontiers-of-computer-vision-at-airbnb-144a4441b72e) `Airbnb` `2019`\n4. [How we Improved Computer Vision Metrics by More Than 5% Only by Cleaning Labelling Errors](https://deepomatic.com/en/how-we-improved-computer-vision-metrics-by-more-than-5-percent-only-by-cleaning-labelling-errors/) `Deepomatic`\n5. [Making machines recognize and transcribe conversations in meetings using audio and video](https://www.microsoft.com/en-us/research/blog/making-machines-recognize-and-transcribe-conversations-in-meetings-using-audio-and-video/) `Microsoft` `2019`\n6. [Powered by AI: Advancing product understanding and building new shopping experiences](https://ai.facebook.com/blog/powered-by-ai-advancing-product-understanding-and-building-new-shopping-experiences/) `Facebook` `2020`\n7. [A Neural Weather Model for Eight-Hour Precipitation Forecasting](https://ai.googleblog.com/2020/03/a-neural-weather-model-for-eight-hour.html) ([Paper](https://arxiv.org/pdf/2003.12140.pdf)) `Google` `2020`\n8. [Machine Learning-based Damage Assessment for Disaster Relief](https://ai.googleblog.com/2020/06/machine-learning-based-damage.html) ([Paper](https://arxiv.org/pdf/1910.06444.pdf)) `Google` `2020`\n9. [RepNet: Counting Repetitions in Videos](https://ai.googleblog.com/2020/06/repnet-counting-repetitions-in-videos.html) ([Paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Dwibedi_Counting_Out_Time_Class_Agnostic_Video_Repetition_Counting_in_the_CVPR_2020_paper.pdf)) `Google` `2020`\n10. [Converting Text to Images for Product Discovery](https://www.amazon.science/blog/converting-text-to-images-for-product-discovery) ([Paper](https://assets.amazon.science/4c/76/5830542547b7a11089ce3af943b4/scipub-972.pdf)) `Amazon` `2020`\n11. [How Disney Uses PyTorch for Animated Character Recognition](https://medium.com/pytorch/how-disney-uses-pytorch-for-animated-character-recognition-a1722a182627) `Disney` `2020`\n12. [Image Captioning as an Assistive Technology](https://www.ibm.com/blogs/research/2020/07/image-captioning-assistive-technology/) ([Video](https://ivc.ischool.utexas.edu/~yz9244/VizWiz_workshop/videos/MMTeam-oral.mp4)) `IBM` `2020`\n13. [AI for AG: Production machine learning for agriculture](https://medium.com/pytorch/ai-for-ag-production-machine-learning-for-agriculture-e8cfdb9849a1) `Blue River` `2020`\n14. [AI for Full-Self Driving at Tesla](https://youtu.be/hx7BXih7zx8?t=513) `Tesla` `2020`\n15. [On-device Supermarket Product Recognition](https://ai.googleblog.com/2020/07/on-device-supermarket-product.html) `Google` `2020`\n16. [Using Machine Learning to Detect Deficient Coverage in Colonoscopy Screenings](https://ai.googleblog.com/2020/08/using-machine-learning-to-detect.html) ([Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9097918)) `Google` `2020`\n17. [Shop The Look: Building a Large Scale Visual Shopping System at Pinterest](https://dl.acm.org/doi/abs/10.1145/3394486.3403372) ([Paper](https://dl.acm.org/doi/pdf/10.1145/3394486.3403372), [Video](https://crossminds.ai/video/5f3369790576dd25aef288d7/)) `Pinterest` `2020`\n18. [Developing Real-Time, Automatic Sign Language Detection for Video Conferencing](https://ai.googleblog.com/2020/10/developing-real-time-automatic-sign.html) ([Paper](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/2eaf0d18ec6bef00d7dd88f39dd4f9ff13eeeeb2.pdf)) `Google` `2020`\n19. [Vision-based Price Suggestion for Online Second-hand Items](https://arxiv.org/abs/2012.06009) ([Paper](https://arxiv.org/pdf/2012.06009.pdf)) `Alibaba` `2020`\n20. [New AI Research to Help Predict COVID-19 Resource Needs From X-rays](https://ai.facebook.com/blog/new-ai-research-to-help-predict-covid-19-resource-needs-from-a-series-of-x-rays/) ([Paper](https://arxiv.org/pdf/2101.04909.pdf), [Model](https://github.com/facebookresearch/CovidPrognosis)) `Facebook` `2021`\n21. [An Efficient Training Approach for Very Large Scale Face Recognition](https://arxiv.org/abs/2105.10375) ([Paper](https://arxiv.org/pdf/2105.10375)) `Alibaba` `2021`\n22. [Identifying Document Types at Scribd](https://tech.scribd.com/blog/2021/identifying-document-types.html) `Scribd` `2021`\n23. [Semi-Supervised Visual Representation Learning for Fashion Compatibility](https://arxiv.org/pdf/2109.08052.pdf) ([Paper](https://arxiv.org/pdf/2109.08052.pdf)) `Walmart` `2021`\n24. [Recognizing People in Photos Through Private On-Device Machine Learning](https://machinelearning.apple.com/research/recognizing-people-photos) `Apple` `2021`\n25. [DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection](https://arxiv.org/pdf/2203.08195.pdf) `Google` `2022`\n26. [Contrastive language and vision learning of general fashion concepts](https://www.nature.com/articles/s41598-022-23052-9) ([Paper](https://www.nature.com/articles/s41598-022-23052-9.pdf))`Coveo` `2022`\n27. [Leveraging Computer Vision for Search Ranking](https://arize.com/resource/bazaarvoice-leveraging-computer-vision-models-for-search-ranking/) `BazaarVoice` `2023`\n\n## Reinforcement Learning\n1. [Deep Reinforcement Learning for Sponsored Search Real-time Bidding](https://arxiv.org/abs/1803.00259) ([Paper](https://arxiv.org/pdf/1803.00259.pdf)) `Alibaba` `2018`\n2. [Budget Constrained Bidding by Model-free Reinforcement Learning in Display Advertising](https://arxiv.org/abs/1802.08365) ([Paper](https://arxiv.org/pdf/1802.08365.pdf)) `Alibaba` `2018`\n3. [Reinforcement Learning for On-Demand Logistics](https://doordash.engineering/2018/09/10/reinforcement-learning-for-on-demand-logistics/) `DoorDash` `2018`\n4. [Reinforcement Learning to Rank in E-Commerce Search Engine](https://arxiv.org/abs/1803.00710) ([Paper](https://arxiv.org/pdf/1803.00710.pdf)) `Alibaba` `2018`\n5. [Dynamic Pricing on E-commerce Platform with Deep Reinforcement Learning](https://arxiv.org/abs/1912.02572) ([Paper](https://arxiv.org/pdf/1912.02572.pdf)) `Alibaba` `2019`\n6. [Productionizing Deep Reinforcement Learning with Spark and MLflow](https://www.youtube.com/watch?v=hy-w69zf4oo) `Zynga` `2020`\n7. [Deep Reinforcement Learning in Production Part1](https://towardsdatascience.com/deep-reinforcement-learning-in-production-7e1e63471e2) [Part 2](https://towardsdatascience.com/deep-reinforcement-learning-in-production-part-2-personalizing-user-notifications-812a68ce2355) `Zynga` `2020`\n8. [Building AI Trading Systems](https://dennybritz.com/blog/ai-trading/) `Denny Britz` `2020`\n9. [Shifting Consumption towards Diverse content via Reinforcement Learning](https://research.atspotify.com/shifting-consumption-towards-diverse-content-via-reinforcement-learning/) ([Paper](https://dl.acm.org/doi/10.1145/3437963.3441775)) `Spotify` `2022`\n10. [Bandits for Online Calibration: An Application to Content Moderation on Social Media Platforms](https://arxiv.org/abs/2211.06516) `Meta` `2022`\n11. [How to Optimise Rankings with Cascade Bandits](https://medium.com/expedia-group-tech/how-to-optimise-rankings-with-cascade-bandits-5d92dfa0f16b) `Expedia` `2022`\n12. [Selecting the Best Image for Each Merchant Using Exploration and Machine Learning](https://doordash.engineering/2023/01/04/selecting-the-best-image-for-each-merchant-using-exploration-and-machine-learning/) `DoorDash` `2023`\n\n## Anomaly Detection\n1. [Detecting Performance Anomalies in External Firmware Deployments](https://netflixtechblog.com/detecting-performance-anomalies-in-external-firmware-deployments-ed41b1bfcf46) `Netflix` `2019`\n2. [Detecting and Preventing Abuse on LinkedIn using Isolation Forests](https://engineering.linkedin.com/blog/2019/isolation-forest) ([Code](https://github.com/linkedin/isolation-forest)) `LinkedIn` `2019`\n3. [Deep Anomaly Detection with Spark and Tensorflow](https://databricks.com/session_eu19/deep-anomaly-detection-from-research-to-production-leveraging-spark-and-tensorflow) [(Hopsworks Video](https://www.youtube.com/watch?v=TgXVU8DSyCQ)) `Swedbank`, `Hopsworks` `2019`\n4. [Preventing Abuse Using Unsupervised Learning](https://www.youtube.com/watch?v=sFRrFWYNAUI) `LinkedIn` `2020`\n5. [The Technology Behind Fighting Harassment on LinkedIn](https://engineering.linkedin.com/blog/2020/fighting-harassment) `LinkedIn` `2020`\n6. [Uncovering Insurance Fraud Conspiracy with Network Learning](https://arxiv.org/abs/2002.12789) ([Paper](https://arxiv.org/pdf/2002.12789.pdf)) `Ant Financial` `2020`\n7. [How Does Spam Protection Work on Stack Exchange?](https://stackoverflow.blog/2020/06/25/how-does-spam-protection-work-on-stack-exchange/) `Stack Exchange` `2020`\n8. [Auto Content Moderation in C2C e-Commerce](https://www.usenix.org/conference/opml20/presentation/ueta) `Mercari` `2020`\n9. [Blocking Slack Invite Spam With Machine Learning](https://slack.engineering/blocking-slack-invite-spam-with-machine-learning/) `Slack` `2020`\n10. [Cloudflare Bot Management: Machine Learning and More](https://blog.cloudflare.com/cloudflare-bot-management-machine-learning-and-more/) `Cloudflare` `2020`\n11. [Anomalies in Oil Temperature Variations in a Tunnel Boring Machine](https://www.youtube.com/watch?v=YV_uLLhPRAk) `SENER` `2020`\n12. [Using Anomaly Detection to Monitor Low-Risk Bank Customers](https://www.youtube.com/watch?v=MExokMM_Bp4&t=3s) `Rabobank` `2020`\n13. [Fighting fraud with Triplet Loss](https://tech.olx.com/fighting-fraud-with-triplet-loss-86e5f79c7a3e) `OLX Group` `2020`\n14. [Facebook is Now Using AI to Sort Content for Quicker Moderation](https://www.theverge.com/2020/11/13/21562596/facebook-ai-moderation) ([Alternative](https://venturebeat.com/2020/11/13/facebooks-redoubled-ai-efforts-wont-stop-the-spread-of-harmful-content/)) `Facebook` `2020`\n15. How AI is getting better at detecting hate speech [Part 1](https://ai.facebook.com/blog/how-ai-is-getting-better-at-detecting-hate-speech/), [Part 2](https://ai.facebook.com/blog/heres-how-were-using-ai-to-help-detect-misinformation/), [Part 3](https://ai.facebook.com/blog/training-ai-to-detect-hate-speech-in-the-real-world/), [Part 4](https://ai.facebook.com/blog/how-facebook-uses-super-efficient-ai-models-to-detect-hate-speech/) `Facebook` `2020`\n16. [Using deep learning to detect abusive sequences of member activity](https://engineering.linkedin.com/blog/2021/using-deep-learning-to-detect-abusive-sequences-of-member-activi) ([Video](https://exchange.scale.com/public/videos/using-deep-learning-to-detect-abusive-sequences-of-member-activity-on-linkedin)) `LinkedIn` `2021`\n17. [Project RADAR: Intelligent Early Fraud Detection System with Humans in the Loop](https://eng.uber.com/project-radar-intelligent-early-fraud-detection/) `Uber` `2022`\n18. [Graph for Fraud Detection](https://engineering.grab.com/graph-for-fraud-detection) `Grab` `2022`\n19. [Bandits for Online Calibration: An Application to Content Moderation on Social Media Platforms](https://arxiv.org/abs/2211.06516) `Meta` `2022`\n20. [Evolving our machine learning to stop mobile bots](https://blog.cloudflare.com/machine-learning-mobile-traffic-bots/) `Cloudflare` `2022`\n21. [Improving the accuracy of our machine learning WAF using data augmentation and sampling](https://blog.cloudflare.com/data-generation-and-sampling-strategies/) `Cloudflare` `2022`\n22. [Machine Learning for Fraud Detection in Streaming Services](https://netflixtechblog.com/machine-learning-for-fraud-detection-in-streaming-services-b0b4ef3be3f6) `Netflix` `2022`\n23. [Pricing at Lyft](https://eng.lyft.com/pricing-at-lyft-8a4022065f8b) `Lyft` `2022`\n\n## Graph\n1. [Building The LinkedIn Knowledge Graph](https://engineering.linkedin.com/blog/2016/10/building-the-linkedin-knowledge-graph) `LinkedIn` `2016`\n2. [Scaling Knowledge Access and Retrieval at Airbnb](https://medium.com/airbnb-engineering/scaling-knowledge-access-and-retrieval-at-airbnb-665b6ba21e95) `Airbnb` `2018`\n3. [Graph Convolutional Neural Networks for Web-Scale Recommender Systems](https://arxiv.org/abs/1806.01973) ([Paper](https://arxiv.org/pdf/1806.01973.pdf))`Pinterest` `2018`\n4. [Food Discovery with Uber Eats: Using Graph Learning to Power Recommendations](https://eng.uber.com/uber-eats-graph-learning/) `Uber` `2019`\n5. [AliGraph: A Comprehensive Graph Neural Network Platform](https://arxiv.org/abs/1902.08730) ([Paper](https://arxiv.org/pdf/1902.08730.pdf)) `Alibaba` `2019`\n6. [Contextualizing Airbnb by Building Knowledge Graph](https://medium.com/airbnb-engineering/contextualizing-airbnb-by-building-knowledge-graph-b7077e268d5a) `Airbnb` `2019`\n7. [Retail Graph â€” Walmartâ€™s Product Knowledge Graph](https://medium.com/walmartlabs/retail-graph-walmarts-product-knowledge-graph-6ef7357963bc) `Walmart` `2020`\n8. [Traffic Prediction with Advanced Graph Neural Networks](https://deepmind.com/blog/article/traffic-prediction-with-advanced-graph-neural-networks) `DeepMind` `2020`\n9. [SimClusters: Community-Based Representations for Recommendations](https://dl.acm.org/doi/10.1145/3394486.3403370) ([Paper](https://dl.acm.org/doi/pdf/10.1145/3394486.3403370), [Video](https://crossminds.ai/video/5f3369790576dd25aef288d5/)) `Twitter` `2020`\n10. [Metapaths guided Neighbors aggregated Network for Heterogeneous Graph Reasoning](https://arxiv.org/abs/2103.06474) ([Paper](https://arxiv.org/pdf/2103.06474.pdf)) `Alibaba` `2021`\n11. [Graph Intention Network for Click-through Rate Prediction in Sponsored Search](https://arxiv.org/abs/2103.16164) ([Paper](https://arxiv.org/pdf/2103.16164.pdf)) `Alibaba` `2021`\n12. [JEL: Applying End-to-End Neural Entity Linking in JPMorgan Chase](https://ojs.aaai.org/index.php/AAAI/article/view/17796) ([Paper](https://www.aaai.org/AAAI21Papers/IAAI-21.DingW.pdf)) `JPMorgan Chase` `2021`\n13. [How AWS uses graph neural networks to meet customer needs](https://www.amazon.science/blog/how-aws-uses-graph-neural-networks-to-meet-customer-needs) `Amazon` `2022`\n14. [Graph for Fraud Detection](https://engineering.grab.com/graph-for-fraud-detection) `Grab` `2022`\n\n## Optimization\n1. [Matchmaking in Lyft Line (Part 1)](https://eng.lyft.com/matchmaking-in-lyft-line-9c2635fe62c4) [(Part 2)](https://eng.lyft.com/matchmaking-in-lyft-line-691a1a32a008) [(Part 3)](https://eng.lyft.com/matchmaking-in-lyft-line-part-3-d8f9497c0e51) `Lyft` `2016`\n2. [The Data and Science behind GrabShare Carpooling](https://ieeexplore.ieee.org/document/8259801) [(Part 1)](https://engineering.grab.com/the-data-and-science-behind-grabshare-part-i) (**PAPER NEEDED**) `Grab` `2017`\n3. [How Trip Inferences and Machine Learning Optimize Delivery Times on Uber Eats](https://eng.uber.com/uber-eats-trip-optimization/) `Uber` `2018`\n4. [Next-Generation Optimization for Dasher Dispatch at DoorDash](https://doordash.engineering/2020/02/28/next-generation-optimization-for-dasher-dispatch-at-doordash/) `DoorDash` `2020` \n5. [Optimization of Passengers Waiting Time in Elevators Using Machine Learning](https://www.youtube.com/watch?v=vXndCC89BCw&t=4s) `Thyssen Krupp AG` `2020`\n6. [Think Out of The Package: Recommending Package Types for E-commerce Shipments](https://www.amazon.science/publications/think-out-of-the-package-recommending-package-types-for-e-commerce-shipments) ([Paper](https://assets.amazon.science/0c/6c/9d0986b94bef92d148f0ac0da1ea/think-out-of-the-package-recommending-package-types-for-e-commerce-shipments.pdf)) `Amazon` `2020`\n7. [Optimizing DoorDashâ€™s Marketing Spend with Machine Learning](https://doordash.engineering/2020/07/31/optimizing-marketing-spend-with-ml/) `DoorDash` `2020`\n8. [Using learning-to-rank to precisely locate where to deliver packages](https://www.amazon.science/blog/using-learning-to-rank-to-precisely-locate-where-to-deliver-packages) ([Paper](https://assets.amazon.science/69/8d/2249945a4e10ba8fc758f7523b0c/getting-your-package-to-the-right-place-supervised-machine-learning-for-geolocation.pdf))`Amazon` `2021`\n\n## Information Extraction\n1. [Unsupervised Extraction of Attributes and Their Values from Product Description](https://www.aclweb.org/anthology/I13-1190/) ([Paper](https://www.aclweb.org/anthology/I13-1190.pdf)) `Rakuten` `2013`\n2. [Using Machine Learning to Index Text from Billions of Images](https://dropbox.tech/machine-learning/using-machine-learning-to-index-text-from-billions-of-images) `Dropbox` `2018`\n3. [Extracting Structured Data from Templatic Documents](https://ai.googleblog.com/2020/06/extracting-structured-data-from.html) ([Paper](https://www.aclweb.org/anthology/I13-1190.pdf)) `Google` `2020`\n4. [AutoKnow: self-driving knowledge collection for products of thousands of types](https://www.amazon.science/publications/autoknow-self-driving-knowledge-collection-for-products-of-thousands-of-types) ([Paper](https://arxiv.org/pdf/2006.13473.pdf), [Video](https://crossminds.ai/video/5f3369730576dd25aef288a6/)) `Amazon` `2020`\n5. [One-shot Text Labeling using Attention and Belief Propagation for Information Extraction](https://arxiv.org/abs/2009.04153) ([Paper](https://arxiv.org/pdf/2009.04153.pdf)) `Alibaba` `2020`\n6. [Information Extraction from Receipts with Graph Convolutional Networks](https://nanonets.com/blog/information-extraction-graph-convolutional-networks/) `Nanonets` `2021`\n\n## Weak Supervision\n1. [Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale](https://dl.acm.org/doi/abs/10.1145/3299869.3314036) ([Paper](https://dl.acm.org/doi/pdf/10.1145/3299869.3314036)) `Google` `2019`\n2. [Osprey: Weak Supervision of Imbalanced Extraction Problems without Code](https://dl.acm.org/doi/abs/10.1145/3329486.3329492) ([Paper](https://ajratner.github.io/assets/papers/Osprey_DEEM.pdf)) `Intel` `2019` \n3. [Overton: A Data System for Monitoring and Improving Machine-Learned Products](https://arxiv.org/abs/1909.05372) ([Paper](https://arxiv.org/pdf/1909.05372.pdf)) `Apple` `2019`\n4. [Bootstrapping Conversational Agents with Weak Supervision](https://www.aaai.org/ojs/index.php/AAAI/article/view/5011) ([Paper](https://arxiv.org/pdf/1812.06176.pdf)) `IBM` `2019`\n\n## Generation\n1. [Better Language Models and Their Implications](https://openai.com/blog/better-language-models/) ([Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf))`OpenAI` `2019`\n2. [Image GPT](https://openai.com/blog/image-gpt/) ([Paper](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf), [Code](https://github.com/openai/image-gpt)) `OpenAI` `2019`\n3. [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) ([Paper](https://arxiv.org/pdf/2005.14165.pdf)) ([GPT-3 Blog post](https://openai.com/blog/openai-api/)) `OpenAI` `2020`\n4. [Deep Learned Super Resolution for Feature Film Production](https://graphics.pixar.com/library/SuperResolution/) ([Paper](https://graphics.pixar.com/library/SuperResolution/paper.pdf)) `Pixar` `2020`\n5. [Unit Test Case Generation with Transformers](https://arxiv.org/pdf/2009.05617.pdf) `Microsoft` `2021`\n\n## Audio\n1. [Improving On-Device Speech Recognition with VoiceFilter-Lite](https://ai.googleblog.com/2020/11/improving-on-device-speech-recognition.html) ([Paper](https://arxiv.org/pdf/2009.04323.pdf))`Google` `2020`\n2. [The Machine Learning Behind Hum to Search](https://ai.googleblog.com/2020/11/the-machine-learning-behind-hum-to.html) `Google` `2020`\n\n## Privacy-preserving Machine Learning\n1. [Federated Learning: Collaborative Machine Learning without Centralized Training Data](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html) ([Paper](https://arxiv.org/pdf/1602.05629)) `Google` `2017`\n2. [Federated Learning with Formal Differential Privacy Guarantees](https://ai.googleblog.com/2022/02/federated-learning-with-formal.html) ([Paper](https://arxiv.org/pdf/2103.00039)) `Google` `2022`\n3. [MPC-based machine learning: Achieving end-to-end privacy-preserving machine learning](https://research.facebook.com/blog/2022/10/mpc-based-machine-learning-achieving-end-to-end-privacy-preserving-machine-learning/) ([Paper](https://research.facebook.com/file/455681589729383/Private-Computation-Framework-2.0-White-Paper.pdf)) `Facebook` `2022`\n\n\n## Validation and A/B Testing\n1. [Overlapping Experiment Infrastructure: More, Better, Faster Experimentation](https://research.google/pubs/pub36500/) ([Paper](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/36500.pdf)) `Google` `2010`\n2. [The Reusable Holdout: Preserving Validity in Adaptive Data Analysis](https://ai.googleblog.com/2015/08/the-reusable-holdout-preserving.html) ([Paper](https://science.sciencemag.org/content/sci/349/6248/636.full.pdf)) `Google` `2015`\n3. [Twitter Experimentation: Technical Overview](https://blog.twitter.com/engineering/en_us/a/2015/twitter-experimentation-technical-overview.html) `Twitter` `2015`\n4. [Itâ€™s All A/Bout Testing: The Netflix Experimentation Platform](https://netflixtechblog.com/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15) `Netflix` `2016`\n5. [Building Pinterestâ€™s A/B Testing Platform](https://medium.com/pinterest-engineering/building-pinterests-a-b-testing-platform-ab4934ace9f4) `Pinterest` `2016` \n6. [Experimenting to Solve Cramming](https://blog.twitter.com/engineering/en_us/topics/insights/2017/Experimenting-To-Solve-Cramming.html) `Twitter` `2017`\n7. [Building an Intelligent Experimentation Platform with Uber Engineering](https://eng.uber.com/experimentation-platform/) `Uber` `2017`\n8. [Scaling Airbnbâ€™s Experimentation Platform](https://medium.com/airbnb-engineering/https-medium-com-jonathan-parks-scaling-erf-23fd17c91166) `Airbnb` `2017`\n9. [Meet Wasabi, an Open Source A/B Testing Platform](https://www.intuit.com/blog/technology/engineering/meet-wasabi-an-open-source-ab-testing-platform/) ([Code](https://github.com/intuit/wasabi)) `Intuit` `2017` \n10. [Analyzing Experiment Outcomes: Beyond Average Treatment Effects](https://eng.uber.com/analyzing-experiment-outcomes/) `Uber` `2018`\n11. [Under the Hood of Uberâ€™s Experimentation Platform](https://eng.uber.com/xp/) `Uber` `2018`\n12. [Constrained Bayesian Optimization with Noisy Experiments](https://research.fb.com/publications/constrained-bayesian-optimization-with-noisy-experiments/) ([Paper](https://arxiv.org/pdf/1706.07094.pdf)) `Facebook` `2018`\n13. [Reliable and Scalable Feature Toggles and A/B Testing SDK at Grab](https://engineering.grab.com/feature-toggles-ab-testing) `Grab` `2018`\n14. [Modeling Conversion Rates and Saving Millions Using Kaplan-Meier and Gamma Distributions](https://better.engineering/modeling-conversion-rates-and-saving-millions-of-dollars-using-kaplan-meier-and-gamma-distributions/) ([Code](https://github.com/better/convoys)) `Better` `2019`\n15. [Detecting Interference: An A/B Test of A/B Tests](https://engineering.linkedin.com/blog/2019/06/detecting-interference--an-a-b-test-of-a-b-tests) `LinkedIn` `2019`\n16. [Announcing a New Framework for Designing Optimal Experiments with Pyro](https://eng.uber.com/oed-pyro-release/) ([Paper](https://papers.nips.cc/paper/9553-variational-bayesian-optimal-experimental-design.pdf)) ([Paper](https://arxiv.org/pdf/1911.00294.pdf)) `Uber` `2020`\n17. [Enabling 10x More Experiments with Traveloka Experiment Platform](https://medium.com/traveloka-engineering/enabling-10x-more-experiments-with-traveloka-experiment-platform-8cea13e952c) `Traveloka` `2020`\n18. [Large Scale Experimentation at Stitch Fix](https://multithreaded.stitchfix.com/blog/2020/07/07/large-scale-experimentation/) ([Paper](http://proceedings.mlr.press/v89/schmit19a/schmit19a.pdf)) `Stitch Fix` `2020`\n19. [Multi-Armed Bandits and the Stitch Fix Experimentation Platform](https://multithreaded.stitchfix.com/blog/2020/08/05/bandits/) `Stitch Fix` `2020`\n20. [Experimentation with Resource Constraints](https://multithreaded.stitchfix.com/blog/2020/11/18/virtual-warehouse/) `Stitch Fix` `2020`\n21. [Computational Causal Inference at Netflix](https://netflixtechblog.com/computational-causal-inference-at-netflix-293591691c62) ([Paper](https://arxiv.org/pdf/2007.10979.pdf)) `Netflix` `2020`\n22. [Key Challenges with Quasi Experiments at Netflix](https://netflixtechblog.com/key-challenges-with-quasi-experiments-at-netflix-89b4f234b852) `Netflix` `2020`\n23. [Making the LinkedIn experimentation engine 20x faster](https://engineering.linkedin.com/blog/2020/making-the-linkedin-experimentation-engine-20x-faster) `LinkedIn` `2020`\n24. [Our Evolution Towards T-REX: The Prehistory of Experimentation Infrastructure at LinkedIn](https://engineering.linkedin.com/blog/2020/our-evolution-towards-t-rex--the-prehistory-of-experimentation-i) `LinkedIn` `2020`\n25. [How to Use Quasi-experiments and Counterfactuals to Build Great Products](https://engineering.shopify.com/blogs/engineering/using-quasi-experiments-counterfactuals) `Shopify` `2020`\n26. [Improving Experimental Power through Control Using Predictions as Covariate](https://doordash.engineering/2020/06/08/improving-experimental-power-through-control-using-predictions-as-covariate-cupac/) `DoorDash` `2020`\n27. [Supporting Rapid Product Iteration with an Experimentation Analysis Platform](https://doordash.engineering/2020/09/09/experimentation-analysis-platform-mvp/) `DoorDash` `2020`\n28. [Improving Online Experiment Capacity by 4X with Parallelization and Increased Sensitivity](https://doordash.engineering/2020/10/07/improving-experiment-capacity-by-4x/) `DoorDash` `2020`\n29. [Leveraging Causal Modeling to Get More Value from Flat Experiment Results](https://doordash.engineering/2020/09/18/causal-modeling-to-get-more-value-from-flat-experiment-results/) `DoorDash` `2020`\n30. [Iterating Real-time Assignment Algorithms Through Experimentation](https://doordash.engineering/2020/12/08/optimizing-real-time-algorithms-experimentation/) `DoorDash` `2020`\n31. [Spotifyâ€™s New Experimentation Platform (Part 1)](https://engineering.atspotify.com/2020/10/29/spotifys-new-experimentation-platform-part-1/) [(Part 2)](https://engineering.atspotify.com/2020/11/02/spotifys-new-experimentation-platform-part-2/) `Spotify` `2020`\n32. [Interpreting A/B Test Results: False Positives and Statistical Significance](https://netflixtechblog.com/interpreting-a-b-test-results-false-positives-and-statistical-significance-c1522d0db27a) `Netflix` `2021`\n33. [Interpreting A/B Test Results: False Negatives and Power](https://netflixtechblog.com/interpreting-a-b-test-results-false-negatives-and-power-6943995cf3a8) `Netflix` `2021`\n34. [Running Experiments with Google Adwords for Campaign Optimization](https://doordash.engineering/2021/02/05/google-adwords-campaign-optimization/) `DoorDash` `2021`\n35. [The 4 Principles DoorDash Used to Increase Its Logistics Experiment Capacity by 1000%](https://doordash.engineering/2021/09/21/the-4-principles-doordash-used-to-increase-its-logistics-experiment-capacity-by-1000/) `DoorDash` `2021`\n36. [Experimentation Platform at Zalando: Part 1 - Evolution](https://engineering.zalando.com/posts/2021/01/experimentation-platform-part1.html) `Zalando` `2021`\n37. [Designing Experimentation Guardrails](https://medium.com/airbnb-engineering/designing-experimentation-guardrails-ed6a976ec669) `Airbnb` `2021`\n38. [How Airbnb Measures Future Value to Standardize Tradeoffs](https://medium.com/airbnb-engineering/how-airbnb-measures-future-value-to-standardize-tradeoffs-3aa99a941ba5) `Airbnb` `2021`\n38. [Network Experimentation at Scale](https://research.fb.com/publications/network-experimentation-at-scale/)([Paper](https://arxiv.org/abs/2012.08591)] `Facebook` `2021`\n39. [Universal Holdout Groups at Disney Streaming](https://medium.com/disney-streaming/universal-holdout-groups-at-disney-streaming-2043360def4f) `Disney` `2021`\n40. [Experimentation is a major focus of Data Science across Netflix](https://netflixtechblog.com/experimentation-is-a-major-focus-of-data-science-across-netflix-f67923f8e985) `Netflix` `2022`\n41. [Search Journey Towards Better Experimentation Practices](https://engineering.atspotify.com/2022/02/search-journey-towards-better-experimentation-practices/) `Spotify` `2022`\n42. [Artificial Counterfactual Estimation: Machine Learning-Based Causal Inference at Airbnb](https://medium.com/airbnb-engineering/artificial-counterfactual-estimation-ace-machine-learning-based-causal-inference-at-airbnb-ee32ee4d0512) `Airbnb` `2022`\n43. [Beyond A/B Test : Speeding up Airbnb Search Ranking Experimentation through Interleaving](https://medium.com/airbnb-engineering/beyond-a-b-test-speeding-up-airbnb-search-ranking-experimentation-through-interleaving-7087afa09c8e) `Airbnb` `2022`\n44. [Challenges in Experimentation](https://eng.lyft.com/challenges-in-experimentation-be9ab98a7ef4) `Lyft` `2022`\n45. [Overtracking and Trigger Analysis: Reducing sample sizes while INCREASING sensitivity](https://booking.ai/overtracking-and-trigger-analysis-how-to-reduce-sample-sizes-and-increase-the-sensitivity-of-71755bad0e5f) `Booking` `2022`\n46. [Meet Dash-AB â€” The Statistics Engine of Experimentation at DoorDash](https://doordash.engineering/2022/05/24/meet-dash-ab-the-statistics-engine-of-experimentation-at-doordash/) `DoorDash` `2022`\n47. [Comparing quantiles at scale in online A/B-testing](https://engineering.atspotify.com/2022/03/comparing-quantiles-at-scale-in-online-a-b-testing) `Spotify` `2022`\n48. [Accelerating our A/B experiments with machine learning](https://dropbox.tech/machine-learning/accelerating-our-a-b-experiments-with-machine-learning-xr) `Dropbox` `2023`\n49. [Supercharging A/B Testing at Uber](https://www.uber.com/blog/supercharging-a-b-testing-at-uber/) `Uber` \n\n## Model Management\n1. [Operationalizing Machine Learningâ€”Managing Provenance from Raw Data to Predictions](https://vimeo.com/274396495) `Comcast` `2018`\n2. [Overton: A Data System for Monitoring and Improving Machine-Learned Products](https://arxiv.org/abs/1909.05372) ([Paper](https://arxiv.org/pdf/1909.05372.pdf)) `Apple` `2019`\n3. [Runway - Model Lifecycle Management at Netflix](https://www.usenix.org/conference/opml20/presentation/cepoi) `Netflix` `2020`\n4. [Managing ML Models @ Scale - Intuitâ€™s ML Platform](https://www.usenix.org/conference/opml20/presentation/wenzel) `Intuit` `2020`\n5. [ML Model Monitoring - 9 Tips From the Trenches](https://building.nubank.com.br/ml-model-monitoring-9-tips-from-the-trenches/) `Nubank` `2021`\n6. [Dealing with Train-serve Skew in Real-time ML Models: A Short Guide](https://building.nubank.com.br/dealing-with-train-serve-skew-in-real-time-ml-models-a-short-guide/) `Nubank` `2023`\n\n## Efficiency\n1. [GrokNet: Unified Computer Vision Model Trunk and Embeddings For Commerce](https://ai.facebook.com/research/publications/groknet-unified-computer-vision-model-trunk-and-embeddings-for-commerce/) ([Paper](https://scontent-sea1-1.xx.fbcdn.net/v/t39.8562-6/99353320_565175057533429_3886205100842024960_n.pdf?_nc_cat=110&_nc_sid=ae5e01&_nc_ohc=WQBaZy1gnmUAX8Ecqtt&_nc_ht=scontent-sea1-1.xx&oh=cab2f11dd9154d817149cb73e8b692a8&oe=5F5A3778)) `Facebook` `2020`\n2. [How We Scaled Bert To Serve 1+ Billion Daily Requests on CPUs](https://blog.roblox.com/2020/05/scaled-bert-serve-1-billion-daily-requests-cpus/) `Roblox` `2020`\n3. [Permute, Quantize, and Fine-tune: Efficient Compression of Neural Networks](https://arxiv.org/abs/2010.15703) ([Paper](https://arxiv.org/pdf/2010.15703.pdf)) `Uber` `2021`\n4. [GPU-accelerated ML Inference at Pinterest](https://medium.com/@Pinterest_Engineering/gpu-accelerated-ml-inference-at-pinterest-ad1b6a03a16d) `Pinterest` `2022`\n\n## Ethics\n1. [Building Inclusive Products Through A/B Testing](https://engineering.linkedin.com/blog/2020/building-inclusive-products-through-a-b-testing) ([Paper](https://arxiv.org/pdf/2002.05819.pdf)) `LinkedIn` `2020`\n2. [LiFT: A Scalable Framework for Measuring Fairness in ML Applications](https://engineering.linkedin.com/blog/2020/lift-addressing-bias-in-large-scale-ai-applications) ([Paper](https://arxiv.org/pdf/2008.07433.pdf)) `LinkedIn` `2020`\n3. [Introducing Twitterâ€™s first algorithmic bias bounty challenge](https://blog.twitter.com/engineering/en_us/topics/insights/2021/algorithmic-bias-bounty-challenge) `Twitter` `2021`\n4. [Examining algorithmic amplification of political content on Twitter](https://blog.twitter.com/en_us/topics/company/2021/rml-politicalcontent) `Twitter` `2021`\n5. [A closer look at how LinkedIn integrates fairness into its AI products](https://engineering.linkedin.com/blog/2022/a-closer-look-at-how-linkedin-integrates-fairness-into-its-ai-pr) `LinkedIn` `2022`\n\n## Infra\n1. [Reengineering Facebook AIâ€™s Deep Learning Platforms for Interoperability](https://ai.facebook.com/blog/reengineering-facebook-ais-deep-learning-platforms-for-interoperability) `Facebook` `2020`\n2. [Elastic Distributed Training with XGBoost on Ray](https://eng.uber.com/elastic-xgboost-ray/) `Uber` `2021`\n\n## MLOps Platforms\n1. [Meet Michelangelo: Uberâ€™s Machine Learning Platform](https://eng.uber.com/michelangelo-machine-learning-platform/) `Uber` `2017`\n2. [Operationalizing Machine Learningâ€”Managing Provenance from Raw Data to Predictions](https://vimeo.com/274396495) `Comcast` `2018`\n3. [Big Data Machine Learning Platform at Pinterest](https://www.slideshare.net/Alluxio/pinterest-big-data-machine-learning-platform-at-pinterest) `Pinterest` `2019`\n4. [Core Modeling at Instagram](https://instagram-engineering.com/core-modeling-at-instagram-a51e0158aa48) `Instagram` `2019`\n5. [Open-Sourcing Metaflow - a Human-Centric Framework for Data Science](https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9) `Netflix` `2019`\n6. [Managing ML Models @ Scale - Intuitâ€™s ML Platform](https://www.usenix.org/conference/opml20/presentation/wenzel) `Intuit` `2020`\n7. [Real-time Machine Learning Inference Platform at Zomato](https://www.youtube.com/watch?v=0-3ES1vzW14) `Zomato` `2020`\n8. [Introducing Flyte: Cloud Native Machine Learning and Data Processing Platform](https://eng.lyft.com/introducing-flyte-cloud-native-machine-learning-and-data-processing-platform-fb2bb3046a59) `Lyft` `2020`\n9. [Building Flexible Ensemble ML Models with a Computational Graph](https://doordash.engineering/2021/01/26/computational-graph-machine-learning-ensemble-model-support/) `DoorDash` `2021`\n10. [LyftLearn: ML Model Training Infrastructure built on Kubernetes](https://eng.lyft.com/lyftlearn-ml-model-training-infrastructure-built-on-kubernetes-aef8218842bb) `Lyft` `2021`\n11. ["You Don''t Need a Bigger Boat": A Full Data Pipeline Built with Open-Source Tools](https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat) ([Paper](https://arxiv.org/abs/2107.07346)) `Coveo` `2021`\n12. [MLOps at GreenSteam: Shipping Machine Learning](https://neptune.ai/blog/mlops-at-greensteam-shipping-machine-learning-case-study) `GreenSteam` `2021`\n13. [Evolving Redditâ€™s ML Model Deployment and Serving Architecture](https://www.reddit.com/r/RedditEng/comments/q14tsw/evolving_reddits_ml_model_deployment_and_serving/) `Reddit` `2021`\n14. [Redesigning Etsyâ€™s Machine Learning Platform](https://www.etsy.com/codeascraft/redesigning-etsys-machine-learning-platform/) `Etsy` `2021`\n15. [Understanding Data Storage and Ingestion for Large-Scale Deep Recommendation Model Training](https://arxiv.org/abs/2108.09373) ([Paper](https://arxiv.org/pdf/2108.09373.pdf)) `Meta` `2021`\n15. [Building a Platform for Serving Recommendations at Etsy](https://www.etsy.com/codeascraft/building-a-platform-for-serving-recommendations-at-etsy) `Etsy` `2022` \n16. [Intelligent Automation Platform: Empowering Conversational AI and Beyond at Airbnb](https://medium.com/airbnb-engineering/intelligent-automation-platform-empowering-conversational-ai-and-beyond-at-airbnb-869c44833ff2) `Airbnb` `2022`\n17. [DARWIN: Data Science and Artificial Intelligence Workbench at LinkedIn](https://engineering.linkedin.com/blog/2022/darwin--data-science-and-artificial-intelligence-workbench-at-li) `LinkedIn` `2022`\n18. [The Magic of Merlin: Shopify''s New Machine Learning Platform](https://shopify.engineering/merlin-shopify-machine-learning-platform) `Shopify` `2022`\n19. [Zalando''s Machine Learning Platform](https://engineering.zalando.com/posts/2022/04/zalando-machine-learning-platform.html) `Zalando` `2022`\n20. [Inside Meta''s AI optimization platform for engineers across the company](https://ai.facebook.com/blog/looper-meta-ai-optimization-platform-for-engineers/) ([Paper](https://arxiv.org/pdf/2110.07554.pdf)) `Meta` `2022`\n21. [Monzoâ€™s machine learning stack](https://monzo.com/blog/2022/04/26/monzos-machine-learning-stack) `Monzo` `2022`\n22. [Evolution of ML Fact Store](https://netflixtechblog.com/evolution-of-ml-fact-store-5941d3231762) `Netflix` `2022`\n23. [Using MLOps to Build a Real-time End-to-End Machine Learning Pipeline](https://www.binance.com/en/blog/all/using-mlops-to-build-a-realtime-endtoend-machine-learning-pipeline-3820048062346322706) `Binance` `2022`\n24. [Serving Machine Learning Models Efficiently at Scale at Zillow](https://www.zillow.com/tech/serving-machine-learning-models-efficiently-at-scale-at-zillow/) `Zillow` `2022`\n25. [Didact AI: The anatomy of an ML-powered stock picking engine](https://principiamundi.com/posts/didact-anatomy/?utm_campaign=Data_Elixir&utm_source=Data_Elixir_407/) `Didact AI` `2022`\n26. [Deployment for Free - A Machine Learning Platform for Stitch Fix''s Data Scientists](https://multithreaded.stitchfix.com/blog/2022/07/14/deployment-for-free/) `Stitch Fix` `2022`\n27. [Machine Learning Operations (MLOps): Overview, Definition, and Architecture](https://arxiv.org/abs/2205.02302) ([Paper](https://arxiv.org/ftp/arxiv/papers/2205/2205.02302.pdf)) `IBM` `2022`\n\n## Practices\n1. [Practical Recommendations for Gradient-Based Training of Deep Architectures](https://arxiv.org/abs/1206.5533) ([Paper](https://arxiv.org/pdf/1206.5533.pdf)) `Yoshua Bengio` `2012`\n2. [Machine Learning: The High Interest Credit Card of Technical Debt](https://research.google/pubs/pub43146/) ([Paper](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/43146.pdf)) ([Paper](https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf)) `Google` `2014`\n3. [Rules of Machine Learning: Best Practices for ML Engineering](https://developers.google.com/machine-learning/guides/rules-of-ml) `Google` `2018`\n4. [On Challenges in Machine Learning Model Management](http://sites.computer.org/debull/A18dec/p5.pdf) `Amazon` `2018`\n5. [Machine Learning in Production: The Booking.com Approach](https://booking.ai/https-booking-ai-machine-learning-production-3ee8fe943c70) `Booking` `2019`\n6. [150 Successful Machine Learning Models: 6 Lessons Learned at Booking.com](https://booking.ai/150-successful-machine-learning-models-6-lessons-learned-at-booking-com-681e09107bec) ([Paper](https://dl.acm.org/doi/pdf/10.1145/3292500.3330744)) `Booking` `2019`\n7. [Successes and Challenges in Adopting Machine Learning at Scale at a Global Bank](https://www.youtube.com/watch?v=QYQKG5OcwEI) `Rabobank` `2019`\n8. [Challenges in Deploying Machine Learning: a Survey of Case Studies](https://arxiv.org/abs/2011.09926) ([Paper](https://arxiv.org/pdf/2011.09926.pdf)) `Cambridge` `2020`\n9. [Reengineering Facebook AIâ€™s Deep Learning Platforms for Interoperability](https://ai.facebook.com/blog/reengineering-facebook-ais-deep-learning-platforms-for-interoperability) `Facebook` `2020`\n10. [The problem with AI developer tools for enterprises](https://towardsdatascience.com/the-problem-with-ai-developer-tools-for-enterprises-and-what-ikea-has-to-do-with-it-b26277841661) `Databricks` `2020`\n11. [Continuous Integration ', '{"language":null,"stars":28541,"forks":3829,"watchers":28541,"open_issues":2,"topics":["applied-data-science","applied-machine-learning","computer-vision","data-discovery","data-engineering","data-quality","data-science","deep-learning","machine-learning","natural-language-processing","production","recsys","reinforcement-learning","search"],"default_branch":"main","size_kb":397,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:eugeneyan:ml-surveys","source_url":"https://github.com/eugeneyan/ml-surveys"},{"type":"has_code","target_id":"github:apache:atlas","source_url":"https://github.com/apache/atlas"},{"type":"has_code","target_id":"github:MarquezProject:marquez","source_url":"https://github.com/MarquezProject/marquez"},{"type":"has_code","target_id":"github:Netflix:metacat","source_url":"https://github.com/Netflix/metacat"},{"type":"has_code","target_id":"github:lyft:amundsen","source_url":"https://github.com/lyft/amundsen"},{"type":"has_code","target_id":"github:linkedin:datahub","source_url":"https://github.com/linkedin/datahub"},{"type":"has_code","target_id":"github:Netflix:nf-data-explorer","source_url":"https://github.com/Netflix/nf-data-explorer"},{"type":"has_code","target_id":"github:feast-dev:feast","source_url":"https://github.com/feast-dev/feast"},{"type":"has_code","target_id":"github:quintoandar:butterfree","source_url":"https://github.com/quintoandar/butterfree"},{"type":"has_code","target_id":"github:Netflix-Skunkworks:riskquant","source_url":"https://github.com/Netflix-Skunkworks/riskquant"},{"type":"has_code","target_id":"github:facebook:prophet","source_url":"https://github.com/facebook/prophet"},{"type":"has_code","target_id":"github:uber:orbit","source_url":"https://github.com/uber/orbit"},{"type":"has_code","target_id":"github:dariasor:TreeExtra","source_url":"https://github.com/dariasor/TreeExtra"},{"type":"has_code","target_id":"github:linkedin:gdmix","source_url":"https://github.com/linkedin/gdmix"},{"type":"has_code","target_id":"github:seznam:DaReCzech","source_url":"https://github.com/seznam/DaReCzech"},{"type":"has_code","target_id":"github:google-research:google-research","source_url":"https://github.com/google-research/google-research"},{"type":"has_code","target_id":"github:linkedin:detext","source_url":"https://github.com/linkedin/detext"},{"type":"has_code","target_id":"github:facebookresearch:TransCoder","source_url":"https://github.com/facebookresearch/TransCoder"},{"type":"has_code","target_id":"github:google-research:pegasus","source_url":"https://github.com/google-research/pegasus"},{"type":"has_code","target_id":"github:salesforce:GeDi","source_url":"https://github.com/salesforce/GeDi"},{"type":"has_code","target_id":"github:facebookresearch:dynalab","source_url":"https://github.com/facebookresearch/dynalab?fbclid=IwAR3qcV7QK2uXm4s4M0XUoQQo4i2DEsDy0LZFKxSQCHhP-3hF6fr2-NDFWX8"},{"type":"has_code","target_id":"github:pytorch:fairseq","source_url":"https://github.com/pytorch/fairseq"},{"type":"has_code","target_id":"github:facebookresearch:CovidPrognosis","source_url":"https://github.com/facebookresearch/CovidPrognosis"},{"type":"has_code","target_id":"github:linkedin:isolation-forest","source_url":"https://github.com/linkedin/isolation-forest"},{"type":"has_code","target_id":"github:openai:image-gpt","source_url":"https://github.com/openai/image-gpt"},{"type":"has_code","target_id":"github:intuit:wasabi","source_url":"https://github.com/intuit/wasabi"},{"type":"has_code","target_id":"github:better:convoys","source_url":"https://github.com/better/convoys"},{"type":"has_code","target_id":"github:jacopotagliabue:you-dont-need-a-bigger-boat","source_url":"https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat"}]', NULL, 'MIT', 'approved', 80, '931399ef09b0954108869aa7146424a3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-eugeneyan-applied-ml from https://github.com/eugeneyan.png
Image converted to WebP: data/images/github-eugeneyan-applied-ml.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-JaidedAI-EasyOCR', 'github--jaidedai--easyocr', 'EasyOCR', 'JaidedAI', 'Ready-to-use OCR with 80+ supported languages and all popular writing scripts including: Latin, Chinese, Arabic, Devanagari, Cyrillic, etc. Try Demo on our website Integrated into Huggingface Spaces ğŸ¤— using Gradio. Try out the Web Demo: - 24 September 2024 - Version 1.7.2 - Fix several compatibilities - Read all release notes - Handwritten text support !example !example2 !example3 Install using For the latest stable release: For the latest development release: Note 1: For Windows, please ins...', '["cnn","crnn","data-mining","deep-learning","easyocr","image-processing","information-retrieval","lstm","machine-learning","ocr","optical-character-recognition","python","pytorch","scene-text","scene-text-recognition","python"]', 'other', 28516, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/JaidedAI/EasyOCR","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# EasyOCR\n\n[![PyPI Status](https://badge.fury.io/py/easyocr.svg)](https://badge.fury.io/py/easyocr)\n[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/JaidedAI/EasyOCR/blob/master/LICENSE)\n[![Tweet](https://img.shields.io/twitter/url/https/github.com/JaidedAI/EasyOCR.svg?style=social)](https://twitter.com/intent/tweet?text=Check%20out%20this%20awesome%20library:%20EasyOCR%20https://github.com/JaidedAI/EasyOCR)\n[![Twitter](https://img.shields.io/badge/twitter-@JaidedAI-blue.svg?style=flat)](https://twitter.com/JaidedAI)\n\nReady-to-use OCR with 80+ [supported languages](https://www.jaided.ai/easyocr) and all popular writing scripts including: Latin, Chinese, Arabic, Devanagari, Cyrillic, etc.\n\n[Try Demo on our website](https://www.jaided.ai/easyocr)\n\nIntegrated into [Huggingface Spaces ğŸ¤—](https://huggingface.co/spaces) using [Gradio](https://github.com/gradio-app/gradio). Try out the Web Demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/tomofi/EasyOCR)\n\n\n## What''s new\n- 24 September 2024 - Version 1.7.2\n    - Fix several compatibilities\n\n- [Read all release notes](https://github.com/JaidedAI/EasyOCR/blob/master/releasenotes.md)\n\n## What''s coming next\n- Handwritten text support\n\n## Examples\n\n![example](examples/example.png)\n\n![example2](examples/example2.png)\n\n![example3](examples/example3.png)\n\n\n## Installation\n\nInstall using `pip`\n\nFor the latest stable release:\n\n``` bash\npip install easyocr\n```\n\nFor the latest development release:\n\n``` bash\npip install git+https://github.com/JaidedAI/EasyOCR.git\n```\n\nNote 1: For Windows, please install torch and torchvision first by following the official instructions here https://pytorch.org. On the pytorch website, be sure to select the right CUDA version you have. If you intend to run on CPU mode only, select `CUDA = None`.\n\nNote 2: We also provide a Dockerfile [here](https://github.com/JaidedAI/EasyOCR/blob/master/Dockerfile).\n\n## Usage\n\n``` python\nimport easyocr\nreader = easyocr.Reader([''ch_sim'',''en'']) # this needs to run only once to load the model into memory\nresult = reader.readtext(''chinese.jpg'')\n```\n\nThe output will be in a list format, each item represents a bounding box, the text detected and confident level, respectively.\n\n``` bash\n[([[189, 75], [469, 75], [469, 165], [189, 165]], ''æ„šå›­è·¯'', 0.3754989504814148),\n ([[86, 80], [134, 80], [134, 128], [86, 128]], ''è¥¿'', 0.40452659130096436),\n ([[517, 81], [565, 81], [565, 123], [517, 123]], ''ä¸œ'', 0.9989598989486694),\n ([[78, 126], [136, 126], [136, 156], [78, 156]], ''315'', 0.8125889301300049),\n ([[514, 126], [574, 126], [574, 156], [514, 156]], ''309'', 0.4971577227115631),\n ([[226, 170], [414, 170], [414, 220], [226, 220]], ''Yuyuan Rd.'', 0.8261902332305908),\n ([[79, 173], [125, 173], [125, 213], [79, 213]], ''W'', 0.9848111271858215),\n ([[529, 173], [569, 173], [569, 213], [529, 213]], ''E'', 0.8405593633651733)]\n```\nNote 1: `[''ch_sim'',''en'']` is the list of languages you want to read. You can pass\nseveral languages at once but not all languages can be used together.\nEnglish is compatible with every language and languages that share common characters are usually compatible with each other.\n\nNote 2: Instead of the filepath `chinese.jpg`, you can also pass an OpenCV image object (numpy array) or an image file as bytes. A URL to a raw image is also acceptable.\n\nNote 3: The line `reader = easyocr.Reader([''ch_sim'',''en''])` is for loading a model into memory. It takes some time but it needs to be run only once.\n\nYou can also set `detail=0` for simpler output.\n\n``` python\nreader.readtext(''chinese.jpg'', detail = 0)\n```\nResult:\n``` bash\n[''æ„šå›­è·¯'', ''è¥¿'', ''ä¸œ'', ''315'', ''309'', ''Yuyuan Rd.'', ''W'', ''E'']\n```\n\nModel weights for the chosen language will be automatically downloaded or you can\ndownload them manually from the [model hub](https://www.jaided.ai/easyocr/modelhub) and put them in the ''~/.EasyOCR/model'' folder\n\nIn case you do not have a GPU, or your GPU has low memory, you can run the model in CPU-only mode by adding `gpu=False`.\n\n``` python\nreader = easyocr.Reader([''ch_sim'',''en''], gpu=False)\n```\n\nFor more information, read the [tutorial](https://www.jaided.ai/easyocr/tutorial) and [API Documentation](https://www.jaided.ai/easyocr/documentation).\n\n#### Run on command line\n\n```shell\n$ easyocr -l ch_sim en -f chinese.jpg --detail=1 --gpu=True\n```\n\n## Train/use your own model\n\nFor recognition model, [Read here](https://github.com/JaidedAI/EasyOCR/blob/master/custom_model.md).\n\nFor detection model (CRAFT), [Read here](https://github.com/JaidedAI/EasyOCR/blob/master/trainer/craft/README.md).\n\n## Implementation Roadmap\n\n- Handwritten support\n- Restructure code to support swappable detection and recognition algorithms\nThe api should be as easy as\n``` python\nreader = easyocr.Reader([''en''], detection=''DB'', recognition = ''Transformer'')\n```\nThe idea is to be able to plug in any state-of-the-art model into EasyOCR. There are a lot of geniuses trying to make better detection/recognition models, but we are not trying to be geniuses here. We just want to make their works quickly accessible to the public ... for free. (well, we believe most geniuses want their work to create a positive impact as fast/big as possible) The pipeline should be something like the below diagram. Grey slots are placeholders for changeable light blue modules.\n\n![plan](examples/easyocr_framework.jpeg)\n\n## Acknowledgement and References\n\nThis project is based on research and code from several papers and open-source repositories.\n\nAll deep learning execution is based on [Pytorch](https://pytorch.org). :heart:\n\nDetection execution uses the CRAFT algorithm from this [official repository](https://github.com/clovaai/CRAFT-pytorch) and their [paper](https://arxiv.org/abs/1904.01941) (Thanks @YoungminBaek from [@clovaai](https://github.com/clovaai)). We also use their pretrained model. Training script is provided by [@gmuffiness](https://github.com/gmuffiness).\n\nThe recognition model is a CRNN ([paper](https://arxiv.org/abs/1507.05717)). It is composed of 3 main components: feature extraction (we are currently using [Resnet](https://arxiv.org/abs/1512.03385)) and VGG, sequence labeling ([LSTM](https://www.bioinf.jku.at/publications/older/2604.pdf)) and decoding ([CTC](https://www.cs.toronto.edu/~graves/icml_2006.pdf)). The training pipeline for recognition execution is a modified version of the [deep-text-recognition-benchmark](https://github.com/clovaai/deep-text-recognition-benchmark) framework. (Thanks [@ku21fan](https://github.com/ku21fan) from [@clovaai](https://github.com/clovaai)) This repository is a gem that deserves more recognition.\n\nBeam search code is based on this [repository](https://github.com/githubharald/CTCDecoder) and his [blog](https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7). (Thanks [@githubharald](https://github.com/githubharald))\n\nData synthesis is based on [TextRecognitionDataGenerator](https://github.com/Belval/TextRecognitionDataGenerator). (Thanks [@Belval](https://github.com/Belval))\n\nAnd a good read about CTC from distill.pub [here](https://distill.pub/2017/ctc/).\n\n## Want To Contribute?\n\nLet''s advance humanity together by making AI available to everyone!\n\n3 ways to contribute:\n\n**Coder:** Please send a PR for small bugs/improvements. For bigger ones, discuss with us by opening an issue first. There is a list of possible bug/improvement issues tagged with [''PR WELCOME''](https://github.com/JaidedAI/EasyOCR/issues?q=is%3Aissue+is%3Aopen+label%3A%22PR+WELCOME%22).\n\n**User:** Tell us how EasyOCR benefits you/your organization to encourage further development. Also post failure cases in [Issue  Section](https://github.com/JaidedAI/EasyOCR/issues) to help improve future models.\n\n**Tech leader/Guru:** If you found this library useful, please spread the word! (See [Yann Lecun''s post](https://www.facebook.com/yann.lecun/posts/10157018122787143) about EasyOCR)\n\n## Guideline for new language request\n\nTo request a new language, we need you to send a PR with the 2 following files:\n\n1. In folder [easyocr/character](https://github.com/JaidedAI/EasyOCR/tree/master/easyocr/character),\nwe need ''yourlanguagecode_char.txt'' that contains list of all characters. Please see format examples from other files in that folder.\n2. In folder [easyocr/dict](https://github.com/JaidedAI/EasyOCR/tree/master/easyocr/dict),\nwe need ''yourlanguagecode.txt'' that contains list of words in your language.\nOn average, we have ~30000 words per language with more than 50000 words for more popular ones.\nMore is better in this file.\n\nIf your language has unique elements (such as 1. Arabic: characters change form when attached to each other + write from right to left 2. Thai: Some characters need to be above the line and some below), please educate us to the best of your ability and/or give useful links. It is important to take care of the detail to achieve a system that really works.\n\nLastly, please understand that our priority will have to go to popular languages or sets of languages that share large portions of their characters with each other (also tell us if this is the case for your language). It takes us at least a week to develop a new model, so you may have to wait a while for the new model to be released.\n\nSee [List of languages in development](https://github.com/JaidedAI/EasyOCR/issues/91)\n\n## Github Issues\n\nDue to limited resources, an issue older than 6 months will be automatically closed. Please open an issue again if it is critical.\n\n## Business Inquiries\n\nFor Enterprise Support, [Jaided AI](https://www.jaided.ai/) offers full service for custom OCR/AI systems from implementation, training/finetuning and deployment. Click [here](https://www.jaided.ai/contactus?ref=github) to contact us.\n', '{"language":"Python","stars":28516,"forks":3504,"watchers":28516,"open_issues":523,"topics":["cnn","crnn","data-mining","deep-learning","easyocr","image-processing","information-retrieval","lstm","machine-learning","ocr","optical-character-recognition","python","pytorch","scene-text","scene-text-recognition"],"default_branch":"master","size_kb":161635,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:JaidedAI:EasyOCR","source_url":"https://github.com/JaidedAI/EasyOCR"},{"type":"has_code","target_id":"github:JaidedAI:EasyOCR","source_url":"https://github.com/JaidedAI/EasyOCR"},{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:JaidedAI:EasyOCR","source_url":"https://github.com/JaidedAI/EasyOCR"},{"type":"has_code","target_id":"github:JaidedAI:EasyOCR.git","source_url":"https://github.com/JaidedAI/EasyOCR.git"},{"type":"has_code","target_id":"github:JaidedAI:EasyOCR","source_url":"https://github.com/JaidedAI/EasyOCR"},{"type":"has_code","target_id":"github:JaidedAI:EasyOCR","source_url":"https://github.com/JaidedAI/EasyOCR"},{"type":"has_code","target_id":"github:JaidedAI:EasyOCR","source_url":"https://github.com/JaidedAI/EasyOCR"},{"type":"has_code","target_id":"github:clovaai:CRAFT-pytorch","source_url":"https://github.com/clovaai/CRAFT-pytorch"},{"type":"has_code","target_id":"github:clovaai:deep-text-recognition-benchmark","source_url":"https://github.com/clovaai/deep-text-recognition-benchmark"},{"type":"has_code","target_id":"github:githubharald:CTCDecoder","source_url":"https://github.com/githubharald/CTCDecoder"},{"type":"has_code","target_id":"github:Belval:TextRecognitionDataGenerator","source_url":"https://github.com/Belval/TextRecognitionDataGenerator"},{"type":"has_code","target_id":"github:JaidedAI:EasyOCR","source_url":"https://github.com/JaidedAI/EasyOCR"},{"type":"has_code","target_id":"github:JaidedAI:EasyOCR","source_url":"https://github.com/JaidedAI/EasyOCR"},{"type":"has_code","target_id":"github:JaidedAI:EasyOCR","source_url":"https://github.com/JaidedAI/EasyOCR"},{"type":"has_code","target_id":"github:JaidedAI:EasyOCR","source_url":"https://github.com/JaidedAI/EasyOCR"},{"type":"has_code","target_id":"github:JaidedAI:EasyOCR","source_url":"https://github.com/JaidedAI/EasyOCR"}]', NULL, 'Apache-2.0', 'approved', 65, '7a497bc1ec9efff485d0e06684dc9260', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-JaidedAI-EasyOCR from https://github.com/JaidedAI.png
Image converted to WebP: data/images/github-JaidedAI-EasyOCR.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-academic-awesome-datascience', 'github--academic--awesome-datascience', 'awesome-datascience', 'academic', '<div align="center" markdown="1"> <sup>Special thanks to Sponsors:</sup> <br> <br> <a href="https://go.warp.dev/awesome-datascience"> <img alt="Warp sponsorship" width="400" src="https://github.com/warpdotdev/brand-assets/blob/main/Github/Sponsor/Warp-Github-LG-04.png?raw=true"> </a> ### Warp, the intelligent terminal for developers Available for MacOS, Linux, & Windows<br> <br> <br> <a href="https://requestly.com/awesomedatascience"> <img alt="Requestly sponsorship" width="400" src="https://...', '["analytics","awesome-list","data-mining","data-science","data-scientists","data-visualization","deep-learning","hacktoberfest","machine-learning","science"]', 'other', 27912, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/academic/awesome-datascience","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center" markdown="1">\n   <sup>Special thanks to Sponsors:</sup>\n   <br> \n   <br>\n   <a href="https://go.warp.dev/awesome-datascience">\n      <img alt="Warp sponsorship" width="400" src="https://github.com/warpdotdev/brand-assets/blob/main/Github/Sponsor/Warp-Github-LG-04.png?raw=true">\n   </a>\n\n   ### [Warp, the intelligent terminal for developers](https://go.warp.dev/awesome-datascience)\n   [Available for MacOS, Linux, & Windows](https://go.warp.dev/awesome-datascience)<br>\n   <br>\n   <br>\n   <a href="https://requestly.com/awesomedatascience">\n      <img alt="Requestly sponsorship" width="400" src="https://github.com/user-attachments/assets/24670320-997d-4d62-9bca-955c59fe883d">\n   </a>\n   ### [Requestly - Free & Open-Source alternative to Postman](https://requestly.com/awesomedatascience)\n   [All-in-one platform to Test, Mock and Intercept APIs](https://requestly.com/awesomedatascience)\n   <br>\n</div>\n\n<hr>\n\n<div align="center"><img src="./assets/head.jpg"></div>\n\n# AWESOME DATA SCIENCE\n\n[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome) \n\n**An open-source Data Science repository to learn and apply towards solving real world problems.**\n\nThis is a shortcut path to start studying **Data Science**. Just follow the steps to answer the questions, "What is Data Science and what should I study to learn Data Science?"\n\n\n<br>\n\n\n## Sponsors\n\n| Sponsor | Pitch |\n| --- | --- |\n| --- | Be the first to sponsor! `github@academic.io` |\n\n\n\n## Table of Contents\n\n- [What is Data Science?](#what-is-data-science)\n- [Where do I Start?](#where-do-i-start)\n- [Training Resources](#training-resources)\n  - [Tutorials](#tutorials)\n  - [Free Courses](#free-courses)\n  - [Massively Open Online Courses](#moocs)\n  - [Intensive Programs](#intensive-programs)\n  - [Colleges](#colleges)\n- [The Data Science Toolbox](#the-data-science-toolbox)\n  - [Algorithms](#algorithms)\n    - [Supervised Learning](#supervised-learning)\n    - [Unsupervised Learning](#unsupervised-learning)\n    - [Semi-Supervised Learning](#semi-supervised-learning)\n    - [Reinforcement Learning](#reinforcement-learning)\n    - [Data  Mining Algorithms](#data-mining-algorithms)\n    - [Deep Learning Architectures](#deep-learning-architectures)\n  - [General Machine Learning Packages](#general-machine-learning-packages)\n  - [Model Evaluation & Monitoring](#model-evaluation--monitoring)\n    - [Evidently AI](#evidently-ai)\n  - [Deep Learning Packages](#deep-learning-packages)\n    - [PyTorch Ecosystem](#pytorch-ecosystem)\n    - [TensorFlow Ecosystem](#tensorflow-ecosystem)\n    - [Keras Ecosystem](#keras-ecosystem)\n  - [Visualization Tools](#visualization-tools)\n  - [Miscellaneous Tools](#miscellaneous-tools)\n- [Literature and Media](#literature-and-media)\n  - [Books](#books)\n    - [Book Deals (Affiliated)](#book-deals-affiliated)\n  - [Journals, Publications, and Magazines](#journals-publications-and-magazines)\n  - [Newsletters](#newsletters)\n  - [Bloggers](#bloggers)\n  - [Presentations](#presentations)\n  - [Podcasts](#podcasts)\n  - [YouTube Videos & Channels](#youtube-videos--channels)\n- [Socialize](#socialize)\n  - [Facebook Accounts](#facebook-accounts)\n  - [Twitter Accounts](#twitter-accounts)\n  - [Telegram Channels](#telegram-channels)\n  - [Slack Communities](#slack-communities)\n  - [GitHub Groups](#github-groups)\n  - [Data Science Competitions](#data-science-competitions)\n- [Fun](#fun)\n  - [Infographics](#infographics)\n  - [Datasets](#datasets)\n  - [Comics](#comics)\n- [Other Awesome Lists](#other-awesome-lists)\n  - [Hobby](#hobby)\n\n## What is Data Science?\n**[`^        back to top        ^`](#awesome-data-science)**\n\nData Science is one of the hottest topics on the Computer and Internet farmland nowadays. People have gathered data from applications and systems until today and now is the time to analyze them. The next steps are producing suggestions from the data and creating predictions about the future. [Here](https://www.quora.com/Data-Science/What-is-data-science) you can find the biggest question for **Data Science** and hundreds of answers from experts.\n\n\n| Link | Preview |\n| --- | --- |\n| [Data Science For Beginners](https://github.com/microsoft/Data-Science-For-Beginners) | Microsoft are pleased to offer a 10-week, 20-lesson curriculum all about Data Science. |\n| [What is Data Science @ O''reilly](https://www.oreilly.com/ideas/what-is-data-science) | _Data scientists combine entrepreneurship with patience, the willingness to build data products incrementally, the ability to explore, and the ability to iterate over a solution. They are inherently interdisciplinary. They can tackle all aspects of a problem, from initial data collection and data conditioning to drawing conclusions. They can think outside the box to come up with new ways to view the problem, or to work with very broadly defined problems: â€œhereâ€™s a lot of data, what can you make from it?â€_ |\n| [What is Data Science @ Quora](https://www.quora.com/Data-Science/What-is-data-science) | Data Science is a combination of a number of aspects of Data such as Technology, Algorithm development, and data interference to study the data, analyse it, and find innovative solutions to difficult problems. Basically Data Science is all about Analysing data and driving for business growth by finding creative ways. |\n| [The sexiest job of 21st century](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century) | _Data scientists today are akin to Wall Street â€œquantsâ€ of the 1980s and 1990s. In those days people with backgrounds in physics and math streamed to investment banks and hedge funds, where they could devise entirely new algorithms and data strategies. Then a variety of universities developed masterâ€™s programs in financial engineering, which churned out a second generation of talent that was more accessible to mainstream firms. The pattern was repeated later in the 1990s with search engineers, whose rarefied skills soon came to be taught in computer science programs._ |\n| [Wikipedia](https://en.wikipedia.org/wiki/Data_science) | _Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data._ |\n| [How to Become a Data Scientist](https://www.mastersindatascience.org/careers/data-scientist/) | _Data scientists are big data wranglers, gathering and analyzing large sets of structured and unstructured data. A data scientistâ€™s role combines computer science, statistics, and mathematics. They analyze, process, and model data then interpret the results to create actionable plans for companies and other organizations._ |\n| [a very short history of #datascience](https://www.forbes.com/sites/gilpress/2013/05/28/a-very-short-history-of-data-science/) | _The story of how data scientists became sexy is mostly the story of the coupling of the mature discipline of statistics with a very young one--computer science.  The term â€œData Scienceâ€ has emerged only recently to specifically designate a new profession that is expected to make sense of the vast stores of big data. But making sense of data has a long history and has been discussed by scientists, statisticians, librarians, computer scientists and others for years. The following timeline traces the evolution of the term â€œData Scienceâ€ and its use, attempts to define it, and related terms._ |\n|[Software Development Resources for Data Scientists](https://www.rstudio.com/blog/software-development-resources-for-data-scientists/)|_Data scientists concentrate on making sense of data through exploratory analysis, statistics, and models. Software developers apply a separate set of knowledge with different tools. Although their focus may seem unrelated, data science teams can benefit from adopting software development best practices. Version control, automated testing, and other dev skills help create reproducible, production-ready code and tools._|\n|[Data Scientist Roadmap](https://www.scaler.com/blog/how-to-become-a-data-scientist/)|_Data science is an excellent career choice in todayâ€™s data-driven world where approx 328.77 million terabytes of data are generated daily. And this number is only increasing day by day, which in turn increases the demand for skilled data scientists who can utilize this data to drive business growth._|\n|[Navigating Your Path to Becoming a Data Scientist](https://www.appliedaicourse.com/blog/how-to-become-a-data-scientist/)|_Data science is one of the most in-demand careers today. With businesses increasingly relying on data to make decisions, the need for skilled data scientists has grown rapidly. Whether itâ€™s tech companies, healthcare organizations, or even government institutions, data scientists play a crucial role in turning raw data into valuable insights. But how do you become a data scientist, especially if youâ€™re just starting out? _|\n\n## Where do I Start?\n**[`^        back to top        ^`](#awesome-data-science)**\n\nWhile not strictly necessary, having a programming language is a crucial skill to be effective as a data scientist. Currently, the most popular language is _Python_, closely followed by _R_. Python is a general-purpose scripting language that sees applications in a wide variety of fields. R is a domain-specific language for statistics, which contains a lot of common statistics tools out of the box.\n\n[Python](https://python.org/) is by far the most popular language in science, due in no small part to the ease at which it can be used and the vibrant ecosystem of user-generated packages. To install packages, there are two main methods: Pip (invoked as `pip install`), the package manager that comes bundled with Python, and [Anaconda](https://www.anaconda.com) (invoked as `conda install`), a powerful package manager that can install packages for Python, R, and can download executables like Git. \n\nUnlike R, Python was not built from the ground up with data science in mind, but there are plenty of third party libraries to make up for this. A much more exhaustive list of packages can be found later in this document, but these four packages are a good set of choices to start your data science journey with: [Scikit-Learn](https://scikit-learn.org/stable/index.html) is a general-purpose data science package which implements the most popular algorithms - it also includes rich documentation, tutorials, and examples of the models it implements. Even if you prefer to write your own implementations, Scikit-Learn is a valuable reference to the nuts-and-bolts behind many of the common algorithms you''ll find. With [Pandas](https://pandas.pydata.org/), one can collect and analyze their data into a convenient table format. [Numpy](https://numpy.org/) provides very fast tooling for mathematical operations, with a focus on vectors and matrices. [Seaborn](https://seaborn.pydata.org/), itself based on the [Matplotlib](https://matplotlib.org/) package, is a quick way to generate beautiful visualizations of your data, with many good defaults available out of the box, as well as a gallery showing how to produce many common visualizations of your data.\n\n When embarking on your journey to becoming a data scientist, the choice of language isn''t particularly important, and both Python and R have their pros and cons. Pick a language you like, and check out one of the [Free courses](#free-courses) we''ve listed below!\n\n## Agents\n\nPlease, contribute about "agents"\n\n### Workflow  \n**[`^        back to top        ^`](#awesome-data-science)**\n- [sim](https://sim.ai) Sim Studio''s interface is a lightweight, intuitive way to quickly build and deploy LLMs that connect with your favorite tools.\n\n\n## Training Resources\n**[`^        back to top        ^`](#awesome-data-science)**\n\nHow do you learn data science? By doing data science, of course! Okay, okay - that might not be particularly helpful when you''re first starting out. In this section, we''ve listed some learning resources, in rough order from least to greatest commitment - [Tutorials](#tutorials), [Massively Open Online Courses (MOOCs)](#moocs), [Intensive Programs](#intensive-programs), and [Colleges](#colleges).\n\n\n### Tutorials\n**[`^        back to top        ^`](#awesome-data-science)**\n\n- [1000 Data Science Projects](https://cloud.blobcity.com/#/ps/explore) you can run on the browser with IPython.\n- [#tidytuesday](https://github.com/rfordatascience/tidytuesday) A weekly data project aimed at the R ecosystem.\n- [Data science your way](https://github.com/jadianes/data-science-your-way)\n- [DataCamp Cheatsheets](https://www.datacamp.com/cheat-sheet) Cheatsheets for data science.\n- [PySpark Cheatsheet](https://github.com/kevinschaich/pyspark-cheatsheet)\n- [Machine Learning, Data Science and Deep Learning with Python ](https://www.manning.com/livevideo/machine-learning-data-science-and-deep-learning-with-python)\n- [Your Guide to Latent Dirichlet Allocation](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d)\n- [Tutorials of source code from the book Genetic Algorithms with Python by Clinton Sheppard](https://github.com/handcraftsman/GeneticAlgorithmsWithPython)\n- [Tutorials to get started on signal processing for machine learning](https://github.com/jinglescode/python-signal-processing)\n- [Realtime deployment](https://www.microprediction.com/python-1) Tutorial on Python time-series model deployment.\n- [Python for Data Science: A Beginnerâ€™s Guide](https://learntocodewith.me/posts/python-for-data-science/)\n- [Minimum Viable Study Plan for Machine Learning Interviews](https://github.com/khangich/machine-learning-interview)\n- [Understand and Know Machine Learning Engineering by Building Solid Projects](http://mlzoomcamp.com/)\n- [12 free Data Science projects to practice Python and Pandas](https://www.datawars.io/articles/12-free-data-science-projects-to-practice-python-and-pandas)\n- [Best CV/Resume for Data Science Freshers](https://enhancv.com/resume-examples/data-scientist/)\n- [Understand Data Science Course in Java](https://www.alter-solutions.com/articles/java-data-science)\n- [Data Analytics Interview Questions (Beginner to Advanced)](https://www.appliedaicourse.com/blog/data-analytics-interview-questions/)\n- [Top 100+ Data Science Interview Questions and Answers](https://www.appliedaicourse.com/blog/data-science-interview-questions/)\n\n### Free Courses\n**[`^        back to top        ^`](#awesome-data-science)**\n\n- [Data Scientist with R](https://www.datacamp.com/tracks/data-scientist-with-r)\n- [Data Scientist with Python](https://www.datacamp.com/tracks/data-scientist-with-python)\n- [Genetic Algorithms OCW Course](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/lecture-1-introduction-and-scope/)\n- [AI Expert Roadmap](https://github.com/AMAI-GmbH/AI-Expert-Roadmap) - Roadmap to becoming an Artificial Intelligence Expert\n- [Convex Optimization](https://www.edx.org/course/convex-optimization) - Convex Optimization (basics of convex analysis; least-squares, linear and quadratic programs, semidefinite programming, minimax, extremal volume, and other problems; optimality conditions, duality theory...)\n- [Learning from Data](https://home.work.caltech.edu/telecourse.html) - Introduction to machine learning covering basic theory, algorithms and applications\n- [Kaggle](https://www.kaggle.com/learn) - Learn about Data Science, Machine Learning, Python etc\n- [ML Observability Fundamentals](https://arize.com/ml-observability-fundamentals/) - Learn how to monitor and root-cause production ML issues.\n- [Weights & Biases Effective MLOps: Model Development](https://www.wandb.courses/courses/effective-mlops-model-development) - Free Course and Certification for building an end-to-end machine using W&B\n- [Python for Data Science by Scaler](https://www.scaler.com/topics/course/python-for-data-science/) - This course is designed to empower beginners with the essential skills to excel in today''s data-driven world. The comprehensive curriculum will give you a solid foundation in statistics, programming, data visualization, and machine learning.\n- [MLSys-NYU-2022](https://github.com/jacopotagliabue/MLSys-NYU-2022/tree/main) - Slides, scripts and materials for the Machine Learning in Finance course at NYU Tandon, 2022.\n- [Hands-on Train and Deploy ML](https://github.com/Paulescu/hands-on-train-and-deploy-ml) - A hands-on course to train and deploy a serverless API that predicts crypto prices.\n- [LLMOps: Building Real-World Applications With Large Language Models](https://www.comet.com/site/llm-course/) - Learn to build modern software with LLMs using the newest tools and techniques in the field.\n- [Prompt Engineering for Vision Models](https://www.deeplearning.ai/short-courses/prompt-engineering-for-vision-models/) - Learn to prompt cutting-edge computer vision models with natural language, coordinate points, bounding boxes, segmentation masks, and even other images in this free course from DeepLearning.AI.\n- [Data Science Course By IBM](https://skillsbuild.org/students/course-catalog/data-science) - Free resources and learn what data science is and how itâ€™s used in different industries.\n\n\n  \n### MOOC''s\n**[`^        back to top        ^`](#awesome-data-science)**\n\n- [Coursera Introduction to Data Science](https://www.coursera.org/specializations/data-science)\n- [Data Science - 9 Steps Courses, A Specialization on Coursera](https://www.coursera.org/specializations/jhu-data-science)\n- [Data Mining - 5 Steps Courses, A Specialization on Coursera](https://www.coursera.org/specializations/data-mining)\n- [Machine Learning â€“ 5 Steps Courses, A Specialization on Coursera](https://www.coursera.org/specializations/machine-learning)\n- [CS 109 Data Science](https://cs109.github.io/2015/)\n- [OpenIntro](https://www.openintro.org/)\n- [CS 171 Visualization](https://www.cs171.org/#!index.md)\n- [Process Mining: Data science in Action](https://www.coursera.org/learn/process-mining)\n- [Oxford Deep Learning](https://www.cs.ox.ac.uk/projects/DeepLearn/)\n- [Oxford Deep Learning - video](https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu)\n- [Oxford Machine Learning](https://www.cs.ox.ac.uk/research/ai_ml/index.html)\n- [UBC Machine Learning - video](https://www.cs.ubc.ca/~nando/540-2013/lectures.html)\n- [Data Science Specialization](https://github.com/DataScienceSpecialization/courses)\n- [Coursera Big Data Specialization](https://www.coursera.org/specializations/big-data)\n- [Statistical Thinking for Data Science and Analytics by Edx](https://www.edx.org/course/statistical-thinking-for-data-science-and-analytic)\n- [Cognitive Class AI by IBM](https://cognitiveclass.ai/)\n- [Udacity - Deep Learning](https://www.udacity.com/course/intro-to-tensorflow-for-deep-learning--ud187)\n- [Keras in Motion](https://www.manning.com/livevideo/keras-in-motion)\n- [Microsoft Professional Program for Data Science](https://academy.microsoft.com/en-us/professional-program/tracks/data-science/)\n- [COMP3222/COMP6246 - Machine Learning Technologies](https://tdgunes.com/COMP6246-2019Fall/)\n- [CS 231 - Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/)\n- [Coursera Tensorflow in practice](https://www.coursera.org/professional-certificates/tensorflow-in-practice)\n- [Coursera Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)\n- [365 Data Science Course](https://365datascience.com/)\n- [Coursera Natural Language Processing Specialization](https://www.coursera.org/specializations/natural-language-processing)\n- [Coursera GAN Specialization](https://www.coursera.org/specializations/generative-adversarial-networks-gans)\n- [Codecademy''s Data Science](https://www.codecademy.com/learn/paths/data-science)\n- [Linear Algebra](https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/) - Linear Algebra course by Gilbert Strang\n- [A 2020 Vision of Linear Algebra (G. Strang)](https://ocw.mit.edu/resources/res-18-010-a-2020-vision-of-linear-algebra-spring-2020/)\n- [Python for Data Science Foundation Course](https://intellipaat.com/academy/course/python-for-data-science-free-training/)\n- [Data Science: Statistics & Machine Learning](https://www.coursera.org/specializations/data-science-statistics-machine-learning)\n- [Machine Learning Engineering for Production (MLOps)](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops)\n- [Recommender Systems Specialization from University of Minnesota](https://www.coursera.org/specializations/recommender-systems) is an intermediate/advanced level specialization focused on Recommender System on the Coursera platform.\n- [Stanford Artificial Intelligence Professional Program](https://online.stanford.edu/programs/artificial-intelligence-professional-program)\n- [Data Scientist with Python](https://app.datacamp.com/learn/career-tracks/data-scientist-with-python)\n- [Programming with Julia](https://www.udemy.com/course/programming-with-julia/)\n- [Scaler Data Science & Machine Learning Program](https://www.scaler.com/data-science-course/)\n- [Data Science Skill Tree](https://labex.io/skilltrees/data-science)\n- [Data Science for Beginners - Learn with AI tutor](https://codekidz.ai/lesson-intro/data-science-368dbf)\n- [Machine Learning for Beginners - Learn with AI tutor](https://codekidz.ai/lesson-intro/machine-lear-36abfb)\n- [Introduction to Data Science](https://www.mygreatlearning.com/academy/learn-for-free/courses/introduction-to-data-science)\n-[Getting Started with Python for Data Science](https://www.codecademy.com/learn/getting-started-with-python-for-data-science) \n- [Google Advanced Data Analytics Certificate](https://grow.google/data-analytics/) â€“ Professional courses in data analysis, statistics, and machine learning fundamentals.\n\n### Intensive Programs\n**[`^        back to top        ^`](#awesome-data-science)**\n\n- [S2DS](https://www.s2ds.org/)\n- [WorldQuant University Applied Data Science Lab](https://www.wqu.edu/adsl)\n\n\n### Colleges\n**[`^        back to top        ^`](#awesome-data-science)**\n\n- [A list of colleges and universities offering degrees in data science.](https://github.com/ryanswanstrom/awesome-datascience-colleges)\n- [Data Science Degree @ Berkeley](https://ischoolonline.berkeley.edu/data-science/)\n- [Data Science Degree @ UVA](https://datascience.virginia.edu/)\n- [Data Science Degree @ Wisconsin](https://datasciencedegree.wisconsin.edu/)\n- [BS in Data Science & Applications](https://study.iitm.ac.in/ds/)\n- [MS in Computer Information Systems @ Boston University](https://www.bu.edu/online/programs/graduate-programs/computer-information-systems-masters-degree/)\n- [MS in Business Analytics @ ASU Online](https://asuonline.asu.edu/online-degree-programs/graduate/master-science-business-analytics/)\n- [MS in Applied Data Science @ Syracuse](https://ischool.syr.edu/academics/applied-data-science-masters-degree/)\n- [M.S. Management & Data Science @ Leuphana](https://www.leuphana.de/en/graduate-school/masters-programmes/management-data-science.html)\n- [Master of Data Science @ Melbourne University](https://study.unimelb.edu.au/find/courses/graduate/master-of-data-science/#overview)\n- [Msc in Data Science @ The University of Edinburgh](https://www.ed.ac.uk/studying/postgraduate/degrees/index.php?r=site/view&id=902)\n- [Master of Management Analytics @ Queen''s University](https://smith.queensu.ca/grad_studies/mma/index.php)\n- [Master of Data Science @ Illinois Institute of Technology](https://www.iit.edu/academics/programs/data-science-mas)\n- [Master of Applied Data Science @ The University of Michigan](https://www.si.umich.edu/programs/master-applied-data-science)\n- [Master Data Science and Artificial Intelligence @ Eindhoven University of Technology](https://www.tue.nl/en/education/graduate-school/master-data-science-and-artificial-intelligence/)\n- [Master''s Degree in Data Science and Computer Engineering @ University of Granada](https://masteres.ugr.es/datcom/)\n\n## The Data Science Toolbox\n**[`^        back to top        ^`](#awesome-data-science)**\n\nThis section is a collection of packages, tools, algorithms, and other useful items in the data science world.\n\n### Algorithms\n**[`^        back to top        ^`](#awesome-data-science)**\n\nThese are some Machine Learning and Data Mining algorithms and models help you to understand your data and derive meaning from it.\n\n#### Three kinds of Machine Learning Systems\n\n- Based on training with human supervision\n- Based on learning incrementally on fly\n- Based on data points comparison and pattern detection\n\n### Comparison\n- [datacompy](https://github.com/capitalone/datacompy) - DataComPy is a package to compare two Pandas DataFrames.\n\n#### Supervised Learning\n\n- [Regression](https://en.wikipedia.org/wiki/Regression)\n- [Linear Regression](https://en.wikipedia.org/wiki/Linear_regression)\n- [Ordinary Least Squares](https://en.wikipedia.org/wiki/Ordinary_least_squares)\n- [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)\n- [Stepwise Regression](https://en.wikipedia.org/wiki/Stepwise_regression)\n- [Multivariate Adaptive Regression Splines](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline)\n- [Softmax Regression](https://d2l.ai/chapter_linear-classification/softmax-regression.html)\n- [Locally Estimated Scatterplot Smoothing](https://en.wikipedia.org/wiki/Local_regression)\n- Classification\n  - [k-nearest neighbor](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n  - [Support Vector Machines](https://en.wikipedia.org/wiki/Support_vector_machine)\n  - [Decision Trees](https://en.wikipedia.org/wiki/Decision_tree)\n  - [ID3 algorithm](https://en.wikipedia.org/wiki/ID3_algorithm)\n  - [C4.5 algorithm](https://en.wikipedia.org/wiki/C4.5_algorithm)\n- [Ensemble Learning](https://scikit-learn.org/stable/modules/ensemble.html)\n  - [Boosting](https://en.wikipedia.org/wiki/Boosting_(machine_learning))\n  - [Stacking](https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python)\n  - [Bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating)\n  - [Random Forest](https://en.wikipedia.org/wiki/Random_forest)\n  - [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost)\n\n#### Unsupervised Learning\n- [Clustering](https://scikit-learn.org/stable/modules/clustering.html#clustering)\n  - [Hierchical clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)\n  - [k-means](https://scikit-learn.org/stable/modules/clustering.html#k-means)\n  - [Density-based clustering](https://scikit-learn.org/stable/modules/clustering.html#dbscan)\n  - [Fuzzy clustering](https://en.wikipedia.org/wiki/Fuzzy_clustering)\n  - [Mixture models](https://en.wikipedia.org/wiki/Mixture_model)\n- [Dimension Reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction)\n  - [Principal Component Analysis (PCA)](https://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca)\n  - [t-SNE; t-distributed Stochastic Neighbor Embedding](https://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-tsne)\n  - [Factor Analysis](https://scikit-learn.org/stable/modules/decomposition.html#factor-analysis)\n  - [Latent Dirichlet Allocation (LDA)](https://scikit-learn.org/stable/modules/decomposition.html#latent-dirichlet-allocation-lda)\n- [Neural Networks](https://en.wikipedia.org/wiki/Neural_network)\n- [Self-organizing map](https://en.wikipedia.org/wiki/Self-organizing_map)\n- [Adaptive resonance theory](https://en.wikipedia.org/wiki/Adaptive_resonance_theory)\n- [Hidden Markov Models (HMM)](https://en.wikipedia.org/wiki/Hidden_Markov_model)\n\n#### Semi-Supervised Learning\n\n- S3VM\n- [Clustering](https://en.wikipedia.org/wiki/Weak_supervision#Cluster_assumption)\n- [Generative models](https://en.wikipedia.org/wiki/Weak_supervision#Generative_models)\n- [Low-density separation](https://en.wikipedia.org/wiki/Weak_supervision#Low-density_separation)\n- [Laplacian regularization](https://en.wikipedia.org/wiki/Weak_supervision#Laplacian_regularization)\n- [Heuristic approaches](https://en.wikipedia.org/wiki/Weak_supervision#Heuristic_approaches)\n\n#### Reinforcement Learning\n\n- [Q Learning](https://en.wikipedia.org/wiki/Q-learning)\n- [SARSA (State-Action-Reward-State-Action) algorithm](https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action)\n- [Temporal difference learning](https://en.wikipedia.org/wiki/Temporal_difference_learning#:~:text=Temporal%20difference%20(TD)%20learning%20refers,estimate%20of%20the%20value%20function.)\n\n#### Data Mining Algorithms\n\n- [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm)\n- [k-Means](https://en.wikipedia.org/wiki/K-means_clustering)\n- [SVM (Support Vector Machine)](https://en.wikipedia.org/wiki/Support_vector_machine)\n- [Apriori](https://en.wikipedia.org/wiki/Apriori_algorithm)\n- [EM (Expectation-Maximization)](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)\n- [PageRank](https://en.wikipedia.org/wiki/PageRank)\n- [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost)\n- [KNN (K-Nearest Neighbors)](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)\n- [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n- [CART (Classification and Regression Trees)](https://en.wikipedia.org/wiki/Decision_tree_learning)\n#### Modern Data Mining Algorithms\n\n- [XGBoost (Extreme Gradient Boosting)](https://en.wikipedia.org/wiki/XGBoost)\n- [LightGBM (Light Gradient Boosting Machine)](https://en.wikipedia.org/wiki/LightGBM)\n- [CatBoost](https://catboost.ai/)\n- [HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise)](https://en.wikipedia.org/wiki/DBSCAN#HDBSCAN)\n- [FP-Growth (Frequent Pattern Growth Algorithm)](https://en.wikipedia.org/wiki/Association_rule_learning#FP-growth_algorithm)\n- [Isolation Forest](https://en.wikipedia.org/wiki/Isolation_forest)\n- [Deep Embedded Clustering (DEC)](https://arxiv.org/abs/1511.06335)\n- [TPU (Top-k Periodic and High-Utility Patterns)](https://arxiv.org/abs/2509.15732)\n- [Context-Aware Rule Mining (Transformer-Based Framework)](https://arxiv.org/abs/2503.11125)\n\n\n#### Deep Learning architectures\n\n- [Multilayer Perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)\n- [Convolutional Neural Network (CNN)](https://en.wikipedia.org/wiki/Convolutional_neural_network)\n- [Recurrent Neural Network (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n- [Boltzmann Machines](https://en.wikipedia.org/wiki/Boltzmann_machine)\n- [Autoencoder](https://www.tensorflow.org/tutorials/generative/autoencoder)\n- [Generative Adversarial Network (GAN)](https://developers.google.com/machine-learning/gan/gan_structure)\n- [Self-Organized Maps](https://en.wikipedia.org/wiki/Self-organizing_map)\n- [Transformer](https://www.tensorflow.org/text/tutorials/transformer)\n- [Conditional Random Field (CRF)](https://towardsdatascience.com/conditional-random-fields-explained-e5b8256da776)\n- [ML System Designs)](https://www.evidentlyai.com/ml-system-design)\n\n### General Machine Learning Packages\n**[`^        back to top        ^`](#awesome-data-science)**\n\n* [scikit-learn](https://scikit-learn.org/)\n* [scikit-multilearn](https://github.com/scikit-multilearn/scikit-multilearn)\n* [sklearn-expertsys](https://github.com/tmadl/sklearn-expertsys)\n* [scikit-feature](https://github.com/jundongl/scikit-feature)\n* [scikit-rebate](https://github.com/EpistasisLab/scikit-rebate)\n* [seqlearn](https://github.com/larsmans/seqlearn)\n* [sklearn-bayes](https://github.com/AmazaspShumik/sklearn-bayes)\n* [sklearn-crfsuite](https://github.com/TeamHG-Memex/sklearn-crfsuite)\n* [sklearn-deap](https://github.com/rsteca/sklearn-deap)\n* [sigopt_sklearn](https://github.com/sigopt/sigopt-sklearn)\n* [sklearn-evaluation](https://github.com/edublancas/sklearn-evaluation)\n* [scikit-image](https://github.com/scikit-image/scikit-image)\n* [scikit-opt](https://github.com/guofei9987/scikit-opt)\n* [scikit-posthocs](https://github.com/maximtrp/scikit-posthocs)\n* [feature-engine](https://feature-engine.trainindata.com/)\n* [pystruct](https://github.com/pystruct/pystruct)\n* [Shogun](https://www.shogun-toolbox.org/)\n* [xLearn](https://github.com/aksnzhy/xlearn)\n* [cuML](https://github.com/rapidsai/cuml)\n* [causalml](https://github.com/uber/causalml)\n* [mlpack](https://github.com/mlpack/mlpack)\n* [MLxtend](https://github.com/rasbt/mlxtend)\n* [modAL](https://github.com/modAL-python/modAL)\n* [Sparkit-learn](https://github.com/lensacom/sparkit-learn)\n* [hyperlearn](https://github.com/danielhanchen/hyperlearn)\n* [dlib](https://github.com/davisking/dlib)\n* [imodels](https://github.com/csinva/imodels)\n* [RuleFit](https://github.com/christophM/rulefit)\n* [pyGAM](https://github.com/dswah/pyGAM)\n* [Deepchecks](https://github.com/deepchecks/deepchecks)\n* [scikit-survival](https://scikit-survival.readthedocs.io/en/stable)\n* [interpretable](https://pypi.org/project/interpretable)\n* [XGBoost](https://github.com/dmlc/xgboost)\n* [LightGBM](https://github.com/microsoft/LightGBM)\n* [CatBoost](https://github.com/catboost/catboost)\n* [PerpetualBooster](https://github.com/perpetual-ml/perpetual)\n* [JAX](https://github.com/google/jax)\n\n\n\n### Deep Learning Packages\n\n#### PyTorch Ecosystem\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [torchvision](https://github.com/pytorch/vision)\n* [torchtext](https://github.com/pytorch/text)\n* [torchaudio](https://github.com/pytorch/audio)\n* [ignite](https://github.com/pytorch/ignite)\n* [PyTorchNet](https://github.com/pytorch/tnt)\n* [PyToune](https://github.com/GRAAL-Research/poutyne)\n* [skorch](https://github.com/skorch-dev/skorch)\n* [PyVarInf](https://github.com/ctallec/pyvarinf)\n* [pytorch_geometric](https://github.com/pyg-team/pytorch_geometric)\n* [GPyTorch](https://github.com/cornellius-gp/gpytorch)\n* [pyro](https://github.com/pyro-ppl/pyro)\n* [Catalyst](https://github.com/catalyst-team/catalyst)\n* [pytorch_tabular](https://github.com/manujosephv/pytorch_tabular)\n* [Yolov3](https://github.com/ultralytics/yolov3)\n* [Yolov5](https://github.com/ultralytics/yolov5)\n* [Yolov8](https://github.com/ultralytics/ultralytics)\n\n#### TensorFlow Ecosystem\n* [TensorFlow](https://github.com/tensorflow/tensorflow)\n* [TensorLayer](https://github.com/tensorlayer/TensorLayer)\n* [TFLearn](https://github.com/tflearn/tflearn)\n* [Sonnet](https://github.com/deepmind/sonnet)\n* [tensorpack](https://github.com/tensorpack/tensorpack)\n* [TRFL](https://github.com/deepmind/trfl)\n* [Polyaxon](https://github.com/polyaxon/polyaxon)\n* [NeuPy](https://github.com/itdxer/neupy)\n* [tfdeploy](https://github.com/riga/tfdeploy)\n* [tensorflow-upstream](https://github.com/ROCmSoftwarePlatform/tensorflow-upstream)\n* [TensorFlow Fold](https://github.com/tensorflow/fold)\n* [tensorlm](https://github.com/batzner/tensorlm)\n* [TensorLight](https://github.com/bsautermeister/tensorlight)\n* [Mesh TensorFlow](https://github.com/tensorflow/mesh)\n* [Ludwig](https://github.com/ludwig-ai/ludwig)\n* [TF-Agents](https://github.com/tensorflow/agents)\n* [TensorForce](https://github.com/tensorforce/tensorforce)\n\n#### Keras Ecosystem\n\n* [Keras](https://keras.io)\n* [keras-contrib](https://github.com/keras-team/keras-contrib)\n* [Hyperas](https://github.com/maxpumperla/hyperas)\n* [Elephas](https://github.com/maxpumperla/elephas)\n* [Hera](https://github.com/keplr-io/hera)\n* [Spektral](https://github.com/danielegrattarola/spektral)\n* [qkeras](https://github.com/google/qkeras)\n* [keras-rl](https://github.com/keras-rl/keras-rl)\n* [Talos](https://github.com/autonomio/talos)\n\n#### Visualization Tools\n**[`^        back to top        ^`](#awesome-data-science)**\n\n- [altair](https://altair-viz.github.io/)\n- [amcharts](https://www.amcharts.com/)\n- [anychart](https://www.anychart.com/)\n- [bokeh](https://bokeh.org/)\n- [Comet](https://www.comet.com/site/products/ml-experiment-tracking/?utm_source=awesome-datascience)\n- [slemma](https://slemma.com/)\n- [cartodb](https://cartodb.github.io/odyssey.js/)\n- [Cube](https://square.github.io/cube/)\n- [d3plus](https://d3plus.org/)\n- [Data-Driven Documents(D3js)](https://d3js.org/)\n- [dygraphs](https://dygraphs.com/)\n- [exhibit](https://www.simile-widgets.org/exhibit/)\n- [gephi](https://gephi.org/)\n- [ggplot2](https://ggplot2.tidyverse.org/)\n- [Glue](http://docs.glueviz.org/en/latest/index.html)\n- [Google Chart Gallery](https://developers.google.com/chart/interactive/docs/gallery)\n- [highcarts](https://www.highcharts.com/)\n- [import.io](https://www.import.io/)\n- [Matplotlib](https://matplotlib.org/)\n- [nvd3](https://nvd3.org/)\n- [Netron](https://github.com/lutzroeder/netron)\n- [Openrefine](https://openrefine.org/)\n- [plot.ly](https://plot.ly/)\n- [raw](https://rawgraphs.io)\n- [Resseract Lite](https://github.com/abistarun/resseract-lite)\n- [Seaborn](https://seaborn.pydata.org/)\n- [techanjs](https://techanjs.org/)\n- [Timeline](https://timeline.knightlab.com/)\n- [variancecharts](https://variancecharts.com/index.html)\n- [vida](https://vida.io/)\n- [vizzu](https://github.com/vizzuhq/vizzu-lib)\n- [Wrangler](http://vis.stanford.edu/wrangler/)\n- [r2d3](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n- [NetworkX](https://networkx.org/)\n- [Redash](https://redash.io/)\n- [C3](https://c3js.org/)\n- [TensorWatch](https://github.com/microsoft/tensorwatch)\n- [geomap](https://pypi.org/project/geomap/)\n- [Dash](https://plotly.com/dash/)\n\n### Miscellaneous Tools\n**[`^        back to top        ^`](#awesome-data-science)**\n\n| Link | Description |\n| --- | --- |\n| [The Data Science Lifecycle Process](https://github.com/dslp/dslp) | The Data Science Lifecycle Process is a process for taking data science teams from Idea to Value repeatedly and sustainably. The process is documented in this repo  |\n| [Data Science Lifecycle Template Repo](https://github.com/dslp/dslp-repo-template) | Template repository for data science lifecycle project  |\n| [RexMex](https://github.com/AstraZeneca/rexmex) | A general purpose recommender metrics library for fair evaluation.  |\n| [ChemicalX](https://github.com/AstraZeneca/chemicalx) | A PyTorch based deep learning library for drug pair scoring.  |\n| [PyTorch Geometric Temporal](https://github.com/benedekrozemberczki/pytorch_geometric_temporal) | Representation learning on dynamic graphs.  |\n| [Little Ball of Fur](https://github.com/benedekrozemberczki/littleballoffur) | A graph sampling library for NetworkX with a Scikit-Learn like API.  |\n| [Karate Club](https://github.com/benedekrozemberczki/karateclub) | An unsupervised machine learning extension library for NetworkX with a Scikit-Learn like API. |\n| [ML Workspace](https://github.com/ml-tooling/ml-workspace) | All-in-one web-based IDE for machine learning and data science. The workspace is deployed as a Docker container and is preloaded with a variety of popular data science libraries (e.g., Tensorflow, PyTorch) and dev tools (e.g., Jupyter, VS Code) |\n| [Neptune.ai](https://neptune.ai) | Community-friendly platform supporting data scientists in creating and sharing machine learning models. Neptune facilitates teamwork, infrastructure management, models comparison and reproducibility. |\n| [steppy](https://github.com/minerva-ml/steppy) | Lightweight, Python library for fast and reproducible machine learning experimentation. Introduces very simple interface that enables clean machine learning pipeline design. |\n| [steppy-toolkit](https://github.com/minerva-ml/steppy-toolkit) | Curated collection of the neural networks, transformers and models that make your machine learning work faster and more effective. |\n| [Datalab from Google](https://cloud.google.com/datalab/docs/) | easily explore, visualize, analyze, and transform data using familiar languages, such as Python and SQL, interactively. |\n| [Hortonworks Sandbox](https://www.cloudera.com/downloads/hortonworks-sandbox.html) | is a personal, portable Hadoop environment that comes with a dozen interactive Hadoop tutorials. |\n| [R](https://www.r-project.org/) | is a free software environment for statistical computing and graphics. |\n| [Tidyverse](https://www.tidyverse.org/) | is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. |\n| [RStudio](https://www.rstudio.com) | IDE â€“ powerful user interface for R. Itâ€™s free and open source, and works on Windows, Mac, and Linux. |\n| [Python - Pandas - Anaconda](https://www.anaconda.com) | Completely free enterprise-ready Python distribution for large-scale data processing, predictive analytics, and scientific computing |\n| [Pandas GUI](https://github.com/adrotog/PandasGUI) | Pandas GUI |\n| [Scikit-Learn](https://scikit-learn.org/stable/) | Machine Learning in Python |\n| [NumPy](https://numpy.org/) | NumPy is fundamental for scientific computing with Python. It supports large, multi-dimensional arrays and matrices and includes an assortment of high-level mathematical functions to operate on these arrays. |\n| [Vaex](https://vaex.io/) | Vaex is a Python library that allows you to visualize large datasets and calculate statistics at high speeds. |\n| [SciPy](https://scipy.org/) | SciPy works with NumPy arrays and provides efficient routines for numerical integration and optimization. |\n| [Data Science Toolbox](https://www.coursera.org/learn/data-scientists-tools) | Coursera Course |\n| [Data Science Toolbox](https://datasciencetoolbox.org/) | Blog |\n| [Wolfram Data Science Platform](https://www.wolfram.com/data-science-platform/) | Take numerical, textual, image, GIS or other data and give it the Wolfram treatment, carrying out a full spectrum of data science analysis and visualization and automatically generate rich interactive reportsâ€”all powered by the revolutionary knowledge-based Wolfram Language. |\n| [Datadog](https://www.datadoghq.com/) | Solutions, code, and devops for high-scale data science. |\n| [Variance](https://variancecharts.com/) | Build powerful data visualizations for the web without writing JavaScript |\n| [Kite Development Kit](http://kitesdk.org/docs/current/index.html) | The Kite Software Development Kit (Apache License, Version 2.0), or Kite for short, is a set of libraries, tools, examples, and documentation focused on making it easier to build systems on top of the Hadoop ecosystem. |\n| [Domino Data Labs](https://www.dominodatalab.com) | Run, scale, share, and deploy your models â€” without any infrastructure or setup. |\n| [Apache Flink](https://flink.apache.org/) | A platform for efficient, distributed, general-purpose data processing. |\n| [Apache Hama](https://hama.apache.org/) | Apache Hama is an Apache Top-Level open source project, allowing you to do advanced analytics beyond MapReduce. |\n| [Weka](https://ml.cms.waikato.ac.nz/weka/index.html) | Weka is a collection of machine learning algorithms for data mining tasks. |\n| [Octave](https://www.gnu.org/software/octave/) | GNU Octave is a high-level interpreted language, primarily intended for numerical computations.(Free Matlab) |\n| [Apache Spark](https://spark.apache.org/) | Lightning-fast cluster computing |\n| [Hydrosphere Mist](https://github.com/Hydrospheredata/mist) | a service for exposing Apache Spark analytics jobs and machine learning models as realtime, batch or reactive web services. |\n| [Data Mechanics](https://www.datamechanics.co) | A data science and engineering platform making Apache Spark more developer-friendly and cost-effective. |\n| [Caffe](https://caffe.berkeleyvision.org/) | Deep Learning Framework |\n| [Torch](http://torch.ch/) | A SCIENTIFIC COMPUTING FRAMEWORK FOR LUAJIT |\n| [Nervana''s python based Deep Learning Framework](https://github.com/NervanaSystems/neon) | IntelÂ® Nervanaâ„¢ reference deep learning framework committed to best performance on all hardware. |\n| [Skale](https://github.com/skale-me/skale) | High performance distributed data processing in NodeJS |\n| [Aerosolve](https://airbnb.io/aerosolve/) | A machine learning package built for humans. |\n| [Intel framework](https://github.com/intel/idlf) | IntelÂ® Deep Learning Framework |\n| [Datawrapper](https://www.datawrapper.de/) | An open source data visualization platform helping everyone to create simple, correct and embeddable charts. Also at [github.com](https://github.com/datawrapper/datawrapper) |\n| [Tensor Flow](https://www.tensorflow.org/) | TensorFlow is an Open Source Software Library for Machine Intelligence |\n| [Natural Language Toolkit](https://www.nltk.org/) | An introductory yet powerful toolkit for natural language processing and classification |\n| [Annotation Lab](https://www.johnsnowlabs.com/annotation-lab/) | Free End-to-End No-Code platform for text annotation and DL model training/tuning. Out-of-the-box support for Named Entity Recognition, Classification, Relation extraction and Assertion Status Spark NLP models. Unlimited support for users, teams, projects, documents. |\n| [nlp-toolkit for node.js](https://www.npmjs.com/package/nlp-toolkit) | This module covers some basic nlp principles and implementations. The main focus is performance. When we deal with sample or training data in nlp, we quickly run out of memory. Therefore every implementation in this module is written as stream to only hold that data in memory that is currently processed at any step. |\n| [Julia](https://julialang.org) | high-level, high-performance dynamic programming language for technical computing |\n| [IJulia](https://github.com/JuliaLang/IJulia.jl) | a Julia-language backend combined with the Jupyter interactive environment |\n| [Apache Zeppelin](https://zeppelin.apache.org/) | Web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala and more  |\n| [Featuretools](https://github.com/alteryx/featuretools) | An open source framework for automated feature engineering written in python |\n| [Optimus](https://github.com/hi-primus/optimus) | Cleansing, pre-processing, feature engineering, exploratory data analysis and easy ML with PySpark backend.  |\n| [Albumentations](https://github.com/albumentations-team/albumentations) | Ğ fast and framework agnostic image augmentation library that implements a diverse set of augmentation techniques. Supports classification, segmentation, and detection out of the box. Was used to win a number of Deep Learning competitions at Kaggle, Topcoder and those that were a part of the CVPR workshops. |\n| [DVC](https://github.com/iterative/dvc) | An open-source data science version control system. It helps track, organize and make data science projects reproducible. In its very basic scenario it helps version control and share large data and model files. |\n| [Lambdo](https://github.com/asavinov/lambdo) | is a workflow engine that significantly simplifies data analysis by combining in one analysis pipeline (i) feature engineering and machine learning (ii) model training and prediction (iii) table population and column evaluation. |\n| [Feast](https://github.com/feast-dev/feast) | A feature store for the management, discovery, and access of machine learning features. Feast provides a consistent view of feature data for both model training and model serving. |\n| [Polyaxon](https://github.com/polyaxon/polyaxon) | A platform for reproducible and scalable machine learning and deep learning. |\n| [UBIAI](https://ubiai.tools) | Easy-to-use text annotation tool for teams with most comprehensive auto-annotation features. Supports NER, relations and document classification as well as OCR annotation for invoice labeling |\n| [Trains](https://github.com/allegroai/clearml) | Auto-Magical Experiment Manager, Version Control & DevOps for AI |\n| [Hopsworks](https://github.com/logicalclocks/hopsworks) | Open-source data-intensive machine learning platform with a feature store. Ingest and manage features for both online (MySQL Cluster)  and offline (Apache Hive) access, train and serve models at scale. |\n| [MindsDB](https://github.com/mindsdb/mindsdb) | MindsDB is an Explainable AutoML framework for developers. With MindsDB you can build, train and use state of the art ML models in as simple as one line of code. |\n| [Lightwood](https://github.com/mindsdb/lightwood) | A Pytorch based framework that breaks down machine learning problems into smaller blocks that can be glued together seamlessly with an objective to build predictive models with one line of code. |\n| [AWS Data Wrangler](https://github.com/awslabs/aws-data-wrangler) | An open-source Python package that extends the power of Pandas library to AWS connecting DataFrames and AWS data related services (Amazon Redshift, AWS Glue, Amazon Athena, Amazon EMR, etc). |\n| [Amazon Rekognition](https://aws.amazon.com/rekognition/) | AWS Rekognition is a service that lets developers working with Amazon Web Services add image analysis to their applications. Catalog assets, automate workflows, and extract meaning from your media and applications.|\n| [Amazon Textract](https://aws.amazon.com/textract/) | Automatically extract printed text, handwriting, and data from any document. |\n| [Amazon Lookout for Vision](https://aws.amazon.com/lookout-for-vision/) | Spot product defects using computer vision to automate quality inspection. Identify missing product components, vehicle and structure damage, and irregularities for comprehensive quality control.|\n| [Amazon CodeGuru](https://aws.amazon.com/codeguru/) | Automate code reviews and optimize application performance with ML-powered recommendations.|\n| [CML](https://github.com/iterative/cml) | An open source toolkit for using continuous integration in data science projects. Automatically train and test models in production-like environments with GitHub Actions & GitLab CI, and autogenerate visual reports on pull/merge requests. |\n| [Dask](https://dask.org/) | An open source Python library to painlessly transition your analytics code to distributed computing systems (Big Data) |\n| [Statsmodels](https://www.statsmodels.org/stable/index.html) | A Python-based inferential statistics, hypothesis testing and regression framework |\n| [Gensim](https://radimrehurek.com/gensim/) | An open-source library for topic modeling of natural language text |\n| [spaCy](https://spacy.io/) | A performant natural language processing toolkit |\n| [Grid Studio](https://github.com/ricklamers/gridstudio) | Grid studio is a web-based spreadsheet application with full integration of the Python programming language. |\n|[Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook)|Python Data Science Handbook: full text in Jupyter Notebooks|\n| [Shapley](https://github.com/benedekrozemberczki/shapley) | A data-driven framework to quantify the value of classifiers in a machine learning ensemble.  |\n| [DAGsHub](https://dagshub.com) | A platform built on open source tools for data, model and pipeline management.  |\n| [Deepnote](https://deepnote.com) | A new kind of data science notebook. Jupyter-compatible, with real-time collaboration and running in the cloud. |\n| [Valohai](https://valohai.com) | An MLOps platform that handles machine orchestration, automatic reproducibility and deployment. |\n| [PyMC3](https://docs.pymc.io/) | A Python Library for Probabalistic Programming (Bayesian Inference and Machine Learning) |\n| [PyStan](https://pypi.org/project/pystan/) | Python interface to Stan (Bayesian inference and modeling) |\n| [hmmlearn](https://pypi.org/project/hmmlearn/) | Unsupervised learning and inference of Hidden Markov Models |\n| [Chaos Genius](https://github.com/chaos-genius/chaos_genius/) | ML powered analytics engine for outlier/anomaly detection and root cause analysis |\n| [Nimblebox](https://nimblebox.ai/) | A full-stack MLOps platform designed to help data scientists and machine learning practitioners around the world discover, create, and launch multi-cloud apps from their web browser. |\n| [Towhee](https://github.com/towhee-io/towhee) | A Python library that helps you encode your unstructured data into embeddings. |\n| [LineaPy](https://github.com/LineaLabs/lineapy) | Ever been frustrated with cleaning up long, messy Jupyter notebooks? With LineaPy, an open source Python library, it takes as little as two lines of code to transform messy development code into production pipelines. |\n| [envd](https://github.com/tensorchord/envd) | ğŸ•ï¸ machine learning development environment for data science and AI/ML engineering teams |\n| [Explore Data Science Libraries](https://kandi.openweaver.com/explore/data-science) | A search engine ğŸ” tool to discover & find a curated list of popular & new libraries, top authors, trending project kits, discussions, tutorials & learning resources |\n| [MLEM](https://github.com/iterative/mlem) | ğŸ¶ Version and deploy your ML models following GitOps principles |\n| [MLflow](https://mlflow.org/) | MLOps framework for managing ML models across their full lifecycle |\n| [cleanlab](https://github.com/cleanlab/cleanlab) | Python library for data-centric AI and automatically detecting various issues in ML datasets |\n| [AutoGluon](https://github.com/awslabs/autogluon) | AutoML to easily produce accurate predictions for image, text, tabular, time-series, and multi-modal data |\n| [Arize AI](https://arize.com/) | Arize AI community tier observability tool for monitoring machine learning models in production and root-causing issues such as data quality and performance drift. |\n| [Aureo.io](https://aureo.io) | Aureo.io is a low-code platform that focuses on building artificial intelligence. It provides users with the capability to create pipelines, automations and integrate them with artificial intelligence models â€“ all with their basic data. |\n| [ERD Lab](https://www.erdlab.io/) | Free cloud based entity relationship diagram (ERD) tool made for developers.\n| [Arize-Phoenix](https://docs.arize.com/phoenix) | MLOps in a notebook - uncover insights, surface problems, monitor, and fine tune your models. |\n| [Comet](https://github.com/comet-ml/comet-examples) | An MLOps platform with experiment tracking, model production management, a model registry, and full data lineage to support your ML workflow from training straight through to production. |\n| [Opik](https://github.com/comet-ml/opik) | Evaluate, test, and ship LLM applications across your dev and production lifecycles. |\n| [Synthical](https://synthical.com) | AI-powered collaborative environment for research. Find relevant papers, create collections to manage bibliography, and summarize content â€” all in one place |\n| [teeplot](https://github.com/mmore500/teeplot) | Workflow tool to automatically organize data visualization output |\n| [Streamlit](https://github.com/streamlit/streamlit) | App framework for Machine Learning and Data Science projects |\n| [Gradio](https://github.com/gradio-app/gradio) | Create customizable UI components around machine learning models |\n| [Weights & Biases](https://github.com/wandb/wandb) | Experiment tracking, dataset versioning, and model management |\n| [DVC](https://github.com/iterative/dvc) | Open-source version control system for machine learning projects |\n| [Optuna](https://github.com/optuna/optuna) | Automatic hyperparameter optimization software framework |\n| [Ray Tune](https://github.com/ray-project/ray) | Scalable hyperparameter tuning library |\n| [Apache Airflow](https://github.com/apache/airflow) | Platform to programmatically author, schedule, and monitor workflows |\n| [Prefect](https://github.com/PrefectHQ/prefect) | Workflow management system for modern data stacks |\n| [Kedro](https://github.com/kedro-org/kedro) | Open-source Python framework for creating reproducible, maintainable data science code |\n| [Hamilton](https://github.com/dagworks-inc/hamilton) | Lightweight library to author and manage reliable data transformations |\n| [SHAP](https://github.com/slundberg/shap) | Game theoretic approach to explain the output of any machine learning model |\n| [InterpretML](https://github.com/interpretml/interpret) | InterpretML implements the Explainable Boosting Machine (EBM), a modern, fully interpretable machine learning model based on Generalized Additive Models (GAMs). This open-source package also provides visualization tools for EBMs, other glass-box models, and black-box explanations |\n| [LIME](https://github.com/marcotcr/lime) | Explaining the predictions of any machine learning classifier |\n| [flyte](https://github.com/flyteorg/flyte) | Workflow automation platform for machine learning |\n| [dbt](https://github.com/dbt-labs/dbt-core) | Data build tool |\n| [zasper](https://github.com/zasper-io/zasper) | Supercharged IDE for Data ScienceÂ |\n| [skrub](https://github.com/skrub-data/skrub/) | A Python library to ease preprocessing and feature engineering for tabular machine learningÂ |\n| [Codeflash](https://www.codeflash.ai/) | Ship Blazing-Fast Python Code â€” Every Time |\n| [Hugging Face](https://huggingface.co/) | Popular open platform for sharing ML models, datasets, and collaborating on NLP and generative AI projects. |\n| [Chinese-Elite](https://github.com/anonym-g/Chinese-Elite) | An open-source project that automatically maps relationship networks by parsing public data using LLMs and visualizes it as an interactive graph. |\n| [Desbordante](https://github.com/desbordante/desbordante-core/) | An open-source data profiler specifically focused on discovery and validation of complex patterns,  such as [numerical association rules](https://colab.research.google.com/github/Desbordante/desbordante-core/blob/main/examples/notebooks/Numerical_Association_Rules.ipynb), [differential dependencies](https://colab.research.google.com/github/Desbordante/desbordante-core/blob/main/examples/notebooks/Differential_Dependencies.ipynb), [denial constraints](https://colab.research.google.com/github/Desbordante/desbordante-core/blob/main/examples/notebooks/Denial_Constraints.ipynb), and more. |\n| [RunMat](https://github.com/runmat-org/runmat) | Fast MATLAB-syntax runtime with automatic CPU/GPU execution and fused array kernels. |\n\n\n## Literature and Media\n**[`^        back to top        ^`](#awesome-data-science)**\n\nThis section includes some additional reading material, channels to watch, and talks to listen to.\n\n### Books\n**[`^        back to top        ^`](#awesome-data-science)**\n\n- [Data Science From Scratch: First Principles with Python](https://www.amazon.com/Data-Science-Scratch-Principles-Python-dp-1492041130/dp/1492041130/ref=dp_ob_title_bk)\n- [Artificial Intelligence with Python - Tutorialspoint](https://www.tutorialspoint.com/artificial_intelligence_with_python/artificial_intelligence_with_python_tutorial.pdf)\n- [Machine Learning from Scratch](https://dafriedman97.github.io/mlbook/content/introduction.html)\n- [Probabilistic Machine Learning: An Introduction](https://probml.github.io/pml-book/book1.html)\n- [How to Lead in Data Science](https://www.manning.com/books/how-to-lead-in-data-science) - Early Access\n- [Fighting Churn With Data](https://www.manning.com/books/fighting-churn-with-data)\n- [Data Science at Scale with Python and Dask](https://www.manning.com/books/data-science-with-python-and-dask)\n- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/)\n- [The Data Science Handbook: Advice and Insights from 25 Amazing Data Scientists](https://www.thedatasciencehandbook.com/)\n- [Think Like a Data Scientist](https://www.manning.com/books/think-like-a-data-scientist)\n- [Introducing Data Science](https://www.manning.com/books/introducing-data-science)\n- [Practical Data Science with R](https://www.manning.com/books/practical-data-science-with-r)\n- [Everyday Data Science](https://www.amazon.com/dp/B08TZ1MT3W/ref=cm_sw_r_cp_apa_fabc_a0ceGbWECF9A8) & [(cheaper PDF version)](https://gum.co/everydaydata)\n- [Exploring Data Science](https://www.manning.com/books/exploring-data-science) - free eBook sampler\n- [Exploring the Data Jungle](https://www.manning.com/books/exploring-the-data-jungle) - free eBook sampler\n- [Classic Computer Science Problems in Python](https://www.manning.com/books/classic-computer-science-problems-in-python)\n- [Math for Programmers](https://www.manning.com/books/math-for-programmers) Early access\n- [R in Action, Third Edition](https://www.manning.com/books/r-in-action-third-edition) Early Access\n- [Data Science Bookcamp](https://www.manning.com/books/data-science-bookcamp) Early access\n- [Data Science Thinking: The Next Scientific, Technological and Economic Revolution](https://www.springer.com/gp/book/9783319950914)\n- [Applied Data Science: Lessons Learned for the Data-Driven Business](https://www.springer.com/gp/book/9783030118204)\n- [The Data Science Handbook](https://www.amazon.com/Data-Science-Handbook-Field-Cady/dp/1119092949)\n- [Essential Natural Language Processing](https://www.manning.com/books/getting-started-with-natural-language-processing) - Early access\n- [Mining Massive Datasets](http://www.mmds.org/) - free e-book comprehended by an online course\n- [Pandas in Action](https://www.manning.com/books/pandas-in-action) - Early access\n- [Genetic Algorithms and Genetic Programming](https://www.taylorfrancis.com/books/9780429141973)\n- [Advances in Evolutionary Algorithms](https://www.intechopen.com/books/advances_in_evolutionary_algorithms) - Free Download\n- [Genetic Programming: New Approaches and Successful Applications](https://www.intechopen.com/books/genetic-programming-new-approaches-and-successful-applications) - Free Download\n- [Evolutionary Algorithms](https://www.intechopen.com/books/evolutionary-algorithms) - Free Download\n- [Advances in Genetic Programming, Vol. 3](http://www0.cs.ucl.ac.uk/staff/W.Langdon/aigp3/) - Free Download\n- [Genetic Algorithms and Evolutionary Computation](https://www.talkorigins.org/faqs/genalg/genalg.html) - Free Download\n- [Convex Optimization](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf) - Convex Optimization book by Stephen Boyd - Free Download\n- [Data Analysis with Python and PySpark](https://www.manning.com/books/data-analysis-with-python-and-pyspark) - Early Access\n- [R for Data Science](https://r4ds.had.co.nz/)\n- [Build a Career in Data Science](https://www.manning.com/books/build-a-career-in-data-science)\n- [Machine Learning Bookcamp](https://mlbookcamp.com/) - Early access\n- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n- [Effective Data Science Infrastructure](https://www.manning.com/books/effective-data-science-infrastructure)\n- [Practical MLOps: How to Get Ready for Production Models](https://valohai.com/mlops-ebook/)\n- [Data Analysis with Python and PySpark](https://www.manning.com/books/data-analysis-with-python-and-pyspark)\n- [Regression, a Friendly guide](https://www.manning.com/books/regression-a-friendly-guide) - Early Access\n- [Streaming Systems: The What, Where, When, and How of Large-Scale Data Processing](https://www.oreilly.com/library/view/streaming-systems/9781491983867/)\n- [Data Science at the Command Line: Facing the Future with Time-Tested Tools](https://www.oreilly.com/library/view/data-science-at/9781491947845/)\n- [Machine Learning with Python - Tutorialspoint](https://www.tutorialspoint.com/machine_learning_with_python/machine_learning_with_python_tutorial.pdf)\n- [Deep Learning](https://www.deeplearningbook.org/)\n- [Designing Cloud Data Platforms](https://www.manning.com/books/designing-cloud-data-platforms) - Early Access\n- [An Introduction to Statistical Learning with Applications in R](https://www.statlearning.com/)\n- [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://hastie.su.domains/ElemStatLearn/)\n- [Deep Learning with PyTorch](https://www.simonandschuster.com/books/Deep-Learning-with-PyTorch/Eli-Stevens/9781617295263)\n- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com)\n- [Deep Learning Cookbook](https://www.oreilly.com/library/view/deep-learning-cookbook/9781491995839/)\n- [Introduction to Machine Learning with Python](https://www.oreilly.com/library/view/introduction-to-machine/9781449369880/)\n- [Artificial Intelligence: Foundations of Computational Agents, 2nd Edition](https://artint.info/index.html) - Free HTML version\n- [The Quest for Artificial Intelligence: A History of Ideas and Achievements](https://ai.stanford.edu/~nilsson/QAI/qai.pdf) - Free Download\n- [Graph Algorithms for Data Science](https://www.manning.com/books/graph-algorithms-for-data-science) - Early Access\n- [Data Mesh in Action](https://www.manning.com/books/data-mesh-in-action) - Early Access\n- [Julia for Data Analysis](https://www.manning.com/books/julia-for-data-analysis) - Early Access\n- [Casual Inference for Data Science](https://www.manning.com/books/julia-for-data-analysis) - Early Access\n- [Regular Expression Puzzles and AI Coding Assistants](https://www.manning.com/books/regular-expression-puzzles-and-ai-coding-assistants) by David Mertz\n- [Dive into Deep Learning](https://d2l.ai/)\n- [Data for All](https://www.manning.com/books/data-for-all)\n- [Interpretable Machine Learning: A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/) - Free GitHub version\n- [Foundations of Data Science](https://www.cs.cornell.edu/jeh/book.pdf) Free Download \n- [Comet for DataScience: Enhance your ability to manage and optimize the life cycle of your data science project](https://www.amazon.com/Comet-Data-Science-Enhance-optimize/dp/1801814430) \n- [Software Engineering for Data Scientists](https://www.manning.com/books/software-engineering-for-data-scientists) - Early Access\n- [Julia for Data Science](https://www.manning.com/books/julia-for-data-science) - Early Access\n- [An Introduction to Statistical Learning](https://www.statlearning.com/) - Download Page\n- [Machine Learning For Absolute Beginners](https://www.amazon.in/Machine-Learning-Absolute-Beginners-Introduction-ebook/dp/B07335JNW1)\n- [Unifying Business, Data, and Code: Designing Data Products with JSON Schema](https://learning.oreilly.com/library/view/unifying-business-data/9781098144999/)\n- [Grokking Bayes](https://www.manning.com/books/grokking-bayes)\n- [Machine Learning Q and AI](https://sebastianraschka.com/books/ml-q-and-ai)\n\n#### Book Deals (Affiliated)\n\n- [eBook sale - Save up to 45% on eBooks!](https://www.manning.com/?utm_source=mikrobusiness&utm_medium=affiliate&utm_campaign=ebook_sale_8_8_22)\n\n- [Causal Machine Learning](https://www.manning.com/books/causal-machine-learning?utm_source=mikrobusiness&utm_medium=affiliate&utm_campaign=book_ness_causal_7_26_22&a_aid=mikrobusiness&a_bid=43a2198b\n)\n- [Managing ML Projects](https://www.manning.com/books/managing-machine-learning-projects?utm_source=mikrobusiness&utm_medium=affiliate&utm_campaign=book_thompson_managing_6_14_22)\n- [Causal Inference for Data Science](https://www.manning.com/books/causal-inference-for-data-science?utm_source=mikrobusiness&utm_medium=affiliate&utm_campaign=book_ruizdevilla_causal_6_6_22)\n- [Data for All](https://www.manning.com/books/data-for-all?utm_source=mikrobusiness&utm_medium=affiliate)\n\n### Journals, Publications and Magazines\n**[`^        back to top        ^`](#awesome-data-science)**\n\n- [ICML](https://icml.cc/2015/) - International Conference on Machine Learning\n- [GECCO](https://gecco-2019.sigevo.org/index.html/HomePage) - The Genetic and Evolutionary Computation Conference (GECCO)\n- [epjdatascience](https://epjdatascience.springeropen.com/)\n- [Journal of Data Science](https://jds-online.org/journal/JDS) - an international journal devoted to applications of statistical methods at large\n- [Big Data Research](https://www.journals.elsevier.com/big-data-research)\n- [Journal of Big Data](https://journalofbigdata.springeropen.com/)\n- [Big Data & Society](https://journals.sagepub.com/home/bds)\n- [Data Science Journal](https://www.jstage.jst.go.jp/browse/dsj)\n- [datatau.com/news](https://www.datatau.com/news) - Like Hacker News, but for data\n- [Data Science Trello Board](https://trello.com/b/rbpEfMld/data-science)\n- [Medium Data Science Topic](https://medium.com/tag/data-science) - Data Science related publications on medium\n- [Towards Data Science Genetic Algorithm Topic](https://towardsdatascience.com/introduction-to-genetic-algorithms-including-example-code-e396e98d8bf3#:~:text=A%20genetic%20algorithm%20is%20a,offspring%20of%20the%20next%20generation.) -Genetic Algorithm related Publications towards Data Science\n- [Maxim AI](https://getmaxim.ai). Tool for AI Agent Simulation, Evaluation & Observability. \n\n### Newsletters\n**[`^        back to top        ^`](#awesome-data-science)**\n\n- [DataTalks.Club](https://datatalks.club). A weekly newsletter about data-related things. [Archive](https://us19.campaign-archive.com/home/?u=0d7822ab98152f5afc118c176&id=97178021aa).\n- [The Analytics Engineering Roundup](https://roundup.getdbt.com/about). A newsletter about data science. [Archive](https://roundup.getdbt.com/archive).\n\n\n### Bloggers\n**[`^        back to top        ^`](#awesome-data-science)**\n\n- [Wes McKinney](https://wesmckinney.com/archives.html) - Wes McKinney Archives.\n- [Matthew Russell](https://miningthesocialweb.com/) - Mining The Social Web.\n- [Greg Reda](http://www.gregreda.com/) - Greg Reda Personal Blog\n- [Julia Evans](https://jvns.ca/) - Recurse Center alumna\n- [Hakan Kardas](https://www.cse.unr.edu/~hkardes/) - Personal Web Page\n- [Sean J. Taylor](https://seanjtaylor.com/) - Personal Web Page\n- [Drew Conway](http://drewconway.com/) - Personal Web Page\n- [Hilary Mason](https://hilarymason.com/) - Personal Web Page\n- [Noah Iliinsky](http://complexdiagrams.com/) - Personal Blog\n- [Matt Harrison](https://hairysun.com/) - Personal Blog\n- [Vamshi Ambati](https://allthingsds.wordpress.com/) - AllThings Data Sciene\n- [Prash Chan](https://www.mdmgeek.com/) - Tech Blog on Master Data Management And Every Buzz Surrounding It\n- [Clare Corthell](http://datasciencemasters.org/) - The Open Source Data Science Masters\n- [Datawrangling](http://www.datawrangling.org) by Peter Skomoroch. MACHINE LEARNING, DATA MINING, AND MORE\n- [Quora Data Science](https://www.quora.com/topic/Data-Science) - Data Science Questions and Answers from experts\n- [Siah](https://openresearch.wordpress.com/) a PhD student at Berkeley\n- [Louis Dorard](https://www.ownml.co/blog/) a technology guy with a penchant for the web and for data, big and small\n- [Machine Learning Mastery](https://machinelearningmastery.com/) about helping professional programmers confidently apply machine learning algorithms to address complex problems.\n- [Daniel Forsyth](https://www.danielforsyth.me/) - Personal Blog\n- [Data Science Weekly](https://www.datascienceweekly.org/) - Weekly News Blog\n- [Revolution Analytics](https://blog.revolutionanalytics.com/) - Data Science Blog\n- [R Bloggers](https://www.r-bloggers.com/) - R Bloggers\n- [The Practical Quant](https://practicalquant.blogspot.com/) Big data\n- [Yet Another Data Blog](https://yet-another-data-blog.blogspot.com/) Yet Another Data Blog\n- [KD Nuggets](https://www.kdnuggets.com/) Data Mining, Analytics, Big Data, Data, Science not a blog a portal\n- [Meta Brown](https://www.metabrown.com/blog/) - Personal Blog\n- [Data Scientist](https://datascientists.com/) is building the data scientist culture.\n- [WhatSTheBigData](https://whatsthebigdata.com/) is some of, all of, or much more than the above and this blog explores its impact on information technology, the business world, government agencies, and our lives.\n- [Tevfik Kosar](https://magnus-notitia.blogspot.com/) - Magnus Notitia\n- [New Data Scientist](https://newdatascientist.blogspot.com/) How a Social Scientist Jumps into the World of Big Data\n- [Harvard Data Science](https://harvarddatascience.com/) - Thoughts on Statistical Computing and Visualization\n- [Data Science 101](https://ryanswanstrom.com/datascience101/) - Learning To Be A Data Scientist\n- [Kaggle Past Solutions](https://www.chioka.in/kaggle-competition-solutions/)\n- [DataScientistJourney](https://datascientistjourney.wordpress.com/category/data-science/)\n- [NYC Taxi Visualization Blog](https://chriswhong.github.io/nyctaxi/)\n- [Data-Mania](https://www.data-mania.com/)\n- [Data-Magnum](https://data-magnum.com/)\n- [datascopeanalytics](https://datascopeanalytics.com/blog/)\n- [Digital transformation](https://tarrysingh.com/)\n- [datascientistjourney](https://datascientistjourney.wordpress.com/category/data-science/)\n- [Data Mania Blog](https://www.data-mania.com/blog/) - [The File Drawer](https://chris-said.io/) - Chris Said''s science blog\n- [Emilio Ferrara''s web page](http://www.emilio.ferrara.name/)\n- [DataNews](https://datanews.tumblr.com/)\n- [Reddit TextMining](https://www.reddit.com/r/textdatamining/)\n- [Periscopic](https://periscopic.com/#!/news)\n- [Hilary Parker](https://hilaryparker.com/)\n- [Data Stories](https://datastori.es/)\n- [Data Science Lab](https://datasciencelab.wordpress.com/)\n- [Meaning of](https://www.kennybastani.com/)\n- [Adventures in Data Land](https://blog.smola.org)\n- [Dataclysm](https://theblog.okcupid.com/)\n- [FlowingData](https://flowingdata.com/) - Visualization and Statistics\n- [Calculated Risk](https://www.calculatedriskblog.com/)\n- [O''reilly Learning Blog](https://www.oreilly.com/content/topics/oreilly-learning/)\n- [Dominodatalab](https://blog.dominodatalab.com/)\n- [i am trask](https://iamtrask.github.io/) - A Machine Learning Craftsmanship Blog\n- [Vademecum of Practical Data Science](https://datasciencevademecum.wordpress.com/) - Handbook and recipes for data-driven solutions of real-world problems\n- [Dataconomy](https://dataconomy.com/) - A blog on the newly emerging data economy\n- [Springboard](https://www.springboard.com/blog/) - A blog with resources for data science learners\n- [Analytics Vidhya](https://www.analyticsvidhya.com/) - A full-fledged website about data science and analytics study material.\n- [Occam''s Razor](https://www.kaushik.net/avinash/) - Focused on Web Analytics.\n- [Data School](https://www.dataschool.io/) - Data science tutorials for beginners!\n- [Colah''s Blog](https://colah.github.io) - Blog for understanding Neural Networks!\n- [Sebastian''s Blog](https://ruder.io/#open) - Blog for NLP and transfer learning!\n- [Distill](https://distill.pub) - Dedicated to clear explanations of machine learning!\n- [Chris Albon''s Website](https://chrisalbon.com/) - Data Science and AI notes\n- [Andrew Carr](https://andrewnc.github.io/blog/blog.html) - Data Science with Esoteric programming languages\n- [floydhub](https://blog.floydhub.com/introduction-to-genetic-algorithms/) - Blog for Evolutionary Algorithms\n- [Jingles](https://jinglescode.github.io/) - Review and extract key concepts from academic papers\n- [nbshare](https://www.nbshare.io/notebooks/data-science/) - Data Science notebooks\n- [Loic Tetrel](https://ltetrel.github.io/) - Data science blog\n- [Chip Huyen''s Blog](https://huyenchip.com/blog/) - ML Engineering, MLOps, and the use of ML in startups\n- [Maria Khalusova](https://www.mariakhalusova.com/) - Data science blog\n- [Aditi Rastogi](https://medium.com/@aditi2507rastogi) - ML,DL,Data Science blog\n- [Santiago Basulto](https://medium.com/@santiagobasulto) - Data Science with Python\n- [Akhil Soni](https://medium.com/@akhil0435) - ML, DL and Data Science\n- [Akhil Soni](https://akhilworld.hashnode.dev/) - ML, DL and Data Science \n- [Applied AI Blogs](https://www.appliedaicourse.com/blog/) - In-depth articles on AI, machine learning, and data science concepts with practical applications.\n- [Scaler Blogs](https://www.scaler.com/blog/) - Educational content on software development, AI, and career growth in tech.\n- [Mlu github](https://mlu-explain.github.io/) - Mlu is developed amazon to help people in ml space you can learn everything from basics here with live diagrams\n\n### Presentations\n**[`^        back to top        ^`](#awesome-data-science)**\n\n- [How to Become a Data Scientist](https://www.slideshare.net/ryanorban/how-to-become-a-data-scientist)\n- [Introduction to Data Science](https://www.slideshare.net/NikoVuokko/introduction-to-data-science-25391618)\n- [Intro to Data Science for Enterprise Big Data](https://www.slideshare.net/pacoid/intro-to-data-science-for-enterprise-big-data)\n- [How to Interview a Data Scientist](https://www.slideshare.net/dtunkelang/how-to-interview-a-data-scientist)\n- [How to Share Data with a Statistician](https://github.com/jtleek/datasharing)\n- [The Science of a Great Career in Data Science](https://www.slideshare.net/katemats/the-science-of-a-great-career-in-data-science)\n- [What Does a Data Scientist Do?](https://www.slideshare.net/datasciencelondon/big-data-sorry-data-science-what-does-a-data-scientist-do)\n- [Building Data Start-Ups: Fast, Big, and Focused](https://www.slideshare.net/medriscoll/driscoll-strata-buildingdatastartups25may2011clean)\n- [How to win data science competitions with Deep Learning](https://www.slideshare.net/0xdata/how-to-win-data-science-competitions-with-deep-learning)\n- [Full-Stack Data Scientist](https://www.slideshare.net/AlexeyGrigorev/fullstack-data-scientist)\n\n### Podcasts\n**[`^        back to top        ^`](#awesome-data-science)**\n\n- [AI at Home](https://podcasts.apple.com/us/podcast/data-science-at-home/id1069871378)\n- [AI Today](https://www.cognilytica.com/aitoday/)\n- [Adversarial Learning](https://adversariallearning.com/)\n- [Chai time Data Science](https://www.youtube.com/playlist?list=PLLvvXm0q8zUbiNdoIazGzlENMXvZ9bd3x)\n- [Data Engineering Podcast](https://www.dataengineeringpodcast.com/)\n- [Data Science at Home](https://datascienceathome.com/)\n- [Data Science Mixer](https://community.alteryx.com/t5/Data-Science-Mixer/bg-p/mixer)\n- [Data Skeptic](https://dataskeptic.com/)\n- [Data Stories](https://datastori.es/)\n- [Datacast](https://jameskle.com/writes/category/Datacast)\n- [DataFramed](https://www.datacamp.com/community/podcast)\n- [DataTalks.Club](https://anchor.fm/datatalksclub)\n- [Gradient Descent](https://wandb.ai/fully-connected/gradient-descent)\n- [Learning Machines 101](https://www.learningmachines101.com/)\n- [Let''s Data (Brazil)](https://www.youtube.com/playlist?list=PLn_z5E4dh_Lj5eogejMxfOiNX3nOhmhmM)\n- [Linear Digressions](https://lineardigressions.com/)\n- [Not So Standard Deviations](https://nssdeviations.com/)\n- [O''Reilly Data Show Podcast](https://www.oreilly.com/radar/topics/oreilly-data-show-podcast/)\n- [Partially Derivative](http://partiallyderivative.com/)\n- [Superdatascience](https://www.superdatascience.com/podcast/)\n- [The Data Engineering Show](https://www.dataengineeringshow.com/)\n- [The Radical AI Podcast](https://www.radicalai.org/)\n- [What''s The Point](https://fivethirtyeight.com/tag/whats-the-point/)\n- [The Analytics Engineering Podcast](https://roundup.getdbt.com/s/the-analytics-engineering-podcast)\n\n### YouTube Videos & Channels\n**[`^        back to top        ^`](#awesome-data-science)**\n\n- [What is machine learning?](https://www.youtube.com/watch?v=WXHM_i-fgGo)\n- [Andrew Ng: Deep Learning, Self-Taught Learning and Unsupervised Feature Learning](https://www.youtube.com/watch?v=n1ViNeWhC24)\n- [Data36 - Data Science for Beginners by Tomi Mester](https://www.youtube.com/c/TomiMesterData36comDataScienceForBeginners)\n- [Deep Learning: Intelligence from Big Data](https://www.youtube.com/watch?v=czLI3oLDe8M)\n- [Interview with Google''s AI and Deep Learning ''Godfather'' Geoffrey Hinton](https://www.youtube.com/watch?v=1Wp3IIpssEc)\n- [Introduction to Deep Learning with Python](https://www.youtube.com/watch?v=S75EdAcXHKk)\n- [What is machine learning, and how does it work?](https://www.youtube.com/watch?v=elojMnjn4kk)\n- [CampusX](https://www.youtube.com/@campusx-official)\n- [Data School](https://www.youtube.com/channel/UCnVzApLJE2ljPZSeQylSEyg) - Data Science Education\n- [Neural Nets for Newbies by Melanie Warrick (May 2015)](https://www.youtube.com/watch?v=Cu6A96TUy_o)\n- [Neural Networks video series by Hugo Larochelle](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH)\n- [Google DeepMind co-founder Shane Legg - Machine Super Intelligence](https://www.youtube.com/watch?v=evNCyRL3DOU)\n- [Data Science Primer](https://www.youtube.com/watch?v=cHzvYxBN9Ls&list=PLPqVjP3T4RIRsjaW07zoGzH-Z4dBACpxY)\n- [Data Science with Genetic Algorithms](https://www.youtube.com/watch?v=lpD38NxTOnk)\n- [Data Science for Beginners](https://www.youtube.com/playlist?list=PL2zq7klxX5ATMsmyRazei7ZXkP1GHt-vs)\n- [DataTalks.Club](https://www.youtube.com/channel/UCDvErgK0j5ur3aLgn6U-LqQ)\n- [Mildlyoverfitted - Tutorials on intermediate ML/DL topics](https://www.youtube.com/channel/UCYBSjwkGTK06NnDnFsOcR7g)\n- [mlops.community - Interviews of industry experts about production ML](https://www.youtube.com/channel/UCYBSjwkGTK06NnDnFsOcR7g)\n- [ML Street Talk - Unabashedly technical and non-commercial, so you will hear no annoying pitches.](https://www.youtube.com/c/machinelearningstreettalk)\n- [Neural networks by 3Blue1Brown ](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n- [Neural networks from scratch by Sentdex](https://www.youtube.com/playlist?list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3)\n- [Manning Publications YouTube channel](https://www.youtube.com/c/ManningPublications/featured)\n- [Ask Dr Chong: How to Lead in Data Science - Part 1](https://youtu.be/JYuQZii5o58)\n- [Ask Dr Chong: How to Lead in Data Science - Part 2](https://youtu.be/SzqIXV-O-ko)\n- [Ask Dr Chong: How to Lead in Data Science - Part 3](https://youtu.be/Ogwm7k_smTA)\n- [Ask Dr Chong: How to Lead in Data Science - Part 4](https://youtu.be/a9usjdzTxTU)\n- [Ask Dr Chong: How to Lead in Data Science - Part 5](https://youtu.be/MYdQq-F3Ws0)\n- [Ask Dr Chong: How to Lead in Data Science - Part 6](https://youtu.be/LOOt4OVC3hY)\n- [Regression Models: Applying simple Poisson regression](https://www.youtube.com/watch?v=9Hk8K8jhiOo)\n- [Deep Learning Architectures](https://www.youtube.com/playlist?list=PLv8Cp2NvcY8DpVcsmOT71kymgMmcr59Mf)\n- [Time Series Modelling and Analysis](https://www.youtube.com/playlist?list=PL3N9eeOlCrP5cK0QRQxeJd6GrQvhAtpBK)\n- [Serrano.Academy](https://www.youtube.com/@SerranoAcademy)\n- [End to End Data Science Playlist](https://www.youtube.com/watch?v=S_F_c9e2bz4&list=PLZoTAELRMXVPS-dOaVbAux22vzqdgoGhG)\n- [Introduction to Data Science - Linkedin](https://www.linkedin.com/learning/introduction-to-data-science-22668235/beginning-your-data-science-exploration?u=42458916)\n\n## Socialize\n**[`^        back to top        ^`](#awesome-data-science)**\n\nBelow are some Social Media links. Connect with other data scientists!\n\n- [Facebook Accounts](#facebook-accounts)\n- [Twitter Accounts](#twitter-accounts)\n- [Telegram Channels](#telegram-channels)\n- [Slack Communities](#slack-communities)\n- [GitHub Groups](#github-groups)\n- [Data Science Competitions](#data-science-competitions)\n\n\n### Facebook Accounts\n**[`^        back to top        ^`](#awesome-data-science)**\n\n- [Data](https://www.facebook.com/data)\n- [Big Data Scientist](https://www.facebook.com/Bigdatascientist)\n- [Data Science Day](https://www.facebook.com/datascienceday/)\n- [Data Science Academy](https://www.facebook.com/nycdatascience)\n- [Facebook Data Science Page](https://www.facebook.com/pages/Data-science/431299473579193?ref=br_rs)\n- [Data Science London](https://www.facebook.com/pages/Data-Science-London/226174337471513)\n- [Data Science Technology and Corporation](https://www.facebook.com/DataScienceTechnologyCorporation?ref=br_rs)\n- [Data Science - Closed Group](https://www.facebook.com/groups/1394010454157077/?ref=br_rs)\n- [Center for Data Science](https://www.facebook.com/centerdatasciences?ref=br_rs)\n- [Big data hadoop NOSQL Hive Hbase](https://www.facebook.com/groups/bigdatahadoop/)\n- [Analytics, Data Mining, Predictive Modeling, Artificial Intelligence](https://www.facebook.com/groups/data.analytics/)\n- [Big Data Analytics using R](https://www.facebook.com/groups/434352233255448/)\n- [Big Data Analytics with R and Hadoop](https://www.facebook.com/groups/rhadoop/)\n- [Big Data Learnings](https://www.facebook.com/groups/bigdatalearnings/)\n- [Big Data, Data Science, Data Mining & Statistics](https://www.facebook.com/groups/bigdatastatistics/)\n- [BigData/Hadoop Expert](https://www.facebook.com/groups/BigDataExpert/)\n- [Data Mining / Machine Learning / AI](https://www.facebook.com/groups/machinelearningforum/)\n- [Data Mining/Big Data - Social Network Ana](https://www.facebook.com/groups/dataminingsocialnetworks/)\n- [Vademecum of Practical Data Science](https://www.facebook.com/datasciencevademecum)\n- [Veri Bilimi Istanbul](https://www.facebook.com/groups/veribilimiistanbul/)\n- [The Data Science Blog](https://www.facebook.com/theDataScienceBlog/)\n\n\n### Twitter Accounts\n**[`^        back to top        ^`](#awesome-data-science)**\n\n| Twitter | Description |\n| --- | --- |\n| [Big Data Combine](https://twitter.com/BigDataCombine) | Rapid-fire, live tryouts for data scientists seeking to monetize their models as trading strategies |\n| Big Data Mania | Data Viz Wiz, Data Journalist, Growth Hacker, Author of Data Science for Dummies (2015) |\n| [Big Data Science](https://twitter.com/analyticbridge) | Big Data, Data Science, Predictive Modeling, Business Analytics, Hadoop, Decision and Operations Research. |\n| Charlie Greenbacker | Director of Data Science at @ExploreAltamira |\n| [Chris Said](https://twitter.com/Chris_Said) | Data scientist at Twitter |\n| [Clare Corthell](https://twitter.com/clarecorthell) | Dev, Design, Data Science @mattermark #hackerei |\n| [DADI Charles-Abner](https://twitter.com/DadiCharles) | #datascientist @Ekimetrics. , #machinelearning #dataviz #DynamicCharts #Hadoop #R #Python #NLP #Bitcoin #dataenthousiast |\n| [Data Science Central](https://twitter.com/DataScienceCtrl) | Data Science Central is the industry''s single resource for Big Data practitioners. |\n| [Data Science London](https://twitter.com/ds_ldn)  | Data Science. Big Data. Data Hacks. Data Junkies. Data Startups. Open Data |\n| [Data Science Renee](https://twitter.com/BecomingDataSci) | Documenting my path from SQL Data Analyst pursuing an Engineering Master''s Degree to Data Scientist |\n| [Data Science Report](https://twitter.com/TedOBrien93) | Mission is to help guide & advance careers in Data Science & Analytics |\n| [Data Science Tips](https://twitter.com/datasciencetips) | Tips and Tricks for Data Scientists around the world! #datascience #bigdata |\n| [Data Vizzard](https://twitter.com/DataVisualizati) | DataViz, Security, Military |\n| [DataScienceX](https://twitter.com/DataScienceX) |  |\n| deeplearning4j | |\n| [DJ Patil](https://twitter.com/dpatil) | White House Data Chief, VP @ RelateIQ. |\n| [Domino Data Lab](https://twitter.com/DominoDataLab) | |\n| [Drew Conway](https://twitter.com/drewconway) | Data nerd, hacker, student of conflict. |\n| Emilio Ferrara | #Networks, #MachineLearning and #DataScience. I work on #Social Media. Postdoc at @IndianaUniv |\n| [Erin Bartolo](https://twitter.com/erinbartolo) | Running with #BigData--enjoying a love/hate relationship with its hype. @iSchoolSU #DataScience Program Mgr. |\n| [Greg Reda](https://twitter.com/gjreda)  | Working @ _GrubHub_ about data and pandas |\n| [Gregory Piatetsky](https://twitter.com/kdnuggets) |  KDnuggets President, Analytics/Big Data/Data Mining/Data Science expert, KDD & SIGKDD co-founder, was Chief Scientist at 2 startups, part-time philosopher. |\n| [Hadley Wickham](https://twitter.com/hadleywickham) |  Chief Scientist at RStudio, and an Adjunct Professor of Statistics at the University of Auckland, Stanford University, and Rice University. |\n| [Hakan Kardas](https://twitter.com/hakan_kardes) | Data Scientist |\n| [Hilary Mason](https://twitter.com/hmason) | Data Scientist in Residence at @accel. |\n| [Jeff Hammerbacher](https://twitter.com/hackingdata)  | ReTweeting about data science |\n| [John Myles White](https://twitter.com/johnmyleswhite)  | Scientist at Facebook and Julia developer. Author of Machine Learning for Hackers and Bandit Algorithms for Website Optimization. Tweets reflect my views only. |\n| [Juan Miguel Lavista](https://twitter.com/BDataScientist) | Principal Data Scientist @ Microsoft Data Science Team |\n| [Julia Evans](https://twitter.com/b0rk) | Hacker - Pandas - Data Analyze |\n| [Kenneth Cukier](https://twitter.com/kncukier) | The Economist''s Data Editor and co-author of Big Data (http://www.big-data-book.com/). |\n| Kevin Davenport | Organizer of https://www.meetup.com/San-Diego-Data-Science-R-Users-Group/ |\n| [Kevin Markham](https://twitter.com/justmarkham) | Data science instructor, and founder of [Data School](https://www.dataschool.io/) |\n| [Kim Rees](https://twitter.com/krees) | Interactive data visualization and tools. Data flaneur. |\n| [Kirk Borne](https://twitter.com/KirkDBorne) | DataScientist, PhD Astrophysicist, Top #BigData Influencer. |\n| Linda Regber | Data storyteller, visualizations. |\n| [Luis Rei](https://twitter.com/lmrei) | PhD Student. Programming, Mobile, Web. Artificial Intelligence, Intelligent Robotics Machine Learning, Data Mining, Natural Language Processing, Data Science. |\n| Mark Stevenson | Data Analytics Recruitment Specialist at Salt (@SaltJobs)  Analytics - Insight - Big Data - Data science |\n| [Matt Harrison](https://twitter.com/__mharrison__) | Opinions of full-stack Python guy, author, instructor, currently playing Data Scientist. Occasional fathering, husbanding, organic gardening. |\n| [Matthew Russell](https://twitter.com/ptwobrussell) | Mining the Social Web. |\n| [Mert NuhoÄŸlu](https://twitter.com/mertnuhoglu)  | Data Scientist at BizQualify, Developer |\n| [Monica Rogati](https://twitter.com/mrogati) | Data @ Jawbone. Turned data into stories & products at LinkedIn. Text mining, applied machine learning, recommender systems. Ex-gamer, ex-machine coder; namer. |\n| [Noah Iliinsky](https://twitter.com/noahi) | Visualization & interaction designer. Practical cyclist. Author of vis books: https://www.oreilly.com/pub/au/4419 |\n| [Paul Miller](https://twitter.com/PaulMiller) | Cloud Computing/ Big Data/ Open Data Analyst & Consultant. Writer, Speaker & Moderator. Gigaom Research Analyst. |\n| [Peter Skomoroch](https://twitter.com/peteskomoroch) | Creating intelligent systems to automate tasks & improve decisions. Entrepreneur, ex-Principal Data Scientist @LinkedIn. Machine Learning, ProductRei, Networks |\n| [Prash Chan](https://twitter.com/MDMGeek) | Solution Architect @ IBM, Master Data Management, Data Quality & Data Governance Blogger. Data Science, Hadoop, Big Data & Cloud. |\n| [Quora Data Science](https://twitter.com/q_datascience)  | Quora''s data science topic |\n| [R-Bloggers](https://twitter.com/Rbloggers) | Tweet blog posts from the R blogosphere, data science conferences, and (!) open jobs for data scientists. |\n| [Rand Hindi](https://twitter.com/randhindi) |  |\n| [Randy Olson](https://twitter.com/randal_olson) | Computer scientist researching artificial intelligence. Data tinkerer. Community leader for @DataIsBeautiful. #OpenScience advocate. |\n| [Recep Erol](https://twitter.com/EROLRecep) | Data Science geek @ UALR |\n| [Ryan Orban](https://twitter.com/ryanorban) | Data scientist, genetic origamist, hardware aficionado |\n| [Sean J. Taylor](https://twitter.com/seanjtaylor) | Social Scientist. Hacker. Facebook Data Science Team. Keywords: Experiments, Causal Inference, Statistics, Machine Learning, Economics. |\n| [Silvia K. Spiva](https://twitter.com/silviakspiva) | #DataScience at Cisco |\n| [Harsh B. Gupta](https://twitter.com/harshbg) | Data Scientist at BBVA Compass |\n| [Spencer Nelson](https://twitter.com/spenczar_n) | Data nerd |\n| [Talha Oz](https://twitter.com/tozCSS) | Enjoys ABM, SNA, DM, ML, NLP, HI, Python, Java. Top percentile Kaggler/data scientist |\n| [Tasos Skarlatidis](https://twitter.com/anskarl) | Complex Event Processing, Big Data, Artificial Intelligence and Machine Learning. Passionate about programming and open-source. |\n| [Terry Timko](https://twitter.com/Terry_Timko) | InfoGov; Bigdata; Data as a Service; Data Science; Open, Social & Business Data Convergence |\n| [Tony Baer](https://twitter.com/TonyBaer) | IT analyst with Ovum covering Big Data & data management with some systems engineering thrown in. |\n| [Tony Ojeda](https://twitter.com/tonyojeda3) | Data Scientist , Author , Entrepreneur. Co-founder @DataCommunityDC. Founder @DistrictDataLab. #DataScience #BigData #DataDC |\n| [Vamshi Ambati](https://twitter.com/vambati) | Data Science @ PayPal. #NLP, #machinelearning; PhD, Carnegie Mellon alumni (Blog: https://allthingsds.wordpress.com ) |\n| [Wes McKinney](https://twitter.com/wesmckinn) | Pandas (Python Data Analysis library). |\n| [WileyEd](https://twitter.com/WileyEd) | Senior Manager - @Seagate Big Data Analytics @McKinsey Alum #BigData + #Analytics Evangelist #Hadoop, #Cloud, #Digital, & #R Enthusiast |\n| [WNYC Data News Team](https://twitter.com/datanews) | The data news crew at @WNYC. Practicing data-driven journalism, making it visual, and showing our work. |\n| [Alexey Grigorev](https://twitter.com/Al_Grigor) | Data science author |\n| [Ä°lker Arslan](https://twitter.com/ilkerarslan_35) | Data science author. Shares mostly about Julia programming |\n| [INEVITABLE](https://twitter.com/WeAreInevitable) | AI & Data Science Start-up Company based in England, UK |\n\n### Telegram Channels\n**[`^        back to top        ^`](#awesome-data-science)**\n\n- [Open Data Science](https://t.me/opendatascience) â€“ First Telegram Data Science channel. Covering all technical and popular staff about anything related to Data Science: AI, Big Data, Machine Learning, Statistics, general Math and the applications of former.\n- [Loss function porn](https://t.me/loss_function_porn) â€” Beautiful posts on DS/ML theme with video or graphic visualization.\n- [Machinelearning](https://t.me/ai_machinelearning_big_data) â€“ Daily ML news.\n\n\n### Slack Communities\n[top](#awesome-data-science)\n\n- [DataTalks.Club](https://datatalks.club)\n\n### GitHub Groups\n- [Berkeley Institute for Data Science](https://github.com/BIDS)\n\n### Data Science Competitions\n\nSome data mining competition platforms\n\n- [Kaggle](https://www.kaggle.com/)\n- [DrivenData](https://www.drivendata.org/)\n- [Analytics Vidhya](https://datahack.analyticsvidhya.com/)\n- [InnoCentive](https://www.innocentive.com/)\n- [Microprediction](https://www.microprediction.com/python-1)\n\n## Fun\n\n- [Infographic](#infographics)\n- [Datasets](#datasets)\n- [Comics](#comics)\n\n\n### Infographics\n**[`^        back to top        ^`](#awesome-data-science)**\n\n| Preview                                                                                                                                                                                                                                     | Description                                                                                                                                                                                                                                                  |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [<img src="https://i.imgur.com/0OoLaa5.png" width="150" />](https://i.imgur.com/0OoLaa5.png)                                                                                                                                                | [Key differences of a data scientist vs. data engineer](https://searchbusinessanalytics.techtarget.com/feature/Key-differences-of-a-data-scientist-vs-data-engineer)                                                                                         |\n| [<img src="https://cloud.githubusercontent.com/assets/182906/19517857/604f88d8-960c-11e6-97d6-16c9738cb824.png" width="150" />](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/DataScienceEightSteps_Full.png)                    | A visual guide to Becoming a Data Scientist in 8 Steps by [DataCamp](https://www.datacamp.com) [(img)](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/DataScienceEightSteps_Full.png)                                                              |\n| [<img src="https://i.imgur.com/W2t2Roz.png" width="150" />](https://i.imgur.com/FxsL3b8.png)                                                                                                                                                | Mindmap on required skills ([img](https://i.imgur.com/FxsL3b8.png))                                                                                                                                                                                          |\n| [<img src="https://i.imgur.com/rb9ruaa.png" width="150" />](https://nirvacana.com/thoughts/wp-content/uploads/2013/07/RoadToDataScientist1.png)                                                                                              | Swami Chandrasekaran made a [Curriculum via Metro map](http://nirvacana.com/thoughts/2013/07/08/becoming-a-data-scientist/).                                                                                                                                            |\n| [<img src="https://i.imgur.com/XBgKF2l.png" width="150" />](https://i.imgur.com/4ZBBvb0.png)                                                                                                                                                | by [@kzawadz](https://twitter.com/kzawadz) via [twitter](https://twitter.com/MktngDistillery/status/538671811991715840)                                                                                                                                      |\n| [<img src="https://i.imgur.com/l9ZGtal.jpg" width="150" />](https://i.imgur.com/xLY3XZn.jpg)                                                                                                                                                | By [Data Science Central](https://www.datasciencecentral.com/)                                                                                                                                                                                                |\n| [<img src="https://i.imgur.com/TWkB4X6.png" width="150" />](https://i.imgur.com/0TydZ4M.png)                                                                                                                                                | Data Science Wars: R vs Python                                                                                                                                                                                                                               |\n| [<img src="https://i.imgur.com/gtTlW5I.png" width="150" />](https://i.imgur.com/HnRwlce.png)                                                                                                                                                | How to select statistical or machine learning techniques                                                                                                                                                                                                     |\n| [<img src="https://scikit-learn.org/1.5/_downloads/b82bf6cd7438a351f19fac60fbc0d927/ml_map.svg" width="150" />](https://scikit-learn.org/1.5/_downloads/b82bf6cd7438a351f19fac60fbc0d927/ml_map.svg)                        ', '{"language":null,"stars":27912,"forks":6321,"watchers":27912,"open_issues":3,"topics":["analytics","awesome-list","data-mining","data-science","data-scientists","data-visualization","deep-learning","hacktoberfest","machine-learning","science"],"default_branch":"live","size_kb":1446,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:warpdotdev:brand-assets","source_url":"https://github.com/warpdotdev/brand-assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:sindresorhus:awesome","source_url":"https://github.com/sindresorhus/awesome"},{"type":"has_code","target_id":"github:microsoft:Data-Science-For-Beginners","source_url":"https://github.com/microsoft/Data-Science-For-Beginners"},{"type":"has_code","target_id":"github:rfordatascience:tidytuesday","source_url":"https://github.com/rfordatascience/tidytuesday"},{"type":"has_code","target_id":"github:jadianes:data-science-your-way","source_url":"https://github.com/jadianes/data-science-your-way"},{"type":"has_code","target_id":"github:kevinschaich:pyspark-cheatsheet","source_url":"https://github.com/kevinschaich/pyspark-cheatsheet"},{"type":"has_code","target_id":"github:handcraftsman:GeneticAlgorithmsWithPython","source_url":"https://github.com/handcraftsman/GeneticAlgorithmsWithPython"},{"type":"has_code","target_id":"github:jinglescode:python-signal-processing","source_url":"https://github.com/jinglescode/python-signal-processing"},{"type":"has_code","target_id":"github:khangich:machine-learning-interview","source_url":"https://github.com/khangich/machine-learning-interview"},{"type":"has_code","target_id":"github:AMAI-GmbH:AI-Expert-Roadmap","source_url":"https://github.com/AMAI-GmbH/AI-Expert-Roadmap"},{"type":"has_code","target_id":"github:jacopotagliabue:MLSys-NYU-2022","source_url":"https://github.com/jacopotagliabue/MLSys-NYU-2022"},{"type":"has_code","target_id":"github:Paulescu:hands-on-train-and-deploy-ml","source_url":"https://github.com/Paulescu/hands-on-train-and-deploy-ml"},{"type":"has_code","target_id":"github:DataScienceSpecialization:courses","source_url":"https://github.com/DataScienceSpecialization/courses"},{"type":"has_code","target_id":"github:ryanswanstrom:awesome-datascience-colleges","source_url":"https://github.com/ryanswanstrom/awesome-datascience-colleges"},{"type":"has_code","target_id":"github:capitalone:datacompy","source_url":"https://github.com/capitalone/datacompy"},{"type":"has_code","target_id":"github:scikit-multilearn:scikit-multilearn","source_url":"https://github.com/scikit-multilearn/scikit-multilearn"},{"type":"has_code","target_id":"github:tmadl:sklearn-expertsys","source_url":"https://github.com/tmadl/sklearn-expertsys"},{"type":"has_code","target_id":"github:jundongl:scikit-feature","source_url":"https://github.com/jundongl/scikit-feature"},{"type":"has_code","target_id":"github:EpistasisLab:scikit-rebate","source_url":"https://github.com/EpistasisLab/scikit-rebate"},{"type":"has_code","target_id":"github:larsmans:seqlearn","source_url":"https://github.com/larsmans/seqlearn"},{"type":"has_code","target_id":"github:AmazaspShumik:sklearn-bayes","source_url":"https://github.com/AmazaspShumik/sklearn-bayes"},{"type":"has_code","target_id":"github:TeamHG-Memex:sklearn-crfsuite","source_url":"https://github.com/TeamHG-Memex/sklearn-crfsuite"},{"type":"has_code","target_id":"github:rsteca:sklearn-deap","source_url":"https://github.com/rsteca/sklearn-deap"},{"type":"has_code","target_id":"github:sigopt:sigopt-sklearn","source_url":"https://github.com/sigopt/sigopt-sklearn"},{"type":"has_code","target_id":"github:edublancas:sklearn-evaluation","source_url":"https://github.com/edublancas/sklearn-evaluation"},{"type":"has_code","target_id":"github:scikit-image:scikit-image","source_url":"https://github.com/scikit-image/scikit-image"},{"type":"has_code","target_id":"github:guofei9987:scikit-opt","source_url":"https://github.com/guofei9987/scikit-opt"},{"type":"has_code","target_id":"github:maximtrp:scikit-posthocs","source_url":"https://github.com/maximtrp/scikit-posthocs"},{"type":"has_code","target_id":"github:pystruct:pystruct","source_url":"https://github.com/pystruct/pystruct"},{"type":"has_code","target_id":"github:aksnzhy:xlearn","source_url":"https://github.com/aksnzhy/xlearn"},{"type":"has_code","target_id":"github:rapidsai:cuml","source_url":"https://github.com/rapidsai/cuml"},{"type":"has_code","target_id":"github:uber:causalml","source_url":"https://github.com/uber/causalml"},{"type":"has_code","target_id":"github:mlpack:mlpack","source_url":"https://github.com/mlpack/mlpack"},{"type":"has_code","target_id":"github:rasbt:mlxtend","source_url":"https://github.com/rasbt/mlxtend"},{"type":"has_code","target_id":"github:modAL-python:modAL","source_url":"https://github.com/modAL-python/modAL"},{"type":"has_code","target_id":"github:lensacom:sparkit-learn","source_url":"https://github.com/lensacom/sparkit-learn"},{"type":"has_code","target_id":"github:danielhanchen:hyperlearn","source_url":"https://github.com/danielhanchen/hyperlearn"},{"type":"has_code","target_id":"github:davisking:dlib","source_url":"https://github.com/davisking/dlib"},{"type":"has_code","target_id":"github:csinva:imodels","source_url":"https://github.com/csinva/imodels"},{"type":"has_code","target_id":"github:christophM:rulefit","source_url":"https://github.com/christophM/rulefit"},{"type":"has_code","target_id":"github:dswah:pyGAM","source_url":"https://github.com/dswah/pyGAM"},{"type":"has_code","target_id":"github:deepchecks:deepchecks","source_url":"https://github.com/deepchecks/deepchecks"},{"type":"has_code","target_id":"github:dmlc:xgboost","source_url":"https://github.com/dmlc/xgboost"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:catboost:catboost","source_url":"https://github.com/catboost/catboost"},{"type":"has_code","target_id":"github:perpetual-ml:perpetual","source_url":"https://github.com/perpetual-ml/perpetual"},{"type":"has_code","target_id":"github:google:jax","source_url":"https://github.com/google/jax"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:vision","source_url":"https://github.com/pytorch/vision"},{"type":"has_code","target_id":"github:pytorch:text","source_url":"https://github.com/pytorch/text"},{"type":"has_code","target_id":"github:pytorch:audio","source_url":"https://github.com/pytorch/audio"},{"type":"has_code","target_id":"github:pytorch:ignite","source_url":"https://github.com/pytorch/ignite"},{"type":"has_code","target_id":"github:pytorch:tnt","source_url":"https://github.com/pytorch/tnt"},{"type":"has_code","target_id":"github:GRAAL-Research:poutyne","source_url":"https://github.com/GRAAL-Research/poutyne"},{"type":"has_code","target_id":"github:skorch-dev:skorch","source_url":"https://github.com/skorch-dev/skorch"},{"type":"has_code","target_id":"github:ctallec:pyvarinf","source_url":"https://github.com/ctallec/pyvarinf"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:cornellius-gp:gpytorch","source_url":"https://github.com/cornellius-gp/gpytorch"},{"type":"has_code","target_id":"github:pyro-ppl:pyro","source_url":"https://github.com/pyro-ppl/pyro"},{"type":"has_code","target_id":"github:catalyst-team:catalyst","source_url":"https://github.com/catalyst-team/catalyst"},{"type":"has_code","target_id":"github:manujosephv:pytorch_tabular","source_url":"https://github.com/manujosephv/pytorch_tabular"},{"type":"has_code","target_id":"github:ultralytics:yolov3","source_url":"https://github.com/ultralytics/yolov3"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:ultralytics","source_url":"https://github.com/ultralytics/ultralytics"},{"type":"has_code","target_id":"github:tensorflow:tensorflow","source_url":"https://github.com/tensorflow/tensorflow"},{"type":"has_code","target_id":"github:tensorlayer:TensorLayer","source_url":"https://github.com/tensorlayer/TensorLayer"},{"type":"has_code","target_id":"github:tflearn:tflearn","source_url":"https://github.com/tflearn/tflearn"},{"type":"has_code","target_id":"github:deepmind:sonnet","source_url":"https://github.com/deepmind/sonnet"},{"type":"has_code","target_id":"github:tensorpack:tensorpack","source_url":"https://github.com/tensorpack/tensorpack"},{"type":"has_code","target_id":"github:deepmind:trfl","source_url":"https://github.com/deepmind/trfl"},{"type":"has_code","target_id":"github:polyaxon:polyaxon","source_url":"https://github.com/polyaxon/polyaxon"},{"type":"has_code","target_id":"github:itdxer:neupy","source_url":"https://github.com/itdxer/neupy"},{"type":"has_code","target_id":"github:riga:tfdeploy","source_url":"https://github.com/riga/tfdeploy"},{"type":"has_code","target_id":"github:ROCmSoftwarePlatform:tensorflow-upstream","source_url":"https://github.com/ROCmSoftwarePlatform/tensorflow-upstream"},{"type":"has_code","target_id":"github:tensorflow:fold","source_url":"https://github.com/tensorflow/fold"},{"type":"has_code","target_id":"github:batzner:tensorlm","source_url":"https://github.com/batzner/tensorlm"},{"type":"has_code","target_id":"github:bsautermeister:tensorlight","source_url":"https://github.com/bsautermeister/tensorlight"},{"type":"has_code","target_id":"github:tensorflow:mesh","source_url":"https://github.com/tensorflow/mesh"},{"type":"has_code","target_id":"github:ludwig-ai:ludwig","source_url":"https://github.com/ludwig-ai/ludwig"},{"type":"has_code","target_id":"github:tensorflow:agents","source_url":"https://github.com/tensorflow/agents"},{"type":"has_code","target_id":"github:tensorforce:tensorforce","source_url":"https://github.com/tensorforce/tensorforce"},{"type":"has_code","target_id":"github:keras-team:keras-contrib","source_url":"https://github.com/keras-team/keras-contrib"},{"type":"has_code","target_id":"github:maxpumperla:hyperas","source_url":"https://github.com/maxpumperla/hyperas"},{"type":"has_code","target_id":"github:maxpumperla:elephas","source_url":"https://github.com/maxpumperla/elephas"},{"type":"has_code","target_id":"github:keplr-io:hera","source_url":"https://github.com/keplr-io/hera"},{"type":"has_code","target_id":"github:danielegrattarola:spektral","source_url":"https://github.com/danielegrattarola/spektral"},{"type":"has_code","target_id":"github:google:qkeras","source_url":"https://github.com/google/qkeras"},{"type":"has_code","target_id":"github:keras-rl:keras-rl","source_url":"https://github.com/keras-rl/keras-rl"},{"type":"has_code","target_id":"github:autonomio:talos","source_url":"https://github.com/autonomio/talos"},{"type":"has_code","target_id":"github:lutzroeder:netron","source_url":"https://github.com/lutzroeder/netron"},{"type":"has_code","target_id":"github:abistarun:resseract-lite","source_url":"https://github.com/abistarun/resseract-lite"},{"type":"has_code","target_id":"github:vizzuhq:vizzu-lib","source_url":"https://github.com/vizzuhq/vizzu-lib"},{"type":"has_code","target_id":"github:microsoft:tensorwatch","source_url":"https://github.com/microsoft/tensorwatch"},{"type":"has_code","target_id":"github:dslp:dslp","source_url":"https://github.com/dslp/dslp"},{"type":"has_code","target_id":"github:dslp:dslp-repo-template","source_url":"https://github.com/dslp/dslp-repo-template"},{"type":"has_code","target_id":"github:AstraZeneca:rexmex","source_url":"https://github.com/AstraZeneca/rexmex"},{"type":"has_code","target_id":"github:AstraZeneca:chemicalx","source_url":"https://github.com/AstraZeneca/chemicalx"},{"type":"has_code","target_id":"github:benedekrozemberczki:pytorch_geometric_temporal","source_url":"https://github.com/benedekrozemberczki/pytorch_geometric_temporal"},{"type":"has_code","target_id":"github:benedekrozemberczki:littleballoffur","source_url":"https://github.com/benedekrozemberczki/littleballoffur"},{"type":"has_code","target_id":"github:benedekrozemberczki:karateclub","source_url":"https://github.com/benedekrozemberczki/karateclub"},{"type":"has_code","target_id":"github:ml-tooling:ml-workspace","source_url":"https://github.com/ml-tooling/ml-workspace"},{"type":"has_code","target_id":"github:minerva-ml:steppy","source_url":"https://github.com/minerva-ml/steppy"},{"type":"has_code","target_id":"github:minerva-ml:steppy-toolkit","source_url":"https://github.com/minerva-ml/steppy-toolkit"},{"type":"has_code","target_id":"github:adrotog:PandasGUI","source_url":"https://github.com/adrotog/PandasGUI"},{"type":"has_code","target_id":"github:Hydrospheredata:mist","source_url":"https://github.com/Hydrospheredata/mist"},{"type":"has_code","target_id":"github:NervanaSystems:neon","source_url":"https://github.com/NervanaSystems/neon"},{"type":"has_code","target_id":"github:skale-me:skale","source_url":"https://github.com/skale-me/skale"},{"type":"has_code","target_id":"github:intel:idlf","source_url":"https://github.com/intel/idlf"},{"type":"has_code","target_id":"github:datawrapper:datawrapper","source_url":"https://github.com/datawrapper/datawrapper"},{"type":"has_code","target_id":"github:JuliaLang:IJulia.jl","source_url":"https://github.com/JuliaLang/IJulia.jl"},{"type":"has_code","target_id":"github:alteryx:featuretools","source_url":"https://github.com/alteryx/featuretools"},{"type":"has_code","target_id":"github:hi-primus:optimus","source_url":"https://github.com/hi-primus/optimus"},{"type":"has_code","target_id":"github:albumentations-team:albumentations","source_url":"https://github.com/albumentations-team/albumentations"},{"type":"has_code","target_id":"github:iterative:dvc","source_url":"https://github.com/iterative/dvc"},{"type":"has_code","target_id":"github:asavinov:lambdo","source_url":"https://github.com/asavinov/lambdo"},{"type":"has_code","target_id":"github:feast-dev:feast","source_url":"https://github.com/feast-dev/feast"},{"type":"has_code","target_id":"github:polyaxon:polyaxon","source_url":"https://github.com/polyaxon/polyaxon"},{"type":"has_code","target_id":"github:allegroai:clearml","source_url":"https://github.com/allegroai/clearml"},{"type":"has_code","target_id":"github:logicalclocks:hopsworks","source_url":"https://github.com/logicalclocks/hopsworks"},{"type":"has_code","target_id":"github:mindsdb:mindsdb","source_url":"https://github.com/mindsdb/mindsdb"},{"type":"has_code","target_id":"github:mindsdb:lightwood","source_url":"https://github.com/mindsdb/lightwood"},{"type":"has_code","target_id":"github:awslabs:aws-data-wrangler","source_url":"https://github.com/awslabs/aws-data-wrangler"},{"type":"has_code","target_id":"github:iterative:cml","source_url":"https://github.com/iterative/cml"},{"type":"has_code","target_id":"github:ricklamers:gridstudio","source_url":"https://github.com/ricklamers/gridstudio"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:benedekrozemberczki:shapley","source_url":"https://github.com/benedekrozemberczki/shapley"},{"type":"has_code","target_id":"github:chaos-genius:chaos_genius","source_url":"https://github.com/chaos-genius/chaos_genius"},{"type":"has_code","target_id":"github:towhee-io:towhee","source_url":"https://github.com/towhee-io/towhee"},{"type":"has_code","target_id":"github:LineaLabs:lineapy","source_url":"https://github.com/LineaLabs/lineapy"},{"type":"has_code","target_id":"github:tensorchord:envd","source_url":"https://github.com/tensorchord/envd"},{"type":"has_code","target_id":"github:iterative:mlem","source_url":"https://github.com/iterative/mlem"},{"type":"has_code","target_id":"github:cleanlab:cleanlab","source_url":"https://github.com/cleanlab/cleanlab"},{"type":"has_code","target_id":"github:awslabs:autogluon","source_url":"https://github.com/awslabs/autogluon"},{"type":"has_code","target_id":"github:comet-ml:comet-examples","source_url":"https://github.com/comet-ml/comet-examples"},{"type":"has_code","target_id":"github:comet-ml:opik","source_url":"https://github.com/comet-ml/opik"},{"type":"has_code","target_id":"github:mmore500:teeplot","source_url":"https://github.com/mmore500/teeplot"},{"type":"has_code","target_id":"github:streamlit:streamlit","source_url":"https://github.com/streamlit/streamlit"},{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:wandb:wandb","source_url":"https://github.com/wandb/wandb"},{"type":"has_code","target_id":"github:iterative:dvc","source_url":"https://github.com/iterative/dvc"},{"type":"has_code","target_id":"github:optuna:optuna","source_url":"https://github.com/optuna/optuna"},{"type":"has_code","target_id":"github:ray-project:ray","source_url":"https://github.com/ray-project/ray"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:PrefectHQ:prefect","source_url":"https://github.com/PrefectHQ/prefect"},{"type":"has_code","target_id":"github:kedro-org:kedro","source_url":"https://github.com/kedro-org/kedro"},{"type":"has_code","target_id":"github:dagworks-inc:hamilton","source_url":"https://github.com/dagworks-inc/hamilton"},{"type":"has_code","target_id":"github:slundberg:shap","source_url":"https://github.com/slundberg/shap"},{"type":"has_code","target_id":"github:interpretml:interpret","source_url":"https://github.com/interpretml/interpret"},{"type":"has_code","target_id":"github:marcotcr:lime","source_url":"https://github.com/marcotcr/lime"},{"type":"has_code","target_id":"github:flyteorg:flyte","source_url":"https://github.com/flyteorg/flyte"},{"type":"has_code","target_id":"github:dbt-labs:dbt-core","source_url":"https://github.com/dbt-labs/dbt-core"},{"type":"has_code","target_id":"github:zasper-io:zasper","source_url":"https://github.com/zasper-io/zasper"},{"type":"has_code","target_id":"github:skrub-data:skrub","source_url":"https://github.com/skrub-data/skrub"},{"type":"has_code","target_id":"github:anonym-g:Chinese-Elite","source_url":"https://github.com/anonym-g/Chinese-Elite"},{"type":"has_code","target_id":"github:desbordante:desbordante-core","source_url":"https://github.com/desbordante/desbordante-core"},{"type":"has_code","target_id":"github:runmat-org:runmat","source_url":"https://github.com/runmat-org/runmat"},{"type":"has_code","target_id":"github:jtleek:datasharing","source_url":"https://github.com/jtleek/datasharing"}]', NULL, 'MIT', 'approved', 80, '19eb9e3eee07fe5fd2a1754a3ec85d6f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-academic-awesome-datascience from https://github.com/academic.png
Image converted to WebP: data/images/github-academic-awesome-datascience.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-dmlc-xgboost', 'github--dmlc--xgboost', 'xgboost', 'dmlc', '<img src="https://xgboost.ai/images/logo/xgboost-logo-trimmed.png" width=200/> eXtreme Gradient Boosting =========== Community | Documentation | Resources | Contributors | Release Notes XGBoost is an optimized distributed gradient boosting library designed to be highly ***efficient***, ***flexible*** and ***portable***. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data scien...', '["distributed-systems","gbdt","gbm","gbrt","machine-learning","xgboost","c++"]', 'other', 27714, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/dmlc/xgboost","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<img src="https://xgboost.ai/images/logo/xgboost-logo-trimmed.png" width=200/> eXtreme Gradient Boosting\n===========\n\n[![XGBoost-CI](https://github.com/dmlc/xgboost/workflows/XGBoost%20CI/badge.svg?branch=master)](https://github.com/dmlc/xgboost/actions)\n[![Documentation Status](https://readthedocs.org/projects/xgboost/badge/?version=latest)](https://xgboost.readthedocs.org)\n[![GitHub license](https://dmlc.github.io/img/apache2.svg)](./LICENSE)\n[![CRAN Status Badge](https://www.r-pkg.org/badges/version/xgboost)](https://cran.r-project.org/web/packages/xgboost)\n[![PyPI version](https://badge.fury.io/py/xgboost.svg)](https://pypi.python.org/pypi/xgboost/)\n[![Conda version](https://img.shields.io/conda/vn/conda-forge/py-xgboost.svg)](https://anaconda.org/conda-forge/py-xgboost)\n[![Optuna](https://img.shields.io/badge/Optuna-integrated-blue)](https://optuna.org)\n[![Twitter](https://img.shields.io/badge/@XGBoostProject--_.svg?style=social&logo=twitter)](https://twitter.com/XGBoostProject)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/dmlc/xgboost/badge)](https://api.securityscorecards.dev/projects/github.com/dmlc/xgboost)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/comet-ml/comet-examples/blob/master/integrations/model-training/xgboost/notebooks/how_to_use_comet_with_xgboost_tutorial.ipynb)\n\n[Community](https://xgboost.ai/community) |\n[Documentation](https://xgboost.readthedocs.org) |\n[Resources](demo/README.md) |\n[Contributors](CONTRIBUTORS.md) |\n[Release Notes](https://xgboost.readthedocs.io/en/latest/changes/index.html)\n\nXGBoost is an optimized distributed gradient boosting library designed to be highly ***efficient***, ***flexible*** and ***portable***.\nIt implements machine learning algorithms under the [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) framework.\nXGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.\nThe same code runs on major distributed environment (Kubernetes, Hadoop, SGE, Dask, Spark, PySpark) and can solve problems beyond billions of examples.\n\nLicense\n-------\nÂ© Contributors, 2021. Licensed under an [Apache-2](https://github.com/dmlc/xgboost/blob/master/LICENSE) license.\n\nContribute to XGBoost\n---------------------\nXGBoost has been developed and used by a group of active community members. Your help is very valuable to make the package better for everyone.\nCheckout the [Community Page](https://xgboost.ai/community).\n\nReference\n---------\n- Tianqi Chen and Carlos Guestrin. [XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754). In 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016\n- XGBoost originates from research project at University of Washington.\n\nSponsors\n--------\nBecome a sponsor and get a logo here. See details at [Sponsoring the XGBoost Project](https://xgboost.ai/sponsors). The funds are used to defray the cost of continuous integration and testing infrastructure (https://xgboost-ci.net).\n\n## Open Source Collective sponsors\n[![Backers on Open Collective](https://opencollective.com/xgboost/backers/badge.svg)](#backers) [![Sponsors on Open Collective](https://opencollective.com/xgboost/sponsors/badge.svg)](#sponsors)\n\n### Sponsors\n[[Become a sponsor](https://opencollective.com/xgboost#sponsor)]\n\n<a href="https://www.nvidia.com/en-us/" target="_blank"><img src="https://raw.githubusercontent.com/xgboost-ai/xgboost-ai.github.io/master/images/sponsors/nvidia.jpg" alt="NVIDIA" width="72" height="72"></a>\n<a href="https://www.comet.com/site/?utm_source=xgboost&utm_medium=github&utm_content=readme" target="_blank"><img src="https://cdn.comet.ml/img/notebook_logo.png" height="72"></a>\n<a href="https://opencollective.com/tomislav1" target="_blank"><img src="https://images.opencollective.com/tomislav1/avatar/256.png" height="72"></a>\n<a href="https://databento.com/?utm_source=xgboost&utm_medium=sponsor&utm_content=display"><img src="https://raw.githubusercontent.com/xgboost-ai/xgboost-ai.github.io/refs/heads/master/images/sponsors/databento.png" height="72"></a>\n\n### Backers\n[[Become a backer](https://opencollective.com/xgboost#backer)]\n\n<a href="https://opencollective.com/xgboost#backers" target="_blank"><img src="https://opencollective.com/xgboost/backers.svg?width=890"></a>\n', '{"language":"C++","stars":27714,"forks":8826,"watchers":27714,"open_issues":481,"topics":["distributed-systems","gbdt","gbm","gbrt","machine-learning","xgboost"],"default_branch":"master","size_kb":35177,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:dmlc:xgboost","source_url":"https://github.com/dmlc/xgboost"},{"type":"has_code","target_id":"github:dmlc:xgboost","source_url":"https://github.com/dmlc/xgboost"},{"type":"has_code","target_id":"github:dmlc:xgboost","source_url":"https://github.com/dmlc/xgboost"}]', NULL, 'Apache-2.0', 'approved', 65, 'aa6e0d01fafe03629d15007b762425ef', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-dmlc-xgboost from https://github.com/dmlc.png
Image converted to WebP: data/images/github-dmlc-xgboost.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-fastai-fastai', 'github--fastai--fastai', 'fastai', 'fastai', '<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! --> !docs You can use fastai without any installation by using Google Colab. In fact, every page of this documentation is also available as an interactive notebook - click â€œOpen in colabâ€ at the top of any page to open it (be sure to change the Colab runtime to â€œGPUâ€ to have it run fast!) See the fast.ai documentation on Using Colab for more information. You can install fastai on your own machines with: . If you plan to develop fastai yo...', '["colab","deep-learning","fastai","gpu","machine-learning","notebooks","python","pytorch","jupyter notebook"]', 'other', 27645, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/fastai/fastai","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Welcome to fastai\n\n<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->\n\n[![CI](https://github.com/fastai/fastai/actions/workflows/main.yml/badge.svg)](https://github.com/fastai/fastai/actions/workflows/main.yml)\n[![PyPI](https://img.shields.io/pypi/v/fastai?color=blue&label=pypi%20version.png)](https://pypi.org/project/fastai/#description)\n[![Conda (channel\nonly)](https://img.shields.io/conda/vn/fastai/fastai?color=seagreen&label=conda%20version.png)](https://anaconda.org/fastai/fastai)\n![docs](https://github.com/fastai/fastai/workflows/docs/badge.svg)\n\n## Installing\n\nYou can use fastai without any installation by using [Google\nColab](https://colab.research.google.com/). In fact, every page of this\ndocumentation is also available as an interactive notebook - click â€œOpen\nin colabâ€ at the top of any page to open it (be sure to change the Colab\nruntime to â€œGPUâ€ to have it run fast!) See the fast.ai documentation on\n[Using Colab](https://course19.fast.ai/start_colab.html) for more information.\n\nYou can install fastai on your own machines with: `pip install fastai`.\n\nIf you plan to develop fastai yourself, or want to be on the cutting\nedge, you can use an editable install (if you do this, you should also\nuse an editable install of\n[fastcore](https://github.com/fastai/fastcore) to go with it.) First\ninstall PyTorch, and then:\n\n    git clone https://github.com/fastai/fastai\n    pip install -e "fastai[dev]"\n\n## Learning fastai\n\nThe best way to get started with fastai (and deep learning) is to read\n[the\nbook](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527),\nand complete [the free course](https://course.fast.ai).\n\nTo see whatâ€™s possible with fastai, take a look at the [Quick\nStart](https://docs.fast.ai/quick_start.html), which shows how to use\naround 5 lines of code to build an image classifier, an image\nsegmentation model, a text sentiment model, a recommendation system, and\na tabular model. For each of the applications, the code is much the\nsame.\n\nRead through the [Tutorials](https://docs.fast.ai/tutorial.html) to\nlearn how to train your own models on your own datasets. Use the\nnavigation sidebar to look through the fastai documentation. Every\nclass, function, and method is documented here.\n\nTo learn about the design and motivation of the library, read the [peer\nreviewed paper](https://www.mdpi.com/2078-2489/11/2/108/htm).\n\n## About fastai\n\nfastai is a deep learning library which provides practitioners with\nhigh-level components that can quickly and easily provide\nstate-of-the-art results in standard deep learning domains, and provides\nresearchers with low-level components that can be mixed and matched to\nbuild new approaches. It aims to do both things without substantial\ncompromises in ease of use, flexibility, or performance. This is\npossible thanks to a carefully layered architecture, which expresses\ncommon underlying patterns of many deep learning and data processing\ntechniques in terms of decoupled abstractions. These abstractions can be\nexpressed concisely and clearly by leveraging the dynamism of the\nunderlying Python language and the flexibility of the PyTorch library.\nfastai includes:\n\n- A new type dispatch system for Python along with a semantic type\n  hierarchy for tensors\n- A GPU-optimized computer vision library which can be extended in pure\n  Python\n- An optimizer which refactors out the common functionality of modern\n  optimizers into two basic pieces, allowing optimization algorithms to\n  be implemented in 4â€“5 lines of code\n- A novel 2-way callback system that can access any part of the data,\n  model, or optimizer and change it at any point during training\n- A new data block API\n- And much moreâ€¦\n\nfastai is organized around two main design goals: to be approachable and\nrapidly productive, while also being deeply hackable and configurable.\nIt is built on top of a hierarchy of lower-level APIs which provide\ncomposable building blocks. This way, a user wanting to rewrite part of\nthe high-level API or add particular behavior to suit their needs does\nnot have to learn how to use the lowest level.\n\n<img alt="Layered API" src="images/layered.png" width="345">\n\n## Migrating from other libraries\n\nItâ€™s very easy to migrate from plain PyTorch, Ignite, or any other\nPyTorch-based library, or even to use fastai in conjunction with other\nlibraries. Generally, youâ€™ll be able to use all your existing data\nprocessing code, but will be able to reduce the amount of code you\nrequire for training, and more easily take advantage of modern best\npractices. Here are migration guides from some popular libraries to help\nyou on your way:\n\n- [Plain PyTorch](https://docs.fast.ai/examples/migrating_pytorch.html)\n- [Ignite](https://docs.fast.ai/examples/migrating_ignite.html)\n- [Lightning](https://docs.fast.ai/examples/migrating_lightning.html)\n- [Catalyst](https://docs.fast.ai/examples/migrating_catalyst.html)\n\n## Windows Support\n\nDue to python multiprocessing issues on Jupyter and Windows,\n`num_workers` of `Dataloader` is reset to 0 automatically to avoid\nJupyter hanging. This makes tasks such as computer vision in Jupyter on\nWindows many times slower than on Linux. This limitation doesnâ€™t exist\nif you use fastai from a script.\n\nSee [this\nexample](https://github.com/fastai/fastai/blob/master/nbs/examples/dataloader_spawn.py)\nto fully leverage the fastai API on Windows.\n\nWe recommend using Windows Subsystem for Linux (WSL) instead â€“ if you do\nthat, you can use the regular Linux installation approach, and you wonâ€™t\nhave any issues with `num_workers`.\n\n## Tests\n\nTo run the tests in parallel, launch:\n\n`nbdev_test`\n\nFor all the tests to pass, youâ€™ll need to install the dependencies\nspecified as part of dev_requirements in settings.ini\n\n`pip install -e .[dev]`\n\nTests are written using `nbdev`, for example see the documentation for\n`test_eq`.\n\n## Contributing\n\nAfter you clone this repository, make sure you have run\n`nbdev_install_hooks` in your terminal. This install Jupyter and git\nhooks to automatically clean, trust, and fix merge conflicts in\nnotebooks.\n\nAfter making changes in the repo, you should run `nbdev_prepare` and\nmake additional and necessary changes in order to pass all the tests.\n\n## Docker Containers\n\nFor those interested in official docker containers for this project,\nthey can be found\n[here](https://github.com/fastai/docker-containers#fastai).\n', '{"language":"Jupyter Notebook","stars":27645,"forks":7664,"watchers":27645,"open_issues":267,"topics":["colab","deep-learning","fastai","gpu","machine-learning","notebooks","python","pytorch"],"default_branch":"main","size_kb":825037,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:fastai:fastai","source_url":"https://github.com/fastai/fastai"},{"type":"has_code","target_id":"github:fastai:fastai","source_url":"https://github.com/fastai/fastai"},{"type":"has_code","target_id":"github:fastai:fastai","source_url":"https://github.com/fastai/fastai"},{"type":"has_code","target_id":"github:fastai:fastcore","source_url":"https://github.com/fastai/fastcore"},{"type":"has_code","target_id":"github:fastai:fastai","source_url":"https://github.com/fastai/fastai"},{"type":"has_code","target_id":"github:fastai:fastai","source_url":"https://github.com/fastai/fastai"},{"type":"has_code","target_id":"github:fastai:docker-containers","source_url":"https://github.com/fastai/docker-containers#fastai"}]', NULL, 'Apache-2.0', 'approved', 65, '932f425ca29f00225fdf219ecfd25988', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-fastai-fastai from https://github.com/fastai.png
Image converted to WebP: data/images/github-fastai-fastai.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ItzCrazyKns-Perplexica', 'github--itzcrazykns--perplexica', 'Perplexica', 'ItzCrazyKns', 'Perplexica is a **privacy-focused AI answering engine** that runs entirely on your own hardware. It combines knowledge from the vast internet with support for **local LLMs** (Ollama) and cloud providers (OpenAI, Claude, Groq), delivering accurate answers with **cited sources** while keeping your searches completely private. !preview Want to know more about its architecture and how it works? You can read it here. ğŸ¤– **Support for all major AI providers** - Use local LLMs through Ollama or conn...', '["ai-search-engine","artificial-intelligence","machine-learning","open-source-ai-search-engine","open-source-perplexity-ai","perplexica","perplexity-ai","search-engine","searxng","searxng-copilot","typescript"]', 'other', 27578, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ItzCrazyKns/Perplexica","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Perplexica ğŸ”\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/ItzCrazyKns/Perplexica?style=social)](https://github.com/ItzCrazyKns/Perplexica/stargazers)\n[![GitHub forks](https://img.shields.io/github/forks/ItzCrazyKns/Perplexica?style=social)](https://github.com/ItzCrazyKns/Perplexica/network/members)\n[![GitHub watchers](https://img.shields.io/github/watchers/ItzCrazyKns/Perplexica?style=social)](https://github.com/ItzCrazyKns/Perplexica/watchers)\n[![Docker Pulls](https://img.shields.io/docker/pulls/itzcrazykns1337/perplexica?color=blue)](https://hub.docker.com/r/itzcrazykns1337/perplexica)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://github.com/ItzCrazyKns/Perplexica/blob/master/LICENSE)\n[![GitHub last commit](https://img.shields.io/github/last-commit/ItzCrazyKns/Perplexica?color=green)](https://github.com/ItzCrazyKns/Perplexica/commits/master)\n[![Discord](https://dcbadge.limes.pink/api/server/26aArMy8tT?style=flat)](https://discord.gg/26aArMy8tT)\n\nPerplexica is a **privacy-focused AI answering engine** that runs entirely on your own hardware. It combines knowledge from the vast internet with support for **local LLMs** (Ollama) and cloud providers (OpenAI, Claude, Groq), delivering accurate answers with **cited sources** while keeping your searches completely private.\n\n![preview](.assets/perplexica-screenshot.png)\n\nWant to know more about its architecture and how it works? You can read it [here](https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/architecture/README.md).\n\n## âœ¨ Features\n\nğŸ¤– **Support for all major AI providers** - Use local LLMs through Ollama or connect to OpenAI, Anthropic Claude, Google Gemini, Groq, and more. Mix and match models based on your needs.\n\nâš¡ **Smart search modes** - Choose Balanced Mode for everyday searches, Fast Mode when you need quick answers, or wait for Quality Mode (coming soon) for deep research.\n\nğŸ¯ **Six specialized focus modes** - Get better results with modes designed for specific tasks: Academic papers, YouTube videos, Reddit discussions, Wolfram Alpha calculations, writing assistance, or general web search.\n\nğŸ” **Web search powered by SearxNG** - Access multiple search engines while keeping your identity private. Support for Tavily and Exa coming soon for even better results.\n\nğŸ“· **Image and video search** - Find visual content alongside text results. Search isn''t limited to just articles anymore.\n\nğŸ“„ **File uploads** - Upload documents and ask questions about them. PDFs, text files, images - Perplexica understands them all.\n\nğŸŒ **Search specific domains** - Limit your search to specific websites when you know where to look. Perfect for technical documentation or research papers.\n\nğŸ’¡ **Smart suggestions** - Get intelligent search suggestions as you type, helping you formulate better queries.\n\nğŸ“š **Discover** - Browse interesting articles and trending content throughout the day. Stay informed without even searching.\n\nğŸ•’ **Search history** - Every search is saved locally so you can revisit your discoveries anytime. Your research is never lost.\n\nâœ¨ **More coming soon** - We''re actively developing new features based on community feedback. Join our Discord to help shape Perplexica''s future!\n\n## Sponsors\n\nPerplexica''s development is powered by the generous support of our sponsors. Their contributions help keep this project free, open-source, and accessible to everyone.\n\n<div align="center">\n  \n  \n<a href="https://www.warp.dev/perplexica">\n  <img alt="Warp Terminal" src=".assets/sponsers/warp.png" width="100%">\n</a>\n\n### **âœ¨ [Try Warp - The AI-Powered Terminal â†’](https://www.warp.dev/perplexica)**\n\nWarp is revolutionizing development workflows with AI-powered features, modern UX, and blazing-fast performance. Used by developers at top companies worldwide.\n\n</div>\n\n---\n\nWe''d also like to thank the following partners for their generous support:\n\n<table>\n  <tr>\n    <td width="100" align="center">\n      <a href="https://dashboard.exa.ai" target="_blank">\n        <img src=".assets/sponsers/exa.png" alt="Exa" width="80" height="80" style="border-radius: .75rem;" />\n      </a>\n    </td>\n    <td>\n      <a href="https://dashboard.exa.ai">Exa</a> â€¢ The Perfect Web Search API for LLMs - web search, crawling, deep research, and answer APIs\n    </td>\n  </tr>\n</table>\n\n## Installation\n\nThere are mainly 2 ways of installing Perplexica - With Docker, Without Docker. Using Docker is highly recommended.\n\n### Getting Started with Docker (Recommended)\n\nPerplexica can be easily run using Docker. Simply run the following command:\n\n```bash\ndocker run -d -p 3000:3000 -v perplexica-data:/home/perplexica/data -v perplexica-uploads:/home/perplexica/uploads --name perplexica itzcrazykns1337/perplexica:latest\n```\n\nThis will pull and start the Perplexica container with the bundled SearxNG search engine. Once running, open your browser and navigate to http://localhost:3000. You can then configure your settings (API keys, models, etc.) directly in the setup screen.\n\n**Note**: The image includes both Perplexica and SearxNG, so no additional setup is required. The `-v` flags create persistent volumes for your data and uploaded files.\n\n#### Using Perplexica with Your Own SearxNG Instance\n\nIf you already have SearxNG running, you can use the slim version of Perplexica:\n\n```bash\ndocker run -d -p 3000:3000 -e SEARXNG_API_URL=http://your-searxng-url:8080 -v perplexica-data:/home/perplexica/data -v perplexica-uploads:/home/perplexica/uploads --name perplexica itzcrazykns1337/perplexica:slim-latest\n```\n\n**Important**: Make sure your SearxNG instance has:\n\n- JSON format enabled in the settings\n- Wolfram Alpha search engine enabled\n\nReplace `http://your-searxng-url:8080` with your actual SearxNG URL. Then configure your AI provider settings in the setup screen at http://localhost:3000.\n\n#### Advanced Setup (Building from Source)\n\nIf you prefer to build from source or need more control:\n\n1. Ensure Docker is installed and running on your system.\n2. Clone the Perplexica repository:\n\n   ```bash\n   git clone https://github.com/ItzCrazyKns/Perplexica.git\n   ```\n\n3. After cloning, navigate to the directory containing the project files.\n\n4. Build and run using Docker:\n\n   ```bash\n   docker build -t perplexica .\n   docker run -d -p 3000:3000 -v perplexica-data:/home/perplexica/data -v perplexica-uploads:/home/perplexica/uploads --name perplexica perplexica\n   ```\n\n5. Access Perplexica at http://localhost:3000 and configure your settings in the setup screen.\n\n**Note**: After the containers are built, you can start Perplexica directly from Docker without having to open a terminal.\n\n### Non-Docker Installation\n\n1. Install SearXNG and allow `JSON` format in the SearXNG settings. Make sure Wolfram Alpha search engine is also enabled.\n2. Clone the repository:\n\n   ```bash\n   git clone https://github.com/ItzCrazyKns/Perplexica.git\n   cd Perplexica\n   ```\n\n3. Install dependencies:\n\n   ```bash\n   npm i\n   ```\n\n4. Build the application:\n\n   ```bash\n   npm run build\n   ```\n\n5. Start the application:\n\n   ```bash\n   npm run start\n   ```\n\n6. Open your browser and navigate to http://localhost:3000 to complete the setup and configure your settings (API keys, models, SearxNG URL, etc.) in the setup screen.\n\n**Note**: Using Docker is recommended as it simplifies the setup process, especially for managing environment variables and dependencies.\n\nSee the [installation documentation](https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/installation) for more information like updating, etc.\n\n### Troubleshooting\n\n#### Local OpenAI-API-Compliant Servers\n\nIf Perplexica tells you that you haven''t configured any chat model providers, ensure that:\n\n1. Your server is running on `0.0.0.0` (not `127.0.0.1`) and on the same port you put in the API URL.\n2. You have specified the correct model name loaded by your local LLM server.\n3. You have specified the correct API key, or if one is not defined, you have put _something_ in the API key field and not left it empty.\n\n#### Ollama Connection Errors\n\nIf you''re encountering an Ollama connection error, it is likely due to the backend being unable to connect to Ollama''s API. To fix this issue you can:\n\n1. **Check your Ollama API URL:** Ensure that the API URL is correctly set in the settings menu.\n2. **Update API URL Based on OS:**\n\n   - **Windows:** Use `http://host.docker.internal:11434`\n   - **Mac:** Use `http://host.docker.internal:11434`\n   - **Linux:** Use `http://<private_ip_of_host>:11434`\n\n   Adjust the port number if you''re using a different one.\n\n3. **Linux Users - Expose Ollama to Network:**\n\n   - Inside `/etc/systemd/system/ollama.service`, you need to add `Environment="OLLAMA_HOST=0.0.0.0:11434"`. (Change the port number if you are using a different one.) Then reload the systemd manager configuration with `systemctl daemon-reload`, and restart Ollama by `systemctl restart ollama`. For more information see [Ollama docs](https://github.com/ollama/ollama/blob/main/docs/faq.md#setting-environment-variables-on-linux)\n\n   - Ensure that the port (default is 11434) is not blocked by your firewall.\n\n#### Lemonade Connection Errors\n\nIf you''re encountering a Lemonade connection error, it is likely due to the backend being unable to connect to Lemonade''s API. To fix this issue you can:\n\n1. **Check your Lemonade API URL:** Ensure that the API URL is correctly set in the settings menu.\n2. **Update API URL Based on OS:**\n\n   - **Windows:** Use `http://host.docker.internal:8000`\n   - **Mac:** Use `http://host.docker.internal:8000`\n   - **Linux:** Use `http://<private_ip_of_host>:8000`\n\n   Adjust the port number if you''re using a different one.\n\n3. **Ensure Lemonade Server is Running:**\n\n   - Make sure your Lemonade server is running and accessible on the configured port (default is 8000).\n   - Verify that Lemonade is configured to accept connections from all interfaces (`0.0.0.0`), not just localhost (`127.0.0.1`).\n   - Ensure that the port (default is 8000) is not blocked by your firewall.\n\n## Using as a Search Engine\n\nIf you wish to use Perplexica as an alternative to traditional search engines like Google or Bing, or if you want to add a shortcut for quick access from your browser''s search bar, follow these steps:\n\n1. Open your browser''s settings.\n2. Navigate to the ''Search Engines'' section.\n3. Add a new site search with the following URL: `http://localhost:3000/?q=%s`. Replace `localhost` with your IP address or domain name, and `3000` with the port number if Perplexica is not hosted locally.\n4. Click the add button. Now, you can use Perplexica directly from your browser''s search bar.\n\n## Using Perplexica''s API\n\nPerplexica also provides an API for developers looking to integrate its powerful search engine into their own applications. You can run searches, use multiple models and get answers to your queries.\n\nFor more details, check out the full documentation [here](https://github.com/ItzCrazyKns/Perplexica/tree/master/docs/API/SEARCH.md).\n\n## Expose Perplexica to network\n\nPerplexica runs on Next.js and handles all API requests. It works right away on the same network and stays accessible even with port forwarding.\n\n## One-Click Deployment\n\n[![Deploy to Sealos](https://raw.githubusercontent.com/labring-actions/templates/main/Deploy-on-Sealos.svg)](https://usw.sealos.io/?openapp=system-template%3FtemplateName%3Dperplexica)\n[![Deploy to RepoCloud](https://d16t0pc4846x52.cloudfront.net/deploylobe.svg)](https://repocloud.io/details/?app_id=267)\n[![Run on ClawCloud](https://raw.githubusercontent.com/ClawCloud/Run-Template/refs/heads/main/Run-on-ClawCloud.svg)](https://template.run.claw.cloud/?referralCode=U11MRQ8U9RM4&openapp=system-fastdeploy%3FtemplateName%3Dperplexica)\n[![Deploy on Hostinger](https://assets.hostinger.com/vps/deploy.svg)](https://www.hostinger.com/vps/docker-hosting?compose_url=https://raw.githubusercontent.com/ItzCrazyKns/Perplexica/refs/heads/master/docker-compose.yaml)\n\n## Upcoming Features\n\n- [x] Add settings page\n- [x] Adding support for local LLMs\n- [x] History Saving features\n- [x] Introducing various Focus Modes\n- [x] Adding API support\n- [x] Adding Discover\n- [ ] Finalizing Copilot Mode\n\n## Support Us\n\nIf you find Perplexica useful, consider giving us a star on GitHub. This helps more people discover Perplexica and supports the development of new features. Your support is greatly appreciated.\n\n### Donations\n\nWe also accept donations to help sustain our project. If you would like to contribute, you can use the following options to donate. Thank you for your support!\n\n| Ethereum                                              |\n| ----------------------------------------------------- |\n| Address: `0xB025a84b2F269570Eb8D4b05DEdaA41D8525B6DD` |\n\n## Contribution\n\nPerplexica is built on the idea that AI and large language models should be easy for everyone to use. If you find bugs or have ideas, please share them in via GitHub Issues. For more information on contributing to Perplexica you can read the [CONTRIBUTING.md](CONTRIBUTING.md) file to learn more about Perplexica and how you can contribute to it.\n\n## Help and Support\n\nIf you have any questions or feedback, please feel free to reach out to us. You can create an issue on GitHub or join our Discord server. There, you can connect with other users, share your experiences and reviews, and receive more personalized help. [Click here](https://discord.gg/EFwsmQDgAu) to join the Discord server. To discuss matters outside of regular support, feel free to contact me on Discord at `itzcrazykns`.\n\nThank you for exploring Perplexica, the AI-powered search engine designed to enhance your search experience. We are constantly working to improve Perplexica and expand its capabilities. We value your feedback and contributions which help us make Perplexica even better. Don''t forget to check back for updates and new features!\n', '{"language":"TypeScript","stars":27578,"forks":2872,"watchers":27578,"open_issues":205,"topics":["ai-search-engine","artificial-intelligence","machine-learning","open-source-ai-search-engine","open-source-perplexity-ai","perplexica","perplexity-ai","search-engine","searxng","searxng-copilot"],"default_branch":"master","size_kb":111162,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:ItzCrazyKns:Perplexica","source_url":"https://github.com/ItzCrazyKns/Perplexica"},{"type":"has_code","target_id":"github:ItzCrazyKns:Perplexica","source_url":"https://github.com/ItzCrazyKns/Perplexica"},{"type":"has_code","target_id":"github:ItzCrazyKns:Perplexica","source_url":"https://github.com/ItzCrazyKns/Perplexica"},{"type":"has_code","target_id":"github:ItzCrazyKns:Perplexica","source_url":"https://github.com/ItzCrazyKns/Perplexica"},{"type":"has_code","target_id":"github:ItzCrazyKns:Perplexica","source_url":"https://github.com/ItzCrazyKns/Perplexica"},{"type":"has_code","target_id":"github:ItzCrazyKns:Perplexica","source_url":"https://github.com/ItzCrazyKns/Perplexica"},{"type":"has_code","target_id":"github:ItzCrazyKns:Perplexica.git","source_url":"https://github.com/ItzCrazyKns/Perplexica.git"},{"type":"has_code","target_id":"github:ItzCrazyKns:Perplexica.git","source_url":"https://github.com/ItzCrazyKns/Perplexica.git"},{"type":"has_code","target_id":"github:ItzCrazyKns:Perplexica","source_url":"https://github.com/ItzCrazyKns/Perplexica"},{"type":"has_code","target_id":"github:ollama:ollama","source_url":"https://github.com/ollama/ollama"},{"type":"has_code","target_id":"github:ItzCrazyKns:Perplexica","source_url":"https://github.com/ItzCrazyKns/Perplexica"}]', NULL, 'MIT', 'approved', 80, 'c6d1e1d3ea94fdc4ce6c4cd5991a290b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ItzCrazyKns-Perplexica from https://github.com/ItzCrazyKns.png
Image converted to WebP: data/images/github-ItzCrazyKns-Perplexica.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-d2l-ai-d2l-en', 'github--d2l-ai--d2l-en', 'd2l-en', 'd2l-ai', '<div align="left"> <img src="https://raw.githubusercontent.com/d2l-ai/d2l-en/master/static/logo-with-text.png" width="350"> </div> Book website | STAT 157 Course at UC Berkeley <h5 align="center"><i>The best way to understand deep learning is learning by doing.</i></h5> <p align="center"> <img width="200" src="static/frontpage/_images/eq.jpg"> <img width="200" src="static/frontpage/_images/figure.jpg"> <img width="200" src="static/frontpage/_images/code.jpg"> <img width="200" src="static/fron...', '["book","computer-vision","data-science","deep-learning","gaussian-processes","hyperparameter-optimization","jax","kaggle","keras","machine-learning","mxnet","natural-language-processing","notebook","python","pytorch","recommender-system","reinforcement-learning","tensorflow","python"]', 'other', 27535, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/d2l-ai/d2l-en","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="left">\n  <img src="https://raw.githubusercontent.com/d2l-ai/d2l-en/master/static/logo-with-text.png" width="350">\n</div>\n\n# D2L.ai: Interactive Deep Learning Book with Multi-Framework Code, Math, and Discussions\n\n[![Continuous Integration](https://github.com/d2l-ai/d2l-en/actions/workflows/ci.yml/badge.svg)](https://github.com/d2l-ai/d2l-en/actions/workflows/ci.yml)\n\n[Book website](https://d2l.ai/) | [STAT 157 Course at UC Berkeley](http://courses.d2l.ai/berkeley-stat-157/index.html)\n\n<h5 align="center"><i>The best way to understand deep learning is learning by doing.</i></h5>\n\n<p align="center">\n  <img width="200"  src="static/frontpage/_images/eq.jpg">\n  <img width="200"  src="static/frontpage/_images/figure.jpg">\n  <img width="200"  src="static/frontpage/_images/code.jpg">\n  <img width="200"  src="static/frontpage/_images/notebook.gif">\n</p>\n\nThis open-source book represents our attempt to make deep learning approachable, teaching you the concepts, the context, and the code. The entire book is drafted in Jupyter notebooks, seamlessly integrating exposition figures, math, and interactive examples with self-contained code.\n\nOur goal is to offer a resource that could\n1. be freely available for everyone;\n1. offer sufficient technical depth to provide a starting point on the path to actually becoming an applied machine learning scientist;\n1. include runnable code, showing readers how to solve problems in practice;\n1. allow for rapid updates, both by us and also by the community at large;\n1. be complemented by a forum for interactive discussion of technical details and to answer questions.\n\n## Universities Using D2L\n<p align="center">\n  <img width="600"  src="static/frontpage/_images/map.png">\n</p>\n\n\n\nIf you find this book useful, please star (â˜…) this repository or cite this book using the following bibtex entry:\n\n```\n@book{zhang2023dive,\n    title={Dive into Deep Learning},\n    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},\n    publisher={Cambridge University Press},\n    note={\url{https://D2L.ai}},\n    year={2023}\n}\n```\n\n\n## Endorsements\n\n> <p>"In less than a decade, the AI revolution has swept from research labs to broad industries to every corner of our daily life.  Dive into Deep Learning is an excellent text on deep learning and deserves attention from anyone who wants to learn why deep learning has ignited the AI revolution: the most powerful technology force of our time."</p>\n> <b>&mdash; Jensen Huang, Founder and CEO, NVIDIA</b>\n\n> <p>"This is a timely, fascinating book, providing with not only a comprehensive overview of deep learning principles but also detailed algorithms with hands-on programming code, and moreover, a state-of-the-art introduction to deep learning in computer vision and natural language processing. Dive into this book if you want to dive into deep learning!"</p>\n> <b>&mdash; Jiawei Han, Michael Aiken Chair Professor, University of Illinois at Urbana-Champaign</b>\n\n> <p>"This is a highly welcome addition to the machine learning literature, with a focus on hands-on experience implemented via the integration of Jupyter notebooks. Students of deep learning should find this invaluable to become proficient in this field."</p>\n> <b>&mdash; Bernhard SchÃ¶lkopf, Director, Max Planck Institute for Intelligent Systems</b>\n\n> <p>"Dive into Deep Learning strikes an excellent balance between hands-on learning and in-depth explanation. I''ve used it in my deep learning course and recommend it to anyone who wants to develop a thorough and practical understanding of deep learning."</p>\n> <b>&mdash; Colin Raffel, Assistant Professor, University of North Carolina, Chapel Hill</b>\n\n## Contributing ([Learn How](https://d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html))\n\nThis open source book has benefited from pedagogical suggestions, typo corrections, and other improvements from community contributors. Your help is valuable for making the book better for everyone.\n\n**Dear [D2L contributors](https://github.com/d2l-ai/d2l-en/graphs/contributors), please email your GitHub ID and name to d2lbook.en AT gmail DOT com so your name will appear on the [acknowledgments](https://d2l.ai/chapter_preface/index.html#acknowledgments). Thanks.**\n\n\n## License Summary\n\nThis open source book is made available under the Creative Commons Attribution-ShareAlike 4.0 International License. See [LICENSE](LICENSE) file.\n\nThe sample and reference code within this open source book is made available under a modified MIT license. See the [LICENSE-SAMPLECODE](LICENSE-SAMPLECODE) file.\n\n[Chinese version](https://github.com/d2l-ai/d2l-zh) | [Discuss and report issues](https://discuss.d2l.ai/) | [Code of conduct](CODE_OF_CONDUCT.md)\n', '{"language":"Python","stars":27535,"forks":4861,"watchers":27535,"open_issues":182,"topics":["book","computer-vision","data-science","deep-learning","gaussian-processes","hyperparameter-optimization","jax","kaggle","keras","machine-learning","mxnet","natural-language-processing","notebook","python","pytorch","recommender-system","reinforcement-learning","tensorflow"],"default_branch":"master","size_kb":317904,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:d2l-ai:d2l-en","source_url":"https://github.com/d2l-ai/d2l-en"},{"type":"has_code","target_id":"github:d2l-ai:d2l-en","source_url":"https://github.com/d2l-ai/d2l-en"},{"type":"has_code","target_id":"github:d2l-ai:d2l-en","source_url":"https://github.com/d2l-ai/d2l-en"},{"type":"has_code","target_id":"github:d2l-ai:d2l-zh","source_url":"https://github.com/d2l-ai/d2l-zh"}]', NULL, 'NOASSERTION', 'approved', 65, '0867be7854ee4d58e9496364079063f9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-d2l-ai-d2l-en from https://github.com/d2l-ai.png
Image converted to WebP: data/images/github-d2l-ai-d2l-en.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-qdrant-qdrant', 'github--qdrant--qdrant', 'qdrant', 'qdrant', '<p align="center"> <picture> <source media="(prefers-color-scheme: dark)" srcset="https://github.com/qdrant/qdrant/raw/master/docs/logo-dark.svg"> <source media="(prefers-color-scheme: light)" srcset="https://github.com/qdrant/qdrant/raw/master/docs/logo-light.svg"> <img height="100" alt="Qdrant" src="https://github.com/qdrant/qdrant/raw/master/docs/logo.svg"> </picture> </p> <p align="center"> <b>Vector Search Engine for the next generation of AI applications</b> </p> <p align=center> <a hre...', '["ai-search","ai-search-engine","embeddings-similarity","hnsw","image-search","knn-algorithm","machine-learning","mlops","nearest-neighbor-search","neural-network","neural-search","recommender-system","search","search-engine","search-engines","similarity-search","vector-database","vector-search","vector-search-engine","rust"]', 'other', 27526, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/qdrant/qdrant","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <picture>\n      <source media="(prefers-color-scheme: dark)" srcset="https://github.com/qdrant/qdrant/raw/master/docs/logo-dark.svg">\n      <source media="(prefers-color-scheme: light)" srcset="https://github.com/qdrant/qdrant/raw/master/docs/logo-light.svg">\n      <img height="100" alt="Qdrant" src="https://github.com/qdrant/qdrant/raw/master/docs/logo.svg">\n  </picture>\n</p>\n\n<p align="center">\n    <b>Vector Search Engine for the next generation of AI applications</b>\n</p>\n\n<p align=center>\n    <a href="https://github.com/qdrant/qdrant/actions/workflows/rust.yml"><img src="https://img.shields.io/github/actions/workflow/status/qdrant/qdrant/rust.yml?style=flat-square" alt="Tests status"></a>\n    <a href="https://api.qdrant.tech/"><img src="https://img.shields.io/badge/Docs-OpenAPI%203.0-success?style=flat-square" alt="OpenAPI Docs"></a>\n    <a href="https://github.com/qdrant/qdrant/blob/master/LICENSE"><img src="https://img.shields.io/github/license/qdrant/qdrant?style=flat-square" alt="Apache 2.0 License"></a>\n    <a href="https://qdrant.to/discord"><img src="https://img.shields.io/discord/907569970500743200?logo=Discord&style=flat-square&color=7289da" alt="Discord"></a>\n    <a href="https://qdrant.to/roadmap"><img src="https://img.shields.io/badge/Roadmap-2025-bc1439.svg?style=flat-square" alt="Roadmap 2025"></a>\n    <a href="https://cloud.qdrant.io/"><img src="https://img.shields.io/badge/Qdrant-Cloud-24386C.svg?logo=cloud&style=flat-square" alt="Qdrant Cloud"></a>\n</p>\n\n**Qdrant** (read: _quadrant_) is a vector similarity search engine and vector database.\nIt provides a production-ready service with a convenient API to store, search, and manage pointsâ€”vectors with an additional payload\nQdrant is tailored to extended filtering support. It makes it useful for all sorts of neural-network or semantic-based matching, faceted search, and other applications.\n\nQdrant is written in Rust ğŸ¦€, which makes it fast and reliable even under high load. See [benchmarks](https://qdrant.tech/benchmarks/).\n\nWith Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\n\nQdrant is also available as a fully managed **[Qdrant Cloud](https://cloud.qdrant.io/)** â›… including a **free tier**.\n\n<p align="center">\n<strong><a href="docs/QUICK_START.md">Quick Start</a> â€¢ <a href="#clients">Client Libraries</a> â€¢ <a href="#demo-projects">Demo Projects</a> â€¢ <a href="#integrations">Integrations</a> â€¢ <a href="#contacts">Contact</a>\n\n</strong>\n</p>\n\n## Getting Started\n\n### Python\n\n```\npip install qdrant-client\n```\n\nThe python client offers a convenient way to start with Qdrant locally:\n\n```python\nfrom qdrant_client import QdrantClient\nqdrant = QdrantClient(":memory:") # Create in-memory Qdrant instance, for testing, CI/CD\n# OR\nclient = QdrantClient(path="path/to/db")  # Persists changes to disk, fast prototyping\n```\n\n### Client-Server\n\nTo experience the full power of Qdrant locally, run the container with this command:\n\n```bash\ndocker run -p 6333:6333 qdrant/qdrant\n```\n\nNow you can connect to this with any client, including Python:\n\n```python\nqdrant = QdrantClient("http://localhost:6333") # Connect to existing Qdrant instance\n```\n\nBefore deploying Qdrant to production, be sure to read our [installation](https://qdrant.tech/documentation/guides/installation/) and [security](https://qdrant.tech/documentation/guides/security/) guides.\n\n### Clients\n\nQdrant offers the following client libraries to help you integrate it into your application stack with ease:\n\n- Official:\n  - [Go client](https://github.com/qdrant/go-client)\n  - [Rust client](https://github.com/qdrant/rust-client)\n  - [JavaScript/TypeScript client](https://github.com/qdrant/qdrant-js)\n  - [Python client](https://github.com/qdrant/qdrant-client)\n  - [.NET/C# client](https://github.com/qdrant/qdrant-dotnet)\n  - [Java client](https://github.com/qdrant/java-client)\n- Community:\n  - [Elixir](https://hexdocs.pm/qdrant/readme.html)\n  - [PHP](https://github.com/hkulekci/qdrant-php)\n  - [Ruby](https://github.com/andreibondarev/qdrant-ruby)\n  - [Java](https://github.com/metaloom/qdrant-java-client)\n\n### Where do I go from here?\n\n- [Quick Start Guide](docs/QUICK_START.md)\n- End to End [Colab Notebook](https://colab.research.google.com/drive/1Bz8RSVHwnNDaNtDwotfPj0w7AYzsdXZ-?usp=sharing) demo with SentenceBERT and Qdrant\n- Detailed [Documentation](https://qdrant.tech/documentation/) are great starting points\n- [Step-by-Step Tutorial](https://qdrant.to/qdrant-tutorial) to create your first neural network project with Qdrant\n\n## Demo Projects<a href="https://replit.com/@qdrant"><img align="right" src="https://replit.com/badge/github/qdrant/qdrant" alt="Run on Repl.it"></a>\n\n### Discover Semantic Text Search ğŸ”\n\nUnlock the power of semantic embeddings with Qdrant, transcending keyword-based search to find meaningful connections in short texts. Deploy a neural search in minutes using a pre-trained neural network, and experience the future of text search. [Try it online!](https://qdrant.to/semantic-search-demo)\n\n### Explore Similar Image Search - Food Discovery ğŸ•\n\nThere''s more to discovery than text search, especially when it comes to food. People often choose meals based on appearance rather than descriptions and ingredients. Let Qdrant help your users find their next delicious meal using visual search, even if they don''t know the dish''s name. [Check it out!](https://qdrant.to/food-discovery)\n\n### Master Extreme Classification - E-commerce Product Categorization ğŸ“º\n\nEnter the cutting-edge realm of extreme classification, an emerging machine learning field tackling multi-class and multi-label problems with millions of labels. Harness the potential of similarity learning models, and see how a pre-trained transformer model and Qdrant can revolutionize e-commerce product categorization. [Play with it online!](https://qdrant.to/extreme-classification-demo)\n\n<details>\n<summary> More solutions </summary>\n\n<table>\n    <tr>\n        <td width="30%">\n            <img src="https://qdrant.tech/content/images/text_search.png">\n        </td>\n        <td width="30%">\n            <img src="https://qdrant.tech/content/images/image_search.png">\n        </td>\n        <td width="30%">\n            <img src="https://qdrant.tech/content/images/recommendations.png">\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Semantic Text Search\n        </td>\n        <td>\n            Similar Image Search\n        </td>\n        <td>\n            Recommendations\n        </td>\n    </tr>\n</table>\n\n<table>\n    <tr>\n        <td>\n            <img width="300px" src="https://qdrant.tech/content/images/chat_bots.png">\n        </td>\n        <td>\n            <img width="300px" src="https://qdrant.tech/content/images/matching_engines.png">\n        </td>\n        <td>\n            <img width="300px" src="https://qdrant.tech/content/images/anomalies_detection.png">\n        </td>\n    </tr>\n    <tr>\n        <td>\n            Chat Bots\n        </td>\n        <td>\n            Matching Engines\n        </td>\n        <td>\n            Anomaly Detection\n        </td>\n    </tr>\n</table>\n\n</details>\n\n## API\n\n### REST\n\nOnline OpenAPI 3.0 documentation is available [here](https://api.qdrant.tech/).\nOpenAPI makes it easy to generate a client for virtually any framework or programming language.\n\nYou can also download raw OpenAPI [definitions](https://github.com/qdrant/qdrant/blob/master/docs/redoc/master/openapi.json).\n\n### gRPC\n\nFor faster production-tier searches, Qdrant also provides a gRPC interface. You can find gRPC documentation [here](https://qdrant.tech/documentation/interfaces/#grpc-interface).\n\n## Features\n\n### Filtering and Payload\n\nQdrant can attach any JSON payloads to vectors, allowing for both the storage and filtering of data based on the values in these payloads.\nPayload supports a wide range of data types and query conditions, including keyword matching, full-text filtering, numerical ranges, geo-locations, and more.\n\nFiltering conditions can be combined in various ways, including `should`, `must`, and `must_not` clauses,\nensuring that you can implement any desired business logic on top of similarity matching.\n\n\n### Hybrid Search with Sparse Vectors\n\nTo address the limitations of vector embeddings when searching for specific keywords, Qdrant introduces support for sparse vectors in addition to the regular dense ones.\n\nSparse vectors can be viewed as an generalization of BM25 or TF-IDF ranking. They enable you to harness the capabilities of transformer-based neural networks to weigh individual tokens effectively.\n\n\n### Vector Quantization and On-Disk Storage\n\nQdrant provides multiple options to make vector search cheaper and more resource-efficient.\nBuilt-in vector quantization reduces RAM usage by up to 97% and dynamically manages the trade-off between search speed and precision.\n\n\n### Distributed Deployment\n\nQdrant offers comprehensive horizontal scaling support through two key mechanisms:\n1. Size expansion via sharding and throughput enhancement via replication\n2. Zero-downtime rolling updates and seamless dynamic scaling of the collections\n\n\n### Highlighted Features\n\n* **Query Planning and Payload Indexes** - leverages stored payload information to optimize query execution strategy.\n* **SIMD Hardware Acceleration** - utilizes modern CPU x86-x64 and Neon architectures to deliver better performance.\n* **Async I/O** - uses `io_uring` to maximize disk throughput utilization even on a network-attached storage.\n* **Write-Ahead Logging** - ensures data persistence with update confirmation, even during power outages.\n\n\n# Integrations\n\nExamples and/or documentation of Qdrant integrations:\n\n- [Cohere](https://docs.cohere.com/docs/qdrant-and-cohere) ([blogpost on building a QA app with Cohere and Qdrant](https://qdrant.tech/articles/qa-with-cohere-and-qdrant/)) - Use Cohere embeddings with Qdrant\n- [DocArray](https://docs.docarray.org/user_guide/storing/index_qdrant/) - Use Qdrant as a document store in DocArray\n- [Haystack](https://haystack.deepset.ai/integrations/qdrant-document-store) - Use Qdrant as a document store with Haystack ([blogpost](https://haystack.deepset.ai/blog/qdrant-integration)).\n- [LangChain](https://python.langchain.com/docs/integrations/providers/qdrant/) ([blogpost](https://qdrant.tech/articles/langchain-integration/)) - Use Qdrant as a memory backend for LangChain.\n- [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/QdrantIndexDemo.html) - Use Qdrant as a Vector Store with LlamaIndex.\n- [OpenAI - ChatGPT retrieval plugin](https://github.com/openai/chatgpt-retrieval-plugin/blob/main/docs/providers/qdrant/setup.md) - Use Qdrant as a memory backend for ChatGPT\n- [Microsoft Semantic Kernel](https://devblogs.microsoft.com/semantic-kernel/the-power-of-persistent-memory-with-semantic-kernel-and-qdrant-vector-database/) - Use Qdrant as persistent memory with Semantic Kernel\n\n## Contacts\n\n- Have questions? Join our [Discord channel](https://qdrant.to/discord) or mention [@qdrant_engine on Twitter](https://qdrant.to/twitter)\n- Want to stay in touch with latest releases? Subscribe to our [Newsletters](https://qdrant.tech/subscribe/)\n- Looking for a managed cloud? Check [pricing](https://qdrant.tech/pricing/), need something personalised? We''re at [info@qdrant.tech](mailto:info@qdrant.tech)\n\n## License\n\nQdrant is licensed under the Apache License, Version 2.0. View a copy of the [License file](https://github.com/qdrant/qdrant/blob/master/LICENSE).\n', '{"language":"Rust","stars":27526,"forks":1934,"watchers":27526,"open_issues":448,"topics":["ai-search","ai-search-engine","embeddings-similarity","hnsw","image-search","knn-algorithm","machine-learning","mlops","nearest-neighbor-search","neural-network","neural-search","recommender-system","search","search-engine","search-engines","similarity-search","vector-database","vector-search","vector-search-engine"],"default_branch":"master","size_kb":40639,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:qdrant:qdrant","source_url":"https://github.com/qdrant/qdrant"},{"type":"has_code","target_id":"github:qdrant:qdrant","source_url":"https://github.com/qdrant/qdrant"},{"type":"has_code","target_id":"github:qdrant:qdrant","source_url":"https://github.com/qdrant/qdrant"},{"type":"has_code","target_id":"github:qdrant:qdrant","source_url":"https://github.com/qdrant/qdrant"},{"type":"has_code","target_id":"github:qdrant:qdrant","source_url":"https://github.com/qdrant/qdrant"},{"type":"has_code","target_id":"github:qdrant:go-client","source_url":"https://github.com/qdrant/go-client"},{"type":"has_code","target_id":"github:qdrant:rust-client","source_url":"https://github.com/qdrant/rust-client"},{"type":"has_code","target_id":"github:qdrant:qdrant-js","source_url":"https://github.com/qdrant/qdrant-js"},{"type":"has_code","target_id":"github:qdrant:qdrant-client","source_url":"https://github.com/qdrant/qdrant-client"},{"type":"has_code","target_id":"github:qdrant:qdrant-dotnet","source_url":"https://github.com/qdrant/qdrant-dotnet"},{"type":"has_code","target_id":"github:qdrant:java-client","source_url":"https://github.com/qdrant/java-client"},{"type":"has_code","target_id":"github:hkulekci:qdrant-php","source_url":"https://github.com/hkulekci/qdrant-php"},{"type":"has_code","target_id":"github:andreibondarev:qdrant-ruby","source_url":"https://github.com/andreibondarev/qdrant-ruby"},{"type":"has_code","target_id":"github:metaloom:qdrant-java-client","source_url":"https://github.com/metaloom/qdrant-java-client"},{"type":"has_code","target_id":"github:qdrant:qdrant","source_url":"https://github.com/qdrant/qdrant"},{"type":"has_code","target_id":"github:openai:chatgpt-retrieval-plugin","source_url":"https://github.com/openai/chatgpt-retrieval-plugin"},{"type":"has_code","target_id":"github:qdrant:qdrant","source_url":"https://github.com/qdrant/qdrant"}]', NULL, 'Apache-2.0', 'approved', 80, 'dabe5466355db96516195959d3e430c0', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-qdrant-qdrant from https://github.com/qdrant.png
Image converted to WebP: data/images/github-qdrant-qdrant.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ChristosChristofidis-awesome-deep-learning', 'github--christoschristofidis--awesome-deep-learning', 'awesome-deep-learning', 'ChristosChristofidis', '* **Books** * **Courses** * **Videos and Lectures** * **Papers** * **Tutorials** * **Researchers** * **Websites** * **Datasets** * **Conferences** * **Frameworks** * **Tools** * **Miscellaneous** * **Contributing** 1. Deep Learning by Yoshua Bengio, Ian Goodfellow and Aaron Courville (05/07/2015) 2. Neural Networks and Deep Learning by Michael Nielsen (Dec 2014) 3. Deep Learning by Microsoft Research (2013) 4. Deep Learning Tutorial by LISA lab, University of Montreal (Jan 6 2015) 5. neuralta...', '["awesome","awesome-list","deep-learning","deep-learning-tutorial","deep-networks","face-images","machine-learning","neural-network","recurrent-networks"]', 'other', 26951, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ChristosChristofidis/awesome-deep-learning","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Awesome Deep Learning [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\n## Table of Contents\n\n* **[Books](#books)**\n\n* **[Courses](#courses)**  \n\n* **[Videos and Lectures](#videos-and-lectures)**  \n\n* **[Papers](#papers)**  \n\n* **[Tutorials](#tutorials)**  \n\n* **[Researchers](#researchers)**  \n\n* **[Websites](#websites)**  \n\n* **[Datasets](#datasets)**\n\n* **[Conferences](#Conferences)**\n\n* **[Frameworks](#frameworks)**  \n\n* **[Tools](#tools)**  \n\n* **[Miscellaneous](#miscellaneous)**  \n\n* **[Contributing](#contributing)**  \n\n\n### Books\n\n1.  [Deep Learning](http://www.deeplearningbook.org/) by Yoshua Bengio, Ian Goodfellow and Aaron Courville  (05/07/2015)\n2.  [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) by  Michael Nielsen (Dec 2014)\n3.  [Deep Learning](http://research.microsoft.com/pubs/209355/DeepLearning-NowPublishing-Vol7-SIG-039.pdf) by Microsoft Research (2013)\n4.  [Deep Learning Tutorial](http://deeplearning.net/tutorial/deeplearning.pdf) by LISA lab, University of Montreal (Jan 6 2015)\n5.  [neuraltalk](https://github.com/karpathy/neuraltalk) by Andrej Karpathy : numpy-based RNN/LSTM implementation\n6.  [An introduction to genetic algorithms](http://www.boente.eti.br/fuzzy/ebook-fuzzy-mitchell.pdf)\n7.  [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/)\n8.  [Deep Learning in Neural Networks: An Overview](http://arxiv.org/pdf/1404.7828v4.pdf)\n9.  [Artificial intelligence and machine learning: Topic wise explanation](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/)\n10. [Grokking Deep Learning for Computer Vision](https://www.manning.com/books/grokking-deep-learning-for-computer-vision)\n11. [Dive into Deep Learning](https://d2l.ai/) - numpy based interactive Deep Learning book\n12. [Practical Deep Learning for Cloud, Mobile, and Edge](https://www.oreilly.com/library/view/practical-deep-learning/9781492034858/) - A book for optimization techniques during production.\n13. [Math and Architectures of Deep Learning](https://www.manning.com/books/math-and-architectures-of-deep-learning) - by Krishnendu Chaudhury\n14. [TensorFlow 2.0 in Action](https://www.manning.com/books/tensorflow-in-action) - by Thushan Ganegedara\n15. [Deep Learning for Natural Language Processing](https://www.manning.com/books/deep-learning-for-natural-language-processing) - by Stephan Raaijmakers\n16. [Deep Learning Patterns and Practices](https://www.manning.com/books/deep-learning-patterns-and-practices) - by Andrew Ferlitsch\n17. [Inside Deep Learning](https://www.manning.com/books/inside-deep-learning) - by Edward Raff\n18. [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition) - by FranÃ§ois Chollet\n19. [Evolutionary Deep Learning](https://www.manning.com/books/evolutionary-deep-learning) - by Micheal Lanham\n20. [Engineering Deep Learning Platforms](https://www.manning.com/books/engineering-deep-learning-platforms) - by Chi Wang and Donald Szeto\n21. [Deep Learning with R, Second Edition](https://www.manning.com/books/deep-learning-with-r-second-edition) - by FranÃ§ois Chollet with Tomasz Kalinowski and J. J. Allaire\n22. [Regularization in Deep Learning](https://www.manning.com/books/regularization-in-deep-learning) - by Liu Peng\n23. [Jax in Action](https://www.manning.com/books/jax-in-action) - by Grigory Sapunov\n24. [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.knowledgeisle.com/wp-content/uploads/2019/12/2-Aur%C3%A9lien-G%C3%A9ron-Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-Tensorflow_-Concepts-Tools-and-Techniques-to-Build-Intelligent-Systems-O%E2%80%99Reilly-Media-2019.pdf) by AurÃ©lien GÃ©ron  | Oct 15, 2019\n\n### Courses\n\n1.  [Machine Learning - Stanford](https://class.coursera.org/ml-005) by Andrew Ng in Coursera (2010-2014)\n2.  [Machine Learning - Caltech](http://work.caltech.edu/lectures.html) by Yaser Abu-Mostafa (2012-2014)\n3.  [Machine Learning - Carnegie Mellon](http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml) by Tom Mitchell (Spring 2011)\n2.  [Neural Networks for Machine Learning](https://class.coursera.org/neuralnets-2012-001) by Geoffrey Hinton in Coursera (2012)\n3.  [Neural networks class](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH) by Hugo Larochelle from UniversitÃ© de Sherbrooke (2013)\n4.  [Deep Learning Course](http://cilvr.cs.nyu.edu/doku.php?id=deeplearning:slides:start) by CILVR lab @ NYU (2014)\n5.  [A.I - Berkeley](https://courses.edx.org/courses/BerkeleyX/CS188x_1/1T2013/courseware/) by Dan Klein and Pieter Abbeel (2013)\n6.  [A.I - MIT](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/) by Patrick Henry Winston (2010)\n7.  [Vision and learning - computers and brains](http://web.mit.edu/course/other/i2course/www/vision_and_learning_fall_2013.html) by Shimon Ullman, Tomaso Poggio, Ethan Meyers @ MIT (2013)\n9.  [Convolutional Neural Networks for Visual Recognition - Stanford](http://vision.stanford.edu/teaching/cs231n/syllabus.html) by Fei-Fei Li, Andrej Karpathy (2017)\n10.  [Deep Learning for Natural Language Processing - Stanford](http://cs224d.stanford.edu/)\n11.  [Neural Networks - usherbrooke](http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html)\n12.  [Machine Learning - Oxford](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/) (2014-2015)\n13.  [Deep Learning - Nvidia](https://developer.nvidia.com/deep-learning-courses) (2015)\n14.  [Graduate Summer School: Deep Learning, Feature Learning](https://www.youtube.com/playlist?list=PLHyI3Fbmv0SdzMHAy0aN59oYnLy5vyyTA) by Geoffrey Hinton, Yoshua Bengio, Yann LeCun, Andrew Ng, Nando de Freitas and several others @ IPAM, UCLA (2012)\n15.  [Deep Learning - Udacity/Google](https://www.udacity.com/course/deep-learning--ud730) by Vincent Vanhoucke and Arpan Chakraborty (2016)\n16.  [Deep Learning - UWaterloo](https://www.youtube.com/playlist?list=PLehuLRPyt1Hyi78UOkMPWCGRxGcA9NVOE) by Prof. Ali Ghodsi at University of Waterloo (2015)\n17.  [Statistical Machine Learning - CMU](https://www.youtube.com/watch?v=azaLcvuql_g&list=PLjbUi5mgii6BWEUZf7He6nowWvGne_Y8r) by Prof. Larry Wasserman\n18.  [Deep Learning Course](https://www.college-de-france.fr/site/en-yann-lecun/course-2015-2016.htm) by Yann LeCun (2016)\n19. [Designing, Visualizing and Understanding Deep Neural Networks-UC Berkeley](https://www.youtube.com/playlist?list=PLkFD6_40KJIxopmdJF_CLNqG3QuDFHQUm)\n20. [UVA Deep Learning Course](http://uvadlc.github.io) MSc in Artificial Intelligence for the University of Amsterdam.\n21. [MIT 6.S094: Deep Learning for Self-Driving Cars](http://selfdrivingcars.mit.edu/)\n22. [MIT 6.S191: Introduction to Deep Learning](http://introtodeeplearning.com/)\n23. [Berkeley CS 294: Deep Reinforcement Learning](http://rll.berkeley.edu/deeprlcourse/)\n24. [Keras in Motion video course](https://www.manning.com/livevideo/keras-in-motion)\n25. [Practical Deep Learning For Coders](http://course.fast.ai/) by Jeremy Howard - Fast.ai\n26. [Introduction to Deep Learning](http://deeplearning.cs.cmu.edu/) by Prof. Bhiksha Raj (2017)\n27. [AI for Everyone](https://www.deeplearning.ai/ai-for-everyone/) by Andrew Ng (2019)\n28. [MIT Intro to Deep Learning 7 day bootcamp](https://introtodeeplearning.com) - A seven day bootcamp designed in MIT to introduce deep learning methods and applications (2019)\n29. [Deep Blueberry: Deep Learning](https://mithi.github.io/deep-blueberry) - A free five-weekend plan to self-learners to learn the basics of deep-learning architectures like CNNs, LSTMs, RNNs, VAEs, GANs, DQN, A3C and more (2019)\n30. [Spinning Up in Deep Reinforcement Learning](https://spinningup.openai.com/) - A free deep reinforcement learning course by OpenAI (2019)\n31. [Deep Learning Specialization - Coursera](https://www.coursera.org/specializations/deep-learning) - Breaking into AI with the best course from Andrew NG.\n32. [Deep Learning - UC Berkeley | STAT-157](https://www.youtube.com/playlist?list=PLZSO_6-bSqHQHBCoGaObUljoXAyyqhpFW) by Alex Smola and Mu Li (2019)\n33. [Machine Learning for Mere Mortals video course](https://www.manning.com/livevideo/machine-learning-for-mere-mortals) by Nick Chase\n34. [Machine Learning Crash Course with TensorFlow APIs](https://developers.google.com/machine-learning/crash-course/) -Google AI\n35. [Deep Learning from the Foundations](https://course.fast.ai/part2) Jeremy Howard - Fast.ai\n36. [Deep Reinforcement Learning (nanodegree) - Udacity](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) a 3-6 month Udacity nanodegree, spanning multiple courses (2018)\n37. [Grokking Deep Learning in Motion](https://www.manning.com/livevideo/grokking-deep-learning-in-motion) by Beau Carnes (2018)\n38. [Face Detection with Computer Vision and Deep Learning](https://www.udemy.com/share/1000gAA0QdcV9aQng=/) by Hakan Cebeci\n39. [Deep Learning Online Course list at Classpert](https://classpert.com/deep-learning) List of Deep Learning online courses (some are free) from Classpert Online Course Search\n40. [AWS Machine Learning](https://aws.training/machinelearning) Machine Learning and Deep Learning Courses from Amazon''s Machine Learning university\n41. [Intro to Deep Learning with PyTorch](https://www.udacity.com/course/deep-learning-pytorch--ud188) - A great introductory course on Deep Learning by Udacity and Facebook AI\n42. [Deep Learning by Kaggle](https://www.kaggle.com/learn/deep-learning) - Kaggle''s  free course on Deep Learning\n43. [Yann LeCunâ€™s Deep Learning Course at CDS](https://cds.nyu.edu/deep-learning/) - DS-GA 1008 Â· SPRING 2021 \n44. [Neural Networks and Deep Learning](https://webcms3.cse.unsw.edu.au/COMP9444/19T3/) - COMP9444 19T3\n45. [Deep Learning A.I.Shelf](http://aishelf.org/category/ia/deep-learning/)\n\n### Videos and Lectures\n\n1.  [How To Create A Mind](https://www.youtube.com/watch?v=RIkxVci-R4k) By Ray Kurzweil\n2.  [Deep Learning, Self-Taught Learning and Unsupervised Feature Learning](https://www.youtube.com/watch?v=n1ViNeWhC24) By Andrew Ng\n3.  [Recent Developments in Deep Learning](https://www.youtube.com/watch?v=vShMxxqtDDs&amp;index=3&amp;list=PL78U8qQHXgrhP9aZraxTT5-X1RccTcUYT) By Geoff Hinton\n4.  [The Unreasonable Effectiveness of Deep Learning](https://www.youtube.com/watch?v=sc-KbuZqGkI) by Yann LeCun\n5.  [Deep Learning of Representations](https://www.youtube.com/watch?v=4xsVFLnHC_0) by Yoshua bengio\n6.  [Principles of Hierarchical Temporal Memory](https://www.youtube.com/watch?v=6ufPpZDmPKA) by Jeff Hawkins\n7.  [Machine Learning Discussion Group - Deep Learning w/ Stanford AI Lab](https://www.youtube.com/watch?v=2QJi0ArLq7s&amp;list=PL78U8qQHXgrhP9aZraxTT5-X1RccTcUYT) by Adam Coates\n8.  [Making Sense of the World with Deep Learning](http://vimeo.com/80821560) By Adam Coates\n9.  [Demystifying Unsupervised Feature Learning ](https://www.youtube.com/watch?v=wZfVBwOO0-k) By Adam Coates\n10.  [Visual Perception with Deep Learning](https://www.youtube.com/watch?v=3boKlkPBckA) By Yann LeCun\n11.  [The Next Generation of Neural Networks](https://www.youtube.com/watch?v=AyzOUbkUf3M) By Geoffrey Hinton at GoogleTechTalks\n12.  [The wonderful and terrifying implications of computers that can learn](http://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn) By Jeremy Howard at TEDxBrussels\n13.  [Unsupervised Deep Learning - Stanford](http://web.stanford.edu/class/cs294a/handouts.html) by Andrew Ng in Stanford (2011)\n14.  [Natural Language Processing](http://web.stanford.edu/class/cs224n/handouts/) By Chris Manning in Stanford\n15.  [A beginners Guide to Deep Neural Networks](http://googleresearch.blogspot.com/2015/09/a-beginners-guide-to-deep-neural.html) By Natalie Hammel and Lorraine Yurshansky\n16.  [Deep Learning: Intelligence from Big Data](https://www.youtube.com/watch?v=czLI3oLDe8M) by Steve Jurvetson (and panel) at VLAB in Stanford.\n17. [Introduction to Artificial Neural Networks and Deep Learning](https://www.youtube.com/watch?v=FoO8qDB8gUU) by Leo Isikdogan at Motorola Mobility HQ\n18. [NIPS 2016 lecture and workshop videos](https://nips.cc/Conferences/2016/Schedule) - NIPS 2016\n19. [Deep Learning Crash Course](https://www.youtube.com/watch?v=oS5fz_mHVz0&list=PLWKotBjTDoLj3rXBL-nEIPRN9V3a9Cx07): a series of mini-lectures by Leo Isikdogan on YouTube (2018)\n20. [Deep Learning Crash Course](https://www.manning.com/livevideo/deep-learning-crash-course) By Oliver Zeigermann\n21. [Deep Learning with R in Motion](https://www.manning.com/livevideo/deep-learning-with-r-in-motion): a live video course that teaches how to apply deep learning to text and images using the powerful Keras library and its R language interface.\n22. [Medical Imaging with Deep Learning Tutorial](https://www.youtube.com/playlist?list=PLheiZMDg_8ufxEx9cNVcOYXsT3BppJP4b): This tutorial is styled as a graduate lecture about medical imaging with deep learning. This will cover the background of popular medical image domains (chest X-ray and histology) as well as methods to tackle multi-modality/view, segmentation, and counting tasks.\n23. [Deepmind x UCL Deeplearning](https://www.youtube.com/playlist?list=PLqYmG7hTraZCDxZ44o4p3N5Anz3lLRVZF): 2020 version \n24. [Deepmind x UCL Reinforcement Learning](https://www.youtube.com/playlist?list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb): Deep Reinforcement Learning\n25. [CMU 11-785 Intro to Deep learning Spring 2020](https://www.youtube.com/playlist?list=PLp-0K3kfddPzCnS4CqKphh-zT3aDwybDe) Course: 11-785, Intro to Deep Learning by Bhiksha Raj \n26. [Machine Learning CS 229](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU) : End part focuses on deep learning By Andrew Ng\n27. [What is Neural Structured Learning by Andrew Ferlitsch](https://youtu.be/LXWSE_9gHd0)\n28. [Deep Learning Design Patterns by Andrew Ferlitsch](https://youtu.be/_DaviS6K0Vc)\n29. [Architecture of a Modern CNN: the design pattern approach by Andrew Ferlitsch](https://youtu.be/QCGSS3kyGo0)\n30. [Metaparameters in a CNN by Andrew Ferlitsch](https://youtu.be/K1PLeggQ33I)\n31. [Multi-task CNN: a real-world example by Andrew Ferlitsch](https://youtu.be/dH2nuI-1-qM)\n32. [A friendly introduction to deep reinforcement learning by Luis Serrano](https://youtu.be/1FyAh07jh0o)\n33. [What are GANs and how do they work? by Edward Raff](https://youtu.be/f6ivp84qFUc)\n34. [Coding a basic WGAN in PyTorch by Edward Raff](https://youtu.be/7VRdaqMDalQ)\n35. [Training a Reinforcement Learning Agent by Miguel Morales](https://youtu.be/8TMT-gHlj_Q)\n36. [Understand what is Deep Learning](https://www.scaler.com/topics/what-is-deep-learning/)\n\n### Papers\n*You can also find the most cited deep learning papers from [here](https://github.com/terryum/awesome-deep-learning-papers)*\n\n1.  [ImageNet Classification with Deep Convolutional Neural Networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n2.  [Using Very Deep Autoencoders for Content Based Image Retrieval](http://www.cs.toronto.edu/~hinton/absps/esann-deep-final.pdf)\n3.  [Learning Deep Architectures for AI](http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf)\n4.  [CMUâ€™s list of papers](http://deeplearning.cs.cmu.edu/)\n5.  [Neural Networks for Named Entity Recognition](http://nlp.stanford.edu/~socherr/pa4_ner.pdf) [zip](http://nlp.stanford.edu/~socherr/pa4-ner.zip)\n6. [Training tricks by YB](http://www.iro.umontreal.ca/~bengioy/papers/YB-tricks.pdf)\n7. [Geoff Hinton''s reading list (all papers)](http://www.cs.toronto.edu/~hinton/deeprefs.html)\n8. [Supervised Sequence Labelling with Recurrent Neural Networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n9.  [Statistical Language Models based on Neural Networks](http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf)\n10.  [Training Recurrent Neural Networks](http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)\n11.  [Recursive Deep Learning for Natural Language Processing and Computer Vision](http://nlp.stanford.edu/~socherr/thesis.pdf)\n12.  [Bi-directional RNN](http://www.di.ufpe.br/~fnj/RNA/bibliografia/BRNN.pdf)\n13.  [LSTM](http://web.eecs.utk.edu/~itamar/courses/ECE-692/Bobby_paper1.pdf)\n14.  [GRU - Gated Recurrent Unit](http://arxiv.org/pdf/1406.1078v3.pdf)\n15.  [GFRNN](http://arxiv.org/pdf/1502.02367v3.pdf) [.](http://jmlr.org/proceedings/papers/v37/chung15.pdf) [.](http://jmlr.org/proceedings/papers/v37/chung15-supp.pdf)\n16.  [LSTM: A Search Space Odyssey](http://arxiv.org/pdf/1503.04069v1.pdf)\n17.  [A Critical Review of Recurrent Neural Networks for Sequence Learning](http://arxiv.org/pdf/1506.00019v1.pdf)\n18.  [Visualizing and Understanding Recurrent Networks](http://arxiv.org/pdf/1506.02078v1.pdf)\n19.  [Wojciech Zaremba, Ilya Sutskever, An Empirical Exploration of Recurrent Network Architectures](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n20.  [Recurrent Neural Network based Language Model](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n21.  [Extensions of Recurrent Neural Network Language Model](http://www.fit.vutbr.cz/research/groups/speech/publi/2011/mikolov_icassp2011_5528.pdf)\n22.  [Recurrent Neural Network based Language Modeling in Meeting Recognition](http://www.fit.vutbr.cz/~imikolov/rnnlm/ApplicationOfRNNinMeetingRecognition_IS2011.pdf)\n23.  [Deep Neural Networks for Acoustic Modeling in Speech Recognition](http://cs224d.stanford.edu/papers/maas_paper.pdf)\n24.  [Speech Recognition with Deep Recurrent Neural Networks](http://www.cs.toronto.edu/~fritz/absps/RNN13.pdf)\n25.  [Reinforcement Learning Neural Turing Machines](http://arxiv.org/pdf/1505.00521v1)\n26.  [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](http://arxiv.org/pdf/1406.1078v3.pdf)\n27. [Google - Sequence to Sequence  Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n28. [Memory Networks](http://arxiv.org/pdf/1410.3916v10)\n29. [Policy Learning with Continuous Memory States for Partially Observed Robotic Control](http://arxiv.org/pdf/1507.01273v1)\n30. [Microsoft - Jointly Modeling Embedding and Translation to Bridge Video and Language](http://arxiv.org/pdf/1505.01861v1.pdf)\n31. [Neural Turing Machines](http://arxiv.org/pdf/1410.5401v2.pdf)\n32. [Ask Me Anything: Dynamic Memory Networks for Natural Language Processing](http://arxiv.org/pdf/1506.07285v1.pdf)\n33. [Mastering the Game of Go with Deep Neural Networks and Tree Search](http://www.nature.com/nature/journal/v529/n7587/pdf/nature16961.pdf)\n34. [Batch Normalization](https://arxiv.org/abs/1502.03167)\n35. [Residual Learning](https://arxiv.org/pdf/1512.03385v1.pdf)\n36. [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/pdf/1611.07004v1.pdf)\n37. [Berkeley AI Research (BAIR) Laboratory](https://arxiv.org/pdf/1611.07004v1.pdf)\n38. [MobileNets by Google](https://arxiv.org/abs/1704.04861)\n39. [Cross Audio-Visual Recognition in the Wild Using Deep Learning](https://arxiv.org/abs/1706.05739)\n40. [Dynamic Routing Between Capsules](https://arxiv.org/abs/1710.09829)\n41. [Matrix Capsules With Em Routing](https://openreview.net/pdf?id=HJWLfGWRb)\n42. [Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n43. [Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661v1.pdf)\n44. [Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf)\n45. [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n46. [Siamese Neural Networks for One-shot Image Recognition](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)\n47. [Unsupervised Translation of Programming Languages](https://arxiv.org/pdf/2006.03511.pdf)\n48. [Matching Networks for One Shot Learning](http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf)\n49. [VOLO: Vision Outlooker for Visual Recognition](https://arxiv.org/pdf/2106.13112.pdf)\n50. [ViT: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929.pdf)\n51. [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://proceedings.mlr.press/v37/ioffe15.pdf)\n52. [DeepFaceDrawing: Deep Generation of Face Images from Sketches](http://geometrylearning.com/paper/DeepFaceDrawing.pdf?fbclid=IwAR0colWFHPGBCB1APZq9JVsWeWtmeZd9oCTNQvR52T5PRUJP_dLOwB8pt0I)\n\n### Tutorials\n\n1.  [UFLDL Tutorial 1](http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial)\n2.  [UFLDL Tutorial 2](http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/)\n3.  [Deep Learning for NLP (without Magic)](http://www.socher.org/index.php/DeepLearningTutorial/DeepLearningTutorial)\n4.  [A Deep Learning Tutorial: From Perceptrons to Deep Networks](http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks)\n5.  [Deep Learning from the Bottom up](http://www.metacademy.org/roadmaps/rgrosse/deep_learning)\n6.  [Theano Tutorial](http://deeplearning.net/tutorial/deeplearning.pdf)\n7.  [Neural Networks for Matlab](http://uk.mathworks.com/help/pdf_doc/nnet/nnet_ug.pdf)\n8.  [Using convolutional neural nets to detect facial keypoints tutorial](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)\n9.  [Torch7 Tutorials](https://github.com/clementfarabet/ipam-tutorials/tree/master/th_tutorials)\n10.  [The Best Machine Learning Tutorials On The Web](https://github.com/josephmisiti/machine-learning-module)\n11. [VGG Convolutional Neural Networks Practical](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/index.html)\n12. [TensorFlow tutorials](https://github.com/nlintz/TensorFlow-Tutorials)\n13. [More TensorFlow tutorials](https://github.com/pkmital/tensorflow_tutorials)\n13. [TensorFlow Python Notebooks](https://github.com/aymericdamien/TensorFlow-Examples)\n14. [Keras and Lasagne Deep Learning Tutorials](https://github.com/Vict0rSch/deep_learning)\n15. [Classification on raw time series in TensorFlow with a LSTM RNN](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition)\n16. [Using convolutional neural nets to detect facial keypoints tutorial](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)\n17. [TensorFlow-World](https://github.com/astorfi/TensorFlow-World)\n18. [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)\n19. [Grokking Deep Learning](https://www.manning.com/books/grokking-deep-learning)\n20. [Deep Learning for Search](https://www.manning.com/books/deep-learning-for-search)\n21. [Keras Tutorial: Content Based Image Retrieval Using a Convolutional Denoising Autoencoder](https://medium.com/sicara/keras-tutorial-content-based-image-retrieval-convolutional-denoising-autoencoder-dc91450cc511)\n22. [Pytorch Tutorial by Yunjey Choi](https://github.com/yunjey/pytorch-tutorial)\n23. [Understanding deep Convolutional Neural Networks with a practical use-case in Tensorflow and Keras](https://ahmedbesbes.com/understanding-deep-convolutional-neural-networks-with-a-practical-use-case-in-tensorflow-and-keras.html)\n24. [Overview and benchmark of traditional and deep learning models in text classification](https://ahmedbesbes.com/overview-and-benchmark-of-traditional-and-deep-learning-models-in-text-classification.html)\n25. [Hardware for AI: Understanding computer hardware & build your own computer](https://github.com/MelAbgrall/HardwareforAI)\n26. [Programming Community Curated Resources](https://hackr.io/tutorials/learn-artificial-intelligence-ai)\n27. [The Illustrated Self-Supervised Learning](https://amitness.com/2020/02/illustrated-self-supervised-learning/)\n28. [Visual Paper Summary: ALBERT (A Lite BERT)](https://amitness.com/2020/02/albert-visual-summary/)\n28. [Semi-Supervised Deep Learning with GANs for Melanoma Detection](https://www.manning.com/liveproject/semi-supervised-deep-learning-with-gans-for-melanoma-detection/)\n29. [Named Entity Recognition using Reformers](https://github.com/SauravMaheshkar/Trax-Examples/blob/main/NLP/NER%20using%20Reformer.ipynb)\n30. [Deep N-Gram Models on Shakespeareâ€™s works](https://github.com/SauravMaheshkar/Trax-Examples/blob/main/NLP/Deep%20N-Gram.ipynb)\n31. [Wide Residual Networks](https://github.com/SauravMaheshkar/Trax-Examples/blob/main/vision/illustrated-wideresnet.ipynb)\n32. [Fashion MNIST using Flax](https://github.com/SauravMaheshkar/Flax-Examples)\n33. [Fake News Classification (with streamlit deployment)](https://github.com/SauravMaheshkar/Fake-News-Classification)\n34. [Regression Analysis for Primary Biliary Cirrhosis](https://github.com/SauravMaheshkar/CoxPH-Model-for-Primary-Biliary-Cirrhosis)\n35. [Cross Matching Methods for Astronomical Catalogs](https://github.com/SauravMaheshkar/Cross-Matching-Methods-for-Astronomical-Catalogs)\n36. [Named Entity Recognition using BiDirectional LSTMs](https://github.com/SauravMaheshkar/Named-Entity-Recognition-)\n37. [Image Recognition App using Tflite and Flutter](https://github.com/SauravMaheshkar/Flutter_Image-Recognition)\n\n## Researchers\n\n1. [Aaron Courville](http://aaroncourville.wordpress.com)\n2. [Abdel-rahman Mohamed](http://www.cs.toronto.edu/~asamir/)\n3. [Adam Coates](http://cs.stanford.edu/~acoates/)\n4. [Alex Acero](http://research.microsoft.com/en-us/people/alexac/)\n5. [ Alex Krizhevsky ](http://www.cs.utoronto.ca/~kriz/index.html)\n6. [ Alexander Ilin ](http://users.ics.aalto.fi/alexilin/)\n7. [ Amos Storkey ](http://homepages.inf.ed.ac.uk/amos/)\n8. [ Andrej Karpathy ](https://karpathy.ai/)\n9. [ Andrew M. Saxe ](http://www.stanford.edu/~asaxe/)\n10. [ Andrew Ng ](http://www.cs.stanford.edu/people/ang/)\n11. [ Andrew W. Senior ](http://research.google.com/pubs/author37792.html)\n12. [ Andriy Mnih ](http://www.gatsby.ucl.ac.uk/~amnih/)\n13. [ Ayse Naz Erkan ](http://www.cs.nyu.edu/~naz/)\n14. [ Benjamin Schrauwen ](http://reslab.elis.ugent.be/benjamin)\n15. [ Bernardete Ribeiro ](https://www.cisuc.uc.pt/people/show/2020)\n16. [ Bo David Chen ](http://vision.caltech.edu/~bchen3/Site/Bo_David_Chen.html)\n17. [ Boureau Y-Lan ](http://cs.nyu.edu/~ylan/)\n18. [ Brian Kingsbury ](http://researcher.watson.ibm.com/researcher/view.php?person=us-bedk)\n19. [ Christopher Manning ](http://nlp.stanford.edu/~manning/)\n20. [ Clement Farabet ](http://www.clement.farabet.net/)\n21. [ Dan Claudiu CireÈ™an ](http://www.idsia.ch/~ciresan/)\n22. [ David Reichert ](http://serre-lab.clps.brown.edu/person/david-reichert/)\n23. [ Derek Rose ](http://mil.engr.utk.edu/nmil/member/5.html)\n24. [ Dong Yu ](http://research.microsoft.com/en-us/people/dongyu/default.aspx)\n25. [ Drausin Wulsin ](http://www.seas.upenn.edu/~wulsin/)\n26. [ Erik M. Schmidt ](http://music.ece.drexel.edu/people/eschmidt)\n27. [ Eugenio Culurciello ](https://engineering.purdue.edu/BME/People/viewPersonById?resource_id=71333)\n28. [ Frank Seide ](http://research.microsoft.com/en-us/people/fseide/)\n29. [ Galen Andrew ](http://homes.cs.washington.edu/~galen/)\n30. [ Geoffrey Hinton ](http://www.cs.toronto.edu/~hinton/)\n31. [ George Dahl ](http://www.cs.toronto.edu/~gdahl/)\n32. [ Graham Taylor ](http://www.uoguelph.ca/~gwtaylor/)\n33. [ GrÃ©goire Montavon ](http://gregoire.montavon.name/)\n34. [ Guido Francisco MontÃºfar ](http://personal-homepages.mis.mpg.de/montufar/)\n35. [ Guillaume Desjardins ](http://brainlogging.wordpress.com/)\n36. [ Hannes Schulz ](http://www.ais.uni-bonn.de/~schulz/)\n37. [ HÃ©lÃ¨ne Paugam-Moisy ](http://www.lri.fr/~hpaugam/)\n38. [ Honglak Lee ](http://web.eecs.umich.edu/~honglak/)\n39. [ Hugo Larochelle ](http://www.dmi.usherb.ca/~larocheh/index_en.html)\n40. [ Ilya Sutskever ](http://www.cs.toronto.edu/~ilya/)\n41. [ Itamar Arel ](http://mil.engr.utk.edu/nmil/member/2.html)\n42. [ James Martens ](http://www.cs.toronto.edu/~jmartens/)\n43. [ Jason Morton ](http://www.jasonmorton.com/)\n44. [ Jason Weston ](http://www.thespermwhale.com/jaseweston/)\n45. [ Jeff Dean ](http://research.google.com/pubs/jeff.html)\n46. [ Jiquan Mgiam ](http://cs.stanford.edu/~jngiam/)\n47. [ Joseph Turian ](http://www-etud.iro.umontreal.ca/~turian/)\n48. [ Joshua Matthew Susskind ](http://aclab.ca/users/josh/index.html)\n49. [ JÃ¼rgen Schmidhuber ](http://www.idsia.ch/~juergen/)\n50. [ Justin A. Blanco ](https://sites.google.com/site/blancousna/)\n51. [ Koray Kavukcuoglu ](http://koray.kavukcuoglu.org/)\n52. [ KyungHyun Cho ](http://users.ics.aalto.fi/kcho/)\n53. [ Li Deng ](http://research.microsoft.com/en-us/people/deng/)\n54. [ Lucas Theis ](http://www.kyb.tuebingen.mpg.de/nc/employee/details/lucas.html)\n55. [ Ludovic Arnold ](http://ludovicarnold.altervista.org/home/)\n56. [ Marc''Aurelio Ranzato ](http://www.cs.nyu.edu/~ranzato/)\n57. [ Martin LÃ¤ngkvist ](http://aass.oru.se/~mlt/)\n58. [ Misha Denil ](http://mdenil.com/)\n59. [ Mohammad Norouzi ](http://www.cs.toronto.edu/~norouzi/)\n60. [ Nando de Freitas ](http://www.cs.ubc.ca/~nando/)\n61. [ Navdeep Jaitly ](http://www.cs.utoronto.ca/~ndjaitly/)\n62. [ Nicolas Le Roux ](http://nicolas.le-roux.name/)\n63. [ Nitish Srivastava ](http://www.cs.toronto.edu/~nitish/)\n64. [ Noel Lopes ](https://www.cisuc.uc.pt/people/show/2028)\n65. [ Oriol Vinyals ](http://www.cs.berkeley.edu/~vinyals/)\n66. [ Pascal Vincent ](http://www.iro.umontreal.ca/~vincentp)\n67. [ Patrick Nguyen ](https://sites.google.com/site/drpngx/)\n68. [ Pedro Domingos ](http://homes.cs.washington.edu/~pedrod/)\n69. [ Peggy Series ](http://homepages.inf.ed.ac.uk/pseries/)\n70. [ Pierre Sermanet ](http://cs.nyu.edu/~sermanet)\n71. [ Piotr Mirowski ](http://www.cs.nyu.edu/~mirowski/)\n72. [ Quoc V. Le ](http://ai.stanford.edu/~quocle/)\n73. [ Reinhold Scherer ](http://bci.tugraz.at/scherer/)\n74. [ Richard Socher ](http://www.socher.org/)\n75. [ Rob Fergus ](http://cs.nyu.edu/~fergus/pmwiki/pmwiki.php)\n76. [ Robert Coop ](http://mil.engr.utk.edu/nmil/member/19.html)\n77. [ Robert Gens ](http://homes.cs.washington.edu/~rcg/)\n78. [ Roger Grosse ](http://people.csail.mit.edu/rgrosse/)\n79. [ Ronan Collobert ](http://ronan.collobert.com/)\n80. [ Ruslan Salakhutdinov ](http://www.utstat.toronto.edu/~rsalakhu/)\n81. [ Sebastian Gerwinn ](http://www.kyb.tuebingen.mpg.de/nc/employee/details/sgerwinn.html)\n82. [ StÃ©phane Mallat ](http://www.cmap.polytechnique.fr/~mallat/)\n83. [ Sven Behnke ](http://www.ais.uni-bonn.de/behnke/)\n84. [ Tapani Raiko ](http://users.ics.aalto.fi/praiko/)\n85. [ Tara Sainath ](https://sites.google.com/site/tsainath/)\n86. [ Tijmen Tieleman ](http://www.cs.toronto.edu/~tijmen/)\n87. [ Tom Karnowski ](http://mil.engr.utk.edu/nmil/member/36.html)\n88. [ TomÃ¡Å¡ Mikolov ](https://research.facebook.com/tomas-mikolov)\n89. [ Ueli Meier ](http://www.idsia.ch/~meier/)\n90. [ Vincent Vanhoucke ](http://vincent.vanhoucke.com)\n91. [ Volodymyr Mnih ](http://www.cs.toronto.edu/~vmnih/)\n92. [ Yann LeCun ](http://yann.lecun.com/)\n93. [ Yichuan Tang ](http://www.cs.toronto.edu/~tang/)\n94. [ Yoshua Bengio ](http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html)\n95. [ Yotaro Kubo ](http://yota.ro/)\n96. [ Youzhi (Will) Zou ](http://ai.stanford.edu/~wzou)\n97. [ Fei-Fei Li ](http://vision.stanford.edu/feifeili)\n98. [ Ian Goodfellow ](https://research.google.com/pubs/105214.html)\n99. [ Robert LaganiÃ¨re ](http://www.site.uottawa.ca/~laganier/)\n100. [Merve AyyÃ¼ce KÄ±zrak](http://www.ayyucekizrak.com/)\n\n\n### Websites\n\n1.  [deeplearning.net](http://deeplearning.net/)\n2.  [deeplearning.stanford.edu](http://deeplearning.stanford.edu/)\n3.  [nlp.stanford.edu](http://nlp.stanford.edu/)\n4.  [ai-junkie.com](http://www.ai-junkie.com/ann/evolved/nnt1.html)\n5.  [cs.brown.edu/research/ai](http://cs.brown.edu/research/ai/)\n6.  [eecs.umich.edu/ai](http://www.eecs.umich.edu/ai/)\n7.  [cs.utexas.edu/users/ai-lab](http://www.cs.utexas.edu/users/ai-lab/)\n8.  [cs.washington.edu/research/ai](http://www.cs.washington.edu/research/ai/)\n9.  [aiai.ed.ac.uk](http://www.aiai.ed.ac.uk/)\n10.  [www-aig.jpl.nasa.gov](http://www-aig.jpl.nasa.gov/)\n11.  [csail.mit.edu](http://www.csail.mit.edu/)\n12.  [cgi.cse.unsw.edu.au/~aishare](http://cgi.cse.unsw.edu.au/~aishare/)\n13.  [cs.rochester.edu/research/ai](http://www.cs.rochester.edu/research/ai/)\n14.  [ai.sri.com](http://www.ai.sri.com/)\n15.  [isi.edu/AI/isd.htm](http://www.isi.edu/AI/isd.htm)\n16.  [nrl.navy.mil/itd/aic](http://www.nrl.navy.mil/itd/aic/)\n17.  [hips.seas.harvard.edu](http://hips.seas.harvard.edu/)\n18.  [AI Weekly](http://aiweekly.co)\n19.  [stat.ucla.edu](http://statistics.ucla.edu/)\n20.  [deeplearning.cs.toronto.edu](http://deeplearning.cs.toronto.edu/i2t)\n21.  [jeffdonahue.com/lrcn/](http://jeffdonahue.com/lrcn/)\n22.  [visualqa.org](http://www.visualqa.org/)\n23.  [www.mpi-inf.mpg.de/departments/computer-vision...](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/)\n24.  [Deep Learning News](http://news.startup.ml/)\n25.  [Machine Learning is Fun! Adam Geitgey''s Blog](https://medium.com/@ageitgey/)\n26.  [Guide to Machine Learning](http://yerevann.com/a-guide-to-deep-learning/)\n27.  [Deep Learning for Beginners](https://spandan-madan.github.io/DeepLearningProject/)\n28.  [Machine Learning Mastery blog](https://machinelearningmastery.com/blog/)\n29.  [ML Compiled](https://ml-compiled.readthedocs.io/en/latest/)\n30.  [Programming Community Curated Resources](https://hackr.io/tutorials/learn-artificial-intelligence-ai)\n31.  [A Beginner''s Guide To Understanding Convolutional Neural Networks](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)\n32.  [ahmedbesbes.com](http://ahmedbesbes.com)\n33.  [amitness.com](https://amitness.com/)\n34.  [AI Summer](https://theaisummer.com/)\n35.  [AI Hub - supported by AAAI, NeurIPS](https://aihub.org/)\n36.  [CatalyzeX: Machine Learning Hub for Builders and Makers](https://www.catalyzeX.com)\n37.  [The Epic Code](https://theepiccode.com/)\n38.  [all AI news](https://allainews.com/)\n\n### Datasets\n\n1.  [MNIST](http://yann.lecun.com/exdb/mnist/) Handwritten digits\n2.  [Google House Numbers](http://ufldl.stanford.edu/housenumbers/) from street view\n3.  [CIFAR-10 and CIFAR-100](http://www.cs.toronto.edu/~kriz/cifar.html)\n4.  [IMAGENET](http://www.image-net.org/)\n5.  [Tiny Images](http://groups.csail.mit.edu/vision/TinyImages/) 80 Million tiny images6.  \n6.  [Flickr Data](https://yahooresearch.tumblr.com/post/89783581601/one-hundred-million-creative-commons-flickr-images) 100 Million Yahoo dataset\n7.  [Berkeley Segmentation Dataset 500](http://www.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/)\n8.  [UC Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/)\n9.  [Flickr 8k](http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/KCCA.html)\n10. [Flickr 30k](http://shannon.cs.illinois.edu/DenotationGraph/)\n11. [Microsoft COCO](http://mscoco.org/home/)\n12. [VQA](http://www.visualqa.org/)\n13. [Image QA](http://www.cs.toronto.edu/~mren/imageqa/data/cocoqa/)\n14. [AT&T Laboratories Cambridge face database](http://www.uk.research.att.com/facedatabase.html)\n15. [AVHRR Pathfinder](http://xtreme.gsfc.nasa.gov)\n16. [Air Freight](http://www.anc.ed.ac.uk/~amos/afreightdata.html) - The Air Freight data set is a ray-traced image sequence along with ground truth segmentation based on textural characteristics. (455 images + GT, each 160x120 pixels). (Formats: PNG)  \n17. [Amsterdam Library of Object Images](http://www.science.uva.nl/~aloi/) - ALOI is a color image collection of one-thousand small objects, recorded for scientific purposes. In order to capture the sensory variation in object recordings, we systematically varied viewing angle, illumination angle, and illumination color for each object, and additionally captured wide-baseline stereo images. We recorded over a hundred images of each object, yielding a total of 110,250 images for the collection. (Formats: png)\n18. [Annotated face, hand, cardiac & meat images](http://www.imm.dtu.dk/~aam/) - Most images & annotations are supplemented by various ASM/AAM analyses using the AAM-API. (Formats: bmp,asf)\n19. [Image Analysis and Computer Graphics](http://www.imm.dtu.dk/image/)  \n21. [Brown University Stimuli](http://www.cog.brown.edu/~tarr/stimuli.html) - A variety of datasets including geons, objects, and "greebles". Good for testing recognition algorithms. (Formats: pict)\n22. [CAVIAR video sequences of mall and public space behavior](http://homepages.inf.ed.ac.uk/rbf/CAVIARDATA1/) - 90K video frames in 90 sequences of various human activities, with XML ground truth of detection and behavior classification (Formats: MPEG2 & JPEG)\n23. [Machine Vision Unit](http://www.ipab.inf.ed.ac.uk/mvu/)\n25. [CCITT Fax standard images](http://www.cs.waikato.ac.nz/~singlis/ccitt.html) - 8 images (Formats: gif)\n26. [CMU CIL''s Stereo Data with Ground Truth](cil-ster.html) - 3 sets of 11 images, including color tiff images with spectroradiometry (Formats: gif, tiff)\n27. [CMU PIE Database](http://www.ri.cmu.edu/projects/project_418.html) - A database of 41,368 face images of 68 people captured under 13 poses, 43 illuminations conditions, and with 4 different expressions.\n28. [CMU VASC Image Database](http://www.ius.cs.cmu.edu/idb/) - Images, sequences, stereo pairs (thousands of images) (Formats: Sun Rasterimage)\n29. [Caltech Image Database](http://www.vision.caltech.edu/html-files/archive.html) - about 20 images - mostly top-down views of small objects and toys. (Formats: GIF)\n30. [Columbia-Utrecht Reflectance and Texture Database](http://www.cs.columbia.edu/CAVE/curet/) - Texture and reflectance measurements for over 60 samples of 3D texture, observed with over 200 different combinations of viewing and illumination directions. (Formats: bmp)\n31. [Computational Colour Constancy Data](http://www.cs.sfu.ca/~colour/data/index.html) - A dataset oriented towards computational color constancy, but useful for computer vision in general. It includes synthetic data, camera sensor data, and over 700 images. (Formats: tiff)\n32. [Computational Vision Lab](http://www.cs.sfu.ca/~colour/)\n34. [Content-based image retrieval database](http://www.cs.washington.edu/research/imagedatabase/groundtruth/) - 11 sets of color images for testing algorithms for content-based retrieval. Most sets have a description file with names of objects in each image. (Formats: jpg)\n35. [Efficient Content-based Retrieval Group](http://www.cs.washington.edu/research/imagedatabase/)\n37. [Densely Sampled View Spheres](http://ls7-www.cs.uni-dortmund.de/~peters/pages/research/modeladaptsys/modeladaptsys_vba_rov.html) - Densely sampled view spheres - upper half of the view sphere of two toy objects with 2500 images each. (Formats: tiff)\n38. [Computer Science VII (Graphical Systems)](http://ls7-www.cs.uni-dortmund.de/)\n40. [Digital Embryos](https://web-beta.archive.org/web/20011216051535/vision.psych.umn.edu/www/kersten-lab/demos/digitalembryo.html) - Digital embryos are novel objects which may be used to develop and test object recognition systems. They have an organic appearance. (Formats: various formats are available on request)\n41. [Univerity of Minnesota Vision Lab](http://vision.psych.umn.edu/users/kersten//kersten-lab/kersten-lab.html) \n42. [El Salvador Atlas of Gastrointestinal VideoEndoscopy](http://www.gastrointestinalatlas.com) - Images and Videos of his-res of studies taken from Gastrointestinal Video endoscopy. (Formats: jpg, mpg, gif)\n43. [FG-NET Facial Aging Database](http://sting.cycollege.ac.cy/~alanitis/fgnetaging/index.htm) - Database contains 1002 face images showing subjects at different ages. (Formats: jpg)\n44. [FVC2000 Fingerprint Databases](http://bias.csr.unibo.it/fvc2000/) - FVC2000 is the First International Competition for Fingerprint Verification Algorithms. Four fingerprint databases constitute the FVC2000 benchmark (3520 fingerprints in all).\n45. [Biometric Systems Lab](http://biolab.csr.unibo.it/home.asp) - University of Bologna\n46. [Face and Gesture images and image sequences](http://www.fg-net.org) - Several image datasets of faces and gestures that are ground truth annotated for benchmarking\n47. [German Fingerspelling Database](http://www-i6.informatik.rwth-aachen.de/~dreuw/database.html) - The database contains 35 gestures and consists of 1400 image sequences that contain gestures of 20 different persons recorded under non-uniform daylight lighting conditions. (Formats: mpg,jpg)  \n48. [Language Processing and Pattern Recognition](http://www-i6.informatik.rwth-aachen.de/)\n50. [Groningen Natural Image Database](http://hlab.phys.rug.nl/archive.html) - 4000+ 1536x1024 (16 bit) calibrated outdoor images (Formats: homebrew)\n51. [ICG Testhouse sequence](http://www.icg.tu-graz.ac.at/~schindler/Data) -  2 turntable sequences from different viewing heights, 36 images each, resolution 1000x750, color (Formats: PPM)\n52. [Institute of Computer Graphics and Vision](http://www.icg.tu-graz.ac.at)\n54. [IEN Image Library](http://www.ien.it/is/vislib/) - 1000+ images, mostly outdoor sequences (Formats: raw, ppm)  \n55. [INRIA''s Syntim images database](http://www-rocq.inria.fr/~tarel/syntim/images.html) - 15 color image of simple objects (Formats: gif)\n56. [INRIA](http://www.inria.fr/)\n57. [INRIA''s Syntim stereo databases](http://www-rocq.inria.fr/~tarel/syntim/paires.html) - 34 calibrated color stereo pairs (Formats: gif)\n58. [Image Analysis Laboratory](http://www.ece.ncsu.edu/imaging/Archives/ImageDataBase/index.html) - Images obtained from a variety of imaging modalities -- raw CFA images, range images and a host of "medical images". (Formats: homebrew)\n59. [Image Analysis Laboratory](http://www.ece.ncsu.edu/imaging)\n61. [Image Database](http://www.prip.tuwien.ac.at/prip/image.html) - An image database including some textures  \n62. [JAFFE Facial Expression Image Database](http://www.mis.atr.co.jp/~mlyons/jaffe.html) - The JAFFE database consists of 213 images of Japanese female subjects posing 6 basic facial expressions as well as a neutral pose. Ratings on emotion adjectives are also available, free of charge, for research purposes. (Formats: TIFF Grayscale images.)\n63. [ATR Research, Kyoto, Japan](http://www.mic.atr.co.jp/)\n64. [JISCT Stereo Evaluation](ftp://ftp.vislist.com/IMAGERY/JISCT/) - 44 image pairs. These data have been used in an evaluation of stereo analysis, as described in the April 1993 ARPA Image Understanding Workshop paper ``The JISCT Stereo Evaluation'''' by R.C.Bolles, H.H.Baker, and M.J.Hannah, 263--274 (Formats: SSI)\n65. [MIT Vision Texture](https://vismod.media.mit.edu/vismod/imagery/VisionTexture/vistex.html) - Image archive (100+ images) (Formats: ppm)\n66. [MIT face images and more](ftp://whitechapel.media.mit.edu/pub/images) - hundreds of images (Formats: homebrew)\n67. [Machine Vision](http://vision.cse.psu.edu/book/testbed/images/) - Images from the textbook by Jain, Kasturi, Schunck (20+ images) (Formats: GIF TIFF)\n68. [Mammography Image Databases](http://marathon.csee.usf.edu/Mammography/Database.html) - 100 or more images of mammograms with ground truth. Additional images available by request, and links to several other mammography databases are provided. (Formats: homebrew)\n69. [ftp://ftp.cps.msu.edu/pub/prip](ftp://ftp.cps.msu.edu/pub/prip) - many images (Formats: unknown)\n70. [Middlebury Stereo Data Sets with Ground Truth](http://www.middlebury.edu/stereo/data.html) - Six multi-frame stereo data sets of scenes containing planar regions. Each data set contains 9 color images and subpixel-accuracy ground-truth data. (Formats: ppm)\n71. [Middlebury Stereo Vision Research Page](http://www.middlebury.edu/stereo) - Middlebury College\n72. [Modis Airborne simulator, Gallery and data set](http://ltpwww.gsfc.nasa.gov/MODIS/MAS/) - High Altitude Imagery from around the world for environmental modeling in support of NASA EOS program (Formats: JPG and HDF)\n73. [NIST Fingerprint and handwriting](ftp://sequoyah.ncsl.nist.gov/pub/databases/data) - datasets - thousands of images (Formats: unknown)\n74. [NIST Fingerprint data](ftp://ftp.cs.columbia.edu/jpeg/other/uuencoded) - compressed multipart uuencoded tar file\n75. [NLM HyperDoc Visible Human Project](http://www.nlm.nih.gov/research/visible/visible_human.html) - Color, CAT and MRI image samples - over 30 images (Formats: jpeg)\n76. [National Design Repository](http://www.designrepository.org) - Over 55,000 3D CAD and solid models of (mostly) mechanical/machined engineering designs. (Formats: gif,vrml,wrl,stp,sat) \n77. [Geometric & Intelligent Computing Laboratory](http://gicl.mcs.drexel.edu)\n79. [OSU (MSU) 3D Object Model Database](http://eewww.eng.ohio-state.edu/~flynn/3DDB/Models/) - several sets of 3D object models collected over several years to use in object recognition research (Formats: homebrew, vrml)\n80. [OSU (MSU/WSU) Range Image Database](http://eewww.eng.ohio-state.edu/~flynn/3DDB/RID/) - Hundreds of real and synthetic images (Formats: gif, homebrew)\n81. [OSU/SAMPL Database: Range Images, 3D Models, Stills, Motion Sequences](http://sampl.eng.ohio-state.edu/~sampl/database.htm) - Over 1000 range images, 3D object models, still images and motion sequences (Formats: gif, ppm, vrml, homebrew)\n82. [Signal Analysis and Machine Perception Laboratory](http://sampl.eng.ohio-state.edu)\n84. [Otago Optical Flow Evaluation Sequences](http://www.cs.otago.ac.nz/research/vision/Research/OpticalFlow/opticalflow.html) - Synthetic and real sequences with machine-readable ground truth optical flow fields, plus tools to generate ground truth for new sequences. (Formats: ppm,tif,homebrew)\n85. [Vision Research Group](http://www.cs.otago.ac.nz/research/vision/index.html)\n87. [ftp://ftp.limsi.fr/pub/quenot/opflow/testdata/piv/](ftp://ftp.limsi.fr/pub/quenot/opflow/testdata/piv/) - Real and synthetic image sequences used for testing a Particle Image Velocimetry application. These images may be used for the test of optical flow and image matching algorithms. (Formats: pgm (raw))\n88. [LIMSI-CNRS/CHM/IMM/vision](http://www.limsi.fr/Recherche/IMM/PageIMM.html)\n89. [LIMSI-CNRS](http://www.limsi.fr/)\n90. [Photometric 3D Surface Texture Database](http://www.taurusstudio.net/research/pmtexdb/index.htm) - This is the first 3D texture database which provides both full real surface rotations and registered photometric stereo data (30 textures, 1680 images). (Formats: TIFF)\n91. [SEQUENCES FOR OPTICAL FLOW ANALYSIS (SOFA)](http://www.cee.hw.ac.uk/~mtc/sofa) - 9 synthetic sequences designed for testing motion analysis applications, including full ground truth of motion and camera parameters. (Formats: gif)\n92. [Computer Vision Group](http://www.cee.hw.ac.uk/~mtc/research.html)\n94. [Sequences for Flow Based Reconstruction](http://www.nada.kth.se/~zucch/CAMERA/PUB/seq.html) - synthetic sequence for testing structure from motion algorithms (Formats: pgm)\n95. [Stereo Images with Ground Truth Disparity and Occlusion](http://www-dbv.cs.uni-bonn.de/stereo_data/) - a small set of synthetic images of a hallway with varying amounts of noise added. Use these images to benchmark your stereo algorithm. (Formats: raw, viff (khoros), or tiff)\n96. [Stuttgart Range Image Database](http://range.informatik.uni-stuttgart.de) - A collection of synthetic range images taken from high-resolution polygonal models available on the web (Formats: homebrew)\n97. [Department Image Understanding](http://www.informatik.uni-stuttgart.de/ipvr/bv/bv_home_engl.html)\n99. [The AR Face Database](http://www2.ece.ohio-state.edu/~aleix/ARdatabase.html) - Contains over 4,000 color images corresponding to 126 people''s faces (70 men and 56 women). Frontal views with variations in facial expressions, illumination, and occlusions. (Formats: RAW (RGB 24-bit))\n100. [Purdue Robot Vision Lab](http://rvl.www.ecn.purdue.edu/RVL/)\n101. [The MIT-CSAIL Database of Objects and Scenes](http://web.mit.edu/torralba/www/database.html) - Database for testing multiclass object detection and scene recognition algorithms. Over 72,000 images with 2873 annotated frames. More than 50 annotated object classes. (Formats: jpg)\n102. [The RVL SPEC-DB (SPECularity DataBase)](http://rvl1.ecn.purdue.edu/RVL/specularity_database/) - A collection of over 300 real images of 100 objects taken under three different illuminaiton conditions (Diffuse/Ambient/Directed). -- Use these images to test algorithms for detecting and compensating specular highlights in color images. (Formats: TIFF )\n103. [Robot Vision Laboratory](http://rvl1.ecn.purdue.edu/RVL/)\n105. [The Xm2vts database](http://xm2vtsdb.ee.surrey.ac.uk) - The XM2VTSDB contains four digital recordings of 295 people taken over a period of four months. This database contains both image and video data of faces.\n106. [Centre for Vision, Speech and Signal Processing](http://www.ee.surrey.ac.uk/Research/CVSSP)\n107. [Traffic Image Sequences and ''Marbled Block'' Sequence](http://i21www.ira.uka.de/image_sequences) - thousands of frames of digitized traffic image sequences as well as the ''Marbled Block'' sequence (grayscale images) (Formats: GIF)\n108. [IAKS/KOGS](http://i21www.ira.uka.de)\n110. [U Bern Face images](ftp://ftp.iam.unibe.ch/pub/Images/FaceImages) - hundreds of images (Formats: Sun rasterfile)\n111. [U Michigan textures](ftp://freebie.engin.umich.edu/pub/misc/textures) (Formats: compressed raw)\n112. [U Oulu wood and knots database](http://www.ee.oulu.fi/~olli/Projects/Lumber.Grading.html) - Includes classifications - 1000+ color images (Formats: ppm)\n113. [UCID - an Uncompressed Colour Image Database](http://vision.doc.ntu.ac.uk/datasets/UCID/ucid.html) - a benchmark database for image retrieval with predefined ground truth. (Formats: tiff)\n115. [UMass Vision Image Archive](http://vis-www.cs.umass.edu/~vislib/) - Large image database with aerial, space, stereo, medical images and more. (Formats: homebrew)\n116. [UNC''s 3D image database](ftp://sunsite.unc.edu/pub/academic/computer-science/virtual-reality/3d) - many images (Formats: GIF)\n117. [USF Range Image Data with Segmentation Ground Truth](http://marathon.csee.usf.edu/range/seg-comp/SegComp.html) - 80 image sets (Formats: Sun rasterimage)\n118. [University of Oulu Physics-based Face Database](http://www.ee.oulu.fi/research/imag/color/pbfd.html) - contains color images of faces under different illuminants and camera calibration conditions as well as skin spectral reflectance measurements of each person.\n119. [Machine Vision and Media Processing Unit](http://www.ee.oulu.fi/mvmp/)\n121. [University of Oulu Texture Database](http://www.outex.oulu.fi) - Database of 320 surface textures, each captured under three illuminants, six spatial resolutions and nine rotation angles. A set of test suites is also provided so that texture segmentation, classification, and retrieval algorithms can be tested in a standard manner. (Formats: bmp, ras, xv)\n122. [Machine Vision Group](http://www.ee.oulu.fi/mvg)\n124. [Usenix face database](ftp://ftp.uu.net/published/usenix/faces) - Thousands of face images from many different sites (circa 994)\n125. [View Sphere Database](http://www-prima.inrialpes.fr/Prima/hall/view_sphere.html) - Images of 8 objects seen from many different view points. The view sphere is sampled using a geodesic with 172 images/sphere. Two sets for training and testing are available. (Formats: ppm)\n126. [PRIMA, GRAVIR](http://www-prima.inrialpes.fr/Prima/)\n127. [Vision-list Imagery Archive](ftp://ftp.vislist.com/IMAGERY/) - Many images, many formats\n128. [Wiry Object Recognition Database](http://www.cs.cmu.edu/~owenc/word.htm) - Thousands of images of a cart, ladder, stool, bicycle, chairs, and cluttered scenes with ground truth labelings of edges and regions. (Formats: jpg)\n129. [3D Vision Group](http://www.cs.cmu.edu/0.000000E+003dvision/)\n131. [Yale Face Database](http://cvc.yale.edu/projects/yalefaces/yalefaces.html) -  165 images (15 individuals) with different lighting, expression, and occlusion configurations.\n132. [Yale Face Database B](http://cvc.yale.edu/projects/yalefacesB/yalefacesB.html) - 5760 single light source images of 10 subjects each seen under 576 viewing conditions (9 poses x 64 illumination conditions). (Formats: PGM)\n133. [Center for Computational Vision and Control](http://cvc.yale.edu/)\n134. [DeepMind QA Corpus](https://github.com/deepmind/rc-data) - Textual QA corpus from CNN and DailyMail. More than 300K documents in total. [Paper](http://arxiv.org/abs/1506.03340) for reference.\n135. [YouTube-8M Dataset](https://research.google.com/youtube8m/) - YouTube-8M is a large-scale labeled video dataset that consists of 8 million YouTube video IDs and associated labels from a diverse vocabulary of 4800 visual entities.\n136. [Open Images dataset](https://github.com/openimages/dataset) - Open Images is a dataset of ~9 million URLs to images that have been annotated with labels spanning over 6000 categories.\n137. [Visual Object Classes Challenge 2012 (VOC2012)](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit) - VOC2012 dataset containing 12k images with 20 annotated classes for object detection and segmentation.\n138. [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) - MNIST like fashion product dataset consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n139. [Large-scale Fashion (DeepFashion) Database](http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html) - Contains over 800,000 diverse fashion images.  Each image in this dataset is labeled with 50 categories, 1,000 descriptive attributes, bounding box and clothing landmarks\n140. [FakeNewsCorpus](https://github.com/several27/FakeNewsCorpus) - Contains about 10 million news articles classified using [opensources.co](http://opensources.co) types\n141. [LLVIP](https://github.com/bupt-ai-cz/LLVIP) - 15488 visible-infrared paired images (30976 images) for low-light vision research, [Project_Page](https://bupt-ai-cz.github.io/LLVIP/)\n142. [MSDA](https://github.com/bupt-ai-cz/Meta-SelfLearning) - Over over 5 million images from 5 different domains for multi-source ocr/text recognition DA research, [Project_Page](https://bupt-ai-cz.github.io/Meta-SelfLearning/)\n143. [SANAD: Single-Label Arabic News Articles Dataset for Automatic Text Categorization](https://data.mendeley.com/datasets/57zpx667y9/2) - SANAD Dataset is a large collection of Arabic news articles that can be used in different Arabic NLP tasks such as Text Classification and Word Embedding. The articles were collected using Python scripts written specifically for three popular news websites: AlKhaleej, AlArabiya and Akhbarona. \n144. [Referit3D](https://referit3d.github.io) - Two large-scale and complementary visio-linguistic datasets (aka Nr3D and Sr3D) for identifying fine-grained 3D objects in ScanNet scenes. Nr3D contains 41.5K natural, free-form utterances, and Sr3d contains 83.5K template-based utterances.\n145. [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) - Stanford released ~100,000 English QA pairs and ~50,000 unanswerable questions\n146. [FQuAD](https://fquad.illuin.tech/) - ~25,000 French QA pairs released by Illuin Technology\n147. [GermanQuAD and GermanDPR](https://www.deepset.ai/germanquad) - deepset released ~14,000 German QA pairs\n148. [SberQuAD](https://github.com/annnyway/QA-for-Russian) - Sberbank released ~90,000 Russian QA pairs\n149. [ArtEmis](http://artemisdataset.org/) - Contains 450K affective annotations of emotional responses and linguistic explanations for 80,000 artworks of WikiArt.\n\n### Conferences\n\n1. [CVPR - IEEE Conference on Computer Vision and Pattern Recognition](http://cvpr2018.thecvf.com)\n2. [AAMAS - International Joint Conference on Autonomous Agents and Multiagent Systems](http://celweb.vuse.vanderbilt.edu/aamas18/)\n3. [IJCAI - 	International Joint Conference on Artificial Intelligence](https://www.ijcai-18.org/)\n4. [ICML - 	International Conference on Machine Learning](https://icml.cc)\n5. [ECML - European Conference on Machine Learning](http://www.ecmlpkdd2018.org)\n6. [KDD - Knowledge Discovery and Data Mining](http://www.kdd.org/kdd2018/)\n7. [NIPS - Neural Information Processing Systems](https://nips.cc/Conferences/2018)\n8. [O''Reilly AI Conference - 	O''Reilly Artificial Intelligence Conference](https://conferences.oreilly.com/artificial-intelligence/ai-ny)\n9. [ICDM - International Conference on Data Mining](https://www.waset.org/conference/2018/07/istanbul/ICDM)\n10. [ICCV - International Conference on Computer Vision](http://iccv2017.thecvf.com)\n11. [AAAI - Association for the Advancement of Artificial Intelligence](https://www.aaai.org)\n12. [MAIS - Montreal AI Symposium](https://montrealaisymposium.wordpress.com/)\n\n### Frameworks\n\n1.  [Caffe](http://caffe.berkeleyvision.org/)  \n2.  [Torch7](http://torch.ch/)\n3.  [Theano](http://deeplearning.net/software/theano/)\n4.  [cuda-convnet](https://code.google.com/p/cuda-convnet2/)\n5.  [convetjs](https://github.com/karpathy/convnetjs)\n5.  [Ccv](http://libccv.org/doc/doc-convnet/)\n6.  [NuPIC](http://numenta.org/nupic.html)\n7.  [DeepLearning4J](http://deeplearning4j.org/)\n8.  [Brain](https://github.com/harthur/brain)\n9.  [DeepLearnToolbox](https://github.com/rasmusbergpalm/DeepLearnToolbox)\n10.  [Deepnet](https://github.com/nitishsrivastava/deepnet)\n11.  [Deeppy](https://github.com/andersbll/deeppy)\n12.  [JavaNN](https://github.com/ivan-vasilev/neuralnetworks)\n13.  [hebel](https://github.com/hannes-brt/hebel)\n14.  [Mocha.jl](https://github.com/pluskid/Mocha.jl)\n15.  [OpenDL](https://github.com/guoding83128/OpenDL)\n16.  [cuDNN](https://developer.nvidia.com/cuDNN)\n17.  [MGL](http://melisgl.github.io/mgl-pax-world/mgl-manual.html)\n18.  [Knet.jl](https://github.com/denizyuret/Knet.jl)\n19.  [Nvidia DIGITS - a web app based on Caffe](https://github.com/NVIDIA/DIGITS)\n20.  [Neon - Python based Deep Learning Framework](https://github.com/NervanaSystems/neon)\n21.  [Keras - Theano based Deep Learning Library](http://keras.io)\n22.  [Chainer - A flexible framework of neural networks for deep learning](http://chainer.org/)\n23.  [RNNLM Toolkit](http://rnnlm.org/)\n24.  [RNNLIB - A recurrent neural network library](http://sourceforge.net/p/rnnl/wiki/Home/)\n25.  [char-rnn](https://github.com/karpathy/char-rnn)\n26.  [MatConvNet: CNNs for MATLAB](https://github.com/vlfeat/matconvnet)\n27.  [Minerva - a fast and flexible tool for deep learning on multi-GPU](https://github.com/dmlc/minerva)\n28.  [Brainstorm - Fast, flexible and fun neural networks.](https://github.com/IDSIA/brainstorm)\n29.  [Tensorflow - Open source software library for numerical computation using data flow graphs](https://github.com/tensorflow/tensorflow)\n30.  [DMTK - Microsoft Distributed Machine Learning Tookit](https://github.com/Microsoft/DMTK)\n31.  [Scikit Flow - Simplified interface for TensorFlow (mimicking Scikit Learn)](https://github.com/google/skflow)\n32.  [MXnet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning framework](https://github.com/apache/incubator-mxnet)\n33.  [Veles - Samsung Distributed machine learning platform](https://github.com/Samsung/veles)\n34.  [Marvin - A Minimalist GPU-only N-Dimensional ConvNets Framework](https://github.com/PrincetonVision/marvin)\n35.  [Apache SINGA - A General Distributed Deep Learning Platform](http://singa.incubator.apache.org/)\n36.  [DSSTNE - Amazon''s library for building Deep Learning models](https://github.com/amznlabs/amazon-dsstne)\n37.  [SyntaxNet - Google''s syntactic parser - A TensorFlow dependency library](https://github.com/tensorflow/models/tree/master/syntaxnet)\n38.  [mlpack - A scalable Machine Learning library](http://mlpack.org/)\n39.  [Torchnet - Torch based Deep Learning Library](https://github.com/torchnet/torchnet)\n40.  [Paddle - PArallel Distributed Deep LEarning by Baidu](https://github.com/baidu/paddle)\n41.  [NeuPy - Theano based Python library for ANN and Deep Learning](http://neupy.com)\n42.  [Lasagne - a lightweight library to build and train neural networks in Theano](https://github.com/Lasagne/Lasagne)\n43.  [nolearn - wrappers and abstractions around existing neural network libraries, most notably Lasagne](https://github.com/dnouri/nolearn)\n44.  [Sonnet - a library for constructing neural networks by Google''s DeepMind](https://github.com/deepmind/sonnet)\n45.  [PyTorch - Tensors and Dynamic neural networks in Python with strong GPU acceleration](https://github.com/pytorch/pytorch)\n46.  [CNTK - Microsoft Cognitive Toolkit](https://github.com/Microsoft/CNTK)\n47.  [Serpent.AI - Game agent framework: Use any video game as a deep learning sandbox](https://github.com/SerpentAI/SerpentAI)\n48.  [Caffe2 - A New Lightweight, Modular, and Scalable Deep Learning Framework](https://github.com/caffe2/caffe2)\n49.  [deeplearn.js - Hardware-accelerated deep learning and linear algebra (NumPy) library for the web](https://github.com/PAIR-code/deeplearnjs)\n50.  [TVM - End to End Deep Learning Compiler Stack for CPUs, GPUs and specialized accelerators](https://tvm.ai/)\n51.  [Coach - Reinforcement Learning Coach by IntelÂ® AI Lab](https://github.com/NervanaSystems/coach)\n52.  [albumentations - A fast and framework agnostic image augmentation library](https://github.com/albu/albumentations)\n53.  [Neuraxle - A general-purpose ML pipelining framework](https://github.com/Neuraxio/Neuraxle)\n54.  [Catalyst: High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing](https://github.com/catalyst-team/catalyst)\n55.  [garage - A toolkit for reproducible reinforcement learning research](https://github.com/rlworkgroup/garage)\n56.  [Detecto - Train and run object detection models with 5-10 lines of code](https://github.com/alankbi/detecto)\n57.  [Karate Club - An unsupervised machine learning library for graph structured data](https://github.com/benedekrozemberczki/karateclub)\n58.  [Synapses - A lightweight library for neural networks that runs anywhere](https://github.com/mrdimosthenis/Synapses)\n59.  [TensorForce - A TensorFlow library for applied reinforcement learning](https://github.com/reinforceio/tensorforce)\n60.  [Hopsworks - A Feature Store for ML and Data-Intensive AI](https://github.com/logicalclocks/hopsworks)\n61.  [Feast - A Feature Store for ML for GCP by Gojek/Google](https://github.com/gojek/feast)\n62.  [PyTorch Geometric Temporal - Representation learning on dynamic graphs](https://github.com/gojek/feast)\n63.  [lightly - A computer vision framework for self-supervised learning](https://github.com/lightly-ai/lightly)\n64.  [Trax â€” Deep Learning with Clear Code and Speed](https://github.com/google/trax)\n65.  [Flax - a neural network ecosystem for JAX that is designed for flexibility](https://github.com/google/flax)\n66.  [QuickVision](https://github.com/Quick-AI/quickvision)\n67.  [Colossal-AI - An Integrated Large-scale Model Training System with Efficient Parallelization Techniques](https://github.com/hpcaitech/ColossalAI)\n68.  [haystack: an open-source neural search framework](https://haystack.deepset.ai/docs/intromd)\n69.  [Maze](https://github.com/enlite-ai/maze) - Application-oriented deep reinforcement learning framework addressing real-world decision problems.\n70.  [InsNet - A neural network library for building instance-dependent NLP models with padding-free dynamic batching](https://github.com/chncwang/InsNet)\n\n### Tools\n\n1.  [Nebullvm](https://github.com/nebuly-ai/nebullvm) - Easy-to-use library to boost deep learning inference leveraging multiple deep learning compilers.\n2.  [Netron](https://github.com/lutzroeder/netron) - Visualizer for deep learning and machine learning models\n2.  [Jupyter Notebook](http://jupyter.org) - Web-based notebook environment for interactive computing\n3.  [TensorBoard](https://github.com/tensorflow/tensorboard) - TensorFlow''s Visualization Toolkit\n4.  [Visual Studio Tools for AI](https://www.microsoft.com/en-us/research/project/visual-studio-code-tools-ai/) - Develop, debug and deploy deep learning and AI solutions\n5.  [TensorWatch](https://github.com/microsoft/tensorwatch) - Debugging and visualization for deep learning\n6. [ML Workspace](https://github.com/ml-tooling/ml-workspace) - All-in-one web-based IDE for machine learning and data science.\n7.  [dowel](https://github.com/rlworkgroup/dowel) - A little logger for machine learning research. Log any object to the console, CSVs, TensorBoard, text log files, and more with just one call to `logger.log()`\n8.  [Neptune](https://neptune.ai/) - Lightweight tool for experiment tracking and results visualization. \n9.  [CatalyzeX](https://chrome.google.com/webstore/detail/code-finder-for-research/aikkeehnlfpamidigaffhfmgbkdeheil) - Browser extension ([Chrome](https://chrome.google.com/webstore/detail/code-finder-for-research/aikkeehnlfpamidigaffhfmgbkdeheil) and [Firefox](https://addons.mozilla.org/en-US/firefox/addon/code-finder-catalyzex/)) that automatically finds and links to code implementations for ML papers anywhere online: Google, Twitter, Arxiv, Scholar, etc.\n10. [Determined](https://github.com/determined-ai/determined) - Deep learning training platform with integrated support for distributed training, hyperparameter tuning, smart GPU scheduling, experiment tracking, and a model registry.\n11. [DAGsHub](https://dagshub.com/) - Community platform for Open Source ML â€“ Manage experiments, data & models and create collaborative ML projects easily.\n12. [hub](https://github.com/activeloopai/Hub) - Fastest unstructured dataset management for TensorFlow/PyTorch by activeloop.ai. Stream & version-control data. Converts large data into single     numpy-like array on the cloud, accessible on any machine.\n13. [DVC](https://dvc.org/) - DVC is built to make ML models shareable and reproducible. It is designed to handle large files, data sets, machine learning models, and metrics as well as code.\n14. [CML](https://cml.dev/) - CML helps you bring your favorite DevOps tools to machine learning.\n15. [MLEM](https://mlem.ai/) - MLEM is a tool to easily package, deploy and serve Machine Learning models. It seamlessly supports a variety of scenarios like real-time serving and batch processing.\n16. [Maxim AI](https://getmaxim.ai) - Tool for AI Agent Simulation, Evaluation & Observability.\n\n\n### Miscellaneous\n\n1.  [Caffe Webinar](http://on-demand-gtc.gputechconf.com/gtcnew/on-demand-gtc.php?searchByKeyword=shelhamer&amp;searchItems=&amp;sessionTopic=&amp;sessionEvent=4&amp;sessionYear=2014&amp;sessionFormat=&amp;submit=&amp;select=+)\n2.  [100 Best Github Resources in Github for DL](http://meta-guide.com/software-meta-guide/100-best-github-deep-learning/)\n3.  [Word2Vec](https://code.google.com/p/word2vec/)\n4.  [Caffe DockerFile](https://github.com/tleyden/docker/tree/master/caffe)\n5.  [TorontoDeepLEarning convnet](https://github.com/TorontoDeepLearning/convnet)\n6.  [gfx.js](https://github.com/clementfarabet/gfx.js)\n7.  [Torch7 Cheat sheet](https://github.com/torch/torch7/wiki/Cheatsheet)\n8. [Misc from MIT''s ''Advanced Natural Language Processing'' course](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-864-advanced-natural-language-processing-fall-2005/)\n9. [Misc from MIT''s ''Machine Learning'' course](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/)\n10. [Misc from MIT''s ''Networks for Learning: Regression and Classification'' course](http://ocw.mit.edu/courses/brain-and-cognitive-sciences/9-520-a-networks-for-learning-regression-and-classification-spring-2001/)\n11. [Misc from MIT''s ''Neural Coding and Perception of Sound'' course](http://ocw.mit.edu/courses/health-sciences-and-technology/hst-723j-neural-coding-and-perception-of-sound-spring-2005/index.htm)\n12. [Implementing a Distributed Deep Learning Network over Spark](http://www.datasciencecentral.com/profiles/blogs/implementing-a-distributed-deep-learning-network-over-spark)\n13. [A chess AI that learns to play chess using deep learning.](https://github.com/erikbern/deep-pink)\n14. [Reproducing the results of "Playing Atari with Deep Reinforcement Learning" by DeepMind](https://github.com/kristjankorjus/Replicating-DeepMind)\n15. [Wiki2Vec. Getting Word2vec vectors for entities and word from Wikipedia Dumps](https://github.com/idio/wiki2vec)\n16. [The original code from the DeepMind article + tweaks](https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner)\n17. [Google deepdream - Neural Network art](https://github.com/google/deepdream)\n18. [An efficient, batched LSTM.](https://gist.github.com/karpathy/587454dc0146a6ae21fc)\n19. [A recurrent neural network designed to generate classical music.](https://github.com/hexahedria/biaxial-rnn-music-composition)\n20. [Memory Networks Implementations - Facebook](https://github.com/facebook/MemNN)\n21. [Face recognition with Google''s FaceNet deep neural network.](https://github.com/cmusatyalab/openface)\n22. [Basic digit recognition neural network](https://github.com/joeledenberg/DigitRecognition)\n23. [Emotion Recognition API Demo - Microsoft](https://www.projectoxford.ai/demo/emotion#detection)\n24. [Proof of concept for loading Caffe models in TensorFlow](https://github.com/ethereon/caffe-tensorflow)\n25. [YOLO: Real-Time Object Detection](http://pjreddie.com/darknet/yolo/#webcam)\n26. [YOLO: Practical Implementation using Python](https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/)\n27. [AlphaGo - A replication of DeepMind''s 2016 Nature publication, "Mastering the game of Go with deep neural networks and tree search"](https://github.com/Rochester-NRT/AlphaGo)\n28. [Machine Learning for Software Engineers](https://github.com/ZuzooVn/machine-learning-for-software-engineers)\n29. [Machine Learning is Fun!](https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471#.oa4rzez3g)\n30. [Siraj Raval''s Deep Learning tutorials](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A)\n31. [Dockerface](https://github.com/natanielruiz/dockerface) - Easy to install and use deep learning Faster R-CNN face detection for images and video in a docker container.\n32. [Awesome Deep Learning Music](https://github.com/ybayle/awesome-deep-learning-music) - Curated list of articles related to deep learning scientific research applied to music\n33. [Awesome Graph Embedding](https://github.com/benedekrozemberczki/awesome-graph-embedding) - Curated list of articles related to deep learning scientific research on graph structured data at the graph level.\n34. [Awesome Network Embedding](https://github.com/chihming/awesome-network-embedding) - Curated list of articles related to deep learning scientific research on graph structured data at the node level.\n35. [Microsoft Recommenders](https://github.com/Microsoft/Recommenders) contains examples, utilities and best practices for building recommendation systems. Implementations of several state-of-the-art algorithms are provided for self-study and customization in your own applications.\n36. [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) - Andrej Karpathy blog post about using RNN for generating text.\n37. [Ladder Network](https://github.com/divamgupta/ladder_network_keras) - Keras Implementation of Ladder Network for Semi-Supervised Learning \n38. [toolbox: Curated list of ML libraries](https://github.com/amitness/toolbox)\n39. [CNN Explainer](https://poloclub.github.io/cnn-explainer/)\n40. [AI Expert Roadmap](https://github.com/AMAI-GmbH/AI-Expert-Roadmap) - Roadmap to becoming an Artificial Intelligence Expert\n41. [Awesome Drug Interactions, Synergy, and Polypharmacy Prediction](https://github.com/AstraZeneca/awesome-polipharmacy-side-effect-prediction/)\n\n-----\n### Contributing\nHave anything in mind that you think is awesome and would fit in this list? Feel free to send a [pull request](https://github.com/ashara12/awesome-deeplearning/pulls).\n\n-----\n## License\n\n[![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)\n\nTo the extent possible under law, [Christos Christofidis](https://linkedin.com/in/Christofidis) has waived all copyright and related or neighboring rights to this work.\n', '{"language":null,"stars":26951,"forks":6231,"watchers":26951,"open_issues":45,"topics":["awesome","awesome-list","deep-learning","deep-learning-tutorial","deep-networks","face-images","machine-learning","neural-network","recurrent-networks"],"default_branch":"master","size_kb":607,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:sindresorhus:awesome","source_url":"https://github.com/sindresorhus/awesome"},{"type":"has_code","target_id":"github:karpathy:neuraltalk","source_url":"https://github.com/karpathy/neuraltalk"},{"type":"has_code","target_id":"github:terryum:awesome-deep-learning-papers","source_url":"https://github.com/terryum/awesome-deep-learning-papers"},{"type":"has_code","target_id":"github:clementfarabet:ipam-tutorials","source_url":"https://github.com/clementfarabet/ipam-tutorials"},{"type":"has_code","target_id":"github:josephmisiti:machine-learning-module","source_url":"https://github.com/josephmisiti/machine-learning-module"},{"type":"has_code","target_id":"github:nlintz:TensorFlow-Tutorials","source_url":"https://github.com/nlintz/TensorFlow-Tutorials"},{"type":"has_code","target_id":"github:pkmital:tensorflow_tutorials","source_url":"https://github.com/pkmital/tensorflow_tutorials"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:Vict0rSch:deep_learning","source_url":"https://github.com/Vict0rSch/deep_learning"},{"type":"has_code","target_id":"github:guillaume-chevalier:LSTM-Human-Activity-Recognition","source_url":"https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition"},{"type":"has_code","target_id":"github:astorfi:TensorFlow-World","source_url":"https://github.com/astorfi/TensorFlow-World"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:MelAbgrall:HardwareforAI","source_url":"https://github.com/MelAbgrall/HardwareforAI"},{"type":"has_code","target_id":"github:SauravMaheshkar:Trax-Examples","source_url":"https://github.com/SauravMaheshkar/Trax-Examples"},{"type":"has_code","target_id":"github:SauravMaheshkar:Trax-Examples","source_url":"https://github.com/SauravMaheshkar/Trax-Examples"},{"type":"has_code","target_id":"github:SauravMaheshkar:Trax-Examples","source_url":"https://github.com/SauravMaheshkar/Trax-Examples"},{"type":"has_code","target_id":"github:SauravMaheshkar:Flax-Examples","source_url":"https://github.com/SauravMaheshkar/Flax-Examples"},{"type":"has_code","target_id":"github:SauravMaheshkar:Fake-News-Classification","source_url":"https://github.com/SauravMaheshkar/Fake-News-Classification"},{"type":"has_code","target_id":"github:SauravMaheshkar:CoxPH-Model-for-Primary-Biliary-Cirrhosis","source_url":"https://github.com/SauravMaheshkar/CoxPH-Model-for-Primary-Biliary-Cirrhosis"},{"type":"has_code","target_id":"github:SauravMaheshkar:Cross-Matching-Methods-for-Astronomical-Catalogs","source_url":"https://github.com/SauravMaheshkar/Cross-Matching-Methods-for-Astronomical-Catalogs"},{"type":"has_code","target_id":"github:SauravMaheshkar:Named-Entity-Recognition-","source_url":"https://github.com/SauravMaheshkar/Named-Entity-Recognition-"},{"type":"has_code","target_id":"github:SauravMaheshkar:Flutter_Image-Recognition","source_url":"https://github.com/SauravMaheshkar/Flutter_Image-Recognition"},{"type":"has_code","target_id":"github:deepmind:rc-data","source_url":"https://github.com/deepmind/rc-data"},{"type":"has_code","target_id":"github:openimages:dataset","source_url":"https://github.com/openimages/dataset"},{"type":"has_code","target_id":"github:zalandoresearch:fashion-mnist","source_url":"https://github.com/zalandoresearch/fashion-mnist"},{"type":"has_code","target_id":"github:several27:FakeNewsCorpus","source_url":"https://github.com/several27/FakeNewsCorpus"},{"type":"has_code","target_id":"github:bupt-ai-cz:LLVIP","source_url":"https://github.com/bupt-ai-cz/LLVIP"},{"type":"has_code","target_id":"github:bupt-ai-cz:Meta-SelfLearning","source_url":"https://github.com/bupt-ai-cz/Meta-SelfLearning"},{"type":"has_code","target_id":"github:annnyway:QA-for-Russian","source_url":"https://github.com/annnyway/QA-for-Russian"},{"type":"has_code","target_id":"github:karpathy:convnetjs","source_url":"https://github.com/karpathy/convnetjs"},{"type":"has_code","target_id":"github:harthur:brain","source_url":"https://github.com/harthur/brain"},{"type":"has_code","target_id":"github:rasmusbergpalm:DeepLearnToolbox","source_url":"https://github.com/rasmusbergpalm/DeepLearnToolbox"},{"type":"has_code","target_id":"github:nitishsrivastava:deepnet","source_url":"https://github.com/nitishsrivastava/deepnet"},{"type":"has_code","target_id":"github:andersbll:deeppy","source_url":"https://github.com/andersbll/deeppy"},{"type":"has_code","target_id":"github:ivan-vasilev:neuralnetworks","source_url":"https://github.com/ivan-vasilev/neuralnetworks"},{"type":"has_code","target_id":"github:hannes-brt:hebel","source_url":"https://github.com/hannes-brt/hebel"},{"type":"has_code","target_id":"github:pluskid:Mocha.jl","source_url":"https://github.com/pluskid/Mocha.jl"},{"type":"has_code","target_id":"github:guoding83128:OpenDL","source_url":"https://github.com/guoding83128/OpenDL"},{"type":"has_code","target_id":"github:denizyuret:Knet.jl","source_url":"https://github.com/denizyuret/Knet.jl"},{"type":"has_code","target_id":"github:NVIDIA:DIGITS","source_url":"https://github.com/NVIDIA/DIGITS"},{"type":"has_code","target_id":"github:NervanaSystems:neon","source_url":"https://github.com/NervanaSystems/neon"},{"type":"has_code","target_id":"github:karpathy:char-rnn","source_url":"https://github.com/karpathy/char-rnn"},{"type":"has_code","target_id":"github:vlfeat:matconvnet","source_url":"https://github.com/vlfeat/matconvnet"},{"type":"has_code","target_id":"github:dmlc:minerva","source_url":"https://github.com/dmlc/minerva"},{"type":"has_code","target_id":"github:IDSIA:brainstorm","source_url":"https://github.com/IDSIA/brainstorm"},{"type":"has_code","target_id":"github:tensorflow:tensorflow","source_url":"https://github.com/tensorflow/tensorflow"},{"type":"has_code","target_id":"github:Microsoft:DMTK","source_url":"https://github.com/Microsoft/DMTK"},{"type":"has_code","target_id":"github:google:skflow","source_url":"https://github.com/google/skflow"},{"type":"has_code","target_id":"github:apache:incubator-mxnet","source_url":"https://github.com/apache/incubator-mxnet"},{"type":"has_code","target_id":"github:Samsung:veles","source_url":"https://github.com/Samsung/veles"},{"type":"has_code","target_id":"github:PrincetonVision:marvin","source_url":"https://github.com/PrincetonVision/marvin"},{"type":"has_code","target_id":"github:amznlabs:amazon-dsstne","source_url":"https://github.com/amznlabs/amazon-dsstne"},{"type":"has_code","target_id":"github:tensorflow:models","source_url":"https://github.com/tensorflow/models"},{"type":"has_code","target_id":"github:torchnet:torchnet","source_url":"https://github.com/torchnet/torchnet"},{"type":"has_code","target_id":"github:baidu:paddle","source_url":"https://github.com/baidu/paddle"},{"type":"has_code","target_id":"github:Lasagne:Lasagne","source_url":"https://github.com/Lasagne/Lasagne"},{"type":"has_code","target_id":"github:dnouri:nolearn","source_url":"https://github.com/dnouri/nolearn"},{"type":"has_code","target_id":"github:deepmind:sonnet","source_url":"https://github.com/deepmind/sonnet"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:Microsoft:CNTK","source_url":"https://github.com/Microsoft/CNTK"},{"type":"has_code","target_id":"github:SerpentAI:SerpentAI","source_url":"https://github.com/SerpentAI/SerpentAI"},{"type":"has_code","target_id":"github:caffe2:caffe2","source_url":"https://github.com/caffe2/caffe2"},{"type":"has_code","target_id":"github:PAIR-code:deeplearnjs","source_url":"https://github.com/PAIR-code/deeplearnjs"},{"type":"has_code","target_id":"github:NervanaSystems:coach","source_url":"https://github.com/NervanaSystems/coach"},{"type":"has_code","target_id":"github:albu:albumentations","source_url":"https://github.com/albu/albumentations"},{"type":"has_code","target_id":"github:Neuraxio:Neuraxle","source_url":"https://github.com/Neuraxio/Neuraxle"},{"type":"has_code","target_id":"github:catalyst-team:catalyst","source_url":"https://github.com/catalyst-team/catalyst"},{"type":"has_code","target_id":"github:rlworkgroup:garage","source_url":"https://github.com/rlworkgroup/garage"},{"type":"has_code","target_id":"github:alankbi:detecto","source_url":"https://github.com/alankbi/detecto"},{"type":"has_code","target_id":"github:benedekrozemberczki:karateclub","source_url":"https://github.com/benedekrozemberczki/karateclub"},{"type":"has_code","target_id":"github:mrdimosthenis:Synapses","source_url":"https://github.com/mrdimosthenis/Synapses"},{"type":"has_code","target_id":"github:reinforceio:tensorforce","source_url":"https://github.com/reinforceio/tensorforce"},{"type":"has_code","target_id":"github:logicalclocks:hopsworks","source_url":"https://github.com/logicalclocks/hopsworks"},{"type":"has_code","target_id":"github:gojek:feast","source_url":"https://github.com/gojek/feast"},{"type":"has_code","target_id":"github:gojek:feast","source_url":"https://github.com/gojek/feast"},{"type":"has_code","target_id":"github:lightly-ai:lightly","source_url":"https://github.com/lightly-ai/lightly"},{"type":"has_code","target_id":"github:google:trax","source_url":"https://github.com/google/trax"},{"type":"has_code","target_id":"github:google:flax","source_url":"https://github.com/google/flax"},{"type":"has_code","target_id":"github:Quick-AI:quickvision","source_url":"https://github.com/Quick-AI/quickvision"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:enlite-ai:maze","source_url":"https://github.com/enlite-ai/maze"},{"type":"has_code","target_id":"github:chncwang:InsNet","source_url":"https://github.com/chncwang/InsNet"},{"type":"has_code","target_id":"github:nebuly-ai:nebullvm","source_url":"https://github.com/nebuly-ai/nebullvm"},{"type":"has_code","target_id":"github:lutzroeder:netron","source_url":"https://github.com/lutzroeder/netron"},{"type":"has_code","target_id":"github:tensorflow:tensorboard","source_url":"https://github.com/tensorflow/tensorboard"},{"type":"has_code","target_id":"github:microsoft:tensorwatch","source_url":"https://github.com/microsoft/tensorwatch"},{"type":"has_code","target_id":"github:ml-tooling:ml-workspace","source_url":"https://github.com/ml-tooling/ml-workspace"},{"type":"has_code","target_id":"github:rlworkgroup:dowel","source_url":"https://github.com/rlworkgroup/dowel"},{"type":"has_code","target_id":"github:determined-ai:determined","source_url":"https://github.com/determined-ai/determined"},{"type":"has_code","target_id":"github:activeloopai:Hub","source_url":"https://github.com/activeloopai/Hub"},{"type":"has_code","target_id":"github:tleyden:docker","source_url":"https://github.com/tleyden/docker"},{"type":"has_code","target_id":"github:TorontoDeepLearning:convnet","source_url":"https://github.com/TorontoDeepLearning/convnet"},{"type":"has_code","target_id":"github:clementfarabet:gfx.js","source_url":"https://github.com/clementfarabet/gfx.js"},{"type":"has_code","target_id":"github:torch:torch7","source_url":"https://github.com/torch/torch7"},{"type":"has_code","target_id":"github:erikbern:deep-pink","source_url":"https://github.com/erikbern/deep-pink"},{"type":"has_code","target_id":"github:kristjankorjus:Replicating-DeepMind","source_url":"https://github.com/kristjankorjus/Replicating-DeepMind"},{"type":"has_code","target_id":"github:idio:wiki2vec","source_url":"https://github.com/idio/wiki2vec"},{"type":"has_code","target_id":"github:kuz:DeepMind-Atari-Deep-Q-Learner","source_url":"https://github.com/kuz/DeepMind-Atari-Deep-Q-Learner"},{"type":"has_code","target_id":"github:google:deepdream","source_url":"https://github.com/google/deepdream"},{"type":"has_code","target_id":"github:hexahedria:biaxial-rnn-music-composition","source_url":"https://github.com/hexahedria/biaxial-rnn-music-composition"},{"type":"has_code","target_id":"github:facebook:MemNN","source_url":"https://github.com/facebook/MemNN"},{"type":"has_code","target_id":"github:cmusatyalab:openface","source_url":"https://github.com/cmusatyalab/openface"},{"type":"has_code","target_id":"github:joeledenberg:DigitRecognition","source_url":"https://github.com/joeledenberg/DigitRecognition"},{"type":"has_code","target_id":"github:ethereon:caffe-tensorflow","source_url":"https://github.com/ethereon/caffe-tensorflow"},{"type":"has_code","target_id":"github:Rochester-NRT:AlphaGo","source_url":"https://github.com/Rochester-NRT/AlphaGo"},{"type":"has_code","target_id":"github:ZuzooVn:machine-learning-for-software-engineers","source_url":"https://github.com/ZuzooVn/machine-learning-for-software-engineers"},{"type":"has_code","target_id":"github:natanielruiz:dockerface","source_url":"https://github.com/natanielruiz/dockerface"},{"type":"has_code","target_id":"github:ybayle:awesome-deep-learning-music","source_url":"https://github.com/ybayle/awesome-deep-learning-music"},{"type":"has_code","target_id":"github:benedekrozemberczki:awesome-graph-embedding","source_url":"https://github.com/benedekrozemberczki/awesome-graph-embedding"},{"type":"has_code","target_id":"github:chihming:awesome-network-embedding","source_url":"https://github.com/chihming/awesome-network-embedding"},{"type":"has_code","target_id":"github:Microsoft:Recommenders","source_url":"https://github.com/Microsoft/Recommenders"},{"type":"has_code","target_id":"github:divamgupta:ladder_network_keras","source_url":"https://github.com/divamgupta/ladder_network_keras"},{"type":"has_code","target_id":"github:amitness:toolbox","source_url":"https://github.com/amitness/toolbox"},{"type":"has_code","target_id":"github:AMAI-GmbH:AI-Expert-Roadmap","source_url":"https://github.com/AMAI-GmbH/AI-Expert-Roadmap"},{"type":"has_code","target_id":"github:AstraZeneca:awesome-polipharmacy-side-effect-prediction","source_url":"https://github.com/AstraZeneca/awesome-polipharmacy-side-effect-prediction"},{"type":"has_code","target_id":"github:ashara12:awesome-deeplearning","source_url":"https://github.com/ashara12/awesome-deeplearning"}]', NULL, NULL, 'pending', 70, 'a46468e54bdab398c9178af6a407bf88', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ChristosChristofidis-awesome-deep-learning from https://github.com/ChristosChristofidis.png
Image converted to WebP: data/images/github-ChristosChristofidis-awesome-deep-learning.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-mozilla-DeepSpeech', 'github--mozilla--deepspeech', 'DeepSpeech', 'mozilla', 'Status ====== This project is now discontinued. Project DeepSpeech ================== .. image:: https://readthedocs.org/projects/deepspeech/badge/?version=latest :target: https://deepspeech.readthedocs.io/?badge=latest :alt: Documentation .. image:: https://github.com/mozilla/DeepSpeech/actions/workflows/macOS-amd64.yml/badge.svg :target: https://github.com/mozilla/DeepSpeech/actions/workflows/macOS-amd64.yml :alt: macOS builds .. image:: https://github.com/mozilla/DeepSpeech/actions/workflo...', '["deep-learning","deepspeech","embedded","machine-learning","neural-networks","offline","on-device","speech-recognition","speech-to-text","tensorflow","c++"]', 'other', 26673, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/mozilla/DeepSpeech","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', 'Status\n======\n\nThis project is now discontinued.\n\nProject DeepSpeech\n==================\n\n.. image:: https://readthedocs.org/projects/deepspeech/badge/?version=latest\n   :target: https://deepspeech.readthedocs.io/?badge=latest\n   :alt: Documentation\n\n\n.. image:: https://github.com/mozilla/DeepSpeech/actions/workflows/macOS-amd64.yml/badge.svg\n   :target: https://github.com/mozilla/DeepSpeech/actions/workflows/macOS-amd64.yml\n   :alt: macOS builds\n\n.. image:: https://github.com/mozilla/DeepSpeech/actions/workflows/lint.yml/badge.svg\n   :target: https://github.com/mozilla/DeepSpeech/actions/workflows/lint.yml\n   :alt: Linters\n\n.. image:: https://github.com/mozilla/DeepSpeech/actions/workflows/docker.yml/badge.svg\n   :target: https://github.com/mozilla/DeepSpeech/actions/workflows/docker.yml\n   :alt: Docker Images\n\n\nDeepSpeech is an open-source Speech-To-Text engine, using a model trained by machine learning techniques based on `Baidu''s Deep Speech research paper <https://arxiv.org/abs/1412.5567>`_. Project DeepSpeech uses Google''s `TensorFlow <https://www.tensorflow.org/>`_ to make the implementation easier.\n\nDocumentation for installation, usage, and training models are available on `deepspeech.readthedocs.io <https://deepspeech.readthedocs.io/?badge=latest>`_.\n\nFor the latest release, including pre-trained models and checkpoints, `see the latest release on GitHub <https://github.com/mozilla/DeepSpeech/releases/latest>`_.\n\nFor contribution guidelines, see `CONTRIBUTING.rst <CONTRIBUTING.rst>`_.\n\nFor contact and support information, see `SUPPORT.rst <SUPPORT.rst>`_.\n', '{"language":"C++","stars":26673,"forks":4099,"watchers":26673,"open_issues":151,"topics":["deep-learning","deepspeech","embedded","machine-learning","neural-networks","offline","on-device","speech-recognition","speech-to-text","tensorflow"],"default_branch":"master","size_kb":50588,"archived":true,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:mozilla:DeepSpeech","source_url":"https://github.com/mozilla/DeepSpeech"},{"type":"has_code","target_id":"github:mozilla:DeepSpeech","source_url":"https://github.com/mozilla/DeepSpeech"},{"type":"has_code","target_id":"github:mozilla:DeepSpeech","source_url":"https://github.com/mozilla/DeepSpeech"},{"type":"has_code","target_id":"github:mozilla:DeepSpeech","source_url":"https://github.com/mozilla/DeepSpeech"},{"type":"has_code","target_id":"github:mozilla:DeepSpeech","source_url":"https://github.com/mozilla/DeepSpeech"},{"type":"has_code","target_id":"github:mozilla:DeepSpeech","source_url":"https://github.com/mozilla/DeepSpeech"},{"type":"has_code","target_id":"github:mozilla:DeepSpeech","source_url":"https://github.com/mozilla/DeepSpeech"}]', NULL, 'MPL-2.0', 'approved', 50, '43192ce7231062e45e0e686591570de0', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-mozilla-DeepSpeech from https://github.com/mozilla.png
Image converted to WebP: data/images/github-mozilla-DeepSpeech.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-terryum-awesome-deep-learning-papers', 'github--terryum--awesome-deep-learning-papers', 'awesome-deep-learning-papers', 'terryum', '[Notice] This list is not being maintained anymore because of the overwhelming amount of deep learning papers published every day since 2017. A curated list of the most cited deep learning papers (2012-2016) We believe that there exist *classic* deep learning papers which are worth reading regardless of their application domain. Rather than providing overwhelming amount of papers, We would like to provide a *curated list* of the awesome deep learning papers which are considered as *must-reads...', '["deep-learning","deep-neural-networks","machine-learning","tex"]', 'other', 26064, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/terryum/awesome-deep-learning-papers","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Awesome - Most Cited Deep Learning Papers\n\n[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\n[Notice] This list is not being maintained anymore because of the overwhelming amount of deep learning papers published every day since 2017.\n\nA curated list of the most cited deep learning papers (2012-2016)\n\nWe believe that there exist *classic* deep learning papers which are worth reading regardless of their application domain. Rather than providing overwhelming amount of papers, We would like to provide a *curated list* of the awesome deep learning papers which are considered as *must-reads* in certain research domains.\n\n## Background\n\nBefore this list, there exist other *awesome deep learning lists*, for example, [Deep Vision](https://github.com/kjw0612/awesome-deep-vision) and [Awesome Recurrent Neural Networks](https://github.com/kjw0612/awesome-rnn). Also, after this list comes out, another awesome list for deep learning beginners, called [Deep Learning Papers Reading Roadmap](https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap), has been created and loved by many deep learning researchers.\n\nAlthough the *Roadmap List* includes lots of important deep learning papers, it feels overwhelming for me to read them all. As I mentioned in the introduction, I believe that seminal works can give us lessons regardless of their application domain. Thus, I would like to introduce **top 100 deep learning papers** here as a good starting point of overviewing deep learning researches.\n\nTo get the news for newly released papers everyday, follow my [twitter](https://twitter.com/TerryUm_ML) or [facebook page](https://www.facebook.com/terryum.io/)! \n\n## Awesome list criteria\n\n1. A list of **top 100 deep learning papers** published from 2012 to 2016 is suggested.\n2. If a paper is added to the list, another paper (usually from *More Papers from 2016" section) should be removed to keep top 100 papers. (Thus, removing papers is also important contributions as well as adding papers)\n3. Papers that are important, but failed to be included in the list, will be listed in *More than Top 100* section.\n4. Please refer to *New Papers* and *Old Papers* sections for the papers published in recent 6 months or before 2012.\n\n*(Citation criteria)*\n- **< 6 months** : *New Papers* (by discussion)\n- **2016** :  +60 citations or "More Papers from 2016"\n- **2015** :  +200 citations\n- **2014** :  +400 citations\n- **2013** :  +600 citations\n- **2012** :  +800 citations\n- **~2012** : *Old Papers* (by discussion)\n\nPlease note that we prefer seminal deep learning papers that can be applied to various researches rather than application papers. For that reason, some papers that meet the criteria may not be accepted while others can be. It depends on the impact of the paper, applicability to other researches scarcity of the research domain, and so on.\n\n**We need your contributions!**\n\nIf you have any suggestions (missing papers, new papers, key researchers or typos), please feel free to edit and pull a request.\n(Please read the [contributing guide](https://github.com/terryum/awesome-deep-learning-papers/blob/master/Contributing.md) for further instructions, though just letting me know the title of papers can also be a big contribution to us.)\n\n(Update) You can download all top-100 papers with [this](https://github.com/terryum/awesome-deep-learning-papers/blob/master/fetch_papers.py) and collect all authors'' names with [this](https://github.com/terryum/awesome-deep-learning-papers/blob/master/get_authors.py). Also, [bib file](https://github.com/terryum/awesome-deep-learning-papers/blob/master/top100papers.bib) for all top-100 papers are available. Thanks, doodhwala, [Sven](https://github.com/sunshinemyson) and [grepinsight](https://github.com/grepinsight)!\n\n+ Can anyone contribute the code for obtaining the statistics of the authors of Top-100 papers?\n\n\n## Contents\n\n* [Understanding / Generalization / Transfer](#understanding--generalization--transfer)\n* [Optimization / Training Techniques](#optimization--training-techniques)\n* [Unsupervised / Generative Models](#unsupervised--generative-models)\n* [Convolutional Network Models](#convolutional-neural-network-models)\n* [Image Segmentation / Object Detection](#image-segmentation--object-detection)\n* [Image / Video / Etc](#image--video--etc)\n* [Natural Language Processing / RNNs](#natural-language-processing--rnns)\n* [Speech / Other Domain](#speech--other-domain)\n* [Reinforcement Learning / Robotics](#reinforcement-learning--robotics)\n* [More Papers from 2016](#more-papers-from-2016)\n\n*(More than Top 100)*\n\n* [New Papers](#new-papers) : Less than 6 months\n* [Old Papers](#old-papers) : Before 2012\n* [HW / SW / Dataset](#hw--sw--dataset) : Technical reports\n* [Book / Survey / Review](#book--survey--review)\n* [Video Lectures / Tutorials / Blogs](#video-lectures--tutorials--blogs)\n* [Appendix: More than Top 100](#appendix-more-than-top-100) : More papers not in the list\n\n* * *\n\n### Understanding / Generalization / Transfer\n- **Distilling the knowledge in a neural network** (2015), G. Hinton et al. [[pdf]](http://arxiv.org/pdf/1503.02531)\n- **Deep neural networks are easily fooled: High confidence predictions for unrecognizable images** (2015), A. Nguyen et al. [[pdf]](http://arxiv.org/pdf/1412.1897)\n- **How transferable are features in deep neural networks?** (2014), J. Yosinski et al. [[pdf]](http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf)\n- **CNN features off-the-Shelf: An astounding baseline for recognition** (2014), A. Razavian et al. [[pdf]](http://www.cv-foundation.org//openaccess/content_cvpr_workshops_2014/W15/papers/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.pdf)\n- **Learning and transferring mid-Level image representations using convolutional neural networks** (2014), M. Oquab et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf)\n- **Visualizing and understanding convolutional networks** (2014), M. Zeiler and R. Fergus [[pdf]](http://arxiv.org/pdf/1311.2901)\n- **Decaf: A deep convolutional activation feature for generic visual recognition** (2014), J. Donahue et al. [[pdf]](http://arxiv.org/pdf/1310.1531)\n\n<!---[Key researchers]  [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Jason Yosinski](https://scholar.google.ca/citations?hl=en&user=gxL1qj8AAAAJ) -->\n\n### Optimization / Training Techniques\n- **Training very deep networks** (2015), R. Srivastava et al. [[pdf]](http://papers.nips.cc/paper/5850-training-very-deep-networks.pdf)\n- **Batch normalization: Accelerating deep network training by reducing internal covariate shift** (2015), S. Loffe and C. Szegedy [[pdf]](http://arxiv.org/pdf/1502.03167)\n- **Delving deep into rectifiers: Surpassing human-level performance on imagenet classification** (2015), K. He et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf)\n- **Dropout: A simple way to prevent neural networks from overfitting** (2014), N. Srivastava et al. [[pdf]](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n- **Adam: A method for stochastic optimization** (2014), D. Kingma and J. Ba [[pdf]](http://arxiv.org/pdf/1412.6980)\n- **Improving neural networks by preventing co-adaptation of feature detectors** (2012), G. Hinton et al. [[pdf]](http://arxiv.org/pdf/1207.0580.pdf)\n- **Random search for hyper-parameter optimization** (2012) J. Bergstra and Y. Bengio [[pdf]](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a)\n\n<!---[Key researchers] [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Christian Szegedy](https://scholar.google.ca/citations?hl=en&user=3QeF7mAAAAAJ), [Sergey Ioffe](https://scholar.google.ca/citations?user=S5zOyIkAAAAJ), [Kaming He](https://scholar.google.ca/citations?hl=en&user=DhtAFkwAAAAJ), [Diederik P. Kingma](https://scholar.google.ca/citations?hl=en&user=yyIoQu4AAAAJ)-->\n\n### Unsupervised / Generative Models\n- **Pixel recurrent neural networks** (2016), A. Oord et al. [[pdf]](http://arxiv.org/pdf/1601.06759v2.pdf)\n- **Improved techniques for training GANs** (2016), T. Salimans et al. [[pdf]](http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf)\n- **Unsupervised representation learning with deep convolutional generative adversarial networks** (2015), A. Radford et al. [[pdf]](https://arxiv.org/pdf/1511.06434v2)\n- **DRAW: A recurrent neural network for image generation** (2015), K. Gregor et al. [[pdf]](http://arxiv.org/pdf/1502.04623)\n- **Generative adversarial nets** (2014), I. Goodfellow et al. [[pdf]](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)\n- **Auto-encoding variational Bayes** (2013), D. Kingma and M. Welling [[pdf]](http://arxiv.org/pdf/1312.6114)\n- **Building high-level features using large scale unsupervised learning** (2013), Q. Le et al. [[pdf]](http://arxiv.org/pdf/1112.6209)\n\n<!---[Key researchers] [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Ian Goodfellow](https://scholar.google.ca/citations?user=iYN86KEAAAAJ), [Alex Graves](https://scholar.google.ca/citations?user=DaFHynwAAAAJ)-->\n### Convolutional Neural Network Models\n- **Rethinking the inception architecture for computer vision** (2016), C. Szegedy et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf)\n- **Inception-v4, inception-resnet and the impact of residual connections on learning** (2016), C. Szegedy et al. [[pdf]](http://arxiv.org/pdf/1602.07261)\n- **Identity Mappings in Deep Residual Networks** (2016), K. He et al. [[pdf]](https://arxiv.org/pdf/1603.05027v2.pdf)\n- **Deep residual learning for image recognition** (2016), K. He et al. [[pdf]](http://arxiv.org/pdf/1512.03385)\n- **Spatial transformer network** (2015), M. Jaderberg et al., [[pdf]](http://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf)\n- **Going deeper with convolutions** (2015), C. Szegedy et al.  [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)\n- **Very deep convolutional networks for large-scale image recognition** (2014), K. Simonyan and A. Zisserman [[pdf]](http://arxiv.org/pdf/1409.1556)\n- **Return of the devil in the details: delving deep into convolutional nets** (2014), K. Chatfield et al. [[pdf]](http://arxiv.org/pdf/1405.3531)\n- **OverFeat: Integrated recognition, localization and detection using convolutional networks** (2013), P. Sermanet et al. [[pdf]](http://arxiv.org/pdf/1312.6229)\n- **Maxout networks** (2013), I. Goodfellow et al. [[pdf]](http://arxiv.org/pdf/1302.4389v4)\n- **Network in network** (2013), M. Lin et al. [[pdf]](http://arxiv.org/pdf/1312.4400)\n- **ImageNet classification with deep convolutional neural networks** (2012), A. Krizhevsky et al. [[pdf]](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n\n<!---[Key researchers]  [Christian Szegedy](https://scholar.google.ca/citations?hl=en&user=3QeF7mAAAAAJ), [Kaming He](https://scholar.google.ca/citations?hl=en&user=DhtAFkwAAAAJ), [Shaoqing Ren](https://scholar.google.ca/citations?hl=en&user=AUhj438AAAAJ), [Jian Sun](https://scholar.google.ca/citations?hl=en&user=ALVSZAYAAAAJ), [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ), [Yann LeCun](https://scholar.google.ca/citations?hl=en&user=WLN3QrAAAAAJ)-->\n\n### Image: Segmentation / Object Detection\n- **You only look once: Unified, real-time object detection** (2016), J. Redmon et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf)\n- **Fully convolutional networks for semantic segmentation** (2015), J. Long et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)\n- **Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks** (2015), S. Ren et al. [[pdf]](http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf)\n- **Fast R-CNN** (2015), R. Girshick [[pdf]](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf)\n- **Rich feature hierarchies for accurate object detection and semantic segmentation** (2014), R. Girshick et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf)\n- **Spatial pyramid pooling in deep convolutional networks for visual recognition** (2014), K. He et al. [[pdf]](http://arxiv.org/pdf/1406.4729)\n- **Semantic image segmentation with deep convolutional nets and fully connected CRFs**, L. Chen et al. [[pdf]](https://arxiv.org/pdf/1412.7062)\n- **Learning hierarchical features for scene labeling** (2013), C. Farabet et al. [[pdf]](https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf)\n\n<!---[Key researchers]  [Ross Girshick](https://scholar.google.ca/citations?hl=en&user=W8VIEZgAAAAJ), [Jeff Donahue](https://scholar.google.ca/citations?hl=en&user=UfbuDH8AAAAJ), [Trevor Darrell](https://scholar.google.ca/citations?hl=en&user=bh-uRFMAAAAJ)-->\n\n### Image / Video / Etc\n- **Image Super-Resolution Using Deep Convolutional Networks** (2016), C. Dong et al. [[pdf]](https://arxiv.org/pdf/1501.00092v3.pdf)\n- **A neural algorithm of artistic style** (2015), L. Gatys et al. [[pdf]](https://arxiv.org/pdf/1508.06576)\n- **Deep visual-semantic alignments for generating image descriptions** (2015), A. Karpathy and L. Fei-Fei [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf)\n- **Show, attend and tell: Neural image caption generation with visual attention** (2015), K. Xu et al. [[pdf]](http://arxiv.org/pdf/1502.03044)\n- **Show and tell: A neural image caption generator** (2015), O. Vinyals et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf)\n- **Long-term recurrent convolutional networks for visual recognition and description** (2015), J. Donahue et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf)\n- **VQA: Visual question answering** (2015), S. Antol et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf)\n- **DeepFace: Closing the gap to human-level performance in face verification** (2014), Y. Taigman et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf):\n- **Large-scale video classification with convolutional neural networks** (2014), A. Karpathy et al. [[pdf]](http://vision.stanford.edu/pdf/karpathy14.pdf)\n- **Two-stream convolutional networks for action recognition in videos** (2014), K. Simonyan et al. [[pdf]](http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf)\n- **3D convolutional neural networks for human action recognition** (2013), S. Ji et al. [[pdf]](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf)\n\n<!---[Key researchers]  [Oriol Vinyals](https://scholar.google.ca/citations?user=NkzyCvUAAAAJ), [Andrej Karpathy](https://scholar.google.ca/citations?user=l8WuQJgAAAAJ)-->\n\n<!---[Key researchers]  [Alex Graves](https://scholar.google.ca/citations?user=DaFHynwAAAAJ)-->\n\n### Natural Language Processing / RNNs\n- **Neural Architectures for Named Entity Recognition** (2016), G. Lample et al. [[pdf]](http://aclweb.org/anthology/N/N16/N16-1030.pdf)\n- **Exploring the limits of language modeling** (2016), R. Jozefowicz et al. [[pdf]](http://arxiv.org/pdf/1602.02410)\n- **Teaching machines to read and comprehend** (2015), K. Hermann et al. [[pdf]](http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf)\n- **Effective approaches to attention-based neural machine translation** (2015), M. Luong et al. [[pdf]](https://arxiv.org/pdf/1508.04025)\n- **Conditional random fields as recurrent neural networks** (2015), S. Zheng and S. Jayasumana. [[pdf]](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf)\n- **Memory networks** (2014), J. Weston et al. [[pdf]](https://arxiv.org/pdf/1410.3916)\n- **Neural turing machines** (2014), A. Graves et al. [[pdf]](https://arxiv.org/pdf/1410.5401)\n- **Neural machine translation by jointly learning to align and translate** (2014), D. Bahdanau et al. [[pdf]](http://arxiv.org/pdf/1409.0473)\n- **Sequence to sequence learning with neural networks** (2014), I. Sutskever et al. [[pdf]](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n- **Learning phrase representations using RNN encoder-decoder for statistical machine translation** (2014), K. Cho et al. [[pdf]](http://arxiv.org/pdf/1406.1078)\n- **A convolutional neural network for modeling sentences** (2014), N. Kalchbrenner et al. [[pdf]](http://arxiv.org/pdf/1404.2188v1)\n- **Convolutional neural networks for sentence classification** (2014), Y. Kim [[pdf]](http://arxiv.org/pdf/1408.5882)\n- **Glove: Global vectors for word representation** (2014), J. Pennington et al. [[pdf]](http://anthology.aclweb.org/D/D14/D14-1162.pdf)\n- **Distributed representations of sentences and documents** (2014), Q. Le and T. Mikolov [[pdf]](http://arxiv.org/pdf/1405.4053)\n- **Distributed representations of words and phrases and their compositionality** (2013), T. Mikolov et al. [[pdf]](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n- **Efficient estimation of word representations in vector space** (2013), T. Mikolov et al.  [[pdf]](http://arxiv.org/pdf/1301.3781)\n- **Recursive deep models for semantic compositionality over a sentiment treebank** (2013), R. Socher et al. [[pdf]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&rep=rep1&type=pdf)\n- **Generating sequences with recurrent neural networks** (2013), A. Graves. [[pdf]](https://arxiv.org/pdf/1308.0850)\n\n<!---[Key researchers]  [Kyunghyun Cho](https://scholar.google.ca/citations?user=0RAmmIAAAAAJ), [Oriol Vinyals](https://scholar.google.ca/citations?user=NkzyCvUAAAAJ), [Richard Socher](https://scholar.google.ca/citations?hl=en&user=FaOcyfMAAAAJ), [Tomas Mikolov](https://scholar.google.ca/citations?user=oBu8kMMAAAAJ), [Christopher D. Manning](https://scholar.google.ca/citations?user=1zmDOdwAAAAJ), [Yoshua Bengio](https://scholar.google.ca/citations?user=kukA0LcAAAAJ)-->\n\n### Speech / Other Domain\n- **End-to-end attention-based large vocabulary speech recognition** (2016), D. Bahdanau et al. [[pdf]](https://arxiv.org/pdf/1508.04395)\n- **Deep speech 2: End-to-end speech recognition in English and Mandarin** (2015), D. Amodei et al. [[pdf]](https://arxiv.org/pdf/1512.02595)\n- **Speech recognition with deep recurrent neural networks** (2013), A. Graves [[pdf]](http://arxiv.org/pdf/1303.5778.pdf)\n- **Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups** (2012), G. Hinton et al. [[pdf]](http://www.cs.toronto.edu/~asamir/papers/SPM_DNN_12.pdf)\n- **Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition** (2012) G. Dahl et al. [[pdf]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.337.7548&rep=rep1&type=pdf)\n- **Acoustic modeling using deep belief networks** (2012), A. Mohamed et al. [[pdf]](http://www.cs.toronto.edu/~asamir/papers/speechDBN_jrnl.pdf)\n\n<!---[Key researchers]  [Alex Graves](https://scholar.google.ca/citations?user=DaFHynwAAAAJ), [Geoffrey Hinton](https://scholar.google.ca/citations?user=JicYPdAAAAAJ), [Dong Yu](https://scholar.google.ca/citations?hl=en&user=tMY31_gAAAAJ)-->\n\n### Reinforcement Learning / Robotics\n- **End-to-end training of deep visuomotor policies** (2016), S. Levine et al. [[pdf]](http://www.jmlr.org/papers/volume17/15-522/source/15-522.pdf)\n- **Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection** (2016), S. Levine et al. [[pdf]](https://arxiv.org/pdf/1603.02199)\n- **Asynchronous methods for deep reinforcement learning** (2016), V. Mnih et al. [[pdf]](http://www.jmlr.org/proceedings/papers/v48/mniha16.pdf)\n- **Deep Reinforcement Learning with Double Q-Learning** (2016), H. Hasselt et al. [[pdf]](https://arxiv.org/pdf/1509.06461.pdf )\n- **Mastering the game of Go with deep neural networks and tree search** (2016), D. Silver et al. [[pdf]](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html)\n- **Continuous control with deep reinforcement learning** (2015), T. Lillicrap et al. [[pdf]](https://arxiv.org/pdf/1509.02971)\n- **Human-level control through deep reinforcement learning** (2015), V. Mnih et al. [[pdf]](http://www.davidqiu.com:8888/research/nature14236.pdf)\n- **Deep learning for detecting robotic grasps** (2015), I. Lenz et al. [[pdf]](http://www.cs.cornell.edu/~asaxena/papers/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf)\n- **Playing atari with deep reinforcement learning** (2013), V. Mnih et al. [[pdf]](http://arxiv.org/pdf/1312.5602.pdf))\n\n<!---[Key researchers]  [Sergey Levine](https://scholar.google.ca/citations?user=8R35rCwAAAAJ), [Volodymyr Mnih](https://scholar.google.ca/citations?hl=en&user=rLdfJ1gAAAAJ), [David Silver](https://scholar.google.ca/citations?user=-8DNE4UAAAAJ)-->\n\n### More Papers from 2016\n- **Layer Normalization** (2016), J. Ba et al. [[pdf]](https://arxiv.org/pdf/1607.06450v1.pdf)\n- **Learning to learn by gradient descent by gradient descent** (2016), M. Andrychowicz et al. [[pdf]](http://arxiv.org/pdf/1606.04474v1)\n- **Domain-adversarial training of neural networks** (2016), Y. Ganin et al. [[pdf]](http://www.jmlr.org/papers/volume17/15-239/source/15-239.pdf)\n- **WaveNet: A Generative Model for Raw Audio** (2016), A. Oord et al. [[pdf]](https://arxiv.org/pdf/1609.03499v2) [[web]](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)\n- **Colorful image colorization** (2016), R. Zhang et al. [[pdf]](https://arxiv.org/pdf/1603.08511)\n- **Generative visual manipulation on the natural image manifold** (2016), J. Zhu et al. [[pdf]](https://arxiv.org/pdf/1609.03552)\n- **Texture networks: Feed-forward synthesis of textures and stylized images** (2016), D Ulyanov et al. [[pdf]](http://www.jmlr.org/proceedings/papers/v48/ulyanov16.pdf)\n- **SSD: Single shot multibox detector** (2016), W. Liu et al. [[pdf]](https://arxiv.org/pdf/1512.02325)\n- **SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 1MB model size** (2016), F. Iandola et al. [[pdf]](http://arxiv.org/pdf/1602.07360)\n- **Eie: Efficient inference engine on compressed deep neural network** (2016), S. Han et al. [[pdf]](http://arxiv.org/pdf/1602.01528)\n- **Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1** (2016), M. Courbariaux et al. [[pdf]](https://arxiv.org/pdf/1602.02830)\n- **Dynamic memory networks for visual and textual question answering** (2016), C. Xiong et al. [[pdf]](http://www.jmlr.org/proceedings/papers/v48/xiong16.pdf)\n- **Stacked attention networks for image question answering** (2016), Z. Yang et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_Stacked_Attention_Networks_CVPR_2016_paper.pdf)\n- **Hybrid computing using a neural network with dynamic external memory** (2016), A. Graves et al. [[pdf]](https://www.gwern.net/docs/2016-graves.pdf)\n- **Google''s neural machine translation system: Bridging the gap between human and machine translation** (2016), Y. Wu et al. [[pdf]](https://arxiv.org/pdf/1609.08144)\n\n* * *\n\n\n### New papers\n*Newly published papers (< 6 months) which are worth reading*\n- MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (2017), Andrew G. Howard et al. [[pdf]](https://arxiv.org/pdf/1704.04861.pdf)\n- Convolutional Sequence to Sequence Learning (2017), Jonas Gehring et al. [[pdf]](https://arxiv.org/pdf/1705.03122)\n- A Knowledge-Grounded Neural Conversation Model (2017), Marjan Ghazvininejad et al. [[pdf]](https://arxiv.org/pdf/1702.01932)\n- Accurate, Large Minibatch SGD:Training ImageNet in 1 Hour (2017), Priya Goyal et al. [[pdf]](https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h3.pdf)\n- TACOTRON: Towards end-to-end speech synthesis (2017), Y. Wang et al. [[pdf]](https://arxiv.org/pdf/1703.10135.pdf)\n- Deep Photo Style Transfer (2017), F. Luan et al. [[pdf]](http://arxiv.org/pdf/1703.07511v1.pdf)\n- Evolution Strategies as a Scalable Alternative to Reinforcement Learning (2017), T. Salimans et al. [[pdf]](http://arxiv.org/pdf/1703.03864v1.pdf)\n- Deformable Convolutional Networks (2017), J. Dai et al. [[pdf]](http://arxiv.org/pdf/1703.06211v2.pdf)\n- Mask R-CNN (2017), K. He et al. [[pdf]](https://128.84.21.199/pdf/1703.06870)\n- Learning to discover cross-domain relations with generative adversarial networks (2017), T. Kim et al. [[pdf]](http://arxiv.org/pdf/1703.05192v1.pdf) \n- Deep voice: Real-time neural text-to-speech (2017), S. Arik et al., [[pdf]](http://arxiv.org/pdf/1702.07825v2.pdf)\n- PixelNet: Representation of the pixels, by the pixels, and for the pixels (2017), A. Bansal et al. [[pdf]](http://arxiv.org/pdf/1702.06506v1.pdf)\n- Batch renormalization: Towards reducing minibatch dependence in batch-normalized models (2017), S. Ioffe. [[pdf]](https://arxiv.org/abs/1702.03275)\n- Wasserstein GAN (2017), M. Arjovsky et al. [[pdf]](https://arxiv.org/pdf/1701.07875v1)\n- Understanding deep learning requires rethinking generalization (2017), C. Zhang et al. [[pdf]](https://arxiv.org/pdf/1611.03530)\n- Least squares generative adversarial networks (2016), X. Mao et al. [[pdf]](https://arxiv.org/abs/1611.04076v2)\n\n\n### Old Papers\n*Classic papers published before 2012*\n- An analysis of single-layer networks in unsupervised feature learning (2011), A. Coates et al. [[pdf]](http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf)\n- Deep sparse rectifier neural networks (2011), X. Glorot et al. [[pdf]](http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf)\n- Natural language processing (almost) from scratch (2011), R. Collobert et al. [[pdf]](http://arxiv.org/pdf/1103.0398)\n- Recurrent neural network based language model (2010), T. Mikolov et al. [[pdf]](http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf)\n- Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al. [[pdf]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&rep=rep1&type=pdf)\n- Learning mid-level features for recognition (2010), Y. Boureau [[pdf]](http://ece.duke.edu/~lcarin/boureau-cvpr-10.pdf)\n- A practical guide to training restricted boltzmann machines (2010), G. Hinton [[pdf]](http://www.csri.utoronto.ca/~hinton/absps/guideTR.pdf)\n- Understanding the difficulty of training deep feedforward neural networks (2010), X. Glorot and Y. Bengio [[pdf]](http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf)\n- Why does unsupervised pre-training help deep learning (2010), D. Erhan et al. [[pdf]](http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf)\n- Learning deep architectures for AI (2009), Y. Bengio. [[pdf]](http://sanghv.com/download/soft/machine%20learning,%20artificial%20intelligence,%20mathematics%20ebooks/ML/learning%20deep%20architectures%20for%20AI%20(2009).pdf)\n- Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations (2009), H. Lee et al. [[pdf]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&rep=rep1&type=pdf)\n- Greedy layer-wise training of deep networks (2007), Y. Bengio et al. [[pdf]](http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_739.pdf)\n- Reducing the dimensionality of data with neural networks, G. Hinton and R. Salakhutdinov. [[pdf]](http://homes.mpimf-heidelberg.mpg.de/~mhelmsta/pdf/2006%20Hinton%20Salakhudtkinov%20Science.pdf)\n- A fast learning algorithm for deep belief nets (2006), G. Hinton et al. [[pdf]](http://nuyoo.utm.mx/~jjf/rna/A8%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets.pdf)\n- Gradient-based learning applied to document recognition (1998), Y. LeCun et al. [[pdf]](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)\n- Long short-term memory (1997), S. Hochreiter and J. Schmidhuber. [[pdf]](http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1997.9.8.1735)\n\n\n### HW / SW / Dataset\n-  SQuAD: 100,000+ Questions for Machine Comprehension of Text (2016), Rajpurkar et al. [[pdf]](https://arxiv.org/pdf/1606.05250.pdf)\n- OpenAI gym (2016), G. Brockman et al. [[pdf]](https://arxiv.org/pdf/1606.01540)\n- TensorFlow: Large-scale machine learning on heterogeneous distributed systems (2016), M. Abadi et al. [[pdf]](http://arxiv.org/pdf/1603.04467)\n- Theano: A Python framework for fast computation of mathematical expressions, R. Al-Rfou et al.\n- Torch7: A matlab-like environment for machine learning, R. Collobert et al. [[pdf]](https://ronan.collobert.com/pub/matos/2011_torch7_nipsw.pdf)\n- MatConvNet: Convolutional neural networks for matlab (2015), A. Vedaldi and K. Lenc [[pdf]](http://arxiv.org/pdf/1412.4564)\n- Imagenet large scale visual recognition challenge (2015), O. Russakovsky et al. [[pdf]](http://arxiv.org/pdf/1409.0575)\n- Caffe: Convolutional architecture for fast feature embedding (2014), Y. Jia et al. [[pdf]](http://arxiv.org/pdf/1408.5093)\n\n\n### Book / Survey / Review\n- On the Origin of Deep Learning (2017), H. Wang and Bhiksha Raj. [[pdf]](https://arxiv.org/pdf/1702.07800)\n- Deep Reinforcement Learning: An Overview (2017), Y. Li, [[pdf]](http://arxiv.org/pdf/1701.07274v2.pdf)\n- Neural Machine Translation and Sequence-to-sequence Models(2017): A Tutorial, G. Neubig. [[pdf]](http://arxiv.org/pdf/1703.01619v1.pdf)\n- Neural Network and Deep Learning (Book, Jan 2017), Michael Nielsen. [[html]](http://neuralnetworksanddeeplearning.com/index.html)\n- Deep learning (Book, 2016), Goodfellow et al. [[html]](http://www.deeplearningbook.org/)\n- LSTM: A search space odyssey (2016), K. Greff et al. [[pdf]](https://arxiv.org/pdf/1503.04069.pdf?utm_content=buffereddc5&utm_medium=social&utm_source=plus.google.com&utm_campaign=buffer)\n- Tutorial on Variational Autoencoders (2016), C. Doersch. [[pdf]](https://arxiv.org/pdf/1606.05908)\n- Deep learning (2015), Y. LeCun, Y. Bengio and G. Hinton [[pdf]](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf)\n- Deep learning in neural networks: An overview (2015), J. Schmidhuber [[pdf]](http://arxiv.org/pdf/1404.7828)\n- Representation learning: A review and new perspectives (2013), Y. Bengio et al. [[pdf]](http://arxiv.org/pdf/1206.5538)\n\n### Video Lectures / Tutorials / Blogs\n\n*(Lectures)*\n- CS231n, Convolutional Neural Networks for Visual Recognition, Stanford University [[web]](http://cs231n.stanford.edu/)\n- CS224d, Deep Learning for Natural Language Processing, Stanford University [[web]](http://cs224d.stanford.edu/)\n- Oxford Deep NLP 2017, Deep Learning for Natural Language Processing, University of Oxford [[web]](https://github.com/oxford-cs-deepnlp-2017/lectures)\n\n*(Tutorials)*\n- NIPS 2016 Tutorials, Long Beach [[web]](https://nips.cc/Conferences/2016/Schedule?type=Tutorial)\n- ICML 2016 Tutorials, New York City [[web]](http://techtalks.tv/icml/2016/tutorials/)\n- ICLR 2016 Videos, San Juan [[web]](http://videolectures.net/iclr2016_san_juan/)\n- Deep Learning Summer School 2016, Montreal [[web]](http://videolectures.net/deeplearning2016_montreal/)\n- Bay Area Deep Learning School 2016, Stanford [[web]](https://www.bayareadlschool.org/)\n\n*(Blogs)*\n- OpenAI [[web]](https://www.openai.com/)\n- Distill [[web]](http://distill.pub/)\n- Andrej Karpathy Blog [[web]](http://karpathy.github.io/)\n- Colah''s Blog [[Web]](http://colah.github.io/)\n- WildML [[Web]](http://www.wildml.com/)\n- FastML [[web]](http://www.fastml.com/)\n- TheMorningPaper [[web]](https://blog.acolyer.org)\n\n### Appendix: More than Top 100\n*(2016)*\n- A character-level decoder without explicit segmentation for neural machine translation (2016), J. Chung et al. [[pdf]](https://arxiv.org/pdf/1603.06147)\n- Dermatologist-level classification of skin cancer with deep neural networks (2017), A. Esteva et al. [[html]](http://www.nature.com/nature/journal/v542/n7639/full/nature21056.html)\n- Weakly supervised object localization with multi-fold multiple instance learning (2017), R. Gokberk et al. [[pdf]](https://arxiv.org/pdf/1503.00949)\n- Brain tumor segmentation with deep neural networks (2017), M. Havaei et al. [[pdf]](https://arxiv.org/pdf/1505.03540)\n- Professor Forcing: A New Algorithm for Training Recurrent Networks (2016), A. Lamb et al. [[pdf]](https://arxiv.org/pdf/1610.09038)\n- Adversarially learned inference (2016), V. Dumoulin et al. [[web]](https://ishmaelbelghazi.github.io/ALI/)[[pdf]](https://arxiv.org/pdf/1606.00704v1)\n- Understanding convolutional neural networks (2016), J. Koushik [[pdf]](https://arxiv.org/pdf/1605.09081v1)\n- Taking the human out of the loop: A review of bayesian optimization (2016), B. Shahriari et al. [[pdf]](https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf)\n- Adaptive computation time for recurrent neural networks (2016), A. Graves [[pdf]](http://arxiv.org/pdf/1603.08983)\n- Densely connected convolutional networks (2016), G. Huang et al. [[pdf]](https://arxiv.org/pdf/1608.06993v1)\n- Region-based convolutional networks for accurate object detection and segmentation (2016), R. Girshick et al. \n- Continuous deep q-learning with model-based acceleration (2016), S. Gu et al. [[pdf]](http://www.jmlr.org/proceedings/papers/v48/gu16.pdf)\n- A thorough examination of the cnn/daily mail reading comprehension task (2016), D. Chen et al. [[pdf]](https://arxiv.org/pdf/1606.02858)\n- Achieving open vocabulary neural machine translation with hybrid word-character models, M. Luong and C. Manning. [[pdf]](https://arxiv.org/pdf/1604.00788)\n- Very Deep Convolutional Networks for Natural Language Processing (2016), A. Conneau et al. [[pdf]](https://arxiv.org/pdf/1606.01781)\n- Bag of tricks for efficient text classification (2016), A. Joulin et al. [[pdf]](https://arxiv.org/pdf/1607.01759)\n- Efficient piecewise training of deep structured models for semantic segmentation (2016), G. Lin et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Lin_Efficient_Piecewise_Training_CVPR_2016_paper.pdf)\n- Learning to compose neural networks for question answering (2016), J. Andreas et al. [[pdf]](https://arxiv.org/pdf/1601.01705)\n- Perceptual losses for real-time style transfer and super-resolution (2016), J. Johnson et al. [[pdf]](https://arxiv.org/pdf/1603.08155)\n- Reading text in the wild with convolutional neural networks (2016), M. Jaderberg et al. [[pdf]](http://arxiv.org/pdf/1412.1842)\n- What makes for effective detection proposals? (2016), J. Hosang et al. [[pdf]](https://arxiv.org/pdf/1502.05082)\n- Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks (2016), S. Bell et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.pdf).\n- Instance-aware semantic segmentation via multi-task network cascades (2016), J. Dai et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Dai_Instance-Aware_Semantic_Segmentation_CVPR_2016_paper.pdf)\n- Conditional image generation with pixelcnn decoders (2016), A. van den Oord et al. [[pdf]](http://papers.nips.cc/paper/6527-tree-structured-reinforcement-learning-for-sequential-object-localization.pdf)\n- Deep networks with stochastic depth (2016), G. Huang et al., [[pdf]](https://arxiv.org/pdf/1603.09382)\n- Consistency and Fluctuations For Stochastic Gradient Langevin Dynamics (2016), Yee Whye Teh et al. [[pdf]](http://www.jmlr.org/papers/volume17/teh16a/teh16a.pdf)\n\n*(2015)*\n- Ask your neurons: A neural-based approach to answering questions about images (2015), M. Malinowski et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Malinowski_Ask_Your_Neurons_ICCV_2015_paper.pdf)\n- Exploring models and data for image question answering (2015), M. Ren et al. [[pdf]](http://papers.nips.cc/paper/5640-stochastic-variational-inference-for-hidden-markov-models.pdf)\n- Are you talking to a machine? dataset and methods for multilingual image question (2015), H. Gao et al. [[pdf]](http://papers.nips.cc/paper/5641-are-you-talking-to-a-machine-dataset-and-methods-for-multilingual-image-question.pdf)\n- Mind''s eye: A recurrent visual representation for image caption generation (2015), X. Chen and C. Zitnick. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Chen_Minds_Eye_A_2015_CVPR_paper.pdf)\n- From captions to visual concepts and back (2015), H. Fang et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Fang_From_Captions_to_2015_CVPR_paper.pdf).\n- Towards AI-complete question answering: A set of prerequisite toy tasks (2015), J. Weston et al. [[pdf]](http://arxiv.org/pdf/1502.05698)\n- Ask me anything: Dynamic memory networks for natural language processing (2015), A. Kumar et al. [[pdf]](http://arxiv.org/pdf/1506.07285)\n- Unsupervised learning of video representations using LSTMs (2015), N. Srivastava et al. [[pdf]](http://www.jmlr.org/proceedings/papers/v37/srivastava15.pdf)\n- Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding (2015), S. Han et al. [[pdf]](https://arxiv.org/pdf/1510.00149)\n- Improved semantic representations from tree-structured long short-term memory networks (2015), K. Tai et al. [[pdf]](https://arxiv.org/pdf/1503.00075)\n- Character-aware neural language models (2015), Y. Kim et al. [[pdf]](https://arxiv.org/pdf/1508.06615)\n- Grammar as a foreign language (2015), O. Vinyals et al. [[pdf]](http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf)\n- Trust Region Policy Optimization (2015), J. Schulman et al. [[pdf]](http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf)\n- Beyond short snippents: Deep networks for video classification (2015) [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf)\n- Learning Deconvolution Network for Semantic Segmentation (2015), H. Noh et al. [[pdf]](https://arxiv.org/pdf/1505.04366v1)\n- Learning spatiotemporal features with 3d convolutional networks (2015), D. Tran et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.pdf)\n- Understanding neural networks through deep visualization (2015), J. Yosinski et al. [[pdf]](https://arxiv.org/pdf/1506.06579)\n- An Empirical Exploration of Recurrent Network Architectures (2015), R. Jozefowicz et al.  [[pdf]](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n- Deep generative image models using aï¿¼ laplacian pyramid of adversarial networks (2015), E.Denton et al. [[pdf]](http://papers.nips.cc/paper/5773-deep-generative-image-models-using-a-laplacian-pyramid-of-adversarial-networks.pdf)\n- Gated Feedback Recurrent Neural Networks (2015), J. Chung et al. [[pdf]](http://www.jmlr.org/proceedings/papers/v37/chung15.pdf)\n- Fast and accurate deep network learning by exponential linear units (ELUS) (2015), D. Clevert et al. [[pdf]](https://arxiv.org/pdf/1511.07289.pdf%5Cnhttp://arxiv.org/abs/1511.07289%5Cnhttp://arxiv.org/abs/1511.07289)\n- Pointer networks (2015), O. Vinyals et al. [[pdf]](http://papers.nips.cc/paper/5866-pointer-networks.pdf)\n- Visualizing and Understanding Recurrent Networks (2015), A. Karpathy et al. [[pdf]](https://arxiv.org/pdf/1506.02078)\n- Attention-based models for speech recognition (2015), J. Chorowski et al. [[pdf]](http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf)\n- End-to-end memory networks (2015), S. Sukbaatar et al. [[pdf]](http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf)\n- Describing videos by exploiting temporal structure (2015), L. Yao et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yao_Describing_Videos_by_ICCV_2015_paper.pdf)\n- A neural conversational model (2015), O. Vinyals and Q. Le. [[pdf]](https://arxiv.org/pdf/1506.05869.pdf)\n- Improving distributional similarity with lessons learned from word embeddings, O. Levy et al. [[pdf]] (https://www.transacl.org/ojs/index.php/tacl/article/download/570/124)\n- Transition-Based Dependency Parsing with Stack Long Short-Term Memory (2015), C. Dyer et al. [[pdf]](http://aclweb.org/anthology/P/P15/P15-1033.pdf)\n- Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs (2015), M. Ballesteros et al. [[pdf]](http://aclweb.org/anthology/D/D15/D15-1041.pdf)\n- Finding function in form: Compositional character models for open vocabulary word representation (2015), W. Ling et al. [[pdf]](http://aclweb.org/anthology/D/D15/D15-1176.pdf)\n\n\n*(~2014)*\n- DeepPose: Human pose estimation via deep neural networks (2014), A. Toshev and C. Szegedy [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf)\n- Learning a Deep Convolutional Network for Image Super-Resolution (2014, C. Dong et al. [[pdf]](https://www.researchgate.net/profile/Chen_Change_Loy/publication/264552416_Lecture_Notes_in_Computer_Science/links/53e583e50cf25d674e9c280e.pdf)\n- Recurrent models of visual attention (2014), V. Mnih et al. [[pdf]](http://arxiv.org/pdf/1406.6247.pdf)\n- Empirical evaluation of gated recurrent neural networks on sequence modeling (2014), J. Chung et al. [[pdf]](https://arxiv.org/pdf/1412.3555)\n- Addressing the rare word problem in neural machine translation (2014), M. Luong et al. [[pdf]](https://arxiv.org/pdf/1410.8206)\n- On the properties of neural machine translation: Encoder-decoder approaches (2014), K. Cho et. al.\n- Recurrent neural network regularization (2014), W. Zaremba et al. [[pdf]](http://arxiv.org/pdf/1409.2329)\n- Intriguing properties of neural networks (2014), C. Szegedy et al. [[pdf]](https://arxiv.org/pdf/1312.6199.pdf)\n- Towards end-to-end speech recognition with recurrent neural networks (2014), A. Graves and N. Jaitly. [[pdf]](http://www.jmlr.org/proceedings/papers/v32/graves14.pdf)\n- Scalable object detection using deep neural networks (2014), D. Erhan et al. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Erhan_Scalable_Object_Detection_2014_CVPR_paper.pdf)\n- On the importance of initialization and momentum in deep learning (2013), I. Sutskever et al. [[pdf]](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_sutskever13.pdf)\n- Regularization of neural networks using dropconnect (2013), L. Wan et al. [[pdf]](http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_wan13.pdf)\n- Learning Hierarchical Features for Scene Labeling (2013), C. Farabet et al. [[pdf]](https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf)\n- Linguistic Regularities in Continuous Space Word Representations (2013), T. Mikolov et al. [[pdf]](http://www.aclweb.org/anthology/N13-1#page=784)\n- Large scale distributed deep networks (2012), J. Dean et al. [[pdf]](http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf)\n- A Fast and Accurate Dependency Parser using Neural Networks. Chen and Manning. [[pdf]](http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf)\n\n\n\n## Acknowledgement\n\nThank you for all your contributions. Please make sure to read the [contributing guide](https://github.com/terryum/awesome-deep-learning-papers/blob/master/Contributing.md) before you make a pull request.\n\n## License\n[![CC0](http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0/)\n\nTo the extent possible under law, [Terry T. Um](https://www.facebook.com/terryum.io/) has waived all copyright and related or neighboring rights to this work.\n', '{"language":"TeX","stars":26064,"forks":4466,"watchers":26064,"open_issues":36,"topics":["deep-learning","deep-neural-networks","machine-learning"],"default_branch":"master","size_kb":147,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:sindresorhus:awesome","source_url":"https://github.com/sindresorhus/awesome"},{"type":"has_code","target_id":"github:kjw0612:awesome-deep-vision","source_url":"https://github.com/kjw0612/awesome-deep-vision"},{"type":"has_code","target_id":"github:kjw0612:awesome-rnn","source_url":"https://github.com/kjw0612/awesome-rnn"},{"type":"has_code","target_id":"github:songrotek:Deep-Learning-Papers-Reading-Roadmap","source_url":"https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap"},{"type":"has_code","target_id":"github:terryum:awesome-deep-learning-papers","source_url":"https://github.com/terryum/awesome-deep-learning-papers"},{"type":"has_code","target_id":"github:terryum:awesome-deep-learning-papers","source_url":"https://github.com/terryum/awesome-deep-learning-papers"},{"type":"has_code","target_id":"github:terryum:awesome-deep-learning-papers","source_url":"https://github.com/terryum/awesome-deep-learning-papers"},{"type":"has_code","target_id":"github:terryum:awesome-deep-learning-papers","source_url":"https://github.com/terryum/awesome-deep-learning-papers"},{"type":"has_code","target_id":"github:oxford-cs-deepnlp-2017:lectures","source_url":"https://github.com/oxford-cs-deepnlp-2017/lectures"},{"type":"has_code","target_id":"github:terryum:awesome-deep-learning-papers","source_url":"https://github.com/terryum/awesome-deep-learning-papers"}]', NULL, NULL, 'pending', 70, '780e31a16f3226dd7b327b02028c9162', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-terryum-awesome-deep-learning-papers from https://github.com/terryum.png
Image converted to WebP: data/images/github-terryum-awesome-deep-learning-papers.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ageron-handson-ml', 'github--ageron--handson-ml', 'handson-ml', 'ageron', 'Machine Learning Notebooks ========================== This project is for the first edition, which is now outdated. <details> This project aims at teaching you the fundamentals of Machine Learning in python. It contains the example code and solutions to the exercises in my O''Reilly book Hands-on Machine Learning with Scikit-Learn and TensorFlow: Use any of the following services. **WARNING**: Please be aware that these services provide temporary environments: anything you do will be deleted a...', '["deep-learning","deprecated","distributed","jupyter-notebook","machine-learning","ml","neural-network","python","scikit-learn","tensorflow","jupyter notebook"]', 'other', 25797, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ageron/handson-ml","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', 'Machine Learning Notebooks\n==========================\n\n# âš  THE <a href="https://github.com/ageron/handson-ml3">THIRD EDITION OF MY BOOK</a> IS NOW AVAILABLE.\n\nThis project is for the first edition, which is now outdated.\n\n<details>\n\nThis project aims at teaching you the fundamentals of Machine Learning in\npython. It contains the example code and solutions to the exercises in my O''Reilly book [Hands-on Machine Learning with Scikit-Learn and TensorFlow](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/):\n\n[![book](http://akamaicovers.oreilly.com/images/9781491962282/cat.gif)](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)\n\n\n## Quick Start\n\n### Want to play with these notebooks online without having to install anything?\nUse any of the following services.\n\n**WARNING**: Please be aware that these services provide temporary environments: anything you do will be deleted after a while, so make sure you download any data you care about.\n\n* **Recommended**: open this repository in [Colaboratory](https://colab.research.google.com/github/ageron/handson-ml/blob/master/):\n<a href="https://colab.research.google.com/github/ageron/handson-ml/blob/master/"><img src="https://colab.research.google.com/img/colab_favicon.ico" width="90" /></a>\n\n* Or open it in [Binder](https://mybinder.org/v2/gh/ageron/handson-ml/master):\n<a href="https://mybinder.org/v2/gh/ageron/handson-ml/master"><img src="https://matthiasbussonnier.com/posts/img/binder_logo_128x128.png" width="90" /></a>\n\n  * _Note_: Most of the time, Binder starts up quickly and works great, but when handson-ml is updated, Binder creates a new environment from scratch, and this can take quite some time.\n\n* Or open it in [Deepnote](https://beta.deepnote.com/launch?template=data-science&url=https%3A//github.com/ageron/handson-ml/blob/master/index.ipynb):\n<a href="https://beta.deepnote.com/launch?template=data-science&url=https%3A//github.com/ageron/handson-ml/blob/master/index.ipynb"><img src="https://www.deepnote.com/static/illustration.png" width="150" /></a>\n\n### Just want to quickly look at some notebooks, without executing any code?\n\nBrowse this repository using [jupyter.org''s notebook viewer](https://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/index.ipynb):\n<a href="https://nbviewer.jupyter.org/github/ageron/handson-ml/blob/master/index.ipynb"><img src="https://jupyter.org/assets/logos/rectanglelogo-greytext-orangebody-greymoons.svg" width="150" /></a>\n\n_Note_: [github.com''s notebook viewer](index.ipynb) also works but it is slower and the math equations are not always displayed correctly.\n\n### Want to run this project using a Docker image?\nRead the [Docker instructions](https://github.com/ageron/handson-ml/tree/master/docker).\n\n### Want to install this project on your own machine?\n\nStart by installing [Anaconda](https://www.anaconda.com/distribution/) (or [Miniconda](https://docs.conda.io/en/latest/miniconda.html)), [git](https://git-scm.com/downloads), and if you have a TensorFlow-compatible GPU, install the [GPU driver](https://www.nvidia.com/Download/index.aspx), as well as the appropriate version of CUDA and cuDNN (see TensorFlow''s documentation for more details).\n\nNext, clone this project by opening a terminal and typing the following commands (do not type the first `$` signs on each line, they just indicate that these are terminal commands):\n\n    $ git clone https://github.com/ageron/handson-ml.git\n    $ cd handson-ml\n\nNext, run the following commands:\n\n    $ conda env create -f environment.yml\n    $ conda activate tf1\n    $ python -m ipykernel install --user --name=python3\n\nFinally, start Jupyter:\n\n    $ jupyter notebook\n\nIf you need further instructions, read the [detailed installation instructions](INSTALL.md).\n\n# FAQ\n\n**Which Python version should I use?**\n\nI recommend Python 3.7. If you follow the installation instructions above, that''s the version you will get. Most code will work with other versions of Python 3, but some libraries do not support Python 3.8 or 3.9 yet, which is why I recommend Python 3.7.\n\n**I''m getting an error when I call `load_housing_data()`**\n\nMake sure you call `fetch_housing_data()` *before* you call `load_housing_data()`. If you''re getting an HTTP error, make sure you''re running the exact same code as in the notebook (copy/paste it if needed). If the problem persists, please check your network configuration.\n\n**I''m getting an SSL error on MacOSX**\n\nYou probably need to install the SSL certificates (see this [StackOverflow question](https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error)). If you downloaded Python from the official website, then run `/Applications/Python\ 3.7/Install\ Certificates.command` in a terminal (change `3.7` to whatever version you installed). If you installed Python using MacPorts, run `sudo port install curl-ca-bundle` in a terminal.\n\n**I''ve installed this project locally. How do I update it to the latest version?**\n\nSee [INSTALL.md](INSTALL.md)\n\n**How do I update my Python libraries to the latest versions, when using Anaconda?**\n\nSee [INSTALL.md](INSTALL.md)\n\n## Contributors\nI would like to thank everyone [who contributed to this project](https://github.com/ageron/handson-ml/graphs/contributors), either by providing useful feedback, filing issues or submitting Pull Requests. Special thanks go to Haesun Park and Ian Beauregard who reviewed every notebook and submitted many PRs, including help on some of the exercise solutions. Thanks as well to Steven Bunkley and Ziembla who created the `docker` directory, and to github user SuperYorio who helped on some exercise solutions.\n\n</details>\n', '{"language":"Jupyter Notebook","stars":25797,"forks":12877,"watchers":25797,"open_issues":144,"topics":["deep-learning","deprecated","distributed","jupyter-notebook","machine-learning","ml","neural-network","python","scikit-learn","tensorflow"],"default_branch":"master","size_kb":87255,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:ageron:handson-ml3\">THIRD","source_url":"https://github.com/ageron/handson-ml3\">THIRD"},{"type":"has_code","target_id":"github:ageron:handson-ml","source_url":"https://github.com/ageron/handson-ml"},{"type":"has_code","target_id":"github:ageron:handson-ml.git","source_url":"https://github.com/ageron/handson-ml.git"},{"type":"has_code","target_id":"github:ageron:handson-ml","source_url":"https://github.com/ageron/handson-ml"}]', NULL, 'Apache-2.0', 'approved', 65, '0e2397a3a8f44c40c97454288d5c9aae', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ageron-handson-ml from https://github.com/ageron.png
Image converted to WebP: data/images/github-ageron-handson-ml.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-datawhalechina-pumpkin-book', 'github--datawhalechina--pumpkin-book', 'pumpkin-book', 'datawhalechina', 'â€œå‘¨å¿—åè€å¸ˆçš„ã€Šæœºå™¨å­¦ä¹ ã€‹ï¼ˆè¥¿ç“œä¹¦ï¼‰æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç»å…¸å…¥é—¨æ•™æä¹‹ä¸€ï¼Œå‘¨è€å¸ˆä¸ºäº†ä½¿å°½å¯èƒ½å¤šçš„è¯»è€…é€šè¿‡è¥¿ç“œä¹¦å¯¹æœºå™¨å­¦ä¹ æœ‰æ‰€äº†è§£, æ‰€ä»¥åœ¨ä¹¦ä¸­å¯¹éƒ¨åˆ†å…¬å¼çš„æ¨å¯¼ç»†èŠ‚æ²¡æœ‰è¯¦è¿°ï¼Œä½†æ˜¯è¿™å¯¹é‚£äº›æƒ³æ·±ç©¶å…¬å¼æ¨å¯¼ç»†èŠ‚çš„è¯»è€…æ¥è¯´å¯èƒ½â€œä¸å¤ªå‹å¥½â€ï¼Œæœ¬ä¹¦æ—¨åœ¨å¯¹è¥¿ç“œä¹¦é‡Œæ¯”è¾ƒéš¾ç†è§£çš„å…¬å¼åŠ ä»¥è§£æï¼Œä»¥åŠå¯¹éƒ¨åˆ†å…¬å¼è¡¥å……å…·ä½“çš„æ¨å¯¼ç»†èŠ‚ã€‚â€ è¯»åˆ°è¿™é‡Œï¼Œå¤§å®¶å¯èƒ½ä¼šç–‘é—®ä¸ºå•¥å‰é¢è¿™æ®µè¯åŠ äº†å¼•å·ï¼Œå› ä¸ºè¿™åªæ˜¯æˆ‘ä»¬æœ€åˆçš„éæƒ³ï¼Œåæ¥æˆ‘ä»¬äº†è§£åˆ°ï¼Œå‘¨è€å¸ˆä¹‹æ‰€ä»¥çœå»è¿™äº›æ¨å¯¼ç»†èŠ‚çš„çœŸå®åŸå› æ˜¯ï¼Œä»–æœ¬å°Šè®¤ä¸ºâ€œç†å·¥ç§‘æ•°å­¦åŸºç¡€æ‰å®ç‚¹çš„å¤§äºŒä¸‹å­¦ç”Ÿåº”è¯¥å¯¹è¥¿ç“œä¹¦ä¸­çš„æ¨å¯¼ç»†èŠ‚æ— å›°éš¾å§ï¼Œè¦ç‚¹åœ¨ä¹¦é‡Œéƒ½æœ‰äº†ï¼Œç•¥å»çš„ç»†èŠ‚åº”èƒ½è„‘è¡¥æˆ–åšç»ƒä¹ â€ã€‚æ‰€ä»¥......æœ¬å—ç“œä¹¦åªèƒ½ç®—æ˜¯æˆ‘ç­‰æ•°å­¦æ¸£æ¸£åœ¨è‡ªå­¦çš„æ—¶å€™è®°ä¸‹æ¥çš„ç¬”è®°ï¼Œå¸Œæœ›èƒ½å¤Ÿå¸®åŠ©å¤§å®¶éƒ½æˆä¸ºä¸€ååˆæ ¼çš„â€œç†å·¥ç§‘æ•°å­¦åŸºç¡€æ‰å®ç‚¹çš„å¤§äºŒä¸‹å­¦ç”Ÿâ€ã€‚ - å—ç“œä¹¦çš„æ‰€æœ‰å†…å®¹éƒ½æ˜¯ä»¥è¥¿ç“œä¹¦çš„å†…å®¹ä¸ºå‰ç½®çŸ¥è¯†è¿›è¡Œè¡¨è¿°çš„ï¼Œæ‰€ä»¥å—ç“œä¹¦çš„æœ€ä½³ä½¿ç”¨æ–¹æ³•æ˜¯ä»¥è¥¿ç“œä¹¦ä¸ºä¸»çº¿ï¼Œé‡åˆ°è‡ªå·±æ¨å¯¼ä¸å‡ºæ¥æˆ–è€…çœ‹ä¸æ‡‚çš„å…¬å¼æ—¶å†æ¥æŸ¥é˜…å—ç“œä¹¦ï¼› - å¯¹äºåˆå­¦æœºå™¨å­¦ä¹ çš„å°ç™½ï¼Œè¥¿ç“œä¹¦ç¬¬1ç« å’Œç¬¬2ç« çš„å…¬å¼**å¼ºçƒˆä¸å»ºè®®æ·±ç©¶**ï¼Œç®€å•è¿‡ä¸€ä¸‹å³å¯ï¼Œç­‰ä½ å­¦å¾—æœ‰ç‚¹é£˜çš„æ—¶å€™å†å›æ¥å•ƒéƒ½æ¥å¾—åŠ...', '["book","machine-learning","pumpkin-book"]', 'other', 25476, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/datawhalechina/pumpkin-book","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', 'â€œå‘¨å¿—åè€å¸ˆçš„ã€Šæœºå™¨å­¦ä¹ ã€‹ï¼ˆè¥¿ç“œä¹¦ï¼‰æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç»å…¸å…¥é—¨æ•™æä¹‹ä¸€ï¼Œå‘¨è€å¸ˆä¸ºäº†ä½¿å°½å¯èƒ½å¤šçš„è¯»è€…é€šè¿‡è¥¿ç“œä¹¦å¯¹æœºå™¨å­¦ä¹ æœ‰æ‰€äº†è§£, æ‰€ä»¥åœ¨ä¹¦ä¸­å¯¹éƒ¨åˆ†å…¬å¼çš„æ¨å¯¼ç»†èŠ‚æ²¡æœ‰è¯¦è¿°ï¼Œä½†æ˜¯è¿™å¯¹é‚£äº›æƒ³æ·±ç©¶å…¬å¼æ¨å¯¼ç»†èŠ‚çš„è¯»è€…æ¥è¯´å¯èƒ½â€œä¸å¤ªå‹å¥½â€ï¼Œæœ¬ä¹¦æ—¨åœ¨å¯¹è¥¿ç“œä¹¦é‡Œæ¯”è¾ƒéš¾ç†è§£çš„å…¬å¼åŠ ä»¥è§£æï¼Œä»¥åŠå¯¹éƒ¨åˆ†å…¬å¼è¡¥å……å…·ä½“çš„æ¨å¯¼ç»†èŠ‚ã€‚â€\n\nè¯»åˆ°è¿™é‡Œï¼Œå¤§å®¶å¯èƒ½ä¼šç–‘é—®ä¸ºå•¥å‰é¢è¿™æ®µè¯åŠ äº†å¼•å·ï¼Œå› ä¸ºè¿™åªæ˜¯æˆ‘ä»¬æœ€åˆçš„éæƒ³ï¼Œåæ¥æˆ‘ä»¬äº†è§£åˆ°ï¼Œå‘¨è€å¸ˆä¹‹æ‰€ä»¥çœå»è¿™äº›æ¨å¯¼ç»†èŠ‚çš„çœŸå®åŸå› æ˜¯ï¼Œä»–æœ¬å°Šè®¤ä¸ºâ€œç†å·¥ç§‘æ•°å­¦åŸºç¡€æ‰å®ç‚¹çš„å¤§äºŒä¸‹å­¦ç”Ÿåº”è¯¥å¯¹è¥¿ç“œä¹¦ä¸­çš„æ¨å¯¼ç»†èŠ‚æ— å›°éš¾å§ï¼Œè¦ç‚¹åœ¨ä¹¦é‡Œéƒ½æœ‰äº†ï¼Œç•¥å»çš„ç»†èŠ‚åº”èƒ½è„‘è¡¥æˆ–åšç»ƒä¹ â€ã€‚æ‰€ä»¥......æœ¬å—ç“œä¹¦åªèƒ½ç®—æ˜¯æˆ‘ç­‰æ•°å­¦æ¸£æ¸£åœ¨è‡ªå­¦çš„æ—¶å€™è®°ä¸‹æ¥çš„ç¬”è®°ï¼Œå¸Œæœ›èƒ½å¤Ÿå¸®åŠ©å¤§å®¶éƒ½æˆä¸ºä¸€ååˆæ ¼çš„â€œç†å·¥ç§‘æ•°å­¦åŸºç¡€æ‰å®ç‚¹çš„å¤§äºŒä¸‹å­¦ç”Ÿâ€ã€‚\n\n## ä½¿ç”¨è¯´æ˜\n- å—ç“œä¹¦çš„æ‰€æœ‰å†…å®¹éƒ½æ˜¯ä»¥è¥¿ç“œä¹¦çš„å†…å®¹ä¸ºå‰ç½®çŸ¥è¯†è¿›è¡Œè¡¨è¿°çš„ï¼Œæ‰€ä»¥å—ç“œä¹¦çš„æœ€ä½³ä½¿ç”¨æ–¹æ³•æ˜¯ä»¥è¥¿ç“œä¹¦ä¸ºä¸»çº¿ï¼Œé‡åˆ°è‡ªå·±æ¨å¯¼ä¸å‡ºæ¥æˆ–è€…çœ‹ä¸æ‡‚çš„å…¬å¼æ—¶å†æ¥æŸ¥é˜…å—ç“œä¹¦ï¼›\n- å¯¹äºåˆå­¦æœºå™¨å­¦ä¹ çš„å°ç™½ï¼Œè¥¿ç“œä¹¦ç¬¬1ç« å’Œç¬¬2ç« çš„å…¬å¼**å¼ºçƒˆä¸å»ºè®®æ·±ç©¶**ï¼Œç®€å•è¿‡ä¸€ä¸‹å³å¯ï¼Œç­‰ä½ å­¦å¾—æœ‰ç‚¹é£˜çš„æ—¶å€™å†å›æ¥å•ƒéƒ½æ¥å¾—åŠï¼›\n- æ¯ä¸ªå…¬å¼çš„è§£æå’Œæ¨å¯¼æˆ‘ä»¬éƒ½åŠ›äº‰ä»¥æœ¬ç§‘æ•°å­¦åŸºç¡€çš„è§†è§’è¿›è¡Œè®²è§£ï¼Œæ‰€ä»¥è¶…çº²çš„æ•°å­¦çŸ¥è¯†æˆ‘ä»¬é€šå¸¸éƒ½ä¼šä»¥é™„å½•å’Œå‚è€ƒæ–‡çŒ®çš„å½¢å¼ç»™å‡ºï¼Œæ„Ÿå…´è¶£çš„åŒå­¦å¯ä»¥ç»§ç»­æ²¿ç€æˆ‘ä»¬ç»™çš„èµ„æ–™è¿›è¡Œæ·±å…¥å­¦ä¹ ï¼›\n- è‹¥å—ç“œä¹¦é‡Œæ²¡æœ‰ä½ æƒ³è¦æŸ¥é˜…çš„å…¬å¼ï¼Œæˆ–è€…ä½ å‘ç°å—ç“œä¹¦å“ªä¸ªåœ°æ–¹æœ‰é”™è¯¯ï¼Œè¯·æ¯«ä¸çŠ¹è±«åœ°å»æˆ‘ä»¬GitHubçš„Issuesï¼ˆ åœ°å€ï¼š[https://github.com/datawhalechina/pumpkin-book/issues](https://github.com/datawhalechina/pumpkin-book/issues) ï¼‰è¿›è¡Œåé¦ˆï¼Œåœ¨å¯¹åº”ç‰ˆå—æäº¤ä½ å¸Œæœ›è¡¥å……çš„å…¬å¼ç¼–å·æˆ–è€…å‹˜è¯¯ä¿¡æ¯ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šåœ¨24å°æ—¶ä»¥å†…ç»™æ‚¨å›å¤ï¼Œè¶…è¿‡24å°æ—¶æœªå›å¤çš„è¯å¯ä»¥å¾®ä¿¡è”ç³»æˆ‘ä»¬ï¼ˆå¾®ä¿¡å·ï¼šat-Sm1lesï¼‰ï¼›\n\n## é…å¥—èµ„æº\nè§†é¢‘æ•™ç¨‹ï¼š[https://www.bilibili.com/video/BV1Mh411e7VU](https://www.bilibili.com/video/BV1Mh411e7VU)\n\nç»„é˜Ÿå­¦ä¹ ï¼š[https://www.datawhale.cn/learn/summary/2](https://www.datawhale.cn/learn/summary/2)\n\nåœ¨çº¿é˜…è¯»ï¼š[https://www.datawhale.cn/learn/summary/2](https://www.datawhale.cn/learn/summary/2)\n\nPDFç‰ˆæœ¬ä¸‹è½½ï¼š[https://github.com/datawhalechina/pumpkin-book/releases](https://github.com/datawhalechina/pumpkin-book/releases)\n\n## çº¸è´¨ç‰ˆ\n<img src="https://raw.githubusercontent.com/datawhalechina/pumpkin-book/master/res/nangua_v2.jpg" width="300" height="316">\n\nè´­ä¹°é“¾æ¥ï¼š[äº¬ä¸œ](https://item.jd.com/13989990.html) | [å½“å½“](http://product.dangdang.com/29579286.html) | [å¤©çŒ«](https://detail.tmall.com/item.htm?abbucket=12&id=720482241470)\n\nå‹˜è¯¯è¡¨ï¼š[https://datawhalechina.github.io/pumpkin-book/#/errata](https://datawhalechina.github.io/pumpkin-book/#/errata)\n\n### çº¸è´¨ç‰ˆå’Œå¼€æºç‰ˆçš„åŒºåˆ«\n\nå¼€æºç‰ˆæœ¬æ˜¯æˆ‘ä»¬å¯„é€å‡ºç‰ˆç¤¾çš„å…¨ä¹¦åˆç¨¿ï¼Œç»ç”±äººæ°‘é‚®ç”µå‡ºç‰ˆç¤¾çš„ç¼–è¾‘è€å¸ˆä»¬å¯¹åˆç¨¿è¿›è¡Œäº†åå¤ä¿®ç¼®æœ€ç»ˆè¯ç”Ÿäº†çº¸è´¨ä¹¦ç±ï¼Œåœ¨æ­¤å‘äººæ°‘é‚®ç”µå‡ºç‰ˆç¤¾çš„ç¼–è¾‘è€å¸ˆçš„è®¤çœŸä¸¥è°¨è¡¨ç¤ºè¡·å¿ƒçš„æ„Ÿè°¢ï¼ï¼ˆé™„ï¼šæ ¡å¯¹æ ·ç¨¿ï¼‰\n\n<img src="https://raw.githubusercontent.com/datawhalechina/pumpkin-book/master/res/yanggao1.jpg" width="300" height="225">\n\n<img src="https://raw.githubusercontent.com/datawhalechina/pumpkin-book/master/res/yanggao2.jpg" width="300" height="225">\n\n## é…å¥—çš„è¥¿ç“œä¹¦ç‰ˆæœ¬\n<img src="https://raw.githubusercontent.com/datawhalechina/pumpkin-book/master/res/xigua.jpg" width="300" height= "350">\n\nç‰ˆæ¬¡ï¼š2016å¹´1æœˆç¬¬1ç‰ˆ\n\nå‹˜è¯¯è¡¨ï¼š[http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm](http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/MLbook2016.htm)\n\n## ç¼–å§”ä¼š\n| èŒè´£ | åå• |\n| :---: | :---: |\n| **ä¸»ç¼–** | [@Sm1les](https://github.com/Sm1les) [@archwalker](https://github.com/archwalker) [@jbb0523](https://blog.csdn.net/jbb0523)|\n| **ç¼–å§”** | [@juxiao](https://github.com/juxiao) [@Majingmin](https://github.com/Majingmin) [@MrBigFan](https://github.com/MrBigFan) [@shanry](https://github.com/shanry) [@Ye980226](https://github.com/Ye980226) |\n\n## å°é¢è®¾è®¡\n| æ„æ€ | åˆ›ä½œ |\n| :---: | :---: |\n| [@Sm1les](https://github.com/Sm1les) | æ—ç‹èŒ‚ç›› | \n\n## è‡´è°¢\nç‰¹åˆ«æ„Ÿè°¢[@awyd234](https://github.com/awyd234)ã€[@feijuan](https://github.com/feijuan)ã€[@Ggmatch](https://github.com/Ggmatch)ã€[@Heitao5200](https://github.com/Heitao5200)ã€[@xhqing](https://github.com/xhqing)ã€[@LongJH](https://github.com/LongJH)ã€[@LilRachel](https://github.com/LilRachel)ã€[@LeoLRH](https://github.com/LeoLRH)ã€[@Nono17](https://github.com/Nono17)ã€[@spareribs](https://github.com/spareribs)ã€[@sunchaothu](https://github.com/sunchaothu)ã€[@StevenLzq](https://github.com/StevenLzq) åœ¨æœ€æ—©æœŸçš„æ—¶å€™å¯¹å—ç“œä¹¦æ‰€åšçš„è´¡çŒ®ã€‚\n\n## å…³æ³¨æˆ‘ä»¬\næ‰«æä¸‹æ–¹äºŒç»´ç å…³æ³¨å…¬ä¼—å·ï¼šDatawhaleï¼Œç„¶åå‘é€â€œå—ç“œä¹¦â€ï¼Œå³å¯è·å–â€œå—ç“œä¹¦è¯»è€…äº¤æµç¾¤â€å…¥ç¾¤æ–¹å¼\n\n<img src="https://raw.githubusercontent.com/datawhalechina/pumpkin-book/master/res/qrcode.jpeg" width="180" height="180">\n\n## LICENSE\næœ¬ä½œå“é‡‡ç”¨[çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™…è®¸å¯åè®®](http://creativecommons.org/licenses/by-nc-sa/4.0/)è¿›è¡Œè®¸å¯ã€‚\n', '{"language":null,"stars":25476,"forks":4818,"watchers":25476,"open_issues":3,"topics":["book","machine-learning","pumpkin-book"],"default_branch":"master","size_kb":11050,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:datawhalechina:pumpkin-book","source_url":"https://github.com/datawhalechina/pumpkin-book"},{"type":"has_code","target_id":"github:datawhalechina:pumpkin-book","source_url":"https://github.com/datawhalechina/pumpkin-book"},{"type":"has_code","target_id":"github:datawhalechina:pumpkin-book","source_url":"https://github.com/datawhalechina/pumpkin-book"},{"type":"has_code","target_id":"github:datawhalechina:pumpkin-book","source_url":"https://github.com/datawhalechina/pumpkin-book"}]', NULL, 'NOASSERTION', 'approved', 65, '97b04ebc8dadafb2e5a304bfc3a2508f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-datawhalechina-pumpkin-book from https://github.com/datawhalechina.png
Image converted to WebP: data/images/github-datawhalechina-pumpkin-book.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-modular-modular', 'github--modular--modular', 'modular', 'modular', '<div align="center"> <img src="https://modular-assets.s3.amazonaws.com/images/modular_github_logo_bg.png"> [About Modular] | [Get started] | [API docs] | [Contributing] | [Changelog] </div> [About Modular]: https://www.modular.com/ [Get started]: https://docs.modular.com/max/get-started [API docs]: https://docs.modular.com/max/api [Contributing]: ./CONTRIBUTING.md [Changelog]: https://docs.modular.com/max/changelog --- [Join us next Thursday, December 11th][dec-meetup] at Modular''s Los Altos ...', '["ai","language","machine-learning","max","modular","mojo","programming-language","mojo"]', 'other', 25312, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/modular/modular","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n    <img src="https://modular-assets.s3.amazonaws.com/images/modular_github_logo_bg.png">\n\n  [About Modular] | [Get started] | [API docs] | [Contributing] | [Changelog]\n</div>\n\n[About Modular]: https://www.modular.com/\n[Get started]: https://docs.modular.com/max/get-started\n[API docs]: https://docs.modular.com/max/api\n[Contributing]: ./CONTRIBUTING.md\n[Changelog]: https://docs.modular.com/max/changelog\n\n---\n[Join us next Thursday, December 11th][dec-meetup] at Modular''s Los Altos\noffices for a [Modular Meetup][meetup-group] going inside the MAX platform!\n\n# Modular Platform\n\n> A unified platform for AI development and deployment, including **MAX**ğŸ§‘â€ğŸš€ and\n**Mojo**ğŸ”¥.\n\nThe Modular Platform is an open and fully-integrated suite of AI libraries\nand tools that accelerates model serving and scales GenAI deployments. It\nabstracts away hardware complexity so you can run the most popular open\nmodels with industry-leading GPU and CPU performance without any code changes.\n\n![](https://docs.modular.com/images/modular-container-stack.png?20250513)\n\n## Get started\n\nYou don''t need to clone this repo.\n\nYou can install Modular as a `pip` or `conda` package and then start an\nOpenAI-compatible endpoint with a model of your choice.\n\nTo get started with the Modular Platform and serve a model using the MAX\nframework, see [the quickstart guide](https://docs.modular.com/max/get-started).\n\n> [!NOTE]\n> **Nightly vs. stable releases**\n> If you cloned the repo and want a stable release, run\n  `git checkout modular/vX.X` to match the version.\n> The `main` branch tracks nightly builds, while the `stable` branch matches\n  the latest released version.\n\nAfter your model endpoint is up and running, you can start sending the model\ninference requests using\n[our OpenAI-compatible REST API](https://docs.modular.com/max/api/serve).\n\nTry running hundreds of other models from\n[our model repository](https://builds.modular.com/?category=models).\n\n## Deploy our container\n\nThe MAX container is our Kubernetes-compatible Docker container for convenient\ndeployment, which uses the MAX framework''s built-in inference server. We have\nseparate containers for NVIDIA and AMD GPU environments, and a unified container\nthat works with both.\n\nFor example, you can start a container for an NVIDIA GPU with this command:\n\n```sh\ndocker run --gpus=1 \\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\n    -p 8000:8000 \\n    modular/max-nvidia-full:latest \\n    --model-path google/gemma-3-27b-it\n```\n\nFor more information, see our [MAX container\ndocs](https://docs.modular.com/max/container) or the [Modular Docker Hub\nrepository](https://hub.docker.com/u/modular).\n\n## About the repo\n\nWe''re constantly open-sourcing more of the Modular Platform and you can find\nall of it in here. As of May, 2025, this repo includes over 450,000 lines of\ncode from over 6000 contributors, providing developers with production-grade\nreference implementations and tools to extend the Modular Platform with new\nalgorithms, operations, and hardware targets. It is quite likely **the world''s\nlargest repository of open source CPU and GPU kernels**!\n\nHighlights include:\n\n- Mojo standard library: [/mojo/stdlib](mojo/stdlib)\n- MAX GPU and CPU kernels: [/max/kernels](max/kernels) (Mojo kernels)\n- MAX inference server: [/max/serve](max/serve) (OpenAI-compatible endpoint)\n- MAX model pipelines: [/max/pipelines](max/pipelines) (Python-based graphs)\n- Code example: [/examples](examples)\n\nThis repo has two major branches:\n\n- The [`main`](https://github.com/modular/modular/tree/main) branch, which is\nin sync with the nightly build and subject to new bugs. Use this branch for\n[contributions](./CONTRIBUTING.md), or if you [installed the nightly\nbuild](https://docs.modular.com/max/packages).\n\n- The [`stable`](https://github.com/modular/modular/tree/stable) branch, which\nis in sync with the last stable released version of Mojo. Use the examples in\nhere if you [installed the stable\nbuild](https://docs.modular.com/max/packages).\n\n## News & Announcements\n\n**[2025/11]** [Modular Platform 25.7][25.7] provides a fully open MAX Python\nAPI, expanded hardware support for NVIDIA Grace superchips, improved Mojo GPU\nprogramming experience, and much more.\n\n**[2025/11]** We met with the community at\n[PyTorch 2025 + the LLVM Developers'' Meeting][pytorch-llvm] to solicit\ncommunity input into how the Modular platform can reduce fragmentation and\nprovide a unified AI stack.\n\n**[2025/09]** [Modular raises $250M][funding] to scale AI''s unified compute\nlayer, bringing total funding to $380M at a $1.6B valuation.\n\n**[2025/09]** [Modular Platform 25.6][25.6] delivers a unified compute layer\nspanning from laptops to datacenter GPUs, with industry-leading throughput on\nNVIDIA Blackwell (B200) and AMD MI355X.\n\n**[2025/08]** [Modular Platform 25.5][25.5] introduces Large Scale Batch\nInference through a partnership with SF Compute + open source launch of the\nMAX Graph API and more.\n\n**[2025/08]** We hosted our [Los Altos Meetup][la-meetup] featuring talks from\nChris Lattner on democratizing AI compute and Inworld AI on production voice AI.\n\n**[2025/06]** [AMD partnership announced][amd] â€” Modular Platform now generally\navailable across AMD''s MI300 and MI325 GPU portfolio.\n\n**[2025/06]** [Modular Hack Weekend][hack-weekend] brought developers together\nto build custom kernels, model architectures, and PyTorch custom ops with\nMojo and MAX.\n\n**[2025/05]** Over 100 engineers gathered at AGI House for our first\n[GPU Kernel Hackathon][hackathon], featuring talks from Modular and\nAnthropic engineers.\n\n[25.7]: https://www.modular.com/blog/modular-25-7-faster-inference-safer-gpu-programming-and-a-more-unified-developer-experience\n[pytorch-llvm]: https://www.modular.com/blog/pytorch-and-llvm-in-2025-keeping-up-with-ai-innovation\n[25.6]: https://www.modular.com/blog/modular-25-6-unifying-the-latest-gpus-from-nvidia-amd-and-apple\n[25.5]: https://www.modular.com/blog/modular-platform-25-5\n[la-meetup]: https://lu.ma/modular-aug-meetup\n[amd]: https://www.modular.com/blog/modular-x-amd-unleashing-ai-performance-on-amd-gpus\n[hack-weekend]: https://www.meetup.com/modular-meetup-group/events/308311461/\n[hackathon]: https://www.modular.com/blog/modverse-48\n[dec-meetup]: https://www.meetup.com/modular-meetup-group/events/311998048/\n[meetup-group]: https://www.meetup.com/modular-meetup-group/\n\n---\n\n## Community & Events\n\nWe host regular meetups, hackathons, and community calls. Join us!\n\n| Channel               | Link                                |\n|-----------------------|-------------------------------------|\n| ğŸ’¬ Discord            | [discord.gg/modular][discord]       |\n| ğŸ’¬ Forum              | [forum.modular.com][forum]          |\n| ğŸ“… Meetup Group       | [meetup.com/modular-meetup-group][meetup-group] |\n| ğŸ¥ Community Meetings | Recordings on [YouTube][youtube]    |\n\n**Upcoming events** will be posted on our [Meetup page][meetup-group] and\n[Discord][discord].\n\n[discord]: https://discord.gg/modular\n[forum]: https://forum.modular.com/\n[youtube]: https://www.youtube.com/@modularinc\n\n## Contribute\n\nThanks for your interest in contributing to this repository!\n\nWe accept contributions to the [Mojo standard library](./mojo), [MAX AI\nkernels](./max/kernels), code examples, and Mojo docs, but currently not to any\nother parts of the repository.\n\nPlease see the [Contribution Guide](./CONTRIBUTING.md) for instructions.\n\nWe also welcome your bug reports.  If you have a bug, please [file an issue\nhere](https://github.com/modular/modular/issues/new/choose).\n\n## Contact us\n\nIf you''d like to chat with the team and other community members, please send a\nmessage to our [Discord channel](https://discord.gg/modular) and [our\nforum board](https://forum.modular.com/).\n\n## License\n\nThis repository and its contributions are licensed under the Apache License\nv2.0 with LLVM Exceptions (see the LLVM [License](https://llvm.org/LICENSE.txt)).\nModular, MAX and Mojo usage and distribution are licensed under the\n[Modular Community License](https://www.modular.com/legal/community).\n\n### Third party licenses\n\nYou are entirely responsible for checking and validating the licenses of\nthird parties (i.e. Huggingface) for related software and libraries that are downloaded.\n\n## Thanks to our contributors\n\n<a href="https://github.com/modular/modular/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=modular/modular" />\n</a>\n', '{"language":"Mojo","stars":25312,"forks":2736,"watchers":25312,"open_issues":788,"topics":["ai","language","machine-learning","max","modular","mojo","programming-language"],"default_branch":"main","size_kb":132873,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:modular:modular","source_url":"https://github.com/modular/modular"},{"type":"has_code","target_id":"github:modular:modular","source_url":"https://github.com/modular/modular"},{"type":"has_code","target_id":"github:modular:modular","source_url":"https://github.com/modular/modular"},{"type":"has_code","target_id":"github:modular:modular","source_url":"https://github.com/modular/modular"}]', NULL, 'NOASSERTION', 'approved', 65, 'ee1744892ede74755510c105f3daf067', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-modular-modular from https://github.com/modular.png
Image converted to WebP: data/images/github-modular-modular.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-shap-shap', 'github--shap--shap', 'shap', 'shap', '<p align="center"> <img src="https://raw.githubusercontent.com/shap/shap/master/docs/artwork/shap_header.svg" width="800" /> </p> --- !License !Tests !Downloads **SHAP (SHapley Additive exPlanations)** is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see papers for details and citations). <!--**SHAP (SHapley Additive exP...', '["deep-learning","explainability","gradient-boosting","interpretability","machine-learning","shap","shapley","jupyter notebook"]', 'other', 24809, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/shap/shap","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '\n\n<p align="center">\n  <img src="https://raw.githubusercontent.com/shap/shap/master/docs/artwork/shap_header.svg" width="800" />\n</p>\n\n---\n[![PyPI](https://img.shields.io/pypi/v/shap)](https://pypi.org/project/shap/)\n[![Conda](https://img.shields.io/conda/vn/conda-forge/shap)](https://anaconda.org/conda-forge/shap)\n![License](https://img.shields.io/github/license/shap/shap)\n![Tests](https://github.com/shap/shap/actions/workflows/run_tests.yml/badge.svg)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/shap/shap/master)\n[![Documentation Status](https://readthedocs.org/projects/shap/badge/?version=latest)](https://shap.readthedocs.io/en/latest/?badge=latest)\n![Downloads](https://img.shields.io/pypi/dm/shap)\n[![PyPI pyversions](https://img.shields.io/pypi/pyversions/shap)](https://pypi.org/pypi/shap/)\n\n\n**SHAP (SHapley Additive exPlanations)** is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see [papers](#citations) for details and citations).\n\n<!--**SHAP (SHapley Additive exPlanations)** is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see our [papers](#citations) for details and citations).-->\n\n\n\n## Install\n\nSHAP can be installed from either [PyPI](https://pypi.org/project/shap) or [conda-forge](https://anaconda.org/conda-forge/shap):\n\n<pre>\npip install shap\n<i>or</i>\nconda install -c conda-forge shap\n</pre>\n\n## Tree ensemble example (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models)\n\nWhile SHAP can explain the output of any machine learning model, we have developed a high-speed exact algorithm for tree ensemble methods (see our [Nature MI paper](https://rdcu.be/b0z70)). Fast C++ implementations are supported for *XGBoost*, *LightGBM*, *CatBoost*, *scikit-learn* and *pyspark* tree models:\n\n```python\nimport xgboost\nimport shap\n\n# train an XGBoost model\nX, y = shap.datasets.california()\nmodel = xgboost.XGBRegressor().fit(X, y)\n\n# explain the model''s predictions using SHAP\n# (same syntax works for LightGBM, CatBoost, scikit-learn, transformers, Spark, etc.)\nexplainer = shap.Explainer(model)\nshap_values = explainer(X)\n\n# visualize the first prediction''s explanation\nshap.plots.waterfall(shap_values[0])\n```\n\n<p align="center">\n  <img width="616" src="./docs/artwork/california_waterfall.png" />\n</p>\n\nThe above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue. Another way to visualize the same explanation is to use a force plot (these are introduced in our [Nature BME paper](https://rdcu.be/baVbR)):\n\n```python\n# visualize the first prediction''s explanation with a force plot\nshap.plots.force(shap_values[0])\n```\n\n<p align="center">\n  <img width="811" src="./docs/artwork/california_instance.png" />\n</p>\n\nIf we take many force plot explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset (in the notebook this plot is interactive):\n\n```python\n# visualize all the training set predictions\nshap.plots.force(shap_values[:500])\n```\n\n<p align="center">\n  <img width="811" src="./docs/artwork/california_dataset.png" />\n</p>\n\nTo understand how a single feature effects the output of the model we can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. Since SHAP values represent a feature''s responsibility for a change in the model output, the plot below represents the change in predicted house price as the latitude changes. Vertical dispersion at a single value of latitude represents interaction effects with other features. To help reveal these interactions we can color by another feature. If we pass the whole explanation tensor to the `color` argument the scatter plot will pick the best feature to color by. In this case it picks longitude.\n\n```python\n# create a dependence scatter plot to show the effect of a single feature across the whole dataset\nshap.plots.scatter(shap_values[:, "Latitude"], color=shap_values)\n```\n\n<p align="center">\n  <img width="544" src="./docs/artwork/california_scatter.png" />\n</p>\n\n\nTo get an overview of which features are most important for a model we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). This reveals for example that higher median incomes increases the predicted home price.\n\n```python\n# summarize the effects of all the features\nshap.plots.beeswarm(shap_values)\n```\n\n<p align="center">\n  <img width="583" src="./docs/artwork/california_beeswarm.png" />\n</p>\n\nWe can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):\n\n```python\nshap.plots.bar(shap_values)\n```\n\n<p align="center">\n  <img width="570" src="./docs/artwork/california_global_bar.png" />\n</p>\n\n## Natural language example (transformers)\n\nSHAP has specific support for natural language models like those in the Hugging Face transformers library. By adding coalitional rules to traditional Shapley values we can form games that explain large modern NLP model using very few function evaluations. Using this functionality is as simple as passing a supported transformers pipeline to SHAP:\n\n```python\nimport transformers\nimport shap\n\n# load a transformers pipeline model\nmodel = transformers.pipeline(''sentiment-analysis'', return_all_scores=True)\n\n# explain the model on two sample inputs\nexplainer = shap.Explainer(model)\nshap_values = explainer(["What a great movie! ...if you have no taste."])\n\n# visualize the first prediction''s explanation for the POSITIVE output class\nshap.plots.text(shap_values[0, :, "POSITIVE"])\n```\n\n<p align="center">\n  <img width="811" src="https://raw.githubusercontent.com/shap/shap/master/docs/artwork/sentiment_analysis_plot.png" />\n</p>\n\n## Deep learning example with DeepExplainer (TensorFlow/Keras models)\n\nDeep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with [DeepLIFT](https://arxiv.org/abs/1704.02685) described in the SHAP NIPS paper. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT. TensorFlow models and Keras models using the TensorFlow backend are supported (there is also preliminary support for PyTorch):\n\n```python\n# ...include code from https://github.com/keras-team/keras/blob/master/examples/demo_mnist_convnet.py\n\nimport shap\nimport numpy as np\n\n# select a set of background examples to take an expectation over\nbackground = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]\n\n# explain predictions of the model on four images\ne = shap.DeepExplainer(model, background)\n# ...or pass tensors directly\n# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)\nshap_values = e.shap_values(x_test[1:5])\n\n# plot the feature attributions\nshap.image_plot(shap_values, -x_test[1:5])\n```\n\n<p align="center">\n  <img width="820" src="https://raw.githubusercontent.com/shap/shap/master/docs/artwork/mnist_image_plot.png" />\n</p>\n\nThe plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model''s output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the ''zero'' image the blank middle is important, while for the ''four'' image the lack of a connection on top makes it a four instead of a nine.\n\n\n## Deep learning example with GradientExplainer (TensorFlow/Keras/PyTorch models)\n\nExpected gradients combines ideas from [Integrated Gradients](https://arxiv.org/abs/1703.01365), SHAP, and [SmoothGrad](https://arxiv.org/abs/1706.03825) into a single expected value equation. This allows an entire dataset to be used as the background distribution (as opposed to a single reference value) and allows local smoothing. If we approximate the model with a linear function between each background data sample and the current input to be explained, and we assume the input features are independent then expected gradients will compute approximate SHAP values. In the example below we have explained how the 7th intermediate layer of the VGG16 ImageNet model impacts the output probabilities.\n\n```python\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nimport keras.backend as K\nimport numpy as np\nimport json\nimport shap\n\n# load pre-trained model and choose two images to explain\nmodel = VGG16(weights=''imagenet'', include_top=True)\nX,y = shap.datasets.imagenet50()\nto_explain = X[[39,41]]\n\n# load the ImageNet class names\nurl = "https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json"\nfname = shap.datasets.cache(url)\nwith open(fname) as f:\n    class_names = json.load(f)\n\n# explain how the input to the 7th layer of the model explains the top two classes\ndef map2layer(x, layer):\n    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))\n    return K.get_session().run(model.layers[layer].input, feed_dict)\ne = shap.GradientExplainer(\n    (model.layers[7].input, model.layers[-1].output),\n    map2layer(X, 7),\n    local_smoothing=0 # std dev of smoothing noise\n)\nshap_values,indexes = e.shap_values(map2layer(to_explain, 7), ranked_outputs=2)\n\n# get the names for the classes\nindex_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n\n# plot the explanations\nshap.image_plot(shap_values, to_explain, index_names)\n```\n\n<p align="center">\n  <img width="500" src="https://raw.githubusercontent.com/shap/shap/master/docs/artwork/gradient_imagenet_plot.png" />\n</p>\n\nPredictions for two input images are explained in the plot above. Red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. By using `ranked_outputs=2` we explain only the two most likely classes for each input (this spares us from explaining all 1,000 classes).\n\n## Model agnostic example with KernelExplainer (explains any function)\n\nKernel SHAP uses a specially-weighted local linear regression to estimate SHAP values for any model. Below is a simple example for explaining a multi-class SVM on the classic iris dataset.\n\n```python\nimport sklearn\nimport shap\nfrom sklearn.model_selection import train_test_split\n\n# print the JS visualization code to the notebook\nshap.initjs()\n\n# train a SVM classifier\nX_train,X_test,Y_train,Y_test = train_test_split(*shap.datasets.iris(), test_size=0.2, random_state=0)\nsvm = sklearn.svm.SVC(kernel=''rbf'', probability=True)\nsvm.fit(X_train, Y_train)\n\n# use Kernel SHAP to explain test set predictions\nexplainer = shap.KernelExplainer(svm.predict_proba, X_train, link="logit")\nshap_values = explainer.shap_values(X_test, nsamples=100)\n\n# plot the SHAP values for the Setosa output of the first instance\nshap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link="logit")\n```\n<p align="center">\n  <img width="810" src="https://raw.githubusercontent.com/shap/shap/master/docs/artwork/iris_instance.png" />\n</p>\n\nThe above explanation shows four features each contributing to push the model output from the base value (the average model output over the training dataset we passed) towards zero. If there were any features pushing the class label higher they would be shown in red.\n\nIf we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset. This is exactly what we do below for all the examples in the iris test set:\n\n```python\n# plot the SHAP values for the Setosa output of all instances\nshap.force_plot(explainer.expected_value[0], shap_values[0], X_test, link="logit")\n```\n<p align="center">\n  <img width="813" src="https://raw.githubusercontent.com/shap/shap/master/docs/artwork/iris_dataset.png" />\n</p>\n\n## SHAP Interaction Values\n\nSHAP interaction values are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented for tree models with `shap.TreeExplainer(model).shap_interaction_values(X)`. This returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. These values often reveal interesting hidden relationships, such as how the increased risk of death peaks for men at age 60 (see the NHANES notebook for details):\n\n<p align="center">\n  <img width="483" src="https://raw.githubusercontent.com/shap/shap/master/docs/artwork/nhanes_age_sex_interaction.png" />\n</p>\n\n## Sample notebooks\n\nThe notebooks below demonstrate different use cases for SHAP. Look inside the notebooks directory of the repository if you want to try playing with the original notebooks yourself.\n\n### TreeExplainer\n\nAn implementation of Tree SHAP, a fast and exact algorithm to compute SHAP values for trees and ensembles of trees.\n\n- [**NHANES survival model with XGBoost and SHAP interaction values**](https://shap.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html) - Using mortality data from 20 years of followup this notebook demonstrates how to use XGBoost and `shap` to uncover complex risk factor relationships.\n\n- [**Census income classification with LightGBM**](https://shap.github.io/shap/notebooks/tree_explainer/Census%20income%20classification%20with%20LightGBM.html) - Using the standard adult census income dataset, this notebook trains a gradient boosting tree model with LightGBM and then explains predictions using `shap`.\n\n- [**League of Legends Win Prediction with XGBoost**](https://shap.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html) - Using a Kaggle dataset of 180,000 ranked matches from League of Legends we train and explain a gradient boosting tree model with XGBoost to predict if a player will win their match.\n\n### DeepExplainer\n\nAn implementation of Deep SHAP, a faster (but only approximate) algorithm to compute SHAP values for deep learning models that is based on connections between SHAP and the DeepLIFT algorithm.\n\n- [**MNIST Digit classification with Keras**](https://shap.github.io/shap/notebooks/deep_explainer/Front%20Page%20DeepExplainer%20MNIST%20Example.html) - Using the MNIST handwriting recognition dataset, this notebook trains a neural network with Keras and then explains predictions using `shap`.\n\n- [**Keras LSTM for IMDB Sentiment Classification**](https://shap.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html) - This notebook trains an LSTM with Keras on the IMDB text sentiment analysis dataset and then explains predictions using `shap`.\n\n### GradientExplainer\n\nAn implementation of expected gradients to approximate SHAP values for deep learning models. It is based on connections between SHAP and the Integrated Gradients algorithm. GradientExplainer is slower than DeepExplainer and makes different approximation assumptions.\n\n- [**Explain an Intermediate Layer of VGG16 on ImageNet**](https://shap.github.io/shap/notebooks/gradient_explainer/Explain%20an%20Intermediate%20Layer%20of%20VGG16%20on%20ImageNet.html) - This notebook demonstrates how to explain the output of a pre-trained VGG16 ImageNet model using an internal convolutional layer.\n\n### LinearExplainer\n\nFor a linear model with independent features we can analytically compute the exact SHAP values. We can also account for feature correlation if we are willing to estimate the feature covariance matrix. LinearExplainer supports both of these options.\n\n- [**Sentiment Analysis with Logistic Regression**](https://shap.github.io/shap/notebooks/linear_explainer/Sentiment%20Analysis%20with%20Logistic%20Regression.html) - This notebook demonstrates how to explain a linear logistic regression sentiment analysis model.\n\n### KernelExplainer\n\nAn implementation of Kernel SHAP, a model agnostic method to estimate SHAP values for any model. Because it makes no assumptions about the model type, KernelExplainer is slower than the other model type specific algorithms.\n\n- [**Census income classification with scikit-learn**](https://shap.github.io/shap/notebooks/Census%20income%20classification%20with%20scikit-learn.html) - Using the standard adult census income dataset, this notebook trains a k-nearest neighbors classifier using scikit-learn and then explains predictions using `shap`.\n\n- [**ImageNet VGG16 Model with Keras**](https://shap.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html) - Explain the classic VGG16 convolutional neural network''s predictions for an image. This works by applying the model agnostic Kernel SHAP method to a super-pixel segmented image.\n\n- [**Iris classification**](https://shap.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html) - A basic demonstration using the popular iris species dataset. It explains predictions from six different models in scikit-learn using `shap`.\n\n## Documentation notebooks\n\nThese notebooks comprehensively demonstrate how to use specific functions and objects.\n\n- [`shap.decision_plot` and `shap.multioutput_decision_plot`](https://shap.github.io/shap/notebooks/plots/decision_plot.html)\n\n- [`shap.dependence_plot`](https://shap.github.io/shap/notebooks/plots/dependence_plot.html)\n\n## Methods Unified by SHAP\n\n1. *LIME:* Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Why should i trust you?: Explaining the predictions of any classifier." Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.\n\n2. *Shapley sampling values:* Strumbelj, Erik, and Igor Kononenko. "Explaining prediction models and individual predictions with feature contributions." Knowledge and information systems 41.3 (2014): 647-665.\n\n3. *DeepLIFT:* Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. "Learning important features through propagating activation differences." arXiv preprint arXiv:1704.02685 (2017).\n\n4. *QII:* Datta, Anupam, Shayak Sen, and Yair Zick. "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems." Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 2016.\n\n5. *Layer-wise relevance propagation:* Bach, Sebastian, et al. "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation." PloS one 10.7 (2015): e0130140.\n\n6. *Shapley regression values:* Lipovetsky, Stan, and Michael Conklin. "Analysis of regression in game theory approach." Applied Stochastic Models in Business and Industry 17.4 (2001): 319-330.\n\n7. *Tree interpreter:* Saabas, Ando. Interpreting random forests. http://blog.datadive.net/interpreting-random-forests/\n\n## Citations\n\nThe algorithms and visualizations used in this package came primarily out of research in [Su-In Lee''s lab](https://suinlee.cs.washington.edu) at the University of Washington, and Microsoft Research. If you use SHAP in your research we would appreciate a citation to the appropriate paper(s):\n\n- For general use of SHAP you can read/cite our [NeurIPS paper](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) ([bibtex](https://raw.githubusercontent.com/shap/shap/master/docs/references/shap_nips.bib)).\n- For TreeExplainer you can read/cite our [Nature Machine Intelligence paper](https://www.nature.com/articles/s42256-019-0138-9) ([bibtex](https://raw.githubusercontent.com/shap/shap/master/docs/references/tree_explainer.bib); [free access](https://rdcu.be/b0z70)).\n- For GPUTreeExplainer you can read/cite [this article](https://arxiv.org/abs/2010.13972).\n- For `force_plot` visualizations and medical applications you can read/cite our [Nature Biomedical Engineering paper](https://www.nature.com/articles/s41551-018-0304-0) ([bibtex](https://raw.githubusercontent.com/shap/shap/master/docs/references/nature_bme.bib); [free access](https://rdcu.be/baVbR)).\n\n<img height="1" width="1" style="display:none" src="https://www.facebook.com/tr?id=189147091855991&ev=PageView&noscript=1" />\n', '{"language":"Jupyter Notebook","stars":24809,"forks":3459,"watchers":24809,"open_issues":605,"topics":["deep-learning","explainability","gradient-boosting","interpretability","machine-learning","shap","shapley"],"default_branch":"master","size_kb":294073,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:shap:shap","source_url":"https://github.com/shap/shap"},{"type":"has_code","target_id":"github:keras-team:keras","source_url":"https://github.com/keras-team/keras"}]', NULL, 'MIT', 'approved', 80, '8b73442bec6eea99b132b4cd0ae99a7d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-shap-shap from https://github.com/shap.png
Image converted to WebP: data/images/github-shap-shap.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-mxgmn-WaveFunctionCollapse', 'github--mxgmn--wavefunctioncollapse', 'WaveFunctionCollapse', 'mxgmn', 'This program generates bitmaps that are locally similar to the input bitmap. <p align="center"><img alt="main collage" src="images/wfc.png"></p> <p align="center"><img alt="main gif" src="images/wfc.gif"></p> Local similarity means that * (C1) The output should contain only those NxN patterns of pixels that are present in the input. * (Weak C2) Distribution of NxN patterns in the input should be similar to the distribution of NxN patterns over a sufficiently large number of outputs. In other ...', '["algorithm","csharp","gamedev","machine-learning","procedural-generation","wfc","c#"]', 'other', 24482, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/mxgmn/WaveFunctionCollapse","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# WaveFunctionCollapse\nThis program generates bitmaps that are locally similar to the input bitmap.\n<p align="center"><img alt="main collage" src="images/wfc.png"></p>\n<p align="center"><img alt="main gif" src="images/wfc.gif"></p>\n\nLocal similarity means that\n\n* (C1) The output should contain only those NxN patterns of pixels that are present in the input.\n* (Weak C2) Distribution of NxN patterns in the input should be similar to the distribution of NxN patterns over a sufficiently large number of outputs. In other words, probability to meet a particular pattern in the output should be close to the density of such patterns in the input.\n\nIn the examples a typical value of N is 3.\n<p align="center"><img alt="local similarity" src="images/patterns.png"></p>\n\nWFC initializes output bitmap in a completely unobserved state, where each pixel value is in superposition of colors of the input bitmap (so if the input was black & white then the unobserved states are shown in different shades of grey). The coefficients in these superpositions are real numbers, not complex numbers, so it doesn''t do the actual quantum mechanics, but it was inspired by QM. Then the program goes into the observation-propagation cycle:\n\n* On each observation step an NxN region is chosen among the unobserved which has the lowest Shannon entropy. This region''s state then collapses into a definite state according to its coefficients and the distribution of NxN patterns in the input.\n* On each propagation step new information gained from the collapse on the previous step propagates through the output.\n\nOn each step the number of non-zero coefficients decreases and in the end we have a completely observed state, the wave function has collapsed.\n\nIt may happen that during propagation all the coefficients for a certain pixel become zero. That means that the algorithm has run into a contradiction and can not continue. The problem of determining whether a certain bitmap allows other nontrivial bitmaps satisfying condition (C1) is NP-hard, so it''s impossible to create a fast solution that always finishes. In practice, however, the algorithm runs into contradictions surprisingly rarely.\n\nWave Function Collapse algorithm has been implemented in\n[C++](https://github.com/math-fehr/fast-wfc),\n[Python](https://github.com/ikarth/wfc_2019f),\n[Kotlin](https://github.com/j-roskopf/WFC),\n[Rust](https://github.com/Elwqnn/wfc),\n[Julia](https://github.com/roberthoenig/WaveFunctionCollapse.jl),\n[Go](https://github.com/shawnridgeway/wfc),\n[Haxe](https://github.com/Mitim-84/WFC-Gen),\n[Java](https://github.com/sjcasey21/wavefunctioncollapse),\n[Clojure](https://github.com/sjcasey21/wavefunctioncollapse-clj),\n[Free Pascal](https://github.com/PascalCorpsman/mini_projects/tree/main/miniprojects/Wave_function_collapse/Overlap_model),\n[Dart](https://github.com/rick-dalley/wfc),\n[p5js](https://github.com/D-T-666/wave-function-collapse-p5),\n[JavaScript](https://github.com/kchapelier/wavefunctioncollapse)\nand adapted to [Unity](https://selfsame.itch.io/unitywfc),\n[Unreal Engine 5](https://docs.unrealengine.com/5.0/en-US/BlueprintAPI/WaveFunctionCollapse/)\nand [Houdini](https://www.sidefx.com/tutorials/wfc-dungeon-generator/).\nYou can [build WFC from source](https://github.com/mxgmn/WaveFunctionCollapse#how-to-build),\ndownload an official [release](https://github.com/mxgmn/WaveFunctionCollapse/releases) for Windows,\ndownload an interactive graphical version from [itch.io](https://exutumno.itch.io/wavefunctioncollapse)\nor [run it in the browser](http://www.kchapelier.com/wfc-example/overlapping-model.html).\nWFC generates levels in [Bad North](https://www.badnorth.com/),\n[Caves of Qud](https://store.steampowered.com/app/333640/Caves_of_Qud/),\n[Dead Static Drive](https://twitter.com/deadstaticdrive),\n[Townscaper](https://store.steampowered.com/app/1291340/Townscaper/),\n[Matrix Awakens](https://www.youtube.com/watch?v=usJrcwN6T4I),\n[several](https://arcadia-clojure.itch.io/proc-skater-2016)\n[smaller](https://arcadia-clojure.itch.io/swapland)\n[games](https://marian42.itch.io/wfc) and many prototypes.\nIt led to [new](https://escholarship.org/uc/item/3rm1w0mn)\n[research](https://hal.inria.fr/hal-01706539v3/document).\nFor [more](https://twitter.com/OskSta/status/784847588893814785)\n[related](https://twitter.com/dwtw/status/810166761270243328)\n[work](https://github.com/mewo2/oisin),\n[explanations](https://trasevol.dog/2017/09/01/di19/),\n[interactive demos](http://oskarstalberg.com/game/wave/wave.html),\n[guides](https://www.dropbox.com/s/zeiat1w8zre9ro8/Knots%20breakdown.png?dl=0),\n[tutorials](http://www.procjam.com/tutorials/wfc/)\nand [examples](https://twitter.com/ExUtumno/status/895684431477747715)\nsee the [ports, forks and spinoffs section](https://github.com/mxgmn/WaveFunctionCollapse#notable-ports-forks-and-spinoffs).\n\nWatch a video demonstration of WFC algorithm on YouTube: [https://youtu.be/DOQTr2Xmlz0](https://youtu.be/DOQTr2Xmlz0)\n\n## Algorithm\n1. Read the input bitmap and count NxN patterns.\n    1. (optional) Augment pattern data with rotations and reflections.\n2. Create an array with the dimensions of the output (called "wave" in the source). Each element of this array represents a state of an NxN region in the output. A state of an NxN region is a superposition of NxN patterns of the input with boolean coefficients (so a state of a pixel in the output is a superposition of input colors with real coefficients). False coefficient means that the corresponding pattern is forbidden, true coefficient means that the corresponding pattern is not yet forbidden.\n3. Initialize the wave in the completely unobserved state, i.e. with all the boolean coefficients being true.\n4. Repeat the following steps:\n    1. Observation:\n        1. Find a wave element with the minimal nonzero entropy. If there is no such elements (if all elements have zero or undefined entropy) then break the cycle (4) and go to step (5).\n        2. Collapse this element into a definite state according to its coefficients and the distribution of NxN patterns in the input.\n    2. Propagation: propagate information gained on the previous observation step.\n5. By now all the wave elements are either in a completely observed state (all the coefficients except one being zero) or in the contradictory state (all the coefficients being zero). In the first case return the output. In the second case finish the work without returning anything.\n\n## Tilemap generation\nThe simplest nontrivial case of the algorithm is when NxN=1x2 (well, NxM). If we simplify it even further by storing not the probabilities of pairs of colors but the probabilities of colors themselves, we get what we call a "simple tiled model". The propagation phase in this model is just adjacency constraint propagation. It''s convenient to initialize the simple tiled model with a list of tiles and their adjacency data (adjacency data can be viewed as a large set of very small samples) rather than a sample bitmap.\n<p align="center"><a href="http://i.imgur.com/jIctSoT.gifv"><img src="images/tile.gif"/></a></p>\n<!--<p align="center">\n  <a href="images/tile.gif">GIF</a> |\n  <a href="http://i.imgur.com/jIctSoT.gifv">GIFV</a>\n</p>-->\n\nLists of all the possible pairs of adjacent tiles in practical tilesets can be quite long, so I implemented a symmetry system for tiles to shorten the enumeration. In this system each tile should be assigned with its symmetry type.\n<p align="center"><img alt="symmetries" src="images/symmetry-system.png"></p>\n\nNote that the tiles have the same symmetry type as their assigned letters (or, in other words, actions of the \ndihedral group D4 are isomorphic for tiles and their corresponding letters). With this system it''s enough to enumerate pairs of adjacent tiles only up to symmetry, which makes lists of adjacencies for tilesets with many symmetrical tiles (even the summer tileset, despite drawings not being symmetrical the system considers such tiles to be symmetrical) several times shorter.\n<p align="center">\n<img alt="knots" src="images/knots.png">\n<img alt="tiled rooms" src="images/rooms.png">\n<img alt="circuit 1" src="images/circuit-1.png">\n<img alt="circuit 2" src="images/circuit-2.png">\n<img alt="circles" src="images/circles.png">\n<img alt="castle" src="images/castle.png">\n<img alt="summer 1" src="images/summer-1.png">\n<img alt="summer 2" src="images/summer-2.png">\n</p>\n\nNote that the unrestrained knot tileset (with all 5 tiles being allowed) is not interesting for WFC, because you can''t run into a situation where you can''t place a tile. We call tilesets with this property "easy". Without special heuristics easy tilesets don''t produce interesting global arrangements, because correlations of tiles in easy tilesets quickly fall off with a distance. Many easy tilesets can be found on [Guy Walker''s website](http://cr31.co.uk/stagecast/wang/tiles_e.html). Consider the "Dual" 2-edge tileset there. How can it generate knots (without t-junctions, not easy) while being easy? The answer is, it can only generate a narrow class of knots, it can''t produce an arbitrary knot.\n\nNote also that Circuit, Summer and Rooms tilesets are non-Wang. That is, their adjacency data cannot be induced from edge labels. For example, in Circuit two Corners cannot be adjacent, yet they can be connected with a Connection tile, and diagonal tracks cannot change direction.\n\n## Higher dimensions\nWFC algorithm in higher dimensions works completely the same way as in dimension 2, though performance becomes an issue. These voxel models were generated with N=2 overlapping tiled model using 5x5x5 and 5x5x2 blocks and additional heuristics (height, density, curvature, ...).\n<p align="center"><img alt="voxels" src="images/castles-3d.png"></p>\n\nHigher resolution screenshots: [1](http://i.imgur.com/0bsjlBY.png), [2](http://i.imgur.com/GduN0Vr.png), [3](http://i.imgur.com/IEOsbIy.png).\n\n[MarkovJunior](https://github.com/mxgmn/MarkovJunior) repository contains an implementation of the 3d simple tiled model with many [tilesets](https://github.com/mxgmn/MarkovJunior/tree/main/resources/tilesets) and [examples](https://github.com/mxgmn/MarkovJunior/blob/main/images/top-1764.png).\n\n## Constrained synthesis\nWFC algorithm supports constraints. Therefore, it can be easily combined with other generative algorithms or with manual creation.\n\nHere is WFC autocompleting a level started by a human:\n<p align="center"><a href="http://i.imgur.com/X3aNDUv.gifv"><img src="images/constrained.gif"/></a></p>\n<!--<p align="center">\n  <a href="images/constrained.gif">GIF</a> |\n  <a href="http://i.imgur.com/X3aNDUv.gifv">GIFV</a>\n</p>-->\n\n[ConvChain](https://github.com/mxgmn/ConvChain) algorithm satisfies the strong version of the condition (C2): the limit distribution of NxN patterns in the outputs it is producing is exactly the same as the distributions of patterns in the input. However, ConvChain doesn''t satisfy (C1): it often produces noticeable defects. It makes sense to run ConvChain first to get a well-sampled configuration and then run WFC to correct local defects. This is similar to a common strategy in optimization: first run a Monte-Carlo method to find a point close to a global optimum and then run a gradient descent from that point for greater accuracy.\n\nP. F. Harrison''s [texture synthesis](https://github.com/mxgmn/TextureSynthesis) algorithm is significantly faster than WFC, but it has trouble with long correlations (for example, it''s difficult for this algorithm to synthesize brick wall textures with correctly aligned bricks). But this is exactly where WFC shines, and Harrison''s algorithm supports constraints. It makes sense first to generate a perfect brick wall blueprint with WFC and then run a constrained texture synthesis algorithm on that blueprint.\n\n## Comments\nWhy the minimal entropy heuristic? I noticed that when humans draw something they often follow the [minimal entropy heuristic](images/lowest-entropy-heuristic.gif) themselves. That''s why the algorithm is so enjoyable to watch.\n\nThe overlapping model relates to the simple tiled model the same way higher order Markov chains relate to order one Markov chains.\n\nWFC''s propagation phase is very similar to the loopy belief propagation algorithm. In fact, I first programmed belief propagation, but then switched to constraint propagation with a saved stationary distribution, because BP is significantly slower without a massive parallelization (on a CPU) and didn''t produce significantly better results in my problems.\n\nNote that the "Simple Knot" and "Trick Knot" samples have 3 colors, not 2.\n\nOne of the dimensions can be time. In particular, d-dimensional WFC captures the behaviour of any (d-1)-dimensional cellular automata.\n\n## Used work\n1. Alexei A. Efros and Thomas K. Leung, [Texture Synthesis by Non-parametric Sampling](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/papers/efros-iccv99.pdf), 1999. WaveFunctionCollapse is a [texture synthesis](https://en.wikipedia.org/wiki/Texture_synthesis) algorithm. Compared to the earlier texture synthesis algorithms, WFC guarantees that the output contains only those NxN patterns that are present in the input. This makes WFC perfect for level generation in games and pixel art, and less suited for large full-color textures.\n2. Paul C. Merrell, [Model Synthesis](http://graphics.stanford.edu/~pmerrell/thesis.pdf), 2009. Merrell derives adjacency constraints between tiles from an example model and generates a new larger model with the AC-3 algorithm. We generalize his approach to work with NxN overlapping patterns of tiles instead of individual tiles. This allows to use a single image as the input to the algorithm. By varying N, we can make the output look more like the input or less. We introduce the [lowest entropy heuristic](images/lowest-entropy-heuristic.gif) that removes the [directional bias](images/directional-bias.png) in generated results, is defined for irregular grids and is better suited for [pre-constrained problems](images/constrained.gif). We implement a tile symmetry system to reduce the sizes of inputs. We visualize partially observed states, either with [color averaging](images/wfc.gif) or [per-voxel voting](https://twitter.com/ExUtumno/status/900395635412787202). Merrell also introduced a method of incrementally modifying the model in parts to reduce the failure rate (which we don''t use here). Recently the author created a [page](https://paulmerrell.org/model-synthesis/) for model synthesis and published [code](https://github.com/merrell42/model-synthesis).\n3. Alan K. Mackworth, [Consistency in Networks of Relations](https://www.cs.ubc.ca/~mack/Publications/AI77.pdf), 1977. WFC translates a texture synthesis problem into a constraint satisfaction problem. Currently it uses the [AC-4 algorithm](http://www.cs.utah.edu/~tch/CS4300/resources/AC4.pdf) by Roger Mohr and Thomas C. Henderson, 1986.\n4. Paul F. Harrison, [Image Texture Tools](http://logarithmic.net/pfh-files/thesis/dissertation.pdf), 2005. WFC was also influenced by the declarative texture synthesis chapter of Paul Harrison''s dissertation. The author defines adjacency data of tiles by labeling their borders and uses backtracking search to fill the tilemap. A [demonstration of the algorithm](https://logarithmic.net/ghost.xhtml) is available on the web.\n\n## How to build\nWFC is a console application that depends only on the standard library. Get [.NET Core](https://www.microsoft.com/net/download) for Windows, Linux or macOS and run\n```\ndotnet run --configuration Release WaveFunctionCollapse.csproj\n```\nGenerated results are saved into the `output` folder. Edit `samples.xml` to change model parameters.\n\nAlternatively, use build instructions from the community for various platforms from the [relevant issue](https://github.com/mxgmn/WaveFunctionCollapse/issues/3). Casey Marshall made a [pull request](https://github.com/mxgmn/WaveFunctionCollapse/pull/18) that makes using the program with the command line more convenient and includes snap packaging.\n\n## Notable ports, forks and spinoffs\n* Emil Ernerfeldt made a [C++ port](https://github.com/emilk/wfc).\n* [Max Aller](https://github.com/nanodeath) made a Kotlin (JVM) library, [Kollapse](https://gitlab.com/nanodeath/kollapse). Joseph Roskopf made a line by line Kotlin [port](https://github.com/j-roskopf/WFC) of the optimized 2018 version. Edwin Jakobs made a [Kotlin library](https://github.com/edwinRNDR/wfc) that supports [3d examples](https://www.youtube.com/watch?v=g4Ih8wxBh1E).\n* [Kevin Chapelier](https://github.com/kchapelier) made a [JavaScript port](http://www.kchapelier.com/wfc-example/overlapping-model.html).\n* Oskar StÃ¥lberg programmed a 3d tiled model, a 2d tiled model for irregular grids on a sphere and is building beautiful 3d tilesets for them: [1](https://twitter.com/OskSta/status/787319655648100352), [2](https://twitter.com/OskSta/status/784847588893814785), [3](https://twitter.com/OskSta/status/784847933686575104), [4](https://twitter.com/OskSta/status/784848286272327680), [5](https://twitter.com/OskSta/status/793545297376972801), [6](https://twitter.com/OskSta/status/793806535898136576), [7](https://twitter.com/OskSta/status/802496920790777856), [8](https://twitter.com/OskSta/status/804291629561577472), [9](https://twitter.com/OskSta/status/806856212260278272), [10](https://twitter.com/OskSta/status/806904557502464000), [11](https://twitter.com/OskSta/status/818857408848130048), [12](https://twitter.com/OskSta/status/832633189277409280), [13](https://twitter.com/OskSta/status/851170356530475008), [14](https://twitter.com/OskSta/status/858301207936458752), [15](https://twitter.com/OskSta/status/863019585162932224).\n* [Joseph Parker](https://github.com/selfsame) adapted [WFC to Unity](https://selfsame.itch.io/unitywfc) and used it generate skateparks in the [Proc Skater 2016](https://arcadia-clojure.itch.io/proc-skater-2016) game, [fantastic plateaus](https://twitter.com/jplur_/status/929482200034226176) in the 2017 game [Swapland](https://arcadia-clojure.itch.io/swapland) and [platform levels](https://twitter.com/jplur_/status/1053458654454865921) in the 2018 game [Bug with a Gun](https://selfsame.itch.io/bug-with-a-gun).\n* [Martin O''Leary](https://github.com/mewo2) applied a [WFC-like algorithm](https://github.com/mewo2/oisin) to poetry generation: [1](https://twitter.com/mewo2/status/789167437518217216), [2](https://twitter.com/mewo2/status/789177702620114945), [3](https://twitter.com/mewo2/status/789187174683987968), [4](https://twitter.com/mewo2/status/789897712372183041).\n* [Nick Nenov](https://github.com/NNNenov) made a [3d voxel tileset](https://twitter.com/NNNenov/status/789903180226301953) based on my Castle tileset. Nick uses text output option in the tiled model to reconstruct 3d models in Cinema 4D.\n* Sean Leffler implemented the [overlapping model in Rust](https://github.com/sdleffler/collapse).\n* rid5x is making an [OCaml version of WFC](https://twitter.com/rid5x/status/782442620459114496).\n* I made an [interactive version](https://twitter.com/ExUtumno/status/798571284342837249) of the overlapping model, you can download the GUI executable from the [WFC itch.io page](https://exutumno.itch.io/wavefunctioncollapse).\n* [Brian Bucklew](https://github.com/unormal) built a level generation pipeline that applies WFC in multiple passes for the [Caves of Qud](http://store.steampowered.com/app/333640) game: [1](https://twitter.com/unormal/status/805987523596091392), [2](https://twitter.com/unormal/status/808566029387448320), [3](https://twitter.com/unormal/status/808523056259993601), [4](https://twitter.com/unormal/status/808523493994364928), [5](https://twitter.com/unormal/status/808519575264497666), [6](https://twitter.com/unormal/status/808519216185876480), [7](https://twitter.com/unormal/status/808795396508123136), [8](https://twitter.com/unormal/status/808860105093632001), [9](https://twitter.com/unormal/status/809637856432033792), [10](https://twitter.com/unormal/status/810239794433425408), [11](https://twitter.com/unormal/status/811034574973243393), [12](https://twitter.com/unormal/status/811720423419314176), [13](https://twitter.com/unormal/status/811034037259276290), [14](https://twitter.com/unormal/status/810971337309224960), [15](https://twitter.com/unormal/status/811405368777723909), [16](https://twitter.com/ptychomancer/status/812053801544757248), [17](https://twitter.com/unormal/status/812159308263788544), [18](https://twitter.com/unormal/status/812158749838340096), [19](https://twitter.com/unormal/status/814569437181476864), [20](https://twitter.com/unormal/status/814570383189876738), [21](https://twitter.com/unormal/status/819725864623603712), [22](https://twitter.com/unormal/status/984719207156862976).\n* [Danny Wynne](https://github.com/dannywynne) implemented a [3d tiled model](https://twitter.com/dwtw/status/810166761270243328).\n* Arvi Teikari programmed a [texture synthesis algorithm with the entropy heuristic](http://www.hempuli.com/blogblog/archives/1598) in Lua. Headchant [ported](https://github.com/headchant/iga) it to work with LÃ–VE.\n* Isaac Karth made a [Python port](https://github.com/ikarth/wfc_python) of the overlapping model.\n* Oskar StÃ¥lberg made an [interactive version](http://oskarstalberg.com/game/wave/wave.html) of the tiled model that runs in the browser.\n* [Matt Rix](https://github.com/MattRix) implemented a 3d tiled model ([1](https://twitter.com/MattRix/status/869403586664570880), [2](https://twitter.com/MattRix/status/870999185167962113), [3](https://twitter.com/MattRix/status/871054734018453505), [4](https://twitter.com/MattRix/status/871056805761359872)) and made a 3-dimensional tiled model where one of the dimensions is time ([1](https://twitter.com/MattRix/status/872674537799913472), [2](https://twitter.com/MattRix/status/872648369625325568), [3](https://twitter.com/MattRix/status/872645716660891648), [4](https://twitter.com/MattRix/status/872641331956518914), [5](https://twitter.com/MattRix/status/979020989181890560)).\n* [Nick Nenov](https://github.com/NNNenov) made a [visual guide](https://www.dropbox.com/s/zeiat1w8zre9ro8/Knots%20breakdown.png?dl=0) to the tile symmetry system.\n* [Isaac Karth](https://github.com/ikarth) and [Adam M. Smith](https://github.com/rndmcnlly) wrote a [paper](https://ieeexplore.ieee.org/document/9421370) ([open access link](https://escholarship.org/uc/item/3rm1w0mn)) in which they examine the role of backtracking and different possible heuristics in WFC, experiment with global constraints and combine WFC with VQ-VAE. Earlier in 2017, the authors wrote a [workshop paper](https://adamsmith.as/papers/wfc_is_constraint_solving_in_the_wild.pdf) where they formulate WFC as an ASP problem, use general constraint solver [clingo](https://github.com/potassco/clingo) to generate bitmaps, trace WFC''s history and give a detailed explanation of the algorithm.\n* Sylvain Lefebvre made a [C++ implementation](https://github.com/sylefeb/VoxModSynth) of 3d model synthesis, described the thought process of designing a sample and provided an example where adjacency constraints ensure that the output is connected (walkable).\n* I generalized 3d WFC to work with cube symmetry group and made a tileset that generates [Escheresque scenes](https://twitter.com/ExUtumno/status/895684431477747715).\n* There are many ways to visualize partially observed wave states. In the code, color values of possible options are averaged to produce the resulting color. Oskar StÃ¥lberg [shows](https://twitter.com/OskSta/status/863019585162932224) partially observed states as semi-transparent boxes, where the box is bigger for a state with more options. In the voxel setting I [visualize](https://twitter.com/ExUtumno/status/900395635412787202) wave states with per-voxel voting.\n* Remy Devaux implemented the tiled model in PICO-8 and wrote an [article](https://trasevol.dog/2017/09/01/di19/) about generation of coherent data with an explanation of WFC.\n* For the upcoming game [Bad North](https://www.badnorth.com/) Oskar StÃ¥lberg [uses](https://twitter.com/OskSta/status/917405214638006273) a heuristic that tries to select such tiles\nthat the resulting observed zone is navigable at each step.\n* William Manning [implemented](https://github.com/heyx3/easywfc) the overlapping model in C# with the primary goal of making code readable, and provided it with WPF GUI.\n* [Joseph Parker](https://gist.github.com/selfsame) wrote a WFC [tutorial](http://www.procjam.com/tutorials/wfc/) for Procjam 2017.\n* [Aman Tiwari](https://github.com/aman-tiwari) formulated the connectivity constraint as an [ASP problem](https://gist.github.com/aman-tiwari/8a7b874cb1fd1270adc203b2af293f4c) for clingo.\n* Matvey Khokhlov programmed a [3d overlapping model](https://github.com/MatveyK/Kazimir).\n* [Sylvain Lefebvre](https://github.com/sylefeb), [Li-Yi Wei](https://github.com/1iyiwei) and [Connelly Barnes](https://github.com/connellybarnes) are [investigating](https://hal.archives-ouvertes.fr/hal-01706539/) the possibility of hiding information inside textures. They made a [tool](https://members.loria.fr/Sylvain.Lefebvre/infotexsyn/) that can encode text messages as WFC tilings and decode them back. This technique allows to use WFC tilings as QR codes.\n* [Mathieu Fehr](https://github.com/math-fehr) and [Nathanael Courant](https://github.com/Ekdohibs) significantly [improved](https://github.com/math-fehr/fast-wfc) the running time of WFC, by an order of magnitude for the overlapping model. I [integrated](https://github.com/mxgmn/WaveFunctionCollapse/commit/fad1066b5000f7e9fbda0ef81bbea56799686670) their improvements into the code.\n* Vasu Mahesh [ported](https://github.com/vasumahesh1/WFC_WebGL) 3d tiled model to TypeScript, made a new tileset and [visualised](https://vasumahesh1.github.io/WFC_WebGL) the generation process in WebGL.\n* [Hwanhee Kim](https://github.com/greentec) experimented with 3d WFC and created/adapted many voxel tilesets: [1](https://twitter.com/greentecq/status/1025348928634408960), [2](https://twitter.com/greentecq/status/1004068394553913344), [3](https://twitter.com/greentecq/status/1005835830802305024), [4](https://twitter.com/greentecq/status/1022851327041265664), [5](https://twitter.com/greentecq/status/1011351814216736769), [6](https://twitter.com/greentecq/status/1008210550944387077), [7](https://twitter.com/greentecq/status/1006390606875070464), [8](https://twitter.com/greentecq/status/1015182718810841088).\n* Oskar StÃ¥lberg gave a [talk](https://www.youtube.com/watch?v=0bcZb-SsnrA) about level generation in Bad North at the Everything Procedural Conference 2018.\n* I [wrote](https://twitter.com/ExUtumno/status/1024314661951467521) about how to generate (approximately) unbiased paths between 2 points with WFC and other algorithms. I [implemented](https://github.com/mxgmn/MarkovJunior/blob/main/models/TilePath.xml) this method in MarkovJunior.\n* [Isaac Karth](https://github.com/ikarth) and [Adam M. Smith](https://github.com/rndmcnlly) published a [preprint](https://arxiv.org/abs/1809.04432) where they describe a system based on WFC that learns from both positive and negative examples, and discuss it in a general context of dialogs with example-driven generators.\n* Brendan Anthony [uses](https://steamcommunity.com/games/314230/announcements/detail/3369147113795750369) WFC to generate wall decorations in the game [Rodina](https://store.steampowered.com/app/314230/Rodina/).\n* Tim Kong implemented the [overlapping model in Haxe](https://github.com/Mitim-84/WFC-Gen).\n* In order to generate connected structures, Boris the Brave applied the [chiseling method](https://www.boristhebrave.com/2018/04/28/random-paths-via-chiseling) to WFC. He published a [library](https://boristhebrave.github.io/DeBroglie) that supports hex grids, additional constraints and backtracking.\n* [Marian Kleineberg](https://github.com/marian42) [created](https://twitter.com/marian42_/status/1061785383057440768) an [infinite city generator](https://marian42.itch.io/wfc) based on the tiled model for Procjam 2018. He wrote an [article](https://marian42.de/article/wfc) describing his approaches to setting adjacencies, backtracking and the online variation of WFC.\n* Sol Bekic [programmed](https://github.com/s-ol/gpWFC) the tiled model that runs on GPU using PyOpenCL. Instead of keeping a queue of nodes to propagate from, it propagates from every node on the grid in parallel.\n* Wouter van Oortmerssen [implemented](https://github.com/aardappel/lobster/commit/703f67472bfd80c26bb626e1d5c22ec91047da98) the tiled model in a single C++ function, with a structure similar to a priority queue for faster observation.\n* Robert Hoenig [implemented](https://github.com/roberthoenig/WaveFunctionCollapse.jl) the overlapping model in Julia, with an option to propagate constraints only locally.\n* [Edwin Jakobs](https://github.com/edwinRNDR) applied WFC to [style transfer](https://twitter.com/voorbeeld/status/1073874337248239616) and [dithering](https://twitter.com/voorbeeld/status/1073875725499985926).\n* Breanna Baltaxe-Admony [applied](https://github.com/bbaltaxe/wfc-piano-roll) WFC to music generation.\n* Shawn Ridgeway made a [Go port](https://github.com/shawnridgeway/wfc).\n* For the Global Game Jam 2019, [Andy Wallace](https://github.com/andymasteroffish) made a [game](http://andymakesgames.tumblr.com/post/182363131350/global-game-jam-2019-maureens-chaotic-dungeon) in which the player can interact with WFC-based level generator by resetting portions of the level with various weapons.\n* Stephen Sherratt wrote a [detailed explanation](https://gridbugs.org/wave-function-collapse/) of the overlapping model and made a [Rust library](https://github.com/stevebob/wfc). For the 7DRL Challenge 2019 he made a roguelike [Get Well Soon](https://gridbugs.org/get-well-soon/) that [uses](https://gridbugs.org/7drl2019-day1/) WFC to generate levels.\n* Florian Drux created a [generalization](https://github.com/lamelizard/GraphWaveFunctionCollapse/blob/master/thesis.pdf) that works on graphs with arbitrary local structure and [implemented](https://github.com/lamelizard/GraphWaveFunctionCollapse) it in Python.\n* Bob Burrough [discovered](https://twitter.com/ExUtumno/status/1119996185199116289) a percolation-like phase transition in one of the tilesets that manifests in spiking contradiction rate.\n* Oskar StÃ¥lberg combined WFC with marching cubes on irregular grids and made a town building toy [Townscaper](https://store.steampowered.com/app/1291340/Townscaper/) based on it: [1](https://twitter.com/OskSta/status/1164926304640229376), [2](https://twitter.com/OskSta/status/1168168400155267072), [3](https://twitter.com/OskSta/status/1181464374839521280), [4](https://twitter.com/OskSta/status/1189109278361165825), [5](https://twitter.com/OskSta/status/1189902695303458816), [6](https://www.youtube.com/watch?v=1hqt8JkYRdI). Oskar gave a number of talks and interviews about the mixed initiative town generation in Townscaper: [EPC2021](https://www.youtube.com/watch?v=NOJYZYqY6_M), [SGC21](https://www.youtube.com/watch?v=Uxeo9c-PX-w), [Konsoll 2021](https://www.youtube.com/watch?v=5xrRTOikBBg), [AI and Games](https://www.youtube.com/watch?v=_1fvJ5sHh6A).\n* In his Rust roguelike tutorial, [Herbert Wolverson](https://github.com/thebracket) wrote a [chapter](http://bfnightly.bracketproductions.com/rustbook/chapter_33.html) about implementing the WFC algorithm from scratch.\n* At the [Game Developers Conference 2019](https://www.youtube.com/watch?v=AdCgi9E90jw) and the [Roguelike Celebration 2019](https://www.youtube.com/watch?v=fnFj3dOKcIQ), [Brian Bucklew](https://github.com/unormal) gave talks about WFC and how Freehold Games uses it to generate levels in [Caves of Qud](https://store.steampowered.com/app/333640/Caves_of_Qud/). The talks discuss problems with overfitting and homogeny, level connectedness and combining WFC with constructive procgen methods.\n* [Boris the Brave](https://github.com/boristhebrave) published a [commercial Unity asset](https://assetstore.unity.com/packages/tools/modeling/tessera-procedural-tile-based-generator-155425) based on the tiled model.\n* Steven Casey ported WFC to [Java](https://github.com/sjcasey21/wavefunctioncollapse) and [Clojure](https://github.com/sjcasey21/wavefunctioncollapse-clj).\n* NuÃ±o de la Serna implemented the 3d tiled model in an [openFrameworks addon](https://github.com/action-script/ofxWFC3D) that supports tiles with no symmetries.\n* [Paul Ambrosiussen](https://github.com/Ambrosiussen) [integrated](https://github.com/sideeffects/SideFXLabs) the overlapping model into Houdini and gave a [talk](https://vimeo.com/400993662) about the algorithm and his implementation at Houdini HIVE 2020.\n* Keijiro Takahashi [implemented](https://github.com/keijiro/WfcMaze) a 3d tiled model and generated Escheresque scenes with it: [1](https://twitter.com/_kzr/status/1248993799960838144), [2](https://twitter.com/_kzr/status/1248990065327345664), [3](https://twitter.com/_kzr/status/1248884103274827777), [4](https://twitter.com/_kzr/status/1248268624495689728), [5](https://twitter.com/_kzr/status/1249348597549682689).\n* Simon Verstraete published a [tutorial](https://www.sidefx.com/tutorials/wfc-dungeon-generator/) about generating game levels for Unreal Engine 4 using the Houdini WFC tool: [0](https://www.youtube.com/watch?v=-5_FIqTDuzc), [1](https://www.youtube.com/watch?v=c06bSBYsFT8), [2](https://www.youtube.com/watch?v=u4NCs1F6zf8), [3](https://www.youtube.com/watch?v=YDpVUl213yo), [4](https://www.youtube.com/watch?v=ldcsvGuoW24).\n* [Ã‰lie Michel](https://github.com/eliemichel) posted a [twitter thread](https://twitter.com/exppad/status/1267045322116734977) that explains the relationship between the overlapping and the tiled models.\n* [Lionel Radisson](https://github.com/MAKIO135) published an interactive [Observable notebook](https://observablehq.com/@makio135/super-mario-wfc) that generates Mario and Zelda-like levels with the overlapping model: [1](https://twitter.com/MAKIO135/status/1271187284424040449), [2](https://twitter.com/MAKIO135/status/1268308728782045184), [3](https://twitter.com/MAKIO135/status/1271015222321561600), [4](https://twitter.com/MAKIO135/status/1271113760472694784).\n* Åukasz Jakubowski, Maciej Kaszlewicz, PaweÅ‚ Kroll and Stefan Radziuk [implemented](https://github.com/ic-pcg/waveFunctionCollapse) the tiled model in C.\n* [Ivan Donchevskii](https://github.com/yvvan) published a [commercial Unreal Engine plugin](https://www.unrealengine.com/marketplace/en-US/product/procedural-environment-generator-wfc) based on the tiled model.\n* [JÃ¡n PerneckÃ½](https://github.com/janper) and [JÃ¡n TÃ³th](https://github.com/yanchith) published a [Grasshopper plugin](https://github.com/subdgtl/Monoceros) that extends the tiled model.\n* Krystian Samp made a [single-file overlapping WFC library in C](https://github.com/krychu/wfc).\n* [Gerald Krystian](https://github.com/amarcolina) made an [interactive tool](https://amarcolina.github.io/WFC-Explorer/) that explores the tiled model where tile adjacencies are induced from edge labels.\n* DeepMind open-ended learning team [used](https://arxiv.org/abs/2107.12808) WFC to generate arenas for reinforcement learning agents.\n* Oskar StÃ¥lberg [made](https://twitter.com/OskSta/status/1447483550257799171) an island generator that combines triangle and quad tiles and uses a custom observation heuristic that doesn''t produce local minimums.\n* [Boris the Brave](https://github.com/boristhebrave) [applied](https://www.boristhebrave.com/2021/11/08/infinite-modifying-in-blocks/) [Paul Merrell''s](https://github.com/merrell42) modifying in blocks technique to the lazy generation of unbounded tile configurations. Marian Kleineberg has [implemented](https://marian42.de/article/infinite-wfc) this method into his [infinite city generator](https://github.com/marian42/wavefunctioncollapse).\n* Vladimir PleskonjiÄ‡ created a [single-header WFC library in C](https://github.com/vplesko/libwfc), accompanied by a CLI tool and a basic GUI tool.\n* Rick Dalley [ported](https://github.com/rick-dalley/wfc) WFC to Dart.\n* Elwann Guillemot [implemented](https://github.com/Elwqnn/wfc) the overlapping model in Rust and made a GUI.\n\n## Credits\nCircles tileset is taken from [Mario Klingemann](https://twitter.com/quasimondo/status/778196128957403136). FloorPlan tileset is taken from [Lingdong Huang](https://github.com/LingDong-/ndwfc). Summer tiles were drawn by Hermann Hillmann. Cat overlapping sample is taken from the Nyan Cat video, Water + Forest + Mountains samples are taken from Ultima IV, 3Bricks sample is taken from Dungeon Crawl Stone Soup, Qud sample was made by Brian Bucklew, MagicOffice + Spirals samples - by rid5x, ColoredCity + Link + Link 2 + Mazelike + RedDot + SmileCity samples - by Arvi Teikari, Wall sample - by Arcaniax, NotKnot + Sand + Wrinkles samples - by Krystian Samp, Circle sample - by Noah Buddy. The rest of the examples and tilesets were made by me. Idea of generating integrated circuits was suggested to me by [Moonasaur](https://twitter.com/Moonasaur/status/759890746350731264) and their style was taken from Zachtronics'' [Ruckingenur II](http://www.zachtronics.com/ruckingenur-ii/). Voxel models were rendered in [MagicaVoxel](http://ephtracy.github.io/).\n<p align="center"><img alt="second collage" src="images/wfc-2.png"></p>\n<p align="center"><img alt="voxel perspective" src="images/castle-3d.png"></p>\n', '{"language":"C#","stars":24482,"forks":1305,"watchers":24482,"open_issues":9,"topics":["algorithm","csharp","gamedev","machine-learning","procedural-generation","wfc"],"default_branch":"master","size_kb":31099,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:math-fehr:fast-wfc","source_url":"https://github.com/math-fehr/fast-wfc"},{"type":"has_code","target_id":"github:ikarth:wfc_2019f","source_url":"https://github.com/ikarth/wfc_2019f"},{"type":"has_code","target_id":"github:j-roskopf:WFC","source_url":"https://github.com/j-roskopf/WFC"},{"type":"has_code","target_id":"github:Elwqnn:wfc","source_url":"https://github.com/Elwqnn/wfc"},{"type":"has_code","target_id":"github:roberthoenig:WaveFunctionCollapse.jl","source_url":"https://github.com/roberthoenig/WaveFunctionCollapse.jl"},{"type":"has_code","target_id":"github:shawnridgeway:wfc","source_url":"https://github.com/shawnridgeway/wfc"},{"type":"has_code","target_id":"github:Mitim-84:WFC-Gen","source_url":"https://github.com/Mitim-84/WFC-Gen"},{"type":"has_code","target_id":"github:sjcasey21:wavefunctioncollapse","source_url":"https://github.com/sjcasey21/wavefunctioncollapse"},{"type":"has_code","target_id":"github:sjcasey21:wavefunctioncollapse-clj","source_url":"https://github.com/sjcasey21/wavefunctioncollapse-clj"},{"type":"has_code","target_id":"github:PascalCorpsman:mini_projects","source_url":"https://github.com/PascalCorpsman/mini_projects"},{"type":"has_code","target_id":"github:rick-dalley:wfc","source_url":"https://github.com/rick-dalley/wfc"},{"type":"has_code","target_id":"github:D-T-666:wave-function-collapse-p5","source_url":"https://github.com/D-T-666/wave-function-collapse-p5"},{"type":"has_code","target_id":"github:kchapelier:wavefunctioncollapse","source_url":"https://github.com/kchapelier/wavefunctioncollapse"},{"type":"has_code","target_id":"github:mxgmn:WaveFunctionCollapse","source_url":"https://github.com/mxgmn/WaveFunctionCollapse#how-to-build"},{"type":"has_code","target_id":"github:mxgmn:WaveFunctionCollapse","source_url":"https://github.com/mxgmn/WaveFunctionCollapse"},{"type":"has_code","target_id":"github:mewo2:oisin","source_url":"https://github.com/mewo2/oisin"},{"type":"has_code","target_id":"github:mxgmn:WaveFunctionCollapse","source_url":"https://github.com/mxgmn/WaveFunctionCollapse#notable-ports-forks-and-spinoffs"},{"type":"has_code","target_id":"github:mxgmn:MarkovJunior","source_url":"https://github.com/mxgmn/MarkovJunior"},{"type":"has_code","target_id":"github:mxgmn:MarkovJunior","source_url":"https://github.com/mxgmn/MarkovJunior"},{"type":"has_code","target_id":"github:mxgmn:MarkovJunior","source_url":"https://github.com/mxgmn/MarkovJunior"},{"type":"has_code","target_id":"github:mxgmn:ConvChain","source_url":"https://github.com/mxgmn/ConvChain"},{"type":"has_code","target_id":"github:mxgmn:TextureSynthesis","source_url":"https://github.com/mxgmn/TextureSynthesis"},{"type":"has_code","target_id":"github:merrell42:model-synthesis","source_url":"https://github.com/merrell42/model-synthesis"},{"type":"has_code","target_id":"github:mxgmn:WaveFunctionCollapse","source_url":"https://github.com/mxgmn/WaveFunctionCollapse"},{"type":"has_code","target_id":"github:mxgmn:WaveFunctionCollapse","source_url":"https://github.com/mxgmn/WaveFunctionCollapse"},{"type":"has_code","target_id":"github:emilk:wfc","source_url":"https://github.com/emilk/wfc"},{"type":"has_code","target_id":"github:j-roskopf:WFC","source_url":"https://github.com/j-roskopf/WFC"},{"type":"has_code","target_id":"github:edwinRNDR:wfc","source_url":"https://github.com/edwinRNDR/wfc"},{"type":"has_code","target_id":"github:mewo2:oisin","source_url":"https://github.com/mewo2/oisin"},{"type":"has_code","target_id":"github:sdleffler:collapse","source_url":"https://github.com/sdleffler/collapse"},{"type":"has_code","target_id":"github:headchant:iga","source_url":"https://github.com/headchant/iga"},{"type":"has_code","target_id":"github:ikarth:wfc_python","source_url":"https://github.com/ikarth/wfc_python"},{"type":"has_code","target_id":"github:potassco:clingo","source_url":"https://github.com/potassco/clingo"},{"type":"has_code","target_id":"github:sylefeb:VoxModSynth","source_url":"https://github.com/sylefeb/VoxModSynth"},{"type":"has_code","target_id":"github:heyx3:easywfc","source_url":"https://github.com/heyx3/easywfc"},{"type":"has_code","target_id":"github:MatveyK:Kazimir","source_url":"https://github.com/MatveyK/Kazimir"},{"type":"has_code","target_id":"github:math-fehr:fast-wfc","source_url":"https://github.com/math-fehr/fast-wfc"},{"type":"has_code","target_id":"github:mxgmn:WaveFunctionCollapse","source_url":"https://github.com/mxgmn/WaveFunctionCollapse"},{"type":"has_code","target_id":"github:vasumahesh1:WFC_WebGL","source_url":"https://github.com/vasumahesh1/WFC_WebGL"},{"type":"has_code","target_id":"github:mxgmn:MarkovJunior","source_url":"https://github.com/mxgmn/MarkovJunior"},{"type":"has_code","target_id":"github:Mitim-84:WFC-Gen","source_url":"https://github.com/Mitim-84/WFC-Gen"},{"type":"has_code","target_id":"github:s-ol:gpWFC","source_url":"https://github.com/s-ol/gpWFC"},{"type":"has_code","target_id":"github:aardappel:lobster","source_url":"https://github.com/aardappel/lobster"},{"type":"has_code","target_id":"github:roberthoenig:WaveFunctionCollapse.jl","source_url":"https://github.com/roberthoenig/WaveFunctionCollapse.jl"},{"type":"has_code","target_id":"github:bbaltaxe:wfc-piano-roll","source_url":"https://github.com/bbaltaxe/wfc-piano-roll"},{"type":"has_code","target_id":"github:shawnridgeway:wfc","source_url":"https://github.com/shawnridgeway/wfc"},{"type":"has_code","target_id":"github:stevebob:wfc","source_url":"https://github.com/stevebob/wfc"},{"type":"has_code","target_id":"github:lamelizard:GraphWaveFunctionCollapse","source_url":"https://github.com/lamelizard/GraphWaveFunctionCollapse"},{"type":"has_code","target_id":"github:lamelizard:GraphWaveFunctionCollapse","source_url":"https://github.com/lamelizard/GraphWaveFunctionCollapse"},{"type":"has_code","target_id":"github:sjcasey21:wavefunctioncollapse","source_url":"https://github.com/sjcasey21/wavefunctioncollapse"},{"type":"has_code","target_id":"github:sjcasey21:wavefunctioncollapse-clj","source_url":"https://github.com/sjcasey21/wavefunctioncollapse-clj"},{"type":"has_code","target_id":"github:action-script:ofxWFC3D","source_url":"https://github.com/action-script/ofxWFC3D"},{"type":"has_code","target_id":"github:sideeffects:SideFXLabs","source_url":"https://github.com/sideeffects/SideFXLabs"},{"type":"has_code","target_id":"github:keijiro:WfcMaze","source_url":"https://github.com/keijiro/WfcMaze"},{"type":"has_code","target_id":"github:ic-pcg:waveFunctionCollapse","source_url":"https://github.com/ic-pcg/waveFunctionCollapse"},{"type":"has_code","target_id":"github:subdgtl:Monoceros","source_url":"https://github.com/subdgtl/Monoceros"},{"type":"has_code","target_id":"github:krychu:wfc","source_url":"https://github.com/krychu/wfc"},{"type":"has_code","target_id":"github:marian42:wavefunctioncollapse","source_url":"https://github.com/marian42/wavefunctioncollapse"},{"type":"has_code","target_id":"github:vplesko:libwfc","source_url":"https://github.com/vplesko/libwfc"},{"type":"has_code","target_id":"github:rick-dalley:wfc","source_url":"https://github.com/rick-dalley/wfc"},{"type":"has_code","target_id":"github:Elwqnn:wfc","source_url":"https://github.com/Elwqnn/wfc"},{"type":"has_code","target_id":"github:LingDong-:ndwfc","source_url":"https://github.com/LingDong-/ndwfc"}]', NULL, 'NOASSERTION', 'approved', 80, '397164620b4dc068e141f924d31c3ad3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-mxgmn-WaveFunctionCollapse from https://github.com/mxgmn.png
Image converted to WebP: data/images/github-mxgmn-WaveFunctionCollapse.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-fastai-fastbook', 'github--fastai--fastbook', 'fastbook', 'fastai', 'English / Spanish / Korean / Chinese / Bengali / Indonesian / Italian / Portuguese / Vietnamese / Japanese These notebooks cover an introduction to deep learning, fastai, and PyTorch. fastai is a layered API for deep learning; for more information, see the fastai paper. Everything in this repo is copyright Jeremy Howard and Sylvain Gugger, 2020 onwards. A selection of chapters is available to read online here. The notebooks in this repo are used for a MOOC and form the basis of this book, whi...', '["book","data-science","deep-learning","fastai","machine-learning","notebooks","python","jupyter notebook"]', 'other', 24128, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/fastai/fastbook","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '[English](./README.md) / [Spanish](./README_es.md) / [Korean](./README_ko.md) / [Chinese](./README_zh.md) / [Bengali](./README_bn.md) / [Indonesian](./README_id.md) / [Italian](./README_it.md) / [Portuguese](./README_pt.md) / [Vietnamese](./README_vn.md) / [Japanese](./README_ja.md)\n\n# The fastai book\n\nThese notebooks cover an introduction to deep learning, [fastai](https://docs.fast.ai/), and [PyTorch](https://pytorch.org/). fastai is a layered API for deep learning; for more information, see [the fastai paper](https://www.mdpi.com/2078-2489/11/2/108). Everything in this repo is copyright Jeremy Howard and Sylvain Gugger, 2020 onwards. A selection of chapters is available to [read online here](https://fastai.github.io/fastbook2e/).\n\nThe notebooks in this repo are used for [a MOOC](https://course.fast.ai) and form the basis of [this book](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527), which is currently available for purchase. It does not have the same GPL restrictions that are on this repository.\n\nThe code in the notebooks and python `.py` files is covered by the GPL v3 license; see the LICENSE file for details. The remainder (including all markdown cells in the notebooks and other prose) is not licensed for any redistribution or change of format or medium, other than making copies of the notebooks or forking this repo for your own private use. No commercial or broadcast use is allowed. We are making these materials freely available to help you learn deep learning, so please respect our copyright and these restrictions.\n\nIf you see someone hosting a copy of these materials somewhere else, please let them know that their actions are not allowed and may lead to legal action. Moreover, they would be hurting the community because we''re not likely to release additional materials in this way if people ignore our copyright.\n\n## Colab\n\nInstead of cloning this repo and opening it on your machine, you can read and work with the notebooks using [Google Colab](https://research.google.com/colaboratory/). This is the recommended approach for folks who are just getting started -- there''s no need to set up a Python development environment on your own machine, since you can just work directly in your web-browser.\n\nYou can open any chapter of the book in Colab by clicking on one of these links: [Introduction to Jupyter](https://colab.research.google.com/github/fastai/fastbook/blob/master/app_jupyter.ipynb) | [Chapter 1, Intro](https://colab.research.google.com/github/fastai/fastbook/blob/master/01_intro.ipynb) | [Chapter 2, Production](https://colab.research.google.com/github/fastai/fastbook/blob/master/02_production.ipynb) | [Chapter 3, Ethics](https://colab.research.google.com/github/fastai/fastbook/blob/master/03_ethics.ipynb) | [Chapter 4, MNIST Basics](https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb) | [Chapter 5, Pet Breeds](https://colab.research.google.com/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb) | [Chapter 6, Multi-Category](https://colab.research.google.com/github/fastai/fastbook/blob/master/06_multicat.ipynb) | [Chapter 7, Sizing and TTA](https://colab.research.google.com/github/fastai/fastbook/blob/master/07_sizing_and_tta.ipynb) | [Chapter 8, Collab](https://colab.research.google.com/github/fastai/fastbook/blob/master/08_collab.ipynb) | [Chapter 9, Tabular](https://colab.research.google.com/github/fastai/fastbook/blob/master/09_tabular.ipynb) | [Chapter 10, NLP](https://colab.research.google.com/github/fastai/fastbook/blob/master/10_nlp.ipynb) | [Chapter 11, Mid-Level API](https://colab.research.google.com/github/fastai/fastbook/blob/master/11_midlevel_data.ipynb) | [Chapter 12, NLP Deep-Dive](https://colab.research.google.com/github/fastai/fastbook/blob/master/12_nlp_dive.ipynb) | [Chapter 13, Convolutions](https://colab.research.google.com/github/fastai/fastbook/blob/master/13_convolutions.ipynb) | [Chapter 14, Resnet](https://colab.research.google.com/github/fastai/fastbook/blob/master/14_resnet.ipynb) | [Chapter 15, Arch Details](https://colab.research.google.com/github/fastai/fastbook/blob/master/15_arch_details.ipynb) | [Chapter 16, Optimizers and Callbacks](https://colab.research.google.com/github/fastai/fastbook/blob/master/16_accel_sgd.ipynb) | [Chapter 17, Foundations](https://colab.research.google.com/github/fastai/fastbook/blob/master/17_foundations.ipynb) | [Chapter 18, GradCAM](https://colab.research.google.com/github/fastai/fastbook/blob/master/18_CAM.ipynb) | [Chapter 19, Learner](https://colab.research.google.com/github/fastai/fastbook/blob/master/19_learner.ipynb) | [Chapter 20, conclusion](https://colab.research.google.com/github/fastai/fastbook/blob/master/20_conclusion.ipynb)\n\n\n## Contributions\n\nIf you make any pull requests to this repo, then you are assigning copyright of that work to Jeremy Howard and Sylvain Gugger. (Additionally, if you are making small edits to spelling or text, please specify the name of the file and a very brief description of what you''re fixing. It''s difficult for reviewers to know which corrections have already been made. Thank you.)\n\n## Citations\n\nIf you wish to cite the book, you may use the following:\n\n```\n@book{howard2020deep,\ntitle={Deep Learning for Coders with Fastai and Pytorch: AI Applications Without a PhD},\nauthor={Howard, J. and Gugger, S.},\nisbn={9781492045526},\nurl={https://books.google.no/books?id=xd6LxgEACAAJ},\nyear={2020},\npublisher={O''Reilly Media, Incorporated}\n}\n```\n\n', '{"language":"Jupyter Notebook","stars":24128,"forks":9310,"watchers":24128,"open_issues":178,"topics":["book","data-science","deep-learning","fastai","machine-learning","notebooks","python"],"default_branch":"master","size_kb":86034,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[]', NULL, 'NOASSERTION', 'approved', 65, '754e31513688e2f1cfaae6bbf6de219f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-fastai-fastbook from https://github.com/fastai.png
Image converted to WebP: data/images/github-fastai-fastbook.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-srbhr-Resume-Matcher', 'github--srbhr--resume-matcher', 'Resume-Matcher', 'srbhr', '<div align="center"> ğ™¹ğš˜ğš’ğš— ğ™³ğš’ğšœğšŒğš˜ğš›ğš âœ¦ ğš†ğšğš‹ğšœğš’ğšğš âœ¦ ğ™·ğš˜ğš  ğšğš˜ ğ™¸ğš—ğšœğšğšŠğš•ğš• âœ¦ ğ™²ğš˜ğš—ğšğš›ğš’ğš‹ğšğšğš˜ğš›ğšœ âœ¦ ğ™³ğš˜ğš—ğšŠğšğš âœ¦ ğšƒğš ğš’ğšğšğšğš›/ğš‡ âœ¦ ğ™»ğš’ğš—ğš”ğšğšğ™¸ğš— **Stop getting auto-rejected by ATS bots.** Resume Matcher is the AI-powered platform that reverse-engineers hiring algorithms to show you exactly how to tailor your resume. Get the keywords, formatting, and insights that actually get you past the first screen and into human hands. Hoping to make this, **VS Code for making ...', '["applicant-tracking-system","ats","hacktoberfest","machine-learning","natural-language-processing","nextjs","python","resume","resume-builder","resume-parser","text-similarity","typescript","vector-search","word-embeddings","python"]', 'other', 24090, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/srbhr/Resume-Matcher","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n\n[![Resume Matcher](assets/page_2.png)](https://www.resumematcher.fyi)\n\n# Resume Matcher\n\n[ğ™¹ğš˜ğš’ğš— ğ™³ğš’ğšœğšŒğš˜ğš›ğš](https://dsc.gg/resume-matcher) âœ¦ [ğš†ğšğš‹ğšœğš’ğšğš](https://resumematcher.fyi) âœ¦ [ğ™·ğš˜ğš  ğšğš˜ ğ™¸ğš—ğšœğšğšŠğš•ğš•](#how-to-install) âœ¦ [ğ™²ğš˜ğš—ğšğš›ğš’ğš‹ğšğšğš˜ğš›ğšœ](#contributors) âœ¦ [ğ™³ğš˜ğš—ğšŠğšğš](#support-the-development-by-donating) âœ¦ [ğšƒğš ğš’ğšğšğšğš›/ğš‡](https://twitter.com/ssrbhr) âœ¦ [ğ™»ğš’ğš—ğš”ğšğšğ™¸ğš—](https://www.linkedin.com/company/resume-matcher/)\n\n**Stop getting auto-rejected by ATS bots.** Resume Matcher is the AI-powered platform that reverse-engineers hiring algorithms to show you exactly how to tailor your resume. Get the keywords, formatting, and insights that actually get you past the first screen and into human hands.\n\nHoping to make this, **VS Code for making resumes**.\n\n</div>\n\n<br>\n\n<div align="center">\n\n![Stars](https://img.shields.io/github/stars/srbhr/Resume-Matcher?labelColor=black&style=for-the-badge&color=c20a71)\n![Apache 2.0](https://img.shields.io/github/license/srbhr/Resume-Matcher?labelColor=black&style=for-the-badge&color=c20a71) ![Forks](https://img.shields.io/github/forks/srbhr/Resume-Matcher?labelColor=black&style=for-the-badge&color=c20a71) ![version](https://img.shields.io/badge/Version-0.1%20Veridis%20Quo-FFF?labelColor=black&logo=LinkedIn&style=for-the-badge&color=c20a71)\n\n[![Discord](https://img.shields.io/discord/1122069176962531400?labelColor=black&logo=discord&logoColor=c20a71&style=for-the-badge&color=c20a71)](https://dsc.gg/resume-matcher) [![Website](https://img.shields.io/badge/website-Resume%20Matcher-FFF?labelColor=black&style=for-the-badge&color=c20a71)](https://resumematcher.fyi) [![LinkedIn](https://img.shields.io/badge/LinkedIn-Resume%20Matcher-FFF?labelColor=black&logo=LinkedIn&style=for-the-badge&color=c20a71)](https://www.linkedin.com/company/resume-matcher/)\n\n<a href="https://trendshift.io/repositories/565" target="_blank"><img src="https://trendshift.io/api/badge/repositories/565" alt="srbhr%2FResume-Matcher | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n\n![Vercel OSS Program](https://vercel.com/oss/program-badge.svg)\n\n</div>\n\n> \[!IMPORTANT]\n>\n> This project is in active development. New features are being added continuously, and we welcome contributions from the community. There are some breaking changes on the `main` branch. If you have any suggestions or feature requests, please feel free to open an issue on GitHub or discuss it on our [Discord](https://dsc.gg/resume-matcher) server.\n\n## Getting started with Resume Matcher\n\nResume Matcher is designed to help you optimize your resume with the aim to highlight your skills and experience in a way that resonates with potential employers.\n\nWe''re actively working on improving the platform, building towards a **VS Code for making resumes**, and adding new features. The best way to stay updated is to join the Discord discussion and be part of the active development community.\n\n> Join our [Discord](https://dsc.gg/resume-matcher) community ğŸ‘‡\n[![Discord](assets/resume_matcher_discord.png)](https://dsc.gg/resume-matcher)\n\n> Follow us on [LinkedIn](https://www.linkedin.com/company/resume-matcher/) âœ¨\n[![LinkedIn](assets/resume_matcher_linkedin.png)](https://www.linkedin.com/company/resume-matcher/)\n\n> â­ Star Resume Matcher to support the development and get updates on GitHub.\n![Star Resume Matcher](assets/star_resume_matcher.png)\n\n## Key Features\n\n![resume_matcher_features](assets/resume_matcher_features.png)\n\n- **Works locally**: No need to upload your resume to a server. Everything runs on your machine with open source AI models by Ollama.\n- **ATS Compatibility**: Get a detailed analysis of your resume''s compatibility with ATS systems.\n- **Instant Match Score**: Upload resume & job description for a quick match score and key improvement areas.\n- **Keyword Optimizer**: Align your resume with job keywords and identify critical content gaps.\n- **Guided Improvements**: Get clear suggestions to make your resume stand out.\n\n### Roadmap\n\nIf you have any suggestions or feature requests, please feel free to open an issue on GitHub. And discuss it on our [Discord](https://dsc.gg/resume-matcher) server.\n\n- Visual keyword highlighting.\n- AI Canvas, which can help to craft impactful, metric-driven resume content.\n- Multi-job description optimization.\n\n## How to Install\n\n![Installation](assets/how_to_install_resumematcher.png)\n\nFollow the instructions in the [SETUP.md](SETUP.md) file to set up the project locally. The setup script will install all the necessary dependencies and configure your environment.\n\nThe project is built using:\n\n- FastAPI for the backend.\n- Next.js for the frontend.\n- Ollama for local AI model serving.\n- Tailwind CSS for styling.\n- SQLite for the database.\n\n| Technology   | Info/Version                               |\n|--------------|---------------------------------------|\n| Python      | 3.12+                   |\n| Next.js      | 15+                   |\n| Ollama       |        0.6.7        |\n\n## Join Us and Contribute\n\n![how to contribute](assets/how_to_contribute.png)\n\nWe welcome contributions from everyone! Whether you''re a developer, designer, or just someone who wants to help out. All the contributors are listed in the [about page](https://resumematcher.fyi/about) on our website and on the GitHub Readme here.\n\nCheck out the roadmap if you would like to work on the features that are planned for the future. If you have any suggestions or feature requests, please feel free to open an issue on GitHub and discuss it on our [Discord](https://dsc.gg/resume-matcher) server.\n\n## Contributors\n\n![Contributors](assets/contributors.png)\n\n<a href="https://github.com/srbhr/Resume-Matcher/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=srbhr/Resume-Matcher" />\n</a>\n\n## Support the Development by Donating\n\n![donate](assets/supporting_resume_matcher.png)\n\nIf you would like to support the development of Resume Matcher, you can do so by donating. Your contributions will help us keep the project alive and continue adding new features.\n\n| Platform  | Link                                   |\n|-----------|----------------------------------------|\n| GitHub    | [![GitHub Sponsors](https://img.shields.io/github/sponsors/srbhr?style=for-the-badge&color=c20a71&labelColor=black&logo=github)](https://github.com/sponsors/srbhr) |\n| Buy Me a Coffee | [![BuyMeACoffee](https://img.shields.io/badge/Buy%20Me%20a%20Coffee-ffdd00?style=for-the-badge&logo=buy-me-a-coffee&color=c20a72&logoColor=white)](https://www.buymeacoffee.com/srbhr) |\n\n<details>\n  <summary><kbd>Star History</kbd></summary>\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=srbhr/resume-matcher&theme=dark&type=Date">\n    <img width="100%" src="https://api.star-history.com/svg?repos=srbhr/resume-matcher&theme=dark&type=Date">\n  </picture>\n</details>\n\n## Resume Matcher is a part of [Vercel Open Source Program](https://vercel.com/oss)\n\n![Vercel OSS Program](https://vercel.com/oss/program-badge.svg)\n', '{"language":"Python","stars":24090,"forks":4517,"watchers":24090,"open_issues":56,"topics":["applicant-tracking-system","ats","hacktoberfest","machine-learning","natural-language-processing","nextjs","python","resume","resume-builder","resume-parser","text-similarity","typescript","vector-search","word-embeddings"],"default_branch":"main","size_kb":113620,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:srbhr:Resume-Matcher","source_url":"https://github.com/srbhr/Resume-Matcher"},{"type":"has_code","target_id":"github:sponsors:srbhr","source_url":"https://github.com/sponsors/srbhr"}]', NULL, 'Apache-2.0', 'approved', 65, 'f5948eb2f1d07780dc7cdadd06c9ec29', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-srbhr-Resume-Matcher from https://github.com/srbhr.png
Image converted to WebP: data/images/github-srbhr-Resume-Matcher.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-trekhleb-homemade-machine-learning', 'github--trekhleb--homemade-machine-learning', 'homemade-machine-learning', 'trekhleb', '> ğŸ‡ºğŸ‡¦ UKRAINE IS BEING ATTACKED BY RUSSIAN ARMY. CIVILIANS ARE GETTING KILLED. RESIDENTIAL AREAS ARE GETTING BOMBED. > - Help Ukraine via: > - Serhiy Prytula Charity Foundation > - Come Back Alive Charity Foundation > - National Bank of Ukraine > - More info on war.ukraine.ua and MFA of Ukraine <hr/> > _Read this in other languages:_ _EspaÃ±ol_ > _You might be interested in:_ > - _Homemade GPT â€¢ JS_ > - _Interactive Machine Learning Experiments_ _For Octave/MatLab version of this repository p...', '["algorithm","jupyter","jupyter-notebook","machine-learning","machine-learning-algorithms","machinelearning","python","jupyter notebook"]', 'other', 23786, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/trekhleb/homemade-machine-learning","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Homemade Machine Learning\n\n> ğŸ‡ºğŸ‡¦ UKRAINE [IS BEING ATTACKED](https://war.ukraine.ua/) BY RUSSIAN ARMY. CIVILIANS ARE GETTING KILLED. RESIDENTIAL AREAS ARE GETTING BOMBED.\n> - Help Ukraine via:\n>   - [Serhiy Prytula Charity Foundation](https://prytulafoundation.org/en/)\n>   - [Come Back Alive Charity Foundation](https://savelife.in.ua/en/donate-en/)\n>   - [National Bank of Ukraine](https://bank.gov.ua/en/news/all/natsionalniy-bank-vidkriv-spetsrahunok-dlya-zboru-koshtiv-na-potrebi-armiyi)\n> - More info on [war.ukraine.ua](https://war.ukraine.ua/) and [MFA of Ukraine](https://twitter.com/MFA_Ukraine)\n\n<hr/>\n\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/trekhleb/homemade-machine-learning/master?filepath=notebooks)\n\n> _Read this in other languages:_ [_EspaÃ±ol_](README.es-ES.md)\n\n\n> _You might be interested in:_\n> - _[Homemade GPT â€¢ JS](https://github.com/trekhleb/homemade-gpt-js)_\n> - _[Interactive Machine Learning Experiments](https://github.com/trekhleb/machine-learning-experiments)_\n\n_For Octave/MatLab version of this repository please check [machine-learning-octave](https://github.com/trekhleb/machine-learning-octave) project._\n\n> This repository contains examples of popular machine learning algorithms implemented in **Python** with mathematics behind them being explained. Each algorithm has interactive **Jupyter Notebook** demo that allows you to play with training data, algorithms configurations and immediately see the results, charts and predictions **right in your browser**. In most cases the explanations are based on [this great machine learning course](https://www.coursera.org/learn/machine-learning) by Andrew Ng.\n\nThe purpose of this repository is _not_ to implement machine learning algorithms by using 3<sup>rd</sup> party library one-liners _but_ rather to practice implementing these algorithms from scratch and get better understanding of the mathematics behind each algorithm. That''s why all algorithms implementations are called "homemade" and not intended to be used for production.\n\n## Supervised Learning\n\nIn supervised learning we have a set of training data as an input and a set of labels or "correct answers" for each training set as an output. Then we''re training our model (machine learning algorithm parameters) to map the input to the output correctly (to do correct prediction). The ultimate purpose is to find such model parameters that will successfully continue correct _inputâ†’output_ mapping (predictions) even for new input examples.\n\n### Regression\n\nIn regression problems we do real value predictions. Basically we try to draw a line/plane/n-dimensional plane along the training examples.\n\n_Usage examples: stock price forecast, sales analysis, dependency of any number, etc._\n\n#### ğŸ¤– Linear Regression\n\n- ğŸ“— [Math | Linear Regression](homemade/linear_regression) - theory and links for further readings\n- âš™ï¸ [Code | Linear Regression](homemade/linear_regression/linear_regression.py) - implementation example\n- â–¶ï¸ [Demo | Univariate Linear Regression](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/linear_regression/univariate_linear_regression_demo.ipynb) - predict `country happiness` score by `economy GDP`\n- â–¶ï¸ [Demo | Multivariate Linear Regression](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/linear_regression/multivariate_linear_regression_demo.ipynb) - predict `country happiness` score by `economy GDP` and `freedom index`\n- â–¶ï¸ [Demo | Non-linear Regression](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/linear_regression/non_linear_regression_demo.ipynb) - use linear regression with _polynomial_ and _sinusoid_ features to predict non-linear dependencies\n\n### Classification\n\nIn classification problems we split input examples by certain characteristic.\n\n_Usage examples: spam-filters, language detection, finding similar documents, handwritten letters recognition, etc._\n\n#### ğŸ¤– Logistic Regression\n\n- ğŸ“— [Math | Logistic Regression](homemade/logistic_regression) - theory and links for further readings\n- âš™ï¸ [Code | Logistic Regression](homemade/logistic_regression/logistic_regression.py) - implementation example\n- â–¶ï¸ [Demo | Logistic Regression (Linear Boundary)](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/logistic_regression_with_linear_boundary_demo.ipynb) - predict Iris flower `class` based on `petal_length` and `petal_width`\n- â–¶ï¸ [Demo | Logistic Regression (Non-Linear Boundary)](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/logistic_regression_with_non_linear_boundary_demo.ipynb) - predict microchip `validity` based on `param_1` and `param_2`\n- â–¶ï¸ [Demo | Multivariate Logistic Regression | MNIST](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/multivariate_logistic_regression_demo.ipynb) - recognize handwritten digits from `28x28` pixel images\n- â–¶ï¸ [Demo | Multivariate Logistic Regression | Fashion MNIST](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/logistic_regression/multivariate_logistic_regression_fashion_demo.ipynb) - recognize clothes types from `28x28` pixel images\n\n## Unsupervised Learning\n\nUnsupervised learning is a branch of machine learning that learns from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data.\n\n### Clustering\n\nIn clustering problems we split the training examples by unknown characteristics. The algorithm itself decides what characteristic to use for splitting.\n\n_Usage examples: market segmentation, social networks analysis, organize computing clusters, astronomical data analysis, image compression, etc._\n\n#### ğŸ¤– K-means Algorithm\n\n- ğŸ“— [Math | K-means Algorithm](homemade/k_means) - theory and links for further readings\n- âš™ï¸ [Code | K-means Algorithm](homemade/k_means/k_means.py) - implementation example\n- â–¶ï¸ [Demo | K-means Algorithm](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/k_means/k_means_demo.ipynb) - split Iris flowers into clusters based on `petal_length` and `petal_width`\n\n### Anomaly Detection\n\nAnomaly detection (also outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.\n\n_Usage examples: intrusion detection, fraud detection, system health monitoring, removing anomalous data from the dataset etc._\n\n#### ğŸ¤– Anomaly Detection using Gaussian Distribution\n\n- ğŸ“— [Math | Anomaly Detection using Gaussian Distribution](homemade/anomaly_detection) - theory and links for further readings\n- âš™ï¸ [Code | Anomaly Detection using Gaussian Distribution](homemade/anomaly_detection/gaussian_anomaly_detection.py) - implementation example\n- â–¶ï¸ [Demo | Anomaly Detection](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/anomaly_detection/anomaly_detection_gaussian_demo.ipynb) - find anomalies in server operational parameters like `latency` and `threshold`\n\n## Neural Network (NN)\n\nThe neural network itself isn''t an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs.\n\n_Usage examples: as a substitute of all other algorithms in general, image recognition, voice recognition, image processing (applying specific style), language translation, etc._\n\n#### ğŸ¤– Multilayer Perceptron (MLP)\n\n- ğŸ“— [Math | Multilayer Perceptron](homemade/neural_network) - theory and links for further readings\n- âš™ï¸ [Code | Multilayer Perceptron](homemade/neural_network/multilayer_perceptron.py) - implementation example\n- â–¶ï¸ [Demo | Multilayer Perceptron | MNIST](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/neural_network/multilayer_perceptron_demo.ipynb) - recognize handwritten digits from `28x28` pixel images\n- â–¶ï¸ [Demo | Multilayer Perceptron | Fashion MNIST](https://nbviewer.jupyter.org/github/trekhleb/homemade-machine-learning/blob/master/notebooks/neural_network/multilayer_perceptron_fashion_demo.ipynb) - recognize the type of clothes from `28x28` pixel images\n\n## Machine Learning Map\n\n![Machine Learning Map](images/machine-learning-map.png)\n\nThe source of the following machine learning topics map is [this wonderful blog post](https://vas3k.ru/blog/machine_learning/)\n\n## Prerequisites\n\n#### Installing Python\n\nMake sure that you have [Python installed](https://realpython.com/installing-python/) on your machine.\n\nYou might want to use [venv](https://docs.python.org/3/library/venv.html) standard Python library\nto create virtual environments and have Python, `pip` and all dependent packages to be installed and \nserved from the local project directory to avoid messing with system wide packages and their \nversions.\n\n#### Installing Dependencies\n\nInstall all dependencies that are required for the project by running:\n\n```bash\npip install -r requirements.txt\n```\n\n#### Launching Jupyter Locally\n\nAll demos in the project may be run directly in your browser without installing Jupyter locally. But if you want to launch [Jupyter Notebook](http://jupyter.org/) locally you may do it by running the following command from the root folder of the project:\n\n```bash\njupyter notebook\n```\nAfter this Jupyter Notebook will be accessible by `http://localhost:8888`.\n\n#### Launching Jupyter Remotely\n\nEach algorithm section contains demo links to [Jupyter NBViewer](http://nbviewer.jupyter.org/). This is fast online previewer for Jupyter notebooks where you may see demo code, charts and data right in your browser without installing anything locally. In case if you want to _change_ the code and _experiment_ with demo notebook you need to launch the notebook in [Binder](https://mybinder.org/). You may do it by simply clicking the _"Execute on Binder"_ link in top right corner of the NBViewer.\n\n![](./images/binder-button-place.png)\n\n## Datasets\n\nThe list of datasets that is being used for Jupyter Notebook demos may be found in [data folder](data).\n\n## Supporting the project\n\nYou may support this project via â¤ï¸ï¸ [GitHub](https://github.com/sponsors/trekhleb) or â¤ï¸ï¸ [Patreon](https://www.patreon.com/trekhleb).\n\n## Author\n\n- [@trekhleb](https://trekhleb.dev)\n', '{"language":"Jupyter Notebook","stars":23786,"forks":4113,"watchers":23786,"open_issues":27,"topics":["algorithm","jupyter","jupyter-notebook","machine-learning","machine-learning-algorithms","machinelearning","python"],"default_branch":"master","size_kb":14467,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:trekhleb:homemade-gpt-js","source_url":"https://github.com/trekhleb/homemade-gpt-js"},{"type":"has_code","target_id":"github:trekhleb:machine-learning-experiments","source_url":"https://github.com/trekhleb/machine-learning-experiments"},{"type":"has_code","target_id":"github:trekhleb:machine-learning-octave","source_url":"https://github.com/trekhleb/machine-learning-octave"},{"type":"has_code","target_id":"github:sponsors:trekhleb","source_url":"https://github.com/sponsors/trekhleb"}]', NULL, 'MIT', 'approved', 80, 'd50e2035d9a282bdc312875f4af44825', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-trekhleb-homemade-machine-learning from https://github.com/trekhleb.png
Image converted to WebP: data/images/github-trekhleb-homemade-machine-learning.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-deepset-ai-haystack', 'github--deepset-ai--haystack', 'haystack', 'deepset-ai', '<div align="center"> <a href="https://haystack.deepset.ai/"><img src="https://raw.githubusercontent.com/deepset-ai/haystack/main/images/banner.png" alt="Green logo of a stylized white ''H'' with the text ''Haystack, by deepset.'' Abstract green and yellow diagrams in the background."></a> | | | | ------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------...', '["agent","agents","ai","gemini","generative-ai","gpt-4","information-retrieval","large-language-models","llm","machine-learning","nlp","orchestration","python","pytorch","question-answering","rag","retrieval-augmented-generation","semantic-search","summarization","transformers","mdx"]', 'other', 23557, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/deepset-ai/haystack","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<div align="center">\n  <a href="https://haystack.deepset.ai/"><img src="https://raw.githubusercontent.com/deepset-ai/haystack/main/images/banner.png" alt="Green logo of a stylized white ''H'' with the text ''Haystack, by deepset.''Â Abstract green and yellow diagrams in the background."></a>\n\n|         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| CI/CD   | [![Tests](https://github.com/deepset-ai/haystack/actions/workflows/tests.yml/badge.svg)](https://github.com/deepset-ai/haystack/actions/workflows/tests.yml) [![types - Mypy](https://img.shields.io/badge/types-Mypy-blue.svg)](https://github.com/python/mypy) [![Coverage Status](https://coveralls.io/repos/github/deepset-ai/haystack/badge.svg?branch=main)](https://coveralls.io/github/deepset-ai/haystack?branch=main) [![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff) |\n| Docs    | [![Website](https://img.shields.io/website?label=documentation&up_message=online&url=https%3A%2F%2Fdocs.haystack.deepset.ai)](https://docs.haystack.deepset.ai)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n| Package | [![PyPI](https://img.shields.io/pypi/v/haystack-ai)](https://pypi.org/project/haystack-ai/) ![PyPI - Downloads](https://img.shields.io/pypi/dm/haystack-ai?color=blue&logo=pypi&logoColor=gold) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/haystack-ai?logo=python&logoColor=gold) [![Conda Version](https://img.shields.io/conda/vn/conda-forge/haystack-ai.svg)](https://anaconda.org/conda-forge/haystack-ai) [![GitHub](https://img.shields.io/github/license/deepset-ai/haystack?color=blue)](LICENSE) [![License Compliance](https://github.com/deepset-ai/haystack/actions/workflows/license_compliance.yml/badge.svg)](https://github.com/deepset-ai/haystack/actions/workflows/license_compliance.yml) |\n| Meta    | [![Discord](https://img.shields.io/discord/993534733298450452?logo=discord)](https://discord.com/invite/xYvH6drSmA) [![Twitter Follow](https://img.shields.io/twitter/follow/haystack_ai)](https://twitter.com/haystack_ai)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n</div>\n\n[Haystack](https://haystack.deepset.ai/) is an end-to-end LLM framework that allows you to build applications powered by\nLLMs, Transformer models, vector search and more. Whether you want to perform retrieval-augmented generation (RAG),\ndocument search, question answering or answer generation, Haystack can orchestrate state-of-the-art embedding models\nand LLMs into pipelines to build end-to-end NLP applications and solve your use case.\n\n## Table of Contents\n\n- [Installation](#installation)\n- [Documentation](#documentation)\n- [Features](#features)\n- [Use Cases](#features)\n- [Hayhooks (REST API Deployment)](#-tip-1)\n- [Haystack Enterprise](#haystack-enterprise-best-practices-and-expert-support)\n- [deepset Studio](#-deepset-studio-your-development-environment-for-haystack)\n- [Telemetry](#telemetry)\n- [ğŸ–– Community](#-community)\n- [Contributing to Haystack](#contributing-to-haystack)\n- [Who Uses Haystack](#who-uses-haystack)\n\n\n## Installation\n\nThe simplest way to get Haystack is via pip:\n\n```sh\npip install haystack-ai\n```\n\nInstall from the `main` branch to try the newest features:\n```sh\npip install git+https://github.com/deepset-ai/haystack.git@main\n```\n\nHaystack supports multiple installation methods including Docker images. For a comprehensive guide please refer\nto the [documentation](https://docs.haystack.deepset.ai/docs/installation).\n\n## Documentation\n\nIf you''re new to the project, check out ["What is Haystack?"](https://haystack.deepset.ai/overview/intro) then go\nthrough the ["Get Started Guide"](https://haystack.deepset.ai/overview/quick-start) and build your first LLM application\nin a matter of minutes. Keep learning with the [tutorials](https://haystack.deepset.ai/tutorials). For more advanced\nuse cases, or just to get some inspiration, you can browse our Haystack recipes in the\n[Cookbook](https://haystack.deepset.ai/cookbook).\n\nAt any given point, hit the [documentation](https://docs.haystack.deepset.ai/docs/intro) to learn more about Haystack, what can it do for you and the technology behind.\n\n## Features\n\n- **Technology agnostic:** Allow users the flexibility to decide what vendor or technology they want and make it easy to switch out any component for another. Haystack allows you to use and compare models available from OpenAI, Cohere and Hugging Face, as well as your own local models or models hosted on Azure, Bedrock and SageMaker.\n- **Explicit:** Make it transparent how different moving parts can â€œtalkâ€ to each other so it''s easier to fit your tech stack and use case.\n- **Flexible:** Haystack provides all tooling in one place: database access, file conversion, cleaning, splitting, training, eval, inference, and more. And whenever custom behavior is desirable, it''s easy to create custom components.\n- **Extensible:** Provide a uniform and easy way for the community and third parties to build their own components and foster an open ecosystem around Haystack.\n\nSome examples of what you can do with Haystack:\n\n-   Build **retrieval augmented generation (RAG)** by making use of one of the available vector databases and customizing your LLM interaction, the sky is the limit ğŸš€\n-   Perform Question Answering **in natural language** to find granular answers in your documents.\n-   Perform **semantic search** and retrieve documents according to meaning.\n-   Build applications that can make complex decisions making to answer complex queries: such as systems that can resolve complex customer queries, do knowledge search on many disconnected resources and so on.\n-   Scale to millions of docs using retrievers and production-scale components.\n-   Use **off-the-shelf models** or **fine-tune** them to your data.\n-   Use **user feedback** to evaluate, benchmark, and continuously improve your models.\n\n> [!TIP]\n>\n> Would you like to deploy and serve Haystack pipelines as REST APIs yourself? [Hayhooks](https://github.com/deepset-ai/hayhooks) provides a simple way to wrap your pipelines with custom logic and expose them via HTTP endpoints, including OpenAI-compatible chat completion endpoints and compatibility with fully-featured chat interfaces like [open-webui](https://openwebui.com/).\n\n## Haystack Enterprise: Best Practices and Expert Support\n\nGet expert support from the Haystack team, build faster with enterprise-grade templates, and scale securely with deployment guides for cloud and on-prem environments - all with **Haystack Enterprise**. Read more about it our [announcement post](https://haystack.deepset.ai/blog/announcing-haystack-enterprise).\n\nğŸ‘‰ [Get Haystack Enterprise](https://www.deepset.ai/products-and-services/haystack-enterprise?utm_source=github.com&utm_medium=referral&utm_campaign=haystack_enterprise)\n\n## deepset Studio: Your Development Environment for Haystack\n\nUse **deepset Studio** to visually create, deploy, and test your Haystack pipelines. Learn more about it in our [announcement post](https://haystack.deepset.ai/blog/announcing-studio).\n\n![studio](https://github.com/user-attachments/assets/e4f09746-20b5-433e-8261-eca224ac23b3)\n\nğŸ‘‰ [Sign up](https://landing.deepset.ai/deepset-studio-signup)!\n\n> [!TIP]\n><img src="https://github.com/deepset-ai/haystack/raw/main/images/deepset-platform-logo-alternative.jpeg"  width=20%>\n>\n> Are you looking for a managed solution that benefits from Haystack? [deepset AI Platform](https://www.deepset.ai/products-and-services/deepset-ai-platform?utm_campaign=developer-relations&utm_source=haystack&utm_medium=readme) is our fully managed, end-to-end platform to integrate LLMs with your data, which uses Haystack for the LLM pipelines architecture.\n\n## Telemetry\n\nHaystack collects **anonymous** usage statistics of pipeline components. We receive an event every time these components are initialized. This way, we know which components are most relevant to our community.\n\nRead more about telemetry in Haystack or how you can opt out in [Haystack docs](https://docs.haystack.deepset.ai/docs/telemetry).\n\n## ğŸ–– Community\n\nIf you have a feature request or a bug report, feel free to open an [issue in Github](https://github.com/deepset-ai/haystack/issues). We regularly check these and you can expect a quick response. If you''d like to discuss a topic, or get more general advice on how to make Haystack work for your project, you can start a thread in [Github Discussions](https://github.com/deepset-ai/haystack/discussions) or our [Discord channel](https://discord.com/invite/VBpFzsgRVF). We also check [ğ• (Twitter)](https://twitter.com/haystack_ai) and [Stack Overflow](https://stackoverflow.com/questions/tagged/haystack).\n\n## Contributing to Haystack\n\nWe are very open to the community''s contributions - be it a quick fix of a typo, or a completely new feature! You don''t need to be a Haystack expert to provide meaningful improvements. To learn how to get started, check out our [Contributor Guidelines](https://github.com/deepset-ai/haystack/blob/main/CONTRIBUTING.md) first.\n\nThere are several ways you can contribute to Haystack:\n- Contribute to the main Haystack project\n- Contribute an integration on [haystack-core-integrations](https://github.com/deepset-ai/haystack-core-integrations)\n- Contribute to the documentation in [haystack/docs-website](https://github.com/deepset-ai/haystack/tree/main/docs-website)\n\n> [!TIP]\n>ğŸ‘‰ **[Check out the full list of issues that are open to contributions](https://github.com/orgs/deepset-ai/projects/14)**\n\n## Who Uses Haystack\n\nHere''s a list of projects and companies using Haystack. Are you also using Haystack? Open a PR or [tell us your story](https://forms.gle/Mm3G1aEST3GAH2rn8).\n\n- Tech & AI Innovators: [Apple](https://www.apple.com/), [Meta](https://www.meta.com/about), [Databricks](https://www.databricks.com/), [NVIDIA](https://developer.nvidia.com/blog/reducing-development-time-for-intelligent-virtual-assistants-in-contact-centers/), [PostHog](https://github.com/PostHog/max-ai#readme)\n- Public Sector: [German Federal Ministry of Research, Technology, and Space (BMFTR)](https://www.deepset.ai/case-studies/german-federal-ministry-research-technology-space-bmftr), [PD, Baden-WÃ¼rttemberg State](https://www.pd-g.de/)\n- Enterprise & Telecom: [Alcatel-Lucent](https://www.al-enterprise.com/), [Intel](https://github.com/intel/open-domain-question-and-answer#readme), [NOS Portugal](https://www.nos.pt/en/welcome), [TELUS Agriculture & Consumer Goods](https://www.telus.com/agcg/en)\n- Aerospace & Hardware: [Airbus](https://www.deepset.ai/case-studies/airbus), [Infineon](https://www.infineon.com/), [LEGO](https://github.com/larsbaunwall/bricky#readme)\n- Media & Entertainment: [Netflix](https://netflix.com), [Comcast](https://arxiv.org/html/2405.00801v2), [Zeit Online](https://www.deepset.ai/case-studies/zeit-online), [Rakuten](https://www.rakuten.com/)\n- Legal & Publishing: [Manz](https://www.deepset.ai/case-studies/manz), [Oxford University Press](https://corp.oup.com/)\n- Startups & Research: [YPulse](https://www.deepset.ai/case-studies/ypulse), [BetterUp](https://www.betterup.com/), [Intel Labs](https://github.com/IntelLabs/fastRAG#readme)\n', '{"language":"MDX","stars":23557,"forks":2510,"watchers":23557,"open_issues":117,"topics":["agent","agents","ai","gemini","generative-ai","gpt-4","information-retrieval","large-language-models","llm","machine-learning","nlp","orchestration","python","pytorch","question-answering","rag","retrieval-augmented-generation","semantic-search","summarization","transformers"],"default_branch":"main","size_kb":55022,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:python:mypy","source_url":"https://github.com/python/mypy"},{"type":"has_code","target_id":"github:astral-sh:ruff","source_url":"https://github.com/astral-sh/ruff"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack.git@main","source_url":"https://github.com/deepset-ai/haystack.git@main"},{"type":"has_code","target_id":"github:deepset-ai:hayhooks","source_url":"https://github.com/deepset-ai/hayhooks"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack-core-integrations","source_url":"https://github.com/deepset-ai/haystack-core-integrations"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:orgs:deepset-ai","source_url":"https://github.com/orgs/deepset-ai"},{"type":"has_code","target_id":"github:PostHog:max-ai","source_url":"https://github.com/PostHog/max-ai#readme"},{"type":"has_code","target_id":"github:intel:open-domain-question-and-answer","source_url":"https://github.com/intel/open-domain-question-and-answer#readme"},{"type":"has_code","target_id":"github:larsbaunwall:bricky","source_url":"https://github.com/larsbaunwall/bricky#readme"},{"type":"has_code","target_id":"github:IntelLabs:fastRAG","source_url":"https://github.com/IntelLabs/fastRAG#readme"}]', NULL, 'Apache-2.0', 'approved', 80, 'f53722665093d18dc16da5a1ed3c1003', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-deepset-ai-haystack from https://github.com/deepset-ai.png
Image converted to WebP: data/images/github-deepset-ai-haystack.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-PaddlePaddle-Paddle', 'github--paddlepaddle--paddle', 'Paddle', 'PaddlePaddle', '<p align="center"> <img align="center" src="doc/imgs/logo.png", width=1600> <p> -------------------------------------------------------------------------------- English | ç®€ä½“ä¸­æ–‡ | æ—¥æœ¬èª !X (formerly Twitter) URL Welcome to the PaddlePaddle GitHub. PaddlePaddle, as the first independent R&D deep learning platform in China, has been officially open-sourced to professional communities since 2016. It is an industrial platform with advanced technologies and rich features that cover core deep learning ...', '["deep-learning","distributed-training","efficiency","machine-learning","neural-network","paddlepaddle","python","scalability","c++"]', 'other', 23483, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/PaddlePaddle/Paddle","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n<img align="center" src="doc/imgs/logo.png", width=1600>\n<p>\n\n--------------------------------------------------------------------------------\n\nEnglish | [ç®€ä½“ä¸­æ–‡](./README_cn.md) | [æ—¥æœ¬èª](./README_ja.md)\n\n[![Documentation Status](https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat)](https://paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n[![Documentation Status](https://img.shields.io/badge/ä¸­æ–‡æ–‡æ¡£-æœ€æ–°-brightgreen.svg)](https://paddlepaddle.org.cn/documentation/docs/zh/guides/index_cn.html)\n[![Release](https://img.shields.io/github/release/PaddlePaddle/Paddle.svg)](https://github.com/PaddlePaddle/Paddle/releases)\n[![License](https://img.shields.io/badge/license-Apache%202-blue.svg)](LICENSE)\n![X (formerly Twitter) URL](https://img.shields.io/twitter/url?url=https%3A%2F%2Fx.com%2FPaddlePaddle)\n\nWelcome to the PaddlePaddle GitHub.\n\nPaddlePaddle, as the first independent R&D deep learning platform in China, has been officially open-sourced to professional communities since 2016. It is an industrial platform with advanced technologies and rich features that cover core deep learning frameworks, basic model libraries, end-to-end development kits, tools & components as well as service platforms.\nPaddlePaddle originates from industrial practices with dedication and commitments to industrialization. It has been widely adopted by a wide range of sectors including manufacturing, agriculture, enterprise service, and so on while serving more than 23.33 million developers, 760,000 companies and generating 1,100,000 models. With such advantages, PaddlePaddle has helped an increasing number of partners commercialize AI.\n\n## Installation\n\n### Latest PaddlePaddle Release: 3.2\nOur vision is to enable deep learning for everyone via PaddlePaddle.\nPlease refer to our [release announcement](https://github.com/PaddlePaddle/Paddle/releases) to track the latest features of PaddlePaddle.\n\n### Install Latest Stable Release or Nightly Release\n\nFor detailed information about installation, please view [Quick Install](https://www.paddlepaddle.org.cn/install/quick)\n\n## **PaddlePaddle New Generation Framework 3.2**\n\n* **Unified Dynamic/Static Graphs and Automatic Parallelism**\n\n    By requiring only minimal tensor partitioning annotations based on a single-card configuration, PaddlePaddle automatically discovers the most efficient distributed parallel strategy. This significantly reduces the costs of industrial development and training, enabling developers to focus more intently on model and algorithm innovation.\n\n* **Integrated Training and Inference for Large Models**\n\n    The same framework supports both training and inference, achieving code reuse and seamless integration between these stages. This provides a unified development experience and maximum training efficiency for the entire large model workflow, offering the industry a superior development experience.\n\n* **High-Order Differentiation for Scientific Computing**\n\n    Provides capabilities such as high-order automatic differentiation, complex number operations, Fourier transforms, compilation optimization, and distributed training support. It facilitates scientific exploration in fields including mathematics, mechanics, materials science, meteorology, and biology, substantially improving the speed of solving differential equations.\n\n* **Neural Network Compiler**\n\n    Adopting an integrated framework design, it supports efficient training and flexible inference for diverse models, including generative and scientific computing models. It achieves an effective balance between computational flexibility and high performance, significantly lowering performance optimization costs.\n\n* **Heterogeneous Multi-Chip Adaptation**\n    Features a mature and complete unified adaptation solution for multiple hardware types. Through standardized interfaces, it abstracts the variations in development interfaces across different chip software stacks, realizing a pluggable architecture.\n\n## Documentation\n\nWe provide [English](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html) and\n[Chinese](https://www.paddlepaddle.org.cn/documentation/docs/zh/guide/index_cn.html) documentation.\n\n* [Guides](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/index_en.html)\n\n  You might want to start from how to implement deep learning basics with PaddlePaddle.\n\n* [Practice](https://www.paddlepaddle.org.cn/documentation/docs/zh/tutorial/index_cn.html)\n\n  So far you have already been familiar with Fluid. And the next step should be building a more efficient model or inventing your original Operator.\n\n* [API Reference](https://www.paddlepaddle.org.cn/documentation/docs/en/api/index_en.html)\n\n   Our new API enables much shorter programs.\n\n* [How to Contribute](https://www.paddlepaddle.org.cn/documentation/docs/en/guides/08_contribution/index_en.html)\n\n   We appreciate your contributions!\n\n## Open Source Community\n\n* [Github Issues](https://github.com/PaddlePaddle/Paddle/issues): bug reports, feature requests, install issues, usage issues, etc.\n* Many of our contribution events offer varying levels of mentorship from experienced community members, please check the events in the pinned issues, and consider attending.\n* Community Blog: <https://pfcc.blog/>\n* See more details about PaddlePaddle community at [community](https://github.com/PaddlePaddle/community).\n\n## Copyright and License\n\nPaddlePaddle is provided under the [Apache-2.0 license](LICENSE).\n', '{"language":"C++","stars":23483,"forks":5902,"watchers":23483,"open_issues":1590,"topics":["deep-learning","distributed-training","efficiency","machine-learning","neural-network","paddlepaddle","python","scalability"],"default_branch":"develop","size_kb":529465,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:PaddlePaddle:Paddle","source_url":"https://github.com/PaddlePaddle/Paddle"},{"type":"has_code","target_id":"github:PaddlePaddle:Paddle","source_url":"https://github.com/PaddlePaddle/Paddle"},{"type":"has_code","target_id":"github:PaddlePaddle:Paddle","source_url":"https://github.com/PaddlePaddle/Paddle"},{"type":"has_code","target_id":"github:PaddlePaddle:community","source_url":"https://github.com/PaddlePaddle/community"}]', NULL, 'Apache-2.0', 'approved', 65, '50f8dfa8a0a834f2cf87314efdf28731', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-PaddlePaddle-Paddle from https://github.com/PaddlePaddle.png
Image converted to WebP: data/images/github-PaddlePaddle-Paddle.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-mlflow-mlflow', 'github--mlflow--mlflow', 'mlflow', 'mlflow', '<h1 align="center" style="border-bottom: none"> <a href="https://mlflow.org/"> <img alt="MLflow logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" /> </a> </h1> <h2 align="center" style="border-bottom: none">Open-Source Platform for Productionizing AI</h2> MLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observabilit...', '["agentops","agents","ai","ai-governance","apache-spark","evaluation","langchain","llm-evaluation","llmops","machine-learning","ml","mlflow","mlops","model-management","observability","open-source","openai","prompt-engineering","python"]', 'other', 23202, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/mlflow/mlflow","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<h1 align="center" style="border-bottom: none">\n    <a href="https://mlflow.org/">\n        <img alt="MLflow logo" src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/logo.svg" width="200" />\n    </a>\n</h1>\n<h2 align="center" style="border-bottom: none">Open-Source Platform for Productionizing AI</h2>\n\nMLflow is an open-source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end **experiment tracking**, **observability**, and **evaluations**, all in one integrated platform.\n\n<div align="center">\n\n[![Python SDK](https://img.shields.io/pypi/v/mlflow)](https://pypi.org/project/mlflow/)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/mlflow)](https://pepy.tech/projects/mlflow)\n[![License](https://img.shields.io/github/license/mlflow/mlflow)](https://github.com/mlflow/mlflow/blob/main/LICENSE)\n<a href="https://twitter.com/intent/follow?screen_name=mlflow" target="_blank">\n<img src="https://img.shields.io/twitter/follow/mlflow?logo=X&color=%20%23f5f5f5"\n      alt="follow on X(Twitter)"></a>\n<a href="https://www.linkedin.com/company/mlflow-org/" target="_blank">\n<img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff"\n      alt="follow on LinkedIn"></a>\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/mlflow/mlflow)\n\n</div>\n\n<div align="center">\n   <div>\n      <a href="https://mlflow.org/"><strong>Website</strong></a> Â·\n      <a href="https://mlflow.org/docs/latest"><strong>Docs</strong></a> Â·\n      <a href="https://github.com/mlflow/mlflow/issues/new/choose"><strong>Feature Request</strong></a> Â·\n      <a href="https://mlflow.org/blog"><strong>News</strong></a> Â·\n      <a href="https://www.youtube.com/@mlflowoss"><strong>YouTube</strong></a> Â·\n      <a href="https://lu.ma/mlflow?k=c"><strong>Events</strong></a>\n   </div>\n</div>\n\n<br>\n\n## ğŸš€ Installation\n\nTo install the MLflow Python package, run the following command:\n\n```\npip install mlflow\n```\n\n## ğŸ“¦ Core Components\n\nMLflow is **the only platform that provides a unified solution for all your AI/ML needs**, including LLMs, Agents, Deep Learning, and traditional machine learning.\n\n### ğŸ’¡ For LLM / GenAI Developers\n\n<table>\n  <tr>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-tracing.png" alt="Tracing" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/llms/tracing/index.html"><strong>ğŸ” Tracing / Observability</strong></a>\n        <br><br>\n        <div>Trace the internal states of your LLM/agentic applications for debugging quality issues and monitoring performance with ease.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/tracing/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n    <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-llm-eval.png" alt="LLM Evaluation" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/"><strong>ğŸ“Š LLM Evaluation</strong></a>\n        <br><br>\n        <div>A suite of automated model evaluation tools, seamlessly integrated with experiment tracking to compare across multiple versions.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/eval-monitor/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-prompt.png" alt="Prompt Management">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/prompt-registry/"><strong>ğŸ¤– Prompt Management</strong></a>\n        <br><br>\n        <div>Version, track, and reuse prompts across your organization, helping maintain consistency and improve collaboration in prompt development.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-registry/create-and-edit-prompts/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-logged-model.png" alt="MLflow Hero">\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/genai/prompt-version-mgmt/version-tracking/"><strong>ğŸ“¦ App Version Tracking</strong></a>\n        <br><br>\n        <div>MLflow keeps track of many moving parts in your AI applications, such as models, prompts, tools, and code, with end-to-end lineage.</div><br>\n        <a href="https://mlflow.org/docs/latest/genai/version-tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n### ğŸ“ For Data Scientists\n\n<table>\n  <tr>\n    <td colspan="2" align="center" >\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-experiment.png" alt="Tracking" width=50%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/tracking/"><strong>ğŸ“ Experiment Tracking</strong></a>\n        <br><br>\n        <div>Track your models, parameters, metrics, and evaluation results in ML experiments and compare them using an interactive UI.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/tracking/quickstart/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-model-registry.png" alt="Model Registry" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/"><strong>ğŸ’¾ Model Registry</strong></a>\n        <br><br>\n        <div> A centralized model store designed to collaboratively manage the full lifecycle and deployment of machine learning models.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/model-registry/tutorial/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n    <td>\n      <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-deployment.png" alt="Deployment" width=100%>\n    <div align="center">\n        <br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/"><strong>ğŸš€ Deployment</strong></a>\n        <br><br>\n        <div> Tools for seamless model deployment to batch and real-time scoring on platforms like Docker, Kubernetes, Azure ML, and AWS SageMaker.</div><br>\n        <a href="https://mlflow.org/docs/latest/ml/deployment/">Getting Started â†’</a>\n        <br><br>\n    </div>\n    </td>\n  </tr>\n</table>\n\n## ğŸŒ Hosting MLflow Anywhere\n\n<div align="center" >\n  <img src="https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-providers.png" alt="Providers" width=100%>\n</div>\n\nYou can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.\n\nTrusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:\n\n- [Amazon SageMaker](https://aws.amazon.com/sagemaker-ai/experiments/)\n- [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/concept-mlflow?view=azureml-api-2)\n- [Databricks](https://www.databricks.com/product/managed-mlflow)\n- [Nebius](https://nebius.com/services/managed-mlflow)\n\nFor hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).\n\n## ğŸ—£ï¸ Supported Programming Languages\n\n- [Python](https://pypi.org/project/mlflow/)\n- [TypeScript / JavaScript](https://www.npmjs.com/package/mlflow-tracing)\n- [Java](https://mvnrepository.com/artifact/org.mlflow/mlflow-client)\n- [R](https://cran.r-project.org/web/packages/mlflow/readme/README.html)\n\n## ğŸ”— Integrations\n\nMLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.\n\n![Integrations](https://raw.githubusercontent.com/mlflow/mlflow/refs/heads/master/assets/readme-integrations.png)\n\n## Usage Examples\n\n### Tracing (Observability) ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))\n\nMLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.\n\n```python\nimport mlflow\nfrom openai import OpenAI\n\n# Enable tracing for OpenAI\nmlflow.openai.autolog()\n\n# Query OpenAI LLM normally\nresponse = OpenAI().chat.completions.create(\n    model="gpt-4o-mini",\n    messages=[{"role": "user", "content": "Hi!"}],\n    temperature=0.1,\n)\n```\n\nThen navigate to the "Traces" tab in the MLflow UI to find the trace records OpenAI query.\n\n### Evaluating LLMs, Prompts, and Agents ([Doc](https://mlflow.org/docs/latest/genai/eval-monitor/index.html))\n\nThe following example runs automatic evaluation for question-answering tasks with several built-in metrics.\n\n```python\nimport os\nimport openai\nimport mlflow\nfrom mlflow.genai.scorers import Correctness, Guidelines\n\nclient = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))\n\n# 1. Define a simple QA dataset\ndataset = [\n    {\n        "inputs": {"question": "Can MLflow manage prompts?"},\n        "expectations": {"expected_response": "Yes!"},\n    },\n    {\n        "inputs": {"question": "Can MLflow create a taco for my lunch?"},\n        "expectations": {\n            "expected_response": "No, unfortunately, MLflow is not a taco maker."\n        },\n    },\n]\n\n\n# 2. Define a prediction function to generate responses\ndef predict_fn(question: str) -> str:\n    response = client.chat.completions.create(\n        model="gpt-4o-mini", messages=[{"role": "user", "content": question}]\n    )\n    return response.choices[0].message.content\n\n\n# 3.Run the evaluation\nresults = mlflow.genai.evaluate(\n    data=dataset,\n    predict_fn=predict_fn,\n    scorers=[\n        # Built-in LLM judge\n        Correctness(),\n        # Custom criteria using LLM judge\n        Guidelines(name="is_english", guidelines="The answer must be in English"),\n    ],\n)\n```\n\nNavigate to the "Evaluations" tab in the MLflow UI to find the evaluation results.\n\n### Tracking Model Training ([Doc](https://mlflow.org/docs/latest/ml/tracking/))\n\nThe following examples trains a simple regression model with scikit-learn, while enabling MLflow''s [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.\n\n```python\nimport mlflow\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Enable MLflow''s automatic experiment tracking for scikit-learn\nmlflow.sklearn.autolog()\n\n# Load the training dataset\ndb = load_diabetes()\nX_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n\nrf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n# MLflow triggers logging automatically upon model fitting\nrf.fit(X_train, y_train)\n```\n\nOnce the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.\n\n```\nmlflow server\n```\n\n## ğŸ’­ Support\n\n- For help or questions about MLflow usage (e.g. "how do I do X?") visit the [documentation](https://mlflow.org/docs/latest).\n- In the documentation, you can ask the question to our AI-powered chat bot. Click on the **"Ask AI"** button at the right bottom.\n- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.\n- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).\n- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)\n  or join us on [Slack](https://mlflow.org/slack).\n\n## ğŸ¤ Contributing\n\nWe happily welcome contributions to MLflow!\n\n- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)\n- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n- Writing about MLflow and sharing your experience\n\nPlease see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.\n\n## â­ï¸ Star History\n\n<a href="https://star-history.com/#mlflow/mlflow&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=mlflow/mlflow&type=Date" />\n </picture>\n</a>\n\n## âœï¸ Citation\n\nIf you use MLflow in your research, please cite it using the "Cite this repository" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.\n\n## ğŸ‘¥ Core Members\n\nMLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members.\n\n- [Ben Wilson](https://github.com/BenWilson2)\n- [Corey Zumar](https://github.com/dbczumar)\n- [Daniel Lok](https://github.com/daniellok-db)\n- [Gabriel Fu](https://github.com/gabrielfu)\n- [Harutaka Kawamura](https://github.com/harupy)\n- [Joel Robin P](https://github.com/joelrobin18)\n- [Serena Ruan](https://github.com/serena-ruan)\n- [Tomu Hirata](https://github.com/TomeHirata)\n- [Weichen Xu](https://github.com/WeichenXu123)\n- [Yuki Watanabe](https://github.com/B-Step62)\n', '{"language":"Python","stars":23202,"forks":5044,"watchers":23202,"open_issues":2114,"topics":["agentops","agents","ai","ai-governance","apache-spark","evaluation","langchain","llm-evaluation","llmops","machine-learning","ml","mlflow","mlops","model-management","observability","open-source","openai","prompt-engineering"],"default_branch":"master","size_kb":1226357,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"},{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"},{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"},{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"},{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"},{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"},{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"},{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"}]', NULL, 'Apache-2.0', 'approved', 80, '21ddb2a844f8d94892ee86f846b2ef77', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-mlflow-mlflow from https://github.com/mlflow.png
Image converted to WebP: data/images/github-mlflow-mlflow.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-sebastianruder-NLP-progress', 'github--sebastianruder--nlp-progress', 'NLP-progress', 'sebastianruder', '- Automatic speech recognition - CCG - Common sense - Constituency parsing - Coreference resolution - Data-to-Text Generation - Dependency parsing - Dialogue - Domain adaptation - Entity linking - Grammatical error correction - Information extraction - Intent Detection and Slot Filling - Keyphrase Extraction and Generation - Language modeling - Lexical normalization - Machine translation - Missing elements - Multi-task learning - Multi-modal - Named entity recognition - Natural language infer...', '["dialogue","machine-learning","machine-translation","named-entity-recognition","natural-language-processing","nlp-tasks","python"]', 'other', 22973, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/sebastianruder/NLP-progress","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Tracking Progress in Natural Language Processing\n\n## Table of contents\n\n### English\n\n- [Automatic speech recognition](english/automatic_speech_recognition.md)\n- [CCG](english/ccg.md)\n- [Common sense](english/common_sense.md)\n- [Constituency parsing](english/constituency_parsing.md)\n- [Coreference resolution](english/coreference_resolution.md)\n- [Data-to-Text Generation](english/data_to_text_generation.md)\n- [Dependency parsing](english/dependency_parsing.md)\n- [Dialogue](english/dialogue.md)\n- [Domain adaptation](english/domain_adaptation.md)\n- [Entity linking](english/entity_linking.md)\n- [Grammatical error correction](english/grammatical_error_correction.md)\n- [Information extraction](english/information_extraction.md)\n- [Intent Detection and Slot Filling](english/intent_detection_slot_filling.md) \n- [Keyphrase Extraction and Generation](english/keyphrase_extraction_generation.md)\n- [Language modeling](english/language_modeling.md)\n- [Lexical normalization](english/lexical_normalization.md)\n- [Machine translation](english/machine_translation.md)\n- [Missing elements](english/missing_elements.md)\n- [Multi-task learning](english/multi-task_learning.md)\n- [Multi-modal](english/multimodal.md)\n- [Named entity recognition](english/named_entity_recognition.md)\n- [Natural language inference](english/natural_language_inference.md)\n- [Part-of-speech tagging](english/part-of-speech_tagging.md)\n- [Paraphrase Generation](english/paraphrase-generation.md)\n- [Question answering](english/question_answering.md)\n- [Relation prediction](english/relation_prediction.md)\n- [Relationship extraction](english/relationship_extraction.md)\n- [Semantic textual similarity](english/semantic_textual_similarity.md)\n- [Semantic parsing](english/semantic_parsing.md)\n- [Semantic role labeling](english/semantic_role_labeling.md)\n- [Sentiment analysis](english/sentiment_analysis.md)\n- [Shallow syntax](english/shallow_syntax.md)\n- [Simplification](english/simplification.md)\n- [Stance detection](english/stance_detection.md)\n- [Summarization](english/summarization.md)\n- [Taxonomy learning](english/taxonomy_learning.md)\n- [Temporal processing](english/temporal_processing.md)\n- [Text classification](english/text_classification.md)\n- [Word sense disambiguation](english/word_sense_disambiguation.md)\n\n### Vietnamese\n\n- [Dependency parsing](vietnamese/vietnamese.md#dependency-parsing)\n- [Intent detection and Slot filling](vietnamese/vietnamese.md#intent-detection-and-slot-filling)\n- [Machine translation](vietnamese/vietnamese.md#machine-translation)\n- [Named entity recognition](vietnamese/vietnamese.md#named-entity-recognition)\n- [Part-of-speech tagging](vietnamese/vietnamese.md#part-of-speech-tagging)\n- [Semantic parsing](vietnamese/vietnamese.md#semantic-parsing)\n- [Word segmentation](vietnamese/vietnamese.md#word-segmentation)\n\n### Hindi\n\n- [Chunking](hindi/hindi.md#chunking)\n- [Part-of-speech tagging](hindi/hindi.md#part-of-speech-tagging)\n- [Machine Translation](hindi/hindi.md#machine-translation)\n\n### Chinese\n\n- [Entity linking](chinese/chinese.md#entity-linking)\n- [Chinese word segmentation](chinese/chinese_word_segmentation.md)\n- [Question answering](chinese/question_answering.md)\n\nFor more tasks, datasets and results in Chinese, check out the [Chinese NLP](https://chinesenlp.xyz/#/) website.\n\n### French\n\n- [Question answering](french/question_answering.md)\n- [Summarization](french/summarization.md)\n\n### Russian\n\n- [Question answering](russian/question_answering.md)\n- [Sentiment Analysis](russian/sentiment-analysis.md)\n- [Summarization](russian/summarization.md)\n\n### Spanish\n\n- [Named Entity Recognition](spanish/named_entity_recognition.md)\n- [Entity linking](spanish/entity_linking.md#entity-linking)\n- [Summarization](spanish/summarization.md)\n\n### Portuguese\n\n- [Question Answering](portuguese/question_answering.md)\n\n### Korean\n\n- [Question Answering](korean/question_answering.md)\n\n### Nepali\n\n- [Machine Translation](nepali/nepali.md#machine-translation)\n\n### Bengali\n- [Part-of-speech Tagging](bengali/part_of_speech_tagging.md)\n- [Emotion Detection](bengali/emotion_detection.md)\n- [Sentiment Analysis](bengali/sentiment_analysis.md)\n\n### Persian\n- [Named entity recognition](persian/named_entity_recognition.md)\n- [Natural language inference](persian/natural_language_inference.md)\n- [Summarization](persian/summarization.md)\n\n### Turkish\n\n- [Summarization](turkish/summarization.md)\n\n### German\n\n- [Question Answering](german/question_answering.md)\n- [Summarization](german/summarization.md)\n\n### Arabic\n- [Language modeling](arabic/language_modeling.md)\n\n\nThis document aims to track the progress in Natural Language Processing (NLP) and give an overview\nof the state-of-the-art (SOTA) across the most common NLP tasks and their corresponding datasets.\n\nIt aims to cover both traditional and core NLP tasks such as dependency parsing and part-of-speech tagging\nas well as more recent ones such as reading comprehension and natural language inference. The main objective\nis to provide the reader with a quick overview of benchmark datasets and the state-of-the-art for their\ntask of interest, which serves as a stepping stone for further research. To this end, if there is a \nplace where results for a task are already published and regularly maintained, such as a public leaderboard,\nthe reader will be pointed there.\n\nIf you want to find this document again in the future, just go to [`nlpprogress.com`](https://nlpprogress.com/)\nor [`nlpsota.com`](http://nlpsota.com/) in your browser.\n\n### Contributing\n\n#### Guidelines\n\n**Results** &nbsp; Results reported in published papers are preferred; an exception may be made for influential preprints.\n\n**Datasets** &nbsp; Datasets should have been used for evaluation in at least one published paper besides \nthe one that introduced the dataset.\n\n**Code** &nbsp; We recommend to add a link to an implementation \nif available. You can add a `Code` column (see below) to the table if it does not exist.\nIn the `Code` column, indicate an official implementation with [Official](http://link_to_implementation).\nIf an unofficial implementation is available, use [Link](http://link_to_implementation) (see below).\nIf no implementation is available, you can leave the cell empty.\n\n#### Adding a new result\n\nIf you would like to add a new result, you can just click on the small edit button in the top-right\ncorner of the file for the respective task (see below).\n\n![Click on the edit button to add a file](img/edit_file.png)\n\nThis allows you to edit the file in Markdown. Simply add a row to the corresponding table in the\nsame format. Make sure that the table stays sorted (with the best result on top). \nAfter you''ve made your change, make sure that the table still looks ok by clicking on the\n"Preview changes" tab at the top of the page. If everything looks good, go to the bottom of the page,\nwhere you see the below form. \n\n![Fill out the file change information](img/propose_file_change.png)\n\nAdd a name for your proposed change, an optional description, indicate that you would like to\n"Create a new branch for this commit and start a pull request", and click on "Propose file change".\n\n#### Adding a new dataset or task\n\nFor adding a new dataset or task, you can also follow the steps above. Alternatively, you can fork the repository.\nIn both cases, follow the steps below:\n\n1. If your task is completely new, create a new file and link to it in the table of contents above.\n2. If not, add your task or dataset to the respective section of the corresponding file (in alphabetical order).\n3. Briefly describe the dataset/task and include relevant references. \n4. Describe the evaluation setting and evaluation metric.\n5. Show how an annotated example of the dataset/task looks like.\n6. Add a download link if available.\n7. Copy the below table and fill in at least two results (including the state-of-the-art)\n  for your dataset/task (change Score to the metric of your dataset). If your dataset/task\n  has multiple metrics, add them to the right of `Score`.\n1. Submit your change as a pull request.\n  \n| Model           | Score  |  Paper / Source | Code |\n| ------------- | :-----:| --- | --- |\n|  |  |  | |\n\n\n### Wish list\n\nThese are tasks and datasets that are still missing:\n\n- Bilingual dictionary induction\n- Discourse parsing\n- Keyphrase extraction\n- Knowledge base population (KBP)\n- More dialogue tasks\n- Semi-supervised learning\n- Frame-semantic parsing (FrameNet full-sentence analysis)\n\n### Exporting into a structured format\n\nYou can extract all the data into a structured, machine-readable JSON format with parsed tasks, descriptions and SOTA tables. \n\nThe instructions are in [structured/README.md](structured/README.md).\n\n### Instructions for building the site locally\n\nInstructions for building the website locally using Jekyll can be found [here](jekyll_instructions.md).\n\n\n', '{"language":"Python","stars":22973,"forks":3622,"watchers":22973,"open_issues":40,"topics":["dialogue","machine-learning","machine-translation","named-entity-recognition","natural-language-processing","nlp-tasks"],"default_branch":"master","size_kb":1399,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[]', NULL, 'MIT', 'approved', 65, '0f9ab7689787985f284bafc31ef1ac60', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-sebastianruder-NLP-progress from https://github.com/sebastianruder.png
Image converted to WebP: data/images/github-sebastianruder-NLP-progress.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-lukasmasuch-best-of-ml-python', 'github--lukasmasuch--best-of-ml-python', 'best-of-ml-python', 'lukasmasuch', '<!-- markdownlint-disable --> <h1 align="center"> Best-of Machine Learning with Python <br> </h1> <p align="center"> <strong>ğŸ†&nbsp; A ranked list of awesome machine learning Python libraries. Updated weekly.</strong> </p> <p align="center"> <a href="https://github.com/ml-tooling/best-of" title="Best-of-badge"><img src="http://bit.ly/3o3EHNN"></a> <a href="#Contents" title="Project Count"><img src="https://img.shields.io/badge/projects-920-blue.svg?color=5ac4bf"></a> <a href="#Contribution" ...', '["automl","chatgpt","data-analysis","data-science","data-visualization","data-visualizations","deep-learning","gpt","gpt-3","jax","keras","machine-learning","ml","nlp","python","pytorch","scikit-learn","tensorflow","transformer"]', 'other', 22915, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/lukasmasuch/best-of-ml-python","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<!-- markdownlint-disable -->\n<h1 align="center">\n    Best-of Machine Learning with Python\n    <br>\n</h1>\n\n<p align="center">\n    <strong>ğŸ†&nbsp; A ranked list of awesome machine learning Python libraries. Updated weekly.</strong>\n</p>\n\n<p align="center">\n    <a href="https://github.com/ml-tooling/best-of" title="Best-of-badge"><img src="http://bit.ly/3o3EHNN"></a>\n    <a href="#Contents" title="Project Count"><img src="https://img.shields.io/badge/projects-920-blue.svg?color=5ac4bf"></a>\n    <a href="#Contribution" title="Contributions are welcome"><img src="https://img.shields.io/badge/contributions-welcome-green.svg"></a>\n    <a href="https://github.com/ml-tooling/best-of-ml-python/releases" title="Best-of Updates"><img src="https://img.shields.io/github/release-date/ml-tooling/best-of-ml-python?color=green&label=updated"></a>\n    <a href="https://mltooling.substack.com/subscribe" title="Subscribe to newsletter"><img src="http://bit.ly/2Md9rxM"></a>\n    <a href="https://twitter.com/mltooling" title="Follow on Twitter"><img src="https://img.shields.io/twitter/follow/mltooling.svg?style=social&label=Follow"></a>\n</p>\n\nThis curated list contains 920 awesome open-source projects with a total of 5.1M stars grouped into 34 categories. All projects are ranked by a project-quality score, which is calculated based on various metrics automatically collected from GitHub and different package managers. If you like to add or update projects, feel free to open an [issue](https://github.com/ml-tooling/best-of-ml-python/issues/new/choose), submit a [pull request](https://github.com/ml-tooling/best-of-ml-python/pulls), or directly edit the [projects.yaml](https://github.com/ml-tooling/best-of-ml-python/edit/main/projects.yaml). Contributions are very welcome!\n\n---\n\n<p align="center">\n     ğŸ§™â€â™‚ï¸&nbsp; Discover other <a href="https://best-of.org">best-of lists</a> or create <a href="https://github.com/best-of-lists/best-of/blob/main/create-best-of-list.md">your own</a>.<br>\n    ğŸ“«&nbsp; Subscribe to our <a href="https://mltooling.substack.com/subscribe">newsletter</a> for updates and trending projects.\n</p>\n\n---\n\n\n## Contents\n\n- [Machine Learning Frameworks](#machine-learning-frameworks) _64 projects_\n- [Data Visualization](#data-visualization) _55 projects_\n- [Text Data & NLP](#text-data--nlp) _103 projects_\n- [Image Data](#image-data) _64 projects_\n- [Graph Data](#graph-data) _36 projects_\n- [Audio Data](#audio-data) _29 projects_\n- [Geospatial Data](#geospatial-data) _22 projects_\n- [Financial Data](#financial-data) _25 projects_\n- [Time Series Data](#time-series-data) _29 projects_\n- [Medical Data](#medical-data) _19 projects_\n- [Tabular Data](#tabular-data) _6 projects_\n- [Optical Character Recognition](#optical-character-recognition) _12 projects_\n- [Data Containers & Structures](#data-containers--structures) _1 projects_\n- [Data Loading & Extraction](#data-loading--extraction) _1 projects_\n- [Web Scraping & Crawling](#web-scraping--crawling) _1 projects_\n- [Data Pipelines & Streaming](#data-pipelines--streaming) _2 projects_\n- [Distributed Machine Learning](#distributed-machine-learning) _36 projects_\n- [Hyperparameter Optimization & AutoML](#hyperparameter-optimization--automl) _52 projects_\n- [Reinforcement Learning](#reinforcement-learning) _23 projects_\n- [Recommender Systems](#recommender-systems) _17 projects_\n- [Privacy Machine Learning](#privacy-machine-learning) _7 projects_\n- [Workflow & Experiment Tracking](#workflow--experiment-tracking) _40 projects_\n- [Model Serialization & Deployment](#model-serialization--deployment) _20 projects_\n- [Model Interpretability](#model-interpretability) _55 projects_\n- [Vector Similarity Search (ANN)](#vector-similarity-search-ann) _13 projects_\n- [Probabilistics & Statistics](#probabilistics--statistics) _24 projects_\n- [Adversarial Robustness](#adversarial-robustness) _9 projects_\n- [GPU & Accelerator Utilities](#gpu--accelerator-utilities) _20 projects_\n- [Tensorflow Utilities](#tensorflow-utilities) _16 projects_\n- [Jax Utilities](#jax-utilities) _3 projects_\n- [Sklearn Utilities](#sklearn-utilities) _19 projects_\n- [Pytorch Utilities](#pytorch-utilities) _32 projects_\n- [Database Clients](#database-clients) _1 projects_\n- [Others](#others) _66 projects_\n\n## Explanation\n- ğŸ¥‡ğŸ¥ˆğŸ¥‰&nbsp; Combined project-quality score\n- â­ï¸&nbsp; Star count from GitHub\n- ğŸ£&nbsp; New project _(less than 6 months old)_\n- ğŸ’¤&nbsp; Inactive project _(6 months no activity)_\n- ğŸ’€&nbsp; Dead project _(12 months no activity)_\n- ğŸ“ˆğŸ“‰&nbsp; Project is trending up or down\n- â•&nbsp; Project was recently added\n- â—ï¸&nbsp; Warning _(e.g. missing/risky license)_\n- ğŸ‘¨â€ğŸ’»&nbsp; Contributors count from GitHub\n- ğŸ”€&nbsp; Fork count from GitHub\n- ğŸ“‹&nbsp; Issue count from GitHub\n- â±ï¸&nbsp; Last update timestamp on package manager\n- ğŸ“¥&nbsp; Download count from package manager\n- ğŸ“¦&nbsp; Number of dependent projects\n- <img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13">&nbsp; Tensorflow related project\n- <img src="https://git.io/JLy1F" style="display:inline;" width="13" height="13">&nbsp; Sklearn related project\n- <img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13">&nbsp; PyTorch related project\n- <img src="https://git.io/JLy1X" style="display:inline;" width="13" height="13">&nbsp; MxNet related project\n- <img src="https://git.io/JLy1N" style="display:inline;" width="13" height="13">&nbsp; Apache Spark related project\n- <img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13">&nbsp; Jupyter related project\n- <img src="https://git.io/JLy1M" style="display:inline;" width="13" height="13">&nbsp; PaddlePaddle related project\n- <img src="https://git.io/JLy1S" style="display:inline;" width="13" height="13">&nbsp; Pandas related project\n- <img src="https://jax.readthedocs.io/en/latest/_static/favicon.png" style="display:inline;" width="13" height="13">&nbsp; Jax related project\n\n<br>\n\n## Machine Learning Frameworks\n\n<a href="#contents"><img align="right" width="15" height="15" src="https://git.io/JtehR" alt="Back to top"></a>\n\n_General-purpose machine learning and deep learning frameworks._\n\n<details><summary><b><a href="https://github.com/tensorflow/tensorflow">Tensorflow</a></b> (ğŸ¥‡56 Â·  â­ 200K) - An Open Source Machine Learning Framework for Everyone. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/tensorflow/tensorflow) (ğŸ‘¨â€ğŸ’» 5K Â· ğŸ”€ 75K Â· ğŸ“¦ 540K Â· ğŸ“‹ 42K - 4% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/tensorflow/tensorflow\n	```\n- [PyPi](https://pypi.org/project/tensorflow) (ğŸ“¥ 26M / month Â· ğŸ“¦ 9.6K Â· â±ï¸ 13.08.2025):\n	```\n	pip install tensorflow\n	```\n- [Conda](https://anaconda.org/conda-forge/tensorflow) (ğŸ“¥ 6M Â· â±ï¸ 27.10.2025):\n	```\n	conda install -c conda-forge tensorflow\n	```\n- [Docker Hub](https://hub.docker.com/r/tensorflow/tensorflow) (ğŸ“¥ 81M Â· â­ 2.8K Â· â±ï¸ 30.10.2025):\n	```\n	docker pull tensorflow/tensorflow\n	```\n</details>\n<details><summary><b><a href="https://github.com/pytorch/pytorch">PyTorch</a></b> (ğŸ¥‡56 Â·  â­ 94K) - Tensors and Dynamic neural networks in Python with strong GPU.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/pytorch/pytorch) (ğŸ‘¨â€ğŸ’» 6K Â· ğŸ”€ 26K Â· ğŸ“¥ 110K Â· ğŸ“¦ 830K Â· ğŸ“‹ 56K - 30% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/pytorch/pytorch\n	```\n- [PyPi](https://pypi.org/project/torch) (ğŸ“¥ 70M / month Â· ğŸ“¦ 30K Â· â±ï¸ 15.10.2025):\n	```\n	pip install torch\n	```\n- [Conda](https://anaconda.org/pytorch/pytorch) (ğŸ“¥ 29M Â· â±ï¸ 25.03.2025):\n	```\n	conda install -c pytorch pytorch\n	```\n</details>\n<details><summary><b><a href="https://github.com/scikit-learn/scikit-learn">scikit-learn</a></b> (ğŸ¥‡53 Â·  â­ 64K) - scikit-learn: machine learning in Python. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1F" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/scikit-learn/scikit-learn) (ğŸ‘¨â€ğŸ’» 3.4K Â· ğŸ”€ 26K Â· ğŸ“¥ 1.1K Â· ğŸ“¦ 1.3M Â· ğŸ“‹ 12K - 17% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/scikit-learn/scikit-learn\n	```\n- [PyPi](https://pypi.org/project/scikit-learn) (ğŸ“¥ 140M / month Â· ğŸ“¦ 35K Â· â±ï¸ 09.09.2025):\n	```\n	pip install scikit-learn\n	```\n- [Conda](https://anaconda.org/conda-forge/scikit-learn) (ğŸ“¥ 40M Â· â±ï¸ 09.09.2025):\n	```\n	conda install -c conda-forge scikit-learn\n	```\n</details>\n<details><summary><b><a href="https://github.com/keras-team/keras">Keras</a></b> (ğŸ¥‡50 Â·  â­ 64K) - Deep Learning for humans. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/keras-team/keras) (ğŸ‘¨â€ğŸ’» 1.4K Â· ğŸ”€ 20K Â· ğŸ“¦ 300K Â· ğŸ“‹ 13K - 2% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/keras-team/keras\n	```\n- [PyPi](https://pypi.org/project/keras) (ğŸ“¥ 19M / month Â· ğŸ“¦ 2K Â· â±ï¸ 27.10.2025):\n	```\n	pip install keras\n	```\n- [Conda](https://anaconda.org/conda-forge/keras) (ğŸ“¥ 4.5M Â· â±ï¸ 28.10.2025):\n	```\n	conda install -c conda-forge keras\n	```\n</details>\n<details><summary><b><a href="https://github.com/dmlc/xgboost">XGBoost</a></b> (ğŸ¥‡46 Â·  â­ 28K) - Scalable, Portable and Distributed Gradient Boosting (GBDT, GBRT or.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/dmlc/xgboost) (ğŸ‘¨â€ğŸ’» 670 Â· ğŸ”€ 8.8K Â· ğŸ“¥ 20K Â· ğŸ“¦ 170K Â· ğŸ“‹ 5.6K - 8% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/dmlc/xgboost\n	```\n- [PyPi](https://pypi.org/project/xgboost) (ğŸ“¥ 31M / month Â· ğŸ“¦ 2.9K Â· â±ï¸ 21.10.2025):\n	```\n	pip install xgboost\n	```\n- [Conda](https://anaconda.org/conda-forge/xgboost) (ğŸ“¥ 6.6M Â· â±ï¸ 16.09.2025):\n	```\n	conda install -c conda-forge xgboost\n	```\n</details>\n<details><summary><b><a href="https://github.com/PaddlePaddle/Paddle">PaddlePaddle</a></b> (ğŸ¥‡46 Â·  â­ 23K) - PArallel Distributed Deep LEarning: Machine Learning.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1M" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/PaddlePaddle/Paddle) (ğŸ‘¨â€ğŸ’» 1.5K Â· ğŸ”€ 5.9K Â· ğŸ“¥ 15K Â· ğŸ“¦ 8.8K Â· ğŸ“‹ 20K - 8% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/PaddlePaddle/Paddle\n	```\n- [PyPi](https://pypi.org/project/paddlepaddle) (ğŸ“¥ 1.6M / month Â· ğŸ“¦ 280 Â· â±ï¸ 30.10.2025):\n	```\n	pip install paddlepaddle\n	```\n</details>\n<details><summary><b><a href="https://github.com/jax-ml/jax">jax</a></b> (ğŸ¥‡45 Â·  â­ 34K) - Composable transformations of Python+NumPy programs: differentiate,.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/jax-ml/jax) (ğŸ‘¨â€ğŸ’» 980 Â· ğŸ”€ 3.2K Â· ğŸ“¦ 47K Â· ğŸ“‹ 6.6K - 24% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/google/jax\n	```\n- [PyPi](https://pypi.org/project/jax) (ğŸ“¥ 12M / month Â· ğŸ“¦ 3.1K Â· â±ï¸ 15.10.2025):\n	```\n	pip install jax\n	```\n- [Conda](https://anaconda.org/conda-forge/jaxlib) (ğŸ“¥ 3.2M Â· â±ï¸ 06.10.2025):\n	```\n	conda install -c conda-forge jaxlib\n	```\n</details>\n<details><summary><b><a href="https://github.com/Lightning-AI/pytorch-lightning">pytorch-lightning</a></b> (ğŸ¥‡45 Â·  â­ 30K) - Pretrain, finetune ANY AI model of ANY size on 1 or.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/Lightning-AI/pytorch-lightning) (ğŸ‘¨â€ğŸ’» 1K Â· ğŸ”€ 3.6K Â· ğŸ“¥ 15K Â· ğŸ“¦ 48K Â· ğŸ“‹ 7.4K - 11% open Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/Lightning-AI/lightning\n	```\n- [PyPi](https://pypi.org/project/pytorch-lightning) (ğŸ“¥ 9.8M / month Â· ğŸ“¦ 1.8K Â· â±ï¸ 05.09.2025):\n	```\n	pip install pytorch-lightning\n	```\n- [Conda](https://anaconda.org/conda-forge/pytorch-lightning) (ğŸ“¥ 1.7M Â· â±ï¸ 05.09.2025):\n	```\n	conda install -c conda-forge pytorch-lightning\n	```\n</details>\n<details><summary><b><a href="https://github.com/statsmodels/statsmodels">StatsModels</a></b> (ğŸ¥‡45 Â·  â­ 11K) - Statsmodels: statistical modeling and econometrics in Python. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/statsmodels/statsmodels) (ğŸ‘¨â€ğŸ’» 470 Â· ğŸ”€ 3.3K Â· ğŸ“¥ 36 Â· ğŸ“¦ 180K Â· ğŸ“‹ 5.8K - 50% open Â· â±ï¸ 22.10.2025):\n\n	```\n	git clone https://github.com/statsmodels/statsmodels\n	```\n- [PyPi](https://pypi.org/project/statsmodels) (ğŸ“¥ 24M / month Â· ğŸ“¦ 5.6K Â· â±ï¸ 07.07.2025):\n	```\n	pip install statsmodels\n	```\n- [Conda](https://anaconda.org/conda-forge/statsmodels) (ğŸ“¥ 22M Â· â±ï¸ 01.10.2025):\n	```\n	conda install -c conda-forge statsmodels\n	```\n</details>\n<details><summary><b><a href="https://github.com/apache/spark">PySpark</a></b> (ğŸ¥ˆ44 Â·  â­ 42K) - Apache Spark Python API. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1N" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/apache/spark) (ğŸ‘¨â€ğŸ’» 3.3K Â· ğŸ”€ 29K Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/apache/spark\n	```\n- [PyPi](https://pypi.org/project/pyspark) (ğŸ“¥ 47M / month Â· ğŸ“¦ 2.1K Â· â±ï¸ 30.10.2025):\n	```\n	pip install pyspark\n	```\n- [Conda](https://anaconda.org/conda-forge/pyspark) (ğŸ“¥ 4.2M Â· â±ï¸ 08.09.2025):\n	```\n	conda install -c conda-forge pyspark\n	```\n</details>\n<details><summary><b><a href="https://github.com/microsoft/LightGBM">LightGBM</a></b> (ğŸ¥ˆ42 Â·  â­ 18K) - A fast, distributed, high performance gradient boosting (GBT, GBDT, GBRT,.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/microsoft/LightGBM) (ğŸ‘¨â€ğŸ’» 330 Â· ğŸ”€ 3.9K Â· ğŸ“¥ 310K Â· ğŸ“¦ 56K Â· ğŸ“‹ 3.6K - 12% open Â· â±ï¸ 28.10.2025):\n\n	```\n	git clone https://github.com/microsoft/LightGBM\n	```\n- [PyPi](https://pypi.org/project/lightgbm) (ğŸ“¥ 11M / month Â· ğŸ“¦ 1.6K Â· â±ï¸ 15.02.2025):\n	```\n	pip install lightgbm\n	```\n- [Conda](https://anaconda.org/conda-forge/lightgbm) (ğŸ“¥ 4.1M Â· â±ï¸ 20.10.2025):\n	```\n	conda install -c conda-forge lightgbm\n	```\n</details>\n<details><summary><b><a href="https://github.com/catboost/catboost">Catboost</a></b> (ğŸ¥ˆ42 Â·  â­ 8.6K) - A fast, scalable, high performance Gradient Boosting on Decision.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/catboost/catboost) (ğŸ‘¨â€ğŸ’» 1.4K Â· ğŸ”€ 1.2K Â· ğŸ“¥ 460K Â· ğŸ“¦ 19 Â· ğŸ“‹ 2.5K - 25% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/catboost/catboost\n	```\n- [PyPi](https://pypi.org/project/catboost) (ğŸ“¥ 5.1M / month Â· ğŸ“¦ 650 Â· â±ï¸ 13.04.2025):\n	```\n	pip install catboost\n	```\n- [Conda](https://anaconda.org/conda-forge/catboost) (ğŸ“¥ 2.2M Â· â±ï¸ 09.08.2025):\n	```\n	conda install -c conda-forge catboost\n	```\n</details>\n<details><summary><b><a href="https://github.com/fastai/fastai">Fastai</a></b> (ğŸ¥ˆ41 Â·  â­ 28K) - The fastai deep learning library. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/fastai/fastai) (ğŸ‘¨â€ğŸ’» 680 Â· ğŸ”€ 7.6K Â· ğŸ“¦ 23K Â· ğŸ“‹ 1.9K - 14% open Â· â±ï¸ 26.10.2025):\n\n	```\n	git clone https://github.com/fastai/fastai\n	```\n- [PyPi](https://pypi.org/project/fastai) (ğŸ“¥ 640K / month Â· ğŸ“¦ 340 Â· â±ï¸ 26.10.2025):\n	```\n	pip install fastai\n	```\n</details>\n<details><summary><b><a href="https://github.com/apache/flink">PyFlink</a></b> (ğŸ¥ˆ39 Â·  â­ 25K) - Apache Flink Python API. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/apache/flink) (ğŸ‘¨â€ğŸ’» 2.1K Â· ğŸ”€ 14K Â· ğŸ“¦ 21 Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/apache/flink\n	```\n- [PyPi](https://pypi.org/project/apache-flink) (ğŸ“¥ 450K / month Â· ğŸ“¦ 38 Â· â±ï¸ 28.10.2025):\n	```\n	pip install apache-flink\n	```\n</details>\n<details><summary><b><a href="https://github.com/google/flax">Flax</a></b> (ğŸ¥ˆ38 Â·  â­ 6.9K) - Flax is a neural network library for JAX that is designed for.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://jax.readthedocs.io/en/latest/_static/favicon.png" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/google/flax) (ğŸ‘¨â€ğŸ’» 280 Â· ğŸ”€ 740 Â· ğŸ“¥ 61 Â· ğŸ“¦ 15K Â· ğŸ“‹ 1.3K - 33% open Â· â±ï¸ 27.10.2025):\n\n	```\n	git clone https://github.com/google/flax\n	```\n- [PyPi](https://pypi.org/project/flax) (ğŸ“¥ 2M / month Â· ğŸ“¦ 740 Â· â±ï¸ 25.09.2025):\n	```\n	pip install flax\n	```\n- [Conda](https://anaconda.org/conda-forge/flax) (ğŸ“¥ 130K Â· â±ï¸ 27.10.2025):\n	```\n	conda install -c conda-forge flax\n	```\n</details>\n<details><summary><b><a href="https://github.com/pytorch/ignite">Ignite</a></b> (ğŸ¥ˆ36 Â·  â­ 4.7K) - High-level library to help with training and evaluating neural.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/pytorch/ignite) (ğŸ‘¨â€ğŸ’» 1K Â· ğŸ”€ 660 Â· ğŸ“¦ 3.9K Â· ğŸ“‹ 1.4K - 10% open Â· â±ï¸ 16.10.2025):\n\n	```\n	git clone https://github.com/pytorch/ignite\n	```\n- [PyPi](https://pypi.org/project/pytorch-ignite) (ğŸ“¥ 170K / month Â· ğŸ“¦ 120 Â· â±ï¸ 30.10.2025):\n	```\n	pip install pytorch-ignite\n	```\n- [Conda](https://anaconda.org/pytorch/ignite) (ğŸ“¥ 250K Â· â±ï¸ 16.10.2025):\n	```\n	conda install -c pytorch ignite\n	```\n</details>\n<details><summary><b><a href="https://github.com/arogozhnikov/einops">einops</a></b> (ğŸ¥ˆ35 Â·  â­ 9.2K) - Flexible and powerful tensor operations for readable and reliable code.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/arogozhnikov/einops) (ğŸ‘¨â€ğŸ’» 34 Â· ğŸ”€ 380 Â· ğŸ“¦ 82K Â· ğŸ“‹ 200 - 17% open Â· â±ï¸ 12.08.2025):\n\n	```\n	git clone https://github.com/arogozhnikov/einops\n	```\n- [PyPi](https://pypi.org/project/einops) (ğŸ“¥ 15M / month Â· ğŸ“¦ 2.6K Â· â±ï¸ 09.02.2025):\n	```\n	pip install einops\n	```\n- [Conda](https://anaconda.org/conda-forge/einops) (ğŸ“¥ 470K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge einops\n	```\n</details>\n<details><summary><b><a href="https://github.com/ivy-llc/ivy">ivy</a></b> (ğŸ¥ˆ34 Â·  â­ 14K) - Convert Machine Learning Code Between Frameworks. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/ivy-llc/ivy) (ğŸ‘¨â€ğŸ’» 1.5K Â· ğŸ”€ 5.6K Â· ğŸ“‹ 17K - 5% open Â· â±ï¸ 10.10.2025):\n\n	```\n	git clone https://github.com/unifyai/ivy\n	```\n- [PyPi](https://pypi.org/project/ivy) (ğŸ“¥ 33K / month Â· ğŸ“¦ 16 Â· â±ï¸ 16.06.2025):\n	```\n	pip install ivy\n	```\n</details>\n<details><summary><b><a href="https://github.com/jina-ai/serve">Jina</a></b> (ğŸ¥ˆ33 Â·  â­ 22K Â· ğŸ’¤) - Build multimodal AI applications with cloud-native stack. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/jina-ai/serve) (ğŸ‘¨â€ğŸ’» 180 Â· ğŸ”€ 2.2K Â· â±ï¸ 24.03.2025):\n\n	```\n	git clone https://github.com/jina-ai/jina\n	```\n- [PyPi](https://pypi.org/project/jina) (ğŸ“¥ 120K / month Â· ğŸ“¦ 29 Â· â±ï¸ 24.03.2025):\n	```\n	pip install jina\n	```\n- [Conda](https://anaconda.org/conda-forge/jina-core) (ğŸ“¥ 110K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge jina-core\n	```\n- [Docker Hub](https://hub.docker.com/r/jinaai/jina) (ğŸ“¥ 1.8M Â· â­ 9 Â· â±ï¸ 24.03.2025):\n	```\n	docker pull jinaai/jina\n	```\n</details>\n<details><summary><b><a href="https://github.com/mlpack/mlpack">mlpack</a></b> (ğŸ¥ˆ33 Â·  â­ 5.5K) - mlpack: a fast, header-only C++ machine learning library. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/mlpack/mlpack) (ğŸ‘¨â€ğŸ’» 340 Â· ğŸ”€ 1.7K Â· ğŸ“‹ 1.7K - 1% open Â· â±ï¸ 27.10.2025):\n\n	```\n	git clone https://github.com/mlpack/mlpack\n	```\n- [PyPi](https://pypi.org/project/mlpack) (ğŸ“¥ 4.7K / month Â· ğŸ“¦ 6 Â· â±ï¸ 22.05.2025):\n	```\n	pip install mlpack\n	```\n- [Conda](https://anaconda.org/conda-forge/mlpack) (ğŸ“¥ 410K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge mlpack\n	```\n</details>\n<details><summary><b><a href="https://github.com/explosion/thinc">Thinc</a></b> (ğŸ¥ˆ33 Â·  â­ 2.9K Â· ğŸ’¤) - A refreshing functional take on deep learning, compatible with your.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/explosion/thinc) (ğŸ‘¨â€ğŸ’» 67 Â· ğŸ”€ 280 Â· ğŸ“¥ 2K Â· ğŸ“¦ 70K Â· ğŸ“‹ 160 - 14% open Â· â±ï¸ 07.03.2025):\n\n	```\n	git clone https://github.com/explosion/thinc\n	```\n- [PyPi](https://pypi.org/project/thinc) (ğŸ“¥ 17M / month Â· ğŸ“¦ 160 Â· â±ï¸ 04.04.2025):\n	```\n	pip install thinc\n	```\n- [Conda](https://anaconda.org/conda-forge/thinc) (ğŸ“¥ 3.9M Â· â±ï¸ 06.07.2025):\n	```\n	conda install -c conda-forge thinc\n	```\n</details>\n<details><summary><b><a href="https://github.com/ludwig-ai/ludwig">Ludwig</a></b> (ğŸ¥‰32 Â·  â­ 12K Â· ğŸ’¤) - Low-code framework for building custom LLMs, neural networks,.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/ludwig-ai/ludwig) (ğŸ‘¨â€ğŸ’» 160 Â· ğŸ”€ 1.2K Â· ğŸ“¦ 340 Â· ğŸ“‹ 1.1K - 4% open Â· â±ï¸ 17.10.2024):\n\n	```\n	git clone https://github.com/ludwig-ai/ludwig\n	```\n- [PyPi](https://pypi.org/project/ludwig) (ğŸ“¥ 3.8K / month Â· ğŸ“¦ 6 Â· â±ï¸ 30.07.2024):\n	```\n	pip install ludwig\n	```\n</details>\n<details><summary><b><a href="https://github.com/skorch-dev/skorch">skorch</a></b> (ğŸ¥‰32 Â·  â­ 6.1K) - A scikit-learn compatible neural network library that wraps.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1F" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/skorch-dev/skorch) (ğŸ‘¨â€ğŸ’» 68 Â· ğŸ”€ 400 Â· ğŸ“¦ 1.7K Â· ğŸ“‹ 540 - 12% open Â· â±ï¸ 23.10.2025):\n\n	```\n	git clone https://github.com/skorch-dev/skorch\n	```\n- [PyPi](https://pypi.org/project/skorch) (ğŸ“¥ 150K / month Â· ğŸ“¦ 110 Â· â±ï¸ 08.08.2025):\n	```\n	pip install skorch\n	```\n- [Conda](https://anaconda.org/conda-forge/skorch) (ğŸ“¥ 810K Â· â±ï¸ 08.08.2025):\n	```\n	conda install -c conda-forge skorch\n	```\n</details>\n<details><summary><b><a href="https://github.com/google-deepmind/sonnet">Sonnet</a></b> (ğŸ¥‰31 Â·  â­ 9.9K) - TensorFlow-based neural network library. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/google-deepmind/sonnet) (ğŸ‘¨â€ğŸ’» 61 Â· ğŸ”€ 1.3K Â· ğŸ“¦ 1.5K Â· ğŸ“‹ 190 - 16% open Â· â±ï¸ 04.08.2025):\n\n	```\n	git clone https://github.com/deepmind/sonnet\n	```\n- [PyPi](https://pypi.org/project/dm-sonnet) (ğŸ“¥ 35K / month Â· ğŸ“¦ 19 Â· â±ï¸ 02.01.2024):\n	```\n	pip install dm-sonnet\n	```\n- [Conda](https://anaconda.org/conda-forge/sonnet) (ğŸ“¥ 47K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge sonnet\n	```\n</details>\n<details><summary><b><a href="https://github.com/google-deepmind/dm-haiku">Haiku</a></b> (ğŸ¥‰31 Â·  â­ 3.1K Â· ğŸ“‰) - JAX-based neural network library. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/google-deepmind/dm-haiku) (ğŸ‘¨â€ğŸ’» 90 Â· ğŸ”€ 260 Â· ğŸ“¦ 2.6K Â· ğŸ“‹ 250 - 29% open Â· â±ï¸ 29.09.2025):\n\n	```\n	git clone https://github.com/deepmind/dm-haiku\n	```\n- [PyPi](https://pypi.org/project/dm-haiku) (ğŸ“¥ 260K / month Â· ğŸ“¦ 200 Â· â±ï¸ 18.09.2025):\n	```\n	pip install dm-haiku\n	```\n- [Conda](https://anaconda.org/conda-forge/dm-haiku) (ğŸ“¥ 44K Â· â±ï¸ 19.09.2025):\n	```\n	conda install -c conda-forge dm-haiku\n	```\n</details>\n<details><summary><b><a href="https://github.com/ROCm/tensorflow-upstream">tensorflow-upstream</a></b> (ğŸ¥‰31 Â·  â­ 700) - TensorFlow ROCm port. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/ROCm/tensorflow-upstream) (ğŸ‘¨â€ğŸ’» 5K Â· ğŸ”€ 100 Â· ğŸ“¥ 31 Â· ğŸ“‹ 400 - 3% open Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/ROCmSoftwarePlatform/tensorflow-upstream\n	```\n- [PyPi](https://pypi.org/project/tensorflow-rocm) (ğŸ“¥ 1.7K / month Â· ğŸ“¦ 9 Â· â±ï¸ 10.01.2024):\n	```\n	pip install tensorflow-rocm\n	```\n</details>\n<details><summary><b><a href="https://github.com/geomstats/geomstats">Geomstats</a></b> (ğŸ¥‰30 Â·  â­ 1.4K) - Computations and statistics on manifolds with geometric structures. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/geomstats/geomstats) (ğŸ‘¨â€ğŸ’» 97 Â· ğŸ”€ 260 Â· ğŸ“¦ 150 Â· ğŸ“‹ 570 - 36% open Â· â±ï¸ 06.10.2025):\n\n	```\n	git clone https://github.com/geomstats/geomstats\n	```\n- [PyPi](https://pypi.org/project/geomstats) (ğŸ“¥ 15K / month Â· ğŸ“¦ 12 Â· â±ï¸ 09.09.2024):\n	```\n	pip install geomstats\n	```\n- [Conda](https://anaconda.org/conda-forge/geomstats) (ğŸ“¥ 8.2K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge geomstats\n	```\n</details>\n<details><summary><b><a href="https://github.com/pyRiemann/pyRiemann">pyRiemann</a></b> (ğŸ¥‰28 Â·  â­ 700) - Machine learning for multivariate data through the Riemannian.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1F" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/pyRiemann/pyRiemann) (ğŸ‘¨â€ğŸ’» 38 Â· ğŸ”€ 170 Â· ğŸ“¦ 480 Â· ğŸ“‹ 110 - 2% open Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/pyRiemann/pyRiemann\n	```\n- [PyPi](https://pypi.org/project/pyriemann) (ğŸ“¥ 75K / month Â· ğŸ“¦ 31 Â· â±ï¸ 23.07.2025):\n	```\n	pip install pyriemann\n	```\n- [Conda](https://anaconda.org/conda-forge/pyriemann) (ğŸ“¥ 16K Â· â±ï¸ 23.07.2025):\n	```\n	conda install -c conda-forge pyriemann\n	```\n</details>\n<details><summary><b><a href="https://github.com/numenta/nupic-legacy">NuPIC</a></b> (ğŸ¥‰27 Â·  â­ 6.4K Â· ğŸ’¤) - Numenta Platform for Intelligent Computing is an implementation of.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/numenta/nupic-legacy) (ğŸ‘¨â€ğŸ’» 120 Â· ğŸ”€ 1.5K Â· ğŸ“¥ 26 Â· ğŸ“¦ 21 Â· ğŸ“‹ 1.8K - 25% open Â· â±ï¸ 03.12.2024):\n\n	```\n	git clone https://github.com/numenta/nupic\n	```\n- [PyPi](https://pypi.org/project/nupic) (ğŸ“¥ 510 / month Â· â±ï¸ 01.09.2016):\n	```\n	pip install nupic\n	```\n</details>\n<details><summary><b><a href="https://github.com/determined-ai/determined">Determined</a></b> (ğŸ¥‰26 Â·  â­ 3.2K Â· ğŸ’¤) - Determined is an open-source machine learning.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/determined-ai/determined) (ğŸ‘¨â€ğŸ’» 120 Â· ğŸ”€ 360 Â· ğŸ“¥ 7.8K Â· ğŸ“‹ 450 - 22% open Â· â±ï¸ 20.03.2025):\n\n	```\n	git clone https://github.com/determined-ai/determined\n	```\n- [PyPi](https://pypi.org/project/determined) (ğŸ“¥ 33K / month Â· ğŸ“¦ 4 Â· â±ï¸ 19.03.2025):\n	```\n	pip install determined\n	```\n</details>\n<details><summary><b><a href="https://github.com/sony/nnabla">Neural Network Libraries</a></b> (ğŸ¥‰26 Â·  â­ 2.8K) - Neural Network Libraries. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/sony/nnabla) (ğŸ‘¨â€ğŸ’» 76 Â· ğŸ”€ 340 Â· ğŸ“¥ 1K Â· ğŸ“‹ 95 - 36% open Â· â±ï¸ 29.08.2025):\n\n	```\n	git clone https://github.com/sony/nnabla\n	```\n- [PyPi](https://pypi.org/project/nnabla) (ğŸ“¥ 1.6K / month Â· ğŸ“¦ 44 Â· â±ï¸ 29.05.2024):\n	```\n	pip install nnabla\n	```\n</details>\n<details><summary><b><a href="https://github.com/deepinv/deepinv">deepinv</a></b> (ğŸ¥‰26 Â·  â­ 540) - DeepInverse: a PyTorch library for solving imaging inverse problems.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/deepinv/deepinv) (ğŸ‘¨â€ğŸ’» 53 Â· ğŸ”€ 120 Â· ğŸ“¥ 24 Â· ğŸ“¦ 23 Â· ğŸ“‹ 350 - 33% open Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/deepinv/deepinv\n	```\n- [PyPi](https://pypi.org/project/deepinv) (ğŸ“¥ 2.4K / month Â· â±ï¸ 08.10.2025):\n	```\n	pip install deepinv\n	```\n</details>\n<details><summary><b><a href="https://github.com/towhee-io/towhee">Towhee</a></b> (ğŸ¥‰23 Â·  â­ 3.4K Â· ğŸ’¤) - Towhee is a framework that is dedicated to making neural data.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/towhee-io/towhee) (ğŸ‘¨â€ğŸ’» 38 Â· ğŸ”€ 260 Â· ğŸ“¥ 2.7K Â· ğŸ“‹ 670 - 0% open Â· â±ï¸ 18.10.2024):\n\n	```\n	git clone https://github.com/towhee-io/towhee\n	```\n- [PyPi](https://pypi.org/project/towhee) (ğŸ“¥ 1.3K / month Â· â±ï¸ 04.12.2023):\n	```\n	pip install towhee\n	```\n</details>\n<details><summary><b><a href="https://github.com/nubank/fklearn">fklearn</a></b> (ğŸ¥‰22 Â·  â­ 1.5K) - fklearn: Functional Machine Learning. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/nubank/fklearn) (ğŸ‘¨â€ğŸ’» 56 Â· ğŸ”€ 170 Â· ğŸ“¦ 16 Â· ğŸ“‹ 64 - 60% open Â· â±ï¸ 23.04.2025):\n\n	```\n	git clone https://github.com/nubank/fklearn\n	```\n- [PyPi](https://pypi.org/project/fklearn) (ğŸ“¥ 750 / month Â· â±ï¸ 26.02.2025):\n	```\n	pip install fklearn\n	```\n</details>\n<details><summary><b><a href="https://github.com/run-house/kubetorch">Runhouse</a></b> (ğŸ¥‰21 Â·  â­ 1.1K) - Distribute and run AI workloads magically in Python, like PyTorch.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/run-house/kubetorch) (ğŸ‘¨â€ğŸ’» 16 Â· ğŸ”€ 41 Â· ğŸ“¥ 79 Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/run-house/runhouse\n	```\n- [PyPi](https://pypi.org/project/runhouse) (ğŸ“¥ 4.5K / month Â· ğŸ“¦ 1 Â· â±ï¸ 10.03.2025):\n	```\n	pip install runhouse\n	```\n</details>\n<details><summary><b><a href="https://github.com/neoml-lib/neoml">NeoML</a></b> (ğŸ¥‰19 Â·  â­ 790) - Machine learning framework for both deep learning and traditional.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/neoml-lib/neoml) (ğŸ‘¨â€ğŸ’» 41 Â· ğŸ”€ 130 Â· ğŸ“¦ 2 Â· ğŸ“‹ 91 - 40% open Â· â±ï¸ 28.10.2025):\n\n	```\n	git clone https://github.com/neoml-lib/neoml\n	```\n- [PyPi](https://pypi.org/project/neoml) (ğŸ“¥ 190 / month Â· â±ï¸ 26.12.2023):\n	```\n	pip install neoml\n	```\n</details>\n<details><summary><b><a href="https://github.com/serengil/chefboost">chefboost</a></b> (ğŸ¥‰19 Â·  â­ 480) - A Lightweight Decision Tree Framework supporting regular algorithms:.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/serengil/chefboost) (ğŸ‘¨â€ğŸ’» 7 Â· ğŸ”€ 100 Â· ğŸ“¦ 72 Â· â±ï¸ 09.07.2025):\n\n	```\n	git clone https://github.com/serengil/chefboost\n	```\n- [PyPi](https://pypi.org/project/chefboost) (ğŸ“¥ 770 / month Â· â±ï¸ 30.10.2024):\n	```\n	pip install chefboost\n	```\n</details>\n<details><summary><b><a href="https://github.com/Xtra-Computing/thundergbm">ThunderGBM</a></b> (ğŸ¥‰18 Â·  â­ 710 Â· ğŸ’¤) - ThunderGBM: Fast GBDTs and Random Forests on GPUs. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/Xtra-Computing/thundergbm) (ğŸ‘¨â€ğŸ’» 12 Â· ğŸ”€ 88 Â· ğŸ“¦ 4 Â· ğŸ“‹ 81 - 48% open Â· â±ï¸ 19.03.2025):\n\n	```\n	git clone https://github.com/Xtra-Computing/thundergbm\n	```\n- [PyPi](https://pypi.org/project/thundergbm) (ğŸ“¥ 220 / month Â· â±ï¸ 19.09.2022):\n	```\n	pip install thundergbm\n	```\n</details>\n<details><summary>Show 26 hidden projects...</summary>\n\n- <b><a href="https://github.com/davisking/dlib">dlib</a></b> (ğŸ¥ˆ40 Â·  â­ 14K) - A toolkit for making real world machine learning and data analysis.. <code><a href="https://tldrlegal.com/search?q=BSL-1.0">â—ï¸BSL-1.0</a></code>\n- <b><a href="https://github.com/apache/mxnet">MXNet</a></b> (ğŸ¥ˆ38 Â·  â­ 21K Â· ğŸ’€) - Lightweight, Portable, Flexible Distributed/Mobile Deep.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1X" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/Theano/Theano">Theano</a></b> (ğŸ¥ˆ37 Â·  â­ 10K Â· ğŸ’€) - Theano was a Python library that allows you to define, optimize, and.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code>\n- <b><a href="https://github.com/mindsdb/mindsdb">MindsDB</a></b> (ğŸ¥ˆ33 Â·  â­ 37K) - Federated query engine for AI - The only MCP Server youll ever need. <code><a href="https://tldrlegal.com/search?q=ICU">â—ï¸ICU</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/VowpalWabbit/vowpal_wabbit">Vowpal Wabbit</a></b> (ğŸ¥ˆ33 Â·  â­ 8.6K Â· ğŸ’€) - Vowpal Wabbit is a machine learning system which pushes the.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code>\n- <b><a href="https://github.com/chainer/chainer">Chainer</a></b> (ğŸ¥ˆ33 Â·  â­ 5.9K Â· ğŸ’€) - A flexible framework of neural networks for deep learning. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/apple/turicreate">Turi Create</a></b> (ğŸ¥‰32 Â·  â­ 11K Â· ğŸ’€) - Turi Create simplifies the development of custom machine.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code>\n- <b><a href="https://github.com/tensorpack/tensorpack">tensorpack</a></b> (ğŸ¥‰32 Â·  â­ 6.3K Â· ğŸ’€) - A Neural Net Training Interface on TensorFlow, with.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/tflearn/tflearn">TFlearn</a></b> (ğŸ¥‰31 Â·  â­ 9.6K Â· ğŸ’€) - Deep learning library featuring a higher-level API for TensorFlow. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/clab/dynet">dyNET</a></b> (ğŸ¥‰31 Â·  â­ 3.4K Â· ğŸ’€) - DyNet: The Dynamic Neural Network Toolkit. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n- <b><a href="https://github.com/microsoft/CNTK">CNTK</a></b> (ğŸ¥‰29 Â·  â­ 18K Â· ğŸ’€) - Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/Lasagne/Lasagne">Lasagne</a></b> (ğŸ¥‰28 Â·  â­ 3.9K Â· ğŸ’€) - Lightweight library to build and train neural networks in Theano. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/shogun-toolbox/shogun">SHOGUN</a></b> (ğŸ¥‰26 Â·  â­ 3.1K Â· ğŸ’€) - Unified and efficient Machine Learning. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code>\n- <b><a href="https://github.com/amaiya/ktrain">ktrain</a></b> (ğŸ¥‰26 Â·  â­ 1.3K Â· ğŸ’€) - ktrain is a Python library that makes deep learning and AI.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/itdxer/neupy">NeuPy</a></b> (ğŸ¥‰25 Â·  â­ 740 Â· ğŸ’€) - NeuPy is a Tensorflow based python library for prototyping and building.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/aksnzhy/xlearn">xLearn</a></b> (ğŸ¥‰24 Â·  â­ 3.1K Â· ğŸ’€) - High performance, easy-to-use, and scalable machine learning (ML).. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n- <b><a href="https://github.com/georgia-tech-db/evadb">EvaDB</a></b> (ğŸ¥‰24 Â·  â­ 2.7K Â· ğŸ’€) - Database system for AI-powered apps. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/NervanaSystems/neon">neon</a></b> (ğŸ¥‰22 Â·  â­ 3.9K Â· ğŸ’€) - Intel Nervana reference deep learning framework committed to best.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n- <b><a href="https://github.com/Xtra-Computing/thundersvm">ThunderSVM</a></b> (ğŸ¥‰22 Â·  â­ 1.6K Â· ğŸ’€) - ThunderSVM: A Fast SVM Library on GPUs and CPUs. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n- <b><a href="https://github.com/pytorchbearer/torchbearer">Torchbearer</a></b> (ğŸ¥‰22 Â·  â­ 640 Â· ğŸ’€) - torchbearer: A model fitting library for PyTorch. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/XiaoMi/mace">mace</a></b> (ğŸ¥‰21 Â·  â­ 5K Â· ğŸ’€) - MACE is a deep learning inference framework optimized for mobile.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n- <b><a href="https://github.com/google/neural-tangents">Neural Tangents</a></b> (ğŸ¥‰21 Â·  â­ 2.4K Â· ğŸ’€) - Fast and Easy Infinite Neural Networks in Python. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n- <b><a href="https://github.com/google/objax">Objax</a></b> (ğŸ¥‰20 Â·  â­ 770 Â· ğŸ’€) - Objax is a machine learning framework that provides an Object.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://jax.readthedocs.io/en/latest/_static/favicon.png" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/poets-ai/elegy">elegy</a></b> (ğŸ¥‰19 Â·  â­ 480 Â· ğŸ’€) - A High Level API for Deep Learning in JAX. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code> <code><img src="https://jax.readthedocs.io/en/latest/_static/favicon.png" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/facebookresearch/StarSpace">StarSpace</a></b> (ğŸ¥‰16 Â·  â­ 4K Â· ğŸ’€) - Learning embeddings for classification, retrieval and ranking. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/HenryNdubuaku/nanodl">nanodl</a></b> (ğŸ¥‰14 Â·  â­ 300 Â· ğŸ’€) - A Jax-based library for building transformers, includes.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://jax.readthedocs.io/en/latest/_static/favicon.png" style="display:inline;" width="13" height="13"></code>\n</details>\n<br>\n\n## Data Visualization\n\n<a href="#contents"><img align="right" width="15" height="15" src="https://git.io/JtehR" alt="Back to top"></a>\n\n_General-purpose and task-specific data visualization libraries._\n\n<details><summary><b><a href="https://github.com/matplotlib/matplotlib">Matplotlib</a></b> (ğŸ¥‡49 Â·  â­ 22K) - matplotlib: plotting with Python. <code>â—Unlicensed</code></summary>\n\n- [GitHub](https://github.com/matplotlib/matplotlib) (ğŸ‘¨â€ğŸ’» 1.9K Â· ğŸ”€ 8.1K Â· ğŸ“¦ 1.9M Â· ğŸ“‹ 11K - 14% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/matplotlib/matplotlib\n	```\n- [PyPi](https://pypi.org/project/matplotlib) (ğŸ“¥ 120M / month Â· ğŸ“¦ 68K Â· â±ï¸ 09.10.2025):\n	```\n	pip install matplotlib\n	```\n- [Conda](https://anaconda.org/conda-forge/matplotlib) (ğŸ“¥ 33M Â· â±ï¸ 15.10.2025):\n	```\n	conda install -c conda-forge matplotlib\n	```\n</details>\n<details><summary><b><a href="https://github.com/plotly/plotly.py">Plotly</a></b> (ğŸ¥‡47 Â·  â­ 18K) - The interactive graphing library for Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/plotly/plotly.py) (ğŸ‘¨â€ğŸ’» 300 Â· ğŸ”€ 2.7K Â· ğŸ“¥ 550 Â· ğŸ“¦ 460K Â· ğŸ“‹ 3.3K - 21% open Â· â±ï¸ 28.10.2025):\n\n	```\n	git clone https://github.com/plotly/plotly.py\n	```\n- [PyPi](https://pypi.org/project/plotly) (ğŸ“¥ 37M / month Â· ğŸ“¦ 9.7K Â· â±ï¸ 02.10.2025):\n	```\n	pip install plotly\n	```\n- [Conda](https://anaconda.org/conda-forge/plotly) (ğŸ“¥ 12M Â· â±ï¸ 03.10.2025):\n	```\n	conda install -c conda-forge plotly\n	```\n- [npm](https://www.npmjs.com/package/plotlywidget) (ğŸ“¥ 2.8K / month Â· ğŸ“¦ 9 Â· â±ï¸ 12.01.2021):\n	```\n	npm install plotlywidget\n	```\n</details>\n<details><summary><b><a href="https://github.com/plotly/dash">dash</a></b> (ğŸ¥‡45 Â·  â­ 24K) - Data Apps & Dashboards for Python. No JavaScript Required. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/plotly/dash) (ğŸ‘¨â€ğŸ’» 190 Â· ğŸ”€ 2.2K Â· ğŸ“¥ 120 Â· ğŸ“¦ 89K Â· ğŸ“‹ 2.1K - 27% open Â· â±ï¸ 21.10.2025):\n\n	```\n	git clone https://github.com/plotly/dash\n	```\n- [PyPi](https://pypi.org/project/dash) (ğŸ“¥ 5.5M / month Â· ğŸ“¦ 1.9K Â· â±ï¸ 22.10.2025):\n	```\n	pip install dash\n	```\n- [Conda](https://anaconda.org/conda-forge/dash) (ğŸ“¥ 2.1M Â· â±ï¸ 11.08.2025):\n	```\n	conda install -c conda-forge dash\n	```\n</details>\n<details><summary><b><a href="https://github.com/bokeh/bokeh">Bokeh</a></b> (ğŸ¥‡45 Â·  â­ 20K) - Interactive Data Visualization in the browser, from Python. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/bokeh/bokeh) (ğŸ‘¨â€ğŸ’» 720 Â· ğŸ”€ 4.2K Â· ğŸ“¦ 100K Â· ğŸ“‹ 8.1K - 10% open Â· â±ï¸ 28.10.2025):\n\n	```\n	git clone https://github.com/bokeh/bokeh\n	```\n- [PyPi](https://pypi.org/project/bokeh) (ğŸ“¥ 5M / month Â· ğŸ“¦ 2.2K Â· â±ï¸ 13.10.2025):\n	```\n	pip install bokeh\n	```\n- [Conda](https://anaconda.org/conda-forge/bokeh) (ğŸ“¥ 18M Â· â±ï¸ 30.08.2025):\n	```\n	conda install -c conda-forge bokeh\n	```\n</details>\n<details><summary><b><a href="https://github.com/mwaskom/seaborn">Seaborn</a></b> (ğŸ¥‡42 Â·  â­ 14K) - Statistical data visualization in Python. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/mwaskom/seaborn) (ğŸ‘¨â€ğŸ’» 220 Â· ğŸ”€ 2K Â· ğŸ“¥ 510 Â· ğŸ“¦ 700K Â· ğŸ“‹ 2.6K - 6% open Â· â±ï¸ 10.07.2025):\n\n	```\n	git clone https://github.com/mwaskom/seaborn\n	```\n- [PyPi](https://pypi.org/project/seaborn) (ğŸ“¥ 31M / month Â· ğŸ“¦ 11K Â· â±ï¸ 25.01.2024):\n	```\n	pip install seaborn\n	```\n- [Conda](https://anaconda.org/conda-forge/seaborn) (ğŸ“¥ 15M Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge seaborn\n	```\n</details>\n<details><summary><b><a href="https://github.com/vega/altair">Altair</a></b> (ğŸ¥‡41 Â·  â­ 10K) - Declarative visualization library for Python. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/vega/altair) (ğŸ‘¨â€ğŸ’» 180 Â· ğŸ”€ 800 Â· ğŸ“¥ 280 Â· ğŸ“¦ 240K Â· ğŸ“‹ 2.1K - 6% open Â· â±ï¸ 27.10.2025):\n\n	```\n	git clone https://github.com/altair-viz/altair\n	```\n- [PyPi](https://pypi.org/project/altair) (ğŸ“¥ 37M / month Â· ğŸ“¦ 920 Â· â±ï¸ 23.11.2024):\n	```\n	pip install altair\n	```\n- [Conda](https://anaconda.org/conda-forge/altair) (ğŸ“¥ 3M Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge altair\n	```\n</details>\n<details><summary><b><a href="https://github.com/voxel51/fiftyone">FiftyOne</a></b> (ğŸ¥ˆ39 Â·  â­ 10K) - Visualize, create, and debug image and video datasets.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/voxel51/fiftyone) (ğŸ‘¨â€ğŸ’» 160 Â· ğŸ”€ 680 Â· ğŸ“¦ 1K Â· ğŸ“‹ 1.8K - 35% open Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/voxel51/fiftyone\n	```\n- [PyPi](https://pypi.org/project/fiftyone) (ğŸ“¥ 170K / month Â· ğŸ“¦ 36 Â· â±ï¸ 20.10.2025):\n	```\n	pip install fiftyone\n	```\n</details>\n<details><summary><b><a href="https://github.com/xflr6/graphviz">Graphviz</a></b> (ğŸ¥ˆ39 Â·  â­ 1.8K) - Simple Python interface for Graphviz. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/xflr6/graphviz) (ğŸ‘¨â€ğŸ’» 24 Â· ğŸ”€ 220 Â· ğŸ“¦ 95K Â· ğŸ“‹ 190 - 6% open Â· â±ï¸ 26.10.2025):\n\n	```\n	git clone https://github.com/xflr6/graphviz\n	```\n- [PyPi](https://pypi.org/project/graphviz) (ğŸ“¥ 26M / month Â· ğŸ“¦ 3.2K Â· â±ï¸ 15.06.2025):\n	```\n	pip install graphviz\n	```\n- [Conda](https://anaconda.org/anaconda/python-graphviz) (ğŸ“¥ 59K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c anaconda python-graphviz\n	```\n</details>\n<details><summary><b><a href="https://github.com/pyvista/pyvista">PyVista</a></b> (ğŸ¥ˆ38 Â·  â­ 3.3K) - 3D plotting and mesh analysis through a streamlined interface for.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/pyvista/pyvista) (ğŸ‘¨â€ğŸ’» 190 Â· ğŸ”€ 590 Â· ğŸ“¥ 960 Â· ğŸ“¦ 5.2K Â· ğŸ“‹ 2K - 35% open Â· â±ï¸ 28.10.2025):\n\n	```\n	git clone https://github.com/pyvista/pyvista\n	```\n- [PyPi](https://pypi.org/project/pyvista) (ğŸ“¥ 1M / month Â· ğŸ“¦ 820 Â· â±ï¸ 26.08.2025):\n	```\n	pip install pyvista\n	```\n- [Conda](https://anaconda.org/conda-forge/pyvista) (ğŸ“¥ 810K Â· â±ï¸ 10.10.2025):\n	```\n	conda install -c conda-forge pyvista\n	```\n</details>\n<details><summary><b><a href="https://github.com/holoviz/holoviews">HoloViews</a></b> (ğŸ¥ˆ38 Â·  â­ 2.8K) - With Holoviews, your data visualizes itself. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/holoviz/holoviews) (ğŸ‘¨â€ğŸ’» 150 Â· ğŸ”€ 410 Â· ğŸ“¦ 17K Â· ğŸ“‹ 3.4K - 31% open Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/holoviz/holoviews\n	```\n- [PyPi](https://pypi.org/project/holoviews) (ğŸ“¥ 820K / month Â· ğŸ“¦ 490 Â· â±ï¸ 29.10.2025):\n	```\n	pip install holoviews\n	```\n- [Conda](https://anaconda.org/conda-forge/holoviews) (ğŸ“¥ 2.4M Â· â±ï¸ 25.06.2025):\n	```\n	conda install -c conda-forge holoviews\n	```\n- [npm](https://www.npmjs.com/package/@pyviz/jupyterlab_pyviz) (ğŸ“¥ 380 / month Â· ğŸ“¦ 7 Â· â±ï¸ 20.06.2025):\n	```\n	npm install @pyviz/jupyterlab_pyviz\n	```\n</details>\n<details><summary><b><a href="https://github.com/pyecharts/pyecharts">pyecharts</a></b> (ğŸ¥ˆ37 Â·  â­ 16K) - Python Echarts Plotting Library. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/pyecharts/pyecharts) (ğŸ‘¨â€ğŸ’» 45 Â· ğŸ”€ 2.9K Â· ğŸ“¥ 75 Â· ğŸ“¦ 5.5K Â· ğŸ“‹ 1.9K - 0% open Â· â±ï¸ 10.10.2025):\n\n	```\n	git clone https://github.com/pyecharts/pyecharts\n	```\n- [PyPi](https://pypi.org/project/pyecharts) (ğŸ“¥ 530K / month Â· ğŸ“¦ 280 Â· â±ï¸ 10.10.2025):\n	```\n	pip install pyecharts\n	```\n</details>\n<details><summary><b><a href="https://github.com/pyqtgraph/pyqtgraph">PyQtGraph</a></b> (ğŸ¥ˆ37 Â·  â­ 4.2K) - Fast data visualization and GUI tools for scientific / engineering.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/pyqtgraph/pyqtgraph) (ğŸ‘¨â€ğŸ’» 300 Â· ğŸ”€ 1.1K Â· ğŸ“¦ 13K Â· ğŸ“‹ 1.4K - 31% open Â· â±ï¸ 02.10.2025):\n\n	```\n	git clone https://github.com/pyqtgraph/pyqtgraph\n	```\n- [PyPi](https://pypi.org/project/pyqtgraph) (ğŸ“¥ 560K / month Â· ğŸ“¦ 1K Â· â±ï¸ 29.04.2024):\n	```\n	pip install pyqtgraph\n	```\n- [Conda](https://anaconda.org/conda-forge/pyqtgraph) (ğŸ“¥ 880K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge pyqtgraph\n	```\n</details>\n<details><summary><b><a href="https://github.com/ydataai/ydata-profiling">pandas-profiling</a></b> (ğŸ¥ˆ35 Â·  â­ 13K) - 1 Line of code data quality profiling & exploratory.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1S" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/ydataai/ydata-profiling) (ğŸ‘¨â€ğŸ’» 140 Â· ğŸ”€ 1.7K Â· ğŸ“¥ 490 Â· ğŸ“¦ 6.9K Â· ğŸ“‹ 850 - 30% open Â· â±ï¸ 19.09.2025):\n\n	```\n	git clone https://github.com/ydataai/pandas-profiling\n	```\n- [PyPi](https://pypi.org/project/pandas-profiling) (ğŸ“¥ 330K / month Â· ğŸ“¦ 180 Â· â±ï¸ 03.02.2023):\n	```\n	pip install pandas-profiling\n	```\n- [Conda](https://anaconda.org/conda-forge/pandas-profiling) (ğŸ“¥ 590K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge pandas-profiling\n	```\n</details>\n<details><summary><b><a href="https://github.com/has2k1/plotnine">plotnine</a></b> (ğŸ¥ˆ35 Â·  â­ 4.4K) - A Grammar of Graphics for Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/has2k1/plotnine) (ğŸ‘¨â€ğŸ’» 110 Â· ğŸ”€ 240 Â· ğŸ“¦ 13K Â· ğŸ“‹ 750 - 10% open Â· â±ï¸ 16.10.2025):\n\n	```\n	git clone https://github.com/has2k1/plotnine\n	```\n- [PyPi](https://pypi.org/project/plotnine) (ğŸ“¥ 2.2M / month Â· ğŸ“¦ 400 Â· â±ï¸ 15.07.2025):\n	```\n	pip install plotnine\n	```\n- [Conda](https://anaconda.org/conda-forge/plotnine) (ğŸ“¥ 560K Â· â±ï¸ 15.07.2025):\n	```\n	conda install -c conda-forge plotnine\n	```\n</details>\n<details><summary><b><a href="https://github.com/SciTools/cartopy">cartopy</a></b> (ğŸ¥ˆ35 Â·  â­ 1.5K) - Cartopy - a cartographic python library with matplotlib support. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/SciTools/cartopy) (ğŸ‘¨â€ğŸ’» 140 Â· ğŸ”€ 390 Â· ğŸ“¦ 8.1K Â· ğŸ“‹ 1.3K - 23% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/SciTools/cartopy\n	```\n- [PyPi](https://pypi.org/project/cartopy) (ğŸ“¥ 810K / month Â· ğŸ“¦ 970 Â· â±ï¸ 01.08.2025):\n	```\n	pip install cartopy\n	```\n- [Conda](https://anaconda.org/conda-forge/cartopy) (ğŸ“¥ 5.6M Â· â±ï¸ 27.10.2025):\n	```\n	conda install -c conda-forge cartopy\n	```\n</details>\n<details><summary><b><a href="https://github.com/vispy/vispy">VisPy</a></b> (ğŸ¥ˆ34 Â·  â­ 3.5K Â· ğŸ“‰) - High-performance interactive 2D/3D data visualization library. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/vispy/vispy) (ğŸ‘¨â€ğŸ’» 210 Â· ğŸ”€ 620 Â· ğŸ“¦ 2.1K Â· ğŸ“‹ 1.5K - 25% open Â· â±ï¸ 13.10.2025):\n\n	```\n	git clone https://github.com/vispy/vispy\n	```\n- [PyPi](https://pypi.org/project/vispy) (ğŸ“¥ 190K / month Â· ğŸ“¦ 200 Â· â±ï¸ 19.05.2025):\n	```\n	pip install vispy\n	```\n- [Conda](https://anaconda.org/conda-forge/vispy) (ğŸ“¥ 980K Â· â±ï¸ 30.08.2025):\n	```\n	conda install -c conda-forge vispy\n	```\n- [npm](https://www.npmjs.com/package/vispy) (ğŸ“¥ 12 / month Â· ğŸ“¦ 3 Â· â±ï¸ 15.03.2020):\n	```\n	npm install vispy\n	```\n</details>\n<details><summary><b><a href="https://github.com/holoviz/datashader">datashader</a></b> (ğŸ¥ˆ34 Â·  â­ 3.5K) - Quickly and accurately render even the largest data. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/holoviz/datashader) (ğŸ‘¨â€ğŸ’» 63 Â· ğŸ”€ 380 Â· ğŸ“¦ 6.3K Â· ğŸ“‹ 620 - 24% open Â· â±ï¸ 09.10.2025):\n\n	```\n	git clone https://github.com/holoviz/datashader\n	```\n- [PyPi](https://pypi.org/project/datashader) (ğŸ“¥ 280K / month Â· ğŸ“¦ 250 Â· â±ï¸ 05.08.2025):\n	```\n	pip install datashader\n	```\n- [Conda](https://anaconda.org/conda-forge/datashader) (ğŸ“¥ 1.6M Â· â±ï¸ 05.08.2025):\n	```\n	conda install -c conda-forge datashader\n	```\n</details>\n<details><summary><b><a href="https://github.com/JetBrains/lets-plot">lets-plot</a></b> (ğŸ¥ˆ34 Â·  â­ 1.7K) - Multiplatform plotting library based on the Grammar of Graphics. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/JetBrains/lets-plot) (ğŸ‘¨â€ğŸ’» 21 Â· ğŸ”€ 54 Â· ğŸ“¥ 3.4K Â· ğŸ“¦ 190 Â· ğŸ“‹ 740 - 21% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/JetBrains/lets-plot\n	```\n- [PyPi](https://pypi.org/project/lets-plot) (ğŸ“¥ 120K / month Â· ğŸ“¦ 16 Â· â±ï¸ 12.09.2025):\n	```\n	pip install lets-plot\n	```\n</details>\n<details><summary><b><a href="https://github.com/amueller/word_cloud">wordcloud</a></b> (ğŸ¥ˆ33 Â·  â­ 10K) - A little word cloud generator in Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/amueller/word_cloud) (ğŸ‘¨â€ğŸ’» 75 Â· ğŸ”€ 2.3K Â· ğŸ“¦ 21 Â· ğŸ“‹ 560 - 24% open Â· â±ï¸ 31.08.2025):\n\n	```\n	git clone https://github.com/amueller/word_cloud\n	```\n- [PyPi](https://pypi.org/project/wordcloud) (ğŸ“¥ 2M / month Â· ğŸ“¦ 550 Â· â±ï¸ 10.11.2024):\n	```\n	pip install wordcloud\n	```\n- [Conda](https://anaconda.org/conda-forge/wordcloud) (ğŸ“¥ 790K Â· â±ï¸ 03.09.2025):\n	```\n	conda install -c conda-forge wordcloud\n	```\n</details>\n<details><summary><b><a href="https://github.com/perspective-dev/perspective">Perspective</a></b> (ğŸ¥ˆ33 Â·  â­ 9.5K) - A data visualization and analytics component, especially.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/perspective-dev/perspective) (ğŸ‘¨â€ğŸ’» 100 Â· ğŸ”€ 1.2K Â· ğŸ“¥ 12K Â· ğŸ“¦ 190 Â· ğŸ“‹ 890 - 12% open Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/finos/perspective\n	```\n- [PyPi](https://pypi.org/project/perspective-python) (ğŸ“¥ 17K / month Â· ğŸ“¦ 31 Â· â±ï¸ 28.10.2025):\n	```\n	pip install perspective-python\n	```\n- [Conda](https://anaconda.org/conda-forge/perspective) (ğŸ“¥ 2.4M Â· â±ï¸ 28.10.2025):\n	```\n	conda install -c conda-forge perspective\n	```\n- [npm](https://www.npmjs.com/package/@finos/perspective-jupyterlab) (ğŸ“¥ 600 / month Â· ğŸ“¦ 6 Â· â±ï¸ 03.09.2025):\n	```\n	npm install @finos/perspective-jupyterlab\n	```\n</details>\n<details><summary><b><a href="https://github.com/lmcinnes/umap">UMAP</a></b> (ğŸ¥ˆ33 Â·  â­ 8K) - Uniform Manifold Approximation and Projection. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/lmcinnes/umap) (ğŸ‘¨â€ğŸ’» 140 Â· ğŸ”€ 850 Â· ğŸ“¦ 1 Â· ğŸ“‹ 860 - 59% open Â· â±ï¸ 26.10.2025):\n\n	```\n	git clone https://github.com/lmcinnes/umap\n	```\n- [PyPi](https://pypi.org/project/umap-learn) (ğŸ“¥ 2.7M / month Â· ğŸ“¦ 1.3K Â· â±ï¸ 03.07.2025):\n	```\n	pip install umap-learn\n	```\n- [Conda](https://anaconda.org/conda-forge/umap-learn) (ğŸ“¥ 3.2M Â· â±ï¸ 03.07.2025):\n	```\n	conda install -c conda-forge umap-learn\n	```\n</details>\n<details><summary><b><a href="https://github.com/holoviz/hvplot">hvPlot</a></b> (ğŸ¥ˆ32 Â·  â­ 1.3K) - A high-level plotting API for pandas, dask, xarray, and networkx built.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/holoviz/hvplot) (ğŸ‘¨â€ğŸ’» 52 Â· ğŸ”€ 110 Â· ğŸ“¦ 7.3K Â· ğŸ“‹ 940 - 41% open Â· â±ï¸ 24.10.2025):\n\n	```\n	git clone https://github.com/holoviz/hvplot\n	```\n- [PyPi](https://pypi.org/project/hvplot) (ğŸ“¥ 310K / month Â· ğŸ“¦ 270 Â· â±ï¸ 29.08.2025):\n	```\n	pip install hvplot\n	```\n- [Conda](https://anaconda.org/conda-forge/hvplot) (ğŸ“¥ 860K Â· â±ï¸ 04.09.2025):\n	```\n	conda install -c conda-forge hvplot\n	```\n</details>\n<details><summary><b><a href="https://github.com/mpld3/mpld3">mpld3</a></b> (ğŸ¥‰31 Â·  â­ 2.4K Â· ğŸ“‰) - An interactive data visualization tool which brings matplotlib.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/mpld3/mpld3) (ğŸ‘¨â€ğŸ’» 54 Â· ğŸ”€ 360 Â· ğŸ“¦ 7.6K Â· ğŸ“‹ 370 - 59% open Â· â±ï¸ 27.07.2025):\n\n	```\n	git clone https://github.com/mpld3/mpld3\n	```\n- [PyPi](https://pypi.org/project/mpld3) (ğŸ“¥ 440K / month Â· ğŸ“¦ 160 Â· â±ï¸ 27.07.2025):\n	```\n	pip install mpld3\n	```\n- [Conda](https://anaconda.org/conda-forge/mpld3) (ğŸ“¥ 280K Â· â±ï¸ 28.07.2025):\n	```\n	conda install -c conda-forge mpld3\n	```\n- [npm](https://www.npmjs.com/package/mpld3) (ğŸ“¥ 900 / month Â· ğŸ“¦ 11 Â· â±ï¸ 27.07.2025):\n	```\n	npm install mpld3\n	```\n</details>\n<details><summary><b><a href="https://github.com/bqplot/bqplot">bqplot</a></b> (ğŸ¥‰30 Â·  â­ 3.7K) - Plotting library for IPython/Jupyter notebooks. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/bqplot/bqplot) (ğŸ‘¨â€ğŸ’» 66 Â· ğŸ”€ 480 Â· ğŸ“¦ 62 Â· ğŸ“‹ 650 - 42% open Â· â±ï¸ 25.08.2025):\n\n	```\n	git clone https://github.com/bqplot/bqplot\n	```\n- [PyPi](https://pypi.org/project/bqplot) (ğŸ“¥ 230K / month Â· ğŸ“¦ 110 Â· â±ï¸ 21.05.2025):\n	```\n	pip install bqplot\n	```\n- [Conda](https://anaconda.org/conda-forge/bqplot) (ğŸ“¥ 1.9M Â· â±ï¸ 02.09.2025):\n	```\n	conda install -c conda-forge bqplot\n	```\n- [npm](https://www.npmjs.com/package/bqplot) (ğŸ“¥ 3K / month Â· ğŸ“¦ 21 Â· â±ï¸ 03.09.2025):\n	```\n	npm install bqplot\n	```\n</details>\n<details><summary><b><a href="https://github.com/man-group/dtale">D-Tale</a></b> (ğŸ¥‰29 Â·  â­ 5K) - Visualizer for pandas data structures. <code><a href="https://tldrlegal.com/search?q=LGPL-2.1">â—ï¸LGPL-2.1</a></code> <code><img src="https://git.io/JLy1S" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/man-group/dtale) (ğŸ‘¨â€ğŸ’» 31 Â· ğŸ”€ 430 Â· ğŸ“¦ 1.5K Â· ğŸ“‹ 610 - 10% open Â· â±ï¸ 30.07.2025):\n\n	```\n	git clone https://github.com/man-group/dtale\n	```\n- [PyPi](https://pypi.org/project/dtale) (ğŸ“¥ 31K / month Â· ğŸ“¦ 53 Â· â±ï¸ 30.07.2025):\n	```\n	pip install dtale\n	```\n- [Conda](https://anaconda.org/conda-forge/dtale) (ğŸ“¥ 480K Â· â±ï¸ 30.07.2025):\n	```\n	conda install -c conda-forge dtale\n	```\n</details>\n<details><summary><b><a href="https://github.com/pavlin-policar/openTSNE">openTSNE</a></b> (ğŸ¥‰29 Â·  â­ 1.6K Â· ğŸ“ˆ) - Extensible, parallel implementations of t-SNE. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/pavlin-policar/openTSNE) (ğŸ‘¨â€ğŸ’» 14 Â· ğŸ”€ 170 Â· ğŸ“¦ 1.1K Â· ğŸ“‹ 150 - 2% open Â· â±ï¸ 27.10.2025):\n\n	```\n	git clone https://github.com/pavlin-policar/openTSNE\n	```\n- [PyPi](https://pypi.org/project/opentsne) (ğŸ“¥ 58K / month Â· ğŸ“¦ 69 Â· â±ï¸ 27.10.2025):\n	```\n	pip install opentsne\n	```\n- [Conda](https://anaconda.org/conda-forge/opentsne) (ğŸ“¥ 500K Â· â±ï¸ 27.10.2025):\n	```\n	conda install -c conda-forge opentsne\n	```\n</details>\n<details><summary><b><a href="https://github.com/predict-idlab/plotly-resampler">Plotly-Resampler</a></b> (ğŸ¥‰27 Â·  â­ 1.2K) - Visualize large time series data with plotly.py. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/predict-idlab/plotly-resampler) (ğŸ‘¨â€ğŸ’» 14 Â· ğŸ”€ 74 Â· ğŸ“¦ 2K Â· ğŸ“‹ 190 - 32% open Â· â±ï¸ 03.09.2025):\n\n	```\n	git clone https://github.com/predict-idlab/plotly-resampler\n	```\n- [PyPi](https://pypi.org/project/plotly-resampler) (ğŸ“¥ 370K / month Â· ğŸ“¦ 38 Â· â±ï¸ 29.08.2025):\n	```\n	pip install plotly-resampler\n	```\n- [Conda](https://anaconda.org/conda-forge/plotly-resampler) (ğŸ“¥ 140K Â· â±ï¸ 09.10.2025):\n	```\n	conda install -c conda-forge plotly-resampler\n	```\n</details>\n<details><summary><b><a href="https://github.com/ContextLab/hypertools">HyperTools</a></b> (ğŸ¥‰26 Â·  â­ 1.9K) - A Python toolbox for gaining geometric insights into high-dimensional.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/ContextLab/hypertools) (ğŸ‘¨â€ğŸ’» 23 Â· ğŸ”€ 160 Â· ğŸ“¥ 73 Â· ğŸ“¦ 510 Â· ğŸ“‹ 200 - 34% open Â· â±ï¸ 10.07.2025):\n\n	```\n	git clone https://github.com/ContextLab/hypertools\n	```\n- [PyPi](https://pypi.org/project/hypertools) (ğŸ“¥ 1.1K / month Â· ğŸ“¦ 2 Â· â±ï¸ 09.07.2025):\n	```\n	pip install hypertools\n	```\n</details>\n<details><summary><b><a href="https://github.com/tensorflow/data-validation">data-validation</a></b> (ğŸ¥‰25 Â·  â­ 780) - Library for exploring and validating machine learning.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/tensorflow/data-validation) (ğŸ‘¨â€ğŸ’» 30 Â· ğŸ”€ 180 Â· ğŸ“¥ 1K Â· ğŸ“‹ 190 - 20% open Â· â±ï¸ 23.06.2025):\n\n	```\n	git clone https://github.com/tensorflow/data-validation\n	```\n- [PyPi](https://pypi.org/project/tensorflow-data-validation) (ğŸ“¥ 150K / month Â· ğŸ“¦ 32 Â· â±ï¸ 09.06.2025):\n	```\n	pip install tensorflow-data-validation\n	```\n</details>\n<details><summary><b><a href="https://github.com/spotify/chartify">Chartify</a></b> (ğŸ¥‰24 Â·  â­ 3.6K Â· ğŸ’¤) - Python library that makes it easy for data scientists to create.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/spotify/chartify) (ğŸ‘¨â€ğŸ’» 27 Â· ğŸ”€ 340 Â· ğŸ“¦ 83 Â· ğŸ“‹ 86 - 62% open Â· â±ï¸ 16.10.2024):\n\n	```\n	git clone https://github.com/spotify/chartify\n	```\n- [PyPi](https://pypi.org/project/chartify) (ğŸ“¥ 1.2K / month Â· ğŸ“¦ 9 Â· â±ï¸ 16.10.2024):\n	```\n	pip install chartify\n	```\n- [Conda](https://anaconda.org/conda-forge/chartify) (ğŸ“¥ 40K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge chartify\n	```\n</details>\n<details><summary><b><a href="https://github.com/ing-bank/popmon">Popmon</a></b> (ğŸ¥‰22 Â·  â­ 510) - Monitor the stability of a Pandas or Spark dataframe. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1S" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1N" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/ing-bank/popmon) (ğŸ‘¨â€ğŸ’» 19 Â· ğŸ”€ 36 Â· ğŸ“¥ 280 Â· ğŸ“¦ 22 Â· ğŸ“‹ 57 - 28% open Â· â±ï¸ 04.09.2025):\n\n	```\n	git clone https://github.com/ing-bank/popmon\n	```\n- [PyPi](https://pypi.org/project/popmon) (ğŸ“¥ 3.4K / month Â· ğŸ“¦ 4 Â· â±ï¸ 04.09.2025):\n	```\n	pip install popmon\n	```\n</details>\n<details><summary><b><a href="https://github.com/vega/ipyvega">vega</a></b> (ğŸ¥‰22 Â·  â­ 390 Â· ğŸ’¤) - IPython/Jupyter notebook module for Vega and Vega-Lite. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/vega/ipyvega) (ğŸ‘¨â€ğŸ’» 15 Â· ğŸ”€ 65 Â· ğŸ“¦ 4 Â· ğŸ“‹ 110 - 14% open Â· â±ï¸ 01.01.2025):\n\n	```\n	git clone https://github.com/vega/ipyvega\n	```\n- [PyPi](https://pypi.org/project/vega) (ğŸ“¥ 26K / month Â· ğŸ“¦ 17 Â· â±ï¸ 25.09.2024):\n	```\n	pip install vega\n	```\n- [Conda](https://anaconda.org/conda-forge/vega) (ğŸ“¥ 940K Â· â±ï¸ 04.10.2025):\n	```\n	conda install -c conda-forge vega\n	```\n</details>\n<details><summary><b><a href="https://github.com/vega/vegafusion">vegafusion</a></b> (ğŸ¥‰21 Â·  â­ 390) - Serverside scaling for Vega and Altair visualizations. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/vega/vegafusion) (ğŸ‘¨â€ğŸ’» 6 Â· ğŸ”€ 26 Â· ğŸ“¥ 6.6K Â· ğŸ“‹ 150 - 36% open Â· â±ï¸ 29.09.2025):\n\n	```\n	git clone https://github.com/vegafusion/vegafusion\n	```\n- [PyPi](https://pypi.org/project/vegafusion-jupyter) (ğŸ“¥ 770 / month Â· ğŸ“¦ 2 Â· â±ï¸ 09.05.2024):\n	```\n	pip install vegafusion-jupyter\n	```\n- [Conda](https://anaconda.org/conda-forge/vegafusion-python-embed) (ğŸ“¥ 520K Â· â±ï¸ 27.10.2025):\n	```\n	conda install -c conda-forge vegafusion-python-embed\n	```\n- [npm](https://www.npmjs.com/package/vegafusion-jupyter) (ğŸ“¥ 1.9K / month Â· ğŸ“¦ 3 Â· â±ï¸ 09.05.2024):\n	```\n	npm install vegafusion-jupyter\n	```\n</details>\n<details><summary>Show 22 hidden projects...</summary>\n\n- <b><a href="https://github.com/ResidentMario/missingno">missingno</a></b> (ğŸ¥‰30 Â·  â­ 4.2K Â· ğŸ’€) - Missing data visualization module for Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/PAIR-code/facets">Facets Overview</a></b> (ğŸ¥‰28 Â·  â­ 7.4K Â· ğŸ’€) - Visualizations for machine learning datasets. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/santosjorge/cufflinks">Cufflinks</a></b> (ğŸ¥‰28 Â·  â­ 3.1K Â· ğŸ’€) - Productivity Tools for Plotly + Pandas. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1S" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/jupyter-widgets/pythreejs">pythreejs</a></b> (ğŸ¥‰27 Â·  â­ 980 Â· ğŸ’€) - A Jupyter - Three.js bridge. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/fbdesignpro/sweetviz">Sweetviz</a></b> (ğŸ¥‰26 Â·  â­ 3.1K Â· ğŸ’€) - Visualize and compare datasets, target values and associations, with.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/AutoViML/AutoViz">AutoViz</a></b> (ğŸ¥‰26 Â·  â­ 1.9K Â· ğŸ’€) - Automatically Visualize any dataset, any size with a single line.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n- <b><a href="https://github.com/tpvasconcelos/ridgeplot">ridgeplot</a></b> (ğŸ¥‰26 Â·  â­ 240) - Beautiful ridgeline plots in Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/adamerose/PandasGUI">PandasGUI</a></b> (ğŸ¥‰24 Â·  â­ 3.3K) - A GUI for Pandas DataFrames. <code><a href="https://tldrlegal.com/search?q=MIT-0">â—ï¸MIT-0</a></code> <code><img src="https://git.io/JLy1S" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/facebookresearch/hiplot">HiPlot</a></b> (ğŸ¥‰24 Â·  â­ 2.8K Â· ğŸ’€) - HiPlot makes understanding high dimensional data easy. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/marcharper/python-ternary">python-ternary</a></b> (ğŸ¥‰24 Â·  â­ 770 Â· ğŸ’€) - Ternary plotting library for python with matplotlib. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/DmitryUlyanov/Multicore-TSNE">Multicore-TSNE</a></b> (ğŸ¥‰23 Â·  â­ 1.9K Â· ğŸ’€) - Parallel t-SNE implementation with Python and Torch.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/PatrikHlobil/Pandas-Bokeh">Pandas-Bokeh</a></b> (ğŸ¥‰22 Â·  â­ 890 Â· ğŸ’€) - Bokeh Plotting Backend for Pandas and GeoPandas. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1S" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/nicolaskruchten/jupyter_pivottablejs">pivottablejs</a></b> (ğŸ¥‰21 Â·  â­ 710 Â· ğŸ’€) - Dragndrop Pivot Tables and Charts for Jupyter/IPython.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/leotac/joypy">joypy</a></b> (ğŸ¥‰21 Â·  â­ 600 Â· ğŸ’€) - Joyplots in Python with matplotlib & pandas. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/gyli/PyWaffle">PyWaffle</a></b> (ğŸ¥‰21 Â·  â­ 600 Â· ğŸ’€) - Make Waffle Charts in Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/sosuneko/PDPbox">PDPbox</a></b> (ğŸ¥‰20 Â·  â­ 860 Â· ğŸ’€) - python partial dependence plot toolbox. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/t-makaro/animatplot">animatplot</a></b> (ğŸ¥‰18 Â·  â­ 410 Â· ğŸ’€) - A python package for animating plots build on matplotlib. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/beringresearch/ivis">ivis</a></b> (ğŸ¥‰18 Â·  â­ 340 Â· ğŸ’€) - Dimensionality reduction in very large datasets using Siamese.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/altair-viz/pdvega">pdvega</a></b> (ğŸ¥‰16 Â·  â­ 340 Â· ğŸ’€) - Interactive plotting for Pandas using Vega-Lite. <code><a href="http://bit.ly/34MBwT8">MIT</a></code>\n- <b><a href="https://github.com/Zsailer/nx_altair">nx-altair</a></b> (ğŸ¥‰16 Â·  â­ 230 Â· ğŸ’€) - Draw interactive NetworkX graphs with Altair. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1E" style="display:inline;" width="13" height="13"></code>\n- <b><a href="https://github.com/data-describe/data-describe">data-describe</a></b> (ğŸ¥‰15 Â·  â­ 300 Â· ğŸ’€) - datadescribe: Pythonic EDA Accelerator for Data Science. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n- <b><a href="https://github.com/biovault/nptsne">nptsne</a></b> (ğŸ¥‰11 Â·  â­ 33 Â· ğŸ’€) - nptsne is a numpy compatible python binary package that offers a.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code>\n</details>\n<br>\n\n## Text Data & NLP\n\n<a href="#contents"><img align="right" width="15" height="15" src="https://git.io/JtehR" alt="Back to top"></a>\n\n_Libraries for processing, cleaning, manipulating, and analyzing text data as well as libraries for NLP tasks such as language detection, fuzzy matching, classification, seq2seq learning, conversational AI, keyword extraction, and translation._\n\n<details><summary><b><a href="https://github.com/huggingface/transformers">transformers</a></b> (ğŸ¥‡54 Â·  â­ 150K) - Transformers: the model-definition framework for.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/huggingface/transformers) (ğŸ‘¨â€ğŸ’» 3.6K Â· ğŸ”€ 31K Â· ğŸ“¦ 400K Â· ğŸ“‹ 19K - 11% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/huggingface/transformers\n	```\n- [PyPi](https://pypi.org/project/transformers) (ğŸ“¥ 93M / month Â· ğŸ“¦ 11K Â· â±ï¸ 14.10.2025):\n	```\n	pip install transformers\n	```\n- [Conda](https://anaconda.org/conda-forge/transformers) (ğŸ“¥ 3.3M Â· â±ï¸ 14.10.2025):\n	```\n	conda install -c conda-forge transformers\n	```\n</details>\n<details><summary><b><a href="https://github.com/nltk/nltk">nltk</a></b> (ğŸ¥‡47 Â·  â­ 14K) - Suite of libraries and programs for symbolic and statistical natural.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/nltk/nltk) (ğŸ‘¨â€ğŸ’» 480 Â· ğŸ”€ 3K Â· ğŸ“¦ 410K Â· ğŸ“‹ 1.9K - 14% open Â· â±ï¸ 22.10.2025):\n\n	```\n	git clone https://github.com/nltk/nltk\n	```\n- [PyPi](https://pypi.org/project/nltk) (ğŸ“¥ 42M / month Â· ğŸ“¦ 6.3K Â· â±ï¸ 01.10.2025):\n	```\n	pip install nltk\n	```\n- [Conda](https://anaconda.org/conda-forge/nltk) (ğŸ“¥ 3.4M Â· â±ï¸ 01.10.2025):\n	```\n	conda install -c conda-forge nltk\n	```\n</details>\n<details><summary><b><a href="https://github.com/BerriAI/litellm">litellm</a></b> (ğŸ¥‡45 Â·  â­ 30K Â· ğŸ“‰) - Python SDK, Proxy Server (LLM Gateway) to call 100+.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code>o</code> <code>t</code> <code>h</code> <code>e</code> <code>r</code> <code>s</code></summary>\n\n- [GitHub](https://github.com/BerriAI/litellm) (ğŸ‘¨â€ğŸ’» 960 Â· ğŸ”€ 4.5K Â· ğŸ“¥ 800 Â· ğŸ“¦ 17K Â· ğŸ“‹ 7.8K - 17% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/BerriAI/litellm\n	```\n- [PyPi](https://pypi.org/project/litellm) (ğŸ“¥ 34M / month Â· ğŸ“¦ 1.9K Â· â±ï¸ 29.10.2025):\n	```\n	pip install litellm\n	```\n</details>\n<details><summary><b><a href="https://github.com/explosion/spaCy">spaCy</a></b> (ğŸ¥‡43 Â·  â­ 33K Â· ğŸ“ˆ) - Industrial-strength Natural Language Processing (NLP) in Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/explosion/spaCy) (ğŸ‘¨â€ğŸ’» 780 Â· ğŸ”€ 4.5K Â· ğŸ“¥ 4.9K Â· ğŸ“¦ 140K Â· ğŸ“‹ 5.8K - 3% open Â· â±ï¸ 28.10.2025):\n\n	```\n	git clone https://github.com/explosion/spaCy\n	```\n- [PyPi](https://pypi.org/project/spacy) (ğŸ“¥ 17M / month Â· ğŸ“¦ 3.2K Â· â±ï¸ 23.05.2025):\n	```\n	pip install spacy\n	```\n- [Conda](https://anaconda.org/conda-forge/spacy) (ğŸ“¥ 6.5M Â· â±ï¸ 06.07.2025):\n	```\n	conda install -c conda-forge spacy\n	```\n</details>\n<details><summary><b><a href="https://github.com/huggingface/sentence-transformers">sentence-transformers</a></b> (ğŸ¥‡42 Â·  â­ 18K) - State-of-the-Art Text Embeddings. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/huggingface/sentence-transformers) (ğŸ‘¨â€ğŸ’» 240 Â· ğŸ”€ 2.7K Â· ğŸ“¦ 120K Â· ğŸ“‹ 2.5K - 51% open Â· â±ï¸ 22.10.2025):\n\n	```\n	git clone https://github.com/UKPLab/sentence-transformers\n	```\n- [PyPi](https://pypi.org/project/sentence-transformers) (ğŸ“¥ 17M / month Â· ğŸ“¦ 3.7K Â· â±ï¸ 22.10.2025):\n	```\n	pip install sentence-transformers\n	```\n- [Conda](https://anaconda.org/conda-forge/sentence-transformers) (ğŸ“¥ 1M Â· â±ï¸ 22.10.2025):\n	```\n	conda install -c conda-forge sentence-transformers\n	```\n</details>\n<details><summary><b><a href="https://github.com/piskvorky/gensim">gensim</a></b> (ğŸ¥‡42 Â·  â­ 16K) - Topic Modelling for Humans. <code><a href="https://tldrlegal.com/search?q=LGPL-2.1">â—ï¸LGPL-2.1</a></code></summary>\n\n- [GitHub](https://github.com/piskvorky/gensim) (ğŸ‘¨â€ğŸ’» 460 Â· ğŸ”€ 4.4K Â· ğŸ“¥ 6.4K Â· ğŸ“¦ 78K Â· ğŸ“‹ 1.9K - 21% open Â· â±ï¸ 16.10.2025):\n\n	```\n	git clone https://github.com/RaRe-Technologies/gensim\n	```\n- [PyPi](https://pypi.org/project/gensim) (ğŸ“¥ 5.2M / month Â· ğŸ“¦ 1.6K Â· â±ï¸ 18.10.2025):\n	```\n	pip install gensim\n	```\n- [Conda](https://anaconda.org/conda-forge/gensim) (ğŸ“¥ 1.8M Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge gensim\n	```\n</details>\n<details><summary><b><a href="https://github.com/google/sentencepiece">sentencepiece</a></b> (ğŸ¥‡42 Â·  â­ 11K) - Unsupervised text tokenizer for Neural Network-based text.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/google/sentencepiece) (ğŸ‘¨â€ğŸ’» 100 Â· ğŸ”€ 1.3K Â· ğŸ“¥ 110K Â· ğŸ“¦ 120K Â· ğŸ“‹ 800 - 3% open Â· â±ï¸ 04.10.2025):\n\n	```\n	git clone https://github.com/google/sentencepiece\n	```\n- [PyPi](https://pypi.org/project/sentencepiece) (ğŸ“¥ 31M / month Â· ğŸ“¦ 2.4K Â· â±ï¸ 12.08.2025):\n	```\n	pip install sentencepiece\n	```\n- [Conda](https://anaconda.org/conda-forge/sentencepiece) (ğŸ“¥ 1.7M Â· â±ï¸ 22.09.2025):\n	```\n	conda install -c conda-forge sentencepiece\n	```\n</details>\n<details><summary><b><a href="https://github.com/huggingface/tokenizers">Tokenizers</a></b> (ğŸ¥‡40 Â·  â­ 10K) - Fast State-of-the-Art Tokenizers optimized for Research and.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/huggingface/tokenizers) (ğŸ‘¨â€ğŸ’» 130 Â· ğŸ”€ 970 Â· ğŸ“¥ 86 Â· ğŸ“¦ 180K Â· ğŸ“‹ 1.1K - 9% open Â· â±ï¸ 16.10.2025):\n\n	```\n	git clone https://github.com/huggingface/tokenizers\n	```\n- [PyPi](https://pypi.org/project/tokenizers) (ğŸ“¥ 81M / month Â· ğŸ“¦ 1.7K Â· â±ï¸ 19.09.2025):\n	```\n	pip install tokenizers\n	```\n- [Conda](https://anaconda.org/conda-forge/tokenizers) (ğŸ“¥ 3.6M Â· â±ï¸ 19.09.2025):\n	```\n	conda install -c conda-forge tokenizers\n	```\n</details>\n<details><summary><b><a href="https://github.com/NVIDIA-NeMo/NeMo">NeMo</a></b> (ğŸ¥‡38 Â·  â­ 16K) - A scalable generative AI framework built for researchers and.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/NVIDIA-NeMo/NeMo) (ğŸ‘¨â€ğŸ’» 460 Â· ğŸ”€ 3.2K Â· ğŸ“¥ 520K Â· ğŸ“¦ 21 Â· ğŸ“‹ 2.8K - 4% open Â· â±ï¸ 29.10.2025):\n\n	```\n	git clone https://github.com/NVIDIA/NeMo\n	```\n- [PyPi](https://pypi.org/project/nemo-toolkit) (ğŸ“¥ 810K / month Â· ğŸ“¦ 18 Â· â±ï¸ 27.10.2025):\n	```\n	pip install nemo-toolkit\n	```\n</details>\n<details><summary><b><a href="https://github.com/deepset-ai/haystack">haystack</a></b> (ğŸ¥‡37 Â·  â­ 23K) - AI orchestration framework to build customizable, production-ready.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/deepset-ai/haystack) (ğŸ‘¨â€ğŸ’» 310 Â· ğŸ”€ 2.5K Â· ğŸ“¦ 1.3K Â· ğŸ“‹ 4.1K - 2% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/deepset-ai/haystack\n	```\n- [PyPi](https://pypi.org/project/haystack) (ğŸ“¥ 7.4K / month Â· ğŸ“¦ 5 Â· â±ï¸ 15.12.2021):\n	```\n	pip install haystack\n	```\n</details>\n<details><summary><b><a href="https://github.com/comet-ml/opik">Opik</a></b> (ğŸ¥‡37 Â·  â­ 15K) - Debug, evaluate, and monitor your LLM applications, RAG systems, and.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/comet-ml/opik) (ğŸ‘¨â€ğŸ’» 81 Â· ğŸ”€ 1.1K Â· ğŸ“¦ 17 Â· ğŸ“‹ 540 - 29% open Â· â±ï¸ 30.10.2025):\n\n	```\n	git clone https://github.com/comet-ml/opik\n	```\n- [PyPi](https://pypi.org/project/opik) (ğŸ“¥ 850K / month Â· ğŸ“¦ 34 Â· â±ï¸ 29.10.2025):\n	```\n	pip install opik\n	```\n</details>\n<details><summary><b><a href="https://github.com/gunthercox/ChatterBot">ChatterBot</a></b> (ğŸ¥‡37 Â·  â­ 14K) - ChatterBot is a machine learning, conversational dialog engine for.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/gunthercox/ChatterBot) (ğŸ‘¨â€ğŸ’» 110 Â· ğŸ”€ 4.5K Â· ğŸ“¦ 6.5K Â· ğŸ“‹ 1.7K - 6% open Â· â±ï¸ 25.10.2025):\n\n	```\n	git clone https://github.com/gunthercox/ChatterBot\n	```\n- [PyPi](https://pypi.org/project/chatterbot) (ğŸ“¥ 20K / month Â· ğŸ“¦ 19 Â· â±ï¸ 16.10.2025):\n	```\n	pip install chatterbot\n	```\n</details>\n<details><summary><b><a href="https://github.com/flairNLP/flair">flair</a></b> (ğŸ¥‡37 Â·  â­ 14K) - A very simple framework for state-of-the-art Natural Language Processing.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/flairNLP/flair) (ğŸ‘¨â€ğŸ’» 280 Â· ğŸ”€ 2.1K Â· ğŸ“¦ 4.1K Â· ğŸ“‹ 2.4K - 1% open Â· â±ï¸ 12.06.2025):\n\n	```\n	git clone https://github.com/flairNLP/flair\n	```\n- [PyPi](https://pypi.org/project/flair) (ğŸ“¥ 180K / month Â· ğŸ“¦ 160 Â· â±ï¸ 05.02.2025):\n	```\n	pip install flair\n	```\n- [Conda](https://anaconda.org/conda-forge/python-flair) (ğŸ“¥ 49K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge python-flair\n	```\n</details>\n<details><summary><b><a href="https://github.com/sloria/TextBlob">TextBlob</a></b> (ğŸ¥‡37 Â·  â­ 9.5K) - Simple, Pythonic, text processing--Sentiment analysis, part-of-speech.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/sloria/TextBlob) (ğŸ‘¨â€ğŸ’» 37 Â· ğŸ”€ 1.2K Â· ğŸ“¥ 140 Â· ğŸ“¦ 60K Â· ğŸ“‹ 280 - 25% open Â· â±ï¸ 18.10.2025):\n\n	```\n	git clone https://github.com/sloria/TextBlob\n	```\n- [PyPi](https://pypi.org/project/textblob) (ğŸ“¥ 1.5M / month Â· ğŸ“¦ 400 Â· â±ï¸ 13.01.2025):\n	```\n	pip install textblob\n	```\n- [Conda](https://anaconda.org/conda-forge/textblob) (ğŸ“¥ 340K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge textblob\n	```\n</details>\n<details><summary><b><a href="https://github.com/facebookresearch/fairseq">fairseq</a></b> (ğŸ¥ˆ36 Â·  â­ 32K) - Facebook AI Research Sequence-to-Sequence Toolkit written in Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/facebookresearch/fairseq) (ğŸ‘¨â€ğŸ’» 430 Â· ğŸ”€ 6.6K Â· ğŸ“¥ 440 Â· ğŸ“¦ 4.4K Â· ğŸ“‹ 4.4K - 30% open Â· â±ï¸ 30.09.2025):\n\n	```\n	git clone https://github.com/facebookresearch/fairseq\n	```\n- [PyPi](https://pypi.org/project/fairseq) (ğŸ“¥ 77K / month Â· ğŸ“¦ 120 Â· â±ï¸ 27.06.2022):\n	```\n	pip install fairseq\n	```\n- [Conda](https://anaconda.org/conda-forge/fairseq) (ğŸ“¥ 170K Â· â±ï¸ 02.10.2025):\n	```\n	conda install -c conda-forge fairseq\n	```\n</details>\n<details><summary><b><a href="https://github.com/stanfordnlp/stanza">stanza</a></b> (ğŸ¥ˆ36 Â·  â­ 7.6K) - Stanford NLP Python library for tokenization, sentence segmentation,.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/stanfordnlp/stanza) (ğŸ‘¨â€ğŸ’» 72 Â· ğŸ”€ 920 Â· ğŸ“¦ 4.1K Â· ğŸ“‹ 950 - 10% open Â· â±ï¸ 05.10.2025):\n\n	```\n	git clone https://github.com/stanfordnlp/stanza\n	```\n- [PyPi](https://pypi.org/project/stanza) (ğŸ“¥ 770K / month Â· ğŸ“¦ 240 Â· â±ï¸ 05.10.2025):\n	```\n	pip install stanza\n	```\n- [Conda](https://anaconda.org/stanfordnlp/stanza) (ğŸ“¥ 9K Â· â±ï¸ 25.03.2025):\n	```\n	conda install -c stanfordnlp stanza\n	```\n</details>\n<details><summary><b><a href="https://github.com/qdrant/qdrant">qdrant</a></b> (ğŸ¥ˆ35 Â·  â­ 27K) - Qdrant - High-performance, massive-scale Vector Database and Vector.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/qdrant/qdrant) (ğŸ‘¨â€ğŸ’» 140 Â· ğŸ”€ 1.9K Â· ğŸ“¥ 500K Â· ğŸ“¦ 120 Â· ğŸ“‹ 1.6K - 22% open Â· â±ï¸ 30.09.2025):\n\n	```\n	git clone https://github.com/qdrant/qdrant\n	```\n</details>\n<details><summary><b><a href="https://github.com/JohnSnowLabs/spark-nlp">spark-nlp</a></b> (ğŸ¥ˆ35 Â·  â­ 4.1K) - State of the Art Natural Language Processing. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1N" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/JohnSnowLabs/spark-nlp) (ğŸ‘¨â€ğŸ’» 120 Â· ğŸ”€ 730 Â· ğŸ“¦ 620 Â· ğŸ“‹ 910 - 2% open Â· â±ï¸ 22.10.2025):\n\n	```\n	git clone https://github.com/JohnSnowLabs/spark-nlp\n	```\n- [PyPi](https://pypi.org/project/spark-nlp) (ğŸ“¥ 1M / month Â· ğŸ“¦ 39 Â· â±ï¸ 22.10.2025):\n	```\n	pip install spark-nlp\n	```\n</details>\n<details><summary><b><a href="https://github.com/RasaHQ/rasa">Rasa</a></b> (ğŸ¥ˆ34 Â·  â­ 21K) - Open source machine learning framework to automate text- and voice-.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/RasaHQ/rasa) (ğŸ‘¨â€ğŸ’» 600 Â· ğŸ”€ 4.9K Â· ğŸ“‹ 6.8K - 2% open Â· â±ï¸ 26.08.2025):\n\n	```\n	git clone https://github.com/RasaHQ/rasa\n	```\n- [PyPi](https://pypi.org/project/rasa) (ğŸ“¥ 110K / month Â· ğŸ“¦ 60 Â· â±ï¸ 14.01.2025):\n	```\n	pip install rasa\n	```\n</details>\n<details><summary><b><a href="https://github.com/tensorflow/text">TensorFlow Text</a></b> (ğŸ¥ˆ34 Â·  â­ 1.3K) - Making text a first-class citizen in TensorFlow. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/tensorflow/text) (ğŸ‘¨â€ğŸ’» 190 Â· ğŸ”€ 360 Â· ğŸ“¦ 10K Â· ğŸ“‹ 370 - 53% open Â· â±ï¸ 18.08.2025):\n\n	```\n	git clone https://github.com/tensorflow/text\n	```\n- [PyPi](https://pypi.org/project/tensorflow-text) (ğŸ“¥ 6.8M / month Â· ğŸ“¦ 230 Â· â±ï¸ 04.04.2025):\n	```\n	pip install tensorflow-text\n	```\n</details>\n<details><summary><b><a href="https://github.com/snowballstem/snowball">snowballstemmer</a></b> (ğŸ¥ˆ34 Â·  â­ 810) - Snowball compiler and stemming algorithms. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code></summary>\n\n- [GitHub](https://github.com/snowballstem/snowball) (ğŸ‘¨â€ğŸ’» 41 Â· ğŸ”€ 190 Â· ğŸ“¦ 11 Â· ğŸ“‹ 120 - 17% open Â· â±ï¸ 28.10.2025):\n\n	```\n	git clone https://github.com/snowballstem/snowball\n	```\n- [PyPi](https://pypi.org/project/snowballstemmer) (ğŸ“¥ 24M / month Â· ğŸ“¦ 550 Â· â±ï¸ 09.05.2025):\n	```\n	pip install snowballstemmer\n	```\n- [Conda](https://anaconda.org/conda-forge/snowballstemmer) (ğŸ“¥ 11M Â· â±ï¸ 20.05.2025):\n	```\n	conda install -c conda-forge snowballstemmer\n	```\n</details>\n<details><summary><b><a href="https://github.com/pytorch/text">torchtext</a></b> (ğŸ¥ˆ32 Â·  â­ 3.6K) - Models, data loaders and abstractions for language processing,.. <code><a href="http://bit.ly/3aKzpTv">BSD-3</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/pytorch/text) (ğŸ‘¨â€ğŸ’» 160 Â· ğŸ”€ 810 Â· ğŸ“‹ 850 - 38% open Â· â±ï¸ 10.09.2025):\n\n	```\n	git clone https://github.com/pytorch/text\n	```\n- [PyPi](https://pypi.org/project/torchtext) (ğŸ“¥ 730K / month Â· ğŸ“¦ 280 Â· â±ï¸ 24.04.2024):\n	```\n	pip install torchtext\n	```\n</details>\n<details><summary><b><a href="https://github.com/jamesturk/jellyfish">jellyfish</a></b> (ğŸ¥ˆ32 Â·  â­ 2.2K) - a python library for doing approximate and phonetic matching of strings. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/jamesturk/jellyfish) (ğŸ‘¨â€ğŸ’» 37 Â· ğŸ”€ 160 Â· ğŸ“¦ 15K Â· â±ï¸ 11.10.2025):\n\n	```\n	git clone https://github.com/jamesturk/jellyfish\n	```\n- [PyPi](https://pypi.org/project/jellyfish) (ğŸ“¥ 8.6M / month Â· ğŸ“¦ 320 Â· â±ï¸ 11.10.2025):\n	```\n	pip install jellyfish\n	```\n- [Conda](https://anaconda.org/conda-forge/jellyfish) (ğŸ“¥ 1.7M Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge jellyfish\n	```\n</details>\n<details><summary><b><a href="https://github.com/deeppavlov/DeepPavlov">DeepPavlov</a></b> (ğŸ¥ˆ31 Â·  â­ 6.9K Â· ğŸ’¤) - An open source library for deep learning end-to-end.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/deeppavlov/DeepPavlov) (ğŸ‘¨â€ğŸ’» 78 Â· ğŸ”€ 1.2K Â· ğŸ“¦ 440 Â· ğŸ“‹ 640 - 4% open Â· â±ï¸ 26.11.2024):\n\n	```\n	git clone https://github.com/deepmipt/DeepPavlov\n	```\n- [PyPi](https://pypi.org/project/deeppavlov) (ğŸ“¥ 11K / month Â· ğŸ“¦ 4 Â· â±ï¸ 12.08.2024):\n	```\n	pip install deeppavlov\n	```\n</details>\n<details><summary><b><a href="https://github.com/rspeer/python-ftfy">ftfy</a></b> (ğŸ¥ˆ31 Â·  â­ 4K Â· ğŸ’¤) - Fixes mojibake and other glitches in Unicode text, after the fact. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/rspeer/python-ftfy) (ğŸ‘¨â€ğŸ’» 22 Â· ğŸ”€ 120 Â· ğŸ“¥ 100 Â· ğŸ“¦ 33K Â· ğŸ“‹ 150 - 7% open Â· â±ï¸ 30.10.2024):\n\n	```\n	git clone https://github.com/rspeer/python-ftfy\n	```\n- [PyPi](https://pypi.org/project/ftfy) (ğŸ“¥ 11M / month Â· ğŸ“¦ 570 Â· â±ï¸ 26.10.2024):\n	```\n	pip install ftfy\n	```\n- [Conda](https://anaconda.org/conda-forge/ftfy) (ğŸ“¥ 380K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge ftfy\n	```\n</details>\n<details><summary><b><a href="https://github.com/allenai/scispacy">SciSpacy</a></b> (ğŸ¥ˆ31 Â·  â­ 1.9K) - A full spaCy pipeline and models for scientific/biomedical documents. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/allenai/scispacy) (ğŸ‘¨â€ğŸ’» 38 Â· ğŸ”€ 240 Â· ğŸ“¦ 1.3K Â· ğŸ“‹ 330 - 11% open Â· â±ï¸ 01.10.2025):\n\n	```\n	git clone https://github.com/allenai/scispacy\n	```\n- [PyPi](https://pypi.org/project/scispacy) (ğŸ“¥ 42K / month Â· ğŸ“¦ 50 Â· â±ï¸ 01.10.2025):\n	```\n	pip install scispacy\n	```\n</details>\n<details><summary><b><a href="https://github.com/cltk/cltk">CLTK</a></b> (ğŸ¥ˆ31 Â·  â­ 870 Â· ğŸ“‰) - The Classical Language Toolkit. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/cltk/cltk) (ğŸ‘¨â€ğŸ’» 120 Â· ğŸ”€ 340 Â· ğŸ“¥ 160 Â· ğŸ“¦ 300 Â· ğŸ“‹ 580 - 0% open Â· â±ï¸ 21.10.2025):\n\n	```\n	git clone https://github.com/cltk/cltk\n	```\n- [PyPi](https://pypi.org/project/cltk) (ğŸ“¥ 14K / month Â· ğŸ“¦ 17 Â· â±ï¸ 21.10.2025):\n	```\n	pip install cltk\n	```\n</details>\n<details><summary><b><a href="https://github.com/dwyl/english-words">english-words</a></b> (ğŸ¥ˆ29 Â·  â­ 12K Â· ğŸ’¤) - A text file containing 479k English words for all your.. <code><a href="http://bit.ly/3rvuUlR">Unlicense</a></code></summary>\n\n- [GitHub](https://github.com/dwyl/english-words) (ğŸ‘¨â€ğŸ’» 34 Â· ğŸ”€ 2K Â· ğŸ“¦ 2 Â· ğŸ“‹ 170 - 75% open Â· â±ï¸ 06.01.2025):\n\n	```\n	git clone https://github.com/dwyl/english-words\n	```\n- [PyPi](https://pypi.org/project/english-words) (ğŸ“¥ 78K / month Â· ğŸ“¦ 15 Â· â±ï¸ 14.08.2025):\n	```\n	pip install english-words\n	```\n</details>\n<details><summary><b><a href="https://github.com/argilla-io/argilla">rubrix</a></b> (ğŸ¥ˆ29 Â·  â­ 4.7K) - Argilla is a collaboration tool for AI engineers and domain experts.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/argilla-io/argilla) (ğŸ‘¨â€ğŸ’» 110 Â· ğŸ”€ 460 Â· ğŸ“¦ 3.1K Â· ğŸ“‹ 2.2K - 0% open Â· â±ï¸ 05.08.2025):\n\n	```\n	git clone https://github.com/recognai/rubrix\n	```\n- [PyPi](https://pypi.org/project/rubrix) (ğŸ“¥ 1.2K / month Â· â±ï¸ 24.10.2022):\n	```\n	pip install rubrix\n	```\n- [Conda](https://anaconda.org/conda-forge/rubrix) (ğŸ“¥ 52K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge rubrix\n	```\n</details>\n<details><summary><b><a href="https://github.com/dedupeio/dedupe">Dedupe</a></b> (ğŸ¥ˆ29 Â·  â­ 4.4K Â· ğŸ“ˆ) - A python library for accurate and scalable fuzzy matching, record.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/dedupeio/dedupe) (ğŸ‘¨â€ğŸ’» 72 Â· ğŸ”€ 560 Â· ğŸ“¦ 370 Â· ğŸ“‹ 820 - 9% open Â· â±ï¸ 29.07.2025):\n\n	```\n	git clone https://github.com/dedupeio/dedupe\n	```\n- [PyPi](https://pypi.org/project/dedupe) (ğŸ“¥ 59K / month Â· ğŸ“¦ 19 Â· â±ï¸ 15.08.2024):\n	```\n	pip install dedupe\n	```\n- [Conda](https://anaconda.org/conda-forge/dedupe) (ğŸ“¥ 130K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge dedupe\n	```\n</details>\n<details><summary><b><a href="https://github.com/life4/textdistance">TextDistance</a></b> (ğŸ¥ˆ28 Â·  â­ 3.5K) - Compute distance between sequences. 30+ algorithms, pure python.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/life4/textdistance) (ğŸ‘¨â€ğŸ’» 18 Â· ğŸ”€ 260 Â· ğŸ“¥ 1.1K Â· ğŸ“¦ 8.8K Â· â±ï¸ 18.04.2025):\n\n	```\n	git clone https://github.com/life4/textdistance\n	```\n- [PyPi](https://pypi.org/project/textdistance) (ğŸ“¥ 1.3M / month Â· ğŸ“¦ 99 Â· â±ï¸ 16.07.2024):\n	```\n	pip install textdistance\n	```\n- [Conda](https://anaconda.org/conda-forge/textdistance) (ğŸ“¥ 970K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge textdistance\n	```\n</details>\n<details><summary><b><a href="https://github.com/explosion/spacy-transformers">spacy-transformers</a></b> (ğŸ¥ˆ28 Â·  â­ 1.4K) - Use pretrained transformers like BERT, XLNet and GPT-2.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code>spacy</code></summary>\n\n- [GitHub](https://github.com/explosion/spacy-transformers) (ğŸ‘¨â€ğŸ’» 23 Â· ğŸ”€ 170 Â· ğŸ“¥ 610 Â· ğŸ“¦ 2.4K Â· â±ï¸ 26.05.2025):\n\n	```\n	git clone https://github.com/explosion/spacy-transformers\n	```\n- [PyPi](https://pypi.org/project/spacy-transformers) (ğŸ“¥ 270K / month Â· ğŸ“¦ 110 Â· â±ï¸ 26.05.2025):\n	```\n	pip install spacy-transformers\n	```\n- [Conda](https://anaconda.org/conda-forge/spacy-transformers) (ğŸ“¥ 140K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge spacy-transformers\n	```\n</details>\n<details><summary><b><a href="https://github.com/unitaryai/detoxify">detoxify</a></b> (ğŸ¥‰26 Â·  â­ 1.1K) - Trained models & code to predict toxic comments on all 3 Jigsaw.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/unitaryai/detoxify) (ğŸ‘¨â€ğŸ’» 14 Â· ğŸ”€ 130 Â· ğŸ“¥ 1.9M Â· ğŸ“¦ 980 Â· ğŸ“‹ 67 - 55% open Â· â±ï¸ 29.07.2025):\n\n	```\n	git clone https://github.com/unitaryai/detoxify\n	```\n- [PyPi](https://pypi.org/project/detoxify) (ğŸ“¥ 140K / month Â· ğŸ“¦ 30 Â· â±ï¸ 01.02.2024):\n	```\n	pip install detoxify\n	```\n</details>\n<details><summary><b><a href="https://github.com/JasonKessler/scattertext">scattertext</a></b> (ğŸ¥‰25 Â·  â­ 2.3K) - Beautiful visualizations of how language differs among document.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code></summary>\n\n- [GitHub](https://github.com/JasonKessler/scattertext) (ğŸ‘¨â€ğŸ’» 14 Â· ğŸ”€ 290 Â· ğŸ“¦ 670 Â· ğŸ“‹ 100 - 22% open Â· â±ï¸ 29.04.2025):\n\n	```\n	git clone https://github.com/JasonKessler/scattertext\n	```\n- [PyPi](https://pypi.org/project/scattertext) (ğŸ“¥ 7.5K / month Â· ğŸ“¦ 5 Â· â±ï¸ 23.09.2024):\n	```\n	pip install scattertext\n	```\n- [Conda](https://anaconda.org/conda-forge/scattertext) (ğŸ“¥ 140K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge scattertext\n	```\n</details>\n<details><summary><b><a href="https://github.com/google-research/text-to-text-transfer-transformer">T5</a></b> (ğŸ¥‰24 Â·  â­ 6.4K) - Code for the paper Exploring the Limits of Transfer Learning with a.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/google-research/text-to-text-transfer-transformer) (ğŸ‘¨â€ğŸ’» 61 Â· ğŸ”€ 780 Â· ğŸ“‹ 450 - 23% open Â· â±ï¸ 28.04.2025):\n\n	```\n	git clone https://github.com/google-research/text-to-text-transfer-transformer\n	```\n- [PyPi](https://pypi.org/project/t5) (ğŸ“¥ 83K / month Â· ğŸ“¦ 2 Â· â±ï¸ 18.10.2021):\n	```\n	pip install t5\n	```\n</details>\n<details><summary><b><a href="https://github.com/zjunlp/DeepKE">DeepKE</a></b> (ğŸ¥‰24 Â·  â­ 4.2K) - [EMNLP 2022] An Open Toolkit for Knowledge Graph Extraction and.. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/zjunlp/DeepKE) (ğŸ‘¨â€ğŸ’» 34 Â· ğŸ”€ 730 Â· ğŸ“¦ 25 Â· â±ï¸ 19.07.2025):\n\n	```\n	git clone https://github.com/zjunlp/deepke\n	```\n- [PyPi](https://pypi.org/project/deepke) (ğŸ“¥ 950 / month Â· â±ï¸ 21.09.2023):\n	```\n	pip install deepke\n	```\n</details>\n<details><summary><b><a href="https://github.com/explosion/sense2vec">sense2vec</a></b> (ğŸ¥‰24 Â·  â­ 1.7K) - Contextually-keyed word vectors. <code><a href="http://bit.ly/34MBwT8">MIT</a></code></summary>\n\n- [GitHub](https://github.com/explosion/sense2vec) (ğŸ‘¨â€ğŸ’» 20 Â· ğŸ”€ 240 Â· ğŸ“¥ 73K Â· ğŸ“¦ 470 Â· ğŸ“‹ 120 - 20% open Â· â±ï¸ 23.04.2025):\n\n	```\n	git clone https://github.com/explosion/sense2vec\n	```\n- [PyPi](https://pypi.org/project/sense2vec) (ğŸ“¥ 3.4K / month Â· ğŸ“¦ 13 Â· â±ï¸ 19.04.2021):\n	```\n	pip install sense2vec\n	```\n- [Conda](https://anaconda.org/conda-forge/sense2vec) (ğŸ“¥ 67K Â· â±ï¸ 22.04.2025):\n	```\n	conda install -c conda-forge sense2vec\n	```\n</details>\n<details><summary><b><a href="https://github.com/IndicoDataSolutions/finetune">finetune</a></b> (ğŸ¥‰23 Â·  â­ 720) - Scikit-learn style model finetuning for NLP. <code><a href="http://bit.ly/3postzC">MPL-2.0</a></code> <code><img src="https://git.io/JLy1A" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1F" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/IndicoDataSolutions/finetune) (ğŸ‘¨â€ğŸ’» 24 Â· ğŸ”€ 79 Â· ğŸ“¦ 16 Â· ğŸ“‹ 190 - 39% open Â· â±ï¸ 21.10.2025):\n\n	```\n	git clone https://github.com/IndicoDataSolutions/finetune\n	```\n- [PyPi](https://pypi.org/project/finetune) (ğŸ“¥ 2.7K / month Â· ğŸ“¦ 2 Â· â±ï¸ 29.09.2023):\n	```\n	pip install finetune\n	```\n</details>\n<details><summary><b><a href="https://github.com/EricFillion/happy-transformer">happy-transformer</a></b> (ğŸ¥‰23 Â·  â­ 540 Â· ğŸ’¤) - Happy Transformer makes it easy to fine-tune and.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code>huggingface</code></summary>\n\n- [GitHub](https://github.com/EricFillion/happy-transformer) (ğŸ‘¨â€ğŸ’» 14 Â· ğŸ”€ 69 Â· ğŸ“¦ 330 Â· ğŸ“‹ 130 - 16% open Â· â±ï¸ 22.03.2025):\n\n	```\n	git clone https://github.com/EricFillion/happy-transformer\n	```\n- [PyPi](https://pypi.org/project/happytransformer) (ğŸ“¥ 2.7K / month Â· ğŸ“¦ 5 Â· â±ï¸ 05.08.2023):\n	```\n	pip install happytransformer\n	```\n</details>\n<details><summary><b><a href="https://github.com/awslabs/sockeye">Sockeye</a></b> (ğŸ¥‰21 Â·  â­ 1.2K Â· ğŸ’¤) - Sequence-to-sequence framework with a focus on Neural.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1X" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/awslabs/sockeye) (ğŸ‘¨â€ğŸ’» 60 Â· ğŸ”€ 320 Â· ğŸ“¥ 21 Â· ğŸ“‹ 310 - 3% open Â· â±ï¸ 24.10.2024):\n\n	```\n	git clone https://github.com/awslabs/sockeye\n	```\n- [PyPi](https://pypi.org/project/sockeye) (ğŸ“¥ 580 / month Â· â±ï¸ 03.03.2023):\n	```\n	pip install sockeye\n	```\n</details>\n<details><summary><b><a href="https://github.com/unum-cloud/UForm">UForm</a></b> (ğŸ¥‰21 Â·  â­ 1.2K) - Pocket-Sized Multimodal AI for content understanding and.. <code><a href="http://bit.ly/3nYMfla">Apache-2</a></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/unum-cloud/UForm) (ğŸ‘¨â€ğŸ’» 21 Â· ğŸ”€ 76 Â· ğŸ“¥ 710 Â· ğŸ“¦ 36 Â· ğŸ“‹ 39 - 38% open Â· â±ï¸ 03.09.2025):\n\n	```\n	git clone https://github.com/unum-cloud/uform\n	```\n- [PyPi](https://pypi.org/project/uform) (ğŸ“¥ 490 / month Â· ğŸ“¦ 2 Â· â±ï¸ 03.09.2025):\n	```\n	pip install uform\n	```\n</details>\n<details><summary><b><a href="https://github.com/webis-de/small-text">small-text</a></b> (ğŸ¥‰20 Â·  â­ 630) - Active Learning for Text Classification in Python. <code><a href="http://bit.ly/34MBwT8">MIT</a></code> <code><img src="https://git.io/JLy1F" style="display:inline;" width="13" height="13"></code> <code><img src="https://git.io/JLy1Q" style="display:inline;" width="13" height="13"></code></summary>\n\n- [GitHub](https://github.com/webis-de/small-text) (ğŸ‘¨â€ğŸ’» 10 Â· ğŸ”€ 76 Â· ğŸ“¦ 34 Â· ğŸ“‹ 74 - 28% open Â· â±ï¸ 28.10.2025):\n\n	```\n	git clone https://github.com/webis-de/small-text\n	```\n- [PyPi](https://pypi.org/project/small-text) (ğŸ“¥ 390 / month Â· â±ï¸ 17.08.2025):\n	```\n	pip install small-text\n	```\n- [Conda](https://anaconda.org/conda-forge/small-text) (ğŸ“¥ 19K Â· â±ï¸ 17.08.2025):\n	```\n	conda install -c conda-forge small-text\n	```\n</details>\n<details><summary><b><a href="https://github.com/dsfsi/textaugment">textaugment</a></b> (ğŸ¥‰19 Â·  â­ 430) - TextAugment: Text Augmentation Library. <code><a href="http://bit.ly/34MBw', '{"language":null,"stars":22915,"forks":3046,"watchers":22915,"open_issues":33,"topics":["automl","chatgpt","data-analysis","data-science","data-visualization","data-visualizations","deep-learning","gpt","gpt-3","jax","keras","machine-learning","ml","nlp","python","pytorch","scikit-learn","tensorflow","transformer"],"default_branch":"main","size_kb":21391,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:ml-tooling:best-of\"","source_url":"https://github.com/ml-tooling/best-of\""},{"type":"has_code","target_id":"github:ml-tooling:best-of-ml-python","source_url":"https://github.com/ml-tooling/best-of-ml-python"},{"type":"has_code","target_id":"github:ml-tooling:best-of-ml-python","source_url":"https://github.com/ml-tooling/best-of-ml-python"},{"type":"has_code","target_id":"github:ml-tooling:best-of-ml-python","source_url":"https://github.com/ml-tooling/best-of-ml-python"},{"type":"has_code","target_id":"github:ml-tooling:best-of-ml-python","source_url":"https://github.com/ml-tooling/best-of-ml-python"},{"type":"has_code","target_id":"github:best-of-lists:best-of","source_url":"https://github.com/best-of-lists/best-of"},{"type":"has_code","target_id":"github:tensorflow:tensorflow\">Tensorflow<","source_url":"https://github.com/tensorflow/tensorflow\">Tensorflow<"},{"type":"has_code","target_id":"github:tensorflow:tensorflow","source_url":"https://github.com/tensorflow/tensorflow"},{"type":"has_code","target_id":"github:tensorflow:tensorflow","source_url":"https://github.com/tensorflow/tensorflow"},{"type":"has_code","target_id":"github:pytorch:pytorch\">PyTorch<","source_url":"https://github.com/pytorch/pytorch\">PyTorch<"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:scikit-learn:scikit-learn\">scikit-learn<","source_url":"https://github.com/scikit-learn/scikit-learn\">scikit-learn<"},{"type":"has_code","target_id":"github:scikit-learn:scikit-learn","source_url":"https://github.com/scikit-learn/scikit-learn"},{"type":"has_code","target_id":"github:scikit-learn:scikit-learn","source_url":"https://github.com/scikit-learn/scikit-learn"},{"type":"has_code","target_id":"github:keras-team:keras\">Keras<","source_url":"https://github.com/keras-team/keras\">Keras<"},{"type":"has_code","target_id":"github:keras-team:keras","source_url":"https://github.com/keras-team/keras"},{"type":"has_code","target_id":"github:keras-team:keras","source_url":"https://github.com/keras-team/keras"},{"type":"has_code","target_id":"github:dmlc:xgboost\">XGBoost<","source_url":"https://github.com/dmlc/xgboost\">XGBoost<"},{"type":"has_code","target_id":"github:dmlc:xgboost","source_url":"https://github.com/dmlc/xgboost"},{"type":"has_code","target_id":"github:dmlc:xgboost","source_url":"https://github.com/dmlc/xgboost"},{"type":"has_code","target_id":"github:PaddlePaddle:Paddle\">PaddlePaddle<","source_url":"https://github.com/PaddlePaddle/Paddle\">PaddlePaddle<"},{"type":"has_code","target_id":"github:PaddlePaddle:Paddle","source_url":"https://github.com/PaddlePaddle/Paddle"},{"type":"has_code","target_id":"github:PaddlePaddle:Paddle","source_url":"https://github.com/PaddlePaddle/Paddle"},{"type":"has_code","target_id":"github:jax-ml:jax\">jax<","source_url":"https://github.com/jax-ml/jax\">jax<"},{"type":"has_code","target_id":"github:jax-ml:jax","source_url":"https://github.com/jax-ml/jax"},{"type":"has_code","target_id":"github:google:jax","source_url":"https://github.com/google/jax"},{"type":"has_code","target_id":"github:Lightning-AI:pytorch-lightning\">pytorch-lightning<","source_url":"https://github.com/Lightning-AI/pytorch-lightning\">pytorch-lightning<"},{"type":"has_code","target_id":"github:Lightning-AI:pytorch-lightning","source_url":"https://github.com/Lightning-AI/pytorch-lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:statsmodels:statsmodels\">StatsModels<","source_url":"https://github.com/statsmodels/statsmodels\">StatsModels<"},{"type":"has_code","target_id":"github:statsmodels:statsmodels","source_url":"https://github.com/statsmodels/statsmodels"},{"type":"has_code","target_id":"github:statsmodels:statsmodels","source_url":"https://github.com/statsmodels/statsmodels"},{"type":"has_code","target_id":"github:apache:spark\">PySpark<","source_url":"https://github.com/apache/spark\">PySpark<"},{"type":"has_code","target_id":"github:apache:spark","source_url":"https://github.com/apache/spark"},{"type":"has_code","target_id":"github:apache:spark","source_url":"https://github.com/apache/spark"},{"type":"has_code","target_id":"github:microsoft:LightGBM\">LightGBM<","source_url":"https://github.com/microsoft/LightGBM\">LightGBM<"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:catboost:catboost\">Catboost<","source_url":"https://github.com/catboost/catboost\">Catboost<"},{"type":"has_code","target_id":"github:catboost:catboost","source_url":"https://github.com/catboost/catboost"},{"type":"has_code","target_id":"github:catboost:catboost","source_url":"https://github.com/catboost/catboost"},{"type":"has_code","target_id":"github:fastai:fastai\">Fastai<","source_url":"https://github.com/fastai/fastai\">Fastai<"},{"type":"has_code","target_id":"github:fastai:fastai","source_url":"https://github.com/fastai/fastai"},{"type":"has_code","target_id":"github:fastai:fastai","source_url":"https://github.com/fastai/fastai"},{"type":"has_code","target_id":"github:apache:flink\">PyFlink<","source_url":"https://github.com/apache/flink\">PyFlink<"},{"type":"has_code","target_id":"github:apache:flink","source_url":"https://github.com/apache/flink"},{"type":"has_code","target_id":"github:apache:flink","source_url":"https://github.com/apache/flink"},{"type":"has_code","target_id":"github:google:flax\">Flax<","source_url":"https://github.com/google/flax\">Flax<"},{"type":"has_code","target_id":"github:google:flax","source_url":"https://github.com/google/flax"},{"type":"has_code","target_id":"github:google:flax","source_url":"https://github.com/google/flax"},{"type":"has_code","target_id":"github:pytorch:ignite\">Ignite<","source_url":"https://github.com/pytorch/ignite\">Ignite<"},{"type":"has_code","target_id":"github:pytorch:ignite","source_url":"https://github.com/pytorch/ignite"},{"type":"has_code","target_id":"github:pytorch:ignite","source_url":"https://github.com/pytorch/ignite"},{"type":"has_code","target_id":"github:arogozhnikov:einops\">einops<","source_url":"https://github.com/arogozhnikov/einops\">einops<"},{"type":"has_code","target_id":"github:arogozhnikov:einops","source_url":"https://github.com/arogozhnikov/einops"},{"type":"has_code","target_id":"github:arogozhnikov:einops","source_url":"https://github.com/arogozhnikov/einops"},{"type":"has_code","target_id":"github:ivy-llc:ivy\">ivy<","source_url":"https://github.com/ivy-llc/ivy\">ivy<"},{"type":"has_code","target_id":"github:ivy-llc:ivy","source_url":"https://github.com/ivy-llc/ivy"},{"type":"has_code","target_id":"github:unifyai:ivy","source_url":"https://github.com/unifyai/ivy"},{"type":"has_code","target_id":"github:jina-ai:serve\">Jina<","source_url":"https://github.com/jina-ai/serve\">Jina<"},{"type":"has_code","target_id":"github:jina-ai:serve","source_url":"https://github.com/jina-ai/serve"},{"type":"has_code","target_id":"github:jina-ai:jina","source_url":"https://github.com/jina-ai/jina"},{"type":"has_code","target_id":"github:mlpack:mlpack\">mlpack<","source_url":"https://github.com/mlpack/mlpack\">mlpack<"},{"type":"has_code","target_id":"github:mlpack:mlpack","source_url":"https://github.com/mlpack/mlpack"},{"type":"has_code","target_id":"github:mlpack:mlpack","source_url":"https://github.com/mlpack/mlpack"},{"type":"has_code","target_id":"github:explosion:thinc\">Thinc<","source_url":"https://github.com/explosion/thinc\">Thinc<"},{"type":"has_code","target_id":"github:explosion:thinc","source_url":"https://github.com/explosion/thinc"},{"type":"has_code","target_id":"github:explosion:thinc","source_url":"https://github.com/explosion/thinc"},{"type":"has_code","target_id":"github:ludwig-ai:ludwig\">Ludwig<","source_url":"https://github.com/ludwig-ai/ludwig\">Ludwig<"},{"type":"has_code","target_id":"github:ludwig-ai:ludwig","source_url":"https://github.com/ludwig-ai/ludwig"},{"type":"has_code","target_id":"github:ludwig-ai:ludwig","source_url":"https://github.com/ludwig-ai/ludwig"},{"type":"has_code","target_id":"github:skorch-dev:skorch\">skorch<","source_url":"https://github.com/skorch-dev/skorch\">skorch<"},{"type":"has_code","target_id":"github:skorch-dev:skorch","source_url":"https://github.com/skorch-dev/skorch"},{"type":"has_code","target_id":"github:skorch-dev:skorch","source_url":"https://github.com/skorch-dev/skorch"},{"type":"has_code","target_id":"github:google-deepmind:sonnet\">Sonnet<","source_url":"https://github.com/google-deepmind/sonnet\">Sonnet<"},{"type":"has_code","target_id":"github:google-deepmind:sonnet","source_url":"https://github.com/google-deepmind/sonnet"},{"type":"has_code","target_id":"github:deepmind:sonnet","source_url":"https://github.com/deepmind/sonnet"},{"type":"has_code","target_id":"github:google-deepmind:dm-haiku\">Haiku<","source_url":"https://github.com/google-deepmind/dm-haiku\">Haiku<"},{"type":"has_code","target_id":"github:google-deepmind:dm-haiku","source_url":"https://github.com/google-deepmind/dm-haiku"},{"type":"has_code","target_id":"github:deepmind:dm-haiku","source_url":"https://github.com/deepmind/dm-haiku"},{"type":"has_code","target_id":"github:ROCm:tensorflow-upstream\">tensorflow-upstream<","source_url":"https://github.com/ROCm/tensorflow-upstream\">tensorflow-upstream<"},{"type":"has_code","target_id":"github:ROCm:tensorflow-upstream","source_url":"https://github.com/ROCm/tensorflow-upstream"},{"type":"has_code","target_id":"github:ROCmSoftwarePlatform:tensorflow-upstream","source_url":"https://github.com/ROCmSoftwarePlatform/tensorflow-upstream"},{"type":"has_code","target_id":"github:geomstats:geomstats\">Geomstats<","source_url":"https://github.com/geomstats/geomstats\">Geomstats<"},{"type":"has_code","target_id":"github:geomstats:geomstats","source_url":"https://github.com/geomstats/geomstats"},{"type":"has_code","target_id":"github:geomstats:geomstats","source_url":"https://github.com/geomstats/geomstats"},{"type":"has_code","target_id":"github:pyRiemann:pyRiemann\">pyRiemann<","source_url":"https://github.com/pyRiemann/pyRiemann\">pyRiemann<"},{"type":"has_code","target_id":"github:pyRiemann:pyRiemann","source_url":"https://github.com/pyRiemann/pyRiemann"},{"type":"has_code","target_id":"github:pyRiemann:pyRiemann","source_url":"https://github.com/pyRiemann/pyRiemann"},{"type":"has_code","target_id":"github:numenta:nupic-legacy\">NuPIC<","source_url":"https://github.com/numenta/nupic-legacy\">NuPIC<"},{"type":"has_code","target_id":"github:numenta:nupic-legacy","source_url":"https://github.com/numenta/nupic-legacy"},{"type":"has_code","target_id":"github:numenta:nupic","source_url":"https://github.com/numenta/nupic"},{"type":"has_code","target_id":"github:determined-ai:determined\">Determined<","source_url":"https://github.com/determined-ai/determined\">Determined<"},{"type":"has_code","target_id":"github:determined-ai:determined","source_url":"https://github.com/determined-ai/determined"},{"type":"has_code","target_id":"github:determined-ai:determined","source_url":"https://github.com/determined-ai/determined"},{"type":"has_code","target_id":"github:sony:nnabla\">Neural","source_url":"https://github.com/sony/nnabla\">Neural"},{"type":"has_code","target_id":"github:sony:nnabla","source_url":"https://github.com/sony/nnabla"},{"type":"has_code","target_id":"github:sony:nnabla","source_url":"https://github.com/sony/nnabla"},{"type":"has_code","target_id":"github:deepinv:deepinv\">deepinv<","source_url":"https://github.com/deepinv/deepinv\">deepinv<"},{"type":"has_code","target_id":"github:deepinv:deepinv","source_url":"https://github.com/deepinv/deepinv"},{"type":"has_code","target_id":"github:deepinv:deepinv","source_url":"https://github.com/deepinv/deepinv"},{"type":"has_code","target_id":"github:towhee-io:towhee\">Towhee<","source_url":"https://github.com/towhee-io/towhee\">Towhee<"},{"type":"has_code","target_id":"github:towhee-io:towhee","source_url":"https://github.com/towhee-io/towhee"},{"type":"has_code","target_id":"github:towhee-io:towhee","source_url":"https://github.com/towhee-io/towhee"},{"type":"has_code","target_id":"github:nubank:fklearn\">fklearn<","source_url":"https://github.com/nubank/fklearn\">fklearn<"},{"type":"has_code","target_id":"github:nubank:fklearn","source_url":"https://github.com/nubank/fklearn"},{"type":"has_code","target_id":"github:nubank:fklearn","source_url":"https://github.com/nubank/fklearn"},{"type":"has_code","target_id":"github:run-house:kubetorch\">Runhouse<","source_url":"https://github.com/run-house/kubetorch\">Runhouse<"},{"type":"has_code","target_id":"github:run-house:kubetorch","source_url":"https://github.com/run-house/kubetorch"},{"type":"has_code","target_id":"github:run-house:runhouse","source_url":"https://github.com/run-house/runhouse"},{"type":"has_code","target_id":"github:neoml-lib:neoml\">NeoML<","source_url":"https://github.com/neoml-lib/neoml\">NeoML<"},{"type":"has_code","target_id":"github:neoml-lib:neoml","source_url":"https://github.com/neoml-lib/neoml"},{"type":"has_code","target_id":"github:neoml-lib:neoml","source_url":"https://github.com/neoml-lib/neoml"},{"type":"has_code","target_id":"github:serengil:chefboost\">chefboost<","source_url":"https://github.com/serengil/chefboost\">chefboost<"},{"type":"has_code","target_id":"github:serengil:chefboost","source_url":"https://github.com/serengil/chefboost"},{"type":"has_code","target_id":"github:serengil:chefboost","source_url":"https://github.com/serengil/chefboost"},{"type":"has_code","target_id":"github:Xtra-Computing:thundergbm\">ThunderGBM<","source_url":"https://github.com/Xtra-Computing/thundergbm\">ThunderGBM<"},{"type":"has_code","target_id":"github:Xtra-Computing:thundergbm","source_url":"https://github.com/Xtra-Computing/thundergbm"},{"type":"has_code","target_id":"github:Xtra-Computing:thundergbm","source_url":"https://github.com/Xtra-Computing/thundergbm"},{"type":"has_code","target_id":"github:davisking:dlib\">dlib<","source_url":"https://github.com/davisking/dlib\">dlib<"},{"type":"has_code","target_id":"github:apache:mxnet\">MXNet<","source_url":"https://github.com/apache/mxnet\">MXNet<"},{"type":"has_code","target_id":"github:Theano:Theano\">Theano<","source_url":"https://github.com/Theano/Theano\">Theano<"},{"type":"has_code","target_id":"github:mindsdb:mindsdb\">MindsDB<","source_url":"https://github.com/mindsdb/mindsdb\">MindsDB<"},{"type":"has_code","target_id":"github:VowpalWabbit:vowpal_wabbit\">Vowpal","source_url":"https://github.com/VowpalWabbit/vowpal_wabbit\">Vowpal"},{"type":"has_code","target_id":"github:chainer:chainer\">Chainer<","source_url":"https://github.com/chainer/chainer\">Chainer<"},{"type":"has_code","target_id":"github:apple:turicreate\">Turi","source_url":"https://github.com/apple/turicreate\">Turi"},{"type":"has_code","target_id":"github:tensorpack:tensorpack\">tensorpack<","source_url":"https://github.com/tensorpack/tensorpack\">tensorpack<"},{"type":"has_code","target_id":"github:tflearn:tflearn\">TFlearn<","source_url":"https://github.com/tflearn/tflearn\">TFlearn<"},{"type":"has_code","target_id":"github:clab:dynet\">dyNET<","source_url":"https://github.com/clab/dynet\">dyNET<"},{"type":"has_code","target_id":"github:microsoft:CNTK\">CNTK<","source_url":"https://github.com/microsoft/CNTK\">CNTK<"},{"type":"has_code","target_id":"github:Lasagne:Lasagne\">Lasagne<","source_url":"https://github.com/Lasagne/Lasagne\">Lasagne<"},{"type":"has_code","target_id":"github:shogun-toolbox:shogun\">SHOGUN<","source_url":"https://github.com/shogun-toolbox/shogun\">SHOGUN<"},{"type":"has_code","target_id":"github:amaiya:ktrain\">ktrain<","source_url":"https://github.com/amaiya/ktrain\">ktrain<"},{"type":"has_code","target_id":"github:itdxer:neupy\">NeuPy<","source_url":"https://github.com/itdxer/neupy\">NeuPy<"},{"type":"has_code","target_id":"github:aksnzhy:xlearn\">xLearn<","source_url":"https://github.com/aksnzhy/xlearn\">xLearn<"},{"type":"has_code","target_id":"github:georgia-tech-db:evadb\">EvaDB<","source_url":"https://github.com/georgia-tech-db/evadb\">EvaDB<"},{"type":"has_code","target_id":"github:NervanaSystems:neon\">neon<","source_url":"https://github.com/NervanaSystems/neon\">neon<"},{"type":"has_code","target_id":"github:Xtra-Computing:thundersvm\">ThunderSVM<","source_url":"https://github.com/Xtra-Computing/thundersvm\">ThunderSVM<"},{"type":"has_code","target_id":"github:pytorchbearer:torchbearer\">Torchbearer<","source_url":"https://github.com/pytorchbearer/torchbearer\">Torchbearer<"},{"type":"has_code","target_id":"github:XiaoMi:mace\">mace<","source_url":"https://github.com/XiaoMi/mace\">mace<"},{"type":"has_code","target_id":"github:google:neural-tangents\">Neural","source_url":"https://github.com/google/neural-tangents\">Neural"},{"type":"has_code","target_id":"github:google:objax\">Objax<","source_url":"https://github.com/google/objax\">Objax<"},{"type":"has_code","target_id":"github:poets-ai:elegy\">elegy<","source_url":"https://github.com/poets-ai/elegy\">elegy<"},{"type":"has_code","target_id":"github:facebookresearch:StarSpace\">StarSpace<","source_url":"https://github.com/facebookresearch/StarSpace\">StarSpace<"},{"type":"has_code","target_id":"github:HenryNdubuaku:nanodl\">nanodl<","source_url":"https://github.com/HenryNdubuaku/nanodl\">nanodl<"},{"type":"has_code","target_id":"github:matplotlib:matplotlib\">Matplotlib<","source_url":"https://github.com/matplotlib/matplotlib\">Matplotlib<"},{"type":"has_code","target_id":"github:matplotlib:matplotlib","source_url":"https://github.com/matplotlib/matplotlib"},{"type":"has_code","target_id":"github:matplotlib:matplotlib","source_url":"https://github.com/matplotlib/matplotlib"},{"type":"has_code","target_id":"github:plotly:plotly.py\">Plotly<","source_url":"https://github.com/plotly/plotly.py\">Plotly<"},{"type":"has_code","target_id":"github:plotly:plotly.py","source_url":"https://github.com/plotly/plotly.py"},{"type":"has_code","target_id":"github:plotly:plotly.py","source_url":"https://github.com/plotly/plotly.py"},{"type":"has_code","target_id":"github:plotly:dash\">dash<","source_url":"https://github.com/plotly/dash\">dash<"},{"type":"has_code","target_id":"github:plotly:dash","source_url":"https://github.com/plotly/dash"},{"type":"has_code","target_id":"github:plotly:dash","source_url":"https://github.com/plotly/dash"},{"type":"has_code","target_id":"github:bokeh:bokeh\">Bokeh<","source_url":"https://github.com/bokeh/bokeh\">Bokeh<"},{"type":"has_code","target_id":"github:bokeh:bokeh","source_url":"https://github.com/bokeh/bokeh"},{"type":"has_code","target_id":"github:bokeh:bokeh","source_url":"https://github.com/bokeh/bokeh"},{"type":"has_code","target_id":"github:mwaskom:seaborn\">Seaborn<","source_url":"https://github.com/mwaskom/seaborn\">Seaborn<"},{"type":"has_code","target_id":"github:mwaskom:seaborn","source_url":"https://github.com/mwaskom/seaborn"},{"type":"has_code","target_id":"github:mwaskom:seaborn","source_url":"https://github.com/mwaskom/seaborn"},{"type":"has_code","target_id":"github:vega:altair\">Altair<","source_url":"https://github.com/vega/altair\">Altair<"},{"type":"has_code","target_id":"github:vega:altair","source_url":"https://github.com/vega/altair"},{"type":"has_code","target_id":"github:altair-viz:altair","source_url":"https://github.com/altair-viz/altair"},{"type":"has_code","target_id":"github:voxel51:fiftyone\">FiftyOne<","source_url":"https://github.com/voxel51/fiftyone\">FiftyOne<"},{"type":"has_code","target_id":"github:voxel51:fiftyone","source_url":"https://github.com/voxel51/fiftyone"},{"type":"has_code","target_id":"github:voxel51:fiftyone","source_url":"https://github.com/voxel51/fiftyone"},{"type":"has_code","target_id":"github:xflr6:graphviz\">Graphviz<","source_url":"https://github.com/xflr6/graphviz\">Graphviz<"},{"type":"has_code","target_id":"github:xflr6:graphviz","source_url":"https://github.com/xflr6/graphviz"},{"type":"has_code","target_id":"github:xflr6:graphviz","source_url":"https://github.com/xflr6/graphviz"},{"type":"has_code","target_id":"github:pyvista:pyvista\">PyVista<","source_url":"https://github.com/pyvista/pyvista\">PyVista<"},{"type":"has_code","target_id":"github:pyvista:pyvista","source_url":"https://github.com/pyvista/pyvista"},{"type":"has_code","target_id":"github:pyvista:pyvista","source_url":"https://github.com/pyvista/pyvista"},{"type":"has_code","target_id":"github:holoviz:holoviews\">HoloViews<","source_url":"https://github.com/holoviz/holoviews\">HoloViews<"},{"type":"has_code","target_id":"github:holoviz:holoviews","source_url":"https://github.com/holoviz/holoviews"},{"type":"has_code","target_id":"github:holoviz:holoviews","source_url":"https://github.com/holoviz/holoviews"},{"type":"has_code","target_id":"github:pyecharts:pyecharts\">pyecharts<","source_url":"https://github.com/pyecharts/pyecharts\">pyecharts<"},{"type":"has_code","target_id":"github:pyecharts:pyecharts","source_url":"https://github.com/pyecharts/pyecharts"},{"type":"has_code","target_id":"github:pyecharts:pyecharts","source_url":"https://github.com/pyecharts/pyecharts"},{"type":"has_code","target_id":"github:pyqtgraph:pyqtgraph\">PyQtGraph<","source_url":"https://github.com/pyqtgraph/pyqtgraph\">PyQtGraph<"},{"type":"has_code","target_id":"github:pyqtgraph:pyqtgraph","source_url":"https://github.com/pyqtgraph/pyqtgraph"},{"type":"has_code","target_id":"github:pyqtgraph:pyqtgraph","source_url":"https://github.com/pyqtgraph/pyqtgraph"},{"type":"has_code","target_id":"github:ydataai:ydata-profiling\">pandas-profiling<","source_url":"https://github.com/ydataai/ydata-profiling\">pandas-profiling<"},{"type":"has_code","target_id":"github:ydataai:ydata-profiling","source_url":"https://github.com/ydataai/ydata-profiling"},{"type":"has_code","target_id":"github:ydataai:pandas-profiling","source_url":"https://github.com/ydataai/pandas-profiling"},{"type":"has_code","target_id":"github:has2k1:plotnine\">plotnine<","source_url":"https://github.com/has2k1/plotnine\">plotnine<"},{"type":"has_code","target_id":"github:has2k1:plotnine","source_url":"https://github.com/has2k1/plotnine"},{"type":"has_code","target_id":"github:has2k1:plotnine","source_url":"https://github.com/has2k1/plotnine"},{"type":"has_code","target_id":"github:SciTools:cartopy\">cartopy<","source_url":"https://github.com/SciTools/cartopy\">cartopy<"},{"type":"has_code","target_id":"github:SciTools:cartopy","source_url":"https://github.com/SciTools/cartopy"},{"type":"has_code","target_id":"github:SciTools:cartopy","source_url":"https://github.com/SciTools/cartopy"},{"type":"has_code","target_id":"github:vispy:vispy\">VisPy<","source_url":"https://github.com/vispy/vispy\">VisPy<"},{"type":"has_code","target_id":"github:vispy:vispy","source_url":"https://github.com/vispy/vispy"},{"type":"has_code","target_id":"github:vispy:vispy","source_url":"https://github.com/vispy/vispy"},{"type":"has_code","target_id":"github:holoviz:datashader\">datashader<","source_url":"https://github.com/holoviz/datashader\">datashader<"},{"type":"has_code","target_id":"github:holoviz:datashader","source_url":"https://github.com/holoviz/datashader"},{"type":"has_code","target_id":"github:holoviz:datashader","source_url":"https://github.com/holoviz/datashader"},{"type":"has_code","target_id":"github:JetBrains:lets-plot\">lets-plot<","source_url":"https://github.com/JetBrains/lets-plot\">lets-plot<"},{"type":"has_code","target_id":"github:JetBrains:lets-plot","source_url":"https://github.com/JetBrains/lets-plot"},{"type":"has_code","target_id":"github:JetBrains:lets-plot","source_url":"https://github.com/JetBrains/lets-plot"},{"type":"has_code","target_id":"github:amueller:word_cloud\">wordcloud<","source_url":"https://github.com/amueller/word_cloud\">wordcloud<"},{"type":"has_code","target_id":"github:amueller:word_cloud","source_url":"https://github.com/amueller/word_cloud"},{"type":"has_code","target_id":"github:amueller:word_cloud","source_url":"https://github.com/amueller/word_cloud"},{"type":"has_code","target_id":"github:perspective-dev:perspective\">Perspective<","source_url":"https://github.com/perspective-dev/perspective\">Perspective<"},{"type":"has_code","target_id":"github:perspective-dev:perspective","source_url":"https://github.com/perspective-dev/perspective"},{"type":"has_code","target_id":"github:finos:perspective","source_url":"https://github.com/finos/perspective"},{"type":"has_code","target_id":"github:lmcinnes:umap\">UMAP<","source_url":"https://github.com/lmcinnes/umap\">UMAP<"},{"type":"has_code","target_id":"github:lmcinnes:umap","source_url":"https://github.com/lmcinnes/umap"},{"type":"has_code","target_id":"github:lmcinnes:umap","source_url":"https://github.com/lmcinnes/umap"},{"type":"has_code","target_id":"github:holoviz:hvplot\">hvPlot<","source_url":"https://github.com/holoviz/hvplot\">hvPlot<"},{"type":"has_code","target_id":"github:holoviz:hvplot","source_url":"https://github.com/holoviz/hvplot"},{"type":"has_code","target_id":"github:holoviz:hvplot","source_url":"https://github.com/holoviz/hvplot"},{"type":"has_code","target_id":"github:mpld3:mpld3\">mpld3<","source_url":"https://github.com/mpld3/mpld3\">mpld3<"},{"type":"has_code","target_id":"github:mpld3:mpld3","source_url":"https://github.com/mpld3/mpld3"},{"type":"has_code","target_id":"github:mpld3:mpld3","source_url":"https://github.com/mpld3/mpld3"},{"type":"has_code","target_id":"github:bqplot:bqplot\">bqplot<","source_url":"https://github.com/bqplot/bqplot\">bqplot<"},{"type":"has_code","target_id":"github:bqplot:bqplot","source_url":"https://github.com/bqplot/bqplot"},{"type":"has_code","target_id":"github:bqplot:bqplot","source_url":"https://github.com/bqplot/bqplot"},{"type":"has_code","target_id":"github:man-group:dtale\">D-Tale<","source_url":"https://github.com/man-group/dtale\">D-Tale<"},{"type":"has_code","target_id":"github:man-group:dtale","source_url":"https://github.com/man-group/dtale"},{"type":"has_code","target_id":"github:man-group:dtale","source_url":"https://github.com/man-group/dtale"},{"type":"has_code","target_id":"github:pavlin-policar:openTSNE\">openTSNE<","source_url":"https://github.com/pavlin-policar/openTSNE\">openTSNE<"},{"type":"has_code","target_id":"github:pavlin-policar:openTSNE","source_url":"https://github.com/pavlin-policar/openTSNE"},{"type":"has_code","target_id":"github:pavlin-policar:openTSNE","source_url":"https://github.com/pavlin-policar/openTSNE"},{"type":"has_code","target_id":"github:predict-idlab:plotly-resampler\">Plotly-Resampler<","source_url":"https://github.com/predict-idlab/plotly-resampler\">Plotly-Resampler<"},{"type":"has_code","target_id":"github:predict-idlab:plotly-resampler","source_url":"https://github.com/predict-idlab/plotly-resampler"},{"type":"has_code","target_id":"github:predict-idlab:plotly-resampler","source_url":"https://github.com/predict-idlab/plotly-resampler"},{"type":"has_code","target_id":"github:ContextLab:hypertools\">HyperTools<","source_url":"https://github.com/ContextLab/hypertools\">HyperTools<"},{"type":"has_code","target_id":"github:ContextLab:hypertools","source_url":"https://github.com/ContextLab/hypertools"},{"type":"has_code","target_id":"github:ContextLab:hypertools","source_url":"https://github.com/ContextLab/hypertools"},{"type":"has_code","target_id":"github:tensorflow:data-validation\">data-validation<","source_url":"https://github.com/tensorflow/data-validation\">data-validation<"},{"type":"has_code","target_id":"github:tensorflow:data-validation","source_url":"https://github.com/tensorflow/data-validation"},{"type":"has_code","target_id":"github:tensorflow:data-validation","source_url":"https://github.com/tensorflow/data-validation"},{"type":"has_code","target_id":"github:spotify:chartify\">Chartify<","source_url":"https://github.com/spotify/chartify\">Chartify<"},{"type":"has_code","target_id":"github:spotify:chartify","source_url":"https://github.com/spotify/chartify"},{"type":"has_code","target_id":"github:spotify:chartify","source_url":"https://github.com/spotify/chartify"},{"type":"has_code","target_id":"github:ing-bank:popmon\">Popmon<","source_url":"https://github.com/ing-bank/popmon\">Popmon<"},{"type":"has_code","target_id":"github:ing-bank:popmon","source_url":"https://github.com/ing-bank/popmon"},{"type":"has_code","target_id":"github:ing-bank:popmon","source_url":"https://github.com/ing-bank/popmon"},{"type":"has_code","target_id":"github:vega:ipyvega\">vega<","source_url":"https://github.com/vega/ipyvega\">vega<"},{"type":"has_code","target_id":"github:vega:ipyvega","source_url":"https://github.com/vega/ipyvega"},{"type":"has_code","target_id":"github:vega:ipyvega","source_url":"https://github.com/vega/ipyvega"},{"type":"has_code","target_id":"github:vega:vegafusion\">vegafusion<","source_url":"https://github.com/vega/vegafusion\">vegafusion<"},{"type":"has_code","target_id":"github:vega:vegafusion","source_url":"https://github.com/vega/vegafusion"},{"type":"has_code","target_id":"github:vegafusion:vegafusion","source_url":"https://github.com/vegafusion/vegafusion"},{"type":"has_code","target_id":"github:ResidentMario:missingno\">missingno<","source_url":"https://github.com/ResidentMario/missingno\">missingno<"},{"type":"has_code","target_id":"github:PAIR-code:facets\">Facets","source_url":"https://github.com/PAIR-code/facets\">Facets"},{"type":"has_code","target_id":"github:santosjorge:cufflinks\">Cufflinks<","source_url":"https://github.com/santosjorge/cufflinks\">Cufflinks<"},{"type":"has_code","target_id":"github:jupyter-widgets:pythreejs\">pythreejs<","source_url":"https://github.com/jupyter-widgets/pythreejs\">pythreejs<"},{"type":"has_code","target_id":"github:fbdesignpro:sweetviz\">Sweetviz<","source_url":"https://github.com/fbdesignpro/sweetviz\">Sweetviz<"},{"type":"has_code","target_id":"github:AutoViML:AutoViz\">AutoViz<","source_url":"https://github.com/AutoViML/AutoViz\">AutoViz<"},{"type":"has_code","target_id":"github:tpvasconcelos:ridgeplot\">ridgeplot<","source_url":"https://github.com/tpvasconcelos/ridgeplot\">ridgeplot<"},{"type":"has_code","target_id":"github:adamerose:PandasGUI\">PandasGUI<","source_url":"https://github.com/adamerose/PandasGUI\">PandasGUI<"},{"type":"has_code","target_id":"github:facebookresearch:hiplot\">HiPlot<","source_url":"https://github.com/facebookresearch/hiplot\">HiPlot<"},{"type":"has_code","target_id":"github:marcharper:python-ternary\">python-ternary<","source_url":"https://github.com/marcharper/python-ternary\">python-ternary<"},{"type":"has_code","target_id":"github:DmitryUlyanov:Multicore-TSNE\">Multicore-TSNE<","source_url":"https://github.com/DmitryUlyanov/Multicore-TSNE\">Multicore-TSNE<"},{"type":"has_code","target_id":"github:PatrikHlobil:Pandas-Bokeh\">Pandas-Bokeh<","source_url":"https://github.com/PatrikHlobil/Pandas-Bokeh\">Pandas-Bokeh<"},{"type":"has_code","target_id":"github:nicolaskruchten:jupyter_pivottablejs\">pivottablejs<","source_url":"https://github.com/nicolaskruchten/jupyter_pivottablejs\">pivottablejs<"},{"type":"has_code","target_id":"github:leotac:joypy\">joypy<","source_url":"https://github.com/leotac/joypy\">joypy<"},{"type":"has_code","target_id":"github:gyli:PyWaffle\">PyWaffle<","source_url":"https://github.com/gyli/PyWaffle\">PyWaffle<"},{"type":"has_code","target_id":"github:sosuneko:PDPbox\">PDPbox<","source_url":"https://github.com/sosuneko/PDPbox\">PDPbox<"},{"type":"has_code","target_id":"github:t-makaro:animatplot\">animatplot<","source_url":"https://github.com/t-makaro/animatplot\">animatplot<"},{"type":"has_code","target_id":"github:beringresearch:ivis\">ivis<","source_url":"https://github.com/beringresearch/ivis\">ivis<"},{"type":"has_code","target_id":"github:altair-viz:pdvega\">pdvega<","source_url":"https://github.com/altair-viz/pdvega\">pdvega<"},{"type":"has_code","target_id":"github:Zsailer:nx_altair\">nx-altair<","source_url":"https://github.com/Zsailer/nx_altair\">nx-altair<"},{"type":"has_code","target_id":"github:data-describe:data-describe\">data-describe<","source_url":"https://github.com/data-describe/data-describe\">data-describe<"},{"type":"has_code","target_id":"github:biovault:nptsne\">nptsne<","source_url":"https://github.com/biovault/nptsne\">nptsne<"},{"type":"has_code","target_id":"github:huggingface:transformers\">transformers<","source_url":"https://github.com/huggingface/transformers\">transformers<"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:nltk:nltk\">nltk<","source_url":"https://github.com/nltk/nltk\">nltk<"},{"type":"has_code","target_id":"github:nltk:nltk","source_url":"https://github.com/nltk/nltk"},{"type":"has_code","target_id":"github:nltk:nltk","source_url":"https://github.com/nltk/nltk"},{"type":"has_code","target_id":"github:BerriAI:litellm\">litellm<","source_url":"https://github.com/BerriAI/litellm\">litellm<"},{"type":"has_code","target_id":"github:BerriAI:litellm","source_url":"https://github.com/BerriAI/litellm"},{"type":"has_code","target_id":"github:BerriAI:litellm","source_url":"https://github.com/BerriAI/litellm"},{"type":"has_code","target_id":"github:explosion:spaCy\">spaCy<","source_url":"https://github.com/explosion/spaCy\">spaCy<"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:huggingface:sentence-transformers\">sentence-transformers<","source_url":"https://github.com/huggingface/sentence-transformers\">sentence-transformers<"},{"type":"has_code","target_id":"github:huggingface:sentence-transformers","source_url":"https://github.com/huggingface/sentence-transformers"},{"type":"has_code","target_id":"github:UKPLab:sentence-transformers","source_url":"https://github.com/UKPLab/sentence-transformers"},{"type":"has_code","target_id":"github:piskvorky:gensim\">gensim<","source_url":"https://github.com/piskvorky/gensim\">gensim<"},{"type":"has_code","target_id":"github:piskvorky:gensim","source_url":"https://github.com/piskvorky/gensim"},{"type":"has_code","target_id":"github:RaRe-Technologies:gensim","source_url":"https://github.com/RaRe-Technologies/gensim"},{"type":"has_code","target_id":"github:google:sentencepiece\">sentencepiece<","source_url":"https://github.com/google/sentencepiece\">sentencepiece<"},{"type":"has_code","target_id":"github:google:sentencepiece","source_url":"https://github.com/google/sentencepiece"},{"type":"has_code","target_id":"github:google:sentencepiece","source_url":"https://github.com/google/sentencepiece"},{"type":"has_code","target_id":"github:huggingface:tokenizers\">Tokenizers<","source_url":"https://github.com/huggingface/tokenizers\">Tokenizers<"},{"type":"has_code","target_id":"github:huggingface:tokenizers","source_url":"https://github.com/huggingface/tokenizers"},{"type":"has_code","target_id":"github:huggingface:tokenizers","source_url":"https://github.com/huggingface/tokenizers"},{"type":"has_code","target_id":"github:NVIDIA-NeMo:NeMo\">NeMo<","source_url":"https://github.com/NVIDIA-NeMo/NeMo\">NeMo<"},{"type":"has_code","target_id":"github:NVIDIA-NeMo:NeMo","source_url":"https://github.com/NVIDIA-NeMo/NeMo"},{"type":"has_code","target_id":"github:NVIDIA:NeMo","source_url":"https://github.com/NVIDIA/NeMo"},{"type":"has_code","target_id":"github:deepset-ai:haystack\">haystack<","source_url":"https://github.com/deepset-ai/haystack\">haystack<"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:comet-ml:opik\">Opik<","source_url":"https://github.com/comet-ml/opik\">Opik<"},{"type":"has_code","target_id":"github:comet-ml:opik","source_url":"https://github.com/comet-ml/opik"},{"type":"has_code","target_id":"github:comet-ml:opik","source_url":"https://github.com/comet-ml/opik"},{"type":"has_code","target_id":"github:gunthercox:ChatterBot\">ChatterBot<","source_url":"https://github.com/gunthercox/ChatterBot\">ChatterBot<"},{"type":"has_code","target_id":"github:gunthercox:ChatterBot","source_url":"https://github.com/gunthercox/ChatterBot"},{"type":"has_code","target_id":"github:gunthercox:ChatterBot","source_url":"https://github.com/gunthercox/ChatterBot"},{"type":"has_code","target_id":"github:flairNLP:flair\">flair<","source_url":"https://github.com/flairNLP/flair\">flair<"},{"type":"has_code","target_id":"github:flairNLP:flair","source_url":"https://github.com/flairNLP/flair"},{"type":"has_code","target_id":"github:flairNLP:flair","source_url":"https://github.com/flairNLP/flair"},{"type":"has_code","target_id":"github:sloria:TextBlob\">TextBlob<","source_url":"https://github.com/sloria/TextBlob\">TextBlob<"},{"type":"has_code","target_id":"github:sloria:TextBlob","source_url":"https://github.com/sloria/TextBlob"},{"type":"has_code","target_id":"github:sloria:TextBlob","source_url":"https://github.com/sloria/TextBlob"},{"type":"has_code","target_id":"github:facebookresearch:fairseq\">fairseq<","source_url":"https://github.com/facebookresearch/fairseq\">fairseq<"},{"type":"has_code","target_id":"github:facebookresearch:fairseq","source_url":"https://github.com/facebookresearch/fairseq"},{"type":"has_code","target_id":"github:facebookresearch:fairseq","source_url":"https://github.com/facebookresearch/fairseq"},{"type":"has_code","target_id":"github:stanfordnlp:stanza\">stanza<","source_url":"https://github.com/stanfordnlp/stanza\">stanza<"},{"type":"has_code","target_id":"github:stanfordnlp:stanza","source_url":"https://github.com/stanfordnlp/stanza"},{"type":"has_code","target_id":"github:stanfordnlp:stanza","source_url":"https://github.com/stanfordnlp/stanza"},{"type":"has_code","target_id":"github:qdrant:qdrant\">qdrant<","source_url":"https://github.com/qdrant/qdrant\">qdrant<"},{"type":"has_code","target_id":"github:qdrant:qdrant","source_url":"https://github.com/qdrant/qdrant"},{"type":"has_code","target_id":"github:qdrant:qdrant","source_url":"https://github.com/qdrant/qdrant"},{"type":"has_code","target_id":"github:JohnSnowLabs:spark-nlp\">spark-nlp<","source_url":"https://github.com/JohnSnowLabs/spark-nlp\">spark-nlp<"},{"type":"has_code","target_id":"github:JohnSnowLabs:spark-nlp","source_url":"https://github.com/JohnSnowLabs/spark-nlp"},{"type":"has_code","target_id":"github:JohnSnowLabs:spark-nlp","source_url":"https://github.com/JohnSnowLabs/spark-nlp"},{"type":"has_code","target_id":"github:RasaHQ:rasa\">Rasa<","source_url":"https://github.com/RasaHQ/rasa\">Rasa<"},{"type":"has_code","target_id":"github:RasaHQ:rasa","source_url":"https://github.com/RasaHQ/rasa"},{"type":"has_code","target_id":"github:RasaHQ:rasa","source_url":"https://github.com/RasaHQ/rasa"},{"type":"has_code","target_id":"github:tensorflow:text\">TensorFlow","source_url":"https://github.com/tensorflow/text\">TensorFlow"},{"type":"has_code","target_id":"github:tensorflow:text","source_url":"https://github.com/tensorflow/text"},{"type":"has_code","target_id":"github:tensorflow:text","source_url":"https://github.com/tensorflow/text"},{"type":"has_code","target_id":"github:snowballstem:snowball\">snowballstemmer<","source_url":"https://github.com/snowballstem/snowball\">snowballstemmer<"},{"type":"has_code","target_id":"github:snowballstem:snowball","source_url":"https://github.com/snowballstem/snowball"},{"type":"has_code","target_id":"github:snowballstem:snowball","source_url":"https://github.com/snowballstem/snowball"},{"type":"has_code","target_id":"github:pytorch:text\">torchtext<","source_url":"https://github.com/pytorch/text\">torchtext<"},{"type":"has_code","target_id":"github:pytorch:text","source_url":"https://github.com/pytorch/text"},{"type":"has_code","target_id":"github:pytorch:text","source_url":"https://github.com/pytorch/text"},{"type":"has_code","target_id":"github:jamesturk:jellyfish\">jellyfish<","source_url":"https://github.com/jamesturk/jellyfish\">jellyfish<"},{"type":"has_code","target_id":"github:jamesturk:jellyfish","source_url":"https://github.com/jamesturk/jellyfish"},{"type":"has_code","target_id":"github:jamesturk:jellyfish","source_url":"https://github.com/jamesturk/jellyfish"},{"type":"has_code","target_id":"github:deeppavlov:DeepPavlov\">DeepPavlov<","source_url":"https://github.com/deeppavlov/DeepPavlov\">DeepPavlov<"},{"type":"has_code","target_id":"github:deeppavlov:DeepPavlov","source_url":"https://github.com/deeppavlov/DeepPavlov"},{"type":"has_code","target_id":"github:deepmipt:DeepPavlov","source_url":"https://github.com/deepmipt/DeepPavlov"},{"type":"has_code","target_id":"github:rspeer:python-ftfy\">ftfy<","source_url":"https://github.com/rspeer/python-ftfy\">ftfy<"},{"type":"has_code","target_id":"github:rspeer:python-ftfy","source_url":"https://github.com/rspeer/python-ftfy"},{"type":"has_code","target_id":"github:rspeer:python-ftfy","source_url":"https://github.com/rspeer/python-ftfy"},{"type":"has_code","target_id":"github:allenai:scispacy\">SciSpacy<","source_url":"https://github.com/allenai/scispacy\">SciSpacy<"},{"type":"has_code","target_id":"github:allenai:scispacy","source_url":"https://github.com/allenai/scispacy"},{"type":"has_code","target_id":"github:allenai:scispacy","source_url":"https://github.com/allenai/scispacy"},{"type":"has_code","target_id":"github:cltk:cltk\">CLTK<","source_url":"https://github.com/cltk/cltk\">CLTK<"},{"type":"has_code","target_id":"github:cltk:cltk","source_url":"https://github.com/cltk/cltk"},{"type":"has_code","target_id":"github:cltk:cltk","source_url":"https://github.com/cltk/cltk"},{"type":"has_code","target_id":"github:dwyl:english-words\">english-words<","source_url":"https://github.com/dwyl/english-words\">english-words<"},{"type":"has_code","target_id":"github:dwyl:english-words","source_url":"https://github.com/dwyl/english-words"},{"type":"has_code","target_id":"github:dwyl:english-words","source_url":"https://github.com/dwyl/english-words"},{"type":"has_code","target_id":"github:argilla-io:argilla\">rubrix<","source_url":"https://github.com/argilla-io/argilla\">rubrix<"},{"type":"has_code","target_id":"github:argilla-io:argilla","source_url":"https://github.com/argilla-io/argilla"},{"type":"has_code","target_id":"github:recognai:rubrix","source_url":"https://github.com/recognai/rubrix"},{"type":"has_code","target_id":"github:dedupeio:dedupe\">Dedupe<","source_url":"https://github.com/dedupeio/dedupe\">Dedupe<"},{"type":"has_code","target_id":"github:dedupeio:dedupe","source_url":"https://github.com/dedupeio/dedupe"},{"type":"has_code","target_id":"github:dedupeio:dedupe","source_url":"https://github.com/dedupeio/dedupe"},{"type":"has_code","target_id":"github:life4:textdistance\">TextDistance<","source_url":"https://github.com/life4/textdistance\">TextDistance<"},{"type":"has_code","target_id":"github:life4:textdistance","source_url":"https://github.com/life4/textdistance"},{"type":"has_code","target_id":"github:life4:textdistance","source_url":"https://github.com/life4/textdistance"},{"type":"has_code","target_id":"github:explosion:spacy-transformers\">spacy-transformers<","source_url":"https://github.com/explosion/spacy-transformers\">spacy-transformers<"},{"type":"has_code","target_id":"github:explosion:spacy-transformers","source_url":"https://github.com/explosion/spacy-transformers"},{"type":"has_code","target_id":"github:explosion:spacy-transformers","source_url":"https://github.com/explosion/spacy-transformers"},{"type":"has_code","target_id":"github:unitaryai:detoxify\">detoxify<","source_url":"https://github.com/unitaryai/detoxify\">detoxify<"},{"type":"has_code","target_id":"github:unitaryai:detoxify","source_url":"https://github.com/unitaryai/detoxify"},{"type":"has_code","target_id":"github:unitaryai:detoxify","source_url":"https://github.com/unitaryai/detoxify"},{"type":"has_code","target_id":"github:JasonKessler:scattertext\">scattertext<","source_url":"https://github.com/JasonKessler/scattertext\">scattertext<"},{"type":"has_code","target_id":"github:JasonKessler:scattertext","source_url":"https://github.com/JasonKessler/scattertext"},{"type":"has_code","target_id":"github:JasonKessler:scattertext","source_url":"https://github.com/JasonKessler/scattertext"},{"type":"has_code","target_id":"github:google-research:text-to-text-transfer-transformer\">T5<","source_url":"https://github.com/google-research/text-to-text-transfer-transformer\">T5<"},{"type":"has_code","target_id":"github:google-research:text-to-text-transfer-transformer","source_url":"https://github.com/google-research/text-to-text-transfer-transformer"},{"type":"has_code","target_id":"github:google-research:text-to-text-transfer-transformer","source_url":"https://github.com/google-research/text-to-text-transfer-transformer"},{"type":"has_code","target_id":"github:zjunlp:DeepKE\">DeepKE<","source_url":"https://github.com/zjunlp/DeepKE\">DeepKE<"},{"type":"has_code","target_id":"github:zjunlp:DeepKE","source_url":"https://github.com/zjunlp/DeepKE"},{"type":"has_code","target_id":"github:zjunlp:deepke","source_url":"https://github.com/zjunlp/deepke"},{"type":"has_code","target_id":"github:explosion:sense2vec\">sense2vec<","source_url":"https://github.com/explosion/sense2vec\">sense2vec<"},{"type":"has_code","target_id":"github:explosion:sense2vec","source_url":"https://github.com/explosion/sense2vec"},{"type":"has_code","target_id":"github:explosion:sense2vec","source_url":"https://github.com/explosion/sense2vec"},{"type":"has_code","target_id":"github:IndicoDataSolutions:finetune\">finetune<","source_url":"https://github.com/IndicoDataSolutions/finetune\">finetune<"},{"type":"has_code","target_id":"github:IndicoDataSolutions:finetune","source_url":"https://github.com/IndicoDataSolutions/finetune"},{"type":"has_code","target_id":"github:IndicoDataSolutions:finetune","source_url":"https://github.com/IndicoDataSolutions/finetune"},{"type":"has_code","target_id":"github:EricFillion:happy-transformer\">happy-transformer<","source_url":"https://github.com/EricFillion/happy-transformer\">happy-transformer<"},{"type":"has_code","target_id":"github:EricFillion:happy-transformer","source_url":"https://github.com/EricFillion/happy-transformer"},{"type":"has_code","target_id":"github:EricFillion:happy-transformer","source_url":"https://github.com/EricFillion/happy-transformer"},{"type":"has_code","target_id":"github:awslabs:sockeye\">Sockeye<","source_url":"https://github.com/awslabs/sockeye\">Sockeye<"},{"type":"has_code","target_id":"github:awslabs:sockeye","source_url":"https://github.com/awslabs/sockeye"},{"type":"has_code","target_id":"github:awslabs:sockeye","source_url":"https://github.com/awslabs/sockeye"},{"type":"has_code","target_id":"github:unum-cloud:UForm\">UForm<","source_url":"https://github.com/unum-cloud/UForm\">UForm<"},{"type":"has_code","target_id":"github:unum-cloud:UForm","source_url":"https://github.com/unum-cloud/UForm"},{"type":"has_code","target_id":"github:unum-cloud:uform","source_url":"https://github.com/unum-cloud/uform"},{"type":"has_code","target_id":"github:webis-de:small-text\">small-text<","source_url":"https://github.com/webis-de/small-text\">small-text<"},{"type":"has_code","target_id":"github:webis-de:small-text","source_url":"https://github.com/webis-de/small-text"},{"type":"has_code","target_id":"github:webis-de:small-text","source_url":"https://github.com/webis-de/small-text"},{"type":"has_code","target_id":"github:dsfsi:textaugment\">textaugment<","source_url":"https://github.com/dsfsi/textaugment\">textaugment<"},{"type":"has_code","target_id":"github:dsfsi:textaugment","source_url":"https://github.com/dsfsi/textaugment"},{"type":"has_code","target_id":"github:dsfsi:textaugment","source_url":"https://github.com/dsfsi/textaugment"},{"type":"has_code","target_id":"github:facebookresearch:vizseq\">VizSeq<","source_url":"https://github.com/facebookresearch/vizseq\">VizSeq<"},{"type":"has_code","target_id":"github:facebookresearch:vizseq","source_url":"https://github.com/facebookresearch/vizseq"},{"type":"has_code","target_id":"github:facebookresearch:vizseq","source_url":"https://github.com/facebookresearch/vizseq"},{"type":"has_code","target_id":"github:allenai:allennlp\">AllenNLP<","source_url":"https://github.com/allenai/allennlp\">AllenNLP<"},{"type":"has_code","target_id":"github:facebookresearch:fastText\">fastText<","source_url":"https://github.com/facebookresearch/fastText\">fastText<"},{"type":"has_code","target_id":"github:OpenNMT:OpenNMT-py\">OpenNMT<","source_url":"https://github.com/OpenNMT/OpenNMT-py\">OpenNMT<"},{"type":"has_code","target_id":"github:facebookresearch:ParlAI\">ParlAI<","source_url":"https://github.com/facebookresearch/ParlAI\">ParlAI<"},{"type":"has_code","target_id":"github:seatgeek:fuzzywuzzy\">fuzzywuzzy<","source_url":"https://github.com/seatgeek/fuzzywuzzy\">fuzzywuzzy<"},{"type":"has_code","target_id":"github:miso-belica:sumy\">Sumy<","source_url":"https://github.com/miso-belica/sumy\">Sumy<"},{"type":"has_code","target_id":"github:undertheseanlp:underthesea\">underthesea<","source_url":"https://github.com/undertheseanlp/underthesea\">underthesea<"},{"type":"has_code","target_id":"github:makcedward:nlpaug\">nlpaug<","source_url":"https://github.com/makcedward/nlpaug\">nlpaug<"},{"type":"has_code","target_id":"github:cjhutto:vaderSentiment\">vaderSentiment<","source_url":"https://github.com/cjhutto/vaderSentiment\">vaderSentiment<"},{"type":"has_code","target_id":"github:chartbeat-labs:textacy\">textacy<","source_url":"https://github.com/chartbeat-labs/textacy\">textacy<"},{"type":"has_code","target_id":"github:DerwenAI:pytextrank\">PyTextRank<","source_url":"https://github.com/DerwenAI/pytextrank\">PyTextRank<"},{"type":"has_code","target_id":"github:bee-san:Ciphey\">Ciphey<","source_url":"https://github.com/bee-san/Ciphey\">Ciphey<"},{"type":"has_code","target_id":"github:fastnlp:fastNLP\">fastNLP<","source_url":"https://github.com/fastnlp/fastNLP\">fastNLP<"},{"type":"has_code","target_id":"github:aboSamoor:polyglot\">polyglot<","source_url":"https://github.com/aboSamoor/polyglot\">polyglot<"},{"type":"has_code","target_id":"github:vi3k6i5:flashtext\">flashtext<","source_url":"https://github.com/vi3k6i5/flashtext\">flashtext<"},{"type":"has_code","target_id":"github:saffsd:langid.py\">langid<","source_url":"https://github.com/saffsd/langid.py\">langid<"},{"type":"has_code","target_id":"github:nipunsadvilkar:pySBD\">pySBD<","source_url":"https://github.com/nipunsadvilkar/pySBD\">pySBD<"},{"type":"has_code","target_id":"github:huggingface:neuralcoref\">neuralcoref<","source_url":"https://github.com/huggingface/neuralcoref\">neuralcoref<"},{"type":"has_code","target_id":"github:dmlc:gluon-nlp\">GluonNLP<","source_url":"https://github.com/dmlc/gluon-nlp\">GluonNLP<"}]', NULL, 'CC-BY-SA-4.0', 'approved', 80, '8533104a823a49264fc8c7b17b4f17b2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-lukasmasuch-best-of-ml-python from https://github.com/lukasmasuch.png
Image converted to WebP: data/images/github-lukasmasuch-best-of-ml-python.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-spmallick-learnopencv', 'github--spmallick--learnopencv', 'learnopencv', 'spmallick', 'This repository contains code for Computer Vision, Deep learning, and AI research articles shared on our blog LearnOpenCV.com. Want to become an expert in AI? AI Courses by OpenCV is a great place to start. <a href="https://opencv.org/courses/"> <p align="center"> <img src="https://learnopencv.com/wp-content/uploads/2023/01/AI-Courses-By-OpenCV-Github.png"> </p> </a> | Blog Post | Code| | ------------- |:-------------| | SAM-3: Whatâ€™s New, How It Works, and Why It Matters | Code | | Image-GS:...', '["ai","computer-vision","computervision","deep-learning","deep-neural-networks","deeplearning","machine-learning","opencv","opencv-cpp","opencv-library","opencv-python","opencv-tutorial","opencv3","jupyter notebook"]', 'other', 22546, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/spmallick/learnopencv","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# LearnOpenCV\n\nThis repository contains code for Computer Vision, Deep learning, and AI research articles shared on our blog [LearnOpenCV.com](https://www.LearnOpenCV.com).\n\nWant to become an expert in AI? [AI Courses by OpenCV](https://opencv.org/courses/) is a great place to start.\n\n<a href="https://opencv.org/courses/">\n\n<p align="center">\n<img src="https://learnopencv.com/wp-content/uploads/2023/01/AI-Courses-By-OpenCV-Github.png">\n</p>\n</a>\n\n## List of Blog Posts\n\n| Blog Post | Code|\n| ------------- |:-------------|\n| [SAM-3: Whatâ€™s New, How It Works, and Why It Matters](https://learnopencv.com/sam-3-whats-new/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SAM-3) |\n| [Image-GS: Adaptive Image Reconstruction using 2D Gaussians](https://learnopencv.com/image-gs-image-reconstruction-using-2d-gaussians/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image_GS_Adaptive_Image_Reconstruction_using_2D_Gaussians) |\n| [Ultimate Guide to Vector Databases and RAG Pipeline](https://learnopencv.com/vector-db-and-rag-pipeline-for-document-rag/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Ultimate_Guide_to_Vector_Databases_and_RAG_pipeline) |\n|[What Makes DeepSeek OCR So Powerful](https://learnopencv.com/what-makes-deepseek-ocr-so-powerful/)|[Code](https://github.com/spmallick/learnopencv/tree/master/What-Makes-DeepSeek-OCR-So-Powerful)|\n| [2D Gaussian Splatting: Geometrically Accurate Radiance Field Reconstruction](https://learnopencv.com/2d-gaussian-splatting-2dgs/) | [Code](https://github.com/spmallick/learnopencv/tree/master/2D_Gaussian_Splatting_Geometrically_Accurate_Radiance_Field_Reconstruction) |\n| [TRM: Tiny Recursive Models](https://learnopencv.com/trm-tiny-ai-models-outsmarting-giants-on-complex-puzzles/) | [Code](https://github.com/spmallick/learnopencv/tree/master/TRM) |\n|[Deploying ML Models on Arduino: From Blink to Think]()|[Code](https://github.com/spmallick/learnopencv/tree/master/Deploying-ML-Models-on-Arduino-From-Blink-to-Think)|\n| [VideoRAG: Redefining Long-Context Video Comprehension](https://learnopencv.com/videorag-long-context-video-comprehension/) | |\n| [AI Agent in Action: Automating Desktop Tasks with VLMs](https://learnopencv.com/build-ai-agents-using-vlm/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Local-VLM-Agents-in-Action-GUI-Automation-with-Moondream3-and-Gemini) |\n| [Top VLM Evaluation Metrics for Optimal Performance Analysis](https://learnopencv.com/vlm-evaluation-metrics/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VLM_Evaluation_Metrics) |\n|[Getting Started with VLM on Jetson Nano](https://learnopencv.com/vlm-on-jetson-nano/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Getting-Started-with-VLM-on-Jetson-Nano)|\n| [VLM on Edge: Worth the Hype or Just a Novelty?](https://learnopencv.com/vlm-on-edge-devices/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VLM-on-Edge-Worth-the-Hype-or-Just-a-Novelty) |\n| [AnomalyCLIP : Harnessing CLIP for Weakly-Supervised Video Anomaly Recognition](https://learnopencv.com/anomalyclip-video-anomaly-recognition/) | [Code](https://github.com/spmallick/learnopencv/tree/master/AnomalyCLIP_Harnessing_CLIP_for_Weakly_Supervised_Video_Anomaly_Recognition) |\n| [AI_for_Video_Understanding_From_Content_Moderation_to_Summarization](https://learnopencv.com/ai-for-video-understanding/) | [Code](https://github.com/spmallick/learnopencv/tree/master/AI_for_Video_Understanding_From_Content_Moderation_to_Summarization) |\n| [Video-RAG: Training-Free Retrieval for Long-Video LVLMs](https://learnopencv.com/video-rag-for-long-videos/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Video-RAG_Training_Free_Retrieval_for_Long_Video_LVLMs) |\n| [Object Detection and Spatial Understanding with VLMs ft. Qwen2.5-VL](https://learnopencv.com/object-detection-with-vlms-ft-qwen2-5-vl/) | [Code](https://github.com/spmallick/learnopencv/tree/master/object-detection-with-vlms) |\n| [LangGraph: Building Self-Correcting RAG Agent for Code Generation](https://learnopencv.com/langgraph-self-correcting-agent-code-generation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/LangGraph_Building_Self_Correcting_RAG_Agent_for_Code_Generation) |\n| [Inside Sinusoidal Position Embeddings: A Sense of Order](https://learnopencv.com/sinusoidal-position-embeddings/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Sinusoidal_Position_Embeddings) |\n| [Inside RoPE: Rotary Magic into Position Embeddings](https://learnopencv.com/rope-position-embeddings/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Inside_RoPE_Position_Embeddings) |\n| [SimLingo-Vision-Language-Action-Model-for-Autonomous-Driving](https://learnopencv.com/simlingo-vision-language-action-model-for-autonomous-driving/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SimLingo-Vision-Language-Action-Model-for-Autonomous-Driving) |\n| [FineTuning Gemma 3n for Medical VQA on ROCOv2](https://learnopencv.com/finetuning-gemma-3n-medical-vqa/) | [Code](https://github.com/spmallick/learnopencv/tree/master/finetuning-gemma3n) |\n| [SmolLM3 Blueprint: SOTA 3B-Parameter LLM](https://learnopencv.com/smollm3-explained/) | |\n| [LangGraph-A-Visual-Automation-and-Summarization-Pipeline](https://learnopencv.com/langgraph-building-a-visual-web-browser-agent/) | [Code](https://github.com/spmallick/learnopencv/tree/master/LangGraph-A-Visual-Automation-and-Summarization-Pipeline) |\n| [Fine-Tuning AnomalyCLIP: Class-Agnostic Zero-Shot Anomaly Detection](https://learnopencv.com/fine-tuning-anomalyclip-medical-anomaly-clip/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-AnomalyCLIP) |\n| [SigLIP 2: DeepMindâ€™s Multilingual Vision-Language Model](https://learnopencv.com/siglip-2-deepminds-multilingual-vision-language-model/) | |\n| [MedGemma: Googleâ€™s Medico VLM for Clinical QA, Imaging, and More](https://learnopencv.com/medgemma-explained/) | [Code](https://github.com/spmallick/learnopencv/tree/master/medgemma) |\n| [Nanonets-OCR-s: Enabling Rich, Structured Markdown for Document Understanding](https://learnopencv.com/nanonets-ocr-s/) | |\n| [Optimizing VJEPA-2: Tackling Latency & Context in Real-Time Video Classification Scripts](https://learnopencv.com/optimizing-vjepa-2-in-real-time-video-classification/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VJEPA-2-Video-Classification) |\n| [V-JEPA 2: Metaâ€™s Breakthrough in AI for the Physical World](https://learnopencv.com/?p=73731&preview_id=73731&preview_nonce=beb70ccf8e&preview=true#heading-7) | [Code](https://github.com/spmallick/learnopencv/tree/master/V-JEPA-2) |\n| [NVIDIA Cosmos Reason1: Video Understanding](https://learnopencv.com/cosmos-reason-vlm-video-vqa/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Cosmos-Reason1-Video-Understanding) |\n| [GR00T N1.5 Explained](https://learnopencv.com/gr00t-n1_5-explained/) |  |\n| [LLaVA](https://learnopencv.com/llava-training-a-visual-assistant/) | [Code](https://github.com/spmallick/learnopencv/tree/master/LLaVA) |\n| [SmolVLA: Affordable & Efficient VLA Robotics on Consumer GPUs](https://learnopencv.com/smolvla-lerobot-vision-language-action-model/) | [Code](https://github.com/spmallick/learnopencv/tree/master/smolvla) |\n| [Fine-Tuning Grounding DINO: Open-Vocabulary Object Detection](https://learnopencv.com/fine-tuning-grounding-dino/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-Grounding-DINO-Open-Vocabulary-Object-Detection) |\n| [Getting Started with Qwen3 â€“ The Thinking Expert](https://learnopencv.com/qwen3/) | [Code](https://github.com/spmallick/learnopencv/tree/master/qwen3) |\n| [Inside the GPU: A Comprehensive Guide to Modern Graphics Architecture](https://learnopencv.com/modern-gpu-architecture-explained/) | |\n| [Distributed Parallel Training: PyTorch](https://learnopencv.com/distributed-parallel-training-pytorch-multi-gpu-setup/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Distributed-Training-PyTorch) |\n| [MONAI: The Definitive Framework for Medical Imaging Powered by PyTorch](https://learnopencv.com/monai-medical-imaging-pytorch/) | |\n| [SANA-Sprint: The One-Step Revolution in High-Quality AI Image Synthesis](https://learnopencv.com/sana-sprint-the-one-step-revolution-in-high-quality-ai-image-synthesis/) | |\n| [FramePack-Video-Diffusion-but-feels-like-Image-Diffusion](https://learnopencv.com/framepack-video-diffusion/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FramePack-Video-Diffusion-but-feels-like-Image-Diffusion) |\n| [Model Weights File Formats in Machine Learning](https://learnopencv.com/model-weights-file-formats-in-machine-learning/) | |\n| [Unsloth: A Guide from Basics to Fine-Tuning Vision Models](https://learnopencv.com/unsloth-guide-efficient-llm-fine-tuning/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Unsloth_A_Guide_From_Basics_to_Fine_Tuning_Vision_Models) |\n| [Iterative Closest Point (ICP) Algorithm Explained](https://learnopencv.com/iterative-closest-point-icp-explained/) | [Code](https://github.com/spmallick/learnopencv/blob/master/Iterative-Closest-Point-ICP) |\n| [MedSAM2 Explained: One Prompt to Segment Anything in Medical Imaging](https://learnopencv.com/medsam2-explained/) | [Code](https://github.com/spmallick/learnopencv/blob/master/medsam2-explained) |\n| [Batch Normalization and Dropout as Regularizers](https://learnopencv.com/batch-normalization-and-dropout-as-regularizers/) | |\n| [DINOv2_by_Meta_A_Self-Supervised_foundational_vision_model](https://learnopencv.com/dinov2-self-supervised-vision-transformer/) | [Code](https://github.com/spmallick/learnopencv/blob/master/DINOv2_by_Meta_A_Self-Supervised_foundational_vision_model) |\n| [Beginner''s Guide to Embedding Models](https://learnopencv.com/embedding-models-explained/) | |\n| [MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors](https://learnopencv.com/mast3r-slam-realtime-dense-slam-explained/) | [Code](https://github.com/spmallick/learnopencv/blob/master/MASt3R-SLAM) |\n| [Google''s A2A Protocol](https://learnopencv.com/googles-a2a-protocol-heres-what-you-need-to-know/) | |\n| [Nvidia SANA : Faster Image Generation](https://learnopencv.com/nvidia-sana-image-generation-model/) | |\n| [Fine-tuning RF-DETR](https://learnopencv.com/rf-detr-object-detection/) | [Code](https://github.com/spmallick/learnopencv/blob/master/Fine-tuning-RF-DETR) |\n| [Qwen2.5-Omni: A Real-Time Multimodal AI](https://learnopencv.com/qwen2.5-omni/) | |\n| [Vision Language Action Models: Robotic Control](https://learnopencv.com/vision-language-action-models-lerobot-policy/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Vision-Language-Action-Models) |\n| [Fine-Tuning Gemma 3 VLM using QLoRA for LaTeX-OCR Dataset](https://learnopencv.com/fine-tuning-gemma-3/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-Gemma-3-VLM-using-QLoRA-for-LaTeX-OCR-Dataset) |\n| [ComfyUI](https://learnopencv.com/introduction-to-comfyui-for-stable-diffusion/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ComfyUI) |\n| [Gemma-3: A Comprehensive Introduction](https://learnopencv.com/gemma-3/) | |\n| [YOLO11 on Raspberry Pi: Optimizing Object Detection for Edge Devices](https://learnopencv.com/yolo11-on-raspberry-pi/) | [Code](https://github.com/spmallick/learnopencv/tree/master/yolo11-on-raspberry-pi) |\n| [VGGT: Visual Geometry Grounded Transformer â€“ For Dense 3D Reconstruction](https://learnopencv.com/vggt-visual-geometry-grounded-transformer-3d-reconstruction/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VGGT-3D-Reconstruction) |\n| [DDIM: The Faster, Improved Version of DDPM for Efficient AI Image Generation](https://learnopencv.com/understanding-ddim/) | [Code](https://github.com/spmallick/learnopencv/tree/master/DDIM-The-Faster-Improved-Version-of-DDPM-for-Efficient-AI-Image-Generation) |\n| [Introduction to Model Context Protocol (MCP)](https://learnopencv.com/introduction-to-model-context-protocol/) | |\n| [MASt3R and MASt3R-SfM Explanation: Image Matching and 3D Reconstruction](https://learnopencv.com/mast3r-sfm-grounding-image-matching-3d/) | [Code](https://github.com/spmallick/learnopencv/tree/master/MASt3R-SfM-3D-Reconstruction-Image-Matching) |\n| [MatAnyone Explained: Consistent Memory for Better Video Matting](https://learnopencv.com/matanyone-for-better-video-matting/) | [Code](https://github.com/spmallick/learnopencv/tree/master/MatAnyone-Explained-Consistent-Memory-for-Better-Video-Matting) |\n| [GraphRAG: For Medical Document Analysis](https://learnopencv.com/graphrag-explained-knowledge-graphs-medical/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Graphrag-Medical-Document-Analysis) |\n| [OmniParser: Vision Based GUI Agent](https://learnopencv.com/omniparser-vision-based-gui-agent/) | |\n| [Fine-Tuning-YOLOv12-Comparison-With-YOLOv11-And-YOLOv7-Based-Darknet](https://learnopencv.com/fine-tuning-yolov12/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-YOLOv12-Comparison-With-YOLOv11-And-YOLOv7-Based-Darknet) |\n| [FineTuning RetinaNet for Wildlife Detection with PyTorch: A Step-by-Step Tutorial](https://learnopencv.com/finetuning-retinanet) | [Code](https://github.com/spmallick/learnopencv/tree/master/finetuning-retinanet) |\n| [DUSt3R: Geometric 3D Vision Made Easy :  Explanation and Results](https://learnopencv.com/dust3r-geometric-3d-vision/) | [Code](https://github.com/spmallick/learnopencv/tree/master/DUSt3R-Dense-3D-Reconstruction) |\n| [YOLOv12: Attention Meets Speed](https://learnopencv.com/yolov12) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv12) |\n| [Video Generation: A Diffusion based approach](https://learnopencv.com/video-generation-models/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Video-Generation-A-Diffusion-based-approach) |\n| [Agentic AI: A Comprehensive Introduction](https://learnopencv.com/agentic-ai/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Agentic-AI-A-Comprehensive-Introduction) |\n| [Finetuning SAM2 for Leaf Disease Segmentation](https://learnopencv.com/finetuning-sam2/) | [Code](https://github.com/spmallick/learnopencv/tree/master/finetuning-sam2) |\n| [Object Insertion in Gaussian Splatting: Paper Explained and Training Code for MCMC and Bilateral Grid](https://learnopencv.com/object-insertion-in-gaussian-splatting/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Object-Insertion-in-Gaussian-Splatting) |\n| [Depth Pro: Sharp Monocular Metric Depth](https://learnopencv.com/depth-pro-monocular-metric-depth) | [Code](https://github.com/spmallick/learnopencv/tree/master/DepthPro-Monocular-Metric-Depth) |\n| [Fine-tuning-Stable-Diffusion-3_5-UI-images](https://learnopencv.com/fine-tuning-stable-diffusion-3-5m/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-tuning-Stable-Diffusion-3_5-UI-images) |\n| [SimSiam: Streamlining SSL with Stop-Gradient Mechanism](https://learnopencv.com/simsiam/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SimSiam-Streamlining-SSL-with-Stop-Gradient-Mechanism) |\n| [Image Captioning using ResNet and LSTM](https://learnopencv.com/image-captioning/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-Captioning-using-ResNet-and-LSTM) |\n| [Molmo VLM: Paper Explanation and Demo](https://learnopencv.com/molmo-vlm) | [Code](https://github.com/spmallick/learnopencv/tree/master/Molmo-VLM-SAM2) |\n| [3D Gaussian Splatting Paper Explanation: Training Custom Datasets with NeRF-Studio Gsplats](https://learnopencv.com/3d-gaussian-splatting/) | [Code](https://github.com/spmallick/learnopencv/tree/master/3D-Gaussian-Splatting-Code) |\n| [FLUX Image Generation: Experimenting with the Parameters](https://learnopencv.com/flux-ai-image-generator/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Flux-Image-Generation) |\n| [Contrastive-Learning-SimCLR-and-BYOL(With Code Example)](https://learnopencv.com/contrastive-learning-simclr-and-byol-with-code-example/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Contrastive-Learning-SimCLR-and-BYOL) |\n| [The Annotated NeRF : Training on Custom Dataset from Scratch in Pytorch](https://learnopencv.com/annotated-nerf-pytorch/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Annotated-NeRF) |\n| [Stable Diffusion 3 and 3.5: Paper Explanation and Inference](https://learnopencv.com/stable-diffusion-3/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Stable-Diffusion-3) |\n| [LightRAG - Legal Document Analysis](https://learnopencv.com/lightrag/) | [Code](https://github.com/spmallick/learnopencv/tree/master/LightRAG-Legal) |\n| [NVIDIA AI Summit 2024 â€“ India Overview](https://learnopencv.com/nvidia-ai-summit-2024-india-overview/) | |\n| [Introduction to Speech to Speech: Most Efficient Form of NLP](https://learnopencv.com/speech-to-speech/) | [Code](https://github.com/spmallick/learnopencv/tree/master/speech-to-speech) |\n| [Training 3D U-Net for Brain Tumor Segmentation (BraTS-GLI)](https://learnopencv.com/3d-u-net-brats/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Training_3D_U-Net_Brain_Tumor_Seg) |\n| [DETR: Overview and Inference](https://learnopencv.com/detr-overview-and-inference/) | [Code](https://github.com/spmallick/learnopencv/tree/master/DETR-Overview_and_Inference) |\n| [YOLO11: Faster Than You Can Imagine!](https://learnopencv.com/yolo11/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLO11) |\n| [Exploring DINO: Self-Supervised Transformers for Road Segmentation with ResNet50 and U-Net](https://learnopencv.com/fine-tune-dino-self-supervised-learning-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Exploring-DINO-dino-road-segmentation) |\n| [Sapiens: Foundation for Human Vision Models by Meta](https://learnopencv.com/sapiens-human-vision-models) | [Code](https://github.com/spmallick/learnopencv/tree/master/Sapiens-Human-Vision-Model-Meta) |\n| [Multimodal RAG with ColPali and Gemini](https://learnopencv.com/multimodal-rag-with-colpali) | [Code](https://github.com/spmallick/learnopencv/tree/master/Multimodal-RAG-with-ColPali-Gemini) |\n| [Building Autonomous Vehicle in Carla: Path Following with PID Control & ROS 2](https://learnopencv.com/pid-controller-ros-2-carla/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Building_Autonomous_Vehicle_in_Carla_Path_Following_with_PID_Control_ROS2) |\n| [Handwritten Text Recognition using OCR](https://learnopencv.com/handwritten-text-recognition-using-ocr/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Handwritten_Text_Recognition_using_OCR) |\n| [Training CLIP from Sratch for Image Retrieval](https://learnopencv.com/clip-model) | [Code](https://github.com/spmallick/learnopencv/tree/master/Training-CLIP-from-Scratch-for-Image-Retrieval) |\n| [Introduction to LiDAR SLAM: LOAM and LeGO-LOAM Paper and Code Explanation with ROS 2 Implementation](https://learnopencv.com/lidar-slam-with-ros2) | [Code](https://github.com/spmallick/learnopencv/tree/master/LeGO-LOAM-ROS2) |\n| [Recommendation System using Vector Search](https://learnopencv.com/recommendation-system-using-vector-search) | [Code](https://github.com/spmallick/learnopencv/tree/master/Recommendation-System-using-Vector-Search) |\n| [Fine Tuning Whisper on Custom Dataset](https://learnopencv.com/fine-tuning-whisper-on-custom-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-Whisper-on-Custom-Dataset) |\n| [SAM 2 â€“ Promptable Segmentation for Images and Videos](https://learnopencv.com/sam-2/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SAM_2_Segment_Anything_Model_2) |\n| [Introduction to Feature Matching Using Neural Networks](https://learnopencv.com/feature-matching/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Feature-Matching-Using-Neural-Networks) |\n| [Introduction to ROS2 (Robot Operating System 2): Tutorial on ROS2 Working, DDS, ROS1 RMW, Topics, Nodes, Publisher, Subscriber in Python](https://learnopencv.com/robot-operating-system-introduction) | [Code](https://github.com/spmallick/learnopencv/tree/master/Introduction-to-ROS2-in-python) |\n| [CVPR 2024 Research Papers - Part- 2](https://learnopencv.com/cvpr-2024-research-papers) | [Code](https://github.com/spmallick/learnopencv/tree/master/cvpr-2024-research-papers-part2) |\n| [CVPR 2024: An Overview and Key Papers](https://learnopencv.com/cvpr2024/) | [Code](https://github.com/spmallick/learnopencv/tree/master/CVPR-2024) |\n| [Object Detection on Edge Device - OAK-D-Lite](https://learnopencv.com/object-detection-on-edge-device) | [Code](https://github.com/spmallick/learnopencv/tree/master/Object-Detection-on-Edge-Devices) |\n| [Fine-Tuning YOLOv10 Models on Custom Dataset](https://learnopencv.com/fine-tuning-yolov10/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-YOLOv10-Models-Custom-Dataset) |\n| [ROS2 and Carla Setup Guide for Ubuntu 22.04](https://learnopencv.com/ros2-and-carla-setup-guide/) |  |\n| [Understanding Visual SLAM for Robotics Perception: Building Monocular SLAM from Scratch in Python](https://learnopencv.com/monocular-slam-in-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Monocular%20SLAM%20for%20Robotics%20implementation%20in%20python) |\n| [Enhancing Image Segmentation using U2-Net: An Approach to Efficient Background Removal](https://learnopencv.com/u2-net-image-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Efficient-Background-Removal-using-U2-Net) |\n| [YOLOv10: The Dual-Head OG of YOLO Series](https://learnopencv.com/yolov10/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv10) |\n| [Fine-tuning Faster R-CNN on Sea Rescue Dataset](https://learnopencv.com/fine-tuning-faster-r-cnn/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-tuning-Faster-R-CNN-on-SeaRescue-Dataset) |\n| [Mastering Recommendation System: A Complete Guide](https://learnopencv.com/recommendation-system/) | |\n| [Automatic Speech Recognition with Diarization : Speech-to-Text](https://learnopencv.com/automatic-speech-recognition/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Automatic-Speech-Recognition-with-Diarization-Speech-to-Text) |\n| [Building MobileViT Image Classification Model from Scratch In Keras 3](https://learnopencv.com/mobilevit-keras-3/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Building%20MobileViT%20from%20Scratch%20in%20Keras%203) |\n| [SDXL Inpainting: Fusing Image Inpainting with Stable Diffusion](https://learnopencv.com/sdxl-inpainting/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SDXL-inpainting) |\n| [YOLOv9 Instance Segmentation on Medical Dataset](https://learnopencv.com/yolov9-instance-segmentation-on-medical-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv9-Instance-Segmentation-on-Medical-Dataset) |\n| [A Comprehensive Guide to Robotics](https://learnopencv.com/a-comprehensive-guide-to-robotics/) | |\n| [Integrating Gradio with OpenCV DNN](https://learnopencv.com/integrating-gradio-with-opencv-dnn/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Integrating-Gradio-with-OpenCV-DNN) |\n| [Fine-Tuning YOLOv9 on Custom Dataset](https://learnopencv.com/fine-tuning-yolov9/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-YOLOv9-Models-Custom-Dataset) |\n| [Dreambooth using Diffusers](https://learnopencv.com/dreambooth-using-diffusers/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Dreambooth_using_Diffusers) |\n| [Introduction to Hugging Face Diffusers](https://learnopencv.com/hugging-face-diffusers/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Introduction_to_Diffusers) |\n| [Introduction to Ultralytics Explorer API](https://learnopencv.com/ultralytics-explorer-api/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Introduction-to-Ultralytics-Explorer-API) |\n| [YOLOv9: Advancing the YOLO Legacy](https://learnopencv.com/yolov9-advancing-the-yolo-legacy/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv9-Advancing-the-YOLO-Legacy) |\n| [Fine-Tuning LLMs using PEFT](https://learnopencv.com/fine-tuning-llms-using-peft/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-LLMs-using-PEFT) |\n| [Depth Anything: Accelerating Monocular Depth Perception](https://learnopencv.com/deciphering-llms/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Depth-Anything) |\n| [Deciphering LLMs: From Transformers to Quantization](https://learnopencv.com/deciphering-llms/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Deciphering-LLMs) |\n| [YOLO Loss Function Part 2: GFL and VFL Loss](https://learnopencv.com/yolo-loss-function-gfl-vfl-loss/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLO-Loss-Functions-Part2) |\n| [YOLOv8-Object-Tracking-and-Counting-with-OpenCV](https://learnopencv.com/yolov8-object-tracking-and-counting-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv8-Object-Tracking-and-Counting-with-OpenCV) |\n| [Stereo Vision in ADAS: Pioneering Depth Perception Beyond LiDAR](https://learnopencv.com/adas-stereo-vision/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ADAS-Stereo-Vision) |\n| [YOLO Loss Function Part 1: SIoU and Focal Loss](https://learnopencv.com/yolo-loss-function-siou-focal-loss/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLO-Loss-Functions-Part1) |\n| [Moving Object Detection with OpenCV](https://learnopencv.com/moving-object-detection-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Moving-Object-Detection-with-OpenCV) |\n| [Integrating ADAS with Keypoint Feature Pyramid Network for 3D LiDAR Object Detection](https://learnopencv.com/3d-lidar-object-detection/) | [Code](https://www.dropbox.com/scl/fi/3n1s68jtfkjmw2f5e5ctv/3D-LiDAR-Object-Detection.zip?rlkey=d8q6xvlxis4oxso4qki87omvc&dl=1) |\n| [Mastering All YOLO Models from YOLOv1 to YOLO-NAS: Papers Explained (2024)](https://learnopencv.com/mastering-all-yolo-models) | |\n| [GradCAM: Enhancing Neural Network Interpretability in the Realm of Explainable AI](https://learnopencv.com/intro-to-gradcam/) | [Code](https://www.dropbox.com/scl/fo/3p3sg5fnvhrvi9vp00i0w/h?rlkey=1x01uz5o7esex7p6c8r534iyn&dl=1) |\n| [Text Summarization using T5: Fine-Tuning and Building Gradio App](https://learnopencv.com/text-summarization-using-t5/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Text-Summarization-using-T5-Fine-Tuning-and-Building-Gradio-App) |\n| [3D LiDAR Visualization using Open3D: A Case Study on 2D KITTI Depth Frames for Autonomous Driving](https://learnopencv.com/3d-lidar-visualization/) | [Code](https://github.com/spmallick/learnopencv/tree/master/3D-LiDAR-Perception) |\n| [Fine Tuning T5: Text2Text Transfer Transformer for Building a Stack Overflow Tag Generator](https://learnopencv.com/fine-tuning-t5/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-T5-Text2Text-Transformer-for-Strack-Overflow-Tag-Generation) |\n| [SegFormer ğŸ¤— : Fine-Tuning for Improved Lane Detection in Autonomous Vehicles](https://learnopencv.com/segformer-fine-tuning-for-lane-detection) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-SegFormer-For-Lane-Detection) |\n| [Fine-Tuning BERT using Hugging Face Transformers](https://learnopencv.com/fine-tuning-bert) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-BERT-using-Hugging-Face-Transformers) |\n| [YOLO-NAS Pose](https://learnopencv.com/yolo-nas-pose) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLO-NAS-Pose) |\n| [BERT: Bidirectional Encoder Representations from Transformers](https://learnopencv.com/bert-bidirectional-encoder-representations-from-transformers/) | [Code](https://github.com/spmallick/learnopencv/tree/master/BERT-Bidirectional-Encoder-Representations-from-Transformers) |\n| [Comparing KerasCV YOLOv8 Models on the Global Wheat Data 2020](https://learnopencv.com/comparing-kerascv-yolov8-models/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Comparing-KerasCV-YOLOv8-Models-on-the-Global-Wheat-Data-2020) |\n| [Top 5 AI papers of September 2023](https://learnopencv.com/top-5-ai-papers-of-september-2023/) | |\n| [Empowering Drivers: The Rise and Role of Advanced Driver Assistance Systems](https://learnopencv.com/advanced-driver-assistance-systems/) | |\n| [Semantic Segmentation using KerasCV DeepLabv3+](https://learnopencv.com/kerascv-deeplabv3-plus-semantic-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Semantic-Segmentation-using-KerasCV-with-DeepLabv3-Plus) |\n| [Object Detection using KerasCV YOLOv8](https://learnopencv.com/object-detection-using-kerascv-yolov8/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Object-Detection-using-KerasCV-YOLOv8) |\n| [Fine-tuning YOLOv8 Pose Models for Animal Pose Estimation](https://learnopencv.com/animal-pose-estimation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-tuning-YOLOv8-Pose-Models-for-Animal-Pose-Estimation) |\n| [Top 5 AI papers of August 2023](https://learnopencv.com/top-5-ai-papers-of-august-2023/) | |\n| [Fine Tuning TrOCR - Training TrOCR to Recognize Curved Text](https://learnopencv.com/fine-tuning-trocr-training-trocr-to-recognize-curved-text/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-TrOCR) |\n| [TrOCR - Getting Started with Transformer Based OCR](https://learnopencv.com/trocr-getting-started-with-transformer-based-ocr/) | [Code](https://github.com/spmallick/learnopencv/tree/master/TrOCR-Getting-Started-with-Transformer-Based-OCR) |\n| [Facial Emotion Recognition](https://learnopencv.com/facial-emotion-recognition/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Facial-Emotion-Recognition) |\n| [Object Keypoint Similarity in Keypoint Detection](https://learnopencv.com/object-keypoint-similarity/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Object-Keypoint-Similarity-in-Keypoint-Detection) |\n| [Real Time Deep SORT with Torchvision Detectors](https://learnopencv.com/real-time-deep-sort-with-torchvision-detectors/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Real_Time_Deep_SORT_using_Torchvision_Detectors) |\n| [Top 5 AI papers of July 2023](https://learnopencv.com/top-5-ai-papers-of-july-2023/) | |\n| [Medical Image Segmentation](https://learnopencv.com/medical-image-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Medical-Image-Segmentation-Using-HuggingFace-&-PyTorch) |\n| [Weighted Boxes Fusion in Object Detection: A Comparison with Non-Maximum Suppression](https://learnopencv.com/weighted-boxes-fusion/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Weighted-Boxes-Fusion-in-Object-Detection) |\n| [Medical Multi-label Classification with PyTorch & Lightning](https://learnopencv.com/medical-multi-label/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Medical_Multi-label_Classification_with_PyTorch_&_Lightning) |\n| [Getting Started with PaddlePaddle: Exploring Object Detection, Segmentation, and Keypoints](https://learnopencv.com/paddlepaddle/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Introduction-to-PaddlePaddle) |\n| [Drone Programming With Computer Vision A Beginners Guide](https://learnopencv.com/drone-programming-with-computer-vision/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Drone-Programming-With-Computer-Vision-A-Beginners-Guide) |\n| [How to Build a Pip Installable Package & Upload to PyPi](https://learnopencv.com/building-pip-installable-package-pypi/) | |\n| [IoU Loss Functions for Faster & More Accurate Object Detection](https://learnopencv.com/iou-loss-functions-object-detection/) | |\n| [Exploring Slicing Aided Hyper Inference for Small Object Detection](https://learnopencv.com/slicing-aided-hyper-inference/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Exploring-Slicing-Aided-Hyper-Inference) |\n| [Advancements in Face Recognition Models, Toolkit and Datasets](https://learnopencv.com/face-recognition-models/) | |\n| [Train YOLO NAS on Custom Dataset](https://learnopencv.com/train-yolo-nas-on-custom-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Train-YOLO-NAS-on-Custom-Dataset) |\n| [Train YOLOv8 Instance Segmentation on Custom Data](https://learnopencv.com/train-yolov8-instance-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Train-YOLOv8-Instance-Segmentation-on-Custom-Data) |\n| [YOLO-NAS: New Object Detection Model Beats YOLOv6 & YOLOv8](https://learnopencv.com/yolo-nas/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLO-NAS_Introduction) |\n| [Segment Anything â€“ A Foundation Model for Image Segmentation](https://learnopencv.com/segment-anything/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Segment-Anything-A-Foundation-Model-for-Image-Segmentation) |\n|[Build a Video to Slides Converter Application using the Power of Background Estimation and Frame Differencing in OpenCV](https://learnopencv.com/video-to-slides-converter-using-background-subtraction/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Build-a-Video-to-Slides-Converter-Application-using-the-Power-of-Background-Estimation-and-Frame-Differencing-in-OpenCV)|\n|[A Closer Look at CVAT: Perfecting Your Annotations](https://learnopencv.com/a-closer-look-at-cvat-perfecting-your-annotations/)|[YouTube](https://www.youtube.com/watch?v=yxX_0-zr-2U&list=PLfYPZalDvZDLvFhjuflhrxk_lLplXUqqB)|\n| [ControlNet - Achieving Superior Image Generation Results](https://learnopencv.com/controlnet/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ControlNet-Achieving-Superior-Image-Generation-Results) |\n| [InstructPix2Pix - Edit Images With Prompts](https://learnopencv.com/instructpix2pix/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstructPix2Pix-Edit-Images-With-Prompts) |\n| [NVIDIA Spring GTC 2023 Day 4: Ending on a High Note with Top Moments from the Finale!](https://learnopencv.com/nvidia-spring-gtc-2023-day-4/) | |\n| [NVIDIA Spring GTC 2023 Day 3: Digging deeper into Deep Learning, Semiconductors & more!](https://learnopencv.com/nvidia-spring-gtc-2023-day-3-digging-deeper-into-deep-learning-semiconductors-more/) | |\n| [NVIDIA Spring GTC 2023 Day 2: Jensenâ€™s keynote & the iPhone moment of AI is here!](https://learnopencv.com/nvidia-spring-gtc-2023-day-2-jensens-keynote-the-iphone-moment-of-ai-is-here/) | |\n| [NVIDIA Spring GTC 2023 Day 1: Welcome to the future!](https://learnopencv.com/nvidia-spring-gtc-2023-day-1-highlights-welcome-to-the-future/) | |\n| [NVIDIA GTC Spring 2023 Curtain Raiser](https://learnopencv.com/nvidia-gtc-spring-2023-curtain-raiser/) | |\n| [Stable Diffusion - A New Paradigm in Generative AI](https://learnopencv.com/stable-diffusion-generative-ai/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Stable-Diffusion-A-New-Paradigm-in-Generative-AI) |\n| [OpenCV Face Recognition â€“ Does Face Recognition Work on AI-Generated Images?](https://learnopencv.com/opencv-face-recognition-api/) | |\n|[An In-Depth Guide to Denoising Diffusion Probabilistic Models â€“ From Theory to Implementation](https://learnopencv.com/denoising-diffusion-probabilistic-models/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Guide-to-training-DDPMs-from-Scratch)|\n|[From Pixels to Paintings: The Rise of Midjourney AI Art](https://learnopencv.com/rise-of-midjourney-ai-art/)| |\n|[Mastering DALLÂ·E 2: A Breakthrough in AI Art Generation](https://learnopencv.com/mastering-dall-e-2/)| |\n|[Top 10 AI Art Generation Tools using Diffusion Models](https://learnopencv.com/ai-art-generation-tools/)| |\n|[The Future of Image Recognition is Here: PyTorch Vision Transformer](https://learnopencv.com/the-future-of-image-recognition-is-here-pytorch-vision-transformer/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Vision_Transformer_PyTorch)|\n|[Understanding Attention Mechanism in Transformer Neural Networks](https://learnopencv.com/attention-mechanism-in-transformer-neural-networks/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Attention_Mechanism_Introduction)|\n| [Deploying a Deep Learning Model using Hugging Face Spaces and Gradio](https://learnopencv.com/deploy-deep-learning-model-huggingface-spaces/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Deploying-a-Deep-Learning-Model-using-Hugging-Face-Spaces-and-Gradio) |\n| [Train YOLOv8 on Custom Dataset â€“ A Complete Tutorial](https://learnopencv.com/train-yolov8-on-custom-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Train-YOLOv8-on-Custom-Dataset-A-Complete-Tutorial) |\n| [Introduction to Diffusion Models for Image Generation](https://learnopencv.com/image-generation-using-diffusion-models/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Introduction-to-Diffusion-Models-for-Image-Generation) |\n| [Building An Automated Image Annotation Tool: PyOpenAnnotate](https://learnopencv.com/building-automated-image-annotation-tool-pyopenannotate/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Building-An-Automated-Image-Annotation-Tool-PyOpenAnnotate/) |\n| [Ultralytics YOLOv8: State-of-the-Art YOLO Models](https://learnopencv.com/ultralytics-yolov8/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Ultralytics-YOLOv8-State-of-the-Art-YOLO-Models) |\n| [Getting Started with YOLOv5 Instance Segmentation](https://learnopencv.com/getting-started-with-yolov5-instance-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Getting-Started-with-YOLOv5-Instance-Segmentation) |\n|[The Ultimate Guide To DeepLabv3 - With PyTorch Inference](https://learnopencv.com/deeplabv3-ultimate-guide/)|[Code](https://github.com/spmallick/learnopencv/tree/master/The-ultimate-guide-to-deeplabv3)|\n|[AI Fitness Trainer using MediaPipe: Squats Analysis](https://learnopencv.com/ai-fitness-trainer-using-mediapipe/)|[Code](https://github.com/spmallick/learnopencv/tree/master/AI-Fitness-Trainer-Using-MediaPipe-Analyzing-Squats)|\n|[YoloR - Paper Explanation & Inference -An In-Depth Analysis](https://learnopencv.com/yolor-paper-explanation-inference-an-in-depth-analysis/)|[Code](https://github.com/spmallick/learnopencv/tree/master/YoloR-paper-explanation-analysis)|\n|[Roadmap To an Automated Image Annotation Tool Using Python](https://learnopencv.com/automated-image-annotation-tool-using-opencv-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Roadmap-To-an-Automated-Image-Annotation-Tool-Using-Python)|\n|[Performance Comparison of YOLO Object Detection Models â€“ An Intensive Study](https://learnopencv.com/performance-comparison-of-yolo-models/)||\n|[FCOS - Anchor Free Object Detection Explained](https://learnopencv.com/fcos-anchor-free-object-detection-explained/)|[Code](https://github.com/spmallick/learnopencv/tree/master/FCOS-Inference-using-PyTorch)|\n| [YOLOv6 Custom Dataset Training â€“ Underwater Trash Detection](https://learnopencv.com/yolov6-custom-dataset-training/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv6-Custom-Dataset-Training-Underwater-Trash-Detection) |\n|[What is EXIF Data in Images?](https://www.learnopencv.com/what-is-exif-data-in-images/)|[Code](https://github.com/spmallick/learnopencv/tree/master/What-is-EXIF-Data-in-Images)|\n|[t-SNE: T-Distributed Stochastic Neighbor Embedding Explained](https://learnopencv.com/t-sne-t-distributed-stochastic-neighbor-embedding-explained/)|[Code](https://github.com/spmallick/learnopencv/tree/master/t-SNE-with-Tensorboard)|\n|[CenterNet: Objects as Points â€“ Anchor-free Object Detection Explained](https://learnopencv.com/centernet-anchor-free-object-detection-explained/)|[Code](https://github.com/spmallick/learnopencv/tree/master/centernet-with-tf-hub)|\n|[YOLOv7 Pose vs MediaPipe in Human Pose Estimation](https://learnopencv.com/yolov7-pose-vs-mediapipe-in-human-pose-estimation/)|[Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv7-Pose-vs-MediaPipe-in-Human-Pose-Estimation)|\n|[YOLOv6 Object Detection â€“ Paper Explanation and Inference](https://learnopencv.com/yolov6-object-detection/)|[Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv6-Object-Detection-Paper-Explanation-and-Inference)|\n|[YOLOX Object Detector Paper Explanation and Custom Training](https://learnopencv.com/yolox-object-detector-paper-explanation-and-custom-training/)|[Code](https://github.com/spmallick/learnopencv/tree/master/YOLOX-Object-Detection-Paper-Explanation-and-Custom-Training)|\n|[Driver Drowsiness Detection Using Mediapipe In Python](https://learnopencv.com/driver-drowsiness-detection-using-mediapipe-in-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Driver-Drowsiness-detection-using-Mediapipe-in-Python)|\n|[GTC 2022 Big Bang AI announcements: Everything you need to know](https://learnopencv.com/gtc-2022-big-bang-ai-announcements-everything-you-need-to-know/)||\n|[NVIDIA GTC 2022 : The most important AI event this Fall](https://learnopencv.com/nvidia-gtc-2022-the-most-important-ai-event-this-fall/)||\n|[Object Tracking and Reidentification with FairMOT](https://learnopencv.com/object-tracking-and-reidentification-with-fairmot/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Object-Tracking-and-Reidentification-with-FairMOT) |\n|[What is Face Detection? â€“ The Ultimate Guide for 2022](https://learnopencv.com/what-is-face-detection-the-ultimate-guide/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Face-Detection-Ultimate-Guide) |\n|[Document Scanner: Custom Semantic Segmentation using PyTorch-DeepLabV3](https://learnopencv.com/custom-document-segmentation-using-deep-learning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Document-Scanner-Custom-Semantic-Segmentation-using-PyTorch-DeepLabV3)|\n|[Fine Tuning YOLOv7 on Custom Dataset](https://learnopencv.com/fine-tuning-yolov7-on-custom-dataset/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Fine-Tuning-YOLOv7)|\n|[Center Stage for Zoom Calls using MediaPipe](https://learnopencv.com/Center-Stage-for-zoom-call-using-mediapipe/)|[Code](https://github.com/spmallick/learnopencv/tree/master/CenterStage)|\n|[Mean Average Precision (mAP) in Object Detection](https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/)||\n|[YOLOv7 Object Detection Paper Explanation and Inference](https://learnopencv.com/yolov7-object-detection-paper-explanation-and-inference/)|[Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv7-Object-Detection-Paper-Explanation-and-Inference)|\n|[Pothole Detection using YOLOv4 and Darknet](https://learnopencv.com/pothole-detection-using-yolov4-and-darknet/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Pothole-Detection-using-YOLOv4-and-Darknet)|\n|[Automatic Document Scanner using OpenCV](https://learnopencv.com/automatic-document-scanner-using-opencv/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Automatic-Document-Scanner)|\n|[Demystifying GPU architectures for deep learning: Part 2](https://learnopencv.com/demystifying-gpu-architectures-for-deep-learning-part-2/)|[Code](https://github.com/spmallick/learnopencv/tree/master/gpu_arch_and_CUDA)|\n|[Demystifying GPU Architectures For Deep Learning](https://learnopencv.com/demystifying-gpu-architectures-for-deep-learning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/gpu_arch_and_CUDA)|\n|[Intersection-over-Union(IoU)-in-Object-Detection-and-Segmentation](https://learnopencv.com/intersection-over-unioniou-in-object-detection-and-segmentation/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Intersection-over-Union-IoU-in-Object-Detection-and-Segmentation)|\n|[Understanding Multiple Object Tracking using DeepSORT](https://learnopencv.com/understanding-multiple-object-tracking-using-deepsort/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Understanding-Multiple-Object-Tracking-using-DeepSORT)|\n|[Optical Character Recognition using PaddleOCR](https://learnopencv.com/optical-character-recognition-using-paddleocr/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Optical-Character-Recognition-using-PaddleOCR)|\n|[Gesture Control in Zoom Call using Mediapipe](https://learnopencv.com/gesture-control-in-zoom-call-using-mediapipe/)|[Code](https://github.com/spmallick/learnopencv/tree/master/zoom-gestures)|\n|[A Deep Dive into Tensorflow Model Optimization](https://learnopencv.com/deep-dive-into-tensorflow-model-optimization-toolkit/)|[Code](https://github.com/spmallick/learnopencv/tree/master/A-Deep-Dive-into-Tensorflow-Model-Optimization)|\n|[DepthAI Pipeline Overview: Creating a Complex Pipeline](https://learnopencv.com/depthai-pipeline-overview-creating-a-complex-pipeline/)|[Code](https://github.com/spmallick/learnopencv/tree/master/OAK-DepthAi-Pipeline-Overview)|\n|[TensorFlow Lite Model Maker: Create Models for On-Device Machine Learning](https://learnopencv.com/tensorflow-lite-model-maker-create-models-for-on-device-machine-learning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Tensorflow-Lite-Model-Maker-Create-Models-for-On-Device-ML)|\n|[TensorFlow Lite: Model Optimization for On Device Machine Learning](https://learnopencv.com/tensorflow-lite-model-optimization-for-on-device-machine-learning)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Lite-Model-Optimization-for-On-Device-MachineLearning)|\n|[Object detection with depth measurement using pre-trained models with OAK-D](https://learnopencv.com/object-detection-with-depth-measurement-with-oak-d/)|[Code](https://github.com/spmallick/learnopencv/tree/master/OAK-Object-Detection-with-Depth)|\n|[Custom Object Detection Training using YOLOv5](https://learnopencv.com/custom-object-detection-training-using-yolov5/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Custom-Object-Detection-Training-using-YOLOv5)|\n|[Object Detection using Yolov5 and OpenCV DNN (C++/Python)](https://learnopencv.com/object-detection-using-yolov5-and-opencv-dnn-in-c-and-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Object-Detection-using-YOLOv5-and-OpenCV-DNN-in-CPP-and-Python)|\n|[Create Snapchat/Instagram filters using Mediapipe](https://learnopencv.com/create-snapchat-instagram-filters-using-mediapipe/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Create-AR-filters-using-Mediapipe)|\n|[AUTOSAR C++ compliant deep learning inference with TensorRT](https://learnopencv.com/autosar-c-compliant-deep-learning-inference-with-tensorrt/)|[Code](https://github.com/spmallick/learnopencv/tree/master/industrial_cv_TensorRT_cpp)|\n|[NVIDIA GTC 2022 Day 4 Highlights: Meet the new Jetson Orin](https://learnopencv.com/nvidia-gtc-2022-day-4-highlights-meet-the-new-jetson-orin/)||\n|[NVIDIA GTC 2022 Day 3 Highlights: Deep Dive into Hopper architecture](https://learnopencv.com/nvidia-gtc-2022-day-3-highlights-deep-dive-into-hopper-architecture/)||\n|[NVIDIA GTC 2022 Day 2 Highlights: Jensenâ€™s Keynote](https://learnopencv.com/nvidia-gtc-2022-day-2-highlights/)||\n|[NVIDIA GTC 2022 Day 1 Highlights: Brilliant Start](https://learnopencv.com/gtc-day-1-highlights/)||\n|[Automatic License Plate Recognition using Python](https://learnopencv.com/automatic-license-plate-recognition-using-deep-learning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/ALPR)|\n|[Building a Poor Body Posture Detection and Alert System using MediaPipe](https://learnopencv.com/building-a-body-posture-analysis-system-using-mediapipe/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Posture-analysis-system-using-MediaPipe-Pose)|\n|[Introduction to MediaPipe](https://learnopencv.com/introduction-to-mediapipe/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Introduction-to-MediaPipe)|\n|[Disparity Estimation using Deep Learning](https://learnopencv.com/disparity-estimation-using-deep-learning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Disparity-Estimation-Using-Deep-Learning)|\n|[How to build Chrome Dino game bot using OpenCV Feature Matching](https://learnopencv.com/how-to-build-chrome-dino-game-bot-using-opencv-feature-matching/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Chrome-Dino-Bot-using-OpenCV-feature-matching)|\n|[Top 10 Sources to Find Computer Vision and AI Models](https://learnopencv.com/top-10-sources-to-find-computer-vision-and-ai-models/)||\n|[Multi-Attribute and Graph-based Object Detection](https://learnopencv.com/multi-attribute-and-graph-based-object-detection/)||\n|[Plastic Waste Detection with Deep Learning](https://learnopencv.com/plastic-waste-detection-with-deep-learning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Plastic-Waste-Detection-with-Deep-Learning)|\n|[Ensemble Deep Learning-based Defect Classification and Detection in SEM Images](https://learnopencv.com/ensemble-deep-learning-based-defect-classification-and-detection-in-sem-images/)||\n|[Building Industrial embedded deep learning inference pipelines with TensorRT](https://learnopencv.com/building-industrial-embedded-deep-learning-inference-pipelines-with-tensorrt/)|[Code](https://github.com/spmallick/learnopencv/tree/master/industrial_cv_TensorRT_python)|\n|[Transfer Learning for Medical Images](https://learnopencv.com/transfer-learning-for-medical-images/)||\n|[Stereo Vision and Depth Estimation using OpenCV AI Kit](https://learnopencv.com/stereo-vision-and-depth-estimation-using-opencv-ai-kit/)|[Code](https://github.com/spmallick/learnopencv/tree/master/oak-getting-started)|\n|[Introduction to OpenCV AI Kit and DepthAI](https://learnopencv.com/introduction-to-opencv-ai-kit-and-depthai/)|[Code](https://github.com/spmallick/learnopencv/tree/master/oak-getting-started)|\n|[WeChat QR Code Scanner in OpenCV](https://learnopencv.com/wechat-qr-code-scanner-in-opencv)|[Code](https://github.com/spmallick/learnopencv/tree/master/WeChat-QRCode-Scanner-OpenCV)|\n|[AI behind the Diwali 2021 â€˜Not just a Cadbury adâ€™](https://learnopencv.com/ai-behind-the-diwali-2021-not-just-a-cadbury-ad/)| |\n|[Model Selection and Benchmarking with Modelplace.AI](https://learnopencv.com/model-selection-and-benchmarking-with-modelplace-ai/)|[Model Zoo](https://modelplace.ai/)|\n|[Real-time style transfer in a zoom meeting](https://learnopencv.com/real-time-style-transfer-in-a-zoom-meeting/)|[Code](https://github.com/spmallick/learnopencv/tree/master/style-transfer-zoom)|\n| [Introduction to OpenVino Deep Learning Workbench](https://learnopencv.com/introduction-to-openvino-deep-learning-workbench/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Introduction-to-OpenVino-Deep-Learning-Workbench) |\n| [Running OpenVino Models on Intel Integrated GPU](https://learnopencv.com/running-openvino-models-on-intel-integrated-gpu/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Running-OpenVino-Models-on-Intel-Integrated-GPU) |\n|[Post Training Quantization with OpenVino Toolkit](https://learnopencv.com/post-training-quantization-with-openvino-toolkit/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Post-Training-Quantization-with-OpenVino-Toolkit)|\n|[Introduction to Intel OpenVINO Toolkit](https://learnopencv.com/introduction-to-intel-openvino-toolkit/)||\n|[Human Action Recognition using Detectron2 and LSTM](https://learnopencv.com/human-action-recognition-using-detectron2-and-lstm/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Human-Action-Recognition-Using-Detectron2-And-Lstm)|\n|[Pix2Pix:Image-to-Image Translation in PyTorch & TensorFlow](https://learnopencv.com/paired-image-to-image-translation-pix2pix/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Image-to-Image-Translation-with-GAN)|\n|[Conditional GAN (cGAN) in PyTorch and TensorFlow](https://learnopencv.com/conditional-gan-cgan-in-pytorch-and-tensorflow/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Conditional-GAN-PyTorch-TensorFlow)|\n|[Deep Convolutional GAN in PyTorch and TensorFlow](https://learnopencv.com/deep-convolutional-gan-in-pytorch-and-tensorflow/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Deep-Convolutional-GAN)|\n|[Introduction to Generative Adversarial Networks (GANs)](https://learnopencv.com/introduction-to-generative-adversarial-networks/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Intro-to-Generative-Adversarial-Network)|\n|[Human Pose Estimation using Keypoint RCNN in PyTorch](https://learnopencv.com/human-pose-estimation-using-keypoint-rcnn-in-pytorch/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Keypoint-RCNN)|\n|[Non Maximum Suppression: Theory and Implementation in PyTorch](https://learnopencv.com/non-maximum-suppression-theory-and-implementation-in-pytorch)|[Code](https://github.com/spmallick/learnopencv/tree/master/Non-Maximum-Suppression)|\n|[MRNet â€“ The Multi-Task Approach](https://learnopencv.com/mrnet-multitask-approach/)| [Code](https://github.com/spmallick/learnopencv/tree/master/MRnet-MultiTask-Approach) |\n|[Generative and Discriminative Models](https://learnopencv.com/generative-and-discriminative-models/)| |\n|[Playing Chrome''s T-Rex Game with Facial Gestures](https://learnopencv.com/playing-chromes-t-rex-game-with-facial-gestures/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Playing-Chrome-TRex-Game-with-Facial-Gestures) |\n|[Variational Autoencoder in TensorFlow](https://learnopencv.com/variational-autoencoder-in-tensorflow/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Variational-Autoencoder-TensorFlow) |\n|[Autoencoder in TensorFlow 2: Beginnerâ€™s Guide](https://learnopencv.com/autoencoder-in-tensorflow-2-beginners-guide/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Autoencoder-in-TensorFlow) |\n|[Deep Learning with OpenCV DNN Module: A Definitive Guide](https://learnopencv.com/deep-learning-with-opencvs-dnn-module-a-definitive-guide/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Deep-Learning-with-OpenCV-DNN-Module) |\n|[Depth perception using stereo camera (Python/C++)](https://learnopencv.com/depth-perception-using-stereo-camera-python-c/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Depth-Perception-Using-Stereo-Camera) |\n|[Contour Detection using OpenCV (Python/C++)](https://learnopencv.com/contour-detection-using-opencv-python-c/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Contour-Detection-using-OpenCV) |\n|[Super Resolution in OpenCV](https://learnopencv.com/super-resolution-in-opencv/)| [Code](https://github.com/spmallick/learnopencv/blob/master/Super-Resolution-in-OpenCV) |\n|[Improving Illumination in Night Time Images](https://learnopencv.com/improving-illumination-in-night-time-images/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Improving-Illumination-in-Night-Time-Images) |\n|[Video Classification and Human Activity Recognition](https://learnopencv.com/introduction-to-video-classification-and-human-activity-recognition/) | [Code](https://github.com/spmallick/learnopencv/tree/master/video-classification-and-human-activity-recognition) |\n|[How to use OpenCV DNN Module with Nvidia GPU on Windows](https://learnopencv.com/how-to-use-opencv-dnn-module-with-nvidia-gpu-on-windows) | [Code](https://github.com/spmallick/learnopencv/tree/master/OpenCV-dnn-gpu-support-Windows) |\n|[How to use OpenCV DNN Module with NVIDIA GPUs](https://learnopencv.com/opencv-dnn-with-gpu-support/) | [Code](https://github.com/spmallick/learnopencv/tree/master/OpenCV-dnn-gpu-support-Linux) |\n|[Code OpenCV in Visual Studio](https://learnopencv.com/code-opencv-in-visual-studio/) | |\n|[Install OpenCV on Windows â€“ C++ / Python](https://learnopencv.com/install-opencv-on-windows/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Install-OpenCV-Windows-exe) |\n|[Face Recognition with ArcFace](https://www.learnopencv.com/face-recognition-with-arcface/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Face-Recognition-with-ArcFace)|\n|[Background Subtraction with OpenCV and BGS Libraries](https://www.learnopencv.com/background-subtraction-with-opencv-and-bgs-libraries/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Background-Subtraction) |\n|[RAFT: Optical Flow estimation using Deep Learning](https://learnopencv.com/optical-flow-using-deep-learning-raft/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Optical-Flow-Estimation-using-Deep-Learning-RAFT)|\n|[Making A Low-Cost Stereo Camera Using OpenCV](https://www.learnopencv.com/making-a-low-cost-stereo-camera-using-opencv/)|[Code](https://github.com/spmallick/learnopencv/tree/master/stereo-camera)|\n|[Optical Flow in OpenCV (C++/Python)](https://www.learnopencv.com/optical-flow-in-opencv)|[Code](https://github.com/spmallick/learnopencv/tree/master/Optical-Flow-in-OpenCV)|\n|[Introduction to Epipolar Geometry and Stereo Vision](https://www.learnopencv.com/introduction-to-epipolar-geometry-and-stereo-vision/)|[Code](https://github.com/spmallick/learnopencv/tree/master/EpipolarGeometryAndStereoVision)|\n|[Classification With Localization: Convert any keras Classifier to a Detector](https://www.learnopencv.com/classification-with-localization/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Classification-with-localization-convert-any-keras-classifier-into-a-detector/README.md) |\n|[Photoshop Filters in OpenCV](https://www.learnopencv.com/photoshop-filters-in-opencv/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Photoshop-Filters-in-OpenCV)|\n|[Tetris Game using OpenCV Python](https://www.learnopencv.com/tetris-with-opencv-python)|[Code](https://github.com/spmallick/learnopencv/tree/master/Tetris)|\n|[Image Classification with OpenCV for Android](https://www.learnopencv.com/image-classification-with-opencv-for-android/) | [Code](https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-Android) |\n|[Image Classification with OpenCV Java](https://www.learnopencv.com/image-classification-with-opencv-java)|[Code](https://github.com/spmallick/learnopencv/tree/master/DNN-OpenCV-Classification-with-Java) |\n|[PyTorch to Tensorflow Model Conversion](https://www.learnopencv.com/pytorch-to-tensorflow-model-conversion/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-TensorFlow-Model-Conversion) |\n|[Snake Game with OpenCV Python](https://www.learnopencv.com/snake-game-with-opencv-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/SnakeGame) |\n|[Stanford MRNet Challenge: Classifying Knee MRIs](https://www.learnopencv.com/stanford-mrnet-challenge-classifying-knee-mris/)|[Code](https://github.com/spmallick/learnopencv/tree/master/MRNet-Single-Model) |\n|[Experiment Logging with TensorBoard and wandb](https://www.learnopencv.com/experiment-logging-with-tensorboard-and-wandb)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Vision-Experiment-Logging) |\n|[Understanding Lens Distortion](https://www.learnopencv.com/understanding-lens-distortion/)|[Code](https://github.com/spmallick/learnopencv/tree/master/UnderstandingLensDistortion) |\n|[Image Matting with state-of-the-art Method â€œF, B, Alpha Mattingâ€](https://www.learnopencv.com/image-matting-with-state-of-the-art-method-f-b-alpha-matting/)|[Code](https://github.com/spmallick/learnopencv/tree/master/FBAMatting) |\n|[Bag Of Tricks For Image Classification - Let''s check if it is working or not](https://www.learnopencv.com/bag-of-tricks-for-image-classification-lets-check-if-it-is-working-or-not/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Bag-Of-Tricks-For-Image-Classification) |\n|[Getting Started with OpenCV CUDA Module](https://www.learnopencv.com/getting-started-opencv-cuda-module/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Getting-Started-OpenCV-CUDA-Module) |\n|[Training a Custom Object Detector with DLIB & Making Gesture Controlled Applications](https://www.learnopencv.com/training-a-custom-object-detector-with-dlib-making-gesture-controlled-applications/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Training_a_custom_hand_detector_with_dlib) |\n|[How To Run Inference Using TensorRT C++ API](https://www.learnopencv.com/how-to-run-inference-using-tensorrt-c-api/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-ONNX-TensorRT-CPP) |\n|[Using Facial Landmarks for Overlaying Faces with Medical Masks](https://www.learnopencv.com/using-facial-landmarks-for-overlaying-faces-with-masks/)|[Code](https://github.com/spmallick/learnopencv/tree/master/FaceMaskOverlay) |\n|[Tensorboard with PyTorch Lightning](https://www.learnopencv.com/tensorboard-with-pytorch-lightning)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorBoard-With-Pytorch-Lightning) |\n|[Otsu''s Thresholding with OpenCV](https://www.learnopencv.com/otsu-thresholding-with-opencv/)|[Code](https://github.com/spmallick/learnopencv/tree/master/otsu-method) |\n|[PyTorch-to-CoreML-model-conversion](https://www.learnopencv.com/pytorch-to-coreml-model-conversion/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-to-CoreML-model-conversion) |\n|[Playing Rock, Paper, Scissors with AI](https://www.learnopencv.com/playing-rock-paper-scissors-with-ai/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Playing-rock-paper-scissors-with-AI) |\n|[CNN Receptive Field Computation Using Backprop with TensorFlow](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Receptive-Field-With-Backprop)|\n|[CNN Fully Convolutional Image Classification with TensorFlow](https://www.learnopencv.com/cnn-fully-convolutional-image-classification-with-tensorflow) | [Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Fully-Convolutional-Image-Classification) |\n|[How to convert a model from PyTorch to TensorRT and speed up inference](https://www.learnopencv.com/how-to-convert-a-model-from-pytorch-to-tensorrt-and-speed-up-inference/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-ONNX-TensorRT) |\n|[Efficient image loading](https://www.learnopencv.com/efficient-image-loading/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Efficient-image-loading) |\n|[Graph Convolutional Networks: Model Relations In Data](https://www.learnopencv.com/graph-convolutional-networks-model-relations-in-data/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Graph-Convolutional-Networks-Model-Relations-In-Data)|\n|[Getting Started with Federated Learning with PyTorch and PySyft](https://www.learnopencv.com/federated-learning-using-pytorch-and-pysyft/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Federated-Learning-Intro)|\n|[Creating a Virtual Pen & Eraser](http://www.learnopencv.com/creating-a-virtual-pen-and-eraser-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Creating-a-Virtual-Pen-and-Eraser) |\n|[Getting Started with PyTorch Lightning](https://www.learnopencv.com/getting-started-with-pytorch-lightning/)|[Code](https://github.com/spmallick/learnopencv/tree/master/Pytorch-Lightning)|\n|[Multi-Label Image Classification with PyTorch: Image Tagging](https://www.learnopencv.com/multi-label-image-classification-with-pytorch-image-tagging/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification-Image-Tagging)|\n|[Funny Mirrors Using OpenCV](https://www.learnopencv.com/Funny-Mirrors-Using-OpenCV/)|[code](https://github.com/spmallick/learnopencv/tree/master/FunnyMirrors)|\n|[t-SNE for ResNet feature visualization](https://www.learnopencv.com/t-sne-for-feature-visualization/)|[Code](https://github.com/spmallick/learnopencv/tree/master/TSNE)|\n|[Multi-Label Image Classification with Pytorch](https://www.learnopencv.com/multi-label-image-classification-with-pytorch/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Multi-Label-Image-Classification)|\n|[CNN Receptive Field Computation Using Backprop](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Receptive-Field-With-Backprop)|\n|[CNN Receptive Field Computation Using Backprop with TensorFlow](https://www.learnopencv.com/cnn-receptive-field-computation-using-backprop-with-tensorflow/)|[Code](https://github.com/spmallick/learnopencv/tree/master/TensorFlow-Receptive-Field-With-Backprop)|\n|[Augmented Reality using AruCo Markers in OpenCV(C++ and Python)](https://www.learnopencv.com/augmented-reality-using-aruco-markers-in-opencv-(c++-python)/) |[Code](https://github.com/spmallick/learnopencv/tree/master/AugmentedRealityWithArucoMarkers)|\n|[Fully Convolutional Image Classification on Arbitrary Sized Image](https://www.learnopencv.com/fully-convolutional-image-classification-on-arbitrary-sized-image/)|[Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Fully-Convolutional-Image-Classification)|\n|[Camera Calibration using OpenCV](https://www.learnopencv.com/camera-calibration-using-opencv/) |[Code](https://github.com/spmallick/learnopencv/tree/master/CameraCalibration)|\n|[Geometry of Image Formation](https://www.learnopencv.com/geometry-of-image-formation/) ||\n|[Ensuring Training Reproducibility in Pytorch](https://www.learnopencv.com/ensuring-training-reproducibility-in-pytorch) ||\n|[Gaze Tracking](https://www.learnopencv.com/gaze-tracking/) ||\n|[Simple Background Estimation in Videos Using OpenCV](https://www.learnopencv.com/simple-background-estimation-in-videos-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VideoBackgroundEstimation)|\n|[Applications of Foreground-Background separation with Semantic Segmentation](https://www.learnopencv.com/applications-of-foreground-background-separation-with-semantic-segmentation/) | [Code](https://github.com/spmallick/learnopencv/tree/master/app-seperation-semseg) |\n|[EfficientNet: Theory + Code](https://www.learnopencv.com/efficientnet-theory-code) | [Code](https://github.com/spmallick/learnopencv/tree/master/EfficientNet) |\n|[PyTorch for Beginners: Mask R-CNN Instance Segmentation with PyTorch](https://www.learnopencv.com/mask-r-cnn-instance-segmentation-with-pytorch/) | [Code](./PyTorch-Mask-RCNN) |\n|[PyTorch for Beginners: Faster R-CNN Object Detection with PyTorch](https://www.learnopencv.com/faster-r-cnn-object-detection-with-pytorch) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-faster-RCNN) |\n|[PyTorch for Beginners: Semantic Segmentation using torchvision](https://www.learnopencv.com/pytorch-for-beginners-semantic-segmentation-using-torchvision/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-Segmentation-torchvision) |\n|[PyTorch for Beginners: Comparison of pre-trained models for Image Classification](https://www.learnopencv.com/image-classification-using-pre-trained-models-using-pytorch/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-classification-pre-trained-models/Image_Classification_using_pre_trained_models.ipynb) |\n|[PyTorch for Beginners: Basics](https://www.learnopencv.com/pytorch-for-beginners-basics/) | [Code](https://github.com/spmallick/learnopencv/tree/master/PyTorch-for-Beginners/PyTorch_for_Beginners.ipynb) |\n|[PyTorch Model Inference using ONNX and Caffe2](https://www.learnopencv.com/pytorch-model-inference-using-onnx-and-caffe2/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Inference-for-PyTorch-Models/ONNX-Caffe2) |\n|[Image Classification Using Transfer Learning in PyTorch](https://www.learnopencv.com/image-classification-using-transfer-learning-in-pytorch/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-Classification-in-PyTorch) |\n|[Hangman: Creating games in OpenCV](https://www.learnopencv.com/hangman-creating-games-in-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Hangman) |\n|[Image Inpainting with OpenCV (C++/Python)](https://www.learnopencv.com/image-inpainting-with-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Image-Inpainting) |\n|[Hough Transform with OpenCV (C++/Python)](https://www.learnopencv.com/hough-transform-with-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Hough-Transform) |\n|[Xeus-Cling: Run C++ code in Jupyter Notebook](https://www.learnopencv.com/xeus-cling-run-c-code-in-jupyter-notebook/) | [Code](https://github.com/spmallick/learnopencv/tree/master/XeusCling) |\n|[Gender & Age Classification using OpenCV Deep Learning ( C++/Python )](https://www.learnopencv.com/age-gender-classification-using-opencv-deep-learning-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/AgeGender) |\n|[Invisibility Cloak using Color Detection and Segmentation with OpenCV](https://www.learnopencv.com/invisibility-cloak-using-color-detection-and-segmentation-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InvisibilityCloak) |\n|[Fast Image Downloader for Open Images V4 (Python)](https://www.learnopencv.com/fast-image-downloader-for-open-images-v4/) | [Code](https://github.com/spmallick/learnopencv/tree/master/downloadOpenImages) |\n|[Deep Learning based Text Detection Using OpenCV (C++/Python)](https://www.learnopencv.com/deep-learning-based-text-detection-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/TextDetectionEAST) |\n|[Video Stabilization Using Point Feature Matching in OpenCV](https://www.learnopencv.com/video-stabilization-using-point-feature-matching-in-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VideoStabilization) |\n|[Training YOLOv3 : Deep Learning based Custom Object Detector](https://www.learnopencv.com/training-yolov3-deep-learning-based-custom-object-detector/) | [Code](https://github.com/spmallick/learnopencv/tree/master/YOLOv3-Training-Snowman-Detector ) |\n|[Using OpenVINO with OpenCV](https://www.learnopencv.com/using-openvino-with-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/OpenVINO-OpenCV) |\n|[Duplicate Search on Quora Dataset](https://www.learnopencv.com/duplicate-search-on-quora-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Quora-Dataset-Duplicate-Search) |\n|[Shape Matching using Hu Moments (C++/Python)](https://www.learnopencv.com/shape-matching-using-hu-moments-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/HuMoments) |\n|[Install OpenCV 4 on CentOS (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-centos-7/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-centos.sh) |\n|[Install OpenCV 3.4.4 on CentOS (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-centos-7/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-centos.sh) |\n|[Install OpenCV 3.4.4 on Red Hat (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-red-hat/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-red-hat.sh) |\n|[Install OpenCV 4 on Red Hat (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-red-hat/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-red-hat.sh) |\n|[Install OpenCV 4 on macOS (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-macos/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstallScripts/installOpenCV-4-macos.sh) |\n|[Install OpenCV 3.4.4 on Raspberry Pi](https://www.learnopencv.com/install-opencv-3-4-4-on-raspberry-pi/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-raspberry-pi.sh) |\n|[Install OpenCV 3.4.4 on macOS (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-macos/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-macos.sh) |\n|[OpenCV QR Code Scanner (C++ and Python)](https://www.learnopencv.com/opencv-qr-code-scanner-c-and-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/QRCode-OpenCV) |\n|[Install OpenCV 3.4.4 on Windows (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-windows/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-3) |\n|[Install OpenCV 3.4.4 on Ubuntu 16.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-16-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-Ubuntu-16-04.sh) |\n|[Install OpenCV 3.4.4 on Ubuntu 18.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-3-4-4-on-ubuntu-18-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-3-on-Ubuntu-18-04.sh) |\n|[Universal Sentence Encoder](https://www.learnopencv.com/universal-sentence-encoder) | [Code](https://github.com/spmallick/learnopencv/blob/master/Universal-Sentence-Encoder) |\n|[Install OpenCV 4 on Raspberry Pi](https://www.learnopencv.com/install-opencv-4-on-raspberry-pi/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-raspberry-pi.sh) |\n|[Install OpenCV 4 on Windows (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-windows/) | [Code](https://github.com/spmallick/learnopencv/tree/master/InstallScripts/Windows-4) |\n|[Face Detection â€“ Dlib, OpenCV, and Deep Learning ( C++ / Python )](https://learnopencv.com/face-detection-opencv-dlib-and-deep-learning-c-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/FaceDetectionComparison)|\n|[Hand Keypoint Detection using Deep Learning and OpenCV](https://www.learnopencv.com/hand-keypoint-detection-using-deep-learning-and-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/HandPose)|\n|[Deep learning based Object Detection and Instance Segmentation using Mask R-CNN in OpenCV (Python / C++)](https://www.learnopencv.com/deep-learning-based-object-detection-and-instance-segmentation-using-mask-r-cnn-in-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Mask-RCNN) |\n|[Install OpenCV 4 on Ubuntu 18.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-ubuntu-18-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-Ubuntu-18-04.sh) |\n|[Install OpenCV 4 on Ubuntu 16.04 (C++ and Python)](https://www.learnopencv.com/install-opencv-4-on-ubuntu-16-04/) | [Code](https://github.com/spmallick/learnopencv/blob/master/InstallScripts/installOpenCV-4-on-Ubuntu-16-04.sh) |\n|[Multi-Person Pose Estimation in OpenCV using OpenPose](https://www.learnopencv.com/multi-person-pose-estimation-in-opencv-using-openpose/) | [Code](https://github.com/spmallick/learnopencv/tree/master/OpenPose-Multi-Person) |\n|[Heatmap for Logo Detection using OpenCV (Python)](https://www.learnopencv.com/heatmap-for-logo-detection-using-opencv-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/heatmap)|\n|[Deep Learning based Object Detection using YOLOv3 with OpenCV ( Python / C++ )](https://www.learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ObjectDetection-YOLO)|\n|[Convex Hull using OpenCV in Python and C++](https://www.learnopencv.com/convex-hull-using-opencv-in-python-and-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ConvexHull)|\n|[MultiTracker : Multiple Object Tracking using OpenCV (C++/Python)](https://www.learnopencv.com/multitracker-multiple-object-tracking-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/MultiObjectTracker) |\n|[Convolutional Neural Network based Image Colorization using OpenCV](https://www.learnopencv.com/convolutional-neural-network-based-image-colorization-using-opencv/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Colorization)|\n|[SVM using scikit-learn](https://www.learnopencv.com/svm-using-scikit-learn-in-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python)|\n|[GOTURN: Deep Learning based Object Tracking](https://www.learnopencv.com/goturn-deep-learning-based-object-tracking/) | [Code](https://github.com/spmallick/learnopencv/tree/master/GOTURN)|\n|[Find the Center of a Blob (Centroid) using OpenCV (C++/Python)](https://www.learnopencv.com/find-center-of-blob-centroid-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/CenterofBlob)|\n|[Support Vector Machines (SVM)](https://www.learnopencv.com/support-vector-machines-svm/)|[Code](https://github.com/spmallick/learnopencv/tree/master/SVM-using-Python)|\n|[Batch Normalization in Deep Networks](https://www.learnopencv.com/batch-normalization-in-deep-networks/) | [Code](https://github.com/spmallick/learnopencv/tree/master/BatchNormalization)|\n|[Deep Learning based Character Classification using Synthetic Dataset](https://www.learnopencv.com/deep-learning-character-classification-using-synthetic-dataset/) | [Code](https://github.com/spmallick/learnopencv/tree/master/CharClassification)|\n|[Image Quality Assessment : BRISQUE](https://www.learnopencv.com/image-quality-assessment-brisque/)| [Code](https://github.com/spmallick/learnopencv/tree/master/ImageMetrics)|\n|[Understanding AlexNet](https://www.learnopencv.com/understanding-alexnet/)||\n|[Deep Learning based Text Recognition (OCR) using Tesseract and OpenCV](https://www.learnopencv.com/deep-learning-based-text-recognition-ocr-using-tesseract-and-opencv/)| [Code](https://github.com/spmallick/learnopencv/tree/master/OCR)|\n|[Deep Learning based Human Pose Estimation using OpenCV ( C++ / Python )](https://www.learnopencv.com/deep-learning-based-human-pose-estimation-using-opencv-cpp-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/OpenPose)|\n|[Number of Parameters and Tensor Sizes in a Convolutional Neural Network (CNN)](https://www.learnopencv.com/number-of-parameters-and-tensor-sizes-in-convolutional-neural-network/)| |\n|[How to convert your OpenCV C++ code into a Python module](https://www.learnopencv.com/how-to-convert-your-opencv-c-code-into-a-python-module/)|[Code](https://github.com/spmallick/learnopencv/tree/master/pymodule)|\n|[CV4Faces : Best Project Award 2018](https://www.learnopencv.com/cv4faces-best-project-award-2018/)| |\n|[Facemark : Facial Landmark Detection using OpenCV](https://www.learnopencv.com/facemark-facial-landmark-detection-using-opencv/)|[Code](https://github.com/spmallick/learnopencv/tree/master/FacialLandmarkDetection)|\n|[Image Alignment (Feature Based) using OpenCV (C++/Python)](https://www.learnopencv.com/image-alignment-feature-based-using-opencv-c-python/)| [Code](https://github.com/spmallick/learnopencv/tree/master/ImageAlignment-FeatureBased)|\n|[Barcode and QR code Scanner using ZBar and OpenCV](https://www.learnopencv.com/barcode-and-qr-code-scanner-using-zbar-and-opencv/)| [Code](https://github.com/spmallick/learnopencv/tree/master/barcode-QRcodeScanner)|\n|[Keras Tutorial : Fine-tuning using pre-trained models](https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Keras-Fine-Tuning)|\n|[OpenCV Transparent API](https://www.learnopencv.com/opencv-transparent-api/)| |\n|[Face Reconstruction using EigenFaces (C++/Python)](https://www.learnopencv.com/face-reconstruction-using-eigenfaces-cpp-python/)|[Code](https://github.com/spmallick/learnopencv/tree/master/ReconstructFaceUsingEigenFaces) |\n|[Eigenface using OpenCV (C++/Python)](https://www.learnopencv.com/eigenface-using-opencv-c-python/)| [Code](https://github.com/spmallick/learnopencv/tree/master/EigenFace)|\n|[Principal Component Analysis](https://www.learnopencv.com/principal-component-analysis/)| |\n|[Keras Tutorial : Transfer Learning using pre-trained models](https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Keras-Transfer-Learning) |\n|[Keras Tutorial : Using pre-trained Imagenet models](https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/)| [Code](https://github.com/spmallick/learnopencv/tree/master/Keras-ImageNet-Models) |\n|[Technical Aspects of a Digital SLR](https://www.learnopencv.com/technical-aspects-of-a-digital-slr/) | |\n|[Using Harry Potter interactive wand with OpenCV to create magic](https://www.learnopencv.com/using-harry-potter-interactive-wand-with-opencv-to-create-magic/)| |\n|[Install OpenCV 3 and Dlib on Windows ( Python only )](https://www.learnopencv.com/install-opencv-3-and-dlib-on-windows-python-only/)| |\n|[Image Classification using Convolutional Neural Networks in Keras](https://www.learnopencv.com/image-classification-using-convolutional-neural-networks-in-keras)      | [Code](https://github.com/spmallick/learnopencv/tree/master/KerasCNN-CIFAR)|\n|[Understanding Autoencoders using Tensorflow (Python)](https://www.learnopencv.com/understanding-autoencoders-using-tensorflow-python/)      | [Code](https://github.com/spmallick/learnopencv/tree/master/DenoisingAutoencoder)|\n|[Best Project Award : Computer Vision for Faces](https://www.learnopencv.com/best-project-award-computer-vision-for-faces/) | |\n|[Understanding Activation Functions in Deep Learning](https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/)      | |\n|[Image Classification using Feedforward Neural Network in Keras](https://www.learnopencv.com/image-classification-using-feedforward-neural-network-in-keras/)      | [Code](https://github.com/kromydas/learnopencv/tree/master/Keras-MLP-MNIST-Classification)|\n|[Exposure Fusion using OpenCV (C++/Python)](https://www.learnopencv.com/exposure-fusion-using-opencv-cpp-python/)      | [Code](https://github.com/spmallick/learnopencv/tree/master/ExposureFusion)|\n|[Understanding Feedforward Neural Networks](https://www.learnopencv.com/understanding-feedforward-neural-networks/)      | |\n|[High Dynamic Range (HDR) Imaging using OpenCV (C++/Python)](http://www.learnopencv.com/high-dynamic-range-hdr-imaging-using-opencv-cpp-python)      | [Code](https://github.com/spmallick/learnopencv/tree/master/hdr)|\n|[Deep learning using Keras â€“ The Basics](http://www.learnopencv.com/deep-learning-using-keras-the-basics)      | [Code](https://github.com/kromydas/learnopencv/tree/master/Keras-Linear-Regression)|\n|[Selective Search for Object Detection (C++ / Python)](http://www.learnopencv.com/selective-search-for-object-detection-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SelectiveSearch) |\n|[Installing Deep Learning Frameworks on Ubuntu with CUDA support](http://www.learnopencv.com/installing-deep-learning-frameworks-on-ubuntu-with-cuda-support/) | |\n|[Parallel Pixel Access in OpenCV using forEach](http://www.learnopencv.com/parallel-pixel-access-in-opencv-using-foreach/) | [Code](https://github.com/spmallick/learnopencv/tree/master/forEach) |\n|[cvui: A GUI lib built on top of OpenCV drawing primitives](http://www.learnopencv.com/cvui-gui-lib-built-on-top-of-opencv-drawing-primitives/) | [Code](https://github.com/spmallick/learnopencv/tree/master/UI-cvui) |\n|[Install Dlib on Windows](http://www.learnopencv.com/install-dlib-on-windows/) | |\n|[Install Dlib on Ubuntu](http://www.learnopencv.com/install-dlib-on-ubuntu/) | |\n|[Install OpenCV3 on Ubuntu](http://www.learnopencv.com/install-opencv3-on-ubuntu/) | |\n|[Read, Write and Display a video using OpenCV ( C++/ Python )](http://www.learnopencv.com/read-write-and-display-a-video-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/VideoReadWriteDisplay) |\n|[Install Dlib on MacOS](http://www.learnopencv.com/install-dlib-on-macos/) | |\n|[Install OpenCV 3 on MacOS](http://www.learnopencv.com/install-opencv3-on-macos/) | |\n|[Install OpenCV 3 on Windows](http://www.learnopencv.com/install-opencv3-on-windows/) | |\n|[Get OpenCV Build Information ( getBuildInformation )](http://www.learnopencv.com/get-opencv-build-information-getbuildinformation/) | |\n|[Color spaces in OpenCV (C++ / Python)](http://www.learnopencv.com/color-spaces-in-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ColorSpaces)|\n|[Neural Networks : A 30,000 Feet View for Beginners](http://www.learnopencv.com/neural-networks-a-30000-feet-view-for-beginners/) | |\n|[Alpha Blending using OpenCV (C++ / Python)](http://www.learnopencv.com/alpha-blending-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/AlphaBlending) |\n|[User stories : How readers of this blog are applying their knowledge to build applications](http://www.learnopencv.com/user-stories-how-readers-of-this-blog-are-applying-their-knowledge-to-build-applications/) | |\n|[How to select a bounding box ( ROI ) in OpenCV (C++/Python) ?](http://www.learnopencv.com/how-to-select-a-bounding-box-roi-in-opencv-cpp-python/) | |\n|[Automatic Red Eye Remover using OpenCV (C++ / Python)](http://www.learnopencv.com/automatic-red-eye-remover-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/RedEyeRemover) |\n|[Bias-Variance Tradeoff in Machine Learning](http://www.learnopencv.com/bias-variance-tradeoff-in-machine-learning/) | |\n|[Embedded Computer Vision: Which device should you choose?](http://www.learnopencv.com/embedded-computer-vision-which-device-should-you-choose/) | |\n|[Object Tracking using OpenCV (C++/Python)](http://www.learnopencv.com/object-tracking-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/tracking) |\n|[Handwritten Digits Classification : An OpenCV ( C++ / Python ) Tutorial](http://www.learnopencv.com/handwritten-digits-classification-an-opencv-c-python-tutorial/) | [Code](https://github.com/spmallick/learnopencv/tree/master/digits-classification) |\n|[Training a better Haar and LBP cascade based Eye Detector using OpenCV](http://www.learnopencv.com/training-better-haar-lbp-cascade-eye-detector-opencv/) | |\n|[Deep Learning Book Gift Recipients](http://www.learnopencv.com/deep-learning-book-gift-recipients/) | |\n|[Minified OpenCV Haar and LBP Cascades](http://www.learnopencv.com/minified-opencv-haar-and-lbp-cascades/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ninjaEyeDetector)|\n|[Deep Learning Book Gift](http://www.learnopencv.com/deep-learning-book-gift/) | |\n|[Histogram of Oriented Gradients](http://www.learnopencv.com/histogram-of-oriented-gradients/) | |\n|[Image Recognition and Object Detection : Part 1](http://www.learnopencv.com/image-recognition-and-object-detection-part1/) | |\n|[Head Pose Estimation using OpenCV and Dlib](http://www.learnopencv.com/head-pose-estimation-using-opencv-and-dlib/) | [Code](https://github.com/spmallick/learnopencv/tree/master/HeadPose) |\n|[Live CV : A Computer Vision Coding Application](http://www.learnopencv.com/live-cv/) | |\n|[Approximate Focal Length for Webcams and Cell Phone Cameras](http://www.learnopencv.com/approximate-focal-length-for-webcams-and-cell-phone-cameras/) | |\n|[Configuring Qt for OpenCV on OSX](http://www.learnopencv.com/configuring-qt-for-opencv-on-osx/) | [Code](https://github.com/spmallick/learnopencv/tree/master/qt-test) |\n|[Rotation Matrix To Euler Angles](http://www.learnopencv.com/rotation-matrix-to-euler-angles/) | [Code](https://github.com/spmallick/learnopencv/tree/master/RotationMatrixToEulerAngles) |\n|[Speeding up Dlibâ€™s Facial Landmark Detector](http://www.learnopencv.com/speeding-up-dlib-facial-landmark-detector/) | |\n|[Warp one triangle to another using OpenCV ( C++ / Python )](http://www.learnopencv.com/warp-one-triangle-to-another-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/WarpTriangle) |\n|[Average Face : OpenCV ( C++ / Python ) Tutorial](http://www.learnopencv.com/average-face-opencv-c-python-tutorial/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FaceAverage) |\n|[Face Swap using OpenCV ( C++ / Python )](http://www.learnopencv.com/face-swap-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FaceSwap) |\n|[Face Morph Using OpenCV â€” C++ / Python](http://www.learnopencv.com/face-morph-using-opencv-cpp-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FaceMorph) |\n|[Deep Learning Example using NVIDIA DIGITS 3 on EC2](http://www.learnopencv.com/deep-learning-example-using-nvidia-digits-3-on-ec2/) | |\n|[NVIDIA DIGITS 3 on EC2](http://www.learnopencv.com/nvidia-digits-3-on-ec2/) | |\n|[Homography Examples using OpenCV ( Python / C ++ )](http://www.learnopencv.com/homography-examples-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Homography) |\n|[Filling holes in an image using OpenCV ( Python / C++ )](http://www.learnopencv.com/filling-holes-in-an-image-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Holes) |\n|[How to find frame rate or frames per second (fps) in OpenCV ( Python / C++ ) ?](http://www.learnopencv.com/how-to-find-frame-rate-or-frames-per-second-fps-in-opencv-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FPS) |\n|[Delaunay Triangulation and Voronoi Diagram using OpenCV ( C++ / Python)](http://www.learnopencv.com/delaunay-triangulation-and-voronoi-diagram-using-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Delaunay) |\n|[OpenCV (C++ vs Python) vs MATLAB for Computer Vision](http://www.learnopencv.com/opencv-c-vs-python-vs-matlab-for-computer-vision/) | |\n|[Facial Landmark Detection](http://www.learnopencv.com/facial-landmark-detection/) | |\n|[Why does OpenCV use BGR color format ?](http://www.learnopencv.com/why-does-opencv-use-bgr-color-format/) | |\n|[Computer Vision for Predicting Facial Attractiveness](http://www.learnopencv.com/computer-vision-for-predicting-facial-attractiveness/) | [Code](https://github.com/spmallick/learnopencv/tree/master/FacialAttractiveness) |\n|[applyColorMap for pseudocoloring in OpenCV ( C++ / Python )](http://www.learnopencv.com/applycolormap-for-pseudocoloring-in-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Colormap) |\n|[Image Alignment (ECC) in OpenCV ( C++ / Python )](http://www.learnopencv.com/image-alignment-ecc-in-opencv-c-python/) | [Code](https://github.com/spmallick/learnopencv/tree/master/ImageAlignment) |\n|[How to find OpenCV version in Python and C++ ?](http://www.learnopencv.com/how-to-find-opencv-version-python-cpp/) | |\n|[Baidu banned from ILSVRC 2015](http://www.learnopencv.com/baidu-banned-from-ilsvrc-2015/) | |\n|[OpenCV Transparent API](http://www.learnopencv.com/opencv-transparent-api/) | |\n|[How Computer Vision Solved the Greatest Soccer Mystery of All Time](http://www.learnopencv.com/how-computer-vision-solved-the-greatest-soccer-mystery-of-all-times/) | |\n|[Embedded Vision Summit 2015](http://www.learnopencv.com/embedded-vision-summit-2015/) | |\n|[Read an Image in OpenCV ( Python, C++ )](http://www.learnopencv.com/read-an-image-in-opencv-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/imread) |\n|[Non-Photorealistic Rendering using OpenCV ( Python, C++ )](http://www.learnopencv.com/non-photorealistic-rendering-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/NonPhotorealisticRendering) |\n|[Seamless Cloning using OpenCV ( Python , C++ )](http://www.learnopencv.com/seamless-cloning-using-opencv-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/SeamlessCloning) |\n|[OpenCV Threshold ( Python , C++ )](http://www.learnopencv.com/opencv-threshold-python-cpp/) | [Code](https://github.com/spmallick/learnopencv/tree/master/Threshold) |\n|[Blob Detection Using OpenCV ( Python, C++ )](http://www.learnopencv.com/blob-detection-using-opencv-python-c/) | [Code](https://github.com/spmallick/learnopencv/tree/master/BlobDetector) |\n|[Turn your OpenCV Code into a Web API in under 10 minutes â€” Part 1](http://www.learnopencv.com/turn-your-opencv-Code-into-a-web-api-in-under-10-minutes-part-1/) | |\n|[How to compile OpenCV sample Code ?](http://www.learnopencv.com/how-to-compile-opencv-sample-Code/) | |\n|[Install OpenCV 3 on Yosemite ( OSX 10.10.x )](http://www.learnopencv.com/install-opencv-3-on-yosemite-osx-10-10-x/) | |\n', '{"language":"Jupyter Notebook","stars":22546,"forks":11734,"watchers":22546,"open_issues":276,"topics":["ai","computer-vision","computervision","deep-learning","deep-neural-networks","deeplearning","machine-learning","opencv","opencv-cpp","opencv-library","opencv-python","opencv-tutorial","opencv3"],"default_branch":"master","size_kb":3810062,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:kromydas:learnopencv","source_url":"https://github.com/kromydas/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:kromydas:learnopencv","source_url":"https://github.com/kromydas/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"},{"type":"has_code","target_id":"github:spmallick:learnopencv","source_url":"https://github.com/spmallick/learnopencv"}]', NULL, NULL, 'pending', 70, '536767570cb59fdfc243e97e8cd75081', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-spmallick-learnopencv from https://github.com/spmallick.png
Image converted to WebP: data/images/github-spmallick-learnopencv.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-MLEveryday-100-Days-Of-ML-Code', 'github--mleveryday--100-days-of-ml-code', '100-Days-Of-ML-Code', 'MLEveryday', 'è‹±æ–‡åŸç‰ˆè¯·ç§»æ­¥Avik-Jainã€‚æ•°æ®åœ¨è¿™é‡Œã€‚ ç¿»è¯‘å‰è¯·å…ˆé˜…è¯»è§„èŒƒã€‚å¸¸è§é—®é¢˜è§£ç­”è§FAQã€‚ - æœ‰ç›‘ç£å­¦ä¹  - æ•°æ®é¢„å¤„ç† - ç®€å•çº¿æ€§å›å½’ - å¤šå…ƒçº¿æ€§å›å½’ - é€»è¾‘å›å½’ - kè¿‘é‚»æ³•(k-NN) - æ”¯æŒå‘é‡æœº(SVM) - å†³ç­–æ ‘ - éšæœºæ£®æ— - æ— ç›‘ç£å­¦ä¹  - K-å‡å€¼èšç±» - å±‚æ¬¡èšç±» æ•°æ®é¢„å¤„ç†å®ç° <p align="center"> <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%201.jpg"> </p> ç®€å•çº¿æ€§å›å½’å®ç° <p align="center"> <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%202.jpg"> </p> å¤šå…ƒçº¿æ€§å›å½’å®ç° <p align="center"> <img src="https://github.com/MachineLearni...', '["100-days-of-ml-code","chinese-simplified","deep-learning","infographics","jupyter-notebook","keras","machine-learning","python","supervised-learning","tensorflow","tutorial","unsupervised-learning","jupyter notebook"]', 'other', 22120, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/MLEveryday/100-Days-Of-ML-Code","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# æœºå™¨å­¦ä¹ 100å¤©\n\nè‹±æ–‡åŸç‰ˆè¯·ç§»æ­¥[Avik-Jain](https://github.com/Avik-Jain/100-Days-Of-ML-Code)ã€‚æ•°æ®åœ¨[è¿™é‡Œ](https://github.com/MachineLearning100/100-Days-Of-ML-Code/tree/master/datasets)ã€‚\n\nç¿»è¯‘å‰è¯·å…ˆé˜…è¯»[è§„èŒƒ](Translation%20specification.MD)ã€‚å¸¸è§é—®é¢˜è§£ç­”è§[FAQ](FAQ.MD)ã€‚\n\n# ç›®å½•\n- æœ‰ç›‘ç£å­¦ä¹ \n  - [æ•°æ®é¢„å¤„ç†](#æ•°æ®é¢„å¤„ç†--ç¬¬1å¤©)\n  - [ç®€å•çº¿æ€§å›å½’](#ç®€å•çº¿æ€§å›å½’--ç¬¬2å¤©)\n  - [å¤šå…ƒçº¿æ€§å›å½’](#å¤šå…ƒçº¿æ€§å›å½’--ç¬¬3å¤©)\n  - [é€»è¾‘å›å½’](#é€»è¾‘å›å½’--ç¬¬4å¤©)\n  - [kè¿‘é‚»æ³•(k-NN)](#kè¿‘é‚»æ³•k-nn--ç¬¬7å¤©)\n  - [æ”¯æŒå‘é‡æœº(SVM)](#æ”¯æŒå‘é‡æœºsvm--ç¬¬12å¤©)\n  - [å†³ç­–æ ‘](#å†³ç­–æ ‘--ç¬¬23å¤©)\n  - [éšæœºæ£®æ—](#éšæœºæ£®æ—--ç¬¬33å¤©)\n- æ— ç›‘ç£å­¦ä¹ \n  - [K-å‡å€¼èšç±»](#k-å‡å€¼èšç±»--ç¬¬43å¤©)\n  - [å±‚æ¬¡èšç±»](#å±‚æ¬¡èšç±»--ç¬¬54å¤©)\n\n## æ•°æ®é¢„å¤„ç† | ç¬¬1å¤©\n[æ•°æ®é¢„å¤„ç†å®ç°](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%201_Data_Preprocessing.md)\n\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%201.jpg">\n</p>\n\n## ç®€å•çº¿æ€§å›å½’ | ç¬¬2å¤©\n[ç®€å•çº¿æ€§å›å½’å®ç°](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%202_Simple_Linear_Regression.md)\n\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%202.jpg">\n</p>\n\n## å¤šå…ƒçº¿æ€§å›å½’ | ç¬¬3å¤©\n[å¤šå…ƒçº¿æ€§å›å½’å®ç°](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%203_Multiple_Linear_Regression.md)\n\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%203.png">\n</p>\n\n## é€»è¾‘å›å½’ | ç¬¬4å¤©\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%204.jpg">\n</p>\n\n## é€»è¾‘å›å½’ | ç¬¬5å¤©\nä»Šå¤©æˆ‘æ·±å…¥ç ”ç©¶äº†é€»è¾‘å›å½’åˆ°åº•æ˜¯ä»€ä¹ˆï¼Œä»¥åŠå®ƒèƒŒåçš„æ•°å­¦æ˜¯ä»€ä¹ˆã€‚å­¦ä¹ äº†å¦‚ä½•è®¡ç®—ä»£ä»·å‡½æ•°ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•æ¥å°†ä»£ä»·å‡½æ•°é™ä½åˆ°æœ€å°ã€‚<br>\nç”±äºæ—¶é—´å…³ç³»ï¼Œæˆ‘å°†éš”å¤©å‘å¸ƒä¿¡æ¯å›¾ã€‚å¦‚æœæœ‰äººåœ¨æœºå™¨å­¦ä¹ é¢†åŸŸæœ‰ä¸€å®šç»éªŒï¼Œå¹¶æ„¿æ„å¸®æˆ‘ç¼–å†™ä»£ç æ–‡æ¡£ï¼Œä¹Ÿäº†è§£githubçš„Markdownè¯­æ³•ï¼Œè¯·åœ¨é¢†è‹±è”ç³»æˆ‘ã€‚\n\n## é€»è¾‘å›å½’ | ç¬¬6å¤©\n[é€»è¾‘å›å½’å®ç°](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%206_Logistic_Regression.md)\n\n## Kè¿‘é‚»æ³•(k-NN) | ç¬¬7å¤©\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%207.jpg">\n</p>\n\n## é€»è¾‘å›å½’èƒŒåçš„æ•°å­¦ | ç¬¬8å¤©\nä¸ºäº†ä½¿æˆ‘å¯¹é€»è¾‘å›å½’çš„è§è§£æ›´åŠ æ¸…æ™°ï¼Œæˆ‘åœ¨ç½‘ä¸Šæœç´¢äº†ä¸€äº›èµ„æºæˆ–æ–‡ç« ï¼Œç„¶åæˆ‘å°±å‘ç°äº†Saishruthi Swaminathançš„<a href = "https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc">è¿™ç¯‡æ–‡ç« </a><br>\n\nå®ƒç»™å‡ºäº†é€»è¾‘å›å½’çš„è¯¦ç»†æè¿°ã€‚è¯·åŠ¡å¿…çœ‹ä¸€çœ‹ã€‚\n\n## æ”¯æŒå‘é‡æœº(SVM) | ç¬¬9å¤©\nç›´è§‚äº†è§£SVMæ˜¯ä»€ä¹ˆä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒæ¥è§£å†³åˆ†ç±»é—®é¢˜ã€‚\n\n## æ”¯æŒå‘é‡æœºå’ŒKè¿‘é‚»æ³• | ç¬¬10å¤©\näº†è§£æ›´å¤šå…³äºSVMå¦‚ä½•å·¥ä½œå’Œå®ç°knnç®—æ³•çš„çŸ¥è¯†ã€‚\n\n## Kè¿‘é‚»æ³•(k-NN) | ç¬¬11å¤©\n[Kè¿‘é‚»æ³•(k-NN)å®ç°](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2011_K-NN.md)\n\n## æ”¯æŒå‘é‡æœº(SVM) | ç¬¬12å¤©\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2012.jpg">\n</p>\n\n## æ”¯æŒå‘é‡æœº(SVM) | ç¬¬13å¤©\n[SVMå®ç°](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2013_SVM.md)\n\n## æ”¯æŒå‘é‡æœº(SVM)çš„å®ç° | ç¬¬14å¤©\nä»Šå¤©æˆ‘åœ¨çº¿æ€§ç›¸å…³æ•°æ®ä¸Šå®ç°äº†SVMã€‚ä½¿ç”¨Scikit-Learnåº“ã€‚åœ¨scikit-learnä¸­æˆ‘ä»¬æœ‰SVCåˆ†ç±»å™¨ï¼Œæˆ‘ä»¬ç”¨å®ƒæ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ã€‚å°†åœ¨ä¸‹ä¸€æ¬¡å®ç°æ—¶ä½¿ç”¨kernel-trickã€‚Pythonä»£ç è§[æ­¤å¤„](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2013_SVM.py),Jupyter notebookè§[æ­¤å¤„](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2013_SVM.ipynb)ã€‚\n\n## æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨(Naive Bayes Classifier)å’Œé»‘ç›’æœºå™¨å­¦ä¹ (Black Box Machine Learning) | ç¬¬15å¤©\nå­¦ä¹ ä¸åŒç±»å‹çš„æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨åŒæ—¶å¼€å§‹<a href="https://bloomberg.github.io/foml/#home">Bloomberg</a>çš„è¯¾ç¨‹ã€‚è¯¾ç¨‹åˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ªæ˜¯é»‘ç›’æœºå™¨å­¦ä¹ ã€‚å®ƒç»™å‡ºäº†é¢„æµ‹å‡½æ•°ï¼Œç‰¹å¾æå–ï¼Œå­¦ä¹ ç®—æ³•ï¼Œæ€§èƒ½è¯„ä¼°ï¼Œäº¤å‰éªŒè¯ï¼Œæ ·æœ¬åå·®ï¼Œéå¹³ç¨³æ€§ï¼Œè¿‡åº¦æ‹Ÿåˆå’Œè¶…å‚æ•°è°ƒæ•´çš„æ•´ä½“è§‚ç‚¹ã€‚\n\n## é€šè¿‡å†…æ ¸æŠ€å·§å®ç°æ”¯æŒå‘é‡æœº | ç¬¬16å¤©\nä½¿ç”¨Scikit-Learnåº“å®ç°äº†SVMç®—æ³•ä»¥åŠå†…æ ¸å‡½æ•°ï¼Œè¯¥å‡½æ•°å°†æˆ‘ä»¬çš„æ•°æ®ç‚¹æ˜ å°„åˆ°æ›´é«˜ç»´åº¦ä»¥æ‰¾åˆ°æœ€ä½³è¶…å¹³é¢ã€‚\n\n## åœ¨Courseraå¼€å§‹æ·±åº¦å­¦ä¹ çš„ä¸“ä¸šè¯¾ç¨‹ | ç¬¬17å¤©\nåœ¨1å¤©å†…å®Œæˆç¬¬1å‘¨å’Œç¬¬2å‘¨å†…å®¹ä»¥åŠå­¦ä¹ è¯¾ç¨‹ä¸­çš„é€»è¾‘å›å½’ç¥ç»ç½‘ç»œã€‚\n\n## ç»§ç»­Courseraä¸Šçš„æ·±åº¦å­¦ä¹ ä¸“ä¸šè¯¾ç¨‹ | ç¬¬18å¤©\nå®Œæˆè¯¾ç¨‹1ã€‚ç”¨Pythonè‡ªå·±å®ç°ä¸€ä¸ªç¥ç»ç½‘ç»œã€‚\n\n## å­¦ä¹ é—®é¢˜å’ŒYaser Abu-Mostafaæ•™æˆ | ç¬¬19å¤©\nå¼€å§‹Yaser Abu-Mostafaæ•™æˆçš„Caltechæœºå™¨å­¦ä¹ è¯¾ç¨‹-CS156ä¸­çš„è¯¾ç¨‹1ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯å¯¹å³å°†åˆ°æ¥çš„è¯¾ç¨‹çš„ä¸€ç§ä»‹ç»ã€‚ä»–ä¹Ÿä»‹ç»äº†æ„ŸçŸ¥ç®—æ³•ã€‚\n\n## æ·±åº¦å­¦ä¹ ä¸“ä¸šè¯¾ç¨‹2 | ç¬¬20å¤©\nå®Œæˆæ”¹è¿›æ·±åº¦ç¥ç»ç½‘ç»œç¬¬1å‘¨å†…å®¹ï¼šå‚æ•°è°ƒæ•´ï¼Œæ­£åˆ™åŒ–å’Œä¼˜åŒ–ã€‚\n\n## ç½‘é¡µæœç½— | ç¬¬21å¤©\nè§‚çœ‹äº†ä¸€äº›å…³äºå¦‚ä½•ä½¿ç”¨Beautiful Soupè¿›è¡Œç½‘ç»œçˆ¬è™«çš„æ•™ç¨‹ï¼Œä»¥ä¾¿æ”¶é›†ç”¨äºæ„å»ºæ¨¡å‹çš„æ•°æ®ã€‚\n\n## å­¦ä¹ è¿˜å¯è¡Œå—? | ç¬¬22å¤©\nå®ŒæˆYaser Abu-Mostafaæ•™æˆçš„Caltechæœºå™¨å­¦ä¹ è¯¾ç¨‹-CS156ä¸­çš„è¯¾ç¨‹2ã€‚å­¦ä¹ Hoeffdingä¸ç­‰å¼ã€‚\n\n## å†³ç­–æ ‘ | ç¬¬23å¤©\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2023%20-%20Chinese.jpg">\n</p>\n\n## ç»Ÿè®¡å­¦ä¹ ç†è®ºçš„ä»‹ç» | ç¬¬24å¤©\nBloomberg MLè¯¾ç¨‹çš„ç¬¬3è¯¾ä»‹ç»äº†ä¸€äº›æ ¸å¿ƒæ¦‚å¿µï¼Œå¦‚è¾“å…¥ç©ºé—´ï¼ŒåŠ¨ä½œç©ºé—´ï¼Œç»“æœç©ºé—´ï¼Œé¢„æµ‹å‡½æ•°ï¼ŒæŸå¤±å‡½æ•°å’Œå‡è®¾ç©ºé—´ã€‚\n\n## å†³ç­–æ ‘ | ç¬¬25å¤©\n[å†³ç­–æ ‘å®ç°](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2025_Decision_Tree.md)\n\n## è·³åˆ°å¤ä¹ çº¿æ€§ä»£æ•° | ç¬¬26å¤©\nå‘ç°YouTubeä¸€ä¸ªç¥å¥‡çš„é¢‘é“[3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw)ï¼Œå®ƒæœ‰ä¸€ä¸ªæ’­æ”¾åˆ—è¡¨ã€Šçº¿æ€§ä»£æ•°çš„æœ¬è´¨ã€‹ã€‚çœ‹å®Œäº†4ä¸ªè§†é¢‘ï¼ŒåŒ…æ‹¬äº†å‘é‡ï¼Œçº¿æ€§ç»„åˆï¼Œè·¨åº¦ï¼ŒåŸºå‘é‡ï¼Œçº¿æ€§å˜æ¢å’ŒçŸ©é˜µä¹˜æ³•ã€‚\n\nBç«™æ’­æ”¾åˆ—è¡¨åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=9450)ã€‚\n\n## è·³åˆ°å¤ä¹ çº¿æ€§ä»£æ•° | ç¬¬27å¤©\nç»§ç»­è§‚çœ‹äº†4ä¸ªè§†é¢‘ï¼Œå†…å®¹åŒ…æ‹¬ä¸‰ç»´å˜æ¢ã€è¡Œåˆ—å¼ã€é€†çŸ©é˜µã€åˆ—ç©ºé—´ã€é›¶ç©ºé—´å’Œéæ–¹çŸ©é˜µã€‚\n\nBç«™æ’­æ”¾åˆ—è¡¨åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=9450)ã€‚\n\n## è·³åˆ°å¤ä¹ çº¿æ€§ä»£æ•° | ç¬¬28å¤©\nç»§ç»­è§‚çœ‹äº†3ä¸ªè§†é¢‘ï¼Œå†…å®¹åŒ…æ‹¬ç‚¹ç§¯å’Œå‰ç§¯ã€‚\n\nBç«™æ’­æ”¾åˆ—è¡¨åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=9450)ã€‚\n\n## è·³åˆ°å¤ä¹ çº¿æ€§ä»£æ•° | ç¬¬29å¤©\nè§‚çœ‹äº†å‰©ä½™çš„è§†é¢‘12åˆ°14ï¼Œå†…å®¹åŒ…æ‹¬ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼ï¼Œä»¥åŠæŠ½è±¡å‘é‡ç©ºé—´ã€‚\n\nBç«™æ’­æ”¾åˆ—è¡¨åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=9450)ã€‚\n\n## å¾®ç§¯åˆ†çš„æœ¬è´¨ | ç¬¬30å¤©\nå®Œæˆä¸Šä¸€æ’­æ”¾åˆ—è¡¨åï¼ŒYouTubeæ¨èäº†æ–°å†…å®¹ã€Šå¾®ç§¯åˆ†çš„æœ¬è´¨ã€‹ï¼Œä»Šå¤©çœ‹å®Œäº†å…¶ä¸­çš„3ä¸ªè§†é¢‘ï¼ŒåŒ…æ‹¬å¯¼æ•°ã€é“¾å¼æ³•åˆ™ã€ä¹˜ç§¯æ³•åˆ™å’ŒæŒ‡æ•°å¯¼æ•°ã€‚\n\nBç«™æ’­æ”¾åˆ—è¡¨åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=13407)ã€‚\n\n## å¾®ç§¯åˆ†çš„æœ¬è´¨ | ç¬¬31å¤©\nè§‚çœ‹äº†2ä¸ªè§†é¢‘ï¼Œå†…å®¹åŒ…æ‹¬éšåˆ†åŒ–ä¸æé™ã€‚\n\nBç«™æ’­æ”¾åˆ—è¡¨åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=13407)ã€‚\n\n## å¾®ç§¯åˆ†çš„æœ¬è´¨ | ç¬¬32å¤©\nè§‚çœ‹äº†å‰©ä½™çš„4ä¸ªè§†é¢‘ï¼Œå†…å®¹åŒ…æ‹¬ç§¯åˆ†ä¸é«˜é˜¶å¯¼æ•°ã€‚\n\nBç«™æ’­æ”¾åˆ—è¡¨åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=13407)ã€‚\n\n## éšæœºæ£®æ— | ç¬¬33å¤©\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2033.png">\n</p>\n\n## éšæœºæ£®æ— | ç¬¬34å¤©\n[éšæœºæ£®æ—å®ç°](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2034_Random_Forests.md)\n\n## ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ | æ·±åº¦å­¦ä¹ ï¼Œç¬¬1ç«  | ç¬¬ 35å¤©\nYoutubeé¢‘é“3Blue1Brownä¸­æœ‰ç²¾å½©çš„è§†é¢‘ä»‹ç»ç¥ç»ç½‘ç»œã€‚è¿™ä¸ªè§†é¢‘æä¾›äº†å¾ˆå¥½çš„è§£é‡Šï¼Œå¹¶ä½¿ç”¨æ‰‹å†™æ•°å­—æ•°æ®é›†æ¼”ç¤ºåŸºæœ¬æ¦‚å¿µã€‚\n\nBç«™è§†é¢‘åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=26587)ã€‚\n\n## æ¢¯åº¦ä¸‹é™æ³•ï¼Œç¥ç»ç½‘ç»œå¦‚ä½•å­¦ä¹  | æ·±åº¦å­¦ä¹ ï¼Œç¬¬2ç«  | ç¬¬36å¤©\nYoutubeé¢‘é“3Blue1Brownå…³äºç¥ç»ç½‘ç»œçš„ç¬¬2éƒ¨åˆ†ï¼Œè¿™ä¸ªè§†é¢‘ç”¨æœ‰è¶£çš„æ–¹å¼è§£é‡Šäº†æ¢¯åº¦ä¸‹é™æ³•ã€‚æ¨èå¿…é¡»è§‚çœ‹169.\n\nBç«™è§†é¢‘åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=26587)ã€‚\n\n## åå‘ä¼ æ’­æ³•ç©¶ç«Ÿåšä»€ä¹ˆï¼Ÿ | æ·±åº¦å­¦ä¹ ï¼Œç¬¬3ç«  | ç¬¬37å¤©\nYoutubeé¢‘é“3Blue1Brownå…³äºç¥ç»ç½‘ç»œçš„ç¬¬3éƒ¨åˆ†ï¼Œè¿™ä¸ªè§†é¢‘ä¸»è¦ä»‹ç»äº†åå¯¼æ•°å’Œåå‘ä¼ æ’­æ³•ã€‚\n\nBç«™è§†é¢‘åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=26587)ã€‚\n\n## åå‘ä¼ æ’­æ³•æ¼”ç®— | æ·±åº¦å­¦ä¹ ï¼Œç¬¬4ç«  | ç¬¬38å¤©\nYoutubeé¢‘é“3Blue1Brownå…³äºç¥ç»ç½‘ç»œçš„ç¬¬3éƒ¨åˆ†ï¼Œè¿™ä¸ªè§†é¢‘ä¸»è¦ä»‹ç»äº†åå¯¼æ•°å’Œåå‘ä¼ æ’­æ³•ã€‚\n\nBç«™è§†é¢‘åœ¨[è¿™é‡Œ](https://space.bilibili.com/88461692/#/channel/detail?cid=26587)ã€‚\n\n## ç¬¬1éƒ¨åˆ† | æ·±åº¦å­¦ä¹ åŸºç¡€Pythonï¼ŒTensorFlowå’ŒKeras | ç¬¬39å¤©\nè§†é¢‘åœ°å€åœ¨[è¿™é‡Œ](https://www.youtube.com/watch?v=wQ8BIBpya2k&t=19s&index=2&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN)ã€‚\n<br>ä¸­æ–‡æ–‡å­—ç‰ˆ[notebook](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2039.ipynb)ã€‚\n\n## ç¬¬2éƒ¨åˆ† | æ·±åº¦å­¦ä¹ åŸºç¡€Pythonï¼ŒTensorFlowå’ŒKeras | ç¬¬40å¤©\nè§†é¢‘åœ°å€åœ¨[è¿™é‡Œ](https://www.youtube.com/watch?v=wQ8BIBpya2k&t=19s&index=2&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN)ã€‚\n<br>ä¸­æ–‡æ–‡å­—ç‰ˆ[notebook](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2040.ipynb)ã€‚\n\n## ç¬¬3éƒ¨åˆ† | æ·±åº¦å­¦ä¹ åŸºç¡€Pythonï¼ŒTensorFlowå’ŒKeras | ç¬¬41å¤©\nè§†é¢‘åœ°å€åœ¨[è¿™é‡Œ](https://www.youtube.com/watch?v=wQ8BIBpya2k&t=19s&index=2&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN)ã€‚\n<br>ä¸­æ–‡æ–‡å­—ç‰ˆ[notebook](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2041.ipynb)ã€‚\n\n## ç¬¬4éƒ¨åˆ† | æ·±åº¦å­¦ä¹ åŸºç¡€Pythonï¼ŒTensorFlowå’ŒKeras | ç¬¬42å¤©\nè§†é¢‘åœ°å€åœ¨[è¿™é‡Œ](https://www.youtube.com/watch?v=wQ8BIBpya2k&t=19s&index=2&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN)ã€‚\n<br>ä¸­æ–‡æ–‡å­—ç‰ˆ[notebook](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Code/Day%2042.ipynb)ã€‚\n\n## K-å‡å€¼èšç±» | ç¬¬43å¤©\nè½¬åˆ°æ— ç›‘ç£å­¦ä¹ ï¼Œå¹¶ç ”ç©¶äº†èšç±»ã€‚å¯åœ¨[ä½œè€…ç½‘ç«™](http://www.avikjain.me/)æŸ¥è¯¢ã€‚å‘ç°ä¸€ä¸ªå¥‡å¦™çš„[åŠ¨ç”»](http://shabal.in/visuals/kmeans/6.html)æœ‰åŠ©äºç†è§£K-å‡å€¼èšç±»ã€‚\n\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2043.jpg">\n</p>\n\n## K-å‡å€¼èšç±» | ç¬¬44å¤©\nå®ç°ï¼ˆå¾…æ·»åŠ ä»£ç ï¼‰\n\n## æ·±å…¥ç ”ç©¶ | NUMPY | ç¬¬45å¤©\nå¾—åˆ°JK VanderPlaså†™çš„ä¹¦ã€ŠPythonæ•°æ®ç§‘å­¦æ‰‹å†Œï¼ˆPython Data Science HandBookï¼‰ã€‹ï¼ŒJupyter notebooksåœ¨[è¿™é‡Œ](https://github.com/jakevdp/PythonDataScienceHandbook)ã€‚\n<br>**[é«˜æ¸…ä¸­æ–‡ç‰ˆpdf](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Other%20Docs/Python%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6%E6%89%8B%E5%86%8C.zip)**\n<br>ç¬¬2ç« ï¼šNumPyä»‹ç»ï¼ŒåŒ…æ‹¬æ•°æ®ç±»å‹ã€æ•°ç»„å’Œæ•°ç»„è®¡ç®—ã€‚\n<br>ä»£ç å¦‚ä¸‹ï¼š\n<br>[2 NumPyå…¥é—¨](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.00-Introduction-to-NumPy.ipynb)\n<br>[2.1 ç†è§£Pythonä¸­çš„æ•°æ®ç±»å‹](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.01-Understanding-Data-Types.ipynb)\n<br>[2.2 NumPyæ•°ç»„åŸºç¡€](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.02-The-Basics-Of-NumPy-Arrays.ipynb)\n<br>[2.3 NumPyæ•°ç»„çš„è®¡ç®—ï¼šé€šç”¨å‡½æ•°](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.03-Computation-on-arrays-ufuncs.ipynb)\n\n## æ·±å…¥ç ”ç©¶ | NUMPY | ç¬¬46å¤©\nç¬¬2ç« ï¼š èšåˆ, æ¯”è¾ƒè¿ç®—ç¬¦å’Œå¹¿æ’­ã€‚\n<br>ä»£ç å¦‚ä¸‹ï¼š\n<br>[2.4 èšåˆï¼šæœ€å°å€¼ã€æœ€å¤§å€¼å’Œå…¶ä»–å€¼](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.04-Computation-on-arrays-aggregates.ipynb)\n<br>[2.5 æ•°ç»„çš„è®¡ç®—ï¼šå¹¿æ’­](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.05-Computation-on-arrays-broadcasting.ipynb)\n<br>[2.6 æ¯”è¾ƒã€æ©ç å’Œå¸ƒå°”è¿ç®—](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.06-Boolean-Arrays-and-Masks.ipynb)\n\n## æ·±å…¥ç ”ç©¶ | NUMPY | ç¬¬47å¤©\nç¬¬2ç« ï¼š èŠ±å“¨çš„ç´¢å¼•ï¼Œæ•°ç»„æ’åºï¼Œç»“æ„åŒ–æ•°æ®ã€‚\n<br>ä»£ç å¦‚ä¸‹ï¼š\n<br>[2.7 èŠ±å“¨çš„ç´¢å¼•](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.07-Fancy-Indexing.ipynb)\n<br>[2.8 æ•°ç»„çš„æ’åº](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.08-Sorting.ipynb)\n<br>[2.9 ç»“æ„åŒ–æ•°æ®ï¼šNumPyçš„ç»“æ„åŒ–æ•°ç»„](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.09-<br>Structured-Data-NumPy.ipynb)\n\n## æ·±å…¥ç ”ç©¶ | PANDAS | ç¬¬48å¤©\nç¬¬3ç« ï¼šPandasæ•°æ®å¤„ç†\n<br>åŒ…å«Pandaså¯¹è±¡ï¼Œæ•°æ®å–å€¼ä¸é€‰æ‹©ï¼Œæ•°å€¼è¿ç®—æ–¹æ³•ï¼Œå¤„ç†ç¼ºå¤±å€¼ï¼Œå±‚çº§ç´¢å¼•ï¼Œåˆå¹¶æ•°æ®é›†ã€‚\n<br>ä»£ç å¦‚ä¸‹ï¼š\n<br>[3 Pandasæ•°æ®å¤„ç†](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.00-Introduction-to-Pandas.ipynb)\n<br>[3.1 Pandaså¯¹è±¡ç®€ä»‹](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.01-Introducing-Pandas-Objects.ipynb)\n<br>[3.2 æ•°æ®å–å€¼ä¸é€‰æ‹©](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.02-Data-Indexing-and-Selection.ipynb)\n<br>[3.3 Pandasæ•°å€¼è¿ç®—æ–¹æ³•](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.03-Operations-in-Pandas.ipynb)\n<br>[3.4 å¤„ç†ç¼ºå¤±å€¼](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.04-Missing-Values.ipynb)\n<br>[3.5 å±‚çº§ç´¢å¼•](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.05-Hierarchical-Indexing.ipynb)\n<br>[3.6 åˆå¹¶æ•°æ®é›†ï¼šConCatå’ŒAppendæ–¹æ³•](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.06-Concat-And-Append.ipynb)\n\n## æ·±å…¥ç ”ç©¶ | PANDAS | ç¬¬49å¤©\nç¬¬3ç« ï¼šå®Œæˆå‰©ä½™å†…å®¹-åˆå¹¶ä¸è¿æ¥ï¼Œç´¯è®¡ä¸åˆ†ç»„ï¼Œæ•°æ®é€è§†è¡¨ã€‚\n<br>ä»£ç å¦‚ä¸‹ï¼š\n<br>[3.7 åˆå¹¶æ•°æ®é›†ï¼šåˆå¹¶ä¸è¿æ¥](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.07-Merge-and-Join.ipynb)\n<br>[3.8 ç´¯è®¡ä¸åˆ†ç»„](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.08-Aggregation-and-Grouping.ipynb)\n<br>[3.9 æ•°æ®é€è§†è¡¨](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.09-Pivot-Tables.ipynb)\n\n## æ·±å…¥ç ”ç©¶ | PANDAS | ç¬¬50å¤©\nç¬¬3ç« ï¼šå‘é‡åŒ–å­—ç¬¦ä¸²æ“ä½œï¼Œå¤„ç†æ—¶é—´åºåˆ—ã€‚\n<br>ä»£ç å¦‚ä¸‹ï¼š\n<br>[3.10 å‘é‡åŒ–å­—ç¬¦ä¸²æ“ä½œ](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.10-Working-With-Strings.ipynb)\n<br>[3.11 å¤„ç†æ—¶é—´åºåˆ—](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.11-Working-with-Time-Series.ipynb)\n<br>[3.12 é«˜æ€§èƒ½Pandasï¼ševal()ä¸query()](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.12-Performance-Eval-and-Query.ipynb)\n\n## æ·±å…¥ç ”ç©¶ | MATPLOTLIB | ç¬¬51å¤©\nç¬¬4ç« ï¼šMatplotlibæ•°æ®å¯è§†åŒ–\n<br>å­¦ä¹ ç®€æ˜“çº¿å½¢å›¾, ç®€æ˜“æ•£ç‚¹å›¾ï¼Œå¯†åº¦å›¾ä¸ç­‰é«˜çº¿å›¾.\n<br>ä»£ç å¦‚ä¸‹ï¼š\n<br>[4 Matplotlibæ•°æ®å¯è§†åŒ–](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.00-Introduction-To-Matplotlib.ipynb)\n<br>[4.1 ç®€æ˜“çº¿å½¢å›¾](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.01-Simple-Line-Plots.ipynb)\n<br>[4.2 ç®€æ˜“æ•£ç‚¹å›¾](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.02-Simple-Scatter-Plots.ipynb)\n<br>[4.3 å¯è§†åŒ–å¼‚å¸¸å¤„ç†](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.03-Errorbars.ipynb)\n<br>[4.4 å¯†åº¦å›¾ä¸ç­‰é«˜çº¿å›¾](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.04-Density-and-Contour-Plots.ipynb)\n\n## æ·±å…¥ç ”ç©¶ | MATPLOTLIB | ç¬¬52å¤©\nç¬¬4ç« ï¼šMatplotlibæ•°æ®å¯è§†åŒ–\n<br>å­¦ä¹ ç›´æ–¹å›¾ï¼Œé…ç½®å›¾ä¾‹ï¼Œé…ç½®é¢œè‰²æ¡ï¼Œå¤šå­å›¾ã€‚\n<br>ä»£ç å¦‚ä¸‹ï¼š \n<br>[4.5 ç›´æ–¹å›¾](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.05-Histograms-and-Binnings.ipynb)\n<br>[4.6 é…ç½®å›¾ä¾‹](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.06-Customizing-Legends.ipynb)\n<br>[4.7 é…ç½®é¢œè‰²æ¡](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.07-Customizing-Colorbars.ipynb)\n<br>[4.8 å¤šå­å›¾](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.08-Multiple-Subplots.ipynb)\n<br>[4.9 æ–‡å­—ä¸æ³¨é‡Š](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.09-Text-and-Annotation.ipynb)\n\n## æ·±å…¥ç ”ç©¶ | MATPLOTLIB | ç¬¬53å¤©\nç¬¬4ç« ï¼šMatplotlibæ•°æ®å¯è§†åŒ–\n<br>å­¦ä¹ ä¸‰ç»´ç»˜å›¾ã€‚\n<br>[4.12 ç”»ä¸‰ç»´å›¾](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.12-Three-Dimensional-Plotting.ipynb)\n\n## å±‚æ¬¡èšç±» | ç¬¬54å¤©\n[åŠ¨ç”»æ¼”ç¤º](https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Other%20Docs/%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB.gif)\n\n<p align="center">\n  <img src="https://github.com/MachineLearning100/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2054.jpg">\n</p>\n', '{"language":"Jupyter Notebook","stars":22120,"forks":5550,"watchers":22120,"open_issues":16,"topics":["100-days-of-ml-code","chinese-simplified","deep-learning","infographics","jupyter-notebook","keras","machine-learning","python","supervised-learning","tensorflow","tutorial","unsupervised-learning"],"default_branch":"master","size_kb":44918,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:MachineLearning100:100-Days-Of-ML-Code","source_url":"https://github.com/MachineLearning100/100-Days-Of-ML-Code"}]', NULL, 'MIT', 'approved', 80, '3884fd349558dc46f776a543750e6718', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-MLEveryday-100-Days-Of-ML-Code from https://github.com/MLEveryday.png
Image converted to WebP: data/images/github-MLEveryday-100-Days-Of-ML-Code.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-patchy631-ai-engineering-hub', 'github--patchy631--ai-engineering-hub', 'ai-engineering-hub', 'patchy631', '<p align="center"> <a href="https://trendshift.io/repositories/12800"> <img src="assets/TRENDING-BADGE.png" alt="Trending Badge" style="width: 250px; height: 55px;" width="250" height="55"/> </a> </p> <p align="center"> <img src="assets/ai-eng-hub.gif" alt="AI Engineering Hub Banner"> </p> --- Welcome to the **AI Engineering Hub** - your comprehensive resource for learning and building with AI! AI Engineering is advancing rapidly, and staying at the forefront requires both deep understanding ...', '["agents","ai","llms","machine-learning","mcp","rag","jupyter notebook"]', 'other', 22080, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/patchy631/ai-engineering-hub","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <a href="https://trendshift.io/repositories/12800">\n    <img src="assets/TRENDING-BADGE.png" alt="Trending Badge" style="width: 250px; height: 55px;" width="250" height="55"/>\n  </a>\n</p>\n\n<p align="center">\n  <img src="assets/ai-eng-hub.gif" alt="AI Engineering Hub Banner">\n</p>\n\n---\n\n# AI Engineering Hub ğŸš€\n\nWelcome to the **AI Engineering Hub** - your comprehensive resource for learning and building with AI!\n\n## ğŸŒŸ Why This Repo?\n\nAI Engineering is advancing rapidly, and staying at the forefront requires both deep understanding and hands-on experience. Here, you will find:\n- **93+ Production-Ready Projects** across all skill levels\n- In-depth tutorials on **LLMs, RAG, Agents, and more**\n- Real-world **AI agent** applications\n- Examples to implement, adapt, and scale in your projects\n\nWhether you''re a beginner, practitioner, or researcher, this repo provides resources for all skill levels to experiment and succeed in AI engineering.\n\n---\n\n## ğŸ“‹ Table of Contents\n\n- [Getting Started](#-getting-started)\n- [Newsletter](#-stay-updated-with-our-newsletter)\n- [Projects by Difficulty](#-projects-by-difficulty)\n  - [Beginner Projects (22)](#-beginner-projects)\n  - [Intermediate Projects (48)](#-intermediate-projects)\n  - [Advanced Projects (23)](#-advanced-projects)\n- [Contributing](#-contribute-to-the-ai-engineering-hub)\n- [License](#-license)\n\n---\n\n## ğŸ¯ Getting Started\n\nNew to AI Engineering? Start here:\n\n1. **Complete Beginners**: Check out the [AI Engineering Roadmap](./ai-engineering-roadmap) for a comprehensive learning path\n2. **Learn the Basics**: Start with [Beginner Projects](#-beginner-projects) like OCR apps and simple RAG implementations\n3. **Build Your Skills**: Move to [Intermediate Projects](#-intermediate-projects) with agents and complex workflows\n4. **Master Advanced Concepts**: Tackle [Advanced Projects](#-advanced-projects) including fine-tuning and production systems\n\n---\n\n## ğŸ“¬ Stay Updated with Our Newsletter!\n\n**Get a FREE Data Science eBook** ğŸ“– with 150+ essential lessons in Data Science when you subscribe to our newsletter! Stay in the loop with the latest tutorials, insights, and exclusive resources. [Subscribe now!](https://join.dailydoseofds.com)\n\n[![Daily Dose of Data Science Newsletter](https://github.com/patchy631/ai-engineering/blob/main/resources/join_ddods.png)](https://join.dailydoseofds.com)\n\n---\n\n## ğŸ“ Projects by Difficulty\n\n### ğŸŸ¢ Beginner Projects\n\nPerfect for getting started with AI engineering. These projects focus on single components and straightforward implementations.\n\n#### OCR & Vision\n- [**LaTeX OCR with Llama**](./LaTeX-OCR-with-Llama) - Convert LaTeX equation images to code using Llama 3.2 vision\n- [**Llama OCR**](./llama-ocr) - 100% local OCR app with Llama 3.2 and Streamlit\n- [**Gemma-3 OCR**](./gemma3-ocr) - Local OCR with structured text extraction using Gemma-3\n- [**Qwen 2.5 OCR**](./qwen-2.5VL-ocr) - Text extraction using Qwen 2.5 VL model\n\n#### Chat Interfaces & UI\n- [**Local ChatGPT with DeepSeek**](./local-chatgpt%20with%20DeepSeek) - Mini-ChatGPT with DeepSeek-R1 and Chainlit\n- [**Local ChatGPT with Llama**](./local-chatgpt) - ChatGPT clone using Llama 3.2 vision\n- [**Local ChatGPT with Gemma 3**](./local-chatgpt%20with%20Gemma%203) - Local chat interface with Gemma 3\n- [**DeepSeek Thinking UI**](./deepseek-thinking-ui) - ChatGPT with visible reasoning using DeepSeek-R1\n- [**Qwen3 Thinking UI**](./qwen3-thinking-ui) - Thinking UI with Qwen3:4B and Streamlit\n- [**GPT-OSS Thinking UI**](./gpt-oss-thinking-ui) - GPT-OSS with reasoning visualization\n- [**Streaming AI Chatbot**](./streaming-ai-chatbot) - Real-time AI streaming with Motia framework\n\n#### Basic RAG\n- [**Simple RAG Workflow**](./simple-rag-workflow) - Basic RAG with LlamaIndex and Ollama\n- [**Document Chat RAG**](./document-chat-rag) - Chat with documents using Llama 3.3\n- [**Fastest RAG Stack**](./fastest-rag-stack) - Fast RAG with SambaNova, LlamaIndex, and Qdrant\n- [**GitHub RAG**](./github-rag) - Chat with GitHub repos locally\n- [**ModernBERT RAG**](./modernbert-rag) - RAG with ModernBert embeddings\n- [**Llama 4 RAG**](./llama-4-rag) - RAG powered by Meta''s Llama 4\n\n#### Multimodal & Media\n- [**Image Generation with Janus-Pro**](./imagegen-janus-pro) - Local image generation with DeepSeek Janus-pro 7B\n- [**Video RAG with Gemini**](./video-rag-gemini) - Chat with videos using Gemini AI\n\n#### Other Tools\n- [**Website to API with FireCrawl**](./Website-to-API-with-FireCrawl) - Convert websites to APIs\n- [**AI News Generator**](./ai_news_generator) - News generation with CrewAI and Cohere\n- [**Siamese Network**](./siamese-network) - Digit similarity detection on MNIST\n\n---\n\n### ğŸŸ¡ Intermediate Projects\n\nMulti-component systems, agentic workflows, and advanced features for experienced practitioners.\n\n#### AI Agents & Workflows\n- [**YouTube Trend Analysis**](./Youtube-trend-analysis) - Analyze YouTube trends with CrewAI and BrightData\n- [**AutoGen Stock Analyst**](./autogen-stock-analyst) - Advanced analyst with Microsoft AutoGen\n- [**Agentic RAG**](./agentic_rag) - RAG with document search and web fallback\n- [**Agentic RAG with DeepSeek**](./agentic_rag_deepseek) - Enterprise agentic RAG with GroundX\n- [**Book Writer Flow**](./book-writer-flow) - Automated book writing with CrewAI\n- [**Content Planner Flow**](./content_planner_flow) - Content workflow with CrewAI Flow\n- [**Brand Monitoring**](./brand-monitoring) - Automated brand monitoring system\n- [**Hotel Booking Crew**](./hotel-booking-crew) - Multi-agent hotel booking with DeepSeek-R1\n- [**Deploy Agentic RAG**](./deploy-agentic-rag) - Private Agentic RAG API with LitServe\n- [**Zep Memory Assistant**](./zep-memory-assistant) - AI Agent with human-like memory\n- [**Agent with MCP Memory**](./agent-with-mcp-memory) - Agents with Graphiti memory and Opik\n- [**ACP Code**](./acp-code) - Agent Communication Protocol demo\n- [**Motia Content Creation**](./motia-content-creation) - Social media automation workflow\n\n#### Voice & Audio\n- [**Real-time Voice Bot**](./real-time-voicebot) - Conversational travel guide with AssemblyAI\n- [**RAG Voice Agent**](./rag-voice-agent) - Real-time RAG Voice Agent with Cartesia\n- [**Chat with Audios**](./chat-with-audios) - RAG over audio files\n- [**Audio Analysis Toolkit**](./audio-analysis-toolkit) - Audio analysis with AssemblyAI\n- [**Multilingual Meeting Notes**](./multilingual-meeting-notes-generator) - Auto meeting notes with language detection\n\n#### Advanced RAG\n- [**RAG with Dockling**](./rag-with-dockling) - RAG over Excel with IBM''s Docling\n- [**Trustworthy RAG**](./trustworthy-rag) - RAG over complex docs with TLM\n- [**Fastest RAG with Milvus and Groq**](./fastest-rag-milvus-groq) - Sub-15ms retrieval latency\n- [**Chat with Code**](./chat-with-code) - Chat with code using Qwen3-Coder\n- [**RAG SQL Router**](./rag-sql-router) - Agent with RAG and SQL routing\n\n#### Multimodal\n- [**DeepSeek Multimodal RAG**](./deepseek-multimodal-RAG) - MultiModal RAG with DeepSeek-Janus-Pro\n- [**ColiVara Website RAG**](./Colivara-deepseek-website-RAG) - MultiModal RAG for websites\n- [**Multimodal RAG with AssemblyAI**](./multimodal-rag-assemblyai) - Audio + vector database + CrewAI\n\n#### MCP (Model Context Protocol)\n- [**Cursor Linkup MCP**](./cursor_linkup_mcp) - Custom MCP with deep web search\n- [**EyeLevel MCP RAG**](./eyelevel-mcp-rag) - MCP for RAG over complex docs\n- [**LlamaIndex MCP**](./llamaindex-mcp) - Local MCP client with LlamaIndex\n- [**MCP Agentic RAG**](./mcp-agentic-rag) - MCP-powered Agentic RAG for Cursor\n- [**MCP Agentic RAG Firecrawl**](./mcp-agentic-rag-firecrawl) - Agentic RAG with Firecrawl\n- [**MCP Video RAG**](./mcp-video-rag) - Video RAG using Ragie via MCP\n- [**MCP Voice Agent**](./mcp-voice-agent) - Voice agent with Firecrawl and Supabase\n- [**SDV MCP**](./sdv-mcp) - Synthetic Data Vault orchestration\n- [**KitOps MCP**](./kitops-mcp) - ML model management with KitOps\n- [**Stagehand Ã— MCP-Use**](./stagehand%20x%20mcp-use) - Web automation with Stagehand MCP\n\n#### Model Comparison & Evaluation\n- [**Evaluation and Observability**](./eval-and-observability) - E2E RAG evaluation with CometML Opik\n- [**Llama 4 vs DeepSeek-R1**](./llama-4_vs_deepseek-r1) - Compare models using RAG\n- [**Qwen3 vs DeepSeek-R1**](./qwen3_vs_deepseek-r1) - Model comparison with Opik\n- [**O3 vs Claude Code**](./o3-vs-claude-code) - Compare Claude 3.7 and o3\n- [**Sonnet4 vs O4**](./sonnet4-vs-o4) - Code generation comparison\n- [**Sonnet4 vs Qwen3-Coder**](./sonnet4-vs-qwen3-coder) - Coder model comparison\n- [**Code Model Comparison**](./code-model-comparison) - Frontier model code comparison\n- [**GPT-OSS vs Qwen3**](./gpt-oss-vs-qwen3) - Reasoning capabilities comparison\n\n---\n\n### ğŸ”´ Advanced Projects\n\nComplex systems, fine-tuning, production deployments, and cutting-edge implementations.\n\n#### Fine-tuning & Model Development\n- [**DeepSeek Fine-tuning**](./DeepSeek-finetuning) - Fine-tune DeepSeek with Unsloth and Ollama\n- [**Build Reasoning Model**](./Build-reasoning-model) - Build DeepSeek-R1-like reasoning models\n- [**Attention Is All You Need Implementation**](./attention-is-all-you-need-impl) - Transformer architecture from scratch\n\n#### Advanced Agent Systems\n- [**NVIDIA Demo**](./nvidia-demo) - Documentation writer with CrewAI Flows and NVIDIA NIM\n- [**Documentation Writer Flow**](./documentation-writer-flow) - Agentic documentation workflow\n- [**Multi-Agent Deep Researcher**](./Multi-Agent-deep-researcher-mcp-windows-linux) - MCP-powered deep researcher\n- [**Multiplatform Deep Researcher**](./multiplatform_deep_researcher) - Multi-platform research with BrightData\n- [**Web Browsing Agent**](./web-browsing-agent) - Browser automation with CrewAI and Stagehand\n- [**Paralegal Agent Crew**](./paralegal-agent-crew) - Intelligent paralegal with RAG\n- [**FireCrawl Agent**](./firecrawl-agent) - Corrective RAG with web search fallback\n- [**Context Engineering Workflow**](./context-engineering-workflow) - Research assistant with TensorLake and Zep\n- [**Parlant Conversational Agent**](./parlant-conversational-agent) - Compliance-driven conversational agent\n- [**Stock Portfolio Analysis Agent**](./stock-portfolio-analysis-agent) - Portfolio analysis with React frontend\n- [**Guidelines vs Traditional Prompt**](./guidelines-vs-traditional-prompt) - Structured guidelines comparison\n\n#### Advanced MCP & Infrastructure\n- [**MindsDB MCP**](./mindsdb-mcp) - Unified MCP for all data sources\n- [**Financial Analyst DeepSeek**](./financial-analyst-deepseek) - MCP financial analysis workflow\n- [**Graphiti MCP**](./graphiti-mcp) - Persistent memory with Zep''s Graphiti\n- [**Pixeltable MCP**](./pixeltable-mcp) - Unified multimodal data orchestration\n- [**Ultimate AI Assistant**](./ultimate-ai-assitant-using-mcp) - Multi-MCP server interface\n\n#### Production Systems\n- [**GroundX Document Pipeline**](./groundX-doc-pipeline) - World-class document processing\n- [**NotebookLM Clone**](./notebook-lm-clone) - Full NotebookLM with RAG, citations, and podcasts\n\n#### Learning Resources\n- [**AI Engineering Roadmap**](./ai-engineering-roadmap) - Complete guide from Python to production AI\n\n---\n\n## ğŸ“¢ Contribute to the AI Engineering Hub!\n\nWe welcome contributors! Whether you want to add new tutorials, improve existing code, or report issues, your contributions make this community thrive. Here''s how to get involved:\n\n1. **Fork** the repository\n2. Create a new branch for your contribution\n3. Submit a **Pull Request** and describe the improvements\n\nCheck out our [contributing guidelines](CONTRIBUTING.md) for more details.\n\n---\n\n## ğŸ“œ License\n\nThis repository is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n---\n\n## ğŸ’¬ Connect\n\nFor discussions, suggestions, and more, feel free to [create an issue](https://github.com/patchy631/ai-engineering/issues) or reach out directly!\n\n**Happy Coding!** ğŸ‰\n', '{"language":"Jupyter Notebook","stars":22080,"forks":3612,"watchers":22080,"open_issues":114,"topics":["agents","ai","llms","machine-learning","mcp","rag"],"default_branch":"main","size_kb":267351,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:patchy631:ai-engineering","source_url":"https://github.com/patchy631/ai-engineering"},{"type":"has_code","target_id":"github:patchy631:ai-engineering","source_url":"https://github.com/patchy631/ai-engineering"}]', NULL, 'MIT', 'approved', 80, 'bb7a98ead00ed58e268a6573ad03b51a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-patchy631-ai-engineering-hub from https://github.com/patchy631.png
Image converted to WebP: data/images/github-patchy631-ai-engineering-hub.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-jina-ai-serve', 'github--jina-ai--serve', 'serve', 'jina-ai', '<a href="https://pypi.org/project/jina/"><img alt="PyPI" src="https://img.shields.io/pypi/v/jina?label=Release&style=flat-square"></a> <a href="https://discord.jina.ai"><img src="https://img.shields.io/discord/1106542220112302130?logo=discord&logoColor=white&style=flat-square"></a> <a href="https://pypistats.org/packages/jina"><img alt="PyPI - Downloads from official pypistats" src="https://img.shields.io/pypi/dm/jina?style=flat-square"></a> <a href="https://github.com/jina-ai/jina/actions/wo...', '["cloud-native","cncf","deep-learning","docker","fastapi","framework","generative-ai","grpc","jaeger","kubernetes","llmops","machine-learning","microservice","mlops","multimodal","neural-search","opentelemetry","orchestration","pipeline","prometheus","python"]', 'other', 21798, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/jina-ai/serve","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Jina-Serve\n<a href="https://pypi.org/project/jina/"><img alt="PyPI" src="https://img.shields.io/pypi/v/jina?label=Release&style=flat-square"></a>\n<a href="https://discord.jina.ai"><img src="https://img.shields.io/discord/1106542220112302130?logo=discord&logoColor=white&style=flat-square"></a>\n<a href="https://pypistats.org/packages/jina"><img alt="PyPI - Downloads from official pypistats" src="https://img.shields.io/pypi/dm/jina?style=flat-square"></a>\n<a href="https://github.com/jina-ai/jina/actions/workflows/cd.yml"><img alt="Github CD status" src="https://github.com/jina-ai/jina/actions/workflows/cd.yml/badge.svg"></a>\n\nJina-serve is a framework for building and deploying AI services that communicate via gRPC, HTTP and WebSockets. Scale your services from local development to production while focusing on your core logic.\n\n## Key Features\n\n- Native support for all major ML frameworks and data types\n- High-performance service design with scaling, streaming, and dynamic batching\n- LLM serving with streaming output\n- Built-in Docker integration and Executor Hub\n- One-click deployment to Jina AI Cloud\n- Enterprise-ready with Kubernetes and Docker Compose support\n\n<details>\n<summary><strong>Comparison with FastAPI</strong></summary>\n\nKey advantages over FastAPI:\n\n- DocArray-based data handling with native gRPC support\n- Built-in containerization and service orchestration\n- Seamless scaling of microservices\n- One-command cloud deployment\n</details>\n\n## Install \n\n```bash\npip install jina\n```\n\nSee guides for [Apple Silicon](https://jina.ai/serve/get-started/install/apple-silicon-m1-m2/) and [Windows](https://jina.ai/serve/get-started/install/windows/).\n\n## Core Concepts\n\nThree main layers:\n- **Data**: BaseDoc and DocList for input/output\n- **Serving**: Executors process Documents, Gateway connects services\n- **Orchestration**: Deployments serve Executors, Flows create pipelines\n\n## Build AI Services\n\nLet''s create a gRPC-based AI service using StableLM:\n\n```python\nfrom jina import Executor, requests\nfrom docarray import DocList, BaseDoc\nfrom transformers import pipeline\n\n\nclass Prompt(BaseDoc):\n    text: str\n\n\nclass Generation(BaseDoc):\n    prompt: str\n    text: str\n\n\nclass StableLM(Executor):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.generator = pipeline(\n            ''text-generation'', model=''stabilityai/stablelm-base-alpha-3b''\n        )\n\n    @requests\n    def generate(self, docs: DocList[Prompt], **kwargs) -> DocList[Generation]:\n        generations = DocList[Generation]()\n        prompts = docs.text\n        llm_outputs = self.generator(prompts)\n        for prompt, output in zip(prompts, llm_outputs):\n            generations.append(Generation(prompt=prompt, text=output))\n        return generations\n```\n\nDeploy with Python or YAML:\n\n```python\nfrom jina import Deployment\nfrom executor import StableLM\n\ndep = Deployment(uses=StableLM, timeout_ready=-1, port=12345)\n\nwith dep:\n    dep.block()\n```\n\n```yaml\njtype: Deployment\nwith:\n uses: StableLM\n py_modules:\n   - executor.py\n timeout_ready: -1\n port: 12345\n```\n\nUse the client:\n\n```python\nfrom jina import Client\nfrom docarray import DocList\nfrom executor import Prompt, Generation\n\nprompt = Prompt(text=''suggest an interesting image generation prompt'')\nclient = Client(port=12345)\nresponse = client.post(''/'', inputs=[prompt], return_type=DocList[Generation])\n```\n\n## Build Pipelines\n\nChain services into a Flow:\n\n```python\nfrom jina import Flow\n\nflow = Flow(port=12345).add(uses=StableLM).add(uses=TextToImage)\n\nwith flow:\n    flow.block()\n```\n\n## Scaling and Deployment\n\n### Local Scaling\n\nBoost throughput with built-in features:\n- Replicas for parallel processing\n- Shards for data partitioning\n- Dynamic batching for efficient model inference\n\nExample scaling a Stable Diffusion deployment:\n\n```yaml\njtype: Deployment\nwith:\n uses: TextToImage\n timeout_ready: -1\n py_modules:\n   - text_to_image.py\n env:\n  CUDA_VISIBLE_DEVICES: RR\n replicas: 2\n uses_dynamic_batching:\n   /default:\n     preferred_batch_size: 10\n     timeout: 200\n```\n\n### Cloud Deployment\n\n#### Containerize Services\n\n1. Structure your Executor:\n```\nTextToImage/\nâ”œâ”€â”€ executor.py\nâ”œâ”€â”€ config.yml\nâ”œâ”€â”€ requirements.txt\n```\n\n2. Configure:\n```yaml\n# config.yml\njtype: TextToImage\npy_modules:\n - executor.py\nmetas:\n name: TextToImage\n description: Text to Image generation Executor\n```\n\n3. Push to Hub:\n```bash\njina hub push TextToImage\n```\n\n#### Deploy to Kubernetes\n```bash\njina export kubernetes flow.yml ./my-k8s\nkubectl apply -R -f my-k8s\n```\n\n#### Use Docker Compose\n```bash\njina export docker-compose flow.yml docker-compose.yml\ndocker-compose up\n```\n\n#### JCloud Deployment\n\nDeploy with a single command:\n```bash\njina cloud deploy jcloud-flow.yml\n```\n\n## LLM Streaming\n\nEnable token-by-token streaming for responsive LLM applications:\n\n1. Define schemas:\n```python\nfrom docarray import BaseDoc\n\n\nclass PromptDocument(BaseDoc):\n    prompt: str\n    max_tokens: int\n\n\nclass ModelOutputDocument(BaseDoc):\n    token_id: int\n    generated_text: str\n```\n\n2. Initialize service:\n```python\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n\nclass TokenStreamingExecutor(Executor):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.model = GPT2LMHeadModel.from_pretrained(''gpt2'')\n```\n\n3. Implement streaming:\n```python\n@requests(on=''/stream'')\nasync def task(self, doc: PromptDocument, **kwargs) -> ModelOutputDocument:\n    input = tokenizer(doc.prompt, return_tensors=''pt'')\n    input_len = input[''input_ids''].shape[1]\n    for _ in range(doc.max_tokens):\n        output = self.model.generate(**input, max_new_tokens=1)\n        if output[0][-1] == tokenizer.eos_token_id:\n            break\n        yield ModelOutputDocument(\n            token_id=output[0][-1],\n            generated_text=tokenizer.decode(\n                output[0][input_len:], skip_special_tokens=True\n            ),\n        )\n        input = {\n            ''input_ids'': output,\n            ''attention_mask'': torch.ones(1, len(output[0])),\n        }\n```\n\n4. Serve and use:\n```python\n# Server\nwith Deployment(uses=TokenStreamingExecutor, port=12345, protocol=''grpc'') as dep:\n    dep.block()\n\n\n# Client\nasync def main():\n    client = Client(port=12345, protocol=''grpc'', asyncio=True)\n    async for doc in client.stream_doc(\n        on=''/stream'',\n        inputs=PromptDocument(prompt=''what is the capital of France ?'', max_tokens=10),\n        return_type=ModelOutputDocument,\n    ):\n        print(doc.generated_text)\n```\n\n## Support\n\nJina-serve is backed by [Jina AI](https://jina.ai) and licensed under [Apache-2.0](./LICENSE).\n', '{"language":"Python","stars":21798,"forks":2237,"watchers":21798,"open_issues":15,"topics":["cloud-native","cncf","deep-learning","docker","fastapi","framework","generative-ai","grpc","jaeger","kubernetes","llmops","machine-learning","microservice","mlops","multimodal","neural-search","opentelemetry","orchestration","pipeline","prometheus"],"default_branch":"master","size_kb":1681072,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:jina-ai:jina","source_url":"https://github.com/jina-ai/jina"},{"type":"has_code","target_id":"github:jina-ai:jina","source_url":"https://github.com/jina-ai/jina"}]', NULL, 'Apache-2.0', 'approved', 65, '4fbc92e72eddf31562ee31f22f403f2f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-jina-ai-serve from https://github.com/jina-ai.png
Image converted to WebP: data/images/github-jina-ai-serve.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-amusi-CVPR2025-Papers-with-Code', 'github--amusi--cvpr2025-papers-with-code', 'CVPR2025-Papers-with-Code', 'amusi', 'CVPR 2025 decisions are now available on OpenReviewï¼22.1% = 2878 / 13008 > æ³¨1ï¼šæ¬¢è¿å„ä½å¤§ä½¬æäº¤issueï¼Œåˆ†äº«CVPR 2025è®ºæ–‡å’Œå¼€æºé¡¹ç›®ï¼ > > æ³¨2ï¼šå…³äºå¾€å¹´CVé¡¶ä¼šè®ºæ–‡ä»¥åŠå…¶ä»–ä¼˜è´¨CVè®ºæ–‡å’Œå¤§ç›˜ç‚¹ï¼Œè¯¦è§ï¼š https://github.com/amusi/daily-paper-computer-vision > > - ICCV 2025 > - ECCV 2024 > - CVPR 2024 æ¬¢è¿æ‰«ç åŠ å…¥ã€CVerå­¦æœ¯äº¤æµç¾¤ã€‘ï¼Œå¯ä»¥è·å–CVPR 2025ç­‰æœ€å‰æ²¿å·¥ä½œï¼è¿™æ˜¯æœ€å¤§çš„è®¡ç®—æœºè§†è§‰AIçŸ¥è¯†æ˜Ÿçƒï¼æ¯æ—¥æ›´æ–°ï¼Œç¬¬ä¸€æ—¶é—´åˆ†äº«æœ€æ–°æœ€å‰æ²¿çš„è®¡ç®—æœºè§†è§‰ã€AIGCã€æ‰©æ•£æ¨¡å‹ã€å¤šæ¨¡æ€ã€æ·±åº¦å­¦ä¹ ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒå’Œé¥æ„Ÿç­‰æ–¹å‘çš„å­¦ä¹ èµ„æ–™ï¼Œå¿«åŠ å…¥å­¦èµ·æ¥ï¼ - 3DGS(Gaussian Splatting) - Agent) - Avatars - Backbone - CLIPEVOS - Mamba - Embodied AI - GAN - GNN - å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM) - å¤§è¯­è¨€æ¨¡å‹...', '["computer-vision","cvpr","cvpr2020","cvpr2021","cvpr2022","cvpr2023","cvpr2024","cvpr2025","deep-learning","image-processing","image-segmentation","machine-learning","object-detection","paper","python","semantic-segmentation","transformer","transformers","visual-tracking"]', 'other', 21599, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/amusi/CVPR2025-Papers-with-Code","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# CVPR 2025 è®ºæ–‡å’Œå¼€æºé¡¹ç›®åˆé›†(Papers with Code)\n\nCVPR 2025 decisions are now available on OpenReviewï¼22.1% = 2878 / 13008\n\n\n> æ³¨1ï¼šæ¬¢è¿å„ä½å¤§ä½¬æäº¤issueï¼Œåˆ†äº«CVPR 2025è®ºæ–‡å’Œå¼€æºé¡¹ç›®ï¼\n>\n> æ³¨2ï¼šå…³äºå¾€å¹´CVé¡¶ä¼šè®ºæ–‡ä»¥åŠå…¶ä»–ä¼˜è´¨CVè®ºæ–‡å’Œå¤§ç›˜ç‚¹ï¼Œè¯¦è§ï¼š https://github.com/amusi/daily-paper-computer-vision\n>\n> - [ICCV 2025](https://github.com/amusi/ICCV2025-Papers-with-Code)\n> - [ECCV 2024](https://github.com/amusi/ECCV2024-Papers-with-Code)\n> - [CVPR 2024](CVPR2024-Papers-with-Code.md)\n\næ¬¢è¿æ‰«ç åŠ å…¥ã€CVerå­¦æœ¯äº¤æµç¾¤ã€‘ï¼Œå¯ä»¥è·å–CVPR 2025ç­‰æœ€å‰æ²¿å·¥ä½œï¼è¿™æ˜¯æœ€å¤§çš„è®¡ç®—æœºè§†è§‰AIçŸ¥è¯†æ˜Ÿçƒï¼æ¯æ—¥æ›´æ–°ï¼Œç¬¬ä¸€æ—¶é—´åˆ†äº«æœ€æ–°æœ€å‰æ²¿çš„è®¡ç®—æœºè§†è§‰ã€AIGCã€æ‰©æ•£æ¨¡å‹ã€å¤šæ¨¡æ€ã€æ·±åº¦å­¦ä¹ ã€è‡ªåŠ¨é©¾é©¶ã€åŒ»ç–—å½±åƒå’Œé¥æ„Ÿç­‰æ–¹å‘çš„å­¦ä¹ èµ„æ–™ï¼Œå¿«åŠ å…¥å­¦èµ·æ¥ï¼\n\n![](CVerå­¦æœ¯äº¤æµç¾¤.png)\n\n# ã€CVPR 2025 è®ºæ–‡å¼€æºç›®å½•ã€‘\n\n- [3DGS(Gaussian Splatting)](#3DGS)\n- [Agent)](#Agent)\n- [Avatars](#Avatars)\n- [Backbone](#Backbone)\n- [CLIP](#CLIP)EVOS\n- [Mamba](#Mamba)\n- [Embodied AI](#Embodied-AI)\n- [GAN](#GAN)\n- [GNN](#GNN)\n- [å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)](#MLLM)\n- [å¤§è¯­è¨€æ¨¡å‹(LLM)](#LLM)\n- [NAS](#NAS)\n- [OCR](#OCR)\n- [NeRF](#NeRF)\n- [DETR](#DETR)\n- [æ‰©æ•£æ¨¡å‹(Diffusion Models)](#Diffusion)\n- [ReID(é‡è¯†åˆ«)](#ReID)\n- [é•¿å°¾åˆ†å¸ƒ(Long-Tail)](#Long-Tail)\n- [Vision Transformer](#Vision-Transformer)\n- [è§†è§‰å’Œè¯­è¨€(Vision-Language)](#VL)\n- [è‡ªç›‘ç£å­¦ä¹ (Self-supervised Learning)](#SSL)\n- [æ•°æ®å¢å¼º(Data Augmentation)](#DA)\n- [ç›®æ ‡æ£€æµ‹(Object Detection)](#Object-Detection)\n- [å¼‚å¸¸æ£€æµ‹(Anomaly Detection)](#Anomaly-Detection)\n- [ç›®æ ‡è·Ÿè¸ª(Visual Tracking)](#VT)\n- [è¯­ä¹‰åˆ†å‰²(Semantic Segmentation)](#Semantic-Segmentation)\n- [å®ä¾‹åˆ†å‰²(Instance Segmentation)](#Instance-Segmentation)\n- [å…¨æ™¯åˆ†å‰²(Panoptic Segmentation)](#Panoptic-Segmentation)\n- [åŒ»å­¦å›¾åƒ(Medical Image)](#MI)\n- [åŒ»å­¦å›¾åƒåˆ†å‰²(Medical Image Segmentation)](#MIS)\n- [è§†é¢‘ç›®æ ‡åˆ†å‰²(Video Object Segmentation)](#VOS)\n- [è§†é¢‘å®ä¾‹åˆ†å‰²(Video Instance Segmentation)](#VIS)\n- [å‚è€ƒå›¾åƒåˆ†å‰²(Referring Image Segmentation)](#RIS)\n- [å›¾åƒæŠ å›¾(Image Matting)](#Matting)\n- [å›¾åƒç¼–è¾‘(Image Editing)](#Image-Editing)\n- [Low-level Vision](#LLV)\n- [è¶…åˆ†è¾¨ç‡(Super-Resolution)](#SR)\n- [å»å™ª(Denoising)](#Denoising)\n- [å»æ¨¡ç³Š(Deblur)](#Deblur)\n- [è‡ªåŠ¨é©¾é©¶(Autonomous Driving)](#Autonomous-Driving)\n- [3Dç‚¹äº‘(3D Point Cloud)](#3D-Point-Cloud)\n- [3Dç›®æ ‡æ£€æµ‹(3D Object Detection)](#3DOD)\n- [3Dè¯­ä¹‰åˆ†å‰²(3D Semantic Segmentation)](#3DSS)\n- [3Dç›®æ ‡è·Ÿè¸ª(3D Object Tracking)](#3D-Object-Tracking)\n- [3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨(3D Semantic Scene Completion)](#3DSSC)\n- [3Dé…å‡†(3D Registration)](#3D-Registration)\n- [3Däººä½“å§¿æ€ä¼°è®¡(3D Human Pose Estimation)](#3D-Human-Pose-Estimation)\n- [3Däººä½“Meshä¼°è®¡(3D Human Mesh Estimation)](#3D-Human-Pose-Estimation)\n- [3D Visual Grounding(3Dè§†è§‰å®šä½)](#3DVG)\n- [åŒ»å­¦å›¾åƒ(Medical Image)](#Medical-Image)\n- [å›¾åƒç”Ÿæˆ(Image Generation)](#Image-Generation)\n- [è§†é¢‘ç”Ÿæˆ(Video Generation)](#Video-Generation)\n- [3Dç”Ÿæˆ(3D Generation)](#3D-Generation)\n- [è§†é¢‘ç†è§£(Video Understanding)](#Video-Understanding)\n- [è¡Œä¸ºæ£€æµ‹(Action Detection)](#Action-Detection)\n- [å…·èº«æ™ºèƒ½(Embodied AI)](#Embodied)\n- [æ–‡æœ¬æ£€æµ‹(Text Detection)](#Text-Detection)\n- [çŸ¥è¯†è’¸é¦(Knowledge Distillation)](#KD)\n- [æ¨¡å‹å‰ªæ(Model Pruning)](#Pruning)\n- [å›¾åƒå‹ç¼©(Image Compression)](#IC)\n- [ä¸‰ç»´é‡å»º(3D Reconstruction)](#3D-Reconstruction)\n- [æ·±åº¦ä¼°è®¡(Depth Estimation)](#Depth-Estimation)\n- [è½¨è¿¹é¢„æµ‹(Trajectory Prediction)](#TP)\n- [è½¦é“çº¿æ£€æµ‹(Lane Detection)](#Lane-Detection)\n- [å›¾åƒæè¿°(Image Captioning)](#Image-Captioning)\n- [è§†è§‰é—®ç­”(Visual Question Answering)](#VQA)\n- [æ‰‹è¯­è¯†åˆ«(Sign Language Recognition)](#SLR)\n- [è§†é¢‘é¢„æµ‹(Video Prediction)](#Video-Prediction)\n- [æ–°è§†ç‚¹åˆæˆ(Novel View Synthesis)](#NVS)\n- [Zero-Shot Learning(é›¶æ ·æœ¬å­¦ä¹ )](#ZSL)\n- [ç«‹ä½“åŒ¹é…(Stereo Matching)](#Stereo-Matching)\n- [ç‰¹å¾åŒ¹é…(Feature Matching)](#Feature-Matching)\n- [æš—å…‰å›¾åƒå¢å¼º(Low-light Image Enhancement)](#Low-light)\n- [åœºæ™¯å›¾ç”Ÿæˆ(Scene Graph Generation)](#SGG)\n- [é£æ ¼è¿ç§»(Style Transfer)](#ST)\n- [éšå¼ç¥ç»è¡¨ç¤º(Implicit Neural Representations)](#INR)\n- [å›¾åƒè´¨é‡è¯„ä»·(Image Quality Assessment)](#IQA)\n- [è§†é¢‘è´¨é‡è¯„ä»·(Video Quality Assessment)](#Video-Quality-Assessment)\n- [å‹ç¼©æ„ŸçŸ¥(Compressive Sensing)](#CS)\n- [æ•°æ®é›†(Datasets)](#Datasets)\n- [æ–°ä»»åŠ¡(New Tasks)](#New-Tasks)\n- [å…¶ä»–(Others)](#Others)\n\n<a name="3DGS"></a>\n\n# 3DGS(Gaussian Splatting)\n\n\n<a name="Agent"></a>\n\n# Agent\n\n**SpiritSight Agent: Advanced GUI Agent with One Look**\n\n- Paper: https://arxiv.org/abs/2503.03196\n- Code: https://hzhiyuan.github.io/SpiritSight-Agent\n\n\n<a name="Avatars"></a>\n\n# Avatars\n\n\n# Backbone\n\n**Building Vision Models upon Heat Conduction**\n\n- Paper: https://arxiv.org/abs/2405.16555\n- Code: https://github.com/MzeroMiko/vHeat\n\n**LSNet: See Large, Focus Small**\n\n- Paper: https://arxiv.org/abs/2503.23135\n- Code: https://github.com/jameslahm/lsnet\n\n\n<a name="CLIP"></a>\n\n# CLIP\n\n\n\n<a name="Mamba"></a>\n\n# Mamba\n\n\n**MambaVision: A Hybrid Mamba-Transformer Vision Backbone**\n\n- Paper: https://arxiv.org/abs/2407.08083\n- Code: https://github.com/NVlabs/MambaVision\n\n**MobileMamba: Lightweight Multi-Receptive Visual Mamba Network**\n\n- Paper: https://arxiv.org/abs/2411.15941\n- Code: https://github.com/lewandofskee/MobileMamba\n\n**MambaIC: State Space Models for High-Performance Learned Image Compression**\n\n- Paper: https://arxiv.org/abs/2503.12461\n- Code: https://arxiv.org/abs/2503.12461\n\n<a name="Embodied-AI"></a>\n\n# Embodied AI\n\n**CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos**\n\n- Project: https://ai4ce.github.io/CityWalker/\n- Paper: https://arxiv.org/abs/2411.17820\n- Code: https://github.com/ai4ce/CityWalker\n\n\n<a name="GAN"></a>\n\n# GAN\n\n<a name="OCR"></a>\n\n# OCR\n\n\n<a name="NeRF"></a>\n\n# NeRF\n\n\n\n<a name="DETR"></a>\n\n# DETR\n\n**Mr. DETR: Instructive Multi-Route Training for Detection Transformers**\n\n- Paper: https://arxiv.org/abs/2412.10028\n- Code: https://github.com/Visual-AI/Mr.DETR\n\n\n<a name="Prompt"></a>\n\n# Prompt\n\n<a name="MLLM"></a>\n\n# å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM)\n\n**LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences**\n\n- Paperï¼š https://arxiv.org/abs/2412.01292\n- Code: https://github.com/Hoyyyaard/LSceneLLM\n\n\n**DynRefer: Delving into Region-level Multimodal Tasks via Dynamic Resolution**\n\n- Paper: https://arxiv.org/abs/2405.16071\n- Code: https://github.com/callsys/DynRefer\n\n\n**Retrieval-Augmented Personalization for Multimodal Large Language Models**\n\n- Project Page: https://hoar012.github.io/RAP-Project/\n- Paper: https://arxiv.org/abs/2410.13360\n- Code: https://github.com/Hoar012/RAP-MLLM\n\n**BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models**\n\n- Paper: https://arxiv.org/abs/2411.15232\n- Code: https://github.com/HealthX-Lab/BiomedCoOp\n\n**FlashSloth: Lightning Multimodal Large Language Models via Embedded Visual Compression**\n\n- Paper: https://arxiv.org/abs/2412.04317\n- Code: https://github.com/codefanw/FlashSloth\n\n**MMRL: Multi-Modal Representation Learning for Vision-Language Models**\n\n- Paper: https://arxiv.org/abs/2503.08497\n- Code: https://github.com/yunncheng/MMRL\n\n**PAVE: Patching and Adapting Video Large Language Models**\n\n- Paper: https://arxiv.org/abs/2503.19794\n- Code: https://github.com/dragonlzm/PAVE\n\n**AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization**\n\n- Paper: https://arxiv.org/abs/2503.23733\n- Code: https://github.com/THUNLP-MT/AdaMMS\n\n\n<a name="LLM"></a>\n\n# å¤§è¯­è¨€æ¨¡å‹(LLM)\n\n\n\n\n<a name="NAS"></a>\n\n# NAS\n\n<a name="ReID"></a>\n\n# ReID(é‡è¯†åˆ«)\n\n**From Poses to Identity: Training-Free Person Re-Identification via Feature Centralization**\n\n- Paper: https://arxiv.org/abs/2503.00938\n- Code: https://github.com/yuanc3/Pose2ID\n\n\n**AirRoom: Objects Matter in Room Reidentification**\n\n- Project: https://sairlab.org/airroom/\n- Paper: https://arxiv.org/abs/2503.01130\n\n\n**IDEA: Inverted Text with Cooperative Deformable Aggregation for Multi-modal Object Re-Identification**\n\n- Paper: https://arxiv.org/abs/2503.10324\n- Code: https://github.com/924973292/IDEA\n\n\n\n<a name="Diffusion"></a>\n\n# æ‰©æ•£æ¨¡å‹(Diffusion Models)\n\n**TinyFusion: Diffusion Transformers Learned Shallow**\n\n- Paper: https://arxiv.org/abs/2412.01199\n- Code: https://github.com/VainF/TinyFusion\n\n**DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any Architecture**\n\n- Paper: https://arxiv.org/abs/2409.03550\n- Code: https://github.com/qianlong0502/DKDM\n\n**Tiled Diffusion**\n\n- Homepage: https://madaror.github.io/tiled-diffusion.github.io/\n- Paper: https://arxiv.org/abs/2412.15185\n- Code: https://github.com/madaror/tiled-diffusion\n\n\n<a name="Vision-Transformer"></a>\n\n# Vision Transformer\n\n\n\n<a name="VL"></a>\n\n# è§†è§‰å’Œè¯­è¨€(Vision-Language)\n\n**NLPrompt: Noise-Label Prompt Learning for Vision-Language Models**\n\n- Paper: https://arxiv.org/abs/2412.01256\n- Code: https://github.com/qunovo/NLPrompt\n\n**PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability**\n\n- Paper: https://arxiv.org/abs/2503.08481\n- Code: https://github.com/unira-zwj/PhysVLM\n\n**MMRL: Multi-Modal Representation Learning for Vision-Language Models**\n\n- Paper: https://arxiv.org/abs/2503.08497\n- Code: https://github.com/yunncheng/MMRL\n\n\n<a name="Object-Detection"></a>\n\n# ç›®æ ‡æ£€æµ‹(Object Detection)\n\n\n**LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models**\n\n- Paper: https://arxiv.org/abs/2501.18954\n- Codeï¼šhttps://github.com/iSEE-Laboratory/LLMDet\n\n**Mr. DETR: Instructive Multi-Route Training for Detection Transformers**\n\n- Paper: https://arxiv.org/abs/2412.10028\n- Code: https://github.com/Visual-AI/Mr.DETR\n\n\n<a name="Anomaly-Detection"></a>\n\n# å¼‚å¸¸æ£€æµ‹(Anomaly Detection)\n\n\n\n<a name="VT"></a>\n\n# ç›®æ ‡è·Ÿè¸ª(Object Tracking)\n\n**Multiple Object Tracking as ID Prediction**\n\n- Paperï¼šhttps://arxiv.org/abs/2403.16848\n- Code: https://github.com/MCG-NJU/MOTIP\n\n**Omnidirectional Multi-Object Tracking**\n\n- Paper:https://arxiv.org/abs/2503.04565\n- Code:https://github.com/xifen523/OmniTrack\n\n\n<a name="MI"></a>\n\n# åŒ»å­¦å›¾åƒ(Medical Image)\n\n\n**BrainMVP: Multi-modal Vision Pre-training for Medical Image Analysis**\n\n- Paper: https://arxiv.org/abs/2410.10604\n- Code: https://github.com/shaohao011/BrainMVP\n\n\n# åŒ»å­¦å›¾åƒåˆ†å‰²(Medical Image Segmentation)\n\n**Test-Time Domain Generalization via Universe Learning: A Multi-Graph Matching Approach for Medical Image Segmentation**\n\n- Paper: https://arxiv.org/abs/2503.13012\n- Code: https://github.com/Yore0/TTDG-MGM\n\n\n<a name="Autonomous-Driving"></a>\n\n# è‡ªåŠ¨é©¾é©¶(Autonomous Driving)\n\n**LiMoE: Mixture of LiDAR Representation Learners from Automotive Scenes**\n\n- Project: https://ldkong.com/LiMoE\n- Paper: https://arxiv.org/abs/2501.04004\n- Code: https://github.com/Xiangxu-0103/LiMoE\n\n\n\n# 3Dç‚¹äº‘(3D-Point-Cloud)\n\n**Unlocking Generalization Power in LiDAR Point Cloud Registration**\n\n- Paper: https://arxiv.org/abs/2503.10149\n- Code: https://github.com/peakpang/UGP\n\n\n<a name="3DOD"></a>\n\n# 3Dç›®æ ‡æ£€æµ‹(3D Object Detection)\n\n\n\n<a name="3DOD"></a>\n\n# 3Dè¯­ä¹‰åˆ†å‰²(3D Semantic Segmentation)\n\n\n\n\n\n<a name="LLV"></a>\n\n# Low-level Vision\n\n\n\n<a name="SR"></a>\n\n# è¶…åˆ†è¾¨ç‡(Super-Resolution)\n\n**AESOP: Auto-Encoded Supervision for Perceptual Image Super-Resolution**\n\n- Paper: https://arxiv.org/abs/2412.00124\n- Code: https://github.com/2minkyulee/AESOP-Auto-Encoded-Supervision-for-Perceptual-Image-Super-Resolution\n\n\n<a name="Denoising"></a>\n\n# å»å™ª(Denoising)\n\n## å›¾åƒå»å™ª(Image Denoising)\n\n<a name="3D-Human-Pose-Estimation"></a>\n\n# 3Däººä½“å§¿æ€ä¼°è®¡(3D Human Pose Estimation)\n\n**Reconstructing Humans with a Biomechanically Accurate Skeleton**\n\n- Homepage: https://isshikihugh.github.io/HSMR/\n- Code: https://github.com/IsshikiHugh/HSMR\n\n<a name="3DVG"></a>\n\n#3D Visual Grounding(3Dè§†è§‰å®šä½)\n\n**ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding**\n\n- Homepage: https://pqh22.github.io/projects/ProxyTransformation/index.html\n\n- Code: https://github.com/pqh22/ProxyTransformation\n\n- Paper: https://arxiv.org/abs/2502.19247\n\n\n<a name="Image-Generation"></a>\n\n# å›¾åƒç”Ÿæˆ(Image Generation)\n\n**Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models**\n\n- Paper: https://arxiv.org/abs/2501.01423\n- Code: https://github.com/hustvl/LightningDiT\n\n**SleeperMark: Towards Robust Watermark against Fine-Tuning Text-to-image Diffusion Models**\n\n- Paper: https://arxiv.org/abs/2412.04852\n- Code: https://github.com/taco-group/SleeperMark\n\n\n**TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation**\n\n- Homepage: https://byteflow-ai.github.io/TokenFlow/\n- Code: https://github.com/ByteFlow-AI/TokenFlow\n- Paper:https://arxiv.org/abs/2412.03069\n\n**PAR: Parallelized Autoregressive Visual Generation**\n\n- Project: https://epiphqny.github.io/PAR-project/\n- Paper: https://arxiv.org/abs/2412.15119\n- Code: https://github.com/Epiphqny/PAR\n\n\n**Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis**\n\n- Project: https://generative-photography.github.io/project/\n- Paper: https://arxiv.org/abs/2412.02168\n- Code: https://github.com/pandayuanyu/generative-photography\n\n\n**OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation**\n\n- Project Page: https://opening-benchmark.github.io/\n- Paper: https://arxiv.org/abs/2411.18499).\n- Code: https://github.com/LanceZPF/OpenING\n\n\n\n\n<a name="Video-Generation"></a>\n\n# è§†é¢‘ç”Ÿæˆ(Video Generation)\n\n**Identity-Preserving Text-to-Video Generation by Frequency Decomposition**\n\n- Paper: https://arxiv.org/abs/2411.17440\n- Code: https://github.com/PKU-YuanGroup/ConsisID\n\n\n**Cinemo: Consistent and Controllable Image Animation with Motion Diffusion Models**\n\n- Paper: https://arxiv.org/abs/2407.15642\n- Code: https://github.com/maxin-cn/Cinemo\n\n**X-Dyna: Expressive Dynamic Human Image Animation**\n\n- Paper: https://arxiv.org/abs/2501.10021\n- Code: https://github.com/bytedance/X-Dyna\n\n**PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation**\n\n- Paper: https://arxiv.org/pdf/2412.00596\n- Code: https://github.com/pittisl/PhyT2V\n\n\n**Timestep Embedding Tells: It''s Time to Cache for Video Diffusion Model**\n\n- Project: https://liewfeng.github.io/TeaCache/\n- Paper: https://arxiv.org/abs/2411.19108\n- Code: https://github.com/ali-vilab/TeaCache\n\n\n**AR-Diffusion: Asynchronous Video Generation with Auto-Regressive Diffusion**\n\n- Project: https://iva-mzsun.github.io/AR-Diffusion\n- Paper: https://arxiv.org/abs/2503.07418\n- Code: https://github.com/iva-mzsun/AR-Diffusion\n\n\n<a name="Image-Editing"></a>\n\n# å›¾åƒç¼–è¾‘(Image Editing)\n\n**Edit Away and My Face Will not Stay: Personal Biometric Defense against Malicious Generative Editing**\n\n- Paper: https://arxiv.org/abs/2411.16832\n- Code: https://github.com/taco-group/FaceLock\n\n\n**h-Edit: Effective and Flexible Diffusion-Based Editing via Doobâ€™s h-Transform**\n\n- Paper: https://arxiv.org/abs/2503.02187\n- Code: https://github.com/nktoan/h-edit\n\n\n<a name="Video-Editing"></a>\n\n# è§†é¢‘ç¼–è¾‘(Video Editing)\n\n\n\n<a name="3D-Generation"></a>\n\n# 3Dç”Ÿæˆ(3D Generation)\n\n\n**Generative Gaussian Splatting for Unbounded 3D City Generation**\n\n- Project: https://haozhexie.com/project/gaussian-city\n- Paper: https://arxiv.org/abs/2406.06526\n- Code: https://github.com/hzxie/GaussianCity\n\n**StdGEN: Semantic-Decomposed 3D Character Generation from Single Images**\n\n- Project: https://stdgen.github.io/\n- Paper: https://arxiv.org/abs/2411.05738\n- Code: https://github.com/hyz317/StdGEN\n\n\n<a name="3D-Reconstruction"></a>\n\n# 3Dé‡å»º(3D Reconstruction)\n\n**Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass**\n\n- Project: https://fast3r-3d.github.io/\n- Paper: https://arxiv.org/abs/2501.13928\n\n\n<a name="HMG"></a>\n\n# äººä½“è¿åŠ¨ç”Ÿæˆ(Human Motion Generation)\n\n**SemGeoMo: Dynamic Contextual Human Motion Generation with Semantic and Geometric Guidance**\n\n- Project: https://4dvlab.github.io/project_page/semgeomo/\n- Paper: https://arxiv.org/abs/2503.01291\n- https://github.com/4DVLab/SemGeoMo\n\n<a name="Video-Understanding"></a>\n\n# è§†é¢‘ç†è§£(Video Understanding)\n\n**Temporal Grounding Videos like Flipping Manga**\n\n- Paper: https://arxiv.org/abs/2411.10332\n- Code: https://github.com/yongliang-wu/NumPro\n\n<a name="Embodied"></a>\n\n# å…·èº«æ™ºèƒ½(Embodied AI)\n\n**Universal Actions for Enhanced Embodied Foundation Models**\n\n- Project: https://2toinf.github.io/UniAct/\n- Paper: https://arxiv.org/abs/2501.10105\n- Code: https://github.com/2toinf/UniAct\n\n**PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability**\n\n- Paper: https://arxiv.org/abs/2503.08481\n- Code: https://github.com/unira-zwj/PhysVLM\n\n\n<a name="KD"></a>\n\n# çŸ¥è¯†è’¸é¦(Knowledge Distillation)\n\n<a name="Depth-Estimation"></a>\n\n\n# æ·±åº¦ä¼°è®¡(Depth Estimation)\n\n**DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos**\n\n- Project: https://depthcrafter.github.io\n- Paper: https://arxiv.org/abs/2409.02095\n- Code: https://github.com/Tencent/DepthCrafter\n\n\n**MonSter: Marry Monodepth to Stereo Unleashes Power**\n\n- Paper: https://arxiv.org/abs/2501.08643\n- Code: https://github.com/Junda24/MonSter\n\n**DEFOM-Stereo: Depth Foundation Model Based Stereo Matching**\n\n- Project: https://insta360-research-team.github.io/DEFOM-Stereo/\n- Paper: https://arxiv.org/abs/2501.09466\n- Code: https://github.com/Insta360-Research-Team/DEFOM-Stereo\n\n\n<a name="Stereo-Matching"></a>\n\n# ç«‹ä½“åŒ¹é…(Stereo Matching)\n\n**MonSter: Marry Monodepth to Stereo Unleashes Power**\n\n- Paper: https://arxiv.org/abs/2501.08643\n- Code: https://github.com/Junda24/MonSter\n\n\n<a name="Low-light"></a>\n\n# æš—å…‰å›¾åƒå¢å¼º(Low-light Image Enhancement)\n\n\n**HVI: A New color space for Low-light Image Enhancement**\n\n- Paper: https://arxiv.org/abs/2502.20272\n- Code: https://github.com/Fediory/HVI-CIDNet\n- Demo: https://huggingface.co/spaces/Fediory/HVI-CIDNet_Low-light-Image-Enhancement_\n\n**ReDDiT: Efficient Diffusion as Low Light Enhancer**\n\n- Paper: https://arxiv.org/abs/2410.12346\n- Code: https://github.com/lgz-0713/ReDDiT\n\n\n\n<a name="IC"></a>\n\n# å›¾åƒå‹ç¼©(Image Compression)](#IC)\n\n**MambaIC: State Space Models for High-Performance Learned Image Compression**\n\n- Paper: https://arxiv.org/abs/2503.12461\n- Code: https://arxiv.org/abs/2503.12461\n\n\n<a name="SGG"></a>\n\n# åœºæ™¯å›¾ç”Ÿæˆ(Scene Graph Generation)\n\n\n\n<a name="ST"></a>\n\n# é£æ ¼è¿ç§»(Style Transfer)\n\n**StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements**\n\n- Project: https://stylestudio-official.github.io/\n- Paper: https://arxiv.org/abs/2412.08503\n- Code: https://github.com/Westlake-AGI-Lab/StyleStudio\n\n\n<a name="IQA"></a>\n\n# å›¾åƒè´¨é‡è¯„ä»·(Image Quality Assessment)\n\n**Auto Cherry-Picker: Learning from High-quality Generative Data Driven by Language**\n\n- Homepage: https://yichengchen24.github.io/projects/autocherrypicker\n- Paper: https://arxiv.org/pdf/2406.20085\n- Code: https://github.com/yichengchen24/ACP\n\n<a name="Video-Quality-Assessment"></a>\n\n# è§†é¢‘è´¨é‡è¯„ä»·(Video Quality Assessment)\n\n<a name="CS"></a>\n\n# å‹ç¼©æ„ŸçŸ¥(Compressive Sensing)\n\n**Using Powerful Prior Knowledge of Diffusion Model in Deep Unfolding Networks for Image Compressive Sensing**\n\n- Paper: https://arxiv.org/abs/2503.08429\n- Code: https://github.com/FengodChen/DMP-DUN-CVPR2025\n\n\n<a name="Datasets"></a>\n\n# æ•°æ®é›†(Datasets)\n\n\n**Objaverse++: Curated 3D Object Dataset with Quality Annotations**\n\n- Paper: https://arxiv.org/abs/2504.07334\n- Code: https://github.com/TCXX/ObjaversePlusPlus\n\n\n<a name="Others"></a>\n\n# å…¶ä»–(Others)\n\n\n**DTGBrepGen: A Novel B-rep Generative Model through Decoupling Topology and Geometry**\n\n- Paper: https://arxiv.org/abs/2503.13110\n- Code: https://github.com/jinli99/DTGBrepGen\n\n\n**Analyzing the Synthetic-to-Real Domain Gap in 3D Hand Pose Estimation**\n\n- Paper: https://arxiv.org/abs/2503.19307\n- Code: https://github.com/delaprada/HandSynthesis.git\n\n**EVOS: Efficient Implicit Neural Training via EVOlutionary Selector**\n\n- Homepage: https://weixiang-zhang.github.io/proj-evos/\n- Paper: https://arxiv.org/abs/2412.10153\n- Code: https://github.com/zwx-open/EVOS-INR\n  ', '{"language":null,"stars":21599,"forks":2770,"watchers":21599,"open_issues":22,"topics":["computer-vision","cvpr","cvpr2020","cvpr2021","cvpr2022","cvpr2023","cvpr2024","cvpr2025","deep-learning","image-processing","image-segmentation","machine-learning","object-detection","paper","python","semantic-segmentation","transformer","transformers","visual-tracking"],"default_branch":"main","size_kb":463,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:amusi:daily-paper-computer-vision","source_url":"https://github.com/amusi/daily-paper-computer-vision"},{"type":"has_code","target_id":"github:amusi:ICCV2025-Papers-with-Code","source_url":"https://github.com/amusi/ICCV2025-Papers-with-Code"},{"type":"has_code","target_id":"github:amusi:ECCV2024-Papers-with-Code","source_url":"https://github.com/amusi/ECCV2024-Papers-with-Code"},{"type":"has_code","target_id":"github:MzeroMiko:vHeat","source_url":"https://github.com/MzeroMiko/vHeat"},{"type":"has_code","target_id":"github:jameslahm:lsnet","source_url":"https://github.com/jameslahm/lsnet"},{"type":"has_code","target_id":"github:NVlabs:MambaVision","source_url":"https://github.com/NVlabs/MambaVision"},{"type":"has_code","target_id":"github:lewandofskee:MobileMamba","source_url":"https://github.com/lewandofskee/MobileMamba"},{"type":"has_code","target_id":"github:ai4ce:CityWalker","source_url":"https://github.com/ai4ce/CityWalker"},{"type":"has_code","target_id":"github:Visual-AI:Mr.DETR","source_url":"https://github.com/Visual-AI/Mr.DETR"},{"type":"has_code","target_id":"github:Hoyyyaard:LSceneLLM","source_url":"https://github.com/Hoyyyaard/LSceneLLM"},{"type":"has_code","target_id":"github:callsys:DynRefer","source_url":"https://github.com/callsys/DynRefer"},{"type":"has_code","target_id":"github:Hoar012:RAP-MLLM","source_url":"https://github.com/Hoar012/RAP-MLLM"},{"type":"has_code","target_id":"github:HealthX-Lab:BiomedCoOp","source_url":"https://github.com/HealthX-Lab/BiomedCoOp"},{"type":"has_code","target_id":"github:codefanw:FlashSloth","source_url":"https://github.com/codefanw/FlashSloth"},{"type":"has_code","target_id":"github:yunncheng:MMRL","source_url":"https://github.com/yunncheng/MMRL"},{"type":"has_code","target_id":"github:dragonlzm:PAVE","source_url":"https://github.com/dragonlzm/PAVE"},{"type":"has_code","target_id":"github:THUNLP-MT:AdaMMS","source_url":"https://github.com/THUNLP-MT/AdaMMS"},{"type":"has_code","target_id":"github:yuanc3:Pose2ID","source_url":"https://github.com/yuanc3/Pose2ID"},{"type":"has_code","target_id":"github:924973292:IDEA","source_url":"https://github.com/924973292/IDEA"},{"type":"has_code","target_id":"github:VainF:TinyFusion","source_url":"https://github.com/VainF/TinyFusion"},{"type":"has_code","target_id":"github:qianlong0502:DKDM","source_url":"https://github.com/qianlong0502/DKDM"},{"type":"has_code","target_id":"github:madaror:tiled-diffusion","source_url":"https://github.com/madaror/tiled-diffusion"},{"type":"has_code","target_id":"github:qunovo:NLPrompt","source_url":"https://github.com/qunovo/NLPrompt"},{"type":"has_code","target_id":"github:unira-zwj:PhysVLM","source_url":"https://github.com/unira-zwj/PhysVLM"},{"type":"has_code","target_id":"github:yunncheng:MMRL","source_url":"https://github.com/yunncheng/MMRL"},{"type":"has_code","target_id":"github:iSEE-Laboratory:LLMDet","source_url":"https://github.com/iSEE-Laboratory/LLMDet"},{"type":"has_code","target_id":"github:Visual-AI:Mr.DETR","source_url":"https://github.com/Visual-AI/Mr.DETR"},{"type":"has_code","target_id":"github:MCG-NJU:MOTIP","source_url":"https://github.com/MCG-NJU/MOTIP"},{"type":"has_code","target_id":"github:xifen523:OmniTrack","source_url":"https://github.com/xifen523/OmniTrack"},{"type":"has_code","target_id":"github:shaohao011:BrainMVP","source_url":"https://github.com/shaohao011/BrainMVP"},{"type":"has_code","target_id":"github:Yore0:TTDG-MGM","source_url":"https://github.com/Yore0/TTDG-MGM"},{"type":"has_code","target_id":"github:Xiangxu-0103:LiMoE","source_url":"https://github.com/Xiangxu-0103/LiMoE"},{"type":"has_code","target_id":"github:peakpang:UGP","source_url":"https://github.com/peakpang/UGP"},{"type":"has_code","target_id":"github:2minkyulee:AESOP-Auto-Encoded-Supervision-for-Perceptual-Image-Super-Resolution","source_url":"https://github.com/2minkyulee/AESOP-Auto-Encoded-Supervision-for-Perceptual-Image-Super-Resolution"},{"type":"has_code","target_id":"github:IsshikiHugh:HSMR","source_url":"https://github.com/IsshikiHugh/HSMR"},{"type":"has_code","target_id":"github:pqh22:ProxyTransformation","source_url":"https://github.com/pqh22/ProxyTransformation"},{"type":"has_code","target_id":"github:hustvl:LightningDiT","source_url":"https://github.com/hustvl/LightningDiT"},{"type":"has_code","target_id":"github:taco-group:SleeperMark","source_url":"https://github.com/taco-group/SleeperMark"},{"type":"has_code","target_id":"github:ByteFlow-AI:TokenFlow","source_url":"https://github.com/ByteFlow-AI/TokenFlow"},{"type":"has_code","target_id":"github:Epiphqny:PAR","source_url":"https://github.com/Epiphqny/PAR"},{"type":"has_code","target_id":"github:pandayuanyu:generative-photography","source_url":"https://github.com/pandayuanyu/generative-photography"},{"type":"has_code","target_id":"github:LanceZPF:OpenING","source_url":"https://github.com/LanceZPF/OpenING"},{"type":"has_code","target_id":"github:PKU-YuanGroup:ConsisID","source_url":"https://github.com/PKU-YuanGroup/ConsisID"},{"type":"has_code","target_id":"github:maxin-cn:Cinemo","source_url":"https://github.com/maxin-cn/Cinemo"},{"type":"has_code","target_id":"github:bytedance:X-Dyna","source_url":"https://github.com/bytedance/X-Dyna"},{"type":"has_code","target_id":"github:pittisl:PhyT2V","source_url":"https://github.com/pittisl/PhyT2V"},{"type":"has_code","target_id":"github:ali-vilab:TeaCache","source_url":"https://github.com/ali-vilab/TeaCache"},{"type":"has_code","target_id":"github:iva-mzsun:AR-Diffusion","source_url":"https://github.com/iva-mzsun/AR-Diffusion"},{"type":"has_code","target_id":"github:taco-group:FaceLock","source_url":"https://github.com/taco-group/FaceLock"},{"type":"has_code","target_id":"github:nktoan:h-edit","source_url":"https://github.com/nktoan/h-edit"},{"type":"has_code","target_id":"github:hzxie:GaussianCity","source_url":"https://github.com/hzxie/GaussianCity"},{"type":"has_code","target_id":"github:hyz317:StdGEN","source_url":"https://github.com/hyz317/StdGEN"},{"type":"has_code","target_id":"github:4DVLab:SemGeoMo","source_url":"https://github.com/4DVLab/SemGeoMo"},{"type":"has_code","target_id":"github:yongliang-wu:NumPro","source_url":"https://github.com/yongliang-wu/NumPro"},{"type":"has_code","target_id":"github:2toinf:UniAct","source_url":"https://github.com/2toinf/UniAct"},{"type":"has_code","target_id":"github:unira-zwj:PhysVLM","source_url":"https://github.com/unira-zwj/PhysVLM"},{"type":"has_code","target_id":"github:Tencent:DepthCrafter","source_url":"https://github.com/Tencent/DepthCrafter"},{"type":"has_code","target_id":"github:Junda24:MonSter","source_url":"https://github.com/Junda24/MonSter"},{"type":"has_code","target_id":"github:Insta360-Research-Team:DEFOM-Stereo","source_url":"https://github.com/Insta360-Research-Team/DEFOM-Stereo"},{"type":"has_code","target_id":"github:Junda24:MonSter","source_url":"https://github.com/Junda24/MonSter"},{"type":"has_code","target_id":"github:Fediory:HVI-CIDNet","source_url":"https://github.com/Fediory/HVI-CIDNet"},{"type":"has_code","target_id":"github:lgz-0713:ReDDiT","source_url":"https://github.com/lgz-0713/ReDDiT"},{"type":"has_code","target_id":"github:Westlake-AGI-Lab:StyleStudio","source_url":"https://github.com/Westlake-AGI-Lab/StyleStudio"},{"type":"has_code","target_id":"github:yichengchen24:ACP","source_url":"https://github.com/yichengchen24/ACP"},{"type":"has_code","target_id":"github:FengodChen:DMP-DUN-CVPR2025","source_url":"https://github.com/FengodChen/DMP-DUN-CVPR2025"},{"type":"has_code","target_id":"github:TCXX:ObjaversePlusPlus","source_url":"https://github.com/TCXX/ObjaversePlusPlus"},{"type":"has_code","target_id":"github:jinli99:DTGBrepGen","source_url":"https://github.com/jinli99/DTGBrepGen"},{"type":"has_code","target_id":"github:delaprada:HandSynthesis.git","source_url":"https://github.com/delaprada/HandSynthesis.git"},{"type":"has_code","target_id":"github:zwx-open:EVOS-INR","source_url":"https://github.com/zwx-open/EVOS-INR"}]', NULL, NULL, 'pending', 70, '63b929d42943ef5a29ef590da4e3a1c8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-amusi-CVPR2025-Papers-with-Code from https://github.com/amusi.png
Image converted to WebP: data/images/github-amusi-CVPR2025-Papers-with-Code.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-zergtant-pytorch-handbook', 'github--zergtant--pytorch-handbook', 'pytorch-handbook', 'zergtant', '!pytorch è¿™æ˜¯ä¸€æœ¬å¼€æºçš„ä¹¦ç±ï¼Œç›®æ ‡æ˜¯å¸®åŠ©é‚£äº›å¸Œæœ›å’Œä½¿ç”¨PyTorchè¿›è¡Œæ·±åº¦å­¦ä¹ å¼€å‘å’Œç ”ç©¶çš„æœ‹å‹å¿«é€Ÿå…¥é—¨ã€‚ ç”±äºæœ¬äººæ°´å¹³æœ‰é™ï¼Œåœ¨å†™æ­¤æ•™ç¨‹çš„æ—¶å€™å‚è€ƒäº†ä¸€äº›ç½‘ä¸Šçš„èµ„æ–™ï¼Œåœ¨è¿™é‡Œå¯¹ä»–ä»¬è¡¨ç¤ºæ•¬æ„ï¼Œæˆ‘ä¼šåœ¨æ¯ä¸ªå¼•ç”¨ä¸­é™„ä¸ŠåŸæ–‡åœ°å€ï¼Œæ–¹ä¾¿å¤§å®¶å‚è€ƒã€‚ æ·±åº¦å­¦ä¹ çš„æŠ€æœ¯åœ¨é£é€Ÿçš„å‘å±•ï¼ŒåŒæ—¶PyTorchä¹Ÿåœ¨ä¸æ–­æ›´æ–°ï¼Œä¸”æœ¬äººä¼šé€æ­¥å®Œå–„ç›¸å…³å†…å®¹ã€‚ ç”±äºPyTorchç‰ˆæœ¬æ›´è¿­ï¼Œæ•™ç¨‹çš„ç‰ˆæœ¬ä¼šä¸PyTorchç‰ˆæœ¬ï¼Œä¿æŒä¸€è‡´ã€‚ pytorchå¤§ç‰ˆæœ¬æ›´æ–°çš„ä¸»è¦å˜åŠ¨æ€»ç»“ å½“å‰ç‰ˆæœ¬ 1.11 å›½å†…çš„é•œåƒï¼Œé€Ÿåº¦å¾ˆå¿«ï¼Œä¸ä¼šè¢«å¢™ï¼šhttps://www.pytorch.wiki/ PDFæ–‡ä»¶ç›®å‰è¿˜æ²¡æœ‰æ‰¾åˆ°å¥½çš„ç”Ÿæˆæ–¹æ³•ï¼Œæœ‰ç†Ÿæ‚‰è¿™æ–¹é¢çš„æœ‹å‹å¯ä»¥è”ç³»æˆ‘ï¼Œæ„Ÿæ¿€ä¸å°½ ç¾¤å·ï¼š760443051 !QR ç‚¹å‡»é“¾æ¥åŠ å…¥ç¾¤èŠã€PyTorch Handbook äº¤æµ6ç¾¤ã€‘ï¼šhttps://jq.qq.com/?_wv=1027&k=X4Ro6uWv 1ç¾¤(985896536)å·²æ»¡ï¼Œ2ç¾¤(681980831) 3ç¾¤(773681699)å·²æ»¡ 4ç¾¤(884017356)å·²æ»¡ 5ç¾¤(894059877)å·²æ»¡ ä¸è¦å†åŠ äº† å…¬ä¼—è´¦å·æ¯æ—¥åˆ†äº«å¹²è´§æ–‡ç«  ...', '["deep-learning","machine-learning","neural-network","pytorch","pytorch-handbook","pytorch-tutorials","jupyter notebook"]', 'other', 21442, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/zergtant/pytorch-handbook","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# PyTorch ä¸­æ–‡æ‰‹å†Œï¼ˆpytorch handbookï¼‰\n![pytorch](pytorch-logo-dark.png)\n\n## ä¹¦ç±ä»‹ç»\nè¿™æ˜¯ä¸€æœ¬å¼€æºçš„ä¹¦ç±ï¼Œç›®æ ‡æ˜¯å¸®åŠ©é‚£äº›å¸Œæœ›å’Œä½¿ç”¨PyTorchè¿›è¡Œæ·±åº¦å­¦ä¹ å¼€å‘å’Œç ”ç©¶çš„æœ‹å‹å¿«é€Ÿå…¥é—¨ã€‚\n\nç”±äºæœ¬äººæ°´å¹³æœ‰é™ï¼Œåœ¨å†™æ­¤æ•™ç¨‹çš„æ—¶å€™å‚è€ƒäº†ä¸€äº›ç½‘ä¸Šçš„èµ„æ–™ï¼Œåœ¨è¿™é‡Œå¯¹ä»–ä»¬è¡¨ç¤ºæ•¬æ„ï¼Œæˆ‘ä¼šåœ¨æ¯ä¸ªå¼•ç”¨ä¸­é™„ä¸ŠåŸæ–‡åœ°å€ï¼Œæ–¹ä¾¿å¤§å®¶å‚è€ƒã€‚\n\næ·±åº¦å­¦ä¹ çš„æŠ€æœ¯åœ¨é£é€Ÿçš„å‘å±•ï¼ŒåŒæ—¶PyTorchä¹Ÿåœ¨ä¸æ–­æ›´æ–°ï¼Œä¸”æœ¬äººä¼šé€æ­¥å®Œå–„ç›¸å…³å†…å®¹ã€‚\n\n## ç‰ˆæœ¬è¯´æ˜\nç”±äºPyTorchç‰ˆæœ¬æ›´è¿­ï¼Œæ•™ç¨‹çš„ç‰ˆæœ¬ä¼šä¸PyTorchç‰ˆæœ¬ï¼Œä¿æŒä¸€è‡´ã€‚\n\n[pytorchå¤§ç‰ˆæœ¬æ›´æ–°çš„ä¸»è¦å˜åŠ¨æ€»ç»“](pytorch-changelog.md)  å½“å‰ç‰ˆæœ¬ 1.11\n\n## åœ¨çº¿ç‰ˆæœ¬å’ŒPDF\n\nå›½å†…çš„é•œåƒï¼Œé€Ÿåº¦å¾ˆå¿«ï¼Œä¸ä¼šè¢«å¢™ï¼šhttps://www.pytorch.wiki/\n\nPDFæ–‡ä»¶ç›®å‰è¿˜æ²¡æœ‰æ‰¾åˆ°å¥½çš„ç”Ÿæˆæ–¹æ³•ï¼Œæœ‰ç†Ÿæ‚‰è¿™æ–¹é¢çš„æœ‹å‹å¯ä»¥è”ç³»æˆ‘ï¼Œæ„Ÿæ¿€ä¸å°½\n\n## QQ 6ç¾¤ \n\nç¾¤å·ï¼š760443051\n\n\n![QR](PyTorch-Handbook-6.png) \n\nç‚¹å‡»é“¾æ¥åŠ å…¥ç¾¤èŠã€PyTorchÂ HandbookÂ äº¤æµ6ç¾¤ã€‘ï¼šhttps://jq.qq.com/?_wv=1027&k=X4Ro6uWv\n\n\n1ç¾¤(985896536)å·²æ»¡ï¼Œ2ç¾¤(681980831) 3ç¾¤(773681699)å·²æ»¡  4ç¾¤(884017356)å·²æ»¡  5ç¾¤(894059877)å·²æ»¡\n\nä¸è¦å†åŠ äº†\n\n## æ–°ç¦åˆ©\n\nå…¬ä¼—è´¦å·æ¯æ—¥åˆ†äº«å¹²è´§æ–‡ç« \n![weixin QR](deephub.jpg) \n\n\n\n## è¯´æ˜\n\n- ä¿®æ”¹é”™åˆ«å­—è¯·ç›´æ¥æissueæˆ–PR\n\n- PRæ—¶è¯·æ³¨æ„ç‰ˆæœ¬\n\n- æœ‰é—®é¢˜ä¹Ÿè¯·ç›´æ¥æissue\n\næ„Ÿè°¢\n\n## ç›®å½•\n\n### ç¬¬ä¸€ç« ï¼šPyTorch å…¥é—¨\n\n1. [PyTorch ç®€ä»‹](chapter1/1.1-pytorch-introduction.md)\n2. [PyTorch ç¯å¢ƒæ­å»º](chapter1/1.2-pytorch-installation.md)\n3. [PyTorch æ·±åº¦å­¦ä¹ ï¼š60åˆ†é’Ÿå¿«é€Ÿå…¥é—¨ï¼ˆå®˜æ–¹ï¼‰](chapter1/1.3-deep-learning-with-pytorch-60-minute-blitz.md)\n    - [å¼ é‡](chapter1/1_tensor_tutorial.ipynb)\n    - [Autogradï¼šè‡ªåŠ¨æ±‚å¯¼](chapter1/2_autograd_tutorial.ipynb) \n    - [ç¥ç»ç½‘ç»œ](chapter1/3_neural_networks_tutorial.ipynb)\n    - [è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨](chapter1/4_cifar10_tutorial.ipynb)\n    - [é€‰è¯»ï¼šæ•°æ®å¹¶è¡Œå¤„ç†ï¼ˆå¤šGPUï¼‰](chapter1/5_data_parallel_tutorial.ipynb)\n4. [ç›¸å…³èµ„æºä»‹ç»](chapter1/1.4-pytorch-resource.md)\n\n### ç¬¬äºŒç«  åŸºç¡€\n#### ç¬¬ä¸€èŠ‚ PyTorch åŸºç¡€\n1. [å¼ é‡](chapter2/2.1.1.pytorch-basics-tensor.ipynb)\n2. [è‡ªåŠ¨æ±‚å¯¼](chapter2/2.1.2-pytorch-basics-autograd.ipynb)\n3. [ç¥ç»ç½‘ç»œåŒ…nnå’Œä¼˜åŒ–å™¨optm](chapter2/2.1.3-pytorch-basics-nerual-network.ipynb)\n4. [æ•°æ®çš„åŠ è½½å’Œé¢„å¤„ç†](chapter2/2.1.4-pytorch-basics-data-loader.ipynb)\n#### ç¬¬äºŒèŠ‚ æ·±åº¦å­¦ä¹ åŸºç¡€åŠæ•°å­¦åŸç†\n\n[æ·±åº¦å­¦ä¹ åŸºç¡€åŠæ•°å­¦åŸç†](chapter2/2.2-deep-learning-basic-mathematics.ipynb)\n\n#### ç¬¬ä¸‰èŠ‚ ç¥ç»ç½‘ç»œç®€ä»‹\n\n[ç¥ç»ç½‘ç»œç®€ä»‹](chapter2/2.3-deep-learning-neural-network-introduction.ipynb)  æ³¨ï¼šæœ¬ç« åœ¨æœ¬åœ°ä½¿ç”¨å¾®è½¯çš„Edgeæ‰“å¼€ä¼šå´©æºƒï¼Œè¯·ä½¿Chrome Firefoxæ‰“å¼€æŸ¥çœ‹\n\n#### ç¬¬å››èŠ‚ å·ç§¯ç¥ç»ç½‘ç»œ\n\n[å·ç§¯ç¥ç»ç½‘ç»œ](chapter2/2.4-cnn.ipynb)\n\n#### ç¬¬äº”èŠ‚ å¾ªç¯ç¥ç»ç½‘ç»œ\n\n[å¾ªç¯ç¥ç»ç½‘ç»œ](chapter2/2.5-rnn.ipynb)\n\n### ç¬¬ä¸‰ç«  å®è·µ\n#### ç¬¬ä¸€èŠ‚ logisticå›å½’äºŒå…ƒåˆ†ç±»\n\n[logisticå›å½’äºŒå…ƒåˆ†ç±»](chapter3/3.1-logistic-regression.ipynb)\n\n\n#### ç¬¬äºŒèŠ‚ CNN:MNISTæ•°æ®é›†æ‰‹å†™æ•°å­—è¯†åˆ«\n\n[CNN:MNISTæ•°æ®é›†æ‰‹å†™æ•°å­—è¯†åˆ«](chapter3/3.2-mnist.ipynb)\n\n#### ç¬¬ä¸‰èŠ‚ RNNå®ä¾‹ï¼šé€šè¿‡Siné¢„æµ‹Cos\n\n[RNNå®ä¾‹ï¼šé€šè¿‡Siné¢„æµ‹Cos](chapter3/3.3-rnn.ipynb)\n\n### ç¬¬å››ç«  æé«˜\n#### ç¬¬ä¸€èŠ‚ Fine-tuning\n\n[Fine-tuning](chapter4/4.1-fine-tuning.ipynb)\n\n#### ç¬¬äºŒèŠ‚ å¯è§†åŒ–\n\n[visdom](chapter4/4.2.1-visdom.ipynb)\n\n[tensorboardx](chapter4/4.2.2-tensorboardx.ipynb) \n\n[å¯è§†åŒ–ç†è§£å·ç§¯ç¥ç»ç½‘ç»œ](chapter4/4.2.3-cnn-visualizing.ipynb)\n\n#### ç¬¬ä¸‰èŠ‚ Fast.ai\n[Fast.ai](chapter4/4.3-fastai.ipynb)\n#### ç¬¬å››èŠ‚ è®­ç»ƒçš„ä¸€äº›æŠ€å·§\n\n#### ç¬¬äº”èŠ‚ å¤šGPUå¹¶è¡Œè®­ç»ƒ\n[å¤šGPUå¹¶è¡Œè®¡ç®—](chapter4/4.5-multiply-gpu-parallel-training.ipynb)\n\n#### è¡¥å……ç¿»è¯‘æ–‡ç« ï¼šåœ¨PyTorchä¸­ä½¿ç”¨DistributedDataParallelè¿›è¡Œå¤šGPUåˆ†å¸ƒå¼æ¨¡å‹è®­ç»ƒ\n[åœ¨PyTorchä¸­ä½¿ç”¨DistributedDataParallelè¿›è¡Œå¤šGPUåˆ†å¸ƒå¼æ¨¡å‹è®­ç»ƒ](chapter4/distributeddataparallel)\n\n\n### ç¬¬äº”ç«  åº”ç”¨\n#### ç¬¬ä¸€èŠ‚ Kaggleä»‹ç»\n[Kaggleä»‹ç»](chapter5/5.1-kaggle.md)\n#### ç¬¬äºŒèŠ‚ ç»“æ„åŒ–æ•°æ®\n[Pytorchå¤„ç†ç»“æ„åŒ–æ•°æ®](chapter5/5.2-Structured-Data.ipynb)\n#### ç¬¬ä¸‰èŠ‚ è®¡ç®—æœºè§†è§‰\n[Fashion MNIST å›¾åƒåˆ†ç±»](chapter5/5.3-Fashion-MNIST.ipynb)\n#### ç¬¬å››èŠ‚ è‡ªç„¶è¯­è¨€å¤„ç†\n#### ç¬¬äº”èŠ‚ ååŒè¿‡æ»¤\n\n### ç¬¬å…­ç«  èµ„æº\n\n[torchaudio](torchaudio/intro.ipynb)\n\n\n### ç¬¬ä¸ƒç«  é™„å½•\n\n[æ ‘è“æ´¾ç¼–è¯‘å®‰è£… pytorch 1.4](pi/)\n\ntransformsçš„å¸¸ç”¨æ“ä½œæ€»ç»“\n\npytorchçš„æŸå¤±å‡½æ•°æ€»ç»“\n\npytorchçš„ä¼˜åŒ–å™¨æ€»ç»“\n\n\n## Script\nscriptç›®å½•æ˜¯æˆ‘å†™çš„å°†ipynbè½¬æ¢æˆåœ¨çº¿çš„ç‰ˆæœ¬å’Œpdfæ–‡ä»¶çš„è„šæœ¬ï¼Œå› ä¸ºè¿˜åœ¨æµ‹è¯•é˜¶æ®µï¼Œæ‰€ä»¥æœ‰ä»€ä¹ˆé—®é¢˜è¯·å¤§å®¶æå‡º\n\n\n## License\n\n![](https://i.creativecommons.org/l/by-nc-sa/3.0/88x31.png)\n\n[æœ¬ä½œå“é‡‡ç”¨çŸ¥è¯†å…±äº«ç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 3.0  ä¸­å›½å¤§é™†è®¸å¯åè®®è¿›è¡Œè®¸å¯](http://creativecommons.org/licenses/by-nc-sa/3.0/cn)\n', '{"language":"Jupyter Notebook","stars":21442,"forks":5439,"watchers":21442,"open_issues":57,"topics":["deep-learning","machine-learning","neural-network","pytorch","pytorch-handbook","pytorch-tutorials"],"default_branch":"master","size_kb":149507,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[]', NULL, NULL, 'pending', 55, '71528c1aef572d9eca061933af1b46b8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-zergtant-pytorch-handbook from https://github.com/zergtant.png
Image converted to WebP: data/images/github-zergtant-pytorch-handbook.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-TheAlgorithms-C', 'github--thealgorithms--c', 'C', 'TheAlgorithms', '<!-- the suffix in the above line is required for doxygen to consider this as the index page of the generated documentation site --> !GitHub repo size The repository is a collection of open-source implementations of a variety of algorithms implemented in C and licensed under GPLv3 License. The algorithms span a variety of topics from computer science, mathematics and statistics, data science, machine learning, engineering, etc.. The implementations and their associated documentations are mean...', '["algorithm-challenges","algorithm-competitions","algorithms","c","community-driven","computer-science","data-structures","datastructures","education","educational","hacktoberfest","interview","interview-questions","learn-to-code","machine-learning","machine-learning-algorithms","mathematics","search","sort","c"]', 'other', 21427, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/TheAlgorithms/C","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# The Algorithms - C # {#mainpage}\n<!-- the suffix in the above line is required for doxygen to consider this as the index page of the generated documentation site -->\n\n[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/TheAlgorithms/C)\n[![CodeQL CI](https://github.com/TheAlgorithms/C/actions/workflows/codeql.yml/badge.svg)](https://github.com/TheAlgorithms/C/actions/workflows/codeql_analysis.yml)\n[![Gitter chat](https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&logo=gitter&style=flat-square)](https://gitter.im/TheAlgorithms)\n[![contributions welcome](https://img.shields.io/static/v1.svg?label=Contributions&message=Welcome&color=0059b3&style=flat-square)](https://github.com/TheAlgorithms/C/blob/master/CONTRIBUTING.md)\n![GitHub repo size](https://img.shields.io/github/repo-size/TheAlgorithms/C?color=red&style=flat-square)\n[![Doxygen CI](https://github.com/TheAlgorithms/C/workflows/Doxygen%20CI/badge.svg)](https://TheAlgorithms.github.io/C)\n[![Awesome CI](https://github.com/TheAlgorithms/C/workflows/Awesome%20CI%20Workflow/badge.svg)](https://github.com/TheAlgorithms/C/actions?query=workflow%3A%22Awesome+CI+Workflow%22)\n[![Income](https://img.shields.io/liberapay/receives/TheAlgorithms.svg?logo=liberapay)](https://liberapay.com/TheAlgorithms)\n[![Discord chat](https://img.shields.io/discord/808045925556682782.svg?logo=discord&colorB=5865F2)](https://the-algorithms.com/discord/)\n[![Donate](https://liberapay.com/assets/widgets/donate.svg)](https://liberapay.com/TheAlgorithms/donate)\n\n## Overview\n\nThe repository is a collection of open-source implementations of a variety of algorithms implemented in C and licensed under [GPLv3 License](https://github.com/TheAlgorithms/C/blob/master/LICENSE). The algorithms span a variety of topics from computer science, mathematics and statistics, data science, machine learning, engineering, etc.. The implementations and their associated documentations are meant to provide a learning resource for educators and students. Hence, one may find more than one implementation for the same objective but using different algorithm strategies and optimizations.\n\n## Features\n\n* The repository provides implementations of various algorithms in one of the most fundamental general purpose languages - [C](https://en.wikipedia.org/wiki/C_(programming_language)).\n* Well documented source code with detailed explanations provide a valuable resource for educators and students alike.\n* Each source code is atomic using standard C library [`libc`](https://en.wikipedia.org/wiki/C_standard_library) and _no external libraries_ are required for their compilation and execution. Thus the fundamentals of the algorithms can be studied in much depth.\n* Source codes are [compiled and tested](https://github.com/TheAlgorithms/C/actions?query=workflow%3A%22Awesome+CI+Workflow%22) for every commit on the latest versions of two major operating systems viz., MacOS and Ubuntu (Linux) using AppleClang 14.0.0 and GNU 11.3.0 respectively.\n* Strict adherence to [C11](https://en.wikipedia.org/wiki/C11_(C_standard_revision)) standard ensures portability of code to embedded systems as well like ESP32, ARM Cortex, etc. with little to no changes.\n* Self-checks within programs ensure correct implementations with confidence.\n* Modular implementations and OpenSource licensing enable the functions to be utilized conveniently in other applications.\n\n## Documentation\n\n[Online Documentation](https://TheAlgorithms.github.io/C) is generated from the repository source codes directly. The documentation contains all resources including source code snippets, details on execution of the programs, diagrammatic representation of program flow, and links to external resources where necessary.\nClick on [Files menu](https://TheAlgorithms.github.io/C/files.html) to see the list of all the files documented with the code.\n\n[Documentation of Algorithms in C](https://thealgorithms.github.io/C) by [The Algorithms Contributors](https://github.com/TheAlgorithms/C/graphs/contributors) is licensed under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/?ref=chooser-v1)<br/>\n<a href="https://creativecommons.org/licenses/by-sa/4.0"><img alt="Creative Commons License" style="height:22px!important;margin-left: 3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg" /><img  alt="Credit must be given to the creator" style="height:22px!important;margin-left: 3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg" /><img alt="Adaptations must be shared under the same terms" style="height:22px!important;margin-left: 3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg" /></a>\n\n## Contributions\n\nAs a community developed and maintained repository, we welcome new un-plagiarized quality contributions. Please read our [Contribution Guidelines](https://github.com/TheAlgorithms/C/blob/master/CONTRIBUTING.md).\n', '{"language":"C","stars":21427,"forks":4696,"watchers":21427,"open_issues":36,"topics":["algorithm-challenges","algorithm-competitions","algorithms","c","community-driven","computer-science","data-structures","datastructures","education","educational","hacktoberfest","interview","interview-questions","learn-to-code","machine-learning","machine-learning-algorithms","mathematics","search","sort"],"default_branch":"master","size_kb":20365,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"},{"type":"has_code","target_id":"github:TheAlgorithms:C","source_url":"https://github.com/TheAlgorithms/C"}]', NULL, 'GPL-3.0', 'approved', 65, '6f2f12db828b51dbd017b318d69de659', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-TheAlgorithms-C from https://github.com/TheAlgorithms.png
Image converted to WebP: data/images/github-TheAlgorithms-C.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-serengil-deepface', 'github--serengil--deepface', 'deepface', 'serengil', '<div align="center"> <div align="center"> <a href="https://trendshift.io/repositories/4227" target="_blank"><img src="https://trendshift.io/api/badge/repositories/4227" alt="serengil%2Fdeepface | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a> <!-- <a href="https://www.producthunt.com/posts/deepface?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-deepface" target="_blank"> <img src="https://api.producthunt.com/widgets/embed-image/v1/featured...', '["age-prediction","arcface","deep-learning","deepface","deepid","emotion-recognition","face-analysis","face-recognition","facenet","facial-expression-recognition","facial-recognition","gender-prediction","machine-learning","openface","python","race-classification","vgg-face","python"]', 'other', 21235, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/serengil/deepface","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# deepface\n\n<div align="center">\n\n[![Downloads](https://static.pepy.tech/personalized-badge/deepface?period=total&units=international_system&left_color=grey&right_color=blue&left_text=downloads)](https://pepy.tech/project/deepface)\n[![Stars](https://img.shields.io/github/stars/serengil/deepface?color=yellow&style=flat&label=%E2%AD%90%20stars)](https://github.com/serengil/deepface/stargazers)\n[![Pulls](https://img.shields.io/docker/pulls/serengil/deepface?logo=docker)](https://hub.docker.com/r/serengil/deepface)\n[![License](http://img.shields.io/:license-MIT-green.svg?style=flat)](https://github.com/serengil/deepface/blob/master/LICENSE)\n[![Tests](https://github.com/serengil/deepface/actions/workflows/tests.yml/badge.svg)](https://github.com/serengil/deepface/actions/workflows/tests.yml)\n[![DOI](http://img.shields.io/:DOI-10.17671/gazibtd.1399077-blue.svg?style=flat)](https://doi.org/10.17671/gazibtd.1399077)\n\n[![Blog](https://img.shields.io/:blog-sefiks.com-blue.svg?style=flat&logo=wordpress)](https://sefiks.com)\n[![YouTube](https://img.shields.io/:youtube-@sefiks-red.svg?style=flat&logo=youtube)](https://www.youtube.com/@sefiks?sub_confirmation=1)\n[![Twitter](https://img.shields.io/:follow-@serengil-blue.svg?style=flat&logo=x)](https://twitter.com/intent/user?screen_name=serengil)\n\n[![Patreon](https://img.shields.io/:become-patron-f96854.svg?style=flat&logo=patreon)](https://www.patreon.com/serengil?repo=deepface)\n[![GitHub Sponsors](https://img.shields.io/github/sponsors/serengil?logo=GitHub&color=lightgray)](https://github.com/sponsors/serengil)\n[![Buy Me a Coffee](https://img.shields.io/badge/-buy_me_a%C2%A0coffee-gray?logo=buy-me-a-coffee)](https://buymeacoffee.com/serengil)\n\n<div align="center">\n  <a href="https://trendshift.io/repositories/4227" target="_blank"><img src="https://trendshift.io/api/badge/repositories/4227" alt="serengil%2Fdeepface | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n  <!--\n  <a href="https://www.producthunt.com/posts/deepface?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-deepface" target="_blank">\n      <img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=753599&theme=light" alt="DeepFace - A Lightweight Deep Face Recognition Library for Python | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" />\n  </a>\n  -->\n</div>\n\n<!--\n[![Hacker News](https://img.shields.io/badge/dynamic/json?color=orange&label=Hacker%20News&query=score&url=https%3A%2F%2Fhacker-news.firebaseio.com%2Fv0%2Fitem%2F42584896.json&logo=y-combinator)](https://news.ycombinator.com/item?id=42584896)\n[![Product Hunt](https://img.shields.io/badge/Product%20Hunt-%E2%96%B2-orange?logo=producthunt)](https://www.producthunt.com/posts/deepface?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-deepface)\n-->\n\n<!-- [![DOI](http://img.shields.io/:DOI-10.1109/ICEET53442.2021.9659697-blue.svg?style=flat)](https://doi.org/10.1109/ICEET53442.2021.9659697) -->\n<!-- [![DOI](http://img.shields.io/:DOI-10.1109/ASYU50717.2020.9259802-blue.svg?style=flat)](https://doi.org/10.1109/ASYU50717.2020.9259802) -->\n\n</div>\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-icon-labeled.png" width="200" height="240"></p>\n\nDeepFace is a lightweight [face recognition](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) and facial attribute analysis ([age](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/), [gender](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/), [emotion](https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/) and [race](https://sefiks.com/2019/11/11/race-and-ethnicity-prediction-in-keras/)) framework for python. It is a hybrid face recognition framework wrapping **state-of-the-art** models: [`VGG-Face`](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/), [`FaceNet`](https://sefiks.com/2018/09/03/face-recognition-with-facenet-in-keras/), [`OpenFace`](https://sefiks.com/2019/07/21/face-recognition-with-openface-in-keras/), [`DeepFace`](https://sefiks.com/2020/02/17/face-recognition-with-facebook-deepface-in-keras/), [`DeepID`](https://sefiks.com/2020/06/16/face-recognition-with-deepid-in-keras/), [`ArcFace`](https://sefiks.com/2020/12/14/deep-face-recognition-with-arcface-in-keras-and-python/), [`Dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/), `SFace`, `GhostFaceNet`, `Buffalo_L`.\n\n[A modern face recognition pipeline](https://sefiks.com/2020/05/01/a-gentle-introduction-to-face-recognition-in-deep-learning/) consists of 5 common stages: [detect](https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/), [align](https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/), [normalize](https://sefiks.com/2020/11/20/facial-landmarks-for-face-recognition-with-dlib/), [represent](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) and [verify](https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/). While DeepFace handles all these common stages in the background, you donâ€™t need to acquire in-depth knowledge about all the processes behind it. You can just call its verification, find or analysis function with a single line of code.\n\n[`Experiments`](https://github.com/serengil/deepface/tree/master/benchmarks) show that **human beings have 97.53% accuracy** on facial recognition tasks whereas those models already reached and passed that accuracy level.\n\n## Installation [![PyPI](https://img.shields.io/pypi/v/deepface.svg)](https://pypi.org/project/deepface/)\n\nThe easiest way to install deepface is to download it from [`PyPI`](https://pypi.org/project/deepface/). It''s going to install the library itself and its prerequisites as well.\n\n```shell\n$ pip install deepface\n```\n\nAlternatively, you can also install deepface from its source code. Source code may have new features not published in pip release yet.\n\n```shell\n$ git clone https://github.com/serengil/deepface.git\n$ cd deepface\n$ pip install -e .\n```\n\nOnce you installed the library, then you will be able to import it and use its functionalities.\n\n```python\nfrom deepface import DeepFace\n```\n\n**Face Verification** - [`Demo`](https://youtu.be/KRCvkNCOphE)\n\nThis function determines whether two facial images belong to the same person or to different individuals. It accepts exact image file paths as input, but also supports NumPy arrays, base64-encoded images, and URLs. The function returns a dictionary, where the key of interest is `verified`: True indicates the images are of the same person, while False means they are of different people. In addition to this binary classification, the function also provides a [`confidence`](https://youtu.be/QQ4vO6UOsFo) score that reflects the likelihood that the two images represent the same person.\n\n```python\nresult = DeepFace.verify(img1_path = "img1.jpg", img2_path = "img2.jpg")\n```\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/verify-credit.jpg" width="99%"></p>\n\n**Face recognition** - [`Demo`](https://youtu.be/Hrjp-EStM_s)\n\n[Face recognition](https://sefiks.com/2020/05/25/large-scale-face-recognition-for-deep-learning/) requires applying face verification many times. Herein, deepface has an out-of-the-box find function to handle this action. It''s going to look for the identity of input image in the database path and it will return list of pandas data frame as output. Meanwhile, facial embeddings of the facial database are stored in a pickle file to be searched faster in next time. Result is going to be the size of faces appearing in the source image. Besides, target images in the database can have many faces as well.\n\n```python\ndfs = DeepFace.find(img_path = "img1.jpg", db_path = "C:/my_db")\n```\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-6-v2.jpg" width="95%"></p>\n\n**Facial Attribute Analysis** - [`Demo`](https://youtu.be/GT2UeN85BdA)\n\nDeepFace also comes with a strong facial attribute analysis module including [`age`](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/), [`gender`](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/), [`facial expression`](https://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/) (including angry, fear, neutral, sad, disgust, happy and surprise) and [`race`](https://sefiks.com/2019/11/11/race-and-ethnicity-prediction-in-keras/) (including asian, white, middle eastern, indian, latino and black) predictions. Result is going to be the size of faces appearing in the source image.\n\n```python\nobjs = DeepFace.analyze(\n  img_path = "img4.jpg", actions = [''age'', ''gender'', ''race'', ''emotion'']\n)\n```\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-2.jpg" width="95%"></p>\n\nAge model got Â± 4.65 MAE; gender model got 97.44% accuracy, 96.29% precision and 95.05% recall as mentioned in its [tutorial](https://sefiks.com/2019/02/13/apparent-age-and-gender-prediction-in-keras/).\n\n**Real Time Analysis** - [`Demo`](https://youtu.be/-c9sSJcx6wI), [`React Demo part-i`](https://youtu.be/IXoah6rhxac), [`React Demo part-ii`](https://youtu.be/_waBA-cH2D4)\n\nYou can run deepface for real time videos as well. Stream function will access your webcam and apply both face recognition and facial attribute analysis. The function starts to analyze a frame if it can focus a face sequentially 5 frames. Then, it shows results 5 seconds.\n\n```python\nDeepFace.stream(db_path = "C:/database")\n```\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/stock-3.jpg" width="90%"></p>\n\nEven though face recognition is based on one-shot learning, you can use multiple face pictures of a person as well. You should rearrange your directory structure as illustrated below.\n\n```bash\nuser\nâ”œâ”€â”€ database\nâ”‚   â”œâ”€â”€ Alice\nâ”‚   â”‚   â”œâ”€â”€ Alice1.jpg\nâ”‚   â”‚   â”œâ”€â”€ Alice2.jpg\nâ”‚   â”œâ”€â”€ Bob\nâ”‚   â”‚   â”œâ”€â”€ Bob.jpg\n```\n\nIf you intend to perform face verification or analysis tasks directly from your browser, [`deepface-react-ui`](https://github.com/serengil/deepface-react-ui) is a separate repository built using ReactJS depending on deepface api.\n\nHere, you can also find some real time demos for various facial recognition models:\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/refs/heads/master/icon/deepface-realtime.jpg" width="90%"></p>\n\n| Task                 | Model    | Demo                                    |\n| ---                  | ---      | ---                                     |\n| Facial Recognition   | DeepFace | [`Video`](https://youtu.be/YjYIMs5ZOfc) |\n| Facial Recognition   | FaceNet  | [`Video`](https://youtu.be/vB1I5vWgTQg) |\n| Facial Recognition   | VGG-Face | [`Video`](https://youtu.be/tSU_lNi0gQQ) |\n| Facial Recognition   | OpenFace | [`Video`](https://youtu.be/-4z2sL6wzP8) |\n| Age & Gender         | Default  | [`Video`](https://youtu.be/tFI7vZn3P7E) |\n| Race & Ethnicity     | Default  | [`Video`](https://youtu.be/-ztiy5eJha8) |\n| Emotion              | Default  | [`Video`](https://youtu.be/Y7DfLvLKScs) |\n| Celebrity Look-Alike | Default  | [`Video`](https://youtu.be/RMgIKU1H8DY) |\n\n**Embeddings** - [`Tutorial`](https://sefiks.com/2025/06/28/what-are-vector-embeddings-and-why-they-matter-in-ai/), [`Demo`](https://youtu.be/OYialFo7Qo4)\n\nFace recognition models basically represent facial images as multi-dimensional vectors. Sometimes, you need those embedding vectors directly. DeepFace comes with a dedicated representation function. Represent function returns a list of embeddings. Result is going to be the size of faces appearing in the image path.\n\n```python\nembedding_objs = DeepFace.represent(img_path = "img.jpg")\n```\n\nEmbeddings can be [plotted](https://sefiks.com/2020/05/01/a-gentle-introduction-to-face-recognition-in-deep-learning/) as below. Each slot is corresponding to a dimension value and dimension value is emphasized with colors. Similar to 2D barcodes, vertical dimension stores no information in the illustration.\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/embedding.jpg" width="95%"></p>\n\nIn summary, the distance between vector embeddings of the same person should be smaller than that between embeddings of different people. When reduced to two-dimensional space, the clusters become clearly distinguishable.\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/refs/heads/master/icon/facenet-pca.png" width="95%"></p>\n\n**Face recognition models** - [`Demo`](https://youtu.be/eKOZawGR3y0)\n\nDeepFace is a **hybrid** face recognition package. It currently wraps many **state-of-the-art** face recognition models: [`VGG-Face`](https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/) , [`FaceNet`](https://sefiks.com/2018/09/03/face-recognition-with-facenet-in-keras/), [`OpenFace`](https://sefiks.com/2019/07/21/face-recognition-with-openface-in-keras/), [`DeepFace`](https://sefiks.com/2020/02/17/face-recognition-with-facebook-deepface-in-keras/), [`DeepID`](https://sefiks.com/2020/06/16/face-recognition-with-deepid-in-keras/), [`ArcFace`](https://sefiks.com/2020/12/14/deep-face-recognition-with-arcface-in-keras-and-python/), [`Dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/), `SFace`, `GhostFaceNet` and `Buffalo_L`. The default configuration uses VGG-Face model.\n\n```python\nmodels = [\n    "VGG-Face", "Facenet", "Facenet512", "OpenFace", "DeepFace",\n    "DeepID", "ArcFace", "Dlib", "SFace", "GhostFaceNet",\n    "Buffalo_L",\n]\n\nresult = DeepFace.verify(\n  img1_path = "img1.jpg", img2_path = "img2.jpg", model_name = models[0]\n)\n\ndfs = DeepFace.find(\n  img_path = "img1.jpg", db_path = "C:/my_db", model_name = models[1]\n)\n\nembeddings = DeepFace.represent(\n  img_path = "img.jpg", model_name = models[2]\n)\n```\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/model-portfolio-20240316.jpg" width="95%"></p>\n\nFaceNet, VGG-Face, ArcFace and Dlib are overperforming ones based on experiments - see [`BENCHMARKS`](https://github.com/serengil/deepface/tree/master/benchmarks) for more details. You can find the measured scores of various models in DeepFace and the reported scores from their original studies in the following table.\n\n| Model          | Measured Score | Declared Score     |\n| -------------- | -------------- | ------------------ |\n| Facenet512     | 98.4%          | 99.6%              |\n| Human-beings   | 97.5%          | 97.5%              |\n| Facenet        | 97.4%          | 99.2%              |\n| Dlib           | 96.8%          | 99.3 %             |\n| VGG-Face       | 96.7%          | 98.9%              |\n| ArcFace        | 96.7%          | 99.5%              |\n| GhostFaceNet   | 93.3%          | 99.7%              |\n| SFace          | 93.0%          | 99.5%              |\n| OpenFace       | 78.7%          | 92.9%              |\n| DeepFace       | 69.0%          | 97.3%              |\n| DeepID         | 66.5%          | 97.4%              |\n\nConducting experiments with those models within DeepFace may reveal disparities compared to the original studies, owing to the adoption of distinct detection or normalization techniques. Furthermore, some models have been released solely with their backbones, lacking pre-trained weights. Thus, we are utilizing their re-implementations instead of the original pre-trained weights.\n\n**Face Detection and Alignment** - [`Demo`](https://youtu.be/GZ2p2hj2H5k)\n\nFace detection and alignment are important early stages of a modern face recognition pipeline. [Experiments](https://github.com/serengil/deepface/tree/master/benchmarks) show that detection increases the face recognition accuracy up to 42%, while alignment increases it up to 6%. [`OpenCV`](https://sefiks.com/2020/02/23/face-alignment-for-face-recognition-in-python-within-opencv/), [`Ssd`](https://sefiks.com/2020/08/25/deep-face-detection-with-opencv-in-python/), [`Dlib`](https://sefiks.com/2020/07/11/face-recognition-with-dlib-in-python/),  [`MtCnn`](https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/), `Faster MtCnn`, [`RetinaFace`](https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/), [`MediaPipe`](https://sefiks.com/2022/01/14/deep-face-detection-with-mediapipe/), `Yolo`, `YuNet` and `CenterFace` detectors are wrapped in deepface.\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/detector-portfolio-v6.jpg" width="95%"></p>\n\nAll deepface functions accept optional detector backend and align input arguments. You can switch among those detectors and alignment modes with these arguments. OpenCV is the default detector and alignment is on by default.\n\n```python\nbackends = [\n    ''opencv'', ''ssd'', ''dlib'', ''mtcnn'', ''fastmtcnn'',\n    ''retinaface'', ''mediapipe'', ''yolov8n'', ''yolov8m'', \n    ''yolov8l'', ''yolov11n'', ''yolov11s'', ''yolov11m'',\n    ''yolov11l'', ''yolov12n'', ''yolov12s'', ''yolov12m'',\n    ''yolov12l'', ''yunet'', ''centerface'',\n]\ndetector = backends[3]\nalign = True\n\nobj = DeepFace.verify(\n  img1_path = "img1.jpg", img2_path = "img2.jpg", detector_backend = detector, align = align\n)\n\ndfs = DeepFace.find(\n  img_path = "img.jpg", db_path = "my_db", detector_backend = detector, align = align\n)\n\nembedding_objs = DeepFace.represent(\n  img_path = "img.jpg", detector_backend = detector, align = align\n)\n\ndemographies = DeepFace.analyze(\n  img_path = "img4.jpg", detector_backend = detector, align = align\n)\n\nface_objs = DeepFace.extract_faces(\n  img_path = "img.jpg", detector_backend = detector, align = align\n)\n```\n\nFace recognition models are actually CNN models and they expect standard sized inputs. So, resizing is required before representation. To avoid deformation, deepface adds black padding pixels according to the target size argument after detection and alignment.\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/detector-outputs-20240414.jpg" width="90%"></p>\n\n[RetinaFace](https://sefiks.com/2021/04/27/deep-face-detection-with-retinaface-in-python/) and [MtCnn](https://sefiks.com/2020/09/09/deep-face-detection-with-mtcnn-in-python/) seem to overperform in detection and alignment stages but they are much slower. If the speed of your pipeline is more important, then you should use opencv or ssd. On the other hand, if you consider the accuracy, then you should use retinaface or mtcnn.\n\nThe performance of RetinaFace is very satisfactory even in the crowd as seen in the following illustration. Besides, it comes with an incredible facial landmark detection performance. Highlighted red points show some facial landmarks such as eyes, nose and mouth. That''s why, alignment score of RetinaFace is high as well.\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/retinaface-results.jpeg" width="90%">\n<br><em>The Yellow Angels - Fenerbahce Women''s Volleyball Team</em>\n</p>\n\nYou can find out more about RetinaFace on this [repo](https://github.com/serengil/retinaface).\n\n**Face Anti Spoofing** - [`Demo`](https://youtu.be/UiK1aIjOBlQ)\n\nDeepFace also includes an anti-spoofing analysis module to understand given image is real or fake. To activate this feature, set the `anti_spoofing` argument to True in any DeepFace tasks.\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/face-anti-spoofing.jpg" width="40%"></p>\n\n```python\n# anti spoofing test in face detection\nface_objs = DeepFace.extract_faces(img_path="dataset/img1.jpg", anti_spoofing = True)\nassert all(face_obj["is_real"] is True for face_obj in face_objs)\n\n# anti spoofing test in real time analysis\nDeepFace.stream(db_path = "C:/database", anti_spoofing = True)\n```\n\n**Similarity** - [`Demo`](https://youtu.be/1EPoS69fHOc)\n\nFace recognition models are regular [convolutional neural networks](https://sefiks.com/2018/03/23/convolutional-autoencoder-clustering-images-with-neural-networks/) and they are responsible to represent faces as vectors. We expect that a face pair of same person should be [more similar](https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/) than a face pair of different persons.\n\nSimilarity could be calculated by different metrics such as [Cosine Similarity](https://sefiks.com/2018/08/13/cosine-similarity-in-machine-learning/), Angular Distance, Euclidean Distance or L2 normalized Euclidean. The default configuration uses cosine similarity. According to [experiments](https://github.com/serengil/deepface/tree/master/benchmarks), no distance metric is overperforming than other.\n\n```python\nmetrics = ["cosine", "euclidean", "euclidean_l2", "angular"]\n\nresult = DeepFace.verify(\n  img1_path = "img1.jpg", img2_path = "img2.jpg", distance_metric = metrics[1]\n)\n\ndfs = DeepFace.find(\n  img_path = "img1.jpg", db_path = "C:/my_db", distance_metric = metrics[2]\n)\n```\n\n**API** - [`Demo`](https://youtu.be/HeKCQ6U9XmI), [`Docker Demo`](https://youtu.be/9Tk9lRQareA)\n\nDeepFace serves an API as well - see [`api folder`](https://github.com/serengil/deepface/tree/master/deepface/api/src) for more details. You can clone deepface source code and run the api with the following command. It will use gunicorn server to get a rest service up. In this way, you can call deepface from an external system such as mobile app or web.\n\n```shell\ncd scripts\n\n# run the service directly\n./service.sh\n\n# run the service via docker\n./dockerize.sh\n```\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-api.jpg" width="90%"></p>\n\nFace recognition, facial attribute analysis and vector representation functions are covered in the API. You are expected to call these functions as http post methods. Default service endpoints will be `http://localhost:5005/verify` for face recognition, `http://localhost:5005/analyze` for facial attribute analysis, and `http://localhost:5005/represent` for vector representation. The API accepts images as file uploads (via form data), or as exact image paths, URLs, or base64-encoded strings (via either JSON or form data), providing versatile options for different client requirements. [Here](https://github.com/serengil/deepface/tree/master/deepface/api/postman), you can find a postman project to find out how these methods should be called.\n\n**Large Scale Facial Recognition** - [`Playlist`](https://www.youtube.com/playlist?list=PLsS_1RYmYQQGSJu_Z3OVhXhGmZ86_zuIm)\n\nIf your task requires facial recognition on large datasets, you should combine DeepFace with a vector index or vector database. This setup will perform [approximate nearest neighbor](https://youtu.be/c10w0Ptn_CU) searches instead of exact ones, allowing you to identify a face in a database containing billions of entries within milliseconds. Common vector index solutions include [Annoy](https://youtu.be/Jpxm914o2xk), [Faiss](https://youtu.be/6AmEvDTKT-k), [Voyager](https://youtu.be/2ZYTV9HlFdU), [NMSLIB](https://youtu.be/EVBhO8rbKbg), [ElasticSearch](https://youtu.be/i4GvuOmzKzo). For vector databases, popular options are [Postgres with its pgvector extension](https://youtu.be/Xfv4hCWvkp0) and [RediSearch](https://youtu.be/yrXlS0d6t4w).\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/deepface-big-data.jpg" width="90%"></p>\n\nConversely, if your task involves facial recognition on small to moderate-sized databases, you can adopt use relational databases such as [Postgres](https://youtu.be/f41sLxn1c0k) or [SQLite](https://youtu.be/_1ShBeWToPg), or NoSQL databases like [Mongo](https://youtu.be/dmprgum9Xu8), [Redis](https://youtu.be/X7DSpUMVTsw) or [Cassandra](https://youtu.be/J_yXpc3Y8Ec) to perform exact nearest neighbor search.\n\n**Encrypt Embeddings** - [`Demo with PHE`](https://youtu.be/8VCu39jFZ7k), [`Tutorial for PHE`](https://sefiks.com/2025/03/04/vector-similarity-search-with-partially-homomorphic-encryption-in-python/), [`Demo with FHE`](https://youtu.be/njjw0PEhH00), [`Tutorial for FHE`](https://sefiks.com/2021/12/01/homomorphic-facial-recognition-with-tenseal/)\n\nVector embeddings, though not reversible, carry sensitive information like fingerprints, making their security crucial. Encrypting them prevents adversarial misuse. Traditional encryption (e.g., AES) is secure but unsuitable for cloud-based distance calculations.\n\n[Homomorphic encryption](https://youtu.be/3ejI0zNPMEQ) allows computations on encrypted data without revealing contentâ€”ideal for secure cloud processing. For example, the cloud can compute encrypted similarity without knowing the data, while only the key holder can decrypt the result. See the  [`LightPHE`](https://github.com/serengil/LightPHE) library for partially homomorphic encryption.\n\n```python\nfrom lightphe import LightPHE\n\n# build an additively homomorphic cryptosystem (e.g. Paillier) on-prem\ncs = LightPHE(algorithm_name = "Paillier", precision = 19)\n\n# define encrypted and plain vectors\nencrypted_alpha = DeepFace.represent("source.jpg", cryptosystem=cs)[0]["encrypted_embedding"]\nbeta = DeepFace.represent("target.jpg")[0]["embedding"]\n\n# dot product of encrypted & plain embedding in cloud - private key not required\nencrypted_cosine_similarity = encrypted_alpha @ beta\n\n# decrypt similarity on-prem - private key required\ncalculated_similarity = cs.decrypt(encrypted_cosine_similarity)[0]\n\n# verification\nprint("same person" if calculated_similarity >= 1 - threshold else "different persons")\n```\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/refs/heads/master/icon/encrypt-embeddings.jpg" width="60%"></p>\n\nFor stronger privacy, fully homomorphic encryption enables dot product computations between encrypted embeddings, but it''s far more computationally intensive. Explore [`CipherFace`](https://github.com/serengil/cipherface) for FHE-based approaches.\n\n### Extended Applications\n\nDeepFace can also be used for fun and insightful applications such as\n\n**Find Your Celebrity Look-Alike** - [`Demo`](https://youtu.be/jaxkEn-Kieo), [`Real-Time Demo`](https://youtu.be/RMgIKU1H8DY), [`Tutorial`](https://sefiks.com/2019/05/05/celebrity-look-alike-face-recognition-with-deep-learning-in-keras/)\n\nDeepFace can analyze your facial features and match them with celebrities, letting you discover which famous personality you resemble the most.\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/celebrity-look-alike.jpg" width="55%"></p>\n\n**Find Which Parent a Child Look More** - [`Demo`](https://youtu.be/nza4tmi9vhE), [`Tutorial`](https://sefiks.com/2022/12/22/decide-whom-your-child-looks-like-with-facial-recognition-mommy-or-daddy/)\n\nDeepFace can also be used to compare a child''s face to their parents'' or relatives'' faces to determine which one the child resembles more.\n\n<p align="center"><img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/parental-look-alike-scaled.jpg" width="90%"></p>\n\n## Contribution\n\nPull requests are more than welcome! If you are planning to contribute a large patch, please create an issue first to get any upfront questions or design decisions out of the way first.\n\nBefore creating a PR, you should run the unit tests and linting locally by running `make test && make lint` command. Once a PR sent, GitHub test workflow will be run automatically and unit test and linting jobs will be available in [GitHub actions](https://github.com/serengil/deepface/actions) before approval.\n\n## Support\n\nThere are many ways to support a project - starringâ­ï¸ the GitHub repo is just one ğŸ™ It really helps the project get discovered by more people.\n\nIf you do like this work, then you can support it financially on [Patreon](https://www.patreon.com/serengil?repo=deepface), [GitHub Sponsors](https://github.com/sponsors/serengil) or [Buy Me a Coffee](https://buymeacoffee.com/serengil). Also, your company''s logo will be shown on README on GitHub if you become a sponsor in gold, silver or bronze tiers.\n\n<a href="https://www.patreon.com/serengil?repo=deepface">\n<img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/patreon.png" width="30%">\n</a>\n\n<!--\n<a href="https://github.com/sponsors/serengil">\n<img src="https://raw.githubusercontent.com/serengil/deepface/refs/heads/master/icon/github_sponsor_button.png" width="37%">\n</a>\n\n<a href="https://buymeacoffee.com/serengil">\n<img src="https://raw.githubusercontent.com/serengil/deepface/master/icon/bmc-button.png" width="25%">\n</a>\n-->\n\n<!--\nAdditionally, you can help us reach a wider audience by upvoting our posts on Hacker News and Product Hunt.\n\n<div style="display: flex; align-items: center; gap: 10px;">\n  <a href="https://news.ycombinator.com/item?id=42584896">\n    <img src="https://hackerbadge.vercel.app/api?id=42584896&type=orange" style="width: 250px; height: 54px;" width="250" alt="Featured on Hacker News">\n  </a>\n  \n  <a href="https://www.producthunt.com/posts/deepface?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-deepface" target="_blank">\n    <img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=753599&theme=light" alt="DeepFace - A Lightweight Deep Face Recognition Library for Python | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" />\n  </a>\n</div>\n-->\n\n## Citation\n\nPlease cite deepface in your publications if it helps your research.\n\n<details open>\n  <summary>S. Serengil and A. Ozpinar, <b>"A Benchmark of Facial Recognition Pipelines and Co-Usability Performances of Modules"</b>, <i>Journal of Information Technologies</i>, vol. 17, no. 2, pp. 95-107, 2024.</summary>\n  \n  ```BibTeX\n  @article{serengil2024lightface,\n    title     = {A Benchmark of Facial Recognition Pipelines and Co-Usability Performances of Modules},\n    author    = {Serengil, Sefik and Ozpinar, Alper},\n    journal   = {Journal of Information Technologies},\n    volume    = {17},\n    number    = {2},\n    pages     = {95-107},\n    year      = {2024},\n    doi       = {10.17671/gazibtd.1399077},\n    url       = {https://dergipark.org.tr/en/pub/gazibtd/issue/84331/1399077},\n    publisher = {Gazi University}\n  }\n  ```\n</details>\n\n<details>\n  <summary>S. I. Serengil and A. Ozpinar, <b>"LightFace: A Hybrid Deep Face Recognition Framework"</b>, <i>2020 Innovations in Intelligent Systems and Applications Conference (ASYU)</i>, 2020, pp. 23-27.</summary>\n  \n  ```BibTeX\n  @inproceedings{serengil2020lightface,\n    title        = {LightFace: A Hybrid Deep Face Recognition Framework},\n    author       = {Serengil, Sefik Ilkin and Ozpinar, Alper},\n    booktitle    = {2020 Innovations in Intelligent Systems and Applications Conference (ASYU)},\n    pages        = {23-27},\n    year         = {2020},\n    doi          = {10.1109/ASYU50717.2020.9259802},\n    url          = {https://ieeexplore.ieee.org/document/9259802},\n    organization = {IEEE}\n  }\n  ```\n</details>\n\n<details>\n  <summary>S. I. Serengil and A. Ozpinar, <b>"HyperExtended LightFace: A Facial Attribute Analysis Framework"</b>, <i>2021 International Conference on Engineering and Emerging Technologies (ICEET)</i>, 2021, pp. 1-4.</summary>\n  \n  ```BibTeX\n  @inproceedings{serengil2021lightface,\n    title        = {HyperExtended LightFace: A Facial Attribute Analysis Framework},\n    author       = {Serengil, Sefik Ilkin and Ozpinar, Alper},\n    booktitle    = {2021 International Conference on Engineering and Emerging Technologies (ICEET)},\n    pages        = {1-4},\n    year         = {2021},\n    doi          = {10.1109/ICEET53442.2021.9659697},\n    url          = {https://ieeexplore.ieee.org/document/9659697},\n    organization = {IEEE}\n  }\n  ```\n</details>\n\nAlso, if you use deepface in your GitHub projects, please add `deepface` in the `requirements.txt`.\n\n## Licence\n\nDeepFace is licensed under the MIT License - see [`LICENSE`](https://github.com/serengil/deepface/blob/master/LICENSE) for more details.\n\nDeepFace wraps some external face recognition models: [VGG-Face](http://www.robots.ox.ac.uk/~vgg/software/vgg_face/), [Facenet](https://github.com/davidsandberg/facenet/blob/master/LICENSE.md) (both 128d and 512d), [OpenFace](https://github.com/iwantooxxoox/Keras-OpenFace/blob/master/LICENSE), [DeepFace](https://github.com/swghosh/DeepFace), [DeepID](https://github.com/Ruoyiran/DeepID/blob/master/LICENSE.md), [ArcFace](https://github.com/leondgarse/Keras_insightface/blob/master/LICENSE), [Dlib](https://github.com/davisking/dlib/blob/master/dlib/LICENSE.txt), [SFace](https://github.com/opencv/opencv_zoo/blob/master/models/face_recognition_sface/LICENSE), [GhostFaceNet](https://github.com/HamadYA/GhostFaceNets/blob/main/LICENSE) and\n[Buffalo_L](https://github.com/deepinsight/insightface/blob/master/README.md). Besides, age, gender and race / ethnicity models were trained on the backbone of VGG-Face with transfer learning. Similarly, DeepFace wraps many face detectors: [OpenCv](https://github.com/opencv/opencv/blob/4.x/LICENSE), [Ssd](https://github.com/opencv/opencv/blob/master/LICENSE), [Dlib](https://github.com/davisking/dlib/blob/master/LICENSE.txt), [MtCnn](https://github.com/ipazc/mtcnn/blob/master/LICENSE), [Fast MtCnn](https://github.com/timesler/facenet-pytorch/blob/master/LICENSE.md), [RetinaFace](https://github.com/serengil/retinaface/blob/master/LICENSE), [MediaPipe](https://github.com/google/mediapipe/blob/master/LICENSE), [YuNet](https://github.com/ShiqiYu/libfacedetection/blob/master/LICENSE), [Yolo](https://github.com/ultralytics/ultralytics/blob/main/LICENSE) and [CenterFace](https://github.com/Star-Clouds/CenterFace/blob/master/LICENSE). Finally, DeepFace is optionally using [face anti spoofing](https://github.com/minivision-ai/Silent-Face-Anti-Spoofing/blob/master/LICENSE) to determine the given images are real or fake. License types will be inherited when you intend to utilize those models. Please check the license types of those models for production purposes.\n\nDeepFace [logo](https://thenounproject.com/term/face-recognition/2965879/) is created by [Adrien Coquet](https://thenounproject.com/coquet_adrien/) and it is licensed under [Creative Commons: By Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/).\n', '{"language":"Python","stars":21235,"forks":2889,"watchers":21235,"open_issues":10,"topics":["age-prediction","arcface","deep-learning","deepface","deepid","emotion-recognition","face-analysis","face-recognition","facenet","facial-expression-recognition","facial-recognition","gender-prediction","machine-learning","openface","python","race-classification","vgg-face"],"default_branch":"master","size_kb":56775,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:sponsors:serengil","source_url":"https://github.com/sponsors/serengil"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:deepface.git","source_url":"https://github.com/serengil/deepface.git"},{"type":"has_code","target_id":"github:serengil:deepface-react-ui","source_url":"https://github.com/serengil/deepface-react-ui"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:retinaface","source_url":"https://github.com/serengil/retinaface"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:serengil:LightPHE","source_url":"https://github.com/serengil/LightPHE"},{"type":"has_code","target_id":"github:serengil:cipherface","source_url":"https://github.com/serengil/cipherface"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:sponsors:serengil","source_url":"https://github.com/sponsors/serengil"},{"type":"has_code","target_id":"github:sponsors:serengil\">","source_url":"https://github.com/sponsors/serengil\">"},{"type":"has_code","target_id":"github:serengil:deepface","source_url":"https://github.com/serengil/deepface"},{"type":"has_code","target_id":"github:davidsandberg:facenet","source_url":"https://github.com/davidsandberg/facenet"},{"type":"has_code","target_id":"github:iwantooxxoox:Keras-OpenFace","source_url":"https://github.com/iwantooxxoox/Keras-OpenFace"},{"type":"has_code","target_id":"github:swghosh:DeepFace","source_url":"https://github.com/swghosh/DeepFace"},{"type":"has_code","target_id":"github:Ruoyiran:DeepID","source_url":"https://github.com/Ruoyiran/DeepID"},{"type":"has_code","target_id":"github:leondgarse:Keras_insightface","source_url":"https://github.com/leondgarse/Keras_insightface"},{"type":"has_code","target_id":"github:davisking:dlib","source_url":"https://github.com/davisking/dlib"},{"type":"has_code","target_id":"github:opencv:opencv_zoo","source_url":"https://github.com/opencv/opencv_zoo"},{"type":"has_code","target_id":"github:HamadYA:GhostFaceNets","source_url":"https://github.com/HamadYA/GhostFaceNets"},{"type":"has_code","target_id":"github:deepinsight:insightface","source_url":"https://github.com/deepinsight/insightface"},{"type":"has_code","target_id":"github:opencv:opencv","source_url":"https://github.com/opencv/opencv"},{"type":"has_code","target_id":"github:opencv:opencv","source_url":"https://github.com/opencv/opencv"},{"type":"has_code","target_id":"github:davisking:dlib","source_url":"https://github.com/davisking/dlib"},{"type":"has_code","target_id":"github:ipazc:mtcnn","source_url":"https://github.com/ipazc/mtcnn"},{"type":"has_code","target_id":"github:timesler:facenet-pytorch","source_url":"https://github.com/timesler/facenet-pytorch"},{"type":"has_code","target_id":"github:serengil:retinaface","source_url":"https://github.com/serengil/retinaface"},{"type":"has_code","target_id":"github:google:mediapipe","source_url":"https://github.com/google/mediapipe"},{"type":"has_code","target_id":"github:ShiqiYu:libfacedetection","source_url":"https://github.com/ShiqiYu/libfacedetection"},{"type":"has_code","target_id":"github:ultralytics:ultralytics","source_url":"https://github.com/ultralytics/ultralytics"},{"type":"has_code","target_id":"github:Star-Clouds:CenterFace","source_url":"https://github.com/Star-Clouds/CenterFace"},{"type":"has_code","target_id":"github:minivision-ai:Silent-Face-Anti-Spoofing","source_url":"https://github.com/minivision-ai/Silent-Face-Anti-Spoofing"}]', NULL, 'MIT', 'approved', 80, '772741bcea4680a613c0526b4aad24d8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-serengil-deepface from https://github.com/serengil.png
Image converted to WebP: data/images/github-serengil-deepface.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-recommenders-team-recommenders', 'github--recommenders-team--recommenders', 'recommenders', 'recommenders-team', '<!-- Copyright (c) Recommenders contributors. Licensed under the MIT License. --> <img src="https://raw.githubusercontent.com/recommenders-team/artwork/main/color/recommenders_color.svg" width="800"> <img align="left" width="300" src="https://raw.githubusercontent.com/recommenders-team/artwork/main/mix/join_recommenders_slack.svg"> <br> We reached 20,000 stars!! We are happy to announce that we have reached 20,000 stars on GitHub! Thank you for your support and contributions to the Recommende...', '["ai","artificial-intelligence","data-science","deep-learning","jupyter-notebook","kubernetes","machine-learning","operationalization","python","ranking","rating","recommendation","recommendation-algorithm","recommendation-engine","recommendation-system","recommender","tutorial","python"]', 'other', 21208, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/recommenders-team/recommenders","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<!--\nCopyright (c) Recommenders contributors.\nLicensed under the MIT License.\n-->\n<img src="https://raw.githubusercontent.com/recommenders-team/artwork/main/color/recommenders_color.svg" width="800">\n\n\n[![Documentation status](https://github.com/recommenders-team/recommenders/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/recommenders-team/recommenders/actions/workflows/pages/pages-build-deployment)\n[![License](https://img.shields.io/github/license/recommenders-team/recommenders.svg)](https://github.com/recommenders-team/recommenders/blob/main/LICENSE)\n[![Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n[![PyPI Version](https://img.shields.io/pypi/v/recommenders.svg?logo=pypi&logoColor=white)](https://pypi.org/project/recommenders)\n[![Python Versions](https://img.shields.io/pypi/pyversions/recommenders.svg?logo=python&logoColor=white)](https://pypi.org/project/recommenders)\n\n[<img align="left" width="300" src="https://raw.githubusercontent.com/recommenders-team/artwork/main/mix/join_recommenders_slack.svg">](https://join.slack.com/t/lfaifoundation/shared_invite/zt-2iyl7zyya-g5rOO5K518CBoevyi28W6w)\n\n<br>\n\n## What''s New (April, 2025)\n\nWe reached 20,000 stars!!\n\nWe are happy to announce that we have reached 20,000 stars on GitHub! Thank you for your support and contributions to the Recommenders project. We are excited to continue building and improving this project with your help.\n\nCheck out the release [Recommenders 1.2.1](https://github.com/recommenders-team/recommenders/releases/tag/1.2.1)!\n\nWe fixed a lot of bugs due to dependencies, improved security, reviewed the notebooks and the libraries.\n\n## Introduction\n\nRecommenders objective is to assist researchers, developers and enthusiasts in prototyping, experimenting with and bringing to production a range of classic and state-of-the-art recommendation systems.\n\nRecommenders is a project under the [Linux Foundation of AI and Data](https://lfaidata.foundation/projects/). \n\nThis repository contains examples and best practices for building recommendation systems, provided as Jupyter notebooks. The examples detail our learnings on five key tasks:\n\n- [Prepare Data](examples/01_prepare_data): Preparing and loading data for each recommendation algorithm.\n- [Model](examples/00_quick_start): Building models using various classical and deep learning recommendation algorithms such as Alternating Least Squares ([ALS](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html#ALS)) or eXtreme Deep Factorization Machines ([xDeepFM](https://arxiv.org/abs/1803.05170)).\n- [Evaluate](examples/03_evaluate): Evaluating algorithms with offline metrics.\n- [Model Select and Optimize](examples/04_model_select_and_optimize): Tuning and optimizing hyperparameters for recommendation models.\n- [Operationalize](examples/05_operationalize): Operationalizing models in a production environment on Azure.\n\nSeveral utilities are provided in [recommenders](recommenders) to support common tasks such as loading datasets in the format expected by different algorithms, evaluating model outputs, and splitting training/test data. Implementations of several state-of-the-art algorithms are included for self-study and customization in your own applications. See the [Recommenders documentation](https://readthedocs.org/projects/microsoft-recommenders/).\n\nFor a more detailed overview of the repository, please see the documents on the [wiki page](https://github.com/microsoft/recommenders/wiki/Documents-and-Presentations).\n\nFor some of the practical scenarios where recommendation systems have been applied, see [scenarios](scenarios). \n\n## Getting Started\n\nWe recommend [conda](https://docs.conda.io/projects/conda/en/latest/glossary.html?highlight=environment#conda-environment) for environment management, and [VS Code](https://code.visualstudio.com/) for development. To install the recommenders package and run an example notebook on Linux/WSL:\n\n```bash\n# 1. Install gcc if it is not installed already. On Ubuntu, this could done by using the command\n# sudo apt install gcc\n\n# 2. Create and activate a new conda environment\nconda create -n <environment_name> python=3.9\nconda activate <environment_name>\n\n# 3. Install the core recommenders package. It can run all the CPU notebooks.\npip install recommenders\n\n# 4. create a Jupyter kernel\npython -m ipykernel install --user --name <environment_name> --display-name <kernel_name>\n\n# 5. Clone this repo within VSCode or using command line:\ngit clone https://github.com/recommenders-team/recommenders.git\n\n# 6. Within VSCode:\n#   a. Open a notebook, e.g., examples/00_quick_start/sar_movielens.ipynb;  \n#   b. Select Jupyter kernel <kernel_name>;\n#   c. Run the notebook.\n```\n\nFor more information about setup on other platforms (e.g., Windows and macOS) and different configurations (e.g., GPU, Spark and experimental features), see the [Setup Guide](SETUP.md).\n\nIn addition to the core package, several extras are also provided, including:\n+ `[gpu]`: Needed for running GPU models.\n+ `[spark]`: Needed for running Spark models.\n+ `[dev]`: Needed for development for the repo.\n+ `[all]`: `[gpu]`|`[spark]`|`[dev]`\n+ `[experimental]`: Models that are not thoroughly tested and/or may require additional steps in installation.\n\n## Algorithms\n\nThe table below lists the recommendation algorithms currently available in the repository. Notebooks are linked under the Example column as Quick start, showcasing an easy to run example of the algorithm, or as Deep dive, explaining in detail the math and implementation of the algorithm.\n\n| Algorithm | Type | Description | Example |\n|-----------|------|-------------|---------|\n| Alternating Least Squares (ALS) | Collaborative Filtering | Matrix factorization algorithm for explicit or implicit feedback in large datasets, optimized for scalability and distributed computing capability. It works in the PySpark environment. | [Quick start](examples/00_quick_start/als_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/als_deep_dive.ipynb) |\n| Attentive Asynchronous Singular Value Decomposition (A2SVD)<sup>*</sup> | Collaborative Filtering | Sequential-based algorithm that aims to capture both long and short-term user preferences using attention mechanism. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Cornac/Bayesian Personalized Ranking (BPR) | Collaborative Filtering | Matrix factorization algorithm for predicting item ranking with implicit feedback. It works in the CPU environment. | [Deep dive](examples/02_model_collaborative_filtering/cornac_bpr_deep_dive.ipynb) |\n| Cornac/Bilateral Variational Autoencoder (BiVAE) | Collaborative Filtering | Generative model for dyadic data (e.g., user-item interactions). It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/cornac_bivae_deep_dive.ipynb) |\n| Convolutional Sequence Embedding Recommendation (Caser) | Collaborative Filtering | Algorithm based on convolutions that aim to capture both userâ€™s general preferences and sequential patterns. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Deep Knowledge-Aware Network (DKN)<sup>*</sup> | Content-Based Filtering | Deep learning algorithm incorporating a knowledge graph and article embeddings for providing news or article recommendations. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/dkn_MIND.ipynb) / [Deep dive](examples/02_model_content_based_filtering/dkn_deep_dive.ipynb) |\n| Extreme Deep Factorization Machine (xDeepFM)<sup>*</sup> | Collaborative Filtering | Deep learning based algorithm for implicit and explicit feedback with user/item features. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/xdeepfm_criteo.ipynb) |\n| Embedding Dot Bias | Collaborative Filtering | General purpose algorithm with embeddings and biases for users and items. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/embdotbias_movielens.ipynb) |\n| LightFM/Factorization Machine | Collaborative Filtering | Factorization Machine algorithm for both implicit and explicit feedbacks. It works in the CPU environment. | [Quick start](examples/02_model_collaborative_filtering/lightfm_deep_dive.ipynb) |\n| LightGBM/Gradient Boosting Tree<sup>*</sup> | Content-Based Filtering | Gradient Boosting Tree algorithm for fast training and low memory usage in content-based problems. It works in the CPU/GPU/PySpark environments. | [Quick start in CPU](examples/00_quick_start/lightgbm_tinycriteo.ipynb) / [Deep dive in PySpark](examples/02_model_content_based_filtering/mmlspark_lightgbm_criteo.ipynb) |\n| LightGCN | Collaborative Filtering | Deep learning algorithm which simplifies the design of GCN for predicting implicit feedback. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb) |\n| GeoIMC<sup>*</sup> | Collaborative Filtering | Matrix completion algorithm that takes into account user and item features using Riemannian conjugate gradient optimization and follows a geometric approach. It works in the CPU environment. | [Quick start](examples/00_quick_start/geoimc_movielens.ipynb) |\n| GRU | Collaborative Filtering | Sequential-based algorithm that aims to capture both long and short-term user preferences using recurrent neural networks. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Multinomial VAE | Collaborative Filtering | Generative model for predicting user/item interactions. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/multi_vae_deep_dive.ipynb) |\n| Neural Recommendation with Long- and Short-term User Representations (LSTUR)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with long- and short-term user interest modeling. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/lstur_MIND.ipynb) |\n| Neural Recommendation with Attentive Multi-View Learning (NAML)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with attentive multi-view learning. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/naml_MIND.ipynb) |\n| Neural Collaborative Filtering (NCF) | Collaborative Filtering | Deep learning algorithm with enhanced performance for user/item implicit feedback. It works in the CPU/GPU environment.| [Quick start](examples/00_quick_start/ncf_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/ncf_deep_dive.ipynb) |\n| Neural Recommendation with Personalized Attention (NPA)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with personalized attention network. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/npa_MIND.ipynb) |\n| Neural Recommendation with Multi-Head Self-Attention (NRMS)<sup>*</sup> | Content-Based Filtering | Neural recommendation algorithm for recommending news articles with multi-head self-attention. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/nrms_MIND.ipynb) |\n| Next Item Recommendation (NextItNet) | Collaborative Filtering | Algorithm based on dilated convolutions and residual network that aims to capture sequential patterns. It considers both user/item interactions and features.  It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Restricted Boltzmann Machines (RBM) | Collaborative Filtering | Neural network based algorithm for learning the underlying probability distribution for explicit or implicit user/item feedback. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/rbm_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/rbm_deep_dive.ipynb) |\n| Riemannian Low-rank Matrix Completion (RLRMC)<sup>*</sup> | Collaborative Filtering | Matrix factorization algorithm using Riemannian conjugate gradients optimization with small memory consumption to predict user/item interactions. It works in the CPU environment. | [Quick start](examples/00_quick_start/rlrmc_movielens.ipynb) |\n| Simple Algorithm for Recommendation (SAR)<sup>*</sup> | Collaborative Filtering | Similarity-based algorithm for implicit user/item feedback.  It works in the CPU environment. | [Quick start](examples/00_quick_start/sar_movielens.ipynb) / [Deep dive](examples/02_model_collaborative_filtering/sar_deep_dive.ipynb) |\n| Self-Attentive Sequential Recommendation (SASRec) | Collaborative Filtering | Transformer based algorithm for sequential recommendation. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sasrec_amazon.ipynb) |\n| Short-term and Long-term Preference Integrated Recommender (SLi-Rec)<sup>*</sup> | Collaborative Filtering | Sequential-based algorithm that aims to capture both long and short-term user preferences using attention mechanism, a time-aware controller and a content-aware controller. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Multi-Interest-Aware Sequential User Modeling (SUM)<sup>*</sup> | Collaborative Filtering | An enhanced memory network-based sequential user model which aims to capture users'' multiple interests. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sequential_recsys_amazondataset.ipynb) |\n| Sequential Recommendation Via Personalized Transformer (SSEPT) | Collaborative Filtering | Transformer based algorithm for sequential recommendation with User embedding. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/sasrec_amazon.ipynb) |\n| Standard VAE | Collaborative Filtering | Generative Model for predicting user/item interactions.  It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/standard_vae_deep_dive.ipynb) |\n| Surprise/Singular Value Decomposition (SVD) | Collaborative Filtering | Matrix factorization algorithm for predicting explicit rating feedback in small datasets. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb) |\n| Term Frequency - Inverse Document Frequency (TF-IDF) | Content-Based Filtering | Simple similarity-based algorithm for content-based recommendations with text datasets. It works in the CPU environment. | [Quick  start](examples/00_quick_start/tfidf_covid.ipynb) |\n| Vowpal Wabbit (VW)<sup>*</sup> | Content-Based Filtering | Fast online learning algorithms, great for scenarios where user features / context are constantly changing. It uses the CPU for online learning. | [Deep dive](examples/02_model_content_based_filtering/vowpal_wabbit_deep_dive.ipynb) |\n| Wide and Deep | Collaborative Filtering | Deep learning algorithm that can memorize feature interactions and generalize user features. It works in the CPU/GPU environment. | [Quick start](examples/00_quick_start/wide_deep_movielens.ipynb) |\n| xLearn/Factorization Machine (FM) & Field-Aware FM (FFM) | Collaborative Filtering | Quick and memory efficient algorithm to predict labels with user/item features. It works in the CPU/GPU environment. | [Deep dive](examples/02_model_collaborative_filtering/fm_deep_dive.ipynb) |\n\n**NOTE**: <sup>*</sup> indicates algorithms invented/contributed by Microsoft.\n\nIndependent or incubating algorithms and utilities are candidates for the [contrib](contrib) folder. This will house contributions which may not easily fit into the core repository or need time to refactor or mature the code and add necessary tests.\n\n| Algorithm | Type | Description | Example |\n|-----------|------|-------------|---------|\n| SARplus <sup>*</sup> | Collaborative Filtering | Optimized implementation of SAR for Spark |  [Quick start](contrib/sarplus/README.md) |\n\n### Algorithm Comparison\n\nWe provide a [benchmark notebook](examples/06_benchmarks/movielens.ipynb) to illustrate how different algorithms could be evaluated and compared. In this notebook, the MovieLens dataset is split into training/test sets at a 75/25 ratio using a stratified split. A recommendation model is trained using each of the collaborative filtering algorithms below. We utilize empirical parameter values reported in literature [here](http://mymedialite.net/examples/datasets.html). For ranking metrics we use `k=10` (top 10 recommended items). We run the comparison on a machine with 4 CPUs, 30Gb of RAM, and 1 GPU GeForce GTX 1660 Ti with 6Gb of memory. Spark ALS is run in local standalone mode. In this table we show the results on Movielens 100k, running the algorithms for 15 epochs.\n\n| Algo | MAP | nDCG@k | Precision@k | Recall@k | RMSE | MAE | R<sup>2</sup> | Explained Variance |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| [ALS](examples/00_quick_start/als_movielens.ipynb) | 0.004732 |	0.044239 |	0.048462 |	0.017796 | 0.965038 |	0.753001 |	0.255647 |	0.251648 |\n| [BiVAE](examples/02_model_collaborative_filtering/cornac_bivae_deep_dive.ipynb) | 0.146126	| 0.475077 |	0.411771 |	0.219145 | N/A |	N/A |	N/A |	N/A |\n| [BPR](examples/02_model_collaborative_filtering/cornac_bpr_deep_dive.ipynb) | 0.132478	| 0.441997 |	0.388229 |	0.212522 | N/A |	N/A |	N/A |	N/A |\n| [embdotbias](examples/00_quick_start/embdotbias_movielens.ipynb) | 0.018954 |	0.117810 |	0.104242 |	0.042450 | 0.992760 | 0.776040 | 0.223344 |	0.223393 |\n| [LightGCN](examples/02_model_collaborative_filtering/lightgcn_deep_dive.ipynb) | 0.088526 | 0.419846 | 0.379626 | 0.144336 | N/A | N/A | N/A | N/A |\n| [NCF](examples/02_model_collaborative_filtering/ncf_deep_dive.ipynb) | 0.107720	| 0.396118 |	0.347296 |	0.180775 | N/A | N/A | N/A | N/A |\n| [SAR](examples/00_quick_start/sar_movielens.ipynb) | 0.110591 |	0.382461 | 	0.330753 | 0.176385 | 1.253805 | 1.048484 |	-0.569363 |	0.030474 |\n| [SVD](examples/02_model_collaborative_filtering/surprise_svd_deep_dive.ipynb) | 0.012873	| 0.095930 |	0.091198 |	0.032783 | 0.938681 | 0.742690 | 0.291967 | 0.291971 |\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Before contributing, please see our [contribution guidelines](CONTRIBUTING.md).\n\nThis project adheres to this [Code of Conduct](CODE_OF_CONDUCT.md) in order to foster a welcoming and inspiring community for all.\n\n<!--\nStopped AzureML MLOps. See #2251\n## Build Status\n\nThese tests are the nightly builds, which compute the asynchronous tests. `main` is our principal branch and `staging` is our development branch. We use [pytest](https://docs.pytest.org/) for testing python utilities in [recommenders](recommenders) and the Recommenders [notebook executor](recommenders/utils/notebook_utils.py) for the [notebooks](examples). \n\nFor more information about the testing pipelines, please see the [test documentation](tests/README.md).\n\n### AzureML Nightly Build Status\n\nThe nightly build tests are run daily on AzureML.\n\n| Build Type | Branch | Status |  | Branch | Status |\n| --- | --- | --- | --- | --- | --- |\n| **Linux CPU** | main | [![azureml-cpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml/badge.svg?branch=main)](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml?query=branch%3Amain) | | staging | [![azureml-cpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml/badge.svg?branch=staging)](https://github.com/microsoft/recommenders/actions/workflows/azureml-cpu-nightly.yml?query=branch%3Astaging) |\n| **Linux GPU** | main | [![azureml-gpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml/badge.svg?branch=main)](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml?query=branch%3Amain) | | staging | [![azureml-gpu-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml/badge.svg?branch=staging)](https://github.com/microsoft/recommenders/actions/workflows/azureml-gpu-nightly.yml?query=branch%3Astaging) |\n| **Linux Spark** | main | [![azureml-spark-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml/badge.svg?branch=main)](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml?query=branch%3Amain) | | staging | [![azureml-spark-nightly](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml/badge.svg?branch=staging)](https://github.com/microsoft/recommenders/actions/workflows/azureml-spark-nightly.yml?query=branch%3Astaging) |\n-->\n\n## References\n\n- **FREE COURSE**: M. GonzÃ¡lez-Fierro, "Recommendation Systems: A Practical Introduction", LinkedIn Learning, 2024. [Available on this link](https://www.linkedin.com/learning/recommendation-systems-a-practical-introduction).\n- D. Li, J. Lian, L. Zhang, K. Ren, D. Lu, T. Wu, X. Xie, "Recommender Systems: Frontiers and Practices", Springer, Beijing, 2024. [Available on this link](https://www.amazon.com/Recommender-Systems-Frontiers-Practices-Dongsheng/dp/9819989639/).\n- A. Argyriou, M. GonzÃ¡lez-Fierro, and L. Zhang, "Microsoft Recommenders: Best Practices for Production-Ready Recommendation Systems", *WWW 2020: International World Wide Web Conference Taipei*, 2020. Available online: https://dl.acm.org/doi/abs/10.1145/3366424.3382692\n- S. Graham,  J.K. Min, T. Wu, "Microsoft recommenders: tools to accelerate developing recommender systems", *RecSys ''19: Proceedings of the 13th ACM Conference on Recommender Systems*, 2019. Available online: https://dl.acm.org/doi/10.1145/3298689.3346967\n- L. Zhang, T. Wu, X. Xie, A. Argyriou, M. GonzÃ¡lez-Fierro and J. Lian, "Building Production-Ready Recommendation System at Scale", *ACM SIGKDD Conference on Knowledge Discovery and Data Mining 2019 (KDD 2019)*, 2019.\n', '{"language":"Python","stars":21208,"forks":3271,"watchers":21208,"open_issues":171,"topics":["ai","artificial-intelligence","data-science","deep-learning","jupyter-notebook","kubernetes","machine-learning","operationalization","python","ranking","rating","recommendation","recommendation-algorithm","recommendation-engine","recommendation-system","recommender","tutorial"],"default_branch":"main","size_kb":230975,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:recommenders-team:recommenders","source_url":"https://github.com/recommenders-team/recommenders"},{"type":"has_code","target_id":"github:recommenders-team:recommenders","source_url":"https://github.com/recommenders-team/recommenders"},{"type":"has_code","target_id":"github:recommenders-team:recommenders","source_url":"https://github.com/recommenders-team/recommenders"},{"type":"has_code","target_id":"github:psf:black","source_url":"https://github.com/psf/black"},{"type":"has_code","target_id":"github:recommenders-team:recommenders","source_url":"https://github.com/recommenders-team/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:recommenders-team:recommenders.git","source_url":"https://github.com/recommenders-team/recommenders.git"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"},{"type":"has_code","target_id":"github:microsoft:recommenders","source_url":"https://github.com/microsoft/recommenders"}]', NULL, 'MIT', 'approved', 80, '0b9886025a6f9299164396ed5bb14f80', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-recommenders-team-recommenders from https://github.com/recommenders-team.png
Image converted to WebP: data/images/github-recommenders-team-recommenders.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-huggingface-datasets', 'github--huggingface--datasets', 'datasets', 'huggingface', '<p align="center"> <picture> <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-dark.svg"> <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg"> <img alt="Hugging Face Datasets Library" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg" width="352" heigh...', '["ai","artificial-intelligence","computer-vision","dataset-hub","datasets","deep-learning","huggingface","llm","machine-learning","natural-language-processing","nlp","numpy","pandas","pytorch","speech","tensorflow","python"]', 'other', 20959, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/huggingface/datasets","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<p align="center">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-dark.svg">\n    <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg">\n    <img alt="Hugging Face Datasets Library" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/datasets-logo-light.svg" width="352" height="59" style="max-width: 100%;">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align="center">\n    <a href="https://github.com/huggingface/datasets/actions/workflows/ci.yml?query=branch%3Amain"><img alt="Build" src="https://github.com/huggingface/datasets/actions/workflows/ci.yml/badge.svg?branch=main"></a>\n    <a href="https://github.com/huggingface/datasets/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/datasets.svg?color=blue"></a>\n    <a href="https://huggingface.co/docs/datasets/index.html"><img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/datasets/index.html.svg?down_color=red&down_message=offline&up_message=online"></a>\n    <a href="https://github.com/huggingface/datasets/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/datasets.svg"></a>\n    <a href="https://huggingface.co/datasets/"><img alt="Number of datasets" src="https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/datasets&color=brightgreen"></a>\n    <a href="CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg"></a>\n    <a href="https://zenodo.org/badge/latestdoi/250213286"><img src="https://zenodo.org/badge/250213286.svg" alt="DOI"></a>\n</p>\n\nğŸ¤— Datasets is a lightweight library providing **two** main features:\n\n- **one-line dataloaders for many public datasets**: one-liners to download and pre-process any of the ![number of datasets](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/datasets&color=brightgreen) major public datasets (image datasets, audio datasets, text datasets in 467 languages and dialects, etc.) provided on the [HuggingFace Datasets Hub](https://huggingface.co/datasets). With a simple command like `squad_dataset = load_dataset("rajpurkar/squad")`, get any of these datasets ready to use in a dataloader for training/evaluating a ML model (Numpy/Pandas/PyTorch/TensorFlow/JAX),\n- **efficient data pre-processing**: simple, fast and reproducible data pre-processing for the public datasets as well as your own local datasets in CSV, JSON, text, PNG, JPEG, WAV, MP3, Parquet, HDF5, etc. With simple commands like `processed_dataset = dataset.map(process_example)`, efficiently prepare the dataset for inspection and ML model evaluation and training.\n\n[ğŸ“ **Documentation**](https://huggingface.co/docs/datasets/) [ğŸ” **Find a dataset in the Hub**](https://huggingface.co/datasets) [ğŸŒŸ **Share a dataset on the Hub**](https://huggingface.co/docs/datasets/share)\n\n<h3 align="center">\n    <a href="https://hf.co/course"><img src="https://raw.githubusercontent.com/huggingface/datasets/main/docs/source/imgs/course_banner.png"></a>\n</h3>\n\nğŸ¤— Datasets is designed to let the community easily add and share new datasets.\n\nğŸ¤— Datasets has many additional interesting features:\n\n- Thrive on large datasets: ğŸ¤— Datasets naturally frees the user from RAM memory limitation, all datasets are memory-mapped using an efficient zero-serialization cost backend (Apache Arrow).\n- Smart caching: never wait for your data to process several times.\n- Lightweight and fast with a transparent and pythonic API (multi-processing/caching/memory-mapping).\n- Built-in interoperability with NumPy, PyTorch, TensorFlow 2, JAX, Pandas, Polars and more.\n- Native support for audio, image and video data.\n- Enable streaming mode to save disk space and start iterating over the dataset immediately.\n\nğŸ¤— Datasets originated from a fork of the awesome [TensorFlow Datasets](https://github.com/tensorflow/datasets) and the HuggingFace team want to deeply thank the TensorFlow Datasets team for building this amazing library.\n\n# Installation\n\n## With pip\n\nğŸ¤— Datasets can be installed from PyPi and has to be installed in a virtual environment (venv or conda for instance)\n\n```bash\npip install datasets\n```\n\n## With conda\n\nğŸ¤— Datasets can be installed using conda as follows:\n\n```bash\nconda install -c huggingface -c conda-forge datasets\n```\n\nFollow the installation pages of TensorFlow and PyTorch to see how to install them with conda.\n\nFor more details on installation, check the installation page in the documentation: https://huggingface.co/docs/datasets/installation\n\n## Installation to use with Machine Learning & Data frameworks frameworks\n\nIf you plan to use ğŸ¤— Datasets with PyTorch (2.0+), TensorFlow (2.6+) or JAX (3.14+) you should also install PyTorch, TensorFlow or JAX.\nğŸ¤— Datasets is also well integrated with data frameworks like PyArrow, Pandas, Polars and Spark, which should be installed separately.\n\nFor more details on using the library with these frameworks, check the quick start page in the documentation: https://huggingface.co/docs/datasets/quickstart\n\n# Usage\n\nğŸ¤— Datasets is made to be very simple to use - the API is centered around a single function, `datasets.load_dataset(dataset_name, **kwargs)`, that instantiates a dataset.\n\nThis library can be used for text/image/audio/etc. datasets. Here is an example to load a text dataset:\n\nHere is a quick example:\n\n```python\nfrom datasets import load_dataset\n\n# Print all the available datasets\nfrom huggingface_hub import list_datasets\nprint([dataset.id for dataset in list_datasets()])\n\n# Load a dataset and print the first example in the training set\nsquad_dataset = load_dataset(''rajpurkar/squad'')\nprint(squad_dataset[''train''][0])\n\n# Process the dataset - add a column with the length of the context texts\ndataset_with_length = squad_dataset.map(lambda x: {"length": len(x["context"])})\n\n# Process the dataset - tokenize the context texts (using a tokenizer from the ğŸ¤— Transformers library)\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(''bert-base-cased'')\n\ntokenized_dataset = squad_dataset.map(lambda x: tokenizer(x[''context'']), batched=True)\n```\n\nIf your dataset is bigger than your disk or if you don''t want to wait to download the data, you can use streaming:\n\n```python\n# If you want to use the dataset immediately and efficiently stream the data as you iterate over the dataset\nimage_dataset = load_dataset(''timm/imagenet-1k-wds'', streaming=True)\nfor example in image_dataset["train"]:\n    break\n```\n\nFor more details on using the library, check the quick start page in the documentation: https://huggingface.co/docs/datasets/quickstart and the specific pages on:\n\n- Loading a dataset: https://huggingface.co/docs/datasets/loading\n- What''s in a Dataset: https://huggingface.co/docs/datasets/access\n- Processing data with ğŸ¤— Datasets: https://huggingface.co/docs/datasets/process\n    - Processing audio data: https://huggingface.co/docs/datasets/audio_process\n    - Processing image data: https://huggingface.co/docs/datasets/image_process\n    - Processing text data: https://huggingface.co/docs/datasets/nlp_process\n- Streaming a dataset: https://huggingface.co/docs/datasets/stream\n- etc.\n\n# Add a new dataset to the Hub\n\nWe have a very detailed step-by-step guide to add a new dataset to the ![number of datasets](https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/datasets&color=brightgreen) datasets already provided on the [HuggingFace Datasets Hub](https://huggingface.co/datasets).\n\nYou can find:\n- [how to upload a dataset to the Hub using your web browser or Python](https://huggingface.co/docs/datasets/upload_dataset) and also\n- [how to upload it using Git](https://huggingface.co/docs/datasets/share).\n\n# Disclaimers\n\nYou can use ğŸ¤— Datasets to load datasets based on versioned git repositories maintained by the dataset authors. For reproducibility reasons, we ask users to pin the `revision` of the repositories they use.\n\nIf you''re a dataset owner and wish to update any part of it (description, citation, license, etc.), or do not want your dataset to be included in the Hugging Face Hub, please get in touch by opening a discussion or a pull request in the Community tab of the dataset page. Thanks for your contribution to the ML community!\n\n## BibTeX\n\nIf you want to cite our ğŸ¤— Datasets library, you can use our [paper](https://huggingface.co/papers/2109.02846):\n\n```bibtex\n@inproceedings{lhoest-etal-2021-datasets,\n    title = "Datasets: A Community Library for Natural Language Processing",\n    author = "Lhoest, Quentin  and\n      Villanova del Moral, Albert  and\n      Jernite, Yacine  and\n      Thakur, Abhishek  and\n      von Platen, Patrick  and\n      Patil, Suraj  and\n      Chaumond, Julien  and\n      Drame, Mariama  and\n      Plu, Julien  and\n      Tunstall, Lewis  and\n      Davison, Joe  and\n      {\v{S}}a{\v{s}}ko, Mario  and\n      Chhablani, Gunjan  and\n      Malik, Bhavitvya  and\n      Brandeis, Simon  and\n      Le Scao, Teven  and\n      Sanh, Victor  and\n      Xu, Canwen  and\n      Patry, Nicolas  and\n      McMillan-Major, Angelina  and\n      Schmid, Philipp  and\n      Gugger, Sylvain  and\n      Delangue, Cl{\''e}ment  and\n      Matussi{\`e}re, Th{\''e}o  and\n      Debut, Lysandre  and\n      Bekman, Stas  and\n      Cistac, Pierric  and\n      Goehringer, Thibault  and\n      Mustar, Victor  and\n      Lagunas, Fran{\c{c}}ois  and\n      Rush, Alexander  and\n      Wolf, Thomas",\n    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",\n    month = nov,\n    year = "2021",\n    address = "Online and Punta Cana, Dominican Republic",\n    publisher = "Association for Computational Linguistics",\n    url = "https://aclanthology.org/2021.emnlp-demo.21",\n    pages = "175--184",\n    abstract = "The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.",\n    eprint={2109.02846},\n    archivePrefix={arXiv},\n    primaryClass={cs.CL},\n}\n```\n\nIf you need to cite a specific version of our ğŸ¤— Datasets library for reproducibility, you can use the corresponding version Zenodo DOI from this [list](https://zenodo.org/search?q=conceptrecid:%224817768%22&sort=-version&all_versions=True).\n', '{"language":"Python","stars":20959,"forks":3025,"watchers":20959,"open_issues":1009,"topics":["ai","artificial-intelligence","computer-vision","dataset-hub","datasets","deep-learning","huggingface","llm","machine-learning","natural-language-processing","nlp","numpy","pandas","pytorch","speech","tensorflow"],"default_branch":"main","size_kb":89723,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:huggingface:datasets","source_url":"https://github.com/huggingface/datasets"},{"type":"has_code","target_id":"github:huggingface:datasets","source_url":"https://github.com/huggingface/datasets"},{"type":"has_code","target_id":"github:huggingface:datasets","source_url":"https://github.com/huggingface/datasets"},{"type":"has_code","target_id":"github:huggingface:datasets","source_url":"https://github.com/huggingface/datasets"},{"type":"has_code","target_id":"github:tensorflow:datasets","source_url":"https://github.com/tensorflow/datasets"},{"type":"has_code","target_id":"github:huggingface:datasets.\",","source_url":"https://github.com/huggingface/datasets.\","}]', NULL, 'Apache-2.0', 'approved', 80, 'e2d8b3cf639b7dd93d544dffe356c5f5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-huggingface-datasets from https://github.com/huggingface.png
Image converted to WebP: data/images/github-huggingface-datasets.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-RasaHQ-rasa', 'github--rasahq--rasa', 'rasa', 'RasaHQ', '<h1 align="center">Rasa Open Source</h1> <div align="center"> !Documentation Build </div> <hr /> ğŸ’¡ **We''re migrating issues to Jira** ğŸ’¡ Starting January 2023, issues for Rasa Open Source are located in this Jira board. You can browse issues without being logged in; if you want to create issues, you''ll need to create a Jira account. <hr /> <img align="right" height="255" src="https://www.rasa.com/assets/img/sara/sara-open-source-2.0.png" alt="An image of Sara, the Rasa mascot bird, holding a...', '["bot","bot-framework","botkit","bots","chatbot","chatbots","chatbots-framework","conversation-driven-development","conversational-agents","conversational-ai","conversational-bots","machine-learning","machine-learning-library","mitie","natural-language-processing","nlp","nlu","rasa","spacy","wit","python"]', 'other', 20912, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/RasaHQ/rasa","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<h1 align="center">Rasa Open Source</h1>\n\n<div align="center">\n\n[![Join the chat on Rasa Community Forum](https://img.shields.io/badge/forum-join%20discussions-brightgreen.svg)](https://forum.rasa.com/?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![PyPI version](https://badge.fury.io/py/rasa.svg)](https://badge.fury.io/py/rasa)\n[![Supported Python Versions](https://img.shields.io/pypi/pyversions/rasa.svg)](https://pypi.python.org/pypi/rasa)\n[![Build Status](https://github.com/RasaHQ/rasa/workflows/Continuous%20Integration/badge.svg)](https://github.com/RasaHQ/rasa/actions)\n[![Coverage Status](https://api.codeclimate.com/v1/badges/756dc6fea1d5d3e127f7/test_coverage)](https://codeclimate.com/github/RasaHQ/rasa/)\n[![Documentation Status](https://img.shields.io/badge/docs-stable-brightgreen.svg)](https://rasa.com/docs)\n![Documentation Build](https://img.shields.io/netlify/d2e447e4-5a5e-4dc7-be5d-7c04ae7ff706?label=Documentation%20Build)\n[![FOSSA Status](https://app.fossa.com/api/projects/custom%2B8141%2Fgit%40github.com%3ARasaHQ%2Frasa.git.svg?type=shield)](https://app.fossa.com/projects/custom%2B8141%2Fgit%40github.com%3ARasaHQ%2Frasa.git?ref=badge_shield)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](https://github.com/orgs/RasaHQ/projects/23)\n\n</div>\n\n<hr />\n\nğŸ’¡ **We''re migrating issues to Jira** ğŸ’¡\n\nStarting January 2023, issues for Rasa Open Source are located in\n[this Jira board](https://rasa-open-source.atlassian.net/browse/OSS). You can browse issues without being logged in;\nif you want to create issues, you''ll need to create a Jira account.\n\n<hr />\n\n<img align="right" height="255" src="https://www.rasa.com/assets/img/sara/sara-open-source-2.0.png" alt="An image of Sara, the Rasa mascot bird, holding a flag that reads Open Source with one wing, and a wrench in the other" title="Rasa Open Source">\n\nRasa is an open source machine learning framework to automate text and voice-based conversations. With Rasa, you can build contextual assistants on:\n- Facebook Messenger\n- Slack\n- Google Hangouts\n- Webex Teams\n- Microsoft Bot Framework\n- Rocket.Chat\n- Mattermost\n- Telegram\n- Twilio\n- Your own custom conversational channels\n\nor voice assistants as:\n- Alexa Skills\n- Google Home Actions\n\nRasa helps you build contextual assistants capable of having layered conversations with\nlots of back-and-forth. In order for a human to have a meaningful exchange with a contextual\nassistant, the assistant needs to be able to use context to build on things that were previously\ndiscussed â€“ Rasa enables you to build assistants that can do this in a scalable way.\n\nThere''s a lot more background information in this\n[blog post](https://medium.com/rasa-blog/a-new-approach-to-conversational-software-2e64a5d05f2a).\n\n---\n- ğŸ¤” [Learn more about Rasa](https://rasa.community/)\n\n- ğŸ¤“ [Read The Docs](https://rasa.com/docs/rasa/)\n\n- ğŸ˜ [Install Rasa](https://rasa.com/docs/rasa/installation/environment-set-up)\n\n- ğŸš€ [Dive deeper in the learning center](https://learning.rasa.com/)\n\n- ğŸ¤— [Contribute](#how-to-contribute)\n\n- â“ [Get enterprise-grade support](https://rasa.com/support/)\n\n- ğŸ¢ [Explore the features of our commercial platform](https://rasa.com/product/rasa-platform/)\n\n- ğŸ“š [Learn more about research papers that leverage Rasa](https://scholar.google.com/scholar?oi=bibs&hl=en&authuser=1&cites=16243802403383697687,353275993797024115,14567308604105196228,9067977709825839723,9855847065463746011&as_sdt=5)\n\n\n\n---\n## Where to get help\n\nThere is extensive documentation in the [Rasa Docs](https://rasa.com/docs/rasa).\nMake sure to select the correct version so you are looking at\nthe docs for the version you installed.\n\nPlease use [Rasa Community Forum](https://forum.rasa.com) for quick answers to\nquestions.\n\n### README Contents:\n- [How to contribute](#how-to-contribute)\n- [Development Internals](#development-internals)\n- [Releases](#releases)\n- [License](#license)\n\n### How to contribute\nWe are very happy to receive and merge your contributions into this repository!\n\nTo contribute via pull request, follow these steps:\n\n1. Create an issue describing the feature you want to work on (or\n   have a look at the [contributor board](https://github.com/orgs/RasaHQ/projects/23))\n2. Write your code, tests and documentation, and format them with ``black``\n3. Create a pull request describing your changes\n\nFor more detailed instructions on how to contribute code, check out these [code contributor guidelines](CONTRIBUTING.md).\n\nYou can find more information about how to contribute to Rasa (in lots of\ndifferent ways!) [on our website.](http://rasa.community).\n\nYour pull request will be reviewed by a maintainer, who will get\nback to you about any necessary changes or questions. You will\nalso be asked to sign a\n[Contributor License Agreement](https://cla-assistant.io/RasaHQ/rasa).\n\n\n## Development Internals\n\n### Installing Poetry\n\nRasa uses Poetry for packaging and dependency management. If you want to build it from source,\nyou have to install Poetry first. Please follow\n[the official guide](https://python-poetry.org/docs/#installation) to see all possible options.\n\nTo update an existing poetry version to the [version](.github/poetry_version.txt), currently used in rasa, run:\n```shell\n    poetry self update <version>\n```\n\n### Managing environments\n\nThe official [Poetry guide](https://python-poetry.org/docs/managing-environments/) suggests to use\n[pyenv](https://github.com/pyenv/pyenv) or any other similar tool to easily switch between Python versions.\nThis is how it can be done:\n\n```bash\npyenv install 3.10.10\npyenv local 3.10.10  # Activate Python 3.10.10 for the current project\n```\n*Note*: If you have trouble installing a specific version of python on your system\nit might be worth trying other supported versions.\n\nBy default, Poetry will try to use the currently activated Python version to create the virtual environment\nfor the current project automatically. You can also create and activate a virtual environment manually â€” in this\ncase, Poetry should pick it up and use it to install the dependencies. For example:\n\n```bash\npython -m venv .venv\nsource .venv/bin/activate\n```\n\nYou can make sure that the environment is picked up by executing\n\n```bash\npoetry env info\n```\n\n### Building from source\n\nTo install dependencies and `rasa` itself in editable mode execute\n\n```bash\nmake install\n```\n\n*Note for macOS users*: under macOS Big Sur we''ve seen some compiler issues for\ndependencies. Using `export SYSTEM_VERSION_COMPAT=1` before the installation helped.\n\n\n#### Installing optional dependencies\n\nIn order to install rasa''s optional dependencies, you need to run:\n\n```bash\nmake install-full\n```\n\n*Note for macOS users*: The command `make install-full` could result in a failure while installing `tokenizers`\n(issue described in depth [here](https://github.com/huggingface/tokenizers/issues/1050)).\n\nIn order to resolve it, you must follow these steps to install a Rust compiler:\n```bash\nbrew install rustup\nrustup-init\n```\n\nAfter initialising the Rust compiler, you should restart the console and check its installation:\n```bash\nrustc --version\n```\n\nIn case the PATH variable had not been automatically setup, run:\n```bash\nexport PATH="$HOME/.cargo/bin:$PATH"\n```\n\n\n### Running and changing the documentation\n\nFirst of all, install all the required dependencies:\n\n```bash\nmake install install-docs\n```\n\nAfter the installation has finished, you can run and view the documentation\nlocally using:\n\n```bash\nmake livedocs\n```\n\nIt should open a new tab with the local version of the docs in your browser;\nif not, visit http://localhost:3000 in your browser.\nYou can now change the docs locally and the web page will automatically reload\nand apply your changes.\n\n### Running the Tests\n\nIn order to run the tests, make sure that you have the development requirements installed:\n\n```bash\nmake prepare-tests-ubuntu # Only on Ubuntu and Debian based systems\nmake prepare-tests-macos  # Only on macOS\n```\n\nThen, run the tests:\n\n```bash\nmake test\n```\n\nThey can also be run at multiple jobs to save some time:\n\n```bash\nJOBS=[n] make test\n```\n\nWhere `[n]` is the number of jobs desired. If omitted, `[n]` will be automatically chosen by pytest.\n\n\n### Running the Integration Tests\n\nIn order to run the integration tests, make sure that you have the development requirements installed:\n\n```bash\nmake prepare-tests-ubuntu # Only on Ubuntu and Debian based systems\nmake prepare-tests-macos  # Only on macOS\n```\n\nThen, you''ll need to start services with the following command which uses\n[Docker Compose](https://docs.docker.com/compose/install/):\n\n```bash\nmake run-integration-containers\n```\n\nFinally, you can run the integration tests like this:\n\n```bash\nmake test-integration\n```\n\n\n### Resolving merge conflicts\n\nPoetry doesn''t include any solution that can help to resolve merge conflicts in\nthe lock file `poetry.lock` by default.\nHowever, there is a great tool called [poetry-merge-lock](https://poetry-merge-lock.readthedocs.io/en/latest/).\nHere is how you can install it:\n\n```bash\npip install poetry-merge-lock\n```\n\nJust execute this command to resolve merge conflicts in `poetry.lock` automatically:\n\n```bash\npoetry-merge-lock\n```\n\n### Build a Docker image locally\n\nIn order to build a Docker image on your local machine execute the following command:\n\n```bash\nmake build-docker\n```\n\nThe Docker image is available on your local machine as `rasa:localdev`.\n\n### Code Style\n\nTo ensure a standardized code style we use the formatter [black](https://github.com/ambv/black).\nTo ensure our type annotations are correct we use the type checker [pytype](https://github.com/google/pytype).\nIf your code is not formatted properly or doesn''t type check, GitHub will fail to build.\n\n#### Formatting\n\nIf you want to automatically format your code on every commit, you can use [pre-commit](https://pre-commit.com/).\nJust install it via `pip install pre-commit` and execute `pre-commit install` in the root folder.\nThis will add a hook to the repository, which reformats files on every commit.\n\nIf you want to set it up manually, install black via `poetry install`.\nTo reformat files execute\n```\nmake formatter\n```\n\n#### Type Checking\n\nIf you want to check types on the codebase, install `mypy` using `poetry install`.\nTo check the types execute\n```\nmake types\n```\n\n### Deploying documentation updates\n\nWe use `Docusaurus v2` to build docs for tagged versions and for the `main` branch.\nTo run Docusaurus, install `Node.js 12.x`.\nThe static site that gets built is pushed to the `documentation` branch of this repo.\n\nWe host the site on netlify. On `main` branch builds (see `.github/workflows/documentation.yml`), we push the built docs to\nthe `documentation` branch. Netlify automatically re-deploys the docs pages whenever there is a change to that branch.\n\n## Releases\nRasa has implemented robust policies governing version naming, as well as release pace for major, minor, and patch releases.\n\nThe values for a given version number (MAJOR.MINOR.PATCH) are incremented as follows:\n- MAJOR version for incompatible API changes or other breaking changes.\n- MINOR version for functionality added in a backward compatible manner.\n- PATCH version for backward compatible bug fixes.\n\nThe following table describes the version types and their expected *release cadence*:\n\n| Version Type |                                                                  Description                                                                  |  Target Cadence |\n|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------|-----------------|\n| Major        | For significant changes, or when any backward-incompatible changes are introduced to the API or data model.                                   | Every 1 - 2 yrs |\n| Minor        | For when new backward-compatible functionality is introduced, a minor feature is introduced, or when a set of smaller features is rolled out. | +/- Quarterly   |\n| Patch        | For backward-compatible bug fixes that fix incorrect behavior.                                                                                | As needed       |\n\nWhile this table represents our target release frequency, we reserve the right to modify it based on changing market conditions and technical requirements.\n\n### Maintenance Policy\nOur End of Life policy defines how long a given release is considered supported, as well as how long a release is\nconsidered to be still in active development or maintenance.\n\nThe maintentance duration and end of life for every release are shown on our website as part of the [Product Release and Maintenance Policy](https://rasa.com/rasa-product-release-and-maintenance-policy/).\n\n### Cutting a Major / Minor release\n#### A week before release day\n\n1. **Make sure the [milestone](https://github.com/RasaHQ/rasa/milestones) already exists and is scheduled for the\ncorrect date.**\n2. **Take a look at the issues & PRs that are in the milestone**: does it look about right for the release highlights\nwe are planning to ship? Does it look like anything is missing? Don''t worry about being aware of every PR that should\nbe in, but it''s useful to take a moment to evaluate what''s assigned to the milestone.\n3. **Post a message on the engineering Slack channel**, letting the team know you''ll be the one cutting the upcoming\nrelease, as well as:\n    1. Providing the link to the appropriate milestone\n    2. Reminding everyone to go over their issues and PRs and please assign them to the milestone\n    3. Reminding everyone of the scheduled date for the release\n\n#### A day before release day\n\n1. **Go over the milestone and evaluate the status of any PR merging that''s happening. Follow up with people on their\nbugs and fixes.** If the release introduces new bugs or regressions that can''t be fixed in time, we should discuss on\nSlack about this and take a decision on how to move forward. If the issue is not ready to be merged in time, we remove the issue / PR from the milestone and notify the PR owner and the product manager on Slack about it. The PR / issue owners are responsible for\ncommunicating any issues which might be release relevant. Postponing the release should be considered as an edge case scenario.\n\n#### Release day! ğŸš€\n\n1. **At the start of the day, post a small message on slack announcing release day!** Communicate you''ll be handling\nthe release, and the time you''re aiming to start releasing (again, no later than 4pm, as issues may arise and\ncause delays). This message should be posted early in the morning and before moving forward with any of the steps of the release,\n   in order to give enough time to people to check their PRs and issues. That way they can plan any remaining work. A template of the slack message can be found [here](https://rasa-hq.slack.com/archives/C36SS4N8M/p1613032208137500?thread_ts=1612876410.068400&cid=C36SS4N8M).\n   The release time should be communicated transparently so that others can plan potentially necessary steps accordingly. If there are bigger changes this should be communicated.\n2. Make sure the milestone is empty (everything has been either merged or moved to the next milestone)\n3. Once everything in the milestone is taken care of, post a small message on Slack communicating you are about to\nstart the release process (in case anything is missing).\n4. **You may now do the release by following the instructions outlined in the\n[Rasa Open Source README](#steps-to-release-a-new-version) !**\n\n#### After a Major release\n\nAfter a Major release has been completed, please follow [these instructions to complete the documentation update](./docs/README.md#manual-steps-after-a-new-version).\n\n### Steps to release a new version\nReleasing a new version is quite simple, as the packages are build and distributed by GitHub Actions.\n\n*Release steps*:\n1. Make sure all dependencies are up to date (**especially Rasa SDK**)\n    - For Rasa SDK, except in the case of a patch release, that means first creating a [new Rasa SDK release](https://github.com/RasaHQ/rasa-sdk#steps-to-release-a-new-version) (make sure the version numbers between the new Rasa and Rasa SDK releases match)\n    - Once the tag with the new Rasa SDK release is pushed and the package appears on [pypi](https://pypi.org/project/rasa-sdk/), the dependency in the rasa repository can be resolved (see below).\n2. If this is a minor / major release: Make sure all fixes from currently supported minor versions have been merged from their respective release branches (e.g. 3.3.x) back into main.\n3. In case of a minor release, create a new branch that corresponds to the new release, e.g.\n   ```bash\n    git checkout -b 1.2.x\n    git push origin 1.2.x\n    ```\n4. Switch to the branch you want to cut the release from (`main` in case of a major, the `<major>.<minor>.x` branch for minors and patches)\n    - Update the `rasa-sdk` entry in `pyproject.toml` with the new release version and run `poetry update`. This creates a new `poetry.lock` file with all dependencies resolved.\n    - Commit the changes with `git commit -am "bump rasa-sdk dependency"` but do not push them. They will be automatically picked up by the following step.\n5. If this is a major release, update the list of actively maintained versions [in the README](#actively-maintained-versions) and in [the docs](./docs/docs/actively-maintained-versions.mdx).\n6. Run `make release`\n7. Create a PR against the release branch (e.g. `1.2.x`)\n8. Once your PR is merged, tag a new release (this SHOULD always happen on the release branch), e.g. using\n    ```bash\n    git checkout 1.2.x\n    git pull origin 1.2.x\n    git tag 1.2.0 -m "next release"\n    git push origin 1.2.0 --tags\n    ```\n    GitHub will build this tag and publish the build artifacts.\n9. After all the steps are completed and if everything goes well then we should see a message automatically posted in the company''s Slack (`product` channel) like this [one](https://rasa-hq.slack.com/archives/C7B08Q5FX/p1614354499046600)\n10. If no message appears in the channel then you can do the following checks:\n    - Check the workflows in [Github Actions](https://github.com/RasaHQ/rasa/actions) and make sure that the merged PR of the current release is completed successfully. To easily find your PR you can use the filters `event: push` and `branch: <version number>` (example on release 2.4 you can see [here](https://github.com/RasaHQ/rasa/actions/runs/643344876))\n    - If the workflow is not completed, then try to re run the workflow in case that solves the problem\n    - If the problem persists, check also the log files and try to find the root cause of the issue\n    - If you still cannot resolve the error, contact the infrastructure team by providing any helpful information from your investigation\n11.  After the message is posted correctly in the `product` channel, check also in the `product-engineering-alerts` channel if there are any alerts related to the Rasa Open Source release like this [one](https://rasa-hq.slack.com/archives/C01585AN2NP/p1615486087001000)\n\n### Cutting a Patch release\n\nPatch releases are simpler to cut, since they are meant to contain only bugfixes.\n\n**The only things you need to do to cut a patch release are:**\n\n1. Notify the engineering team on Slack that you are planning to cut a patch, in case someone has an important fix\nto add.\n2. Make sure the bugfix(es) are in the release branch you will use (p.e if you are cutting a `2.0.4` patch, you will\nneed your fixes to be on the `2.0.x` release branch). All patch releases must come from a `.x` branch!\n3. Once you''re ready to release the Rasa Open Source patch, checkout the branch, run `make release` and follow the\nsteps + get the PR merged.\n4. Once the PR is in, pull the `.x` branch again and push the tag!\n\n### Additional Release Tasks \n**Note: This is only required if the released version is the highest version available.\nFor instance, perform the following steps when version > [version](https://github.com/RasaHQ/rasa/blob/main/rasa/version.py) on main.**\n\nIn order to check compatibility between the new released Rasa version to the latest version of Rasa X/Enterprise, we perform the following steps:\n1. Following a new Rasa release, an automated pull request is created in [Rasa-X-Demo](https://github.com/RasaHQ/rasa-x-demo/pulls). \n2. Once the above PR is merged, follow instructions [here](https://github.com/RasaHQ/rasa-x-demo/blob/master/.github/VERSION_BUMPER_PR_COMMENT.md), to release a version.\n3. Update the new version in the Rasa X/Enterprise [env file](https://github.com/RasaHQ/rasa-x/blob/main/.env).\nThe [Rasa-X-Demo](https://github.com/RasaHQ/rasa-x-demo) project uses the new updated Rasa version to train and test a model which in turn is used by our CI to run tests in the Rasa X/Enterprise repository, \nthus validating compatibility between Rasa and Rasa X/Enterprise.\n\n### Actively maintained versions\n\nPlease refer to the [Rasa Product Release and Maintenance Policy](https://rasa.com/rasa-product-release-and-maintenance-policy/) page.\n\n## License\nLicensed under the Apache License, Version 2.0.\nCopyright 2022 Rasa Technologies GmbH. [Copy of the license](LICENSE.txt).\n\nA list of the Licenses of the dependencies of the project can be found at\nthe bottom of the\n[Libraries Summary](https://libraries.io/github/RasaHQ/rasa).\n', '{"language":"Python","stars":20912,"forks":4905,"watchers":20912,"open_issues":142,"topics":["bot","bot-framework","botkit","bots","chatbot","chatbots","chatbots-framework","conversation-driven-development","conversational-agents","conversational-ai","conversational-bots","machine-learning","machine-learning-library","mitie","natural-language-processing","nlp","nlu","rasa","spacy","wit"],"default_branch":"3.6.x","size_kb":1611206,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:RasaHQ:rasa","source_url":"https://github.com/RasaHQ/rasa"},{"type":"has_code","target_id":"github:RasaHQ:rasa","source_url":"https://github.com/RasaHQ/rasa"},{"type":"has_code","target_id":"github:orgs:RasaHQ","source_url":"https://github.com/orgs/RasaHQ"},{"type":"has_code","target_id":"github:orgs:RasaHQ","source_url":"https://github.com/orgs/RasaHQ"},{"type":"has_code","target_id":"github:pyenv:pyenv","source_url":"https://github.com/pyenv/pyenv"},{"type":"has_code","target_id":"github:huggingface:tokenizers","source_url":"https://github.com/huggingface/tokenizers"},{"type":"has_code","target_id":"github:ambv:black","source_url":"https://github.com/ambv/black"},{"type":"has_code","target_id":"github:google:pytype","source_url":"https://github.com/google/pytype"},{"type":"has_code","target_id":"github:RasaHQ:rasa","source_url":"https://github.com/RasaHQ/rasa"},{"type":"has_code","target_id":"github:RasaHQ:rasa-sdk","source_url":"https://github.com/RasaHQ/rasa-sdk#steps-to-release-a-new-version"},{"type":"has_code","target_id":"github:RasaHQ:rasa","source_url":"https://github.com/RasaHQ/rasa"},{"type":"has_code","target_id":"github:RasaHQ:rasa","source_url":"https://github.com/RasaHQ/rasa"},{"type":"has_code","target_id":"github:RasaHQ:rasa","source_url":"https://github.com/RasaHQ/rasa"},{"type":"has_code","target_id":"github:RasaHQ:rasa-x-demo","source_url":"https://github.com/RasaHQ/rasa-x-demo"},{"type":"has_code","target_id":"github:RasaHQ:rasa-x-demo","source_url":"https://github.com/RasaHQ/rasa-x-demo"},{"type":"has_code","target_id":"github:RasaHQ:rasa-x","source_url":"https://github.com/RasaHQ/rasa-x"},{"type":"has_code","target_id":"github:RasaHQ:rasa-x-demo","source_url":"https://github.com/RasaHQ/rasa-x-demo"}]', NULL, 'Apache-2.0', 'approved', 80, 'c0d9c8935d68694e56650513baa322b0', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-RasaHQ-rasa from https://github.com/RasaHQ.png
Image converted to WebP: data/images/github-RasaHQ-rasa.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Zeyi-Lin-HivisionIDPhotos', 'github--zeyi-lin--hivisionidphotos', 'HivisionIDPhotos', 'Zeyi-Lin', '<div align="center"> <img alt="hivision_logo" src="assets/hivision_logo.png" width=120 height=120> <h1>HivisionIDPhoto</h1> English / ä¸­æ–‡ / æ—¥æœ¬èª / í•œêµ­ì–´ [![][release-shield]][release-link] [![][dockerhub-shield]][dockerhub-link] [![][github-stars-shield]][github-stars-link] [![][github-issues-shield]][github-issues-link] [![][github-contributors-shield]][github-contributors-link] [![][github-forks-shield]][github-forks-link] [![][license-shield]][license-link] [![][wechat-shield]][wechat-link] [!...', '["cnn","demo","docker","face-recognition","fastapi","gradio","idphoto","machine-learning","matting","mtcnn","tools","unet","python"]', 'other', 20299, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n\n<img alt="hivision_logo" src="assets/hivision_logo.png" width=120 height=120>\n<h1>HivisionIDPhoto</h1>\n\n[English](README_EN.md) / ä¸­æ–‡ / [æ—¥æœ¬èª](README_JP.md) / [í•œêµ­ì–´](README_KO.md)\n\n[![][release-shield]][release-link]\n[![][dockerhub-shield]][dockerhub-link]\n[![][github-stars-shield]][github-stars-link]\n[![][github-issues-shield]][github-issues-link]\n[![][github-contributors-shield]][github-contributors-link]\n[![][github-forks-shield]][github-forks-link]\n[![][license-shield]][license-link]  \n[![][wechat-shield]][wechat-link]\n[![][spaces-shield]][spaces-link]\n[![][swanhub-demo-shield]][swanhub-demo-link]\n[![][modelscope-shield]][modelscope-link]\n[![][modelers-shield]][modelers-link]\n[![][compshare-shield]][compshare-link]\n\n[![][trendshift-shield]][trendshift-link]\n[![][hellogithub-shield]][hellogithub-link]\n\n<img src="assets/demoImage.jpg" width=900>\n\n</div>\n\n> **ç›¸å…³é¡¹ç›®**ï¼š\n>\n> - [SwanLab](https://github.com/SwanHubX/SwanLab)ï¼šä¸€ä¸ªå¼€æºã€ç°ä»£åŒ–è®¾è®¡çš„æ·±åº¦å­¦ä¹ è®­ç»ƒè·Ÿè¸ªä¸å¯è§†åŒ–å·¥å…·ï¼ŒåŒæ—¶æ”¯æŒäº‘ç«¯/ç¦»çº¿ä½¿ç”¨ï¼Œå›½å†…å¥½ç”¨çš„Wandbå¹³æ›¿ï¼›é€‚é…30+ä¸»æµæ¡†æ¶ï¼ˆPyTorchã€HuggingFace Transformersã€LLaMA Factoryã€Lightningç­‰ï¼‰ï¼Œæ¬¢è¿ä½¿ç”¨ï¼\n\n\n<br>\n\n# ç›®å½•\n\n- [æœ€è¿‘æ›´æ–°](#-æœ€è¿‘æ›´æ–°)\n- [é¡¹ç›®ç®€ä»‹](#-é¡¹ç›®ç®€ä»‹)\n- [ç¤¾åŒº](#-ç¤¾åŒº)\n- [å‡†å¤‡å·¥ä½œ](#-å‡†å¤‡å·¥ä½œ)\n- [Demoå¯åŠ¨](#-è¿è¡Œ-gradio-demo)\n- [Pythonæ¨ç†](#-python-æ¨ç†)\n- [APIæœåŠ¡éƒ¨ç½²](#ï¸-éƒ¨ç½²-api-æœåŠ¡)\n- [Dockeréƒ¨ç½²](#-docker-éƒ¨ç½²)\n- [è”ç³»æˆ‘ä»¬](#-è”ç³»æˆ‘ä»¬)\n- [FAQ](#faq)\n- [æ„Ÿè°¢æ”¯æŒ](#-æ„Ÿè°¢æ”¯æŒ)\n- [License](#-lincese)\n- [å¼•ç”¨](#-å¼•ç”¨)\n\n<br>\n\n# ğŸ¤© æœ€è¿‘æ›´æ–°\n\n- åœ¨çº¿ä½“éªŒï¼š [![Spaces](https://img.shields.io/badge/ğŸ¤—-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/TheEeeeLin/HivisionIDPhotos)ã€[![][modelscope-shield]][modelscope-link]ã€[![][modelers-shield]][modelers-link]ã€[![][compshare-shield]][compshare-link]\n\n- 2024.11.20: Gradio Demoå¢åŠ **æ‰“å°æ’ç‰ˆ**é€‰é¡¹å¡ï¼Œæ”¯æŒå…­å¯¸ã€äº”å¯¸ã€A4ã€3Rã€4Räº”ç§æ’ç‰ˆå°ºå¯¸\n- 2024.11.16: APIæ¥å£å¢åŠ ç¾é¢œå‚æ•°\n- 2024.09.25: å¢åŠ **äº”å¯¸ç›¸çº¸**å’Œ**JPEGä¸‹è½½**é€‰é¡¹ï½œé»˜è®¤ç…§ç‰‡ä¸‹è½½æ”¯æŒ300DPI\n- 2024.09.24: APIæ¥å£å¢åŠ base64å›¾åƒä¼ å…¥é€‰é¡¹ | Gradio Demoå¢åŠ **æ’ç‰ˆç…§è£å‰ªçº¿**åŠŸèƒ½\n- 2024.09.22: Gradio Demoå¢åŠ **é‡å…½æ¨¡å¼**ï¼Œå¯è®¾ç½®å†…å­˜åŠ è½½ç­–ç•¥ | APIæ¥å£å¢åŠ **dpiã€face_alignment**å‚æ•°\n- 2024.09.18: Gradio Demoå¢åŠ **åˆ†äº«æ¨¡ç‰ˆç…§**åŠŸèƒ½ã€å¢åŠ **ç¾å¼è¯ä»¶ç…§**èƒŒæ™¯é€‰é¡¹\n- 2024.09.17: Gradio Demoå¢åŠ **è‡ªå®šä¹‰åº•è‰²-HEXè¾“å…¥**åŠŸèƒ½ | **ï¼ˆç¤¾åŒºè´¡çŒ®ï¼‰C++ç‰ˆæœ¬** - [HivisionIDPhotos-cpp](https://github.com/zjkhahah/HivisionIDPhotos-cpp) è´¡çŒ® by [zjkhahah](https://github.com/zjkhahah)\n- 2024.09.16: Gradio Demoå¢åŠ **äººè„¸æ—‹è½¬å¯¹é½**åŠŸèƒ½ï¼Œè‡ªå®šä¹‰å°ºå¯¸è¾“å…¥æ”¯æŒ**æ¯«ç±³**å•ä½\n\n<br>\n\n# é¡¹ç›®ç®€ä»‹\n\n> ğŸš€ è°¢è°¢ä½ å¯¹æˆ‘ä»¬çš„å·¥ä½œæ„Ÿå…´è¶£ã€‚æ‚¨å¯èƒ½è¿˜æƒ³æŸ¥çœ‹æˆ‘ä»¬åœ¨å›¾åƒé¢†åŸŸçš„å…¶ä»–æˆæœï¼Œæ¬¢è¿æ¥ä¿¡:zeyi.lin@swanhub.co.\n\nHivisionIDPhoto æ—¨åœ¨å¼€å‘ä¸€ç§å®ç”¨ã€ç³»ç»Ÿæ€§çš„è¯ä»¶ç…§æ™ºèƒ½åˆ¶ä½œç®—æ³•ã€‚\n\nå®ƒåˆ©ç”¨ä¸€å¥—å®Œå–„çš„AIæ¨¡å‹å·¥ä½œæµç¨‹ï¼Œå®ç°å¯¹å¤šç§ç”¨æˆ·æ‹ç…§åœºæ™¯çš„è¯†åˆ«ã€æŠ å›¾ä¸è¯ä»¶ç…§ç”Ÿæˆã€‚\n\n**HivisionIDPhoto å¯ä»¥åšåˆ°ï¼š**\n\n1. è½»é‡çº§æŠ å›¾ï¼ˆçº¯ç¦»çº¿ï¼Œä»…éœ€ **CPU** å³å¯å¿«é€Ÿæ¨ç†ï¼‰\n2. æ ¹æ®ä¸åŒå°ºå¯¸è§„æ ¼ç”Ÿæˆä¸åŒçš„æ ‡å‡†è¯ä»¶ç…§ã€å…­å¯¸æ’ç‰ˆç…§\n3. æ”¯æŒ çº¯ç¦»çº¿ æˆ– ç«¯äº‘ æ¨ç†\n4. ç¾é¢œ\n5. æ™ºèƒ½æ¢æ­£è£…ï¼ˆwaitingï¼‰\n\n<div align="center">\n<img src="assets/demo.png" width=900>\n</div>\n\n---\n\nå¦‚æœ HivisionIDPhoto å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯· star è¿™ä¸ª repo æˆ–æ¨èç»™ä½ çš„æœ‹å‹ï¼Œè§£å†³è¯ä»¶ç…§åº”æ€¥åˆ¶ä½œé—®é¢˜ï¼\n\n<br>\n\n# ğŸ  ç¤¾åŒº\n\næˆ‘ä»¬åˆ†äº«äº†ä¸€äº›ç”±ç¤¾åŒºæ„å»ºçš„HivisionIDPhotosçš„æœ‰è¶£åº”ç”¨å’Œæ‰©å±•ï¼š\n\n| [HivisionIDPhotos-ComfyUI][community-hivision-comfyui] | [HivisionIDPhotos-wechat-weapp][community-hivision-wechat] |\n| :----------------------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------------: |\n| <a href="https://github.com/AIFSH/HivisionIDPhotos-ComfyUI"> <img src="assets/comfyui.png" width="900" alt="ComfyUI workflow"> </a>  | <a href="https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp"> <img src="assets/community-wechat-miniprogram.png" width="900" alt="ComfyUI workflow"> </a>  |\n|ComfyUIè¯ä»¶ç…§å¤„ç†å·¥ä½œæµ | è¯ä»¶ç…§å¾®ä¿¡å°ç¨‹åºï¼ˆJAVAåç«¯+åŸç”Ÿå‰ç«¯ï¼‰ |\n\n| [HivisionIDPhotos-Uniapp][community-hivision-uniapp] | [HivisionIDPhotos-web](https://github.com/jkm199/HivisionIDPhotos-web)|\n| :------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------------: |\n| <a href="https://github.com/soulerror/HivisionIDPhotos-Uniapp"> <img src="assets/community-uniapp-wechat-miniprogram.png" width="900" alt="HivisionIDPhotos-uniapp"> </a>  | <a href="https://github.com/jkm199/HivisionIDPhotos-web"> <img src="assets/community-web.png" width="900" alt="HivisionIDPhotos-uniapp"> </a>  |\n| è¯ä»¶ç…§å¾®ä¿¡å°ç¨‹åºï¼ˆuniappï¼‰| è¯ä»¶ç…§åº”ç”¨ç½‘é¡µç‰ˆ |\n\n\n- [HivisionIDPhotos-cpp](https://github.com/zjkhahah/HivisionIDPhotos-cpp): HivisionIDphotos C++ç‰ˆæœ¬ï¼Œç”± [zjkhahah](https://github.com/zjkhahah) æ„å»º\n- [ai-idphoto](https://github.com/wmlcjj/ai-idphoto): [HivisionIDPhotos-wechat-weapp](https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp) çš„uniappå¤šç«¯å…¼å®¹ç‰ˆï¼Œç”± [wmlcjj](https://github.com/wmlcjj) è´¡çŒ®\n- [HivisionIDPhotos-uniapp-WeChat-gpto1](https://github.com/jkm199/HivisionIDPhotos-uniapp-WeChat-gpto1/): ç”±gpt-o1è¾…åŠ©å®Œæˆå¼€å‘çš„è¯ä»¶ç…§å¾®ä¿¡å°ç¨‹åºï¼Œç”± [jkm199](https://github.com/jkm199) è´¡çŒ®\n- [HivisionIDPhotos-windows-GUI](https://github.com/zhaoyun0071/HivisionIDPhotos-windows-GUI)ï¼šWindowså®¢æˆ·ç«¯åº”ç”¨ï¼Œç”± [zhaoyun0071](https://github.com/zhaoyun0071) æ„å»º\n- [HivisionIDPhotos-NAS](https://github.com/ONG-Leo/HivisionIDPhotos-NAS): ç¾¤æ™–NASéƒ¨ç½²ä¸­æ–‡æ•™ç¨‹ï¼Œç”± [ONG-Leo](https://github.com/ONG-Leo) è´¡çŒ®\n\n\n<br>\n\n# ğŸ”§ å‡†å¤‡å·¥ä½œ\n\nç¯å¢ƒå®‰è£…ä¸ä¾èµ–ï¼š\n- Python >= 3.7ï¼ˆé¡¹ç›®ä¸»è¦æµ‹è¯•åœ¨ python 3.10ï¼‰\n- OS: Linux, Windows, MacOS\n\n## 1. å…‹éš†é¡¹ç›®\n\n```bash\ngit clone https://github.com/Zeyi-Lin/HivisionIDPhotos.git\ncd  HivisionIDPhotos\n```\n\n## 2. å®‰è£…ä¾èµ–ç¯å¢ƒ\n\n> å»ºè®® conda åˆ›å»ºä¸€ä¸ª python3.10 è™šæ‹Ÿç¯å¢ƒåï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤\n\n```bash\npip install -r requirements.txt\npip install -r requirements-app.txt\n```\n\n## 3. ä¸‹è½½äººåƒæŠ å›¾æ¨¡å‹æƒé‡æ–‡ä»¶\n\n**æ–¹å¼ä¸€ï¼šè„šæœ¬ä¸‹è½½**\n\n```bash\npython scripts/download_model.py --models all\n# å¦‚éœ€æŒ‡å®šä¸‹è½½æŸä¸ªæ¨¡å‹\n# python scripts/download_model.py --models modnet_photographic_portrait_matting\n```\n\n**æ–¹å¼äºŒï¼šç›´æ¥ä¸‹è½½**\n\næ¨¡å‹å‡å­˜åˆ°é¡¹ç›®çš„`hivision/creator/weights`ç›®å½•ä¸‹ï¼š\n\n| äººåƒæŠ å›¾æ¨¡å‹ | ä»‹ç» | ä¸‹è½½ |\n| -- | -- | -- |\n| MODNet | [MODNet](https://github.com/ZHKKKe/MODNet)å®˜æ–¹æƒé‡ | [ä¸‹è½½](https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/modnet_photographic_portrait_matting.onnx)(24.7MB)|\n| hivision_modnet | å¯¹çº¯è‰²æ¢åº•é€‚é…æ€§æ›´å¥½çš„æŠ å›¾æ¨¡å‹ | [ä¸‹è½½](https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/hivision_modnet.onnx)(24.7MB) |\n| rmbg-1.4 | [BRIA AI](https://huggingface.co/briaai/RMBG-1.4) å¼€æºçš„æŠ å›¾æ¨¡å‹ | [ä¸‹è½½](https://huggingface.co/briaai/RMBG-1.4/resolve/main/onnx/model.onnx?download=true)(176.2MB)åé‡å‘½åä¸º`rmbg-1.4.onnx` |\n| birefnet-v1-lite | [ZhengPeng7](https://github.com/ZhengPeng7/BiRefNet) å¼€æºçš„æŠ å›¾æ¨¡å‹ï¼Œæ‹¥æœ‰æœ€å¥½çš„åˆ†å‰²ç²¾åº¦ | [ä¸‹è½½](https://github.com/ZhengPeng7/BiRefNet/releases/download/v1/BiRefNet-general-bb_swin_v1_tiny-epoch_232.onnx)(224MB)åé‡å‘½åä¸º`birefnet-v1-lite.onnx` |\n\n> å¦‚æœä¸‹è½½ç½‘é€Ÿä¸é¡ºåˆ©ï¼šå‰å¾€[SwanHub](https://swanhub.co/ZeYiLin/HivisionIDPhotos_models/tree/main)ä¸‹è½½ã€‚\n\n\n## 4. äººè„¸æ£€æµ‹æ¨¡å‹é…ç½®ï¼ˆå¯é€‰ï¼‰\n\n| æ‹“å±•äººè„¸æ£€æµ‹æ¨¡å‹ | ä»‹ç» | ä½¿ç”¨æ–‡æ¡£ |\n| -- | -- | -- |\n| MTCNN | **ç¦»çº¿**äººè„¸æ£€æµ‹æ¨¡å‹ï¼Œé«˜æ€§èƒ½CPUæ¨ç†ï¼ˆæ¯«ç§’çº§ï¼‰ï¼Œä¸ºé»˜è®¤æ¨¡å‹ï¼Œæ£€æµ‹ç²¾åº¦è¾ƒä½ | Cloneæ­¤é¡¹ç›®åç›´æ¥ä½¿ç”¨ |\n| RetinaFace | **ç¦»çº¿**äººè„¸æ£€æµ‹æ¨¡å‹ï¼ŒCPUæ¨ç†é€Ÿåº¦ä¸­ç­‰ï¼ˆç§’çº§ï¼‰ï¼Œç²¾åº¦è¾ƒé«˜| [ä¸‹è½½](https://github.com/Zeyi-Lin/HivisionIDPhotos/releases/download/pretrained-model/retinaface-resnet50.onnx)åæ”¾åˆ°`hivision/creator/retinaface/weights`ç›®å½•ä¸‹ |\n| Face++ | æ—·è§†æ¨å‡ºçš„åœ¨çº¿äººè„¸æ£€æµ‹APIï¼Œæ£€æµ‹ç²¾åº¦è¾ƒé«˜ï¼Œ[å®˜æ–¹æ–‡æ¡£](https://console.faceplusplus.com.cn/documents/4888373) | [ä½¿ç”¨æ–‡æ¡£](docs/face++_CN.md)|\n\n## 5. æ€§èƒ½å‚è€ƒ\n\n> æµ‹è¯•ç¯å¢ƒä¸ºMac M1 Max 64GBï¼ŒéGPUåŠ é€Ÿï¼Œæµ‹è¯•å›¾ç‰‡åˆ†è¾¨ç‡ä¸º 512x715(1) ä¸ 764Ã—1146(2)ã€‚\n\n| æ¨¡å‹ç»„åˆ | å†…å­˜å ç”¨ | æ¨ç†æ—¶é•¿(1) | æ¨ç†æ—¶é•¿(2) |\n| -- | -- | -- | -- |\n| MODNet + mtcnn | 410MB | 0.207s | 0.246s |\n| MODNet + retinaface | 405MB | 0.571s | 0.971s |\n| birefnet-v1-lite + retinaface | 6.20GB | 7.063s | 7.128s |\n\n## 6. GPUæ¨ç†åŠ é€Ÿï¼ˆå¯é€‰ï¼‰\n\nåœ¨å½“å‰ç‰ˆæœ¬ï¼Œå¯è¢«è‹±ä¼Ÿè¾¾GPUåŠ é€Ÿçš„æ¨¡å‹ä¸º`birefnet-v1-lite`ï¼Œå¹¶è¯·ç¡®ä¿ä½ æœ‰16GBå·¦å³çš„æ˜¾å­˜ã€‚\n\nå¦‚éœ€ä½¿ç”¨è‹±ä¼Ÿè¾¾GPUåŠ é€Ÿæ¨ç†ï¼Œåœ¨ç¡®ä¿ä½ å·²ç»å®‰è£…[CUDA](https://developer.nvidia.com/cuda-downloads)ä¸[cuDNN](https://developer.nvidia.com/cudnn)åï¼Œæ ¹æ®[onnxruntime-gpuæ–‡æ¡£](https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#cuda-12x)æ‰¾åˆ°å¯¹åº”çš„`onnxruntime-gpu`ç‰ˆæœ¬å®‰è£…ï¼Œä»¥åŠæ ¹æ®[pytorchå®˜ç½‘](https://pytorch.org/get-started/locally/)æ‰¾åˆ°å¯¹åº”çš„`torch`ç‰ˆæœ¬å®‰è£…ã€‚\n\n```bash\n# å‡å¦‚ä½ çš„ç”µè„‘å®‰è£…çš„æ˜¯CUDA 12.x, cuDNN 8\n# å®‰è£…torchæ˜¯å¯é€‰çš„ï¼Œå¦‚æœä½ å§‹ç»ˆé…ç½®ä¸å¥½cuDNNï¼Œé‚£ä¹ˆè¯•è¯•å®‰è£…torch\npip install onnxruntime-gpu==1.18.0\npip install torch --index-url https://download.pytorch.org/whl/cu121\n```\n\nå®Œæˆå®‰è£…åï¼Œè°ƒç”¨`birefnet-v1-lite`æ¨¡å‹å³å¯åˆ©ç”¨GPUåŠ é€Ÿæ¨ç†ã€‚\n\n> TIPS: CUDA æ”¯æŒå‘ä¸‹å…¼å®¹ã€‚æ¯”å¦‚ä½ çš„ CUDA ç‰ˆæœ¬ä¸º 12.6ï¼Œ`torch` å®˜æ–¹ç›®å‰æ”¯æŒçš„æœ€é«˜ç‰ˆæœ¬ä¸º 12.4ï¼ˆ<12.6ï¼‰ï¼Œ`torch`ä»å¯ä»¥æ­£å¸¸ä½¿ç”¨CUDAã€‚\n\n<br>\n\n# âš¡ï¸ è¿è¡Œ Gradio Demo\n\n```bash\npython app.py\n```\n\nè¿è¡Œç¨‹åºå°†ç”Ÿæˆä¸€ä¸ªæœ¬åœ° Web é¡µé¢ï¼Œåœ¨é¡µé¢ä¸­å¯å®Œæˆè¯ä»¶ç…§çš„æ“ä½œä¸äº¤äº’ã€‚\n\n<img src="assets/harry.png" width=900>\n\n<br>\n\n# ğŸš€ Python æ¨ç†\n\næ ¸å¿ƒå‚æ•°ï¼š\n\n- `-i`: è¾“å…¥å›¾åƒè·¯å¾„\n- `-o`: ä¿å­˜å›¾åƒè·¯å¾„\n- `-t`: æ¨ç†ç±»å‹ï¼Œæœ‰idphotoã€human_mattingã€add_backgroundã€generate_layout_photoså¯é€‰\n- `--matting_model`: äººåƒæŠ å›¾æ¨¡å‹æƒé‡é€‰æ‹©\n- `--face_detect_model`: äººè„¸æ£€æµ‹æ¨¡å‹é€‰æ‹©\n\næ›´å¤šå‚æ•°å¯é€šè¿‡`python inference.py --help`æŸ¥çœ‹\n\n## 1. è¯ä»¶ç…§åˆ¶ä½œ\n\nè¾“å…¥ 1 å¼ ç…§ç‰‡ï¼Œè·å¾— 1 å¼ æ ‡å‡†è¯ä»¶ç…§å’Œ 1 å¼ é«˜æ¸…è¯ä»¶ç…§çš„ 4 é€šé“é€æ˜ png\n\n```python\npython inference.py -i demo/images/test0.jpg -o ./idphoto.png --height 413 --width 295\n```\n\n## 2. äººåƒæŠ å›¾\n\nè¾“å…¥ 1 å¼ ç…§ç‰‡ï¼Œè·å¾— 1å¼  4 é€šé“é€æ˜ png\n\n```python\npython inference.py -t human_matting -i demo/images/test0.jpg -o ./idphoto_matting.png --matting_model hivision_modnet\n```\n\n## 3. é€æ˜å›¾å¢åŠ åº•è‰²\n\nè¾“å…¥ 1 å¼  4 é€šé“é€æ˜ pngï¼Œè·å¾— 1 å¼ å¢åŠ äº†åº•è‰²çš„ 3é€šé“å›¾åƒ\n\n```python\npython inference.py -t add_background -i ./idphoto.png -o ./idphoto_ab.jpg  -c 4f83ce -k 30 -r 1\n```\n\n## 4. å¾—åˆ°å…­å¯¸æ’ç‰ˆç…§\n\nè¾“å…¥ 1 å¼  3 é€šé“ç…§ç‰‡ï¼Œè·å¾— 1 å¼ å…­å¯¸æ’ç‰ˆç…§\n\n```python\npython inference.py -t generate_layout_photos -i ./idphoto_ab.jpg -o ./idphoto_layout.jpg  --height 413 --width 295 -k 200\n```\n\n## 5. è¯ä»¶ç…§è£å‰ª\n\nè¾“å…¥ 1 å¼  4 é€šé“ç…§ç‰‡ï¼ˆæŠ å›¾å¥½çš„å›¾åƒï¼‰ï¼Œè·å¾— 1 å¼ æ ‡å‡†è¯ä»¶ç…§å’Œ 1 å¼ é«˜æ¸…è¯ä»¶ç…§çš„ 4 é€šé“é€æ˜ png\n\n```python\npython inference.py -t idphoto_crop -i ./idphoto_matting.png -o ./idphoto_crop.png --height 413 --width 295\n```\n\n\n<br>\n\n# âš¡ï¸ éƒ¨ç½² API æœåŠ¡\n\n## å¯åŠ¨åç«¯\n\n```\npython deploy_api.py\n```\n\n## è¯·æ±‚ API æœåŠ¡\n\nè¯¦ç»†è¯·æ±‚æ–¹å¼è¯·å‚è€ƒ [API æ–‡æ¡£](docs/api_CN.md)ï¼ŒåŒ…å«ä»¥ä¸‹è¯·æ±‚ç¤ºä¾‹ï¼š\n- [cURL](docs/api_CN.md#curl-è¯·æ±‚ç¤ºä¾‹)\n- [Python](docs/api_CN.md#python-è¯·æ±‚ç¤ºä¾‹)\n\n<br>\n\n# ğŸ³ Docker éƒ¨ç½²\n\n## 1. æ‹‰å–æˆ–æ„å»ºé•œåƒ\n\n> ä»¥ä¸‹æ–¹å¼ä¸‰é€‰ä¸€\n\n**æ–¹å¼ä¸€ï¼šæ‹‰å–æœ€æ–°é•œåƒï¼š**\n\n```bash\ndocker pull linzeyi/hivision_idphotos\n```\n\n**æ–¹å¼äºŒï¼šDockrfile ç›´æ¥æ„å»ºé•œåƒï¼š**\n\nåœ¨ç¡®ä¿å°†è‡³å°‘ä¸€ä¸ª[æŠ å›¾æ¨¡å‹æƒé‡æ–‡ä»¶](#3-ä¸‹è½½æƒé‡æ–‡ä»¶)æ”¾åˆ°`hivision/creator/weights`ä¸‹åï¼Œåœ¨é¡¹ç›®æ ¹ç›®å½•æ‰§è¡Œï¼š\n\n```bash\ndocker build -t linzeyi/hivision_idphotos .\n```\n\n**æ–¹å¼ä¸‰ï¼šDocker compose æ„å»ºï¼š**\n\nåœ¨ç¡®ä¿å°†è‡³å°‘ä¸€ä¸ª[æŠ å›¾æ¨¡å‹æƒé‡æ–‡ä»¶](#3-ä¸‹è½½æƒé‡æ–‡ä»¶)æ”¾åˆ°`hivision/creator/weights`ä¸‹åï¼Œåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰§è¡Œï¼š\n\n```bash\ndocker compose build\n```\n\n## 2. è¿è¡ŒæœåŠ¡\n\n**å¯åŠ¨ Gradio Demo æœåŠ¡**\n\nè¿è¡Œä¸‹é¢çš„å‘½ä»¤ï¼Œåœ¨ä½ çš„æœ¬åœ°è®¿é—® [http://127.0.0.1:7860](http://127.0.0.1:7860/) å³å¯ä½¿ç”¨ã€‚\n\n```bash\ndocker run -d -p 7860:7860 linzeyi/hivision_idphotos\n```\n\n**å¯åŠ¨ API åç«¯æœåŠ¡**\n\n```bash\ndocker run -d -p 8080:8080 linzeyi/hivision_idphotos python3 deploy_api.py\n```\n\n**ä¸¤ä¸ªæœåŠ¡åŒæ—¶å¯åŠ¨**\n\n```bash\ndocker compose up -d\n```\n\n## ç¯å¢ƒå˜é‡\n\næœ¬é¡¹ç›®æä¾›äº†ä¸€äº›é¢å¤–çš„é…ç½®é¡¹ï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡è¿›è¡Œè®¾ç½®ï¼š\n\n| ç¯å¢ƒå˜é‡ | ç±»å‹	| æè¿° | ç¤ºä¾‹ |\n|--|--|--|--|\n| FACE_PLUS_API_KEY	 | å¯é€‰	| è¿™æ˜¯ä½ åœ¨ Face++ æ§åˆ¶å°ç”³è¯·çš„ API å¯†é’¥	 | `7-fZStDJÂ·Â·Â·Â·` |\n| FACE_PLUS_API_SECRET	 | å¯é€‰	| Face++ APIå¯†é’¥å¯¹åº”çš„Secret | `VTee824EÂ·Â·Â·Â·` |\n| RUN_MODE | å¯é€‰ | è¿è¡Œæ¨¡å¼ï¼Œå¯é€‰å€¼ä¸º`beast`(é‡å…½æ¨¡å¼)ã€‚é‡å…½æ¨¡å¼ä¸‹äººè„¸æ£€æµ‹å’ŒæŠ å›¾æ¨¡å‹å°†ä¸é‡Šæ”¾å†…å­˜ï¼Œä»è€Œè·å¾—æ›´å¿«çš„äºŒæ¬¡æ¨ç†é€Ÿåº¦ã€‚å»ºè®®å†…å­˜16GBä»¥ä¸Šå°è¯•ã€‚ | `beast` |\n| DEFAULT_LANG | å¯é€‰ | Gradio Demoå¯åŠ¨æ—¶çš„é»˜è®¤è¯­è¨€| `en` |\n\ndockerä½¿ç”¨ç¯å¢ƒå˜é‡ç¤ºä¾‹ï¼š\n```bash\ndocker run  -d -p 7860:7860 \\n    -e FACE_PLUS_API_KEY=7-fZStDJÂ·Â·Â·Â· \\n    -e FACE_PLUS_API_SECRET=VTee824EÂ·Â·Â·Â· \\n    -e RUN_MODE=beast \\n    -e DEFAULT_LANG=en \\n    linzeyi/hivision_idphotos  \n```\n\n<br>\n\n# FAQ\n\n## 1. å¦‚ä½•ä¿®æ”¹é¢„è®¾å°ºå¯¸å’Œé¢œè‰²ï¼Ÿ\n\n- å°ºå¯¸ï¼šä¿®æ”¹[size_list_CN.csv](demo/assets/size_list_CN.csv)åå†æ¬¡è¿è¡Œ `app.py` å³å¯ï¼Œå…¶ä¸­ç¬¬ä¸€åˆ—ä¸ºå°ºå¯¸åï¼Œç¬¬äºŒåˆ—ä¸ºé«˜åº¦ï¼Œç¬¬ä¸‰åˆ—ä¸ºå®½åº¦ã€‚\n- é¢œè‰²ï¼šä¿®æ”¹[color_list_CN.csv](demo/assets/color_list_CN.csv)åå†æ¬¡è¿è¡Œ `app.py` å³å¯ï¼Œå…¶ä¸­ç¬¬ä¸€åˆ—ä¸ºé¢œè‰²åï¼Œç¬¬äºŒåˆ—ä¸ºHexå€¼ã€‚\n\n## 2. å¦‚ä½•ä¿®æ”¹æ°´å°å­—ä½“ï¼Ÿ\n\n1. å°†å­—ä½“æ–‡ä»¶æ”¾åˆ°`hivision/plugin/font`æ–‡ä»¶å¤¹ä¸‹\n2. ä¿®æ”¹`hivision/plugin/watermark.py`çš„`font_file`å‚æ•°å€¼ä¸ºå­—ä½“æ–‡ä»¶å\n\n## 3. å¦‚ä½•æ·»åŠ ç¤¾äº¤åª’ä½“æ¨¡æ¿ç…§ï¼Ÿ\n\n1. å°†æ¨¡æ¿å›¾ç‰‡æ”¾åˆ°`hivision/plugin/template/assets`æ–‡ä»¶å¤¹ä¸‹ã€‚æ¨¡æ¿å›¾ç‰‡æ˜¯ä¸€ä¸ª4é€šé“çš„é€æ˜pngã€‚\n2. åœ¨`hivision/plugin/template/assets/template_config.json`æ–‡ä»¶ä¸­æ·»åŠ æœ€æ–°çš„æ¨¡æ¿ä¿¡æ¯ï¼Œå…¶ä¸­`width`ä¸ºæ¨¡æ¿å›¾å®½åº¦(px)ï¼Œ`height`ä¸ºæ¨¡æ¿å›¾é«˜åº¦(px)ï¼Œ`anchor_points`ä¸ºæ¨¡æ¿ä¸­é€æ˜åŒºåŸŸçš„å››ä¸ªè§’çš„åæ ‡(px)ï¼›`rotation`ä¸ºé€æ˜åŒºåŸŸç›¸å¯¹äºå‚ç›´æ–¹å‘çš„æ—‹è½¬è§’åº¦ï¼Œ>0ä¸ºé€†æ—¶é’ˆï¼Œ<0ä¸ºé¡ºæ—¶é’ˆã€‚\n3. åœ¨`demo/processor.py`çš„`_generate_image_template`å‡½æ•°ä¸­çš„`TEMPLATE_NAME_LIST`å˜é‡æ·»åŠ æœ€æ–°çš„æ¨¡æ¿å\n\n<img src="assets/social_template.png" width="500">\n\n## 4. å¦‚ä½•ä¿®æ”¹Gradio Demoçš„é¡¶éƒ¨å¯¼èˆªæ ï¼Ÿ\n\n- ä¿®æ”¹`demo/assets/title.md`\n\n## 5. å¦‚ä½•æ·»åŠ /ä¿®æ”¹ã€Œæ‰“å°æ’ç‰ˆã€ä¸­çš„å°ºå¯¸ï¼Ÿ\n\n- ä¿®æ”¹`demo/locales.py`ä¸­çš„`print_switch`å­—å…¸ï¼Œæ·»åŠ /ä¿®æ”¹æ–°çš„å°ºå¯¸åç§°å’Œå°ºå¯¸å‚æ•°ï¼Œç„¶åé‡æ–°è¿è¡Œ`python app.py`\n\n<br>\n\n# ğŸ“§ è”ç³»æˆ‘ä»¬\n\nå¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·å‘é‚®ä»¶è‡³ zeyi.lin@swanhub.co\n\n<br>\n\n# ğŸ™ æ„Ÿè°¢æ”¯æŒ\n\n[![Stargazers repo roster for @Zeyi-Lin/HivisionIDPhotos](https://reporoster.com/stars/Zeyi-Lin/HivisionIDPhotos)](https://github.com/Zeyi-Lin/HivisionIDPhotos/stargazers)\n\n[![Forkers repo roster for @Zeyi-Lin/HivisionIDPhotos](https://reporoster.com/forks/Zeyi-Lin/HivisionIDPhotos)](https://github.com/Zeyi-Lin/HivisionIDPhotos/network/members)\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Zeyi-Lin/HivisionIDPhotos&type=Date)](https://star-history.com/#Zeyi-Lin/HivisionIDPhotos&Date)\n\nè´¡çŒ®è€…ä»¬ï¼š\n\n<a href="https://github.com/Zeyi-Lin/HivisionIDPhotos/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=Zeyi-Lin/HivisionIDPhotos" />\n</a>\n\n[Zeyi-Lin](https://github.com/Zeyi-Lin)ã€[SAKURA-CAT](https://github.com/SAKURA-CAT)ã€[Feudalman](https://github.com/Feudalman)ã€[swpfY](https://github.com/swpfY)ã€[Kaikaikaifang](https://github.com/Kaikaikaifang)ã€[ShaohonChen](https://github.com/ShaohonChen)ã€[KashiwaByte](https://github.com/KashiwaByte)\n\n<br>\n\n# ğŸ“œ Lincese\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE).\n\n<br>\n\n# ğŸ“š å¼•ç”¨\n\nå¦‚æœæ‚¨åœ¨ç ”ç©¶æˆ–é¡¹ç›®ä¸­ä½¿ç”¨äº†HivisionIDPhotosï¼Œè¯·è€ƒè™‘å¼•ç”¨æˆ‘ä»¬çš„å·¥ä½œã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹BibTeXæ¡ç›®ï¼š\n\n```bibtex\n@misc{hivisionidphotos,\n      title={{HivisionIDPhotos: A Lightweight and Efficient AI ID Photos Tool}},\n      author={Zeyi Lin and SwanLab Team},\n      year={2024},\n      publisher={GitHub},\n      url = {\url{https://github.com/Zeyi-Lin/HivisionIDPhotos}},\n}\n```\n\n\n\n\n[github-stars-shield]: https://img.shields.io/github/stars/zeyi-lin/hivisionidphotos?color=ffcb47&labelColor=black&style=flat-square\n[github-stars-link]: https://github.com/zeyi-lin/hivisionidphotos/stargazers\n\n[swanhub-demo-shield]: https://swanhub.co/git/repo/SwanHub%2FAuto-README/file/preview?ref=main&path=swanhub.svg\n[swanhub-demo-link]: https://swanhub.co/ZeYiLin/HivisionIDPhotos/demo\n\n[spaces-shield]: https://img.shields.io/badge/ğŸ¤—-Open%20in%20Spaces-blue\n[spaces-link]: https://huggingface.co/spaces/TheEeeeLin/HivisionIDPhotos\n\n<!-- å¾®ä¿¡ç¾¤é“¾æ¥ -->\n[wechat-shield]: https://img.shields.io/badge/WeChat-å¾®ä¿¡-4cb55e\n[wechat-link]: https://docs.qq.com/doc/DUkpBdk90eWZFS2JW\n\n<!-- Github Release -->\n[release-shield]: https://img.shields.io/github/v/release/zeyi-lin/hivisionidphotos?color=369eff&labelColor=black&logo=github&style=flat-square\n[release-link]: https://github.com/zeyi-lin/hivisionidphotos/releases\n\n[license-shield]: https://img.shields.io/badge/license-apache%202.0-white?labelColor=black&style=flat-square\n[license-link]: https://github.com/Zeyi-Lin/HivisionIDPhotos/blob/master/LICENSE\n\n[github-issues-shield]: https://img.shields.io/github/issues/zeyi-lin/hivisionidphotos?color=ff80eb&labelColor=black&style=flat-square\n[github-issues-link]: https://github.com/zeyi-lin/hivisionidphotos/issues\n\n[dockerhub-shield]: https://img.shields.io/docker/v/linzeyi/hivision_idphotos?color=369eff&label=docker&labelColor=black&logoColor=white&style=flat-square\n[dockerhub-link]: https://hub.docker.com/r/linzeyi/hivision_idphotos/tags\n\n[trendshift-shield]: https://trendshift.io/api/badge/repositories/11622\n[trendshift-link]: https://trendshift.io/repositories/11622\n\n[hellogithub-shield]: https://abroad.hellogithub.com/v1/widgets/recommend.svg?rid=8ea1457289fb4062ba661e5299e733d6&claim_uid=Oh5UaGjfrblg0yZ\n[hellogithub-link]: https://hellogithub.com/repository/8ea1457289fb4062ba661e5299e733d6\n\n[github-contributors-shield]: https://img.shields.io/github/contributors/zeyi-lin/hivisionidphotos?color=c4f042&labelColor=black&style=flat-square\n[github-contributors-link]: https://github.com/zeyi-lin/hivisionidphotos/graphs/contributors\n\n[github-forks-shield]: https://img.shields.io/github/forks/zeyi-lin/hivisionidphotos?color=8ae8ff&labelColor=black&style=flat-square\n[github-forks-link]: https://github.com/zeyi-lin/hivisionidphotos/network/members\n\n[modelscope-shield]: https://img.shields.io/badge/Demo_on_ModelScope-purple?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjIzIiBoZWlnaHQ9IjIwMCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KCiA8Zz4KICA8dGl0bGU+TGF5ZXIgMTwvdGl0bGU+CiAgPHBhdGggaWQ9InN2Z18xNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTAsODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTUiIGZpbGw9IiM2MjRhZmYiIGQ9Im05OS4xNCwxMTUuNDlsMjUuNjUsMGwwLDI1LjY1bC0yNS42NSwwbDAsLTI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTYiIGZpbGw9IiM2MjRhZmYiIGQ9Im0xNzYuMDksMTQxLjE0bC0yNS42NDk5OSwwbDAsMjIuMTlsNDcuODQsMGwwLC00Ny44NGwtMjIuMTksMGwwLDI1LjY1eiIvPgogIDxwYXRoIGlkPSJzdmdfMTciIGZpbGw9IiMzNmNmZDEiIGQ9Im0xMjQuNzksODkuODRsMjUuNjUsMGwwLDI1LjY0OTk5bC0yNS42NSwwbDAsLTI1LjY0OTk5eiIvPgogIDxwYXRoIGlkPSJzdmdfMTgiIGZpbGw9IiMzNmNmZDEiIGQ9Im0wLDY0LjE5bDI1LjY1LDBsMCwyNS42NWwtMjUuNjUsMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzE5IiBmaWxsPSIjNjI0YWZmIiBkPSJtMTk4LjI4LDg5Ljg0bDI1LjY0OTk5LDBsMCwyNS42NDk5OWwtMjUuNjQ5OTksMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIwIiBmaWxsPSIjMzZjZmQxIiBkPSJtMTk4LjI4LDY0LjE5bDI1LjY0OTk5LDBsMCwyNS42NWwtMjUuNjQ5OTksMGwwLC0yNS42NXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIxIiBmaWxsPSIjNjI0YWZmIiBkPSJtMTUwLjQ0LDQybDAsMjIuMTlsMjUuNjQ5OTksMGwwLDI1LjY1bDIyLjE5LDBsMCwtNDcuODRsLTQ3Ljg0LDB6Ii8+CiAgPHBhdGggaWQ9InN2Z18yMiIgZmlsbD0iIzM2Y2ZkMSIgZD0ibTczLjQ5LDg5Ljg0bDI1LjY1LDBsMCwyNS42NDk5OWwtMjUuNjUsMGwwLC0yNS42NDk5OXoiLz4KICA8cGF0aCBpZD0ic3ZnXzIzIiBmaWxsPSIjNjI0YWZmIiBkPSJtNDcuODQsNjQuMTlsMjUuNjUsMGwwLC0yMi4xOWwtNDcuODQsMGwwLDQ3Ljg0bDIyLjE5LDBsMCwtMjUuNjV6Ii8+CiAgPHBhdGggaWQ9InN2Z18yNCIgZmlsbD0iIzYyNGFmZiIgZD0ibTQ3Ljg0LDExNS40OWwtMjIuMTksMGwwLDQ3Ljg0bDQ3Ljg0LDBsMCwtMjIuMTlsLTI1LjY1LDBsMCwtMjUuNjV6Ii8+CiA8L2c+Cjwvc3ZnPg==&labelColor=white\n[modelscope-link]: https://modelscope.cn/studios/SwanLab/HivisionIDPhotos\n\n[modelers-shield]: https://img.shields.io/badge/Demo_on_Modelers-c42a2a?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxMjQiIGhlaWdodD0iNjQiIHZpZXdCb3g9IjAgMCAxMjQgNjQiIGZpbGw9Im5vbmUiPgo8cGF0aCBkPSJNNDIuNzc4MyAwSDI2LjU5NzdWMTUuNzc4N0g0Mi43NzgzVjBaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0xNi41MDg4IDQuMTc5MkgwLjMyODEyNVYxOS45NTc5SDE2LjUwODhWNC4xNzkyWiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTIzLjk1MiA0LjE3OTJIMTA3Ljc3MVYxOS45NTc5SDEyMy45NTJWNC4xNzkyWiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTYuNTA4OCA0NS40NjE5SDAuMzI4MTI1VjYxLjI0MDZIMTYuNTA4OFY0NS40NjE5WiIgZmlsbD0iIzI0NDk5QyIvPgo8cGF0aCBkPSJNMTIzLjk1MiA0NS40NjE5SDEwNy43NzFWNjEuMjQwNkgxMjMuOTUyVjQ1LjQ2MTlaIiBmaWxsPSIjMjQ0OTlDIi8+CjxwYXRoIGQ9Ik0zMi43MDggMTUuNzc4OEgxNi41MjczVjMxLjU1NzVIMzIuNzA4VjE1Ljc3ODhaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik01Mi44NDg2IDE1Ljc3ODhIMzYuNjY4VjMxLjU1NzVINTIuODQ4NlYxNS43Nzg4WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNOTcuNzIzNyAwSDgxLjU0M1YxNS43Nzg3SDk3LjcyMzdWMFoiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTg3LjY1MzQgMTUuNzc4OEg3MS40NzI3VjMxLjU1NzVIODcuNjUzNFYxNS43Nzg4WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNMTA3Ljc5NCAxNS43Nzg4SDkxLjYxMzNWMzEuNTU3NUgxMDcuNzk0VjE1Ljc3ODhaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0yNC42NzQ4IDMxLjU1NzZIOC40OTQxNFY0Ny4zMzYzSDI0LjY3NDhWMzEuNTU3NloiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTYwLjg3OTkgMzEuNTU3Nkg0NC42OTkyVjQ3LjMzNjNINjAuODc5OVYzMS41NTc2WiIgZmlsbD0iI0RFMDQyOSIvPgo8cGF0aCBkPSJNNzkuNjIwMSAzMS41NTc2SDYzLjQzOTVWNDcuMzM2M0g3OS42MjAxVjMxLjU1NzZaIiBmaWxsPSIjREUwNDI5Ii8+CjxwYXRoIGQ9Ik0xMTUuODI1IDMxLjU1NzZIOTkuNjQ0NVY0Ny4zMzYzSDExNS44MjVWMzEuNTU3NloiIGZpbGw9IiNERTA0MjkiLz4KPHBhdGggZD0iTTcwLjI1NDkgNDcuMzM1OUg1NC4wNzQyVjYzLjExNDdINzAuMjU0OVY0Ny4zMzU5WiIgZmlsbD0iI0RFMDQyOSIvPgo8L3N2Zz4=&labelColor=white\n[modelers-link]: https://modelers.cn/spaces/SwanLab/HivisionIDPhotos\n\n[compshare-shield]: https://www-s.ucloud.cn/2025/02/dbef8b07ea3d316006d9c22765c3cd53_1740104342584.svg\n[compshare-link]: https://www.compshare.cn/images-detail?ImageID=compshareImage-17jacgm4ju16&ytag=HG_GPU_HivisionIDPhotos\n\n<!-- ç¤¾åŒºé¡¹ç›®é“¾æ¥ -->\n[community-hivision-comfyui]: https://github.com/AIFSH/HivisionIDPhotos-ComfyUI\n[community-hivision-wechat]: https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp\n[community-hivision-uniapp]: https://github.com/soulerror/HivisionIDPhotos-Uniapp\n[community-hivision-cpp]: https://github.com/zjkhahah/HivisionIDPhotos-cpp\n[community-hivision-windows-gui]: https://github.com/zhaoyun0071/HivisionIDPhotos-windows-GUI\n[community-hivision-nas]: https://github.com/ONG-Leo/HivisionIDPhotos-NAS', '{"language":"Python","stars":20299,"forks":2288,"watchers":20299,"open_issues":91,"topics":["cnn","demo","docker","face-recognition","fastapi","gradio","idphoto","machine-learning","matting","mtcnn","tools","unet"],"default_branch":"master","size_kb":48948,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:SwanHubX:SwanLab","source_url":"https://github.com/SwanHubX/SwanLab"},{"type":"has_code","target_id":"github:zjkhahah:HivisionIDPhotos-cpp","source_url":"https://github.com/zjkhahah/HivisionIDPhotos-cpp"},{"type":"has_code","target_id":"github:AIFSH:HivisionIDPhotos-ComfyUI\">","source_url":"https://github.com/AIFSH/HivisionIDPhotos-ComfyUI\">"},{"type":"has_code","target_id":"github:no1xuan:HivisionIDPhotos-wechat-weapp\">","source_url":"https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp\">"},{"type":"has_code","target_id":"github:jkm199:HivisionIDPhotos-web","source_url":"https://github.com/jkm199/HivisionIDPhotos-web"},{"type":"has_code","target_id":"github:soulerror:HivisionIDPhotos-Uniapp\">","source_url":"https://github.com/soulerror/HivisionIDPhotos-Uniapp\">"},{"type":"has_code","target_id":"github:jkm199:HivisionIDPhotos-web\">","source_url":"https://github.com/jkm199/HivisionIDPhotos-web\">"},{"type":"has_code","target_id":"github:zjkhahah:HivisionIDPhotos-cpp","source_url":"https://github.com/zjkhahah/HivisionIDPhotos-cpp"},{"type":"has_code","target_id":"github:wmlcjj:ai-idphoto","source_url":"https://github.com/wmlcjj/ai-idphoto"},{"type":"has_code","target_id":"github:no1xuan:HivisionIDPhotos-wechat-weapp","source_url":"https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp"},{"type":"has_code","target_id":"github:jkm199:HivisionIDPhotos-uniapp-WeChat-gpto1","source_url":"https://github.com/jkm199/HivisionIDPhotos-uniapp-WeChat-gpto1"},{"type":"has_code","target_id":"github:zhaoyun0071:HivisionIDPhotos-windows-GUI","source_url":"https://github.com/zhaoyun0071/HivisionIDPhotos-windows-GUI"},{"type":"has_code","target_id":"github:ONG-Leo:HivisionIDPhotos-NAS","source_url":"https://github.com/ONG-Leo/HivisionIDPhotos-NAS"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos.git","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos.git"},{"type":"has_code","target_id":"github:ZHKKKe:MODNet","source_url":"https://github.com/ZHKKKe/MODNet"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos"},{"type":"has_code","target_id":"github:ZhengPeng7:BiRefNet","source_url":"https://github.com/ZhengPeng7/BiRefNet"},{"type":"has_code","target_id":"github:ZhengPeng7:BiRefNet","source_url":"https://github.com/ZhengPeng7/BiRefNet"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos}},","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos}},"},{"type":"has_code","target_id":"github:zeyi-lin:hivisionidphotos","source_url":"https://github.com/zeyi-lin/hivisionidphotos"},{"type":"has_code","target_id":"github:zeyi-lin:hivisionidphotos","source_url":"https://github.com/zeyi-lin/hivisionidphotos"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos"},{"type":"has_code","target_id":"github:zeyi-lin:hivisionidphotos","source_url":"https://github.com/zeyi-lin/hivisionidphotos"},{"type":"has_code","target_id":"github:zeyi-lin:hivisionidphotos","source_url":"https://github.com/zeyi-lin/hivisionidphotos"},{"type":"has_code","target_id":"github:zeyi-lin:hivisionidphotos","source_url":"https://github.com/zeyi-lin/hivisionidphotos"},{"type":"has_code","target_id":"github:AIFSH:HivisionIDPhotos-ComfyUI","source_url":"https://github.com/AIFSH/HivisionIDPhotos-ComfyUI"},{"type":"has_code","target_id":"github:no1xuan:HivisionIDPhotos-wechat-weapp","source_url":"https://github.com/no1xuan/HivisionIDPhotos-wechat-weapp"},{"type":"has_code","target_id":"github:soulerror:HivisionIDPhotos-Uniapp","source_url":"https://github.com/soulerror/HivisionIDPhotos-Uniapp"},{"type":"has_code","target_id":"github:zjkhahah:HivisionIDPhotos-cpp","source_url":"https://github.com/zjkhahah/HivisionIDPhotos-cpp"},{"type":"has_code","target_id":"github:zhaoyun0071:HivisionIDPhotos-windows-GUI","source_url":"https://github.com/zhaoyun0071/HivisionIDPhotos-windows-GUI"},{"type":"has_code","target_id":"github:ONG-Leo:HivisionIDPhotos-NAS","source_url":"https://github.com/ONG-Leo/HivisionIDPhotos-NAS"}]', NULL, 'Apache-2.0', 'approved', 80, '21a6274ee7cb01e58c4db02a7a7397e7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Zeyi-Lin-HivisionIDPhotos from https://github.com/Zeyi-Lin.png
Image converted to WebP: data/images/github-Zeyi-Lin-HivisionIDPhotos.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-onnx-onnx', 'github--onnx--onnx', 'onnx', 'onnx', '<!-- Copyright (c) ONNX Project Contributors SPDX-License-Identifier: Apache-2.0 --> <p align="center"><img width="40%" src="https://github.com/onnx/onnx/raw/main/docs/onnx-horizontal-color.png" /></p> Open Neural Network Exchange (ONNX) is an open ecosystem that empowers AI developers to choose the right tools as their project evolves. ONNX provides an open source format for AI models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definit...', '["deep-learning","deep-neural-networks","dnn","keras","machine-learning","ml","neural-network","onnx","pytorch","scikit-learn","tensorflow","python"]', 'other', 19983, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/onnx/onnx","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<!--\nCopyright (c) ONNX Project Contributors\n\nSPDX-License-Identifier: Apache-2.0\n-->\n\n<p align="center"><img width="40%" src="https://github.com/onnx/onnx/raw/main/docs/onnx-horizontal-color.png" /></p>\n\n[![PyPI - Version](https://img.shields.io/pypi/v/onnx.svg)](https://pypi.org/project/onnx)\n[![CI](https://github.com/onnx/onnx/actions/workflows/main.yml/badge.svg)](https://github.com/onnx/onnx/actions/workflows/main.yml)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/3313/badge)](https://bestpractices.coreinfrastructure.org/projects/3313)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/onnx/onnx/badge)](https://api.securityscorecards.dev/projects/github.com/onnx/onnx)\n[![REUSE compliant](https://api.reuse.software/badge/github.com/onnx/onnx)](https://api.reuse.software/info/github.com/onnx/onnx)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![abi3 compatible](https://img.shields.io/badge/abi3-compatible-brightgreen)](https://docs.python.org/3/c-api/stable.html)\n\n[Open Neural Network Exchange (ONNX)](https://onnx.ai) is an open ecosystem that empowers AI developers\nto choose the right tools as their project evolves. ONNX provides an open source format for AI models, both deep learning and traditional ML. It defines an extensible computation graph model, as well as definitions of built-in operators and standard\ndata types. Currently we focus on the capabilities needed for inferencing (scoring).\n\nONNX is [widely supported](http://onnx.ai/supported-tools) and can be found in many frameworks, tools, and hardware. Enabling interoperability between different frameworks and streamlining the path from research to production helps increase the speed of innovation in the AI community. We invite the community to join us and further evolve ONNX.\n\n\n# Use ONNX\n\n* [Documentation of ONNX Python Package](https://onnx.ai/onnx/)\n* [Tutorials for creating ONNX models](https://github.com/onnx/tutorials)\n* [Pre-trained ONNX models](https://github.com/onnx/models)\n\n# Learn about the ONNX spec\n\n* [Overview](https://github.com/onnx/onnx/blob/main/docs/Overview.md)\n* [ONNX intermediate representation spec](https://github.com/onnx/onnx/blob/main/docs/IR.md)\n* [Versioning principles of the spec](https://github.com/onnx/onnx/blob/main/docs/Versioning.md)\n* [Operators documentation](https://github.com/onnx/onnx/blob/main/docs/Operators.md)\n* [Operators documentation](https://onnx.ai/onnx/operators/index.html) (latest release)\n* [Python API Overview](https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md)\n\n# Programming utilities for working with ONNX Graphs\n\n* [Shape and Type Inference](https://github.com/onnx/onnx/blob/main/docs/ShapeInference.md)\n* [Graph Optimization](https://github.com/onnx/optimizer)\n* [Opset Version Conversion](https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/version_converter.md)\n\n# Contribute\n\nONNX is a community project and the open governance model is described [here](https://github.com/onnx/onnx/blob/main/community/readme.md). We encourage you to join the effort and contribute feedback, ideas, and code. You can participate in the [Special Interest Groups](https://github.com/onnx/onnx/blob/main/community/sigs.md) and [Working Groups](https://github.com/onnx/onnx/blob/main/community/working-groups.md) to shape the future of ONNX.\n\nCheck out our [contribution guide](https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md) to get started.\n\nIf you think some operator should be added to ONNX specification, please read\n[this document](https://github.com/onnx/onnx/blob/main/docs/AddNewOp.md).\n\n# Community meetings\n\nThe schedules of the regular meetings of the Steering Committee, the working groups and the SIGs can be found [here](https://onnx.ai/calendar)\n\nCommunity Meetups are held at least once a year. Content from previous community meetups are at:\n\n* 2020.04.09 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14091402/LF+AI+Day+-ONNX+Community+Virtual+Meetup+-+Silicon+Valley+-+2020+April+9>\n* 2020.10.14 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14092138/LF+AI+Day+-+ONNX+Community+Workshop+-+2020+October+14>\n* 2021.03.24 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14092424/Instructions+for+Event+Hosts+-+LF+AI+Data+Day+-+ONNX+Virtual+Community+Meetup+-+March+2021>\n* 2021.10.21 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14093194/LF+AI+Data+Day+ONNX+Community+Virtual+Meetup+-+October+2021>\n* 2022.06.24 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14093969/ONNX+Community+Day+-+2022+June+24>\n* 2023.06.28 <https://lf-aidata.atlassian.net/wiki/spaces/DL/pages/14094507/ONNX+Community+Day+2023+-+June+28>\n\n# Discuss\n\nWe encourage you to open [Issues](https://github.com/onnx/onnx/issues), or use [Slack](https://lfaifoundation.slack.com/) (If you have not joined yet, please use this [link](https://join.slack.com/t/lfaifoundation/shared_invite/zt-o65errpw-gMTbwNr7FnNbVXNVFkmyNA) to join the group) for more real-time discussion.\n\n# Follow Us\n\nStay up to date with the latest ONNX news. [[Facebook](https://www.facebook.com/onnxai/)] [[Twitter/X](https://twitter.com/onnxai)]\n\n# Roadmap\n\nA roadmap process takes place every year. More details can be found [here](https://github.com/onnx/steering-committee/tree/main/roadmap)\n\n# Installation\n\nONNX released packages are published in PyPi.\n\n```sh\npip install onnx # or pip install onnx[reference] for optional reference implementation dependencies\n```\n\n[ONNX weekly packages](https://pypi.org/project/onnx-weekly/) are published in PyPI to enable experimentation and early testing.\n\nDetailed install instructions, including Common Build Options and Common Errors can be found [here](https://github.com/onnx/onnx/blob/main/INSTALL.md)\n\n# Python ABI3 Compatibility\n\nThis package provides [abi3](https://docs.python.org/3/c-api/stable.html)-compatible wheels, allowing a single binary wheel to work across multiple Python versions (from 3.12 onwards).\n\n\n# Testing\n\nONNX uses [pytest](https://docs.pytest.org) as test driver. In order to run tests, you will first need to install `pytest`:\n\n```sh\npip install pytest\n```\n\nAfter installing pytest, use the following command to run tests.\n\n```sh\npytest\n```\n\n# Development\n\nCheck out the [contributor guide](https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md) for instructions.\n\n# Reproducible Builds (Linux)\n\nThis project provides reproducible builds for Linux.\n\nA *reproducible build* means that the same source code will always produce identical binary outputs, no matter who builds it or where it is built.\n\nTo achieve this, we use the [`SOURCE_DATE_EPOCH`](https://reproducible-builds.org/docs/source-date-epoch/) standard. This ensures that build timestamps and other time-dependent information are fixed, making the output bit-for-bit identical across different environments.\n\n### Why this matters\n- **Transparency**: Anyone can verify that the distributed binaries were created from the published source code.\n- **Security**: Prevents tampering or hidden changes in the build process.\n- **Trust**: Users can be confident that the binaries they download are exactly what the maintainers intended.\n\nIf you prefer, you can use the prebuilt reproducible binaries instead of building from source yourself.\n\n# License\n\n[Apache License v2.0](LICENSE)\n\n# Trademark\nCheckout [https://trademarks.justia.com](https://trademarks.justia.com/877/25/onnx-87725026.html) for the trademark.\n\n[General rules of the Linux Foundation on Trademark usage](https://www.linuxfoundation.org/legal/trademark-usage)\n\n# Code of Conduct\n\n[ONNX Open Source Code of Conduct](https://onnx.ai/codeofconduct.html)\n', '{"language":"Python","stars":19983,"forks":3837,"watchers":19983,"open_issues":285,"topics":["deep-learning","deep-neural-networks","dnn","keras","machine-learning","ml","neural-network","onnx","pytorch","scikit-learn","tensorflow"],"default_branch":"main","size_kb":44847,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:astral-sh:ruff","source_url":"https://github.com/astral-sh/ruff"},{"type":"has_code","target_id":"github:onnx:tutorials","source_url":"https://github.com/onnx/tutorials"},{"type":"has_code","target_id":"github:onnx:models","source_url":"https://github.com/onnx/models"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:optimizer","source_url":"https://github.com/onnx/optimizer"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:steering-committee","source_url":"https://github.com/onnx/steering-committee"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"},{"type":"has_code","target_id":"github:onnx:onnx","source_url":"https://github.com/onnx/onnx"}]', NULL, 'Apache-2.0', 'approved', 65, '189b13342e81ca36f0232e7dc4caeb65', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-onnx-onnx from https://github.com/onnx.png
Image converted to WebP: data/images/github-onnx-onnx.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-EthicalML-awesome-production-machine-learning', 'github--ethicalml--awesome-production-machine-learning', 'awesome-production-machine-learning', 'EthicalML', 'This repository contains a curated list of awesome open source libraries that will help you deploy, monitor, version, scale, and secure your production machine learning ğŸš€ You can keep up to date by watching this github repo to get a summary of the new production ML libraries added every month via releases ğŸ¤© Additionally, we provide a search toolkit that helps you quickly navigate through the toolchain. | | | | |-|-|-| | ğŸ”§ AutoML | ğŸ§® Computation & Communication Optimisation | ğŸ·ï¸ Data Anno...', '["awesome","awesome-list","data-mining","deep-learning","explainability","interpretability","large-scale-machine-learning","large-scale-ml","machine-learning","machine-learning-operations","ml-operations","ml-ops","mlops","privacy-preserving","privacy-preserving-machine-learning","privacy-preserving-ml","production-machine-learning","production-ml","responsible-ai"]', 'other', 19709, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/EthicalML/awesome-production-machine-learning","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)\n[![X](https://img.shields.io/badge/X-%23000000?logo=X&logoColor=white)](https://twitter.com/EthicalML)\n\n# Awesome Production Machine Learning\n\nThis repository contains a curated list of awesome open source libraries that will help you deploy, monitor, version, scale, and secure your production machine learning ğŸš€\n\nYou can keep up to date by watching this github repo to get a summary of the new production ML libraries added every month [via releases](https://github.com/EthicalML/awesome-production-machine-learning/releases) ğŸ¤©\n\nAdditionally, we provide a [search toolkit](https://huggingface.co/spaces/zhiminy/Awesome-Production-Machine-Learning-Search) that helps you quickly navigate through the toolchain.\n\n## Quick links to sections on this page\n\n| | | |\n|-|-|-|\n| [ğŸ”§ AutoML](#automl) | [ğŸ§® Computation & Communication Optimisation](#computation-and-communication-optimisation) | [ğŸ·ï¸ Data Annotation & Synthesis](#data-annotation-and-synthesis) |\n| [ğŸ§µ Data Pipeline](#data-pipeline) | [ğŸ““ Data Science Notebook](#data-science-notebook) | [ğŸ’¾ Data Storage Optimisation](#data-storage-optimisation) |\n| [ğŸ’¸ Data Stream Processing](#data-stream-processing) | [ğŸ’ª Deployment & Serving](#deployment-and-serving) | [ğŸ“ˆ Evaluation & Monitoring](#evaluation-and-monitoring) |\n| [ğŸ” Explainability & Fairness](#explainability-and-fairness) | [ğŸ Feature Store](#feature-store) | [ğŸ”´ Industry-strength Anomaly Detection](#industry-strength-anomaly-detection) |\n| [ğŸ‘ï¸ Industry-strength Computer Vision](#industry-strength-computer-vision) | [ğŸ”¥ Industry-strength Information Retrieval](#industry-strength-information-retrieval) | [ğŸ”  Industry-strength Natural Language Processing](#industry-strength-nlp) |\n| [ğŸ™Œ Industry-strength Recommender System](#industry-strength-recommender-system) | [ğŸ• Industry-strength Reinforcement Learning](#industry-strength-reinforcement-learning) | [ğŸ“Š Industry-strength Visualisation](#industry-strength-visualisation) |\n| [ğŸ“… Metadata Management](#metadata-management) | [ğŸ“œ Model, Data & Experiment Management](#model-data-and-experiment-management) | [ğŸ”© Model Storage Optimisation](#model-storage-optimisation) |\n| [ğŸ Model Training & Orchestration](#model-training-and-orchestration) | [ğŸ” Privacy & Safety](#privacy-and-safety) |\n\n## Contributing to the list\n\nPlease review our [CONTRIBUTING.md](https://github.com/EthicalML/awesome-production-machine-learning/blob/master/CONTRIBUTING.md) requirements when submitting a PR to help us keep the list clean and up-to-date - thank you to the community for supporting its steady growth ğŸš€\n\n<picture>\n  <source\n    media="(prefers-color-scheme: dark)"\n    srcset="\n      https://api.star-history.com/svg?repos=EthicalML/awesome-production-machine-learning&type=Date&theme=dark\n    "\n  />\n  <source\n    media="(prefers-color-scheme: light)"\n    srcset="\n      https://api.star-history.com/svg?repos=EthicalML/awesome-production-machine-learning&type=Date\n    "\n  />\n  <img\n    alt="Star History Chart"\n    src="https://api.star-history.com/svg?repos=EthicalML/awesome-production-machine-learning&type=Date"\n  />\n</picture>\n\n## 10 Min Video Overview\n\n<table>\n  <tr>\n    <td width="30%">\n        This <a href="https://www.youtube.com/watch?v=Ynb6X0KZKxY">10 minute video</a> provides an overview of the motivations for machine learning operations as well as a high level overview on some of the tools in this repo. This <a href="https://www.youtube.com/watch?v=NycftytgPnk">newer video</a> covers the an updated 2024 version of the state of MLOps.\n    </td>\n    <td width="70%">\n        <a href="https://www.youtube.com/watch?v=Ynb6X0KZKxY"><img src="images/video.png"></a>\n    </td>\n  </tr>\n</table>\n\n## Want to receive recurrent updates on this repo and other advancements?\n\n<table>\n  <tr>\n    <td width="30%">\n         You can join the <a href="https://ethical.institute/mle.html">Machine Learning Engineer</a> newsletter. Join over 70,000 ML professionals and enthusiasts who receive weekly curated articles & tutorials on production Machine Learning.\n    </td>\n    <td width="70%">\n        <a href="https://ethical.institute/mle.html"><img src="images/mleng.png"></a>\n    </td>\n  </tr>\n  <tr>\n    <td width="30%">\n         Also check out the <a href="https://github.com/EthicalML/awesome-production-genai/">Awesome Production GenAI</a> List, where we aim to map a curated list of awesome open source libraries to deploy, monitor, version and scale your generative artificial intelligence applications and systems.\n    </td>\n    <td width="70%">\n        <a href="https://github.com/EthicalML/awesome-production-genai/"><img src="images/list.jpg"></a>\n    </td>\n  </tr>\n</table>\n\n# Main Content\n\n## AutoML\n* [AutoGluon](https://github.com/autogluon/autogluon) ![](https://img.shields.io/github/stars/autogluon/autogluon.svg?cacheSeconds=86400) - Automated feature, model, and hyperparameter selection for tabular, image, and text data on top of popular machine learning libraries (Scikit-Learn, LightGBM, CatBoost, PyTorch, MXNet).\n* [Autokeras](https://github.com/keras-team/autokeras) ![](https://img.shields.io/github/stars/keras-team/autokeras.svg?cacheSeconds=86400) - AutoML library for Keras based on ["Auto-Keras: Efficient Neural Architecture Search with Network Morphism"](https://arxiv.org/abs/1806.10282).\n* [auto-sklearn](https://github.com/automl/auto-sklearn) ![](https://img.shields.io/github/stars/automl/auto-sklearn.svg?cacheSeconds=86400) - Framework to automate algorithm and hyperparameter tuning for sklearn.\n* [Ax](https://github.com/facebook/Ax) ![](https://img.shields.io/github/stars/facebook/Ax.svg?cacheSeconds=86400) - Ax is an accessible, general-purpose platform for understanding, managing, deploying, and automating adaptive experiments.\n* [BoTorch](https://github.com/meta-pytorch/botorch) ![](https://img.shields.io/github/stars/meta-pytorch/botorch.svg?cacheSeconds=86400) - BoTorch is a library for Bayesian Optimization built on PyTorch.\n* [EvalML](https://github.com/alteryx/evalml) ![](https://img.shields.io/github/stars/alteryx/evalml.svg?cacheSeconds=86400) - EvalML is an AutoML library which builds, optimizes, and evaluates machine learning pipelines using domain-specific objective functions.\n* [Feature Engine](https://github.com/feature-engine/feature_engine) ![](https://img.shields.io/github/stars/feature-engine/feature_engine.svg?cacheSeconds=86400) - Feature-engine is a Python library that contains several transformers to engineer features for use in machine learning models.\n* [Featuretools](https://github.com/alteryx/featuretools) ![](https://img.shields.io/github/stars/alteryx/featuretools.svg?cacheSeconds=86400) - An open source framework for automated feature engineering.\n* [FLAML](https://github.com/microsoft/FLAML) ![](https://img.shields.io/github/stars/microsoft/FLAML.svg?cacheSeconds=86400) - FLAML is a fast library for automated machine learning & tuning.\n* [HEBO](https://github.com/huawei-noah/HEBO) ![](https://img.shields.io/github/stars/huawei-noah/HEBO.svg?cacheSeconds=86400) - Set of open-source hyperparameter optimization frameworks, including the winning submission to the [NeurIPS 2020 Black-Box Optimisation Challenge](https://bbochallenge.com/leaderboard) tested on hyperparameter tuning tasks. \n* [Katib](https://github.com/kubeflow/katib) ![](https://img.shields.io/github/stars/kubeflow/katib.svg?cacheSeconds=86400) - A Kubernetes-based system for Hyperparameter Tuning and Neural Architecture Search.\n* [keras-tuner](https://github.com/keras-team/keras-tuner) ![](https://img.shields.io/github/stars/keras-team/keras-tuner.svg?cacheSeconds=86400) - Keras Tuner is an easy-to-use, distributable hyperparameter optimisation framework that solves the pain points of performing a hyperparameter search. Keras Tuner makes it easy to define a search space and leverage included algorithms to find the best hyperparameter values.\n* [Optuna](https://github.com/optuna/optuna) ![](https://img.shields.io/github/stars/optuna/optuna.svg?cacheSeconds=86400) - Optuna is an automatic hyperparameter optimisation software framework, particularly designed for machine learning.\n* [OSS Vizier](https://github.com/google/vizier) ![](https://img.shields.io/github/stars/google/vizier.svg?cacheSeconds=86400) - OSS Vizier is a Python-based service for black-box optimisation and research, one of the first hyperparameter tuning services designed to work at scale.\n* [TPOT](https://github.com/epistasislab/tpot) ![](https://img.shields.io/github/stars/epistasislab/tpot.svg?cacheSeconds=86400) - Automation of sklearn pipeline creation (including feature selection, pre-processor, etc.).\n* [tsfresh](https://github.com/blue-yonder/tsfresh) ![](https://img.shields.io/github/stars/blue-yonder/tsfresh.svg?cacheSeconds=86400) - Automatic extraction of relevant features from time series.\n\n## Computation and Communication Optimisation\n\n* [Accelerate](https://github.com/huggingface/accelerate) ![](https://img.shields.io/github/stars/huggingface/accelerate.svg?cacheSeconds=86400) - Accelerate abstracts exactly and only the boilerplate code related to multi-GPU/TPU/mixed-precision and leaves the rest of your code unchanged.\n* [Adapters](https://github.com/adapter-hub/adapters) ![](https://img.shields.io/github/stars/adapter-hub/adapters.svg?cacheSeconds=86400) - Adapters is a unified library for parameter-efficient and modular transfer learning.\n* [BitBLAS](https://github.com/microsoft/BitBLAS) ![](https://img.shields.io/github/stars/microsoft/BitBLAS.svg?cacheSeconds=86400) - BitBLAS is a library to support mixed-precision BLAS operations on GPUs\n* [Colossal-AI](https://github.com/hpcaitech/ColossalAI) ![](https://img.shields.io/github/stars/hpcaitech/ColossalAI.svg?cacheSeconds=86400) - A unified deep learning system for big model era, which helps users to efficiently and quickly deploy large AI model training and inference.\n* [Composer](https://github.com/mosaicml/composer) ![](https://img.shields.io/github/stars/mosaicml/composer.svg?cacheSeconds=86400) - Composer is a PyTorch library that enables you to train neural networks faster, at lower cost, and to higher accuracy.\n* [CuDF](https://github.com/rapidsai/cudf) ![](https://img.shields.io/github/stars/rapidsai/cudf.svg?cacheSeconds=86400) - Built based on the Apache Arrow columnar memory format, cuDF is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data.\n* [CuML](https://github.com/rapidsai/cuml) ![](https://img.shields.io/github/stars/rapidsai/cuml.svg?cacheSeconds=86400) - cuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions that share compatible APIs with other RAPIDS projects.\n* [CuPy](https://github.com/cupy/cupy) ![](https://img.shields.io/github/stars/cupy/cupy.svg?cacheSeconds=86400) - An implementation of NumPy-compatible multi-dimensional array on CUDA. CuPy consists of the core multi-dimensional array class, cupy.ndarray, and many functions on it.\n* [DEAP](https://github.com/DEAP/deap) ![](https://img.shields.io/github/stars/DEAP/deap.svg?cacheSeconds=86400) - A novel evolutionary computation framework for rapid prototyping and testing of ideas. It seeks to make algorithms explicit and data structures transparent. It works in perfect harmony with parallelisation mechanisms such as multiprocessing and SCOOP.\n* [DeepEP](https://github.com/deepseek-ai/DeepEP) ![](https://img.shields.io/github/stars/deepseek-ai/DeepEP.svg?cacheSeconds=86400) - DeepEP is a communication library tailored for Mixture-of-Experts (MoE) and expert parallelism (EP). It provides high-throughput and low-latency all-to-all GPU kernels, which are also known as MoE dispatch and combine. The library also supports low-precision operations, including FP8.\n* [DGL](https://github.com/dmlc/dgl) ![](https://img.shields.io/github/stars/dmlc/dgl.svg?cacheSeconds=86400) - DGL is an easy-to-use, high performance and scalable Python package for deep learning on graphs.\n* [DLRover](https://github.com/intelligent-machine-learning/dlrover) ![](https://img.shields.io/github/stars/intelligent-machine-learning/dlrover.svg?cacheSeconds=86400) - DLRover makes the distributed training of large AI models easy, stable, fast and green.\n* [Dask](https://github.com/dask/dask) ![](https://img.shields.io/github/stars/dask/dask.svg?cacheSeconds=86400) - Distributed parallel processing framework for Pandas and NumPy computations.\n* [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) ![](https://img.shields.io/github/stars/deepspeedai/DeepSpeed.svg?cacheSeconds=86400) - DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.\n* [FlagGems](https://github.com/FlagOpen/FlagGems) ![](https://img.shields.io/github/stars/FlagOpen/FlagGems.svg?cacheSeconds=86400) - FlagGems is a high-performance general operator library implemented in OpenAI Triton. It builds on a collection of backend neutral kernels that aims to accelerate LLM training and inference across diverse hardware platforms.\n* [Flashlight](https://github.com/flashlight/flashlight) ![](https://img.shields.io/github/stars/flashlight/flashlight.svg?cacheSeconds=86400) - A fast, flexible machine learning library written entirely in C++ from the Facebook AI Research and the creators of Torch, TensorFlow, Eigen and Deep Speech.\n* [Flax](https://github.com/google/flax) ![](https://img.shields.io/github/stars/google/flax.svg?cacheSeconds=86400) - A neural network library and ecosystem for JAX designed for flexibility.\n* [GPUStack](https://github.com/gpustack/gpustack) ![](https://img.shields.io/github/stars/gpustack/gpustack.svg?cacheSeconds=86400) - GPUStack is an open-source GPU cluster manager for running AI models.\n* [Hivemind](https://github.com/learning-at-home/hivemind) ![](https://img.shields.io/github/stars/learning-at-home/hivemind.svg?cacheSeconds=86400) - Decentralized deep learning in PyTorch.\n* [Horovod](https://github.com/horovod/horovod) ![](https://img.shields.io/github/stars/horovod/horovod.svg?cacheSeconds=86400) - Uber''s distributed training framework for TensorFlow, Keras, and PyTorch.\n* [Jax](https://github.com/jax-ml/jax) ![](https://img.shields.io/github/stars/jax-ml/jax.svg?cacheSeconds=86400) - Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more.\n* [Kompute](https://github.com/lava-nc/lava) ![](https://img.shields.io/github/stars/lava-nc/lava.svg?cacheSeconds=86400) - Blazing fast, lightweight and mobile phone-enabled Vulkan compute framework optimized for advanced GPU data processing usecases.\n* [Lava](https://github.com/KomputeProject/kompute) ![](https://img.shields.io/github/stars/KomputeProject/kompute.svg?cacheSeconds=86400) - Lava is an open source framework to develop applications for neuromorphic hardware architectures.\n* [Liger Kernel](https://github.com/linkedin/Liger-Kernel) ![](https://img.shields.io/github/stars/linkedin/Liger-Kernel.svg?cacheSeconds=86400) - Liger Kernel is a collection of Triton kernels designed specifically for LLM training.\n* [LightGBM](https://github.com/microsoft/LightGBM) ![](https://img.shields.io/github/stars/microsoft/LightGBM.svg?cacheSeconds=86400) - LightGBM is a gradient boosting framework that uses tree based learning algorithms.\n* [MLX](https://github.com/ml-explore/mlx) ![](https://img.shields.io/github/stars/ml-explore/mlx.svg?cacheSeconds=86400) - MLX is an array framework for machine learning on Apple silicon.\n* [Modin](https://github.com/modin-project/modin) ![](https://img.shields.io/github/stars/modin-project/modin.svg?cacheSeconds=86400) - Speed up your Pandas workflows by changing a single line of code.\n* [NVIDIA TensorRT](https://github.com/NVIDIA/TensorRT) ![](https://img.shields.io/github/stars/NVIDIA/TensorRT.svg?cacheSeconds=86400) - TensorRT is a C++ library for high-performance inference on NVIDIA GPUs and deep learning accelerators.\n* [Nevergrad](https://github.com/facebookresearch/nevergrad) ![](https://img.shields.io/github/stars/facebookresearch/nevergrad.svg?cacheSeconds=86400) - Nevergrad is a gradient-free optimisation platform.\n* [Norse](https://github.com/norse/norse) ![](https://img.shields.io/github/stars/norse/norse.svg?cacheSeconds=86400) - Norse aims to exploit the advantages of bio-inspired neural components, which are sparse and event-driven - a fundamental difference from artificial neural networks.\n* [Numba](https://github.com/numba/numba) ![](https://img.shields.io/github/stars/numba/numba.svg?cacheSeconds=86400)  - A compiler for Python array and numerical functions.\n* [Optimum](https://github.com/huggingface/optimum) ![](https://img.shields.io/github/stars/huggingface/optimum.svg?cacheSeconds=86400) - Optimum is an extension of Transformers and Diffusers, providing a set of optimization tools enabling maximum efficiency to train and run models on targeted hardware while keeping things easy to use.\n* [PEFT](https://github.com/huggingface/peft) ![](https://img.shields.io/github/stars/huggingface/peft.svg?cacheSeconds=86400) - Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model''s parameters.\n* [PaddlePaddle](https://github.com/PaddlePaddle/Paddle) ![](https://img.shields.io/github/stars/PaddlePaddle/Paddle.svg?cacheSeconds=86400) - PaddlePaddle is a framework to perform large-scale deep network training, using data sources distributed across hundreds of nodes. \n* [PyG](https://github.com/pyg-team/pytorch_geometric) ![](https://img.shields.io/github/stars/pyg-team/pytorch_geometric.svg?cacheSeconds=86400) - PyG (PyTorch Geometric) is a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data.\n* [PyTorch Lightning](https://github.com/Lightning-AI/pytorch-lightning) ![](https://img.shields.io/github/stars/Lightning-AI/pytorch-lightning.svg?cacheSeconds=86400) - PyTorch Lightning pretrains, finetunes and deploys AI models on multiple GPUs, TPUs with zero code changes.\n* [PyTorch](https://github.com/pytorch/pytorch) ![](https://img.shields.io/github/stars/pytorch/pytorch.svg?cacheSeconds=86400) - PyTorch is a library to develop and train neural network based deep learning models.\n* [Ray](https://github.com/ray-project/ray) ![](https://img.shields.io/github/stars/ray-project/ray.svg?cacheSeconds=86400) - Ray is a flexible, high-performance distributed execution framework for machine learning.\n* [SetFit](https://github.com/huggingface/setfit) ![](https://img.shields.io/github/stars/huggingface/setfit.svg?cacheSeconds=86400) - SetFit is an efficient and prompt-free framework for few-shot fine-tuning of Sentence Transformers.\n* [Sonnet](https://github.com/google-deepmind/sonnet) ![](https://img.shields.io/github/stars/google-deepmind/sonnet.svg?cacheSeconds=86400) - Sonnet is a library built on top of TensorFlow 2 designed to provide simple, composable abstractions for machine learning research.\n* [Streaming](https://github.com/mosaicml/streaming) ![](https://img.shields.io/github/stars/mosaicml/streaming.svg?cacheSeconds=86400) - A Data Streaming Library for Efficient Neural Network Training.\n* [TensorFlow](https://github.com/tensorflow/tensorflow) ![](https://img.shields.io/github/stars/tensorflow/tensorflow.svg?cacheSeconds=86400) - TensorFlow is a leading library designed for developing and deploying state-of-the-art  machine learning applications.\n* [ThunderKittens](https://github.com/HazyResearch/ThunderKittens) ![](https://img.shields.io/github/stars/HazyResearch/ThunderKittens.svg?cacheSeconds=86400) ThunderKittens is a framework to make it easy to write fast deep learning kernels in CUDA.\n* [TorchOpt](https://github.com/metaopt/torchopt) ![](https://img.shields.io/github/stars/metaopt/torchopt.svg?cacheSeconds=86400) - TorchOpt is an efficient library for differentiable optimization built upon PyTorch.\n* [Triton](https://github.com/triton-lang/triton) ![](https://img.shields.io/github/stars/triton-lang/triton.svg?cacheSeconds=86400) - Triton is a language and compiler for writing highly efficient custom Deep-Learning primitives. The aim of Triton is to provide an open-source environment to write fast code at higher productivity than CUDA, but also with higher flexibility than other existing DSLs.\n* [Vaex](https://github.com/vaexio/vaex) ![](https://img.shields.io/github/stars/vaexio/vaex.svg?cacheSeconds=86400) Vaex is a high performance Python library for lazy Out-of-Core DataFrames (similar to Pandas), to visualize and explore big tabular datasets. Vaex uses memory mapping, zero memory copy policy and lazy computations for best performance (no memory wasted).\n* [Vowpal Wabbit](https://github.com/VowpalWabbit/vowpal_wabbit) ![](https://img.shields.io/github/stars/VowpalWabbit/vowpal_wabbit.svg?cacheSeconds=86400) Vowpal Wabbit is a machine learning system which pushes the frontier of machine learning with techniques such as online, hashing, allreduce, reductions, learning2search, active, and interactive learning.\n* [XGBoost](https://github.com/dmlc/xgboost) ![](https://img.shields.io/github/stars/dmlc/xgboost.svg?cacheSeconds=86400) - XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.\n* [YDF](https://github.com/google/yggdrasil-decision-forests) ![](https://img.shields.io/github/stars/google/yggdrasil-decision-forests.svg?cacheSeconds=86400) - YDF (Yggdrasil Decision Forests) is a library to train, evaluate, interpret, and serve Random Forest, Gradient Boosted Decision Trees, CART and Isolation forest models.\n* [bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes) ![](https://img.shields.io/github/stars/bitsandbytes-foundation/bitsandbytes.svg?cacheSeconds=86400) - Bitsandbytes library is a lightweight Python wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and 8 & 4-bit quantization functions.\n* [einops](https://github.com/arogozhnikov/einops) ![](https://img.shields.io/github/stars/arogozhnikov/einops.svg?cacheSeconds=86400) - Flexible and powerful tensor operations for readable and reliable code.\n* [scikit-learn](https://github.com/scikit-learn/scikit-learn) ![](https://img.shields.io/github/stars/scikit-learn/scikit-learn.svg?cacheSeconds=86400) - Scikit-learn is a powerful machine learning library that provides a wide variety of modules for data access, data preparation and statistical model building. \n* [snnTorch](https://github.com/jeshraghian/snntorch) ![](https://img.shields.io/github/stars/jeshraghian/snntorch.svg?cacheSeconds=86400) - snnTorch is a deep and online learning library with spiking neural networks.\n* [torchdistill](https://github.com/yoshitomo-matsubara/torchdistill) ![](https://img.shields.io/github/stars/yoshitomo-matsubara/torchdistill.svg?cacheSeconds=86400) - torchdistill offers various state-of-the-art knowledge distillation methods and enables you to design (new) experiments simply by editing a declarative yaml config file instead of Python code.\n* [torchkeras](https://github.com/lyhue1991/torchkeras?tab=readme-ov-file) ![](https://img.shields.io/github/stars/lyhue1991/torchkeras?tab=readme-ov-file.svg?cacheSeconds=86400) The torchkeras library is a simple tool for training neural network in pytorch jusk in a keras style.\n* [veScale](https://github.com/volcengine/veScale) ![](https://img.shields.io/github/stars/volcengine/veScale.svg?cacheSeconds=86400) - veScale is a PyTorch native LLM training framework.\n* [yellowbrick](https://github.com/DistrictDataLabs/yellowbrick) ![](https://img.shields.io/github/stars/DistrictDataLabs/yellowbrick.svg?cacheSeconds=86400) - yellowbrick is a matplotlib-based model evaluation plots for scikit-learn and other machine learning libraries.\n\n## Data Annotation and Synthesis\n* [Argilla](https://github.com/argilla-io/argilla) ![](https://img.shields.io/github/stars/argilla-io/argilla.svg?cacheSeconds=86400) - Argilla helps domain experts and data teams to build better NLP datasets in less time.\n* [cleanlab](https://github.com/cleanlab/cleanlab) ![](https://img.shields.io/github/stars/cleanlab/cleanlab.svg?cacheSeconds=86400) - Python library for data-centric AI. Can automatically: find mislabeled data, detect outliers, estimate consensus + annotator-quality for multi-annotator datasets, suggest which data is best to (re)label next.\n* [COCO Annotator](https://github.com/jsbroks/coco-annotator) ![](https://img.shields.io/github/stars/jsbroks/coco-annotator.svg?cacheSeconds=86400) - Web-based image segmentation tool for object detection, localization and keypoints\n* [CVAT](https://github.com/cvat-ai/cvat) ![](https://img.shields.io/github/stars/cvat-ai/cvat.svg?cacheSeconds=86400) - CVAT (Computer Vision Annotation Tool) is OpenCV''s web-based annotation tool for both videos and images for computer algorithms.\n* [Doccano](https://github.com/doccano/doccano) ![](https://img.shields.io/github/stars/doccano/doccano.svg?cacheSeconds=86400) - Open source text annotation tools for humans, providing functionality for sentiment analysis, named entity recognition, and machine translation.\n* [Gretel Synthetics](https://github.com/gretelai/gretel-synthetics) ![](https://img.shields.io/github/stars/gretelai/gretel-synthetics.svg?cacheSeconds=86400) - Gretel Synthetics is a synthetic data generators for structured and unstructured text, featuring differentially private learning.\n* [Label Studio](https://github.com/HumanSignal/label-studio) ![](https://img.shields.io/github/stars/HumanSignal/label-studio.svg?cacheSeconds=86400) - Multi-domain data labeling and annotation tool with standardized output format.\n* [NeMo Curator](https://github.com/NVIDIA/NeMo-Curator) ![](https://img.shields.io/github/stars/NVIDIA/NeMo-Curator.svg?cacheSeconds=86400) - NeMo Curator is a GPU-accelerated framework for efficient large language model data curation.\n* [refinery](https://github.com/code-kern-ai/refinery) ![](https://img.shields.io/github/stars/code-kern-ai/refinery.svg?cacheSeconds=86400) - The data scientist''s open-source choice to scale, assess and maintain natural language data.\n* [SDV](https://github.com/sdv-dev/SDV) ![](https://img.shields.io/github/stars/sdv-dev/SDV.svg?cacheSeconds=86400) - Synthetic Data Vault (SDV) is a Synthetic Data Generation ecosystem of libraries that allows users to easily learn single-table, multi-table and timeseries datasets to later on generate new Synthetic Data that has the same format and statistical properties as the original dataset.\n* [Semantic Segmentation Editor](https://github.com/Hitachi-Automotive-And-Industry-Lab/semantic-segmentation-editor) ![](https://img.shields.io/github/stars/Hitachi-Automotive-And-Industry-Lab/semantic-segmentation-editor.svg?cacheSeconds=86400) - Hitachi''s Open source tool for labelling camera and LIDAR data.\n* [synthcity](https://github.com/vanderschaarlab/synthcity) ![](https://img.shields.io/github/stars/vanderschaarlab/synthcity.svg?cacheSeconds=86400) - synthcity is a library for generating and evaluating synthetic tabular data.\n* [ViPE](https://github.com/nv-tlabs/vipe) ![](https://img.shields.io/github/stars/nv-tlabs/vipe.svg?cacheSeconds=86400) - ViPE is a spatial AI tool for annotating camera poses and dense depth maps from raw videos.\n* [YData Synthetic](https://github.com/ydataai/ydata-synthetic) ![](https://img.shields.io/github/stars/ydataai/ydata-synthetic.svg?cacheSeconds=86400) - YData Synthetic is a package to generate synthetic tabular and time-series data leveraging the state of the art generative models.\n\n## Data Pipeline\n* [Apache Airflow](https://github.com/apache/airflow) ![](https://img.shields.io/github/stars/apache/airflow.svg?cacheSeconds=86400) - Data Pipeline framework built in Python, including scheduler, DAG definition and a UI for visualisation.\n* [Apache Nifi](https://github.com/apache/nifi) ![](https://img.shields.io/github/stars/apache/nifi.svg?cacheSeconds=86400) - Apache NiFi was made for dataflow. It supports highly configurable directed graphs of data routing, transformation, and system mediation logic.\n* [Apache Oozie](https://github.com/apache/oozie) ![](https://img.shields.io/github/stars/apache/oozie.svg?cacheSeconds=86400) - Workflow scheduler for Hadoop jobs.\n* [Argo Workflows](https://github.com/argoproj/argo-workflows) ![](https://img.shields.io/github/stars/argoproj/argo-workflows.svg?cacheSeconds=86400) - Argo Workflows is an open source container-native workflow engine for orchestrating parallel jobs on Kubernetes. Argo Workflows is implemented as a Kubernetes CRD (Custom Resource Definition).\n* [Couler](https://github.com/couler-proj/couler) ![](https://img.shields.io/github/stars/couler-proj/couler.svg?cacheSeconds=86400) - Unified interface for constructing and managing machine learning workflows on different workflow engines, such as Argo Workflows, Tekton Pipelines, and Apache Airflow.\n* [DataTrove](https://github.com/huggingface/datatrove) ![](https://img.shields.io/github/stars/huggingface/datatrove.svg?cacheSeconds=86400) - DataTrove is a library to process, filter and deduplicate text data at a very large scale.\n* [Dagster](https://github.com/dagster-io/dagster) ![](https://img.shields.io/github/stars/dagster-io/dagster.svg?cacheSeconds=86400) - A data orchestrator for machine learning, analytics, and ETL.\n* [DBT](https://github.com/dbt-labs/dbt-core) ![](https://img.shields.io/github/stars/dbt-labs/dbt-core.svg?cacheSeconds=86400) - ETL tool for running transformations inside data warehouses.\n* [Flyte](https://github.com/flyteorg/flyte) ![](https://img.shields.io/github/stars/flyteorg/flyte.svg?cacheSeconds=86400) - Lyftâ€™s Cloud Native Machine Learning and Data Processing Platform - [(Demo)](https://youtu.be/KdUJGSP1h9U?t=1451).\n* [Genie](https://github.com/Netflix/genie) ![](https://img.shields.io/github/stars/Netflix/genie.svg?cacheSeconds=86400) - Job orchestration engine to interface and trigger the execution of jobs from Hadoop-based systems.\n* [Hamilton](https://github.com/dagworks-inc/hamilton) ![](https://img.shields.io/github/stars/dagworks-inc/hamilton.svg?cacheSeconds=86400) - Hamilton is a micro-orchestration framework for defining dataflows. Runs anywhere python runs (e.g. jupyter, fastAPI, spark, ray, dask). Brings software engineering best practices without you knowing it. Use it to define feature engineering transforms, end-to-end model pipelines, and LLM workflows. It complements macro-orchestration systems (e.g. kedro, luigi, airflow, dbt, etc.) as it replaces the code within those macro tasks. Comes with a self-hostable UI that captures lineage & provenance, execution telemetry & data summaries, and builds a self-populating catalog; usable in development as well as production.\n* [Instill VDP](https://github.com/instill-ai/instill-core) ![](https://img.shields.io/github/stars/instill-ai/instill-core.svg?cacheSeconds=86400) - Instill VDP (Versatile Data Pipeline) aims to streamline the data processing pipelines from inception to completion.\n* [Instructor](https://github.com/instructor-ai/instructor) ![](https://img.shields.io/github/stars/instructor-ai/instructor.svg?cacheSeconds=86400) - Instructor makes it easy to get structured data like JSON from LLMs like GPT-3.5, GPT-4, GPT-4-Vision, and open-source models.\n* [Kedro](https://github.com/kedro-org/kedro) ![](https://img.shields.io/github/stars/kedro-org/kedro.svg?cacheSeconds=86400) - Kedro is a workflow development tool that helps you build data pipelines that are robust, scalable, deployable, reproducible and versioned.\n* [Luigi](https://github.com/spotify/luigi) ![](https://img.shields.io/github/stars/spotify/luigi.svg?cacheSeconds=86400) - Luigi is a Python module that helps you build complex pipelines of batch jobs, handling dependency resolution, workflow management, visualisation, etc..\n* [Metaflow](https://github.com/Netflix/metaflow) ![](https://img.shields.io/github/stars/Netflix/metaflow.svg?cacheSeconds=86400) - A framework for data scientists to easily build and manage real-life data science projects.\n* [Pachyderm](https://github.com/pachyderm/pachyderm) ![](https://img.shields.io/github/stars/pachyderm/pachyderm.svg?cacheSeconds=86400) - Open source distributed processing framework build on Kubernetes focused mainly on dynamic building of production machine learning pipelines - [(Video)](https://www.youtube.com/watch?v=LamKVhe2RSM).\n* [Ploomber](https://github.com/ploomber/ploomber) ![](https://img.shields.io/github/stars/ploomber/ploomber.svg?cacheSeconds=86400) - The fastest way to build data pipelines. Develop iteratively, deploy anywhere.\n* [Pixeltable](https://github.com/pixeltable/pixeltable) ![](https://img.shields.io/github/stars/pixeltable/pixeltable.svg?cacheSeconds=86400) â€“ Open-source Python library providing declarative, incremental data infrastructure for building and managing multimodal AI workloads.\n* [Prefect Core](https://github.com/PrefectHQ/prefect) ![](https://img.shields.io/github/stars/PrefectHQ/prefect.svg?cacheSeconds=86400) - Workflow management system that makes it easy to take your data pipelines and add semantics like retries, logging, dynamic mapping, caching, failure notifications, and more.\n* [SeqIO](https://github.com/google/seqio) ![](https://img.shields.io/github/stars/google/seqio.svg?cacheSeconds=86400) - SeqIO is a library for processing sequential data to be fed into downstream sequence models.\n* [Snakemake](https://github.com/snakemake/snakemake) ![](https://img.shields.io/github/stars/snakemake/snakemake.svg?cacheSeconds=86400) - Workflow management system for reproducible and scalable data analyses.\n* [Towhee](https://github.com/towhee-io/towhee) ![](https://img.shields.io/github/stars/towhee-io/towhee.svg?cacheSeconds=86400) - General-purpose machine learning pipeline for generating embedding vectors using one or many ML models.\n* [unstructured](https://github.com/Unstructured-IO/unstructured) ![](https://img.shields.io/github/stars/Unstructured-IO/unstructured.svg?cacheSeconds=86400) - unstructured streamlines and optimizes the data processing workflow for LLMs, ingesting and pre-processing images and text documents, such as PDFs, HTML, Word docs, and many more. \n* [ZenML](https://github.com/zenml-io/zenml) ![](https://img.shields.io/github/stars/zenml-io/zenml.svg?cacheSeconds=86400) - ZenML is an extensible, open-source MLOps framework to create reproducible ML pipelines with a focus on automated metadata tracking, caching, and many integrations to other tools.\n\n## Data Science Notebook\n* [Apache Zeppelin](https://github.com/apache/zeppelin) ![](https://img.shields.io/github/stars/apache/zeppelin.svg?cacheSeconds=86400) - Web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala and more.\n* [Deepnote](https://github.com/deepnote/deepnote) ![](https://img.shields.io/github/stars/deepnote/deepnote.svg?cacheSeconds=86400) - Deepnote is a drop-in replacement for Jupyter with an AI-first design, sleek UI, new blocks, and native data integrations. Use Python, R, and SQL locally in your favorite IDE, then scale to Deepnote cloud for real-time collaboration, Deepnote agent, and deployable data apps.\n* [Jupyter Notebooks](https://github.com/jupyter/notebook) ![](https://img.shields.io/github/stars/jupyter/notebook.svg?cacheSeconds=86400) - Web interface python sandbox environments for reproducible development\n* [Marimo](https://github.com/marimo-team/marimo) ![](https://img.shields.io/github/stars/marimo-team/marimo.svg?cacheSeconds=86400) - Reactive Python notebook â€” run reproducible experiments, execute as a script, deploy as an app, and version with git.\n* [Papermill](https://github.com/nteract/papermill) ![](https://img.shields.io/github/stars/nteract/papermill.svg?cacheSeconds=86400) - Papermill is a library for parameterizing notebooks and executing them like Python scripts.\n* [Polynote](https://github.com/polynote/polynote) ![](https://img.shields.io/github/stars/polynote/polynote.svg?cacheSeconds=86400) - Polynote is an experimental polyglot notebook environment. Currently, it supports Scala and Python (with or without Spark), SQL, and Vega.\n* [RMarkdown](https://github.com/rstudio/rmarkdown) ![](https://img.shields.io/github/stars/rstudio/rmarkdown.svg?cacheSeconds=86400) - The rmarkdown package is a next generation implementation of R Markdown based on Pandoc.\n* [Stencila](https://github.com/stencila/stencila) ![](https://img.shields.io/github/stars/stencila/stencila.svg?cacheSeconds=86400) - Stencila is a platform for creating, collaborating on, and sharing data driven content. Content that is transparent and reproducible.\n* [VoilÃ ](https://github.com/voila-dashboards/voila) ![](https://img.shields.io/github/stars/voila-dashboards/voila.svg?cacheSeconds=86400) - VoilÃ  turns Jupyter notebooks into standalone web applications that can e.g. be used as dashboards.\n* [.NET Interactive](https://github.com/dotnet/interactive) ![](https://img.shields.io/github/stars/dotnet/interactive.svg?cacheSeconds=86400) - .NET Interactive takes the power of .NET and embeds it into your interactive experiences.\n\n## Data Storage Optimisation\n* [AIStore](https://github.com/NVIDIA/aistore) ![](https://img.shields.io/github/stars/NVIDIA/aistore.svg?cacheSeconds=86400) - AIStore is a lightweight object storage system with the capability to linearly scale out with each added storage node and a special focus on petascale deep learning.\n* [Alluxio](https://github.com/Alluxio/alluxio) ![](https://img.shields.io/github/stars/Alluxio/alluxio.svg?cacheSeconds=86400) - A virtual distributed storage system that bridges the gab between computation frameworks and storage systems.\n* [Apache Arrow](https://github.com/apache/arrow) ![](https://img.shields.io/github/stars/apache/arrow.svg?cacheSeconds=86400) - In-memory columnar representation of data compatible with Pandas, Hadoop-based systems, etc..\n* [Apache Druid](https://github.com/apache/druid) ![](https://img.shields.io/github/stars/apache/druid.svg?cacheSeconds=86400) - A high performance real-time analytics database. Check this [article](https://towardsdatascience.com/introduction-to-druid-4bf285b92b5a) for introduction.\n* [Apache Hudi](https://github.com/apache/hudi) ![](https://img.shields.io/github/stars/apache/hudi.svg?cacheSeconds=86400) - Hudi is a transactional data lake platform that brings core warehouse and database functionality directly to a data lake. Hudi is great for streaming workloads, and also allows creation of efficient incremental batch pipelines. Supports popular query engines including Spark, Flink, Presto, Trino, Hive, etc. More info [here](https://hudi.apache.org/).\n* [Apache Iceberg](https://github.com/apache/iceberg) ![](https://img.shields.io/github/stars/apache/iceberg.svg?cacheSeconds=86400) - Iceberg is an ACID-compliant, high-performance format built for huge analytic tables (containing tens of petabytes of data), and it brings the reliability and simplicity of SQL tables to big data, while making it possible for engines like Spark, Trino, Flink, Presto, Hive and Impala to safely work with the same tables, at the same time. More info [here](https://iceberg.apache.org/).\n* [Apache Ignite](https://github.com/apache/ignite) ![](https://img.shields.io/github/stars/apache/ignite.svg?cacheSeconds=86400) - A memory-centric distributed database, caching, and processing platform for transactional, analytical, and streaming workloads delivering in-memory speeds at petabyte scale - [Demo](https://www.youtube.com/watch?v=Xt4PWQ__YPw).\n* [Apache Parquet](https://github.com/apache/parquet-java) ![](https://img.shields.io/github/stars/apache/parquet-java.svg?cacheSeconds=86400) - On-disk columnar representation of data compatible with Pandas, Hadoop-based systems, etc..\n* [Apache Pinot](https://github.com/apache/pinot) ![](https://img.shields.io/github/stars/apache/pinot.svg?cacheSeconds=86400) - A realtime distributed OLAP datastore. Comparison of the open source OLAP systems for big data: ClickHouse, Druid, and Pinot is found [here](https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7).\n* [Casibase](https://github.com/casibase/casibase) ![](https://img.shields.io/github/stars/casibase/casibase.svg?cacheSeconds=86400) - Casibase is a LangChain-like RAG (Retrieval-Augmented Generation) knowledge database with web UI and Enterprise SSO.\n* [Chroma](https://github.com/chroma-core/chroma) ![](https://img.shields.io/github/stars/chroma-core/chroma.svg?cacheSeconds=86400) - Chroma is an open-source embedding database.\n* [ClickHouse](https://github.com/ClickHouse/ClickHouse) ![](https://img.shields.io/github/stars/ClickHouse/ClickHouse.svg?cacheSeconds=86400) - ClickHouse is an open source column oriented database management system.\n* [Delta Lake](https://github.com/delta-io/delta) ![](https://img.shields.io/github/stars/delta-io/delta.svg?cacheSeconds=86400) - Delta Lake is a storage layer that brings scalable, ACID transactions to Apache Spark and other big-data engines.\n* [EdgeDB](https://github.com/geldata/gel) ![](https://img.shields.io/github/stars/geldata/gel.svg?cacheSeconds=86400) - Gel supercharges Postgres with a modern data model, graph queries, Auth & AI solutions, and much more.\n* [GPTCache](https://github.com/zilliztech/GPTCache) ![](https://img.shields.io/github/stars/zilliztech/GPTCache.svg?cacheSeconds=86400) - GPTCache is a library for creating semantic cache for large language model queries.\n* [InfluxDB](https://github.com/influxdata/influxdb) ![](https://img.shields.io/github/stars/influxdata/influxdb.svg?cacheSeconds=86400) Scalable datastore for metrics, events, and real-time analytics.\n* [Milvus](https://github.com/milvus-io/milvus) ![](https://img.shields.io/github/stars/milvus-io/milvus.svg?cacheSeconds=86400) Milvus is a cloud-native, open-source vector database built to manage embedding vectors generated by machine learning models and neural networks.\n* [Marqo](https://github.com/marqo-ai/marqo) ![](https://img.shields.io/github/stars/marqo-ai/marqo.svg?cacheSeconds=86400) Marqo is an end-to-end vector search engine.\n* [pgvector](https://github.com/pgvector/pgvector) ![](https://img.shields.io/github/stars/pgvector/pgvector.svg?cacheSeconds=86400) pgvector helps with vector similarity search for Postgres.\n* [PostgresML](https://github.com/postgresml/postgresml) ![](https://img.shields.io/github/stars/postgresml/postgresml.svg?cacheSeconds=86400) PostgresML is a machine learning extension for PostgreSQL that enables you to perform training and inference on text and tabular data using SQL queries.\n* [Safetensors](https://github.com/huggingface/safetensors) ![](https://img.shields.io/github/stars/huggingface/safetensors.svg?cacheSeconds=86400) Simple, safe way to store and distribute tensors.\n* [TimescaleDB](https://github.com/timescale/timescaledb) ![](https://img.shields.io/github/stars/timescale/timescaledb.svg?cacheSeconds=86400) An open-source time-series SQL database optimized for fast ingest and complex queries packaged as a PostgreSQL extension - [(Video)](https://www.youtube.com/watch?v=zbjub8BQPyE).\n* [Weaviate](https://github.com/weaviate/weaviate) ![](https://img.shields.io/github/stars/weaviate/weaviate.svg?cacheSeconds=86400) - A low-latency vector search engine (GraphQL, RESTful) with out-of-the-box support for different media types. Modules include Semantic Search, Q&A, Classification, Customizable Models (PyTorch/TensorFlow/Keras), and more.\n* [Zarr](https://github.com/zarr-developers/zarr-python) ![](https://img.shields.io/github/stars/zarr-developers/zarr-python.svg?cacheSeconds=86400) - Python implementation of chunked, compressed, N-dimensional arrays designed for use in parallel computing.\n\n## Data Stream Processing\n* [Apache Beam](https://github.com/apache/beam) ![](https://img.shields.io/github/stars/apache/beam.svg?cacheSeconds=86400) Apache Beam is a unified programming model for Batch and Streaming.\n* [Apache Flink](https://github.com/apache/flink) ![](https://img.shields.io/github/stars/apache/flink.svg?cacheSeconds=86400) - Open source stream processing framework with powerful stream and batch processing capabilities.\n* [Apache Kafka](https://github.com/apache/kafka) ![](https://img.shields.io/github/stars/apache/kafka.svg?cacheSeconds=86400) - Kafka client library for building applications and microservices where the input and output are stored in kafka clusters.\n* [Apache Samza](https://github.com/apache/samza) ![](https://img.shields.io/github/stars/apache/samza.svg?cacheSeconds=86400) - Distributed stream processing framework. It uses Apache Kafka for messaging, and Apache Hadoop YARN to provide fault tolerance, processor isolation, security, and resource management.\n* [Apache Spark](https://github.com/apache/spark) ![](https://img.shields.io/github/stars/apache/spark.svg?cacheSeconds=86400) - Micro-batch processing for streams using the apache spark framework as a backend supporting stateful exactly-once semantics.\n* [Bytewax](https://github.com/bytewax/bytewax) ![](https://img.shields.io/github/stars/bytewax/bytewax.svg?cacheSeconds=86400) - Flexible Python-centric stateful stream processing framework built on top of Rust engine.\n* [FastStream](https://github.com/airtai/faststream) ![](https://img.shields.io/github/stars/airtai/faststream.svg?cacheSeconds=86400) - A modern broker-agnostic streaming Python framework supporting Apache Kafka, RabbitMQ and NATS protocols, inspired by FastAPI and easily integratable with other web frameworks.\n* [MOA](https://github.com/Waikato/moa) ![](https://img.shields.io/github/stars/Waikato/moa.svg?cacheSeconds=86400) - MOA (Massive Online Analysis) is an open source framework for Big Data stream mining.\n* [MosaicML Streaming](https://github.com/mosaicml/streaming) ![](https://img.shields.io/github/stars/mosaicml/streaming.svg?cacheSeconds=86400) - Fast, deterministic streaming of large datasets from cloud storage for distributed model training.\n* [RisingWave](https://github.com/risingwavelabs/risingwave) ![](https://img.shields.io/github/stars/risingwavelabs/risingwave.svg?cacheSeconds=86400) - A distributed SQL streaming database that unifies stream processing and low-latency serving, ideal for building and serving features for online machine learning.\n* [TensorStore](https://github.com/google/tensorstore) ![](https://img.shields.io/github/stars/google/tensorstore.svg?cacheSeconds=86400) - Library for reading and writing large multi-dimensional arrays.\n\n\n## Deployment and Serving\n* [Agenta](https://github.com/Agenta-AI/agenta) ![](https://img.shields.io/github/stars/Agenta-AI/agenta.svg?cacheSeconds=86400) - Agenta provides end-to-end tools for the entire LLMOps workflow: building (LLM playground, evaluation), deploying (prompt and configuration management), and  (LLM observability and tracing).\n* [AirLLM](https://github.com/lyogavin/airllm) ![](https://img.shields.io/github/stars/lyogavin/airllm.svg?cacheSeconds=86400) - AirLLM optimizes inference memory usage, allowing 70B large language models to run inference on a single 4GB GPU card without quantization, distillation and pruning.\n* [AITemplate](https://github.com/facebookincubator/AITemplate) ![](https://img.shields.io/github/stars/facebookincubator/AITemplate.svg?cacheSeconds=86400) - AITemplate (AIT) is a Python framework that transforms deep neural networks into CUDA (NVIDIA GPU) / HIP (AMD GPU) C++ code for lightning-fast inference serving.\n* [BentoML](https://github.com/bentoml/BentoML) ![](https://img.shields.io/github/stars/bentoml/BentoML.svg?cacheSeconds=86400) - BentoML is an open source framework for high performance ML model serving.\n* [BISHENG](https://github.com/dataelement/bisheng) ![](https://img.shields.io/github/stars/dataelement/bisheng.svg?cacheSeconds=86400) - BISHENG is an open LLM application devops platform, focusing on enterprise scenarios.\n* [DeepDetect](https://github.com/jolibrain/deepdetect) ![](https://img.shields.io/github/stars/jolibrain/deepdetect.svg?cacheSeconds=86400) - Machine Learning production server for TensorFlow, XGBoost and Cafe models written in C++ and maintained by Jolibrain.\n* [Dynamo](https://github.com/ai-dynamo/dynamo) ![](https://img.shields.io/github/stars/ai-dynamo/dynamo.svg?cacheSeconds=86400) - NVIDIA Dynamo is a high-throughput, low-latency inference framework designed for serving generative AI and reasoning models in multi-node distributed environments.\n* [exo](https://github.com/exo-explore/exo) ![](https://img.shields.io/github/stars/exo-explore/exo.svg?cacheSeconds=86400) - exo helps you run your AI cluster at home with everyday devices.\n* [Genkit](https://github.com/firebase/genkit) ![](https://img.shields.io/github/stars/firebase/genkit.svg?cacheSeconds=86400) - Genkit is an open source framework for building AI-powered apps with familiar code-centric patterns. Genkit makes it easy to develop, integrate, and test AI features with observability and evaluations.\n* [Inference](https://github.com/roboflow/inference) ![](https://img.shields.io/github/stars/roboflow/inference.svg?cacheSeconds=86400) - A fast, production-ready inference server for computer vision supporting deployment of many popular model architectures and fine-tuned models. With Inference, you can deploy models such as YOLOv5, YOLOv8, CLIP, SAM, and CogVLM on your own hardware using Docker.\n* [Infinity](https://github.com/michaelfeil/infinity) ![](https://img.shields.io/github/stars/michaelfeil/infinity.svg?cacheSeconds=86400) - Infinity is a high-throughput, low-latency REST API for serving text-embeddings, reranking models and clip. \n* [IPEX-LLM](https://github.com/intel/ipex-llm) ![](https://img.shields.io/github/stars/intel/ipex-llm.svg?cacheSeconds=86400) - IPEX-LLM is a PyTorch library for running LLM on Intel CPU and GPU (e.g., local PC with iGPU, discrete GPU such as Arc, Flex and Max) with very low latency.\n* [LiteLLM](https://github.com/BerriAI/litellm) ![](https://img.shields.io/github/stars/BerriAI/litellm.svg?cacheSeconds=86400) - LiteLLM is a Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq.\n* [Jina-serve](https://github.com/jina-ai/serve) ![](https://img.shields.io/github/stars/jina-ai/serve.svg?cacheSeconds=86400) - Jina-serve is a framework for building and deploying AI services that communicate via gRPC, HTTP and WebSockets.\n* [Kiln](https://github.com/kiln-ai/kiln) ![](https://img.shields.io/github/stars/kiln-ai/kiln.svg?cacheSeconds=86400) - Kiln is an OSS tool for fine-tuning LLM models, synthetic data generation, and collaborating on datasets.\n* [KServe](https://github.com/kserve/kserve) ![](https://img.shields.io/github/stars/kserve/kserve.svg?cacheSeconds=86400) - KServe provides a Kubernetes Custom Resource Definition for serving predictive and generative ML.\n* [KTransformers](https://github.com/kvcache-ai/ktransformers) ![](https://img.shields.io/github/stars/kvcache-ai/ktransformers.svg?cacheSeconds=86400) - KTransformers is a flexible framework for experiencing cutting-edge LLM inference optimizations.\n* [Langtrace](https://github.com/Scale3-Labs/langtrace) ![](https://img.shields.io/github/stars/Scale3-Labs/langtrace.svg?cacheSeconds=86400) - Langtrace is an open-source, Open Telemetry based end-to-end observability tool for LLM applications, providing real-time tracing, evaluations and metrics for popular LLMs, LLM frameworks, vectorDBs and more.\n* [Lepton AI](https://github.com/leptonai/leptonai) ![](https://img.shields.io/github/stars/leptonai/leptonai.svg?cacheSeconds=86400) - LeptonAI Python library allows you to build an AI service from Python code with ease.\n* [LightLLM](https://github.com/ModelTC/lightllm) ![](https://img.shields.io/github/stars/ModelTC/lightllm.svg?cacheSeconds=86400) - LightLLM is a Python-based LLM (Large Language Model) inference and serving framework, notable for its * [llama.cpp](https://github.com/ggml-org/llama.cpp) ![](https://img.shields.io/github/stars/ggml-org/llama.cpp.svg?cacheSeconds=86400) - llama.cpp is an open source software library that performs inference on various large language models such as Llama.\n* [LMDeploy](https://github.com/InternLM/lmdeploy) ![](https://img.shields.io/github/stars/InternLM/lmdeploy.svg?cacheSeconds=86400) - LMDeploy is a toolkit for compressing, deploying, and serving LLM.\n* [LM Studio](https://github.com/lmstudio-ai/lms) ![](https://img.shields.io/github/stars/lmstudio-ai/lms.svg?cacheSeconds=86400) - LM Studio is a tool for deploying LLM models locally on the computer, even on a relatively modest machine, provided it meets the minimum requirements.\n* [LocalAI](https://github.com/mudler/LocalAI) ![](https://img.shields.io/github/stars/mudler/LocalAI.svg?cacheSeconds=86400) - LocalAI is a drop-in replacement REST API that''s compatible with OpenAI API specifications for local inferencing.\n* [MindsDB](https://github.com/mindsdb/mindsdb) ![](https://img.shields.io/github/stars/mindsdb/mindsdb.svg?cacheSeconds=86400) - MindsDB is the platform to create, serve, and fine-tune models in real-time from your database, vector store, and application data.\n* [MLRun](https://github.com/mlrun/mlrun)![](https://img.shields.io/github/stars/mlrun/mlrun.svg?cacheSeconds=86400)- MLRun is an open MLOps framework for quickly building and managing continuous ML and generative AI applications across their lifecycle.\n* [MLServer](https://github.com/SeldonIO/mlserver) ![](https://img.shields.io/github/stars/SeldonIO/mlserver.svg?cacheSeconds=86400) - An inference server for your machine learning models, including support for multiple frameworks, multi-model serving and more.\n* [Mosec](https://github.com/mosecorg/mosec) ![](https://img.shields.io/github/stars/mosecorg/mosec.svg?cacheSeconds=86400) - A rust-powered and multi-stage pipelined model server which offers dynamic batching and more. Super easy to implement and deploy as micro-services.\n* [nndeploy](https://github.com/nndeploy/nndeploy) ![](https://img.shields.io/github/stars/nndeploy/nndeploy.svg?cacheSeconds=86400) - An Easy-to-Use and High-Performance AI deployment framework.\n* [Nuclio](https://github.com/nuclio/nuclio) ![](https://img.shields.io/github/stars/nuclio/nuclio.svg?cacheSeconds=86400) - A high-performance "serverless" framework focused on data, I/O, and compute-intensive workloads. It is well integrated with popular data science tools, such as Jupyter and Kubeflow; supports a variety of data and streaming sources; and supports execution over CPUs and GPUs.\n* [OpenLLM](https://github.com/bentoml/OpenLLM) ![](https://img.shields.io/github/stars/bentoml/OpenLLM.svg?cacheSeconds=86400) - OpenLLM allows developers to run any open-source LLMs (Llama 3.1, Qwen2, Phi3 and more) or custom models as OpenAI-compatible APIs with a single command.\n* [OpenVINO](https://github.com/openvinotoolkit/openvino) ![](https://img.shields.io/github/stars/openvinotoolkit/openvino.svg?cacheSeconds=86400) - OpenVINO is an open-source toolkit for optimizing and deploying AI inference.\n* [Open WebUI](https://github.com/open-webui/open-webui) ![](https://img.shields.io/github/stars/open-webui/open-webui.svg?cacheSeconds=86400) - Open WebUI is an extensible, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline. It supports various LLM runners like Ollama and OpenAI-compatible APIs, with built-in inference engine for RAG, making it a powerful AI deployment solution.\n* [OptiLLM](https://github.com/algorithmicsuperintelligence/optillm) ![](https://img.shields.io/github/stars/algorithmicsuperintelligence/optillm.svg?cacheSeconds=86400) - OptiLLM is an OpenAI API-compatible optimizing inference proxy that implements 20+ state-of-the-art techniques to dramatically improve LLM accuracy and performance on reasoning tasks - without requiring any model training or fine-tuning.\n* [PowerInfer](https://github.com/SJTU-IPADS/PowerInfer) ![](https://img.shields.io/github/stars/SJTU-IPADS/PowerInfer.svg?cacheSeconds=86400) - PowerInfer is a CPU/GPU LLM inference engine leveraging activation locality for your device.\n* [Prompt2Model](https://github.com/neulab/prompt2model) ![](https://img.shields.io/github/stars/neulab/prompt2model.svg?cacheSeconds=86400) - Prompt2Model is a system that takes a natural language task description (like the prompts used for LLMs such as ChatGPT) to train a small special-purpose model that is conducive for deployment.\n* [Seldon Core](https://github.com/SeldonIO/seldon-core) ![](https://img.shields.io/github/stars/SeldonIO/seldon-core.svg?cacheSeconds=86400) - Open source platform for deploying and  machine learning models in Kubernetes - [(Video)](https://www.youtube.com/watch?v=pDlapGtecbY).\n* [SGLang](https://github.com/sgl-project/sglang) ![](https://img.shields.io/github/stars/sgl-project/sglang.svg?cacheSeconds=86400) - SGLang is a fast serving framework for large language models and vision language models.\n* [SkyPilot](https://github.com/skypilot-org/skypilot) ![](https://img.shields.io/github/stars/skypilot-org/skypilot.svg?cacheSeconds=86400) - SkyPilot is a framework for running LLMs, AI, and batch jobs on any cloud, offering maximum cost savings, highest GPU availability, and managed execution.\n* [Tensorflow Serving](https://github.com/tensorflow/serving) ![](https://img.shields.io/github/stars/tensorflow/serving.svg?cacheSeconds=86400) - High-performant framework to serve Tensorflow models via grpc protocol able to handle 100k requests per second per core.\n* [text-generation-inference](https://github.com/huggingface/text-generation-inference) ![](https://img.shields.io/github/stars/huggingface/text-generation-inference.svg?cacheSeconds=86400) - Large Language Model Text Generation Inference.\n* [TorchServe](https://github.com/pytorch/serve) ![](https://img.shields.io/github/stars/pytorch/serve.svg?cacheSeconds=86400) - TorchServe is a flexible and easy to use tool for serving PyTorch models.\n* [torchtune](https://github.com/meta-pytorch/torchtune) ![](https://img.shields.io/github/stars/meta-pytorch/torchtune.svg?cacheSeconds=86400) - torchtune is a PyTorch library for easily authoring, post-training, and experimenting with LLMs.\n* [Transformer Lab](https://github.com/transformerlab/transformerlab-app) ![](https://img.shields.io/github/stars/transformerlab/transformerlab-app.svg?cacheSeconds=86400) - Transformer Lab is an open-source LLM workspace for finetuning, evaluating, exporting, and testing models locally across inference engines and platforms.\n* [Triton Inference Server](https://github.com/triton-inference-server/server) ![](https://img.shields.io/github/stars/triton-inference-server/server.svg?cacheSeconds=86400) - Triton is a high performance open source serving software to deploy AI models from any framework on GPU & CPU while maximizing utilization.\n* [Vercel AI](https://github.com/vercel/ai) ![](https://img.shields.io/github/stars/vercel/ai.svg?cacheSeconds=86400) - Vercel AI is a TypeScript toolkit designed to help you build AI-powered applications using popular frameworks like Next.js, React, Svelte, Vue and runtimes like Node.js.\n* [Vespa](https://github.com/vespa-engine/vespa) ![](https://img.shields.io/github/stars/vespa-engine/vespa.svg?cacheSeconds=86400) - Search, make inferences in and organize vectors, tensors, text and structured data, at serving time and any scale.\n* [vLLM](https://github.com/vllm-project/vllm) ![](https://img.shields.io/github/stars/vllm-project/vllm.svg?cacheSeconds=86400) - vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs.\n\n\n## Evaluation and Monitoring\n* [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval) ![](https://img.shields.io/github/stars/tatsu-lab/alpaca_eval.svg?cacheSeconds=86400) - AlpacaEval is an automatic evaluator for instruction-following language models.\n* [ANN-Benchmarks](https://github.com/erikbern/ann-benchmarks) ![](https://img.shields.io/github/stars/erikbern/ann-benchmarks.svg?cacheSeconds=86400) - ANN-Benchmarks is a benchmarking environment for approximate nearest neighbor algorithms search.\n* [ARES](https://github.com/stanford-futuredata/ARES) ![](https://img.shields.io/github/stars/stanford-futuredata/ARES.svg?cacheSeconds=86400) - ARES is a framework for automatically evaluating Retrieval-Augmented Generation (RAG) models.\n* [BEIR](https://github.com/beir-cellar/beir) ![](https://img.shields.io/github/stars/beir-cellar/beir.svg?cacheSeconds=86400) - BEIR is a heterogeneous benchmark containing diverse IR tasks. It also provides a common and easy framework for evaluation of your NLP-based retrieval models within the benchmark.\n* [Code Generation LM Evaluation Harness](https://github.com/bigcode-project/bigcode-evaluation-harness) ![](https://img.shields.io/github/stars/bigcode-project/bigcode-evaluation-harness.svg?cacheSeconds=86400) - Code Generation LM Evaluation Harness is a framework for the evaluation of code generation models.\n* [COMET](https://github.com/Unbabel/COMET) ![](https://img.shields.io/github/stars/Unbabel/COMET.svg?cacheSeconds=86400) - COMET is an open-source framework for machine learning evaluation.\n* [C-Eval](https://github.com/hkust-nlp/ceval) ![](https://img.shields.io/github/stars/hkust-nlp/ceval.svg?cacheSeconds=86400) - C-Eval is a comprehensive Chinese evaluation suite for foundation models.\n* [Deepchecks](https://github.com/deepchecks/deepchecks) ![](https://img.shields.io/github/stars/deepchecks/deepchecks.svg?cacheSeconds=86400) - Deepchecks is a holistic open-source solution for all of your AI & ML validation needs, enabling you to test your data and models from research to production thoroughly.\n* [DeepEval](https://github.com/confident-ai/deepeval) ![](https://img.shields.io/github/stars/confident-ai/deepeval.svg?cacheSeconds=86400) - DeepEval is a simple-to-use, open-source evaluation framework for LLM applications.\n* [DomainBed](https://github.com/facebookresearch/DomainBed) ![](https://img.shields.io/github/stars/facebookresearch/DomainBed.svg?cacheSeconds=86400) - DomainBed is a test suite containing benchmark datasets and algorithms for domain generalization\n* [EvalAI](https://github.com/Cloud-CV/EvalAI) ![](https://img.shields.io/github/stars/Cloud-CV/EvalAI.svg?cacheSeconds=86400) - EvalAI is an open-source platform for evaluating and comparing AI algorithms at scale.\n* [Evalchemy](https://github.com/mlfoundations/evalchemy) ![](https://img.shields.io/github/stars/mlfoundations/evalchemy.svg?cacheSeconds=86400) - Evalchemy is a unified and easy-to-use toolkit for evaluating post-trained language models.\n* [EvalPlus](https://github.com/evalplus/evalplus) ![](https://img.shields.io/github/stars/evalplus/evalplus.svg?cacheSeconds=86400) - EvalPlus is a robust evaluation framework for LLM4Code, featuring expanded HumanEval+ and MBPP+ benchmarks, efficiency assessment (EvalPerf), and a secure, extensible evaluation toolkit.\n* [Evals](https://github.com/openai/evals) ![](https://img.shields.io/github/stars/openai/evals.svg?cacheSeconds=86400) - Evals is a framework for evaluating OpenAI models and an open-source registry of benchmarks.\n* [EvalScope](https://github.com/modelscope/evalscope) ![](https://img.shields.io/github/stars/modelscope/evalscope.svg?cacheSeconds=86400) - EvalScope is a streamlined and customizable framework for efficient large model evaluation and performance benchmarking.\n* [Evaluate](https://github.com/huggingface/evaluate) ![](https://img.shields.io/github/stars/huggingface/evaluate.svg?cacheSeconds=86400) - Evaluate is a library that makes evaluating and comparing models and reporting their performance easier and more standardized.\n* [Evidently](https://github.com/evidentlyai/evidently) ![](https://img.shields.io/github/stars/evidentlyai/evidently.svg?cacheSeconds=86400) - Evidently is an open-source framework to evaluate, test and monitor ML and LLM-powered systems.\n* [GAOKAO-Bench](https://github.com/OpenLMLab/GAOKAO-Bench) ![](https://img.shields.io/github/stars/OpenLMLab/GAOKAO-Bench.svg?cacheSeconds=86400) - GAOKAO-Bench is an evaluation framework that uses Chinese National College Entrance Examination (GAOKAO) questions as a dataset to assess large models'' language comprehension and logical reasoning abilities.\n* [Giskard](https://github.com/Giskard-AI/giskard)![](https://img.shields.io/github/stars/Giskard-AI/giskard.svg?cacheSeconds=86400) - Giskard is an open-source Python library that automatically detects performance, bias & security issues in AI applications.\n* [HumanEval](https://github.com/openai/human-eval)![](https://img.shields.io/github/stars/openai/human-eval.svg?cacheSeconds=86400) - HumanEval is a benchmark for evaluating the functional correctness of code generation models using Python programming problems with unit tests.\n* [Helicone](https://github.com/Helicone/helicone) ![](https://img.shields.io/github/stars/Helicone/helicone.svg?cacheSeconds=86400) - Helicone is the all-in-one, open-source LLM developer platform.\n* [HELM](https://github.com/stanford-crfm/helm) ![](https://img.shields.io/github/stars/stanford-crfm/helm.svg?cacheSeconds=86400) - HELM (Holistic Evaluation of Language Models) provides tools for the holistic evaluation of language models, including standardized datasets, a unified API for various models, diverse metrics, r, and fairness perturbations, a prompt construction framework, and a proxy server for unified model access.\n* [Inspect](https://github.com/UKGovernmentBEIS/inspect_ai) ![](https://img.shields.io/github/stars/UKGovernmentBEIS/inspect_ai.svg?cacheSeconds=86400) - Inspect is a framework for large language model evaluations.\n* [JiWER](https://github.com/jitsi/jiwer) ![](https://img.shields.io/github/stars/jitsi/jiwer.svg?cacheSeconds=86400) - JiWER is a simple and fast python package to evaluate an automatic speech recognition system. \n* [Laminar](https://github.com/lmnr-ai/lmnr) ![](https://img.shields.io/github/stars/lmnr-ai/lmnr.svg?cacheSeconds=86400) - Laminar is an open-source platform to trace, evaluate, label, and analyze LLM data for AI products.\n* [Langfuse](https://github.com/langfuse/langfuse) ![](https://img.shields.io/github/stars/langfuse/langfuse.svg?cacheSeconds=86400) - Langfuse is an observability & analytics solution for LLM-based applications.\n* [LangTest](https://github.com/JohnSnowLabs/langtest) ![](https://img.shields.io/github/stars/JohnSnowLabs/langtest.svg?cacheSeconds=86400) - LangTest is a comprehensive evaluation toolkit for NLP models.\n* [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) ![](https://img.shields.io/github/stars/EleutherAI/lm-evaluation-harness.svg?cacheSeconds=86400) - Language Model Evaluation Harness is a framework to test generative language models on a large number of different evaluation tasks.\n* [LangWatch](https://github.com/langwatch/langwatch) ![](https://img.shields.io/github/stars/langwatch/langwatch.svg?cacheSeconds=86400) - LangWatch is a visual interface for DSPy and a complete LLM Ops platform for monitoring, experimenting, measuring and improving LLM pipelines, with a fair-code distribution model.\n* [LightEval](https://github.com/huggingface/lighteval) ![](https://img.shields.io/github/stars/huggingface/lighteval.svg?cacheSeconds=86400) - LightEval is a lightweight LLM evaluation suite.\n* [LLMonitor](https://github.com/lunary-ai/lunary) ![](https://img.shields.io/github/stars/lunary-ai/lunary.svg?cacheSeconds=86400) - LLMonitor is an observability & analytics for AI apps and agents.\n* [LLMPerf](https://github.com/ray-project/llmperf) ![](https://img.shields.io/github/stars/ray-project/llmperf.svg?cacheSeconds=86400) - LLMPerf is a tool for evaluating the performance of LLM APIs.\n* [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval) ![](https://img.shields.io/github/stars/EvolvingLMMs-Lab/lmms-eval.svg?cacheSeconds=86400) - lmms-eval is an evaluation framework meticulously crafted for consistent and efficient evaluation of LMM.\n* [Melting Pot](https://github.com/google-deepmind/meltingpot) ![](https://img.shields.io/github/stars/google-deepmind/meltingpot.svg?cacheSeconds=86400) - Melting Pot is a suite of test scenarios for multi-agent reinforcement learning.\n* [Meta-World](https://github.com/Farama-Foundation/Metaworld) ![](https://img.shields.io/github/stars/Farama-Foundation/Metaworld.svg?cacheSeconds=86400) - Meta-World is an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of 50 distinct robotic manipulation tasks.\n* [mir_eval](https://github.com/mir-evaluation/mir_eval) ![](https://img.shields.io/github/stars/mir-evaluation/mir_eval.svg?cacheSeconds=86400) - mir_eval is a Python library which provides a transparent, standardized, and straightforward way to evaluate Music Information Retrieval systems.\n* [MLPerf Inference](https://github.com/mlcommons/inference) ![](https://img.shields.io/github/stars/mlcommons/inference.svg?cacheSeconds=86400) - MLPerf Inference is a benchmark suite for measuring how fast systems can run models in a variety of deployment scenarios.\n* [Massive Text Embedding Benchmark](https://github.com/mlcommons/inference) ![](https://img.shields.io/github/stars/mlcommons/inference.svg?cacheSeconds=86400) - Massive Text Embedding Benchmark (MTEB) is a comprehensive evaluation framework that assesses the performance of text embedding models across diverse tasks and languages, encompassing 8 embedding tasks, 58 datasets, and 112 languages.\n* [NannyML](https://github.com/NannyML/nannyml) ![](https://img.shields.io/github/stars/NannyML/nannyml.svg?cacheSeconds=86400) - NannyML is a library that allows you to estimate post-deployment model performance (without access to targets), detect data drift, and intelligently link data drift alerts back to changes in model performance.\n* [OGB](https://github.com/snap-stanford/ogb) ![](https://img.shields.io/github/stars/snap-stanford/ogb.svg?cacheSeconds=86400) - The Open Graph Benchmark (OGB) is a collection of benchmark datasets, data loaders, and evaluators for graph machine learning.\n* [Ollama Grid Search](https://github.com/dezoito/ollama-grid-search) ![](https://img.shields.io/github/stars/dezoito/ollama-grid-search.svg?cacheSeconds=86400) - Ollama Grid Search automates the process of selecting the best models, prompts, or inference parameters for a given use-case, allowing you to iterate over their combinations and to visually inspect the results.\n* [OpenCompass](https://github.com/open-compass/OpenCompass) ![](https://img.shields.io/github/stars/open-compass/OpenCompass.svg?cacheSeconds=86400) - OpenCompass is an LLM evaluation platform, supporting a wide range of models (LLaMA, LLaMa2, ChatGLM2, ChatGPT, Claude, etc) over 50+ datasets.\n* [OpenLIT](https://github.com/openlit/openlit) ![](https://img.shields.io/github/stars/openlit/openlit.svg?cacheSeconds=86400) - OpenLIT is an open-source AI engineering platform that simplifies LLM workflows with observability, monitoring, guardrails, evaluations, and seamless integrations. \n* [OpenLLMetry](https://github.com/traceloop/openllmetry) ![](https://img.shields.io/github/stars/traceloop/openllmetry.svg?cacheSeconds=86400) - OpenLLMetry provides developers with deep visibility into Large Language Model applications through performance monitoring, execution tracing, and debugging capabilities.\n* [Opik](https://github.com/comet-ml/opik) ![](https://img.shields.io/github/stars/comet-ml/opik.svg?cacheSeconds=86400) - Opik is an open-source platform for evaluating, testing and monitoring LLM applications.\n* [Overcooked-AI](https://github.com/HumanCompatibleAI/overcooked_ai) ![](https://img.shields.io/github/stars/HumanCompatibleAI/overcooked_ai.svg?cacheSeconds=86400) - Overcooked-AI is a benchmark environment for fully cooperative human-AI task performance, based on the wildly popular video game Overcooked.\n* [Phoenix](https://github.com/Arize-ai/phoenix) ![](https://img.shields.io/github/stars/Arize-ai/phoenix.svg?cacheSeconds=86400) - Phoenix is an open-source AI observability platform designed for experimentation, evaluation, and troubleshooting.\n* [PromptBench](https://github.com/microsoft/promptbench) ![](https://img.shields.io/github/stars/microsoft/promptbench.svg?cacheSeconds=86400) - PromptBench is a unified evaluation framework for large language models\n* [Promptfoo](https://github.com/promptfoo/promptfoo) ![](https://img.shields.io/github/stars/promptfoo/promptfoo.svg?cacheSeconds=86400) - Promptfoo is a developer-friendly local tool for testing LLM applications. \n* [Prometheus-Eval](https://github.com/prometheus-eval/prometheus-eval) ![](https://img.shields.io/github/stars/prometheus-eval/prometheus-eval.svg?cacheSeconds=86400) - RagaAI Catalyst is a comprehensive platform designed to enhance the management and optimization of LLM projects. \n* [RagaAI Catalyst](https://github.com/raga-ai-hub/RagaAI-Catalyst) ![](https://img.shields.io/github/stars/raga-ai-hub/RagaAI-Catalyst.svg?cacheSeconds=86400) - Prometheus-Eval is a collection of tools for training, evaluating, and using language models specialized in evaluating other language models.\n* [Ragas](https://github.com/explodinggradients/ragas) ![](https://img.shields.io/github/stars/explodinggradients/ragas.svg?cacheSeconds=86400) - Ragas is a framework to evaluate RAG pipelines.\n* [RAGChecker](https://github.com/amazon-science/RAGChecker) ![](https://img.shields.io/github/stars/amazon-science/RAGChecker.svg?cacheSeconds=86400) - RAGChecker is an advanced automatic evaluation framework designed to assess and diagnose Retrieval-Augmented Generation (RAG) systems.\n* [RewardBench](https://github.com/allenai/reward-bench) ![](https://img.shields.io/github/stars/allenai/reward-bench.svg?cacheSeconds=86400) - RewardBench is a benchmark designed to evaluate the capabilities and safety of reward models.\n* [RLBench](https://github.com/stepjam/RLBench) ![](https://img.shields.io/github/stars/stepjam/RLBench.svg?cacheSeconds=86400) - RLBench is an ambitious large-scale benchmark and learning environment designed to facilitate research in a number of vision-guided manipulation research areas, including: reinforcement learning, imitation learning, multi-task learning, geometric computer vision, and in particular, few-shot learning.\n* [SimplerEnv](https://github.com/simpler-env/SimplerEnv) ![](https://img.shields.io/github/stars/simpler-env/SimplerEnv.svg?cacheSeconds=86400) - SimplerEnv is a simulated manipulation policy evaluation environments for real robot setups.\n* [SwanLab](https://github.com/SwanHubX/SwanLab) ![](https://img.shields.io/github/stars/SwanHubX/SwanLab.svg?cacheSeconds=86400) - SwanLab is an AI training tracking and visualization tool.\n* [Speech-to-Text Benchmark](https://github.com/Picovoice/speech-to-text-benchmark) ![](https://img.shields.io/github/stars/Picovoice/speech-to-text-benchmark.svg?cacheSeconds=86400) - Speech-to-Text Benchmark is a minimalist and extensible framework for benchmarking different speech-to-text engines.\n* [TensorFlow Model Analysis](https://github.com/tensorflow/model-analysis) ![](https://img.shields.io/github/stars/tensorflow/model-analysis.svg?cacheSeconds=86400) - TensorFlow Model Analysis (TFMA) is a library for evaluating TensorFlow models on large amounts of data in a distributed manner, using the same metrics defined in their trainer.\n* [TorchBench](https://github.com/pytorch/benchmark) ![](https://img.shields.io/github/stars/pytorch/benchmark.svg?cacheSeconds=86400) - TorchBench is a collection of open source benchmarks used to evaluate PyTorch performance.\n* [TruLens](https://github.com/truera/trulens) ![](https://img.shields.io/github/stars/truera/trulens.svg?cacheSeconds=86400) - TruLens provides a set of tools for evaluating and tracking LLM experiments.\n* [TrustLLM](https://github.com/HowieHwong/TrustLLM) ![](https://img.shields.io/github/stars/HowieHwong/TrustLLM.svg?cacheSeconds=86400) - TrustLLM is a comprehensive framework to evaluate the trustworthiness of large language models, which includes principles, surveys, and benchmarks.\n* [VBench](https://github.com/Vchitect/VBench) ![](https://img.shields.io/github/stars/Vchitect/VBench.svg?cacheSeconds=86400) - VBench is a comprehensive benchmark suite for video generative models.\n* [VLMEvalKit](https://github.com/open-compass/VLMEvalKit) ![](https://img.shields.io/github/stars/open-compass/VLMEvalKit.svg?cacheSeconds=86400) - VLMEvalKit is an open-source evaluation toolkit of large vision-language models (LVLMs).\n\n## Explainability and Fairness\n* [Aequitas](https://github.com/dssg/aequitas) ![](https://img.shields.io/github/stars/dssg/aequitas.svg?cacheSeconds=86400) - An open-source bias audit toolkit for data scientists, machine learning researchers, and policymakers to audit machine learning models for discrimination and bias, and to make informed and equitable decisions around developing and deploying predictive risk-assessment tools.\n* [AI Explainability 360](https://github.com/Trusted-AI/AIX360) ![](https://img.shields.io/github/stars/Trusted-AI/AIX360.svg?cacheSeconds=86400) - Interpretability and explainability of data and machine learning models including a comprehensive set of algorithms that cover different dimensions of explanations along with proxy explainability metrics.\n* [AI Fairness 360](https://github.com/Trusted-AI/AIF360) ![](https://img.shields.io/github/stars/Trusted-AI/AIF360.svg?cacheSeconds=86400) - A comprehensive set of fairness metrics for datasets and machine learning models, explanations for these metrics, and algorithms to mitigate bias in datasets and models.\n* [Alibi](https://github.com/SeldonIO/alibi) ![](https://img.shields.io/github/stars/SeldonIO/alibi.svg?cacheSeconds=86400) - Alibi is an open source Python library aimed at machine learning model inspection and interpretation. The initial focus on the library is on black-box, instance based model explanations.\n* [captum](https://github.com/pytorch/captum) ![](https://img.shields.io/github/stars/pytorch/captum.svg?cacheSeconds=86400) - model interpretability and understanding library for PyTorch developed by Facebook. It contains general purpose implementations of integrated gradients, saliency maps, smoothgrad, vargrad and others for PyTorch models.\n* [Fairlearn](https://github.com/fairlearn/fairlearn) ![](https://img.shields.io/github/stars/fairlearn/fairlearn.svg?cacheSeconds=86400) - Fairlearn is a python toolkit to assess and mitigate unfairness in machine learning models.\n* [InterpretML](https://github.com/interpretml/interpret) ![](https://img.shields.io/github/stars/interpretml/interpret.svg?cacheSeconds=86400) - InterpretML is an open-source package for training interpretable models and explaining blackbox systems.\n* [Lightly](https://github.com/lightly-ai/lightly) ![](https://img.shields.io/github/stars/lightly-ai/lightly.svg?cacheSeconds=86400) - A python framework for self-supervised learning on images. The learned representations can be used to analyze the distribution in unlabeled data and rebalance datasets.\n* [LOFO Importance](https://github.com/aerdem4/lofo-importance) ![](https://img.shields.io/github/stars/aerdem4/lofo-importance.svg?cacheSeconds=86400) - LOFO (Leave One Feature Out) Importance calculates the importances of a set of features based on a metric of choice, for a model of choice, by iteratively removing each feature from the set, and evaluating the performance of the model, with a validation scheme of choice, based on the chosen metric.\n* [mljar-supervised](https://github.com/mljar/mljar-supervised) ![](https://img.shields.io/github/stars/mljar/mljar-supervised.svg?cacheSeconds=86400) - A Python package for AutoML on tabular data with feature engineering, hyper-parameters tuning, explanations and automatic documentation.\n* [Quantus](https://github.com/understandable-machine-intelligence-lab/Quantus) ![](https://img.shields.io/github/stars/understandable-machine-intelligence-lab/Quantus.svg?cacheSeconds=86400) - Quantus is an eXplainable AI toolkit for responsible evaluation of neural network explanations\n* [SHAP](https://github.com/shap/shap) ![](https://img.shields.io/github/stars/shap/shap.svg?cacheSeconds=86400) - SHapley Additive exPlanations is a unified approach to explain the output of any machine learning model.\n* [SHAPash](https://github.com/MAIF/shapash) ![](https://img.shields.io/github/stars/MAIF/shapash.svg?cacheSeconds=86400) - Shapash is a Python library that provides several types of visualization that display explicit labels that everyone can understand.\n* [WhatIf](https://github.com/pair-code/what-if-tool) ![](https://img.shields.io/github/stars/pair-code/what-if-tool.svg?cacheSeconds=86400) - An easy-to-use interface for expanding understanding of a black-box classification or regression ML model.\n\n## Feature Store\n* [FEAST](https://github.com/feast-dev/feast)  ![](https://img.shields.io/github/stars/feast-dev/feast.svg?cacheSeconds=86400) - Feast (Feature Store) is an open source feature store for machine learning. Feast is the fastest path to manage existing infrastructure to productionize analytic data for model training and online inference.\n* [Featureform](https://github.com/featureform/featureform) ![](https://img.shields.io/github/stars/featureform/featureform.svg?cacheSeconds=86400) - A virtual featurestore. Plug-&-play with your existing infra. Data Scientist approved. Discovery, Governance, Lineage, & Collaboration just a pip install away. Supports pandas, Python, spark, SQL + integrations with major cloud vendors. \n* [Hopsworks Feature Store](https://github.com/logicalclocks/hopsworks) ![](https://img.shields.io/github/stars/logicalclocks/hopsworks.svg?cacheSeconds=86400) - Offline/Online Feature Store for ML [(Video)](https://www.youtube.com/watch?v=N1BjPk1smdg).\n\n## Industry-strength Anomaly Detection\n* [Alibi Detect](https://github.com/SeldonIO/alibi-detect) ![](https://img.shields.io/github/stars/SeldonIO/alibi-detect.svg?cacheSeconds=86400) - alibi-detect is a Python package focused on outlier, adversarial and concept drift detection.\n* [Darts](https://github.com/unit8co/darts) ![](https://img.shields.io/github/stars/unit8co/darts.svg?cacheSeconds=86400) - Darts is a library for user-friendly forecasting and anomaly detection on time series.\n* [Deequ](https://github.com/awslabs/deequ) ![](https://img.shields.io/github/stars/awslabs/deequ.svg?cacheSeconds=86400) - A library built on top of Apache Spark for defining "unit tests for data", which measure data quality in large datasets.\n* [PyOD](https://github.com/yzhao062/pyod) ![](https://img.shields.io/github/stars/yzhao062/pyod.svg?cacheSeconds=86400) - A Python Toolbox for Scalable Outlier Detection (Anomaly Detection).\n* [TFDV](https://github.com/tensorflow/data-validation) ![](https://img.shields.io/github/stars/tensorflow/data-validation.svg?cacheSeconds=86400) - TFDV (Tensorflow Data Validation) is a library for exploring and validating machine learning data.\n\n## Industry Strength Computer Vision\n* [Deep Lake](https://github.com/activeloopai/deeplake) ![](https://img.shields.io/github/stars/activeloopai/deeplake.svg?cacheSeconds=86400) - Deep Lake is a data infrastructure optimized for computer vision.\n* [Detectron2](https://github.com/facebookresearch/detectron2) ![](https://img.shields.io/github/stars/facebookresearch/detectron2.svg?cacheSeconds=86400) - Detectron2 is Facebook AI Research''s next generation library that provides state-of-the-art detection and segmentation algorithms.\n* [KerasCV](https://github.com/keras-team/keras-cv) ![](https://img.shields.io/github/stars/keras-team/keras-cv.svg?cacheSeconds=86400) - KerasCV is a library of modular computer vision oriented Keras components.\n* [Kornia](https://github.com/kornia/kornia) ![](https://img.shields.io/github/stars/kornia/kornia.svg?cacheSeconds=86400) - Kornia is a differentiable computer vision library built on PyTorch that provides a rich set of differentiable image processing and geometric vision algorithms.\n* [LAVIS](https://github.com/salesforce/LAVIS) ![](https://img.shields.io/github/stars/salesforce/LAVIS.svg?cacheSeconds=86400) - LAVIS is a deep learning library for LAnguage-and-VISion intelligence research and applications.\n* [libcom](https://github.com/bcmi/libcom) ![](https://img.shields.io/github/stars/bcmi/libcom.svg?cacheSeconds=86400) - libcom is an image composition toolbox.\n* [LightlyTrain](https://github.com/lightly-ai/lightly-train) ![](https://img.shields.io/github/stars/lightly-ai/lightly-train.svg?cacheSeconds=86400) - Pretrain computer vision models on unlabeled data for industrial applications.\n* [MMCV](https://github.com/open-mmlab/mmcv) ![](https://img.shields.io/github/stars/open-mmlab/mmcv.svg?cacheSeconds=86400) - MMCV is a foundational computer vision library from OpenMMLab that provides essential functionalities like image and video processing, data transformation and augmentation, CNN architectures, and optimized CUDA operations.\n* [SuperGradients](https://github.com/Deci-AI/super-gradients) ![](https://img.shields.io/github/stars/Deci-AI/super-gradients.svg?cacheSeconds=86400) - SuperGradients is an open-source library for training PyTorch-based computer vision models.\n* [supervision](https://github.com/roboflow/supervision) ![](https://img.shields.io/github/stars/roboflow/supervision.svg?cacheSeconds=86400) - Supervision is a Python library designed for efficient computer vision pipeline management, providing tools for annotation, visualization, and monitoring of models.\n* [VideoSys](https://github.com/NUS-HPC-AI-Lab/VideoSys) ![](https://img.shields.io/github/stars/NUS-HPC-AI-Lab/VideoSys.svg?cacheSeconds=86400) - VideoSys supports many diffusion models with our various acceleration techniques, enabling these models to run faster and consume less memory.\n\n## Industry Strength Information Retrieval\n* [AutoRAG](https://github.com/Marker-Inc-Korea/AutoRAG) ![](https://img.shields.io/github/stars/Marker-Inc-Korea/AutoRAG.svg?cacheSeconds=86400) - AutoRAG is a RAG AutoML tool for automatically finds an optimal RAG pipeline for your data.\n* [BGE](https://github.com/FlagOpen/FlagEmbedding) ![](https://img.shields.io/github/stars/FlagOpen/FlagEmbedding.svg?cacheSeconds=86400) - BGE builds one-stop retrieval toolkit for search and RAG.\n* [Cognita](https://github.com/truefoundry/cognita) ![](https://img.shields.io/github/stars/truefoundry/cognita.svg?cacheSeconds=86400) - Cognita is a RAG framework for building modular and production-ready applications.\n* [DocArray](https://github.com/docarray/docarray) ![](https://img.shields.io/github/stars/docarray/docarray.svg?cacheSeconds=86400) - DocArray is a library for nested, unstructured, multimodal data in transit, including text, image, audio, video, 3D mesh, etc. It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer multimodal data with a Pythonic API.\n* [Faiss](https://github.com/facebookresearch/faiss) ![](https://img.shields.io/github/stars/facebookresearch/faiss.svg?cacheSeconds=86400) - Faiss is a library for efficient similarity search and clustering of dense vectors.\n* [fastRAG](https://github.com/IntelLabs/fastRAG) ![](https://img.shields.io/github/stars/IntelLabs/fastRAG.svg?cacheSeconds=86400) - fastRAG is a research framework for efficient and optimized retrieval augmented generative pipelines, incorporating state-of-the-art LLMs and Information Retrieval.\n* [GraphRAG](https://github.com/microsoft/graphrag) ![](https://img.shields.io/github/stars/microsoft/graphrag.svg?cacheSeconds=86400) - GraphRAG is a data pipeline and transformation suite that is designed to extract meaningful, structured data from unstructured text using the power of LLMs.\n* [HippoRAG](https://github.com/OSU-NLP-Group/HippoRAG) ![](https://img.shields.io/github/stars/OSU-NLP-Group/HippoRAG.svg?cacheSeconds=86400) - HippoRAG is a novel retrieval augmented generation (RAG) framework inspired by the neurobiology of human long-term memory that enables LLMs to continuously integrate knowledge across external documents.\n* [JamAI Base](https://github.com/EmbeddedLLM/JamAIBase) ![](https://img.shields.io/github/stars/EmbeddedLLM/JamAIBase.svg?cacheSeconds=86400) - JamAI Base is an open-source RAG (Retrieval-Augmented Generation) backend platform that integrates an embedded database (SQLite) and an embedded vector database (LanceDB) with managed memory and RAG capabilities. It features built-in LLM, vector embeddings, and reranker orchestration and management, all accessible through a convenient, intuitive, spreadsheet-like UI and a simple REST API.\n* [LangExtract](https://github.com/google/langextract) ![](https://img.shields.io/github/stars/google/langextract.svg?cacheSeconds=86400) - LangExtract is a Python library that uses LLMs to extract structured information from unstructured text documents based on user-defined instructions. It processes materials such as clinical notes or reports, identifying and organizing key details while ensuring the extracted data corresponds to the source text.\n* [LightRAG](https://github.com/HKUDS/LightRAG) ![](https://img.shields.io/github/stars/HKUDS/LightRAG.svg?cacheSeconds=86400) - A simple and fast retrieval-augmented generation framework.\n* [llmware](https://github.com/llmware-ai/llmware) ![](https://img.shields.io/github/stars/llmware-ai/llmware.svg?cacheSeconds=86400) - llmware provides a unified framework for building LLM-based applications (e.g, RAG, Agents), using small, specialized models that can be deployed privately, integrated with enterprise knowledge sources safely and securely, and cost-effectively tuned and adapted for any business process.\n* [Mem0](https://github.com/mem0ai/mem0) ![](https://img.shields.io/github/stars/mem0ai/mem0.svg?cacheSeconds=86400) - Mem0 enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions.\n* [NGT](https://github.com/yahoojapan/NGT) ![](https://img.shields.io/github/stars/yahoojapan/NGT.svg?cacheSeconds=86400) - NGT provides commands and a library for performing high-speed approximate nearest neighbor searches against a large volume of data in high dimensional vector data space.\n* [NMSLIB](https://github.com/nmslib/nmslib) ![](https://img.shields.io/github/stars/nmslib/nmslib.svg?cacheSeconds=86400) - Non-Metric Space Library (NMSLIB): An efficient similarity search library and a toolkit for evaluation of k-NN methods for generic non-metric spaces.\n* [Qdrant](https://github.com/qdrant/qdrant) ![](https://img.shields.io/github/stars/qdrant/qdrant.svg?cacheSeconds=86400) - An open source vector similarity search engine with extended filtering support.\n* [R2R](https://github.com/SciPhi-AI/R2R) ![](https://img.shields.io/github/stars/SciPhi-AI/R2R.svg?cacheSeconds=86400) - R2R (RAG to Riches) is a comprehensive platform for building, deploying, and scaling RAG applications with hybrid search, multimodal support, and advanced observability.\n* [RAGFlow](https://github.com/infiniflow/ragflow) ![](https://img.shields.io/github/stars/infiniflow/ragflow.svg?cacheSeconds=86400) - RAGFlow is a RAG engine based on deep document understanding.\n* [RAGxplorer](https://github.com/gabrielchua/RAGxplorer) ![](https://img.shields.io/github/stars/gabrielchua/RAGxplorer.svg?cacheSeconds=86400) - RAGxplorer is a tool to build RAG visualisations.\n* [RAG-FiT](https://github.com/IntelLabs/RAG-FiT) ![](https://img.shields.io/github/stars/IntelLabs/RAG-FiT.svg?cacheSeconds=86400) - RAG-FiT is a library designed to improve LLMs ability to use external information by fine-tuning models on specially created RAG-augmented datasets.\n* [TextWorld](https://github.com/microsoft/TextWorld) ![](https://img.shields.io/github/stars/microsoft/TextWorld.svg?cacheSeconds=86400) - TextWorld is a text-based game generator and extensible sandbox learning environment for training and testing reinforcement learning (RL) agents.\n* [Vanna](https://github.com/vanna-ai/vanna) ![](https://img.shields.io/github/stars/vanna-ai/vanna.svg?cacheSeconds=86400) - Vanna is a RAG framework for SQL generation and related functionality.\n\n## Industry Strength Natural Language Processing\n* [aisuite](https://github.com/andrewyng/aisuite) ![](https://img.shields.io/github/stars/andrewyng/aisuite.svg?cacheSeconds=86400) - aisuite is a simple, unified interface to multiple generative AI providers.\n* [Align-Anything](https://github.com/PKU-Alignment/align-anything) ![](https://img.shields.io/github/stars/PKU-Alignment/align-anything.svg?cacheSeconds=86400) - Align-Anything aims to align any modality large models (any-to-any models), including LLMs, VLMs, and others, with human intentions and values\n* [BERTopic](https://github.com/MaartenGr/BERTopic) ![](https://img.shields.io/github/stars/MaartenGr/BERTopic.svg?cacheSeconds=86400) - BERTopic is a topic modeling technique that leverages transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions.\n* [Burr](https://github.com/dagworks-inc/burr) ![](https://img.shields.io/github/stars/dagworks-inc/burr.svg?cacheSeconds=86400) - Burr helps you develop applications that make decisions (chatbot, agent, simulation). It comes with production-ready features (telemetry, persistence, deployment, etc.) and the open-source, free, and local-first Burr UI.\n* [CodeTF](https://github.com/salesforce/CodeTF) ![](https://img.shields.io/github/stars/salesforce/CodeTF.svg?cacheSeconds=86400) - CodeTF is a one-stop Python transformer-based library for code large language models (Code LLMs) and code intelligence, provides a seamless interface for training and inferencing on code intelligence tasks like code summarization, translation, code generation and so on. \n* [Dify](https://github.com/langgenius/dify) ![](https://img.shields.io/github/stars/langgenius/dify.svg?cacheSeconds=86400) - Dify is an open-source LLM app development platform whose intuitive interface combines agentic AI workflow, RAG pipeline, agent capabilities, model management, observability features and more, letting you quickly go from prototype to production.\n* [dspy](https://github.com/stanfordnlp/dspy) ![](https://img.shields.io/github/stars/stanfordnlp/dspy.svg?cacheSeconds=86400) - A framework for programming with foundation models.\n* [Dust](https://github.com/dust-tt/dust) ![](https://img.shields.io/github/stars/dust-tt/dust.svg?cacheSeconds=86400) - Dust assists in the design and deployment of large language model apps.\n* [ESPnet](https://github.com/espnet/espnet) ![](https://img.shields.io/github/stars/espnet/espnet.svg?cacheSeconds=86400) - ESPnet is an end-to-end speech processing toolkit.\n* [FastChat](https://github.com/lm-sys/FastChat) ![](https://img.shields.io/github/stars/lm-sys/FastChat.svg?cacheSeconds=86400) - FastChat is an open platform for training, serving, and evaluating large language model based chatbots.\n* [Flair](https://github.com/flairNLP/flair) ![](https://img.shields.io/github/stars/flairNLP/flair.svg?cacheSeconds=86400) - Simple framework for state-of-the-art NLP developed by Zalando which builds directly on PyTorch.\n* [Gensim](https://github.com/piskvorky/gensim) ![](https://img.shields.io/github/stars/piskvorky/gensim.svg?cacheSeconds=86400) - Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.\n* [gpt-fast](https://github.com/meta-pytorch/gpt-fast) ![](https://img.shields.io/github/stars/meta-pytorch/gpt-fast.svg?cacheSeconds=86400) - Simple and efficient pytorch-native transformer text generation.\n* [h2oGPT](https://github.com/h2oai/h2ogpt) ![](https://img.shields.io/github/stars/h2oai/h2ogpt.svg?cacheSeconds=86400) - h2oGPT is an open source generative AI, gives organizations like yours the power to own large language models while preserving your data ownership.\n* [Haystack](https://github.com/deepset-ai/haystack) ![](https://img.shields.io/github/stars/deepset-ai/haystack.svg?cacheSeconds=86400) - Haystack is an open source NLP framework to interact with your data using Transformer models and LLMs (GPT-3 and alike). Haystack offers production-ready tools to quickly build ChatGPT-like question answering, semantic search, text generation, and more.\n* [Interactive Composition Explorer](https://github.com/oughtinc/ice) ![](https://img.shields.io/github/stars/oughtinc/ice.svg?cacheSeconds=86400) - ICE is a Python library and trace visualizer for language model programs.\n* [Lamini](https://github.com/lamini-ai/lamini) ![](https://img.shields.io/github/stars/lamini-ai/lamini.svg?cacheSeconds=86400) - Lamini is an LLM engine for rapidly customizing models.\n* [LangChain](https://github.com/langchain-ai/langchain) ![](https://img.shields.io/github/stars/langchain-ai/langchain.svg?cacheSeconds=86400) - LangChain assists in building applications with LLMs through composability.\n* [LlamaIndex](https://github.com/run-llama/llama_index) ![](https://img.shields.io/github/stars/run-llama/llama_index.svg?cacheSeconds=86400) - LlamaIndex (GPT Index) is a data framework for your LLM application.\n* [LLaMA](https://github.com/meta-llama/llama) ![](https://img.shields.io/github/stars/meta-llama/llama.svg?cacheSeconds=86400) - LLaMA is intended as a minimal, hackable and readable example to load LLaMA (arXiv) models and run inference.\n* [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) ![](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory.svg?cacheSeconds=86400) - LLaMA-Factory makes it easy to fine-tunes 100+ large language models with zero-code CLI and Web UI\n* [LLMBox](https://github.com/Alpha-VLLM/LLaMA2-Accessory) ![](https://img.shields.io/github/stars/Alpha-VLLM/LLaMA2-Accessory.svg?cacheSeconds=86400) - LLMBox is a comprehensive library for implementing LLMs, including a unified training pipeline and comprehensive model evaluation.\n* [LLaMA2-Accessory](https://github.com/RUCAIBox/LLMBox) ![](https://img.shields.io/github/stars/RUCAIBox/LLMBox.svg?cacheSeconds=86400) - LLaMA2-Accessory is an open-source toolkit for pretraining, finetuning and deployment of Large Language Models (LLMs) and multimodal LLMs.\n* [LMFlow](https://github.com/OptimalScale/LMFlow) ![](https://img.shields.io/github/stars/OptimalScale/LMFlow.svg?cacheSeconds=86400) - LMFlow is an extensible, convenient, and efficient toolbox for finetuning large machine learning models.\n* [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) ![](https://img.shields.io/github/stars/NVIDIA/Megatron-LM.svg?cacheSeconds=86400) - Megatron-LM is a highly optimized and efficient library for training large language models.\n* [MindNLP](https://github.com/mindspore-lab/mindnlp) ![](https://img.shields.io/github/stars/mindspore-lab/mindnlp.svg?cacheSeconds=86400) - MindNLP is an easy-to-use and high-performance NLP and LLM framework based on MindSpore, compatible with models and datasets of Huggingface.\n* [MLC LLM](https://github.com/mlc-ai/mlc-llm) ![](https://img.shields.io/github/stars/mlc-ai/mlc-llm.svg?cacheSeconds=86400) - MLC LLM is a universal solution that allows any language models to be deployed natively on a diverse set of hardware backends and native applications, plus a productive framework for everyone to further optimize model performance for their own use cases.\n* [Ollam', '{"language":null,"stars":19709,"forks":2479,"watchers":19709,"open_issues":5,"topics":["awesome","awesome-list","data-mining","deep-learning","explainability","interpretability","large-scale-machine-learning","large-scale-ml","machine-learning","machine-learning-operations","ml-operations","ml-ops","mlops","privacy-preserving","privacy-preserving-machine-learning","privacy-preserving-ml","production-machine-learning","production-ml","responsible-ai"],"default_branch":"master","size_kb":2865,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:EthicalML:awesome-production-machine-learning","source_url":"https://github.com/EthicalML/awesome-production-machine-learning"},{"type":"has_code","target_id":"github:EthicalML:awesome-production-machine-learning","source_url":"https://github.com/EthicalML/awesome-production-machine-learning"},{"type":"has_code","target_id":"github:EthicalML:awesome-production-genai","source_url":"https://github.com/EthicalML/awesome-production-genai"},{"type":"has_code","target_id":"github:EthicalML:awesome-production-genai","source_url":"https://github.com/EthicalML/awesome-production-genai"},{"type":"has_code","target_id":"github:autogluon:autogluon","source_url":"https://github.com/autogluon/autogluon"},{"type":"has_code","target_id":"github:keras-team:autokeras","source_url":"https://github.com/keras-team/autokeras"},{"type":"has_code","target_id":"github:automl:auto-sklearn","source_url":"https://github.com/automl/auto-sklearn"},{"type":"has_code","target_id":"github:facebook:Ax","source_url":"https://github.com/facebook/Ax"},{"type":"has_code","target_id":"github:meta-pytorch:botorch","source_url":"https://github.com/meta-pytorch/botorch"},{"type":"has_code","target_id":"github:alteryx:evalml","source_url":"https://github.com/alteryx/evalml"},{"type":"has_code","target_id":"github:feature-engine:feature_engine","source_url":"https://github.com/feature-engine/feature_engine"},{"type":"has_code","target_id":"github:alteryx:featuretools","source_url":"https://github.com/alteryx/featuretools"},{"type":"has_code","target_id":"github:microsoft:FLAML","source_url":"https://github.com/microsoft/FLAML"},{"type":"has_code","target_id":"github:huawei-noah:HEBO","source_url":"https://github.com/huawei-noah/HEBO"},{"type":"has_code","target_id":"github:kubeflow:katib","source_url":"https://github.com/kubeflow/katib"},{"type":"has_code","target_id":"github:keras-team:keras-tuner","source_url":"https://github.com/keras-team/keras-tuner"},{"type":"has_code","target_id":"github:optuna:optuna","source_url":"https://github.com/optuna/optuna"},{"type":"has_code","target_id":"github:google:vizier","source_url":"https://github.com/google/vizier"},{"type":"has_code","target_id":"github:epistasislab:tpot","source_url":"https://github.com/epistasislab/tpot"},{"type":"has_code","target_id":"github:blue-yonder:tsfresh","source_url":"https://github.com/blue-yonder/tsfresh"},{"type":"has_code","target_id":"github:huggingface:accelerate","source_url":"https://github.com/huggingface/accelerate"},{"type":"has_code","target_id":"github:adapter-hub:adapters","source_url":"https://github.com/adapter-hub/adapters"},{"type":"has_code","target_id":"github:microsoft:BitBLAS","source_url":"https://github.com/microsoft/BitBLAS"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:mosaicml:composer","source_url":"https://github.com/mosaicml/composer"},{"type":"has_code","target_id":"github:rapidsai:cudf","source_url":"https://github.com/rapidsai/cudf"},{"type":"has_code","target_id":"github:rapidsai:cuml","source_url":"https://github.com/rapidsai/cuml"},{"type":"has_code","target_id":"github:cupy:cupy","source_url":"https://github.com/cupy/cupy"},{"type":"has_code","target_id":"github:DEAP:deap","source_url":"https://github.com/DEAP/deap"},{"type":"has_code","target_id":"github:deepseek-ai:DeepEP","source_url":"https://github.com/deepseek-ai/DeepEP"},{"type":"has_code","target_id":"github:dmlc:dgl","source_url":"https://github.com/dmlc/dgl"},{"type":"has_code","target_id":"github:intelligent-machine-learning:dlrover","source_url":"https://github.com/intelligent-machine-learning/dlrover"},{"type":"has_code","target_id":"github:dask:dask","source_url":"https://github.com/dask/dask"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:FlagOpen:FlagGems","source_url":"https://github.com/FlagOpen/FlagGems"},{"type":"has_code","target_id":"github:flashlight:flashlight","source_url":"https://github.com/flashlight/flashlight"},{"type":"has_code","target_id":"github:google:flax","source_url":"https://github.com/google/flax"},{"type":"has_code","target_id":"github:gpustack:gpustack","source_url":"https://github.com/gpustack/gpustack"},{"type":"has_code","target_id":"github:learning-at-home:hivemind","source_url":"https://github.com/learning-at-home/hivemind"},{"type":"has_code","target_id":"github:horovod:horovod","source_url":"https://github.com/horovod/horovod"},{"type":"has_code","target_id":"github:jax-ml:jax","source_url":"https://github.com/jax-ml/jax"},{"type":"has_code","target_id":"github:lava-nc:lava","source_url":"https://github.com/lava-nc/lava"},{"type":"has_code","target_id":"github:KomputeProject:kompute","source_url":"https://github.com/KomputeProject/kompute"},{"type":"has_code","target_id":"github:linkedin:Liger-Kernel","source_url":"https://github.com/linkedin/Liger-Kernel"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:ml-explore:mlx","source_url":"https://github.com/ml-explore/mlx"},{"type":"has_code","target_id":"github:modin-project:modin","source_url":"https://github.com/modin-project/modin"},{"type":"has_code","target_id":"github:NVIDIA:TensorRT","source_url":"https://github.com/NVIDIA/TensorRT"},{"type":"has_code","target_id":"github:facebookresearch:nevergrad","source_url":"https://github.com/facebookresearch/nevergrad"},{"type":"has_code","target_id":"github:norse:norse","source_url":"https://github.com/norse/norse"},{"type":"has_code","target_id":"github:numba:numba","source_url":"https://github.com/numba/numba"},{"type":"has_code","target_id":"github:huggingface:optimum","source_url":"https://github.com/huggingface/optimum"},{"type":"has_code","target_id":"github:huggingface:peft","source_url":"https://github.com/huggingface/peft"},{"type":"has_code","target_id":"github:PaddlePaddle:Paddle","source_url":"https://github.com/PaddlePaddle/Paddle"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:Lightning-AI:pytorch-lightning","source_url":"https://github.com/Lightning-AI/pytorch-lightning"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:ray-project:ray","source_url":"https://github.com/ray-project/ray"},{"type":"has_code","target_id":"github:huggingface:setfit","source_url":"https://github.com/huggingface/setfit"},{"type":"has_code","target_id":"github:google-deepmind:sonnet","source_url":"https://github.com/google-deepmind/sonnet"},{"type":"has_code","target_id":"github:mosaicml:streaming","source_url":"https://github.com/mosaicml/streaming"},{"type":"has_code","target_id":"github:tensorflow:tensorflow","source_url":"https://github.com/tensorflow/tensorflow"},{"type":"has_code","target_id":"github:HazyResearch:ThunderKittens","source_url":"https://github.com/HazyResearch/ThunderKittens"},{"type":"has_code","target_id":"github:metaopt:torchopt","source_url":"https://github.com/metaopt/torchopt"},{"type":"has_code","target_id":"github:triton-lang:triton","source_url":"https://github.com/triton-lang/triton"},{"type":"has_code","target_id":"github:vaexio:vaex","source_url":"https://github.com/vaexio/vaex"},{"type":"has_code","target_id":"github:VowpalWabbit:vowpal_wabbit","source_url":"https://github.com/VowpalWabbit/vowpal_wabbit"},{"type":"has_code","target_id":"github:dmlc:xgboost","source_url":"https://github.com/dmlc/xgboost"},{"type":"has_code","target_id":"github:google:yggdrasil-decision-forests","source_url":"https://github.com/google/yggdrasil-decision-forests"},{"type":"has_code","target_id":"github:bitsandbytes-foundation:bitsandbytes","source_url":"https://github.com/bitsandbytes-foundation/bitsandbytes"},{"type":"has_code","target_id":"github:arogozhnikov:einops","source_url":"https://github.com/arogozhnikov/einops"},{"type":"has_code","target_id":"github:scikit-learn:scikit-learn","source_url":"https://github.com/scikit-learn/scikit-learn"},{"type":"has_code","target_id":"github:jeshraghian:snntorch","source_url":"https://github.com/jeshraghian/snntorch"},{"type":"has_code","target_id":"github:yoshitomo-matsubara:torchdistill","source_url":"https://github.com/yoshitomo-matsubara/torchdistill"},{"type":"has_code","target_id":"github:lyhue1991:torchkeras","source_url":"https://github.com/lyhue1991/torchkeras?tab=readme-ov-file"},{"type":"has_code","target_id":"github:volcengine:veScale","source_url":"https://github.com/volcengine/veScale"},{"type":"has_code","target_id":"github:DistrictDataLabs:yellowbrick","source_url":"https://github.com/DistrictDataLabs/yellowbrick"},{"type":"has_code","target_id":"github:argilla-io:argilla","source_url":"https://github.com/argilla-io/argilla"},{"type":"has_code","target_id":"github:cleanlab:cleanlab","source_url":"https://github.com/cleanlab/cleanlab"},{"type":"has_code","target_id":"github:jsbroks:coco-annotator","source_url":"https://github.com/jsbroks/coco-annotator"},{"type":"has_code","target_id":"github:cvat-ai:cvat","source_url":"https://github.com/cvat-ai/cvat"},{"type":"has_code","target_id":"github:doccano:doccano","source_url":"https://github.com/doccano/doccano"},{"type":"has_code","target_id":"github:gretelai:gretel-synthetics","source_url":"https://github.com/gretelai/gretel-synthetics"},{"type":"has_code","target_id":"github:HumanSignal:label-studio","source_url":"https://github.com/HumanSignal/label-studio"},{"type":"has_code","target_id":"github:NVIDIA:NeMo-Curator","source_url":"https://github.com/NVIDIA/NeMo-Curator"},{"type":"has_code","target_id":"github:code-kern-ai:refinery","source_url":"https://github.com/code-kern-ai/refinery"},{"type":"has_code","target_id":"github:sdv-dev:SDV","source_url":"https://github.com/sdv-dev/SDV"},{"type":"has_code","target_id":"github:Hitachi-Automotive-And-Industry-Lab:semantic-segmentation-editor","source_url":"https://github.com/Hitachi-Automotive-And-Industry-Lab/semantic-segmentation-editor"},{"type":"has_code","target_id":"github:vanderschaarlab:synthcity","source_url":"https://github.com/vanderschaarlab/synthcity"},{"type":"has_code","target_id":"github:nv-tlabs:vipe","source_url":"https://github.com/nv-tlabs/vipe"},{"type":"has_code","target_id":"github:ydataai:ydata-synthetic","source_url":"https://github.com/ydataai/ydata-synthetic"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:nifi","source_url":"https://github.com/apache/nifi"},{"type":"has_code","target_id":"github:apache:oozie","source_url":"https://github.com/apache/oozie"},{"type":"has_code","target_id":"github:argoproj:argo-workflows","source_url":"https://github.com/argoproj/argo-workflows"},{"type":"has_code","target_id":"github:couler-proj:couler","source_url":"https://github.com/couler-proj/couler"},{"type":"has_code","target_id":"github:huggingface:datatrove","source_url":"https://github.com/huggingface/datatrove"},{"type":"has_code","target_id":"github:dagster-io:dagster","source_url":"https://github.com/dagster-io/dagster"},{"type":"has_code","target_id":"github:dbt-labs:dbt-core","source_url":"https://github.com/dbt-labs/dbt-core"},{"type":"has_code","target_id":"github:flyteorg:flyte","source_url":"https://github.com/flyteorg/flyte"},{"type":"has_code","target_id":"github:Netflix:genie","source_url":"https://github.com/Netflix/genie"},{"type":"has_code","target_id":"github:dagworks-inc:hamilton","source_url":"https://github.com/dagworks-inc/hamilton"},{"type":"has_code","target_id":"github:instill-ai:instill-core","source_url":"https://github.com/instill-ai/instill-core"},{"type":"has_code","target_id":"github:instructor-ai:instructor","source_url":"https://github.com/instructor-ai/instructor"},{"type":"has_code","target_id":"github:kedro-org:kedro","source_url":"https://github.com/kedro-org/kedro"},{"type":"has_code","target_id":"github:spotify:luigi","source_url":"https://github.com/spotify/luigi"},{"type":"has_code","target_id":"github:Netflix:metaflow","source_url":"https://github.com/Netflix/metaflow"},{"type":"has_code","target_id":"github:pachyderm:pachyderm","source_url":"https://github.com/pachyderm/pachyderm"},{"type":"has_code","target_id":"github:ploomber:ploomber","source_url":"https://github.com/ploomber/ploomber"},{"type":"has_code","target_id":"github:pixeltable:pixeltable","source_url":"https://github.com/pixeltable/pixeltable"},{"type":"has_code","target_id":"github:PrefectHQ:prefect","source_url":"https://github.com/PrefectHQ/prefect"},{"type":"has_code","target_id":"github:google:seqio","source_url":"https://github.com/google/seqio"},{"type":"has_code","target_id":"github:snakemake:snakemake","source_url":"https://github.com/snakemake/snakemake"},{"type":"has_code","target_id":"github:towhee-io:towhee","source_url":"https://github.com/towhee-io/towhee"},{"type":"has_code","target_id":"github:Unstructured-IO:unstructured","source_url":"https://github.com/Unstructured-IO/unstructured"},{"type":"has_code","target_id":"github:zenml-io:zenml","source_url":"https://github.com/zenml-io/zenml"},{"type":"has_code","target_id":"github:apache:zeppelin","source_url":"https://github.com/apache/zeppelin"},{"type":"has_code","target_id":"github:deepnote:deepnote","source_url":"https://github.com/deepnote/deepnote"},{"type":"has_code","target_id":"github:jupyter:notebook","source_url":"https://github.com/jupyter/notebook"},{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:nteract:papermill","source_url":"https://github.com/nteract/papermill"},{"type":"has_code","target_id":"github:polynote:polynote","source_url":"https://github.com/polynote/polynote"},{"type":"has_code","target_id":"github:rstudio:rmarkdown","source_url":"https://github.com/rstudio/rmarkdown"},{"type":"has_code","target_id":"github:stencila:stencila","source_url":"https://github.com/stencila/stencila"},{"type":"has_code","target_id":"github:voila-dashboards:voila","source_url":"https://github.com/voila-dashboards/voila"},{"type":"has_code","target_id":"github:dotnet:interactive","source_url":"https://github.com/dotnet/interactive"},{"type":"has_code","target_id":"github:NVIDIA:aistore","source_url":"https://github.com/NVIDIA/aistore"},{"type":"has_code","target_id":"github:Alluxio:alluxio","source_url":"https://github.com/Alluxio/alluxio"},{"type":"has_code","target_id":"github:apache:arrow","source_url":"https://github.com/apache/arrow"},{"type":"has_code","target_id":"github:apache:druid","source_url":"https://github.com/apache/druid"},{"type":"has_code","target_id":"github:apache:hudi","source_url":"https://github.com/apache/hudi"},{"type":"has_code","target_id":"github:apache:iceberg","source_url":"https://github.com/apache/iceberg"},{"type":"has_code","target_id":"github:apache:ignite","source_url":"https://github.com/apache/ignite"},{"type":"has_code","target_id":"github:apache:parquet-java","source_url":"https://github.com/apache/parquet-java"},{"type":"has_code","target_id":"github:apache:pinot","source_url":"https://github.com/apache/pinot"},{"type":"has_code","target_id":"github:casibase:casibase","source_url":"https://github.com/casibase/casibase"},{"type":"has_code","target_id":"github:chroma-core:chroma","source_url":"https://github.com/chroma-core/chroma"},{"type":"has_code","target_id":"github:ClickHouse:ClickHouse","source_url":"https://github.com/ClickHouse/ClickHouse"},{"type":"has_code","target_id":"github:delta-io:delta","source_url":"https://github.com/delta-io/delta"},{"type":"has_code","target_id":"github:geldata:gel","source_url":"https://github.com/geldata/gel"},{"type":"has_code","target_id":"github:zilliztech:GPTCache","source_url":"https://github.com/zilliztech/GPTCache"},{"type":"has_code","target_id":"github:influxdata:influxdb","source_url":"https://github.com/influxdata/influxdb"},{"type":"has_code","target_id":"github:milvus-io:milvus","source_url":"https://github.com/milvus-io/milvus"},{"type":"has_code","target_id":"github:marqo-ai:marqo","source_url":"https://github.com/marqo-ai/marqo"},{"type":"has_code","target_id":"github:pgvector:pgvector","source_url":"https://github.com/pgvector/pgvector"},{"type":"has_code","target_id":"github:postgresml:postgresml","source_url":"https://github.com/postgresml/postgresml"},{"type":"has_code","target_id":"github:huggingface:safetensors","source_url":"https://github.com/huggingface/safetensors"},{"type":"has_code","target_id":"github:timescale:timescaledb","source_url":"https://github.com/timescale/timescaledb"},{"type":"has_code","target_id":"github:weaviate:weaviate","source_url":"https://github.com/weaviate/weaviate"},{"type":"has_code","target_id":"github:zarr-developers:zarr-python","source_url":"https://github.com/zarr-developers/zarr-python"},{"type":"has_code","target_id":"github:apache:beam","source_url":"https://github.com/apache/beam"},{"type":"has_code","target_id":"github:apache:flink","source_url":"https://github.com/apache/flink"},{"type":"has_code","target_id":"github:apache:kafka","source_url":"https://github.com/apache/kafka"},{"type":"has_code","target_id":"github:apache:samza","source_url":"https://github.com/apache/samza"},{"type":"has_code","target_id":"github:apache:spark","source_url":"https://github.com/apache/spark"},{"type":"has_code","target_id":"github:bytewax:bytewax","source_url":"https://github.com/bytewax/bytewax"},{"type":"has_code","target_id":"github:airtai:faststream","source_url":"https://github.com/airtai/faststream"},{"type":"has_code","target_id":"github:Waikato:moa","source_url":"https://github.com/Waikato/moa"},{"type":"has_code","target_id":"github:mosaicml:streaming","source_url":"https://github.com/mosaicml/streaming"},{"type":"has_code","target_id":"github:risingwavelabs:risingwave","source_url":"https://github.com/risingwavelabs/risingwave"},{"type":"has_code","target_id":"github:google:tensorstore","source_url":"https://github.com/google/tensorstore"},{"type":"has_code","target_id":"github:Agenta-AI:agenta","source_url":"https://github.com/Agenta-AI/agenta"},{"type":"has_code","target_id":"github:lyogavin:airllm","source_url":"https://github.com/lyogavin/airllm"},{"type":"has_code","target_id":"github:facebookincubator:AITemplate","source_url":"https://github.com/facebookincubator/AITemplate"},{"type":"has_code","target_id":"github:bentoml:BentoML","source_url":"https://github.com/bentoml/BentoML"},{"type":"has_code","target_id":"github:dataelement:bisheng","source_url":"https://github.com/dataelement/bisheng"},{"type":"has_code","target_id":"github:jolibrain:deepdetect","source_url":"https://github.com/jolibrain/deepdetect"},{"type":"has_code","target_id":"github:ai-dynamo:dynamo","source_url":"https://github.com/ai-dynamo/dynamo"},{"type":"has_code","target_id":"github:exo-explore:exo","source_url":"https://github.com/exo-explore/exo"},{"type":"has_code","target_id":"github:firebase:genkit","source_url":"https://github.com/firebase/genkit"},{"type":"has_code","target_id":"github:roboflow:inference","source_url":"https://github.com/roboflow/inference"},{"type":"has_code","target_id":"github:michaelfeil:infinity","source_url":"https://github.com/michaelfeil/infinity"},{"type":"has_code","target_id":"github:intel:ipex-llm","source_url":"https://github.com/intel/ipex-llm"},{"type":"has_code","target_id":"github:BerriAI:litellm","source_url":"https://github.com/BerriAI/litellm"},{"type":"has_code","target_id":"github:jina-ai:serve","source_url":"https://github.com/jina-ai/serve"},{"type":"has_code","target_id":"github:kiln-ai:kiln","source_url":"https://github.com/kiln-ai/kiln"},{"type":"has_code","target_id":"github:kserve:kserve","source_url":"https://github.com/kserve/kserve"},{"type":"has_code","target_id":"github:kvcache-ai:ktransformers","source_url":"https://github.com/kvcache-ai/ktransformers"},{"type":"has_code","target_id":"github:Scale3-Labs:langtrace","source_url":"https://github.com/Scale3-Labs/langtrace"},{"type":"has_code","target_id":"github:leptonai:leptonai","source_url":"https://github.com/leptonai/leptonai"},{"type":"has_code","target_id":"github:ModelTC:lightllm","source_url":"https://github.com/ModelTC/lightllm"},{"type":"has_code","target_id":"github:ggml-org:llama.cpp","source_url":"https://github.com/ggml-org/llama.cpp"},{"type":"has_code","target_id":"github:InternLM:lmdeploy","source_url":"https://github.com/InternLM/lmdeploy"},{"type":"has_code","target_id":"github:lmstudio-ai:lms","source_url":"https://github.com/lmstudio-ai/lms"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mindsdb:mindsdb","source_url":"https://github.com/mindsdb/mindsdb"},{"type":"has_code","target_id":"github:mlrun:mlrun","source_url":"https://github.com/mlrun/mlrun"},{"type":"has_code","target_id":"github:SeldonIO:mlserver","source_url":"https://github.com/SeldonIO/mlserver"},{"type":"has_code","target_id":"github:mosecorg:mosec","source_url":"https://github.com/mosecorg/mosec"},{"type":"has_code","target_id":"github:nndeploy:nndeploy","source_url":"https://github.com/nndeploy/nndeploy"},{"type":"has_code","target_id":"github:nuclio:nuclio","source_url":"https://github.com/nuclio/nuclio"},{"type":"has_code","target_id":"github:bentoml:OpenLLM","source_url":"https://github.com/bentoml/OpenLLM"},{"type":"has_code","target_id":"github:openvinotoolkit:openvino","source_url":"https://github.com/openvinotoolkit/openvino"},{"type":"has_code","target_id":"github:open-webui:open-webui","source_url":"https://github.com/open-webui/open-webui"},{"type":"has_code","target_id":"github:algorithmicsuperintelligence:optillm","source_url":"https://github.com/algorithmicsuperintelligence/optillm"},{"type":"has_code","target_id":"github:SJTU-IPADS:PowerInfer","source_url":"https://github.com/SJTU-IPADS/PowerInfer"},{"type":"has_code","target_id":"github:neulab:prompt2model","source_url":"https://github.com/neulab/prompt2model"},{"type":"has_code","target_id":"github:SeldonIO:seldon-core","source_url":"https://github.com/SeldonIO/seldon-core"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:skypilot-org:skypilot","source_url":"https://github.com/skypilot-org/skypilot"},{"type":"has_code","target_id":"github:tensorflow:serving","source_url":"https://github.com/tensorflow/serving"},{"type":"has_code","target_id":"github:huggingface:text-generation-inference","source_url":"https://github.com/huggingface/text-generation-inference"},{"type":"has_code","target_id":"github:pytorch:serve","source_url":"https://github.com/pytorch/serve"},{"type":"has_code","target_id":"github:meta-pytorch:torchtune","source_url":"https://github.com/meta-pytorch/torchtune"},{"type":"has_code","target_id":"github:transformerlab:transformerlab-app","source_url":"https://github.com/transformerlab/transformerlab-app"},{"type":"has_code","target_id":"github:triton-inference-server:server","source_url":"https://github.com/triton-inference-server/server"},{"type":"has_code","target_id":"github:vercel:ai","source_url":"https://github.com/vercel/ai"},{"type":"has_code","target_id":"github:vespa-engine:vespa","source_url":"https://github.com/vespa-engine/vespa"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:tatsu-lab:alpaca_eval","source_url":"https://github.com/tatsu-lab/alpaca_eval"},{"type":"has_code","target_id":"github:erikbern:ann-benchmarks","source_url":"https://github.com/erikbern/ann-benchmarks"},{"type":"has_code","target_id":"github:stanford-futuredata:ARES","source_url":"https://github.com/stanford-futuredata/ARES"},{"type":"has_code","target_id":"github:beir-cellar:beir","source_url":"https://github.com/beir-cellar/beir"},{"type":"has_code","target_id":"github:bigcode-project:bigcode-evaluation-harness","source_url":"https://github.com/bigcode-project/bigcode-evaluation-harness"},{"type":"has_code","target_id":"github:Unbabel:COMET","source_url":"https://github.com/Unbabel/COMET"},{"type":"has_code","target_id":"github:hkust-nlp:ceval","source_url":"https://github.com/hkust-nlp/ceval"},{"type":"has_code","target_id":"github:deepchecks:deepchecks","source_url":"https://github.com/deepchecks/deepchecks"},{"type":"has_code","target_id":"github:confident-ai:deepeval","source_url":"https://github.com/confident-ai/deepeval"},{"type":"has_code","target_id":"github:facebookresearch:DomainBed","source_url":"https://github.com/facebookresearch/DomainBed"},{"type":"has_code","target_id":"github:Cloud-CV:EvalAI","source_url":"https://github.com/Cloud-CV/EvalAI"},{"type":"has_code","target_id":"github:mlfoundations:evalchemy","source_url":"https://github.com/mlfoundations/evalchemy"},{"type":"has_code","target_id":"github:evalplus:evalplus","source_url":"https://github.com/evalplus/evalplus"},{"type":"has_code","target_id":"github:openai:evals","source_url":"https://github.com/openai/evals"},{"type":"has_code","target_id":"github:modelscope:evalscope","source_url":"https://github.com/modelscope/evalscope"},{"type":"has_code","target_id":"github:huggingface:evaluate","source_url":"https://github.com/huggingface/evaluate"},{"type":"has_code","target_id":"github:evidentlyai:evidently","source_url":"https://github.com/evidentlyai/evidently"},{"type":"has_code","target_id":"github:OpenLMLab:GAOKAO-Bench","source_url":"https://github.com/OpenLMLab/GAOKAO-Bench"},{"type":"has_code","target_id":"github:Giskard-AI:giskard","source_url":"https://github.com/Giskard-AI/giskard"},{"type":"has_code","target_id":"github:openai:human-eval","source_url":"https://github.com/openai/human-eval"},{"type":"has_code","target_id":"github:Helicone:helicone","source_url":"https://github.com/Helicone/helicone"},{"type":"has_code","target_id":"github:stanford-crfm:helm","source_url":"https://github.com/stanford-crfm/helm"},{"type":"has_code","target_id":"github:UKGovernmentBEIS:inspect_ai","source_url":"https://github.com/UKGovernmentBEIS/inspect_ai"},{"type":"has_code","target_id":"github:jitsi:jiwer","source_url":"https://github.com/jitsi/jiwer"},{"type":"has_code","target_id":"github:lmnr-ai:lmnr","source_url":"https://github.com/lmnr-ai/lmnr"},{"type":"has_code","target_id":"github:langfuse:langfuse","source_url":"https://github.com/langfuse/langfuse"},{"type":"has_code","target_id":"github:JohnSnowLabs:langtest","source_url":"https://github.com/JohnSnowLabs/langtest"},{"type":"has_code","target_id":"github:EleutherAI:lm-evaluation-harness","source_url":"https://github.com/EleutherAI/lm-evaluation-harness"},{"type":"has_code","target_id":"github:langwatch:langwatch","source_url":"https://github.com/langwatch/langwatch"},{"type":"has_code","target_id":"github:huggingface:lighteval","source_url":"https://github.com/huggingface/lighteval"},{"type":"has_code","target_id":"github:lunary-ai:lunary","source_url":"https://github.com/lunary-ai/lunary"},{"type":"has_code","target_id":"github:ray-project:llmperf","source_url":"https://github.com/ray-project/llmperf"},{"type":"has_code","target_id":"github:EvolvingLMMs-Lab:lmms-eval","source_url":"https://github.com/EvolvingLMMs-Lab/lmms-eval"},{"type":"has_code","target_id":"github:google-deepmind:meltingpot","source_url":"https://github.com/google-deepmind/meltingpot"},{"type":"has_code","target_id":"github:Farama-Foundation:Metaworld","source_url":"https://github.com/Farama-Foundation/Metaworld"},{"type":"has_code","target_id":"github:mir-evaluation:mir_eval","source_url":"https://github.com/mir-evaluation/mir_eval"},{"type":"has_code","target_id":"github:mlcommons:inference","source_url":"https://github.com/mlcommons/inference"},{"type":"has_code","target_id":"github:mlcommons:inference","source_url":"https://github.com/mlcommons/inference"},{"type":"has_code","target_id":"github:NannyML:nannyml","source_url":"https://github.com/NannyML/nannyml"},{"type":"has_code","target_id":"github:snap-stanford:ogb","source_url":"https://github.com/snap-stanford/ogb"},{"type":"has_code","target_id":"github:dezoito:ollama-grid-search","source_url":"https://github.com/dezoito/ollama-grid-search"},{"type":"has_code","target_id":"github:open-compass:OpenCompass","source_url":"https://github.com/open-compass/OpenCompass"},{"type":"has_code","target_id":"github:openlit:openlit","source_url":"https://github.com/openlit/openlit"},{"type":"has_code","target_id":"github:traceloop:openllmetry","source_url":"https://github.com/traceloop/openllmetry"},{"type":"has_code","target_id":"github:comet-ml:opik","source_url":"https://github.com/comet-ml/opik"},{"type":"has_code","target_id":"github:HumanCompatibleAI:overcooked_ai","source_url":"https://github.com/HumanCompatibleAI/overcooked_ai"},{"type":"has_code","target_id":"github:Arize-ai:phoenix","source_url":"https://github.com/Arize-ai/phoenix"},{"type":"has_code","target_id":"github:microsoft:promptbench","source_url":"https://github.com/microsoft/promptbench"},{"type":"has_code","target_id":"github:promptfoo:promptfoo","source_url":"https://github.com/promptfoo/promptfoo"},{"type":"has_code","target_id":"github:prometheus-eval:prometheus-eval","source_url":"https://github.com/prometheus-eval/prometheus-eval"},{"type":"has_code","target_id":"github:raga-ai-hub:RagaAI-Catalyst","source_url":"https://github.com/raga-ai-hub/RagaAI-Catalyst"},{"type":"has_code","target_id":"github:explodinggradients:ragas","source_url":"https://github.com/explodinggradients/ragas"},{"type":"has_code","target_id":"github:amazon-science:RAGChecker","source_url":"https://github.com/amazon-science/RAGChecker"},{"type":"has_code","target_id":"github:allenai:reward-bench","source_url":"https://github.com/allenai/reward-bench"},{"type":"has_code","target_id":"github:stepjam:RLBench","source_url":"https://github.com/stepjam/RLBench"},{"type":"has_code","target_id":"github:simpler-env:SimplerEnv","source_url":"https://github.com/simpler-env/SimplerEnv"},{"type":"has_code","target_id":"github:SwanHubX:SwanLab","source_url":"https://github.com/SwanHubX/SwanLab"},{"type":"has_code","target_id":"github:Picovoice:speech-to-text-benchmark","source_url":"https://github.com/Picovoice/speech-to-text-benchmark"},{"type":"has_code","target_id":"github:tensorflow:model-analysis","source_url":"https://github.com/tensorflow/model-analysis"},{"type":"has_code","target_id":"github:pytorch:benchmark","source_url":"https://github.com/pytorch/benchmark"},{"type":"has_code","target_id":"github:truera:trulens","source_url":"https://github.com/truera/trulens"},{"type":"has_code","target_id":"github:HowieHwong:TrustLLM","source_url":"https://github.com/HowieHwong/TrustLLM"},{"type":"has_code","target_id":"github:Vchitect:VBench","source_url":"https://github.com/Vchitect/VBench"},{"type":"has_code","target_id":"github:open-compass:VLMEvalKit","source_url":"https://github.com/open-compass/VLMEvalKit"},{"type":"has_code","target_id":"github:dssg:aequitas","source_url":"https://github.com/dssg/aequitas"},{"type":"has_code","target_id":"github:Trusted-AI:AIX360","source_url":"https://github.com/Trusted-AI/AIX360"},{"type":"has_code","target_id":"github:Trusted-AI:AIF360","source_url":"https://github.com/Trusted-AI/AIF360"},{"type":"has_code","target_id":"github:SeldonIO:alibi","source_url":"https://github.com/SeldonIO/alibi"},{"type":"has_code","target_id":"github:pytorch:captum","source_url":"https://github.com/pytorch/captum"},{"type":"has_code","target_id":"github:fairlearn:fairlearn","source_url":"https://github.com/fairlearn/fairlearn"},{"type":"has_code","target_id":"github:interpretml:interpret","source_url":"https://github.com/interpretml/interpret"},{"type":"has_code","target_id":"github:lightly-ai:lightly","source_url":"https://github.com/lightly-ai/lightly"},{"type":"has_code","target_id":"github:aerdem4:lofo-importance","source_url":"https://github.com/aerdem4/lofo-importance"},{"type":"has_code","target_id":"github:mljar:mljar-supervised","source_url":"https://github.com/mljar/mljar-supervised"},{"type":"has_code","target_id":"github:understandable-machine-intelligence-lab:Quantus","source_url":"https://github.com/understandable-machine-intelligence-lab/Quantus"},{"type":"has_code","target_id":"github:shap:shap","source_url":"https://github.com/shap/shap"},{"type":"has_code","target_id":"github:MAIF:shapash","source_url":"https://github.com/MAIF/shapash"},{"type":"has_code","target_id":"github:pair-code:what-if-tool","source_url":"https://github.com/pair-code/what-if-tool"},{"type":"has_code","target_id":"github:feast-dev:feast","source_url":"https://github.com/feast-dev/feast"},{"type":"has_code","target_id":"github:featureform:featureform","source_url":"https://github.com/featureform/featureform"},{"type":"has_code","target_id":"github:logicalclocks:hopsworks","source_url":"https://github.com/logicalclocks/hopsworks"},{"type":"has_code","target_id":"github:SeldonIO:alibi-detect","source_url":"https://github.com/SeldonIO/alibi-detect"},{"type":"has_code","target_id":"github:unit8co:darts","source_url":"https://github.com/unit8co/darts"},{"type":"has_code","target_id":"github:awslabs:deequ","source_url":"https://github.com/awslabs/deequ"},{"type":"has_code","target_id":"github:yzhao062:pyod","source_url":"https://github.com/yzhao062/pyod"},{"type":"has_code","target_id":"github:tensorflow:data-validation","source_url":"https://github.com/tensorflow/data-validation"},{"type":"has_code","target_id":"github:activeloopai:deeplake","source_url":"https://github.com/activeloopai/deeplake"},{"type":"has_code","target_id":"github:facebookresearch:detectron2","source_url":"https://github.com/facebookresearch/detectron2"},{"type":"has_code","target_id":"github:keras-team:keras-cv","source_url":"https://github.com/keras-team/keras-cv"},{"type":"has_code","target_id":"github:kornia:kornia","source_url":"https://github.com/kornia/kornia"},{"type":"has_code","target_id":"github:salesforce:LAVIS","source_url":"https://github.com/salesforce/LAVIS"},{"type":"has_code","target_id":"github:bcmi:libcom","source_url":"https://github.com/bcmi/libcom"},{"type":"has_code","target_id":"github:lightly-ai:lightly-train","source_url":"https://github.com/lightly-ai/lightly-train"},{"type":"has_code","target_id":"github:open-mmlab:mmcv","source_url":"https://github.com/open-mmlab/mmcv"},{"type":"has_code","target_id":"github:Deci-AI:super-gradients","source_url":"https://github.com/Deci-AI/super-gradients"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"},{"type":"has_code","target_id":"github:NUS-HPC-AI-Lab:VideoSys","source_url":"https://github.com/NUS-HPC-AI-Lab/VideoSys"},{"type":"has_code","target_id":"github:Marker-Inc-Korea:AutoRAG","source_url":"https://github.com/Marker-Inc-Korea/AutoRAG"},{"type":"has_code","target_id":"github:FlagOpen:FlagEmbedding","source_url":"https://github.com/FlagOpen/FlagEmbedding"},{"type":"has_code","target_id":"github:truefoundry:cognita","source_url":"https://github.com/truefoundry/cognita"},{"type":"has_code","target_id":"github:docarray:docarray","source_url":"https://github.com/docarray/docarray"},{"type":"has_code","target_id":"github:facebookresearch:faiss","source_url":"https://github.com/facebookresearch/faiss"},{"type":"has_code","target_id":"github:IntelLabs:fastRAG","source_url":"https://github.com/IntelLabs/fastRAG"},{"type":"has_code","target_id":"github:microsoft:graphrag","source_url":"https://github.com/microsoft/graphrag"},{"type":"has_code","target_id":"github:OSU-NLP-Group:HippoRAG","source_url":"https://github.com/OSU-NLP-Group/HippoRAG"},{"type":"has_code","target_id":"github:EmbeddedLLM:JamAIBase","source_url":"https://github.com/EmbeddedLLM/JamAIBase"},{"type":"has_code","target_id":"github:google:langextract","source_url":"https://github.com/google/langextract"},{"type":"has_code","target_id":"github:HKUDS:LightRAG","source_url":"https://github.com/HKUDS/LightRAG"},{"type":"has_code","target_id":"github:llmware-ai:llmware","source_url":"https://github.com/llmware-ai/llmware"},{"type":"has_code","target_id":"github:mem0ai:mem0","source_url":"https://github.com/mem0ai/mem0"},{"type":"has_code","target_id":"github:yahoojapan:NGT","source_url":"https://github.com/yahoojapan/NGT"},{"type":"has_code","target_id":"github:nmslib:nmslib","source_url":"https://github.com/nmslib/nmslib"},{"type":"has_code","target_id":"github:qdrant:qdrant","source_url":"https://github.com/qdrant/qdrant"},{"type":"has_code","target_id":"github:SciPhi-AI:R2R","source_url":"https://github.com/SciPhi-AI/R2R"},{"type":"has_code","target_id":"github:infiniflow:ragflow","source_url":"https://github.com/infiniflow/ragflow"},{"type":"has_code","target_id":"github:gabrielchua:RAGxplorer","source_url":"https://github.com/gabrielchua/RAGxplorer"},{"type":"has_code","target_id":"github:IntelLabs:RAG-FiT","source_url":"https://github.com/IntelLabs/RAG-FiT"},{"type":"has_code","target_id":"github:microsoft:TextWorld","source_url":"https://github.com/microsoft/TextWorld"},{"type":"has_code","target_id":"github:vanna-ai:vanna","source_url":"https://github.com/vanna-ai/vanna"},{"type":"has_code","target_id":"github:andrewyng:aisuite","source_url":"https://github.com/andrewyng/aisuite"},{"type":"has_code","target_id":"github:PKU-Alignment:align-anything","source_url":"https://github.com/PKU-Alignment/align-anything"},{"type":"has_code","target_id":"github:MaartenGr:BERTopic","source_url":"https://github.com/MaartenGr/BERTopic"},{"type":"has_code","target_id":"github:dagworks-inc:burr","source_url":"https://github.com/dagworks-inc/burr"},{"type":"has_code","target_id":"github:salesforce:CodeTF","source_url":"https://github.com/salesforce/CodeTF"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:stanfordnlp:dspy","source_url":"https://github.com/stanfordnlp/dspy"},{"type":"has_code","target_id":"github:dust-tt:dust","source_url":"https://github.com/dust-tt/dust"},{"type":"has_code","target_id":"github:espnet:espnet","source_url":"https://github.com/espnet/espnet"},{"type":"has_code","target_id":"github:lm-sys:FastChat","source_url":"https://github.com/lm-sys/FastChat"},{"type":"has_code","target_id":"github:flairNLP:flair","source_url":"https://github.com/flairNLP/flair"},{"type":"has_code","target_id":"github:piskvorky:gensim","source_url":"https://github.com/piskvorky/gensim"},{"type":"has_code","target_id":"github:meta-pytorch:gpt-fast","source_url":"https://github.com/meta-pytorch/gpt-fast"},{"type":"has_code","target_id":"github:h2oai:h2ogpt","source_url":"https://github.com/h2oai/h2ogpt"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:oughtinc:ice","source_url":"https://github.com/oughtinc/ice"},{"type":"has_code","target_id":"github:lamini-ai:lamini","source_url":"https://github.com/lamini-ai/lamini"},{"type":"has_code","target_id":"github:langchain-ai:langchain","source_url":"https://github.com/langchain-ai/langchain"},{"type":"has_code","target_id":"github:run-llama:llama_index","source_url":"https://github.com/run-llama/llama_index"},{"type":"has_code","target_id":"github:meta-llama:llama","source_url":"https://github.com/meta-llama/llama"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:Alpha-VLLM:LLaMA2-Accessory","source_url":"https://github.com/Alpha-VLLM/LLaMA2-Accessory"},{"type":"has_code","target_id":"github:RUCAIBox:LLMBox","source_url":"https://github.com/RUCAIBox/LLMBox"},{"type":"has_code","target_id":"github:OptimalScale:LMFlow","source_url":"https://github.com/OptimalScale/LMFlow"},{"type":"has_code","target_id":"github:NVIDIA:Megatron-LM","source_url":"https://github.com/NVIDIA/Megatron-LM"},{"type":"has_code","target_id":"github:mindspore-lab:mindnlp","source_url":"https://github.com/mindspore-lab/mindnlp"},{"type":"has_code","target_id":"github:mlc-ai:mlc-llm","source_url":"https://github.com/mlc-ai/mlc-llm"},{"type":"has_code","target_id":"github:ollama:ollama","source_url":"https://github.com/ollama/ollama"}]', NULL, 'MIT', 'approved', 80, 'b520f9adc848bae9a418f5e23d0c4535', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-EthicalML-awesome-production-machine-learning from https://github.com/EthicalML.png
Image converted to WebP: data/images/github-EthicalML-awesome-production-machine-learning.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-tensorflow-tfjs', 'github--tensorflow--tfjs', 'tfjs', 'tensorflow', 'TensorFlow.js is an open-source hardware-accelerated JavaScript library for training and deploying machine learning models. **Develop ML in the Browser** <br/> Use flexible and intuitive APIs to build models from scratch using the low-level JavaScript linear algebra library or the high-level layers API. **Develop ML in Node.js** <br/> Execute native TensorFlow with the same TensorFlow.js API under the Node.js runtime. **Run Existing models** <br/> Use TensorFlow.js model converters to run pre...', '["deep-learning","deep-neural-network","gpu-acceleration","javascript","machine-learning","neural-network","typescript","wasm","web-assembly","webgl","typescript"]', 'other', 19033, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/tensorflow/tfjs","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# TensorFlow.js\n\nTensorFlow.js is an open-source hardware-accelerated JavaScript library for\ntraining and deploying machine learning models.\n\n\n**Develop ML in the Browser** <br/>\nUse flexible and intuitive APIs to build models from scratch using the low-level\nJavaScript linear algebra library or the high-level layers API.\n\n**Develop ML in Node.js** <br/>\nExecute native TensorFlow with the same TensorFlow.js API under the Node.js\nruntime.\n\n**Run Existing models** <br/>\nUse TensorFlow.js model converters to run pre-existing TensorFlow models right\nin the browser.\n\n**Retrain Existing models** <br/>\nRetrain pre-existing ML models using sensor data connected to the browser or\nother client-side data.\n\n## About this repo\n\nThis repository contains the logic and scripts that combine\nseveral packages.\n\nAPIs:\n- [TensorFlow.js Core](/tfjs-core),\n  a flexible low-level API for neural networks and numerical computation.\n- [TensorFlow.js Layers](/tfjs-layers),\n  a high-level API which implements functionality similar to\n  [Keras](https://keras.io/).\n- [TensorFlow.js Data](/tfjs-data),\n  a simple API to load and prepare data analogous to\n  [tf.data](https://www.tensorflow.org/guide/datasets).\n- [TensorFlow.js Converter](/tfjs-converter),\n  tools to import a TensorFlow SavedModel to TensorFlow.js\n- [TensorFlow.js Vis](/tfjs-vis),\n  in-browser visualization for TensorFlow.js models\n- [TensorFlow.js AutoML](/tfjs-automl),\n  Set of APIs to load and run models produced by\n  [AutoML Edge](https://cloud.google.com/vision/automl/docs/edge-quickstart).\n\n\nBackends/Platforms:\n- [TensorFlow.js CPU Backend](/tfjs-backend-cpu), pure-JS backend for Node.js and the browser.\n- [TensorFlow.js WebGL Backend](/tfjs-backend-webgl), WebGL backend for the browser.\n- [TensorFlow.js WASM Backend](/tfjs-backend-wasm), WebAssembly backend for the browser.\n- [TensorFlow.js WebGPU](/tfjs-backend-webgpu), WebGPU backend for the browser.\n- [TensorFlow.js Node](/tfjs-node), Node.js platform via TensorFlow C++ adapter.\n- [TensorFlow.js React Native](/tfjs-react-native), React Native platform via expo-gl adapter.\n\nIf you care about bundle size, you can import those packages individually.\n\nIf you are looking for Node.js support, check out the [TensorFlow.js Node directory](/tfjs-node).\n\n## Examples\n\nCheck out our\n[examples repository](https://github.com/tensorflow/tfjs-examples)\nand our [tutorials](https://js.tensorflow.org/tutorials/).\n\n## Gallery\n\nBe sure to check out [the gallery](GALLERY.md) of all projects related to TensorFlow.js.\n\n## Pre-trained models\n\nBe sure to also check out our [models repository](https://github.com/tensorflow/tfjs-models) where we host pre-trained models\non NPM.\n\n## Benchmarks\n\n* [Local benchmark tool](https://tfjs-benchmarks.web.app/). Use this webpage tool to collect the performance related metrics (speed, memory, etc) of TensorFlow.js models and kernels **on your local device** with CPU, WebGL or WASM backends. You can benchmark custom models by following this [guide](https://github.com/tensorflow/tfjs/blob/master/e2e/benchmarks/local-benchmark/README.md).\n* [Multi-device benchmark tool](https://github.com/tensorflow/tfjs/tree/master/e2e/benchmarks/browserstack-benchmark/README.md). Use this tool to collect the same performance related metrics **on a collection of remote devices**.\n\n## Getting started\n\nThere are two main ways to get TensorFlow.js in your JavaScript project:\nvia <a href="https://developer.mozilla.org/en-US/docs/Learn/HTML/Howto/Use_JavaScript_within_a_webpage" target="_blank">script tags</a> <strong>or</strong> by installing it from <a href="https://www.npmjs.com/" target="_blank">NPM</a>\nand using a build tool like <a href="https://parceljs.org/" target="_blank">Parcel</a>,\n<a href="https://webpack.js.org/" target="_blank">WebPack</a>, or <a href="https://rollupjs.org/guide/en" target="_blank">Rollup</a>.\n\n### via Script Tag\n\nAdd the following code to an HTML file:\n\n```html\n<html>\n  <head>\n    <!-- Load TensorFlow.js -->\n    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js"> </script>\n\n\n    <!-- Place your code in the script tag below. You can also use an external .js file -->\n    <script>\n      // Notice there is no ''import'' statement. ''tf'' is available on the index-page\n      // because of the script tag above.\n\n      // Define a model for linear regression.\n      const model = tf.sequential();\n      model.add(tf.layers.dense({units: 1, inputShape: [1]}));\n\n      // Prepare the model for training: Specify the loss and the optimizer.\n      model.compile({loss: ''meanSquaredError'', optimizer: ''sgd''});\n\n      // Generate some synthetic data for training.\n      const xs = tf.tensor2d([1, 2, 3, 4], [4, 1]);\n      const ys = tf.tensor2d([1, 3, 5, 7], [4, 1]);\n\n      // Train the model using the data.\n      model.fit(xs, ys).then(() => {\n        // Use the model to do inference on a data point the model hasn''t seen before:\n        // Open the browser devtools to see the output\n        model.predict(tf.tensor2d([5], [1, 1])).print();\n      });\n    </script>\n  </head>\n\n  <body>\n  </body>\n</html>\n```\n\nOpen up that HTML file in your browser, and the code should run!\n\n### via NPM\n\nAdd TensorFlow.js to your project using <a href="https://yarnpkg.com/en/" target="_blank">yarn</a> <em>or</em> <a href="https://docs.npmjs.com/cli/npm" target="_blank">npm</a>. <b>Note:</b> Because\nwe use ES2017 syntax (such as `import`), this workflow assumes you are using a modern browser or a bundler/transpiler\nto convert your code to something older browsers understand. See our\n<a href=''https://github.com/tensorflow/tfjs-examples'' target="_blank">examples</a>\nto see how we use <a href="https://parceljs.org/" target="_blank">Parcel</a> to build\nour code. However, you are free to use any build tool that you prefer.\n\n\n\n```js\nimport * as tf from ''@tensorflow/tfjs'';\n\n// Define a model for linear regression.\nconst model = tf.sequential();\nmodel.add(tf.layers.dense({units: 1, inputShape: [1]}));\n\n// Prepare the model for training: Specify the loss and the optimizer.\nmodel.compile({loss: ''meanSquaredError'', optimizer: ''sgd''});\n\n// Generate some synthetic data for training.\nconst xs = tf.tensor2d([1, 2, 3, 4], [4, 1]);\nconst ys = tf.tensor2d([1, 3, 5, 7], [4, 1]);\n\n// Train the model using the data.\nmodel.fit(xs, ys).then(() => {\n  // Use the model to do inference on a data point the model hasn''t seen before:\n  model.predict(tf.tensor2d([5], [1, 1])).print();\n});\n```\n\nSee our <a href="https://js.tensorflow.org/tutorials/" target="_blank">tutorials</a>, <a href="https://github.com/tensorflow/tfjs-examples" target="_blank">examples</a>\nand <a href="https://js.tensorflow.org/api/latest/">documentation</a> for more details.\n\n## Importing pre-trained models\n\nWe support porting pre-trained models from:\n- [TensorFlow SavedModel](https://www.tensorflow.org/js/tutorials/conversion/import_saved_model)\n- [Keras](https://js.tensorflow.org/tutorials/import-keras.html)\n\n## Various ops supported in different backends\n\nPlease refer below :\n- [TFJS Ops Matrix](https://docs.google.com/spreadsheets/d/1D25XtWaBrmUEErbGQB0QmNhH-xtwHo9LDl59w0TbxrI/edit#gid=0)\n\n## Find out more\n\n[TensorFlow.js](https://js.tensorflow.org) is a part of the\n[TensorFlow](https://www.tensorflow.org) ecosystem. For more info:\n- For help from the community, use the `tfjs` tag on the [TensorFlow Forum](https://discuss.tensorflow.org/tag/tfjs).\n- [TensorFlow.js Website](https://js.tensorflow.org)\n- [Tutorials](https://js.tensorflow.org/tutorials)\n- [API reference](https://js.tensorflow.org/api/latest/)\n- [TensorFlow.js Blog](https://blog.tensorflow.org/search?label=TensorFlow.js)\n\nThanks, <a href="https://www.browserstack.com/">BrowserStack</a>, for providing testing support.\n', '{"language":"TypeScript","stars":19033,"forks":2013,"watchers":19033,"open_issues":615,"topics":["deep-learning","deep-neural-network","gpu-acceleration","javascript","machine-learning","neural-network","typescript","wasm","web-assembly","webgl"],"default_branch":"master","size_kb":174153,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:tensorflow:tfjs-examples","source_url":"https://github.com/tensorflow/tfjs-examples"},{"type":"has_code","target_id":"github:tensorflow:tfjs-models","source_url":"https://github.com/tensorflow/tfjs-models"},{"type":"has_code","target_id":"github:tensorflow:tfjs","source_url":"https://github.com/tensorflow/tfjs"},{"type":"has_code","target_id":"github:tensorflow:tfjs","source_url":"https://github.com/tensorflow/tfjs"},{"type":"has_code","target_id":"github:tensorflow:tfjs-examples''","source_url":"https://github.com/tensorflow/tfjs-examples''"},{"type":"has_code","target_id":"github:tensorflow:tfjs-examples\"","source_url":"https://github.com/tensorflow/tfjs-examples\""}]', NULL, 'Apache-2.0', 'approved', 65, '2a1e9f2e877aba4f08d3ab75492016b4', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-tensorflow-tfjs from https://github.com/tensorflow.png
Image converted to WebP: data/images/github-tensorflow-tfjs.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Unity-Technologies-ml-agents', 'github--unity-technologies--ml-agents', 'ml-agents', 'Unity-Technologies', '(latest release) (all releases) **The Unity Machine Learning Agents Toolkit** (ML-Agents) is an open-source project that enables games and simulations to serve as environments for training intelligent agents. We provide implementations (based on PyTorch) of state-of-the-art algorithms to enable game developers and hobbyists to easily train intelligent agents for 2D, 3D and VR/AR games. Researchers can also use the provided simple-to-use Python API to train Agents using reinforcement learning,...', '["deep-learning","deep-reinforcement-learning","machine-learning","neural-networks","reinforcement-learning","unity","unity3d","c#"]', 'other', 18919, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Unity-Technologies/ml-agents","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Unity ML-Agents Toolkit\n\n[![docs badge](https://img.shields.io/badge/docs-reference-blue.svg)](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest)\n\n[![license badge](https://img.shields.io/badge/license-Apache--2.0-green.svg)](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/LICENSE.md)\n\n([latest release](https://github.com/Unity-Technologies/ml-agents/releases/tag/latest_release)) ([all releases](https://github.com/Unity-Technologies/ml-agents/releases))\n\n**The Unity Machine Learning Agents Toolkit** (ML-Agents) is an open-source project that enables games and simulations to serve as environments for training intelligent agents. We provide implementations (based on PyTorch) of state-of-the-art algorithms to enable game developers and hobbyists to easily train intelligent agents for 2D, 3D and VR/AR games. Researchers can also use the provided simple-to-use Python API to train Agents using reinforcement learning, imitation learning, neuroevolution, or any other methods. These trained agents can be used for multiple purposes, including controlling NPC behavior (in a variety of settings such as multi-agent and adversarial), automated testing of game builds and evaluating different game design decisions pre-release. The ML-Agents Toolkit is mutually beneficial for both game developers and AI researchers as it provides a central platform where advances in AI can be evaluated on Unityâ€™s rich environments and then made accessible to the wider research and game developer communities.\n\n## Features\n- 17+ [example Unity environments](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Learning-Environment-Examples.html)\n- Support for multiple environment configurations and training scenarios\n- Flexible Unity SDK that can be integrated into your game or custom Unity scene\n- Support for training single-agent, multi-agent cooperative, and multi-agent competitive scenarios via several Deep Reinforcement Learning algorithms (PPO, SAC, MA-POCA, self-play).\n- Support for learning from demonstrations through two Imitation Learning algorithms (BC and GAIL).\n- Quickly and easily add your own [custom training algorithm](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-Custom-Trainer-Plugin.html) and/or components.\n- Easily definable Curriculum Learning scenarios for complex tasks\n- Train robust agents using environment randomization\n- Flexible agent control with On Demand Decision Making\n- Train using multiple concurrent Unity environment instances\n- Utilizes the [Inference Engine](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Inference-Engine.html) to provide native cross-platform support\n- Unity environment [control from Python](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-LLAPI.html)\n- Wrap Unity learning environments as a [gym](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-Gym-API.html) environment\n- Wrap Unity learning environments as a [PettingZoo](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Python-PettingZoo-API.html) environment\n\n## Releases & Documentation\n\n> **âš ï¸ Documentation Migration Notice**\n> We have moved to [Unity Package documentation](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest) as the **primary developer documentation** and have **deprecated** the maintenance of [web docs](https://unity-technologies.github.io/ml-agents/). Please use the Unity Package documentation for the most up-to-date information.\n\nThe table below shows our latest release, including our `develop` branch which is under active development and may be unstable. A few helpful guidelines:\n\n- The [Versioning page](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Versioning.html) overviews how we manage our GitHub releases and the versioning process for each of the ML-Agents components.\n- The [Releases page](https://github.com/Unity-Technologies/ml-agents/releases) contains details of the changes between releases.\n- The [Migration page](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Migrating.html) contains details on how to upgrade from earlier releases of the ML-Agents Toolkit.\n- The `com.unity.ml-agents` package is [verified](https://docs.unity3d.com/2020.1/Documentation/Manual/pack-safe.html) for Unity 2020.1 and later. Verified packages releases are numbered 1.0.x.\n\n|      **Version**       |  **Release Date**   |                                  **Source**                                   |                                                 **Documentation**                                                  |                                      **Download**                                      |                  **Python Package**                   |                                   **Unity Package**                                   |\n|:----------------------:|:-------------------:|:-----------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------------:|:-----------------------------------------------------:|:-------------------------------------------------------------------------------------:|\n|     **Release 23**     | **August 28, 2025** | **[source](https://github.com/Unity-Technologies/ml-agents/tree/release_23)** |              **[docs](https://docs.unity3d.com/Packages/com.unity.ml-agents@4.0/manual/index.html)**               | **[download](https://github.com/Unity-Technologies/ml-agents/archive/release_23.zip)** | **[1.1.0](https://pypi.org/project/mlagents/1.1.0/)** |                                       **4.0.0**                                       |\n| **develop (unstable)** |         --          |    [source](https://github.com/Unity-Technologies/ml-agents/tree/develop)     | [docs](https://github.com/Unity-Technologies/ml-agents/tree/develop/com.unity.ml-agents/Documentation~/index.md)   |    [download](https://github.com/Unity-Technologies/ml-agents/archive/develop.zip)     |                         --                            |                                          --                                           |\n\n\n\nIf you are a researcher interested in a discussion of Unity as an AI platform, see a pre-print of our [reference paper on Unity and the ML-Agents Toolkit](https://arxiv.org/abs/1809.02627).\n\nIf you use Unity or the ML-Agents Toolkit to conduct research, we ask that you cite the following paper as a reference:\n\n```\n@article{juliani2020,\n  title={Unity: A general platform for intelligent agents},\n  author={Juliani, Arthur and Berges, Vincent-Pierre and Teng, Ervin and Cohen, Andrew and Harper, Jonathan and Elion, Chris and Goy, Chris and Gao, Yuan and Henry, Hunter and Mattar, Marwan and Lange, Danny},\n  journal={arXiv preprint arXiv:1809.02627},\n  url={https://arxiv.org/pdf/1809.02627.pdf},\n  year={2020}\n}\n```\n\nAdditionally, if you use the MA-POCA trainer in your research, we ask that you cite the following paper as a reference:\n\n```\n@article{cohen2022,\n  title={On the Use and Misuse of Absorbing States in Multi-agent Reinforcement Learning},\n  author={Cohen, Andrew and Teng, Ervin and Berges, Vincent-Pierre and Dong, Ruo-Ping and Henry, Hunter and Mattar, Marwan and Zook, Alexander and Ganguly, Sujoy},\n  journal={RL in Games Workshop AAAI 2022},\n  url={http://aaai-rlg.mlanctot.info/papers/AAAI22-RLG_paper_32.pdf},\n  year={2022}\n}\n```\n\n\n## Additional Resources\n\n* [Unity Discussions](https://discussions.unity.com/tag/ml-agents)\n* [ML-Agents tutorials by CodeMonkeyUnity](https://www.youtube.com/playlist?list=PLzDRvYVwl53vehwiN_odYJkPBzcqFw110)\n* [Introduction to ML-Agents by Huggingface](https://huggingface.co/learn/deep-rl-course/en/unit5/introduction)\n* [Community created ML-Agents projects](https://discussions.unity.com/t/post-your-ml-agents-project/816756)\n* [ML-Agents models on Huggingface](https://huggingface.co/models?library=ml-agents)\n* [Blog posts](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/Blog-posts.html)\n* [Discord](https://discord.com/channels/489222168727519232/1202574086115557446)\n\n## Community and Feedback\n\nThe ML-Agents Toolkit is an open-source project and we encourage and welcome contributions. If you wish to contribute, be sure to review our [contribution guidelines](https://docs.unity3d.com/Packages/com.unity.ml-agents@latest/index.html?subfolder=/manual/CONTRIBUTING.html) and [code of conduct](https://github.com/Unity-Technologies/ml-agents/blob/release/4.0.0/CODE_OF_CONDUCT.md).\n\nFor problems with the installation and setup of the ML-Agents Toolkit, or discussions about how to best setup or train your agents, please create a new thread on the [Unity ML-Agents discussion forum](https://discussions.unity.com/tag/ml-agents). Be sure to include as many details as possible to help others assist you effectively. If you run into any other problems using the ML-Agents Toolkit or have a specific feature request, please [submit a GitHub issue](https://github.com/Unity-Technologies/ml-agents/issues).\n\nPlease tell us which samples you would like to see shipped with the ML-Agents Unity package by replying to [this discussion thread](https://discussions.unity.com/t/help-shape-the-future-of-ml-agents/1661019).\n\n## Privacy\n\nIn order to improve the developer experience for Unity ML-Agents Toolkit, we have added in-editor analytics. Please refer to "Information that is passively collected by Unity" in the [Unity Privacy Policy](https://unity3d.com/legal/privacy-policy).\n', '{"language":"C#","stars":18919,"forks":4397,"watchers":18919,"open_issues":54,"topics":["deep-learning","deep-reinforcement-learning","machine-learning","neural-networks","reinforcement-learning","unity","unity3d"],"default_branch":"develop","size_kb":3019543,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"},{"type":"has_code","target_id":"github:Unity-Technologies:ml-agents","source_url":"https://github.com/Unity-Technologies/ml-agents"}]', NULL, 'NOASSERTION', 'approved', 65, 'e1cfb29ac478ac460f6686612807e35f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Unity-Technologies-ml-agents from https://github.com/Unity-Technologies.png
Image converted to WebP: data/images/github-Unity-Technologies-ml-agents.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-iperov-DeepFaceLab', 'github--iperov--deepfacelab', 'DeepFaceLab', 'iperov', '<table align="center" border="0"> <tr><td colspan=2 align="center"> <a href="https://arxiv.org/abs/2005.05535"> <img src="https://static.arxiv.org/static/browse/0.3.0/images/icons/favicon.ico" width=14></img> https://arxiv.org/abs/2005.05535</a> </td></tr> <tr><td colspan=2 align="center"> <p align="center"> </p> DeepFaceLab is used by such popular youtube channels as | deeptomcruise| 1facerussia| arnoldschwarzneggar |---|---|---| | mariahcareyathome?| diepnep| mr__heisenberg| deepcaprio |---...', '["arxiv","creating-deepfakes","deep-face-swap","deep-learning","deep-neural-networks","deepface","deepfacelab","deepfakes","deeplearning","face-swap","faceswap","fakeapp","machine-learning","neural-nets","neural-networks","python"]', 'other', 18868, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/iperov/DeepFaceLab","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<table align="center" border="0">\n\n<tr><td colspan=2 align="center">\n\n# DeepFaceLab  \n\n<a href="https://arxiv.org/abs/2005.05535">\n\n<img src="https://static.arxiv.org/static/browse/0.3.0/images/icons/favicon.ico" width=14></img>\nhttps://arxiv.org/abs/2005.05535</a>\n\n</td></tr>\n<tr><td colspan=2 align="center">\n\n<p align="center">\n\n![](doc/logo_tensorflow.png)\n![](doc/logo_cuda.png)\n![](doc/logo_directx.png)\n\n</p>\n\nDeepFaceLab is used by such popular youtube channels as\n\n|![](doc/tiktok_icon.png) [deeptomcruise](https://www.tiktok.com/@deeptomcruise)|![](doc/tiktok_icon.png) [1facerussia](https://www.tiktok.com/@1facerussia)|![](doc/tiktok_icon.png) [arnoldschwarzneggar](https://www.tiktok.com/@arnoldschwarzneggar)\n|---|---|---|\n\n|![](doc/tiktok_icon.png) [mariahcareyathome?](https://www.tiktok.com/@mariahcareyathome?)|![](doc/tiktok_icon.png) [diepnep](https://www.tiktok.com/@diepnep)|![](doc/tiktok_icon.png) [mr__heisenberg](https://www.tiktok.com/@mr__heisenberg)|![](doc/tiktok_icon.png) [deepcaprio](https://www.tiktok.com/@deepcaprio)\n|---|---|---|---|\n\n|![](doc/youtube_icon.png) [VFXChris Ume](https://www.youtube.com/channel/UCGf4OlX_aTt8DlrgiH3jN3g/videos)|![](doc/youtube_icon.png) [Sham00k](https://www.youtube.com/channel/UCZXbWcv7fSZFTAZV4beckyw/videos)|\n|---|---|\n\n|![](doc/youtube_icon.png) [Collider videos](https://www.youtube.com/watch?v=A91P2qtPT54&list=PLayt6616lBclvOprvrC8qKGCO-mAhPRux)|![](doc/youtube_icon.png) [iFake](https://www.youtube.com/channel/UCC0lK2Zo2BMXX-k1Ks0r7dg/videos)|![](doc/youtube_icon.png) [NextFace](https://www.youtube.com/channel/UCFh3gL0a8BS21g-DHvXZEeQ/videos)|\n|---|---|---|\n\n|![](doc/youtube_icon.png) [Futuring Machine](https://www.youtube.com/channel/UCC5BbFxqLQgfnWPhprmQLVg)|![](doc/youtube_icon.png) [RepresentUS](https://www.youtube.com/channel/UCRzgK52MmetD9aG8pDOID3g)|![](doc/youtube_icon.png) [Corridor Crew](https://www.youtube.com/c/corridorcrew/videos)|\n|---|---|---|\n\n|![](doc/youtube_icon.png) [DeepFaker](https://www.youtube.com/channel/UCkHecfDTcSazNZSKPEhtPVQ)|![](doc/youtube_icon.png) [DeepFakes in movie](https://www.youtube.com/c/DeepFakesinmovie/videos)|\n|---|---|\n\n|![](doc/youtube_icon.png) [DeepFakeCreator](https://www.youtube.com/channel/UCkNFhcYNLQ5hr6A6lZ56mKA)|![](doc/youtube_icon.png) [Jarkan](https://www.youtube.com/user/Jarkancio/videos)|\n|---|---|\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n# What can I do using DeepFaceLab?\n\n</td></tr>\n<tr><td colspan=2 align="center">\n\n## Replace the face\n\n<img src="doc/replace_the_face.jpg" align="center">\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n## De-age the face\n\n</td></tr>\n\n<tr><td align="center" width="50%">\n\n<img src="doc/deage_0_1.jpg" align="center">\n\n</td>\n<td align="center" width="50%">\n\n<img src="doc/deage_0_2.jpg" align="center">\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n![](doc/youtube_icon.png) https://www.youtube.com/watch?v=Ddx5B-84ebo\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n## Replace the head\n\n</td></tr>\n\n<tr><td align="center" width="50%">\n\n<img src="doc/head_replace_1_1.jpg" align="center">\n\n</td>\n<td align="center" width="50%">\n\n<img src="doc/head_replace_1_2.jpg" align="center">\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n![](doc/youtube_icon.png) https://www.youtube.com/watch?v=RTjgkhMugVw\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n# Native resolution progress\n\n</td></tr>\n<tr><td colspan=2 align="center">\n\n<img src="doc/deepfake_progress.png" align="center">\n\n</td></tr>\n<tr><td colspan=2 align="center">\n\n<img src="doc/make_everything_ok.png" align="center">\n\nUnfortunately, there is no "make everything ok" button in DeepFaceLab. You should spend time studying the workflow and growing your skills. A skill in programs such as *AfterEffects* or *Davinci Resolve* is also desirable.\n\n</td></tr>\n<tr><td colspan=2 align="center">\n\n## Mini tutorial\n\n<a href="https://www.youtube.com/watch?v=kOIMXt8KK8M">\n\n<img src="doc/mini_tutorial.jpg" align="center">\n\n</a>\n\n</td></tr>\n<tr><td colspan=2 align="center">\n\n## Releases\n\n</td></tr>\n\n<tr><td align="right">\n<a href="https://tinyurl.com/2p9cvt25">Windows (magnet link)</a>\n</td><td align="center">Last release. Use torrent client to download.</td></tr>\n\n<tr><td align="right">\n<a href="https://mega.nz/folder/Po0nGQrA#dbbttiNWojCt8jzD4xYaPw">Windows (Mega.nz)</a>\n</td><td align="center">Contains new and prev releases.</td></tr>\n\n<tr><td align="right">\n<a href="https://disk.yandex.ru/d/7i5XTKIKVg5UUg">Windows (yandex.ru)</a>\n</td><td align="center">Contains new and prev releases.</td></tr>\n\n<tr><td align="right">\n<a href="https://github.com/nagadit/DeepFaceLab_Linux">Linux (github)</a>\n</td><td align="center">by @nagadit</td></tr>\n\n<tr><td align="right">\n<a href="https://github.com/elemantalcode/dfl">CentOS Linux (github)</a>\n</td><td align="center">May be outdated. By @elemantalcode</td></tr>\n\n</table>\n\n<table align="center" border="0">\n\n<tr><td colspan=2 align="center">\n\n### Communication groups\n\n</td></tr>\n\n<tr><td align="right">\n<a href="https://discord.gg/rxa7h9M6rH">Discord</a>\n</td><td align="center">Official discord channel. English / Russian.</td></tr>\n\n<tr><td colspan=2 align="center">\n\n## Related works\n\n</td></tr>\n\n<tr><td align="right">\n<a href="https://github.com/iperov/DeepFaceLive">DeepFaceLive</a>\n</td><td align="center">Real-time face swap for PC streaming or video calls</td></tr>\n\n</td></tr>\n</table>\n\n<table align="center" border="0">\n\n<tr><td colspan=2 align="center">\n\n## How I can help the project?\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n### Star this repo\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\nRegister github account and push "Star" button.\n\n</td></tr>\n\n</table>\n\n<table align="center" border="0">\n<tr><td colspan=2 align="center">\n\n## Meme zone\n\n</td></tr>\n\n<tr><td align="center" width="50%">\n\n<img src="doc/meme1.jpg" align="center">\n\n</td>\n\n<td align="center" width="50%">\n\n<img src="doc/meme2.jpg" align="center">\n\n</td></tr>\n\n<tr><td colspan=2 align="center">\n\n<sub>#deepfacelab #faceswap #face-swap #deep-learning #deeplearning #deep-neural-networks #deepface #deep-face-swap #neural-networks #neural-nets #tensorflow #cuda #nvidia</sub>\n\n</td></tr>\n\n\n\n</table>\n', '{"language":"Python","stars":18868,"forks":785,"watchers":18868,"open_issues":538,"topics":["arxiv","creating-deepfakes","deep-face-swap","deep-learning","deep-neural-networks","deepface","deepfacelab","deepfakes","deeplearning","face-swap","faceswap","fakeapp","machine-learning","neural-nets","neural-networks"],"default_branch":"master","size_kb":848594,"archived":true,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:nagadit:DeepFaceLab_Linux\">Linux","source_url":"https://github.com/nagadit/DeepFaceLab_Linux\">Linux"},{"type":"has_code","target_id":"github:elemantalcode:dfl\">CentOS","source_url":"https://github.com/elemantalcode/dfl\">CentOS"},{"type":"has_code","target_id":"github:iperov:DeepFaceLive\">DeepFaceLive<","source_url":"https://github.com/iperov/DeepFaceLive\">DeepFaceLive<"}]', NULL, 'GPL-3.0', 'approved', 65, '1e7526b64e2c0b3aa06217762eaf6d43', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-iperov-DeepFaceLab from https://github.com/iperov.png
Image converted to WebP: data/images/github-iperov-DeepFaceLab.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-amark-gun', 'github--amark--gun', 'gun', 'amark', '<p id="readme"><a href="https://gun.eco/"><img width="40%" src="https://cldup.com/TEy9yGh45l.svg"/></a><img width="50%" align="right" vspace="25" src="https://gun.eco/see/demo.gif"/></p> !Build **GUN** is an ecosystem of **tools** that let you build community run and encrypted applications - like an Open Source Firebase or a Decentralized Dropbox. The Internet Archive and 100s of other apps run GUN in-production. + Multiplayer by default with realtime p2p state synchronization! + Graph data l...', '["artificial-intelligence","big-data","blockchain","crdt","crypto","cryptography","dapp","database","decentralized","dweb","encryption","end-to-end","graph","machine-learning","metaverse","offline-first","p2p","protocol","realtime","web3","javascript"]', 'other', 18785, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/amark/gun","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p id="readme"><a href="https://gun.eco/"><img width="40%" src="https://cldup.com/TEy9yGh45l.svg"/></a><img width="50%" align="right" vspace="25" src="https://gun.eco/see/demo.gif"/></p>\n\n[![](https://data.jsdelivr.com/v1/package/npm/gun/badge)](https://www.jsdelivr.com/package/npm/gun)\n![Build](https://github.com/amark/gun/actions/workflows/ci.yml/badge.svg)\n[![Gitter](https://img.shields.io/gitter/room/amark/gun.js.svg)](http://chat.gun.eco)\n\n**GUN** is an [ecosystem](https://gun.eco/docs/Ecosystem) of **tools** that let you build [community run](https://www.nbcnews.com/tech/tech-news/these-technologists-think-internet-broken-so-they-re-building-another-n1030136) and [encrypted applications](https://gun.eco/docs/Cartoon-Cryptography) - like an Open Source Firebase or a Decentralized Dropbox.\n\nThe [Internet Archive](https://news.ycombinator.com/item?id=17685682) and [100s of other apps](https://github.com/amark/gun/wiki/awesome-gun) run GUN in-production.\n\n + Multiplayer by default with realtime p2p state synchronization!\n + Graph data lets you use key/value, tables, documents, videos, & more!\n + Local-first, offline, and decentralized with end-to-end encryption.\n\nDecentralized alternatives to [Zoom](https://www.zdnet.com/article/era-hatches-meething-an-open-source-browser-based-video-conferencing-system/), [Reddit](https://notabug.io/t/whatever/comments/36588a16b9008da4e3f15663c2225e949eca4a15/gpu-bot-test), [Instagram](https://iris.to/), [Slack](https://iris.to/), [YouTube](https://d.tube/), [Stripe](https://twitter.com/marknadal/status/1422717427427647489), [Wikipedia](https://news.ycombinator.com/item?id=17685682), Facebook [Horizon](https://twitter.com/marknadal/status/1424476179189305347) and more have already pushed terabytes of daily P2P traffic on GUN. We are a [friendly community](http://chat.gun.eco/) creating a [free fun future for freedom](https://youtu.be/1HJdrBk3BlE):\n\n<table>\n<tr>\n<a href="https://youtu.be/s_m16-w6bBI"><img width="31%" src="https://gun.eco/see/3dvr.gif" title="3D VR"/></a>\n<a href="https://github.com/cstefanache/cstefanache.github.io/blob/06697003449e4fc531fd32ee068bab532976f47b/_posts/2016-08-02-gun-db-artificial-knowledge-sharing.md"><img width="31%" src="https://gun.eco/see/aiml.gif" title="AI/ML"/></a>\n<a href="http://gps.gunDB.io/"><img width="31%" src="https://gun.eco/see/gps.gif" title="GPS"/></a>\n</tr>\n<tr>\n<a href="https://github.com/lmangani/gun-scape#gun-scape"><img width="31%" src="https://gun.eco/see/dataviz.gif" title="Data Viz"/></a>\n<a href="https://github.com/amark/gun/wiki/Auth"><img width="31%" src="https://gun.eco/see/p2p.gif" title="P2P"/></a>\n<a href="https://github.com/Stefdv/gun-ui-lcd#okay-what-about-gundb-"><img width="31%" src="https://gun.eco/see/iot.gif" title="IoT"/></a>\n</tr>\n<tr>\n<a href="http://chat.gun.eco"><img width="31%" src="https://gun.eco/see/vr-world.gif" title="VR World"/></a>\n<a href="https://youtu.be/1ASrmQ-CwX4"><img width="31%" src="https://gun.eco/see/ar.gif" title="AR"/></a>\n<a href="https://meething.space/"><img width="31%" src="https://gun.eco/see/video-conf.gif" title="Video Confernece"/></a>\n</tr>\n</table>\n\n## Quickstart\n\nGUN is *super easy* to get started with:\n\n - Try the [interactive tutorial](https://gun.eco/docs/Todo-Dapp) in the browser (**5min** ~ average developer).\n - Or `npm install gun` and run the examples with `cd node_modules/gun && npm start` (**5min** ~ average developer).\n\n> **Note:** If you don''t have [node](http://nodejs.org/) or [npm](https://www.npmjs.com/), read [this](https://github.com/amark/gun/blob/master/examples/install.sh) first.\n> If the `npm` command line didn''t work, you may need to `mkdir node_modules` first or use `sudo`.\n\n- An online demo of the examples are available here: http://try.axe.eco/\n- Or write a quick app: ([try now in a playground](https://jsbin.com/kadobamevo/edit?js,console))\n```html\n<script src="https://cdn.jsdelivr.net/npm/gun/gun.js"></script>\n<script>\n// import GUN from ''gun''; // in ESM\n// GUN = require(''gun''); // in NodeJS\n// GUN = require(''gun/gun''); // in React\ngun = GUN();\n\ngun.get(''mark'').put({\n  name: "Mark",\n  email: "mark@gun.eco",\n});\n\ngun.get(''mark'').on((data, key) => {\n  console.log("realtime updates:", data);\n});\n\nsetInterval(() => { gun.get(''mark'').get(''live'').put(Math.random()) }, 9);\n</script>\n```\n- Or try something **mind blowing**, like saving circular references to a table of documents! ([play](http://jsbin.com/wefozepume/edit?js,console))\n```javascript\ncat = {name: "Fluffy", species: "kitty"};\nmark = {boss: cat};\ncat.slave = mark;\n\n// partial updates merge with existing data!\ngun.get(''mark'').put(mark);\n\n// access the data as if it is a document.\ngun.get(''mark'').get(''boss'').get(''name'').once(function(data, key){\n  // `once` grabs the data once, no subscriptions.\n  console.log("Mark''s boss is", data);\n});\n\n// traverse a graph of circular references!\ngun.get(''mark'').get(''boss'').get(''slave'').once(function(data, key){\n  console.log("Mark is the cat''s slave!", data);\n});\n\n// add both of them to a table!\ngun.get(''list'').set(gun.get(''mark'').get(''boss''));\ngun.get(''list'').set(gun.get(''mark''));\n\n// grab each item once from the table, continuously:\ngun.get(''list'').map().once(function(data, key){\n  console.log("Item:", data);\n});\n\n// live update the table!\ngun.get(''list'').set({type: "cucumber", goal: "jumping cat"});\n```\n\nWant to keep building more? **Jump to [THE DOCUMENTATION](#documentation)!**\n\n# About\nFirst & foremost, GUN is **a community of the nicest and most helpful people** out there. So [I want to invite you](http://chat.gun.eco) to come tell us about what **you** are working on & wanting to build (new or old school alike! Just be nice as well.) and ask us your questions directly. :)\n\n<p align="center"><a href="https://www.youtube.com/watch?v=oTQXzhm8w_8"><img width="250" src="https://img.youtube.com/vi/oTQXzhm8w_8/0.jpg"><br/>Watch the 100 second intro!</a></p>\n\nThe GUN ecosystem stack is a collection of independent and modular tools covering everything from [CRDT](https://crdt.tech/) [conflict resolution](https://gun.eco/distributed/matters.html), [cryptographic security](https://gun.eco/docs/Cartoon-Cryptography) & [encryption](https://gun.eco/docs/SEA), [radix storage serialization](https://gun.eco/docs/RAD), [mesh networking](https://gun.eco/docs/DAM) & [routing algorithms](https://gun.eco/docs/Routing), to distributed systems [correctness & load testing](https://github.com/gundb/panic-server), CPU scheduled [JSON parser](https://github.com/amark/gun/blob/master/lib/yson.js) to prevent UI lag, and more!\n\n<div><img width="48%" src="https://gun.eco/see/stack.png"/>\n<img width="48%" align="right" src="https://gun.eco/see/layers.png"/></div>\n\nOn that note, let''s get some official shout outs covered first:\n\n### Support\n\n<p align="center">\nThanks to:\n\n<table>\n<tr>\n<td vlign="center"><a href="https://mozilla.org/builders"><img height="100" src="https://user-images.githubusercontent.com/1423657/81992335-85346480-9643-11ea-8754-8275e98e06bc.png"></a></td>\n<td vlign="center"><a href="http://unstoppabledomains.com/"><img src="https://gun.eco/img/unstoppable.png"></a></td>\n<td vlign="center"><a href="https://mask.io/"><img src="https://dimensiondev.github.io/Mask-VI/assets/Logo/MB--Logo--CombH-Circle--Blue.svg" width="250"></a></td>\n</tr>\n<tr>\n<td vlign="center">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.ajar.org/"><img src="https://www.ajar.org/logo.png" height="120"></a></td>\n<td vlign="center"><a href="https://wallie.io/"><img src="https://raw.githubusercontent.com/gundb/gun-site/master/img/wallie.png" width="250"></a></td>\n<td vlign="center">&nbsp;&nbsp;<a href="https://ghostdrive.com/"><img src="https://gun.eco/img/ghostdrive.png" height="120"></a></td>\n</tr>\n</table>\n\n<a href="https://github.com/robertheessels">Robert Heessels</a>,\n<a href="http://qxip.net/">Lorenzo Mangani</a>,\n<a href="https://nlnet.nl/">NLnet Foundation</a>,\n<a href="http://github.com/samliu">Sam Liu</a>,\n<a href="http://github.com/ddombrow">Daniel Dombrowsky</a>,\n<a href="http://github.com/vincentwoo">Vincent Woo</a>,\n<a href="http://github.com/coolaj86">AJ ONeal</a>,\n<a href="http://github.com/ottman">Bill Ottman</a>,\n<a href="http://github.com/mikewlange">Mike Lange</a>,\n<a href="http://github.com/ctrlplusb">Sean Matheson</a>,\n<a href="http://github.com/alanmimms">Alan Mimms</a>,\n<a href="https://github.com/dfreire">DÃ¡rio Freire</a>,\n<a href="http://github.com/velua">John Williamson</a>,\n<a href="http://github.com/finwo">Robin Bron</a>,\n<a href="http://github.com/ElieMakhoul">Elie Makhoul</a>,\n<a href="http://github.com/mikestaub">Mike Staub</a>,\n<a href="http://github.com/bmatusiak">Bradley Matusiak</a>,\n<a href="https://github.com/sjuxax">Jeff Cook</a>,\n<a href="https://github.com/nmauersberg">Nico</a>,\n<a href="https://github.com/ajartille">Aaron Artille</a>,\n<a href="https://github.com/timjrobinson">Tim Robinson</a>,\n<a href="https://github.com/hibas123">Fabian Stamm</a>,\n<a href="https://twitter.com/mikestaub">Mike Staub</a>,\n<a href="https://hunterowens.com/">Hunter Owens</a>,\n<a href="https://github.com/JacobMillner">Jacob Millner</a>,\n<a href="https://github.com/b-lack">Gerrit Balindt</a>,\n<a href="https://github.com/gabriellemon">Gabriel Lemon</a>,\n<a href="https://github.com/murageyun">Murage Martin</a>,\n<a href="https://github.com/octalmage">Jason Stallings</a>\n</p>\n\n - Join others in sponsoring code: https://www.patreon.com/gunDB !\n - Ask questions: http://stackoverflow.com/questions/tagged/gun ?\n - Found a bug? Report at: https://github.com/amark/gun/issues ;\n - **Need help**? Chat with us: http://chat.gun.eco .\n\n### History\n\n[GUN](https://gun.eco) was created by [Mark Nadal](https://twitter.com/marknadal) in 2014 after he had spent 4 years trying to get his collaborative web app to scale up with traditional databases.\n\n<img width="250px" src="https://gun.eco/see/problem.png" align="left" title="pain point" style="margin: 0 1em 1em 0"> After he realized [Master-Slave database architecture causes one big bottleneck](https://gun.eco/distributed/matters.html), he (as a complete newbie outsider) naively decided **to question the status quo** and shake things up with controversial, heretical, and contrarian experiments:\n\n**The NoDB** - no master, no servers, no "single source of truth", not built with a real programming language or real hardware, no DevOps, no locking, not *just* SQL or NoSQL but both (**all** - graphs, documents, tables, key/value).\n\nThe goal was to build a P2P database that could survive living inside **any** browser, and could correctly sync data between **any** device after assuming **any** offline-first activity.\n\n<img src="https://gun.eco/see/compare.png" title="comparison table">\n\nTechnically, **GUN is a graph synchronization protocol** with a *lightweight embedded engine*, capable of doing *[20M+ API ops/sec](https://gun.eco/docs/Performance)* in **just ~9KB gzipped size**.\n\n## Documentation\n\n<table>\n  <tr>\n    <td style="border: 0;"><h3><a href="https://gun.eco/docs/API">API reference</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://gun.eco/docs/Todo-Dapp">Tutorials</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://github.com/amark/gun/tree/master/examples">Examples</a></h3></td>\n  </tr>\n  <tr>\n    <td style="border: 0;"><h3><a href="https://github.com/brysgo/graphql-gun">GraphQL</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://github.com/PenguinMan98/electrontest">Electron</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://gun.eco/docs/React-Native">React & Native</a></h3></td>\n  </tr>\n  <tr>\n    <td style="border: 0;"><h3><a href="https://github.com/sjones6/vue-gun">Vue</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://gun.eco/docs/Svelte">Svelte</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://github.com/Stefdv/gun-ui-lcd#syncing">Webcomponents</a></h3></td>\n  </tr>\n  <tr>\n    <td style="border: 0;"><h3><a href="https://gun.eco/docs/CAP-Theorem">CAP Theorem Tradeoffs</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://gun.eco/distributed/matters.html">How Data Sync Works</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://gun.eco/docs/Porting-GUN">How GUN is Built</a></h3></td>\n  </tr>\n  <tr>\n    <td style="border: 0;"><h3><a href="https://gun.eco/docs/Auth">Crypto Auth</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://github.com/amark/gun/wiki/Awesome-GUN">Modules</a></h3></td>\n    <td style="border: 0;"><h3><a href="https://gun.eco/docs/Roadmap">Roadmap</a></h3></td>\n  </tr>\n</table>\n\nThis would not be possible without **community contributors**, big shout out to:\n\n**[ajmeyghani](https://github.com/ajmeyghani) ([Learn GUN Basics with Diagrams](https://medium.com/@ajmeyghani/gundb-a-graph-database-in-javascript-3860a08d873c))**; **[anywhichway](https://github.com/anywhichway) ([Block Storage](https://github.com/anywhichway/gun-block))**; **[beebase](https://github.com/beebase) ([Quasar](https://github.com/beebase/gun-vuex-quasar))**; **[BrockAtkinson](https://github.com/BrockAtkinson) ([brunch config](https://github.com/BrockAtkinson/brunch-gun))**; **[Brysgo](https://github.com/brysgo) ([GraphQL](https://github.com/brysgo/graphql-gun))**; **[d3x0r](https://github.com/d3x0r) ([SQLite](https://github.com/d3x0r/gun-db))**; **[forrestjt](https://github.com/forrestjt) ([file.js](https://github.com/amark/gun/blob/master/lib/file.js))**; **[hillct](https://github.com/hillct) (Docker)**; **[JosePedroDias](https://github.com/josepedrodias) ([graph visualizer](http://acor.sl.pt:9966))**; **[JuniperChicago](https://github.com/JuniperChicago) ([cycle.js bindings](https://github.com/JuniperChicago/cycle-gun))**; **[jveres](https://github.com/jveres) ([todoMVC](https://github.com/jveres/todomvc))**; **[kristianmandrup](https://github.com/kristianmandrup) ([edge](https://github.com/kristianmandrup/gun-edge))**; **[Lightnet](https://github.com/Lightnet)** ([Awesome Vue User Examples](https://glitch.com/edit/#!/jsvuegunui?path=README.md:1:0) & [User Kitchen Sink Playground](https://gdb-auth-vue-node.glitch.me/)); **[lmangani](https://github.com/lmangani) ([Cytoscape Visualizer](https://github.com/lmangani/gun-scape), [Cassandra](https://github.com/lmangani/gun-cassandra), [Fastify](https://github.com/lmangani/fastify-gundb), [LetsEncrypt](https://github.com/lmangani/polyGun-letsencrypt))**; **[mhelander](https://github.com/mhelander) ([SEA](https://github.com/amark/gun/blob/master/sea.js))**; [omarzion](https://github.com/omarzion) ([Sticky Note App](https://github.com/omarzion/stickies)); [PsychoLlama](https://github.com/PsychoLlama) ([LevelDB](https://github.com/PsychoLlama/gun-level)); **[RangerMauve](https://github.com/RangerMauve) ([schema](https://github.com/gundb/gun-schema))**; **[robertheessels](https://github.com/swifty) ([gun-p2p-auth](https://github.com/swifty/gun-p2p-auth))**; **[rogowski](https://github.com/rogowski) (AXE)**; [sbeleidy](https://github.com/sbeleidy); **[sbiaudet](https://github.com/sbiaudet) ([C# Port](https://github.com/sbiaudet/cs-gun))**; **[Sean Matheson](https://github.com/ctrlplusb) ([Observable/RxJS/Most.js bindings](https://github.com/ctrlplusb/gun-most))**; **[Shadyzpop](https://github.com/Shadyzpop) ([React Native example](https://github.com/amark/gun/tree/master/examples/react-native))**; **[sjones6](https://github.com/sjones6) ([Flint](https://github.com/sjones6/gun-flint))**; RIP **[Stefdv](https://github.com/stefdv) (Polymer/web components)**; **[zrrrzzt](https://github.com/zrrrzzt) ([JWT Auth](https://gist.github.com/zrrrzzt/6f88dc3cedee4ee18588236756d2cfce))**; **[xmonader](https://github.com/xmonader) ([Python Port](https://github.com/xmonader/pygundb))**; \n\nI am missing many others, apologies, will be adding them soon! This list is infinitely old & way out of date, if you want to be listed in it please make a PR! :)\n\n## Testing\n\nYou will need to `npm install -g mocha` first. Then in the gun root folder run `npm test`. Tests will trigger persistent writes to the DB, so subsequent runs of the test will fail. You must clear the DB before running the tests again. This can be done by running `rm -rf *data*` command in the project directory.\n\n## Shims\n\n > These are only needed for NodeJS & React Native, they shim the native Browser WebCrypto API.\n\nIf you want to use [SEA](https://gun.eco/docs/SEA) for `User` auth and security, you will need to install:\n\n`npm install @peculiar/webcrypto --save`\n\nPlease see [our React Native docs](https://gun.eco/docs/React-Native) for installation instructions!\n\nThen you can require [SEA](https://gun.eco/docs/SEA) without an error:\n\n```javascript\nGUN = require(''gun/gun'');\nSEA = require(''gun/sea'');\n```\n\n## Deploy\n\n > Note: The default examples that get auto-deployed on `npm start` CDN-ify all GUN files, modules, & storage.\n \n > Note: Moving forward, AXE will start to automatically cluster your peer into a shared DHT. You may want to disable this to run an isolated network.\n \n > Note: When deploying a web application using GUN on a cloud provider, you may have to set `CI=false` in your `.env`. This prevents GUN-specific warnings from being treated as errors when deploying your app. You may also resolve this by modifying your webpack config to not try to build the GUN dependencies.\n\nTo quickly spin up a GUN relay peer for your development team, utilize [Heroku](http://heroku.com), [Docker](http://docker.com), or any others listed below. Or some variant thereof [Dokku](http://dokku.viewdocs.io/dokku/), K8s, etc. ! Or use all of them so your relays are decentralized too!\n\n### Linux\n\n`SSH` into the home directory of a clean OS install with `sudo` ability. Set any environment variables you need (see below), then do:\n\n```bash\ncurl -o- https://raw.githubusercontent.com/amark/gun/master/examples/install.sh | bash\n```\n\n > Read [install.sh](https://github.com/amark/gun/blob/master/examples/install.sh) first!\n > If `curl` is not found, *copy&paste* the contents of install.sh into your ssh.\n\nYou can now safely `CTRL+A+D` to escape without stopping the peer. To stop everything `killall screen` or `killall node`.\n\nEnvironment variables may need to be set like `export HTTPS_CERT=~/cert.pem HTTPS_KEY=~/key.pem PORT=443`. You can also look at a sample [nginx](https://gun.eco/docs/nginx) config. For production deployments, you probably will want to use something like `pm2` or better to keep the peer alive after machine reboots.\n\n### [Dome](https://www.trydome.io/)\n[Deploy GUN in one-click](https://app.trydome.io/signup?package=gun) with [Dome](https://trydome.io) and receive a free trial:\n\n[![Deploy to Dome](https://trydome.io/button.svg)](https://app.trydome.io/signup?package=gun)\n\n### [Heroku](https://www.heroku.com/)\n\n[![Deploy](https://www.herokucdn.com/deploy/button.svg)](https://heroku.com/deploy?template=https://github.com/amark/gun)\n\n > Heroku deletes your data every 15 minutes, one way to fix this is by adding [cheap storage](https://gun.eco/docs/Using-Amazon-S3-for-Storage).\n\nOr:\n\n```bash\ngit clone https://github.com/amark/gun.git\ncd gun\nheroku create\ngit push -f heroku HEAD:master\n```\n\nThen visit the URL in the output of the ''heroku create'' step, in a browser. Make sure to set any environment config vars in the settings tab.\n\n### [Zeet.co](https://www.zeet.co/)\n\n[![Deploy](https://deploy.zeet.co/gun.svg)](https://deploy.zeet.co/?url=https://github.com/amark/gun)\n\nThen visit the URL in the output of the ''now --npm'' step, in your browser.\n\n### [Docker](https://www.docker.com/)\n\n > Warning: Docker image is community contributed and may be old with missing security updates, please check version numbers to compare.\n\n[![Docker Automated build](https://img.shields.io/docker/automated/gundb/gun.svg)](https://hub.docker.com/r/gundb/gun/) [![](https://images.microbadger.com/badges/image/gundb/gun.svg)](https://microbadger.com/images/gundb/gun "Get your own image badge on microbadger.com") [![Docker Pulls](https://img.shields.io/docker/pulls/gundb/gun.svg)](https://hub.docker.com/r/gundb/gun/) [![Docker Stars](https://img.shields.io/docker/stars/gundb/gun.svg)](https://hub.docker.com/r/gundb/gun/)\n\nPull from the [Docker Hub](https://hub.docker.com/r/gundb/gun/) [![](https://images.microbadger.com/badges/commit/gundb/gun.svg)](https://microbadger.com/images/gundb/gun). Or:\n\n```bash\ndocker run -p 8765:8765 gundb/gun\n```\n\nOr build the [Docker](https://docs.docker.com/engine/installation/) image locally:\n\n```bash\ngit clone https://github.com/amark/gun.git\ncd gun\ndocker build -t myrepo/gundb:v1 .\ndocker run -p 8765:8765 myrepo/gundb:v1\n```\n\nOr, if you prefer your Docker image with metadata labels (Linux/Mac only):\n\n```bash\nnpm run docker\ndocker run -p 8765:8765 username/gun:git\n```\n\nThen visit [http://localhost:8765](http://localhost:8765) in your browser.\n\n## License\n\nDesigned with â™¥ by Mark Nadal, the GUN team, and many amazing contributors.\n\nOpenly licensed under [Zlib / MIT / Apache 2.0](https://github.com/amark/gun/blob/master/LICENSE.md).\n\n[![FOSSA Status](https://app.fossa.io/api/projects/git%2Bhttps%3A%2F%2Fgithub.com%2Famark%2Fgun.svg?size=large)](https://app.fossa.io/projects/git%2Bhttps%3A%2F%2Fgithub.com%2Famark%2Fgun?ref=badge_large)\n\n[YouTube](https://www.youtube.com/channel/UCQAtpf-zi9Pp4__2nToOM8g) . [Twitter](https://twitter.com/marknadal)\n', '{"language":"JavaScript","stars":18785,"forks":1221,"watchers":18785,"open_issues":312,"topics":["artificial-intelligence","big-data","blockchain","crdt","crypto","cryptography","dapp","database","decentralized","dweb","encryption","end-to-end","graph","machine-learning","metaverse","offline-first","p2p","protocol","realtime","web3"],"default_branch":"master","size_kb":32905,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:cstefanache:cstefanache.github.io","source_url":"https://github.com/cstefanache/cstefanache.github.io"},{"type":"has_code","target_id":"github:lmangani:gun-scape","source_url":"https://github.com/lmangani/gun-scape#gun-scape\"><img"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:Stefdv:gun-ui-lcd","source_url":"https://github.com/Stefdv/gun-ui-lcd#okay-what-about-gundb-\"><img"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:gundb:panic-server","source_url":"https://github.com/gundb/panic-server"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:nmauersberg\">Nico<:a>,","source_url":"https://github.com/nmauersberg\">Nico</a>,"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:brysgo:graphql-gun\">GraphQL<","source_url":"https://github.com/brysgo/graphql-gun\">GraphQL<"},{"type":"has_code","target_id":"github:PenguinMan98:electrontest\">Electron<","source_url":"https://github.com/PenguinMan98/electrontest\">Electron<"},{"type":"has_code","target_id":"github:sjones6:vue-gun\">Vue<","source_url":"https://github.com/sjones6/vue-gun\">Vue<"},{"type":"has_code","target_id":"github:Stefdv:gun-ui-lcd","source_url":"https://github.com/Stefdv/gun-ui-lcd#syncing\">Webcomponents<"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:anywhichway:gun-block","source_url":"https://github.com/anywhichway/gun-block"},{"type":"has_code","target_id":"github:beebase:gun-vuex-quasar","source_url":"https://github.com/beebase/gun-vuex-quasar"},{"type":"has_code","target_id":"github:BrockAtkinson:brunch-gun","source_url":"https://github.com/BrockAtkinson/brunch-gun"},{"type":"has_code","target_id":"github:brysgo:graphql-gun","source_url":"https://github.com/brysgo/graphql-gun"},{"type":"has_code","target_id":"github:d3x0r:gun-db","source_url":"https://github.com/d3x0r/gun-db"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:JuniperChicago:cycle-gun","source_url":"https://github.com/JuniperChicago/cycle-gun"},{"type":"has_code","target_id":"github:jveres:todomvc","source_url":"https://github.com/jveres/todomvc"},{"type":"has_code","target_id":"github:kristianmandrup:gun-edge","source_url":"https://github.com/kristianmandrup/gun-edge"},{"type":"has_code","target_id":"github:lmangani:gun-scape","source_url":"https://github.com/lmangani/gun-scape"},{"type":"has_code","target_id":"github:lmangani:gun-cassandra","source_url":"https://github.com/lmangani/gun-cassandra"},{"type":"has_code","target_id":"github:lmangani:fastify-gundb","source_url":"https://github.com/lmangani/fastify-gundb"},{"type":"has_code","target_id":"github:lmangani:polyGun-letsencrypt","source_url":"https://github.com/lmangani/polyGun-letsencrypt"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:omarzion:stickies","source_url":"https://github.com/omarzion/stickies"},{"type":"has_code","target_id":"github:PsychoLlama:gun-level","source_url":"https://github.com/PsychoLlama/gun-level"},{"type":"has_code","target_id":"github:gundb:gun-schema","source_url":"https://github.com/gundb/gun-schema"},{"type":"has_code","target_id":"github:swifty:gun-p2p-auth","source_url":"https://github.com/swifty/gun-p2p-auth"},{"type":"has_code","target_id":"github:sbiaudet:cs-gun","source_url":"https://github.com/sbiaudet/cs-gun"},{"type":"has_code","target_id":"github:ctrlplusb:gun-most","source_url":"https://github.com/ctrlplusb/gun-most"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:sjones6:gun-flint","source_url":"https://github.com/sjones6/gun-flint"},{"type":"has_code","target_id":"github:xmonader:pygundb","source_url":"https://github.com/xmonader/pygundb"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:amark:gun.git","source_url":"https://github.com/amark/gun.git"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"},{"type":"has_code","target_id":"github:amark:gun.git","source_url":"https://github.com/amark/gun.git"},{"type":"has_code","target_id":"github:amark:gun","source_url":"https://github.com/amark/gun"}]', NULL, 'NOASSERTION', 'approved', 80, 'c175fb4df68a242905c9453f8af1695f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-amark-gun from https://github.com/amark.png
Image converted to WebP: data/images/github-amark-gun.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-afshinea-stanford-cs-229-machine-learning', 'github--afshinea--stanford-cs-229-machine-learning', 'stanford-cs-229-machine-learning', 'afshinea', 'Available in Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© - English - EspaÃ±ol - ÙØ§Ø±Ø³ÛŒ - FranÃ§ais - í•œêµ­ì–´ - PortuguÃªs - TÃ¼rkÃ§e - Tiáº¿ng Viá»‡t - ç®€ä¸­ - ç¹ä¸­ This repository aims at summing up in the same place all the important notions that are covered in Stanford''s CS 229 Machine Learning course, and include: - **Refreshers** in related topics that highlight the key points of the **prerequisites of the course**. - **Cheatsheets for each machine learning field**, as well as another dedicated to tips and tricks to have in mind when trainin...', '["cheatsheet","cs229","data-science","deep-learning","machine-learning","ml-cheatsheet","supervised-learning","unsupervised-learning"]', 'other', 18764, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Machine Learning cheatsheets for Stanford''s CS 229\n\nAvailable in [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/ar) -  [English](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/en) -  [EspaÃ±ol](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/es) -  [ÙØ§Ø±Ø³ÛŒ](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/fa) -  [FranÃ§ais](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/fr) -  [í•œêµ­ì–´](https://stanford.edu/~shervine/l/ko/teaching/cs-229/cheatsheet-machine-learning-tips-and-tricks) -  [PortuguÃªs](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/pt) -  [TÃ¼rkÃ§e](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/tr) - [Tiáº¿ng Viá»‡t](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/vi) -  [ç®€ä¸­](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/zh) -  [ç¹ä¸­](https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/zh-tw)\n\n## Goal\nThis repository aims at summing up in the same place all the important notions that are covered in Stanford''s CS 229 Machine Learning course, and include:\n- **Refreshers** in related topics that highlight the key points of the **prerequisites of the course**.\n- **Cheatsheets for each machine learning field**, as well as another dedicated to tips and tricks to have in mind when training a model.\n- All elements of the above combined in an **ultimate compilation of concepts**, to have with you at all times!\n\n## Content\n#### VIP Cheatsheets\n|<a href="https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/cheatsheet-supervised-learning.pdf"><img src="https://stanford.edu/~shervine/teaching/cs-229/illustrations/cover/en-001.png?" alt="Illustration" width="220px"/></a>|<a href="https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/cheatsheet-unsupervised-learning.pdf"><img src="https://stanford.edu/~shervine/teaching/cs-229/illustrations/cover/en-002.png" alt="Illustration" width="220px"/></a>|<a href="https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/cheatsheet-deep-learning.pdf"><img src="https://stanford.edu/~shervine/teaching/cs-229/illustrations/cover/en-003.png" alt="Illustration" width="220px"/></a>|<a href="https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/cheatsheet-machine-learning-tips-and-tricks.pdf"><img src="https://stanford.edu/~shervine/teaching/cs-229/illustrations/cover/en-004.png" alt="Illustration" width="220px"/></a>|\n|:--:|:--:|:--:|:--:|\n|Supervised Learning|Unsupervised Learning|Deep Learning|Tips and tricks|\n\n#### VIP Refreshers\n|<a href="https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/refresher-probabilities-statistics.pdf"><img src="https://stanford.edu/~shervine/teaching/cs-229/illustrations/cover/en-005.png" alt="Illustration" width="220px"/></a>|<a href="https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/refresher-algebra-calculus.pdf"><img src="https://stanford.edu/~shervine/teaching/cs-229/illustrations/cover/en-006.png#1" alt="Illustration" width="220px"/></a>|\n|:--:|:--:|\n|Probabilities and Statistics|Algebra and Calculus|\n\n\n#### Super VIP Cheatsheet\n|<a href="https://github.com/afshinea/stanford-cs-229-machine-learning/blob/master/en/super-cheatsheet-machine-learning.pdf"><img src="https://stanford.edu/~shervine/teaching/cs-229/illustrations/cover/en-007.png" alt="Illustration" width="400px"/></a>|\n|:--:|\n|All the above gathered in one place|\n\n## Website\nThis material is also available on a dedicated [website](https://stanford.edu/~shervine/teaching/cs-229), so that you can enjoy reading it from any device.\n\n## Translation\nWould you like to see these cheatsheets in your native language? You can help us translating them on [this dedicated repo](https://github.com/shervinea/cheatsheet-translation)!\n\n## Authors\n[Afshine Amidi](https://twitter.com/afshinea) (Ecole Centrale Paris, MIT) and [Shervine Amidi](https://twitter.com/shervinea) (Ecole Centrale Paris, Stanford University)\n', '{"language":null,"stars":18764,"forks":4115,"watchers":18764,"open_issues":20,"topics":["cheatsheet","cs229","data-science","deep-learning","machine-learning","ml-cheatsheet","supervised-learning","unsupervised-learning"],"default_branch":"master","size_kb":38339,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:afshinea:stanford-cs-229-machine-learning","source_url":"https://github.com/afshinea/stanford-cs-229-machine-learning"},{"type":"has_code","target_id":"github:shervinea:cheatsheet-translation","source_url":"https://github.com/shervinea/cheatsheet-translation"}]', NULL, 'MIT', 'approved', 65, 'ef9bb54ad7e4eadb52330d57ee5edb61', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-afshinea-stanford-cs-229-machine-learning from https://github.com/afshinea.png
Image converted to WebP: data/images/github-afshinea-stanford-cs-229-machine-learning.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-onnxruntime', 'github--microsoft--onnxruntime', 'onnxruntime', 'microsoft', '<p align="center"><img width="50%" src="docs/images/ONNX_Runtime_logo_dark.png" /></p> **ONNX Runtime is a cross-platform inference and training machine-learning accelerator**. **ONNX Runtime inference** can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, driver...', '["ai-framework","deep-learning","hardware-acceleration","machine-learning","neural-networks","onnx","pytorch","scikit-learn","tensorflow","c++"]', 'other', 18600, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/onnxruntime","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center"><img width="50%" src="docs/images/ONNX_Runtime_logo_dark.png" /></p>\n\n**ONNX Runtime is a cross-platform inference and training machine-learning accelerator**.\n\n**ONNX Runtime inference** can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms. [Learn more &rarr;](https://www.onnxruntime.ai/docs/#onnx-runtime-for-inferencing)\n\n**ONNX Runtime training** can accelerate the model training time on multi-node NVIDIA GPUs for transformer models with a one-line addition for existing PyTorch training scripts. [Learn more &rarr;](https://www.onnxruntime.ai/docs/#onnx-runtime-for-training)\n\n## Get Started & Resources\n\n* **General Information**: [onnxruntime.ai](https://onnxruntime.ai)\n\n* **Usage documentation and tutorials**: [onnxruntime.ai/docs](https://onnxruntime.ai/docs)\n\n* **YouTube video tutorials**: [youtube.com/@ONNXRuntime](https://www.youtube.com/@ONNXRuntime)\n\n* [**Upcoming Release Roadmap**](https://onnxruntime.ai/roadmap)\n\n* **Companion sample repositories**:\n  - ONNX Runtime Inferencing: [microsoft/onnxruntime-inference-examples](https://github.com/microsoft/onnxruntime-inference-examples)\n  - ONNX Runtime Training: [microsoft/onnxruntime-training-examples](https://github.com/microsoft/onnxruntime-training-examples)\n\n## Releases\n\nThe current release and past releases can be found here: https://github.com/microsoft/onnxruntime/releases.\n\nFor details on the upcoming release, including release dates, announcements, features, and guidance on submitting feature requests, please visit the release roadmap: https://onnxruntime.ai/roadmap.\n\n## Data/Telemetry\n\nWindows distributions of this project may collect usage data and send it to Microsoft to help improve our products and services. See the [privacy statement](docs/Privacy.md) for more details.\n\n## Contributions and Feedback\n\nWe welcome contributions! Please see the [contribution guidelines](CONTRIBUTING.md).\n\nFor feature requests or bug reports, please file a [GitHub Issue](https://github.com/Microsoft/onnxruntime/issues).\n\nFor general discussion or questions, please use [GitHub Discussions](https://github.com/microsoft/onnxruntime/discussions).\n\n## Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## License\n\nThis project is licensed under the [MIT License](LICENSE).\n', '{"language":"C++","stars":18600,"forks":3588,"watchers":18600,"open_issues":1204,"topics":["ai-framework","deep-learning","hardware-acceleration","machine-learning","neural-networks","onnx","pytorch","scikit-learn","tensorflow"],"default_branch":"main","size_kb":1401142,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:microsoft:onnxruntime-inference-examples","source_url":"https://github.com/microsoft/onnxruntime-inference-examples"},{"type":"has_code","target_id":"github:microsoft:onnxruntime-training-examples","source_url":"https://github.com/microsoft/onnxruntime-training-examples"},{"type":"has_code","target_id":"github:microsoft:onnxruntime","source_url":"https://github.com/microsoft/onnxruntime"},{"type":"has_code","target_id":"github:Microsoft:onnxruntime","source_url":"https://github.com/Microsoft/onnxruntime"},{"type":"has_code","target_id":"github:microsoft:onnxruntime","source_url":"https://github.com/microsoft/onnxruntime"}]', NULL, 'MIT', 'approved', 65, 'c14f78b6f6bf7a387071c76da43965ca', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-onnxruntime from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-onnxruntime.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-AI4Finance-Foundation-FinGPT', 'github--ai4finance-foundation--fingpt', 'FinGPT', 'AI4Finance-Foundation', '<div align="center"> <img align="center" width="30%" alt="image" src="https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139"> </div> !License <div align="center"> <img align="center" src=figs/logo_transparent_background.png width="40%"/> </div> Let us not expect Wall Street to open-source LLMs or open APIs, due to FinTech institutes'' internal regulations and policies. Blueprint of FinGPT <https://huggingface.co/FinGPT> !Visitors - [Model Release]...', '["chatgpt","finance","fingpt","fintech","large-language-models","machine-learning","nlp","prompt-engineering","pytorch","reinforcement-learning","robo-advisor","sentiment-analysis","technical-analysis","jupyter notebook"]', 'other', 18173, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/AI4Finance-Foundation/FinGPT","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<div align="center">\n<img align="center" width="30%" alt="image" src="https://github.com/AI4Finance-Foundation/FinGPT/assets/31713746/e0371951-1ce1-488e-aa25-0992dafcc139">\n</div>\n\n# FinGPT: Open-Source Financial Large Language Models\n[![Downloads](https://static.pepy.tech/badge/fingpt)](https://pepy.tech/project/fingpt)\n[![Downloads](https://static.pepy.tech/badge/fingpt/week)](https://pepy.tech/project/fingpt)\n[![Join Discord](https://img.shields.io/badge/Discord-Join-blue)](https://discord.gg/trsr8SXpW5)\n[![Python 3.8](https://img.shields.io/badge/python-3.6-blue.svg)](https://www.python.org/downloads/release/python-360/)\n[![PyPI](https://img.shields.io/pypi/v/fingpt.svg)](https://pypi.org/project/fingpt/)\n![License](https://img.shields.io/github/license/AI4Finance-Foundation/fingpt.svg?color=brightgreen)\n![](https://img.shields.io/github/issues-raw/AI4Finance-Foundation/fingpt?label=Issues)\n![](https://img.shields.io/github/issues-closed-raw/AI4Finance-Foundation/fingpt?label=Closed+Issues)\n![](https://img.shields.io/github/issues-pr-raw/AI4Finance-Foundation/fingpt?label=Open+PRs)\n![](https://img.shields.io/github/issues-pr-closed-raw/AI4Finance-Foundation/fingpt?label=Closed+PRs)\n\n<div align="center">\n<img align="center" src=figs/logo_transparent_background.png width="40%"/>\n</div>\n\nLet us not expect Wall Street to open-source LLMs or open APIs, due to FinTech institutes'' internal regulations and policies.\n\n[Blueprint of FinGPT](https://arxiv.org/abs/2306.06031)\n\n<https://huggingface.co/FinGPT>\n\n![Visitors](https://api.visitorbadge.io/api/VisitorHit?user=AI4Finance-Foundation&repo=FinGPT&countColor=%23B17A)\n[![](https://dcbadge.limes.pink/api/server/trsr8SXpW5?cb=1)](https://discord.gg/trsr8SXpW5)\n\n\n## What''s New:\n - [Model Release] Nov, 2023: We release [FinGPT-Forecaster](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Forecaster)!  ğŸ”¥[Demo](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster), [Medium Blog](https://medium.datadriveninvestor.com/introducing-fingpt-forecaster-the-future-of-robo-advisory-services-50add34e3d3c) & [Model](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) are available on HuggingfaceğŸ¤—!\n - [Paper Acceptance] Oct, 2023: ["FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets"](https://arxiv.org/abs/2310.04793) is acceptedğŸ‰ by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023 \n - [Paper Acceptance] Oct, 2023: ["FinGPT: Democratizing Internet-scale Data for Financial Large Language Models"](https://arxiv.org/abs/2307.10485) is acceptedğŸ‰ by [Instruction Workshop](https://an-instructive-workshop.github.io/) @ NeurIPS 2023\n - [Model Release] Oct, 2023: We release the [financial multi-task LLMs](https://huggingface.co/FinGPT) ğŸ”¥ produced when evaluating base-LLMs on [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)\n - [Paper Acceptance] Sep, 2023: ["Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models"](https://arxiv.org/abs/2310.04027) is acceptedğŸ‰ by [ACM International Conference on AI in Finance (ICAIF-23)](https://ai-finance.org/icaif-23-accepted-papers/)\n - [Model Release] Aug, 2023: We release the [financial sentiment analysis model](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) ğŸ”¥\n - [Paper Acceptance] Jul, 2023: ["Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models"](https://arxiv.org/abs/2306.12659) is acceptedğŸ‰ by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023\n - [Paper Acceptance] Jul, 2023: ["FinGPT: Open-Source Financial Large Language Models"](https://arxiv.org/abs/2306.06031) is acceptedğŸ‰ by [FinLLM 2023](https://finllm.github.io/workshop/#/fcb)@IJCAI 2023\n - [Medium Blog] Jun 2023: [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)\n\n## Why FinGPT?\n\n1). Finance is highly dynamic. [BloombergGPT](https://arxiv.org/abs/2303.17564) trained an LLM using a mixture of finance data and general-purpose data, which took about 53 days, at a cost of around **$3M**). It is costly to retrain an LLM model like BloombergGPT every month or every week, thus lightweight adaptation is highly favorable. FinGPT can be fine-tuned swiftly to incorporate new data (the cost falls significantly, less than **$300 per fine-tuning**).\n\n2). Democratizing Internet-scale financial data is critical, say allowing timely updates of the model (monthly or weekly updates) using an automatic data curation pipeline.  BloombergGPT has privileged data access and APIs, while FinGPT presents a more accessible alternative. It prioritizes lightweight adaptation, leveraging the best available open-source LLMs.\n\n3). The key technology is "RLHF (Reinforcement learning from human feedback)", which is missing in BloombergGPT. RLHF enables an LLM model to learn individual preferences (risk-aversion level, investing habits, personalized robo-advisor, etc.), which is the "secret" ingredient of ChatGPT and GPT4.\n\n\n### Milestone of AI Robo-Advisor: FinGPT-Forecaster\n\nTry the latest released FinGPT-Forecaster demo at our [HuggingFace Space](https://huggingface.co/spaces/FinGPT/FinGPT-Forecaster)\n\nThe dataset for FinGPT-Forecaster: https://huggingface.co/datasets/FinGPT/fingpt-forecaster-dow30-202305-202405\n\n![demo_interface](fingpt/FinGPT_Forecaster/figs/interface.png)\n\nEnter the following inputs:\n\n1) ticker symbol (e.g. AAPL, MSFT, NVDA)\n2) the day from which you want the prediction to happen (yyyy-mm-dd)\n3) the number of past weeks where market news are retrieved\n4) whether to add the latest basic financials as additional information\n\nClick Submitï¼ And you''ll be responded with a well-rounded analysis of the company and a prediction for next week''s stock price movement!\n\nFor detailed and more customized implementation, please refer to [FinGPT-Forecaster](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Forecaster)\n\n\n## FinGPT Demos: \n\n### Current State-of-the-arts for Financial Sentiment Analysis\n\n* [FinGPT V3 (Updated on 10/12/2023)](./fingpt)\n  \n  * What''s new: **Best trainable and inferable FinGPT for sentiment analysis on a single RTX 3090, which is even better than GPT-4 and ChatGPT Finetuning.**\n  \n  * [FinGPT v3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) series are LLMs finetuned with the LoRA method on the News and Tweets sentiment analysis dataset which achieve the best scores on most of the financial sentiment analysis datasets with low cost.\n  \n  * FinGPT v3.3 use llama2-13b as base model; FinGPT v3.2 uses llama2-7b as base model; FinGPT v3.1 uses chatglm2-6B as base model.\n  \n  * Benchmark Results:\n  \n  * | Weighted F1                                                  |    FPB    |  FiQA-SA  |   TFNS    |   NWGI    |      Devices       |    Time     |      Cost      |\n    | ------------------------------------------------------------ | :-------: | :-------: | :-------: | :-------: | :----------------: | :---------: | :------------: |\n    | [FinGPT v3.3](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora)| **0.882** |   0.874   | **0.903** | **0.643** |    1 Ã— RTX 3090    | 17.25 hours |     $17.25     |\n    | FinGPT v3.2|   0.850   |   0.860   |   0.894   |   0.636   |      1 Ã— A100      |  5.5 hours  |    $ 22.55     |\n    | FinGPT v3.1|   0.855   |   0.850   |   0.875   |   0.642   |      1 Ã— A100      |  5.5 hours  |    $ 22.55     |\n    | FinGPT (8bit)                                                |   0.855   |   0.847   |   0.879   |   0.632   |    1 Ã— RTX 3090    | 6.47 hours  |     $ 6.47     |\n    | FinGPT (QLoRA)                                               |   0.777   |   0.752   |   0.828   |   0.583   |    1 Ã— RTX 3090    | 4.15 hours  |     $ 4.15     |\n    | OpenAI Fine-tune                                             |   0.878   | **0.887** |   0.883   |     -     |         -          |      -      |       -        |\n    | GPT-4                                                        |   0.833   |   0.630   |   0.808   |     -     |         -          |      -      |       -        |\n    | FinBERT                                                      |   0.880   |   0.596   |   0.733   |   0.538   | 4 Ã— NVIDIA K80 GPU |      -      |       -        |\n    | Llama2-7B                                                    |   0.390   |   0.800   |   0.296   |   0.503   |    2048 Ã— A100     |   21 days   | $ 4.23 million |\n    | BloombergGPT                                                 |   0.511   |   0.751   |     -     |     -     |     512 Ã— A100     |   53 days   | $ 2.67 million |\n  \n    **Cost per GPU hour.** For **A100 GPUs**, the AWS p4d.24xlarge instance, equipped with 8 A100 GPUs is used as a benchmark to estimate the costs. Note that BloombergGPT also used p4d.24xlarge As of July 11, 2023, the hourly rate for this instance stands at $32.773. Consequently, the estimated cost per GPU hour comes to $32.77 divided by 8, resulting in approximately **$4.10**. With this value as the reference unit price (1 GPU hour). **BloombergGPT estimated cost= 512 x 53 x 24 = 651,264 GPU hours x $4.10 = $2,670,182.40**. For **RTX 3090**, we assume its cost per hour is approximately **$1.0**, which is actually much higher than available GPUs from platforms like vast.ai.\n  \n  * Reproduce the results by running [benchmarks](./fingpt/FinGPT_Sentiment_Analysis_v3/benchmark/benchmarks.ipynb), and the detailed tutorial is on the way.\n  * Finetune your own FinGPT v3 model with the LoRA method on only an RTX 3090 with this [notebook](./fingpt/FinGPT_Sentiment_Analysis_v3/training_8bit/train_Llama2_13B.ipynb) in 8bit or this [notebook](./fingpt/FinGPT_Sentiment_Analysis_v3/training_int4/train.ipynb) in int4 (QLoRA)\n  \n* [FinGPT V1](./fingpt)\n  + **FinGPT by finetuning ChatGLM2 / Llama2 with LoRA with the market-labeled data for the Chinese Market**\n \n## Instruction Tuning Datasets and Models\nThe datasets we used, and the **multi-task financial LLM** models are available at <https://huggingface.co/FinGPT>\n\n[Our Code](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark)\n  \n  | Datasets | Train Rows |  Test Rows |Description  |\n  | --------- | ----------------- | ------------ | --------------------- |\n  | [fingpt-sentiment-train](https://huggingface.co/datasets/FinGPT/fingpt-sentiment-train) | 76.8K | N/A|Sentiment Analysis Training Instructions |\n  | [fingpt-finred](https://huggingface.co/datasets/FinGPT/fingpt-finred)| 27.6k | 5.11k | Financial Relation Extraction Instructions |\n  | [fingpt-headline](https://huggingface.co/datasets/FinGPT/fingpt-headline) | 82.2k | 20.5k | Financial Headline Analysis Instructions|\n  | [fingpt-ner](https://huggingface.co/datasets/FinGPT/fingpt-ner) | 511   | 98  | Financial Named-Entity Recognition Instructions|\n  | [fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa) | 17.1k   | N/A  | Financial Q&A Instructions|\n  | [fingpt-fineval](https://huggingface.co/datasets/FinGPT/fingpt-fineval) | 1.06k   | 265  | Chinese Multiple-Choice Questions Instructions|\n\n  Multi-task financial LLMs Models:\n```python\n  demo_tasks = [\n      ''Financial Sentiment Analysis'',\n      ''Financial Relation Extraction'',\n      ''Financial Headline Classification'',\n      ''Financial Named Entity Recognition'',]\n  demo_inputs = [\n      "Glaxo''s ViiV Healthcare Signs China Manufacturing Deal With Desano",\n      "Apple Inc. Chief Executive Steve Jobs sought to soothe investor concerns about his health on Monday, saying his weight loss was caused by a hormone imbalance that is relatively simple to treat.",\n      ''gold trades in red in early trade; eyes near-term range at rs 28,300-28,600'',\n      ''This LOAN AND SECURITY AGREEMENT dated January 27 , 1999 , between SILICON VALLEY BANK (" Bank "), a California - chartered bank with its principal place of business at 3003 Tasman Drive , Santa Clara , California 95054 with a loan production office located at 40 William St ., Ste .'',]\n  demo_instructions = [\n      ''What is the sentiment of this news? Please choose an answer from {negative/neutral/positive}.'',\n      ''Given phrases that describe the relationship between two words/phrases as options, extract the word/phrase pair and the corresponding lexical relationship between them from the input text. The output format should be "relation1: word1, word2; relation2: word3, word4". Options: product/material produced, manufacturer, distributed by, industry, position held, original broadcaster, owned by, founded by, distribution format, headquarters location, stock exchange, currency, parent organization, chief executive officer, director/manager, owner of, operator, member of, employer, chairperson, platform, subsidiary, legal form, publisher, developer, brand, business division, location of formation, creator.'',\n      ''Does the news headline talk about price going up? Please choose an answer from {Yes/No}.'',\n      ''Please extract entities and their types from the input sentence, entity types should be chosen from {person/organization/location}.'',]\n```\n\n  | Models | Description  | Function |\n  | --------- | --------------------- |---------------- |\n  | [fingpt-mt_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_llama2-7b_lora)| Fine-tuned Llama2-7b model with LoRA | Multi-Task |\n  | [fingpt-mt_falcon-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_falcon-7b_lora)| Fine-tuned falcon-7b model with LoRA  | Multi-Task |\n  | [fingpt-mt_bloom-7b1_lora](https://huggingface.co/FinGPT/fingpt-mt_bloom-7b1_lora) | Fine-tuned bloom-7b1 model with LoRA | Multi-Task |\n  | [fingpt-mt_mpt-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_mpt-7b_lora) | Fine-tuned mpt-7b model with LoRA | Multi-Task |\n  | [fingpt-mt_chatglm2-6b_lora](https://huggingface.co/FinGPT/fingpt-mt_chatglm2-6b_lora) | Fine-tuned chatglm-6b model with LoRA | Multi-Task |\n  | [fingpt-mt_qwen-7b_lora](https://huggingface.co/FinGPT/fingpt-mt_qwen-7b_lora) | Fine-tuned qwen-7b model with LoRA | Multi-Task |\n  | [fingpt-sentiment_llama2-13b_lora](https://huggingface.co/FinGPT/fingpt-sentiment_llama2-13b_lora) | Fine-tuned llama2-13b model with LoRA | Single-Task |\n  | [fingpt-forecaster_dow30_llama2-7b_lora](https://huggingface.co/FinGPT/fingpt-forecaster_dow30_llama2-7b_lora) | Fine-tuned llama2-7b model with LoRA | Single-Task |\n\n  \n## Tutorials\n[[Training] Beginnerâ€™s Guide to FinGPT: Training with LoRA and ChatGLM2â€“6B One Notebook, $10 GPU](https://byfintech.medium.com/beginners-guide-to-fingpt-training-with-lora-chatglm2-6b-9eb5ace7fe99)\n\n## Understanding FinGPT: An Educational Blog Series\n+ [FinGPT: Powering the Future of Finance with 20 Cutting-Edge Applications\n](https://medium.datadriveninvestor.com/fingpt-powering-the-future-of-finance-with-20-cutting-edge-applications-7c4d082ad3d8)\n+ [FinGPT I: Why We Built the First Open-Source Large Language Model for Finance\n](https://medium.datadriveninvestor.com/fingpt-i-why-we-built-the-first-open-source-large-language-model-for-finance-c01b5517ca)\n+ [FinGPT II: Cracking the Financial Sentiment Analysis Task Using Instruction Tuning of General-Purpose Large Language Models\n](https://medium.datadriveninvestor.com/fingpt-ii-cracking-the-financial-sentiment-analysis-task-using-instruction-tuning-of-3333bce428c4)\n\n\n## FinGPT Ecosystem\n### FinGPT embraces a full-stack framework for FinLLMs with five layers:\n1. **Data source layer**: This layer assures comprehensive market coverage, addressing the temporal sensitivity of financial data through real-time information capture.\n2. **Data engineering layer**: Primed for real-time NLP data processing, this layer tackles the inherent challenges of high temporal sensitivity and low signal-to-noise ratio in financial data.\n3. **LLMs layer**: Focusing on a range of fine-tuning methodologies such as LoRA, this layer mitigates the highly dynamic nature of financial data, ensuring the modelâ€™s relevance and accuracy.\n4. **Task layer**: This layer is responsible for executing fundamental tasks. These tasks serve as the benchmarks for performance evaluations and cross-comparisons in the realm of FinLLMs\n5. **Application layer**: Showcasing practical applications and demos, this layer highlights the potential capability of FinGPT in the financial sector.\n\n* FinGPT Framework: Open-Source Financial Large Language Models\n\n<div align="center">\n<img align="center" src=figs/FinGPT_framework_20240301.png>\n</div>\n\n* [FinGPT-RAG](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_RAG): We present a retrieval-augmented large language model framework specifically designed for financial sentiment analysis, optimizing information depth and context through external knowledge retrieval, thereby ensuring nuanced predictions.\n\n<div align="center">\n<img align="center" src=figs/FinGPT_RAG_framework.png>\n</div>\n\n* [FinGPT-FinNLP](https://github.com/AI4Finance-Foundation/FinNLP): FinNLP provides a playground for all people interested in LLMs and NLP in Finance. Here we provide full pipelines for LLM training and finetuning in the field of finance. The full architecture is shown in the following picture. Detail codes and introductions can be found [here](https://github.com/AI4Finance-Foundation/FinNLP). Or you may refer to the [wiki](https://ai4finance-foundation.github.io/FinNLP/)\n\n<div align="center">\n<img align="center" src=figs/FinGPT_FinNLP_data_source.png>\n</div>\n\n* [FinGPT-Benchmark](https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Benchmark): We introduce a novel Instruction Tuning paradigm optimized for open-source Large Language Models (LLMs) in finance, enhancing their adaptability to diverse financial datasets while also facilitating cost-effective, systematic benchmarking from task-specific, multi-task, and zero-shot instruction tuning tasks. \n\n\n<div align="center">\n<img align="center" src=figs/FinGPT_Benchmark_20231110.png>\n</div>\n\n\n\n## Open-Source Base Model used in the LLMs layer of FinGPT\n* Feel free to contribute more open-source base models tailored for various language-specific financial markets.\n\n| Base Model |Pretraining Tokens|Context Length  | Model Advantages |Model Size|Experiment Results |  Applications |\n|  ----  |  ----  |  ----  |   ----  |   ----  |  ----  | ----  |\n| [Llama-2](https://github.com/facebookresearch/llama)|2 Trillion|4096| Llama-2 excels on English-based market data | [llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b-hf) and [Llama-2-13b](https://huggingface.co/meta-llama/Llama-2-13b-hf) | llama-2 consistently shows superior fine-tuning results  | Financial Sentiment Analysis, Robo-Advisor |\n| [Falcon](https://github.com/falconry/falcon) |1,500B|2048|  Maintains high-quality results while being more resource-efficient | [falcon-7b](https://huggingface.co/tiiuae/falcon-7b) |Good for English market data  | Financial Sentiment Analysis |\n| [MPT](https://github.com/mosaicml/llm-foundry) |1T|2048| MPT models can be trained with high throughput efficiency and stable convergence | [mpt-7b](https://huggingface.co/mosaicml/mpt-7b) |Good for English market data  | Financial Sentiment Analysis |\n| [Bloom](https://github.com/bigscience-workshop/bigscience/tree/master/train/tr11-176B-ml#readme) |366B|2048| Worldâ€™s largest open multilingual language model  | [bloom-7b1](https://huggingface.co/bigscience/bloom-7b1) |Good for English market data  | Financial Sentiment Analysis |\n| [ChatGLM2](https://github.com/THUDM/ChatGLM2-6B)|1.4T  |32K |Exceptional capability for Chinese language expression| [chatglm2-6b](https://huggingface.co/THUDM/chatglm2-6b) |Shows prowess for Chinese market data  | Financial Sentiment Analysis, Financial Report Summary |\n| [Qwen](https://github.com/QwenLM/Qwen-7B)|2.2T  |8k |Fast response and high accuracy| [qwen-7b](https://huggingface.co/tangger/Qwen-7B-Chat) |Effective for Chinese market data  | Financial Sentiment Analysis|\n| [InternLM](https://github.com/InternLM/InternLM) |1.8T  |8k |Can flexibly and independently construct workflows |[internlm-7b](https://huggingface.co/internlm/internlm-7b) |Effective for Chinese market data  | Financial Sentiment Analysis |\n\n* Benchmark Results for the above open-source Base Models in the financial sentiment analysis task using the same instruction template for SFT (LoRA):\n  | Weighted F1/Acc  |Llama2 |Falcon |  MPT|Bloom |ChatGLM2|Qwen|InternLM |\n  | --------- | ----------------- | ------------ | --------------------- | ---------------- | --------------- | ----------------- |----------------- |\n  | [FPB](https://huggingface.co/datasets/financial_phrasebank) | 0.863/0.863 | 0.846/0.849  | **0.872**/**0.872**   | 0.810/0.810 | 0.850/0.849 |0.854/0.854| 0.709/0.714 |\n  | [FiQA-SA](https://huggingface.co/datasets/pauri32/fiqa-2018)| **0.871**/0.855| 0.840/0.811  | 0.863/0.844 | 0.771/0.753| 0.864/**0.862** | 0.867/0.851  |0.679/0.687 |\n  | [TFNS](https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment) | 0.896/0.895 | 0.893/0.893 | **0.907**/**0.907** | 0.840/0.840 | 0.859/0.858 | 0.883/0.882|0.729/0.731|\n  | [NWGI](https://huggingface.co/datasets/oliverwang15/news_with_gpt_instructions) | **0.649/0.651**   | 0.636/0.638  | 0.640/0.641| 0.573/0.574| 0.619/0.629 |0.638/0.643|0.498/0.503|\n\n### All Thanks To Our Contributors :\n<a href="https://github.com/AI4Finance-Foundation/FinGPT/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=AI4Finance-Foundation/FinGPT" />\n</a>\n\n## News\n\n+ [Columbia Perspectives on ChatGPT](https://datascience.columbia.edu/news/2023/columbia-perspectives-on-chatgpt/?utm_source=sendinblue&utm_campaign=DSI%20Newsletter%20April%202023&utm_medium=email)\n+ [MIT Technology Review] [ChatGPT is about to revolutionize the economy. We need to decide what that looks like](https://www.technologyreview.com/2023/03/25/1070275/chatgpt-revolutionize-economy-decide-what-looks-like/)\n+ [BloombergGPT] [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564)\n+ [Finextra] [ChatGPT and Bing AI to sit as panellists at fintech conference](https://www.finextra.com/newsarticle/41973/chatgpt-and-bing-ai-to-sit-as-panellists-at-fintech-conference)\n\n## ChatGPT at AI4Finance\n\n+ [YouTube video] [I Built a Trading Bot with ChatGPT](https://www.youtube.com/watch?v=fhBw3j_O9LE), combining ChatGPT and FinRL.\n+ [Hey, ChatGPT! Explain FinRL code to me!](https://medium.com/@ai4finance/hey-chatgpt-explain-finrl-code-to-me-6a91d612296f)\n\n## Introductory\n\n+ [Sparks of artificial general intelligence: Early experiments with GPT-4](https://arxiv.org/abs/2303.12712)\n+ [GPT-4] [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)\n+ [InstructGPT] [Training language models to follow instructions with human feedback](https://openreview.net/forum?id=TG8KACxEON) NeurIPS 2022.\n\n[The Journey of Open AI GPT models](https://medium.com/walmartglobaltech/the-journey-of-open-ai-gpt-models-32d95b7b7fb2).  GPT models explained. Open AI''s GPT-1, GPT-2, GPT-3.\n\n+ [GPT-3] [Language models are few-shot learners](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html) NeurIPS 2020.\n+ [GPT-2] [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n+ [GPT-1] [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n+ [Transformer] [Attention is All you Need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) NeurIPS 2017.\n\n## (Financial) Big Data\n\n+ [BloombergGPT] [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564)\n\n+ [WHATâ€™S IN MY AI?](https://lifearchitect.ai/whats-in-my-ai/) A Comprehensive Analysis of Datasets Used to Train GPT-1, GPT-2, GPT-3, GPT-NeoX-20B, Megatron-11B, MT-NLG, and Gopher\n\n+ [FinRL-Meta Repo](https://github.com/AI4Finance-Foundation/FinRL-Meta) and paper [FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/0bf54b80686d2c4dc0808c2e98d430f7-Abstract-Datasets_and_Benchmarks.html). Advances in Neural Information Processing Systems, 2022.\n\n+ [AI4Finance] [FinNLP](https://github.com/AI4Finance-Foundation/FinNLP) Democratizing Internet-scale financial data.\n\n## Interesting Demos\n\n+ [GPT-3 Creative Fiction](https://gwern.net/gpt-3#prompts-as-programming) Creative writing by OpenAIâ€™s GPT-3 model, demonstrating poetry, dialogue, puns, literary parodies, and storytelling. Plus advice on effective GPT-3 prompt programming & avoiding common errors.\n\n## ChatGPT for FinTech\n\n**ChatGPT Trading Bot**\n+ [YouTube video] [ChatGPT Trading strategy 20097% returns](https://www.youtube.com/watch?v=unsa_gXPAJ4)\n+ [YouTube video] [ChatGPT Coding - Make A Profitable Trading Strategy In Five Minutes!](https://www.youtube.com/watch?v=4SG2884RcDY)\n+ [YouTube video] [Easy Automated Live Trading using ChatGPT (+9660.3% hands free)](https://www.youtube.com/watch?v=dIEZVPVOZPQ)\n+ [YouTube video] [ChatGPT Trading Strategy 893% Returns](https://www.youtube.com/watch?v=YxjvjK5AD2M)\n+ [YouTube video] [ChatGPT 10 Million Trading Strategy](https://www.youtube.com/watch?v=9VPfd08uU4Q)\n+ [YouTube video] [ChatGPT: Your Crypto Assistant](https://www.youtube.com/watch?v=LpzeshX6s2w)\n+ [YouTube video] [Generate Insane Trading Returns with ChatGPT and TradingView](https://www.youtube.com/watch?v=ekz6ugJE1h0&t=3s)\n\n<!--- \n**(Fast and accurate) Sentiment Analysis**\n\n   GPT-3 can help study customer surveys, social media tweets from customers/users.\n\n   Tweets\n+ [Tweet Classifier](https://platform.openai.com/playground/p/default-tweet-classifier?model=text-davinci-003)\n+ [Advanced Tweet Classifier](https://platform.openai.com/playground/p/default-adv-tweet-classifier?model=text-davinci-003)\n\n  Financial News\n+ [Algorithmic Trading using Sentiment Analysis on News Articles](https://towardsdatascience.com/https-towardsdatascience-com-algorithmic-trading-using-sentiment-analysis-on-news-articles-83db77966704)\n+ [Accessing Historical Financial News Headlines with Python](https://python.plainenglish.io/access-historical-financial-news-headlines-with-python-be1b8faaea9f)\n\n**PromptNet** Analogy to ImageNet and WordNet, it is critical to build a PromptNet.\n\n+ [Awesome_Prompting_Papers_in_Computer_Vision](https://github.com/ttengwang/Awesome_Prompting_Papers_in_Computer_Vision)\n+ [OpenPrompt](https://github.com/thunlp/OpenPrompt)\n+ [promptsource](https://github.com/bigscience-workshop/promptsource)\n\n**Robo-advisor**\n\n**Coding-tutor**\n\n+ [Hey, ChatGPT! Explain FinRL code to me!](https://medium.com/@ai4finance/hey-chatgpt-explain-finrl-code-to-me-6a91d612296f)\n\n**Blogs about ChatGPT for FinTech**\n\n## ChatGPT APIs\n\nPrompting as a new programming paradigm!\n+ [Towards Data Science] [GPT-3: Creative Potential of NLP](https://towardsdatascience.com/gpt-3-creative-potential-of-nlp-d5ccae16c1ab)\n+ [YouTube video] [OpenAI GPT-3 - Prompt Engineering For Financial NLP](https://www.youtube.com/watch?v=Nl2Cdbao5Ws)\n\n+ [OpenAI API for GPT-3](https://platform.openai.com/docs/models/gpt-3)\n+ [ChatGPT-wrapper: python and shell](https://github.com/mmabrouk/chatgpt-wrapper)\n+ [OpenAI Examples Library](https://platform.openai.com/examples)\n+ [GPT-3 Sandbox (Github)](https://github.com/shreyashankar/gpt3-sandbox) Enable users to create cool web demos using OpenAI GPT-3 API.\n+ [Exploring the Capabilities of the ChatGPT API: A Beginnerâ€™s Guide](https://levelup.gitconnected.com/exploring-the-capabilities-of-the-chatgpt-api-a-beginners-guide-e9089d49961f)\n+ [Reverse engineered ChatGPT API](https://github.com/acheong08/ChatGPT)\n\n**Prompting programming**\n\n## ChatGPT relatives: \n\n[A Release Timeline](https://github.com/osanseviero/ml_timeline) of many LLMs.\n\n[PaLM](https://arxiv.org/abs/2204.02311)\n\n[Chincella](https://arxiv.org/abs/2203.15556)\n\nInteresting evaluations:\n+ [RLHF for pretraining](https://arxiv.org/abs/2302.08582)\n\n+ [Compare ChatGPT with GPT3.5](https://arxiv.org/pdf/2302.06476.pdf)\n\n+ [Is ChatGPT A Good Translator? A Preliminary Study](https://arxiv.org/pdf/2301.08745.pdf)\n\n+ [A Multitask, Multilingual, Multimodal Evaluation of ChatGPT\non Reasoning, Hallucination, and Interactivity](https://arxiv.org/pdf/2302.04023.pdf)\n\n[YouTube video] [Physics Solution: ChatGPT vs. Google](https://www.youtube.com/watch?v=x4dIx9VYQoM)\n---> \n\n## Citing FinGPT\n```\n@article{yang2023fingpt,\n  title={FinGPT: Open-Source Financial Large Language Models},\n  author={Yang, Hongyang and Liu, Xiao-Yang and Wang, Christina Dan},\n  journal={FinLLM Symposium at IJCAI 2023},\n  year={2023}\n}\n@article{zhang2023instructfingpt,\n      title={Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models}, \n      author={Boyu Zhang and Hongyang Yang and Xiao-Yang Liu},\n      journal={FinLLM Symposium at IJCAI 2023},\n      year={2023}\n}\n@article{zhang2023fingptrag,\n  title={Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models},\n  author={Zhang, Boyu and Yang, Hongyang and Zhou, tianyu and Babar, Ali and Liu, Xiao-Yang},\n journal = {ACM International Conference on AI in Finance (ICAIF)},\n  year={2023}\n}\n\n@article{wang2023fingptbenchmark,\n  title={FinGPT: Instruction Tuning Benchmark for Open-Source Large Language Models in Financial Datasets},\n  author={Wang, Neng and Yang, Hongyang and Wang, Christina Dan},\n  journal={NeurIPS Workshop on Instruction Tuning and Instruction Following},\n  year={2023}\n}\n@article{2023finnlp,\n  title={Data-centric FinGPT: Democratizing Internet-scale Data for Financial Large Language Models},\n  author={Liu, Xiao-Yang and Wang, Guoxuan and Yang, Hongyang and Zha, Daochen},\n  journal={NeurIPS Workshop on Instruction Tuning and Instruction Following},\n  year={2023}\n}\n\n```\n\n<div align="center">\n<a href="https://finllm.github.io/workshop/#/fcb" target="_blank">\n<img align="center" src=figs/fingpt_best_presentation.png width="65%">\n</div>\n\n\n## LICENSE\n\nMIT License\n\n**Disclaimer: We are sharing codes for academic purposes under the MIT education license. Nothing herein is financial advice, and NOT a recommendation to trade real money. Please use common sense and always first consult a professional before trading or investing.**\n\n', '{"language":"Jupyter Notebook","stars":18173,"forks":2561,"watchers":18173,"open_issues":83,"topics":["chatgpt","finance","fingpt","fintech","large-language-models","machine-learning","nlp","prompt-engineering","pytorch","reinforcement-learning","robo-advisor","sentiment-analysis","technical-analysis"],"default_branch":"master","size_kb":12579,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinGPT","source_url":"https://github.com/AI4Finance-Foundation/FinGPT"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinGPT","source_url":"https://github.com/AI4Finance-Foundation/FinGPT"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinGPT","source_url":"https://github.com/AI4Finance-Foundation/FinGPT"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinGPT","source_url":"https://github.com/AI4Finance-Foundation/FinGPT"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinGPT","source_url":"https://github.com/AI4Finance-Foundation/FinGPT"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinGPT","source_url":"https://github.com/AI4Finance-Foundation/FinGPT"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinNLP","source_url":"https://github.com/AI4Finance-Foundation/FinNLP"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinNLP","source_url":"https://github.com/AI4Finance-Foundation/FinNLP"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinGPT","source_url":"https://github.com/AI4Finance-Foundation/FinGPT"},{"type":"has_code","target_id":"github:facebookresearch:llama","source_url":"https://github.com/facebookresearch/llama"},{"type":"has_code","target_id":"github:falconry:falcon","source_url":"https://github.com/falconry/falcon"},{"type":"has_code","target_id":"github:mosaicml:llm-foundry","source_url":"https://github.com/mosaicml/llm-foundry"},{"type":"has_code","target_id":"github:bigscience-workshop:bigscience","source_url":"https://github.com/bigscience-workshop/bigscience"},{"type":"has_code","target_id":"github:THUDM:ChatGLM2-6B","source_url":"https://github.com/THUDM/ChatGLM2-6B"},{"type":"has_code","target_id":"github:QwenLM:Qwen-7B","source_url":"https://github.com/QwenLM/Qwen-7B"},{"type":"has_code","target_id":"github:InternLM:InternLM","source_url":"https://github.com/InternLM/InternLM"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinGPT","source_url":"https://github.com/AI4Finance-Foundation/FinGPT"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinRL-Meta","source_url":"https://github.com/AI4Finance-Foundation/FinRL-Meta"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinNLP","source_url":"https://github.com/AI4Finance-Foundation/FinNLP"},{"type":"has_code","target_id":"github:ttengwang:Awesome_Prompting_Papers_in_Computer_Vision","source_url":"https://github.com/ttengwang/Awesome_Prompting_Papers_in_Computer_Vision"},{"type":"has_code","target_id":"github:thunlp:OpenPrompt","source_url":"https://github.com/thunlp/OpenPrompt"},{"type":"has_code","target_id":"github:bigscience-workshop:promptsource","source_url":"https://github.com/bigscience-workshop/promptsource"},{"type":"has_code","target_id":"github:mmabrouk:chatgpt-wrapper","source_url":"https://github.com/mmabrouk/chatgpt-wrapper"},{"type":"has_code","target_id":"github:shreyashankar:gpt3-sandbox","source_url":"https://github.com/shreyashankar/gpt3-sandbox"},{"type":"has_code","target_id":"github:acheong08:ChatGPT","source_url":"https://github.com/acheong08/ChatGPT"},{"type":"has_code","target_id":"github:osanseviero:ml_timeline","source_url":"https://github.com/osanseviero/ml_timeline"}]', NULL, 'MIT', 'approved', 80, 'f9f857e8b6c8ccb83103417c6368281f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-AI4Finance-Foundation-FinGPT from https://github.com/AI4Finance-Foundation.png
Image converted to WebP: data/images/github-AI4Finance-Foundation-FinGPT.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-meta-llama-llama-cookbook', 'github--meta-llama--llama-cookbook', 'llama-cookbook', 'meta-llama', '<h1 align="center"> Llama Cookbook </h1> <p align="center"> <a href="https://llama.developer.meta.com/join_waitlist?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main"><img src="https://img.shields.io/badge/Llama_API-Join_Waitlist-brightgreen?logo=meta" /></a> <a href="https://llama.developer.meta.com/docs?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main"><img src="https://img.shields.io/badge/Llama_API-Documentation-4BA9FE?logo=meta" /></a> </p> <p align="center"> <a ...', '["ai","finetuning","langchain","llama","llama2","llm","machine-learning","python","pytorch","vllm","jupyter notebook"]', 'other', 18067, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/meta-llama/llama-cookbook","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<h1 align="center"> Llama Cookbook </h1>\n<p align="center">\n	<a href="https://llama.developer.meta.com/join_waitlist?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main"><img src="https://img.shields.io/badge/Llama_API-Join_Waitlist-brightgreen?logo=meta" /></a>\n	<a href="https://llama.developer.meta.com/docs?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main"><img src="https://img.shields.io/badge/Llama_API-Documentation-4BA9FE?logo=meta" /></a>\n\n</p>\n<p align="center">\n	<a href="https://github.com/meta-llama/llama-models/blob/main/models/?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main"><img alt="Llama Model cards" src="https://img.shields.io/badge/Llama_OSS-Model_cards-green?logo=meta" /></a>\n	<a href="https://www.llama.com/docs/overview/?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main"><img alt="Llama Documentation" src="https://img.shields.io/badge/Llama_OSS-Documentation-4BA9FE?logo=meta" /></a>\n	<a href="https://huggingface.co/meta-llama"><img alt="Hugging Face meta-llama" src="https://img.shields.io/badge/Hugging_Face-meta--llama-yellow?logo=huggingface" /></a>\n\n</p>\n<p align="center">\n	<a href="https://github.com/meta-llama/synthetic-data-kit"><img alt="Llama Tools Syntethic Data Kit" src="https://img.shields.io/badge/Llama_Tools-synthetic--data--kit-orange?logo=meta" /></a>\n	<a href="https://github.com/meta-llama/llama-prompt-ops"><img alt="Llama Tools Syntethic Data Kit" src="https://img.shields.io/badge/Llama_Tools-llama--prompt--ops-orange?logo=meta" /></a>\n</p>\n<h2> Official Guide to building with Llama </h2>\n\n\n\nWelcome to the official repository for helping you get started with [inference](https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/inference/), [fine-tuning](https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/finetuning) and [end-to-end use-cases](https://github.com/meta-llama/llama-cookbook/tree/main/end-to-end-use-cases) of building with the Llama Model family.\n\nThis repository covers the most popular community approaches, use-cases and the latest recipes for Llama Text and Vision models.\n\n## Latest Llama 4 recipes\n\n* [Get started](./getting-started/build_with_llama_api.ipynb) with [Llama API](https://bit.ly/llama-api-main)\n* Integrate [Llama API](https://bit.ly/llama-api-main) with [WhatsApp](./end-to-end-use-cases/whatsapp_llama_4_bot/README.md)\n* 5M long context using [Llama 4 Scout](./getting-started/build_with_llama_4.ipynb)\n* Analyze research papers with [Llama 4 Maverick](./end-to-end-use-cases/research_paper_analyzer/README.md)\n* Create a character mind map from a book using [Llama 4 Maverick](./end-to-end-use-cases/book-character-mindmap/README.md)\n\n## Repository Structure:\n\n- [3P Integrations](https://github.com/meta-llama/llama-cookbook/tree/main/3p-integrations): Getting Started Recipes and End to End Use-Cases from various Llama providers\n- [End to End Use Cases](https://github.com/meta-llama/llama-cookbook/tree/main/end-to-end-use-cases): As the name suggests, spanning various domains and applications\n- [Getting Started](https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/): Reference for inferencing, fine-tuning and RAG examples\n- [src](https://github.com/meta-llama/llama-cookbook/tree/main/src/): Contains the src for the original llama-recipes library along with some FAQs for fine-tuning.\n\n> Note: We recently did a refactor of the repo, [archive-main](https://github.com/meta-llama/llama-cookbook/tree/archive-main) is a snapshot branch from before the refactor\n\n## FAQ:\n\n- **Q:** What happened to llama-recipes?\n  **A:** We recently renamed llama-recipes to llama-cookbook.\n\n- **Q:** I have some questions for Fine-Tuning, is there a section to address these?\n  **A:** Check out the Fine-Tuning FAQ [here](https://github.com/meta-llama/llama-cookbook/tree/main/src/docs/).\n\n- **Q:** Some links are broken/folders are missing:\n  **A:** We recently did a refactor of the repo, [archive-main](https://github.com/meta-llama/llama-cookbook/tree/archive-main) is a snapshot branch from before the refactor.\n\n- **Q:** Where can we find details about the latest models?\n  **A:** Official [Llama models website](https://www.llama.com).\n\n## Contributing\n\nPlease read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us.\n\n## License\n<!-- markdown-link-check-disable -->\nSee the License file for Meta Llama 4 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama4/USE_POLICY.md)\n\nSee the License file for Meta Llama 3.3 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/USE_POLICY.md)\n\nSee the License file for Meta Llama 3.2 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/USE_POLICY.md)\n\nSee the License file for Meta Llama 3.1 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/USE_POLICY.md)\n\nSee the License file for Meta Llama 3 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3/USE_POLICY.md)\n\nSee the License file for Meta Llama 2 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama2/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama2/USE_POLICY.md)\n<!-- markdown-link-check-enable -->\n', '{"language":"Jupyter Notebook","stars":18067,"forks":2656,"watchers":18067,"open_issues":61,"topics":["ai","finetuning","langchain","llama","llama2","llm","machine-learning","python","pytorch","vllm"],"default_branch":"main","size_kb":279925,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:synthetic-data-kit\"><img","source_url":"https://github.com/meta-llama/synthetic-data-kit\"><img"},{"type":"has_code","target_id":"github:meta-llama:llama-prompt-ops\"><img","source_url":"https://github.com/meta-llama/llama-prompt-ops\"><img"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-cookbook","source_url":"https://github.com/meta-llama/llama-cookbook"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"}]', NULL, 'MIT', 'approved', 65, 'fa1d80b2fdbe0393fe2c52b13cf3bf0c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-meta-llama-llama-cookbook from https://github.com/meta-llama.png
Image converted to WebP: data/images/github-meta-llama-llama-cookbook.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-keon-awesome-nlp', 'github--keon--awesome-nlp', 'awesome-nlp', 'keon', 'A curated list of resources dedicated to Natural Language Processing !Awesome NLP Logo Read this in English, Traditional Chinese _Please read the contribution guidelines before contributing. Please add your favourite NLP resource by raising a pull request_ * Research Summaries and Trends * Prominent NLP Research Labs * Tutorials * Reading Content * Videos and Courses * Books * Libraries * Node.js * Python * C++ * Java * Kotlin * Scala * R * Clojure * Ruby * Rust * NLP++ * Julia * Services * A...', '["awesome","awesome-list","deep-learning","language","machine-learning","natural-language-processing","nlp","text-mining"]', 'other', 18022, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/keon/awesome-nlp","fetched_at":"2025-12-08T10:30:37.948Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# awesome-nlp\n\n[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\nA curated list of resources dedicated to Natural Language Processing\n\n![Awesome NLP Logo](/images/logo.jpg)\n\nRead this in [English](./README.md), [Traditional Chinese](./README-ZH-TW.md)\n\n_Please read the [contribution guidelines](contributing.md) before contributing. Please add your favourite NLP resource by raising a [pull request](https://github.com/keonkim/awesome-nlp/pulls)_\n\n## Contents\n\n* [Research Summaries and Trends](#research-summaries-and-trends)\n* [Prominent NLP Research Labs](#prominent-nlp-research-labs)\n* [Tutorials](#tutorials)\n  * [Reading Content](#reading-content)\n  * [Videos and Courses](#videos-and-online-courses)\n  * [Books](#books)\n* [Libraries](#libraries)\n  * [Node.js](#node-js)\n  * [Python](#python)\n  * [C++](#c++)\n  * [Java](#java)\n  * [Kotlin](#kotlin)\n  * [Scala](#scala)\n  * [R](#R)\n  * [Clojure](#clojure)\n  * [Ruby](#ruby)\n  * [Rust](#rust)\n  * [NLP++](#NLP++)\n  * [Julia](#julia)\n* [Services](#services)\n* [Annotation Tools](#annotation-tools)\n* [Datasets](#datasets)\n* [NLP in Korean](#nlp-in-korean)\n* [NLP in Arabic](#nlp-in-arabic)\n* [NLP in Chinese](#nlp-in-chinese)\n* [NLP in German](#nlp-in-german)\n* [NLP in Polish](#nlp-in-polish)\n* [NLP in Spanish](#nlp-in-spanish)\n* [NLP in Indic Languages](#nlp-in-indic-languages)\n* [NLP in Thai](#nlp-in-thai)\n* [NLP in Danish](#nlp-in-danish)\n* [NLP in Vietnamese](#nlp-in-vietnamese)\n* [NLP for Dutch](#nlp-for-dutch)\n* [NLP in Indonesian](#nlp-in-indonesian)\n* [NLP in Urdu](#nlp-in-urdu)\n* [NLP in Persian](#nlp-in-persian)\n* [NLP in Ukrainian](#nlp-in-ukrainian)\n* [NLP in Hungarian](#nlp-in-hungarian)\n* [NLP in Portuguese](#nlp-in-portuguese)\n* [Other Languages](#other-languages)\n* [Credits](#credits)\n\n## Research Summaries and Trends\n\n* [NLP-Overview](https://nlpoverview.com/) is an up-to-date overview of deep learning techniques applied to NLP, including theory, implementations, applications, and state-of-the-art results. This is a great Deep NLP Introduction for researchers.\n* [NLP-Progress](https://nlpprogress.com/) tracks the progress in Natural Language Processing, including the datasets and the current state-of-the-art for the most common NLP tasks\n* [NLP''s ImageNet moment has arrived](https://thegradient.pub/nlp-imagenet/)\n* [ACL 2018 Highlights: Understanding Representation and Evaluation in More Challenging Settings](http://ruder.io/acl-2018-highlights/)\n* [Four deep learning trends from ACL 2017. Part One: Linguistic Structure and Word Embeddings](https://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-1.html)\n* [Four deep learning trends from ACL 2017. Part Two: Interpretability and Attention](https://www.abigailsee.com/2017/08/30/four-deep-learning-trends-from-acl-2017-part-2.html)\n* [Highlights of EMNLP 2017: Exciting Datasets, Return of the Clusters, and More!](http://blog.aylien.com/highlights-emnlp-2017-exciting-datasets-return-clusters/)\n* [Deep Learning for Natural Language Processing (NLP): Advancements & Trends](https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/?utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=The%20Wild%20Week%20in%20AI)\n* [Survey of the State of the Art in Natural Language Generation](https://arxiv.org/abs/1703.09902)\n\n## Prominent NLP Research Labs\n[Back to Top](#contents)\n\n* [The Berkeley NLP Group](http://nlp.cs.berkeley.edu/index.shtml) - Notable contributions include a tool to reconstruct long dead languages, referenced [here](https://www.bbc.com/news/science-environment-21427896) and by taking corpora from 637 languages currently spoken in Asia and the Pacific and recreating their descendant.\n* [Language Technologies Institute, Carnegie Mellon University](http://www.cs.cmu.edu/~nasmith/nlp-cl.html) - Notable projects include [Avenue Project](http://www.cs.cmu.edu/~avenue/), a syntax driven machine translation system for endangered languages like Quechua and Aymara and previously, [Noah''s Ark](http://www.cs.cmu.edu/~ark/) which created [AQMAR](http://www.cs.cmu.edu/~ark/AQMAR/) to improve NLP tools for Arabic.\n* [NLP research group, Columbia University](http://www1.cs.columbia.edu/nlp/index.cgi) - Responsible for creating BOLT ( interactive error handling for speech translation systems) and an un-named project to characterize laughter in dialogue.\n* [The Center or Language and Speech Processing, John Hopkins University](http://clsp.jhu.edu/) - Recently in the news for developing speech recognition software to create a diagnostic test or Parkinson''s Disease, [here](https://www.clsp.jhu.edu/2019/03/27/speech-recognition-software-and-machine-learning-tools-are-being-used-to-create-diagnostic-test-for-parkinsons-disease/#.XNFqrIkzYdU).\n* [Computational Linguistics and Information Processing Group, University of Maryland](https://wiki.umiacs.umd.edu/clip/index.php/Main_Page) - Notable contributions include [Human-Computer Cooperation or Word-by-Word Question Answering](http://www.umiacs.umd.edu/~jbg/projects/IIS-1652666) and modeling development of phonetic representations. \n* [Penn Natural Language Processing, University of Pennsylvania](https://nlp.cis.upenn.edu/)- Famous for creating the [Penn Treebank](https://www.seas.upenn.edu/~pdtb/).\n* [The Stanford Nautral Language Processing Group](https://nlp.stanford.edu/)- One of the top NLP research labs in the world, notable for creating [Stanford CoreNLP](https://nlp.stanford.edu/software/corenlp.shtml) and their [coreference resolution system](https://nlp.stanford.edu/software/dcoref.shtml)\n\n\n## Tutorials\n[Back to Top](#contents)\n\n### Reading Content\n\nGeneral Machine Learning\n\n* [Machine Learning 101](https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/edit?usp=sharing) from Google''s Senior Creative Engineer explains Machine Learning for engineer''s and executives alike\n* [AI Playbook](https://aiplaybook.a16z.com/) - a16z AI playbook is a great link to forward to your managers or content for your presentations\n* [Ruder''s Blog](http://ruder.io/#open) by [Sebastian Ruder](https://twitter.com/seb_ruder) for commentary on the best of NLP Research\n* [How To Label Data](https://www.lighttag.io/how-to-label-data/) guide to managing larger linguistic annotation projects\n* [Depends on the Definition](https://www.depends-on-the-definition.com/) collection of blog posts covering a wide array of NLP topics with detailed implementation\n\nIntroductions and Guides to NLP\n\n* [Understand & Implement Natural Language Processing](https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/)\n* [NLP in Python](http://github.com/NirantK/nlp-python-deep-learning) - Collection of Github notebooks\n* [Natural Language Processing: An Introduction](https://academic.oup.com/jamia/article/18/5/544/829676) - Oxford\n* [Deep Learning for NLP with Pytorch](https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html)\n* [Hands-On NLTK Tutorial](https://github.com/hb20007/hands-on-nltk-tutorial) - NLTK Tutorials, Jupyter notebooks\n* [Natural Language Processing with Python â€“ Analyzing Text with the Natural Language Toolkit](https://www.nltk.org/book/) - An online and print book introducing NLP concepts using NLTK. The book''s authors also wrote the NLTK library.\n* [Train a new language model from scratch](https://huggingface.co/blog/how-to-train) - Hugging Face ğŸ¤—\n* [The Super Duper NLP Repo (SDNLPR)](https://notebooks.quantumstat.com/): Collection of Colab notebooks covering a wide array of NLP task implementations.\n\nBlogs and Newsletters\n\n* [Deep Learning, NLP, and Representations](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\n* [The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/) and [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n* [Natural Language Processing](https://nlpers.blogspot.com/) by Hal DaumÃ© III\n* [arXiv: Natural Language Processing (Almost) from Scratch](https://arxiv.org/pdf/1103.0398.pdf)\n* [Karpathy''s The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness)\n* [Machine Learning Mastery: Deep Learning for Natural Language Processing](https://machinelearningmastery.com/category/natural-language-processing)\n* [Visual NLP Paper Summaries](https://amitness.com/categories/#nlp)\n\n### Videos and Online Courses\n[Back to Top](#contents)\n\n* [Advanced Natural Language Processing](https://people.cs.umass.edu/~miyyer/cs685_f20/) - CS 685, UMass Amherst CS\n* [Deep Natural Language Processing](https://github.com/oxford-cs-deepnlp-2017/lectures) - Lectures series from Oxford\n* [Deep Learning for Natural Language Processing (cs224-n)](https://web.stanford.edu/class/cs224n/) - Richard Socher and Christopher Manning''s Stanford Course\n* [Neural Networks for NLP](http://phontron.com/class/nn4nlp2017/) - Carnegie Mellon Language Technology Institute there\n* [Deep NLP Course](https://github.com/yandexdataschool/nlp_course) by Yandex Data School, covering important ideas from text embedding to machine translation including sequence modeling, language models and so on.\n* [fast.ai Code-First Intro to Natural Language Processing](https://www.fast.ai/2019/07/08/fastai-nlp/) - This covers a blend of traditional NLP topics (including regex, SVD, naive bayes, tokenization) and recent neural network approaches (including RNNs, seq2seq, GRUs, and the Transformer), as well as addressing urgent ethical issues, such as bias and disinformation. Find the Jupyter Notebooks [here](https://github.com/fastai/course-nlp)\n* [Machine Learning University - Accelerated Natural Language Processing](https://www.youtube.com/playlist?list=PL8P_Z6C4GcuWfAq8Pt6PBYlck4OprHXsw) - Lectures go from introduction to NLP and text processing to Recurrent Neural Networks and Transformers.\nMaterial can be found [here](https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp).\n* [Applied Natural Language Processing](https://www.youtube.com/playlist?list=PLH-xYrxjfO2WyR3pOAB006CYMhNt4wTqp)- Lecture series from IIT Madras taking from the basics all the way to autoencoders and everything. The github notebooks for this course are also available [here](https://github.com/Ramaseshanr/anlp)\n\n\n### Books\n\n* [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) - free, by Prof. Dan Jurafsy\n* [Natural Language Processing](https://github.com/jacobeisenstein/gt-nlp-class) - free, NLP notes by Dr. Jacob Eisenstein at GeorgiaTech\n* [NLP with PyTorch](https://github.com/joosthub/PyTorchNLPBook) - Brian & Delip Rao\n* [Text Mining in R](https://www.tidytextmining.com)\n* [Natural Language Processing with Python](https://www.nltk.org/book/)\n* [Practical Natural Language Processing](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/)\n* [Natural Language Processing with Spark NLP](https://www.oreilly.com/library/view/natural-language-processing/9781492047759/)\n* [Deep Learning for Natural Language Processing](https://www.manning.com/books/deep-learning-for-natural-language-processing) by Stephan Raaijmakers\n* [Real-World Natural Language Processing](https://www.manning.com/books/real-world-natural-language-processing) - by Masato Hagiwara\n* [Natural Language Processing in Action, Second Edition](https://www.manning.com/books/natural-language-processing-in-action-second-edition) - by Hobson Lane and Maria Dyshel\n## Libraries\n\n[Back to Top](#contents)\n\n* <a id="node-js">**Node.js and Javascript** - Node.js Libaries for NLP</a> | [Back to Top](#contents)\n  * [Twitter-text](https://github.com/twitter/twitter-text) - A JavaScript implementation of Twitter''s text processing library\n  * [Knwl.js](https://github.com/benhmoore/Knwl.js) - A Natural Language Processor in JS\n  * [Retext](https://github.com/retextjs/retext) - Extensible system for analyzing and manipulating natural language\n  * [NLP Compromise](https://github.com/spencermountain/compromise) - Natural Language processing in the browser\n  * [Natural](https://github.com/NaturalNode/natural) - general natural language facilities for node\n  * [Poplar](https://github.com/synyi/poplar) - A web-based annotation tool for natural language processing (NLP)\n  * [NLP.js](https://github.com/axa-group/nlp.js) - An NLP library for building bots\n  * [node-question-answering](https://github.com/huggingface/node-question-answering) - Fast and production-ready question answering w/ DistilBERT in Node.js\n\n* <a id="python"> **Python** - Python NLP Libraries</a> | [Back to Top](#contents)\n  - [sentimental-onix](https://github.com/sloev/sentimental-onix) Sentiment models for spacy using onnx\n  - [TextAttack](https://github.com/QData/TextAttack) - Adversarial attacks, adversarial training, and data augmentation in NLP\n  - [TextBlob](http://textblob.readthedocs.org/) - Providing a consistent API for diving into common natural language processing (NLP) tasks. Stands on the giant shoulders of [Natural Language Toolkit (NLTK)](https://www.nltk.org/) and [Pattern](https://github.com/clips/pattern), and plays nicely with both :+1:\n  - [spaCy](https://github.com/explosion/spaCy) - Industrial strength NLP with Python and Cython :+1:\n  - [Speedster](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/speedster) - Automatically apply SOTA optimization techniques to achieve the maximum inference speed-up on your hardware\n    - [textacy](https://github.com/chartbeat-labs/textacy) - Higher level NLP built on spaCy\n  - [gensim](https://radimrehurek.com/gensim/index.html) - Python library to conduct unsupervised semantic modelling from plain text :+1:\n  - [scattertext](https://github.com/JasonKessler/scattertext) - Python library to produce d3 visualizations of how language differs between corpora\n  - [GluonNLP](https://github.com/dmlc/gluon-nlp) - A deep learning toolkit for NLP, built on MXNet/Gluon, for research prototyping and industrial deployment of state-of-the-art models on a wide range of NLP tasks.\n  - [AllenNLP](https://github.com/allenai/allennlp) - An NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks.\n  - [PyTorch-NLP](https://github.com/PetrochukM/PyTorch-NLP) - NLP research toolkit designed to support rapid prototyping with better data loaders, word vector loaders, neural network layer representations, common NLP metrics such as BLEU\n  - [Rosetta](https://github.com/columbia-applied-data-science/rosetta) - Text processing tools and wrappers (e.g. Vowpal Wabbit)\n  - [PyNLPl](https://github.com/proycon/pynlpl) - Python Natural Language Processing Library. General purpose NLP library for Python, handles some specific formats like ARPA language models, Moses phrasetables, GIZA++ alignments.\n  - [foliapy](https://github.com/proycon/foliapy) - Python library for working with [FoLiA](https://proycon.github.io/folia/), an XML format for linguistic annotation.\n  - [PySS3](https://github.com/sergioburdisso/pyss3) - Python package that implements a novel white-box machine learning model for text classification, called SS3. Since SS3 has the ability to visually explain its rationale, this package also comes with easy-to-use interactive visualizations tools ([online demos](http://tworld.io/ss3/)).\n  - [jPTDP](https://github.com/datquocnguyen/jPTDP) - A toolkit for joint part-of-speech (POS) tagging and dependency parsing. jPTDP provides pre-trained models for 40+ languages.\n  - [BigARTM](https://github.com/bigartm/bigartm) - a fast library for topic modelling\n  - [Snips NLU](https://github.com/snipsco/snips-nlu) - A production ready library for intent parsing\n  - [Chazutsu](https://github.com/chakki-works/chazutsu) - A library for downloading&parsing standard NLP research datasets\n  - [Word Forms](https://github.com/gutfeeling/word_forms) - Word forms can accurately generate all possible forms of an English word\n  - [Multilingual Latent Dirichlet Allocation (LDA)](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA) - A multilingual and extensible document clustering pipeline\n  - [Natural Language Toolkit (NLTK)](https://www.nltk.org/) - A library containing a wide variety of NLP functionality, supporting over 50 corpora.\n  - [NLP Architect](https://github.com/NervanaSystems/nlp-architect) - A library for exploring the state-of-the-art deep learning topologies and techniques for NLP and NLU\n  - [Flair](https://github.com/zalandoresearch/flair) - A very simple framework for state-of-the-art multilingual NLP built on PyTorch. Includes BERT, ELMo and Flair embeddings.\n  - [Kashgari](https://github.com/BrikerMan/Kashgari) - Simple, Keras-powered multilingual NLP framework, allows you to build your models in 5 minutes for named entity recognition (NER), part-of-speech tagging (PoS) and text classification tasks. Includes BERT and word2vec embedding.\n  - [FARM](https://github.com/deepset-ai/FARM) - Fast & easy transfer learning for NLP. Harvesting language models for the industry. Focus on Question Answering.\n  - [Haystack](https://github.com/deepset-ai/haystack) - End-to-end Python framework for building natural language search interfaces to data. Leverages Transformers and the State-of-the-Art of NLP. Supports DPR, Elasticsearch, HuggingFaceâ€™s Modelhub, and much more!\n  - [Rita DSL](https://github.com/zaibacu/rita-dsl) - a DSL, loosely based on [RUTA on Apache UIMA](https://uima.apache.org/ruta.html). Allows to define language patterns (rule-based NLP) which are then translated into [spaCy](https://spacy.io/), or if you prefer less features and lightweight - regex patterns.\n  - [Transformers](https://github.com/huggingface/transformers) - Natural Language Processing for TensorFlow 2.0 and PyTorch.\n  - [Tokenizers](https://github.com/huggingface/tokenizers) - Tokenizers optimized for Research and Production.\n  - [fairSeq](https://github.com/pytorch/fairseq) Facebook AI Research implementations of SOTA seq2seq models in Pytorch. \n  - [corex_topic](https://github.com/gregversteeg/corex_topic) - Hierarchical Topic Modeling with Minimal Domain Knowledge\n  - [Sockeye](https://github.com/awslabs/sockeye) - Neural Machine Translation (NMT) toolkit that powers Amazon Translate.\n  - [DL Translate](https://github.com/xhlulu/dl-translate) - A deep learning-based translation library for 50 languages, built on `transformers` and Facebook''s mBART Large.\n  - [Jury](https://github.com/obss/jury) - Evaluation of NLP model outputs offering various automated metrics.\n  - [python-ucto](https://github.com/proycon/python-ucto) - Unicode-aware regular-expression based tokenizer for various languages. Python binding to C++ library, supports [FoLiA format](https://proycon.github.io/folia).\n\n- <a id="c++">**C++** - C++ Libraries</a> | [Back to Top](#contents)\n  - [InsNet](https://github.com/chncwang/InsNet) - A neural network library for building instance-dependent NLP models with padding-free dynamic batching.\n  - [MIT Information Extraction Toolkit](https://github.com/mit-nlp/MITIE) - C, C++, and Python tools for named entity recognition and relation extraction\n  - [CRF++](https://taku910.github.io/crfpp/) - Open source implementation of Conditional Random Fields (CRFs) for segmenting/labeling sequential data & other Natural Language Processing tasks.\n  - [CRFsuite](http://www.chokkan.org/software/crfsuite/) - CRFsuite is an implementation of Conditional Random Fields (CRFs) for labeling sequential data.\n  - [BLLIP Parser](https://github.com/BLLIP/bllip-parser) - BLLIP Natural Language Parser (also known as the Charniak-Johnson parser)\n  - [colibri-core](https://github.com/proycon/colibri-core) - C++ library, command line tools, and Python binding for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.\n  - [ucto](https://github.com/LanguageMachines/ucto) - Unicode-aware regular-expression based tokenizer for various languages. Tool and C++ library. Supports FoLiA format.\n  - [libfolia](https://github.com/LanguageMachines/libfolia) - C++ library for the [FoLiA format](https://proycon.github.io/folia/)\n  - [frog](https://github.com/LanguageMachines/frog) - Memory-based NLP suite developed for Dutch: PoS tagger, lemmatiser, dependency parser, NER, shallow parser, morphological analyzer.\n  - [MeTA](https://github.com/meta-toolkit/meta) - [MeTA : ModErn Text Analysis](https://meta-toolkit.org/) is a C++ Data Sciences Toolkit that facilitates mining big text data.\n  - [Mecab (Japanese)](https://taku910.github.io/mecab/)\n  - [Moses](http://statmt.org/moses/)\n  - [StarSpace](https://github.com/facebookresearch/StarSpace) - a library from Facebook for creating embeddings of word-level, paragraph-level, document-level and for text classification\n\n- <a id="java">**Java** - Java NLP Libraries</a> | [Back to Top](#contents)\n  - [Stanford NLP](https://nlp.stanford.edu/software/index.shtml)\n  - [OpenNLP](https://opennlp.apache.org/)\n  - [NLP4J](https://emorynlp.github.io/nlp4j/)\n  - [Word2vec in Java](https://deeplearning4j.org/docs/latest/deeplearning4j-nlp-word2vec)\n  - [ReVerb](https://github.com/knowitall/reverb/) Web-Scale Open Information Extraction\n  - [OpenRegex](https://github.com/knowitall/openregex) An efficient and flexible token-based regular expression language and engine.\n  - [CogcompNLP](https://github.com/CogComp/cogcomp-nlp) - Core libraries developed in the U of Illinois'' Cognitive Computation Group.\n  - [MALLET](http://mallet.cs.umass.edu/) - MAchine Learning for LanguagE Toolkit - package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.\n  - [RDRPOSTagger](https://github.com/datquocnguyen/RDRPOSTagger) - A robust POS tagging toolkit available (in both Java & Python) together with pre-trained models for 40+ languages.\n\n- <a id="kotlin">**Kotlin** - Kotlin NLP Libraries</a> | [Back to Top](#contents)\n  - [Lingua](https://github.com/pemistahl/lingua/) A language detection library for Kotlin and Java, suitable for long and short text alike\n  - [Kotidgy](https://github.com/meiblorn/kotidgy) â€” an index-based text data generator written in Kotlin\n\n- <a id="scala">**Scala** - Scala NLP Libraries</a> | [Back to Top](#contents)\n  - [Saul](https://github.com/CogComp/saul) - Library for developing NLP systems, including built in modules like SRL, POS, etc.\n  - [ATR4S](https://github.com/ispras/atr4s) - Toolkit with state-of-the-art [automatic term recognition](https://en.wikipedia.org/wiki/Terminology_extraction) methods.\n  - [tm](https://github.com/ispras/tm) - Implementation of topic modeling based on regularized multilingual [PLSA](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis).\n  - [word2vec-scala](https://github.com/Refefer/word2vec-scala) - Scala interface to word2vec model; includes operations on vectors like word-distance and word-analogy.\n  - [Epic](https://github.com/dlwh/epic) - Epic is a high performance statistical parser written in Scala, along with a framework for building complex structured prediction models.\n  - [Spark NLP](https://github.com/JohnSnowLabs/spark-nlp) - Spark NLP is a natural language processing library built on top of Apache Spark ML that provides simple, performant & accurate NLP annotations for machine learning pipelines that scale easily in a distributed environment.\n\n- <a id="R">**R** - R NLP Libraries</a> | [Back to Top](#contents)\n  - [text2vec](https://github.com/dselivanov/text2vec) - Fast vectorization, topic modeling, distances and GloVe word embeddings in R.\n  - [wordVectors](https://github.com/bmschmidt/wordVectors) - An R package for creating and exploring word2vec and other word embedding models\n  - [RMallet](https://github.com/mimno/RMallet) - R package to interface with the Java machine learning tool MALLET\n  - [dfr-browser](https://github.com/agoldst/dfr-browser) - Creates d3 visualizations for browsing topic models of text in a web browser.\n  - [dfrtopics](https://github.com/agoldst/dfrtopics) - R package for exploring topic models of text.\n  - [sentiment_classifier](https://github.com/kevincobain2000/sentiment_classifier) - Sentiment Classification using Word Sense Disambiguation and WordNet Reader\n  - [jProcessing](https://github.com/kevincobain2000/jProcessing) - Japanese Natural Langauge Processing Libraries, with Japanese sentiment classification\n  - [corporaexplorer](https://kgjerde.github.io/corporaexplorer/) - An R package for dynamic exploration of text collections\n  - [tidytext](https://github.com/juliasilge/tidytext) - Text mining using tidy tools\n  - [spacyr](https://github.com/quanteda/spacyr) - R wrapper to spaCy NLP\n  - [CRAN Task View: Natural Language Processing](https://github.com/cran-task-views/NaturalLanguageProcessing/)\n\n- <a id="clojure">**Clojure**</a> | [Back to Top](#contents)\n  - [Clojure-openNLP](https://github.com/dakrone/clojure-opennlp) - Natural Language Processing in Clojure (opennlp)\n  - [Infections-clj](https://github.com/r0man/inflections-clj) - Rails-like inflection library for Clojure and ClojureScript\n  - [postagga](https://github.com/fekr/postagga) - A library to parse natural language in Clojure and ClojureScript\n\n- <a id="ruby">**Ruby**</a> | [Back to Top](#contents)\n  - Kevin Dias''s [A collection of Natural Language Processing (NLP) Ruby libraries, tools and software](https://github.com/diasks2/ruby-nlp)\n  - [Practical Natural Language Processing done in Ruby](https://github.com/arbox/nlp-with-ruby)\n\n- <a id="rust">**Rust**</a> | [Back to Top](#contents)\n  - [whatlang](https://github.com/greyblake/whatlang-rs) â€” Natural language recognition library based on trigrams\n  - [snips-nlu-rs](https://github.com/snipsco/snips-nlu-rs) - A production ready library for intent parsing\n  - [rust-bert](https://github.com/guillaume-be/rust-bert) - Ready-to-use NLP pipelines and Transformer-based models\n\n- <a id="NLP++">**NLP++** - NLP++ Language</a> | [Back to Top](#contents)\n  - [VSCode Language Extension](https://marketplace.visualstudio.com/items?itemName=dehilster.nlp) - NLP++ Language Extension for VSCode\n  - [nlp-engine](https://github.com/VisualText/nlp-engine) - NLP++ engine to run NLP++ code on Linux including a full English parser\n  - [VisualText](http://visualtext.org) - Homepage for the NLP++ Language\n  - [NLP++ Wiki](http://wiki.naturalphilosophy.org/index.php?title=NLP%2B%2B) - Wiki entry for the NLP++ language\n\n- <a id="julia">**Julia**</a> | [Back to Top](#contents)\n  - [CorpusLoaders](https://github.com/JuliaText/CorpusLoaders.jl) - A variety of loaders for various NLP corpora\n  - [Languages](https://github.com/JuliaText/Languages.jl) - A package for working with human languages\n  - [TextAnalysis](https://github.com/JuliaText/TextAnalysis.jl) - Julia package for text analysis\n  - [TextModels](https://github.com/JuliaText/TextModels.jl) - Neural Network based models for Natural Language Processing\n  - [WordTokenizers](https://github.com/JuliaText/WordTokenizers.jl) - High performance tokenizers for natural language processing and other related tasks\n  - [Word2Vec](https://github.com/JuliaText/Word2Vec.jl) - Julia interface to word2vec\n\n### Services\n\nNLP as API with higher level functionality such as NER, Topic tagging and so on | [Back to Top](#contents)\n\n- [Wit-ai](https://github.com/wit-ai/wit) - Natural Language Interface for apps and devices\n- [IBM Watson''s Natural Language Understanding](https://github.com/watson-developer-cloud/natural-language-understanding-nodejs) - API and Github demo\n- [Amazon Comprehend](https://aws.amazon.com/comprehend/) - NLP and ML suite covers most common tasks like NER, tagging, and sentiment analysis\n- [Google Cloud Natural Language API](https://cloud.google.com/natural-language/) - Syntax Analysis, NER, Sentiment Analysis, and Content tagging in atleast 9 languages include English and Chinese (Simplified and Traditional).\n- [ParallelDots](https://www.paralleldots.com/text-analysis-apis) - High level Text Analysis API Service ranging from Sentiment Analysis to Intent Analysis\n- [Microsoft Cognitive Service](https://azure.microsoft.com/en-us/services/cognitive-services/text-analytics/)\n- [TextRazor](https://www.textrazor.com/)\n- [Rosette](https://www.rosette.com/)\n- [Textalytic](https://www.textalytic.com) - Natural Language Processing in the Browser with sentiment analysis, named entity extraction, POS tagging, word frequencies, topic modeling, word clouds, and more\n- [NLP Cloud](https://nlpcloud.io) - SpaCy NLP models (custom and pre-trained ones) served through a RESTful API for named entity recognition (NER), POS tagging, and more.\n- [Cloudmersive](https://cloudmersive.com/nlp-api) - Unified and free NLP APIs that perform actions such as speech tagging, text rephrasing, language translation/detection, and sentence parsing\n\n### Annotation Tools\n\n- [GATE](https://gate.ac.uk/overview.html) - General Architecture and Text Engineering is 15+ years old, free and open source\n- [Anafora](https://github.com/weitechen/anafora) is free and open source, web-based raw text annotation tool\n- [brat](https://brat.nlplab.org/) - brat rapid annotation tool is an online environment for collaborative text annotation\n- [doccano](https://github.com/chakki-works/doccano) - doccano is free, open-source, and provides annotation features for text classification, sequence labeling and sequence to sequence\n- [INCEpTION](https://inception-project.github.io) - A semantic annotation platform offering intelligent assistance and knowledge management\n- [tagtog](https://www.tagtog.net/), team-first web tool to find, create, maintain, and share datasets - costs $\n- [prodigy](https://prodi.gy/) is an annotation tool powered by active learning, costs $\n- [LightTag](https://lighttag.io) - Hosted and managed text annotation tool for teams, costs $\n- [rstWeb](https://corpling.uis.georgetown.edu/rstweb/info/) - open source local or online tool for discourse tree annotations\n- [GitDox](https://corpling.uis.georgetown.edu/gitdox/) - open source server annotation tool with GitHub version control and validation for XML data and collaborative spreadsheet grids\n- [Label Studio](https://www.heartex.ai/) - Hosted and managed text annotation tool for teams, freemium based, costs $\n- [Datasaur](https://datasaur.ai/) support various NLP tasks for individual or teams, freemium based\n- [Konfuzio](https://konfuzio.com/en/) - team-first hosted and on-prem text, image and PDF annotation tool powered by active learning, freemium based, costs $\n- [UBIAI](https://ubiai.tools/) - Easy-to-use text annotation tool for teams with most comprehensive auto-annotation features. Supports NER, relations and document classification as well as OCR annotation for invoice labeling, costs $\n- [Shoonya](https://github.com/AI4Bharat/Shoonya-Backend) - Shoonya is free and open source data annotation platform with wide varities of organization and workspace level management system. Shoonya is data agnostic, can be used by teams to annotate data with various level of verification stages at scale.\n- [Annotation Lab](https://www.johnsnowlabs.com/annotation-lab/) - Free End-to-End No-Code platform for text annotation and DL model training/tuning. Out-of-the-box support for Named Entity Recognition, Classification, Relation extraction and Assertion Status Spark NLP models. Unlimited support for users, teams, projects, documents. Not FOSS. \n- [FLAT](https://github.com/proycon/flat) - FLAT is a web-based linguistic annotation environment based around the [FoLiA format](http://proycon.github.io/folia), a rich XML-based format for linguistic annotation. Free and open source.\n\n\n## Techniques\n\n### Text Embeddings\n\n#### Word Embeddings\n\n- Thumb Rule: **fastText >> GloVe > word2vec**\n\n- [word2vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) - [implementation](https://code.google.com/archive/p/word2vec/) - [explainer blog](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\n- [glove](https://nlp.stanford.edu/pubs/glove.pdf) - [explainer blog](https://blog.acolyer.org/2016/04/22/glove-global-vectors-for-word-representation/)\n- fasttext - [implementation](https://github.com/facebookresearch/fastText) - [paper](https://arxiv.org/abs/1607.04606) - [explainer blog](https://towardsdatascience.com/fasttext-under-the-hood-11efc57b2b3)\n\n#### Sentence and Language Model Based Word Embeddings\n\n[Back to Top](#contents)\n\n- ElMo - [Deep Contextualized Word Representations](https://arxiv.org/abs/1802.05365) - [PyTorch implmentation](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md) - [TF Implementation](https://github.com/allenai/bilm-tf)\n- ULMFiT - [Universal Language Model Fine-tuning for Text Classification](https://arxiv.org/abs/1801.06146) by Jeremy Howard and Sebastian Ruder\n- InferSent - [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](https://arxiv.org/abs/1705.02364) by facebook\n- CoVe - [Learned in Translation: Contextualized Word Vectors](https://arxiv.org/abs/1708.00107)\n- Pargraph vectors - from [Distributed Representations of Sentences and Documents](https://cs.stanford.edu/~quocle/paragraph_vector.pdf). See [doc2vec tutorial at gensim](https://rare-technologies.com/doc2vec-tutorial/)\n- [sense2vec](https://arxiv.org/abs/1511.06388) - on word sense disambiguation\n- [Skip Thought Vectors](https://arxiv.org/abs/1506.06726) - word representation method\n- [Adaptive skip-gram](https://arxiv.org/abs/1502.07257) - similar approach, with adaptive properties\n- [Sequence to Sequence Learning](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) - word vectors for machine translation\n\n### Question Answering and Knowledge Extraction\n\n[Back to Top](#contents)\n\n- [DrQA](https://github.com/facebookresearch/DrQA) - Open Domain Question Answering work by Facebook Research on Wikipedia data\n- [Document-QA](https://github.com/allenai/document-qa) - Simple and Effective Multi-Paragraph Reading Comprehension by AllenAI\n- [Template-Based Information Extraction without the Templates](https://www.usna.edu/Users/cs/nchamber/pubs/acl2011-chambers-templates.pdf)\n- [Privee: An Architecture for Automatically Analyzing Web Privacy Policies](https://www.sebastianzimmeck.de/zimmeckAndBellovin2014Privee.pdf)\n\n## Datasets\n\n[Back to Top](#contents)\n\n- [nlp-datasets](https://github.com/niderhoff/nlp-datasets) great collection of nlp datasets\n- [gensim-data](https://github.com/RaRe-Technologies/gensim-data) - Data repository for pretrained NLP models and NLP corpora.\n- [tiny_qa_benchmark_pp](https://github.com/vincentkoc/tiny_qa_benchmark_pp/) - Repository of tiny NLP multi-lingual QA datasets and library to generate your own synthetic copies.\n\n## Multilingual NLP Frameworks\n\n[Back to Top](#contents)\n\n- [UDPipe](https://github.com/ufal/udpipe) is a trainable pipeline for tokenizing, tagging, lemmatizing and parsing Universal Treebanks and other CoNLL-U files. Primarily written in C++, offers a fast and reliable solution for multilingual NLP processing.\n- [NLP-Cube](https://github.com/adobe/NLP-Cube) : Natural Language Processing Pipeline - Sentence Splitting, Tokenization, Lemmatization, Part-of-speech Tagging and Dependency Parsing. New platform, written in Python with Dynet 2.0. Offers standalone (CLI/Python bindings) and server functionality (REST API).\n- [UralicNLP](https://github.com/mikahama/uralicNLP) is an NLP library mostly for many endangered Uralic languages such as Sami languages, Mordvin languages, Mari languages, Komi languages and so on. Also some non-endangered languages are supported such as Finnish together with non-Uralic languages such as Swedish and Arabic. UralicNLP can do morphological analysis, generation, lemmatization and disambiguation.\n\n## NLP in Korean\n\n[Back to Top](#contents)\n\n### Libraries\n\n- [KoNLPy](http://konlpy.org) - Python package for Korean natural language processing.\n- [Mecab (Korean)](https://eunjeon.blogspot.com/) - C++ library for Korean NLP\n- [KoalaNLP](https://koalanlp.github.io/koalanlp/) - Scala library for Korean Natural Language Processing.\n- [KoNLP](https://cran.r-project.org/package=KoNLP) - R package for Korean Natural language processing\n\n### Blogs and Tutorials\n\n- [dsindex''s blog](https://dsindex.github.io/)\n- [Kangwon University''s NLP course in Korean](http://cs.kangwon.ac.kr/~leeck/NLP/)\n\n### Datasets\n\n- [KAIST Corpus](http://semanticweb.kaist.ac.kr/home/index.php/KAIST_Corpus) - A corpus from the Korea Advanced Institute of Science and Technology in Korean.\n- [Naver Sentiment Movie Corpus in Korean](https://github.com/e9t/nsmc/)\n- [Chosun Ilbo archive](http://srchdb1.chosun.com/pdf/i_archive/) - dataset in Korean from one of the major newspapers in South Korea, the Chosun Ilbo.\n- [Chat data](https://github.com/songys/Chatbot_data) - Chatbot data in Korean\n- [Petitions](https://github.com/akngs/petitions) - Collect expired petition data from the Blue House National Petition Site.\n- [Korean Parallel corpora](https://github.com/j-min/korean-parallel-corpora) - Neural Machine Translation(NMT) Dataset for **Korean to French** & **Korean to English**\n- [KorQuAD](https://korquad.github.io/) - Korean SQuAD dataset with Wiki HTML source. Mentions both v1.0 and v2.1 at the time of adding to Awesome NLP\n\n## NLP in Arabic\n\n[Back to Top](#contents)\n\n### Libraries\n\n- [goarabic](https://github.com/01walid/goarabic) - Go package for Arabic text processing\n- [jsastem](https://github.com/ejtaal/jsastem) - Javascript for Arabic stemming\n- [PyArabic](https://pypi.org/project/PyArabic/) - Python libraries for Arabic\n- [RFTokenizer](https://github.com/amir-zeldes/RFTokenizer) - trainable Python segmenter for Arabic, Hebrew and Coptic\n\n### Datasets\n\n- [Multidomain Datasets](https://github.com/hadyelsahar/large-arabic-sentiment-analysis-resouces) - Largest Available Multi-Domain Resources for Arabic Sentiment Analysis\n- [LABR](https://github.com/mohamedadaly/labr) - LArge Arabic Book Reviews dataset\n- [Arabic Stopwords](https://github.com/mohataher/arabic-stop-words) - A list of Arabic stopwords from various resources\n\n## NLP in Chinese\n\n[Back to Top](#contents)\n\n### Libraries\n\n- [jieba](https://github.com/fxsjy/jieba#jieba-1) - Python package for Words Segmentation Utilities in Chinese\n- [SnowNLP](https://github.com/isnowfy/snownlp) - Python package for Chinese NLP\n- [FudanNLP](https://github.com/FudanNLP/fnlp) - Java library for Chinese text processing\n- [HanLP](https://github.com/hankcs/HanLP) - The multilingual NLP library\n\n### Anthology\n- [funNLP](https://github.com/fighting41love/funNLP) - Collection of NLP tools and resources mainly for Chinese\n\n## NLP in German\n\n- [German-NLP](https://github.com/adbar/German-NLP) - Curated list of open-access/open-source/off-the-shelf resources and tools developed with a particular focus on German\n\n## NLP in Polish\n\n- [Polish-NLP](https://github.com/ksopyla/awesome-nlp-polish) - A curated list of resources dedicated to Natural Language Processing (NLP) in polish. Models, tools, datasets.\n\n## NLP in Spanish\n\n[Back to Top](#contents)\n\n### Libraries\n\n- [spanlp](https://github.com/jfreddypuentes/spanlp) - Python library to detect, censor and clean profanity, vulgarities, hateful words, racism, xenophobia and bullying in texts written in Spanish. It contains data of 21 Spanish-speaking countries.\n\n### Data\n\n- [Columbian Political Speeches](https://github.com/dav009/LatinamericanTextResources)\n- [Copenhagen Treebank](https://mbkromann.github.io/copenhagen-dependency-treebank/)\n- [Spanish Billion words corpus with Word2Vec embeddings](https://github.com/crscardellino/sbwce)\n- [Compilation of Spanish Unannotated Corpora](https://github.com/josecannete/spanish-unannotated-corpora)\n\n### Word and Sentence Embeddings\n- [Spanish Word Embeddings Computed with Different Methods and from Different Corpora](https://github.com/dccuchile/spanish-word-embeddings)\n- [Spanish Word Embeddings Computed from Large Corpora and Different Sizes Using fastText](https://github.com/BotCenter/spanishWordEmbeddings)\n- [Spanish Sentence Embeddings Computed from Large Corpora Using sent2vec](https://github.com/BotCenter/spanishSent2Vec)\n- [Beto - BERT for Spanish](https://github.com/dccuchile/beto)\n\n\n## NLP in Indic languages\n\n[Back to Top](#contents)\n\n### Data, Corpora and Treebanks\n\n- [Hindi Dependency Treebank](https://ltrc.iiit.ac.in/treebank_H2014/) - A multi-representational multi-layered treebank for Hindi and Urdu\n- [Universal Dependencies Treebank in Hindi](https://universaldependencies.org/treebanks/hi_hdtb/index.html)\n  - [Parallel Universal Dependencies Treebank in Hindi](http://universaldependencies.org/treebanks/hi_pud/index.html) - A smaller part of the above-mentioned treebank.\n- [ISI FIRE Stopwords List (Hindi and Bangla)](https://www.isical.ac.in/~fire/data/)\n- [Peter Graham''s Stopwords List](https://github.com/6/stopwords-json)\n- [NLTK Corpus](https://www.nltk.org/book/ch02.html) 60k Words POS Tagged, Bangla, Hindi, Marathi, Telugu\n- [Hindi Movie Reviews Dataset](https://github.com/goru001/nlp-for-hindi) ~1k Samples, 3 polarity classes\n- [BBC News Hindi Dataset](https://github.com/NirantK/hindi2vec/releases/tag/bbc-hindi-v0.1) 4.3k Samples, 14 classes\n- [IIT Patna Hindi ABSA Dataset](https://github.com/pnisarg/ABSA) 5.4k Samples, 12 Domains, 4k aspect terms, aspect and sentence level polarity in 4 classes\n- [Bangla ABSA](https://github.com/AtikRahman/Bangla_Datasets_ABSA) 5.5k Samples, 2 Domains, 10 aspect terms\n- [IIT Patna Movie Review Sentiment Dataset](https://www.iitp.ac.in/~ai-nlp-ml/resources.html) 2k Samples, 3 polarity labels\n\n#### Corpora/Datasets that need a login/access can be gained via email\n\n- [SAIL 2015](http://amitavadas.com/SAIL/) Twitter and Facebook labelled sentiment samples in Hindi, Bengali, Tamil, Telugu.\n- [IIT Bombay NLP Resources](http://www.cfilt.iitb.ac.in/Sentiment_Analysis_Resources.html) Sentiwordnet, Movie and Tourism parallel labelled corpora, polarity labelled sense annotated corpus, Marathi polarity labelled corpus.\n- [TDIL-IC aggregates a lot of useful resources and provides access to otherwise gated datasets](https://tdil-dc.in/index.php?option=com_catalogue&task=viewTools&id=83&lang=en)\n\n### Language Models and Word Embeddings\n\n- [Hindi2Vec](https://nirantk.com/hindi2vec/) and [nlp-for-hindi](https://github.com/goru001/nlp-for-hindi) ULMFIT style languge model\n- [IIT Patna Bilingual Word Embeddings Hi-En](https://www.iitp.ac.in/~ai-nlp-ml/resources.html)\n- [Fasttext word embeddings in a whole bunch of languages, trained on Common Crawl](https://fasttext.cc/docs/en/crawl-vectors.html)\n- [Hindi and Bengali Word2Vec](https://github.com/Kyubyong/wordvectors)\n- [Hindi and Urdu Elmo Model](https://github.com/HIT-SCIR/ELMoForManyLangs)\n- [Sanskrit Albert](https://huggingface.co/surajp/albert-base-sanskrit) Trained on Sanskrit Wikipedia and OSCAR corpus\n\n### Libraries and Tooling\n\n- [Multi-Task Deep Morphological Analyzer](https://github.com/Saurav0074/mt-dma) Deep Network based Morphological Parser for Hindi and Urdu\n- [Anoop Kunchukuttan](https://github.com/anoopkunchukuttan/indic_nlp_library) 18 Languages, whole host of features from tokenization to translation\n- [SivaReddy''s Dependency Parser](http://sivareddy.in/downloads) Dependency Parser and Pos Tagger for Kannada, Hindi and Telugu. [Python3 Port](https://github.com/CalmDownKarm/sivareddydependencyparser)\n- [iNLTK](https://github.com/goru001/inltk) - A Natural Language Toolkit for Indic Languages (Indian subcontinent languages) built on top of Pytorch/Fastai, which aims to provide out of the box support for common NLP tasks.\n\n## NLP in Thai\n\n[Back to Top](#contents)\n\n### Libraries\n\n- [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp) - Thai NLP in Python Package\n- [JTCC](https://github.com/wittawatj/jtcc) - A character cluster library in Java\n- [CutKum](https://github.com/pucktada/cutkum) - Word segmentation with deep learning in TensorFlow\n- [Thai Language Toolkit](https://pypi.python.org/pypi/tltk/) - Based on a paper by Wirote Aroonmanakun in 2002 with included dataset\n- [SynThai](https://github.com/KenjiroAI/SynThai) - Word segmentation and POS tagging using deep learning in Python\n\n### Data\n\n- [Inter-BEST](https://www.nectec.or.th/corpus/index.php?league=pm) - A text corpus with 5 million words with word segmentation\n- [Prime Minister 29](https://github.com/PyThaiNLP/lexicon-thai/tree/master/thai-corpus/Prime%20Minister%2029) - Dataset containing speeches of the current Prime Minister of Thailand\n\n## NLP in Danish\n\n- [Named Entity Recognition for Danish](https://github.com/ITUnlp/daner)\n- [DaNLP](https://github.com/alexandrainst/danlp) - NLP resources in Danish\n- [Awesome Danish](https://github.com/fnielsen/awesome-danish) - A curated list of awesome resources for Danish language technology\n\n## NLP in Vietnamese\n\n### Libraries\n\n- [underthesea](https://github.com/undertheseanlp/underthesea) - Vietnamese NLP Toolkit\n- [vn.vitk](https://github.com/phuonglh/vn.vitk) - A Vietnamese Text Processing Toolkit\n- [VnCoreNLP](https://github.com/vncorenlp/VnCoreNLP) - A Vietnamese natural language processing toolkit\n- [PhoBERT](https://github.com/VinAIResearch/PhoBERT) - Pre-trained language models for Vietnamese\n- [pyvi](https://github.com/trungtv/pyvi) - Python Vietnamese Core NLP Toolkit\n\n### Data\n\n- [Vietnamese treebank](https://vlsp.hpda.vn/demo/?page=resources&lang=en) - 10,000 sentences for the constituency parsing task\n- [BKTreeBank](https://arxiv.org/pdf/1710.05519.pdf) - a Vietnamese Dependency Treebank\n- [UD_Vietnamese](https://github.com/UniversalDependencies/UD_Vietnamese-VTB) - Vietnamese Universal Dependency Treebank\n- [VIVOS](https://ailab.hcmus.edu.vn/vivos/) - a free Vietnamese speech corpus consisting of 15 hours of recording speech by AILab\n- [VNTQcorpus(big).txt](http://viet.jnlp.org/download-du-lieu-tu-vung-corpus) - 1.75 million sentences in news\n- [ViText2SQL](https://github.com/VinAIResearch/ViText2SQL) - A dataset for Vietnamese Text-to-SQL semantic parsing (EMNLP-2020 Findings)\n- [EVB Corpus](https://github.com/qhungngo/EVBCorpus) - 20,000,000 words (20 million) from 15 bilingual books, 100 parallel English-Vietnamese / Vietnamese-English texts, 250 parallel law and ordinance texts, 5,000 news articles, and 2,000 film subtitles.\n\n\n## NLP for Dutch\n\n[Back to Top](#contents)\n\n- [python-frog](https://github.com/proycon/python-frog) - Python binding to Frog, an NLP suite for Dutch. (pos tagging, lemmatisation, dependency parsing, NER)\n- [SimpleNLG_NL](https://github.com/rfdj/SimpleNLG-NL) - Dutch surface realiser used for Natural Language Generation in Dutch, based on the SimpleNLG implementation for English and French.\n- [Alpino](https://github.com/rug-compling/alpino) - Dependency parser for Dutch (also does PoS tagging and Lemmatisation).\n- [Kaldi NL](https://github.com/opensource-spraakherkenning-nl/Kaldi_NL) - Dutch Speech Recognition models based on [Kaldi](http://kaldi-asr.org/).\n- [spaCy](https://spacy.io/) - [Dutch model](https://spacy.io/models/nl) available. - Industrial strength NLP with Python and Cython. \n\n\n## NLP in Indonesian\n\n### Datasets\n- Kompas and Tempo collections at [ILPS](http://ilps.science.uva.nl/resources/bahasa/)\n- [PANL10N for PoS tagging](http://www.panl10n.net/english/outputs/Indonesia/UI/0802/UI-1M-tagged.zip): 39K sentences and 900K word tokens\n- [IDN for PoS tagging](https://github.com/famrashel/idn-tagged-corpus): This corpus contains 10K sentences and 250K word tokens\n- [Indonesian Treebank](https://github.com/famrashel/idn-treebank) and [Universal Dependencies-Indonesian](https://github.com/UniversalDependencies/UD_Indonesian-GSD)\n- [IndoSum](https://github.com/kata-ai/indosum) for text summarization and classification both\n- [Wordnet-Bahasa](http://wn-msa.sourceforge.net/) - large, free, semantic dictionary\n- IndoBenchmark [IndoNLU](https://github.com/indobenchmark/indonlu) includes pre-trained language model (IndoBERT), FastText model, Indo4B corpus, and several NLU benchmark datasets\n\n### Libraries & Embedding\n- Natural language toolkit [bahasa](https://github.com/kangfend/bahasa)\n- [Indonesian Word Embedding](https://github.com/galuhsahid/indonesian-word-embedding)\n- Pretrained [Indonesian fastText Text Embedding](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.id.zip) trained on Wikipedia\n- IndoBenchmark [IndoNLU](https://github.com/indobenchmark/indonlu) includes pretrained language model (IndoBERT), FastText model, Indo4B corpus, and several NLU benchmark datasets\n\n## NLP in Urdu\n\n### Datasets\n- [Collection of Urdu datasets](https://github.com/mirfan899/Urdu) for POS, NER and NLP tasks\n\n### Libraries\n- [Natural Language Processing library](https://github.com/urduhack/urduhack) for ( ğŸ‡µğŸ‡°)Urdu language\n\n## NLP in Persian\n\n[Back to Top](#contents)\n\n### Libraries\n- [Hazm](https://github.com/roshan-research/hazm) - Persian NLP Toolkit.\n- [Parsivar](https://github.com/ICTRC/Parsivar): A Language Processing Toolkit for Persian\n- [Perke](https://github.com/AlirezaTheH/perke): Perke is a Python keyphrase extraction package for Persian language. It provides an end-to-end keyphrase extraction pipeline in which each component can be easily modified or extended to develop new models.\n- [Perstem](https://github.com/jonsafari/perstem): Persian stemmer, morphological analyzer, transliterator, and partial part-of-speech tagger\n- [ParsiAnalyzer](https://github.com/NarimanN2/ParsiAnalyzer): Persian Analyzer For Elasticsearch\n- [virastar](https://github.com/aziz/virastar): Cleaning up Persian text!\n\n### Datasets\n- [Bijankhan Corpus](https://dbrg.ut.ac.ir/Ø¨ÛŒÚ˜Ù†%E2%80%8CØ®Ø§Ù†/): Bijankhan corpus is a tagged corpus that is suitable for natural language processing research on the Persian (Farsi) language. This collection is gathered form daily news and common texts. In this collection all documents are categorized into different subjects such as political, cultural and so on. Totally, there are 4300 different subjects. The Bijankhan collection contains about 2.6 millions manually tagged words with a tag set that contains 40 Persian POS tags.\n- [Uppsala Persian Corpus (UPC)](https://sites.google.com/site/mojganserajicom/home/upc): Uppsala Persian Corpus (UPC) is a large, freely available Persian corpus. The corpus is a modified version of the Bijankhan corpus with additional sentence segmentation and consistent tokenization containing 2,704,028 tokens and annotated with 31 part-of-speech tags. The part-of-speech tags are listed with explanations in [this table](https://sites.google.com/site/mojganserajicom/home/upc/Table_tag.pdf).\n- [Large-Scale Colloquial Persian](http://hdl.handle.net/11234/1-3195): Large Scale Colloquial Persian Dataset (LSCP) is hierarchically organized in asemantic taxonomy that focuses on multi-task informal Persian language understanding as a comprehensive problem. LSCP includes 120M sentences from 27M casual Persian tweets with its dependency relations in syntactic annotation, Part-of-speech tags, sentiment polarity and automatic translation of original Persian sentences in English (EN), German (DE), Czech (CS), Italian (IT) and Hindi (HI) spoken languages. Learn more about this project at [LSCP webpage](https://iasbs.ac.ir/~ansari/lscp/).\n- [ArmanPersoNERCorpus](https://github.com/HaniehP/PersianNER): The dataset includes 250,015 tokens and 7,682 Persian sentences in total. It is available in 3 folds to be used in turn as training and test sets. Each file contains one token, along with its manually annotated named-entity tag, per line. Each sentence is separated with a newline. The NER tags are in IOB format.\n- [FarsiYar PersianNER](https://github.com/Text-Mining/Persian-NER): The dataset includes about 25,000,000 tokens and about 1,000,000 Persian sentences in total based on [Persian Wikipedia Corpus](https://github.com/Text-Mining/Persian-Wikipedia-Corpus). The NER tags are in IOB format. More than 1000 volunteers contributed tag improvements to this dataset via web panel or android app. They release updated tags every two weeks.\n- [PERLEX](http://farsbase.net/PERLEX.html): The first Persian dataset for relation extraction, which is an expert translated version of the â€œSemeval-2010-Task-8â€ dataset. Link to the relevant publication.\n- [Persian Syntactic Dependency Treebank](http://dadegan.ir/catalog/perdt): This treebank is supplied for free noncommercial use. For commercial uses feel free to contact us. The number of annotated sentences is 29,982 sentences including samples from almost all verbs of the Persian valency lexicon.\n- [Uppsala Persian Dependency Treebank (UPDT)](http://stp.lingfil.uu.se/~mojgan/UPDT.html): Dependency-based syntactically annotated corpus.\n- [Hamshahri](https://dbrg.ut.ac.ir/hamshahri/): Hamshahri collection is a standard reliable Persian text collection that was used at Cross Language Evaluation Forum (CLEF) during years 2008 and 2009 for evaluation of Persian information retrieval systems.\n\n\n## NLP in Ukrainian\n\n[Back to Top](#contents)\n\n- [awesome-ukrainian-nlp](https://github.com/asivokon/awesome-ukrainian-nlp) - a curated list of Ukrainian NLP datasets, models, etc.\n- [UkrainianLT](https://github.com/Helsinki-NLP/UkrainianLT) - another curated list with a focus on machine translation and speech processing\n\n\n## NLP in Hungarian\n\n[Back to Top](#contents)\n\n- [awesome-hungarian-nlp](https://github.com/oroszgy/awesome-hungarian-nlp): A curated list of free resources dedicated to Hungarian Natural Language Processing.\n\n## NLP in Portuguese\n\n[Back to Top](#contents)\n\n- [Portuguese-nlp](https://github.com/ajdavidl/Portuguese-NLP) - a List of resources and tools developed with focus on Portuguese.\n\n## Other Languages\n\n- Russian: [pymorphy2](https://github.com/kmike/pymorphy2) - a good pos-tagger for Russian\n- Asian Languages: Thai, Lao, Chinese, Japanese, and Korean [ICU Tokenizer](https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-icu-tokenizer.html) implementation in ElasticSearch\n- Ancient Languages: [CLTK](https://github.com/cltk/cltk): The Classical Language Toolkit is a Python library and collection of texts for doing NLP in ancient languages\n- Hebrew: [NLPH_Resources](https://github.com/NLPH/NLPH_Resources) - A collection of papers, corpora and linguistic resources for NLP in Hebrew\n\n[Back to Top](#contents)\n\n[Credits](./CREDITS.md) for initial curators and sources\n\n## License\n[License](./LICENSE) - CC0\n', '{"language":null,"stars":18022,"forks":2724,"watchers":18022,"open_issues":10,"topics":["awesome","awesome-list","deep-learning","language","machine-learning","natural-language-processing","nlp","text-mining"],"default_branch":"master","size_kb":555,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:sindresorhus:awesome","source_url":"https://github.com/sindresorhus/awesome"},{"type":"has_code","target_id":"github:keonkim:awesome-nlp","source_url":"https://github.com/keonkim/awesome-nlp"},{"type":"has_code","target_id":"github:NirantK:nlp-python-deep-learning","source_url":"http://github.com/NirantK/nlp-python-deep-learning"},{"type":"has_code","target_id":"github:hb20007:hands-on-nltk-tutorial","source_url":"https://github.com/hb20007/hands-on-nltk-tutorial"},{"type":"has_code","target_id":"github:oxford-cs-deepnlp-2017:lectures","source_url":"https://github.com/oxford-cs-deepnlp-2017/lectures"},{"type":"has_code","target_id":"github:yandexdataschool:nlp_course","source_url":"https://github.com/yandexdataschool/nlp_course"},{"type":"has_code","target_id":"github:fastai:course-nlp","source_url":"https://github.com/fastai/course-nlp"},{"type":"has_code","target_id":"github:aws-samples:aws-machine-learning-university-accelerated-nlp","source_url":"https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp"},{"type":"has_code","target_id":"github:Ramaseshanr:anlp","source_url":"https://github.com/Ramaseshanr/anlp"},{"type":"has_code","target_id":"github:jacobeisenstein:gt-nlp-class","source_url":"https://github.com/jacobeisenstein/gt-nlp-class"},{"type":"has_code","target_id":"github:joosthub:PyTorchNLPBook","source_url":"https://github.com/joosthub/PyTorchNLPBook"},{"type":"has_code","target_id":"github:twitter:twitter-text","source_url":"https://github.com/twitter/twitter-text"},{"type":"has_code","target_id":"github:benhmoore:Knwl.js","source_url":"https://github.com/benhmoore/Knwl.js"},{"type":"has_code","target_id":"github:retextjs:retext","source_url":"https://github.com/retextjs/retext"},{"type":"has_code","target_id":"github:spencermountain:compromise","source_url":"https://github.com/spencermountain/compromise"},{"type":"has_code","target_id":"github:NaturalNode:natural","source_url":"https://github.com/NaturalNode/natural"},{"type":"has_code","target_id":"github:synyi:poplar","source_url":"https://github.com/synyi/poplar"},{"type":"has_code","target_id":"github:axa-group:nlp.js","source_url":"https://github.com/axa-group/nlp.js"},{"type":"has_code","target_id":"github:huggingface:node-question-answering","source_url":"https://github.com/huggingface/node-question-answering"},{"type":"has_code","target_id":"github:sloev:sentimental-onix","source_url":"https://github.com/sloev/sentimental-onix"},{"type":"has_code","target_id":"github:QData:TextAttack","source_url":"https://github.com/QData/TextAttack"},{"type":"has_code","target_id":"github:clips:pattern","source_url":"https://github.com/clips/pattern"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:nebuly-ai:nebullvm","source_url":"https://github.com/nebuly-ai/nebullvm"},{"type":"has_code","target_id":"github:chartbeat-labs:textacy","source_url":"https://github.com/chartbeat-labs/textacy"},{"type":"has_code","target_id":"github:JasonKessler:scattertext","source_url":"https://github.com/JasonKessler/scattertext"},{"type":"has_code","target_id":"github:dmlc:gluon-nlp","source_url":"https://github.com/dmlc/gluon-nlp"},{"type":"has_code","target_id":"github:allenai:allennlp","source_url":"https://github.com/allenai/allennlp"},{"type":"has_code","target_id":"github:PetrochukM:PyTorch-NLP","source_url":"https://github.com/PetrochukM/PyTorch-NLP"},{"type":"has_code","target_id":"github:columbia-applied-data-science:rosetta","source_url":"https://github.com/columbia-applied-data-science/rosetta"},{"type":"has_code","target_id":"github:proycon:pynlpl","source_url":"https://github.com/proycon/pynlpl"},{"type":"has_code","target_id":"github:proycon:foliapy","source_url":"https://github.com/proycon/foliapy"},{"type":"has_code","target_id":"github:sergioburdisso:pyss3","source_url":"https://github.com/sergioburdisso/pyss3"},{"type":"has_code","target_id":"github:datquocnguyen:jPTDP","source_url":"https://github.com/datquocnguyen/jPTDP"},{"type":"has_code","target_id":"github:bigartm:bigartm","source_url":"https://github.com/bigartm/bigartm"},{"type":"has_code","target_id":"github:snipsco:snips-nlu","source_url":"https://github.com/snipsco/snips-nlu"},{"type":"has_code","target_id":"github:chakki-works:chazutsu","source_url":"https://github.com/chakki-works/chazutsu"},{"type":"has_code","target_id":"github:gutfeeling:word_forms","source_url":"https://github.com/gutfeeling/word_forms"},{"type":"has_code","target_id":"github:ArtificiAI:Multilingual-Latent-Dirichlet-Allocation-LDA","source_url":"https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA"},{"type":"has_code","target_id":"github:NervanaSystems:nlp-architect","source_url":"https://github.com/NervanaSystems/nlp-architect"},{"type":"has_code","target_id":"github:zalandoresearch:flair","source_url":"https://github.com/zalandoresearch/flair"},{"type":"has_code","target_id":"github:BrikerMan:Kashgari","source_url":"https://github.com/BrikerMan/Kashgari"},{"type":"has_code","target_id":"github:deepset-ai:FARM","source_url":"https://github.com/deepset-ai/FARM"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:zaibacu:rita-dsl","source_url":"https://github.com/zaibacu/rita-dsl"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:tokenizers","source_url":"https://github.com/huggingface/tokenizers"},{"type":"has_code","target_id":"github:pytorch:fairseq","source_url":"https://github.com/pytorch/fairseq"},{"type":"has_code","target_id":"github:gregversteeg:corex_topic","source_url":"https://github.com/gregversteeg/corex_topic"},{"type":"has_code","target_id":"github:awslabs:sockeye","source_url":"https://github.com/awslabs/sockeye"},{"type":"has_code","target_id":"github:xhlulu:dl-translate","source_url":"https://github.com/xhlulu/dl-translate"},{"type":"has_code","target_id":"github:obss:jury","source_url":"https://github.com/obss/jury"},{"type":"has_code","target_id":"github:proycon:python-ucto","source_url":"https://github.com/proycon/python-ucto"},{"type":"has_code","target_id":"github:chncwang:InsNet","source_url":"https://github.com/chncwang/InsNet"},{"type":"has_code","target_id":"github:mit-nlp:MITIE","source_url":"https://github.com/mit-nlp/MITIE"},{"type":"has_code","target_id":"github:BLLIP:bllip-parser","source_url":"https://github.com/BLLIP/bllip-parser"},{"type":"has_code","target_id":"github:proycon:colibri-core","source_url":"https://github.com/proycon/colibri-core"},{"type":"has_code","target_id":"github:LanguageMachines:ucto","source_url":"https://github.com/LanguageMachines/ucto"},{"type":"has_code","target_id":"github:LanguageMachines:libfolia","source_url":"https://github.com/LanguageMachines/libfolia"},{"type":"has_code","target_id":"github:LanguageMachines:frog","source_url":"https://github.com/LanguageMachines/frog"},{"type":"has_code","target_id":"github:meta-toolkit:meta","source_url":"https://github.com/meta-toolkit/meta"},{"type":"has_code","target_id":"github:facebookresearch:StarSpace","source_url":"https://github.com/facebookresearch/StarSpace"},{"type":"has_code","target_id":"github:knowitall:reverb","source_url":"https://github.com/knowitall/reverb"},{"type":"has_code","target_id":"github:knowitall:openregex","source_url":"https://github.com/knowitall/openregex"},{"type":"has_code","target_id":"github:CogComp:cogcomp-nlp","source_url":"https://github.com/CogComp/cogcomp-nlp"},{"type":"has_code","target_id":"github:datquocnguyen:RDRPOSTagger","source_url":"https://github.com/datquocnguyen/RDRPOSTagger"},{"type":"has_code","target_id":"github:pemistahl:lingua","source_url":"https://github.com/pemistahl/lingua"},{"type":"has_code","target_id":"github:meiblorn:kotidgy","source_url":"https://github.com/meiblorn/kotidgy"},{"type":"has_code","target_id":"github:CogComp:saul","source_url":"https://github.com/CogComp/saul"},{"type":"has_code","target_id":"github:ispras:atr4s","source_url":"https://github.com/ispras/atr4s"},{"type":"has_code","target_id":"github:ispras:tm","source_url":"https://github.com/ispras/tm"},{"type":"has_code","target_id":"github:Refefer:word2vec-scala","source_url":"https://github.com/Refefer/word2vec-scala"},{"type":"has_code","target_id":"github:dlwh:epic","source_url":"https://github.com/dlwh/epic"},{"type":"has_code","target_id":"github:JohnSnowLabs:spark-nlp","source_url":"https://github.com/JohnSnowLabs/spark-nlp"},{"type":"has_code","target_id":"github:dselivanov:text2vec","source_url":"https://github.com/dselivanov/text2vec"},{"type":"has_code","target_id":"github:bmschmidt:wordVectors","source_url":"https://github.com/bmschmidt/wordVectors"},{"type":"has_code","target_id":"github:mimno:RMallet","source_url":"https://github.com/mimno/RMallet"},{"type":"has_code","target_id":"github:agoldst:dfr-browser","source_url":"https://github.com/agoldst/dfr-browser"},{"type":"has_code","target_id":"github:agoldst:dfrtopics","source_url":"https://github.com/agoldst/dfrtopics"},{"type":"has_code","target_id":"github:kevincobain2000:sentiment_classifier","source_url":"https://github.com/kevincobain2000/sentiment_classifier"},{"type":"has_code","target_id":"github:kevincobain2000:jProcessing","source_url":"https://github.com/kevincobain2000/jProcessing"},{"type":"has_code","target_id":"github:juliasilge:tidytext","source_url":"https://github.com/juliasilge/tidytext"},{"type":"has_code","target_id":"github:quanteda:spacyr","source_url":"https://github.com/quanteda/spacyr"},{"type":"has_code","target_id":"github:cran-task-views:NaturalLanguageProcessing","source_url":"https://github.com/cran-task-views/NaturalLanguageProcessing"},{"type":"has_code","target_id":"github:dakrone:clojure-opennlp","source_url":"https://github.com/dakrone/clojure-opennlp"},{"type":"has_code","target_id":"github:r0man:inflections-clj","source_url":"https://github.com/r0man/inflections-clj"},{"type":"has_code","target_id":"github:fekr:postagga","source_url":"https://github.com/fekr/postagga"},{"type":"has_code","target_id":"github:diasks2:ruby-nlp","source_url":"https://github.com/diasks2/ruby-nlp"},{"type":"has_code","target_id":"github:arbox:nlp-with-ruby","source_url":"https://github.com/arbox/nlp-with-ruby"},{"type":"has_code","target_id":"github:greyblake:whatlang-rs","source_url":"https://github.com/greyblake/whatlang-rs"},{"type":"has_code","target_id":"github:snipsco:snips-nlu-rs","source_url":"https://github.com/snipsco/snips-nlu-rs"},{"type":"has_code","target_id":"github:guillaume-be:rust-bert","source_url":"https://github.com/guillaume-be/rust-bert"},{"type":"has_code","target_id":"github:VisualText:nlp-engine","source_url":"https://github.com/VisualText/nlp-engine"},{"type":"has_code","target_id":"github:JuliaText:CorpusLoaders.jl","source_url":"https://github.com/JuliaText/CorpusLoaders.jl"},{"type":"has_code","target_id":"github:JuliaText:Languages.jl","source_url":"https://github.com/JuliaText/Languages.jl"},{"type":"has_code","target_id":"github:JuliaText:TextAnalysis.jl","source_url":"https://github.com/JuliaText/TextAnalysis.jl"},{"type":"has_code","target_id":"github:JuliaText:TextModels.jl","source_url":"https://github.com/JuliaText/TextModels.jl"},{"type":"has_code","target_id":"github:JuliaText:WordTokenizers.jl","source_url":"https://github.com/JuliaText/WordTokenizers.jl"},{"type":"has_code","target_id":"github:JuliaText:Word2Vec.jl","source_url":"https://github.com/JuliaText/Word2Vec.jl"},{"type":"has_code","target_id":"github:wit-ai:wit","source_url":"https://github.com/wit-ai/wit"},{"type":"has_code","target_id":"github:watson-developer-cloud:natural-language-understanding-nodejs","source_url":"https://github.com/watson-developer-cloud/natural-language-understanding-nodejs"},{"type":"has_code","target_id":"github:weitechen:anafora","source_url":"https://github.com/weitechen/anafora"},{"type":"has_code","target_id":"github:chakki-works:doccano","source_url":"https://github.com/chakki-works/doccano"},{"type":"has_code","target_id":"github:AI4Bharat:Shoonya-Backend","source_url":"https://github.com/AI4Bharat/Shoonya-Backend"},{"type":"has_code","target_id":"github:proycon:flat","source_url":"https://github.com/proycon/flat"},{"type":"has_code","target_id":"github:facebookresearch:fastText","source_url":"https://github.com/facebookresearch/fastText"},{"type":"has_code","target_id":"github:allenai:allennlp","source_url":"https://github.com/allenai/allennlp"},{"type":"has_code","target_id":"github:allenai:bilm-tf","source_url":"https://github.com/allenai/bilm-tf"},{"type":"has_code","target_id":"github:facebookresearch:DrQA","source_url":"https://github.com/facebookresearch/DrQA"},{"type":"has_code","target_id":"github:allenai:document-qa","source_url":"https://github.com/allenai/document-qa"},{"type":"has_code","target_id":"github:niderhoff:nlp-datasets","source_url":"https://github.com/niderhoff/nlp-datasets"},{"type":"has_code","target_id":"github:RaRe-Technologies:gensim-data","source_url":"https://github.com/RaRe-Technologies/gensim-data"},{"type":"has_code","target_id":"github:vincentkoc:tiny_qa_benchmark_pp","source_url":"https://github.com/vincentkoc/tiny_qa_benchmark_pp"},{"type":"has_code","target_id":"github:ufal:udpipe","source_url":"https://github.com/ufal/udpipe"},{"type":"has_code","target_id":"github:adobe:NLP-Cube","source_url":"https://github.com/adobe/NLP-Cube"},{"type":"has_code","target_id":"github:mikahama:uralicNLP","source_url":"https://github.com/mikahama/uralicNLP"},{"type":"has_code","target_id":"github:e9t:nsmc","source_url":"https://github.com/e9t/nsmc"},{"type":"has_code","target_id":"github:songys:Chatbot_data","source_url":"https://github.com/songys/Chatbot_data"},{"type":"has_code","target_id":"github:akngs:petitions","source_url":"https://github.com/akngs/petitions"},{"type":"has_code","target_id":"github:j-min:korean-parallel-corpora","source_url":"https://github.com/j-min/korean-parallel-corpora"},{"type":"has_code","target_id":"github:01walid:goarabic","source_url":"https://github.com/01walid/goarabic"},{"type":"has_code","target_id":"github:ejtaal:jsastem","source_url":"https://github.com/ejtaal/jsastem"},{"type":"has_code","target_id":"github:amir-zeldes:RFTokenizer","source_url":"https://github.com/amir-zeldes/RFTokenizer"},{"type":"has_code","target_id":"github:hadyelsahar:large-arabic-sentiment-analysis-resouces","source_url":"https://github.com/hadyelsahar/large-arabic-sentiment-analysis-resouces"},{"type":"has_code","target_id":"github:mohamedadaly:labr","source_url":"https://github.com/mohamedadaly/labr"},{"type":"has_code","target_id":"github:mohataher:arabic-stop-words","source_url":"https://github.com/mohataher/arabic-stop-words"},{"type":"has_code","target_id":"github:fxsjy:jieba","source_url":"https://github.com/fxsjy/jieba#jieba-1"},{"type":"has_code","target_id":"github:isnowfy:snownlp","source_url":"https://github.com/isnowfy/snownlp"},{"type":"has_code","target_id":"github:FudanNLP:fnlp","source_url":"https://github.com/FudanNLP/fnlp"},{"type":"has_code","target_id":"github:hankcs:HanLP","source_url":"https://github.com/hankcs/HanLP"},{"type":"has_code","target_id":"github:fighting41love:funNLP","source_url":"https://github.com/fighting41love/funNLP"},{"type":"has_code","target_id":"github:adbar:German-NLP","source_url":"https://github.com/adbar/German-NLP"},{"type":"has_code","target_id":"github:ksopyla:awesome-nlp-polish","source_url":"https://github.com/ksopyla/awesome-nlp-polish"},{"type":"has_code","target_id":"github:jfreddypuentes:spanlp","source_url":"https://github.com/jfreddypuentes/spanlp"},{"type":"has_code","target_id":"github:dav009:LatinamericanTextResources","source_url":"https://github.com/dav009/LatinamericanTextResources"},{"type":"has_code","target_id":"github:crscardellino:sbwce","source_url":"https://github.com/crscardellino/sbwce"},{"type":"has_code","target_id":"github:josecannete:spanish-unannotated-corpora","source_url":"https://github.com/josecannete/spanish-unannotated-corpora"},{"type":"has_code","target_id":"github:dccuchile:spanish-word-embeddings","source_url":"https://github.com/dccuchile/spanish-word-embeddings"},{"type":"has_code","target_id":"github:BotCenter:spanishWordEmbeddings","source_url":"https://github.com/BotCenter/spanishWordEmbeddings"},{"type":"has_code","target_id":"github:BotCenter:spanishSent2Vec","source_url":"https://github.com/BotCenter/spanishSent2Vec"},{"type":"has_code","target_id":"github:dccuchile:beto","source_url":"https://github.com/dccuchile/beto"},{"type":"has_code","target_id":"github:6:stopwords-json","source_url":"https://github.com/6/stopwords-json"},{"type":"has_code","target_id":"github:goru001:nlp-for-hindi","source_url":"https://github.com/goru001/nlp-for-hindi"},{"type":"has_code","target_id":"github:NirantK:hindi2vec","source_url":"https://github.com/NirantK/hindi2vec"},{"type":"has_code","target_id":"github:pnisarg:ABSA","source_url":"https://github.com/pnisarg/ABSA"},{"type":"has_code","target_id":"github:AtikRahman:Bangla_Datasets_ABSA","source_url":"https://github.com/AtikRahman/Bangla_Datasets_ABSA"},{"type":"has_code","target_id":"github:goru001:nlp-for-hindi","source_url":"https://github.com/goru001/nlp-for-hindi"},{"type":"has_code","target_id":"github:Kyubyong:wordvectors","source_url":"https://github.com/Kyubyong/wordvectors"},{"type":"has_code","target_id":"github:HIT-SCIR:ELMoForManyLangs","source_url":"https://github.com/HIT-SCIR/ELMoForManyLangs"},{"type":"has_code","target_id":"github:Saurav0074:mt-dma","source_url":"https://github.com/Saurav0074/mt-dma"},{"type":"has_code","target_id":"github:anoopkunchukuttan:indic_nlp_library","source_url":"https://github.com/anoopkunchukuttan/indic_nlp_library"},{"type":"has_code","target_id":"github:CalmDownKarm:sivareddydependencyparser","source_url":"https://github.com/CalmDownKarm/sivareddydependencyparser"},{"type":"has_code","target_id":"github:goru001:inltk","source_url":"https://github.com/goru001/inltk"},{"type":"has_code","target_id":"github:PyThaiNLP:pythainlp","source_url":"https://github.com/PyThaiNLP/pythainlp"},{"type":"has_code","target_id":"github:wittawatj:jtcc","source_url":"https://github.com/wittawatj/jtcc"},{"type":"has_code","target_id":"github:pucktada:cutkum","source_url":"https://github.com/pucktada/cutkum"},{"type":"has_code","target_id":"github:KenjiroAI:SynThai","source_url":"https://github.com/KenjiroAI/SynThai"},{"type":"has_code","target_id":"github:PyThaiNLP:lexicon-thai","source_url":"https://github.com/PyThaiNLP/lexicon-thai"},{"type":"has_code","target_id":"github:ITUnlp:daner","source_url":"https://github.com/ITUnlp/daner"},{"type":"has_code","target_id":"github:alexandrainst:danlp","source_url":"https://github.com/alexandrainst/danlp"},{"type":"has_code","target_id":"github:fnielsen:awesome-danish","source_url":"https://github.com/fnielsen/awesome-danish"},{"type":"has_code","target_id":"github:undertheseanlp:underthesea","source_url":"https://github.com/undertheseanlp/underthesea"},{"type":"has_code","target_id":"github:phuonglh:vn.vitk","source_url":"https://github.com/phuonglh/vn.vitk"},{"type":"has_code","target_id":"github:vncorenlp:VnCoreNLP","source_url":"https://github.com/vncorenlp/VnCoreNLP"},{"type":"has_code","target_id":"github:VinAIResearch:PhoBERT","source_url":"https://github.com/VinAIResearch/PhoBERT"},{"type":"has_code","target_id":"github:trungtv:pyvi","source_url":"https://github.com/trungtv/pyvi"},{"type":"has_code","target_id":"github:UniversalDependencies:UD_Vietnamese-VTB","source_url":"https://github.com/UniversalDependencies/UD_Vietnamese-VTB"},{"type":"has_code","target_id":"github:VinAIResearch:ViText2SQL","source_url":"https://github.com/VinAIResearch/ViText2SQL"},{"type":"has_code","target_id":"github:qhungngo:EVBCorpus","source_url":"https://github.com/qhungngo/EVBCorpus"},{"type":"has_code","target_id":"github:proycon:python-frog","source_url":"https://github.com/proycon/python-frog"},{"type":"has_code","target_id":"github:rfdj:SimpleNLG-NL","source_url":"https://github.com/rfdj/SimpleNLG-NL"},{"type":"has_code","target_id":"github:rug-compling:alpino","source_url":"https://github.com/rug-compling/alpino"},{"type":"has_code","target_id":"github:opensource-spraakherkenning-nl:Kaldi_NL","source_url":"https://github.com/opensource-spraakherkenning-nl/Kaldi_NL"},{"type":"has_code","target_id":"github:famrashel:idn-tagged-corpus","source_url":"https://github.com/famrashel/idn-tagged-corpus"},{"type":"has_code","target_id":"github:famrashel:idn-treebank","source_url":"https://github.com/famrashel/idn-treebank"},{"type":"has_code","target_id":"github:UniversalDependencies:UD_Indonesian-GSD","source_url":"https://github.com/UniversalDependencies/UD_Indonesian-GSD"},{"type":"has_code","target_id":"github:kata-ai:indosum","source_url":"https://github.com/kata-ai/indosum"},{"type":"has_code","target_id":"github:indobenchmark:indonlu","source_url":"https://github.com/indobenchmark/indonlu"},{"type":"has_code","target_id":"github:kangfend:bahasa","source_url":"https://github.com/kangfend/bahasa"},{"type":"has_code","target_id":"github:galuhsahid:indonesian-word-embedding","source_url":"https://github.com/galuhsahid/indonesian-word-embedding"},{"type":"has_code","target_id":"github:indobenchmark:indonlu","source_url":"https://github.com/indobenchmark/indonlu"},{"type":"has_code","target_id":"github:mirfan899:Urdu","source_url":"https://github.com/mirfan899/Urdu"},{"type":"has_code","target_id":"github:urduhack:urduhack","source_url":"https://github.com/urduhack/urduhack"},{"type":"has_code","target_id":"github:roshan-research:hazm","source_url":"https://github.com/roshan-research/hazm"},{"type":"has_code","target_id":"github:ICTRC:Parsivar","source_url":"https://github.com/ICTRC/Parsivar"},{"type":"has_code","target_id":"github:AlirezaTheH:perke","source_url":"https://github.com/AlirezaTheH/perke"},{"type":"has_code","target_id":"github:jonsafari:perstem","source_url":"https://github.com/jonsafari/perstem"},{"type":"has_code","target_id":"github:NarimanN2:ParsiAnalyzer","source_url":"https://github.com/NarimanN2/ParsiAnalyzer"},{"type":"has_code","target_id":"github:aziz:virastar","source_url":"https://github.com/aziz/virastar"},{"type":"has_code","target_id":"github:HaniehP:PersianNER","source_url":"https://github.com/HaniehP/PersianNER"},{"type":"has_code","target_id":"github:Text-Mining:Persian-NER","source_url":"https://github.com/Text-Mining/Persian-NER"},{"type":"has_code","target_id":"github:Text-Mining:Persian-Wikipedia-Corpus","source_url":"https://github.com/Text-Mining/Persian-Wikipedia-Corpus"},{"type":"has_code","target_id":"github:asivokon:awesome-ukrainian-nlp","source_url":"https://github.com/asivokon/awesome-ukrainian-nlp"},{"type":"has_code","target_id":"github:Helsinki-NLP:UkrainianLT","source_url":"https://github.com/Helsinki-NLP/UkrainianLT"},{"type":"has_code","target_id":"github:oroszgy:awesome-hungarian-nlp","source_url":"https://github.com/oroszgy/awesome-hungarian-nlp"},{"type":"has_code","target_id":"github:ajdavidl:Portuguese-NLP","source_url":"https://github.com/ajdavidl/Portuguese-NLP"},{"type":"has_code","target_id":"github:kmike:pymorphy2","source_url":"https://github.com/kmike/pymorphy2"},{"type":"has_code","target_id":"github:cltk:cltk","source_url":"https://github.com/cltk/cltk"},{"type":"has_code","target_id":"github:NLPH:NLPH_Resources","source_url":"https://github.com/NLPH/NLPH_Resources"}]', NULL, 'CC0-1.0', 'approved', 80, '66f6c4e1c256156496c918dd3e8d25ea', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-keon-awesome-nlp from https://github.com/keon.png
Image converted to WebP: data/images/github-keon-awesome-nlp.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-LightGBM', 'github--microsoft--lightgbm', 'LightGBM', 'microsoft', '<img src=https://github.com/microsoft/LightGBM/blob/master/docs/logo/LightGBM_logo_black_text.svg width=300 /> Light Gradient Boosting Machine =============================== LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages: - Faster training speed and higher efficiency. - Lower memory usage. - Better accuracy. - Support of parallel, distributed, and GPU learning. - Capable of handl...', '["data-mining","decision-trees","distributed","gbdt","gbm","gbrt","gradient-boosting","kaggle","lightgbm","machine-learning","microsoft","parallel","python","r","c++"]', 'other', 17914, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/LightGBM","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<img src=https://github.com/microsoft/LightGBM/blob/master/docs/logo/LightGBM_logo_black_text.svg width=300 />\n\nLight Gradient Boosting Machine\n===============================\n\n[![C++ GitHub Actions Build Status](https://github.com/microsoft/LightGBM/actions/workflows/cpp.yml/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions/workflows/cpp.yml)\n[![Python-package GitHub Actions Build Status](https://github.com/microsoft/LightGBM/actions/workflows/python_package.yml/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions/workflows/python_package.yml)\n[![R-package GitHub Actions Build Status](https://github.com/microsoft/LightGBM/actions/workflows/r_package.yml/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions/workflows/r_package.yml)\n[![CUDA Version GitHub Actions Build Status](https://github.com/microsoft/LightGBM/actions/workflows/cuda.yml/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions/workflows/cuda.yml)\n[![SWIG Wrapper GitHub Actions Build Status](https://github.com/microsoft/LightGBM/actions/workflows/swig.yml/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions/workflows/swig.yml)\n[![Static Analysis GitHub Actions Build Status](https://github.com/microsoft/LightGBM/actions/workflows/static_analysis.yml/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions/workflows/static_analysis.yml)\n[![Azure Pipelines Build Status](https://lightgbm-ci.visualstudio.com/lightgbm-ci/_apis/build/status/Microsoft.LightGBM?branchName=master)](https://lightgbm-ci.visualstudio.com/lightgbm-ci/_build/latest?definitionId=1)\n[![Appveyor Build Status](https://ci.appveyor.com/api/projects/status/1ys5ot401m0fep6l/branch/master?svg=true)](https://ci.appveyor.com/project/guolinke/lightgbm/branch/master)\n[![Documentation Status](https://readthedocs.org/projects/lightgbm/badge/?version=latest)](https://lightgbm.readthedocs.io/)\n[![Link checks](https://github.com/microsoft/LightGBM/actions/workflows/lychee.yml/badge.svg?branch=master)](https://github.com/microsoft/LightGBM/actions/workflows/lychee.yml)\n[![License](https://img.shields.io/github/license/microsoft/lightgbm.svg)](https://github.com/microsoft/LightGBM/blob/master/LICENSE)\n[![EffVer Versioning](https://img.shields.io/badge/version_scheme-EffVer-0097a7)](https://jacobtomlinson.dev/effver)\n[![StackOverflow questions](https://img.shields.io/stackexchange/stackoverflow/t/lightgbm?logo=stackoverflow&logoColor=white&label=StackOverflow%20questions)](https://stackoverflow.com/questions/tagged/lightgbm?sort=votes)\n[![Python Versions](https://img.shields.io/pypi/pyversions/lightgbm.svg?logo=python&logoColor=white)](https://pypi.org/project/lightgbm)\n[![PyPI Version](https://img.shields.io/pypi/v/lightgbm.svg?logo=pypi&logoColor=white)](https://pypi.org/project/lightgbm)\n[![conda Version](https://img.shields.io/conda/vn/conda-forge/lightgbm?logo=conda-forge&logoColor=white&label=conda)](https://anaconda.org/conda-forge/lightgbm)\n[![CRAN Version](https://www.r-pkg.org/badges/version/lightgbm)](https://cran.r-project.org/package=lightgbm)\n[![NuGet Version](https://img.shields.io/nuget/v/lightgbm?logo=nuget&logoColor=white)](https://www.nuget.org/packages/LightGBM)\n\nLightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n- Faster training speed and higher efficiency.\n- Lower memory usage.\n- Better accuracy.\n- Support of parallel, distributed, and GPU learning.\n- Capable of handling large-scale data.\n\nFor further details, please refer to [Features](https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst).\n\nBenefiting from these advantages, LightGBM is being widely-used in many [winning solutions](https://github.com/microsoft/LightGBM/blob/master/examples/README.md#machine-learning-challenge-winning-solutions) of machine learning competitions.\n\n[Comparison experiments](https://github.com/microsoft/LightGBM/blob/master/docs/Experiments.rst#comparison-experiment) on public datasets show that LightGBM can outperform existing boosting frameworks on both efficiency and accuracy, with significantly lower memory consumption. What''s more, [distributed learning experiments](https://github.com/microsoft/LightGBM/blob/master/docs/Experiments.rst#parallel-experiment) show that LightGBM can achieve a linear speed-up by using multiple machines for training in specific settings.\n\nGet Started and Documentation\n-----------------------------\n\nOur primary documentation is at https://lightgbm.readthedocs.io/ and is generated from this repository. If you are new to LightGBM, follow [the installation instructions](https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html) on that site.\n\nNext you may want to read:\n\n- [**Examples**](https://github.com/microsoft/LightGBM/tree/master/examples) showing command line usage of common tasks.\n- [**Features**](https://github.com/microsoft/LightGBM/blob/master/docs/Features.rst) and algorithms supported by LightGBM.\n- [**Parameters**](https://github.com/microsoft/LightGBM/blob/master/docs/Parameters.rst) is an exhaustive list of customization you can make.\n- [**Distributed Learning**](https://github.com/microsoft/LightGBM/blob/master/docs/Parallel-Learning-Guide.rst) and [**GPU Learning**](https://github.com/microsoft/LightGBM/blob/master/docs/GPU-Tutorial.rst) can speed up computation.\n- [**FLAML**](https://www.microsoft.com/en-us/research/project/fast-and-lightweight-automl-for-large-scale-data/articles/flaml-a-fast-and-lightweight-automl-library/) provides automated tuning for LightGBM ([code examples](https://microsoft.github.io/FLAML/docs/Examples/AutoML-for-LightGBM/)).\n- [**Optuna Hyperparameter Tuner**](https://medium.com/optuna/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258) provides automated tuning for LightGBM hyperparameters ([code examples](https://github.com/optuna/optuna-examples/blob/main/lightgbm/lightgbm_tuner_simple.py)).\n- [**Understanding LightGBM Parameters (and How to Tune Them using Neptune)**](https://neptune.ai/blog/lightgbm-parameters-guide).\n\nDocumentation for contributors:\n\n- [**How we update readthedocs.io**](https://github.com/microsoft/LightGBM/blob/master/docs/README.rst).\n- Check out the [**Development Guide**](https://github.com/microsoft/LightGBM/blob/master/docs/Development-Guide.rst).\n\nNews\n----\n\nPlease refer to changelogs at [GitHub releases](https://github.com/microsoft/LightGBM/releases) page.\n\nExternal (Unofficial) Repositories\n----------------------------------\n\nProjects listed here offer alternative ways to use LightGBM.\nThey are not maintained or officially endorsed by the `LightGBM` development team.\n\nJPMML (Java PMML converter): https://github.com/jpmml/jpmml-lightgbm\n\nNyoka (Python PMML converter): https://github.com/SoftwareAG/nyoka\n\nTreelite (model compiler for efficient deployment): https://github.com/dmlc/treelite\n\nlleaves (LLVM-based model compiler for efficient inference): https://github.com/siboehm/lleaves\n\nHummingbird (model compiler into tensor computations): https://github.com/microsoft/hummingbird\n\nGBNet (use `LightGBM` as a [PyTorch Module](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html)): https://github.com/mthorrell/gbnet\n\ncuML Forest Inference Library (GPU-accelerated inference): https://github.com/rapidsai/cuml\n\ndaal4py (Intel CPU-accelerated inference): https://github.com/intel/scikit-learn-intelex/tree/master/daal4py\n\nm2cgen (model appliers for various languages): https://github.com/BayesWitnesses/m2cgen\n\nleaves (Go model applier): https://github.com/dmitryikh/leaves\n\nONNXMLTools (ONNX converter): https://github.com/onnx/onnxmltools\n\nSHAP (model output explainer): https://github.com/slundberg/shap\n\nShapash (model visualization and interpretation): https://github.com/MAIF/shapash\n\ndtreeviz (decision tree visualization and model interpretation): https://github.com/parrt/dtreeviz\n\nsupertree (interactive visualization of decision trees): https://github.com/mljar/supertree\n\nSynapseML (LightGBM on Spark): https://github.com/microsoft/SynapseML\n\nKubeflow Fairing (LightGBM on Kubernetes): https://github.com/kubeflow/fairing\n\nKubeflow Operator (LightGBM on Kubernetes): https://github.com/kubeflow/xgboost-operator\n\nlightgbm_ray (LightGBM on Ray): https://github.com/ray-project/lightgbm_ray\n\nRay (distributed computing framework): https://github.com/ray-project/ray\n\nMars (LightGBM on Mars): https://github.com/mars-project/mars\n\nML.NET (.NET/C#-package): https://github.com/dotnet/machinelearning\n\nLightGBM.NET (.NET/C#-package): https://github.com/rca22/LightGBM.Net\n\nLightGBM Ruby (Ruby gem): https://github.com/ankane/lightgbm-ruby\n\nLightGBM4j (Java high-level binding): https://github.com/metarank/lightgbm4j\n\nLightGBM4J (JVM interface for LightGBM written in Scala): https://github.com/seek-oss/lightgbm4j\n\nJulia-package: https://github.com/IQVIA-ML/LightGBM.jl\n\nlightgbm3 (Rust binding): https://github.com/Mottl/lightgbm3-rs\n\nMLServer (inference server for LightGBM): https://github.com/SeldonIO/MLServer\n\nMLflow (experiment tracking, model monitoring framework): https://github.com/mlflow/mlflow\n\nFLAML (AutoML library for hyperparameter optimization): https://github.com/microsoft/FLAML\n\nMLJAR AutoML (AutoML on tabular data): https://github.com/mljar/mljar-supervised\n\nOptuna (hyperparameter optimization framework): https://github.com/optuna/optuna\n\nLightGBMLSS (probabilistic modelling with LightGBM): https://github.com/StatMixedML/LightGBMLSS\n\nmlforecast (time series forecasting with LightGBM): https://github.com/Nixtla/mlforecast\n\nskforecast (time series forecasting with LightGBM): https://github.com/JoaquinAmatRodrigo/skforecast\n\n`{bonsai}` (R `{parsnip}`-compliant interface): https://github.com/tidymodels/bonsai\n\n`{mlr3extralearners}` (R `{mlr3}`-compliant interface): https://github.com/mlr-org/mlr3extralearners\n\nlightgbm-transform (feature transformation binding): https://github.com/microsoft/lightgbm-transform\n\n`postgresml` (LightGBM training and prediction in SQL, via a Postgres extension): https://github.com/postgresml/postgresml\n\n`pyodide` (run `lightgbm` Python-package in a web browser): https://github.com/pyodide/pyodide\n\n`vaex-ml` (Python DataFrame library with its own interface to LightGBM): https://github.com/vaexio/vaex\n\nSupport\n-------\n\n- Ask a question [on Stack Overflow with the `lightgbm` tag](https://stackoverflow.com/questions/ask?tags=lightgbm), we monitor this for new questions.\n- Open **bug reports** and **feature requests** on [GitHub issues](https://github.com/microsoft/LightGBM/issues).\n\nHow to Contribute\n-----------------\n\nCheck [CONTRIBUTING](https://github.com/microsoft/LightGBM/blob/master/CONTRIBUTING.md) page.\n\nMicrosoft Open Source Code of Conduct\n-------------------------------------\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\nReference Papers\n----------------\n\nYu Shi, Guolin Ke, Zhuoming Chen, Shuxin Zheng, Tie-Yan Liu. "Quantized Training of Gradient Boosting Decision Trees" ([link](https://proceedings.neurips.cc/paper/2022/hash/77911ed9e6e864ca1a3d165b2c3cb258-Abstract.html)). Advances in Neural Information Processing Systems 35 (NeurIPS 2022), pp. 18822-18833.\n\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu. "[LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://proceedings.neurips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html)". Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 3149-3157.\n\nQi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhi-Ming Ma, Tie-Yan Liu. "[A Communication-Efficient Parallel Algorithm for Decision Tree](https://proceedings.neurips.cc/paper/2016/hash/10a5ab2db37feedfdeaab192ead4ac0e-Abstract.html)". Advances in Neural Information Processing Systems 29 (NIPS 2016), pp. 1279-1287.\n\nHuan Zhang, Si Si and Cho-Jui Hsieh. "[GPU Acceleration for Large-scale Tree Boosting](https://arxiv.org/abs/1706.08359)". SysML Conference, 2018.\n\nLicense\n-------\n\nThis project is licensed under the terms of the MIT license. See [LICENSE](https://github.com/microsoft/LightGBM/blob/master/LICENSE) for additional details.\n', '{"language":"C++","stars":17914,"forks":3965,"watchers":17914,"open_issues":471,"topics":["data-mining","decision-trees","distributed","gbdt","gbm","gbrt","gradient-boosting","kaggle","lightgbm","machine-learning","microsoft","parallel","python","r"],"default_branch":"master","size_kb":24815,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:optuna:optuna-examples","source_url":"https://github.com/optuna/optuna-examples"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:jpmml:jpmml-lightgbm","source_url":"https://github.com/jpmml/jpmml-lightgbm"},{"type":"has_code","target_id":"github:SoftwareAG:nyoka","source_url":"https://github.com/SoftwareAG/nyoka"},{"type":"has_code","target_id":"github:dmlc:treelite","source_url":"https://github.com/dmlc/treelite"},{"type":"has_code","target_id":"github:siboehm:lleaves","source_url":"https://github.com/siboehm/lleaves"},{"type":"has_code","target_id":"github:microsoft:hummingbird","source_url":"https://github.com/microsoft/hummingbird"},{"type":"has_code","target_id":"github:mthorrell:gbnet","source_url":"https://github.com/mthorrell/gbnet"},{"type":"has_code","target_id":"github:rapidsai:cuml","source_url":"https://github.com/rapidsai/cuml"},{"type":"has_code","target_id":"github:intel:scikit-learn-intelex","source_url":"https://github.com/intel/scikit-learn-intelex"},{"type":"has_code","target_id":"github:BayesWitnesses:m2cgen","source_url":"https://github.com/BayesWitnesses/m2cgen"},{"type":"has_code","target_id":"github:dmitryikh:leaves","source_url":"https://github.com/dmitryikh/leaves"},{"type":"has_code","target_id":"github:onnx:onnxmltools","source_url":"https://github.com/onnx/onnxmltools"},{"type":"has_code","target_id":"github:slundberg:shap","source_url":"https://github.com/slundberg/shap"},{"type":"has_code","target_id":"github:MAIF:shapash","source_url":"https://github.com/MAIF/shapash"},{"type":"has_code","target_id":"github:parrt:dtreeviz","source_url":"https://github.com/parrt/dtreeviz"},{"type":"has_code","target_id":"github:mljar:supertree","source_url":"https://github.com/mljar/supertree"},{"type":"has_code","target_id":"github:microsoft:SynapseML","source_url":"https://github.com/microsoft/SynapseML"},{"type":"has_code","target_id":"github:kubeflow:fairing","source_url":"https://github.com/kubeflow/fairing"},{"type":"has_code","target_id":"github:kubeflow:xgboost-operator","source_url":"https://github.com/kubeflow/xgboost-operator"},{"type":"has_code","target_id":"github:ray-project:lightgbm_ray","source_url":"https://github.com/ray-project/lightgbm_ray"},{"type":"has_code","target_id":"github:ray-project:ray","source_url":"https://github.com/ray-project/ray"},{"type":"has_code","target_id":"github:mars-project:mars","source_url":"https://github.com/mars-project/mars"},{"type":"has_code","target_id":"github:dotnet:machinelearning","source_url":"https://github.com/dotnet/machinelearning"},{"type":"has_code","target_id":"github:rca22:LightGBM.Net","source_url":"https://github.com/rca22/LightGBM.Net"},{"type":"has_code","target_id":"github:ankane:lightgbm-ruby","source_url":"https://github.com/ankane/lightgbm-ruby"},{"type":"has_code","target_id":"github:metarank:lightgbm4j","source_url":"https://github.com/metarank/lightgbm4j"},{"type":"has_code","target_id":"github:seek-oss:lightgbm4j","source_url":"https://github.com/seek-oss/lightgbm4j"},{"type":"has_code","target_id":"github:IQVIA-ML:LightGBM.jl","source_url":"https://github.com/IQVIA-ML/LightGBM.jl"},{"type":"has_code","target_id":"github:Mottl:lightgbm3-rs","source_url":"https://github.com/Mottl/lightgbm3-rs"},{"type":"has_code","target_id":"github:SeldonIO:MLServer","source_url":"https://github.com/SeldonIO/MLServer"},{"type":"has_code","target_id":"github:mlflow:mlflow","source_url":"https://github.com/mlflow/mlflow"},{"type":"has_code","target_id":"github:microsoft:FLAML","source_url":"https://github.com/microsoft/FLAML"},{"type":"has_code","target_id":"github:mljar:mljar-supervised","source_url":"https://github.com/mljar/mljar-supervised"},{"type":"has_code","target_id":"github:optuna:optuna","source_url":"https://github.com/optuna/optuna"},{"type":"has_code","target_id":"github:StatMixedML:LightGBMLSS","source_url":"https://github.com/StatMixedML/LightGBMLSS"},{"type":"has_code","target_id":"github:Nixtla:mlforecast","source_url":"https://github.com/Nixtla/mlforecast"},{"type":"has_code","target_id":"github:JoaquinAmatRodrigo:skforecast","source_url":"https://github.com/JoaquinAmatRodrigo/skforecast"},{"type":"has_code","target_id":"github:tidymodels:bonsai","source_url":"https://github.com/tidymodels/bonsai"},{"type":"has_code","target_id":"github:mlr-org:mlr3extralearners","source_url":"https://github.com/mlr-org/mlr3extralearners"},{"type":"has_code","target_id":"github:microsoft:lightgbm-transform","source_url":"https://github.com/microsoft/lightgbm-transform"},{"type":"has_code","target_id":"github:postgresml:postgresml","source_url":"https://github.com/postgresml/postgresml"},{"type":"has_code","target_id":"github:pyodide:pyodide","source_url":"https://github.com/pyodide/pyodide"},{"type":"has_code","target_id":"github:vaexio:vaex","source_url":"https://github.com/vaexio/vaex"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"},{"type":"has_code","target_id":"github:microsoft:LightGBM","source_url":"https://github.com/microsoft/LightGBM"}]', NULL, 'MIT', 'approved', 80, 'd94c4aee547622b9de8eac8d47e8a208', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-LightGBM from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-LightGBM.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-marimo-team-marimo', 'github--marimo-team--marimo', 'marimo', 'marimo-team', '<p align="center"> <img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-thick.svg"> </p> <p align="center"> <em>A reactive Python notebook that''s reproducible, git-friendly, and deployable as scripts or apps.</em> </p> <p align="center"> <a href="https://docs.marimo.io" target="_blank"><strong>Docs</strong></a> Â· <a href="https://marimo.io/discord?ref=readme" target="_blank"><strong>Discord</strong></a> Â· <a href="https://docs.marimo.io/examples/" t...', '["artificial-intelligence","dag","data-science","data-visualization","dataflow","developer-tools","machine-learning","notebooks","pipeline","python","reactive","sql","web-app","python"]', 'other', 17676, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/marimo-team/marimo","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-thick.svg">\n</p>\n\n<p align="center">\n  <em>A reactive Python notebook that''s reproducible, git-friendly, and deployable as scripts or apps.</em>\n</p>\n\n<p align="center">\n  <a href="https://docs.marimo.io" target="_blank"><strong>Docs</strong></a> Â·\n  <a href="https://marimo.io/discord?ref=readme" target="_blank"><strong>Discord</strong></a> Â·\n  <a href="https://docs.marimo.io/examples/" target="_blank"><strong>Examples</strong></a> Â·\n  <a href="https://marimo.io/gallery/" target="_blank"><strong>Gallery</strong></a> Â·\n  <a href="https://www.youtube.com/@marimo-team/" target="_blank"><strong>YouTube</strong></a>\n</p>\n\n<p align="center">\n  <b>English</b>\n  <b> | </b>\n  <a href="https://github.com/marimo-team/marimo/blob/main/README_Traditional_Chinese.md" target="_blank"><b>ç¹é«”ä¸­æ–‡</b></a>\n  <b> | </b>\n  <a href="https://github.com/marimo-team/marimo/blob/main/README_Chinese.md" target="_blank"><b>ç®€ä½“ä¸­æ–‡</b></a>\n  <b> | </b>\n  <a href="https://github.com/marimo-team/marimo/blob/main/README_Japanese.md" target="_blank"><b>æ—¥æœ¬èª</b></a>\n  <b> | </b>\n  <a href="https://github.com/marimo-team/marimo/blob/main/README_Spanish.md" target="_blank"><b>EspaÃ±ol</b></a>\n</p>\n\n<p align="center">\n  <a href="https://pypi.org/project/marimo/"><img src="https://img.shields.io/pypi/v/marimo?color=%2334D058&label=pypi"/></a>\n  <a href="https://anaconda.org/conda-forge/marimo"><img src="https://img.shields.io/conda/vn/conda-forge/marimo.svg"/></a>\n  <a href="https://marimo.io/discord?ref=readme"><img src="https://shields.io/discord/1059888774789730424" alt="discord"/></a>\n  <img alt="Pepy Total Downloads" src="https://img.shields.io/pepy/dt/marimo?label=pypi%20%7C%20downloads"/>\n  <img alt="Conda Downloads" src="https://img.shields.io/conda/d/conda-forge/marimo"/>\n  <a href="https://github.com/marimo-team/marimo/blob/main/LICENSE"><img src="https://img.shields.io/pypi/l/marimo"/></a>\n</p>\n\n**marimo** is a reactive Python notebook: run a cell or interact with a UI\nelement, and marimo automatically runs dependent cells (or <a href="#expensive-notebooks">marks them as stale</a>), keeping code and outputs\nconsistent. marimo notebooks are stored as pure Python (with first-class SQL support), executable as scripts,\nand deployable as apps.\n\n**Highlights**.\n\n- ğŸš€ **batteries-included:** replaces `jupyter`, `streamlit`, `jupytext`, `ipywidgets`, `papermill`, and more\n- âš¡ï¸ **reactive**: run a cell, and marimo reactively [runs all dependent cells](https://docs.marimo.io/guides/reactivity.html) or <a href="#expensive-notebooks">marks them as stale</a>\n- ğŸ–ï¸ **interactive:** [bind sliders, tables, plots, and more](https://docs.marimo.io/guides/interactivity.html) to Python â€” no callbacks required\n- ğŸ **git-friendly:** stored as `.py` files\n- ğŸ›¢ï¸ **designed for data**: query dataframes, databases, warehouses, or lakehouses [with SQL](https://docs.marimo.io/guides/working_with_data/sql.html), filter and search [dataframes](https://docs.marimo.io/guides/working_with_data/dataframes.html)\n- ğŸ¤– **AI-native**: [generate cells with AI](https://docs.marimo.io/guides/generate_with_ai/) tailored for data work\n- ğŸ”¬ **reproducible:** [no hidden state](https://docs.marimo.io/guides/reactivity.html#no-hidden-state), deterministic execution, [built-in package management](https://docs.marimo.io/guides/package_management/)\n- ğŸƒ **executable:** [execute as a Python script](https://docs.marimo.io/guides/scripts.html), parameterized by CLI args\n- ğŸ›œ **shareable**: [deploy as an interactive web app](https://docs.marimo.io/guides/apps.html) or [slides](https://docs.marimo.io/guides/apps.html#slides-layout), [run in the browser via WASM](https://docs.marimo.io/guides/wasm.html)\n- ğŸ§© **reusable:** [import functions and classes](https://docs.marimo.io/guides/reusing_functions/) from one notebook to another\n- ğŸ§ª **testable:** [run pytest](https://docs.marimo.io/guides/testing/) on notebooks\n- âŒ¨ï¸ **a modern editor**: [GitHub Copilot](https://docs.marimo.io/guides/editor_features/ai_completion.html#github-copilot), [AI assistants](https://docs.marimo.io/guides/editor_features/ai_completion.html), vim keybindings, variable explorer, and [more](https://docs.marimo.io/guides/editor_features/index.html)\n- ğŸ§‘â€ğŸ’» **use your favorite editor**: run in [VS Code or Cursor](https://marketplace.visualstudio.com/items?itemName=marimo-team.vscode-marimo), or edit in neovim, Zed, [or any other text editor](https://docs.marimo.io/guides/editor_features/watching/)\n\n```python\npip install marimo && marimo tutorial intro\n```\n\n_Try marimo at [our online playground](https://marimo.app/l/c7h6pz), which runs entirely in the browser!_\n\n_Jump to the [quickstart](#quickstart) for a primer on our CLI._\n\n## A reactive programming environment\n\nmarimo guarantees your notebook code, outputs, and program state are consistent. This [solves many problems](https://docs.marimo.io/faq.html#faq-problems) associated with traditional notebooks like Jupyter.\n\n**A reactive programming environment.**\nRun a cell and marimo _reacts_ by automatically running the cells that\nreference its variables, eliminating the error-prone task of manually\nre-running cells. Delete a cell and marimo scrubs its variables from program\nmemory, eliminating hidden state.\n\n<img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/reactive.gif" width="700px" />\n\n<a name="expensive-notebooks"></a>\n\n**Compatible with expensive notebooks.** marimo lets you [configure the runtime\nto be\nlazy](https://docs.marimo.io/guides/configuration/runtime_configuration.html),\nmarking affected cells as stale instead of automatically running them. This\ngives you guarantees on program state while preventing accidental execution of\nexpensive cells.\n\n**Synchronized UI elements.** Interact with [UI\nelements](https://docs.marimo.io/guides/interactivity.html) like [sliders](https://docs.marimo.io/api/inputs/slider.html#slider),\n[dropdowns](https://docs.marimo.io/api/inputs/dropdown.html), [dataframe\ntransformers](https://docs.marimo.io/api/inputs/dataframe.html), and [chat\ninterfaces](https://docs.marimo.io/api/inputs/chat.html), and the cells that\nuse them are automatically re-run with their latest values.\n\n<img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-ui.gif" width="700px" />\n\n**Interactive dataframes.** [Page through, search, filter, and\nsort](https://docs.marimo.io/guides/working_with_data/dataframes.html)\nmillions of rows blazingly fast, no code required.\n\n<img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-df.gif" width="700px" />\n\n**Generate cells with data-aware AI.** [Generate code with an AI\nassistant](https://docs.marimo.io/guides/editor_features/ai_completion/) that is highly\nspecialized for working with data, with context about your variables in memory;\n[zero-shot entire notebooks](https://docs.marimo.io/guides/generate_with_ai/text_to_notebook/).\nCustomize the system prompt, bring your own API keys, or use local models.\n\n<img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-generate-with-ai.gif" width="700px" />\n\n**Query data with SQL.** Build [SQL](https://docs.marimo.io/guides/working_with_data/sql.html) queries\nthat depend on Python values and execute them against dataframes, databases, lakehouses,\nCSVs, Google Sheets, or anything else using our built-in SQL engine, which\nreturns the result as a Python dataframe.\n\n<img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-sql-cell.png" width="700px" />\n\nYour notebooks are still pure Python, even if they use SQL.\n\n**Dynamic markdown.** Use markdown parametrized by Python variables to tell\ndynamic stories that depend on Python data.\n\n**Built-in package management.** marimo has built-in support for all major\npackage managers, letting you [install packages on import](https://docs.marimo.io/guides/editor_features/package_management.html). marimo can even\n[serialize package\nrequirements](https://docs.marimo.io/guides/package_management/inlining_dependencies/)\nin notebook files, and auto install them in\nisolated venv sandboxes.\n\n**Deterministic execution order.** Notebooks are executed in a deterministic\norder, based on variable references instead of cells'' positions on the page.\nOrganize your notebooks to best fit the stories you''d like to tell.\n\n**Performant runtime.** marimo runs only those cells that need to be run by\nstatically analyzing your code.\n\n**Batteries-included.** marimo comes with GitHub Copilot, AI assistants, Ruff\ncode formatting, HTML export, fast code completion, a [VS Code\nextension](https://marketplace.visualstudio.com/items?itemName=marimo-team.vscode-marimo),\nan interactive dataframe viewer, and [many more](https://docs.marimo.io/guides/editor_features/index.html)\nquality-of-life features.\n\n## Quickstart\n\n_The [marimo concepts\nplaylist](https://www.youtube.com/watch?v=3N6lInzq5MI&list=PLNJXGo8e1XT9jP7gPbRdm1XwloZVFvLEq)\non our [YouTube channel](https://www.youtube.com/@marimo-team) gives an\noverview of many features._\n\n**Installation.** In a terminal, run\n\n```bash\npip install marimo  # or conda install -c conda-forge marimo\nmarimo tutorial intro\n```\n\nTo install with additional dependencies that unlock SQL cells, AI completion, and more,\nrun\n\n```bash\npip install "marimo[recommended]"\n```\n\n**Create notebooks.**\n\nCreate or edit notebooks with\n\n```bash\nmarimo edit\n```\n\n**Run apps.** Run your notebook as a web app, with Python\ncode hidden and uneditable:\n\n```bash\nmarimo run your_notebook.py\n```\n\n<img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-model-comparison.gif" style="border-radius: 8px" width="450px" />\n\n**Execute as scripts.** Execute a notebook as a script at the\ncommand line:\n\n```bash\npython your_notebook.py\n```\n\n**Automatically convert Jupyter notebooks.** Automatically convert Jupyter\nnotebooks to marimo notebooks with the CLI\n\n```bash\nmarimo convert your_notebook.ipynb > your_notebook.py\n```\n\nor use our [web interface](https://marimo.io/convert).\n\n**Tutorials.**\nList all tutorials:\n\n```bash\nmarimo tutorial --help\n```\n\n**Share cloud-based notebooks.** Use\n[molab](https://molab.marimo.io/notebooks), a cloud-based marimo notebook\nservice similar to Google Colab, to create and share notebook links.\n\n## Questions?\n\nSee the [FAQ](https://docs.marimo.io/faq.html) at our docs.\n\n## Learn more\n\nmarimo is easy to get started with, with lots of room for power users.\nFor example, here''s an embedding visualizer made in marimo\n([video](https://marimo.io/videos/landing/full.mp4)):\n\n<img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/embedding.gif" width="700px" />\n\nCheck out our [docs](https://docs.marimo.io),\n[usage examples](https://docs.marimo.io/examples/), and our [gallery](https://marimo.io/gallery) to learn more.\n\n<table border="0">\n  <tr>\n    <td>\n      <a target="_blank" href="https://docs.marimo.io/getting_started/key_concepts.html">\n        <img src="https://docs.marimo.io/_static/reactive.gif" style="max-height: 150px; width: auto; display: block" />\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.marimo.io/api/inputs/index.html">\n        <img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/readme-ui.gif" style="max-height: 150px; width: auto; display: block" />\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.marimo.io/guides/working_with_data/plotting.html">\n        <img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/docs-intro.gif" style="max-height: 150px; width: auto; display: block" />\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.marimo.io/api/layouts/index.html">\n        <img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/outputs.gif" style="max-height: 150px; width: auto; display: block" />\n      </a>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <a target="_blank" href="https://docs.marimo.io/getting_started/key_concepts.html"> Tutorial </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.marimo.io/api/inputs/index.html"> Inputs </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.marimo.io/guides/working_with_data/plotting.html"> Plots </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.marimo.io/api/layouts/index.html"> Layout </a>\n    </td>\n  </tr>\n  <tr>\n    <td>\n      <a target="_blank" href="https://marimo.app/l/c7h6pz">\n        <img src="https://marimo.io/shield.svg"/>\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://marimo.app/l/0ue871">\n        <img src="https://marimo.io/shield.svg"/>\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://marimo.app/l/lxp1jk">\n        <img src="https://marimo.io/shield.svg"/>\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://marimo.app/l/14ovyr">\n        <img src="https://marimo.io/shield.svg"/>\n      </a>\n    </td>\n  </tr>\n</table>\n\n## Contributing\n\nWe appreciate all contributions! You don''t need to be an expert to help out.\nPlease see [CONTRIBUTING.md](https://github.com/marimo-team/marimo/blob/main/CONTRIBUTING.md) for more details on how to get\nstarted.\n\n> Questions? Reach out to us [on Discord](https://marimo.io/discord?ref=readme).\n\n## Community\n\nWe''re building a community. Come hang out with us!\n\n- ğŸŒŸ [Star us on GitHub](https://github.com/marimo-team/marimo)\n- ğŸ’¬ [Chat with us on Discord](https://marimo.io/discord?ref=readme)\n- ğŸ“§ [Subscribe to our Newsletter](https://marimo.io/newsletter)\n- â˜ï¸ [Join our Cloud Waitlist](https://marimo.io/cloud)\n- âœï¸ [Start a GitHub Discussion](https://github.com/marimo-team/marimo/discussions)\n- ğŸ¦‹ [Follow us on Bluesky](https://bsky.app/profile/marimo.io)\n- ğŸ¦ [Follow us on Twitter](https://twitter.com/marimo_io)\n- ğŸ¥ [Subscribe on YouTube](https://www.youtube.com/@marimo-team)\n- ğŸ•´ï¸ [Follow us on LinkedIn](https://www.linkedin.com/company/marimo-io)\n\n**A NumFOCUS affiliated project.** marimo is a core part of the broader Python\necosystem and is a member of the NumFOCUS community, which includes projects\nsuch as NumPy, SciPy, and Matplotlib.\n\n<img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/numfocus_affiliated_project.png" height="40px" />\n\n\n## Inspiration âœ¨\n\nmarimo is a **reinvention** of the Python notebook as a reproducible, interactive,\nand shareable Python program, instead of an error-prone JSON scratchpad.\n\nWe believe that the tools we use shape the way we think â€” better tools, for\nbetter minds. With marimo, we hope to provide the Python community with a\nbetter programming environment to do research and communicate it; to experiment\nwith code and share it; to learn computational science and teach it.\n\nOur inspiration comes from many places and projects, especially\n[Pluto.jl](https://github.com/fonsp/Pluto.jl),\n[ObservableHQ](https://observablehq.com/tutorials), and\n[Bret Victor''s essays](http://worrydream.com/). marimo is part of\na greater movement toward reactive dataflow programming. From\n[IPyflow](https://github.com/ipyflow/ipyflow), [streamlit](https://github.com/streamlit/streamlit),\n[TensorFlow](https://github.com/tensorflow/tensorflow),\n[PyTorch](https://github.com/pytorch/pytorch/tree/main),\n[JAX](https://github.com/google/jax), and\n[React](https://github.com/facebook/react), the ideas of functional,\ndeclarative, and reactive programming are transforming a broad range of tools\nfor the better.\n\n<p align="right">\n  <img src="https://raw.githubusercontent.com/marimo-team/marimo/main/docs/_static/marimo-logotype-horizontal.png" height="200px">\n</p>\n', '{"language":"Python","stars":17676,"forks":822,"watchers":17676,"open_issues":526,"topics":["artificial-intelligence","dag","data-science","data-visualization","dataflow","developer-tools","machine-learning","notebooks","pipeline","python","reactive","sql","web-app"],"default_branch":"main","size_kb":187194,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:marimo-team:marimo","source_url":"https://github.com/marimo-team/marimo"},{"type":"has_code","target_id":"github:fonsp:Pluto.jl","source_url":"https://github.com/fonsp/Pluto.jl"},{"type":"has_code","target_id":"github:ipyflow:ipyflow","source_url":"https://github.com/ipyflow/ipyflow"},{"type":"has_code","target_id":"github:streamlit:streamlit","source_url":"https://github.com/streamlit/streamlit"},{"type":"has_code","target_id":"github:tensorflow:tensorflow","source_url":"https://github.com/tensorflow/tensorflow"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:google:jax","source_url":"https://github.com/google/jax"},{"type":"has_code","target_id":"github:facebook:react","source_url":"https://github.com/facebook/react"}]', NULL, 'Apache-2.0', 'approved', 80, 'fbe56949c1c26aa89c02a9efedb030d1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-marimo-team-marimo from https://github.com/marimo-team.png
Image converted to WebP: data/images/github-marimo-team-marimo.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-AUTOMATIC1111-stable-diffusion-webui', 'github--automatic1111--stable-diffusion-webui', 'stable-diffusion-webui', 'AUTOMATIC1111', 'A web interface for Stable Diffusion, implemented using Gradio library. Detailed feature showcase with images: - Original txt2img and img2img modes - One click install and run script (but you still must install python and git) - Outpainting - Inpainting - Color Sketch - Prompt Matrix - Stable Diffusion Upscale - Attention, specify parts of text that the model should pay more attention to - a man in a - will pay more attention to tuxedo - a man in a - alternative syntax - select text and press...', '["ai","ai-art","deep-learning","diffusion","gradio","image-generation","image2image","img2img","pytorch","stable-diffusion","text2image","torch","txt2img","unstable","upscaling","web","python"]', 'other', 158820, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Stable Diffusion web UI\nA web interface for Stable Diffusion, implemented using Gradio library.\n\n![](screenshot.png)\n\n## Features\n[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):\n- Original txt2img and img2img modes\n- One click install and run script (but you still must install python and git)\n- Outpainting\n- Inpainting\n- Color Sketch\n- Prompt Matrix\n- Stable Diffusion Upscale\n- Attention, specify parts of text that the model should pay more attention to\n    - a man in a `((tuxedo))` - will pay more attention to tuxedo\n    - a man in a `(tuxedo:1.21)` - alternative syntax\n    - select text and press `Ctrl+Up` or `Ctrl+Down` (or `Command+Up` or `Command+Down` if you''re on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)\n- Loopback, run img2img processing multiple times\n- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters\n- Textual Inversion\n    - have as many embeddings as you want and use any names you like for them\n    - use multiple embeddings with different numbers of vectors per token\n    - works with half precision floating point numbers\n    - train embeddings on 8GB (also reports of 6GB working)\n- Extras tab with:\n    - GFPGAN, neural network that fixes faces\n    - CodeFormer, face restoration tool as an alternative to GFPGAN\n    - RealESRGAN, neural network upscaler\n    - ESRGAN, neural network upscaler with a lot of third party models\n    - SwinIR and Swin2SR ([see here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092)), neural network upscalers\n    - LDSR, Latent diffusion super resolution upscaling\n- Resizing aspect ratio options\n- Sampling method selection\n    - Adjust sampler eta values (noise multiplier)\n    - More advanced noise setting options\n- Interrupt processing at any time\n- 4GB video card support (also reports of 2GB working)\n- Correct seeds for batches\n- Live prompt token length validation\n- Generation parameters\n     - parameters you used to generate images are saved with that image\n     - in PNG chunks for PNG, in EXIF for JPEG\n     - can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI\n     - can be disabled in settings\n     - drag and drop an image/text-parameters to promptbox\n- Read Generation Parameters Button, loads parameters in promptbox to UI\n- Settings page\n- Running arbitrary python code from UI (must run with `--allow-code` to enable)\n- Mouseover hints for most UI elements\n- Possible to change defaults/mix/max/step values for UI elements via text config\n- Tiling support, a checkbox to create images that can be tiled like textures\n- Progress bar and live image generation preview\n    - Can use a separate neural network to produce previews with almost none VRAM or compute requirement\n- Negative prompt, an extra text field that allows you to list what you don''t want to see in generated image\n- Styles, a way to save part of prompt and easily apply them via dropdown later\n- Variations, a way to generate same image but with tiny differences\n- Seed resizing, a way to generate same image but at slightly different resolution\n- CLIP interrogator, a button that tries to guess prompt from an image\n- Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway\n- Batch Processing, process a group of files using img2img\n- Img2img Alternative, reverse Euler method of cross attention control\n- Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions\n- Reloading checkpoints on the fly\n- Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one\n- [Custom scripts](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts) with many extensions from community\n- [Composable-Diffusion](https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/), a way to use multiple prompts at once\n     - separate prompts using uppercase `AND`\n     - also supports weights for prompts: `a cat :1.2 AND a dog AND a penguin :2.2`\n- No token limit for prompts (original stable diffusion lets you use up to 75 tokens)\n- DeepDanbooru integration, creates danbooru style tags for anime prompts\n- [xformers](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers), major speed increase for select cards: (add `--xformers` to commandline args)\n- via extension: [History tab](https://github.com/yfszzx/stable-diffusion-webui-images-browser): view, direct and delete images conveniently within the UI\n- Generate forever option\n- Training tab\n     - hypernetworks and embeddings options\n     - Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)\n- Clip skip\n- Hypernetworks\n- Loras (same as Hypernetworks but more pretty)\n- A separate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your prompt\n- Can select to load a different VAE from settings screen\n- Estimated completion time in progress bar\n- API\n- Support for dedicated [inpainting model](https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion) by RunwayML\n- via extension: [Aesthetic Gradients](https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients), a way to generate images with a specific aesthetic by using clip images embeds (implementation of [https://github.com/vicgalle/stable-diffusion-aesthetic-gradients](https://github.com/vicgalle/stable-diffusion-aesthetic-gradients))\n- [Stable Diffusion 2.0](https://github.com/Stability-AI/stablediffusion) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20) for instructions\n- [Alt-Diffusion](https://arxiv.org/abs/2211.06679) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#alt-diffusion) for instructions\n- Now without any bad letters!\n- Load checkpoints in safetensors format\n- Eased resolution restriction: generated image''s dimensions must be a multiple of 8 rather than 64\n- Now with a license!\n- Reorder elements in the UI from settings screen\n- [Segmind Stable Diffusion](https://huggingface.co/segmind/SSD-1B) support\n\n## Installation and Running\nMake sure the required [dependencies](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies) are met and follow the instructions available for:\n- [NVidia](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs) (recommended)\n- [AMD](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs) GPUs.\n- [Intel CPUs, Intel GPUs (both integrated and discrete)](https://github.com/openvinotoolkit/stable-diffusion-webui/wiki/Installation-on-Intel-Silicon) (external wiki page)\n- [Ascend NPUs](https://github.com/wangshuai09/stable-diffusion-webui/wiki/Install-and-run-on-Ascend-NPUs) (external wiki page)\n\nAlternatively, use online services (like Google Colab):\n\n- [List of Online Services](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)\n\n### Installation on Windows 10/11 with NVidia-GPUs using release package\n1. Download `sd.webui.zip` from [v1.0.0-pre](https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.0.0-pre) and extract its contents.\n2. Run `update.bat`.\n3. Run `run.bat`.\n> For more details see [Install-and-Run-on-NVidia-GPUs](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs)\n\n### Automatic Installation on Windows\n1. Install [Python 3.10.6](https://www.python.org/downloads/release/python-3106/) (Newer version of Python does not support torch), checking "Add Python to PATH".\n2. Install [git](https://git-scm.com/download/win).\n3. Download the stable-diffusion-webui repository, for example by running `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`.\n4. Run `webui-user.bat` from Windows Explorer as normal, non-administrator, user.\n\n### Automatic Installation on Linux\n1. Install the dependencies:\n```bash\n# Debian-based:\nsudo apt install wget git python3 python3-venv libgl1 libglib2.0-0\n# Red Hat-based:\nsudo dnf install wget git python3 gperftools-libs libglvnd-glx\n# openSUSE-based:\nsudo zypper install wget git python3 libtcmalloc4 libglvnd\n# Arch-based:\nsudo pacman -S wget git python3\n```\nIf your system is very new, you need to install python3.11 or python3.10:\n```bash\n# Ubuntu 24.04\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt update\nsudo apt install python3.11\n\n# Manjaro/Arch\nsudo pacman -S yay\nyay -S python311 # do not confuse with python3.11 package\n\n# Only for 3.11\n# Then set up env variable in launch script\nexport python_cmd="python3.11"\n# or in webui-user.sh\npython_cmd="python3.11"\n```\n2. Navigate to the directory you would like the webui to be installed and execute the following command:\n```bash\nwget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh\n```\nOr just clone the repo wherever you want:\n```bash\ngit clone https://github.com/AUTOMATIC1111/stable-diffusion-webui\n```\n\n3. Run `webui.sh`.\n4. Check `webui-user.sh` for options.\n### Installation on Apple Silicon\n\nFind the instructions [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon).\n\n## Contributing\nHere''s how to add code to this repo: [Contributing](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)\n\n## Documentation\n\nThe documentation was moved from this README over to the project''s [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki).\n\nFor the purposes of getting Google and other search engines to crawl the wiki, here''s a link to the (not for humans) [crawlable wiki](https://github-wiki-see.page/m/AUTOMATIC1111/stable-diffusion-webui/wiki).\n\n## Credits\nLicenses for borrowed code can be found in `Settings -> Licenses` screen, and also in `html/licenses.html` file.\n\n- Stable Diffusion - https://github.com/Stability-AI/stablediffusion, https://github.com/CompVis/taming-transformers, https://github.com/mcmonkey4eva/sd3-ref\n- k-diffusion - https://github.com/crowsonkb/k-diffusion.git\n- Spandrel - https://github.com/chaiNNer-org/spandrel implementing\n  - GFPGAN - https://github.com/TencentARC/GFPGAN.git\n  - CodeFormer - https://github.com/sczhou/CodeFormer\n  - ESRGAN - https://github.com/xinntao/ESRGAN\n  - SwinIR - https://github.com/JingyunLiang/SwinIR\n  - Swin2SR - https://github.com/mv-lab/swin2sr\n- LDSR - https://github.com/Hafiidz/latent-diffusion\n- MiDaS - https://github.com/isl-org/MiDaS\n- Ideas for optimizations - https://github.com/basujindal/stable-diffusion\n- Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing.\n- Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion)\n- Sub-quadratic Cross Attention layer optimization - Alex Birch (https://github.com/Birch-san/diffusers/pull/1), Amin Rezaei (https://github.com/AminRezaei0x443/memory-efficient-attention)\n- Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (we''re not using his code, but we are using his ideas).\n- Idea for SD upscale - https://github.com/jquesnelle/txt2imghd\n- Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot\n- CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator\n- Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch\n- xformers - https://github.com/facebookresearch/xformers\n- DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru\n- Sampling in float32 precision from a float16 UNet - marunine for the idea, Birch-san for the example Diffusers implementation (https://github.com/Birch-san/diffusers-play/tree/92feee6)\n- Instruct pix2pix - Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) - https://github.com/timothybrooks/instruct-pix2pix\n- Security advice - RyotaK\n- UniPC sampler - Wenliang Zhao - https://github.com/wl-zhao/UniPC\n- TAESD - Ollin Boer Bohan - https://github.com/madebyollin/taesd\n- LyCORIS - KohakuBlueleaf\n- Restart sampling - lambertae - https://github.com/Newbeeer/diffusion_restart_sampling\n- Hypertile - tfernd - https://github.com/tfernd/HyperTile\n- Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.\n- (You)\n', '{"language":"Python","stars":158820,"forks":29490,"watchers":158820,"open_issues":2442,"topics":["ai","ai-art","deep-learning","diffusion","gradio","image-generation","image2image","img2img","pytorch","stable-diffusion","text2image","torch","txt2img","unstable","upscaling","web"],"default_branch":"master","size_kb":36442,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:yfszzx:stable-diffusion-webui-images-browser","source_url":"https://github.com/yfszzx/stable-diffusion-webui-images-browser"},{"type":"has_code","target_id":"github:runwayml:stable-diffusion","source_url":"https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui-aesthetic-gradients","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients"},{"type":"has_code","target_id":"github:vicgalle:stable-diffusion-aesthetic-gradients](https:","source_url":"https://github.com/vicgalle/stable-diffusion-aesthetic-gradients](https:"},{"type":"has_code","target_id":"github:Stability-AI:stablediffusion","source_url":"https://github.com/Stability-AI/stablediffusion"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:openvinotoolkit:stable-diffusion-webui","source_url":"https://github.com/openvinotoolkit/stable-diffusion-webui"},{"type":"has_code","target_id":"github:wangshuai09:stable-diffusion-webui","source_url":"https://github.com/wangshuai09/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui.git`.","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`."},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:Stability-AI:stablediffusion,","source_url":"https://github.com/Stability-AI/stablediffusion,"},{"type":"has_code","target_id":"github:CompVis:taming-transformers,","source_url":"https://github.com/CompVis/taming-transformers,"},{"type":"has_code","target_id":"github:mcmonkey4eva:sd3-ref","source_url":"https://github.com/mcmonkey4eva/sd3-ref"},{"type":"has_code","target_id":"github:crowsonkb:k-diffusion.git","source_url":"https://github.com/crowsonkb/k-diffusion.git"},{"type":"has_code","target_id":"github:chaiNNer-org:spandrel","source_url":"https://github.com/chaiNNer-org/spandrel"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN.git","source_url":"https://github.com/TencentARC/GFPGAN.git"},{"type":"has_code","target_id":"github:sczhou:CodeFormer","source_url":"https://github.com/sczhou/CodeFormer"},{"type":"has_code","target_id":"github:xinntao:ESRGAN","source_url":"https://github.com/xinntao/ESRGAN"},{"type":"has_code","target_id":"github:JingyunLiang:SwinIR","source_url":"https://github.com/JingyunLiang/SwinIR"},{"type":"has_code","target_id":"github:mv-lab:swin2sr","source_url":"https://github.com/mv-lab/swin2sr"},{"type":"has_code","target_id":"github:Hafiidz:latent-diffusion","source_url":"https://github.com/Hafiidz/latent-diffusion"},{"type":"has_code","target_id":"github:isl-org:MiDaS","source_url":"https://github.com/isl-org/MiDaS"},{"type":"has_code","target_id":"github:basujindal:stable-diffusion","source_url":"https://github.com/basujindal/stable-diffusion"},{"type":"has_code","target_id":"github:Doggettx:stable-diffusion,","source_url":"https://github.com/Doggettx/stable-diffusion,"},{"type":"has_code","target_id":"github:invoke-ai:InvokeAI","source_url":"https://github.com/invoke-ai/InvokeAI"},{"type":"has_code","target_id":"github:lstein:stable-diffusion","source_url":"http://github.com/lstein/stable-diffusion"},{"type":"has_code","target_id":"github:Birch-san:diffusers","source_url":"https://github.com/Birch-san/diffusers"},{"type":"has_code","target_id":"github:AminRezaei0x443:memory-efficient-attention","source_url":"https://github.com/AminRezaei0x443/memory-efficient-attention"},{"type":"has_code","target_id":"github:rinongal:textual_inversion","source_url":"https://github.com/rinongal/textual_inversion"},{"type":"has_code","target_id":"github:jquesnelle:txt2imghd","source_url":"https://github.com/jquesnelle/txt2imghd"},{"type":"has_code","target_id":"github:parlance-zz:g-diffuser-bot","source_url":"https://github.com/parlance-zz/g-diffuser-bot"},{"type":"has_code","target_id":"github:pharmapsychotic:clip-interrogator","source_url":"https://github.com/pharmapsychotic/clip-interrogator"},{"type":"has_code","target_id":"github:energy-based-model:Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch","source_url":"https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch"},{"type":"has_code","target_id":"github:facebookresearch:xformers","source_url":"https://github.com/facebookresearch/xformers"},{"type":"has_code","target_id":"github:KichangKim:DeepDanbooru","source_url":"https://github.com/KichangKim/DeepDanbooru"},{"type":"has_code","target_id":"github:Birch-san:diffusers-play","source_url":"https://github.com/Birch-san/diffusers-play"},{"type":"has_code","target_id":"github:timothybrooks:instruct-pix2pix","source_url":"https://github.com/timothybrooks/instruct-pix2pix"},{"type":"has_code","target_id":"github:wl-zhao:UniPC","source_url":"https://github.com/wl-zhao/UniPC"},{"type":"has_code","target_id":"github:madebyollin:taesd","source_url":"https://github.com/madebyollin/taesd"},{"type":"has_code","target_id":"github:Newbeeer:diffusion_restart_sampling","source_url":"https://github.com/Newbeeer/diffusion_restart_sampling"},{"type":"has_code","target_id":"github:tfernd:HyperTile","source_url":"https://github.com/tfernd/HyperTile"}]', NULL, 'AGPL-3.0', 'approved', 80, '14ea93d459ddb16be471bacb57307e6c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-AUTOMATIC1111-stable-diffusion-webui from https://github.com/AUTOMATIC1111.png
Image converted to WebP: data/images/github-AUTOMATIC1111-stable-diffusion-webui.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-opencv-opencv', 'github--opencv--opencv', 'opencv', 'opencv', '* Homepage: <https://opencv.org> * Courses: <https://opencv.org/courses> * Docs: <https://docs.opencv.org/4.x/> * Q&A forum: <https://forum.opencv.org> * previous forum (read only): <http://answers.opencv.org> * Issue tracking: <https://github.com/opencv/opencv/issues> * Additional OpenCV functionality: <https://github.com/opencv/opencv_contrib> * Donate to OpenCV: <https://opencv.org/support/> Please read the contribution guidelines before starting work on a pull request. * One pull request ...', '["c-plus-plus","computer-vision","deep-learning","image-processing","opencv","c++"]', 'other', 85191, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/opencv/opencv","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '## OpenCV: Open Source Computer Vision Library\n\n\n### Resources\n\n* Homepage: <https://opencv.org>\n  * Courses: <https://opencv.org/courses>\n* Docs: <https://docs.opencv.org/4.x/>\n* Q&A forum: <https://forum.opencv.org>\n  * previous forum (read only): <http://answers.opencv.org>\n* Issue tracking: <https://github.com/opencv/opencv/issues>\n* Additional OpenCV functionality: <https://github.com/opencv/opencv_contrib>\n* Donate to OpenCV: <https://opencv.org/support/>\n\n\n### Contributing\n\nPlease read the [contribution guidelines](https://github.com/opencv/opencv/wiki/How_to_contribute) before starting work on a pull request.\n\n#### Summary of the guidelines:\n\n* One pull request per issue;\n* Choose the right base branch;\n* Include tests and documentation;\n* Clean up "oops" commits before submitting;\n* Follow the [coding style guide](https://github.com/opencv/opencv/wiki/Coding_Style_Guide).\n\n### Additional Resources\n\n* [Submit your OpenCV-based project](https://form.jotform.com/233105358823151) for inclusion in Community Friday on opencv.org\n* [Subscribe to the OpenCV YouTube Channel](http://youtube.com/@opencvofficial) featuring OpenCV Live, an hour-long streaming show\n* [Follow OpenCV on LinkedIn](http://linkedin.com/company/opencv/) for daily posts showing the state-of-the-art in computer vision & AI\n* [Apply to be an OpenCV Volunteer](https://form.jotform.com/232745316792159) to help organize events and online campaigns as well as amplify them\n* [Follow OpenCV on Mastodon](http://mastodon.social/@opencv) in the Fediverse\n* [Follow OpenCV on Twitter](https://twitter.com/opencvlive)\n* [OpenCV.ai](https://opencv.ai): Computer Vision and AI development services from the OpenCV team.\n', '{"language":"C++","stars":85191,"forks":56386,"watchers":85191,"open_issues":2709,"topics":["c-plus-plus","computer-vision","deep-learning","image-processing","opencv"],"default_branch":"4.x","size_kb":551435,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:opencv:opencv","source_url":"https://github.com/opencv/opencv"},{"type":"has_code","target_id":"github:opencv:opencv_contrib>","source_url":"https://github.com/opencv/opencv_contrib>"},{"type":"has_code","target_id":"github:opencv:opencv","source_url":"https://github.com/opencv/opencv"},{"type":"has_code","target_id":"github:opencv:opencv","source_url":"https://github.com/opencv/opencv"}]', NULL, 'Apache-2.0', 'approved', 50, '84038d88524e6cb7775a0c4d84743d12', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-opencv-opencv from https://github.com/opencv.png
Image converted to WebP: data/images/github-opencv-opencv.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-infiniflow-ragflow', 'github--infiniflow--ragflow', 'ragflow', 'infiniflow', '<div align="center"> <a href="https://demo.ragflow.io/"> <img src="web/src/assets/logo-with-text.svg" width="520" alt="ragflow logo"> </a> </div> <p align="center"> <a href="./README.md"><img alt="README in English" src="https://img.shields.io/badge/English-DBEDFA"></a> <a href="./README_zh.md"><img alt="ç®€ä½“ä¸­æ–‡ç‰ˆè‡ªè¿°æ–‡ä»¶" src="https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-DFE0E5"></a> <a href="./README_tzh.md"><img alt="ç¹é«”ç‰ˆä¸­æ–‡è‡ªè¿°æ–‡ä»¶" src="https://img.shields.io/badge/ç¹é«”ä¸­æ–‡-DFE0E5"></a> <a href="./README_ja.md"><img...', '["agent","agentic","agentic-ai","agentic-workflow","ai","ai-search","deep-learning","deep-research","deepseek","deepseek-r1","document-parser","document-understanding","graphrag","llm","mcp","multi-agent","ollama","openai","rag","retrieval-augmented-generation","python"]', 'other', 68943, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/infiniflow/ragflow","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n<a href="https://demo.ragflow.io/">\n<img src="web/src/assets/logo-with-text.svg" width="520" alt="ragflow logo">\n</a>\n</div>\n\n<p align="center">\n  <a href="./README.md"><img alt="README in English" src="https://img.shields.io/badge/English-DBEDFA"></a>\n  <a href="./README_zh.md"><img alt="ç®€ä½“ä¸­æ–‡ç‰ˆè‡ªè¿°æ–‡ä»¶" src="https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-DFE0E5"></a>\n  <a href="./README_tzh.md"><img alt="ç¹é«”ç‰ˆä¸­æ–‡è‡ªè¿°æ–‡ä»¶" src="https://img.shields.io/badge/ç¹é«”ä¸­æ–‡-DFE0E5"></a>\n  <a href="./README_ja.md"><img alt="æ—¥æœ¬èªã®README" src="https://img.shields.io/badge/æ—¥æœ¬èª-DFE0E5"></a>\n  <a href="./README_ko.md"><img alt="í•œêµ­ì–´" src="https://img.shields.io/badge/í•œêµ­ì–´-DFE0E5"></a>\n  <a href="./README_id.md"><img alt="Bahasa Indonesia" src="https://img.shields.io/badge/Bahasa Indonesia-DFE0E5"></a>\n  <a href="./README_pt_br.md"><img alt="PortuguÃªs(Brasil)" src="https://img.shields.io/badge/PortuguÃªs(Brasil)-DFE0E5"></a>\n</p>\n\n<p align="center">\n    <a href="https://x.com/intent/follow?screen_name=infiniflowai" target="_blank">\n        <img src="https://img.shields.io/twitter/follow/infiniflow?logo=X&color=%20%23f5f5f5" alt="follow on X(Twitter)">\n    </a>\n    <a href="https://demo.ragflow.io" target="_blank">\n        <img alt="Static Badge" src="https://img.shields.io/badge/Online-Demo-4e6b99">\n    </a>\n    <a href="https://hub.docker.com/r/infiniflow/ragflow" target="_blank">\n        <img src="https://img.shields.io/docker/pulls/infiniflow/ragflow?label=Docker%20Pulls&color=0db7ed&logo=docker&logoColor=white&style=flat-square" alt="docker pull infiniflow/ragflow:v0.22.1">\n    </a>\n    <a href="https://github.com/infiniflow/ragflow/releases/latest">\n        <img src="https://img.shields.io/github/v/release/infiniflow/ragflow?color=blue&label=Latest%20Release" alt="Latest Release">\n    </a>\n    <a href="https://github.com/infiniflow/ragflow/blob/main/LICENSE">\n        <img height="21" src="https://img.shields.io/badge/License-Apache--2.0-ffffff?labelColor=d4eaf7&color=2e6cc4" alt="license">\n    </a>\n    <a href="https://deepwiki.com/infiniflow/ragflow">\n        <img alt="Ask DeepWiki" src="https://deepwiki.com/badge.svg">\n    </a>\n</p>\n\n<h4 align="center">\n  <a href="https://ragflow.io/docs/dev/">Document</a> |\n  <a href="https://github.com/infiniflow/ragflow/issues/4214">Roadmap</a> |\n  <a href="https://twitter.com/infiniflowai">Twitter</a> |\n  <a href="https://discord.gg/NjYzJD3GM3">Discord</a> |\n  <a href="https://demo.ragflow.io">Demo</a>\n</h4>\n\n<div align="center" style="margin-top:20px;margin-bottom:20px;">\n<img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/ragflow-octoverse.png" width="1200"/>\n</div>\n\n<div align="center">\n<a href="https://trendshift.io/repositories/9064" target="_blank"><img src="https://trendshift.io/api/badge/repositories/9064" alt="infiniflow%2Fragflow | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n</div>\n\n<details open>\n<summary><b>ğŸ“• Table of Contents</b></summary>\n\n- ğŸ’¡ [What is RAGFlow?](#-what-is-ragflow)\n- ğŸ® [Demo](#-demo)\n- ğŸ“Œ [Latest Updates](#-latest-updates)\n- ğŸŒŸ [Key Features](#-key-features)\n- ğŸ” [System Architecture](#-system-architecture)\n- ğŸ¬ [Get Started](#-get-started)\n- ğŸ”§ [Configurations](#-configurations)\n- ğŸ”§ [Build a Docker image](#-build-a-docker-image)\n- ğŸ”¨ [Launch service from source for development](#-launch-service-from-source-for-development)\n- ğŸ“š [Documentation](#-documentation)\n- ğŸ“œ [Roadmap](#-roadmap)\n- ğŸ„ [Community](#-community)\n- ğŸ™Œ [Contributing](#-contributing)\n\n</details>\n\n## ğŸ’¡ What is RAGFlow?\n\n[RAGFlow](https://ragflow.io/) is a leading open-source Retrieval-Augmented Generation (RAG) engine that fuses cutting-edge RAG with Agent capabilities to create a superior context layer for LLMs. It offers a streamlined RAG workflow adaptable to enterprises of any scale. Powered by a converged context engine and pre-built agent templates, RAGFlow enables developers to transform complex data into high-fidelity, production-ready AI systems with exceptional efficiency and precision.\n\n## ğŸ® Demo\n\nTry our demo at [https://demo.ragflow.io](https://demo.ragflow.io).\n\n<div align="center" style="margin-top:20px;margin-bottom:20px;">\n<img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/chunking.gif" width="1200"/>\n<img src="https://raw.githubusercontent.com/infiniflow/ragflow-docs/refs/heads/image/image/agentic-dark.gif" width="1200"/>\n</div>\n\n## ğŸ”¥ Latest Updates\n\n- 2025-11-19 Supports Gemini 3 Pro.\n- 2025-11-12 Supports data synchronization from Confluence, S3, Notion, Discord, Google Drive.\n- 2025-10-23 Supports MinerU & Docling as document parsing methods.\n- 2025-10-15 Supports orchestrable ingestion pipeline.\n- 2025-08-08 Supports OpenAI''s latest GPT-5 series models.\n- 2025-08-01 Supports agentic workflow and MCP.\n- 2025-05-23 Adds a Python/JavaScript code executor component to Agent.\n- 2025-05-05 Supports cross-language query.\n- 2025-03-19 Supports using a multi-modal model to make sense of images within PDF or DOCX files.\n\n## ğŸ‰ Stay Tuned\n\nâ­ï¸ Star our repository to stay up-to-date with exciting new features and improvements! Get instant notifications for new\nreleases! ğŸŒŸ\n\n<div align="center" style="margin-top:20px;margin-bottom:20px;">\n<img src="https://github.com/user-attachments/assets/18c9707e-b8aa-4caf-a154-037089c105ba" width="1200"/>\n</div>\n\n## ğŸŒŸ Key Features\n\n### ğŸ­ **"Quality in, quality out"**\n\n- [Deep document understanding](./deepdoc/README.md)-based knowledge extraction from unstructured data with complicated\n  formats.\n- Finds "needle in a data haystack" of literally unlimited tokens.\n\n### ğŸ± **Template-based chunking**\n\n- Intelligent and explainable.\n- Plenty of template options to choose from.\n\n### ğŸŒ± **Grounded citations with reduced hallucinations**\n\n- Visualization of text chunking to allow human intervention.\n- Quick view of the key references and traceable citations to support grounded answers.\n\n### ğŸ” **Compatibility with heterogeneous data sources**\n\n- Supports Word, slides, excel, txt, images, scanned copies, structured data, web pages, and more.\n\n### ğŸ›€ **Automated and effortless RAG workflow**\n\n- Streamlined RAG orchestration catered to both personal and large businesses.\n- Configurable LLMs as well as embedding models.\n- Multiple recall paired with fused re-ranking.\n- Intuitive APIs for seamless integration with business.\n\n## ğŸ” System Architecture\n\n<div align="center" style="margin-top:20px;margin-bottom:20px;">\n<img src="https://github.com/user-attachments/assets/31b0dd6f-ca4f-445a-9457-70cb44a381b2" width="1000"/>\n</div>\n\n## ğŸ¬ Get Started\n\n### ğŸ“ Prerequisites\n\n- CPU >= 4 cores\n- RAM >= 16 GB\n- Disk >= 50 GB\n- Docker >= 24.0.0 & Docker Compose >= v2.26.1\n- [gVisor](https://gvisor.dev/docs/user_guide/install/): Required only if you intend to use the code executor (sandbox) feature of RAGFlow.\n\n> [!TIP]\n> If you have not installed Docker on your local machine (Windows, Mac, or Linux), see [Install Docker Engine](https://docs.docker.com/engine/install/).\n\n### ğŸš€ Start up the server\n\n1. Ensure `vm.max_map_count` >= 262144:\n\n   > To check the value of `vm.max_map_count`:\n   >\n   > ```bash\n   > $ sysctl vm.max_map_count\n   > ```\n   >\n   > Reset `vm.max_map_count` to a value at least 262144 if it is not.\n   >\n   > ```bash\n   > # In this case, we set it to 262144:\n   > $ sudo sysctl -w vm.max_map_count=262144\n   > ```\n   >\n   > This change will be reset after a system reboot. To ensure your change remains permanent, add or update the\n   > `vm.max_map_count` value in **/etc/sysctl.conf** accordingly:\n   >\n   > ```bash\n   > vm.max_map_count=262144\n   > ```\n   >\n2. Clone the repo:\n\n   ```bash\n   $ git clone https://github.com/infiniflow/ragflow.git\n   ```\n3. Start up the server using the pre-built Docker images:\n\n> [!CAUTION]\n> All Docker images are built for x86 platforms. We don''t currently offer Docker images for ARM64.\n> If you are on an ARM64 platform, follow [this guide](https://ragflow.io/docs/dev/build_docker_image) to build a Docker image compatible with your system.\n\n> The command below downloads the `v0.22.1` edition of the RAGFlow Docker image. See the following table for descriptions of different RAGFlow editions. To download a RAGFlow edition different from `v0.22.1`, update the `RAGFLOW_IMAGE` variable accordingly in **docker/.env** before using `docker compose` to start the server.\n\n```bash\n   $ cd ragflow/docker\n  \n   # git checkout v0.22.1\n   # Optional: use a stable tag (see releases: https://github.com/infiniflow/ragflow/releases)\n   # This step ensures the **entrypoint.sh** file in the code matches the Docker image version.\n   \n   # Use CPU for DeepDoc tasks:\n   $ docker compose -f docker-compose.yml up -d\n\n   # To use GPU to accelerate DeepDoc tasks:\n   # sed -i ''1i DEVICE=gpu'' .env\n   # docker compose -f docker-compose.yml up -d\n```\n\n> Note: Prior to `v0.22.0`, we provided both images with embedding models and slim images without embedding models. Details as follows:\n\n| RAGFlow image tag | Image size (GB) | Has embedding models? | Stable?                  |\n| ----------------- | --------------- | --------------------- | ------------------------ |\n| v0.21.1           | &approx;9       | âœ”ï¸                    | Stable release           |\n| v0.21.1-slim      | &approx;2       | âŒ                    | Stable release           |\n\n> Starting with `v0.22.0`, we ship only the slim edition and no longer append the **-slim** suffix to the image tag.\n\n4. Check the server status after having the server up and running:\n\n   ```bash\n   $ docker logs -f docker-ragflow-cpu-1\n   ```\n\n   _The following output confirms a successful launch of the system:_\n\n   ```bash\n\n         ____   ___    ______ ______ __\n        / __ \ /   |  / ____// ____// /____  _      __\n       / /_/ // /| | / / __ / /_   / // __ \| | /| / /\n      / _, _// ___ |/ /_/ // __/  / // /_/ /| |/ |/ /\n     /_/ |_|/_/  |_|\____//_/    /_/ \____/ |__/|__/\n\n    * Running on all addresses (0.0.0.0)\n   ```\n\n   > If you skip this confirmation step and directly log in to RAGFlow, your browser may prompt a `network anormal`\n   > error because, at that moment, your RAGFlow may not be fully initialized.\n   >\n5. In your web browser, enter the IP address of your server and log in to RAGFlow.\n\n   > With the default settings, you only need to enter `http://IP_OF_YOUR_MACHINE` (**sans** port number) as the default\n   > HTTP serving port `80` can be omitted when using the default configurations.\n   >\n6. In [service_conf.yaml.template](./docker/service_conf.yaml.template), select the desired LLM factory in `user_default_llm` and update\n   the `API_KEY` field with the corresponding API key.\n\n   > See [llm_api_key_setup](https://ragflow.io/docs/dev/llm_api_key_setup) for more information.\n   >\n\n   _The show is on!_\n\n## ğŸ”§ Configurations\n\nWhen it comes to system configurations, you will need to manage the following files:\n\n- [.env](./docker/.env): Keeps the fundamental setups for the system, such as `SVR_HTTP_PORT`, `MYSQL_PASSWORD`, and\n  `MINIO_PASSWORD`.\n- [service_conf.yaml.template](./docker/service_conf.yaml.template): Configures the back-end services. The environment variables in this file will be automatically populated when the Docker container starts. Any environment variables set within the Docker container will be available for use, allowing you to customize service behavior based on the deployment environment.\n- [docker-compose.yml](./docker/docker-compose.yml): The system relies on [docker-compose.yml](./docker/docker-compose.yml) to start up.\n\n> The [./docker/README](./docker/README.md) file provides a detailed description of the environment settings and service\n> configurations which can be used as `${ENV_VARS}` in the [service_conf.yaml.template](./docker/service_conf.yaml.template) file.\n\nTo update the default HTTP serving port (80), go to [docker-compose.yml](./docker/docker-compose.yml) and change `80:80`\nto `<YOUR_SERVING_PORT>:80`.\n\nUpdates to the above configurations require a reboot of all containers to take effect:\n\n> ```bash\n> $ docker compose -f docker-compose.yml up -d\n> ```\n\n### Switch doc engine from Elasticsearch to Infinity\n\nRAGFlow uses Elasticsearch by default for storing full text and vectors. To switch to [Infinity](https://github.com/infiniflow/infinity/), follow these steps:\n\n1. Stop all running containers:\n\n   ```bash\n   $ docker compose -f docker/docker-compose.yml down -v\n   ```\n\n> [!WARNING]\n> `-v` will delete the docker container volumes, and the existing data will be cleared.\n\n2. Set `DOC_ENGINE` in **docker/.env** to `infinity`.\n3. Start the containers:\n\n   ```bash\n   $ docker compose -f docker-compose.yml up -d\n   ```\n\n> [!WARNING]\n> Switching to Infinity on a Linux/arm64 machine is not yet officially supported.\n\n## ğŸ”§ Build a Docker image\n\nThis image is approximately 2 GB in size and relies on external LLM and embedding services.\n\n```bash\ngit clone https://github.com/infiniflow/ragflow.git\ncd ragflow/\ndocker build --platform linux/amd64 -f Dockerfile -t infiniflow/ragflow:nightly .\n```\n\n## ğŸ”¨ Launch service from source for development\n\n1. Install `uv` and `pre-commit`, or skip this step if they are already installed:\n\n   ```bash\n   pipx install uv pre-commit\n   ```\n2. Clone the source code and install Python dependencies:\n\n   ```bash\n   git clone https://github.com/infiniflow/ragflow.git\n   cd ragflow/\n   uv sync --python 3.10 # install RAGFlow dependent python modules\n   uv run download_deps.py\n   pre-commit install\n   ```\n3. Launch the dependent services (MinIO, Elasticsearch, Redis, and MySQL) using Docker Compose:\n\n   ```bash\n   docker compose -f docker/docker-compose-base.yml up -d\n   ```\n\n   Add the following line to `/etc/hosts` to resolve all hosts specified in **docker/.env** to `127.0.0.1`:\n\n   ```\n   127.0.0.1       es01 infinity mysql minio redis sandbox-executor-manager\n   ```\n4. If you cannot access HuggingFace, set the `HF_ENDPOINT` environment variable to use a mirror site:\n\n   ```bash\n   export HF_ENDPOINT=https://hf-mirror.com\n   ```\n5. If your operating system does not have jemalloc, please install it as follows:\n\n   ```bash\n   # Ubuntu\n   sudo apt-get install libjemalloc-dev\n   # CentOS\n   sudo yum install jemalloc\n   # OpenSUSE\n   sudo zypper install jemalloc\n   # macOS\n   sudo brew install jemalloc\n   ```\n6. Launch backend service:\n\n   ```bash\n   source .venv/bin/activate\n   export PYTHONPATH=$(pwd)\n   bash docker/launch_backend_service.sh\n   ```\n7. Install frontend dependencies:\n\n   ```bash\n   cd web\n   npm install\n   ```\n8. Launch frontend service:\n\n   ```bash\n   npm run dev\n   ```\n\n   _The following output confirms a successful launch of the system:_\n\n   ![](https://github.com/user-attachments/assets/0daf462c-a24d-4496-a66f-92533534e187)\n9. Stop RAGFlow front-end and back-end service after development is complete:\n\n   ```bash\n   pkill -f "ragflow_server.py|task_executor.py"\n   ```\n\n## ğŸ“š Documentation\n\n- [Quickstart](https://ragflow.io/docs/dev/)\n- [Configuration](https://ragflow.io/docs/dev/configurations)\n- [Release notes](https://ragflow.io/docs/dev/release_notes)\n- [User guides](https://ragflow.io/docs/dev/category/guides)\n- [Developer guides](https://ragflow.io/docs/dev/category/developers)\n- [References](https://ragflow.io/docs/dev/category/references)\n- [FAQs](https://ragflow.io/docs/dev/faq)\n\n## ğŸ“œ Roadmap\n\nSee the [RAGFlow Roadmap 2025](https://github.com/infiniflow/ragflow/issues/4214)\n\n## ğŸ„ Community\n\n- [Discord](https://discord.gg/NjYzJD3GM3)\n- [Twitter](https://twitter.com/infiniflowai)\n- [GitHub Discussions](https://github.com/orgs/infiniflow/discussions)\n\n## ğŸ™Œ Contributing\n\nRAGFlow flourishes via open-source collaboration. In this spirit, we embrace diverse contributions from the community.\nIf you would like to be a part, review our [Contribution Guidelines](https://ragflow.io/docs/dev/contributing) first.\n', '{"language":"Python","stars":68943,"forks":7475,"watchers":68943,"open_issues":2955,"topics":["agent","agentic","agentic-ai","agentic-workflow","ai","ai-search","deep-learning","deep-research","deepseek","deepseek-r1","document-parser","document-understanding","graphrag","llm","mcp","multi-agent","ollama","openai","rag","retrieval-augmented-generation"],"default_branch":"main","size_kb":96735,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:infiniflow:ragflow","source_url":"https://github.com/infiniflow/ragflow"},{"type":"has_code","target_id":"github:infiniflow:ragflow","source_url":"https://github.com/infiniflow/ragflow"},{"type":"has_code","target_id":"github:infiniflow:ragflow","source_url":"https://github.com/infiniflow/ragflow"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:infiniflow:ragflow.git","source_url":"https://github.com/infiniflow/ragflow.git"},{"type":"has_code","target_id":"github:infiniflow:ragflow","source_url":"https://github.com/infiniflow/ragflow"},{"type":"has_code","target_id":"github:infiniflow:infinity","source_url":"https://github.com/infiniflow/infinity"},{"type":"has_code","target_id":"github:infiniflow:ragflow.git","source_url":"https://github.com/infiniflow/ragflow.git"},{"type":"has_code","target_id":"github:infiniflow:ragflow.git","source_url":"https://github.com/infiniflow/ragflow.git"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:infiniflow:ragflow","source_url":"https://github.com/infiniflow/ragflow"},{"type":"has_code","target_id":"github:orgs:infiniflow","source_url":"https://github.com/orgs/infiniflow"}]', NULL, 'Apache-2.0', 'approved', 80, '1bf14b9f5e66e050a308364cc1171fc9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-infiniflow-ragflow from https://github.com/infiniflow.png
Image converted to WebP: data/images/github-infiniflow-ragflow.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-dair-ai-Prompt-Engineering-Guide', 'github--dair-ai--prompt-engineering-guide', 'Prompt-Engineering-Guide', 'dair-ai', '<h5 align="center"> Sponsored by&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://serpapi.com/"><img src="https://cdn.rawgit.com/standard/standard/master/docs/logos/serpapi.png" height=35 valign="middle"></a> </h5> Prompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics. Prompt engineering skills help to better understand the capabilities and limitations of large language mod...', '["agent","agents","ai-agents","chatgpt","deep-learning","generative-ai","language-model","llms","openai","prompt-engineering","rag","mdx"]', 'other', 67389, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/dair-ai/Prompt-Engineering-Guide","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# Prompt Engineering Guide\n\n<h5 align="center">\n  Sponsored by&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://serpapi.com/"><img src="https://cdn.rawgit.com/standard/standard/master/docs/logos/serpapi.png" height=35 valign="middle"></a>\n</h5>\n\nPrompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics. Prompt engineering skills help to better understand the capabilities and limitations of large language models (LLMs). Researchers use prompt engineering to improve the capacity of LLMs on a wide range of common and complex tasks such as question answering and arithmetic reasoning. Developers use prompt engineering to design robust and effective prompting techniques that interface with LLMs and other tools.\n\nMotivated by the high interest in developing with LLMs, we have created this new prompt engineering guide that contains all the latest papers, learning guides, lectures, references, and tools related to prompt engineering for LLMs.\n\nğŸŒ [Prompt Engineering Guide (Web Version)](https://www.promptingguide.ai/)\n\nğŸ‰ We are excited to launch our new prompt engineering, RAG, and AI Agents courses under the DAIR.AI Academy. [Join Now](https://dair-ai.thinkific.com/bundles/pro)!\n\nThe courses are meant to compliment this guide and provide a more hands-on approach to learning about prompt engineering, context engineering, and AI Agents. \n\nUse code PROMPTING20 to get an extra 20% off.\n\nHappy Prompting!\n\n---\n## Announcements / Updates\n\n- ğŸ“ We now offer self-paced prompt engineering courses under our DAIR.AI Academy. [Join Now](https://dair-ai.thinkific.com/bundles/pro)! \n- ğŸ“ New course on Prompt Engineering for LLMs announced! [Enroll here](https://maven.com/dair-ai/prompt-engineering-llms)!\n- ğŸ’¼ We now offer several [services](https://www.promptingguide.ai/services) like corporate training, consulting, and talks.\n- ğŸŒ We now support 13 languages! Welcoming more translations.\n- ğŸ‘©â€ğŸ“ We crossed 3 million learners in January 2024!\n- ğŸ‰ We have launched a new web version of the guide [here](https://www.promptingguide.ai/)\n- ğŸ”¥ We reached #1 on Hacker News on 21 Feb 2023\n- ğŸ‰ The First Prompt Engineering Lecture went live [here](https://youtu.be/dOxUroR57xs)\n\n[Join our Discord](https://discord.com/invite/SKgkVT8BGJ)\n\n[Follow us on Twitter](https://twitter.com/dair_ai)\n\n[Subscribe to our YouTube](https://www.youtube.com/channel/UCyna_OxOWL7IEuOwb7WhmxQ)\n\n[Subscribe to our Newsletter](https://nlpnews.substack.com/)\n\n---\n\n## Guides\nYou can also find the most up-to-date guides on our new website [https://www.promptingguide.ai/](https://www.promptingguide.ai/).\n\n- [Prompt Engineering - Introduction](https://www.promptingguide.ai/introduction)\n  - [Prompt Engineering - LLM Settings](https://www.promptingguide.ai/introduction/settings)\n  - [Prompt Engineering - Basics of Prompting](https://www.promptingguide.ai/introduction/basics)\n  - [Prompt Engineering - Prompt Elements](https://www.promptingguide.ai/introduction/elements)\n  - [Prompt Engineering - General Tips for Designing Prompts](https://www.promptingguide.ai/introduction/tips)\n  - [Prompt Engineering - Examples of Prompts](https://www.promptingguide.ai/introduction/examples)\n- [Prompt Engineering - Techniques](https://www.promptingguide.ai/techniques)\n  - [Prompt Engineering - Zero-Shot Prompting](https://www.promptingguide.ai/techniques/zeroshot)\n  - [Prompt Engineering - Few-Shot Prompting](https://www.promptingguide.ai/techniques/fewshot)\n  - [Prompt Engineering - Chain-of-Thought Prompting](https://www.promptingguide.ai/techniques/cot)\n  - [Prompt Engineering - Self-Consistency](https://www.promptingguide.ai/techniques/consistency)\n  - [Prompt Engineering - Generate Knowledge Prompting](https://www.promptingguide.ai/techniques/knowledge)\n  - [Prompt Engineering - Prompt Chaining](https://www.promptingguide.ai/techniques/prompt_chaining)\n  - [Prompt Engineering - Tree of Thoughts (ToT)](https://www.promptingguide.ai/techniques/tot)\n  - [Prompt Engineering - Retrieval Augmented Generation](https://www.promptingguide.ai/techniques/rag)\n  - [Prompt Engineering - Automatic Reasoning and Tool-use (ART)](https://www.promptingguide.ai/techniques/art)\n  - [Prompt Engineering - Automatic Prompt Engineer](https://www.promptingguide.ai/techniques/ape)\n  - [Prompt Engineering - Active-Prompt](https://www.promptingguide.ai/techniques/activeprompt)\n  - [Prompt Engineering - Directional Stimulus Prompting](https://www.promptingguide.ai/techniques/dsp)\n  - [Prompt Engineering - Program-Aided Language Models](https://www.promptingguide.ai/techniques/pal)\n  - [Prompt Engineering - ReAct Prompting](https://www.promptingguide.ai/techniques/react)\n  - [Prompt Engineering - Multimodal CoT Prompting](https://www.promptingguide.ai/techniques/multimodalcot)\n  - [Prompt Engineering - Graph Prompting](https://www.promptingguide.ai/techniques/graph)\n- [Prompt Engineering - Applications](https://www.promptingguide.ai/applications)\n  - [Prompt Engineering - Function Calling](https://www.promptingguide.ai/applications/function_calling)\n  - [Prompt Engineering - Generating Data](https://www.promptingguide.ai/applications/generating)\n  - [Prompt Engineering - Generating Synthetic Dataset for RAG](https://www.promptingguide.ai/applications/synthetic_rag)\n  - [Prompt Engineering - Takling Generated Datasets Diversity](https://www.promptingguide.ai/applications/generating_textbooks)\n  - [Prompt Engineering - Generating Code](https://www.promptingguide.ai/applications/coding)\n  - [Prompt Engineering - Graduate Job Classification Case Study](https://www.promptingguide.ai/applications/workplace_casestudy)\n- [Prompt Engineering - Prompt Hub](https://www.promptingguide.ai/prompts)\n  - [Prompt Engineering - Classification](https://www.promptingguide.ai/prompts/classification)\n  - [Prompt Engineering - Coding](https://www.promptingguide.ai/prompts/coding)\n  - [Prompt Engineering - Creativity](https://www.promptingguide.ai/prompts/creativity)\n  - [Prompt Engineering - Evaluation](https://www.promptingguide.ai/prompts/evaluation)\n  - [Prompt Engineering - Information Extraction](https://www.promptingguide.ai/prompts/information-extraction)\n  - [Prompt Engineering - Image Generation](https://www.promptingguide.ai/prompts/image-generation)\n  - [Prompt Engineering - Mathematics](https://www.promptingguide.ai/prompts/mathematics)\n  - [Prompt Engineering - Question Answering](https://www.promptingguide.ai/prompts/question-answering)\n  - [Prompt Engineering - Reasoning](https://www.promptingguide.ai/prompts/reasoning)\n  - [Prompt Engineering - Text Summarization](https://www.promptingguide.ai/prompts/text-summarization)\n  - [Prompt Engineering - Truthfulness](https://www.promptingguide.ai/prompts/truthfulness)\n  - [Prompt Engineering - Adversarial Prompting](https://www.promptingguide.ai/prompts/adversarial-prompting)\n- [Prompt Engineering - Models](https://www.promptingguide.ai/models)\n  - [Prompt Engineering - ChatGPT](https://www.promptingguide.ai/models/chatgpt)\n  - [Prompt Engineering - Code Llama](https://www.promptingguide.ai/models/code-llama)\n  - [Prompt Engineering - Flan](https://www.promptingguide.ai/models/flan)\n  - [Prompt Engineering - Gemini](https://www.promptingguide.ai/models/gemini)\n  - [Prompt Engineering - GPT-4](https://www.promptingguide.ai/models/gpt-4)\n  - [Prompt Engineering - LLaMA](https://www.promptingguide.ai/models/llama)\n  - [Prompt Engineering - Mistral 7B](https://www.promptingguide.ai/models/mistral-7b)\n  - [Prompt Engineering - Mixtral](https://www.promptingguide.ai/models/mixtral)\n  - [Prompt Engineering - OLMo](https://www.promptingguide.ai/models/olmo)\n  - [Prompt Engineering - Phi-2](https://www.promptingguide.ai/models/phi-2)\n  - [Prompt Engineering - Model Collection](https://www.promptingguide.ai/models/collection)\n- [Prompt Engineering - Risks and Misuses](https://www.promptingguide.ai/risks)\n  - [Prompt Engineering - Adversarial Prompting](https://www.promptingguide.ai/risks/adversarial)\n  - [Prompt Engineering - Factuality](https://www.promptingguide.ai/risks/factuality)\n  - [Prompt Engineering - Biases](https://www.promptingguide.ai/risks/biases)\n- [Prompt Engineering - Papers](https://www.promptingguide.ai/papers)\n  - [Prompt Engineering - Overviews](https://www.promptingguide.ai/papers#overviews)\n  - [Prompt Engineering - Approaches](https://www.promptingguide.ai/papers#approaches)\n  - [Prompt Engineering - Applications](https://www.promptingguide.ai/papers#applications)\n  - [Prompt Engineering - Collections](https://www.promptingguide.ai/papers#collections)\n- [Prompt Engineering - Tools](https://www.promptingguide.ai/tools)\n- [Prompt Engineering - Notebooks](https://www.promptingguide.ai/notebooks)\n- [Prompt Engineering - Datasets](https://www.promptingguide.ai/datasets)\n- [Prompt Engineering - Additional Readings](https://www.promptingguide.ai/readings)\n\n\n---\n## Lecture\n\nWe have published a 1 hour lecture that provides a comprehensive overview of prompting techniques, applications, and tools.\n- [Video Lecture](https://youtu.be/dOxUroR57xs)\n- [Notebook with code](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/notebooks/pe-lecture.ipynb)\n- [Slides](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/lecture/Prompt-Engineering-Lecture-Elvis.pdf)\n\n---\n## Running the guide locally\n\nTo run the guide locally, for example to check the correct implementation of a new translation, you will need to:\n\n1. Install Node >=18.0.0\n1. Install `pnpm` if not present in your system. Check [here](https://pnpm.io/installation) for detailed instructions.\n1. Install the dependencies: `pnpm i next react react-dom nextra nextra-theme-docs`\n1. Boot the guide with `pnpm dev`\n2. Browse the guide at `http://localhost:3000/`\n\n---\n## Appearances\nSome places where we have been featured:\n- Wall Street Journal - [ChatGPT Can Give Great Answers. But Only If You Know How to Ask the Right Question](https://www.wsj.com/articles/chatgpt-ask-the-right-question-12d0f035)\n- Forbes - [Mom, Dad, I Want To Be A Prompt Engineer](https://www.forbes.com/sites/craigsmith/2023/04/05/mom-dad-i-want-to-be-a-prompt-engineer/?sh=7f1213159c8e)\n- Markettechpost - [Best Free Prompt Engineering Resources (2023)](https://www.marktechpost.com/2023/04/04/best-free-prompt-engineering-resources-2023/)\n\n\n---\nIf you are using the guide for your work or research, please cite us as follows:\n\n```\n@article{Saravia_Prompt_Engineering_Guide_2022,\nauthor = {Saravia, Elvis},\njournal = {https://github.com/dair-ai/Prompt-Engineering-Guide},\nmonth = {12},\ntitle = {{Prompt Engineering Guide}},\nyear = {2022}\n}\n```\n\n## License\n\n[MIT License](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/LICENSE.md)\n\n\nFeel free to open a PR if you think something is missing here. Always welcome feedback and suggestions. Just open an issue!\n', '{"language":"MDX","stars":67389,"forks":7087,"watchers":67389,"open_issues":231,"topics":["agent","agents","ai-agents","chatgpt","deep-learning","generative-ai","language-model","llms","openai","prompt-engineering","rag"],"default_branch":"main","size_kb":70677,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:dair-ai:Prompt-Engineering-Guide","source_url":"https://github.com/dair-ai/Prompt-Engineering-Guide"},{"type":"has_code","target_id":"github:dair-ai:Prompt-Engineering-Guide","source_url":"https://github.com/dair-ai/Prompt-Engineering-Guide"},{"type":"has_code","target_id":"github:dair-ai:Prompt-Engineering-Guide},","source_url":"https://github.com/dair-ai/Prompt-Engineering-Guide},"},{"type":"has_code","target_id":"github:dair-ai:Prompt-Engineering-Guide","source_url":"https://github.com/dair-ai/Prompt-Engineering-Guide"}]', NULL, 'MIT', 'approved', 80, 'bcd11aa7e7a7cd811e1b6ce06e43d4f7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-dair-ai-Prompt-Engineering-Guide from https://github.com/dair-ai.png
Image converted to WebP: data/images/github-dair-ai-Prompt-Engineering-Guide.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-CorentinJ-Real-Time-Voice-Cloning', 'github--corentinj--real-time-voice-cloning', 'Real-Time-Voice-Cloning', 'CorentinJ', 'This repository is an implementation of Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis (SV2TTS) with a vocoder that works in real-time. This was my master''s thesis. SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text. **Video demonstration** (click...', '["deep-learning","python","pytorch","tensorflow","tts","voice-cloning","python"]', 'other', 58981, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/CorentinJ/Real-Time-Voice-Cloning","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Real-Time Voice Cloning\n\nThis repository is an implementation of [Transfer Learning from Speaker Verification to\nMultispeaker Text-To-Speech Synthesis](https://arxiv.org/pdf/1806.04558.pdf) (SV2TTS) with a vocoder that works in real-time. This was my [master''s thesis](https://matheo.uliege.be/handle/2268.2/6801).\n\nSV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.\n\n**Video demonstration** (click the picture):\n\n[![Toolbox demo](https://i.imgur.com/8lFUlgz.png)](https://www.youtube.com/watch?v=-O_hYhToKoA)\n\n### Papers implemented\n\n| URL                                                    | Designation            | Title                                                                                    | Implementation source                                   |\n| ------------------------------------------------------ | ---------------------- | ---------------------------------------------------------------------------------------- | ------------------------------------------------------- |\n| [**1806.04558**](https://arxiv.org/pdf/1806.04558.pdf) | **SV2TTS**             | **Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis** | This repo                                               |\n| [1802.08435](https://arxiv.org/pdf/1802.08435.pdf)     | WaveRNN (vocoder)      | Efficient Neural Audio Synthesis                                                         | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) |\n| [1703.10135](https://arxiv.org/pdf/1703.10135.pdf)     | Tacotron (synthesizer) | Tacotron: Towards End-to-End Speech Synthesis                                            | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) |\n| [1710.10467](https://arxiv.org/pdf/1710.10467.pdf)     | GE2E (encoder)         | Generalized End-To-End Loss for Speaker Verification                                     | This repo                                               |\n\n## Heads up\n\nLike everything else in Deep Learning, this repo has quickly gotten old. Many SaaS apps (often paying) will give you a better audio quality than this repository will. If you wish for an open-source solution with a high voice quality:\n\n- Check out [paperswithcode](https://paperswithcode.com/task/speech-synthesis/) for other repositories and recent research in the field of speech synthesis.\n- Check out [Chatterbox](https://github.com/resemble-ai/chatterbox) for a similar project up to date with the 2025 SOTA in voice cloning\n\n## Running the toolbox\n\nBoth Windows and Linux are supported.\n1. Install [ffmpeg](https://ffmpeg.org/download.html#get-packages). This is necessary for reading audio files. Check if it''s installed by running in a command line\n```\nffmpeg\n```\n2. Install uv for python package management\n```\n# On Windows:\npowershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"\n# On Linux\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Alternatively, on any platform if you have pip installed you can do\npip install -U uv\n```\n3. Run one of the following commands\n```\n# Run the toolbox if you have an NVIDIA GPU\nuv run --extra cuda demo_toolbox.py\n# Use this if you don''t\nuv run --extra cpu demo_toolbox.py\n\n# Run in command line if you don''t want the GUI\nuv run --extra cuda demo_cli.py\nuv run --extra cpu demo_cli.py\n```\nUv will automatically create a .venv directory for you with an appropriate python environment. [Open an issue](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues) if this fails for you\n\n### (Optional) Download Pretrained Models\n\nPretrained models are now downloaded automatically. If this doesn''t work for you, you can manually download them [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models).\n\n### (Optional) Download Datasets\n\nFor playing with the toolbox alone, I only recommend downloading [`LibriSpeech/train-clean-100`](https://www.openslr.org/resources/12/train-clean-100.tar.gz). Extract the contents as `<datasets_root>/LibriSpeech/train-clean-100` where `<datasets_root>` is a directory of your choosing. Other datasets are supported in the toolbox, see [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets). You''re free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.\n', '{"language":"Python","stars":58981,"forks":9391,"watchers":58981,"open_issues":172,"topics":["deep-learning","python","pytorch","tensorflow","tts","voice-cloning"],"default_branch":"master","size_kb":369680,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:fatchord:WaveRNN","source_url":"https://github.com/fatchord/WaveRNN"},{"type":"has_code","target_id":"github:fatchord:WaveRNN","source_url":"https://github.com/fatchord/WaveRNN"},{"type":"has_code","target_id":"github:resemble-ai:chatterbox","source_url":"https://github.com/resemble-ai/chatterbox"},{"type":"has_code","target_id":"github:CorentinJ:Real-Time-Voice-Cloning","source_url":"https://github.com/CorentinJ/Real-Time-Voice-Cloning"},{"type":"has_code","target_id":"github:CorentinJ:Real-Time-Voice-Cloning","source_url":"https://github.com/CorentinJ/Real-Time-Voice-Cloning"},{"type":"has_code","target_id":"github:CorentinJ:Real-Time-Voice-Cloning","source_url":"https://github.com/CorentinJ/Real-Time-Voice-Cloning"}]', NULL, 'NOASSERTION', 'approved', 65, '072f82d256fbe882b3f3b139e090f96e', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-CorentinJ-Real-Time-Voice-Cloning from https://github.com/CorentinJ.png
Image converted to WebP: data/images/github-CorentinJ-Real-Time-Voice-Cloning.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-coqui-ai-TTS', 'github--coqui-ai--tts', 'TTS', 'coqui-ai', '- ğŸ“£ â“TTSv2 is here with 16 languages and better performance across the board. - ğŸ“£ â“TTS fine-tuning code is out. Check the example recipes. - ğŸ“£ â“TTS can now stream with <200ms latency. - ğŸ“£ â“TTS, our production TTS model that can speak 13 languages, is released Blog Post, Demo, Docs - ğŸ“£ ğŸ¶Bark is now available for inference with unconstrained voice cloning. Docs - ğŸ“£ You can use ~1100 Fairseq models with ğŸ¸TTS. - ğŸ“£ ğŸ¸TTS now supports ğŸ¢Tortoise with faster inference. Docs <div align="cent...', '["deep-learning","glow-tts","hifigan","melgan","multi-speaker-tts","python","pytorch","speaker-encoder","speaker-encodings","speech","speech-synthesis","tacotron","text-to-speech","tts","tts-model","vocoder","voice-cloning","voice-conversion","voice-synthesis","python"]', 'other', 43766, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/coqui-ai/TTS","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '\n## ğŸ¸Coqui.ai News\n- ğŸ“£ â“TTSv2 is here with 16 languages and better performance across the board.\n- ğŸ“£ â“TTS fine-tuning code is out. Check the [example recipes](https://github.com/coqui-ai/TTS/tree/dev/recipes/ljspeech).\n- ğŸ“£ â“TTS can now stream with <200ms latency.\n- ğŸ“£ â“TTS, our production TTS model that can speak 13 languages, is released [Blog Post](https://coqui.ai/blog/tts/open_xtts), [Demo](https://huggingface.co/spaces/coqui/xtts), [Docs](https://tts.readthedocs.io/en/dev/models/xtts.html)\n- ğŸ“£ [ğŸ¶Bark](https://github.com/suno-ai/bark) is now available for inference with unconstrained voice cloning. [Docs](https://tts.readthedocs.io/en/dev/models/bark.html)\n- ğŸ“£ You can use [~1100 Fairseq models](https://github.com/facebookresearch/fairseq/tree/main/examples/mms) with ğŸ¸TTS.\n- ğŸ“£ ğŸ¸TTS now supports ğŸ¢Tortoise with faster inference. [Docs](https://tts.readthedocs.io/en/dev/models/tortoise.html)\n\n<div align="center">\n<img src="https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2" />\n\n## <img src="https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png" height="56"/>\n\n\n**ğŸ¸TTS is a library for advanced Text-to-Speech generation.**\n\nğŸš€ Pretrained models in +1100 languages.\n\nğŸ› ï¸ Tools for training new models and fine-tuning existing models in any language.\n\nğŸ“š Utilities for dataset analysis and curation.\n______________________________________________________________________\n\n[![Discord](https://img.shields.io/discord/1037326658807533628?color=%239B59B6&label=chat%20on%20discord)](https://discord.gg/5eXr5seRrv)\n[![License](<https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg>)](https://opensource.org/licenses/MPL-2.0)\n[![PyPI version](https://badge.fury.io/py/TTS.svg)](https://badge.fury.io/py/TTS)\n[![Covenant](https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667)](https://github.com/coqui-ai/TTS/blob/master/CODE_OF_CONDUCT.md)\n[![Downloads](https://pepy.tech/badge/tts)](https://pepy.tech/project/tts)\n[![DOI](https://zenodo.org/badge/265612440.svg)](https://zenodo.org/badge/latestdoi/265612440)\n\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/aux_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/style_check.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/text_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/tts_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/vocoder_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests0.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests1.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests2.yml/badge.svg)\n[![Docs](<https://readthedocs.org/projects/tts/badge/?version=latest&style=plastic>)](https://tts.readthedocs.io/en/latest/)\n\n</div>\n\n______________________________________________________________________\n\n## ğŸ’¬ Where to ask questions\nPlease use our dedicated channels for questions and discussion. Help is much more valuable if it''s shared publicly so that more people can benefit from it.\n\n| Type                            | Platforms                               |\n| ------------------------------- | --------------------------------------- |\n| ğŸš¨ **Bug Reports**              | [GitHub Issue Tracker]                  |\n| ğŸ **Feature Requests & Ideas** | [GitHub Issue Tracker]                  |\n| ğŸ‘©â€ğŸ’» **Usage Questions**          | [GitHub Discussions]                    |\n| ğŸ—¯ **General Discussion**       | [GitHub Discussions] or [Discord]   |\n\n[github issue tracker]: https://github.com/coqui-ai/tts/issues\n[github discussions]: https://github.com/coqui-ai/TTS/discussions\n[discord]: https://discord.gg/5eXr5seRrv\n[Tutorials and Examples]: https://github.com/coqui-ai/TTS/wiki/TTS-Notebooks-and-Tutorials\n\n\n## ğŸ”— Links and Resources\n| Type                            | Links                               |\n| ------------------------------- | --------------------------------------- |\n| ğŸ’¼ **Documentation**              | [ReadTheDocs](https://tts.readthedocs.io/en/latest/)\n| ğŸ’¾ **Installation**               | [TTS/README.md](https://github.com/coqui-ai/TTS/tree/dev#installation)|\n| ğŸ‘©â€ğŸ’» **Contributing**               | [CONTRIBUTING.md](https://github.com/coqui-ai/TTS/blob/main/CONTRIBUTING.md)|\n| ğŸ“Œ **Road Map**                   | [Main Development Plans](https://github.com/coqui-ai/TTS/issues/378)\n| ğŸš€ **Released Models**            | [TTS Releases](https://github.com/coqui-ai/TTS/releases) and [Experimental Models](https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models)|\n| ğŸ“° **Papers**                    | [TTS Papers](https://github.com/erogol/TTS-papers)|\n\n\n## ğŸ¥‡ TTS Performance\n<p align="center"><img src="https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png" width="800" /></p>\n\nUnderlined "TTS*" and "Judy*" are **internal** ğŸ¸TTS models that are not released open-source. They are here to show the potential. Models prefixed with a dot (.Jofish .Abe and .Janice) are real human voices.\n\n## Features\n- High-performance Deep Learning models for Text2Speech tasks.\n    - Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).\n    - Speaker Encoder to compute speaker embeddings efficiently.\n    - Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)\n- Fast and efficient model training.\n- Detailed training logs on the terminal and Tensorboard.\n- Support for Multi-speaker TTS.\n- Efficient, flexible, lightweight but feature complete `Trainer API`.\n- Released and ready-to-use models.\n- Tools to curate Text2Speech datasets under```dataset_analysis```.\n- Utilities to use and test your models.\n- Modular (but not too much) code base enabling easy implementation of new ideas.\n\n## Model Implementations\n### Spectrogram models\n- Tacotron: [paper](https://arxiv.org/abs/1703.10135)\n- Tacotron2: [paper](https://arxiv.org/abs/1712.05884)\n- Glow-TTS: [paper](https://arxiv.org/abs/2005.11129)\n- Speedy-Speech: [paper](https://arxiv.org/abs/2008.03802)\n- Align-TTS: [paper](https://arxiv.org/abs/2003.01950)\n- FastPitch: [paper](https://arxiv.org/pdf/2006.06873.pdf)\n- FastSpeech: [paper](https://arxiv.org/abs/1905.09263)\n- FastSpeech2: [paper](https://arxiv.org/abs/2006.04558)\n- SC-GlowTTS: [paper](https://arxiv.org/abs/2104.05557)\n- Capacitron: [paper](https://arxiv.org/abs/1906.03402)\n- OverFlow: [paper](https://arxiv.org/abs/2211.06892)\n- Neural HMM TTS: [paper](https://arxiv.org/abs/2108.13320)\n- Delightful TTS: [paper](https://arxiv.org/abs/2110.12612)\n\n### End-to-End Models\n- â“TTS: [blog](https://coqui.ai/blog/tts/open_xtts)\n- VITS: [paper](https://arxiv.org/pdf/2106.06103)\n- ğŸ¸ YourTTS: [paper](https://arxiv.org/abs/2112.02418)\n- ğŸ¢ Tortoise: [orig. repo](https://github.com/neonbjb/tortoise-tts)\n- ğŸ¶ Bark: [orig. repo](https://github.com/suno-ai/bark)\n\n### Attention Methods\n- Guided Attention: [paper](https://arxiv.org/abs/1710.08969)\n- Forward Backward Decoding: [paper](https://arxiv.org/abs/1907.09006)\n- Graves Attention: [paper](https://arxiv.org/abs/1910.10288)\n- Double Decoder Consistency: [blog](https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/)\n- Dynamic Convolutional Attention: [paper](https://arxiv.org/pdf/1910.10288.pdf)\n- Alignment Network: [paper](https://arxiv.org/abs/2108.10447)\n\n### Speaker Encoder\n- GE2E: [paper](https://arxiv.org/abs/1710.10467)\n- Angular Loss: [paper](https://arxiv.org/pdf/2003.11982.pdf)\n\n### Vocoders\n- MelGAN: [paper](https://arxiv.org/abs/1910.06711)\n- MultiBandMelGAN: [paper](https://arxiv.org/abs/2005.05106)\n- ParallelWaveGAN: [paper](https://arxiv.org/abs/1910.11480)\n- GAN-TTS discriminators: [paper](https://arxiv.org/abs/1909.11646)\n- WaveRNN: [origin](https://github.com/fatchord/WaveRNN/)\n- WaveGrad: [paper](https://arxiv.org/abs/2009.00713)\n- HiFiGAN: [paper](https://arxiv.org/abs/2010.05646)\n- UnivNet: [paper](https://arxiv.org/abs/2106.07889)\n\n### Voice Conversion\n- FreeVC: [paper](https://arxiv.org/abs/2210.15418)\n\nYou can also help us implement more models.\n\n## Installation\nğŸ¸TTS is tested on Ubuntu 18.04 with **python >= 3.9, < 3.12.**.\n\nIf you are only interested in [synthesizing speech](https://tts.readthedocs.io/en/latest/inference.html) with the released ğŸ¸TTS models, installing from PyPI is the easiest option.\n\n```bash\npip install TTS\n```\n\nIf you plan to code or train models, clone ğŸ¸TTS and install it locally.\n\n```bash\ngit clone https://github.com/coqui-ai/TTS\npip install -e .[all,dev,notebooks]  # Select the relevant extras\n```\n\nIf you are on Ubuntu (Debian), you can also run following commands for installation.\n\n```bash\n$ make system-deps  # intended to be used on Ubuntu (Debian). Let us know if you have a different OS.\n$ make install\n```\n\nIf you are on Windows, ğŸ‘‘@GuyPaddock wrote installation instructions [here](https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system).\n\n\n## Docker Image\nYou can also try TTS without install with the docker image.\nSimply run the following command and you will be able to run TTS without installing it.\n\n```bash\ndocker run --rm -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu\npython3 TTS/server/server.py --list_models #To get the list of available models\npython3 TTS/server/server.py --model_name tts_models/en/vctk/vits # To start a server\n```\n\nYou can then enjoy the TTS server [here](http://[::1]:5002/)\nMore details about the docker images (like GPU support) can be found [here](https://tts.readthedocs.io/en/latest/docker_images.html)\n\n\n## Synthesizing speech by ğŸ¸TTS\n\n### ğŸ Python API\n\n#### Running a multi-speaker and multi-lingual model\n\n```python\nimport torch\nfrom TTS.api import TTS\n\n# Get device\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\n\n# List available ğŸ¸TTS models\nprint(TTS().list_models())\n\n# Init TTS\ntts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(device)\n\n# Run TTS\n# â— Since this model is multi-lingual voice cloning model, we must set the target speaker_wav and language\n# Text to speech list of amplitude values as output\nwav = tts.tts(text="Hello world!", speaker_wav="my/cloning/audio.wav", language="en")\n# Text to speech to a file\ntts.tts_to_file(text="Hello world!", speaker_wav="my/cloning/audio.wav", language="en", file_path="output.wav")\n```\n\n#### Running a single speaker model\n\n```python\n# Init TTS with the target model name\ntts = TTS(model_name="tts_models/de/thorsten/tacotron2-DDC", progress_bar=False).to(device)\n\n# Run TTS\ntts.tts_to_file(text="Ich bin eine Testnachricht.", file_path=OUTPUT_PATH)\n\n# Example voice cloning with YourTTS in English, French and Portuguese\ntts = TTS(model_name="tts_models/multilingual/multi-dataset/your_tts", progress_bar=False).to(device)\ntts.tts_to_file("This is voice cloning.", speaker_wav="my/cloning/audio.wav", language="en", file_path="output.wav")\ntts.tts_to_file("C''est le clonage de la voix.", speaker_wav="my/cloning/audio.wav", language="fr-fr", file_path="output.wav")\ntts.tts_to_file("Isso Ã© clonagem de voz.", speaker_wav="my/cloning/audio.wav", language="pt-br", file_path="output.wav")\n```\n\n#### Example voice conversion\n\nConverting the voice in `source_wav` to the voice of `target_wav`\n\n```python\ntts = TTS(model_name="voice_conversion_models/multilingual/vctk/freevc24", progress_bar=False).to("cuda")\ntts.voice_conversion_to_file(source_wav="my/source.wav", target_wav="my/target.wav", file_path="output.wav")\n```\n\n#### Example voice cloning together with the voice conversion model.\nThis way, you can clone voices by using any model in ğŸ¸TTS.\n\n```python\n\ntts = TTS("tts_models/de/thorsten/tacotron2-DDC")\ntts.tts_with_vc_to_file(\n    "Wie sage ich auf Italienisch, dass ich dich liebe?",\n    speaker_wav="target/speaker.wav",\n    file_path="output.wav"\n)\n```\n\n#### Example text to speech using **Fairseq models in ~1100 languages** ğŸ¤¯.\nFor Fairseq models, use the following name format: `tts_models/<lang-iso_code>/fairseq/vits`.\nYou can find the language ISO codes [here](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)\nand learn about the Fairseq models [here](https://github.com/facebookresearch/fairseq/tree/main/examples/mms).\n\n```python\n# TTS with on the fly voice conversion\napi = TTS("tts_models/deu/fairseq/vits")\napi.tts_with_vc_to_file(\n    "Wie sage ich auf Italienisch, dass ich dich liebe?",\n    speaker_wav="target/speaker.wav",\n    file_path="output.wav"\n)\n```\n\n### Command-line `tts`\n\n<!-- begin-tts-readme -->\n\nSynthesize speech on command line.\n\nYou can either use your trained model or choose a model from the provided list.\n\nIf you don''t specify any models, then it uses LJSpeech based English model.\n\n#### Single Speaker Models\n\n- List provided models:\n\n  ```\n  $ tts --list_models\n  ```\n\n- Get model info (for both tts_models and vocoder_models):\n\n  - Query by type/name:\n    The model_info_by_name uses the name as it from the --list_models.\n    ```\n    $ tts --model_info_by_name "<model_type>/<language>/<dataset>/<model_name>"\n    ```\n    For example:\n    ```\n    $ tts --model_info_by_name tts_models/tr/common-voice/glow-tts\n    $ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2\n    ```\n  - Query by type/idx:\n    The model_query_idx uses the corresponding idx from --list_models.\n\n    ```\n    $ tts --model_info_by_idx "<model_type>/<model_query_idx>"\n    ```\n\n    For example:\n\n    ```\n    $ tts --model_info_by_idx tts_models/3\n    ```\n\n  - Query info for model info by full name:\n    ```\n    $ tts --model_info_by_name "<model_type>/<language>/<dataset>/<model_name>"\n    ```\n\n- Run TTS with default models:\n\n  ```\n  $ tts --text "Text for TTS" --out_path output/path/speech.wav\n  ```\n\n- Run TTS and pipe out the generated TTS wav file data:\n\n  ```\n  $ tts --text "Text for TTS" --pipe_out --out_path output/path/speech.wav | aplay\n  ```\n\n- Run a TTS model with its default vocoder model:\n\n  ```\n  $ tts --text "Text for TTS" --model_name "<model_type>/<language>/<dataset>/<model_name>" --out_path output/path/speech.wav\n  ```\n\n  For example:\n\n  ```\n  $ tts --text "Text for TTS" --model_name "tts_models/en/ljspeech/glow-tts" --out_path output/path/speech.wav\n  ```\n\n- Run with specific TTS and vocoder models from the list:\n\n  ```\n  $ tts --text "Text for TTS" --model_name "<model_type>/<language>/<dataset>/<model_name>" --vocoder_name "<model_type>/<language>/<dataset>/<model_name>" --out_path output/path/speech.wav\n  ```\n\n  For example:\n\n  ```\n  $ tts --text "Text for TTS" --model_name "tts_models/en/ljspeech/glow-tts" --vocoder_name "vocoder_models/en/ljspeech/univnet" --out_path output/path/speech.wav\n  ```\n\n- Run your own TTS model (Using Griffin-Lim Vocoder):\n\n  ```\n  $ tts --text "Text for TTS" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n  ```\n\n- Run your own TTS and Vocoder models:\n\n  ```\n  $ tts --text "Text for TTS" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n      --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json\n  ```\n\n#### Multi-speaker Models\n\n- List the available speakers and choose a <speaker_id> among them:\n\n  ```\n  $ tts --model_name "<language>/<dataset>/<model_name>"  --list_speaker_idxs\n  ```\n\n- Run the multi-speaker TTS model with the target speaker ID:\n\n  ```\n  $ tts --text "Text for TTS." --out_path output/path/speech.wav --model_name "<language>/<dataset>/<model_name>"  --speaker_idx <speaker_id>\n  ```\n\n- Run your own multi-speaker TTS model:\n\n  ```\n  $ tts --text "Text for TTS" --out_path output/path/speech.wav --model_path path/to/model.pth --config_path path/to/config.json --speakers_file_path path/to/speaker.json --speaker_idx <speaker_id>\n  ```\n\n### Voice Conversion Models\n\n```\n$ tts --out_path output/path/speech.wav --model_name "<language>/<dataset>/<model_name>" --source_wav <path/to/speaker/wav> --target_wav <path/to/reference/wav>\n```\n\n<!-- end-tts-readme -->\n\n## Directory Structure\n```\n|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)\n|- utils/           (common utilities.)\n|- TTS\n    |- bin/             (folder for all the executables.)\n      |- train*.py                  (train your target model.)\n      |- ...\n    |- tts/             (text to speech models)\n        |- layers/          (model layer definitions)\n        |- models/          (model definitions)\n        |- utils/           (model specific utilities.)\n    |- speaker_encoder/ (Speaker Encoder models.)\n        |- (same)\n    |- vocoder/         (Vocoder models.)\n        |- (same)\n```\n', '{"language":"Python","stars":43766,"forks":5836,"watchers":43766,"open_issues":9,"topics":["deep-learning","glow-tts","hifigan","melgan","multi-speaker-tts","python","pytorch","speaker-encoder","speaker-encodings","speech","speech-synthesis","tacotron","text-to-speech","tts","tts-model","vocoder","voice-cloning","voice-conversion","voice-synthesis"],"default_branch":"dev","size_kb":170196,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:suno-ai:bark","source_url":"https://github.com/suno-ai/bark"},{"type":"has_code","target_id":"github:facebookresearch:fairseq","source_url":"https://github.com/facebookresearch/fairseq"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:tts","source_url":"https://github.com/coqui-ai/tts"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:erogol:TTS-papers","source_url":"https://github.com/erogol/TTS-papers"},{"type":"has_code","target_id":"github:neonbjb:tortoise-tts","source_url":"https://github.com/neonbjb/tortoise-tts"},{"type":"has_code","target_id":"github:suno-ai:bark","source_url":"https://github.com/suno-ai/bark"},{"type":"has_code","target_id":"github:fatchord:WaveRNN","source_url":"https://github.com/fatchord/WaveRNN"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:facebookresearch:fairseq","source_url":"https://github.com/facebookresearch/fairseq"}]', NULL, 'MPL-2.0', 'approved', 80, '5747c748ef83dceaa79bf0b85569e273', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-coqui-ai-TTS from https://github.com/coqui-ai.png
Image converted to WebP: data/images/github-coqui-ai-TTS.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-hpcaitech-ColossalAI', 'github--hpcaitech--colossalai', 'ColossalAI', 'hpcaitech', '<div id="top" align="center"> Colossal-AI: Making large AI models cheaper, faster, and more accessible <h3> <a href="https://arxiv.org/abs/2110.14883"> Paper </a> | <a href="https://www.colossalai.org/"> Documentation </a> | <a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples"> Examples </a> | <a href="https://github.com/hpcaitech/ColossalAI/discussions"> Forum </a> | <a href="https://colossalai.org/zh-Hans/docs/get_started/bonus/">GPU Cloud Playground </a> | <a href="https://...', '["ai","big-model","data-parallelism","deep-learning","distributed-computing","foundation-models","heterogeneous-training","hpc","inference","large-scale","model-parallelism","pipeline-parallelism","python"]', 'other', 41289, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/hpcaitech/ColossalAI","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# Colossal-AI\n<div id="top" align="center">\n\n   [![logo](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/colossal-ai_logo_vertical.png)](https://www.colossalai.org/)\n\n   Colossal-AI: Making large AI models cheaper, faster, and more accessible\n\n   <h3> <a href="https://arxiv.org/abs/2110.14883"> Paper </a> |\n   <a href="https://www.colossalai.org/"> Documentation </a> |\n   <a href="https://github.com/hpcaitech/ColossalAI/tree/main/examples"> Examples </a> |\n   <a href="https://github.com/hpcaitech/ColossalAI/discussions"> Forum </a> |\n   <a href="https://colossalai.org/zh-Hans/docs/get_started/bonus/">GPU Cloud Playground </a> |\n   <a href="https://hpc-ai.com/blog"> Blog </a></h3>\n\n   [![GitHub Repo stars](https://img.shields.io/github/stars/hpcaitech/ColossalAI?style=social)](https://github.com/hpcaitech/ColossalAI/stargazers)\n   [![Build](https://github.com/hpcaitech/ColossalAI/actions/workflows/build_on_schedule.yml/badge.svg)](https://github.com/hpcaitech/ColossalAI/actions/workflows/build_on_schedule.yml)\n   [![Documentation](https://readthedocs.org/projects/colossalai/badge/?version=latest)](https://colossalai.readthedocs.io/en/latest/?badge=latest)\n   [![CodeFactor](https://www.codefactor.io/repository/github/hpcaitech/colossalai/badge)](https://www.codefactor.io/repository/github/hpcaitech/colossalai)\n   [![HuggingFace badge](https://img.shields.io/badge/%F0%9F%A4%97HuggingFace-Join-yellow)](https://huggingface.co/hpcai-tech)\n   [![slack badge](https://img.shields.io/badge/Slack-join-blueviolet?logo=slack&amp)](https://github.com/hpcaitech/public_assets/tree/main/colossalai/contact/slack)\n   [![WeChat badge](https://img.shields.io/badge/å¾®ä¿¡-åŠ å…¥-green?logo=wechat&amp)](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png)\n\n\n   | [English](README.md) | [ä¸­æ–‡](docs/README-zh-Hans.md) |\n\n</div>\n\n## Instantly Run Colossal-AI on Enterprise-Grade GPUs\n\nSkip the setup. Access a powerful, pre-configured Colossal-AI environment on [**HPC-AI Cloud**](https://hpc-ai.com/?utm_source=github&utm_medium=social&utm_campaign=promotion-colossalai).\n\nTrain your models and scale your AI workload in one click!\n\n* **NVIDIA Blackwell B200s**: Experience the next generation of AI performance ([See Benchmarks](https://hpc-ai.com/blog/b200)). Now available on cloud from **$2.47/hr**.\n* **Cost-Effective H200 Cluster**: Get premier performance with on-demand rental from just **$1.99/hr**.\n\n[**Get Started Now & Claim Your Free Credits â†’**](https://hpc-ai.com/?utm_source=github&utm_medium=social&utm_campaign=promotion-colossalai)\n\n<div align="center">\n   <a href="https://hpc-ai.com/?utm_source=github&utm_medium=social&utm_campaign=promotion-colossalai">\n   <img src="https://github.com/hpcaitech/public_assets/blob/main/colossalai/img/2-3.png" width="850" />\n   </a>\n</div>\n\n### Colossal-AI Benchmark\n\nTo see how these performance gains translate to real-world applications, we conducted a large language model training benchmark using Colossal-AI on Llama-like models. The tests were run on both 8-card and 16-card configurations for 7B and 70B models, respectively.\n\n|              GPU              |  GPUs  | Model Size |    Parallelism    | Batch Size per DP | Seqlen | Throughput | TFLOPS/GPU  | Peak Mem(MiB)  |\n| :-----------------------------: | :--------: | :-------------: | :------------------: | :-----------: | :--------------: | :-------------: | :-------------: | :-------------: |\n|         H200            |     8     |      7B       |   zero2(dp8)     | 36 |        4096     |       17.13 samp/s     |       534.18     |       119040.02     |\n|         H200            |     16     |      70B       |   zero2     | 48 |        4096     |       3.27 samp/s     |       469.1     |       150032.23     |\n|         B200            |     8     |      7B       |   zero1(dp2)+tp2+pp4     | 128 |        4096     |       25.83 samp/s     |       805.69     |       100119.77     |\n|         H200            |     16     |      70B       |   zero1(dp2)+tp2+pp4     | 128 |        4096     |       5.66 samp/s     |       811.79     |       100072.02     |\n\nThe results from the Colossal-AI benchmark provide the most practical insight. For the 7B model on 8 cards, the **B200 achieved a 50% higher throughput** and a significant increase in TFLOPS per GPU. For the 70B model on 16 cards, the B200 again demonstrated a clear advantage, with **over 70% higher throughput and TFLOPS per GPU**. These numbers show that the B200''s performance gains translate directly to faster training times for large-scale models.\n\n## Latest News\n* [2025/02] [DeepSeek 671B Fine-Tuning Guide Revealedâ€”Unlock the Upgraded DeepSeek Suite with One Click, AI Players Ecstatic!](https://company.hpc-ai.com/blog/shocking-release-deepseek-671b-fine-tuning-guide-revealed-unlock-the-upgraded-deepseek-suite-with-one-click-ai-players-ecstatic)\n* [2024/12] [The development cost of video generation models has saved by 50%! Open-source solutions are now available with H200 GPU vouchers](https://company.hpc-ai.com/blog/the-development-cost-of-video-generation-models-has-saved-by-50-open-source-solutions-are-now-available-with-h200-gpu-vouchers) [[code]](https://github.com/hpcaitech/Open-Sora/blob/main/scripts/train.py) [[vouchers]](https://colossalai.org/zh-Hans/docs/get_started/bonus/)\n* [2024/10] [How to build a low-cost Sora-like app? Solutions for you](https://company.hpc-ai.com/blog/how-to-build-a-low-cost-sora-like-app-solutions-for-you)\n* [2024/09] [Singapore Startup HPC-AI Tech Secures 50 Million USD in Series A Funding to Build the Video Generation AI Model and GPU Platform](https://company.hpc-ai.com/blog/singapore-startup-hpc-ai-tech-secures-50-million-usd-in-series-a-funding-to-build-the-video-generation-ai-model-and-gpu-platform)\n* [2024/09] [Reducing AI Large Model Training Costs by 30% Requires Just a Single Line of Code From FP8 Mixed Precision Training Upgrades](https://company.hpc-ai.com/blog/reducing-ai-large-model-training-costs-by-30-requires-just-a-single-line-of-code-from-fp8-mixed-precision-training-upgrades)\n* [2024/06] [Open-Sora Continues Open Source: Generate Any 16-Second 720p HD Video with One Click, Model Weights Ready to Use](https://hpc-ai.com/blog/open-sora-from-hpc-ai-tech-team-continues-open-source-generate-any-16-second-720p-hd-video-with-one-click-model-weights-ready-to-use)\n* [2024/05] [Large AI Models Inference Speed Doubled, Colossal-Inference Open Source Release](https://hpc-ai.com/blog/colossal-inference)\n* [2024/04] [Open-Sora Unveils Major Upgrade: Embracing Open Source with Single-Shot 16-Second Video Generation and 720p Resolution](https://hpc-ai.com/blog/open-soras-comprehensive-upgrade-unveiled-embracing-16-second-video-generation-and-720p-resolution-in-open-source)\n* [2024/04] [Most cost-effective solutions for inference, fine-tuning and pretraining, tailored to LLaMA3 series](https://hpc-ai.com/blog/most-cost-effective-solutions-for-inference-fine-tuning-and-pretraining-tailored-to-llama3-series)\n\n## Table of Contents\n<ul>\n <li><a href="#Why-Colossal-AI">Why Colossal-AI</a> </li>\n <li><a href="#Features">Features</a> </li>\n <li>\n   <a href="#Colossal-AI-in-the-Real-World">Colossal-AI for Real World Applications</a>\n   <ul>\n     <li><a href="#Open-Sora">Open-Sora: Revealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models</a></li>\n     <li><a href="#Colossal-LLaMA-2">Colossal-LLaMA-2: One Half-Day of Training Using a Few Hundred Dollars Yields Similar Results to Mainstream Large Models, Open-Source and Commercial-Free Domain-Specific Llm Solution</a></li>\n     <li><a href="#ColossalChat">ColossalChat: An Open-Source Solution for Cloning ChatGPT With a Complete RLHF Pipeline</a></li>\n     <li><a href="#AIGC">AIGC: Acceleration of Stable Diffusion</a></li>\n     <li><a href="#Biomedicine">Biomedicine: Acceleration of AlphaFold Protein Structure</a></li>\n   </ul>\n </li>\n <li>\n   <a href="#Parallel-Training-Demo">Parallel Training Demo</a>\n   <ul>\n     <li><a href="#LLaMA3">LLaMA 1/2/3 </a></li>\n     <li><a href="#MoE">MoE</a></li>\n     <li><a href="#GPT-3">GPT-3</a></li>\n     <li><a href="#GPT-2">GPT-2</a></li>\n     <li><a href="#BERT">BERT</a></li>\n     <li><a href="#PaLM">PaLM</a></li>\n     <li><a href="#OPT">OPT</a></li>\n     <li><a href="#ViT">ViT</a></li>\n     <li><a href="#Recommendation-System-Models">Recommendation System Models</a></li>\n   </ul>\n </li>\n <li>\n   <a href="#Single-GPU-Training-Demo">Single GPU Training Demo</a>\n   <ul>\n     <li><a href="#GPT-2-Single">GPT-2</a></li>\n     <li><a href="#PaLM-Single">PaLM</a></li>\n   </ul>\n </li>\n <li>\n   <a href="#Inference">Inference</a>\n   <ul>\n     <li><a href="#Colossal-Inference">Colossal-Inference: Large AI  Models Inference Speed Doubled</a></li>\n     <li><a href="#Grok-1">Grok-1: 314B model of PyTorch + HuggingFace Inference</a></li>\n     <li><a href="#SwiftInfer">SwiftInfer:Breaks the Length Limit of LLM for Multi-Round Conversations with 46% Acceleration</a></li>\n   </ul>\n </li>\n <li>\n   <a href="#Installation">Installation</a>\n   <ul>\n     <li><a href="#PyPI">PyPI</a></li>\n     <li><a href="#Install-From-Source">Install From Source</a></li>\n   </ul>\n </li>\n <li><a href="#Use-Docker">Use Docker</a></li>\n <li><a href="#Community">Community</a></li>\n <li><a href="#Contributing">Contributing</a></li>\n <li><a href="#Cite-Us">Cite Us</a></li>\n</ul>\n\n## Why Colossal-AI\n<div align="center">\n   <a href="https://youtu.be/KnXSfjqkKN0">\n   <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/JamesDemmel_Colossal-AI.png" width="600" />\n   </a>\n\n   Prof. James Demmel (UC Berkeley): Colossal-AI makes training AI models efficient, easy, and scalable.\n</div>\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n## Features\n\nColossal-AI provides a collection of parallel components for you. We aim to support you to write your\ndistributed deep learning models just like how you write your model on your laptop. We provide user-friendly tools to kickstart\ndistributed training and inference in a few lines.\n\n- Parallelism strategies\n  - Data Parallelism\n  - Pipeline Parallelism\n  - 1D, [2D](https://arxiv.org/abs/2104.05343), [2.5D](https://arxiv.org/abs/2105.14500), [3D](https://arxiv.org/abs/2105.14450) Tensor Parallelism\n  - [Sequence Parallelism](https://arxiv.org/abs/2105.13120)\n  - [Zero Redundancy Optimizer (ZeRO)](https://arxiv.org/abs/1910.02054)\n  - [Auto-Parallelism](https://arxiv.org/abs/2302.02599)\n\n- Heterogeneous Memory Management\n  - [PatrickStar](https://arxiv.org/abs/2108.05818)\n\n- Friendly Usage\n  - Parallelism based on the configuration file\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n## Colossal-AI in the Real World\n### Open-Sora\n\n[Open-Sora](https://github.com/hpcaitech/Open-Sora)ï¼šRevealing Complete Model Parameters, Training Details, and Everything for Sora-like Video Generation Models\n[[code]](https://github.com/hpcaitech/Open-Sora)\n[[blog]](https://hpc-ai.com/blog/open-sora-from-hpc-ai-tech-team-continues-open-source-generate-any-16-second-720p-hd-video-with-one-click-model-weights-ready-to-use)\n[[Model weights]](https://github.com/hpcaitech/Open-Sora?tab=readme-ov-file#model-weights)\n[[Demo]](https://github.com/hpcaitech/Open-Sora?tab=readme-ov-file#-latest-demo)\n[[GPU Cloud Playground]](https://cloud.luchentech.com/)\n[[OpenSora Image]](https://cloud.luchentech.com/doc/docs/image/open-sora/)\n\n<div align="center">\n   <a href="https://youtu.be/ilMQpU71ddI?si=J4JSPzZ03ycYmlki">\n   <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/sora/opensora-v1.2.png" width="700" />\n   </a>\n</div>\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n### Colossal-LLaMA-2\n\n[[GPU Cloud Playground]](https://cloud.luchentech.com/)\n[[LLaMA3 Image]](https://cloud.luchentech.com/doc/docs/image/llama)\n\n- 7B: One half-day of training using a few hundred dollars yields similar results to mainstream large models, open-source and commercial-free domain-specific LLM solution.\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Colossal-LLaMA-2)\n[[blog]](https://www.hpc-ai.tech/blog/one-half-day-of-training-using-a-few-hundred-dollars-yields-similar-results-to-mainstream-large-models-open-source-and-commercial-free-domain-specific-llm-solution)\n[[HuggingFace model weights]](https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-7b-base)\n[[Modelscope model weights]](https://www.modelscope.cn/models/colossalai/Colossal-LLaMA-2-7b-base/summary)\n\n- 13B: Construct refined 13B private model with just $5000 USD.\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Colossal-LLaMA-2)\n[[blog]](https://hpc-ai.com/blog/colossal-llama-2-13b)\n[[HuggingFace model weights]](https://huggingface.co/hpcai-tech/Colossal-LLaMA-2-13b-base)\n[[Modelscope model weights]](https://www.modelscope.cn/models/colossalai/Colossal-LLaMA-2-13b-base/summary)\n\n|              Model              |  Backbone  | Tokens Consumed |     MMLU (5-shot)    | CMMLU (5-shot)| AGIEval (5-shot) | GAOKAO (0-shot) | CEval (5-shot)  |\n| :-----------------------------: | :--------: | :-------------: | :------------------: | :-----------: | :--------------: | :-------------: | :-------------: |\n|          Baichuan-7B            |     -      |      1.2T       |    42.32 (42.30)     | 44.53 (44.02) |        38.72     |       36.74     |       42.80     |\n|       Baichuan-13B-Base         |     -      |      1.4T       |    50.51 (51.60)     | 55.73 (55.30) |        47.20     |       51.41     |       53.60     |\n|       Baichuan2-7B-Base         |     -      |      2.6T       |    46.97 (54.16)     | 57.67 (57.07) |        45.76     |       52.60     |       54.00     |\n|       Baichuan2-13B-Base        |     -      |      2.6T       |    54.84 (59.17)     | 62.62 (61.97) |        52.08     |       58.25     |       58.10     |\n|           ChatGLM-6B            |     -      |      1.0T       |    39.67 (40.63)     |   41.17 (-)   |        40.10     |       36.53     |       38.90     |\n|          ChatGLM2-6B            |     -      |      1.4T       |    44.74 (45.46)     |   49.40 (-)   |        46.36     |       45.49     |       51.70     |\n|          InternLM-7B            |     -      |      1.6T       |    46.70 (51.00)     |   52.00 (-)   |        44.77     |       61.64     |       52.80     |\n|            Qwen-7B              |     -      |      2.2T       |    54.29 (56.70)     | 56.03 (58.80) |        52.47     |       56.42     |       59.60     |\n|           Llama-2-7B            |     -      |      2.0T       |    44.47 (45.30)     |   32.97 (-)   |        32.60     |       25.46     |         -       |\n| Linly-AI/Chinese-LLaMA-2-7B-hf  | Llama-2-7B |      1.0T       |        37.43         |     29.92     |        32.00     |       27.57     |         -       |\n| wenge-research/yayi-7b-llama2   | Llama-2-7B |        -        |        38.56         |     31.52     |        30.99     |       25.95     |         -       |\n| ziqingyang/chinese-llama-2-7b   | Llama-2-7B |        -        |        33.86         |     34.69     |        34.52     |       25.18     |        34.2     |\n| TigerResearch/tigerbot-7b-base  | Llama-2-7B |      0.3T       |        43.73         |     42.04     |        37.64     |       30.61     |         -       |\n|  LinkSoul/Chinese-Llama-2-7b    | Llama-2-7B |        -        |        48.41         |     38.31     |        38.45     |       27.72     |         -       |\n|       FlagAlpha/Atom-7B         | Llama-2-7B |      0.1T       |        49.96         |     41.10     |        39.83     |       33.00     |         -       |\n| IDEA-CCNL/Ziya-LLaMA-13B-v1.1   | Llama-13B  |      0.11T      |        50.25         |     40.99     |        40.04     |       30.54     |         -       |\n|  **Colossal-LLaMA-2-7b-base**   | Llama-2-7B |   **0.0085T**   |        53.06         |     49.89     |        51.48     |       58.82     |        50.2     |\n|  **Colossal-LLaMA-2-13b-base**  | Llama-2-13B |   **0.025T**    |        56.42         |     61.80     |        54.69     |       69.53     |        60.3     |\n\n\n### ColossalChat\n\n<div align="center">\n   <a href="https://www.youtube.com/watch?v=HcTiHzApHm0">\n   <img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chat/ColossalChat%20YouTube.png" width="700" />\n   </a>\n</div>\n\n[ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat): An open-source solution for cloning [ChatGPT](https://openai.com/blog/chatgpt/) with a complete RLHF pipeline.\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Chat)\n[[blog]](https://medium.com/@yangyou_berkeley/colossalchat-an-open-source-solution-for-cloning-chatgpt-with-a-complete-rlhf-pipeline-5edf08fb538b)\n[[demo]](https://www.youtube.com/watch?v=HcTiHzApHm0)\n[[tutorial]](https://www.youtube.com/watch?v=-qFBZFmOJfg)\n\n<p id="ColossalChat-Speed" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chat/ColossalChat%20Speed.jpg" width=450/>\n</p>\n\n- Up to 10 times faster for RLHF PPO Stage3 Training\n\n<p id="ColossalChat_scaling" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT%20scaling.png" width=800/>\n</p>\n\n- Up to 7.73 times faster for single server training and 1.42 times faster for single-GPU inference\n\n<p id="ColossalChat-1GPU" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/ChatGPT-1GPU.jpg" width=450/>\n</p>\n\n- Up to 10.3x growth in model capacity on one GPU\n- A mini demo training process requires only 1.62GB of GPU memory (any consumer-grade GPU)\n\n<p id="ColossalChat-LoRA" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/applications/chatgpt/LoRA%20data.jpg" width=600/>\n</p>\n\n- Increase the capacity of the fine-tuning model by up to 3.7 times on a single GPU\n- Keep at a sufficiently high running speed\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n\n### AIGC\nAcceleration of AIGC (AI-Generated Content) models such as [Stable Diffusion v1](https://github.com/CompVis/stable-diffusion) and [Stable Diffusion v2](https://github.com/Stability-AI/stablediffusion).\n<p id="diffusion_train" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20v2.png" width=800/>\n</p>\n\n- [Training](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce Stable Diffusion memory consumption by up to 5.6x and hardware cost by up to 46x (from A100 to RTX3060).\n\n<p id="diffusion_demo" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/DreamBooth.png" width=800/>\n</p>\n\n- [DreamBooth Fine-tuning](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/dreambooth): Personalize your model using just 3-5 images of the desired subject.\n\n<p id="inference-sd" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/Stable%20Diffusion%20Inference.jpg" width=800/>\n</p>\n\n- [Inference](https://github.com/hpcaitech/ColossalAI/tree/main/examples/images/diffusion): Reduce inference GPU memory consumption by 2.5x.\n\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n### Biomedicine\nAcceleration of [AlphaFold Protein Structure](https://alphafold.ebi.ac.uk/)\n\n<p id="FastFold" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/FastFold.jpg" width=800/>\n</p>\n\n- [FastFold](https://github.com/hpcaitech/FastFold): Accelerating training and inference on GPU Clusters, faster data processing, inference sequence containing more than 10000 residues.\n\n<p id="FastFold-Intel" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/data%20preprocessing%20with%20Intel.jpg" width=600/>\n</p>\n\n- [FastFold with Intel](https://github.com/hpcaitech/FastFold): 3x inference acceleration and 39% cost reduce.\n\n<p id="xTrimoMultimer" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/xTrimoMultimer_Table.jpg" width=800/>\n</p>\n\n- [xTrimoMultimer](https://github.com/biomap-research/xTrimoMultimer): accelerating structure prediction of protein monomers and multimer by 11x.\n\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n## Parallel Training Demo\n### LLaMA3\n<p align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/LLaMA3-70B-H100.png" width=600/>\n</p>\n\n- 70 billion parameter LLaMA3 model training accelerated by 18%\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)\n[[GPU Cloud Playground]](https://cloud.luchentech.com/)\n[[LLaMA3 Image]](https://cloud.luchentech.com/doc/docs/image/llama)\n\n### LLaMA2\n<p align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/llama2_pretraining.png" width=600/>\n</p>\n\n- 70 billion parameter LLaMA2 model training accelerated by 195%\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)\n[[blog]](https://www.hpc-ai.tech/blog/70b-llama2-training)\n\n### LLaMA1\n<p align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/LLaMA_pretraining.png" width=600/>\n</p>\n\n- 65-billion-parameter large model pretraining accelerated by 38%\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/llama)\n[[blog]](https://www.hpc-ai.tech/blog/large-model-pretraining)\n\n### MoE\n<p align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/MOE_training.png" width=800/>\n</p>\n\n- Enhanced MoE parallelism, Open-source MoE model training can be 9 times more efficient\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe)\n[[blog]](https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-efficient)\n\n### GPT-3\n<p align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT3-v5.png" width=700/>\n</p>\n\n- Save 50% GPU resources and 10.7% acceleration\n\n### GPT-2\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2.png" width=800/>\n\n- 11x lower GPU memory consumption, and superlinear scaling efficiency with Tensor Parallelism\n\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/(updated)GPT-2.png" width=800>\n\n- 24x larger model size on the same hardware\n- over 3x acceleration\n### BERT\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/BERT.png" width=800/>\n\n- 2x faster training, or 50% longer sequence length\n\n### PaLM\n- [PaLM-colossalai](https://github.com/hpcaitech/PaLM-colossalai): Scalable implementation of Google''s Pathways Language Model ([PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)).\n\n### OPT\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/OPT_update.png" width=800/>\n\n- [Open Pretrained Transformer (OPT)](https://github.com/facebookresearch/metaseq), a 175-Billion parameter AI language model released by Meta, which stimulates AI programmers to perform various downstream tasks and application deployments because of public pre-trained model weights.\n- 45% speedup fine-tuning OPT at low cost in lines. [[Example]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/opt) [[Online Serving]](https://colossalai.org/docs/advanced_tutorials/opt_service)\n\nPlease visit our [documentation](https://www.colossalai.org/) and [examples](https://github.com/hpcaitech/ColossalAI/tree/main/examples) for more details.\n\n### ViT\n<p align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/ViT.png" width="450" />\n</p>\n\n- 14x larger batch size, and 5x faster training for Tensor Parallelism = 64\n\n### Recommendation System Models\n- [Cached Embedding](https://github.com/hpcaitech/CachedEmbedding), utilize software cache to train larger embedding tables with a smaller GPU memory budget.\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n## Single GPU Training Demo\n\n### GPT-2\n<p id="GPT-2-Single" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2-GPU1.png" width=450/>\n</p>\n\n- 20x larger model size on the same hardware\n\n<p id="GPT-2-NVME" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/GPT2-NVME.png" width=800/>\n</p>\n\n- 120x larger model size on the same hardware (RTX 3080)\n\n### PaLM\n<p id="PaLM-Single" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/PaLM-GPU1.png" width=450/>\n</p>\n\n- 34x larger model size on the same hardware\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n\n## Inference\n### Colossal-Inference\n<p align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-1.png" width=1000/>\n</p>\n\n<p align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/inference/colossal-inference-v1-2.png" width=1000/>\n</p>\n\n - Large AI models inference speed doubled, compared to the offline inference performance of vLLM in some cases.\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/colossalai/inference)\n[[blog]](https://hpc-ai.com/blog/colossal-inference)\n[[GPU Cloud Playground]](https://cloud.luchentech.com/)\n[[LLaMA3 Image]](https://cloud.luchentech.com/doc/docs/image/llama)\n\n### Grok-1\n<p id="Grok-1" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/examples/images/grok-1-inference.jpg" width=600/>\n</p>\n\n - 314 Billion Parameter Grok-1 Inference Accelerated by 3.8x, an easy-to-use Python + PyTorch + HuggingFace version for Inference.\n\n[[code]](https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/grok-1)\n[[blog]](https://hpc-ai.com/blog/314-billion-parameter-grok-1-inference-accelerated-by-3.8x-efficient-and-easy-to-use-pytorchhuggingface-version-is-here)\n[[HuggingFace Grok-1 PyTorch model weights]](https://huggingface.co/hpcai-tech/grok-1)\n[[ModelScope Grok-1 PyTorch model weights]](https://www.modelscope.cn/models/colossalai/grok-1-pytorch/summary)\n\n### SwiftInfer\n<p id="SwiftInfer" align="center">\n<img src="https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/SwiftInfer.jpg" width=800/>\n</p>\n\n- [SwiftInfer](https://github.com/hpcaitech/SwiftInfer): Inference performance improved by 46%, open source solution breaks the length limit of LLM for multi-round conversations\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n## Installation\n\nRequirements:\n- PyTorch >= 2.2\n- Python >= 3.7\n- CUDA >= 11.0\n- [NVIDIA GPU Compute Capability](https://developer.nvidia.com/cuda-gpus) >= 7.0 (V100/RTX20 and higher)\n- Linux OS\n\nIf you encounter any problem with installation, you may want to raise an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose) in this repository.\n\n### Install from PyPI\n\nYou can easily install Colossal-AI with the following command. **By default, we do not build PyTorch extensions during installation.**\n\n```bash\npip install colossalai\n```\n\n**Note: only Linux is supported for now.**\n\nHowever, if you want to build the PyTorch extensions during installation, you can set `BUILD_EXT=1`.\n\n```bash\nBUILD_EXT=1 pip install colossalai\n```\n\n**Otherwise, CUDA kernels will be built during runtime when you actually need them.**\n\nWe also keep releasing the nightly version to PyPI every week. This allows you to access the unreleased features and bug fixes in the main branch.\nInstallation can be made via\n\n```bash\npip install colossalai-nightly\n```\n\n### Download From Source\n\n> The version of Colossal-AI will be in line with the main branch of the repository. Feel free to raise an issue if you encounter any problems. :)\n\n```shell\ngit clone https://github.com/hpcaitech/ColossalAI.git\ncd ColossalAI\n\n# install colossalai\npip install .\n```\n\nBy default, we do not compile CUDA/C++ kernels. ColossalAI will build them during runtime.\nIf you want to install and enable CUDA kernel fusion (compulsory installation when using fused optimizer):\n\n```shell\nBUILD_EXT=1 pip install .\n```\n\nFor Users with CUDA 10.2, you can still build ColossalAI from source. However, you need to manually download the cub library and copy it to the corresponding directory.\n\n```bash\n# clone the repository\ngit clone https://github.com/hpcaitech/ColossalAI.git\ncd ColossalAI\n\n# download the cub library\nwget https://github.com/NVIDIA/cub/archive/refs/tags/1.8.0.zip\nunzip 1.8.0.zip\ncp -r cub-1.8.0/cub/ colossalai/kernel/cuda_native/csrc/kernels/include/\n\n# install\nBUILD_EXT=1 pip install .\n```\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n## Use Docker\n\n### Pull from DockerHub\n\nYou can directly pull the docker image from our [DockerHub page](https://hub.docker.com/r/hpcaitech/colossalai). The image is automatically uploaded upon release.\n\n\n### Build On Your Own\n\nRun the following command to build a docker image from Dockerfile provided.\n\n> Building Colossal-AI from scratch requires GPU support, you need to use Nvidia Docker Runtime as the default when doing `docker build`. More details can be found [here](https://stackoverflow.com/questions/59691207/docker-build-with-nvidia-runtime).\n> We recommend you install Colossal-AI from our [project page](https://www.colossalai.org) directly.\n\n\n```bash\ncd ColossalAI\ndocker build -t colossalai ./docker\n```\n\nRun the following command to start the docker container in interactive mode.\n\n```bash\ndocker run -ti --gpus all --rm --ipc=host colossalai bash\n```\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n## Community\n\nJoin the Colossal-AI community on [Forum](https://github.com/hpcaitech/ColossalAI/discussions),\n[Slack](https://join.slack.com/t/colossalaiworkspace/shared_invite/zt-z7b26eeb-CBp7jouvu~r0~lcFzX832w),\nand [WeChat(å¾®ä¿¡)](https://raw.githubusercontent.com/hpcaitech/public_assets/main/colossalai/img/WeChat.png "qrcode") to share your suggestions, feedback, and questions with our engineering team.\n\n## Contributing\nReferring to the successful attempts of [BLOOM](https://bigscience.huggingface.co/) and [Stable Diffusion](https://en.wikipedia.org/wiki/Stable_Diffusion), any and all developers and partners with computing powers, datasets, models are welcome to join and build the Colossal-AI community, making efforts towards the era of big AI models!\n\nYou may contact us or participate in the following ways:\n1. [Leaving a Star â­](https://github.com/hpcaitech/ColossalAI/stargazers) to show your like and support. Thanks!\n2. Posting an [issue](https://github.com/hpcaitech/ColossalAI/issues/new/choose), or submitting a PR on GitHub follow the guideline in [Contributing](https://github.com/hpcaitech/ColossalAI/blob/main/CONTRIBUTING.md)\n3. Send your official proposal to email contact@hpcaitech.com\n\nThanks so much to all of our amazing contributors!\n\n<a href="https://github.com/hpcaitech/ColossalAI/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=hpcaitech/ColossalAI"  width="800px"/>\n</a>\n\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n\n\n## CI/CD\n\nWe leverage the power of [GitHub Actions](https://github.com/features/actions) to automate our development, release and deployment workflows. Please check out this [documentation](.github/workflows/README.md) on how the automated workflows are operated.\n\n\n## Cite Us\n\nThis project is inspired by some related projects (some by our team and some by other organizations). We would like to credit these amazing projects as listed in the [Reference List](./docs/REFERENCE.md).\n\nTo cite this project, you can use the following BibTeX citation.\n\n```\n@inproceedings{10.1145/3605573.3605613,\nauthor = {Li, Shenggui and Liu, Hongxin and Bian, Zhengda and Fang, Jiarui and Huang, Haichen and Liu, Yuliang and Wang, Boxiang and You, Yang},\ntitle = {Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training},\nyear = {2023},\nisbn = {9798400708435},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3605573.3605613},\ndoi = {10.1145/3605573.3605613},\nabstract = {The success of Transformer models has pushed the deep learning model scale to billions of parameters, but the memory limitation of a single GPU has led to an urgent need for training on multi-GPU clusters. However, the best practice for choosing the optimal parallel strategy is still lacking, as it requires domain expertise in both deep learning and parallel computing. The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism and is integrated with heterogeneous training and zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.},\nbooktitle = {Proceedings of the 52nd International Conference on Parallel Processing},\npages = {766â€“775},\nnumpages = {10},\nkeywords = {datasets, gaze detection, text tagging, neural networks},\nlocation = {Salt Lake City, UT, USA},\nseries = {ICPP ''23}\n}\n```\n\nColossal-AI has been accepted as official tutorial by top conferences [NeurIPS](https://nips.cc/), [SC](https://sc22.supercomputing.org/), [AAAI](https://aaai.org/Conferences/AAAI-23/),\n[PPoPP](https://ppopp23.sigplan.org/), [CVPR](https://cvpr2023.thecvf.com/), [ISC](https://www.isc-hpc.com/), [NVIDIA GTC](https://www.nvidia.com/en-us/on-demand/session/gtcspring23-S51482/) ,etc.\n\n<p align="right">(<a href="#top">back to top</a>)</p>\n', '{"language":"Python","stars":41289,"forks":4540,"watchers":41289,"open_issues":481,"topics":["ai","big-model","data-parallelism","deep-learning","distributed-computing","foundation-models","heterogeneous-training","hpc","inference","large-scale","model-parallelism","pipeline-parallelism"],"default_branch":"main","size_kb":66459,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:public_assets","source_url":"https://github.com/hpcaitech/public_assets"},{"type":"has_code","target_id":"github:hpcaitech:public_assets","source_url":"https://github.com/hpcaitech/public_assets"},{"type":"has_code","target_id":"github:hpcaitech:Open-Sora","source_url":"https://github.com/hpcaitech/Open-Sora"},{"type":"has_code","target_id":"github:hpcaitech:Open-Sora","source_url":"https://github.com/hpcaitech/Open-Sora"},{"type":"has_code","target_id":"github:hpcaitech:Open-Sora","source_url":"https://github.com/hpcaitech/Open-Sora"},{"type":"has_code","target_id":"github:hpcaitech:Open-Sora","source_url":"https://github.com/hpcaitech/Open-Sora?tab=readme-ov-file#model-weights"},{"type":"has_code","target_id":"github:hpcaitech:Open-Sora","source_url":"https://github.com/hpcaitech/Open-Sora?tab=readme-ov-file#-latest-demo"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:CompVis:stable-diffusion","source_url":"https://github.com/CompVis/stable-diffusion"},{"type":"has_code","target_id":"github:Stability-AI:stablediffusion","source_url":"https://github.com/Stability-AI/stablediffusion"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:FastFold","source_url":"https://github.com/hpcaitech/FastFold"},{"type":"has_code","target_id":"github:hpcaitech:FastFold","source_url":"https://github.com/hpcaitech/FastFold"},{"type":"has_code","target_id":"github:biomap-research:xTrimoMultimer","source_url":"https://github.com/biomap-research/xTrimoMultimer"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:PaLM-colossalai","source_url":"https://github.com/hpcaitech/PaLM-colossalai"},{"type":"has_code","target_id":"github:facebookresearch:metaseq","source_url":"https://github.com/facebookresearch/metaseq"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:CachedEmbedding","source_url":"https://github.com/hpcaitech/CachedEmbedding"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:SwiftInfer","source_url":"https://github.com/hpcaitech/SwiftInfer"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI.git","source_url":"https://github.com/hpcaitech/ColossalAI.git"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI.git","source_url":"https://github.com/hpcaitech/ColossalAI.git"},{"type":"has_code","target_id":"github:NVIDIA:cub","source_url":"https://github.com/NVIDIA/cub"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:hpcaitech:ColossalAI","source_url":"https://github.com/hpcaitech/ColossalAI"},{"type":"has_code","target_id":"github:features:actions","source_url":"https://github.com/features/actions"}]', NULL, 'Apache-2.0', 'approved', 80, 'ef5c449867797e26cd61ab916b071a34', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-hpcaitech-ColossalAI from https://github.com/hpcaitech.png
Image converted to WebP: data/images/github-hpcaitech-ColossalAI.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-floodsung-Deep-Learning-Papers-Reading-Roadmap', 'github--floodsung--deep-learning-papers-reading-roadmap', 'Deep-Learning-Papers-Reading-Roadmap', 'floodsung', '>If you are a newcomer to the Deep Learning area, the first question you may have is "Which paper should I start reading from?" >Here is a reading roadmap of Deep Learning papers! The roadmap is constructed in accordance with the following four guidelines: - From outline to detail - From old to state-of-the-art - from generic to specific areas - focus on state-of-the-art You will find many papers that are quite new but really worth reading. I would continue adding papers to this roadmap. ----...', '["deep-learning","python"]', 'other', 39379, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Deep Learning Papers Reading Roadmap\n\n>If you are a newcomer to the Deep Learning area, the first question you may have is "Which paper should I start reading from?"\n\n>Here is a reading roadmap of Deep Learning papers!\n\nThe roadmap is constructed in accordance with the following four guidelines:\n\n- From outline to detail\n- From old to state-of-the-art\n- from generic to specific areas\n- focus on state-of-the-art\n\nYou will find many papers that are quite new but really worth reading.\n\nI would continue adding papers to this roadmap.\n\n\n---------------------------------------\n\n# 1 Deep Learning History and Basics\n\n## 1.0 Book\n\n**[0]** Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. "**Deep learning**." An MIT Press book. (2015). [[html]](http://www.deeplearningbook.org/) **(Deep Learning Bible, you can read this book while reading following papers.)** :star::star::star::star::star:\n\n## 1.1 Survey\n\n**[1]** LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. "**Deep learning**." Nature 521.7553 (2015): 436-444. [[pdf]](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf) **(Three Giants'' Survey)** :star::star::star::star::star:\n\n## 1.2 Deep Belief Network(DBN)(Milestone of Deep Learning Eve)\n\n**[2]** Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. "**A fast learning algorithm for deep belief nets**." Neural computation 18.7 (2006): 1527-1554. [[pdf]](http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf)**(Deep Learning Eve)** :star::star::star:\n\n**[3]** Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. "**Reducing the dimensionality of data with neural networks**." Science 313.5786 (2006): 504-507. [[pdf]](http://www.cs.toronto.edu/~hinton/science.pdf) **(Milestone, Show the promise of deep learning)** :star::star::star:\n\n## 1.3 ImageNet Evolutionï¼ˆDeep Learning broke out from hereï¼‰\n\n**[4]** Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "**Imagenet classification with deep convolutional neural networks**." Advances in neural information processing systems. 2012. [[pdf]](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) **(AlexNet, Deep Learning Breakthrough)** :star::star::star::star::star:\n\n**[5]** Simonyan, Karen, and Andrew Zisserman. "**Very deep convolutional networks for large-scale image recognition**." arXiv preprint arXiv:1409.1556 (2014). [[pdf]](https://arxiv.org/pdf/1409.1556.pdf) **(VGGNet,Neural Networks become very deep!)** :star::star::star:\n\n**[6]** Szegedy, Christian, et al. "**Going deeper with convolutions**." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf) **(GoogLeNet)** :star::star::star:\n\n**[7]** He, Kaiming, et al. "**Deep residual learning for image recognition**." arXiv preprint arXiv:1512.03385 (2015). [[pdf]](https://arxiv.org/pdf/1512.03385.pdf) **(ResNet,Very very deep networks, CVPR best paper)** :star::star::star::star::star:\n\n## 1.4 Speech Recognition Evolution\n\n**[8]** Hinton, Geoffrey, et al. "**Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups**." IEEE Signal Processing Magazine 29.6 (2012): 82-97. [[pdf]](http://cs224d.stanford.edu/papers/maas_paper.pdf) **(Breakthrough in speech recognition)**:star::star::star::star:\n\n**[9]** Graves, Alex, Abdel-rahman Mohamed, and Geoffrey Hinton. "**Speech recognition with deep recurrent neural networks**." 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [[pdf]](http://arxiv.org/pdf/1303.5778.pdf) **(RNN)**:star::star::star:\n\n**[10]** Graves, Alex, and Navdeep Jaitly. "**Towards End-To-End Speech Recognition with Recurrent Neural Networks**." ICML. Vol. 14. 2014. [[pdf]](http://www.jmlr.org/proceedings/papers/v32/graves14.pdf):star::star::star:\n\n**[11]** Sak, HaÅŸim, et al. "**Fast and accurate recurrent neural network acoustic models for speech recognition**." arXiv preprint arXiv:1507.06947 (2015). [[pdf]](http://arxiv.org/pdf/1507.06947) **(Google Speech Recognition System)** :star::star::star:\n\n**[12]** Amodei, Dario, et al. "**Deep speech 2: End-to-end speech recognition in english and mandarin**." arXiv preprint arXiv:1512.02595 (2015). [[pdf]](https://arxiv.org/pdf/1512.02595.pdf) **(Baidu Speech Recognition System)** :star::star::star::star:\n\n**[13]** W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig "**Achieving Human Parity in Conversational Speech Recognition**." arXiv preprint arXiv:1610.05256 (2016). [[pdf]](https://arxiv.org/pdf/1610.05256v1) **(State-of-the-art in speech recognition, Microsoft)** :star::star::star::star:\n\n>After reading above papers, you will have a basic understanding of the Deep Learning history, the basic architectures of Deep Learning model(including CNN, RNN, LSTM) and how deep learning can be applied to image and speech recognition issues. The following papers will take you in-depth understanding of the Deep Learning method, Deep Learning in different areas of application and the frontiers. I suggest that you can choose the following papers based on your interests and research direction.\n\n#2 Deep Learning Method\n\n## 2.1 Model\n\n**[14]** Hinton, Geoffrey E., et al. "**Improving neural networks by preventing co-adaptation of feature detectors**." arXiv preprint arXiv:1207.0580 (2012). [[pdf]](https://arxiv.org/pdf/1207.0580.pdf) **(Dropout)** :star::star::star:\n\n**[15]** Srivastava, Nitish, et al. "**Dropout: a simple way to prevent neural networks from overfitting**." Journal of Machine Learning Research 15.1 (2014): 1929-1958. [[pdf]](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) :star::star::star:\n\n**[16]** Ioffe, Sergey, and Christian Szegedy. "**Batch normalization: Accelerating deep network training by reducing internal covariate shift**." arXiv preprint arXiv:1502.03167 (2015). [[pdf]](http://arxiv.org/pdf/1502.03167) **(An outstanding Work in 2015)** :star::star::star::star:\n\n**[17]** Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton. "**Layer normalization**." arXiv preprint arXiv:1607.06450 (2016). [[pdf]](https://arxiv.org/pdf/1607.06450.pdf?utm_source=sciontist.com&utm_medium=refer&utm_campaign=promote) **(Update of Batch Normalization)** :star::star::star::star:\n\n**[18]** Courbariaux, Matthieu, et al. "**Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to+ 1 orâˆ’1**." [[pdf]](https://pdfs.semanticscholar.org/f832/b16cb367802609d91d400085eb87d630212a.pdf) **(New Model,Fast)**  :star::star::star:\n\n**[19]** Jaderberg, Max, et al. "**Decoupled neural interfaces using synthetic gradients**." arXiv preprint arXiv:1608.05343 (2016). [[pdf]](https://arxiv.org/pdf/1608.05343) **(Innovation of Training Method,Amazing Work)** :star::star::star::star::star:\n\n**[20]** Chen, Tianqi, Ian Goodfellow, and Jonathon Shlens. "Net2net: Accelerating learning via knowledge transfer." arXiv preprint arXiv:1511.05641 (2015). [[pdf]](https://arxiv.org/abs/1511.05641) **(Modify previously trained network to reduce training epochs)** :star::star::star:\n\n**[21]** Wei, Tao, et al. "Network Morphism." arXiv preprint arXiv:1603.01670 (2016). [[pdf]](https://arxiv.org/abs/1603.01670) **(Modify previously trained network to reduce training epochs)** :star::star::star:\n\n## 2.2 Optimization\n\n**[22]** Sutskever, Ilya, et al. "**On the importance of initialization and momentum in deep learning**." ICML (3) 28 (2013): 1139-1147. [[pdf]](http://www.jmlr.org/proceedings/papers/v28/sutskever13.pdf) **(Momentum optimizer)** :star::star:\n\n**[23]** Kingma, Diederik, and Jimmy Ba. "**Adam: A method for stochastic optimization**." arXiv preprint arXiv:1412.6980 (2014). [[pdf]](http://arxiv.org/pdf/1412.6980) **(Maybe used most often currently)** :star::star::star:\n\n**[24]** Andrychowicz, Marcin, et al. "**Learning to learn by gradient descent by gradient descent**." arXiv preprint arXiv:1606.04474 (2016). [[pdf]](https://arxiv.org/pdf/1606.04474) **(Neural Optimizer,Amazing Work)** :star::star::star::star::star:\n\n**[25]** Han, Song, Huizi Mao, and William J. Dally. "**Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding**." CoRR, abs/1510.00149 2 (2015). [[pdf]](https://pdfs.semanticscholar.org/5b6c/9dda1d88095fa4aac1507348e498a1f2e863.pdf) **(ICLR best paper, new direction to make NN running fast,DeePhi Tech Startup)** :star::star::star::star::star:\n\n**[26]** Iandola, Forrest N., et al. "**SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 1MB model size**." arXiv preprint arXiv:1602.07360 (2016). [[pdf]](http://arxiv.org/pdf/1602.07360) **(Also a new direction to optimize NN,DeePhi Tech Startup)** :star::star::star::star:\n\n**[27]** Glorat Xavier, Bengio Yoshua, et al. "**Understanding the difficulty of training deep forward neural networks**." Proceedings of the thirteenth International Conference on Artificial Intelligence and Statistics, PMLR 9:249-256,2010. [[pdf]](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) :star::star::star::star:\n\n## 2.3 Unsupervised Learning / Deep Generative Model\n\n**[28]** Le, Quoc V. "**Building high-level features using large scale unsupervised learning**." 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013. [[pdf]](http://arxiv.org/pdf/1112.6209.pdf&embed) **(Milestone, Andrew Ng, Google Brain Project, Cat)** :star::star::star::star:\n\n\n**[29]** Kingma, Diederik P., and Max Welling. "**Auto-encoding variational bayes**." arXiv preprint arXiv:1312.6114 (2013). [[pdf]](http://arxiv.org/pdf/1312.6114) **(VAE)** :star::star::star::star:\n\n**[30]** Goodfellow, Ian, et al. "**Generative adversarial nets**." Advances in Neural Information Processing Systems. 2014. [[pdf]](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf) **(GAN,super cool idea)** :star::star::star::star::star:\n\n**[31]** Radford, Alec, Luke Metz, and Soumith Chintala. "**Unsupervised representation learning with deep convolutional generative adversarial networks**." arXiv preprint arXiv:1511.06434 (2015). [[pdf]](http://arxiv.org/pdf/1511.06434) **(DCGAN)** :star::star::star::star:\n\n**[32]** Gregor, Karol, et al. "**DRAW: A recurrent neural network for image generation**." arXiv preprint arXiv:1502.04623 (2015). [[pdf]](http://jmlr.org/proceedings/papers/v37/gregor15.pdf) **(VAE with attention, outstanding work)** :star::star::star::star::star:\n\n**[33]** Oord, Aaron van den, Nal Kalchbrenner, and Koray Kavukcuoglu. "**Pixel recurrent neural networks**." arXiv preprint arXiv:1601.06759 (2016). [[pdf]](http://arxiv.org/pdf/1601.06759) **(PixelRNN)** :star::star::star::star:\n\n**[34]** Oord, Aaron van den, et al. "Conditional image generation with PixelCNN decoders." arXiv preprint arXiv:1606.05328 (2016). [[pdf]](https://arxiv.org/pdf/1606.05328) **(PixelCNN)** :star::star::star::star:\n\n**[34]** S. Mehri et al., "**SampleRNN: An Unconditional End-to-End Neural Audio Generation Model**." arXiv preprint 	arXiv:1612.07837 (2016). [[pdf]](https://arxiv.org/pdf/1612.07837.pdf) :star::star::star::star::star:\n\n## 2.4 RNN / Sequence-to-Sequence Model\n\n**[35]** Graves, Alex. "**Generating sequences with recurrent neural networks**." arXiv preprint arXiv:1308.0850 (2013). [[pdf]](http://arxiv.org/pdf/1308.0850) **(LSTM, very nice generating result, show the power of RNN)** :star::star::star::star:\n\n**[36]** Cho, Kyunghyun, et al. "**Learning phrase representations using RNN encoder-decoder for statistical machine translation**." arXiv preprint arXiv:1406.1078 (2014). [[pdf]](http://arxiv.org/pdf/1406.1078) **(First Seq-to-Seq Paper)** :star::star::star::star:\n\n**[37]** Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. "**Sequence to sequence learning with neural networks**." Advances in neural information processing systems. 2014. [[pdf]](https://arxiv.org/pdf/1409.3215.pdf) **(Outstanding Work)** :star::star::star::star::star:\n\n**[38]** Bahdanau, Dzmitry, KyungHyun Cho, and Yoshua Bengio. "**Neural Machine Translation by Jointly Learning to Align and Translate**." arXiv preprint arXiv:1409.0473 (2014). [[pdf]](https://arxiv.org/pdf/1409.0473v7.pdf) :star::star::star::star:\n\n**[39]** Vinyals, Oriol, and Quoc Le. "**A neural conversational model**." arXiv preprint arXiv:1506.05869 (2015). [[pdf]](http://arxiv.org/pdf/1506.05869.pdf%20(http://arxiv.org/pdf/1506.05869.pdf)) **(Seq-to-Seq on Chatbot)** :star::star::star:\n\n## 2.5 Neural Turing Machine\n\n**[40]** Graves, Alex, Greg Wayne, and Ivo Danihelka. "**Neural turing machines**." arXiv preprint arXiv:1410.5401 (2014). [[pdf]](http://arxiv.org/pdf/1410.5401.pdf) **(Basic Prototype of Future Computer)** :star::star::star::star::star:\n\n**[41]** Zaremba, Wojciech, and Ilya Sutskever. "**Reinforcement learning neural Turing machines**." arXiv preprint arXiv:1505.00521 362 (2015). [[pdf]](https://pdfs.semanticscholar.org/f10e/071292d593fef939e6ef4a59baf0bb3a6c2b.pdf) :star::star::star:\n\n**[42]** Weston, Jason, Sumit Chopra, and Antoine Bordes. "**Memory networks**." arXiv preprint arXiv:1410.3916 (2014). [[pdf]](http://arxiv.org/pdf/1410.3916) :star::star::star:\n\n\n**[43]** Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. "**End-to-end memory networks**." Advances in neural information processing systems. 2015. [[pdf]](http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf) :star::star::star::star:\n\n**[44]** Vinyals, Oriol, Meire Fortunato, and Navdeep Jaitly. "**Pointer networks**." Advances in Neural Information Processing Systems. 2015. [[pdf]](http://papers.nips.cc/paper/5866-pointer-networks.pdf) :star::star::star::star:\n\n**[45]** Graves, Alex, et al. "**Hybrid computing using a neural network with dynamic external memory**." Nature (2016). [[pdf]](https://www.dropbox.com/s/0a40xi702grx3dq/2016-graves.pdf) **(Milestone,combine above papers'' ideas)** :star::star::star::star::star:\n\n## 2.6 Deep Reinforcement Learning\n\n**[46]** Mnih, Volodymyr, et al. "**Playing atari with deep reinforcement learning**." arXiv preprint arXiv:1312.5602 (2013). [[pdf]](http://arxiv.org/pdf/1312.5602.pdf)) **(First Paper named deep reinforcement learning)** :star::star::star::star:\n\n**[47]** Mnih, Volodymyr, et al. "**Human-level control through deep reinforcement learning**." Nature 518.7540 (2015): 529-533. [[pdf]](https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf) **(Milestone)** :star::star::star::star::star:\n\n**[48]** Wang, Ziyu, Nando de Freitas, and Marc Lanctot. "**Dueling network architectures for deep reinforcement learning**." arXiv preprint arXiv:1511.06581 (2015). [[pdf]](http://arxiv.org/pdf/1511.06581) **(ICLR best paper,great idea)**  :star::star::star::star:\n\n**[49]** Mnih, Volodymyr, et al. "**Asynchronous methods for deep reinforcement learning**." arXiv preprint arXiv:1602.01783 (2016). [[pdf]](http://arxiv.org/pdf/1602.01783) **(State-of-the-art method)** :star::star::star::star::star:\n\n**[50]** Lillicrap, Timothy P., et al. "**Continuous control with deep reinforcement learning**." arXiv preprint arXiv:1509.02971 (2015). [[pdf]](http://arxiv.org/pdf/1509.02971) **(DDPG)** :star::star::star::star:\n\n**[51]** Gu, Shixiang, et al. "**Continuous Deep Q-Learning with Model-based Acceleration**." arXiv preprint arXiv:1603.00748 (2016). [[pdf]](http://arxiv.org/pdf/1603.00748) **(NAF)** :star::star::star::star:\n\n**[52]** Schulman, John, et al. "**Trust region policy optimization**." CoRR, abs/1502.05477 (2015). [[pdf]](http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf) **(TRPO)** :star::star::star::star:\n\n**[53]** Silver, David, et al. "**Mastering the game of Go with deep neural networks and tree search**." Nature 529.7587 (2016): 484-489. [[pdf]](http://willamette.edu/~levenick/cs448/goNature.pdf) **(AlphaGo)** :star::star::star::star::star:\n\n## 2.7 Deep Transfer Learning / Lifelong Learning / especially for RL\n\n**[54]** Bengio, Yoshua. "**Deep Learning of Representations for Unsupervised and Transfer Learning**." ICML Unsupervised and Transfer Learning 27 (2012): 17-36. [[pdf]](http://www.jmlr.org/proceedings/papers/v27/bengio12a/bengio12a.pdf) **(A Tutorial)** :star::star::star:\n\n**[55]** Silver, Daniel L., Qiang Yang, and Lianghao Li. "**Lifelong Machine Learning Systems: Beyond Learning Algorithms**." AAAI Spring Symposium: Lifelong Machine Learning. 2013. [[pdf]](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.696.7800&rep=rep1&type=pdf) **(A brief discussion about lifelong learning)**  :star::star::star:\n\n**[56]** Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. "**Distilling the knowledge in a neural network**." arXiv preprint arXiv:1503.02531 (2015). [[pdf]](http://arxiv.org/pdf/1503.02531) **(Godfather''s Work)** :star::star::star::star:\n\n**[57]** Rusu, Andrei A., et al. "**Policy distillation**." arXiv preprint arXiv:1511.06295 (2015). [[pdf]](http://arxiv.org/pdf/1511.06295) **(RL domain)** :star::star::star:\n\n**[58]** Parisotto, Emilio, Jimmy Lei Ba, and Ruslan Salakhutdinov. "**Actor-mimic: Deep multitask and transfer reinforcement learning**." arXiv preprint arXiv:1511.06342 (2015). [[pdf]](http://arxiv.org/pdf/1511.06342) **(RL domain)** :star::star::star:\n\n**[59]** Rusu, Andrei A., et al. "**Progressive neural networks**." arXiv preprint arXiv:1606.04671 (2016). [[pdf]](https://arxiv.org/pdf/1606.04671) **(Outstanding Work, A novel idea)** :star::star::star::star::star:\n\n\n## 2.8 One Shot Deep Learning\n\n**[60]** Lake, Brenden M., Ruslan Salakhutdinov, and Joshua B. Tenenbaum. "**Human-level concept learning through probabilistic program induction**." Science 350.6266 (2015): 1332-1338. [[pdf]](http://clm.utexas.edu/compjclub/wp-content/uploads/2016/02/lake2015.pdf) **(No Deep Learning,but worth reading)** :star::star::star::star::star:\n\n**[61]** Koch, Gregory, Richard Zemel, and Ruslan Salakhutdinov. "**Siamese Neural Networks for One-shot Image Recognition**."(2015) [[pdf]](http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf) :star::star::star:\n\n**[62]** Santoro, Adam, et al. "**One-shot Learning with Memory-Augmented Neural Networks**." arXiv preprint arXiv:1605.06065 (2016). [[pdf]](http://arxiv.org/pdf/1605.06065) **(A basic step to one shot learning)** :star::star::star::star:\n\n**[63]** Vinyals, Oriol, et al. "**Matching Networks for One Shot Learning**." arXiv preprint arXiv:1606.04080 (2016). [[pdf]](https://arxiv.org/pdf/1606.04080) :star::star::star:\n\n**[64]** Hariharan, Bharath, and Ross Girshick. "**Low-shot visual object recognition**." arXiv preprint arXiv:1606.02819 (2016). [[pdf]](http://arxiv.org/pdf/1606.02819) **(A step to large data)** :star::star::star::star:\n\n\n# 3 Applications\n\n## 3.1 NLP(Natural Language Processing)\n\n**[1]** Antoine Bordes, et al. "**Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing**." AISTATS(2012) [[pdf]](https://www.hds.utc.fr/~bordesan/dokuwiki/lib/exe/fetch.php?id=en%3Apubli&cache=cache&media=en:bordes12aistats.pdf) :star::star::star::star:\n\n**[2]** Mikolov, et al. "**Distributed representations of words and phrases and their compositionality**." ANIPS(2013): 3111-3119 [[pdf]](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) **(word2vec)** :star::star::star:\n\n**[3]** Sutskever, et al. "**â€œSequence to sequence learning with neural networks**." ANIPS(2014) [[pdf]](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) :star::star::star:\n\n**[4]** Ankit Kumar, et al. "**â€œAsk Me Anything: Dynamic Memory Networks for Natural Language Processing**." arXiv preprint arXiv:1506.07285(2015) [[pdf]](https://arxiv.org/abs/1506.07285) :star::star::star::star:\n\n**[5]** Yoon Kim, et al. "**Character-Aware Neural Language Models**." NIPS(2015) arXiv preprint arXiv:1508.06615(2015) [[pdf]](https://arxiv.org/abs/1508.06615) :star::star::star::star:\n\n**[6]** Jason Weston, et al. "**Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks**." arXiv preprint arXiv:1502.05698(2015) [[pdf]](https://arxiv.org/abs/1502.05698) **(bAbI tasks)** :star::star::star:\n\n**[7]** Karl Moritz Hermann, et al. "**Teaching Machines to Read and Comprehend**." arXiv preprint arXiv:1506.03340(2015) [[pdf]](https://arxiv.org/abs/1506.03340) **(CNN/DailyMail cloze style questions)** :star::star:\n\n**[8]** Alexis Conneau, et al. "**Very Deep Convolutional Networks for Natural Language Processing**." arXiv preprint arXiv:1606.01781(2016) [[pdf]](https://arxiv.org/abs/1606.01781) **(state-of-the-art in text classification)** :star::star::star:\n\n**[9]** Armand Joulin, et al. "**Bag of Tricks for Efficient Text Classification**." arXiv preprint arXiv:1607.01759(2016) [[pdf]](https://arxiv.org/abs/1607.01759) **(slightly worse than state-of-the-art, but a lot faster)** :star::star::star:\n\n## 3.2 Object Detection\n\n**[1]** Szegedy, Christian, Alexander Toshev, and Dumitru Erhan. "**Deep neural networks for object detection**." Advances in Neural Information Processing Systems. 2013. [[pdf]](http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf) :star::star::star:\n\n**[2]** Girshick, Ross, et al. "**Rich feature hierarchies for accurate object detection and semantic segmentation**." Proceedings of the IEEE conference on computer vision and pattern recognition. 2014. [[pdf]](http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf) **(RCNN)** :star::star::star::star::star:\n\n**[3]** He, Kaiming, et al. "**Spatial pyramid pooling in deep convolutional networks for visual recognition**." European Conference on Computer Vision. Springer International Publishing, 2014. [[pdf]](http://arxiv.org/pdf/1406.4729) **(SPPNet)** :star::star::star::star:\n\n**[4]** Girshick, Ross. "**Fast r-cnn**." Proceedings of the IEEE International Conference on Computer Vision. 2015. [[pdf]](https://pdfs.semanticscholar.org/8f67/64a59f0d17081f2a2a9d06f4ed1cdea1a0ad.pdf) :star::star::star::star:\n\n**[5]** Ren, Shaoqing, et al. "**Faster R-CNN: Towards real-time object detection with region proposal networks**." Advances in neural information processing systems. 2015. [[pdf]](https://arxiv.org/pdf/1506.01497.pdf) :star::star::star::star:\n\n**[6]** Redmon, Joseph, et al. "**You only look once: Unified, real-time object detection**." arXiv preprint arXiv:1506.02640 (2015). [[pdf]](http://homes.cs.washington.edu/~ali/papers/YOLO.pdf) **(YOLO,Oustanding Work, really practical)** :star::star::star::star::star:\n\n**[7]** Liu, Wei, et al. "**SSD: Single Shot MultiBox Detector**." arXiv preprint arXiv:1512.02325 (2015). [[pdf]](http://arxiv.org/pdf/1512.02325) :star::star::star:\n\n**[8]** Dai, Jifeng, et al. "**R-FCN: Object Detection via\nRegion-based Fully Convolutional Networks**." arXiv preprint arXiv:1605.06409 (2016). [[pdf]](https://arxiv.org/abs/1605.06409) :star::star::star::star:\n\n**[9]** He, Gkioxari, et al. "**Mask R-CNN**" arXiv preprint arXiv:1703.06870 (2017). [[pdf]](https://arxiv.org/abs/1703.06870) :star::star::star::star:\n\n**[10]** Bochkovskiy, Alexey, et al. "**YOLOv4: Optimal Speed and Accuracy of Object Detection.**"  arXiv preprint arXiv:2004.10934 (2020). [[pdf]](https://arxiv.org/pdf/2004.10934) :star::star::star::star:\n\n\n**[11]** Tan, Mingxing, et al. â€œ**EfficientDet: Scalable and Efficient Object Detection.**" arXiv preprint arXiv:1911.09070 (2019). [[pdf]](https://arxiv.org/pdf/1911.09070) :star::star::star::star::star:\n\n\n## 3.3 Visual Tracking\n\n**[1]** Wang, Naiyan, and Dit-Yan Yeung. "**Learning a deep compact image representation for visual tracking**." Advances in neural information processing systems. 2013. [[pdf]](http://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf) **(First Paper to do visual tracking using Deep Learning,DLT Tracker)** :star::star::star:\n\n**[2]** Wang, Naiyan, et al. "**Transferring rich feature hierarchies for robust visual tracking**." arXiv preprint arXiv:1501.04587 (2015). [[pdf]](http://arxiv.org/pdf/1501.04587) **(SO-DLT)** :star::star::star::star:\n\n**[3]** Wang, Lijun, et al. "**Visual tracking with fully convolutional networks**." Proceedings of the IEEE International Conference on Computer Vision. 2015. [[pdf]](http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Wang_Visual_Tracking_With_ICCV_2015_paper.pdf) **(FCNT)** :star::star::star::star:\n\n**[4]** Held, David, Sebastian Thrun, and Silvio Savarese. "**Learning to Track at 100 FPS with Deep Regression Networks**." arXiv preprint arXiv:1604.01802 (2016). [[pdf]](http://arxiv.org/pdf/1604.01802) **(GOTURN,Really fast as a deep learning method,but still far behind un-deep-learning methods)** :star::star::star::star:\n\n**[5]** Bertinetto, Luca, et al. "**Fully-Convolutional Siamese Networks for Object Tracking**." arXiv preprint arXiv:1606.09549 (2016). [[pdf]](https://arxiv.org/pdf/1606.09549) **(SiameseFC,New state-of-the-art for real-time object tracking)** :star::star::star::star:\n\n**[6]** Martin Danelljan, Andreas Robinson, Fahad Khan, Michael Felsberg. "**Beyond Correlation Filters: Learning Continuous Convolution Operators for Visual Tracking**." ECCV (2016) [[pdf]](http://www.cvl.isy.liu.se/research/objrec/visualtracking/conttrack/C-COT_ECCV16.pdf) **(C-COT)** :star::star::star::star:\n\n**[7]** Nam, Hyeonseob, Mooyeol Baek, and Bohyung Han. "**Modeling and Propagating CNNs in a Tree Structure for Visual Tracking**." arXiv preprint arXiv:1608.07242 (2016). [[pdf]](https://arxiv.org/pdf/1608.07242) **(VOT2016 Winner,TCNN)** :star::star::star::star:\n\n## 3.4 Image Caption\n**[1]** Farhadi,Ali,etal. "**Every picture tells a story: Generating sentences from images**". In Computer VisionECCV 2010. Springer Berlin Heidelberg:15-29, 2010. [[pdf]](https://www.cs.cmu.edu/~afarhadi/papers/sentence.pdf) :star::star::star:\n\n**[2]** Kulkarni, Girish, et al. "**Baby talk: Understanding and generating image descriptions**". In Proceedings of the 24th CVPR, 2011. [[pdf]](http://tamaraberg.com/papers/generation_cvpr11.pdf):star::star::star::star:\n\n**[3]** Vinyals, Oriol, et al. "**Show and tell: A neural image caption generator**". In arXiv preprint arXiv:1411.4555, 2014. [[pdf]](https://arxiv.org/pdf/1411.4555.pdf):star::star::star:\n\n**[4]** Donahue, Jeff, et al. "**Long-term recurrent convolutional networks for visual recognition and description**". In arXiv preprint arXiv:1411.4389 ,2014. [[pdf]](https://arxiv.org/pdf/1411.4389.pdf)\n\n**[5]** Karpathy, Andrej, and Li Fei-Fei. "**Deep visual-semantic alignments for generating image descriptions**". In arXiv preprint arXiv:1412.2306, 2014. [[pdf]](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf):star::star::star::star::star:\n\n**[6]** Karpathy, Andrej, Armand Joulin, and Fei Fei F. Li. "**Deep fragment embeddings for bidirectional image sentence mapping**". In Advances in neural information processing systems, 2014. [[pdf]](https://arxiv.org/pdf/1406.5679v1.pdf):star::star::star::star:\n\n**[7]** Fang, Hao, et al. "**From captions to visual concepts and back**". In arXiv preprint arXiv:1411.4952, 2014. [[pdf]](https://arxiv.org/pdf/1411.4952v3.pdf):star::star::star::star::star:\n\n**[8]** Chen, Xinlei, and C. Lawrence Zitnick. "**Learning a recurrent visual representation for image caption generation**". In arXiv preprint arXiv:1411.5654, 2014. [[pdf]](https://arxiv.org/pdf/1411.5654v1.pdf):star::star::star::star:\n\n**[9]** Mao, Junhua, et al. "**Deep captioning with multimodal recurrent neural networks (m-rnn)**". In arXiv preprint arXiv:1412.6632, 2014. [[pdf]](https://arxiv.org/pdf/1412.6632v5.pdf):star::star::star:\n\n**[10]** Xu, Kelvin, et al. "**Show, attend and tell: Neural image caption generation with visual attention**". In arXiv preprint arXiv:1502.03044, 2015. [[pdf]](https://arxiv.org/pdf/1502.03044v3.pdf):star::star::star::star::star:\n\n## 3.5 Machine Translation\n\n> Some milestone papers are listed in RNN / Seq-to-Seq topic.\n\n**[1]** Luong, Minh-Thang, et al. "**Addressing the rare word problem in neural machine translation**." arXiv preprint arXiv:1410.8206 (2014). [[pdf]](http://arxiv.org/pdf/1410.8206) :star::star::star::star:\n\n\n**[2]** Sennrich, et al. "**Neural Machine Translation of Rare Words with Subword Units**". In arXiv preprint arXiv:1508.07909, 2015. [[pdf]](https://arxiv.org/pdf/1508.07909.pdf):star::star::star:\n\n**[3]** Luong, Minh-Thang, Hieu Pham, and Christopher D. Manning. "**Effective approaches to attention-based neural machine translation**." arXiv preprint arXiv:1508.04025 (2015). [[pdf]](http://arxiv.org/pdf/1508.04025) :star::star::star::star:\n\n**[4]** Chung, et al. "**A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation**". In arXiv preprint arXiv:1603.06147, 2016. [[pdf]](https://arxiv.org/pdf/1603.06147.pdf):star::star:\n\n**[5]** Lee, et al. "**Fully Character-Level Neural Machine Translation without Explicit Segmentation**". In arXiv preprint arXiv:1610.03017, 2016. [[pdf]](https://arxiv.org/pdf/1610.03017.pdf):star::star::star::star::star:\n\n**[6]** Wu, Schuster, Chen, Le, et al. "**Google''s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation**". In arXiv preprint arXiv:1609.08144v2, 2016. [[pdf]](https://arxiv.org/pdf/1609.08144v2.pdf) **(Milestone)** :star::star::star::star:\n\n## 3.6 Robotics\n\n**[1]** KoutnÃ­k, Jan, et al. "**Evolving large-scale neural networks for vision-based reinforcement learning**." Proceedings of the 15th annual conference on Genetic and evolutionary computation. ACM, 2013. [[pdf]](http://repository.supsi.ch/4550/1/koutnik2013gecco.pdf) :star::star::star:\n\n**[2]** Levine, Sergey, et al. "**End-to-end training of deep visuomotor policies**." Journal of Machine Learning Research 17.39 (2016): 1-40. [[pdf]](http://www.jmlr.org/papers/volume17/15-522/15-522.pdf) :star::star::star::star::star:\n\n**[3]** Pinto, Lerrel, and Abhinav Gupta. "**Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours**." arXiv preprint arXiv:1509.06825 (2015). [[pdf]](http://arxiv.org/pdf/1509.06825) :star::star::star:\n\n**[4]** Levine, Sergey, et al. "**Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection**." arXiv preprint arXiv:1603.02199 (2016). [[pdf]](http://arxiv.org/pdf/1603.02199) :star::star::star::star:\n\n**[5]** Zhu, Yuke, et al. "**Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning**." arXiv preprint arXiv:1609.05143 (2016). [[pdf]](https://arxiv.org/pdf/1609.05143) :star::star::star::star:\n\n**[6]** Yahya, Ali, et al. "**Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search**." arXiv preprint arXiv:1610.00673 (2016). [[pdf]](https://arxiv.org/pdf/1610.00673) :star::star::star::star:\n\n**[7]** Gu, Shixiang, et al. "**Deep Reinforcement Learning for Robotic Manipulation**." arXiv preprint arXiv:1610.00633 (2016). [[pdf]](https://arxiv.org/pdf/1610.00633) :star::star::star::star:\n\n**[8]** A Rusu, M Vecerik, Thomas RothÃ¶rl, N Heess, R Pascanu, R Hadsell."**Sim-to-Real Robot Learning from Pixels with Progressive Nets**." arXiv preprint arXiv:1610.04286 (2016). [[pdf]](https://arxiv.org/pdf/1610.04286.pdf) :star::star::star::star:\n\n**[9]** Mirowski, Piotr, et al. "**Learning to navigate in complex environments**." arXiv preprint arXiv:1611.03673 (2016). [[pdf]](https://arxiv.org/pdf/1611.03673) :star::star::star::star:\n\n## 3.7 Art\n\n**[1]** Mordvintsev, Alexander; Olah, Christopher; Tyka, Mike (2015). "**Inceptionism: Going Deeper into Neural Networks**". Google Research. [[html]](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html) **(Deep Dream)**\n:star::star::star::star:\n\n**[2]** Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. "**A neural algorithm of artistic style**." arXiv preprint arXiv:1508.06576 (2015). [[pdf]](http://arxiv.org/pdf/1508.06576) **(Outstanding Work, most successful method currently)** :star::star::star::star::star:\n\n**[3]** Zhu, Jun-Yan, et al. "**Generative Visual Manipulation on the Natural Image Manifold**." European Conference on Computer Vision. Springer International Publishing, 2016. [[pdf]](https://arxiv.org/pdf/1609.03552) **(iGAN)** :star::star::star::star:\n\n**[4]** Champandard, Alex J. "**Semantic Style Transfer and Turning Two-Bit Doodles into Fine Artworks**." arXiv preprint arXiv:1603.01768 (2016). [[pdf]](http://arxiv.org/pdf/1603.01768) **(Neural Doodle)** :star::star::star::star:\n\n**[5]** Zhang, Richard, Phillip Isola, and Alexei A. Efros. "**Colorful Image Colorization**." arXiv preprint arXiv:1603.08511 (2016). [[pdf]](http://arxiv.org/pdf/1603.08511) :star::star::star::star:\n\n**[6]** Johnson, Justin, Alexandre Alahi, and Li Fei-Fei. "**Perceptual losses for real-time style transfer and super-resolution**." arXiv preprint arXiv:1603.08155 (2016). [[pdf]](https://arxiv.org/pdf/1603.08155.pdf) :star::star::star::star:\n\n**[7]** Vincent Dumoulin, Jonathon Shlens and Manjunath Kudlur. "**A learned representation for artistic style**." arXiv preprint arXiv:1610.07629 (2016). [[pdf]](https://arxiv.org/pdf/1610.07629v1.pdf) :star::star::star::star:\n\n**[8]** Gatys, Leon and Ecker, et al."**Controlling Perceptual Factors in Neural Style Transfer**." arXiv preprint arXiv:1611.07865 (2016). [[pdf]](https://arxiv.org/pdf/1611.07865.pdf) **(control style transfer over spatial location,colour information and across spatial scale)**:star::star::star::star:\n\n**[9]** Ulyanov, Dmitry and Lebedev, Vadim, et al. "**Texture Networks: Feed-forward Synthesis of Textures and Stylized Images**." arXiv preprint arXiv:1603.03417(2016). [[pdf]](http://arxiv.org/abs/1603.03417) **(texture generation and style transfer)** :star::star::star::star:\n\n**[10]** Yijun Li, Ming-Yu Liu ,Xueting Li, Ming-Hsuan Yang,Jan Kautz (NVIDIA). "**A Closed-form Solution to Photorealistic Image Stylization**." arXiv preprint arXiv:1802.06474(2018). [[pdf]](https://arxiv.org/pdf/1802.06474.pdf) **(Very fast and ultra realistic style transfer)** :star::star::star::star:\n\n## 3.8 Object Segmentation\n\n**[1]** J. Long, E. Shelhamer, and T. Darrell, â€œ**Fully convolutional networks for semantic segmentation**.â€ in CVPR, 2015. [[pdf]](https://arxiv.org/pdf/1411.4038v2.pdf) :star::star::star::star::star:\n\n**[2]** L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. "**Semantic image segmentation with deep convolutional nets and fully connected crfs**." In ICLR, 2015. [[pdf]](https://arxiv.org/pdf/1606.00915v1.pdf) :star::star::star::star::star:\n\n**[3]** Pinheiro, P.O., Collobert, R., Dollar, P. "**Learning to segment object candidates.**" In: NIPS. 2015. [[pdf]](https://arxiv.org/pdf/1506.06204v2.pdf) :star::star::star::star:\n\n**[4]** Dai, J., He, K., Sun, J. "**Instance-aware semantic segmentation via multi-task network cascades**." in CVPR. 2016 [[pdf]](https://arxiv.org/pdf/1512.04412v1.pdf) :star::star::star:\n\n**[5]** Dai, J., He, K., Sun, J. "**Instance-sensitive Fully Convolutional Networks**." arXiv preprint arXiv:1603.08678 (2016). [[pdf]](https://arxiv.org/pdf/1603.08678v1.pdf) :star::star::star:\n\n\n', '{"language":"Python","stars":39379,"forks":7339,"watchers":39379,"open_issues":91,"topics":["deep-learning"],"default_branch":"master","size_kb":3638,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[]', NULL, NULL, 'pending', 70, '33c0664ff7300dbf71c36e2eb87e075b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-floodsung-Deep-Learning-Papers-Reading-Roadmap from https://github.com/floodsung.png
Image converted to WebP: data/images/github-floodsung-Deep-Learning-Papers-Reading-Roadmap.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-naptha-tesseract.js', 'github--naptha--tesseract.js', 'tesseract.js', 'naptha', '<p align="center"> <a href="https://tesseract.projectnaptha.com/"> <picture> <source media="(prefers-color-scheme: dark)" srcset="./docs/images/tesseract_dark.png"> <img width="256px" height="256px" alt="Tesseract.js" src="./docs/images/tesseract.png"> </picture> </a> </p> !Lint & Test !CodeQL !npm !jsDelivr hits (npm) Tesseract.js is a javascript library that gets words in almost any language out of images. (Demo) Image Recognition Video Real-time Recognition <p align="center"> <a href="http...', '["deep-learning","javascript","ocr","tesseract","webassembly","javascript"]', 'other', 37560, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/naptha/tesseract.js","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <a href="https://tesseract.projectnaptha.com/">\n    <picture>\n      <source media="(prefers-color-scheme: dark)" srcset="./docs/images/tesseract_dark.png">\n      <img width="256px" height="256px" alt="Tesseract.js" src="./docs/images/tesseract.png">\n    </picture>\n  </a>\n</p>\n\n![Lint & Test](https://github.com/naptha/tesseract.js/workflows/Node.js%20CI/badge.svg)\n![CodeQL](https://github.com/naptha/tesseract.js/workflows/CodeQL/badge.svg)\n[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://github.com/naptha/tesseract.js) \n[![Financial Contributors on Open Collective](https://opencollective.com/tesseractjs/all/badge.svg?label=financial+contributors)](https://opencollective.com/tesseractjs) [![npm version](https://badge.fury.io/js/tesseract.js.svg)](https://badge.fury.io/js/tesseract.js)\n[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://github.com/naptha/tesseract.js/graphs/commit-activity)\n[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Code Style](https://badgen.net/badge/code%20style/airbnb/ff5a5f?icon=airbnb)](https://github.com/airbnb/javascript)\n![npm](https://img.shields.io/npm/dm/tesseract.js?label=npm%20downloads)\n![jsDelivr hits (npm)](https://img.shields.io/jsdelivr/npm/hm/tesseract.js?label=jsdelivr%20hits)\n\nTesseract.js is a javascript library that gets words in [almost any language](./docs/tesseract_lang_list.md) out of images. ([Demo](http://tesseract.projectnaptha.com/))\n\nImage Recognition\n\n[![fancy demo gif](./docs/images/demo.gif)](http://tesseract.projectnaptha.com)\n\nVideo Real-time Recognition\n\n<p align="center">\n  <a href="https://github.com/jeromewu/tesseract.js-video"><img alt="Tesseract.js Video" src="./docs/images/video-demo.gif"></a>\n</p>\n\nTesseract.js works in the browser using [webpack](https://webpack.js.org/), esm, or plain script tags with a [CDN](#CDN) and on the server with [Node.js](https://nodejs.org/en/).\nAfter you [install it](#installation), using it is as simple as:\n\n```javascript\nimport { createWorker } from ''tesseract.js'';\n\n(async () => {\n  const worker = await createWorker(''eng'');\n  const ret = await worker.recognize(''https://tesseract.projectnaptha.com/img/eng_bw.png'');\n  console.log(ret.data.text);\n  await worker.terminate();\n})();\n```\nWhen recognizing multiple images, users should create a worker once, run `worker.recognize` for each image, and then run `worker.terminate()` once at the end (rather than running the above snippet for every image). \n\n## Installation\nTesseract.js works with a `<script>` tag via local copy or CDN, with webpack via `npm` and on Node.js with `npm/yarn`.\n\n### CDN\n```html\n<!-- v5 -->\n<script src=''https://cdn.jsdelivr.net/npm/tesseract.js@5/dist/tesseract.min.js''></script>\n```\nAfter including the script the `Tesseract` variable will be globally available and a worker can be created using `Tesseract.createWorker`.\n\nAlternatively, an ESM build (used with `import` syntax) can be found at `https://cdn.jsdelivr.net/npm/tesseract.js@5/dist/tesseract.esm.min.js`. \n\n### Node.js\n\n**Requires Node.js v14 or higher**\n\n```shell\n# For latest version\nnpm install tesseract.js\nyarn add tesseract.js\n\n# For old versions\nnpm install tesseract.js@3.0.3\nyarn add tesseract.js@3.0.3\n```\n\n## Project Scope\nTesseract.js aims to bring the [Tesseract](https://github.com/tesseract-ocr/tesseract) OCR engine (a separate project) to the browser and Node.js, and works by wrapping a [WebAssembly port](https://github.com/naptha/tesseract.js-core) of Tesseract.  This project does not modify core Tesseract features.  Most notably, **Tesseract.js does not support PDF files and does not modify the Tesseract recognition model to improve accuracy.**\n\nIf your project requires features outside of this scope, consider the [Scribe.js library](https://github.com/scribeocr/scribe.js).  Scribe.js is an alternative library created to accommodate common feature requests that are outside of the scope of this repo.  Scribe.js includes improvements to the Tesseract recognition model and supports extracting text from PDF documents, among other features.  For more information see [Scribe.js vs. Tesseract.js](https://github.com/scribeocr/scribe.js/blob/master/docs/scribe_vs_tesseract.md).\n\n## Documentation\n\n* [Workers vs. Schedulers](./docs/workers_vs_schedulers.md)\n* [Examples](./docs/examples.md)\n* [Supported Image Formats](./docs/image-format.md)\n* [API](./docs/api.md)\n* [Local Installation](./docs/local-installation.md)\n* [FAQ](./docs/faq.md)\n\n## Community Projects and Examples\nThe following are examples and projects built by the community using Tesseract.js. Officially supported examples are found in the [examples](https://github.com/naptha/tesseract.js/tree/master/examples) directory. \n\n- Projects\n   - Scribe OCR: web application for scanning documents (images and PDFs)\n      - Site at [scribeocr.com](https://scribeocr.com/), repo at [github.com/scribeocr/scribeocr](https://github.com/scribeocr/scribeocr)\n   - Chrome Extension (with Manifest V3): https://github.com/Tshetrim/Image-To-Text-OCR-extension-for-ChatGPT\n- Examples\n   - Converting PDF to text: https://github.com/racosa/pdf2text-ocr\n   - Use `blocks` output to generate granular data [word/symbol level]: https://github.com/Kishlay-notabot/tesseract-bbox-examples\n   - Electron: https://github.com/Balearica/tesseract.js-electron\n   - Typescript: https://github.com/Balearica/tesseract.js-typescript\n \nIf you have a project or example repo that uses Tesseract.js, feel free to add it to this list using a pull request. Examples submitted should be well documented such that new users can run them; projects should be functional and actively maintained.\n\n## Major changes in v6\nVersion 6 changes are documented in [this issue](https://github.com/naptha/tesseract.js/issues/993).  Highlights are below.\n - Fixed memory leak in previous versions\n - Overall reductions in runtime and memory usage\n - Breaking changes:\n    - All outputs formats other than `text` are disabled by default.\n      - To re-enable the `hocr` output (for example), set the following: `worker.recognize(image, {}, { hocr: true })`\n    - Minor changes to the structure of the JavaScript object (`blocks`) output\n    - See [this issue](https://github.com/naptha/tesseract.js/issues/993) for full list\n\n## Major changes in v5\nVersion 5 changes are documented in [this issue](https://github.com/naptha/tesseract.js/issues/820).  Highlights are below.\n\n - Significantly smaller files by default (54% smaller for English, 73% smaller for Chinese)\n    - This results in a ~50% reduction in runtime for first-time users (who do not have the files cached yet)\n - Significantly lower memory usage\n - Breaking changes:\n    - `createWorker` arguments changed\n       - Setting non-default language and OEM now happens in `createWorker`\n          - E.g. `createWorker("chi_sim", 1)`\n    - `worker.initialize` and `worker.loadLanguage` functions should be deleted from code\n    - See [this issue](https://github.com/naptha/tesseract.js/issues/820) for full list\n\nUpgrading from v2 to v5?  See [this guide](https://github.com/naptha/tesseract.js/issues/771).\n\n## Major changes in v4\nVersion 4 includes many new features and bug fixes--see [this issue](https://github.com/naptha/tesseract.js/issues/662) for a full list.  Several highlights are below. \n\n- Added rotation preprocessing options (including auto-rotate) for significantly better accuracy\n- Processed images (rotated, grayscale, binary) can now be retrieved\n- Improved support for parallel processing (schedulers)\n- Breaking changes:\n  - `createWorker` is now async\n  - `getPDF` function replaced by `pdf` recognize option\n\n## Contributing\n\n### Development\nTo run a development copy of Tesseract.js do the following:\n```shell\n# First we clone the repository\ngit clone https://github.com/naptha/tesseract.js.git\ncd tesseract.js\n\n# Then we install the dependencies\nnpm install\n\n# And finally we start the development server\nnpm start\n```\n\nThe development server will be available at http://localhost:3000/examples/browser/basic-efficient.html in your favorite browser.\nIt will automatically rebuild `tesseract.min.js` and `worker.min.js` when you change files in the **src** folder.\n\n### Building Static Files\nTo build the compiled static files just execute the following:\n```shell\nnpm run build\n```\nThis will output the files into the `dist` directory.\n\n### Run Tests\n**Always confirm the automated tests pass before submitting a pull request.**  To run the automated tests locally, run the following commands.\n```shell\nnpm run lint\nnpm run test\n```\n\n## Contributors\n\n### Code Contributors\n\nThis project exists thanks to all the people who contribute. [[Contribute](https://github.com/naptha/tesseract.js?tab=readme-ov-file#contributing)].\n<a href="https://github.com/naptha/tesseract.js/graphs/contributors"><img src="https://opencollective.com/tesseractjs/contributors.svg?width=890&button=false" /></a>\n\n### Financial Contributors\n\nBecome a financial contributor and help us sustain our community. [[Contribute](https://opencollective.com/tesseractjs/contribute)]\n\n#### Individuals\n\n<a href="https://opencollective.com/tesseractjs"><img src="https://opencollective.com/tesseractjs/individuals.svg?width=890"></a>\n\n#### Organizations\n\nSupport this project with your organization. Your logo will show up here with a link to your website. [[Contribute](https://opencollective.com/tesseractjs/contribute)]\n\n<a href="https://opencollective.com/tesseractjs/organization/0/website"><img src="https://opencollective.com/tesseractjs/organization/0/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/1/website"><img src="https://opencollective.com/tesseractjs/organization/1/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/2/website"><img src="https://opencollective.com/tesseractjs/organization/2/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/3/website"><img src="https://opencollective.com/tesseractjs/organization/3/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/4/website"><img src="https://opencollective.com/tesseractjs/organization/4/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/5/website"><img src="https://opencollective.com/tesseractjs/organization/5/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/6/website"><img src="https://opencollective.com/tesseractjs/organization/6/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/7/website"><img src="https://opencollective.com/tesseractjs/organization/7/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/8/website"><img src="https://opencollective.com/tesseractjs/organization/8/avatar.svg"></a>\n<a href="https://opencollective.com/tesseractjs/organization/9/website"><img src="https://opencollective.com/tesseractjs/organization/9/avatar.svg"></a>\n', '{"language":"JavaScript","stars":37560,"forks":2347,"watchers":37560,"open_issues":32,"topics":["deep-learning","javascript","ocr","tesseract","webassembly"],"default_branch":"master","size_kb":109409,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:airbnb:javascript","source_url":"https://github.com/airbnb/javascript"},{"type":"has_code","target_id":"github:jeromewu:tesseract.js-video\"><img","source_url":"https://github.com/jeromewu/tesseract.js-video\"><img"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:naptha:tesseract.js-core","source_url":"https://github.com/naptha/tesseract.js-core"},{"type":"has_code","target_id":"github:scribeocr:scribe.js","source_url":"https://github.com/scribeocr/scribe.js"},{"type":"has_code","target_id":"github:scribeocr:scribe.js","source_url":"https://github.com/scribeocr/scribe.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:scribeocr:scribeocr","source_url":"https://github.com/scribeocr/scribeocr"},{"type":"has_code","target_id":"github:Tshetrim:Image-To-Text-OCR-extension-for-ChatGPT","source_url":"https://github.com/Tshetrim/Image-To-Text-OCR-extension-for-ChatGPT"},{"type":"has_code","target_id":"github:racosa:pdf2text-ocr","source_url":"https://github.com/racosa/pdf2text-ocr"},{"type":"has_code","target_id":"github:Kishlay-notabot:tesseract-bbox-examples","source_url":"https://github.com/Kishlay-notabot/tesseract-bbox-examples"},{"type":"has_code","target_id":"github:Balearica:tesseract.js-electron","source_url":"https://github.com/Balearica/tesseract.js-electron"},{"type":"has_code","target_id":"github:Balearica:tesseract.js-typescript","source_url":"https://github.com/Balearica/tesseract.js-typescript"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"},{"type":"has_code","target_id":"github:naptha:tesseract.js.git","source_url":"https://github.com/naptha/tesseract.js.git"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js?tab=readme-ov-file#contributing"},{"type":"has_code","target_id":"github:naptha:tesseract.js","source_url":"https://github.com/naptha/tesseract.js"}]', NULL, 'Apache-2.0', 'approved', 80, 'c15e542340f89ea1bab186fa33a95835', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-naptha-tesseract.js from https://github.com/naptha.png
Image converted to WebP: data/images/github-naptha-tesseract.js.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-TencentARC-GFPGAN', 'github--tencentarc--gfpgan', 'GFPGAN', 'TencentARC', '<p align="center"> <img src="assets/gfpgan_logo.png" height=130> </p> <div align="center"> <!-- <a href="https://twitter.com/_Xintao_" style="text-decoration:none;"> <img src="https://user-images.githubusercontent.com/17445847/187162058-c764ced6-952f-404b-ac85-ba95cce18e7b.png" width="4%" alt="" /> </a> --> </div> 1. :boom: **Updated** online demo: . Here is the backup. 1. :boom: **Updated** online demo: 1. Colab Demo for GFPGAN <a href="https://colab.research.google.com/drive/1sVsoBd9AjckIXT...', '["deep-learning","face-restoration","gan","gfpgan","image-restoration","pytorch","super-resolution","python"]', 'other', 37261, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/TencentARC/GFPGAN","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <img src="assets/gfpgan_logo.png" height=130>\n</p>\n\n## <div align="center"><b><a href="README.md">English</a> | <a href="README_CN.md">ç®€ä½“ä¸­æ–‡</a></b></div>\n\n<div align="center">\n<!-- <a href="https://twitter.com/_Xintao_" style="text-decoration:none;">\n    <img src="https://user-images.githubusercontent.com/17445847/187162058-c764ced6-952f-404b-ac85-ba95cce18e7b.png" width="4%" alt="" />\n</a> -->\n\n[![download](https://img.shields.io/github/downloads/TencentARC/GFPGAN/total.svg)](https://github.com/TencentARC/GFPGAN/releases)\n[![PyPI](https://img.shields.io/pypi/v/gfpgan)](https://pypi.org/project/gfpgan/)\n[![Open issue](https://img.shields.io/github/issues/TencentARC/GFPGAN)](https://github.com/TencentARC/GFPGAN/issues)\n[![Closed issue](https://img.shields.io/github/issues-closed/TencentARC/GFPGAN)](https://github.com/TencentARC/GFPGAN/issues)\n[![LICENSE](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/TencentARC/GFPGAN/blob/master/LICENSE)\n[![python lint](https://github.com/TencentARC/GFPGAN/actions/workflows/pylint.yml/badge.svg)](https://github.com/TencentARC/GFPGAN/blob/master/.github/workflows/pylint.yml)\n[![Publish-pip](https://github.com/TencentARC/GFPGAN/actions/workflows/publish-pip.yml/badge.svg)](https://github.com/TencentARC/GFPGAN/blob/master/.github/workflows/publish-pip.yml)\n</div>\n\n1. :boom: **Updated** online demo: [![Replicate](https://img.shields.io/static/v1?label=Demo&message=Replicate&color=blue)](https://replicate.com/tencentarc/gfpgan). Here is the [backup](https://replicate.com/xinntao/gfpgan).\n1. :boom: **Updated** online demo: [![Huggingface Gradio](https://img.shields.io/static/v1?label=Demo&message=Huggingface%20Gradio&color=orange)](https://huggingface.co/spaces/Xintao/GFPGAN)\n1. [Colab Demo](https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo) for GFPGAN <a href="https://colab.research.google.com/drive/1sVsoBd9AjckIXThgtZhGrHRfFI6UUYOo"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="google colab logo"></a>; (Another [Colab Demo](https://colab.research.google.com/drive/1Oa1WwKB4M4l1GmR7CtswDVgOCOeSLChA?usp=sharing) for the original paper model)\n\n<!-- 3. Online demo: [Replicate.ai](https://replicate.com/xinntao/gfpgan) (may need to sign in, return the whole image)\n4. Online demo: [Baseten.co](https://app.baseten.co/applications/Q04Lz0d/operator_views/8qZG6Bg) (backed by GPU, returns the whole image)\n5. We provide a *clean* version of GFPGAN, which can run without CUDA extensions. So that it can run in **Windows** or on **CPU mode**. -->\n\n> :rocket: **Thanks for your interest in our work. You may also want to check our new updates on the *tiny models* for *anime images and videos* in [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN/blob/master/docs/anime_video_model.md)** :blush:\n\nGFPGAN aims at developing a **Practical Algorithm for Real-world Face Restoration**.<br>\nIt leverages rich and diverse priors encapsulated in a pretrained face GAN (*e.g.*, StyleGAN2) for blind face restoration.\n\n:question: Frequently Asked Questions can be found in [FAQ.md](FAQ.md).\n\n:triangular_flag_on_post: **Updates**\n\n- :white_check_mark: Add [RestoreFormer](https://github.com/wzhouxiff/RestoreFormer) inference codes.\n- :white_check_mark: Add [V1.4 model](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth), which produces slightly more details and better identity than V1.3.\n- :white_check_mark: Add **[V1.3 model](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth)**, which produces **more natural** restoration results, and better results on *very low-quality* / *high-quality* inputs. See more in [Model zoo](#european_castle-model-zoo), [Comparisons.md](Comparisons.md)\n- :white_check_mark: Integrated to [Huggingface Spaces](https://huggingface.co/spaces) with [Gradio](https://github.com/gradio-app/gradio). See [Gradio Web Demo](https://huggingface.co/spaces/akhaliq/GFPGAN).\n- :white_check_mark: Support enhancing non-face regions (background) with [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN).\n- :white_check_mark: We provide a *clean* version of GFPGAN, which does not require CUDA extensions.\n- :white_check_mark: We provide an updated model without colorizing faces.\n\n---\n\nIf GFPGAN is helpful in your photos/projects, please help to :star: this repo or recommend it to your friends. Thanks:blush:\nOther recommended projects:<br>\n:arrow_forward: [Real-ESRGAN](https://github.com/xinntao/Real-ESRGAN): A practical algorithm for general image restoration<br>\n:arrow_forward: [BasicSR](https://github.com/xinntao/BasicSR): An open-source image and video restoration toolbox<br>\n:arrow_forward: [facexlib](https://github.com/xinntao/facexlib): A collection that provides useful face-relation functions<br>\n:arrow_forward: [HandyView](https://github.com/xinntao/HandyView): A PyQt5-based image viewer that is handy for view and comparison<br>\n\n---\n\n### :book: GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior\n\n> [[Paper](https://arxiv.org/abs/2101.04061)] &emsp; [[Project Page](https://xinntao.github.io/projects/gfpgan)] &emsp; [Demo] <br>\n> [Xintao Wang](https://xinntao.github.io/), [Yu Li](https://yu-li.github.io/), [Honglun Zhang](https://scholar.google.com/citations?hl=en&user=KjQLROoAAAAJ), [Ying Shan](https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en) <br>\n> Applied Research Center (ARC), Tencent PCG\n\n<p align="center">\n  <img src="https://xinntao.github.io/projects/GFPGAN_src/gfpgan_teaser.jpg">\n</p>\n\n---\n\n## :wrench: Dependencies and Installation\n\n- Python >= 3.7 (Recommend to use [Anaconda](https://www.anaconda.com/download/#linux) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html))\n- [PyTorch >= 1.7](https://pytorch.org/)\n- Option: NVIDIA GPU + [CUDA](https://developer.nvidia.com/cuda-downloads)\n- Option: Linux\n\n### Installation\n\nWe now provide a *clean* version of GFPGAN, which does not require customized CUDA extensions. <br>\nIf you want to use the original model in our paper, please see [PaperModel.md](PaperModel.md) for installation.\n\n1. Clone repo\n\n    ```bash\n    git clone https://github.com/TencentARC/GFPGAN.git\n    cd GFPGAN\n    ```\n\n1. Install dependent packages\n\n    ```bash\n    # Install basicsr - https://github.com/xinntao/BasicSR\n    # We use BasicSR for both training and inference\n    pip install basicsr\n\n    # Install facexlib - https://github.com/xinntao/facexlib\n    # We use face detection and face restoration helper in the facexlib package\n    pip install facexlib\n\n    pip install -r requirements.txt\n    python setup.py develop\n\n    # If you want to enhance the background (non-face) regions with Real-ESRGAN,\n    # you also need to install the realesrgan package\n    pip install realesrgan\n    ```\n\n## :zap: Quick Inference\n\nWe take the v1.3 version for an example. More models can be found [here](#european_castle-model-zoo).\n\nDownload pre-trained models: [GFPGANv1.3.pth](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth)\n\n```bash\nwget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P experiments/pretrained_models\n```\n\n**Inference!**\n\n```bash\npython inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2\n```\n\n```console\nUsage: python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.3 -s 2 [options]...\n\n  -h                   show this help\n  -i input             Input image or folder. Default: inputs/whole_imgs\n  -o output            Output folder. Default: results\n  -v version           GFPGAN model version. Option: 1 | 1.2 | 1.3. Default: 1.3\n  -s upscale           The final upsampling scale of the image. Default: 2\n  -bg_upsampler        background upsampler. Default: realesrgan\n  -bg_tile             Tile size for background sampler, 0 for no tile during testing. Default: 400\n  -suffix              Suffix of the restored faces\n  -only_center_face    Only restore the center face\n  -aligned             Input are aligned faces\n  -ext                 Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto\n```\n\nIf you want to use the original model in our paper, please see [PaperModel.md](PaperModel.md) for installation and inference.\n\n## :european_castle: Model Zoo\n\n| Version | Model Name  | Description |\n| :---: | :---:        |     :---:      |\n| V1.3 | [GFPGANv1.3.pth](https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth) | Based on V1.2; **more natural** restoration results; better results on very low-quality / high-quality inputs. |\n| V1.2 | [GFPGANCleanv1-NoCE-C2.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth) | No colorization; no CUDA extensions are required. Trained with more data with pre-processing. |\n| V1 | [GFPGANv1.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANv1.pth) | The paper model, with colorization. |\n\nThe comparisons are in [Comparisons.md](Comparisons.md).\n\nNote that V1.3 is not always better than V1.2. You may need to select different models based on your purpose and inputs.\n\n| Version | Strengths  | Weaknesses |\n| :---: | :---:        |     :---:      |\n|V1.3 |  âœ“ natural outputs<br> âœ“better results on very low-quality inputs <br> âœ“ work on relatively high-quality inputs <br>âœ“ can have repeated (twice) restorations | âœ— not very sharp <br> âœ— have a slight change on identity |\n|V1.2 |  âœ“ sharper output <br> âœ“ with beauty makeup | âœ— some outputs are unnatural |\n\nYou can find **more models (such as the discriminators)** here: [[Google Drive](https://drive.google.com/drive/folders/17rLiFzcUMoQuhLnptDsKolegHWwJOnHu?usp=sharing)], OR [[Tencent Cloud è…¾è®¯å¾®äº‘](https://share.weiyun.com/ShYoCCoc)]\n\n## :computer: Training\n\nWe provide the training codes for GFPGAN (used in our paper). <br>\nYou could improve it according to your own needs.\n\n**Tips**\n\n1. More high quality faces can improve the restoration quality.\n2. You may need to perform some pre-processing, such as beauty makeup.\n\n**Procedures**\n\n(You can try a simple version ( `options/train_gfpgan_v1_simple.yml`) that does not require face component landmarks.)\n\n1. Dataset preparation: [FFHQ](https://github.com/NVlabs/ffhq-dataset)\n\n1. Download pre-trained models and other data. Put them in the `experiments/pretrained_models` folder.\n    1. [Pre-trained StyleGAN2 model: StyleGAN2_512_Cmul1_FFHQ_B12G4_scratch_800k.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/StyleGAN2_512_Cmul1_FFHQ_B12G4_scratch_800k.pth)\n    1. [Component locations of FFHQ: FFHQ_eye_mouth_landmarks_512.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/FFHQ_eye_mouth_landmarks_512.pth)\n    1. [A simple ArcFace model: arcface_resnet18.pth](https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/arcface_resnet18.pth)\n\n1. Modify the configuration file `options/train_gfpgan_v1.yml` accordingly.\n\n1. Training\n\n> python -m torch.distributed.launch --nproc_per_node=4 --master_port=22021 gfpgan/train.py -opt options/train_gfpgan_v1.yml --launcher pytorch\n\n## :scroll: License and Acknowledgement\n\nGFPGAN is released under Apache License Version 2.0.\n\n## BibTeX\n\n    @InProceedings{wang2021gfpgan,\n        author = {Xintao Wang and Yu Li and Honglun Zhang and Ying Shan},\n        title = {Towards Real-World Blind Face Restoration with Generative Facial Prior},\n        booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n        year = {2021}\n    }\n\n## :e-mail: Contact\n\nIf you have any question, please email `xintao.wang@outlook.com` or `xintaowang@tencent.com`.\n', '{"language":"Python","stars":37261,"forks":6251,"watchers":37261,"open_issues":397,"topics":["deep-learning","face-restoration","gan","gfpgan","image-restoration","pytorch","super-resolution"],"default_branch":"master","size_kb":5467,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:xinntao:Real-ESRGAN","source_url":"https://github.com/xinntao/Real-ESRGAN"},{"type":"has_code","target_id":"github:wzhouxiff:RestoreFormer","source_url":"https://github.com/wzhouxiff/RestoreFormer"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:xinntao:Real-ESRGAN","source_url":"https://github.com/xinntao/Real-ESRGAN"},{"type":"has_code","target_id":"github:xinntao:Real-ESRGAN","source_url":"https://github.com/xinntao/Real-ESRGAN"},{"type":"has_code","target_id":"github:xinntao:BasicSR","source_url":"https://github.com/xinntao/BasicSR"},{"type":"has_code","target_id":"github:xinntao:facexlib","source_url":"https://github.com/xinntao/facexlib"},{"type":"has_code","target_id":"github:xinntao:HandyView","source_url":"https://github.com/xinntao/HandyView"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN.git","source_url":"https://github.com/TencentARC/GFPGAN.git"},{"type":"has_code","target_id":"github:xinntao:BasicSR","source_url":"https://github.com/xinntao/BasicSR"},{"type":"has_code","target_id":"github:xinntao:facexlib","source_url":"https://github.com/xinntao/facexlib"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:NVlabs:ffhq-dataset","source_url":"https://github.com/NVlabs/ffhq-dataset"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN","source_url":"https://github.com/TencentARC/GFPGAN"}]', NULL, 'NOASSERTION', 'approved', 80, 'deafc64194ae3c25aa7fc67b5e376707', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-TencentARC-GFPGAN from https://github.com/TencentARC.png
Image converted to WebP: data/images/github-TencentARC-GFPGAN.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-babysor-MockingBird', 'github--babysor--mockingbird', 'MockingBird', 'babysor', '> ğŸš§ While I no longer actively update this repo, you can find me continuously pushing this tech forward to good side and open-source. I''m also building an optimized and cloud hosted version: https://noiz.ai/ and it''s free but not ready for commersial use now. > !mockingbird <a href="https://trendshift.io/repositories/3869" target="_blank"><img src="https://trendshift.io/api/badge/repositories/3869" alt="babysor%2FMockingBird | Trendshift" style="width: 250px; height: 55px;" width="250" heigh...', '["ai","deep-learning","pytorch","speech","text-to-speech","tts","python"]', 'other', 36801, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/babysor/MockingBird","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '> ğŸš§ While I no longer actively update this repo, you can find me continuously pushing this tech forward to good side and open-source. I''m also building an optimized and cloud hosted version: https://noiz.ai/ and it''s free but not ready for commersial use now.\n>\n![mockingbird](https://user-images.githubusercontent.com/12797292/131216767-6eb251d6-14fc-4951-8324-2722f0cd4c63.jpg)\n<a href="https://trendshift.io/repositories/3869" target="_blank"><img src="https://trendshift.io/api/badge/repositories/3869" alt="babysor%2FMockingBird | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n\n[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat)](http://choosealicense.com/licenses/mit/)\n\n> English | [ä¸­æ–‡](README-CN.md)| [ä¸­æ–‡Linux](README-LINUX-CN.md)\n\n## Features\nğŸŒ **Chinese** supported mandarin and tested with multiple datasets: aidatatang_200zh, magicdata, aishell3, data_aishell, and etc.\n\nğŸ¤© **PyTorch** worked for pytorch, tested in version of 1.9.0(latest in August 2021), with GPU Tesla T4 and GTX 2060\n\nğŸŒ **Windows + Linux** run in both Windows OS and linux OS (even in M1 MACOS)\n\nğŸ¤© **Easy & Awesome** effect with only newly-trained synthesizer, by reusing the pretrained encoder/vocoder\n\nğŸŒ **Webserver Ready** to serve your result with remote calling\n\n### [DEMO VIDEO](https://www.bilibili.com/video/BV17Q4y1B7mY/)\n\n## Quick Start\n\n### 1. Install Requirements\n#### 1.1 General Setup\n> Follow the original repo to test if you got all environment ready.\n**Python 3.7 or higher ** is needed to run the toolbox.\n\n* Install [PyTorch](https://pytorch.org/get-started/locally/).\n> If you get an `ERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu102 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2 )` This error is probably due to a low version of python, try using 3.9 and it will install successfully\n* Install [ffmpeg](https://ffmpeg.org/download.html#get-packages).\n* Run `pip install -r requirements.txt` to install the remaining necessary packages.\n> The recommended environment here is `Repo Tag 0.0.1` `Pytorch1.9.0 with Torchvision0.10.0 and cudatoolkit10.2` `requirements.txt` `webrtcvad-wheels` because `requirements. txt` was exported a few months ago, so it doesn''t work with newer versions\n* Install webrtcvad `pip install webrtcvad-wheels`(If you need)\n\nor\n- install dependencies withÂ `conda`Â orÂ `mamba`\n\n  ```conda env create -n env_name -f env.yml```\n\n  ```mamba env create -n env_name -f env.yml```\n\n  will create a virtual environment where necessary dependencies are installed. Switch to the new environment byÂ `conda activate env_name`Â and enjoy it.\n  > env.yml only includes the necessary dependencies to run the projectï¼Œtemporarily without monotonic-align. You can check the official website to install the GPU version of pytorch.\n\n#### 1.2 Setup with a M1 Mac\n> The following steps are a workaround to directly use the original `demo_toolbox.py`without the changing of codes.\n>\n  >  Since the major issue comes with the PyQt5 packages used in `demo_toolbox.py` not compatible with M1 chips, were one to attempt on training models with the M1 chip, either that person can forgo `demo_toolbox.py`, or one can try the `web.py` in the project.\n\n##### 1.2.1 Install `PyQt5`, with [ref](https://stackoverflow.com/a/68038451/20455983) here.\n  * Create and open a Rosetta Terminal, with [ref](https://dev.to/courier/tips-and-tricks-to-setup-your-apple-m1-for-development-547g) here.\n  * Use system Python to create a virtual environment for the project\n    ```\n    /usr/bin/python3 -m venv /PathToMockingBird/venv\n    source /PathToMockingBird/venv/bin/activate\n    ```\n  * Upgrade pip and install `PyQt5`\n    ```\n    pip install --upgrade pip\n    pip install pyqt5\n    ```\n##### 1.2.2 Install `pyworld` and `ctc-segmentation`\n\n> Both packages seem to be unique to this project and are not seen in the original [Real-Time Voice Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning) project. When installing with `pip install`, both packages lack wheels so the program tries to directly compile from c code and could not find `Python.h`.\n\n  * Install `pyworld`\n      * `brew install python` `Python.h` can come with Python installed by brew\n      * `export CPLUS_INCLUDE_PATH=/opt/homebrew/Frameworks/Python.framework/Headers` The filepath of brew-installed `Python.h` is unique to M1 MacOS and listed above. One needs to manually add the path to the environment variables.\n      * `pip install pyworld` that should do.\n\n\n  * Install`ctc-segmentation`\n    > Same method does not apply to `ctc-segmentation`, and one needs to compile it from the source code on [github](https://github.com/lumaku/ctc-segmentation).\n    * `git clone https://github.com/lumaku/ctc-segmentation.git`\n    * `cd ctc-segmentation`\n    * `source /PathToMockingBird/venv/bin/activate` If the virtual environment hasn''t been deployed, activate it.\n    * `cythonize -3 ctc_segmentation/ctc_segmentation_dyn.pyx`\n    * `/usr/bin/arch -x86_64 python setup.py build` Build with x86 architecture.\n    * `/usr/bin/arch -x86_64 python setup.py install --optimize=1 --skip-build`Install with x86 architecture.\n\n##### 1.2.3 Other dependencies\n  * `/usr/bin/arch -x86_64 pip install torch torchvision torchaudio` Pip installing `PyTorch` as an example, articulate that it''s installed with x86 architecture\n  * `pip install ffmpeg`  Install ffmpeg\n  * `pip install -r requirements.txt` Install other requirements.\n\n##### 1.2.4 Run the Inference Time (with Toolbox)\n  > To run the project on x86 architecture. [ref](https://youtrack.jetbrains.com/issue/PY-46290/Allow-running-Python-under-Rosetta-2-in-PyCharm-for-Apple-Silicon).\n  * `vim /PathToMockingBird/venv/bin/pythonM1` Create an executable file `pythonM1` to condition python interpreter at `/PathToMockingBird/venv/bin`.\n  * Write in the following content:\n    ```\n    #!/usr/bin/env zsh\n    mydir=${0:a:h}\n    /usr/bin/arch -x86_64 $mydir/python "$@"\n    ```\n  * `chmod +x pythonM1` Set the file as executable.\n  * If using PyCharm IDE, configure project interpreter to `pythonM1`([steps here](https://www.jetbrains.com/help/pycharm/configuring-python-interpreter.html#add-existing-interpreter)), if using command line python, run `/PathToMockingBird/venv/bin/pythonM1 demo_toolbox.py`\n\n\n### 2. Prepare your models\n> Note that we are using the pretrained encoder/vocoder but not synthesizer, since the original model is incompatible with the Chinese symbols. It means the demo_cli is not working at this moment, so additional synthesizer models are required.\n\nYou can either train your models or use existing ones:\n\n#### 2.1 Train encoder with your dataset (Optional)\n\n* Preprocess with the audios and the mel spectrograms:\n`python encoder_preprocess.py <datasets_root>` Allowing parameter `--dataset {dataset}` to support the datasets you want to preprocess. Only the train set of these datasets will be used. Possible names: librispeech_other, voxceleb1, voxceleb2. Use comma to sperate multiple datasets.\n\n* Train the encoder: `python encoder_train.py my_run <datasets_root>/SV2TTS/encoder`\n> For training, the encoder uses visdom. You can disable it with `--no_visdom`, but it''s nice to have. Run "visdom" in a separate CLI/process to start your visdom server.\n\n#### 2.2 Train synthesizer with your dataset\n* Download dataset and unzip: make sure you can access all .wav in folder\n* Preprocess with the audios and the mel spectrograms:\n`python pre.py <datasets_root>`\nAllowing parameter `--dataset {dataset}` to support aidatatang_200zh, magicdata, aishell3, data_aishell, etc.If this parameter is not passed, the default dataset will be aidatatang_200zh.\n\n* Train the synthesizer:\n`python train.py --type=synth mandarin <datasets_root>/SV2TTS/synthesizer`\n\n* Go to next step when you see attention line show and loss meet your need in training folder *synthesizer/saved_models/*.\n\n#### 2.3 Use pretrained model of synthesizer\n> Thanks to the community, some models will be shared:\n\n| author | Download link | Preview Video | Info |\n| --- | ----------- | ----- |----- |\n| @author | https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g  [Baidu](https://pan.baidu.com/s/1iONvRxmkI-t1nHqxKytY3g) 4j5d  |  | 75k steps trained by multiple datasets\n| @author | https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw  [Baidu](https://pan.baidu.com/s/1fMh9IlgKJlL2PIiRTYDUvw) codeï¼šom7f  |  | 25k steps trained by multiple datasets, only works under version 0.0.1\n|@FawenYo | https://yisiou-my.sharepoint.com/:u:/g/personal/lawrence_cheng_fawenyo_onmicrosoft_com/EWFWDHzee-NNg9TWdKckCc4BC7bK2j9cCbOWn0-_tK0nOg?e=n0gGgC  | [input](https://github.com/babysor/MockingBird/wiki/audio/self_test.mp3) [output](https://github.com/babysor/MockingBird/wiki/audio/export.wav) | 200k steps with local accent of Taiwan, only works under version 0.0.1\n|@miven| https://pan.baidu.com/s/1PI-hM3sn5wbeChRryX-RCQ code: 2021 https://www.aliyundrive.com/s/AwPsbo8mcSP code: z2m0 | https://www.bilibili.com/video/BV1uh411B7AD/ | only works under version 0.0.1\n\n#### 2.4 Train vocoder (Optional)\n> note: vocoder has little difference in effect, so you may not need to train a new one.\n* Preprocess the data:\n`python vocoder_preprocess.py <datasets_root> -m <synthesizer_model_path>`\n> `<datasets_root>` replace with your dataset rootï¼Œ`<synthesizer_model_path>`replace with directory of your best trained models of sythensizer, e.g. *sythensizer\saved_mode\xxx*\n\n* Train the wavernn vocoder:\n`python vocoder_train.py mandarin <datasets_root>`\n\n* Train the hifigan vocoder\n`python vocoder_train.py mandarin <datasets_root> hifigan`\n\n### 3. Launch\n#### 3.1 Using the web server\nYou can then try to run:`python web.py` and open it in browser, default as `http://localhost:8080`\n\n#### 3.2 Using the Toolbox\nYou can then try the toolbox:\n`python demo_toolbox.py -d <datasets_root>`\n\n#### 3.3 Using the command line\nYou can then try the command:\n`python gen_voice.py <text_file.txt> your_wav_file.wav`\nyou may need to install cn2an by "pip install cn2an" for better digital number result.\n\n## Reference\n> This repository is forked from [Real-Time-Voice-Cloning](https://github.com/CorentinJ/Real-Time-Voice-Cloning) which only support English.\n\n| URL | Designation | Title | Implementation source |\n| --- | ----------- | ----- | --------------------- |\n| [1803.09017](https://arxiv.org/abs/1803.09017) | GlobalStyleToken (synthesizer)| Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis | This repo |\n| [2010.05646](https://arxiv.org/abs/2010.05646) | HiFi-GAN (vocoder)| Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis | This repo |\n| [2106.02297](https://arxiv.org/abs/2106.02297) | Fre-GAN (vocoder)| Fre-GAN: Adversarial Frequency-consistent Audio Synthesis | This repo |\n|[**1806.04558**](https://arxiv.org/pdf/1806.04558.pdf) | **SV2TTS** | **Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis** | This repo |\n|[1802.08435](https://arxiv.org/pdf/1802.08435.pdf) | WaveRNN (vocoder) | Efficient Neural Audio Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) |\n|[1703.10135](https://arxiv.org/pdf/1703.10135.pdf) | Tacotron (synthesizer) | Tacotron: Towards End-to-End Speech Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN)\n|[1710.10467](https://arxiv.org/pdf/1710.10467.pdf) | GE2E (encoder)| Generalized End-To-End Loss for Speaker Verification | This repo |\n\n## F Q&A\n#### 1.Where can I download the dataset?\n| Dataset | Original Source | Alternative Sources |\n| --- | ----------- | ---------------|\n| aidatatang_200zh | [OpenSLR](http://www.openslr.org/62/) | [Google Drive](https://drive.google.com/file/d/110A11KZoVe7vy6kXlLb6zVPLb_J91I_t/view?usp=sharing) |\n| magicdata | [OpenSLR](http://www.openslr.org/68/) | [Google Drive (Dev set)](https://drive.google.com/file/d/1g5bWRUSNH68ycC6eNvtwh07nX3QhOOlo/view?usp=sharing) |\n| aishell3 | [OpenSLR](https://www.openslr.org/93/) | [Google Drive](https://drive.google.com/file/d/1shYp_o4Z0X0cZSKQDtFirct2luFUwKzZ/view?usp=sharing) |\n| data_aishell | [OpenSLR](https://www.openslr.org/33/) |  |\n> After unzip aidatatang_200zh, you need to unzip all the files under `aidatatang_200zh\corpus\train`\n\n#### 2.What is`<datasets_root>`?\nIf the dataset path is `D:\data\aidatatang_200zh`,then `<datasets_root>` is`D:\data`\n\n#### 3.Not enough VRAM\nTrain the synthesizerï¼šadjust the batch_size in `synthesizer/hparams.py`\n```\n//Before\ntts_schedule = [(2,  1e-3,  20_000,  12),   # Progressive training schedule\n                (2,  5e-4,  40_000,  12),   # (r, lr, step, batch_size)\n                (2,  2e-4,  80_000,  12),   #\n                (2,  1e-4, 160_000,  12),   # r = reduction factor (# of mel frames\n                (2,  3e-5, 320_000,  12),   #     synthesized for each decoder iteration)\n                (2,  1e-5, 640_000,  12)],  # lr = learning rate\n//After\ntts_schedule = [(2,  1e-3,  20_000,  8),   # Progressive training schedule\n                (2,  5e-4,  40_000,  8),   # (r, lr, step, batch_size)\n                (2,  2e-4,  80_000,  8),   #\n                (2,  1e-4, 160_000,  8),   # r = reduction factor (# of mel frames\n                (2,  3e-5, 320_000,  8),   #     synthesized for each decoder iteration)\n                (2,  1e-5, 640_000,  8)],  # lr = learning rate\n```\n\nTrain Vocoder-Preprocess the dataï¼šadjust the batch_size in `synthesizer/hparams.py`\n```\n//Before\n### Data Preprocessing\n        max_mel_frames = 900,\n        rescale = True,\n        rescaling_max = 0.9,\n        synthesis_batch_size = 16,                  # For vocoder preprocessing and inference.\n//After\n### Data Preprocessing\n        max_mel_frames = 900,\n        rescale = True,\n        rescaling_max = 0.9,\n        synthesis_batch_size = 8,                  # For vocoder preprocessing and inference.\n```\n\nTrain Vocoder-Train the vocoderï¼šadjust the batch_size in `vocoder/wavernn/hparams.py`\n```\n//Before\n# Training\nvoc_batch_size = 100\nvoc_lr = 1e-4\nvoc_gen_at_checkpoint = 5\nvoc_pad = 2\n\n//After\n# Training\nvoc_batch_size = 6\nvoc_lr = 1e-4\nvoc_gen_at_checkpoint = 5\nvoc_pad =2\n```\n\n#### 4.If it happens `RuntimeError: Error(s) in loading state_dict for Tacotron: size mismatch for encoder.embedding.weight: copying a param with shape torch.Size([70, 512]) from checkpoint, the shape in current model is torch.Size([75, 512]).`\nPlease refer to issue [#37](https://github.com/babysor/MockingBird/issues/37)\n\n#### 5. How to improve CPU and GPU occupancy rate?\nAdjust the batch_size as appropriate to improve\n\n\n#### 6. What if it happens `the page file is too small to complete the operation`\nPlease refer to this [video](https://www.youtube.com/watch?v=Oh6dga-Oy10&ab_channel=CodeProf) and change the virtual memory to 100G (102400), for example : When the file is placed in the D disk, the virtual memory of the D disk is changed.\n\n#### 7. When should I stop during training?\nFYI, my attention came after 18k steps and loss became lower than 0.4 after 50k steps.\n![attention_step_20500_sample_1](https://user-images.githubusercontent.com/7423248/128587252-f669f05a-f411-4811-8784-222156ea5e9d.png)\n![step-135500-mel-spectrogram_sample_1](https://user-images.githubusercontent.com/7423248/128587255-4945faa0-5517-46ea-b173-928eff999330.png)\n', '{"language":"Python","stars":36801,"forks":5271,"watchers":36801,"open_issues":481,"topics":["ai","deep-learning","pytorch","speech","text-to-speech","tts"],"default_branch":"main","size_kb":130670,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:CorentinJ:Real-Time-Voice-Cloning","source_url":"https://github.com/CorentinJ/Real-Time-Voice-Cloning"},{"type":"has_code","target_id":"github:lumaku:ctc-segmentation","source_url":"https://github.com/lumaku/ctc-segmentation"},{"type":"has_code","target_id":"github:lumaku:ctc-segmentation.git`","source_url":"https://github.com/lumaku/ctc-segmentation.git`"},{"type":"has_code","target_id":"github:babysor:MockingBird","source_url":"https://github.com/babysor/MockingBird"},{"type":"has_code","target_id":"github:babysor:MockingBird","source_url":"https://github.com/babysor/MockingBird"},{"type":"has_code","target_id":"github:CorentinJ:Real-Time-Voice-Cloning","source_url":"https://github.com/CorentinJ/Real-Time-Voice-Cloning"},{"type":"has_code","target_id":"github:fatchord:WaveRNN","source_url":"https://github.com/fatchord/WaveRNN"},{"type":"has_code","target_id":"github:fatchord:WaveRNN","source_url":"https://github.com/fatchord/WaveRNN"},{"type":"has_code","target_id":"github:babysor:MockingBird","source_url":"https://github.com/babysor/MockingBird"}]', NULL, 'NOASSERTION', 'approved', 80, '473fbc62234591e753af7239acb679bb', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-babysor-MockingBird from https://github.com/babysor.png
Image converted to WebP: data/images/github-babysor-MockingBird.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-mli-paper-reading', 'github--mli--paper-reading', 'paper-reading', 'mli', '| æ—¥æœŸ | æ ‡é¢˜ | å°é¢ | æ—¶é•¿ | è§†é¢‘ï¼ˆæ’­æ”¾æ•°ï¼‰ | | --: | -- | -- | --: | -- | | 1/10/25 | OpenAI Sora ä¸Š<br />(åŒ…å«Movie Genå’ŒHunyuanVideo) | <img src="imgs/sora.jpg" width="200px"/> | 1:04:18 | <br /> | | 9/04/24 | Llama 3.1è®ºæ–‡ç²¾è¯» Â· 5. æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ | <img src="imgs/llama3-process.jpg" width="200px"/> | 10:41| <br /> | | 8/28/24 | Llama 3.1è®ºæ–‡ç²¾è¯» Â· 4. è®­ç»ƒinfra | <img src="imgs/llama3-training-infra.webp" width="200px"/> | 25:04| <br /> | | 8/13/24 | Llama 3.1è®ºæ–‡ç²¾è¯» Â· 3. æ¨¡å‹ | <img src="imgs/llama3-model.webp" width="200px"/> |...', '["deep-learning","paper","reading-list"]', 'other', 32098, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/mli/paper-reading","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# æ·±åº¦å­¦ä¹ è®ºæ–‡ç²¾è¯»\n\n## å½•åˆ¶å®Œæˆçš„è®ºæ–‡\n\n| æ—¥æœŸ | æ ‡é¢˜ | å°é¢ | æ—¶é•¿ | è§†é¢‘ï¼ˆæ’­æ”¾æ•°ï¼‰ |\n| --: | -- | -- | --: | -- |\n| 1/10/25 | [OpenAI Sora](https://openai.com/index/video-generation-models-as-world-simulators/) ä¸Š<br />(åŒ…å«Movie Genå’ŒHunyuanVideo) | <img src="imgs/sora.jpg" width="200px"/> | 1:04:18 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1VdcxesEAt)](https://www.bilibili.com/video/BV1VdcxesEAt/?share_source=copy_web&vd_source=5d037e935914fc22e2e978cdccf5cdfe)<br />[![](https://img.shields.io/youtube/views/5MGq7dSOghY?style=social)](https://youtu.be/5MGq7dSOghY?si=lY-OsadDsTeKf-ub)  |\n| 9/04/24 | Llama 3.1è®ºæ–‡ç²¾è¯» Â· 5. æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ | <img src="imgs/llama3-process.jpg" width="200px"/> | 10:41| [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1c8HbeaEXi)](https://www.bilibili.com/video/BV1c8HbeaEXi)<br />  |\n| 8/28/24 | Llama 3.1è®ºæ–‡ç²¾è¯» Â· 4. è®­ç»ƒinfra | <img src="imgs/llama3-training-infra.webp" width="200px"/> | 25:04| [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1b4421f7fa)](https://www.bilibili.com/video/BV1b4421f7fa)<br />[![](https://img.shields.io/youtube/views/6XidEHVjS1A?style=social)](https://www.youtube.com/watch?v=6XidEHVjS1A)  |\n| 8/13/24 | Llama 3.1è®ºæ–‡ç²¾è¯» Â· 3. æ¨¡å‹ | <img src="imgs/llama3-model.webp" width="200px"/> | 26:14| [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Q4421Z7Tj)](https://www.bilibili.com/video/BV1Q4421Z7Tj)<br />[![](https://img.shields.io/youtube/views/G6gF-5g1Gg4?style=social)](https://www.youtube.com/watch?v=G6gF-5g1Gg4)  |\n| 8/05/24 | [Llama 3.1è®ºæ–‡ç²¾è¯» Â· 2. é¢„è®­ç»ƒæ•°æ®](https://arxiv.org/pdf/2407.21783) | <img src="imgs/llama3-pretrain-data.jpg" width="200px"/> | 23:37| [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1u142187S5)](https://www.bilibili.com/video/BV1u142187S5)[![](https://img.shields.io/youtube/views/wXFr3zIE8FM?style=social)](https://www.youtube.com/watch?v=wXFr3zIE8FM)|\n| 7/31/24 | Llama 3.1è®ºæ–‡ç²¾è¯» Â· 1. å¯¼è¨€ | <img src="imgs/llama3-intro.jpg" width="200px"/> | 18:53| [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1WM4m1y7Uh)](https://www.bilibili.com/video/BV1WM4m1y7Uh)<br />[![](https://img.shields.io/youtube/views/-PztagF3wQE?style=social)](https://www.youtube.com/watch?v=-PztagF3wQE)  |\n| 3/30/23 | [GPT-4](https://openai.com/research/gpt-4) | <img src="imgs/gpt4.jpg" width="200px"/> | 1:20:38 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1vM4y1U7b5)](https://www.bilibili.com/video/BV1vM4y1U7b5)<br />[![](https://img.shields.io/youtube/views/K0SZ9mdygTw?style=social)](https://youtu.be/K0SZ9mdygTw)  |\n| 3/23/23 | å¤§æ¨¡å‹æ—¶ä»£ä¸‹åšç§‘ç ”çš„å››ä¸ªæ€è·¯ | <img src="imgs/limited-resources.jpg" width="200px"/> | 1:06:29 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1oX4y1d7X6)](https://www.bilibili.com/video/BV1oX4y1d7X6)<br />[![](https://img.shields.io/youtube/views/sh79Z8i15PI?style=social)](https://youtu.be/sh79Z8i15PI) |\n| 3/10/23 | [Anthropic LLM](https://arxiv.org/pdf/2204.05862.pdf) | <img src="imgs/anthropic_lm.jpg" width="200px"/> | 1:01:51 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1XY411B7nM)](https://www.bilibili.com/video/BV1XY411B7nM)<br />[![](https://img.shields.io/youtube/views/iqX0pgNDon0?style=social)](https://youtu.be/iqX0pgNDon0) |\n| 1/20/23 | [Helm](https://arxiv.org/pdf/2211.09110.pdf) å…¨é¢è¯­è¨€æ¨¡å‹è¯„æµ‹ | <img src="imgs/helm.jpg" width="200px"/> | 1:23:37 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1z24y1B7uX)](https://www.bilibili.com/video/BV1z24y1B7uX)<br />[![](https://img.shields.io/youtube/views/WgFEw9U3BXA?style=social)](https://youtu.be/WgFEw9U3BXA) |\n| 1/11/23 | å¤šæ¨¡æ€è®ºæ–‡ä¸²è®²Â·ä¸‹ |  <img src="imgs/multimodal-2.jpg" width="200px"/> | 1:03:29 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1fA411Z772)](https://www.bilibili.com/video/BV1fA411Z772) <br />[![](https://img.shields.io/youtube/views/S1le41J76lQ?style=social)](https://youtu.be/S1le41J76lQ) |\n| 12/29/22 | [Instruct GPT](https://arxiv.org/pdf/2203.02155.pdf) | <img src="imgs/instruct-gpt.jpg" width="200px"/> | 1:07:10 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1hd4y187CR)](https://www.bilibili.com/video/BV1hd4y187CR) <br />[![](https://img.shields.io/youtube/views/zfIGAwD1jOQ?style=social)](https://youtu.be/zfIGAwD1jOQ) |\n| 12/19/22 | [Neural Corpus Indexer](https://arxiv.org/pdf/2206.02743.pdf) æ–‡æ¡£æ£€ç´¢ | <img src="imgs/nci.jpg" width="200px"/> | 55:47 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Se411w7Sn)](https://www.bilibili.com/video/BV1Se411w7Sn) <br />[![](https://img.shields.io/youtube/views/QRffZMSGJyU?style=social)](https://youtu.be/QRffZMSGJyU) |\n| 12/12/22 | å¤šæ¨¡æ€è®ºæ–‡ä¸²è®²Â·ä¸Š | <img src="imgs/multimodal-1.jpg" width="200px"/> | 1:12:27 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Vd4y1v77v)](https://www.bilibili.com/video/BV1Vd4y1v77v) <br />[![](https://img.shields.io/youtube/views/6pzBOQAXUB8?style=social)](https://youtu.be/6pzBOQAXUB8)  |\n| 11/14/22 | [OpenAI Whisper](https://cdn.openai.com/papers/whisper.pdf) ç²¾è¯» | <img src="imgs/whisper.jpg" width="200px"/> | 1:12:16 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1VG4y1t74x)](https://www.bilibili.com/video/BV1VG4y1t74x) <br />[![](https://img.shields.io/youtube/views/3eXCJd32UnM?style=social)](https://youtu.be/3eXCJd32UnM) |\n| 11/07/22 | åœ¨è®² OpenAI Whisper å‰å…ˆåšäº†ä¸€ä¸ªå‰ªè§†é¢‘å°å·¥å…· | <img src="imgs/autocut.jpg" width="200px"/> | 23:39 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Pe4y1t7de)](https://www.bilibili.com/video/BV1Pe4y1t7de) <br />[![](https://img.shields.io/youtube/views/PwVlvCPDnrI?style=social)](https://youtu.be/PwVlvCPDnrI)  |\n| 10/23/22 | [Chain of Thought](https://arxiv.org/pdf/2201.11903.pdf) è®ºæ–‡ã€ä»£ç å’Œèµ„æº | <img src="imgs/cot.jpg" width="200px"/> | 33:21 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1t8411e7Ug)](https://www.bilibili.com/video/BV1t8411e7Ug)<br />[![](https://img.shields.io/youtube/views/H4J59iG3t5o?style=social)](https://youtu.be/H4J59iG3t5o) |\n| 9/17/22 | CLIP æ”¹è¿›å·¥ä½œä¸²è®²ï¼ˆä¸‹ï¼‰ | <img src="imgs/clipx-part2.jpg" width="200px"/> | 1:04:26 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1gg411U7n4)](https://www.bilibili.com/video/BV1gg411U7n4)<br />[![](https://img.shields.io/youtube/views/ugJeBivv65s?style=social)](https://youtu.be/ugJeBivv65s) |\n| 9/2/22 | CLIP æ”¹è¿›å·¥ä½œä¸²è®²ï¼ˆä¸Šï¼‰ | <img src="imgs/clipx-part1.jpg" width="200px"/> | 1:14:43 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1FV4y1p7Lm)](https://www.bilibili.com/video/BV1FV4y1p7Lm)<br />[![](https://img.shields.io/youtube/views/x4CDhZz_Dvg?style=social)](https://youtu.be/x4CDhZz_Dvg) |\n| 7/29/22 | [ViLT](https://arxiv.org/pdf/2102.03334.pdf) è®ºæ–‡ç²¾è¯» | <img src="imgs/vilt.jpg" width="200px"/> | 1:03:26 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV14r4y1j74y)](https://www.bilibili.com/video/BV14r4y1j74y)<br />[![](https://img.shields.io/youtube/views/ug8YvZOjOCE?style=social)](https://youtu.be/ug8YvZOjOCE) |\n| 7/22/22 | ç†ç”±ã€è®ºæ®å’Œæ‹…ä¿ã€[ç ”ç©¶çš„è‰ºæœ¯](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)Â·å››ã€‘ | <img src="imgs/craft_research_p4.jpg" width="200px"/> | 44:14 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1SB4y1a75c)](https://www.bilibili.com/video/BV1SB4y1a75c) |\n| 7/15/22 | å¦‚ä½•è®²å¥½æ•…äº‹ã€æ•…äº‹é‡Œçš„è®ºç‚¹ã€[ç ”ç©¶çš„è‰ºæœ¯](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)Â·ä¸‰ã€‘| <img src="imgs/craft_research_p3.jpg" width="200px"/> | 43:56 |[![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1WB4y1v7ST)](https://www.bilibili.com/video/BV1WB4y1v7ST)|\n| 7/8/22 | [DALLÂ·E 2](https://arxiv.org/pdf/2204.06125.pdf) é€æ®µç²¾è¯» | <img src="imgs/dalle2.jpg" width="200px"/> | 1:27:54 |[![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV17r4y1u77B)](https://www.bilibili.com/video/BV17r4y1u77B)<br />[![](https://img.shields.io/youtube/views/hO57mntSMl0?style=social)](https://youtu.be/hO57mntSMl0)|\n| 7/1/22 | æ˜ç™½é—®é¢˜çš„é‡è¦æ€§ã€[ç ”ç©¶çš„è‰ºæœ¯](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)Â·äºŒã€‘| <img src="imgs/craft_research_p2.jpg" width="200px"/> | 1:03:40 |[![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV11S4y1v7S2)](https://www.bilibili.com/video/BV11S4y1v7S2/)|\n| 6/24/22 | è·Ÿè¯»è€…å»ºç«‹è”ç³»ã€[ç ”ç©¶çš„è‰ºæœ¯](https://press.uchicago.edu/ucp/books/book/chicago/C/bo23521678.html)Â·ä¸€ã€‘ | <img src="imgs/craft_research_p1.jpg" width="200px"/> | 45:01 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1hY411T7vy)](https://www.bilibili.com/video/BV1hY411T7vy/) |\n| 6/17/22 | [Zero](https://arxiv.org/pdf/1910.02054.pdf) é€æ®µç²¾è¯» | <img src="imgs/zero.jpg" width="200px"/> | 52:21 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1tY411g7ZT)](https://www.bilibili.com/video/BV1tY411g7ZT/) |\n| 6/10/22 | [DETR](https://arxiv.org/pdf/2005.12872.pdf) é€æ®µç²¾è¯» | <img src="imgs/detr.jpg" width="200px"/> | 54:22 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1GB4y1X72R)](https://www.bilibili.com/video/BV1GB4y1X72R/) |\n| 6/3/22 | [Megatron LM](https://arxiv.org/pdf/1909.08053.pdf) é€æ®µç²¾è¯» | <img src="imgs/megatron_lm.jpg" width="200px"/> | 56:07 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1nB4y1R7Yz)](https://www.bilibili.com/video/BV1nB4y1R7Yz/) |\n| 5/27/22 | [GPipe](https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf) é€æ®µç²¾è¯» | <img src="imgs/gpipe.jpg" width="200px"/> | 58:47 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1v34y1E7zu)](https://www.bilibili.com/video/BV1v34y1E7zu/) <br />[![](https://img.shields.io/youtube/views/eXjRpS_BTbs?style=social)](https://youtu.be/eXjRpS_BTbs)  |\n| 5/5/22 | [Pathways](https://arxiv.org/pdf/2203.12533.pdf) é€æ®µç²¾è¯» | <img src="imgs/pathways.jpg" width="200px"/> | 1:02:13 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1xB4y1m7Xi)](https://www.bilibili.com/video/BV1xB4y1m7Xi/) <br />[![](https://img.shields.io/youtube/views/8hS1ZtgG0wU?style=social)](https://youtu.be/8hS1ZtgG0wU) |\n| 4/28/22 | [è§†é¢‘ç†è§£è®ºæ–‡ä¸²è®²](https://arxiv.org/pdf/2012.06567.pdf)ï¼ˆä¸‹ï¼‰ | <img src="imgs/video-survey-p2.jpg" width="200px"/> | 1:08:32 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV11Y411P7ep)](https://www.bilibili.com/video/BV11Y411P7ep/) <br />[![](https://img.shields.io/youtube/views/J2YC0-k57NM?style=social)](https://youtu.be/J2YC0-k57NM) |\n| 4/21/22 | [å‚æ•°æœåŠ¡å™¨ï¼ˆParameter Serverï¼‰](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf) é€æ®µç²¾è¯» | <img src="imgs/ps.jpg" width="200px"/> | 1:37:40 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1YA4y197G8)](https://www.bilibili.com/video/BV1YA4y197G8/) <br />[![](https://img.shields.io/youtube/views/xt-AwUrDxQk?style=social)](https://youtu.be/xt-AwUrDxQk) |\n| 4/14/22 | [è§†é¢‘ç†è§£è®ºæ–‡ä¸²è®²](https://arxiv.org/pdf/2012.06567.pdf)ï¼ˆä¸Šï¼‰ | <img src="imgs/video-survey-p1.jpg" width="200px"/> | 51:15 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1fL4y157yA)](https://www.bilibili.com/video/BV1fL4y157yA/) <br />[![](https://img.shields.io/youtube/views/gK7AGO6okhc?style=social)](https://youtu.be/gK7AGO6okhc) |\n| 3/31/22 | [I3D](https://arxiv.org/pdf/1705.07750.pdf) è®ºæ–‡ç²¾è¯» | <img src="imgs/i3d.jpg" width="200px"/> | 52:31 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1tY4y1p7hq)](https://www.bilibili.com/video/BV1tY4y1p7hq/) <br />[![](https://img.shields.io/youtube/views/9lIkKiAn6uE?style=social)](https://youtu.be/9lIkKiAn6uE) |\n| 3/24/22 | æ–¯å¦ç¦ 2022 å¹´ [AI æŒ‡æ•°æŠ¥å‘Š](https://aiindex.stanford.edu/wp-content/uploads/2022/03/2022-AI-Index-Report_Master.pdf) ç²¾è¯» | <img src="imgs/ai_index_22.jpg" width="200px"/> | 1:19:56 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1s44y1N7eu)](https://www.bilibili.com/video/BV1s44y1N7eu/) <br />[![](https://img.shields.io/youtube/views/K8h_xjQ6ufY?style=social)](https://youtu.be/K8h_xjQ6ufY) |\n| 3/17/22 | [AlphaCode](https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf) è®ºæ–‡ç²¾è¯» | <img src="imgs/alphacode.jpg" width="200px"/> | 44:00 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1ab4y1s7rc)](https://www.bilibili.com/video/BV1ab4y1s7rc/) <br />[![](https://img.shields.io/youtube/views/t8Gzkca9pW4?style=social)](https://youtu.be/t8Gzkca9pW4) |\n| 3/10/22 | [OpenAI Codex](https://arxiv.org/pdf/2107.03374.pdf) è®ºæ–‡ç²¾è¯» | <img src="imgs/codex.jpg" width="200px"/> | 47:58 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1iY41137Zi)](https://www.bilibili.com/video/BV1iY41137Zi/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1490959755963666432)](https://www.zhihu.com/zvideo/1490959755963666432)<br />[![](https://img.shields.io/youtube/views/oZriUGkQSNM?style=social)](https://youtu.be/oZriUGkQSNM) |\n| 3/3/22 | [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf), [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [GPT-3](https://arxiv.org/abs/2005.14165) ç²¾è¯» | <img src="imgs/gpt3.jpg" width="200px"/> | 1:29:58 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1AF411b7xQ)](https://www.bilibili.com/video/BV1AF411b7xQ/)<br />[![](https://img.shields.io/youtube/views/t70Bl3w7bxY?style=social)](https://youtu.be/t70Bl3w7bxY) |\n| 2/24/22 | [Two-Stream](https://proceedings.neurips.cc/paper/2014/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Paper.pdf) é€æ®µç²¾è¯» |  <img src="imgs/twostream.jpg" width="200px"/> | 52:57 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1mq4y1x7RU)](https://www.bilibili.com/video/BV1mq4y1x7RU/)<br />[![](https://img.shields.io/youtube/views/vuqwKP2iDe0?style=social)](https://youtu.be/vuqwKP2iDe0) |\n| 2/10/22 | [CLIP](https://openai.com/blog/clip/) é€æ®µç²¾è¯» | <img src="imgs/clip.jpg" width="200px"/> | 1:38:25 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1SL4y1s7LQ)](https://www.bilibili.com/video/BV1SL4y1s7LQ/)<br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1475706654562299904)](https://www.zhihu.com/zvideo/1475706654562299904) <br />[![](https://img.shields.io/youtube/views/OZF1t_Hieq8?style=social)](https://youtu.be/OZF1t_Hieq8) |\n| 2/6/22 | ä½ ï¼ˆè¢«ï¼‰åæ§½è¿‡[è®ºæ–‡ä¸å¤Ÿ novel](https://perceiving-systems.blog/en/post/novelty-in-science) å—ï¼Ÿ| <img src="imgs/novelty.jpg" width="200px"/> | 14:11 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1ea41127Bq)](https://www.bilibili.com/video/BV1ea41127Bq/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1475719090198876161)](https://www.zhihu.com/zvideo/1475719090198876161) |\n| 1/23/22 | [AlphaFold 2](https://www.nature.com/articles/s41586-021-03819-2.pdf) ç²¾è¯» | <img src="imgs/alphafold_2.jpg" width="200px"/> |  1:15:28 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1oR4y1K7Xr)](https://www.bilibili.com/video/BV1oR4y1K7Xr/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1469132410537717760)](https://www.zhihu.com/zvideo/1469132410537717760)  <br />[![](https://img.shields.io/youtube/views/Oy3OCoGUr-w?style=social)](https://youtu.be/Oy3OCoGUr-w) |\n| 1/18/22 | å¦‚ä½•åˆ¤æ–­ï¼ˆä½ è‡ªå·±çš„ï¼‰ç ”ç©¶å·¥ä½œçš„ä»·å€¼ | <img src="imgs/research_value.jpg" width="200px"/> |  9:59 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1oL411c7Us)](https://www.bilibili.com/video/BV1oL411c7Us/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1475716940051869696)](https://www.zhihu.com/zvideo/1475716940051869696) |\n| 1/15/22 | [Swin Transformer](https://arxiv.org/pdf/2103.14030.pdf) ç²¾è¯» | <img src="imgs/swin_transformer.jpg" width="200px"/> | 1:00:21 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV13L4y1475U)](https://www.bilibili.com/video/BV13L4y1475U/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1466282983652691968)](https://www.zhihu.com/zvideo/1466282983652691968)   <br />[![](https://img.shields.io/youtube/views/luP3-Fs0QCo?style=social)](https://youtu.be/luP3-Fs0QCo) |\n| 1/7/22 | [æŒ‡å¯¼æ•°å­¦ç›´è§‰](https://www.nature.com/articles/s41586-021-04086-x.pdf) | <img src="imgs/math_conj.jpg" width="200px"/> | 52:51 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1YZ4y1S72j)](https://www.bilibili.com/video/BV1YZ4y1S72j/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1464060386375299072)](https://www.zhihu.com/zvideo/1464060386375299072)  <br />[![](https://img.shields.io/youtube/views/czFGjvhtss8?style=social)](https://youtu.be/czFGjvhtss8) |\n| 1/5/22 | AlphaFold 2 é¢„å‘Š | <img src="imgs/alphafold_2_preview.jpg" width="200px"/> | 03:28 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Eu411U7Te)](https://www.bilibili.com/video/BV1Eu411U7Te/) |\n| 12/20/21 | [å¯¹æ¯”å­¦ä¹ ](#contrastive_learning)è®ºæ–‡ç»¼è¿° | <img src="imgs/contrastive.jpg" width="200px"/> | 1:32:01 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV19S4y1M7hm)](https://www.bilibili.com/video/BV19S4y1M7hm/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1460828005077164032)](https://www.zhihu.com/zvideo/1460828005077164032)  <br />[![](https://img.shields.io/youtube/views/1pvxufGRuW4?style=social)](https://www.youtube.com/watch?v=1pvxufGRuW4) |\n| 12/15/21 | [MoCo](https://arxiv.org/pdf/1911.05722.pdf) é€æ®µç²¾è¯» | <img src="imgs/mocov1.jpg" width="200px"/> | 1:24:11 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1C3411s7t9)](https://www.bilibili.com/video/BV1C3411s7t9/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1454723120678936576)](https://www.zhihu.com/zvideo/1454723120678936576)   <br />[![](https://img.shields.io/youtube/views/1pvxufGRuW4?style=social)](https://www.youtube.com/watch?v=1pvxufGRuW4) |\n| 12/9/21 | å¦‚ä½•æ‰¾ç ”ç©¶æƒ³æ³• 1 | <img src="imgs/mae_idea.jpg" width="200px"/> | 5:34 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1qq4y1z7F2)](https://www.bilibili.com/video/BV1qq4y1z7F2/) |\n| 12/8/21 | [MAE](https://arxiv.org/pdf/2111.06377.pdf) é€æ®µç²¾è¯» | <img src="imgs/mae.jpg" width="200px"/> | 47:04 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1sq4y1q77t)](https://www.bilibili.com/video/BV1sq4y1q77t/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1452458167968251904)](https://www.zhihu.com/zvideo/1452458167968251904)  <br />[![](https://img.shields.io/youtube/views/mYlX2dpdHHM?style=social)](https://youtu.be/mYlX2dpdHHM) |\n| 11/29/21 | [ViT](https://arxiv.org/pdf/2010.11929.pdf) é€æ®µç²¾è¯» | <img src="imgs/vit.jpg" width="200px"/> | 1:11:30 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV15P4y137jb)](https://www.bilibili.com/video/BV15P4y137jb/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1449195245754380288)](https://www.zhihu.com/zvideo/1449195245754380288)  <br />[![](https://img.shields.io/youtube/views/FRFt3x0bO94?style=social)](https://youtu.be/FRFt3x0bO94) |\n| 11/18/21 | [BERT](https://arxiv.org/abs/1810.04805) é€æ®µç²¾è¯» | <img src="imgs/bert.jpg" width="200px"/> | 45:49  | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1PL411M7eQ)](https://www.bilibili.com/video/BV1PL411M7eQ/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1445340200976785408)](https://www.zhihu.com/zvideo/1445340200976785408)  <br />[![](https://img.shields.io/youtube/views/ULD3uIb2MHQ?style=social)](https://youtu.be/ULD3uIb2MHQ) |\n| 11/9/21 | [GAN](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf) é€æ®µç²¾è¯» | <img src="imgs/gan.jpg" width="200px"/> | 46:16  | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1rb4y187vD)](https://www.bilibili.com/video/BV1rb4y187vD/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1442091389241159681)](https://www.zhihu.com/zvideo/1442091389241159681)  <br />[![](https://img.shields.io/youtube/views/g_0HtlrLiDo?style=social)](https://www.youtube.com/watch?v=g_0HtlrLiDo) |\n| 11/3/21 | é›¶åŸºç¡€å¤šå›¾è¯¦è§£ [å›¾ç¥ç»ç½‘ç»œ](https://distill.pub/2021/gnn-intro/)ï¼ˆGNN/GCNï¼‰ | <img src="imgs/gnn.jpg" width="200px"/> | 1:06:19 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1iT4y1d7zP)](https://www.bilibili.com/video/BV1iT4y1d7zP/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1439540657619087360)](https://www.zhihu.com/zvideo/1439540657619087360)  <br />[![](https://img.shields.io/youtube/views/sejA2PtCITw?style=social)](https://youtu.be/sejA2PtCITw) |\n| 10/27/21 | [Transformer](https://arxiv.org/abs/1706.03762) é€æ®µç²¾è¯»<br> ï¼ˆè§†é¢‘ä¸­æåˆ°çš„æ–‡çŒ® [^transformer]) |<img src="imgs/transformer.jpg" width="200px"/> | 1:27:05 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1pu411o7BE)](https://www.bilibili.com/video/BV1pu411o7BE/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1437034536677404672)](https://www.zhihu.com/zvideo/1437034536677404672)  <br />[![](https://img.shields.io/youtube/views/nzqlFIcCSWQ?style=social)](https://youtu.be/nzqlFIcCSWQ) |\n| 10/22/21 | [ResNet](https://arxiv.org/abs/1512.03385) è®ºæ–‡é€æ®µç²¾è¯» | <img src="imgs/resnet-2.jpg" width="200px"/> | 53:46 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1P3411y7nn)](https://www.bilibili.com/video/BV1P3411y7nn/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1434795406001180672)](https://www.zhihu.com/zvideo/1434795406001180672)  <br />[![](https://img.shields.io/youtube/views/pWMnzCX4cwQ?style=social)](https://www.youtube.com/watch?v=pWMnzCX4cwQ) |\n| 10/21/21 | æ’‘èµ·è®¡ç®—æœºè§†è§‰åŠè¾¹å¤©çš„ [ResNet](https://arxiv.org/abs/1512.03385) | <img src="imgs/resnet-1.jpg" width="200px"/> | 11:50 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1Fb4y1h73E)](https://www.bilibili.com/video/BV1Fb4y1h73E/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1434787226101751808)](https://www.zhihu.com/zvideo/1434787226101751808)  <br />[![](https://img.shields.io/youtube/views/NnSldWhSqvY?style=social)](https://www.youtube.com/watch?v=NnSldWhSqvY) |\n| 10/15/21 | [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) è®ºæ–‡é€æ®µç²¾è¯» | <img src="imgs/alexnet-2.jpg" width="200px"/> | 55:21 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1hq4y157t1)](https://www.bilibili.com/video/BV1hq4y157t1/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1432354207483871232)](https://www.zhihu.com/zvideo/1432354207483871232)  <br />[![](https://img.shields.io/youtube/views/wYmlILPsLlY?style=social)](https://www.youtube.com/watch?v=wYmlILPsLlY) |\n| 10/14/21 | 9å¹´åé‡è¯»æ·±åº¦å­¦ä¹ å¥ åŸºä½œä¹‹ä¸€ï¼š[AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) | <img src="imgs/alexnet-1.jpg" width="200px"/> | 19:59 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1ih411J7Kz)](https://www.bilibili.com/video/BV1ih411J7Kz/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1432155856322920448)](https://www.zhihu.com/zvideo/1432155856322920448)  <br />[![](https://img.shields.io/youtube/views/vdYH0fE6thY?style=social)](https://www.youtube.com/watch?v=vdYH0fE6thY) |\n| 10/06/21 | å¦‚ä½•è¯»è®ºæ–‡ | <img src="imgs/read-paper.jpg" width="200px"/> | 06:39 | [![bilibili](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=bilibili&query=data.stat.view&url=https%3A%2F%2Fapi.bilibili.com%2Fx%2Fweb-interface%2Fview%3Fbvid%3DBV1H44y1t75x)](https://www.bilibili.com/video/BV1H44y1t75x/) <br />[![zhihu](https://img.shields.io/badge/dynamic/json?label=views&style=social&logo=zhihu&query=video.play_count&url=https://www.zhihu.com/api/v4/zvideos/1428973951632969728)](https://www.zhihu.com/zvideo/1428973951632969728)  <br />[![](https://img.shields.io/youtube/views/txjl_Q4jCyQ?style=social)](https://www.youtube.com/watch?v=txjl_Q4jCyQ&list=PLFXJ6jwg0qW-7UM8iUTj3qKqdhbQULP5I&index=1) |\n\n[^transformer]: 1 [æ–¯å¦ç¦100+ä½œè€…çš„200+é¡µç»¼è¿°](https://arxiv.org/abs/2108.07258)ï¼Œ2 [å¯¹LayerNormçš„æ–°ç ”ç©¶](https://arxiv.org/pdf/1911.07013.pdf)ï¼Œ3 [å¯¹Attentionåœ¨Transformeré‡Œé¢ä½œç”¨çš„ç ”ç©¶](https://arxiv.org/abs/2103.03404)\n\n\n## æ‰€æœ‰è®ºæ–‡\n\nåŒ…æ‹¬å·²ç»å½•åˆ¶å®Œæˆå’Œä¹‹åå°†è¦ä»‹ç»çš„è®ºæ–‡ã€‚é€‰å–çš„åŸåˆ™æ˜¯10å¹´å†…æ·±åº¦å­¦ä¹ é‡Œæœ‰å½±å“åŠ›æ–‡ç« ï¼ˆå¿…è¯»æ–‡ç« ï¼‰ï¼Œæˆ–è€…è¿‘æœŸæ¯”è¾ƒæœ‰æ„æ€çš„æ–‡ç« ã€‚å½“ç„¶è¿™åå¹´é‡Œé‡è¦çš„å·¥ä½œå¤ªå¤šäº†ï¼Œä¸å¯èƒ½ä¸€ä¸€è¿‡ä¸€éã€‚åœ¨é€‰å–çš„æ—¶å€™æˆ‘ä¼šåå‘ä¸€äº›ä¹‹å‰ [ç›´æ’­è¯¾](https://c.d2l.ai/zh-v2/) ä¸­æ²¡è®²åˆ°è¿‡çš„ã€‚ æ¬¢è¿å¤§å®¶åœ¨ [è®¨è®ºåŒº](https://github.com/mli/paper-reading/discussions) é‡Œæä¾›å»ºï¼ˆç‚¹ï¼‰è®®ï¼ˆæ­Œï¼‰ã€‚\n\næ€»è®ºæ–‡æ•° 67ï¼Œå½•åˆ¶å®Œæˆæ•° 32\n\nï¼ˆè¿™é‡Œå¼•ç”¨é‡‡ç”¨çš„æ˜¯ semanticscholarï¼Œæ˜¯å› ä¸ºå®ƒæä¾› [API](https://api.semanticscholar.org/api-docs/graph#operation/get_graph_get_paper) å¯ä»¥è‡ªåŠ¨è·å–ï¼Œä¸ç”¨æ‰‹åŠ¨æ›´æ–°ã€‚ï¼‰\n\n### è®¡ç®—æœºè§†è§‰ - CNN\n\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| âœ…      | 2012 | [AlexNet](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) | æ·±åº¦å­¦ä¹ çƒ­æ½®çš„å¥ åŸºä½œ                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fabd1c342495432171beb7ca8fd9551ef13cbd0ff%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff) |\n| | 2014 | [VGG](https://arxiv.org/pdf/1409.1556.pdf) | ä½¿ç”¨ 3x3 å·ç§¯æ„é€ æ›´æ·±çš„ç½‘ç»œ                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Feb42cf88027de515750f230b23b1a057dc782108%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman/eb42cf88027de515750f230b23b1a057dc782108) |\n| | 2014 | [GoogleNet](https://arxiv.org/pdf/1409.4842.pdf) | ä½¿ç”¨å¹¶è¡Œæ¶æ„æ„é€ æ›´æ·±çš„ç½‘ç»œ                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe15cf50aa89fee8535703b9f9512fca5bfc43327%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Going-deeper-with-convolutions-Szegedy-Liu/e15cf50aa89fee8535703b9f9512fca5bfc43327) |\n|  âœ…  | 2015 |  [ResNet](https://arxiv.org/pdf/1512.03385.pdf) | æ„å»ºæ·±å±‚ç½‘ç»œéƒ½è¦æœ‰çš„æ®‹å·®è¿æ¥ã€‚               |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2c03df8b48bf3fa39054345bafabfeff15bfd11d%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Deep-Residual-Learning-for-Image-Recognition-He-Zhang/2c03df8b48bf3fa39054345bafabfeff15bfd11d)  |\n|  | 2017 | [MobileNet](https://arxiv.org/pdf/1704.04861.pdf) | é€‚åˆç»ˆç«¯è®¾å¤‡çš„å°CNN                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3647d6d0f151dc05626449ee09cc7bce55be497e%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/MobileNets%3A-Efficient-Convolutional-Neural-Networks-Howard-Zhu/3647d6d0f151dc05626449ee09cc7bce55be497e)  |\n| | 2019 | [EfficientNet](https://arxiv.org/pdf/1905.11946.pdf) | é€šè¿‡æ¶æ„æœç´¢å¾—åˆ°çš„CNN                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/EfficientNet%3A-Rethinking-Model-Scaling-for-Neural-Tan-Le/4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9)  |\n| | 2021 |  [Non-deep networks](https://arxiv.org/pdf/2110.07641.pdf) | è®©ä¸æ·±çš„ç½‘ç»œä¹Ÿèƒ½åœ¨ImageNetåˆ·åˆ°SOTA                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0d7f6086772079bc3e243b7b375a9ca1a517ba8b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Non-deep-Networks-Goyal-Bochkovskiy/0d7f6086772079bc3e243b7b375a9ca1a517ba8b) |\n\n### è®¡ç®—æœºè§†è§‰ - Transformer\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| âœ… | 2020 | [ViT](https://arxiv.org/pdf/2010.11929.pdf) | Transformeræ€å…¥CVç•Œ                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7b15fa1b8d413fbe14ef7a97f651f47f5aff3903%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/An-Image-is-Worth-16x16-Words%3A-Transformers-for-at-Dosovitskiy-Beyer/7b15fa1b8d413fbe14ef7a97f651f47f5aff3903)  |\n| âœ… | 2021 | [Swin Transformer](https://arxiv.org/pdf/2103.14030.pdf) | å¤šå±‚æ¬¡çš„Vision Transformer                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc8b25fab5608c3e033d34b4483ec47e68ba109b7%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Swin-Transformer%3A-Hierarchical-Vision-Transformer-Liu-Lin/c8b25fab5608c3e033d34b4483ec47e68ba109b7) |\n| | 2021 | [MLP-Mixer](https://arxiv.org/pdf/2105.01601.pdf) | ä½¿ç”¨MLPæ›¿æ¢self-attention            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2def61f556f9a5576ace08911496b7c7e4f970a4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/MLP-Mixer%3A-An-all-MLP-Architecture-for-Vision-Tolstikhin-Houlsby/2def61f556f9a5576ace08911496b7c7e4f970a4)  |\n| âœ… | 2021 | [MAE](https://arxiv.org/pdf/2111.06377.pdf) | BERTçš„CVç‰ˆ             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc1962a8cf364595ed2838a097e9aa7cd159d3118%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Masked-Autoencoders-Are-Scalable-Vision-Learners-He-Chen/c1962a8cf364595ed2838a097e9aa7cd159d3118)  |\n\n### ç”Ÿæˆæ¨¡å‹\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                              | ç®€ä»‹         | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------- | ------------ | ------------------------------------------------------------ |\n|  âœ… | 2014 | [GAN](https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf) | ç”Ÿæˆæ¨¡å‹çš„å¼€åˆ›å·¥ä½œ                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F54e325aee6b2d476bbbb88615ac15e251c6e8214%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Generative-Adversarial-Nets-Goodfellow-Pouget-Abadie/54e325aee6b2d476bbbb88615ac15e251c6e8214)  |\n|  | 2015 | [DCGAN](https://arxiv.org/pdf/1511.06434.pdf) | ä½¿ç”¨CNNçš„GAN          |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8388f1be26329fa45e5807e968a641ce170ea078%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Unsupervised-Representation-Learning-with-Deep-Radford-Metz/8388f1be26329fa45e5807e968a641ce170ea078)  |\n|  | 2016 | [pix2pix](https://arxiv.org/pdf/1611.07004.pdf) |           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8acbe90d5b852dadea7810345451a99608ee54c7%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Image-to-Image-Translation-with-Conditional-Isola-Zhu/8acbe90d5b852dadea7810345451a99608ee54c7)  |\n|  | 2016 | [SRGAN](https://arxiv.org/pdf/1609.04802.pdf) | å›¾ç‰‡è¶…åˆ†è¾¨ç‡          |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdf0c54fe61f0ffb9f0e36a17c2038d9a1964cba3%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Photo-Realistic-Single-Image-Super-Resolution-Using-Ledig-Theis/df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3)  |\n|  | 2017 | [WGAN](https://arxiv.org/abs/1701.07875) | è®­ç»ƒæ›´åŠ å®¹æ˜“          |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2f85b7376769473d2bed56f855f115e23d727094%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Wasserstein-GAN-Arjovsky-Chintala/2f85b7376769473d2bed56f855f115e23d727094)  |\n|  | 2017 | [CycleGAN](https://arxiv.org/abs/1703.10593) |           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc43d954cf8133e6254499f3d68e45218067e4941%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Unpaired-Image-to-Image-Translation-Using-Networks-Zhu-Park/c43d954cf8133e6254499f3d68e45218067e4941)  |\n|  | 2018 | [StyleGAN](https://arxiv.org/abs/1812.04948) |           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fceb2ebef0b41e31c1a21b28c2734123900c005e2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/A-Style-Based-Generator-Architecture-for-Generative-Karras-Laine/ceb2ebef0b41e31c1a21b28c2734123900c005e2)  |\n| | 2019 | [StyleGAN2](https://arxiv.org/pdf/1912.04958.pdf) |        |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff3e3d1f86a534a3654d0ee263142e44f4e2c61e9%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Analyzing-and-Improving-the-Image-Quality-of-Karras-Laine/f3e3d1f86a534a3654d0ee263142e44f4e2c61e9)  |\n| | 2020 | [DDPM](https://arxiv.org/pdf/2006.11239.pdf) | Diffusion Models   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F289db3be7bf77e06e75541ba93269de3d604ac72%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Denoising-Diffusion-Probabilistic-Models-Ho-Jain/289db3be7bf77e06e75541ba93269de3d604ac72)  |\n| | 2021 | [Improved DDPM](https://arxiv.org/pdf/2102.09672.pdf) | æ”¹è¿›çš„ DDPM   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fde18baa4964804cf471d85a5a090498242d2e79f%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Improved-Denoising-Diffusion-Probabilistic-Models-Nichol-Dhariwal/de18baa4964804cf471d85a5a090498242d2e79f)  |\n| | 2021 | [Guided Diffusion Models](https://arxiv.org/pdf/2105.05233.pdf) | å·ç§°è¶…è¶Š GAN  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F64ea8f180d0682e6c18d1eb688afdb2027c02794%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Diffusion-Models-Beat-GANs-on-Image-Synthesis-Dhariwal-Nichol/64ea8f180d0682e6c18d1eb688afdb2027c02794)  |\n| | 2021 | [StyleGAN3](https://arxiv.org/pdf/2106.12423.pdf) |        |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc1ff08b59f00c44f34dfdde55cd53370733a2c19%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Alias-Free-Generative-Adversarial-Networks-Karras-Aittala/c1ff08b59f00c44f34dfdde55cd53370733a2c19)  |\n|  âœ…  | 2022 | [DALL.E 2](https://arxiv.org/pdf/2204.06125.pdf) | CLIP + Diffusion modelsï¼Œæ–‡æœ¬ç”Ÿæˆå›¾åƒæ–°é«˜åº¦     |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc57293882b2561e1ba03017902df9fc2f289dea2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Hierarchical-Text-Conditional-Image-Generation-with-Ramesh-Dhariwal/c57293882b2561e1ba03017902df9fc2f289dea2)  |\n|  âœ…  | 2024 | [Sora](https://openai.com/index/video-generation-models-as-world-simulators/) | å¼€å¯è§†é¢‘ç”Ÿæˆçƒ­æ½®     |  |\n|  âœ…  | 2024 | [Movie Gen](https://arxiv.org/pdf/2410.13720) | ç²¾ç¡®çš„æ–‡æœ¬æŒ‡å¯¼è§†é¢‘ç¼–è¾‘ã€ä¸ªæ€§åŒ–è§†é¢‘ç”Ÿæˆ     |  |\n|  âœ…  | 2025 | [HunyuanVideo](https://arxiv.org/pdf/2412.03603) | å¼€æºè§†é¢‘ç”Ÿæˆæ¡†æ¶     |  |\n\n### è®¡ç®—æœºè§†è§‰ - Object Detection\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                              | ç®€ä»‹         | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------- | ------------ | ------------------------------------------------------------ |\n|        | 2014 | [R-CNN](https://arxiv.org/pdf/1311.2524v5.pdf)    | Two-stage             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2f4df08d9072fc2ac181b7fced6a245315ce05c8%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/2f4df08d9072fc2ac181b7fced6a245315ce05c8)  |\n|        | 2015 | [Fast R-CNN](http://arxiv.org/abs/1504.08083v2)   |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7ffdbc358b63378f07311e883dddacc9faeeaf4b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/7ffdbc358b63378f07311e883dddacc9faeeaf4b)  |\n|        | 2015 | [Faster R-CNN](http://arxiv.org/abs/1506.01497v3) |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F424561d8585ff8ebce7d5d07de8dbf7aae5e7270%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/424561d8585ff8ebce7d5d07de8dbf7aae5e7270)  |\n|        | 2016 | [SSD](http://arxiv.org/abs/1512.02325v5)          | Single stage          |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0)  |\n|        | 2016 | [YOLO](http://arxiv.org/abs/1506.02640v5)         |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff8e79ac0ea341056ef20f2616628b3e964764cfd%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/f8e79ac0ea341056ef20f2616628b3e964764cfd)  |\n|        | 2017 | [Mask R-CNN](http://arxiv.org/abs/1703.06870v3)   |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fea99a5535388196d0d44be5b4d7dd02029a43bb2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ea99a5535388196d0d44be5b4d7dd02029a43bb2)  |\n|        | 2017 | [YOLOv2](http://arxiv.org/abs/1612.08242v1)       |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F7d39d69b23424446f0400ef603b2e3e22d0309d6%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/7d39d69b23424446f0400ef603b2e3e22d0309d6)  |\n|        | 2018 | [YOLOv3](http://arxiv.org/abs/1804.02767v1)       |                       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe4845fb1e624965d4f036d7fd32e8dcdd2408148%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/e4845fb1e624965d4f036d7fd32e8dcdd2408148)  |\n|        | 2019 | [CenterNet](https://arxiv.org/pdf/1904.07850.pdf) | Anchor free           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6a2e2fd1b5bb11224daef98b3fb6d029f68a73f2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Objects-as-Points-Zhou-Wang/6a2e2fd1b5bb11224daef98b3fb6d029f68a73f2)  |\n|   âœ…     | 2020 | [DETR](https://arxiv.org/pdf/2005.12872.pdf)      | Transformer           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F962dc29fdc3fbdc5930a10aba114050b82fe5a3e%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/End-to-End-Object-Detection-with-Transformers-Carion-Massa/962dc29fdc3fbdc5930a10aba114050b82fe5a3e)  |\n\n<a name="contrastive_learning"></a>\n\n### è®¡ç®—æœºè§†è§‰ - å¯¹æ¯”å­¦ä¹ \n\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| âœ…      | 2018 | [InstDisc](https://arxiv.org/pdf/1805.01978.pdf) | æå‡ºå®ä¾‹åˆ¤åˆ«å’Œmemory bankåšå¯¹æ¯”å­¦ä¹                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F155b7782dbd713982a4133df3aee7adfd0b6b304%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Unsupervised-Feature-Learning-via-Non-parametric-Wu-Xiong/155b7782dbd713982a4133df3aee7adfd0b6b304)  |\n| âœ…      | 2018 | [CPC](https://arxiv.org/pdf/1807.03748.pdf) | å¯¹æ¯”é¢„æµ‹ç¼–ç ï¼Œå›¾åƒè¯­éŸ³æ–‡æœ¬å¼ºåŒ–å­¦ä¹ å…¨éƒ½èƒ½åš                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb227f3e4c0dc96e5ac5426b85485a70f2175a205%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Representation-Learning-with-Contrastive-Predictive-Oord-Li/b227f3e4c0dc96e5ac5426b85485a70f2175a205) |\n| âœ…      | 2019 | [InvaSpread](https://arxiv.org/pdf/1904.03436.pdf) | ä¸€ä¸ªç¼–ç å™¨çš„ç«¯åˆ°ç«¯å¯¹æ¯”å­¦ä¹                    |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fe4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Unsupervised-Embedding-Learning-via-Invariant-and-Ye-Zhang/e4bde6fe33b6c2cf9d1647ac0b041f7d1ba29c5b)  |\n| âœ…  | 2019 |  [CMC](https://arxiv.org/pdf/1906.05849.pdf) | å¤šè§†è§’ä¸‹çš„å¯¹æ¯”å­¦ä¹                |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F97f4d09175705be4677d675fa27e55defac44800%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Contrastive-Multiview-Coding-Tian-Krishnan/97f4d09175705be4677d675fa27e55defac44800)  |\n| âœ… | 2019 | [MoCov1](https://arxiv.org/pdf/1911.05722.pdf) | æ— ç›‘ç£è®­ç»ƒæ•ˆæœä¹Ÿå¾ˆå¥½                   | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fec46830a4b275fd01d4de82bffcabe6da086128f%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Momentum-Contrast-for-Unsupervised-Visual-Learning-He-Fan/ec46830a4b275fd01d4de82bffcabe6da086128f) |\n|  âœ… | 2020 |  [SimCLRv1](https://arxiv.org/pdf/2002.05709.pdf) |  ç®€å•çš„å¯¹æ¯”å­¦ä¹  (æ•°æ®å¢å¼º + MLP head + å¤§batchè®­ç»ƒä¹…)                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F34733eaf66007516347a40ad5d9bbe1cc9dacb6b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/A-Simple-Framework-for-Contrastive-Learning-of-Chen-Kornblith/34733eaf66007516347a40ad5d9bbe1cc9dacb6b)  |\n|  âœ… | 2020 | [MoCov2](https://arxiv.org/pdf/2003.04297.pdf) | MoCov1 + improvements from SimCLRv1                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fa1b8a8df281bbaec148a897927a49ea47ea31515%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Improved-Baselines-with-Momentum-Contrastive-Chen-Fan/a1b8a8df281bbaec148a897927a49ea47ea31515)  |\n|  âœ… | 2020 |  [SimCLRv2](https://arxiv.org/pdf/2006.10029.pdf) | å¤§çš„è‡ªç›‘ç£é¢„è®­ç»ƒæ¨¡å‹å¾ˆé€‚åˆåšåŠç›‘ç£å­¦ä¹                    |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3e7f5f4382ac6f9c4fef6197dd21abf74456acd1%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Big-Self-Supervised-Models-are-Strong-Learners-Chen-Kornblith/3e7f5f4382ac6f9c4fef6197dd21abf74456acd1)  |\n| âœ…  | 2020 |  [BYOL](https://arxiv.org/pdf/2006.07733.pdf) | ä¸éœ€è¦è´Ÿæ ·æœ¬çš„å¯¹æ¯”å­¦ä¹                    | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F38f93092ece8eee9771e61c1edaf11b1293cae1b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Bootstrap-Your-Own-Latent%3A-A-New-Approach-to-Grill-Strub/38f93092ece8eee9771e61c1edaf11b1293cae1b) |\n|  âœ… | 2020 |  [SWaV](https://arxiv.org/pdf/2006.09882.pdf) | èšç±»å¯¹æ¯”å­¦ä¹                    | [![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F10161d83d29fc968c4612c9e9e2b61a2fc25842e%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Unsupervised-Learning-of-Visual-Features-by-Cluster-Caron-Misra/10161d83d29fc968c4612c9e9e2b61a2fc25842e) |\n|  âœ… | 2020 |  [SimSiam](https://arxiv.org/pdf/2011.10566.pdf) | åŒ–ç¹ä¸ºç®€çš„å­ªç”Ÿè¡¨å¾å­¦ä¹                    |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Exploring-Simple-Siamese-Representation-Learning-Chen-He/0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d)  |\n| âœ… | 2021 | [MoCov3](https://arxiv.org/pdf/2104.02057.pdf) | å¦‚ä½•æ›´ç¨³å®šçš„è‡ªç›‘ç£è®­ç»ƒViT                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F739ceacfafb1c4eaa17509351b647c773270b3ae%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/An-Empirical-Study-of-Training-Self-Supervised-Chen-Xie/739ceacfafb1c4eaa17509351b647c773270b3ae)  |\n|  âœ… | 2021 |  [DINO](https://arxiv.org/pdf/2104.14294.pdf) | transformeråŠ è‡ªç›‘ç£åœ¨è§†è§‰ä¹Ÿå¾ˆé¦™                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fad4a0938c48e61b7827869e4ac3baffd0aefab35%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Emerging-Properties-in-Self-Supervised-Vision-Caron-Touvron/ad4a0938c48e61b7827869e4ac3baffd0aefab35)  |\n\n\n### è®¡ç®—æœºè§†è§‰ - è§†é¢‘ç†è§£\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| âœ… | 2014 |  [DeepVideo](https://cs.stanford.edu/people/karpathy/deepvideo/) | æå‡ºsports1Mæ•°æ®é›†ï¼Œç”¨æ·±åº¦å­¦ä¹ åšè§†é¢‘ç†è§£ |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6d4c9c923e9f145d1c01a2de2afc38ec23c44253%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Large-Scale-Video-Classification-with-Convolutional-Karpathy-Toderici/6d4c9c923e9f145d1c01a2de2afc38ec23c44253)  |\n| âœ… | 2014 |  [Two-stream](https://arxiv.org/pdf/1406.2199.pdf) | å¼•å…¥å…‰æµåšæ—¶åºå»ºæ¨¡ï¼Œç¥ç»ç½‘ç»œé¦–æ¬¡è¶…è¶Šæ‰‹å·¥ç‰¹å¾ |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F67dccc9a856b60bdc4d058d83657a089b8ad4486%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Two-Stream-Convolutional-Networks-for-Action-in-Simonyan-Zisserman/67dccc9a856b60bdc4d058d83657a089b8ad4486)  |\n| âœ… | 2014 |  [C3D](https://arxiv.org/pdf/1412.0767.pdf) |  æ¯”è¾ƒæ·±çš„3D-CNNåšè§†é¢‘ç†è§£ |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fd25c65d261ea0e6a458be4c50c40ffe5bc508f77%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Learning-Spatiotemporal-Features-with-3D-Networks-Tran-Bourdev/d25c65d261ea0e6a458be4c50c40ffe5bc508f77)  |\n| âœ… | 2015 |  [Beyond-short-snippets](https://arxiv.org/pdf/1503.08909.pdf) | å°è¯•ä½¿ç”¨LSTM  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5418b2a482720e013d487a385c26fae0f017c6a6%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Beyond-short-snippets%3A-Deep-networks-for-video-Ng-Hausknecht/5418b2a482720e013d487a385c26fae0f017c6a6)  |\n| âœ… | 2016 |  [Convolutional fusion](https://arxiv.org/pdf/1604.06573.pdf) | åšearly fusionæ¥åŠ å¼ºæ—¶ç©ºé—´å»ºæ¨¡    |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9d9aced120e530484609164c836da64548693484%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Convolutional-Two-Stream-Network-Fusion-for-Video-Feichtenhofer-Pinz/9d9aced120e530484609164c836da64548693484)  |\n| âœ… | 2016 |  [TSN](https://arxiv.org/pdf/1608.00859.pdf) | è¶…çº§æœ‰æ•ˆçš„è§†é¢‘åˆ†æ®µå»ºæ¨¡ï¼Œbag of tricks in video |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fea3d7de6c0880e14455b9acb28f1bc1234321456%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Temporal-Segment-Networks%3A-Towards-Good-Practices-Wang-Xiong/ea3d7de6c0880e14455b9acb28f1bc1234321456)  |\n| âœ… | 2017 |  [I3D](https://arxiv.org/pdf/1705.07750.pdf) | æå‡ºKineticsæ•°æ®é›†ï¼Œè†¨èƒ€2Dç½‘ç»œåˆ°3Dï¼Œå¼€å¯3D-CNNæ—¶ä»£  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fb61a3f8b80bbd44f24544dc915f52fd30bbdf485%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Quo-Vadis%2C-Action-Recognition-A-New-Model-and-the-Carreira-Zisserman/b61a3f8b80bbd44f24544dc915f52fd30bbdf485)  |\n| âœ… | 2017 |  [R2+1D](https://arxiv.org/pdf/1711.11248.pdf) | æ‹†åˆ†3Då·ç§¯æ ¸ï¼Œä½¿3Dç½‘ç»œå®¹æ˜“ä¼˜åŒ–  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F89c3050522a0bb9820c32dc7444e003ef0d3e2e4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/A-Closer-Look-at-Spatiotemporal-Convolutions-for-Tran-Wang/89c3050522a0bb9820c32dc7444e003ef0d3e2e4)  |\n| âœ… | 2017 |  [Non-local](https://arxiv.org/pdf/1711.07971.pdf) | å¼•å…¥è‡ªæ³¨æ„åŠ›åšè§†è§‰é—®é¢˜  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8899094797e82c5c185a0893896320ef77f60e64%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Non-local-Neural-Networks-Wang-Girshick/8899094797e82c5c185a0893896320ef77f60e64)  |\n| âœ… | 2018 |  [SlowFast](https://arxiv.org/pdf/1812.03982.pdf) | å¿«æ…¢ä¸¤æ”¯æå‡æ•ˆç‡   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8b47b9c3c35b2b2a78bff7822605b3040f87d699%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/SlowFast-Networks-for-Video-Recognition-Feichtenhofer-Fan/8b47b9c3c35b2b2a78bff7822605b3040f87d699)  |\n| âœ… | 2021 |  [TimeSformer](https://arxiv.org/pdf/2102.05095.pdf) | è§†é¢‘ä¸­ç¬¬ä¸€ä¸ªå¼•å…¥transformerï¼Œå¼€å¯video transformeræ—¶ä»£ |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc143ea9e30b1f2d93a9c060253845423f9e60e1f%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Is-Space-Time-Attention-All-You-Need-for-Video-Bertasius-Wang/c143ea9e30b1f2d93a9c060253845423f9e60e1f)  |\n\n\n### å¤šæ¨¡æ€å­¦ä¹ \n\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| âœ… | 2021 |  [CLIP](https://openai.com/blog/clip/) | å›¾ç‰‡å’Œæ–‡æœ¬ä¹‹é—´çš„å¯¹æ¯”å­¦ä¹                    |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Learning-Transferable-Visual-Models-From-Natural-Radford-Kim/6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4)  |\n| âœ… | 2021 |  [ViLT](https://arxiv.org/pdf/2102.03334.pdf) | ç¬¬ä¸€ä¸ªæ‘†è„±äº†ç›®æ ‡æ£€æµ‹çš„è§†è§‰æ–‡æœ¬æ¨¡å‹      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0839722fb5369c0abaff8515bfc08299efc790a1%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ViLT%3A-Vision-and-Language-Transformer-Without-or-Kim-Son/0839722fb5369c0abaff8515bfc08299efc790a1)  |\n| âœ… | 2021 |  [ViLD](https://arxiv.org/pdf/2104.13921.pdf) | CLIPè’¸é¦å¸®åŠ©å¼€é›†ç›®æ ‡æ£€æµ‹      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcf9b8da26d9b92e75ba49616ed2a1033f59fce14%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Open-vocabulary-Object-Detection-via-Vision-and-Gu-Lin/cf9b8da26d9b92e75ba49616ed2a1033f59fce14)  |\n| âœ… | 2021 |  [GLIP](https://arxiv.org/pdf/2112.03857.pdf) | è”åˆç›®æ ‡æ£€æµ‹å’Œæ–‡æœ¬å®šä½           |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5341b412383c43f4a693ad63ec4489e3ec7688c8%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Grounded-Language-Image-Pre-training-Li-Zhang/5341b412383c43f4a693ad63ec4489e3ec7688c8)  |\n| âœ… | 2021 |  [CLIP4Clip](https://arxiv.org/pdf/2104.08860.pdf) | æ‹¿CLIPç›´æ¥åšè§†é¢‘æ–‡æœ¬retrieval       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F281ad83e06d731d5d686acf07cd701576f1188c4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/CLIP4Clip%3A-An-Empirical-Study-of-CLIP-for-End-to-Luo-Ji/281ad83e06d731d5d686acf07cd701576f1188c4)  |\n| âœ… | 2021 |  [ActionCLIP](https://arxiv.org/pdf/2109.08472.pdf) | ç”¨å¤šæ¨¡æ€å¯¹æ¯”å­¦ä¹ æœ‰ç›‘ç£çš„åšè§†é¢‘åŠ¨ä½œåˆ†ç±»   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdc05240a06326b5b1664f7e8c95c330b08cd0349%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ActionCLIP%3A-A-New-Paradigm-for-Video-Action-Wang-Xing/dc05240a06326b5b1664f7e8c95c330b08cd0349)  |\n| âœ… | 2021 |  [PointCLIP](https://arxiv.org/pdf/2112.02413.pdf) | 3Då˜2Dï¼Œå·§å¦™åˆ©ç”¨CLIPåšç‚¹äº‘  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff3ce9ba3fcec362b70263a7ed63d9404975496a0%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/PointCLIP%3A-Point-Cloud-Understanding-by-CLIP-Zhang-Guo/f3ce9ba3fcec362b70263a7ed63d9404975496a0)  |\n| âœ… | 2022 |  [LSeg](https://arxiv.org/pdf/2201.03546.pdf) | æœ‰ç›‘ç£çš„å¼€é›†åˆ†å‰²                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcc9826c222ac1e81b4b374dd9e0df130f298b1e8%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Language-driven-Semantic-Segmentation-Li-Weinberger/cc9826c222ac1e81b4b374dd9e0df130f298b1e8)  |\n| âœ… | 2022 |  [GroupViT](https://arxiv.org/pdf/2202.11094.pdf) | åªç”¨å›¾åƒæ–‡æœ¬å¯¹ä¹Ÿèƒ½æ— ç›‘ç£åšåˆ†å‰²        |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0b5f27a5766c5d1394a6282ad94fec21d620bd6b%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/GroupViT%3A-Semantic-Segmentation-Emerges-from-Text-Xu-Mello/0b5f27a5766c5d1394a6282ad94fec21d620bd6b)  |\n| âœ… | 2022 |  [CLIPasso](https://arxiv.org/pdf/2202.05822.pdf) | CLIPè·¨ç•Œç”Ÿæˆç®€ç¬”ç”»   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9dec819778bebae4a468c7813f7638534c826f52%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/CLIPasso%3A-Semantically-Aware-Object-Sketching-Vinker-Pajouheshgar/9dec819778bebae4a468c7813f7638534c826f52)  |\n| âœ… | 2022 |  [DepthCLIP](https://arxiv.org/pdf/2207.01077.pdf) | ç”¨æ–‡æœ¬è·¨ç•Œä¼°è®¡æ·±åº¦   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9d0afe58801fe9e5537902e853d6e9e385340a92%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Can-Language-Understand-Depth-Zhang-Zeng/9d0afe58801fe9e5537902e853d6e9e385340a92)  |\n\n\n\n### è‡ªç„¶è¯­è¨€å¤„ç† - Transformer\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| âœ… | 2017 | [Transformer](https://arxiv.org/abs/1706.03762) | ç»§MLPã€CNNã€RNNåçš„ç¬¬å››å¤§ç±»æ¶æ„                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F204e3073870fae3d05bcbc2f6a8e263d9b72e776%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776)  |\n| âœ… | 2018 | [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) | ä½¿ç”¨ Transformer è§£ç å™¨æ¥åšé¢„è®­ç»ƒ               |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcd18800a0fe0b668a1cc19f2ec95b5003d0a5035%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035)  |\n| âœ… | 2018 | [BERT](https://arxiv.org/abs/1810.04805) | Transformerä¸€ç»ŸNLPçš„å¼€å§‹                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdf2b0e26d0599ce3e70df8a9da02e51594e0e992%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/BERT%3A-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang/df2b0e26d0599ce3e70df8a9da02e51594e0e992)  |\n| âœ… | 2019 | [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)  |  æ›´å¤§çš„ GPT æ¨¡å‹ï¼Œæœç€zero-shot learningè¿ˆäº†ä¸€å¤§æ­¥             |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F9405cc0d6169988371b2755e573cc28650d14dfe%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)  |\n| âœ… | 2020 |  [GPT-3](https://arxiv.org/abs/2005.14165) | 100å€æ›´å¤§çš„ GPT-2ï¼Œfew-shot learningæ•ˆæœæ˜¾è‘—                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F6b85b63579a916f705a8e10a49bd8d849d91b1fc%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Language-Models-are-Few-Shot-Learners-Brown-Mann/6b85b63579a916f705a8e10a49bd8d849d91b1fc)  |\n| âœ… | 2024 |  [Llama 3.1](https://arxiv.org/pdf/2407.21783) | å¼ºå¤§çš„Metaå¼€æºæ¨¡å‹ - åŠ¨æ€æ‰©å±•ï¼Œå¤šæ¨¡æ€å­¦ä¹ ï¼Œé›¶æ ·æœ¬å­¦ä¹ ï¼Œé«˜æ•ˆè®¡ç®—                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F4176a4cecfaef26b2c503827493867e703f3411a%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/4176a4cecfaef26b2c503827493867e703f3411a)  |\n\n### ç³»ç»Ÿ\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n|  âœ… |  2014 | [å‚æ•°æœåŠ¡å™¨](https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf) | æ”¯æŒåƒäº¿å‚æ•°çš„ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F0de0c3240bda7972bd0a3c8369ebc4b4f2e4f9c2%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Scaling-Distributed-Machine-Learning-with-the-Li-Andersen/0de0c3240bda7972bd0a3c8369ebc4b4f2e4f9c2)  |\n| âœ…  | 2018 | [GPipe](https://proceedings.neurips.cc/paper/2019/file/093f65e080a295f8076b1c5722a46aa2-Paper.pdf) | æµæ°´çº¿ï¼ˆPipelineï¼‰å¹¶è¡Œ      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fc18663fea10c8a303d045fd2c1f33cacf9b73ca3%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/GPipe%3A-Efficient-Training-of-Giant-Neural-Networks-Huang-Cheng/c18663fea10c8a303d045fd2c1f33cacf9b73ca3)  |\n| âœ… | 2019 | [Megatron-LM](https://arxiv.org/pdf/1909.08053.pdf) | å¼ é‡ï¼ˆTensorï¼‰å¹¶è¡Œ      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F8323c591e119eb09b28b29fd6c7bc76bd889df7a%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Megatron-LM%3A-Training-Multi-Billion-Parameter-Using-Shoeybi-Patwary/8323c591e119eb09b28b29fd6c7bc76bd889df7a) |\n| âœ… | 2019 | [Zero](https://arxiv.org/pdf/1910.02054.pdf) | å‚æ•°åˆ†ç‰‡      |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F00c957711b12468cb38424caccdf5291bb354033%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/ZeRO%3A-Memory-optimizations-Toward-Training-Trillion-Rajbhandari-Rasley/00c957711b12468cb38424caccdf5291bb354033)  |\n| âœ… |  2022 | [Pathways](https://arxiv.org/pdf/2203.12533.pdf) |  å°†Jaxæ‹“å±•åˆ°ä¸ŠåƒTPUæ ¸ä¸Š       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F512e9aa873a1cd7eb61ce1f25ca7df6acb7e2352%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Pathways%3A-Asynchronous-Distributed-Dataflow-for-ML-Barham-Chowdhery/512e9aa873a1cd7eb61ce1f25ca7df6acb7e2352)  |\n\n### å›¾ç¥ç»ç½‘ç»œ\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n|  âœ… |  2021 | [å›¾ç¥ç»ç½‘ç»œä»‹ç»](https://distill.pub/2021/gnn-intro/) | GNNçš„å¯è§†åŒ–ä»‹ç»                  |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F2c0e0440882a42be752268d0b64243243d752a74%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/A-Gentle-Introduction-to-Graph-Neural-Networks-S%C3%A1nchez-Lengeling-Reif/2c0e0440882a42be752268d0b64243243d752a74)  |\n\n### ä¼˜åŒ–ç®—æ³•\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| | 2014 | [Adam](https://arxiv.org/abs/1412.6980) | æ·±åº¦å­¦ä¹ é‡Œæœ€å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•ä¹‹ä¸€                   |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fa6cb366736791bcccc5c8639de5a8f9636bf87e8%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Adam%3A-A-Method-for-Stochastic-Optimization-Kingma-Ba/a6cb366736791bcccc5c8639de5a8f9636bf87e8)  |\n| | 2016 |  [ä¸ºä»€ä¹ˆè¶…å¤§çš„æ¨¡å‹æ³›åŒ–æ€§ä¸é”™](https://arxiv.org/abs/1611.03530)   |               |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F54ddb00fa691728944fd8becea90a373d21597cf%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Understanding-deep-learning-requires-rethinking-Zhang-Bengio/54ddb00fa691728944fd8becea90a373d21597cf)  |\n| | 2017 | [ä¸ºä»€ä¹ˆMomentumæœ‰æ•ˆ](https://distill.pub/2017/momentum/) | Distillçš„å¯è§†åŒ–ä»‹ç»            |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3e8ccf9d3d843c9855c5d76ab66d3e775384da72%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Why-Momentum-Really-Works-Goh/3e8ccf9d3d843c9855c5d76ab66d3e775384da72)  |\n\n\n### æ–°é¢†åŸŸåº”ç”¨\n\n\n| å·²å½•åˆ¶ | å¹´ä»½ | åå­—                                                         | ç®€ä»‹                 | å¼•ç”¨ |\n| ------ | ---- | ------------------------------------------------------------ | -------------------- | ------------------------------------------------------------ |\n| | 2016 | [AlphaGo](https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf) | å¼ºåŒ–å­¦ä¹ å‡ºåœˆ                 |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F846aedd869a00c09b40f1f1f35673cb22bc87490%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Mastering-the-game-of-Go-with-deep-neural-networks-Silver-Huang/846aedd869a00c09b40f1f1f35673cb22bc87490)  |\n| | 2020 | [AlphaFold](https://discovery.ucl.ac.uk/id/eprint/10089234/1/343019_3_art_0_py4t4l_convrt.pdf) | èµ¢å¾—æ¯”èµ›çš„çš„è›‹ç™½è´¨3Dç»“æ„é¢„æµ‹ |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F3a083d843f891b3574494c385699c21766ce8b7a%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Improved-protein-structure-prediction-using-from-Senior-Evans/3a083d843f891b3574494c385699c21766ce8b7a)  |\n| âœ… | 2021 | [AlphaFold 2](https://www.nature.com/articles/s41586-021-03819-2.pdf) | åŸå­çº§åˆ«ç²¾åº¦çš„è›‹ç™½è´¨3Dç»“æ„é¢„æµ‹       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fdc32a984b651256a8ec282be52310e6bd33d9815%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Highly-accurate-protein-structure-prediction-with-Jumper-Evans/dc32a984b651256a8ec282be52310e6bd33d9815)  |\n| âœ… | 2021 | [Codex](https://arxiv.org/pdf/2107.03374.pdf) | ä½¿ç”¨æ³¨é‡Šç”Ÿæˆä»£ç        |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Facbdbf49f9bc3f151b93d9ca9a06009f4f6eb269%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Evaluating-Large-Language-Models-Trained-on-Code-Chen-Tworek/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269)  |\n| âœ… | 2021 | [æŒ‡å¯¼æ•°å­¦ç›´è§‰](https://www.nature.com/articles/s41586-021-04086-x.pdf) | åˆ†æä¸åŒæ•°å­¦ç‰©ä½“ä¹‹å‰çš„è”ç³»æ¥å¸®åŠ©å‘ç°æ–°å®šç†         |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Ff672b8fb430606fee0bb368f16603531ce1e90c4%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Advancing-mathematics-by-guiding-human-intuition-AI-Davies-Velickovic/f672b8fb430606fee0bb368f16603531ce1e90c4)  |\n| âœ… | 2022 | [AlphaCode](https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf) | åª²ç¾ä¸€èˆ¬ç¨‹åºå‘˜çš„ç¼–ç¨‹è§£é¢˜æ°´å¹³       |[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F5cbe278b65a81602a864184bbca37de91448a5f5%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/Competition-Level-Code-Generation-with-AlphaCode-Li-Choi/5cbe278b65a81602a864184bbca37de91448a5f5)  |\n\n', '{"language":null,"stars":32098,"forks":2753,"watchers":32098,"open_issues":1,"topics":["deep-learning","paper","reading-list"],"default_branch":"main","size_kb":9001,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:mli:paper-reading","source_url":"https://github.com/mli/paper-reading"}]', NULL, 'Apache-2.0', 'approved', 80, '0ce0c189873cc44bc9aea40a09abb5b9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-mli-paper-reading from https://github.com/mli.png
Image converted to WebP: data/images/github-mli-paper-reading.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-yunjey-pytorch-tutorial', 'github--yunjey--pytorch-tutorial', 'pytorch-tutorial', 'yunjey', '<p align="center"><img width="40%" src="logo/pytorch_logo_2018.svg" /></p> -------------------------------------------------------------------------------- This repository provides tutorial code for deep learning researchers to learn PyTorch. In the tutorial, most of the models were implemented with less than 30 lines of code. Before starting this tutorial, it is recommended to finish Official Pytorch Tutorial. <br/> * PyTorch Basics * Linear Regression * Logistic Regression * Feedforward Neu...', '["deep-learning","neural-networks","pytorch","pytorch-tutorial","python"]', 'other', 31997, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/yunjey/pytorch-tutorial","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center"><img width="40%" src="logo/pytorch_logo_2018.svg" /></p>\n\n--------------------------------------------------------------------------------\n\nThis repository provides tutorial code for deep learning researchers to learn [PyTorch](https://github.com/pytorch/pytorch). In the tutorial, most of the models were implemented with less than 30 lines of code. Before starting this tutorial, it is recommended to finish [Official Pytorch Tutorial](http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html).\n\n\n<br/>\n\n## Table of Contents\n\n#### 1. Basics\n* [PyTorch Basics](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/pytorch_basics/main.py)\n* [Linear Regression](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/linear_regression/main.py#L22-L23)\n* [Logistic Regression](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/logistic_regression/main.py#L33-L34)\n* [Feedforward Neural Network](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/01-basics/feedforward_neural_network/main.py#L37-L49)\n\n#### 2. Intermediate\n* [Convolutional Neural Network](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/convolutional_neural_network/main.py#L35-L56)\n* [Deep Residual Network](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/deep_residual_network/main.py#L76-L113)\n* [Recurrent Neural Network](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/recurrent_neural_network/main.py#L39-L58)\n* [Bidirectional Recurrent Neural Network](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/bidirectional_recurrent_neural_network/main.py#L39-L58)\n* [Language Model (RNN-LM)](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model/main.py#L30-L50)\n\n#### 3. Advanced\n* [Generative Adversarial Networks](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/generative_adversarial_network/main.py#L41-L57)\n* [Variational Auto-Encoder](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py#L38-L65)\n* [Neural Style Transfer](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/neural_style_transfer)\n* [Image Captioning (CNN-RNN)](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning)\n\n#### 4. Utilities\n* [TensorBoard in PyTorch](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/04-utils/tensorboard)\n\n\n<br/>\n\n## Getting Started\n```bash\n$ git clone https://github.com/yunjey/pytorch-tutorial.git\n$ cd pytorch-tutorial/tutorials/PATH_TO_PROJECT\n$ python main.py\n```\n\n<br/>\n\n## Dependencies\n* [Python 2.7 or 3.5+](https://www.continuum.io/downloads)\n* [PyTorch 0.4.0+](http://pytorch.org/)\n\n\n\n\n', '{"language":"Python","stars":31997,"forks":8276,"watchers":31997,"open_issues":89,"topics":["deep-learning","neural-networks","pytorch","pytorch-tutorial"],"default_branch":"master","size_kb":13110,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial.git","source_url":"https://github.com/yunjey/pytorch-tutorial.git"}]', NULL, 'MIT', 'approved', 65, '813384de2a7b87f7a91b75b7dda66737', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-yunjey-pytorch-tutorial from https://github.com/yunjey.png
Image converted to WebP: data/images/github-yunjey-pytorch-tutorial.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-huggingface-diffusers', 'github--huggingface--diffusers', 'diffusers', 'huggingface', '<!--- Copyright 2022 - The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the Lice...', '["deep-learning","diffusion","flux","image-generation","image2image","image2video","latent-diffusion-models","pytorch","qwen-image","score-based-generative-modeling","stable-diffusion","stable-diffusion-diffusers","text2image","text2video","video2video","python"]', 'other', 31972, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/huggingface/diffusers","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<!---\nCopyright 2022 - The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the "License");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an "AS IS" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align="center">\n    <br>\n    <img src="https://raw.githubusercontent.com/huggingface/diffusers/main/docs/source/en/imgs/diffusers_library.jpg" width="400"/>\n    <br>\n<p>\n<p align="center">\n    <a href="https://github.com/huggingface/diffusers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/datasets.svg?color=blue"></a>\n    <a href="https://github.com/huggingface/diffusers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/diffusers.svg"></a>\n    <a href="https://pepy.tech/project/diffusers"><img alt="GitHub release" src="https://static.pepy.tech/badge/diffusers/month"></a>\n    <a href="CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg"></a>\n    <a href="https://twitter.com/diffuserslib"><img alt="X account" src="https://img.shields.io/twitter/url/https/twitter.com/diffuserslib.svg?style=social&label=Follow%20%40diffuserslib"></a>\n</p>\n\nğŸ¤— Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you''re looking for a simple inference solution or training your own diffusion models, ğŸ¤— Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nğŸ¤— Diffusers offers three core components:\n\n- State-of-the-art [diffusion pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview) that can be run in inference with just a few lines of code.\n- Interchangeable noise [schedulers](https://huggingface.co/docs/diffusers/api/schedulers/overview) for different diffusion speeds and output quality.\n- Pretrained [models](https://huggingface.co/docs/diffusers/api/models/overview) that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.\n\n## Installation\n\nWe recommend installing ğŸ¤— Diffusers in a virtual environment from PyPI or Conda. For more details about installing [PyTorch](https://pytorch.org/get-started/locally/), please refer to their official documentation.\n\n### PyTorch\n\nWith `pip` (official package):\n\n```bash\npip install --upgrade diffusers[torch]\n```\n\nWith `conda` (maintained by the community):\n\n```sh\nconda install -c conda-forge diffusers\n```\n\n### Apple Silicon (M1/M2) support\n\nPlease refer to the [How to use Stable Diffusion in Apple Silicon](https://huggingface.co/docs/diffusers/optimization/mps) guide.\n\n## Quickstart\n\nGenerating outputs is super easy with ğŸ¤— Diffusers. To generate an image from text, use the `from_pretrained` method to load any pretrained diffusion model (browse the [Hub](https://huggingface.co/models?library=diffusers&sort=downloads) for 30,000+ checkpoints):\n\n```python\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipeline = DiffusionPipeline.from_pretrained("stable-diffusion-v1-5/stable-diffusion-v1-5", torch_dtype=torch.float16)\npipeline.to("cuda")\npipeline("An image of a squirrel in Picasso style").images[0]\n```\n\nYou can also dig into the models and schedulers toolbox to build your own diffusion system:\n\n```python\nfrom diffusers import DDPMScheduler, UNet2DModel\nfrom PIL import Image\nimport torch\n\nscheduler = DDPMScheduler.from_pretrained("google/ddpm-cat-256")\nmodel = UNet2DModel.from_pretrained("google/ddpm-cat-256").to("cuda")\nscheduler.set_timesteps(50)\n\nsample_size = model.config.sample_size\nnoise = torch.randn((1, 3, sample_size, sample_size), device="cuda")\ninput = noise\n\nfor t in scheduler.timesteps:\n    with torch.no_grad():\n        noisy_residual = model(input, t).sample\n        prev_noisy_sample = scheduler.step(noisy_residual, t, input).prev_sample\n        input = prev_noisy_sample\n\nimage = (input / 2 + 0.5).clamp(0, 1)\nimage = image.cpu().permute(0, 2, 3, 1).numpy()[0]\nimage = Image.fromarray((image * 255).round().astype("uint8"))\nimage\n```\n\nCheck out the [Quickstart](https://huggingface.co/docs/diffusers/quicktour) to launch your diffusion journey today!\n\n## How to navigate the documentation\n\n| **Documentation**                                                   | **What can I learn?**                                                                                                                                                                           |\n|---------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [Tutorial](https://huggingface.co/docs/diffusers/tutorials/tutorial_overview)                                                            | A basic crash course for learning how to use the library''s most important features like using models and schedulers to build your own diffusion system, and training your own diffusion model.  |\n| [Loading](https://huggingface.co/docs/diffusers/using-diffusers/loading)                                                             | Guides for how to load and configure all the components (pipelines, models, and schedulers) of the library, as well as how to use different schedulers.                                         |\n| [Pipelines for inference](https://huggingface.co/docs/diffusers/using-diffusers/overview_techniques)                                             | Guides for how to use pipelines for different inference tasks, batched generation, controlling generated outputs and randomness, and how to contribute a pipeline to the library.               |\n| [Optimization](https://huggingface.co/docs/diffusers/optimization/fp16)                                                        | Guides for how to optimize your diffusion model to run faster and consume less memory.                                                                                                          |\n| [Training](https://huggingface.co/docs/diffusers/training/overview) | Guides for how to train a diffusion model for different tasks with different training techniques.                                                                                               |\n## Contribution\n\nWe â¤ï¸  contributions from the open-source community!\nIf you want to contribute to this library, please check out our [Contribution guide](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md).\nYou can look out for [issues](https://github.com/huggingface/diffusers/issues) you''d like to tackle to contribute to the library.\n- See [Good first issues](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22) for general opportunities to contribute\n- See [New model/pipeline](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+pipeline%2Fmodel%22) to contribute exciting new diffusion models / diffusion pipelines\n- See [New scheduler](https://github.com/huggingface/diffusers/issues?q=is%3Aopen+is%3Aissue+label%3A%22New+scheduler%22)\n\nAlso, say ğŸ‘‹ in our public Discord channel <a href="https://discord.gg/G7tWnz98XR"><img alt="Join us on Discord" src="https://img.shields.io/discord/823813159592001537?color=5865F2&logo=discord&logoColor=white"></a>. We discuss the hottest trends about diffusion models, help each other with contributions, personal projects or just hang out â˜•.\n\n\n## Popular Tasks & Pipelines\n\n<table>\n  <tr>\n    <th>Task</th>\n    <th>Pipeline</th>\n    <th>ğŸ¤— Hub</th>\n  </tr>\n  <tr style="border-top: 2px solid black">\n    <td>Unconditional Image Generation</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/ddpm"> DDPM </a></td>\n    <td><a href="https://huggingface.co/google/ddpm-ema-church-256"> google/ddpm-ema-church-256 </a></td>\n  </tr>\n  <tr style="border-top: 2px solid black">\n    <td>Text-to-Image</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/text2img">Stable Diffusion Text-to-Image</a></td>\n      <td><a href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5"> stable-diffusion-v1-5/stable-diffusion-v1-5 </a></td>\n  </tr>\n  <tr>\n    <td>Text-to-Image</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/unclip">unCLIP</a></td>\n      <td><a href="https://huggingface.co/kakaobrain/karlo-v1-alpha"> kakaobrain/karlo-v1-alpha </a></td>\n  </tr>\n  <tr>\n    <td>Text-to-Image</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/deepfloyd_if">DeepFloyd IF</a></td>\n      <td><a href="https://huggingface.co/DeepFloyd/IF-I-XL-v1.0"> DeepFloyd/IF-I-XL-v1.0 </a></td>\n  </tr>\n  <tr>\n    <td>Text-to-Image</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/kandinsky">Kandinsky</a></td>\n      <td><a href="https://huggingface.co/kandinsky-community/kandinsky-2-2-decoder"> kandinsky-community/kandinsky-2-2-decoder </a></td>\n  </tr>\n  <tr style="border-top: 2px solid black">\n    <td>Text-guided Image-to-Image</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/controlnet">ControlNet</a></td>\n      <td><a href="https://huggingface.co/lllyasviel/sd-controlnet-canny"> lllyasviel/sd-controlnet-canny </a></td>\n  </tr>\n  <tr>\n    <td>Text-guided Image-to-Image</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/pix2pix">InstructPix2Pix</a></td>\n      <td><a href="https://huggingface.co/timbrooks/instruct-pix2pix"> timbrooks/instruct-pix2pix </a></td>\n  </tr>\n  <tr>\n    <td>Text-guided Image-to-Image</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/img2img">Stable Diffusion Image-to-Image</a></td>\n      <td><a href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5"> stable-diffusion-v1-5/stable-diffusion-v1-5 </a></td>\n  </tr>\n  <tr style="border-top: 2px solid black">\n    <td>Text-guided Image Inpainting</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint">Stable Diffusion Inpainting</a></td>\n      <td><a href="https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-inpainting"> stable-diffusion-v1-5/stable-diffusion-inpainting </a></td>\n  </tr>\n  <tr style="border-top: 2px solid black">\n    <td>Image Variation</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/image_variation">Stable Diffusion Image Variation</a></td>\n      <td><a href="https://huggingface.co/lambdalabs/sd-image-variations-diffusers"> lambdalabs/sd-image-variations-diffusers </a></td>\n  </tr>\n  <tr style="border-top: 2px solid black">\n    <td>Super Resolution</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/upscale">Stable Diffusion Upscale</a></td>\n      <td><a href="https://huggingface.co/stabilityai/stable-diffusion-x4-upscaler"> stabilityai/stable-diffusion-x4-upscaler </a></td>\n  </tr>\n  <tr>\n    <td>Super Resolution</td>\n    <td><a href="https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/latent_upscale">Stable Diffusion Latent Upscale</a></td>\n      <td><a href="https://huggingface.co/stabilityai/sd-x2-latent-upscaler"> stabilityai/sd-x2-latent-upscaler </a></td>\n  </tr>\n</table>\n\n## Popular libraries using ğŸ§¨ Diffusers\n\n- https://github.com/microsoft/TaskMatrix\n- https://github.com/invoke-ai/InvokeAI\n- https://github.com/InstantID/InstantID\n- https://github.com/apple/ml-stable-diffusion\n- https://github.com/Sanster/lama-cleaner\n- https://github.com/IDEA-Research/Grounded-Segment-Anything\n- https://github.com/ashawkey/stable-dreamfusion\n- https://github.com/deep-floyd/IF\n- https://github.com/bentoml/BentoML\n- https://github.com/bmaltais/kohya_ss\n- +14,000 other amazing GitHub repositories ğŸ’ª\n\nThank you for using us â¤ï¸.\n\n## Credits\n\nThis library concretizes previous work by many different authors and would not have been possible without their great research and implementations. We''d like to thank, in particular, the following implementations which have helped us in our development and without which the API could not have been as polished today:\n\n- @CompVis'' latent diffusion models library, available [here](https://github.com/CompVis/latent-diffusion)\n- @hojonathanho original DDPM implementation, available [here](https://github.com/hojonathanho/diffusion) as well as the extremely useful translation into PyTorch by @pesser, available [here](https://github.com/pesser/pytorch_diffusion)\n- @ermongroup''s DDIM implementation, available [here](https://github.com/ermongroup/ddim)\n- @yang-song''s Score-VE and Score-VP implementations, available [here](https://github.com/yang-song/score_sde_pytorch)\n\nWe also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, available [here](https://github.com/heejkoo/Awesome-Diffusion-Models) as well as @crowsonkb and @rromb for useful discussions and insights.\n\n## Citation\n\n```bibtex\n@misc{von-platen-etal-2022-diffusers,\n  author = {Patrick von Platen and Suraj Patil and Anton Lozhkov and Pedro Cuenca and Nathan Lambert and Kashif Rasul and Mishig Davaadorj and Dhruv Nair and Sayak Paul and William Berman and Yiyi Xu and Steven Liu and Thomas Wolf},\n  title = {Diffusers: State-of-the-art diffusion models},\n  year = {2022},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\url{https://github.com/huggingface/diffusers}}\n}\n```\n', '{"language":"Python","stars":31972,"forks":6575,"watchers":31972,"open_issues":948,"topics":["deep-learning","diffusion","flux","image-generation","image2image","image2video","latent-diffusion-models","pytorch","qwen-image","score-based-generative-modeling","stable-diffusion","stable-diffusion-diffusers","text2image","text2video","video2video"],"default_branch":"main","size_kb":89556,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:microsoft:TaskMatrix","source_url":"https://github.com/microsoft/TaskMatrix"},{"type":"has_code","target_id":"github:invoke-ai:InvokeAI","source_url":"https://github.com/invoke-ai/InvokeAI"},{"type":"has_code","target_id":"github:InstantID:InstantID","source_url":"https://github.com/InstantID/InstantID"},{"type":"has_code","target_id":"github:apple:ml-stable-diffusion","source_url":"https://github.com/apple/ml-stable-diffusion"},{"type":"has_code","target_id":"github:Sanster:lama-cleaner","source_url":"https://github.com/Sanster/lama-cleaner"},{"type":"has_code","target_id":"github:IDEA-Research:Grounded-Segment-Anything","source_url":"https://github.com/IDEA-Research/Grounded-Segment-Anything"},{"type":"has_code","target_id":"github:ashawkey:stable-dreamfusion","source_url":"https://github.com/ashawkey/stable-dreamfusion"},{"type":"has_code","target_id":"github:deep-floyd:IF","source_url":"https://github.com/deep-floyd/IF"},{"type":"has_code","target_id":"github:bentoml:BentoML","source_url":"https://github.com/bentoml/BentoML"},{"type":"has_code","target_id":"github:bmaltais:kohya_ss","source_url":"https://github.com/bmaltais/kohya_ss"},{"type":"has_code","target_id":"github:CompVis:latent-diffusion","source_url":"https://github.com/CompVis/latent-diffusion"},{"type":"has_code","target_id":"github:hojonathanho:diffusion","source_url":"https://github.com/hojonathanho/diffusion"},{"type":"has_code","target_id":"github:pesser:pytorch_diffusion","source_url":"https://github.com/pesser/pytorch_diffusion"},{"type":"has_code","target_id":"github:ermongroup:ddim","source_url":"https://github.com/ermongroup/ddim"},{"type":"has_code","target_id":"github:yang-song:score_sde_pytorch","source_url":"https://github.com/yang-song/score_sde_pytorch"},{"type":"has_code","target_id":"github:heejkoo:Awesome-Diffusion-Models","source_url":"https://github.com/heejkoo/Awesome-Diffusion-Models"},{"type":"has_code","target_id":"github:huggingface:diffusers}}","source_url":"https://github.com/huggingface/diffusers}}"}]', NULL, 'Apache-2.0', 'approved', 80, '2d38181d2c984df1b26b2e0abfdda324', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-huggingface-diffusers from https://github.com/huggingface.png
Image converted to WebP: data/images/github-huggingface-diffusers.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-tatsu-lab-stanford-alpaca', 'github--tatsu-lab--stanford-alpaca', 'stanford_alpaca', 'tatsu-lab', '<p align="center" width="100%"> <img src="assets/logo.png" alt="Stanford-Alpaca" style="width: 50%; min-width: 300px; display: block; margin: auto;"> </p> This is the repo for the Stanford Alpaca project, which aims to build and share an instruction-following LLaMA model. The repo contains: - The 52K data used for fine-tuning the model. - The code for generating the data. - The code for fine-tuning the model. - The code for recovering Alpaca-7B weights from our released weight diff. Note: We ...', '["deep-learning","instruction-following","language-model","python"]', 'other', 30243, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/tatsu-lab/stanford_alpaca","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '\n<p align="center" width="100%">\n<img src="assets/logo.png" alt="Stanford-Alpaca" style="width: 50%; min-width: 300px; display: block; margin: auto;">\n</p>\n\n# Stanford Alpaca: An Instruction-following LLaMA Model\n\n[![Code License](https://img.shields.io/badge/Code%20License-Apache_2.0-green.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/LICENSE)\n[![Data License](https://img.shields.io/badge/Data%20License-CC%20By%20NC%204.0-red.svg)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/DATA_LICENSE)\n[![Weight Diff License](https://img.shields.io/badge/Weight%20Diff%20License-CC%20By%20NC%204.0-yellow)](https://github.com/tatsu-lab/stanford_alpaca/blob/main/WEIGHT_DIFF_LICENSE)\n[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\nThis is the repo for the Stanford Alpaca project, which aims to build and share an instruction-following LLaMA model. The repo contains:\n\n- The [52K data](#data-release) used for fine-tuning the model.\n- The code for [generating the data](#data-generation-process).\n- The code for [fine-tuning the model](#fine-tuning).\n- The code for [recovering Alpaca-7B weights from our released weight diff](#recovering-alpaca-weights).\n\nNote: We thank the community for feedback on Stanford-Alpaca and supporting our research. Our live demo is suspended until further notice.\n\n**Usage and License Notices**: Alpaca is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes. \nThe weight diff is also CC BY NC 4.0 (allowing only non-commercial use).\n\n## Overview\n\nThe current Alpaca model is fine-tuned from a 7B LLaMA model [1] on 52K instruction-following data generated by the techniques in the Self-Instruct [2] paper, with some modifications that we discuss in the next section.\nIn a preliminary human evaluation, we found that the Alpaca 7B model behaves similarly to the `text-davinci-003` model on the Self-Instruct instruction-following evaluation suite [2].\n\nAlpaca is still under development, and there are many limitations that have to be addressed.\nImportantly, we have not yet fine-tuned the Alpaca model to be safe and harmless.\nWe thus encourage users to be cautious when interacting with Alpaca, and to report any concerning behavior to help improve the safety and ethical considerations of the model.\n\nOur initial release contains the data generation procedure, dataset, and training recipe. We intend to release the model weights if we are given permission to do so by the creators of LLaMA. For now, we have chosen to host a live demo to help readers better understand the capabilities and limits of Alpaca, as well as a way to help us better evaluate Alpaca''s performance on a broader audience.\n\n**Please read our release [blog post](https://crfm.stanford.edu/2023/03/13/alpaca.html) for more details about the model, our discussion of the potential harm and limitations of Alpaca models, and our thought process for releasing a reproducible model.**\n\n[1]: LLaMA: Open and Efficient Foundation Language Models. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. https://arxiv.org/abs/2302.13971v1\n\n[2]: Self-Instruct: Aligning Language Model with Self Generated Instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi. https://arxiv.org/abs/2212.10560\n\n## Data Release\n\n[`alpaca_data.json`](./alpaca_data.json) contains 52K instruction-following data we used for fine-tuning the Alpaca model.\nThis JSON file is a list of dictionaries, each dictionary contains the following fields:\n\n- `instruction`: `str`, describes the task the model should perform. Each of the 52K instructions is unique.\n- `input`: `str`, optional context or input for the task. For example, when the instruction is "Summarize the following article", the input is the article. Around 40% of the examples have an input.\n- `output`: `str`, the answer to the instruction as generated by `text-davinci-003`.\n\nWe used the following prompts for fine-tuning the Alpaca model:\n\n- for examples with a non-empty input field:\n\n ```\n Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n \n ### Instruction:\n {instruction}\n \n ### Input:\n {input}\n \n ### Response:\n ```\n\n- for examples with an empty input field:\n\n ```\n Below is an instruction that describes a task. Write a response that appropriately completes the request.\n \n ### Instruction:\n {instruction}\n \n ### Response:\n ```\n\n During inference (eg for the web demo), we use the user instruction with an empty input field (second option).\n\n## Data Generation Process\n\n<details>\n<summary> <strong> Running the code </strong> </summary>\n\n1. Set environment variables `OPENAI_API_KEY` to your OpenAI API key.\n2. Install the dependencies with `pip install -r requirements.txt`.\n3. Run `python -m generate_instruction generate_instruction_following_data` to generate the data.\n\n</details>\n\nWe built on the data generation pipeline from [self-instruct](https://github.com/yizhongw/self-instruct) and made the following modifications:\n\n- We used `text-davinci-003` to generate the instruction data instead of `davinci`.\n- We wrote a new prompt (`prompt.txt`) that explicitly gave the requirement of instruction generation to `text-davinci-003`. Note: there is a slight error in the prompt we used, and future users should incorporate the edit in <https://github.com/tatsu-lab/stanford_alpaca/pull/24>\n- We adopted much more aggressive batch decoding, i.e., generating 20 instructions at once, which significantly reduced the cost of data generation.\n- We simplified the data generation pipeline by discarding the difference between classification and non-classification instructions.\n- We only generated a single instance for each instruction, instead of 2 to 3 instances as in [1].\n\nThis produced an instruction-following dataset with 52K examples obtained at a much lower cost (less than $500).\nIn a preliminary study, we also find our 52K generated data to be much more diverse than the data released by [self-instruct](https://github.com/yizhongw/self-instruct/blob/main/data/seed_tasks.jsonl).\nWe plot the below figure (in the style of Figure 2 in the [self-instruct paper](https://arxiv.org/abs/2212.10560) to demonstrate the diversity of our data.\nThe inner circle of the plot represents the root verb of the instructions, and the outer circle represents the direct objects.\n\n[//]: # (![parse_analysis]&#40;assert/parse_analysis.png | width=100&#41;)\n[<img src="assets/parse_analysis.png" width="750" />](./assets/parse_analysis.png)\n\n## Fine-tuning\n\nWe fine-tune our models using standard Hugging Face training code.\nWe fine-tune LLaMA-7B and LLaMA-13B with the following hyperparameters:\n\n| Hyperparameter | LLaMA-7B | LLaMA-13B |\n|----------------|----------|-----------|\n| Batch size     | 128      | 128       |\n| Learning rate  | 2e-5     | 1e-5      |\n| Epochs         | 3        | 5         |\n| Max length     | 512      | 512       |\n| Weight decay   | 0        | 0         |\n\nTo reproduce our fine-tuning runs for LLaMA, first install the requirements\n\n```bash\npip install -r requirements.txt\n```\n\nBelow is a command that fine-tunes LLaMA-7B with our dataset on a machine with 4 A100 80G GPUs in FSDP `full_shard` mode.\nWe were able to reproduce a model of similar quality as the one we hosted in our demo with the following command using **Python 3.10**.\nReplace `<your_random_port>` with a port of your own, `<your_path_to_hf_converted_llama_ckpt_and_tokenizer>` with the\npath to your converted checkpoint and tokenizer (following instructions in the PR), and `<your_output_dir>` with where you want to store your outputs.\n\n```bash\ntorchrun --nproc_per_node=4 --master_port=<your_random_port> train.py \\n    --model_name_or_path <your_path_to_hf_converted_llama_ckpt_and_tokenizer> \\n    --data_path ./alpaca_data.json \\n    --bf16 True \\n    --output_dir <your_output_dir> \\n    --num_train_epochs 3 \\n    --per_device_train_batch_size 4 \\n    --per_device_eval_batch_size 4 \\n    --gradient_accumulation_steps 8 \\n    --evaluation_strategy "no" \\n    --save_strategy "steps" \\n    --save_steps 2000 \\n    --save_total_limit 1 \\n    --learning_rate 2e-5 \\n    --weight_decay 0. \\n    --warmup_ratio 0.03 \\n    --lr_scheduler_type "cosine" \\n    --logging_steps 1 \\n    --fsdp "full_shard auto_wrap" \\n    --fsdp_transformer_layer_cls_to_wrap ''LlamaDecoderLayer'' \\n    --tf32 True\n```\n\nThe same script also works for OPT fine-tuning. Here''s an example for fine-tuning OPT-6.7B\n\n```bash\ntorchrun --nproc_per_node=4 --master_port=<your_random_port> train.py \\n    --model_name_or_path "facebook/opt-6.7b" \\n    --data_path ./alpaca_data.json \\n    --bf16 True \\n    --output_dir <your_output_dir> \\n    --num_train_epochs 3 \\n    --per_device_train_batch_size 4 \\n    --per_device_eval_batch_size 4 \\n    --gradient_accumulation_steps 8 \\n    --evaluation_strategy "no" \\n    --save_strategy "steps" \\n    --save_steps 2000 \\n    --save_total_limit 1 \\n    --learning_rate 2e-5 \\n    --weight_decay 0. \\n    --warmup_ratio 0.03 \\n    --lr_scheduler_type "cosine" \\n    --logging_steps 1 \\n    --fsdp "full_shard auto_wrap" \\n    --fsdp_transformer_layer_cls_to_wrap ''OPTDecoderLayer'' \\n    --tf32 True\n```\n\nNote the given training script is meant to be simple and easy to use, and is not particularly optimized.\nTo run on more gpus, you may prefer to turn down `gradient_accumulation_steps` to keep a global batch size of 128. Global batch size has not been tested for optimality.\n\n### Addressing OOM\n\nNaively, fine-tuning a 7B model requires about 7 x 4 x 4 = 112 GB of VRAM. Commands given above enable parameter sharding, so no redundant model copy is stored on any GPU.\nIf you''d like to further reduce the memory footprint, here are some options:\n\n- Turn on CPU offload for FSDP with `--fsdp "full_shard auto_wrap offload"`. This saves VRAM at the cost of longer runtime.\n- In our experience, DeepSpeed stage-3 (with offload) can at times be more memory efficient than FSDP with offload. Here''s an example to use DeepSpeed stage-3 with 4 GPUs with both parameter and optimizer offload:\n    ```bash\n    pip install deepspeed\n    torchrun --nproc_per_node=4 --master_port=<your_random_port> train.py \\n        --model_name_or_path <your_path_to_hf_converted_llama_ckpt_and_tokenizer> \\n        --data_path ./alpaca_data.json \\n        --bf16 True \\n        --output_dir <your_output_dir> \\n        --num_train_epochs 3 \\n        --per_device_train_batch_size 4 \\n        --per_device_eval_batch_size 4 \\n        --gradient_accumulation_steps 8 \\n        --evaluation_strategy "no" \\n        --save_strategy "steps" \\n        --save_steps 2000 \\n        --save_total_limit 1 \\n        --learning_rate 2e-5 \\n        --weight_decay 0. \\n        --warmup_ratio 0.03 \\n        --deepspeed "./configs/default_offload_opt_param.json" \\n        --tf32 True\n    ```\n  - The DeepSpeed library also provides some [helpful functions](https://deepspeed.readthedocs.io/en/latest/memory.html) to estimate memory usage. \n- [LoRA](https://arxiv.org/abs/2106.09685) fine-tunes low-rank slices of the query, key, and value embedding heads. This can reduce the total memory footprint from 112GB to about 7x4=28GB. We may release our re-implemention of this in the future, but for now the [peft](https://github.com/huggingface/peft) codebase can be a useful resource.\n\n## Recovering Alpaca Weights\n\nThe weight diff between Alpaca-7B and LLaMA-7B is located [here](https://huggingface.co/tatsu-lab/alpaca-7b-wdiff/tree/main).\nTo recover the original Alpaca-7B weights, follow these steps:\n```text\n1. Convert Meta''s released weights into huggingface format. Follow this guide:\n    https://huggingface.co/docs/transformers/main/model_doc/llama\n2. Make sure you cloned the released weight diff into your local machine. The weight diff is located at:\n    https://huggingface.co/tatsu-lab/alpaca-7b/tree/main\n3. Run this function with the correct paths. E.g.,\n    python weight_diff.py recover --path_raw <path_to_step_1_dir> --path_diff <path_to_step_2_dir> --path_tuned <path_to_store_recovered_weights>\n```\n\nOnce step 3 completes, you should have a directory with the recovered weights, from which you can load the model like the following\n\n```python\nimport transformers\nalpaca_model = transformers.AutoModelForCausalLM.from_pretrained("<path_to_store_recovered_weights>")\nalpaca_tokenizer = transformers.AutoTokenizer.from_pretrained("<path_to_store_recovered_weights>")\n```\n\n### Authors\n\nAll grad students below contributed equally and the order is determined by random draw.\n\n- [Rohan Taori](https://www.rohantaori.com/)\n- [Ishaan Gulrajani](https://ishaan.io/)\n- [Tianyi Zhang](https://tiiiger.github.io/)\n- [Yann Dubois](https://yanndubs.github.io/)\n- [Xuechen Li](https://www.lxuechen.com/)\n\nAll advised by [Tatsunori B. Hashimoto](https://thashim.github.io/). Yann is also advised by [Percy Liang](https://cs.stanford.edu/~pliang/) and Xuechen is also advised by [Carlos Guestrin](https://guestrin.su.domains/).\n\n### Citation\n\nPlease cite the repo if you use the data or code in this repo.\n\n```\n@misc{alpaca,\n  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },\n  title = {Stanford Alpaca: An Instruction-following LLaMA model},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},\n}\n```\n\nNaturally, you should also cite the original LLaMA paper [1] and the Self-Instruct paper [2].\n\n### Acknowledgements\n\nWe thank Yizhong Wang for his help in explaining the data generation pipeline in Self-Instruct and providing the code for the parse analysis plot.\nWe thank Yifan Mai for helpful support, and members of the Stanford NLP Group as well as the Center for Research on Foundation Models (CRFM) for their helpful feedback.\n', '{"language":"Python","stars":30243,"forks":4031,"watchers":30243,"open_issues":188,"topics":["deep-learning","instruction-following","language-model"],"default_branch":"main","size_kb":8654,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:tatsu-lab:stanford_alpaca","source_url":"https://github.com/tatsu-lab/stanford_alpaca"},{"type":"has_code","target_id":"github:tatsu-lab:stanford_alpaca","source_url":"https://github.com/tatsu-lab/stanford_alpaca"},{"type":"has_code","target_id":"github:tatsu-lab:stanford_alpaca","source_url":"https://github.com/tatsu-lab/stanford_alpaca"},{"type":"has_code","target_id":"github:psf:black","source_url":"https://github.com/psf/black"},{"type":"has_code","target_id":"github:yizhongw:self-instruct","source_url":"https://github.com/yizhongw/self-instruct"},{"type":"has_code","target_id":"github:tatsu-lab:stanford_alpaca","source_url":"https://github.com/tatsu-lab/stanford_alpaca"},{"type":"has_code","target_id":"github:yizhongw:self-instruct","source_url":"https://github.com/yizhongw/self-instruct"},{"type":"has_code","target_id":"github:huggingface:peft","source_url":"https://github.com/huggingface/peft"},{"type":"has_code","target_id":"github:tatsu-lab:stanford_alpaca}},","source_url":"https://github.com/tatsu-lab/stanford_alpaca}},"}]', NULL, 'Apache-2.0', 'approved', 80, '8d49d6bde35a2cbfe80a7c368952f807', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-tatsu-lab-stanford-alpaca from https://github.com/tatsu-lab.png
Image converted to WebP: data/images/github-tatsu-lab-stanford-alpaca.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-deezer-spleeter', 'github--deezer--spleeter', 'spleeter', 'deezer', '<img src="https://github.com/deezer/spleeter/raw/master/images/spleeter_logo.png" height="80" /> !PyPI - Python Version > :warning: Spleeter 2.1.0 release introduces some breaking changes, including new CLI option naming for input, and the drop > of dedicated GPU package. Please read CHANGELOG for more details. **Spleeter** is Deezer source separation library with pretrained models written in Python and uses Tensorflow. It makes it easy to train source separation model (assuming you have a da...', '["audio-processing","bass","deep-learning","deezer","drums","model","pretrained-models","python","tensorflow","vocals","python"]', 'other', 27852, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/deezer/spleeter","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<img src="https://github.com/deezer/spleeter/raw/master/images/spleeter_logo.png" height="80" />\n\n[![Github actions](https://github.com/deezer/spleeter/workflows/pytest/badge.svg)](https://github.com/deezer/spleeter/actions) ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/spleeter) [![PyPI version](https://badge.fury.io/py/spleeter.svg)](https://badge.fury.io/py/spleeter) [![Conda](https://img.shields.io/conda/vn/deezer-research/spleeter)](https://anaconda.org/deezer-research/spleeter) [![Docker Pulls](https://img.shields.io/docker/pulls/deezer/spleeter)](https://hub.docker.com/r/deezer/spleeter) [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/deezer/spleeter/blob/master/spleeter.ipynb) [![Gitter chat](https://badges.gitter.im/gitterHQ/gitter.png)](https://gitter.im/spleeter/community) [![status](https://joss.theoj.org/papers/259e5efe669945a343bad6eccb89018b/status.svg)](https://joss.theoj.org/papers/259e5efe669945a343bad6eccb89018b)\n\n> :warning: [Spleeter 2.1.0](https://pypi.org/project/spleeter/) release introduces some breaking changes, including new CLI option naming for input, and the drop\n> of dedicated GPU package. Please read [CHANGELOG](CHANGELOG.md) for more details.\n\n## About\n\n**Spleeter** is [Deezer](https://www.deezer.com/) source separation library with pretrained models\nwritten in [Python](https://www.python.org/) and uses [Tensorflow](https://tensorflow.org/). It makes it easy\nto train source separation model (assuming you have a dataset of isolated sources), and provides\nalready trained state of the art model for performing various flavour of separation :\n\n* Vocals (singing voice) / accompaniment separation ([2 stems](https://github.com/deezer/spleeter/wiki/2.-Getting-started#using-2stems-model))\n* Vocals / drums / bass / other separation ([4 stems](https://github.com/deezer/spleeter/wiki/2.-Getting-started#using-4stems-model))\n* Vocals / drums / bass / piano / other separation ([5 stems](https://github.com/deezer/spleeter/wiki/2.-Getting-started#using-5stems-model))\n\n2 stems and 4 stems models have [high performances](https://github.com/deezer/spleeter/wiki/Separation-Performances) on the [musdb](https://sigsep.github.io/datasets/musdb.html) dataset. **Spleeter** is also very fast as it can perform separation of audio files to 4 stems 100x faster than real-time when run on a GPU.\n\nWe designed **Spleeter** so you can use it straight from [command line](https://github.com/deezer/spleeter/wiki/2.-Getting-started#usage)\nas well as directly in your own development pipeline as a [Python library](https://github.com/deezer/spleeter/wiki/4.-API-Reference#separator). It can be installed with [pip](https://github.com/deezer/spleeter/wiki/1.-Installation#using-pip) or be used with\n[Docker](https://github.com/deezer/spleeter/wiki/2.-Getting-started#using-docker-image).\n\n### Projects and Softwares using **Spleeter**\n\nSince it''s been released, there are multiple forks exposing **Spleeter** through either a Guided User Interface (GUI) or a standalone free or paying website. Please note that we do not host, maintain or directly support any of these initiatives.\n\nThat being said, many cool projects have been built on top of ours. Notably the porting to the *Ableton Live* ecosystem through the [Spleeter 4 Max](https://github.com/diracdeltas/spleeter4max#spleeter-for-max) project.\n\n**Spleeter** pre-trained models have also been used by professionnal audio softwares. Here''s a non-exhaustive list:\n\n* [iZotope](https://www.izotope.com/en/shop/rx-8-standard.html) in its *Music Rebalance* feature within **RX 8**\n* [SpectralLayers](https://new.steinberg.net/spectralayers/) in its *Unmix* feature in **SpectralLayers 7**\n* [Acon Digital](https://acondigital.com/products/acoustica-audio-editor/) within **Acoustica 7**\n* [VirtualDJ](https://www.virtualdj.com/stems/) in their stem isolation feature\n* [Algoriddim](https://www.algoriddim.com/apps) in their **NeuralMix** and **djayPRO** app suite\n\nğŸ†• **Spleeter** is a baseline in the ongoing [Music Demixing Challenge](https://www.aicrowd.com/challenges/music-demixing-challenge-ismir-2021)!\n\n## Spleeter Pro (Commercial version)\n\nCheck out our commercial version : [Spleeter Pro](https://www.deezer-techservices.com/solutions/spleeter/). Benefit from our expertise for precise audio separation, faster processing speeds, and dedicated professional support. \n\n## Quick start\n\nWant to try it out but don''t want to install anything ? We have set up a [Google Colab](https://colab.research.google.com/github/deezer/spleeter/blob/master/spleeter.ipynb).\n\nReady to dig into it ? In a few lines you can install **Spleeter**  and separate the vocal and accompaniment parts from an example audio file.\nYou need first to install `ffmpeg` and `libsndfile`. It can be done on most platform using [Conda](https://github.com/deezer/spleeter/wiki/1.-Installation#using-conda):\n\n```bash\n# install dependencies using conda\nconda install -c conda-forge ffmpeg libsndfile\n# install spleeter with pip\npip install spleeter\n# download an example audio file (if you don''t have wget, use another tool for downloading)\nwget https://github.com/deezer/spleeter/raw/master/audio_example.mp3\n# separate the example audio into two components\nspleeter separate -p spleeter:2stems -o output audio_example.mp3\n```\n\n> :warning: Note that we no longer recommend using `conda` for installing spleeter.\n\n> âš ï¸ There are known issues with Apple M1 chips, mostly due to TensorFlow compatibility. Until these are fixed, you can use [this workaround](https://github.com/deezer/spleeter/issues/607#issuecomment-1021669444).\n\nYou should get two separated audio files (`vocals.wav` and `accompaniment.wav`) in the `output/audio_example` folder.\n\nFor a detailed documentation, please check the [repository wiki](https://github.com/deezer/spleeter/wiki/1.-Installation)\n\n## Development and Testing\n\nThis project is managed using [Poetry](https://python-poetry.org/docs/basic-usage/), to run test suite you\ncan execute the following set of commands:\n\n```bash\n# Clone spleeter repository\ngit clone https://github.com/Deezer/spleeter && cd spleeter\n# Install poetry\npip install poetry\n# Install spleeter dependencies\npoetry install\n# Run unit test suite\npoetry run pytest tests/\n```\n\n## Reference\n\n* Deezer Research - Source Separation Engine Story - deezer.io blog post:\n  * [English version](https://deezer.io/releasing-spleeter-deezer-r-d-source-separation-engine-2b88985e797e)\n  * [Japanese version](http://dzr.fm/splitterjp)\n* [Music Source Separation tool with pre-trained models / ISMIR2019 extended abstract](http://archives.ismir.net/ismir2019/latebreaking/000036.pdf)\n\nIf you use **Spleeter** in your work, please cite:\n\n```BibTeX\n@article{spleeter2020,\n  doi = {10.21105/joss.02154},\n  url = {https://doi.org/10.21105/joss.02154},\n  year = {2020},\n  publisher = {The Open Journal},\n  volume = {5},\n  number = {50},\n  pages = {2154},\n  author = {Romain Hennequin and Anis Khlif and Felix Voituret and Manuel Moussallam},\n  title = {Spleeter: a fast and efficient music source separation tool with pre-trained models},\n  journal = {Journal of Open Source Software},\n  note = {Deezer Research}\n}\n```\n\n## License\n\nThe code of **Spleeter** is [MIT-licensed](LICENSE).\n\n## Disclaimer\n\nIf you plan to use **Spleeter** on copyrighted material, make sure you get proper authorization from right owners beforehand.\n\n## Troubleshooting\n\n**Spleeter** is a complex piece of software and although we continously try to improve and test it you may encounter unexpected issues running it. If that''s the case please check the [FAQ page](https://github.com/deezer/spleeter/wiki/5.-FAQ) first as well as the list of [currently open issues](https://github.com/deezer/spleeter/issues)\n\n### Windows users\n\n   It appears that sometimes the shortcut command `spleeter` does not work properly on windows. This is a known issue that we will hopefully fix soon. In the meantime replace `spleeter separate` by `python -m spleeter separate` in command line and it should work.\n\n## Contributing\n\nIf you would like to participate in the development of **Spleeter** you are more than welcome to do so. Don''t hesitate to throw us a pull request and we''ll do our best to examine it quickly. Please check out our [guidelines](.github/CONTRIBUTING.md) first.\n\n## Note\n\nThis repository include a demo audio file `audio_example.mp3` which is an excerpt\nfrom Slow Motion Dream by Steven M Bryant (c) copyright 2011 Licensed under a Creative\nCommons Attribution (3.0) [license](http://dig.ccmixter.org/files/stevieb357/34740)\nFt: CSoul,Alex Beroza & Robert Siekawitch\n', '{"language":"Python","stars":27852,"forks":3054,"watchers":27852,"open_issues":272,"topics":["audio-processing","bass","deep-learning","deezer","drums","model","pretrained-models","python","tensorflow","vocals"],"default_branch":"master","size_kb":9630,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:diracdeltas:spleeter4max","source_url":"https://github.com/diracdeltas/spleeter4max#spleeter-for-max"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:Deezer:spleeter","source_url":"https://github.com/Deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"},{"type":"has_code","target_id":"github:deezer:spleeter","source_url":"https://github.com/deezer/spleeter"}]', NULL, 'MIT', 'approved', 65, '592da5b210e457129c7145904a825b2f', NULL, NULL, CURRENT_TIMESTAMP);
