/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-CrucibleAI-ControlNetMediaPipeFace', 'huggingface--crucibleai--controlnetmediapipeface', 'ControlNetMediaPipeFace', 'CrucibleAI', '--- language: - en thumbnail: '''' tags: - controlnet - laion - face - mediapipe - image-to-image license: openrail base_model: stabilityai/stable-diffusion-2-1-base datasets: - LAION-Face - LAION pipeline_tag: image-to-image --- - Overview: Samples, Contents, and Construction - Usage: Downloading, Training, and Inference - License - Credits and Thanks This dataset is designed to train a ControlNet with human facial expressions. It includes keypoints for pupils to allow gaze direction. Training...', '["diffusers","safetensors","controlnet","laion","face","mediapipe","image-to-image","en","dataset:laion-face","dataset:laion","arxiv:2302.05543","arxiv:2112.10752","arxiv:2210.08402","base_model:stabilityai/stable-diffusion-2-1-base","license:openrail","region:us"]', 'image-to-image', 574, 1187, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage:\n- en\nthumbnail: ''''\ntags:\n- controlnet\n- laion\n- face\n- mediapipe\n- image-to-image\nlicense: openrail\nbase_model: stabilityai/stable-diffusion-2-1-base\ndatasets:\n- LAION-Face\n- LAION\npipeline_tag: image-to-image\n---\n\n# ControlNet LAION Face Dataset\n\n## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n\n## Samples:\n\nCherry-picked from ControlNet + Stable Diffusion v2.1 Base\n\n|Input|Face Detection|Output|\n|:---:|:---:|:---:|\n|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/happy_source.jpg">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/happy_annotation.png">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/happy_result.png">|\n|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/neutral_source.jpg">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/neutral_annotation.png">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/neutral_result.png">|\n|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sad_source.jpg">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sad_annotation.png">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sad_result.png">|\n|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/screaming_source.jpg">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/screaming_annotation.png">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/screaming_result.png">|\n|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sideways_source.jpg">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sideways_annotation.png">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/sideways_result.png">|\n|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/surprised_source.jpg">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/surprised_annotation.png">|<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/surprised_result.png">|\n\nImages with multiple faces are also supported:\n\n<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/family_source.jpg">\n\n<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/family_annotation.png">\n\n<img src="https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/family_result.png">\n\n\n## Dataset Contents:\n\n- train_laion_face.py - Entrypoint for ControlNet training.\n- laion_face_dataset.py - Code for performing dataset iteration.  Cropping and resizing happens here.\n- tool_download_face_targets.py - A tool to read metadata.json and populate the target folder.\n- tool_generate_face_poses.py - The original file used to generate the source images.  Included for reproducibility, but not required for training.\n- training/laion-face-processed/prompt.jsonl - Read by laion_face_dataset.  Includes prompts for the images.\n- training/laion-face-processed/metadata.json - Excerpts from LAION for the relevant data.  Also used for downloading the target dataset.\n- training/laion-face-processed/source/xxxxxxxxx.jpg - Images with detections performed.  Generated from the target images.\n- training/laion-face-processed/target/xxxxxxxxx.jpg - Selected images from LAION Face.\n\n## Dataset Construction:\n\nSource images were generated by pulling slice 00000 from LAION Face and passing them through MediaPipe''s face detector with special configuration parameters.  \n\nThe colors and line thicknesses used for MediaPipe are as follows:\n\n```\nf_thick = 2\nf_rad = 1\nright_iris_draw = DrawingSpec(color=(10, 200, 250), thickness=f_thick, circle_radius=f_rad)\nright_eye_draw = DrawingSpec(color=(10, 200, 180), thickness=f_thick, circle_radius=f_rad)\nright_eyebrow_draw = DrawingSpec(color=(10, 220, 180), thickness=f_thick, circle_radius=f_rad)\nleft_iris_draw = DrawingSpec(color=(250, 200, 10), thickness=f_thick, circle_radius=f_rad)\nleft_eye_draw = DrawingSpec(color=(180, 200, 10), thickness=f_thick, circle_radius=f_rad)\nleft_eyebrow_draw = DrawingSpec(color=(180, 220, 10), thickness=f_thick, circle_radius=f_rad)\nmouth_draw = DrawingSpec(color=(10, 180, 10), thickness=f_thick, circle_radius=f_rad)\nhead_draw = DrawingSpec(color=(10, 200, 10), thickness=f_thick, circle_radius=f_rad)\n\niris_landmark_spec = {468: right_iris_draw, 473: left_iris_draw}\n```\n\nWe have implemented a method named `draw_pupils` which modifies some functionality from MediaPipe.  It exists as a stopgap until some pending changes are merged.\n\n\n# Usage:\n\nThe containing ZIP file should be decompressed into the root of the ControlNet directory.  The `train_laion_face.py`, `laion_face_dataset.py`, and other `.py` files should sit adjacent to `tutorial_train.py` and `tutorial_train_sd21.py`.  We are assuming a checkout of the ControlNet repo at 0acb7e5, but there is no direct dependency on the repository.\n\n## Downloading:\n\nFor copyright reasons, we cannot include the original target files.  We have provided a script (tool_download_face_targets.py) which will read from training/laion-face-processed/metadata.json and populate the target folder.  This file has no requirements, but will use tqdm if it is installed.\n\n## Training:\n\nWhen the targets folder is fully populated, training can be run on a machine with at least 24 gigabytes of VRAM.  Our model was trained for 200 hours (four epochs) on an A6000.\n\n```bash\npython tool_add_control.py ./models/v1-5-pruned-emaonly.ckpt ./models/controlnet_sd15_laion_face.ckpt\npython ./train_laion_face_sd15.py\n```\n\n## Inference:\n\nWe have provided `gradio_face2image.py`.  Update the following two lines to point them to your trained model.\n\n```\nmodel = create_model(''./models/cldm_v21.yaml'').cpu()  # If you fine-tune on SD2.1 base, this does not need to change.\nmodel.load_state_dict(load_state_dict(''./models/control_sd21_openpose.pth'', location=''cuda''))\n```\n\nThe model has some limitations: while it is empirically better at tracking gaze and mouth poses than previous attempts, it may still ignore controls.  Adding details to the prompt like, "looking right" can abate bad behavior. \n\n## ðŸ§¨ Diffusers\n\nIt is recommended to use the checkpoint with [Stable Diffusion 2.1 - Base](stabilityai/stable-diffusion-2-1-base) as the checkpoint has been trained on it.\nExperimentally, the checkpoint can be used with other diffusion models such as dreamboothed stable diffusion.\n\nTo use with Stable Diffusion 1.5, insert `subfolder="diffusion_sd15"` into the from_pretrained arguments.  A v1.5 half-precision variant is provided but untested.\n\n1. Install `diffusers` and related packages:\n```\n$ pip install diffusers transformers accelerate\n```\n\n2. Run code:\n```py\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nfrom diffusers.utils import load_image\n\nimage = load_image(\n    "https://huggingface.co/CrucibleAI/ControlNetMediaPipeFace/resolve/main/samples_laion_face_dataset/family_annotation.png"\n)\n\n# Stable Diffusion 2.1-base:\ncontrolnet = ControlNetModel.from_pretrained("CrucibleAI/ControlNetMediaPipeFace", torch_dtype=torch.float16, variant="fp16")\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n	"stabilityai/stable-diffusion-2-1-base", controlnet=controlnet, safety_checker=None, torch_dtype=torch.float16\n)\n# OR\n# Stable Diffusion 1.5:\ncontrolnet = ControlNetModel.from_pretrained("CrucibleAI/ControlNetMediaPipeFace", subfolder="diffusion_sd15")\npipe = StableDiffusionControlNetPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", controlnet=controlnet, safety_checker=None)\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n\n# Remove if you do not have xformers installed\n# see https://huggingface.co/docs/diffusers/v0.13.0/en/optimization/xformers#installing-xformers\n# for installation instructions\npipe.enable_xformers_memory_efficient_attention()\npipe.enable_model_cpu_offload()\n\nimage = pipe("a happy family at a dentist advertisement", image=image, num_inference_steps=30).images[0]\nimage.save(''./images.png'')\n```\n\n\n# License:\n\n### Source Images: (/training/laion-face-processed/source/)\nThis work is marked with CC0 1.0. To view a copy of this license, visit http://creativecommons.org/publicdomain/zero/1.0\n\n### Trained Models:\nOur trained ControlNet checkpoints are released under CreativeML Open RAIL-M.\n\n### Source Code:\nlllyasviel/ControlNet is licensed under the Apache License 2.0\n\nOur modifications are released under the same license.\n\n\n# Credits and Thanks:\n\nGreatest thanks to Zhang et al. for ControlNet, Rombach et al. (StabilityAI) for Stable Diffusion, and Schuhmann et al. for LAION.\n\nSample images for this document were obtained from Unsplash and are CC0.\n\n```\n@misc{zhang2023adding,\n  title={Adding Conditional Control to Text-to-Image Diffusion Models}, \n  author={Lvmin Zhang and Maneesh Agrawala},\n  year={2023},\n  eprint={2302.05543},\n  archivePrefix={arXiv},\n  primaryClass={cs.CV}\n}\n\n@misc{rombach2021highresolution,\n      title={High-Resolution Image Synthesis with Latent Diffusion Models}, \n      author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and BjÃ¶rn Ommer},\n      year={2021},\n      eprint={2112.10752},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n\n@misc{schuhmann2022laion5b,\n      title={LAION-5B: An open large-scale dataset for training next generation image-text models}, \n      author={Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev},\n      year={2022},\n      eprint={2210.08402},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\nThis project was made possible by Crucible AI.', '{"pipeline_tag":"image-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":34147689491,"files_count":47,"spaces_count":20,"gated":false,"private":false,"config":{}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2302.05543","source_url":"https://arxiv.org/abs/2302.05543"},{"type":"based_on_paper","target_id":"arxiv:2112.10752","source_url":"https://arxiv.org/abs/2112.10752"},{"type":"based_on_paper","target_id":"arxiv:2210.08402","source_url":"https://arxiv.org/abs/2210.08402"}]', NULL, 'OpenRAIL', 'approved', 77.6, '414bff0a823c910296b264b53da5cd50', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-TripoSR', 'huggingface--stabilityai--triposr', 'TripoSR', 'stabilityai', '--- datasets: - allenai/objaverse tags: - 3d extra_gated_fields: Name: text Email: text Country: text Organization or Affiliation: text I ALLOW Stability AI to email me about new model releases: checkbox license: mit pipeline_tag: image-to-3d --- > Try our new model: **SF3D** with several improvements such as faster generation and more game-ready assets. > > The model is available here and we also have a demo. TripoSR is a fast and feed-forward 3D generative model developed in collaboration b...', '["3d","image-to-3d","dataset:allenai/objaverse","arxiv:2311.04400","arxiv:2403.02151","license:mit","region:us"]', 'image-to-3d', 572, 28366, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/TripoSR","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ndatasets:\n- allenai/objaverse\ntags:\n- 3d\nextra_gated_fields:\n  Name: text\n  Email: text\n  Country: text\n  Organization or Affiliation: text\n  I ALLOW Stability AI to email me about new model releases: checkbox\nlicense: mit\npipeline_tag: image-to-3d\n---\n\n> Try our new model: **SF3D** with several improvements such as faster generation and more game-ready assets.\n> \n> The model is available [here](https://huggingface.co/stabilityai/stable-fast-3d) and we also have a [demo](https://huggingface.co/spaces/stabilityai/stable-fast-3d).\n \n# TripoSR\n![](figures/input800.mp4)\nTripoSR is a fast and feed-forward 3D generative model developed in collaboration between Stability AI and Tripo AI.\n\n\n## Model Details\n\n### Model Description\n\nWe closely follow [LRM](https://arxiv.org/abs/2311.04400) network architecture for the model design, where TripoSR incorporates a series of technical advancements over the LRM model in terms of both data curation as well as model and training improvements. For more technical details and evaluations, please refer to [our tech report](https://arxiv.org/abs/2403.02151).\n\n* **Developed by**: [Stability AI](https://stability.ai/), [Tripo AI](https://tripo3d.ai/)\n* **Model type**: Feed-forward 3D reconstruction from a single image\n* **License**: MIT\n* **Hardware**: We train `TripoSR` for 5 days on 22 GPU nodes each with 8 A100 40GB GPUs\n\n### Model Sources\n\n* **Repository**: https://github.com/VAST-AI-Research/TripoSR\n* **Tech report**: https://arxiv.org/abs/2403.02151\n* **Demo**: https://huggingface.co/spaces/stabilityai/TripoSR\n\n### Training Dataset\n\nWe use renders from the [Objaverse](https://objaverse.allenai.org/objaverse-1.0) dataset, utilizing our enhanced rendering method that more closely replicate the distribution of images found in the real world, significantly improving our modelâ€™s ability to generalize. We selected a carefully curated subset of the Objaverse dataset for the training data, which is available under the CC-BY license. \n\n\n## Usage\n\n* For usage instructions, please refer to our [TripoSR GitHub repository](https://github.com/VAST-AI-Research/TripoSR)\n\n* You can also try it in [our gradio demo](https://huggingface.co/spaces/stabilityai/TripoSR)\n\n\n### Misuse, Malicious Use, and Out-of-Scope Use\n\nThe model should not be used to intentionally create or disseminate 3D models that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.', '{"pipeline_tag":"image-to-3d","library_name":null,"framework":null,"params":null,"storage_bytes":5262702691,"files_count":6,"spaces_count":60,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:VAST-AI-Research:TripoSR","source_url":"https://github.com/VAST-AI-Research/TripoSR"},{"type":"has_code","target_id":"github:VAST-AI-Research:TripoSR","source_url":"https://github.com/VAST-AI-Research/TripoSR"},{"type":"based_on_paper","target_id":"arxiv:2311.04400","source_url":"https://arxiv.org/abs/2311.04400"},{"type":"based_on_paper","target_id":"arxiv:2403.02151","source_url":"https://arxiv.org/abs/2403.02151"}]', NULL, 'MIT', 'approved', 62.6, 'c47993efce76945342e2749d12c3e5c7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-NousResearch-Yarn-Mistral-7b-128k', 'huggingface--nousresearch--yarn-mistral-7b-128k', 'Yarn-Mistral-7b-128k', 'NousResearch', '--- datasets: - emozilla/yarn-train-tokenized-16k-mistral metrics: - perplexity library_name: transformers license: apache-2.0 language: - en --- Preprint (arXiv) GitHub !yarn Nous-Yarn-Mistral-7b-128k is a state-of-the-art language model for long context, further pretrained on long context data for 1500 steps using the YaRN extension method. It is an extension of Mistral-7B-v0.1 and supports a 128k token context window. To use, pass when loading the model, for example In addition you will ne...', '["transformers","pytorch","mistral","text-generation","custom_code","en","dataset:emozilla/yarn-train-tokenized-16k-mistral","arxiv:2309.00071","license:apache-2.0","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 571, 2201, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ndatasets:\n- emozilla/yarn-train-tokenized-16k-mistral\nmetrics:\n- perplexity\nlibrary_name: transformers\nlicense: apache-2.0\nlanguage:\n- en\n---\n\n# Model Card: Nous-Yarn-Mistral-7b-128k\n\n[Preprint (arXiv)](https://arxiv.org/abs/2309.00071)  \n[GitHub](https://github.com/jquesnelle/yarn)\n![yarn](https://raw.githubusercontent.com/jquesnelle/yarn/mistral/data/proofpile-long-small-mistral.csv.png)\n\n## Model Description\n\nNous-Yarn-Mistral-7b-128k is a state-of-the-art language model for long context, further pretrained on long context data for 1500 steps using the YaRN extension method.\nIt is an extension of [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) and supports a 128k token context window.\n\nTo use, pass `trust_remote_code=True` when loading the model, for example\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained("NousResearch/Yarn-Mistral-7b-128k",\n  use_flash_attention_2=True,\n  torch_dtype=torch.bfloat16,\n  device_map="auto",\n  trust_remote_code=True)\n```\n\nIn addition you will need to use the latest version of `transformers` (until 4.35 comes out)\n```sh\npip install git+https://github.com/huggingface/transformers\n```\n\n## Benchmarks\n\nLong context benchmarks:\n| Model | Context Window | 8k PPL | 16k PPL | 32k PPL | 64k PPL | 128k PPL |\n|-------|---------------:|------:|----------:|-----:|-----:|------------:|\n| [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) | 8k | 2.96 | - | - | - | - |\n| [Yarn-Mistral-7b-64k](https://huggingface.co/NousResearch/Yarn-Mistral-7b-64k) | 64k | 3.04 | 2.65 | 2.44 | 2.20 | - |\n| [Yarn-Mistral-7b-128k](https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k) | 128k | 3.08 | 2.68 | 2.47 | 2.24 | 2.19 |\n\nShort context benchmarks showing that quality degradation is minimal:\n| Model | Context Window | ARC-c | Hellaswag | MMLU | Truthful QA |\n|-------|---------------:|------:|----------:|-----:|------------:|\n| [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) | 8k | 59.98 | 83.31 | 64.16 | 42.15 |\n| [Yarn-Mistral-7b-64k](https://huggingface.co/NousResearch/Yarn-Mistral-7b-64k) | 64k | 59.38 | 81.21 | 61.32 | 42.50 |\n| [Yarn-Mistral-7b-128k](https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k) | 128k | 58.87 | 80.58 | 60.64 | 42.46 |\n\n## Collaborators\n\n - [bloc97](https://github.com/bloc97): Methods, paper and evals\n - [@theemozilla](https://twitter.com/theemozilla): Methods, paper, model training, and evals\n - [@EnricoShippole](https://twitter.com/EnricoShippole): Model training\n - [honglu2875](https://github.com/honglu2875): Paper and evals\n\nThe authors would like to thank LAION AI for their support of compute for this model.\nIt was trained on the [JUWELS](https://www.fz-juelich.de/en/ias/jsc/systems/supercomputers/juwels) supercomputer.', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":28967555174,"files_count":14,"spaces_count":34,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"auto_map":{"AutoConfig":"configuration_mistral.MistralConfig","AutoModelForCausalLM":"modeling_mistral_yarn.MistralForCausalLM"},"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:jquesnelle:yarn","source_url":"https://github.com/jquesnelle/yarn"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"}]', NULL, 'Apache-2.0', 'approved', 62.6, '74a499e02b21468d0871814eebe88ff5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-fofr-sdxl-emoji', 'huggingface--fofr--sdxl-emoji', 'sdxl-emoji', 'fofr', '--- license: creativeml-openrail-m tags: - text-to-image - stable-diffusion - lora - diffusers base_model: stabilityai/stable-diffusion-xl-base-1.0 pivotal_tuning: true textual_embeddings: embeddings.pti instance_prompt: an <s0><s1> emoji inference: false --- !lora_image > Grab your replicate token here You may also do inference via the API with Node.js or curl, and locally with COG and Docker, check out the Replicate API page for this model Replicate SDXL LoRAs are trained with Pivotal Tunin...', '["diffusers","text-to-image","stable-diffusion","lora","license:creativeml-openrail-m","region:us"]', 'text-to-image', 571, 122, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/fofr/sdxl-emoji","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n  - text-to-image\n  - stable-diffusion\n  - lora\n  - diffusers\nbase_model: stabilityai/stable-diffusion-xl-base-1.0\npivotal_tuning: true\ntextual_embeddings: embeddings.pti\ninstance_prompt: an <s0><s1> emoji\ninference: false\n---\n# sdxl-emoji LoRA by [fofr](https://replicate.com/fofr)\n### An SDXL fine-tune based on Apple Emojis\n\n![lora_image](https://replicate.delivery/pbxt/a3z81v5vwlKfLq1H5uBqpVmkHalOVup0jSLma9E2UaF3tawIA/out-0.png)\n>\n\n## Inference with Replicate API\nGrab your replicate token [here](https://replicate.com/account)\n```bash\npip install replicate\nexport REPLICATE_API_TOKEN=r8_*************************************\n```\n\n```py\nimport replicate\n\noutput = replicate.run(\n    "sdxl-emoji@sha256:dee76b5afde21b0f01ed7925f0665b7e879c50ee718c5f78a9d38e04d523cc5e",\n    input={"prompt": "A TOK emoji of a man"}\n)\nprint(output)\n```\nYou may also do inference via the API with Node.js or curl, and locally with COG and Docker, [check out the Replicate API page for this model](https://replicate.com/fofr/sdxl-emoji/api)\n\n## Inference with ðŸ§¨ diffusers\nReplicate SDXL LoRAs are trained with Pivotal Tuning, which combines training a concept via Dreambooth LoRA with training a new token with Textual Inversion.\nAs `diffusers` doesn''t yet support textual inversion for SDXL, we will use cog-sdxl `TokenEmbeddingsHandler` class.\n\nThe trigger tokens for your prompt will be `<s0><s1>`\n\n```shell\npip install diffusers transformers accelerate safetensors huggingface_hub\ngit clone https://github.com/replicate/cog-sdxl cog_sdxl\n```\n\n```py\nimport torch\nfrom huggingface_hub import hf_hub_download\nfrom diffusers import DiffusionPipeline\nfrom cog_sdxl.dataset_and_utils import TokenEmbeddingsHandler\nfrom diffusers.models import AutoencoderKL\n\npipe = DiffusionPipeline.from_pretrained(\n        "stabilityai/stable-diffusion-xl-base-1.0",\n        torch_dtype=torch.float16,\n        variant="fp16",\n).to("cuda")\n\npipe.load_lora_weights("fofr/sdxl-emoji", weight_name="lora.safetensors")\n\ntext_encoders = [pipe.text_encoder, pipe.text_encoder_2]\ntokenizers = [pipe.tokenizer, pipe.tokenizer_2]\n\nembedding_path = hf_hub_download(repo_id="fofr/sdxl-emoji", filename="embeddings.pti", repo_type="model")\nembhandler = TokenEmbeddingsHandler(text_encoders, tokenizers)\nembhandler.load_embeddings(embedding_path)\nprompt="A <s0><s1> emoji of a man"\nimages = pipe(\n    prompt,\n    cross_attention_kwargs={"scale": 0.8},\n).images\n#your output image\nimages[0]\n```\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":185968776,"files_count":5,"spaces_count":20,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:replicate:cog-sdxl","source_url":"https://github.com/replicate/cog-sdxl"}]', NULL, 'creativeml-openrail-m', 'approved', 62.6, '72a1c3c596db84c658f91b72c117ee39', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-3.2-11B-Vision', 'huggingface--meta-llama--llama-3.2-11b-vision', 'Llama-3.2-11B-Vision', 'meta-llama', '', '["transformers","safetensors","mllama","image-to-text","facebook","meta","pytorch","llama","llama-3","image-text-to-text","en","de","fr","it","pt","hi","es","th","arxiv:2204.05149","license:llama3.2","text-generation-inference","endpoints_compatible","region:us"]', 'image-text-to-text', 570, 11604, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-3.2-11B-Vision","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":10642941475,"storage_bytes":52480505184,"files_count":20,"spaces_count":70,"gated":"manual","private":false,"config":{"architectures":["MllamaForConditionalGeneration"],"model_type":"mllama","tokenizer_config":{"bos_token":"<|begin_of_text|>","eos_token":"<|end_of_text|>","pad_token":"<|finetune_right_pad_id|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2204.05149","source_url":"https://arxiv.org/abs/2204.05149"}]', NULL, 'llama3.2', 'approved', 37.6, 'd97adc0f3aeb21aa05d38d7b4820e00c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-3n-E2B-it-litert-preview', 'huggingface--google--gemma-3n-e2b-it-litert-preview', 'gemma-3n-E2B-it-litert-preview', 'google', '', '["image-text-to-text","arxiv:1905.07830","arxiv:1905.10044","arxiv:1911.11641","arxiv:1904.09728","arxiv:1705.03551","arxiv:1911.01547","arxiv:1907.10641","arxiv:1903.00161","arxiv:2210.03057","arxiv:2502.12404","arxiv:2411.19799","arxiv:2009.03300","arxiv:2502.21228","arxiv:2311.12022","arxiv:2403.07974","arxiv:2108.07732","arxiv:2107.03374","license:gemma","region:us"]', 'image-text-to-text', 570, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-3n-E2B-it-litert-preview","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-text-to-text","library_name":null,"framework":null,"params":null,"storage_bytes":3136926777,"files_count":3,"spaces_count":0,"gated":"manual","private":false,"config":null}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1903.00161","source_url":"https://arxiv.org/abs/1903.00161"},{"type":"based_on_paper","target_id":"arxiv:2210.03057","source_url":"https://arxiv.org/abs/2210.03057"},{"type":"based_on_paper","target_id":"arxiv:2502.12404","source_url":"https://arxiv.org/abs/2502.12404"},{"type":"based_on_paper","target_id":"arxiv:2411.19799","source_url":"https://arxiv.org/abs/2411.19799"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:2502.21228","source_url":"https://arxiv.org/abs/2502.21228"},{"type":"based_on_paper","target_id":"arxiv:2311.12022","source_url":"https://arxiv.org/abs/2311.12022"},{"type":"based_on_paper","target_id":"arxiv:2403.07974","source_url":"https://arxiv.org/abs/2403.07974"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"}]', NULL, 'Gemma', 'approved', 37.6, '581a1d8c55441f56830427924f113f38', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen2.5-VL-3B-Instruct', 'huggingface--qwen--qwen2.5-vl-3b-instruct', 'Qwen2.5-VL-3B-Instruct', 'Qwen', '--- license_name: qwen-research license_link: https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/blob/main/LICENSE language: - en pipeline_tag: image-text-to-text tags: - multimodal library_name: transformers --- <a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> In the past five months since Qwen2-VLâ€™s release, num...', '["transformers","safetensors","qwen2_5_vl","image-to-text","multimodal","image-text-to-text","conversational","en","arxiv:2309.00071","arxiv:2409.12191","arxiv:2308.12966","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'image-text-to-text', 569, 7688461, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '\n---\nlicense_name: qwen-research\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: image-text-to-text\ntags:\n- multimodal\nlibrary_name: transformers\n---\n\n# Qwen2.5-VL-3B-Instruct\n<a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Introduction\n\nIn the past five months since Qwen2-VLâ€™s release, numerous developers have built new models on the Qwen2-VL vision-language models, providing us with valuable feedback. During this period, we focused on building more useful vision-language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5-VL.\n\n#### Key Enhancements:\n* **Understand things visually**: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.\n\n* **Being agentic**: Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use.\n\n* **Understanding long videos and capturing events**: Qwen2.5-VL can comprehend videos of over 1 hour, and this time it has a new ability of cpaturing event by pinpointing the relevant video segments.\n\n* **Capable of visual localization in different formats**: Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\n\n* **Generating structured outputs**: for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc.\n\n\n#### Model Architecture Updates:\n\n* **Dynamic Resolution and Frame Rate Training for Video Understanding**:\n\nWe extend dynamic resolution to the temporal dimension by adopting dynamic FPS sampling, enabling the model to comprehend videos at various sampling rates. Accordingly, we update mRoPE in the time dimension with IDs and absolute time alignment, enabling the model to learn temporal sequence and speed, and ultimately acquire the ability to pinpoint specific moments.\n\n<p align="center">\n    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL/qwen2.5vl_arc.jpeg" width="80%"/>\n<p>\n\n\n* **Streamlined and Efficient Vision Encoder**\n\nWe enhance both training and inference speeds by strategically implementing window attention into the ViT. The ViT architecture is further optimized with SwiGLU and RMSNorm, aligning it with the structure of the Qwen2.5 LLM.\n\n\nWe have three models with 3, 7 and 72 billion parameters. This repo contains the instruction-tuned 3B Qwen2.5-VL model. For more information, visit our [Blog](https://qwenlm.github.io/blog/qwen2.5-vl/) and [GitHub](https://github.com/QwenLM/Qwen2.5-VL).\n\n\n\n## Evaluation\n\n### Image benchmark\n\n| Benchmark | InternVL2.5-4B |Qwen2-VL-7B |Qwen2.5-VL-3B |\n| :--- | :---:  | :---: | :---: |\n| MMMU<sub>val</sub>  | 52.3 | 54.1 | 53.1| \n| MMMU-Pro<sub>val</sub>  | **32.7** | 30.5 | 31.6|\n| AI2D<sub>test</sub> | 81.4 | **83.0** | 81.5 |\n| DocVQA<sub>test</sub>  | 91.6 | 94.5 | **93.9** | \n| InfoVQA<sub>test</sub>  | 72.1 | 76.5 | **77.1** |\n| TextVQA<sub>val</sub>  | 76.8 | **84.3** | 79.3|\n| MMBench-V1.1<sub>test</sub>  | 79.3 | **80.7** | 77.6 | \n| MMStar | 58.3 | **60.7** | 55.9 | \n| MathVista<sub>testmini</sub>  | 60.5 | 58.2 | **62.3** |\n| MathVision<sub>full</sub>  | 20.9 | 16.3  | **21.2** |\n\n\n### Video benchmark\n| Benchmark | InternVL2.5-4B | Qwen2-VL-7B | Qwen2.5-VL-3B |\n| :--- | :---:  | :---: | :---: |\n| MVBench | 71.6 | 67.0 | 67.0 |\n| VideoMME | 63.6/62.3 | 69.0/63.3 | 67.6/61.5 |\n| MLVU | 48.3 | - | 68.2 |\n| LVBench | - | - | 43.3 |\n| MMBench-Video | 1.73 | 1.44 | 1.63 |\n| EgoSchema | - | - | 64.8 |\n| PerceptionTest | - | - | 66.9 |\n| TempCompass | - | - | 64.4 |\n| LongVideoBench | 55.2 | 55.6 | 54.2 |\n| CharadesSTA/mIoU | - | - | 38.8 |\n\n\n### Agent benchmark\n| Benchmarks              | Qwen2.5-VL-3B |\n|-------------------------|---------------|\n| ScreenSpot              |     55.5    |\n| ScreenSpot Pro          |     23.9    |\n| AITZ_EM                 |  	76.9    |\n| Android Control High_EM |    	63.7    |\n| Android Control Low_EM  |  	22.2    |\n| AndroidWorld_SR         | 	90.8  	|\n| MobileMiniWob++_SR      | 	67.9    |\n\n## Requirements\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers accelerate\n```\nor you might encounter the following error:\n```\nKeyError: ''qwen2_5_vl''\n```\n\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen2.5-VL with ðŸ¤– ModelScope and ðŸ¤— Transformers.\n\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers accelerate\n```\nor you might encounter the following error:\n```\nKeyError: ''qwen2_5_vl''\n```\n\n\nWe offer a toolkit to help you handle various types of visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved images and videos. You can install it using the following command:\n\n```bash\n# It''s highly recommanded to use `[decord]` feature for faster video loading.\npip install qwen-vl-utils[decord]==0.0.8\n```\n\nIf you are not using Linux, you might not be able to install `decord` from PyPI. In that case, you can use `pip install qwen-vl-utils` which will fall back to using torchvision for video processing. However, you can still [install decord from source](https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source) to get decord used when loading video.\n\n### Using ðŸ¤—  Transformers to Chat\n\nHere we show a code snippet to show you how to use the chat model with `transformers` and `qwen_vl_utils`:\n\n```python\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    "Qwen/Qwen2.5-VL-3B-Instruct", torch_dtype="auto", device_map="auto"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n#     "Qwen/Qwen2.5-VL-3B-Instruct",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation="flash_attention_2",\n#     device_map="auto",\n# )\n\n# default processer\nprocessor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-3B-Instruct")\n\n# The default range for the number of visual tokens per image in the model is 4-16384.\n# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-3B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image",\n                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",\n            },\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors="pt",\n)\ninputs = inputs.to("cuda")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n<details>\n<summary>Multi image inference</summary>\n\n```python\n# Messages containing multiple images and a text query\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "file:///path/to/image1.jpg"},\n            {"type": "image", "image": "file:///path/to/image2.jpg"},\n            {"type": "text", "text": "Identify the similarities between these images."},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors="pt",\n)\ninputs = inputs.to("cuda")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n</details>\n\n<details>\n<summary>Video inference</summary>\n\n```python\n# Messages containing a images list as a video and a text query\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "video",\n                "video": [\n                    "file:///path/to/frame1.jpg",\n                    "file:///path/to/frame2.jpg",\n                    "file:///path/to/frame3.jpg",\n                    "file:///path/to/frame4.jpg",\n                ],\n            },\n            {"type": "text", "text": "Describe this video."},\n        ],\n    }\n]\n\n# Messages containing a local video path and a text query\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "video",\n                "video": "file:///path/to/video1.mp4",\n                "max_pixels": 360 * 420,\n                "fps": 1.0,\n            },\n            {"type": "text", "text": "Describe this video."},\n        ],\n    }\n]\n\n# Messages containing a video url and a text query\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "video",\n                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",\n            },\n            {"type": "text", "text": "Describe this video."},\n        ],\n    }\n]\n\n#In Qwen 2.5 VL, frame rate information is also input into the model to align with absolute time.\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    fps=fps,\n    padding=True,\n    return_tensors="pt",\n    **video_kwargs,\n)\ninputs = inputs.to("cuda")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\nVideo URL compatibility largely depends on the third-party library version. The details are in the table below. change the backend by `FORCE_QWENVL_VIDEO_READER=torchvision` or `FORCE_QWENVL_VIDEO_READER=decord` if you prefer not to use the default one.\n\n| Backend     | HTTP | HTTPS |\n|-------------|------|-------|\n| torchvision >= 0.19.0 | âœ…  | âœ…   |\n| torchvision < 0.19.0  | âŒ  | âŒ   |\n| decord      | âœ…  | âŒ   |\n</details>\n\n<details>\n<summary>Batch inference</summary>\n\n```python\n# Sample messages for batch inference\nmessages1 = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "file:///path/to/image1.jpg"},\n            {"type": "image", "image": "file:///path/to/image2.jpg"},\n            {"type": "text", "text": "What are the common elements in these pictures?"},\n        ],\n    }\n]\nmessages2 = [\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": "Who are you?"},\n]\n# Combine messages for batch processing\nmessages = [messages1, messages2]\n\n# Preparation for batch inference\ntexts = [\n    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n    for msg in messages\n]\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=texts,\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors="pt",\n)\ninputs = inputs.to("cuda")\n\n# Batch Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_texts = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_texts)\n```\n</details>\n\n### ðŸ¤– ModelScope\nWe strongly advise users especially those in mainland China to use ModelScope. `snapshot_download` can help you solve issues concerning downloading checkpoints.\n\n\n### More Usage Tips\n\nFor input images, we support local files, base64, and URLs. For videos, we currently only support local files.\n\n```python\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\n## Local file path\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "file:///path/to/your/image.jpg"},\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n## Image URL\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "http://path/to/your/image.jpg"},\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n## Base64 encoded image\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "data:image;base64,/9j/..."},\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n```\n#### Image Resolution for performance boost\n\nThe model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs, such as a token count range of 256-1280, to balance speed and memory usage.\n\n```python\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n    "Qwen/Qwen2.5-VL-3B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels\n)\n```\n\nBesides, We provide two methods for fine-grained control over the image size input to the model:\n\n1. Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels.\n   \n2. Specify exact dimensions: Directly set `resized_height` and `resized_width`. These values will be rounded to the nearest multiple of 28.\n\n```python\n# min_pixels and max_pixels\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image",\n                "image": "file:///path/to/your/image.jpg",\n                "resized_height": 280,\n                "resized_width": 420,\n            },\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n# resized_height and resized_width\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image",\n                "image": "file:///path/to/your/image.jpg",\n                "min_pixels": 50176,\n                "max_pixels": 50176,\n            },\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n\n```\n{\n	...,\n    "type": "yarn",\n    "mrope_section": [\n        16,\n        24,\n        24\n    ],\n    "factor": 4,\n    "original_max_position_embeddings": 32768\n}\n```\n\nHowever, it should be noted that this method has a significant impact on the performance of temporal and spatial localization tasks, and is therefore not recommended for use.\n\nAt the same time, for long video inputs, since MRoPE itself is more economical with ids, the max_position_embeddings can be directly modified to a larger value, such as 64k.\n\n\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5-VL,\n    title = {Qwen2.5-VL},\n    url = {https://qwenlm.github.io/blog/qwen2.5-vl/},\n    author = {Qwen Team},\n    month = {January},\n    year = {2025}\n}\n\n@article{Qwen2VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model''s Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":3754622976,"storage_bytes":7509337976,"files_count":14,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2_5_VLForConditionalGeneration"],"model_type":"qwen2_5_vl","processor_config":{"chat_template":"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message[''role''] != ''system'' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message[''role''] }}\n{% if message[''content''] is string %}{{ message[''content''] }}<|im_end|>\n{% else %}{% for content in message[''content''] %}{% if content[''type''] == ''image'' or ''image'' in content or ''image_url'' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content[''type''] == ''video'' or ''video'' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif ''text'' in content %}{{ content[''text''] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"},"tokenizer_config":{"bos_token":null,"chat_template":"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message[''role''] != ''system'' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message[''role''] }}\n{% if message[''content''] is string %}{{ message[''content''] }}<|im_end|>\n{% else %}{% for content in message[''content''] %}{% if content[''type''] == ''image'' or ''image'' in content or ''image_url'' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content[''type''] == ''video'' or ''video'' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif ''text'' in content %}{{ content[''text''] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen2.5-VL","source_url":"https://github.com/QwenLM/Qwen2.5-VL"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:dmlc:decord","source_url":"https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2409.12191","source_url":"https://arxiv.org/abs/2409.12191"},{"type":"based_on_paper","target_id":"arxiv:2308.12966","source_url":"https://arxiv.org/abs/2308.12966"}]', NULL, NULL, 'pending', 67.6, '6e2dd9e5e50a2e2c613a779533661cd7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen2.5-VL-72B-Instruct', 'huggingface--qwen--qwen2.5-vl-72b-instruct', 'Qwen2.5-VL-72B-Instruct', 'Qwen', '--- license: other license_name: qwen license_link: https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct/blob/main/LICENSE language: - en pipeline_tag: image-text-to-text tags: - multimodal library_name: transformers --- <a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> In the past five months since Qwen2-VLâ€™s relea...', '["transformers","safetensors","qwen2_5_vl","image-to-text","multimodal","image-text-to-text","conversational","en","arxiv:2309.00071","arxiv:2409.12191","arxiv:2308.12966","license:other","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'image-text-to-text', 569, 135334, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '\n---\nlicense: other\nlicense_name: qwen\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: image-text-to-text\ntags:\n- multimodal\nlibrary_name: transformers\n---\n\n# Qwen2.5-VL-72B-Instruct\n<a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Introduction\n\nIn the past five months since Qwen2-VLâ€™s release, numerous developers have built new models on the Qwen2-VL vision-language models, providing us with valuable feedback. During this period, we focused on building more useful vision-language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5-VL.\n\n#### Key Enhancements:\n* **Understand things visually**: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.\n\n* **Being agentic**: Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use.\n\n* **Understanding long videos and capturing events**: Qwen2.5-VL can comprehend videos of over 1 hour, and this time it has a new ability of cpaturing event by pinpointing the relevant video segments.\n\n* **Capable of visual localization in different formats**: Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\n\n* **Generating structured outputs**: for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc.\n\n\n#### Model Architecture Updates:\n\n* **Dynamic Resolution and Frame Rate Training for Video Understanding**:\n\nWe extend dynamic resolution to the temporal dimension by adopting dynamic FPS sampling, enabling the model to comprehend videos at various sampling rates. Accordingly, we update mRoPE in the time dimension with IDs and absolute time alignment, enabling the model to learn temporal sequence and speed, and ultimately acquire the ability to pinpoint specific moments.\n\n<p align="center">\n    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL/qwen2.5vl_arc.jpeg" width="80%"/>\n<p>\n\n* **Streamlined and Efficient Vision Encoder**\n\nWe enhance both training and inference speeds by strategically implementing window attention into the ViT. The ViT architecture is further optimized with SwiGLU and RMSNorm, aligning it with the structure of the Qwen2.5 LLM.\n\n\nWe have three models with 3, 7 and 72 billion parameters. This repo contains the instruction-tuned 72B Qwen2.5-VL model. For more information, visit our [Blog](https://qwenlm.github.io/blog/qwen2.5-vl/) and [GitHub](https://github.com/QwenLM/Qwen2.5-VL).\n\n\n\n## Evaluation\n\n### Image benchmark\n\n|      Benchmarks        | GPT4o     | Claude3.5 Sonnet  | Gemini-2-flash  | InternVL2.5-78B | Qwen2-VL-72B | Qwen2.5-VL-72B |\n|-----------------------|-----------|-------------------|-----------------|-----------------|--------------|----------------|\n| MMMU<sub>val</sub>    | 70.3      | 70.4              | 70.7            | 70.1                 | 64.5         | 70.2          |\n| MMMU_Pro              | 54.5      | 54.7              | 57.0            | 48.6              | 46.2         | 51.1           |\n| MathVista_MINI        | 63.8      | 65.4              | 73.1            | 76.6                | 70.5         | 74.8           |\n| MathVision_FULL       | 30.4      | 38.3              | 41.3            | 32.2               | 25.9         | 38.1           |\n| Hallusion Bench       | 55.0      | 55.16             |                | 57.4             | 58.1         | 55.16           |\n| MMBench_DEV_EN_V11    | 82.1      | 83.4              | 83.0            | 88.5             | 86.6         | 88           |\n| AI2D_TEST             | 84.6      | 81.2              |                 | 89.1           | 88.1         | 88.4           |\n| ChartQA_TEST          | 86.7      | 90.8              | 85.2            | 88.3               | 88.3         | 89.5           |\n| DocVQA_VAL            | 91.1      | 95.2              | 92.1            | 96.5             | 96.1         |      96.4      |\n| MMStar                | 64.7      | 65.1              | 69.4            | 69.5             | 68.3         |       70.8         |\n| MMVet_turbo           | 69.1      |  70.1              |                 | 72.3           | 74.0         |       76.19         |\n| OCRBench              | 736       | 788               |                 | 854               | 877          |         885       |\n| OCRBench-V2(en/zh)    |  46.5/32.3 |  45.2/39.6         | 51.9/43.1       | 45/46.2     | 47.8/46.1    | 61.5/63.7    |\n| CC-OCR                | 66.6     | 62.7              | 73.0            | 64.7          | 68.7       |79.8           |\n\n\n### Video benchmark\n| Benchmarks          | GPT4o | Gemini-1.5-Pro | InternVL2.5-78B | Qwen2VL-72B | Qwen2.5VL-72B |\n|---------------------|-------|----------------|-----------------|-------------|---------------|\n| VideoMME w/o sub.   | 71.9  | 75.0           | 72.1            | 71.2        | 73.3          |\n| VideoMME w sub.     | 77.2  | 81.3           | 74.0            | 77.8        | 79.1          |\n| MVBench             | 64.6  | 60.5           | 76.4            | 73.6        | 70.4          |\n| MMBench-Video       | 1.63  | 1.30           | 1.97            | 1.70        | 2.02          |\n| LVBench             | 30.8  | 33.1           | -               | 41.3        | 47.3          |\n| EgoSchema           | 72.2  | 71.2           | -               | 77.9        | 76.2          |\n| PerceptionTest_test | -     | -              | -               | 68.0        | 73.2          |\n| MLVU_M-Avg_dev      | 64.6  | -              | 75.7            |             | 74.6          |\n| TempCompass_overall | 73.8  | -              | -               |             | 74.8          |\n\n\n### Agent benchmark\n\n| Benchmarks              | GPT4o       | Gemini 2.0 | Claude | Aguvis-72B | Qwen2VL-72B | Qwen2.5VL-72B |\n|-------------------------|-------------|------------|--------|------------|-------------|---------------|\n| ScreenSpot              | 18.1        | 84.0       | 83.0   |            |             | 87.1          |\n| ScreenSpot Pro          |             |            | 17.1   |            | 1.6         | 43.6          |\n| AITZ_EM                 | 35.3        |            |        |            | 72.8        | 83.2          |\n| Android Control High_EM |             |            |        | 66.4       | 59.1        | 67.36         |\n| Android Control Low_EM  |             |            |        | 84.4       | 59.2        | 93.7          |\n| AndroidWorld_SR         | 34.5% (SoM) |            | 27.9%  | 26.1%      |             | 35%           |\n| MobileMiniWob++_SR      |             |            |        | 66%        |             | 68%           |\n| OSWorld                 |             |            | 14.90  | 10.26      |             | 8.83          |\n\n\n## Requirements\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers accelerate\n```\nor you might encounter the following error:\n```\nKeyError: ''qwen2_5_vl''\n```\n\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen2.5-VL with ðŸ¤– ModelScope and ðŸ¤— Transformers.\n\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers accelerate\n```\nor you might encounter the following error:\n```\nKeyError: ''qwen2_5_vl''\n```\n\n\nWe offer a toolkit to help you handle various types of visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved images and videos. You can install it using the following command:\n\n```bash\n# It''s highly recommanded to use `[decord]` feature for faster video loading.\npip install qwen-vl-utils[decord]==0.0.8\n```\n\nIf you are not using Linux, you might not be able to install `decord` from PyPI. In that case, you can use `pip install qwen-vl-utils` which will fall back to using torchvision for video processing. However, you can still [install decord from source](https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source) to get decord used when loading video.\n\n### Using ðŸ¤—  Transformers to Chat\n\nHere we show a code snippet to show you how to use the chat model with `transformers` and `qwen_vl_utils`:\n\n```python\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    "Qwen/Qwen2.5-VL-72B-Instruct", torch_dtype="auto", device_map="auto"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n#     "Qwen/Qwen2.5-VL-72B-Instruct",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation="flash_attention_2",\n#     device_map="auto",\n# )\n\n# default processer\nprocessor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-72B-Instruct")\n\n# The default range for the number of visual tokens per image in the model is 4-16384.\n# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-72B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image",\n                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",\n            },\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors="pt",\n)\ninputs = inputs.to("cuda")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n<details>\n<summary>Multi image inference</summary>\n\n```python\n# Messages containing multiple images and a text query\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "file:///path/to/image1.jpg"},\n            {"type": "image", "image": "file:///path/to/image2.jpg"},\n            {"type": "text", "text": "Identify the similarities between these images."},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors="pt",\n)\ninputs = inputs.to("cuda")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n</details>\n\n<details>\n<summary>Video inference</summary>\n\n```python\n# Messages containing a images list as a video and a text query\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "video",\n                "video": [\n                    "file:///path/to/frame1.jpg",\n                    "file:///path/to/frame2.jpg",\n                    "file:///path/to/frame3.jpg",\n                    "file:///path/to/frame4.jpg",\n                ],\n            },\n            {"type": "text", "text": "Describe this video."},\n        ],\n    }\n]\n\n# Messages containing a local video path and a text query\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "video",\n                "video": "file:///path/to/video1.mp4",\n                "max_pixels": 360 * 420,\n                "fps": 1.0,\n            },\n            {"type": "text", "text": "Describe this video."},\n        ],\n    }\n]\n\n# Messages containing a video url and a text query\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "video",\n                "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4",\n            },\n            {"type": "text", "text": "Describe this video."},\n        ],\n    }\n]\n\n#In Qwen 2.5 VL, frame rate information is also input into the model to align with absolute time.\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    fps=fps,\n    padding=True,\n    return_tensors="pt",\n    **video_kwargs,\n)\ninputs = inputs.to("cuda")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\nVideo URL compatibility largely depends on the third-party library version. The details are in the table below. change the backend by `FORCE_QWENVL_VIDEO_READER=torchvision` or `FORCE_QWENVL_VIDEO_READER=decord` if you prefer not to use the default one.\n\n| Backend     | HTTP | HTTPS |\n|-------------|------|-------|\n| torchvision >= 0.19.0 | âœ…  | âœ…   |\n| torchvision < 0.19.0  | âŒ  | âŒ   |\n| decord      | âœ…  | âŒ   |\n</details>\n\n<details>\n<summary>Batch inference</summary>\n\n```python\n# Sample messages for batch inference\nmessages1 = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "file:///path/to/image1.jpg"},\n            {"type": "image", "image": "file:///path/to/image2.jpg"},\n            {"type": "text", "text": "What are the common elements in these pictures?"},\n        ],\n    }\n]\nmessages2 = [\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": "Who are you?"},\n]\n# Combine messages for batch processing\nmessages = [messages1, messages2]\n\n# Preparation for batch inference\ntexts = [\n    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n    for msg in messages\n]\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=texts,\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors="pt",\n)\ninputs = inputs.to("cuda")\n\n# Batch Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_texts = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_texts)\n```\n</details>\n\n### ðŸ¤– ModelScope\nWe strongly advise users especially those in mainland China to use ModelScope. `snapshot_download` can help you solve issues concerning downloading checkpoints.\n\n\n### More Usage Tips\n\nFor input images, we support local files, base64, and URLs. For videos, we currently only support local files.\n\n```python\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\n## Local file path\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "file:///path/to/your/image.jpg"},\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n## Image URL\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "http://path/to/your/image.jpg"},\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n## Base64 encoded image\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image", "image": "data:image;base64,/9j/..."},\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n```\n#### Image Resolution for performance boost\n\nThe model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs, such as a token count range of 256-1280, to balance speed and memory usage.\n\n```python\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n    "Qwen/Qwen2.5-VL-72B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels\n)\n```\n\nBesides, We provide two methods for fine-grained control over the image size input to the model:\n\n1. Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels.\n   \n2. Specify exact dimensions: Directly set `resized_height` and `resized_width`. These values will be rounded to the nearest multiple of 28.\n\n```python\n# min_pixels and max_pixels\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image",\n                "image": "file:///path/to/your/image.jpg",\n                "resized_height": 280,\n                "resized_width": 420,\n            },\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n# resized_height and resized_width\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image",\n                "image": "file:///path/to/your/image.jpg",\n                "min_pixels": 50176,\n                "max_pixels": 50176,\n            },\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n\n```json\n{\n	...,\n    "type": "yarn",\n    "mrope_section": [\n        16,\n        24,\n        24\n    ],\n    "factor": 4,\n    "original_max_position_embeddings": 32768\n}\n```\n\nHowever, it should be noted that this method has a significant impact on the performance of temporal and spatial localization tasks, and is therefore not recommended for use.\n\nAt the same time, for long video inputs, since MRoPE itself is more economical with ids, the max_position_embeddings can be directly modified to a larger value, such as 64k.\n\n\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5-VL,\n    title = {Qwen2.5-VL},\n    url = {https://qwenlm.github.io/blog/qwen2.5-vl/},\n    author = {Qwen Team},\n    month = {January},\n    year = {2025}\n}\n\n@article{Qwen2VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model''s Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":73410777344,"storage_bytes":146821823583,"files_count":50,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2_5_VLForConditionalGeneration"],"model_type":"qwen2_5_vl","processor_config":{"chat_template":"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message[''role''] != ''system'' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message[''role''] }}\n{% if message[''content''] is string %}{{ message[''content''] }}<|im_end|>\n{% else %}{% for content in message[''content''] %}{% if content[''type''] == ''image'' or ''image'' in content or ''image_url'' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content[''type''] == ''video'' or ''video'' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif ''text'' in content %}{{ content[''text''] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"},"tokenizer_config":{"bos_token":null,"chat_template":"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message[''role''] != ''system'' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message[''role''] }}\n{% if message[''content''] is string %}{{ message[''content''] }}<|im_end|>\n{% else %}{% for content in message[''content''] %}{% if content[''type''] == ''image'' or ''image'' in content or ''image_url'' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content[''type''] == ''video'' or ''video'' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif ''text'' in content %}{{ content[''text''] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen2.5-VL","source_url":"https://github.com/QwenLM/Qwen2.5-VL"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:dmlc:decord","source_url":"https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2409.12191","source_url":"https://arxiv.org/abs/2409.12191"},{"type":"based_on_paper","target_id":"arxiv:2308.12966","source_url":"https://arxiv.org/abs/2308.12966"}]', NULL, 'Other', 'approved', 77.6, '99c98e784da12fdad3d0720fe721606c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nerijs-pixel-art-xl', 'huggingface--nerijs--pixel-art-xl', 'pixel-art-xl', 'nerijs', '--- license: creativeml-openrail-m tags: - text-to-image - stable-diffusion - lora - diffusers base_model: stabilityai/stable-diffusion-xl-base-1.0 instance_prompt: pixel art widget: - text: pixel art, a cute corgi, simple, flat colors --- !F1hS8XHXwAQrMEW.jpeg !F1hS489X0AE-PK5.jpeg Downscale 8 times to get pixel perfect images (use Nearest Neighbors) Use a fixed VAE to avoid artifacts (0.9 or fp16 fix) Use it with a LCM Lora! Use 8 steps and guidance scale of 1.5 1.2 Lora strength for the Pi...', '["diffusers","text-to-image","stable-diffusion","lora","license:creativeml-openrail-m","region:us"]', 'text-to-image', 568, 4708, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nerijs/pixel-art-xl","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n  - text-to-image\n  - stable-diffusion\n  - lora\n  - diffusers\nbase_model: stabilityai/stable-diffusion-xl-base-1.0\ninstance_prompt: pixel art\nwidget:\n  - text: pixel art, a cute corgi, simple, flat colors\n---\n# Pixel Art XL\n## Consider supporting further research on [Patreon](https://www.patreon.com/user?u=29466374) or [Twitter](https://twitter.com/nerijs)\n\n![F1hS8XHXwAQrMEW.jpeg](https://cdn-uploads.huggingface.co/production/uploads/6303f37c3926de1f7ec42d3e/SSOQ9lfB1PVhXVWJiL7Mx.jpeg)\n![F1hS489X0AE-PK5.jpeg](https://cdn-uploads.huggingface.co/production/uploads/6303f37c3926de1f7ec42d3e/tY19J3xWDlSY2hhTTHySc.jpeg)\n\n\nDownscale 8 times to get pixel perfect images (use Nearest Neighbors)\nUse a fixed VAE to avoid artifacts (0.9 or fp16 fix)\n\n### Need more performance?\nUse it with a LCM Lora!\n\nUse 8 steps and guidance scale of 1.5\n1.2 Lora strength for the Pixel Art XL works better\n\n```python\nfrom diffusers import DiffusionPipeline, LCMScheduler\nimport torch\n\nmodel_id = "stabilityai/stable-diffusion-xl-base-1.0"\nlcm_lora_id = "latent-consistency/lcm-lora-sdxl"\npipe = DiffusionPipeline.from_pretrained(model_id, variant="fp16")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\npipe.load_lora_weights(lcm_lora_id, adapter_name="lora")\npipe.load_lora_weights("./pixel-art-xl.safetensors", adapter_name="pixel")\n\npipe.set_adapters(["lora", "pixel"], adapter_weights=[1.0, 1.2])\npipe.to(device="cuda", dtype=torch.float16)\n\nprompt = "pixel, a cute corgi"\nnegative_prompt = "3d render, realistic"\n\nnum_images = 9\n\nfor i in range(num_images):\n    img = pipe(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=8,\n        guidance_scale=1.5,\n    ).images[0]\n    \n    img.save(f"lcm_lora_{i}.png")\n```\n\n### Tips:\nDon''t use refiner\n\nWorks great with only 1 text encoder\n\nNo style prompt required\n\nNo trigger keyword require\n\nWorks great with isometric and non-isometric\n\nWorks with 0.9 and 1.0\n\n#### Changelog\nv1: Initial release', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":170543052,"files_count":3,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'creativeml-openrail-m', 'approved', 62.6, 'cab9fcaaf900e659744f77ad0ea36286', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen2.5-Coder-7B-Instruct', 'huggingface--qwen--qwen2.5-coder-7b-instruct', 'Qwen2.5-Coder-7B-Instruct', 'Qwen', '--- license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE language: - en base_model: - Qwen/Qwen2.5-Coder-7B pipeline_tag: text-generation library_name: transformers tags: - code - codeqwen - chat - qwen - qwen-coder --- <a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/...', '["transformers","safetensors","qwen2","text-generation","code","codeqwen","chat","qwen","qwen-coder","conversational","en","arxiv:2409.12186","arxiv:2309.00071","arxiv:2407.10671","base_model:qwen/qwen2.5-coder-7b","base_model:finetune:qwen/qwen2.5-coder-7b","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 568, 663875, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct/blob/main/LICENSE\nlanguage:\n- en\nbase_model:\n- Qwen/Qwen2.5-Coder-7B\npipeline_tag: text-generation\nlibrary_name: transformers\ntags:\n- code\n- codeqwen\n- chat\n- qwen\n- qwen-coder\n---\n\n\n# Qwen2.5-Coder-7B-Instruct\n<a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Introduction\n\nQwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5:\n\n- Significantly improvements in **code generation**, **code reasoning** and **code fixing**. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o.\n- A more comprehensive foundation for real-world applications such as **Code Agents**. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies.\n- **Long-context Support** up to 128K tokens.\n\n**This repo contains the instruction-tuned 7B Qwen2.5-Coder model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 7.61B\n- Number of Paramaters (Non-Embedding): 6.53B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 28 for Q and 4 for KV\n- Context Length: Full 131,072 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n  \nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/), [GitHub](https://github.com/QwenLM/Qwen2.5-Coder), [Documentation](https://qwen.readthedocs.io/en/latest/), [Arxiv](https://arxiv.org/abs/2409.12186).\n\n## Requirements\n\nThe code of Qwen2.5-Coder has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: ''qwen2''\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen2.5-Coder-7B-Instruct"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = "write a quick sort algorithm."\nmessages = [\n    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  "rope_scaling": {\n    "factor": 4.0,\n    "original_max_position_embeddings": 32768,\n    "type": "yarn"\n  }\n}\n```\n\nFor deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [ðŸ“‘ blog](https://qwenlm.github.io/blog/qwen2.5-coder-family/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@article{hui2024qwen2,\n      title={Qwen2. 5-Coder Technical Report},\n      author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Dang, Kai and others},\n      journal={arXiv preprint arXiv:2409.12186},\n      year={2024}\n}\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7615616512,"storage_bytes":30462543728,"files_count":14,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen2.5-Coder","source_url":"https://github.com/QwenLM/Qwen2.5-Coder"},{"type":"based_on_paper","target_id":"arxiv:2409.12186","source_url":"https://arxiv.org/abs/2409.12186"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2407.10671","source_url":"https://arxiv.org/abs/2407.10671"}]', NULL, 'Apache-2.0', 'approved', 62.6, '135938a1f3d2efb2a2da64876a40f0b2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nvidia-Llama-3.1-Nemotron-70B-Instruct', 'huggingface--nvidia--llama-3.1-nemotron-70b-instruct', 'Llama-3.1-Nemotron-70B-Instruct', 'nvidia', '--- license: llama3.1 language: - en inference: false fine-tuning: false tags: - nvidia - llama3.1 datasets: - nvidia/HelpSteer2 base_model: meta-llama/Llama-3.1-70B-Instruct library_name: nemo --- Llama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries. This model reaches Arena Hard of 85.0, AlpacaEval 2 LC of 57.6 and GPT-4-Turbo MT-Bench of 8.98, which are known to be predictive of LMSys Chatbot Ar...', '["nemo","nvidia","llama3.1","en","dataset:nvidia/helpsteer2","arxiv:2410.01257","arxiv:2310.05344","arxiv:2311.09528","arxiv:2406.08673","base_model:meta-llama/llama-3.1-70b-instruct","license:llama3.1","region:us"]', 'other', 567, 69, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: llama3.1\nlanguage:\n- en\ninference: false\nfine-tuning: false\ntags:\n- nvidia\n- llama3.1\ndatasets:\n- nvidia/HelpSteer2\nbase_model: meta-llama/Llama-3.1-70B-Instruct\nlibrary_name: nemo\n---\n# Model Overview\n\n## Description:\n\nLlama-3.1-Nemotron-70B-Instruct is a large language model customized by NVIDIA to improve the helpfulness of LLM generated responses to user queries.\n\n\nThis model reaches [Arena Hard](https://github.com/lmarena/arena-hard-auto) of 85.0, [AlpacaEval 2 LC](https://tatsu-lab.github.io/alpaca_eval/) of 57.6 and [GPT-4-Turbo MT-Bench](https://github.com/lm-sys/FastChat/pull/3158) of 8.98, which are known to be predictive of [LMSys Chatbot Arena Elo](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)\n\nAs of 1 Oct 2024, this model is #1 on all three automatic alignment benchmarks (verified tab for AlpacaEval 2 LC), edging out strong frontier models such as GPT-4o and Claude 3.5 Sonnet.\n\nAs of Oct 24th, 2024 the model has Elo Score of 1267(+-7), rank 9 and style controlled rank of 26 on [ChatBot Arena leaderboard](https://lmarena.ai/?leaderboard).\n\nThis model was trained using RLHF (specifically, REINFORCE), [Llama-3.1-Nemotron-70B-Reward](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward) and [HelpSteer2-Preference prompts](https://huggingface.co/datasets/nvidia/HelpSteer2) on a Llama-3.1-70B-Instruct model as the initial policy.\n\nIf you prefer to use the model in the HuggingFace Transformers codebase, we have done a model conversion format into [Llama-3.1-Nemotron-70B-Instruct-HF](https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF) .\n\nTry hosted inference for free at [build.nvidia.com](https://build.nvidia.com/nvidia/llama-3_1-nemotron-70b-instruct) - it comes with an OpenAI-compatible API interface.\n\n\nSee details on our paper at [https://arxiv.org/abs/2410.01257](https://arxiv.org/abs/2410.01257) - as a preview, this model can correctly the question ```How many r in strawberry?``` without specialized prompting or additional reasoning tokens:\n\n```\nA sweet question!\nLetâ€™s count the â€œRâ€s in â€œstrawberryâ€:\n1. S\n2. T\n3. R\n4. A\n5. W\n6. B\n7. E\n8. R\n9. R\n10. Y\nThere are **3 â€œRâ€s** in the word â€œstrawberryâ€.\n```\n\nNote: This model is a demonstration of our techniques for improving helpfulness in general-domain instruction following. It has not been tuned for performance in specialized domains such as math.\n\n## License\nYour use of this model is governed by the [NVIDIA Open Model License](https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf).\nAdditional Information: [Llama 3.1 Community License Agreement](https://www.llama.com/llama3_1/license/). Built with Llama.\n\n\n## Evaluation Metrics\n\nAs of 1 Oct 2024, Llama-3.1-Nemotron-70B-Instruct performs best on Arena Hard, AlpacaEval 2 LC (verified tab) and MT Bench (GPT-4-Turbo)\n\n | Model  | Arena Hard | AlpacaEval | MT-Bench | Mean Response Length |\n|:-----------------------------|:----------------|:-----|:----------|:-------|\n|Details | (95% CI) | 2 LC (SE) | (GPT-4-Turbo) | (# of Characters for MT-Bench)| \n| _**Llama-3.1-Nemotron-70B-Instruct**_ | **85.0** (-1.5, 1.5) | **57.6** (1.65) | **8.98** | 2199.8 | \n| Llama-3.1-70B-Instruct | 55.7 (-2.9, 2.7) | 38.1 (0.90)  | 8.22 | 1728.6 |\n| Llama-3.1-405B-Instruct | 69.3 (-2.4, 2.2) | 39.3 (1.43) | 8.49 | 1664.7 |\n| Claude-3-5-Sonnet-20240620 | 79.2 (-1.9, 1.7) | 52.4 (1.47) | 8.81 | 1619.9 |\n| GPT-4o-2024-05-13 | 79.3 (-2.1, 2.0) | 57.5 (1.47) | 8.74 | 1752.2 |\n         \n## Usage:\n\nWe demonstrate inference using NVIDIA NeMo Framework, which allows hassle-free model deployment based on [NVIDIA TRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), a highly optimized inference solution focussing on high throughput and low latency.\n\nPre-requisite: You would need at least a machine with 4 40GB or 2 80GB NVIDIA GPUs, and 150GB of free disk space. \n\n1. Please sign up to get **free and immediate** access to [NVIDIA NeMo Framework container](https://developer.nvidia.com/nemo-framework). If you donâ€™t have an NVIDIA NGC account, you will be prompted to sign up for an account before proceeding.\n2. If you donâ€™t have an NVIDIA NGC API key, sign into [NVIDIA NGC](https://ngc.nvidia.com/setup), selecting organization/team: ea-bignlp/ga-participants and click Generate API key. Save this key for the next step. Else, skip this step. \n3. On your machine, docker login to nvcr.io using\n   ```\n   docker login nvcr.io\n   Username: $oauthtoken\n   Password: <Your Saved NGC API Key>\n   ```\n4. Download the required container\n   ```\n   docker pull nvcr.io/nvidia/nemo:24.05.llama3.1\n   ```\n   \n5. Download the checkpoint\n   ```\n   git lfs install\n   git clone https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct\n   ```\n\n6. Run Docker container\n   (In addition, to use Llama3.1 tokenizer, you need to ```export HF_HOME=<YOUR_HF_HOME_CONTAINING_TOKEN_WITH_LLAMA3.1_70B_ACCESS>```)\n   ```\n   docker run --gpus all -it --rm --shm-size=150g -p 8000:8000 -v ${PWD}/Llama-3.1-Nemotron-70B-Instruct:/opt/checkpoints/Llama-3.1-Nemotron-70B-Instruct,${HF_HOME}:/hf_home -w /opt/NeMo nvcr.io/nvidia/nemo:24.05.llama3.1\n   ```\n   \n7. Within the container, start the server in the background. This step does both conversion of the nemo checkpoint to TRT-LLM and then deployment using TRT-LLM. For an explanation of each argument and advanced usage, please refer to [NeMo FW Deployment Guide](https://docs.nvidia.com/nemo-framework/user-guide/latest/deployment/llm/in_framework.html)\n   \n   ```\n   HF_HOME=/hf_home python scripts/deploy/nlp/deploy_inframework_triton.py --nemo_checkpoint /opt/checkpoints/Llama-3.1-Nemotron-70B-Instruct --model_type="llama" --triton_model_name nemotron --triton_http_address 0.0.0.0 --triton_port 8000 --num_gpus 2 --max_input_len 3072 --max_output_len 1024 --max_batch_size 1 &\n   ```\n   \n8. Once the server is ready (i.e. when you see this messages below), you are ready to launch your client code\n\n    ```\n    Started HTTPService at 0.0.0.0:8000\n    Started GRPCInferenceService at 0.0.0.0:8001\n    Started Metrics Service at 0.0.0.0:8002\n    ```\n\n    ```\n    python scripts/deploy/nlp/query_inframework.py -mn nemotron -p "How many r in strawberry?" -mol 1024\n    ```\n    \n \n\n## References(s):\n\n* [HelpSteer2-Preference](https://arxiv.org/abs/2410.01257)\n* [SteerLM method](https://arxiv.org/abs/2310.05344)\n* [HelpSteer](https://arxiv.org/abs/2311.09528)\n* [HelpSteer2](https://arxiv.org/abs/2406.08673)\n* [Introducing Llama 3.1: Our most capable models to date](https://ai.meta.com/blog/meta-llama-3-1/) \n* [Meta''s Llama 3.1 Webpage](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1) \n* [Meta''s Llama 3.1 Model Card](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md)\n\n\n## Model Architecture: \n**Architecture Type:** Transformer <br>\n**Network Architecture:** Llama 3.1 <br>\n\n## Input:\n**Input Type(s):** Text <br>\n**Input Format:** String <br>\n**Input Parameters:** One Dimensional (1D) <br>\n**Other Properties Related to Input:** Max of 128k tokens<br>\n\n## Output:\n**Output Type(s):** Text <br>\n**Output Format:** String <br>\n**Output Parameters:** One Dimensional (1D) <br>\n**Other Properties Related to Output:**  Max of 4k tokens <br>\n\n\n## Software Integration:\n**Supported Hardware Microarchitecture Compatibility:** <br>\n* NVIDIA Ampere <br>\n* NVIDIA Hopper <br>\n* NVIDIA Turing <br>\n**Supported Operating System(s):** Linux <br>\n\n## Model Version: \nv1.0\n\n# Training & Evaluation: \n\n* REINFORCE implemented in NeMo Aligner\n\n## Datasets:\n\n**Data Collection Method by dataset** <br>\n* [Hybrid: Human, Synthetic] <br>\n\n**Labeling Method by dataset** <br>\n* [Human] <br>\n\n**Link:** \n* [HelpSteer2](https://huggingface.co/datasets/nvidia/HelpSteer2)\n\n**Properties (Quantity, Dataset Descriptions, Sensor(s)):** <br>\n* 21, 362 prompt-responses built to make more models more aligned with human preference - specifically more helpful, factually-correct, coherent, and customizable based on complexity and verbosity.\n* 20, 324 prompt-responses used for training and 1, 038 used for validation.\n\n\n# Inference:\n**Engine:** [Triton](https://developer.nvidia.com/triton-inference-server) <br>\n**Test Hardware:** H100, A100 80GB, A100 40GB <br>\n\n\n## Ethical Considerations:\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse.  For more detailed information on ethical considerations for this model, please see the Model Card++ Explainability, Bias, Safety & Security, and Privacy Subcards.  Please report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n\n## Citation\n\nIf you find this model useful, please cite the following works\n\n```bibtex\n@misc{wang2024helpsteer2preferencecomplementingratingspreferences,\n      title={HelpSteer2-Preference: Complementing Ratings with Preferences}, \n      author={Zhilin Wang and Alexander Bukharin and Olivier Delalleau and Daniel Egert and Gerald Shen and Jiaqi Zeng and Oleksii Kuchaiev and Yi Dong},\n      year={2024},\n      eprint={2410.01257},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2410.01257}, \n}\n```', '{"pipeline_tag":null,"library_name":"nemo","framework":"nemo","params":null,"storage_bytes":141130115749,"files_count":3711,"spaces_count":54,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:lmarena:arena-hard-auto","source_url":"https://github.com/lmarena/arena-hard-auto"},{"type":"has_code","target_id":"github:lm-sys:FastChat","source_url":"https://github.com/lm-sys/FastChat"},{"type":"has_code","target_id":"github:NVIDIA:TensorRT-LLM","source_url":"https://github.com/NVIDIA/TensorRT-LLM"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"based_on_paper","target_id":"arxiv:2410.01257","source_url":"https://arxiv.org/abs/2410.01257"},{"type":"based_on_paper","target_id":"arxiv:2310.05344","source_url":"https://arxiv.org/abs/2310.05344"},{"type":"based_on_paper","target_id":"arxiv:2311.09528","source_url":"https://arxiv.org/abs/2311.09528"},{"type":"based_on_paper","target_id":"arxiv:2406.08673","source_url":"https://arxiv.org/abs/2406.08673"}]', NULL, 'llama3.1', 'approved', 62.5, 'ad788aab833f2f9a96c70c06fd39f866', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-reducto-RolmOCR', 'huggingface--reducto--rolmocr', 'RolmOCR', 'reducto', '--- library_name: transformers license: apache-2.0 datasets: - allenai/olmOCR-mix-0225 base_model: Qwen/Qwen2.5-VL-7B-Instruct --- Earlier this year, the Allen Institute for AI released olmOCR, an open-source tool that performs document OCR using the Qwen2-VL-7B vision language model (VLM). We were excited to see a high-quality, openly available approach to parsing PDFs and other complex documents â€” and curious to explore what else might be possible using newer foundation models and some ligh...', '["transformers","safetensors","qwen2_5_vl","image-to-text","dataset:allenai/olmocr-mix-0225","base_model:qwen/qwen2.5-vl-7b-instruct","base_model:finetune:qwen/qwen2.5-vl-7b-instruct","license:apache-2.0","text-generation-inference","endpoints_compatible","region:us"]', 'image-to-text', 567, 3843, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/reducto/RolmOCR","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlibrary_name: transformers\nlicense: apache-2.0\ndatasets:\n- allenai/olmOCR-mix-0225\nbase_model: Qwen/Qwen2.5-VL-7B-Instruct\n---\n\n# RolmOCR by [Reducto AI](https://reducto.ai/)\n\nEarlier this year, the [Allen Institute for AI](https://allenai.org/) released olmOCR, an open-source tool that performs document OCR using the Qwen2-VL-7B vision language model (VLM). We were excited to see a high-quality, openly available approach to parsing PDFs and other complex documents â€” and curious to explore what else might be possible using newer foundation models and some lightweight optimizations.\n\nThe result is **RolmOCR**, a drop-in alternative to olmOCR thatâ€™s faster, uses less memory, and still performs well on a variety of document types. We''re releasing it under **Apache 2.0** for anyone to try out, explore, or build on.\n\nThis model is a fine-tuned version of [Qwen/Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct) on the full [allenai/olmOCR-mix-0225](https://huggingface.co/datasets/allenai/olmOCR-mix-0225) dataset.\n\n## Key changes\nWe made three notable changes:Â \n\n1. **New Base Model**: We swapped in a more recent version of the existing model (Qwen2.5-VL-7B) as the foundation.\n\n2. **No Metadata inputs**: Unlike the original, we donâ€™t use metadata extracted from PDFs. This significantly reduces prompt length, which in turn lowers both processing time and VRAM usage â€” without hurting accuracy in most cases.Â \n\n3. **Rotation of training data:** About 15% of the training data was rotated to enhance robustness to off-angle documents. We otherwise use the same training set.Â \n\n## Usage\n\nHost your model with vLLM:\n```bash\nexport VLLM_USE_V1=1\nvllm serve reducto/RolmOCR \n```\n\nCall the model via openai compatible server:\n```python\n# HOST YOUR OPENAI COMPATIBLE API WITH THE FOLLOWING COMMAND in VLLM:\n# export VLLM_USE_V1=1\n# vllm serve reducto/RolmOCR \n\nfrom openai import OpenAI\nimport base64\n\nclient = OpenAI(api_key="123", base_url="http://localhost:8000/v1")\n\nmodel = "reducto/RolmOCR-7b"\n\ndef encode_image(image_path):\n    with open(image_path, "rb") as image_file:\n        return base64.b64encode(image_file.read()).decode("utf-8")\n\ndef ocr_page_with_rolm(img_base64):\n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\n                "role": "user",\n                "content": [\n                    {\n                        "type": "image_url",\n                        "image_url": {"url": f"data:image/png;base64,{img_base64}"},\n                    },\n                    {\n                        "type": "text",\n                        "text": "Return the plain text representation of this document as if you were reading it naturally.\n",\n                    },\n                ],\n            }\n        ],\n        temperature=0.2,\n        max_tokens=4096\n    )\n    return response.choices[0].message.content\n\ntest_img_path = "path/to/image.png"\nimg_base64 = encode_image(test_img_path)\nprint(ocr_page_with_rolm(img_base64))\n```\n\n## Limitations\n\n- RolmOCR, like other VLM-based OCR solutions, still suffer from hallucination or dropping contents.\n- Unlike the [Reducto Parsing API](https://app.reducto.ai/), RolmOCR cannot output layout bounding boxes.\n- We have not evaluated the performance of any quantized versions.\n\n## BibTex and citation info\n```\n@misc{RolmOCR,\n  author = {Reducto AI},\n  title = {RolmOCR: A Faster, Lighter Open Source OCR Model},\n  year = {2025},\n}\n```', '{"pipeline_tag":"image-to-text","library_name":"transformers","framework":"transformers","params":8292166656,"storage_bytes":16595836440,"files_count":17,"spaces_count":16,"gated":false,"private":false,"config":{"architectures":["Qwen2_5_VLForConditionalGeneration"],"model_type":"qwen2_5_vl","processor_config":{"chat_template":"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message[''role''] != ''system'' %}<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n{% endif %}<|im_start|>{{ message[''role''] }}\n{% if message[''content''] is string %}{{ message[''content''] }}<|im_end|>\n{% else %}{% for content in message[''content''] %}{% if content[''type''] == ''image'' or ''image'' in content or ''image_url'' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>{% elif content[''type''] == ''video'' or ''video'' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>{% elif ''text'' in content %}{{ content[''text''] }}{% endif %}{% endfor %}<|im_end|>\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"},"tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are a helpful assistant.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[]', NULL, 'Apache-2.0', 'approved', 62.5, 'f25dcb024a7f19e7e497494eb33f655c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-Phi-3.5-MoE-instruct', 'huggingface--microsoft--phi-3.5-moe-instruct', 'Phi-3.5-MoE-instruct', 'microsoft', '--- license: mit license_link: https://huggingface.co/microsoft/Phi-3.5-MoE-instruct/resolve/main/LICENSE language: - multilingual pipeline_tag: text-generation tags: - nlp - code widget: - messages: - role: user content: Can you provide ways to eat combinations of bananas and dragonfruits? library_name: transformers --- Phi-3.5-MoE is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available documents - with a focus on very...', '["transformers","safetensors","phimoe","text-generation","nlp","code","conversational","custom_code","multilingual","arxiv:2404.14219","arxiv:2407.13833","arxiv:2403.06412","license:mit","region:us"]', 'text-generation', 566, 108995, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/Phi-3.5-MoE-instruct","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Phi-3.5-MoE-instruct/resolve/main/LICENSE\nlanguage:\n- multilingual\npipeline_tag: text-generation\ntags:\n- nlp\n- code\nwidget:\n- messages:\n  - role: user\n    content: Can you provide ways to eat combinations of bananas and dragonfruits?\nlibrary_name: transformers\n---\n\n## Model Summary\n\nPhi-3.5-MoE is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available documents - with a focus on very high-quality, reasoning dense data. The model supports multilingual and comes with 128K context length (in tokens). The model underwent a rigorous enhancement process, incorporating supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures. \n\nðŸ¡ [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\nðŸ“° [Phi-3 Microsoft Blog](https://aka.ms/phi3.5-techblog) <br>\nðŸ“– [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) <br>\nðŸ‘©â€ðŸ³ [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\nðŸ–¥ï¸ [Try It](https://aka.ms/try-phi3.5moe) <br>\n\nMoE references:\nðŸ“œ[Phi-3.5-MoE Blog](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/announcing-the-availability-of-phi-3-5-moe-in-azure-ai-studio/ba-p/4256278) | ðŸ˜[GRIN MoE](https://huggingface.co/microsoft/GRIN-MoE)\n\n**Phi-3.5**: [[mini-instruct]](https://huggingface.co/microsoft/Phi-3.5-mini-instruct); [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct) ; [[vision-instruct]](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)\n\n## Intended Uses\n\n### Primary Use Cases\n\nThe model is intended for commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. \n\n### Use Case Considerations\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\n\n***Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.*** \n\n## Usage\n\n### Requirements\nPhi-3.5-MoE-instruct is integrated in the official version of `transformers` starting from **4.46.0**. \nThe current `transformers` version can be verified with: `pip list | grep transformers`.\n\nExamples of required packages:\n```\nflash_attn==2.5.8\ntorch==2.3.1\naccelerate==0.31.0\ntransformers==4.46.0\n```\n\nPhi-3.5-MoE-instruct is also available in [Azure AI Studio](https://aka.ms/try-phi3.5moe)\n\n### Tokenizer\n\nPhi-3.5-MoE-Instruct supports a vocabulary size of up to `32064` tokens. The [tokenizer files](https://huggingface.co/microsoft/Phi-3.5-moe-instruct/blob/main/added_tokens.json) already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model''s vocabulary size.\n\n### Input Formats\nGiven the nature of the training data, the Phi-3.5-MoE-instruct model is best suited for prompts using the chat format as follows:\n\n```\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|>\n```\n\n### Loading the model locally\nAfter obtaining the Phi-3.5-MoE-instruct model checkpoints, users can use this sample code for inference.\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n\ntorch.random.manual_seed(0) \n\nmodel = AutoModelForCausalLM.from_pretrained( \n    "microsoft/Phi-3.5-MoE-instruct",  \n    device_map="cuda",  \n    torch_dtype="auto",  \n    trust_remote_code=False,  \n) \n\ntokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3.5-MoE-instruct") \n\nmessages = [ \n    {"role": "system", "content": "You are a helpful AI assistant."}, \n    {"role": "user", "content": "Can you provide ways to eat combinations of bananas and dragonfruits?"}, \n    {"role": "assistant", "content": "Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey."}, \n    {"role": "user", "content": "What about solving an 2x + 3 = 7 equation?"}, \n] \n\npipe = pipeline( \n    "text-generation", \n    model=model, \n    tokenizer=tokenizer, \n) \n\ngeneration_args = { \n    "max_new_tokens": 500, \n    "return_full_text": False, \n    "temperature": 0.0, \n    "do_sample": False, \n} \n\noutput = pipe(messages, **generation_args) \nprint(output[0][''generated_text''])\n```\n\n## Benchmarks\n\nTo understand the capabilities, we compare Phi-3.5-MoE with a set of models over a variety of benchmarks using our internal benchmark platform. At the high-level overview of the model quality on representative benchmarks:\n\n| Category | Benchmark | Phi-3.5-MoE-instruct | Mistral-Nemo-12B-instruct-2407 | Llama-3.1-8B-instruct | Gemma-2-9b-It | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|--|--|\n| Popular aggregated benchmark | Arena Hard | 37.9 | 39.4 | 25.7 | 42.0 | 55.2 | 75.0 |\n| | BigBench Hard CoT (0-shot) | 79.1 | 60.2 | 63.4 | 63.5 | 66.7 | 80.4 |\n| | MMLU (5-shot) | 78.9 | 67.2 | 68.1 | 71.3 | 78.7 | 77.2 |\n| | MMLU-Pro (0-shot, CoT) | 54.3 | 40.7 | 44.0 | 50.1 | 57.2 | 62.8 |\n| Reasoning | ARC Challenge (10-shot) | 91.0 | 84.8 | 83.1 | 89.8 | 92.8 | 93.5 |\n| | BoolQ (2-shot) | 84.6 | 82.5 | 82.8 | 85.7 | 85.8 | 88.7 |\n| | GPQA (0-shot, CoT) | 36.8 | 28.6 | 26.3 | 29.2 | 37.5 | 41.1 |\n| | HellaSwag (5-shot) | 83.8 | 76.7 | 73.5 | 80.9 | 67.5 | 87.1 |\n| | OpenBookQA (10-shot) | 89.6 | 84.4 | 84.8 | 89.6 | 89.0 | 90.0 |\n| | PIQA (5-shot) | 88.6 | 83.5 | 81.2 | 83.7 | 87.5 | 88.7 |\n| | Social IQA (5-shot) | 78.0 | 75.3 | 71.8 | 74.7 | 77.8 | 82.9 |\n| | TruthfulQA (MC2) (10-shot) | 77.5 | 68.1 | 69.2 | 76.6 | 76.6 | 78.2 |\n| | WinoGrande (5-shot) | 81.3 | 70.4 | 64.7 | 74.0 | 74.7 | 76.9 |\n| Multilingual | Multilingual MMLU (5-shot) | 69.9 | 58.9 | 56.2 | 63.8 | 77.2 | 72.9 |\n| | MGSM (0-shot CoT) | 58.7 | 63.3 | 56.7 | 75.1 | 75.8 | 81.7 |\n| Math | GSM8K (8-shot, CoT) | 88.7 | 84.2 | 82.4 | 84.9 | 82.4 | 91.3 |\n| | MATH (0-shot, CoT) | 59.5 | 31.2 | 47.6 | 50.9 | 38.0 | 70.2 |\n| Long context | Qasper | 40.0 | 30.7 | 37.2 | 13.9 | 43.5 | 39.8 |\n| | SQuALITY | 24.1 | 25.8 | 26.2 | 0.0 | 23.5 | 23.8 |\n| Code Generation | HumanEval (0-shot) | 70.7 | 63.4 | 66.5 | 61.0 | 74.4 | 86.6 |\n| | MBPP (3-shot) | 80.8 | 68.1 | 69.4 | 69.3 | 77.5 | 84.1 |\n| **Average** | | **69.2** | **61.3** | **61.0** | **63.3** | **68.5** | **74.9** |\n\nWe take a closer look at different categories across 80 public benchmark datasets at the table below:\n| Category | Phi-3.5-MoE-instruct | Mistral-Nemo-12B-instruct-2407 | Llama-3.1-8B-instruct | Gemma-2-9b-It | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|--|\n| Popular aggregated benchmark | 62.6 | 51.9 | 50.3 | 56.7 | 64.5 | 73.9 |\n| Reasoning | 78.7 | 72.2 | 70.5 | 75.4 | 77.7 | 80.0 |\n| Language understanding | 71.8 | 67.0 | 62.9 | 72.8 | 66.6 | 76.8 |\n| Robustness | 75.6 | 65.2 | 59.8 | 64.7 | 68.9 | 77.5 |\n| Long context | 25.5 | 24.5 | 25.5 | 0.0 | 27.0 | 25.4 |\n| Math | 74.1 | 57.7 | 65.0 | 67.9 | 60.2 | 80.8 |\n| Code generation | 68.3 | 56.9 | 65.8 | 58.3 | 66.8 | 69.9 |\n| Multilingual | 65.8 | 55.3 | 47.5 | 59.6 | 64.3 | 76.6 |\n\nOverall, Phi-3.5-MoE with only **6.6B active parameters** achieves a similar level of language understanding and math as much larger models. Moreover, the model outperforms bigger models in reasoning capability and only behind GPT-4o-mini. However, it is still fundamentally limited by its size for certain tasks. The model simply does not have the capacity to store too much factual knowledge, therefore, users may experience factual incorrectness. However, we believe such weakness can be resolved by augmenting Phi-3.5 with a search engine, particularly when using the model under RAG settings.\n\n### Multilingual\n\nThe table below highlights multilingual capability of Phi-3.5-MoE on multilingual MMLU, MEGA, and multilingual MMLU-pro datasets. Overall, we observed that even with just 6.6B active parameters, the model is very competitive on multilingual tasks in comparison to other models with a much bigger active parameters.\n\n| Category | Phi-3.5-MoE-instruct | Mistral-Nemo-12B-instruct-2407 | Llama-3.1-8B-instruct | Gemma-2-9b-It | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|--|\n| Multilingual MMLU | 69.9 | 58.9 | 56.2 | 63.8 | 77.2 | 72.9 |\n| Multilingual MMLU-Pro | 45.3 | 34.0 | 21.4 | 43.0 | 57.9 | 53.2 |\n| MGSM | 58.7 | 63.3 | 56.7 | 75.1 | 75.8 | 81.7 |\n| MEGA MLQA | 65.3 | 61.2 | 45.2 | 54.4 | 61.6 | 70.0 |\n| MEGA TyDi QA | 67.1 | 63.7 | 54.5 | 65.6 | 63.6 | 81.8 |\n| MEGA UDPOS | 60.4 | 58.2 | 54.1 | 56.6 | 62.4 | 66.0 |\n| MEGA XCOPA | 76.6 | 10.8 | 21.1 | 31.2 | 95.0 | 90.3 |\n| MEGA XStoryCloze | 82.8 | 92.3 | 71.0 | 87.0 | 20.7 | 96.6 |\n| **Average** | **65.8** | **55.3** | **47.5** | **59.6** | **64.3** | **76.6** |\n\n### Long Context\n\nPhi-3.5-MoE supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA, multilingual context retrieval. We see that Phi-3.5 is clearly better than Gemma-2 family which only supports 8K context length. Phi-3.5-MoE-instruct is very competitive with other much larger open-weight models such as Llama-3.1-8B-instruct, and Mistral-Nemo-12B-instruct-2407.\n\n| Benchmark | Phi-3.5-MoE-instruct | Mistral-Nemo-12B-instruct-2407 | Llama-3.1-8B-instruct | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|\n| GovReport | 26.4 | 25.6 | 25.1 | 27.8 | 24.8 |\n| QMSum | 19.9 | 22.1 | 21.6 | 24.0 | 21.7 |\n| Qasper | 40.0 | 30.7 | 37.2 | 43.5 | 39.8 |\n| SQuALITY | 24.1 | 25.8 | 26.2 | 23.5 | 23.8 |\n| SummScreenFD | 16.9 | 18.2 | 17.6 | 16.3 | 17.0 |\n| **Average** | **25.5** | **24.5** | **25.5** | **27.0** | **25.4** |\n\nRULER: a retrieval-based benchmark for long context understanding\n| Model | 4K | 8K | 16K | 32K | 64K | 128K | Average |\n|--|--|--|--|--|--|--|--|\n| Phi-3.5-MoE-instruct | 94.8 | 93 | 93.2 | 91.6 | 85.7 | 64.2 | **87.1** |\n| Llama-3.1-8B-instruct | 95.5 | 93.8 | 91.6 | 87.4 | 84.7 | 77.0 | **88.3** |\n| Mistral-Nemo-12B-instruct-2407 | 87.8 | 87.2 | 87.7 | 69.0 | 46.8 | 19.0 | **66.2** |\n\nRepoQA: a benchmark for long context code understanding\n| Model | Python | C++ | Rust | Java | TypeScript | Average |\n|--|--|--|--|--|--|--|\n| Phi-3.5-MoE-instruct | 89 | 74 | 81 | 88 | 95 | **85** |\n| Llama-3.1-8B-instruct | 80 | 65 | 73 | 76 | 63 | **71** |\n| Mistral-7B-instruct-v0.3 | 61 | 57 | 51 | 61 | 80 | **62** |\n\n## Training\n\n### Model\n\n**Architecture:** Phi-3.5-MoE has 16x3.8B parameters with **6.6B active parameters** when using 2 experts. The model is a mixture-of-expert decoder-only Transformer model using the tokenizer with vocabulary size of 32,064.<br>\n**Inputs:** Text. It is best suited for prompts using chat format.<br>\n**Context length:** 128K tokens<br>\n**GPUs:** 512 H100-80G<br>\n**Training time:** 23 days<br>\n**Training data:** 4.9T tokens<br>\n**Outputs:** Generated text in response to the input<br>\n**Dates:** Trained between April and August 2024<br>\n**Status:** This is a static model trained on an offline dataset with cutoff date October 2023 for publicly available data. Future versions of the tuned models may be released as we improve models.<br>\n**Supported languages:** Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian<br>\n**Release date:** August 2024<br>\n\n### Training Datasets\nOur training data includes a wide variety of sources, totaling 4.9 trillion tokens (including 10% multilingual), and is a combination of \n1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code;\n2) newly created synthetic, â€œtextbook-likeâ€ data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);\n3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. \n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://arxiv.org/pdf/2404.14219).\n\n## Responsible AI Considerations\n\nLike other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:  \n* Quality of Service: The Phi models are trained primarily on English text and some additional multilingual text. Languages other than English will experience worse performance as well as performance disparities across non-English. English language varieties with less representation in the training data might experience worse performance than standard American English.\n* Multilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 3 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards.\n* Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.\n* Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.\n* Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.\n* Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as "typing, math, random, collections, datetime, itertools". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n* Long Conversation: Phi-3 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift\n\nDevelopers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi-3 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include: \n* Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n* High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\n* Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\n* Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\n* Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Safety Evaluation and Red-Teaming\n\nWe leveraged various evaluation techniques including red teaming, adversarial conversation simulations, and multilingual safety evaluation benchmark datasets to \nevaluate Phi-3.5 models'' propensity to produce undesirable outputs across multiple languages and risk categories. \nSeveral approaches were used to compensate for the limitations of one approach alone. Findings across the various evaluation methods indicate that safety \npost-training that was done as detailed in the [Phi-3 Safety Post-Training paper](https://arxiv.org/pdf/2407.13833) had a positive impact across multiple languages and risk categories as observed by \nrefusal rates (refusal to output undesirable outputs) and robustness to jailbreak techniques. Note, however, while comprehensive red team evaluations were conducted \nacross all models in the prior release of Phi models, red teaming was largely focused on Phi-3.5 MOE across multiple languages and risk categories for this release as \nit is the largest and more capable model of the three models. Details on prior red team evaluations across Phi models can be found in the [Phi-3 Safety Post-Training paper](https://arxiv.org/pdf/2407.13833). \nFor this release, insights from red teaming indicate that the models may refuse to generate undesirable outputs in English, even when the request for undesirable output \nis in another language. Models may also be more susceptible to longer multi-turn jailbreak techniques across both English and non-English languages. These findings \nhighlight the need for industry-wide investment in the development of high-quality safety evaluation datasets across multiple languages, including low resource languages, \nand risk areas that account for cultural nuances where those languages are spoken.\n\n## Software\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n## Hardware\nNote that by default, the Phi-3.5-MoE-instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\n* NVIDIA A100\n* NVIDIA A6000\n* NVIDIA H100\n  \n## License\nThe model is licensed under the [MIT license](./LICENSE).\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must followâ€¯[Microsoftâ€™s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-partyâ€™s policies.\n\n\n## Appendix A: Korean benchmarks\n\nThe prompt is the same as the [CLIcK paper](https://arxiv.org/abs/2403.06412) prompt. The experimental results below were given with max_tokens=512 (zero-shot), max_tokens=1024 (5-shot), temperature=0.01. No system prompt used.\n\n- GPT-4o: 2024-05-13 version\n- GPT-4o-mini: 2024-07-18 version\n- GPT-4-turbo: 2024-04-09 version\n- GPT-3.5-turbo: 2023-06-13 version\n\nOverall, the Phi-3.5 MoE model with just 6.6B active params outperforms GPT-3.5-Turbo.\n\n| Benchmarks               |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:-------------------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| CLIcK                    |                  56.44 |                           29.12 |                   47.82 |    80.46 |         68.5  |         72.82 |           50.98 |\n| HAERAE 1.0               |                  61.83 |                           36.41 |                   53.9  |    85.7  |         76.4  |         77.76 |           52.67 |\n| KMMLU (0-shot, CoT)      |                  47.43 |                           30.82 |                   38.54 |    64.26 |         52.63 |         58.75 |           40.3  |\n| KMMLU (5-shot)           |                  47.92 |                           29.98 |                   20.21 |    64.28 |         51.62 |         59.29 |           42.28 |\n| KMMLU-HARD (0-shot, CoT) |                  25.34 |                           25.68 |                   24.03 |    39.62 |         24.56 |         30.56 |           20.97 |\n| KMMLU-HARD (5-shot)      |                  25.66 |                           25.73 |                   15.81 |    40.94 |         24.63 |         31.12 |           21.19 |\n| **Average**              |          **45.82** |                **29.99** |            **29.29** |    **62.54** |         **50.08** |         **56.74** |           **39.61** |\n\n#### CLIcK (Cultural and Linguistic Intelligence in Korean)\n\n##### Accuracy by supercategory\n| supercategory   |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Culture         |                  58.44 |                           29.74 |                   51.15 |    81.89 |         70.95 |         73.61 |           53.38 |\n| Language        |                  52.31 |                           27.85 |                   40.92 |    77.54 |         63.54 |         71.23 |           46    |\n| **Overall**     |                  56.44 |                           29.12 |                   47.82 |    80.46 |         68.5  |         72.82 |           50.98 |\n\n##### Accuracy by category\n| supercategory   | category    |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|:------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Culture         | Economy     |                  77.97 |                           28.81 |                   66.1  |    94.92 |         83.05 |         89.83 |           64.41 |\n| Culture         | Geography   |                  60.31 |                           29.01 |                   54.2  |    80.15 |         77.86 |         82.44 |           53.44 |\n| Culture         | History     |                  33.93 |                           30    |                   29.64 |    66.92 |         48.4  |         46.4  |           31.79 |\n| Culture         | Law         |                  52.51 |                           22.83 |                   44.29 |    70.78 |         57.53 |         61.19 |           41.55 |\n| Culture         | Politics    |                  70.24 |                           33.33 |                   59.52 |    88.1  |         83.33 |         89.29 |           65.48 |\n| Culture         | Pop Culture |                  80.49 |                           34.15 |                   60.98 |    97.56 |         85.37 |         92.68 |           75.61 |\n| Culture         | Society     |                  74.43 |                           31.72 |                   65.05 |    92.88 |         85.44 |         86.73 |           71.2  |\n| Culture         | Tradition   |                  58.11 |                           31.98 |                   54.95 |    87.39 |         74.77 |         79.28 |           55.86 |\n| Language        | Functional  |                  48    |                           24    |                   32.8  |    84.8  |         64.8  |         80    |           40    |\n| Language        | Grammar     |                  29.58 |                           23.33 |                   22.92 |    57.08 |         42.5  |         47.5  |           30    |\n| Language        | Textual     |                  73.33 |                           33.33 |                   59.65 |    91.58 |         80.7  |         87.37 |           62.11 |\n\n#### HAERAE 1.0\n\n| category              |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| General Knowledge     |                  39.77 |                           28.41 |                   34.66 |    77.27 |         53.41 |         66.48 |           40.91 |\n| History               |                  60.64 |                           22.34 |                   44.15 |    92.02 |         84.57 |         78.72 |           30.32 |\n| Loan Words            |                  70.41 |                           35.5  |                   63.31 |    79.88 |         76.33 |         78.11 |           59.17 |\n| Rare Words            |                  63.95 |                           42.96 |                   63.21 |    87.9  |         81.98 |         79.01 |           61.23 |\n| Reading Comprehension |                  64.43 |                           41.16 |                   51.9  |    85.46 |         77.18 |         80.09 |           56.15 |\n| Standard Nomenclature |                  66.01 |                           32.68 |                   58.82 |    88.89 |         75.82 |         79.08 |           53.59 |\n| **Overall**           |                  61.83 |                           36.41 |                   53.9  |    85.7  |         76.4  |         77.76 |           52.67 |\n\n#### KMMLU (0-shot, CoT)\n\n| supercategory   |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                  45.15 |                           31.68 |                   37.03 |    61.52 |         49.29 |         55.98 |           38.47 |\n| HUMSS           |                  49.75 |                           26.47 |                   37.29 |    69.45 |         56.59 |         63    |           40.9  |\n| Other           |                  47.24 |                           31.01 |                   39.15 |    63.79 |         52.35 |         57.53 |           40.19 |\n| STEM            |                  49.08 |                           31.9  |                   40.42 |    65.16 |         54.74 |         60.84 |           42.24 |\n| **Overall**     |                  47.43 |                           30.82 |                   38.54 |    64.26 |         52.63 |         58.75 |           40.3  |\n\n#### KMMLU (5-shot)\n\n| supercategory   |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                  45.9  |                           29.98 |                   19.24 |    61.47 |         48.66 |         56.85 |           40.22 |\n| HUMSS           |                  49.18 |                           27.27 |                   22.5  |    68.79 |         55.95 |         63.68 |           43.35 |\n| Other           |                  48.43 |                           30.76 |                   20.95 |    64.21 |         51.1  |         57.85 |           41.92 |\n| STEM            |                  49.21 |                           30.73 |                   19.55 |    65.28 |         53.29 |         61.08 |           44.43 |\n| **Overall**     |                  47.92 |                           29.98 |                   20.21 |    64.28 |         51.62 |         59.29 |           42.28 |\n\n#### KMMLU-HARD (0-shot, CoT)\n\n| supercategory   |   Phi-3.5-MoE-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024)|   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                  25.83 |                           26.17 |                   26.25 |    37.12 |         22.25 |         29.17 |           21.07 |\n| HUMSS           |                  21.52 |                           24.38 |                   20.21 |    41.97 |         23.31 |         31.51 |           19.44 |\n| Other           |                  24.82 |                           24.82 |                   23.88 |    40.39 |         26.48 |         29.59 |           22.22 |\n| STEM            |                  28.18 |                           26.91 |                   24.64 |    39.82 |         26.36 |         32.18 |           20.91 |\n| **Overall**     |                  25.34 |                           25.68 |                   24.03 |    39.62 |         24.56 |         30.56 |           20.97 |\n\n#### KMMLU-HARD (5-shot) \n\n| supercategory   |   Phi-3.5-MoE-Instruct |  Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|-----------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                  21    |                           29    |                   12    |    31    |         21    |         25    |           20    |\n| HUMSS           |                  22.88 |                           19.92 |                   14    |    43.98 |         23.47 |         33.53 |           19.53 |\n| Other           |                  25.13 |                           27.27 |                   12.83 |    39.84 |         28.34 |         29.68 |           23.22 |\n| STEM            |                  21.75 |                           25.25 |                   12.75 |    40.25 |         23.25 |         27.25 |           19.75 |\n| **Overall**     |                  25.66 |                           25.73 |                   15.81 |    40.94 |         24.63 |         31.12 |           21.19 |', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":41873153344,"storage_bytes":83747055891,"files_count":34,"spaces_count":12,"gated":false,"private":false,"config":{"architectures":["PhiMoEForCausalLM"],"auto_map":{"AutoConfig":"configuration_phimoe.PhiMoEConfig","AutoModelForCausalLM":"modeling_phimoe.PhiMoEForCausalLM"},"model_type":"phimoe","tokenizer_config":{"bos_token":"<s>","chat_template":"{% for message in messages %}{% if message[''role''] == ''system'' and message[''content''] %}{{''<|system|>\n'' + message[''content''] + ''<|end|>\n''}}{% elif message[''role''] == ''user'' %}{{''<|user|>\n'' + message[''content''] + ''<|end|>\n''}}{% elif message[''role''] == ''assistant'' %}{{''<|assistant|>\n'' + message[''content''] + ''<|end|>\n''}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ ''<|assistant|>\n'' }}{% else %}{{ eos_token }}{% endif %}","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:microsoft:Phi-3CookBook","source_url":"https://github.com/microsoft/Phi-3CookBook"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:HazyResearch:flash-attention","source_url":"https://github.com/HazyResearch/flash-attention"},{"type":"based_on_paper","target_id":"arxiv:2404.14219","source_url":"https://arxiv.org/abs/2404.14219"},{"type":"based_on_paper","target_id":"arxiv:2407.13833","source_url":"https://arxiv.org/abs/2407.13833"},{"type":"based_on_paper","target_id":"arxiv:2403.06412","source_url":"https://arxiv.org/abs/2403.06412"}]', NULL, 'MIT', 'approved', 77.5, '1733594dcbb8a5175564c1a097c6813f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-playgroundai-playground-v2-1024px-aesthetic', 'huggingface--playgroundai--playground-v2-1024px-aesthetic', 'playground-v2-1024px-aesthetic', 'playgroundai', '--- license: other license_name: playground-v2-community license_link: https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic/blob/main/LICENSE.md tags: - text-to-image - playground inference: parameters: guidance_scale: 3.0 --- This repository contains a model that generates highly aesthetic images of resolution 1024x1024. You can use the model with Hugging Face ðŸ§¨ Diffusers. !image/png **Playground v2** is a diffusion-based text-to-image generative model. The model was trained f...', '["diffusers","safetensors","text-to-image","playground","license:other","endpoints_compatible","diffusers:stablediffusionxlpipeline","region:us"]', 'text-to-image', 563, 774, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: playground-v2-community\nlicense_link: https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic/blob/main/LICENSE.md\ntags:\n- text-to-image\n- playground\ninference:\n  parameters:\n    guidance_scale: 3.0\n---\n# Playground v2 â€“ 1024px Aesthetic Model\n\nThis repository contains a model that generates highly aesthetic images of resolution 1024x1024. You can use the model with Hugging Face ðŸ§¨ Diffusers.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/63855d851769b7c4b10e1f76/p0up5GNQgO0vVIiJ672K7.png)\n\n\n**Playground v2** is a diffusion-based text-to-image generative model. The model was trained from scratch by the research team at [Playground](https://playground.com). \n\nImages generated by Playground v2 are favored **2.5** times more than those produced by Stable Diffusion XL, according to Playgroundâ€™s [user study](#user-study).\n\nWe are thrilled to release [intermediate checkpoints](#intermediate-base-models) at different training stages, including evaluation metrics, to the community. We hope this will encourage further research into foundational models for image generation.\n\nLastly, we introduce a new benchmark, [MJHQ-30K](#mjhq-30k-benchmark), for automatic evaluation of a modelâ€™s aesthetic quality.\n\nPlease see our [blog](https://blog.playgroundai.com/playground-v2/) for more details.\n\n### Model Description\n\n- **Developed by:** [Playground](https://playground.com)\n- **Model type:** Diffusion-based text-to-image generative model\n- **License:** [Playground v2 Community License](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic/blob/main/LICENSE.md)\n- **Summary:** This model generates images based on text prompts. It is a Latent Diffusion Model that uses two fixed, pre-trained text encoders ([OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip) and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main)). It follows the same architecture as [Stable Diffusion XL](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0).\n\n### Using the model with ðŸ§¨ Diffusers\n\nInstall diffusers >= 0.24.0 and some dependencies:\n```\npip install transformers accelerate safetensors\n```\n\nTo use the model, run the following snippet.\n\n**Note**: It is recommend to use **`guidance_scale=3.0`**.\n\n```python\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\n    "playgroundai/playground-v2-1024px-aesthetic",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    add_watermarker=False,\n    variant="fp16"\n)\npipe.to("cuda")\n\nprompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"\nimage  = pipe(prompt=prompt, guidance_scale=3.0).images[0]\n```\n\n### Using the model with Automatic1111/ComfyUI\n\nIn order to use the model with software such as Automatic1111 or ComfyUI you can use [`playground-v2.fp16.safetensors`](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic/blob/main/playground-v2.fp16.safetensors) file.\n\n### User Study\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/63855d851769b7c4b10e1f76/8VzBkSYaUU3dt509Co9sk.png)\n\nAccording to user studies conducted by Playground, involving over 2,600 prompts and thousands of users, the images generated by Playground v2 are favored **2.5** times more than those produced by [Stable Diffusion XL](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0).\n\nWe report user preference metrics on [PartiPrompts](https://github.com/google-research/parti), following standard practice, and on an internal prompt dataset curated by the Playground team. The â€œInternal 1Kâ€ prompt dataset is diverse and covers various categories and tasks.\n\nDuring the user study, we give users instructions to evaluate image pairs based on both (1) their aesthetic preference and (2) the image-text alignment.\n\n### MJHQ-30K Benchmark\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/63855d851769b7c4b10e1f76/o3Bt62qFsTO9DkeX2yLua.png)\n\n| Model                                 | Overall FID   |\n| ------------------------------------- | ----- |\n| SDXL-1-0-refiner                      | 9.55  |\n| [playground-v2-1024px-aesthetic](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic)        | **7.07**  |\n\nWe introduce a new benchmark, [MJHQ-30K](https://huggingface.co/datasets/playgroundai/MJHQ-30K), for automatic evaluation of a modelâ€™s aesthetic quality. The benchmark computes FID on a high-quality dataset to gauge aesthetic quality.\n\nWe have curated a high-quality dataset from Midjourney, featuring 10 common categories, with each category containing 3,000 samples. Following common practice, we use aesthetic score and CLIP score to ensure high image quality and high image-text alignment. Furthermore, we take extra care to make the data diverse within each category.\n\nFor Playground v2, we report both the overall FID and per-category FID. All FID metrics are computed at resolution 1024x1024. Our benchmark results show that our model outperforms SDXL-1-0-refiner in overall FID and all category FIDs, especially in people and fashion categories. This is in line with the results of the user study, which indicates a correlation between human preference and FID score on the MJHQ-30K benchmark.\n\nWe release this benchmark to the public and encourage the community to adopt it for benchmarking their modelsâ€™ aesthetic quality.\n\n### Intermediate Base Models\n\n| Model                        | FID    | Clip Score |\n| ---------------------------- | ------ | ---------- |\n| SDXL-1-0-refiner             | 13.04  | 32.62      |\n| [playground-v2-256px-base](https://huggingface.co/playgroundai/playground-v2-256px-base)     | 9.83   | 31.90      |\n| [playground-v2-512px-base](https://huggingface.co/playgroundai/playground-v2-512px-base)     | 9.55   | 32.08      |\n\n\nApart from [playground-v2-1024px-aesthetic](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic), we release intermediate checkpoints at different training stages to the community in order to foster foundation model research in pixels. Here, we report the FID score and CLIP score on the MSCOCO14 evaluation set for the reference purposes. (Note that our reported numbers may differ from the numbers reported in SDXLâ€™s published results, as our prompt list may be different.)\n\n### How to cite us\n\n```\n@misc{playground-v2,\n      url={[https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic)},\n      title={Playground v2},\n      author={Li, Daiqing and Kamko, Aleks and Sabet, Ali and Akhgari, Ehsan and Xu, Linmiao and Doshi, Suhail}\n}\n```', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":76318365586,"files_count":35,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionXLPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:mlfoundations:open_clip","source_url":"https://github.com/mlfoundations/open_clip"},{"type":"has_code","target_id":"github:openai:CLIP","source_url":"https://github.com/openai/CLIP"},{"type":"has_code","target_id":"github:google-research:parti","source_url":"https://github.com/google-research/parti"}]', NULL, 'Other', 'approved', 62.5, '776d436834d410b2dc60e7b99621a237', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen2.5-1.5B-Instruct', 'huggingface--qwen--qwen2.5-1.5b-instruct', 'Qwen2.5-1.5B-Instruct', 'Qwen', '--- license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct/blob/main/LICENSE language: - en pipeline_tag: text-generation base_model: Qwen/Qwen2.5-1.5B tags: - chat library_name: transformers --- Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2: - Significantly **mo...', '["transformers","safetensors","qwen2","text-generation","chat","conversational","en","arxiv:2407.10671","base_model:qwen/qwen2.5-1.5b","base_model:finetune:qwen/qwen2.5-1.5b","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 563, 5515322, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\nbase_model: Qwen/Qwen2.5-1.5B\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# Qwen2.5-1.5B-Instruct\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 1.5B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings\n- Number of Parameters: 1.54B\n- Number of Paramaters (Non-Embedding): 1.31B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 12 for Q and 2 for KV\n- Context Length: Full 32,768 tokens and generation 8192 tokens\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: ''qwen2''\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen2.5-1.5B-Instruct"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [ðŸ“‘ blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":1543714304,"storage_bytes":3087467144,"files_count":10,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2407.10671","source_url":"https://arxiv.org/abs/2407.10671"}]', NULL, 'Apache-2.0', 'approved', 62.5, 'adfee1e46e8057f1c6bc5f622f7cd383', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Ministral-8B-Instruct-2410', 'huggingface--mistralai--ministral-8b-instruct-2410', 'Ministral-8B-Instruct-2410', 'mistralai', '--- library_name: vllm language: - en - fr - de - es - it - pt - zh - ja - ru - ko license: other license_name: mrl inference: false license_link: https://mistral.ai/licenses/MRL-0.1.md extra_gated_prompt: >- # Mistral AI Research License If You want to use a Mistral Model, a Derivative or an Output for any purpose that is not expressly authorized under this Agreement, You must request a license from Mistral AI, which Mistral AI may grant to You in Mistral AI''s sole discretion. To discuss suc...', '["vllm","safetensors","mistral","mistral-common","en","fr","de","es","it","pt","zh","ja","ru","ko","license:other","region:us"]', 'other', 563, 261441, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Ministral-8B-Instruct-2410","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: vllm\nlanguage:\n- en\n- fr\n- de\n- es\n- it\n- pt\n- zh\n- ja\n- ru\n- ko\nlicense: other\nlicense_name: mrl\ninference: false\nlicense_link: https://mistral.ai/licenses/MRL-0.1.md\nextra_gated_prompt: >-\n  # Mistral AI Research License\n\n  If You want to use a Mistral Model, a Derivative or an Output for any purpose\n  that is not expressly authorized under this Agreement, You must request a\n  license from Mistral AI, which Mistral AI may grant to You in Mistral AI''s\n  sole discretion. To discuss such a license, please contact Mistral AI via the\n  website contact form: https://mistral.ai/contact/\n\n  ## 1. Scope and acceptance\n\n  **1.1. Scope of the Agreement.** This Agreement applies to any use,\n  modification, or Distribution of any Mistral Model by You, regardless of the\n  source You obtained a copy of such Mistral Model.\n\n  **1.2. Acceptance.** By accessing, using, modifying, Distributing a Mistral\n  Model, or by creating, using or distributing a Derivative of the Mistral\n  Model, You agree to be bound by this Agreement.\n\n  **1.3. Acceptance on behalf of a third-party.** If You accept this Agreement\n  on behalf of Your employer or another person or entity, You warrant and\n  represent that You have the authority to act and accept this Agreement on\n  their behalf. In such a case, the word "You" in this Agreement will refer to\n  Your employer or such other person or entity.\n\n  ## 2. License\n\n  **2.1. Grant of rights**.  Subject to Section 3 below, Mistral AI hereby\n  grants You a non-exclusive, royalty-free, worldwide, non-sublicensable,\n  non-transferable, limited license to use, copy, modify, and Distribute under\n  the conditions provided in Section 2.2 below, the Mistral Model and any\n  Derivatives made by or for Mistral AI and to create Derivatives of the Mistral\n  Model.\n\n  **2.2. Distribution of Mistral Model and Derivatives made by or for Mistral\n  AI.** Subject to Section 3 below, You may Distribute copies of the Mistral\n  Model and/or Derivatives made by or for Mistral AI, under the following\n  conditions: You must make available a copy of this Agreement to third-party\n  recipients of the Mistral Models and/or Derivatives made by or for Mistral AI\n  you Distribute, it being specified that any rights to use the Mistral Models\n  and/or Derivatives made by or for Mistral AI shall be directly granted by\n  Mistral AI to said third-party recipients pursuant to the Mistral AI Research\n  License agreement executed between these parties; You must retain in all\n  copies of the Mistral Models the following attribution notice within a\n  "Notice" text file distributed as part of such copies: "Licensed by Mistral AI\n  under the Mistral AI Research License".\n\n  **2.3. Distribution of Derivatives made by or for You.** Subject to Section 3\n  below, You may Distribute any Derivatives made by or for You under additional\n  or different terms and conditions, provided that: In any event, the use and\n  modification of Mistral Model and/or Derivatives made by or for Mistral AI\n  shall remain governed by the terms and conditions of this Agreement; You\n  include in any such Derivatives made by or for You prominent notices stating\n  that You modified the concerned Mistral Model; and Any terms and conditions\n  You impose on any third-party recipients relating to Derivatives made by or\n  for You shall neither limit such third-party recipients'' use of the Mistral\n  Model or any Derivatives made by or for Mistral AI in accordance with the\n  Mistral AI Research License nor conflict with any of its terms and conditions.\n\n  ## 3. Limitations\n\n  **3.1. Misrepresentation.** You must not misrepresent or imply, through any\n  means, that the Derivatives made by or for You and/or any modified version of\n  the Mistral Model You Distribute under your name and responsibility is an\n  official product of Mistral AI or has been endorsed, approved or validated by\n  Mistral AI, unless You are authorized by Us to do so in writing.\n\n  **3.2. Usage Limitation.** You shall only use the Mistral Models, Derivatives\n  (whether or not created by Mistral AI) and Outputs for Research Purposes.\n\n  ## 4. Intellectual Property\n\n  **4.1. Trademarks.** No trademark licenses are granted under this Agreement,\n  and in connection with the Mistral Models, You may not use any name or mark\n  owned by or associated with Mistral AI or any of its affiliates, except (i) as\n  required for reasonable and customary use in describing and Distributing the\n  Mistral Models and Derivatives made by or for Mistral AI and (ii) for\n  attribution purposes as required by this Agreement.\n\n  **4.2. Outputs.** We claim no ownership rights in and to the Outputs. You are\n  solely responsible for the Outputs You generate and their subsequent uses in\n  accordance with this Agreement. Any Outputs shall be subject to the\n  restrictions set out in Section 3 of this Agreement.\n\n  **4.3. Derivatives.** By entering into this Agreement, You accept that any\n  Derivatives that You may create or that may be created for You shall be\n  subject to the restrictions set out in Section 3 of this Agreement.\n\n  ## 5. Liability\n\n  **5.1. Limitation of liability.** In no event, unless required by applicable\n  law (such as deliberate and grossly negligent acts) or agreed to in writing,\n  shall Mistral AI be liable to You for damages, including any direct, indirect,\n  special, incidental, or consequential damages of any character arising as a\n  result of this Agreement or out of the use or inability to use the Mistral\n  Models and Derivatives (including but not limited to damages for loss of data,\n  loss of goodwill, loss of expected profit or savings, work stoppage, computer\n  failure or malfunction, or any damage caused by malware or security breaches),\n  even if  Mistral AI has been advised of the possibility of such damages.\n\n  **5.2. Indemnification.** You agree to indemnify and hold harmless Mistral AI\n  from and against any claims, damages, or losses arising out of or related to\n  Your use or Distribution of the Mistral Models and Derivatives.\n\n  ## 6. Warranty\n\n  **6.1. Disclaimer.** Unless required by applicable law or prior agreed to by\n  Mistral AI in writing, Mistral AI provides the Mistral Models and Derivatives\n  on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n  express or implied, including, without limitation, any warranties or\n  conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n  PARTICULAR PURPOSE. Mistral AI does not represent nor warrant that the Mistral\n  Models and Derivatives will be error-free, meet Your or any third party''s\n  requirements, be secure or will allow You or any third party to achieve any\n  kind of result or generate any kind of content. You are solely responsible for\n  determining the appropriateness of using or Distributing the Mistral Models\n  and Derivatives and assume any risks associated with Your exercise of rights\n  under this Agreement.\n\n  ## 7. Termination\n\n  **7.1. Term.** This Agreement is effective as of the date of your acceptance\n  of this Agreement or access to the concerned Mistral Models or Derivatives and\n  will continue until terminated in accordance with the following terms.\n\n  **7.2. Termination.** Mistral AI may terminate this Agreement at any time if\n  You are in breach of this Agreement. Upon termination of this Agreement, You\n  must cease to use all Mistral Models and Derivatives and shall permanently\n  delete any copy thereof. The following provisions, in their relevant parts,\n  will survive any termination or expiration of this Agreement, each for the\n  duration necessary to achieve its own intended purpose (e.g. the liability\n  provision will survive until the end of the applicable limitation\n  period):Sections 5 (Liability), 6(Warranty), 7 (Termination) and 8 (General\n  Provisions).\n\n  **7.3. Litigation.** If You initiate any legal action or proceedings against\n  Us or any other entity (including a cross-claim or counterclaim in a lawsuit),\n  alleging that the Model or a Derivative, or any part thereof, infringe upon\n  intellectual property or other rights owned or licensable by You, then any\n  licenses granted to You under this Agreement will immediately terminate as of\n  the date such legal action or claim is filed or initiated.\n\n  ## 8. General provisions\n\n  **8.1. Governing laws.** This Agreement will be governed by the laws of\n  France, without regard to choice of law principles, and the UN Convention on\n  Contracts for the International Sale of Goods does not apply to this\n  Agreement.\n\n  **8.2. Competent jurisdiction.** The courts of Paris shall have exclusive\n  jurisdiction of any dispute arising out of this Agreement.\n\n  **8.3. Severability.** If any provision of this Agreement is held to be\n  invalid, illegal or unenforceable, the remaining provisions shall be\n  unaffected thereby and remain valid as if such provision had not been set\n  forth herein.\n\n  ## 9. Definitions\n\n  "Agreement": means this Mistral AI Research License agreement governing the\n  access, use, and Distribution of the Mistral Models, Derivatives and Outputs.\n\n  "Derivative": means any (i) modified version of the Mistral Model (including\n  but not limited to any customized or fine-tuned version thereof), (ii) work\n  based on the Mistral Model, or (iii) any other derivative work thereof.\n\n  "Distribution", "Distributing", "Distribute" or "Distributed": means\n  supplying, providing or making available, by any means, a copy of the Mistral\n  Models and/or the Derivatives as the case may be, subject to Section 3 of this\n  Agreement.\n\n  "Mistral AI", "We" or "Us": means Mistral AI, a French sociÃ©tÃ© par actions\n  simplifiÃ©e registered in the Paris commercial registry under the number 952\n  418 325, and having its registered seat at 15, rue des Halles, 75001 Paris.\n\n  "Mistral Model": means the foundational large language model(s), and its\n  elements which include algorithms, software, instructed checkpoints,\n  parameters, source code (inference code, evaluation code and, if applicable,\n  fine-tuning code) and any other elements associated thereto made available by\n  Mistral AI under this Agreement, including, if any, the technical\n  documentation, manuals and instructions for the use and operation thereof.\n\n  "Research Purposes": means any use of a Mistral Model,  Derivative, or Output\n  that is solely for (a) personal, scientific or academic research, and (b) for\n  non-profit and non-commercial purposes, and not directly or indirectly\n  connected to any commercial activities or business operations. For\n  illustration purposes, Research Purposes does not include (1) any usage of the\n  Mistral Model, Derivative or Output by individuals or contractors employed in\n  or engaged by companies in the context of (a) their daily tasks, or (b) any\n  activity (including but not limited to any testing or proof-of-concept) that\n  is intended to generate revenue, nor (2) any Distribution by a commercial\n  entity of the Mistral Model, Derivative or Output whether in return for\n  payment or free of charge, in any medium or form, including but not limited to\n  through a hosted or managed service (e.g. SaaS, cloud instances, etc.), or\n  behind a software layer.\n\n  "Outputs": means any content generated by the operation of the Mistral Models\n  or the Derivatives from  a prompt (i.e., text instructions) provided by users.\n  For the avoidance of doubt, Outputs do not include any components of a Mistral\n  Models, such as any fine-tuned versions of the Mistral Models, the weights, or\n  parameters.\n\n  "You": means the individual or entity entering into this Agreement with\n  Mistral AI.\n\n\n  *Mistral AI processes your personal data below to provide the model and\n  enforce its license. If you are affiliated with a commercial entity, we may\n  also send you communications about our models. For more information on your\n  rights and data handling, please see our <a\n  href="https://mistral.ai/terms/">privacy policy</a>.*\nextra_gated_fields:\n  First Name: text\n  Last Name: text\n  Country: country\n  Affiliation: text\n  Job title: text\n  I understand that I can only use the model, any derivative versions and their outputs for non-commercial research purposes: checkbox\n  I understand that if I am a commercial entity, I am not permitted to use or distribute the model internally or externally, or expose it in my own offerings without a commercial license: checkbox\n  I understand that if I upload the model, or any derivative version, on any platform, I must include the Mistral Research License: checkbox\n  I understand that for commercial use of the model, I can contact Mistral or use the Mistral AI API on la Plateforme or any of our cloud provider partners: checkbox\n  By clicking Submit below I accept the terms of the license and acknowledge that the information I provide will be collected stored processed and shared in accordance with the Mistral Privacy Policy: checkbox\n  geo: ip_location\nextra_gated_description: >-\n  Mistral AI processes your personal data below to provide the model and enforce\n  its license. If you are affiliated with a commercial entity, we may also send\n  you communications about our models. For more information on your rights and\n  data handling, please see our <a href="https://mistral.ai/terms/">privacy\n  policy</a>.\nextra_gated_button_content: Submit\ntags:\n- mistral-common\n---\n\n# Model Card for Ministral-8B-Instruct-2410\n\nWe introduce two new state-of-the-art models for local intelligence, on-device computing, and at-the-edge use cases. We call them les Ministraux: Ministral 3B and Ministral 8B. \n\nThe Ministral-8B-Instruct-2410 Language Model is an instruct fine-tuned model significantly outperforming existing models of similar size, released under the Mistral Research License.\n\nIf you are interested in using Ministral-3B or Ministral-8B commercially, outperforming Mistral-7B, [reach out to us](https://mistral.ai/contact/).\n\nFor more details about les Ministraux please refer to our release [blog post](https://mistral.ai/news/ministraux).\n\n## Ministral 8B Key features\n- Released under the **Mistral Research License**, reach out to us for a commercial license\n- Trained with a **128k context window** with **interleaved sliding-window attention**\n- Trained on a large proportion of **multilingual and code data**\n- Supports **function calling**\n- Vocabulary size of **131k**, using the **V3-Tekken** tokenizer\n\n### Basic Instruct Template (V3-Tekken)\n\n```\n<s>[INST]user message[/INST]assistant response</s>[INST]new user message[/INST]\n```\n\n*For more information about the tokenizer please refer to [mistral-common](https://github.com/mistralai/mistral-common)*\n\n## Ministral 8B Architecture\n\n| Feature               | Value                |\n|:---------------------:|:--------------------:|\n| **Architecture**      | Dense Transformer    |\n| **Parameters**        | 8,019,808,256        |\n| **Layers**            | 36                   |\n| **Heads**             | 32                   |\n| **Dim**               | 4096                 |\n| **KV Heads (GQA)**    | 8                    |\n| **Hidden Dim**        | 12288                |\n| **Head Dim**          | 128                  |\n| **Vocab Size**        | 131,072              |\n| **Context Length**    | 128k                 |\n| **Attention Pattern** | Ragged (128k,32k,32k,32k) |\n\n## Benchmarks\n\n#### Base Models\n\n<u>Knowledge & Commonsense</u>\n\n| Model       | MMLU | AGIEval | Winogrande | Arc-c | TriviaQA |\n|:-------------:|:------:|:---------:|:------------:|:-------:|:----------:|\n| Mistral 7B Base  | 62.5 | 42.5    | 74.2   | 67.9  | 62.5 |\n| Llama 3.1 8B Base | 64.7 | 44.4    | 74.6       | 46.0  | 60.2     |\n| ***Ministral 8B Base*** | ***<u>65.0</u>*** | ***<u>48.3</u>*** | ***<u>75.3</u>***   | ***<u>71.9</u>*** | ***<u>65.5</u>*** |\n|  |  |     |        |   |      |\n| Gemma 2 2B Base | 52.4 | 33.8    | 68.7   | 42.6  | 47.8     |\n| Llama 3.2 3B Base | 56.2 | 37.4    | 59.6       | 43.1  | 50.7     |\n| ***Ministral 3B Base*** | ***<u>60.9</u>*** | ***<u>42.1</u>***    | ***<u>72.7</u>***       | ***<u>64.2</u>*** | ***<u>56.7</u>***     |\n\n<u>Code & Math</u>\n\n| Model       | HumanEval pass@1 |GSM8K maj@8 |\n|:-------------:|:-------------------:|:---------------:|\n| Mistral 7B Base  | 26.8              | 32.0           |\n| Llama 3.1 8B Base | ***<u>37.8</u>***          | 42.2           |\n| ***Ministral 8B Base***  | 34.8              | ***<u>64.5</u>***       |\n|   |               |            |\n| Gemma 2 2B  | 20.1              | 35.5           |\n| Llama 3.2 3B | 14.6              | 33.5           |\n| ***Ministral 3B*** | ***<u>34.2</u>***          | ***<u>50.9</u>***       |\n\n<u>Multilingual</u>\n\n| Model       | French MMLU | German MMLU | Spanish MMLU |\n|:-------------:|:-------------:|:-------------:|:-------------:|\n| Mistral 7B Base  | 50.6         | 49.6         | 51.4         |\n| Llama 3.1 8B Base | 50.8         | 52.8         | 54.6         |\n| ***Ministral 8B Base*** | ***<u>57.5</u>***     | ***<u>57.4</u>***     | ***<u>59.6</u>***     |\n|   |          |          |          |\n| Gemma 2 2B Base  | 41.0         | 40.1         | 41.7         |\n| Llama 3.2 3B Base | 42.3         | 42.2         | 43.1         |\n| ***Ministral 3B Base*** | ***<u>49.1</u>***     | ***<u>48.3</u>***     | ***<u>49.5</u>***     |\n\n### Instruct Models\n\n<u>Chat/Arena (gpt-4o judge)</u>\n\n| Model       | MTBench | Arena Hard | Wild bench |\n|:-------------:|:---------:|:------------:|:------------:|\n| Mistral 7B Instruct v0.3  | 6.7     | 44.3       | 33.1       |\n| Llama 3.1 8B Instruct | 7.5     | 62.4       | 37.0       |\n| Gemma 2 9B Instruct | 7.6     | 68.7       | ***<u>43.8</u>***       |\n| ***Ministral 8B Instruct*** | ***<u>8.3</u>*** | ***<u>70.9</u>***   | 41.3   |\n|   |      |        |        |\n| Gemma 2 2B Instruct  | 7.5     | 51.7       | 32.5       |\n| Llama 3.2 3B Instruct | 7.2     | 46.0       | 27.2       |\n| ***Ministral 3B Instruct*** | ***<u>8.1</u>*** | ***<u>64.3</u>***   | ***<u>36.3</u>***   |\n\n<u>Code & Math</u>\n\n| Model       | MBPP pass@1 | HumanEval pass@1 | Math maj@1 |\n|:-------------:|:-------------:|:------------------:|:-------------:|\n| Mistral 7B Instruct v0.3  | 50.2        | 38.4             | 13.2        |\n| Gemma 2 9B Instruct | 68.5   | 67.7             | 47.4        |\n Llama 3.1 8B Instruct | 69.7   | 67.1             | 49.3        |\n| ***Ministral 8B Instruct*** | ***<u>70.0</u>***        | ***<u>76.8</u>***         | ***<u>54.5</u>***   |\n|   |         |              |         |\n| Gemma 2 2B Instruct  | 54.5        | 42.7             | 22.8        |\n| Llama 3.2 3B Instruct | 64.6        | 61.0             | 38.4        |\n| ***Ministral 3B* Instruct** | ***<u>67.7</u>***   | ***<u>77.4</u>***         | ***<u>51.7</u>***   |\n\n<u>Function calling</u>\n\n| Model       | Internal bench |\n|:-------------:|:-----------------:|\n| Mistral 7B Instruct v0.3  | 6.9             |\n| Llama 3.1 8B Instruct | N/A             |\n| Gemma 2 9B Instruct | N/A             |\n| ***Ministral 8B Instruct*** | ***<u>31.6</u>***       |\n|   |              |\n| Gemma 2 2B Instruct  | N/A             |\n| Llama 3.2 3B Instruct | N/A             |\n| ***Ministral 3B Instruct*** | ***<u>28.4</u>***       |\n\n## Usage Examples\n\n### vLLM (recommended)\n\nWe recommend using this model with the [vLLM library](https://github.com/vllm-project/vllm)\nto implement production-ready inference pipelines.\n\n> [!IMPORTANT]\n> Currently vLLM is capped at 32k context size because interleaved attention kernels for paged attention are not yet implemented in vLLM.\n> Attention kernels for paged attention are being worked on and as soon as it is fully supported in vLLM, this model card will be updated.\n> To take advantage of the full 128k context size we recommend [Mistral Inference](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410#mistral-inference)\n\n**_Installation_**\n\n\nMake sure you install `vLLM >= v0.6.4`:\n\n```\npip install --upgrade vllm\n```\n\nAlso make sure you have `mistral_common >= 1.4.4` installed:\n\n```\npip install --upgrade mistral_common\n```\n\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/Dockerfile).\n\n**_Offline_**\n\n```py\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\n\nmodel_name = "mistralai/Ministral-8B-Instruct-2410"\n\nsampling_params = SamplingParams(max_tokens=8192)\n\n# note that running Ministral 8B on a single GPU requires 24 GB of GPU RAM\n# If you want to divide the GPU requirement over multiple devices, please add *e.g.* `tensor_parallel=2`\nllm = LLM(model=model_name, tokenizer_mode="mistral", config_format="mistral", load_format="mistral")\n\nprompt = "Do we need to think for 10 seconds to find the answer of 1 + 1?"\n\nmessages = [\n    {\n        "role": "user",\n        "content": prompt\n    },\n]\n\noutputs = llm.chat(messages, sampling_params=sampling_params)\n\nprint(outputs[0].outputs[0].text)\n# You don''t need to think for 10 seconds to find the answer to 1 + 1. The answer is 2,\n# and you can easily add these two numbers in your mind very quickly without any delay.\n```\n\n**_Server_**\n\nYou can also use Ministral-8B in a server/client setting. \n\n1. Spin up a server:\n\n\n```\nvllm serve mistralai/Ministral-8B-Instruct-2410 --tokenizer_mode mistral --config_format mistral --load_format mistral\n```\n\n**Note:** Running Ministral-8B on a single GPU requires 24 GB of GPU RAM. \n\nIf you want to divide the GPU requirement over multiple devices, please add *e.g.* `--tensor_parallel=2`\n\n2. And ping the client:\n\n```\ncurl --location ''http://<your-node-url>:8000/v1/chat/completions'' \\n--header ''Content-Type: application/json'' \\n--header ''Authorization: Bearer token'' \\n--data ''{\n    "model": "mistralai/Ministral-8B-Instruct-2410",\n    "messages": [\n      {\n        "role": "user",\n        "content": "Do we need to think for 10 seconds to find the answer of 1 + 1?"\n      }\n    ]\n}''\n\n```\n\n### Mistral-inference\n\nWe recommend using [mistral-inference](https://github.com/mistralai/mistral-inference) to quickly try out / "vibe-check" the model.\n\n\n**_Install_**\n\nMake sure to have `mistral_inference >= 1.5.0` installed.\n\n```\npip install mistral_inference --upgrade\n```\n\n**_Download_**\n\n```py\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath(''mistral_models'', ''8B-Instruct'')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id="mistralai/Ministral-8B-Instruct-2410", allow_patterns=["params.json", "consolidated.safetensors", "tekken.json"], local_dir=mistral_models_path)\n```\n\n### Chat\n\nAfter installing `mistral_inference`, a `mistral-chat` CLI command should be available in your environment. You can chat with the model using\n\n```\nmistral-chat $HOME/mistral_models/8B-Instruct --instruct --max_tokens 256\n```\n\n### Passkey detection\n\n> [!IMPORTANT]\n> In this example the passkey message has over >100k tokens and mistral-inference\n> does not have a chunked pre-fill mechanism. Therefore you will need a lot of\n> GPU memory in order to run the below example (80 GB). For a more memory-efficient\n> solution we recommend using vLLM.\n\n```py\nfrom mistral_inference.transformer import Transformer\nfrom pathlib import Path\nimport json\nfrom mistral_inference.generate import generate\nfrom huggingface_hub import hf_hub_download\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\ndef load_passkey_request() -> ChatCompletionRequest:\n    passkey_file = hf_hub_download(repo_id="mistralai/Ministral-8B-Instruct-2410", filename="passkey_example.json")\n\n    with open(passkey_file, "r") as f:\n        data = json.load(f)\n\n    message_content = data["messages"][0]["content"]\n    return ChatCompletionRequest(messages=[UserMessage(content=message_content)])\n\ntokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tekken.json")\nmodel = Transformer.from_folder(mistral_models_path, softmax_fp32=False)\n\ncompletion_request = load_passkey_request()\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)  # The pass key is 13005.\n```\n\n\n### Instruct following\n\n```py\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\n\ntokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tekken.json")\nmodel = Transformer.from_folder(mistral_models_path)\n\ncompletion_request = ChatCompletionRequest(messages=[UserMessage(content="How often does the letter r occur in Mistral?")])\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n### Function calling\n\n```py\nfrom mistral_common.protocol.instruct.tool_calls import Function, Tool\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.tekken import SpecialTokenPolicy\n\n\ntokenizer = MistralTokenizer.from_file(f"{mistral_models_path}/tekken.json")\ntekken = tokenizer.instruct_tokenizer.tokenizer\ntekken.special_token_policy = SpecialTokenPolicy.IGNORE\n\nmodel = Transformer.from_folder(mistral_models_path)\n\ncompletion_request = ChatCompletionRequest(\n    tools=[\n        Tool(\n            function=Function(\n                name="get_current_weather",\n                description="Get the current weather",\n                parameters={\n                    "type": "object",\n                    "properties": {\n                        "location": {\n                            "type": "string",\n                            "description": "The city and state, e.g. San Francisco, CA",\n                        },\n                        "format": {\n                            "type": "string",\n                            "enum": ["celsius", "fahrenheit"],\n                            "description": "The temperature unit to use. Infer this from the users location.",\n                        },\n                    },\n                    "required": ["location", "format"],\n                },\n            )\n        )\n    ],\n    messages=[\n        UserMessage(content="What''s the weather like today in Paris?"),\n        ],\n)\n\ntokens = tokenizer.encode_chat_completion(completion_request).tokens\n\nout_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\nresult = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n\nprint(result)\n```\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Abou Chahine, Alexandre Sablayrolles, Alexis Tacnet, Alodie Boissonnet, Alok Kothari, AmÃ©lie HÃ©liou, Andy Lo, Anna Peronnin, Antoine Meunier, Antoine Roux, Antonin Faure, Aritra Paul, Arthur Darcet, Arthur Mensch, Audrey Herblin-Stoop, Augustin Garreau, Austin Birky, Avinash Sooriyarachchi, Baptiste RoziÃ¨re, Barry Conklin, Bastien Bouillon, Blanche Savary de Beauregard, Carole Rambaud, Caroline Feldman, Charles de Freminville, Charline Mauro, Chih-Kuan Yeh, Chris Bamford, Clement Auguy, Corentin Heintz, Cyriaque Dubois, Devendra Singh Chaplot, Diego Las Casas, Diogo Costa, ElÃ©onore Arcelin, Emma Bou Hanna, Etienne Metzger, Fanny Olivier Autran, Francois Lesage, Garance Gourdel, Gaspard Blanchet, Gaspard Donada Vidal, Gianna Maria Lengyel, Guillaume Bour, Guillaume Lample, Gustave Denis, Harizo Rajaona, Himanshu Jaju, Ian Mack, Ian Mathew, Jean-Malo Delignon, Jeremy Facchetti, Jessica Chudnovsky, Joachim Studnia, Justus Murke, Kartik Khandelwal, Kenneth Chiu, Kevin Riera, Leonard Blier, Leonard Suslian, Leonardo Deschaseaux, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Sophia Yang, Margaret Jennings, Marie Pellat, Marie Torelli, Marjorie Janiewicz, Mathis Felardos, Maxime Darrin, Michael Hoff, MickaÃ«l Seznec, Misha Jessel Kenyon, Nayef Derwiche, Nicolas Carmont Zaragoza, Nicolas Faurie, Nicolas Moreau, Nicolas Schuhl, Nikhil Raghuraman, Niklas Muhs, Olivier de Garrigues, Patricia RozÃ©, Patricia Wang, Patrick von Platen, Paul Jacob, Pauline Buche, Pavankumar Reddy Muddireddy, Perry Savas, Pierre Stock, Pravesh Agrawal, Renaud de Peretti, Romain Sauvestre, Romain Sinthe, Roman Soletskyi, Sagar Vaze, Sandeep Subramanian, Saurabh Garg, Soham Ghosh, Sylvain Regnier, Szymon Antoniak, Teven Le Scao, Theophile Gervet, Thibault Schueller, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, Valeriia Nemychnikova, Wendy Shang, William El Sayed, William Marshall', '{"pipeline_tag":null,"library_name":"vllm","framework":"vllm","params":8019808256,"storage_bytes":32111185775,"files_count":16,"spaces_count":17,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","chat_template":"{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- bos_token }}\n{%- for message in loop_messages %}\n    {%- if message[\"role\"] == \"user\" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \"[AVAILABLE_TOOLS][\" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- ''{\"type\": \"function\", \"function\": {'' }}\n                {%- for key, val in tool.items() if key != \"return\" %}\n                    {%- if val is string %}\n                        {{- ''\"'' + key + ''\": \"'' + val + ''\"'' }}\n                    {%- else %}\n                        {{- ''\"'' + key + ''\": '' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \", \" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \"}}\" }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- else %}\n                    {{- \"]\" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \"[/AVAILABLE_TOOLS]\" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \"[INST]\" + system_message + \"\\n\\n\" + message[\"content\"] + \"[/INST]\" }}\n        {%- else %}\n            {{- \"[INST]\" + message[\"content\"] + \"[/INST]\" }}\n        {%- endif %}\n    {%- elif (message.tool_calls is defined and message.tool_calls is not none) %}\n        {{- \"[TOOL_CALLS][\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n            {%- endif %}\n            {{- '', \"id\": \"'' + tool_call.id + ''\"}'' }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]\" + eos_token }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\"role\"] == \"assistant\" %}\n        {{- message[\"content\"] + eos_token}}\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- ''[TOOL_RESULTS]{\"content\": '' + content|string + \", \" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n        {%- endif %}\n        {{- ''\"call_id\": \"'' + message.tool_call_id + ''\"}[/TOOL_RESULTS]'' }}\n    {%- else %}\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n    {%- endif %}\n{%- endfor %}\n","eos_token":"</s>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:mistralai:mistral-common","source_url":"https://github.com/mistralai/mistral-common"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:mistralai:mistral-inference","source_url":"https://github.com/mistralai/mistral-inference"}]', NULL, 'Other', 'approved', 77.5, '0517519afb09e825b8ade511f0012a6f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-HuggingFaceTB-SmolVLM-Instruct', 'huggingface--huggingfacetb--smolvlm-instruct', 'SmolVLM-Instruct', 'HuggingFaceTB', '--- library_name: transformers license: apache-2.0 datasets: - HuggingFaceM4/the_cauldron - HuggingFaceM4/Docmatix pipeline_tag: image-text-to-text language: - en base_model: - HuggingFaceTB/SmolLM2-1.7B-Instruct - google/siglip-so400m-patch14-384 --- <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/SmolVLM.png" width="800" height="auto" alt="Image description"> SmolVLM is a compact open multimodal model that accepts arbitrary sequences of image and text...', '["transformers","onnx","safetensors","idefics3","image-to-text","image-text-to-text","conversational","en","dataset:huggingfacem4/the_cauldron","dataset:huggingfacem4/docmatix","arxiv:2504.05299","base_model:huggingfacetb/smollm2-1.7b-instruct","license:apache-2.0","endpoints_compatible","region:us"]', 'image-text-to-text', 562, 51141, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlibrary_name: transformers\nlicense: apache-2.0\ndatasets:\n- HuggingFaceM4/the_cauldron\n- HuggingFaceM4/Docmatix\npipeline_tag: image-text-to-text\nlanguage:\n- en\nbase_model:\n- HuggingFaceTB/SmolLM2-1.7B-Instruct\n- google/siglip-so400m-patch14-384\n---\n\n<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/SmolVLM.png" width="800" height="auto" alt="Image description">\n\n# SmolVLM\n\nSmolVLM is a compact open multimodal model that accepts arbitrary sequences of image and text inputs to produce text outputs. Designed for efficiency, SmolVLM can answer questions about images, describe visual content, create stories grounded on multiple images, or function as a pure language model without visual inputs. Its lightweight architecture makes it suitable for on-device applications while maintaining strong performance on multimodal tasks.\n\n## Model Summary\n\n- **Developed by:** Hugging Face ðŸ¤—\n- **Model type:** Multi-modal model (image+text)\n- **Language(s) (NLP):** English\n- **License:** Apache 2.0\n- **Architecture:** Based on [Idefics3](https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3) (see technical summary)\n\n## Resources\n\n- **Demo:** [SmolVLM Demo](https://huggingface.co/spaces/HuggingFaceTB/SmolVLM)\n- **Blog:** [Blog post](https://huggingface.co/blog/smolvlm)\n\n## Uses\n\nSmolVLM can be used for inference on multimodal (image + text) tasks where the input comprises text queries along with one or more images. Text and images can be interleaved arbitrarily, enabling tasks like image captioning, visual question answering, and storytelling based on visual content. The model does not support image generation.\n\nTo fine-tune SmolVLM on a specific task, you can follow the fine-tuning tutorial.\n<!-- todo: add link to fine-tuning tutorial -->\n\n### Technical Summary\n\nSmolVLM leverages the lightweight SmolLM2 language model to provide a compact yet powerful multimodal experience. It introduces several changes compared to previous Idefics models:\n\n- **Image compression:** We introduce a more radical image compression compared to Idefics3 to enable the model to infer faster and use less RAM.\n- **Visual Token Encoding:** SmolVLM uses 81 visual tokens to encode image patches of size 384Ã—384. Larger images are divided into patches, each encoded separately, enhancing efficiency without compromising performance.\n\nMore details about the training and architecture are available in our technical report.\n\n\n### How to get started\n\nYou can use transformers to load, infer and fine-tune SmolVLM.\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom transformers.image_utils import load_image\n\nDEVICE = "cuda" if torch.cuda.is_available() else "cpu"\n\n# Load images\nimage1 = load_image("https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg")\nimage2 = load_image("https://huggingface.co/spaces/merve/chameleon-7b/resolve/main/bee.jpg")\n\n# Initialize processor and model\nprocessor = AutoProcessor.from_pretrained("HuggingFaceTB/SmolVLM-Instruct")\nmodel = AutoModelForVision2Seq.from_pretrained(\n    "HuggingFaceTB/SmolVLM-Instruct",\n    torch_dtype=torch.bfloat16,\n    _attn_implementation="flash_attention_2" if DEVICE == "cuda" else "eager",\n).to(DEVICE)\n\n# Create input messages\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image"},\n            {"type": "image"},\n            {"type": "text", "text": "Can you describe the two images?"}\n        ]\n    },\n]\n\n# Prepare inputs\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image1, image2], return_tensors="pt")\ninputs = inputs.to(DEVICE)\n\n# Generate outputs\ngenerated_ids = model.generate(**inputs, max_new_tokens=500)\ngenerated_texts = processor.batch_decode(\n    generated_ids,\n    skip_special_tokens=True,\n)\n\nprint(generated_texts[0])\n"""\nAssistant: The first image shows a green statue of the Statue of Liberty standing on a stone pedestal in front of a body of water. \nThe statue is holding a torch in its right hand and a tablet in its left hand. The water is calm and there are no boats or other objects visible. \nThe sky is clear and there are no clouds. The second image shows a bee on a pink flower. \nThe bee is black and yellow and is collecting pollen from the flower. The flower is surrounded by green leaves.\n"""\n```\n\n\n### Model optimizations\n\n**Precision**: For better performance, load and run the model in half-precision (`torch.float16` or `torch.bfloat16`) if your hardware supports it.\n\n```python\nfrom transformers import AutoModelForVision2Seq\nimport torch\n\nmodel = AutoModelForVision2Seq.from_pretrained(\n    "HuggingFaceTB/SmolVLM-Instruct",\n    torch_dtype=torch.bfloat16\n).to("cuda")\n```\n\nYou can also load SmolVLM with 4/8-bit quantization using bitsandbytes, torchao or Quanto. Refer to [this page](https://huggingface.co/docs/transformers/en/main_classes/quantization) for other options.\n\n```python\nfrom transformers import AutoModelForVision2Seq, BitsAndBytesConfig\nimport torch\n\nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\nmodel = AutoModelForVision2Seq.from_pretrained(\n    "HuggingFaceTB/SmolVLM-Instruct",\n    quantization_config=quantization_config,\n)\n```\n\n**Vision Encoder Efficiency**: Adjust the image resolution by setting `size={"longest_edge": N*384}` when initializing the processor, where N is your desired value. The default `N=4` works well, which results in input images of\nsize 1536Ã—1536. For documents, `N=5` might be beneficial. Decreasing N can save GPU memory and is appropriate for lower-resolution images. This is also useful if you want to fine-tune on videos.\n\n\n## Misuse and Out-of-scope Use\n\nSmolVLM is not intended for high-stakes scenarios or critical decision-making processes that affect an individual''s well-being or livelihood. The model may produce content that appears factual but may not be accurate. Misuse includes, but is not limited to:\n\n- Prohibited Uses:\n  - Evaluating or scoring individuals (e.g., in employment, education, credit)\n  - Critical automated decision-making\n  - Generating unreliable factual content\n- Malicious Activities:\n  - Spam generation\n  - Disinformation campaigns\n  - Harassment or abuse\n  - Unauthorized surveillance\n\n### License\n\nSmolVLM is built upon [the shape-optimized SigLIP](https://huggingface.co/google/siglip-so400m-patch14-384) as image encoder and [SmolLM2](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct) for text decoder part.\n\nWe release the SmolVLM checkpoints under the Apache 2.0 license.\n\n## Training Details\n\n### Training Data\n\nThe training data comes from [The Cauldron](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron) and [Docmatix](https://huggingface.co/datasets/HuggingFaceM4/Docmatix) datasets, with emphasis on document understanding (25%) and image captioning (18%), while maintaining balanced coverage across other crucial capabilities like visual reasoning, chart comprehension, and general instruction following.\n<img src="https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct/resolve/main/mixture_the_cauldron.png" alt="Example Image" style="width:90%;" />\n\n\n\n\n## Evaluation\n\n| Model             | MMMU (val) | MathVista (testmini) | MMStar (val) | DocVQA (test) | TextVQA (val) | Min GPU RAM required (GB) |\n|-------------------|------------|----------------------|--------------|---------------|---------------|---------------------------|\n| SmolVLM           | 38.8       | 44.6                | 42.1         | 81.6          | 72.7          | 5.02                      |\n| Qwen-VL 2B        | 41.1       | 47.8                | 47.5         | 90.1          | 79.7          | 13.70                     |\n| InternVL2 2B      | 34.3       | 46.3                | 49.8         | 86.9          | 73.4          | 10.52                     |\n| PaliGemma 3B 448px| 34.9       | 28.7                | 48.3         | 32.2          | 56.0          | 6.72                      |\n| moondream2        | 32.4       | 24.3                | 40.3         | 70.5          | 65.2          | 3.87                      |\n| MiniCPM-V-2       | 38.2       | 39.8                | 39.1         | 71.9          | 74.1          | 7.88                      |\n| MM1.5 1B          | 35.8       | 37.2                | 0.0          | 81.0          | 72.5          | NaN                       |\n\n# Citation information\nYou can cite us in the following way:\n```bibtex\n@article{marafioti2025smolvlm,\n  title={SmolVLM: Redefining small and efficient multimodal models}, \n  author={AndrÃ©s Marafioti and Orr Zohar and Miquel FarrÃ© and Merve Noyan and Elie Bakouch and Pedro Cuenca and Cyril Zakka and Loubna Ben Allal and Anton Lozhkov and Nouamane Tazi and Vaibhav Srivastav and Joshua Lochner and Hugo Larcher and Mathieu Morlon and Lewis Tunstall and Leandro von Werra and Thomas Wolf},\n  journal={arXiv preprint arXiv:2504.05299},\n  year={2025}\n}\n```', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":2246272880,"storage_bytes":45793562188,"files_count":43,"spaces_count":33,"gated":false,"private":false,"config":{"architectures":["Idefics3ForConditionalGeneration"],"model_type":"idefics3","processor_config":{"chat_template":"<|im_start|>{% for message in messages %}{{message[''role''] | capitalize}}{% if message[''content''][0][''type''] == ''image'' %}{{'':''}}{% else %}{{'': ''}}{% endif %}{% for line in message[''content''] %}{% if line[''type''] == ''text'' %}{{line[''text'']}}{% elif line[''type''] == ''image'' %}{{ ''<image>'' }}{% endif %}{% endfor %}<end_of_utterance>\n{% endfor %}{% if add_generation_prompt %}{{ ''Assistant:'' }}{% endif %}"},"tokenizer_config":{"bos_token":"<|im_start|>","eos_token":"<end_of_utterance>","pad_token":"<|im_end|>","chat_template":"<|im_start|>{% for message in messages %}{{message[''role''] | capitalize}}{% if message[''content''][0][''type''] == ''image'' %}{{'':''}}{% else %}{{'': ''}}{% endif %}{% for line in message[''content''] %}{% if line[''type''] == ''text'' %}{{line[''text'']}}{% elif line[''type''] == ''image'' %}{{ ''<image>'' }}{% endif %}{% endfor %}<end_of_utterance>\n{% endfor %}{% if add_generation_prompt %}{{ ''Assistant:'' }}{% endif %}","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2504.05299","source_url":"https://arxiv.org/abs/2504.05299"}]', NULL, 'Apache-2.0', 'approved', 62.5, '5414b7b0827faabc007108779722d231', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-databricks-dbrx-base', 'huggingface--databricks--dbrx-base', 'dbrx-base', 'databricks', '', '["transformers","safetensors","dbrx","text-generation","moe","conversational","arxiv:2211.15841","arxiv:2304.11277","license:other","text-generation-inference","region:us"]', 'text-generation', 561, 5, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/databricks/dbrx-base","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":131596523520,"storage_bytes":263193089336,"files_count":73,"spaces_count":15,"gated":"manual","private":false,"config":{"architectures":["DbrxForCausalLM"],"model_type":"dbrx","tokenizer_config":{"bos_token":"<|endoftext|>","chat_template":"{% if messages[0][''role''] == ''system'' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][''content''] %}{% elif ''system'' not in messages[0][''role''] %}{% set loop_messages = messages %}{% set system_message = ''You are DBRX, created by Databricks. You were last updated in December 2023. You answer questions based on information available up to that point.\nYOU PROVIDE SHORT RESPONSES TO SHORT QUESTIONS OR STATEMENTS, but provide thorough responses to more complex and open-ended questions.\nYou assist with various tasks, from writing to coding (using markdown for code blocks â€” remember to use ``` with code, JSON, and tables).\n(You do not have real-time data access or code execution capabilities. You avoid stereotyping and provide balanced perspectives on controversial topics. You do not provide song lyrics, poems, or news articles and do not divulge details of your training data.)\nThis is your system prompt, guiding your responses. Do not reference it, just respond to the user. If you find yourself talking about this message, stop. You should be responding appropriately and usually that means not mentioning this.\nYOU DO NOT MENTION ANY OF THIS INFORMATION ABOUT YOURSELF UNLESS THE INFORMATION IS DIRECTLY PERTINENT TO THE USER\\''S QUERY.'' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{% if system_message != false %}{{ ''<|im_start|>system\n'' + system_message | trim + ''<|im_end|>\n''}}{% endif %}{{ ''<|im_start|>'' + message[''role''] + ''\n'' + message[''content''] + ''<|im_end|>'' }}{% else %}{{ ''\n'' + ''<|im_start|>'' + message[''role''] + ''\n'' + message[''content''] + ''<|im_end|>'' }}{% endif %}{% if (add_generation_prompt == true and loop.last) %}{{ ''\n'' + ''<|im_start|>'' + ''assistant'' + ''\n'' }}{% endif %}{% endfor %}","eos_token":"<|endoftext|>","pad_token":"<|pad|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2211.15841","source_url":"https://arxiv.org/abs/2211.15841"},{"type":"based_on_paper","target_id":"arxiv:2304.11277","source_url":"https://arxiv.org/abs/2304.11277"}]', NULL, 'Other', 'approved', 37.5, '4232b6c9fd06db21115385a568b58e3d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-flan-ul2', 'huggingface--google--flan-ul2', 'flan-ul2', 'google', '--- language: - en - fr - ro - de - multilingual widget: - text: ''Translate to German: My name is Arthur'' example_title: Translation - text: >- Please answer to the following question. Who is going to be the next Ballon d''or? example_title: Question Answering - text: >- Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering. example_title: Logical reasoning - text: >- Please answer the following question. What is the boiling point of Nitrogen? e...', '["transformers","pytorch","t5","text2text-generation","flan-ul2","en","fr","ro","de","multilingual","dataset:svakulenk0/qrecc","dataset:taskmaster2","dataset:djaym7/wiki_dialog","dataset:deepmind/code_contests","dataset:lambada","dataset:gsm8k","dataset:aqua_rat","dataset:esnli","dataset:quasc","dataset:qed","dataset:c4","arxiv:2205.05131","license:apache-2.0","text-generation-inference","endpoints_compatible","region:us"]', 'other', 560, 5769, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/flan-ul2","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage:\n  - en\n  - fr\n  - ro\n  - de\n  - multilingual\nwidget:\n  - text: ''Translate to German:  My name is Arthur''\n    example_title: Translation\n  - text: >-\n      Please answer to the following question. Who is going to be the next\n      Ballon d''or?\n    example_title: Question Answering\n  - text: >-\n      Q: Can Geoffrey Hinton have a conversation with George Washington? Give\n      the rationale before answering.\n    example_title: Logical reasoning\n  - text: >-\n      Please answer the following question. What is the boiling point of\n      Nitrogen?\n    example_title: Scientific knowledge\n  - text: >-\n      Answer the following yes/no question. Can you write a whole Haiku in a\n      single tweet?\n    example_title: Yes/no question\n  - text: >-\n      Answer the following yes/no question by reasoning step-by-step. Can you\n      write a whole Haiku in a single tweet?\n    example_title: Reasoning task\n  - text: ''Q: ( False or not False or False ) is? A: Let''''s think step by step''\n    example_title: Boolean Expressions\n  - text: >-\n      The square root of x is the cube root of y. What is y to the power of 2,\n      if x = 4?\n    example_title: Math reasoning\n  - text: >-\n      Premise:  At my age you will probably have learnt one lesson. Hypothesis: \n      It''s not certain how many lessons you''ll learn by your thirties. Does the\n      premise entail the hypothesis?\n    example_title: Premise and hypothesis\n  - text: >-\n      Answer the following question by reasoning step by step. \n      The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?\n    example_title: Chain of thought\ntags:\n  - text2text-generation\n  - flan-ul2\ndatasets:\n  - svakulenk0/qrecc\n  - taskmaster2\n  - djaym7/wiki_dialog\n  - deepmind/code_contests\n  - lambada\n  - gsm8k\n  - aqua_rat\n  - esnli\n  - quasc\n  - qed\n  - c4\nlicense: apache-2.0\n---\n\n\n# Model card for Flan-UL2\n\n![model image](https://raw.githubusercontent.com/google-research/google-research/master/ul2/figs/ul2.png)\n\n#  Table of Contents\n\n0. [TL;DR](#TL;DR)\n1. [Using the model](#using-the-model)\n2. [Results](#results)\n3. [Introduction to UL2](#introduction-to-ul2)\n4. [Training](#training)\n5. [Contribution](#contribution)\n6. [Citation](#citation)\n\n# TL;DR\n\nFlan-UL2 is an encoder decoder model based on the `T5` architecture. It uses the same configuration as the [`UL2 model`](https://huggingface.co/google/ul2)  released earlier last year. It was fine tuned using the "Flan" prompt tuning \nand dataset collection.\n\nAccording to the original [blog](https://www.yitay.net/blog/flan-ul2-20b) here are the notable improvements:\n- The original UL2 model was only trained with receptive field of 512, which made it non-ideal for N-shot prompting where N is large. \n- The Flan-UL2 checkpoint uses a receptive field of 2048 which makes it more usable for few-shot in-context learning.\n- The original UL2 model also had mode switch tokens that was rather mandatory to get good performance. However, they were a little cumbersome as this requires often some changes during inference or finetuning. In this update/change, we continue training UL2 20B for an additional 100k steps (with small batch) to forget â€œmode tokensâ€ before applying Flan instruction tuning. This Flan-UL2 checkpoint does not require mode tokens anymore.\n\n# Using the model \n\n## Converting from T5x to huggingface\n\nYou can use the [`convert_t5x_checkpoint_to_pytorch.py`](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/convert_t5x_checkpoint_to_pytorch.py) script and pass the argument `strict = False`. The final layer norm is missing from the original dictionnary, that is why we are passing the `strict = False` argument.\n```bash\npython convert_t5x_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --config_file PATH_TO_CONFIG --pytorch_dump_path PATH_TO_SAVE\n```\nWe used the same config file as [`google/ul2`](https://huggingface.co/google/ul2/blob/main/config.json).\n\n## Running the model\n\nFor more efficient memory usage, we advise you to load the model in `8bit` using `load_in_8bit` flag as follows (works only under GPU):\n\n```python\n# pip install accelerate transformers bitsandbytes\nfrom transformers import T5ForConditionalGeneration, AutoTokenizer\nimport torch\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-ul2", device_map="auto", load_in_8bit=True)                                                                 \ntokenizer = AutoTokenizer.from_pretrained("google/flan-ul2")\n\ninput_string = "Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?"                                               \n\ninputs = tokenizer(input_string, return_tensors="pt").input_ids.to("cuda")\noutputs = model.generate(inputs, max_length=200)\n\nprint(tokenizer.decode(outputs[0]))\n# <pad> They have 23 - 20 = 3 apples left. They have 3 + 6 = 9 apples. Therefore, the answer is 9.</s>\n```\n\nOtherwise, you can load and run the model in `bfloat16` as follows:\n\n```python\n# pip install accelerate transformers\nfrom transformers import T5ForConditionalGeneration, AutoTokenizer\nimport torch\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-ul2", torch_dtype=torch.bfloat16, device_map="auto")                                                                 \ntokenizer = AutoTokenizer.from_pretrained("google/flan-ul2")\n\ninput_string = "Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?"                                               \n\ninputs = tokenizer(input_string, return_tensors="pt").input_ids.to("cuda")\noutputs = model.generate(inputs, max_length=200)\n\nprint(tokenizer.decode(outputs[0]))\n# <pad> They have 23 - 20 = 3 apples left. They have 3 + 6 = 9 apples. Therefore, the answer is 9.</s>\n```\n\n# Results\n\n## Performance improvment \n\nThe reported results are the following : \n|  | MMLU | BBH | MMLU-CoT | BBH-CoT | Avg |\n| :--- | :---: | :---: | :---: | :---: | :---: |\n| FLAN-PaLM 62B | 59.6 | 47.5 | 56.9 | 44.9 | 49.9 |\n| FLAN-PaLM 540B | 73.5 | 57.9 | 70.9 | 66.3 | 67.2 |\n| FLAN-T5-XXL 11B | 55.1 | 45.3 | 48.6 | 41.4 | 47.6 |\n| FLAN-UL2 20B | 55.7(+1.1%) | 45.9(+1.3%) | 52.2(+7.4%) | 42.7(+3.1%) | 49.1(+3.2%) |\n\n\n# Introduction to UL2\n\nThis entire section has been copied from the [`google/ul2`](https://huggingface.co/google/ul2) model card and might be subject of change with respect to `flan-ul2`.\n\nUL2 is a unified framework for pretraining models that are universally effective across datasets and setups. UL2 uses Mixture-of-Denoisers (MoD), apre-training objective that combines diverse pre-training paradigms together. UL2 introduces a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes.\n\n![model image](https://raw.githubusercontent.com/google-research/google-research/master/ul2/figs/ul2.png)\n\n**Abstract**\n\nExisting pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. \n\nFor more information, please take a look at the original paper.\n\nPaper: [Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131v1)\n\nAuthors: *Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler* \n\n\n## Training\n\n### Flan UL2\nThe Flan-UL2 model was initialized using the `UL2` checkpoints, and was then trained additionally using Flan Prompting. This means that the original training corpus is `C4`, \n\nIn â€œScaling Instruction-Finetuned language models (Chung et al.)â€ (also referred to sometimes as the Flan2 paper), the key idea is to train a large language model on a collection of datasets. These datasets are phrased as instructions which enable generalization across diverse tasks. Flan has been primarily trained on academic tasks. In Flan2, we released a series of T5 models ranging from 200M to 11B parameters that have been instruction tuned with Flan.\n\nThe Flan datasets have also been open sourced in â€œThe Flan Collection: Designing Data and Methods for Effective Instruction Tuningâ€ (Longpre et al.). See Google AI Blogpost: â€œThe Flan Collection: Advancing Open Source Methods for Instruction Tuningâ€. \n\n## UL2 PreTraining\n\nThe model is pretrained on the C4 corpus. For pretraining, the model is trained on a total of 1 trillion tokens on C4 (2 million steps)\nwith a batch size of 1024. The sequence length is set to 512/512 for inputs and targets. \nDropout is set to 0 during pretraining. Pre-training took slightly more than one month for about 1 trillion\ntokens. The model has 32 encoder layers and 32 decoder layers, `dmodel` of 4096 and `df` of 16384. \nThe dimension of each head is 256 for a total of 16 heads. Our model uses a model parallelism of 8. \nThe same sentencepiece tokenizer as T5 of vocab size 32000 is used (click [here](https://huggingface.co/docs/transformers/v4.20.0/en/model_doc/t5#transformers.T5Tokenizer) for more information about the T5 tokenizer).\n\nUL-20B can be interpreted as a model that is quite similar to T5 but trained with a different objective and slightly different scaling knobs. \nUL-20B was trained using the [Jax](https://github.com/google/jax) and [T5X](https://github.com/google-research/t5x) infrastructure.\n\nThe training objective during pretraining is a mixture of different denoising strategies that are explained in the following:\n\n### Mixture of Denoisers\n\nTo quote the paper:\n> We conjecture that a strong universal model has to be exposed to solving diverse set of problems\n> during pre-training. Given that pre-training is done using self-supervision, we argue that such diversity\n> should be injected to the objective of the model, otherwise the model might suffer from lack a certain\n> ability, like long-coherent text generation.\n> Motivated by this, as well as current class of objective functions, we define three main paradigms that\n> are used during pre-training:\n\n- **R-Denoiser**: The regular denoising is the standard span corruption introduced in [T5](https://huggingface.co/docs/transformers/v4.20.0/en/model_doc/t5)\n that uses a range of 2 to 5 tokens as the span length, which masks about 15% of\ninput tokens. These spans are short and potentially useful to acquire knowledge instead of\nlearning to generate fluent text.\n\n- **S-Denoiser**: A specific case of denoising where we observe a strict sequential order when\nframing the inputs-to-targets task, i.e., prefix language modeling. To do so, we simply\npartition the input sequence into two sub-sequences of tokens as context and target such that\nthe targets do not rely on future information. This is unlike standard span corruption where\nthere could be a target token with earlier position than a context token. Note that similar to\nthe Prefix-LM setup, the context (prefix) retains a bidirectional receptive field. We note that\nS-Denoising with very short memory or no memory is in similar spirit to standard causal\nlanguage modeling.\n\n- **X-Denoiser**: An extreme version of denoising where the model must recover a large part\nof the input, given a small to moderate part of it. This simulates a situation where a model\nneeds to generate long target from a memory with relatively limited information. To do\nso, we opt to include examples with aggressive denoising where approximately 50% of the\ninput sequence is masked. This is by increasing the span length and/or corruption rate. We\nconsider a pre-training task to be extreme if it has a long span (e.g., â‰¥ 12 tokens) or have\na large corruption rate (e.g., â‰¥ 30%). X-denoising is motivated by being an interpolation\nbetween regular span corruption and language model like objectives.\n\nSee the following diagram for a more visual explanation:\n\n![mixture-of-denoisers](https://raw.githubusercontent.com/google-research/google-research/master/ul2/figs/mod.png)\n\n**Important**: For more details, please see sections 3.1.2 of the [paper](https://arxiv.org/pdf/2205.05131v1.pdf).\n\n## Fine-tuning\n\nThe model was continously fine-tuned after N pretraining steps where N is typically from 50k to 100k.\nIn other words, after each Nk steps of pretraining, the model is finetuned on each downstream task. See section 5.2.2 of [paper](https://arxiv.org/pdf/2205.05131v1.pdf) to get an overview of all datasets that were used for fine-tuning).\n\nAs the model is continuously finetuned, finetuning is stopped on a task once it has reached state-of-the-art to save compute.\nIn total, the model was trained for 2.65 million steps.\n\n**Important**: For more details, please see sections 5.2.1 and 5.2.2 of the [paper](https://arxiv.org/pdf/2205.05131v1.pdf).\n\n# Contribution\n\nThis model was originally contributed by [Yi Tay](https://www.yitay.net/?author=636616684c5e64780328eece), and added to the Hugging Face ecosystem by [Younes Belkada](https://huggingface.co/ybelkada) & [Arthur Zucker](https://huggingface.co/ArthurZ).\n\n# Citation\n\nIf you want to cite this work, please consider citing the [blogpost](https://www.yitay.net/blog/flan-ul2-20b) announcing the release of `Flan-UL2`.', '{"pipeline_tag":null,"library_name":"transformers","framework":"transformers","params":null,"storage_bytes":161423361013,"files_count":17,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["T5ForConditionalGeneration"],"model_type":"t5","tokenizer_config":{"eos_token":"</s>","pad_token":"<pad>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:google:jax","source_url":"https://github.com/google/jax"},{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"based_on_paper","target_id":"arxiv:2205.05131","source_url":"https://arxiv.org/abs/2205.05131"}]', NULL, 'Apache-2.0', 'approved', 77.5, 'ec3fec8020f497a03dca297c8ce88d9a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-bert-bert-base-multilingual-cased', 'huggingface--google-bert--bert-base-multilingual-cased', 'bert-base-multilingual-cased', 'google-bert', '--- language: - multilingual - af - sq - ar - an - hy - ast - az - ba - eu - bar - be - bn - inc - bs - br - bg - my - ca - ceb - ce - zh - cv - hr - cs - da - nl - en - et - fi - fr - gl - ka - de - el - gu - ht - he - hi - hu - is - io - id - ga - it - ja - jv - kn - kk - ky - ko - la - lv - lt - roa - nds - lm - mk - mg - ms - ml - mr - mn - min - ne - new - nb - nn - oc - fa - pms - pl - pt - pa - ro - ru - sco - sr - hr - scn - sk - sl - aze - es - su - sw - sv - tl - tg - th - ta - tt -...', '["transformers","pytorch","tf","jax","safetensors","bert","fill-mask","multilingual","af","sq","ar","an","hy","ast","az","ba","eu","bar","be","bn","inc","bs","br","bg","my","ca","ceb","ce","zh","cv","hr","cs","da","nl","en","et","fi","fr","gl","ka","de","el","gu","ht","he","hi","hu","is","io","id","ga","it","ja","jv","kn","kk","ky","ko","la","lv","lt","roa","nds","lm","mk","mg","ms","ml","mr","mn","min","ne","new","nb","nn","oc","fa","pms","pl","pt","pa","ro","ru","sco","sr","scn","sk","sl","aze","es","su","sw","sv","tl","tg","th","ta","tt","te","tr","uk","ud","uz","vi","vo","war","cy","fry","pnb","yo","dataset:wikipedia","arxiv:1810.04805","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'fill-mask', 558, 4646256, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google-bert/bert-base-multilingual-cased","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: \n- multilingual\n- af\n- sq\n- ar\n- an\n- hy\n- ast\n- az\n- ba\n- eu\n- bar\n- be\n- bn\n- inc\n- bs\n- br\n- bg\n- my\n- ca\n- ceb\n- ce\n- zh\n- cv\n- hr\n- cs\n- da\n- nl\n- en\n- et\n- fi\n- fr\n- gl\n- ka\n- de\n- el\n- gu\n- ht\n- he\n- hi\n- hu\n- is\n- io\n- id\n- ga\n- it\n- ja\n- jv\n- kn\n- kk\n- ky\n- ko\n- la\n- lv\n- lt\n- roa\n- nds\n- lm\n- mk\n- mg\n- ms\n- ml\n- mr\n- mn\n- min\n- ne\n- new\n- nb\n- nn\n- oc\n- fa\n- pms\n- pl\n- pt\n- pa\n- ro\n- ru\n- sco\n- sr\n- hr\n- scn\n- sk\n- sl\n- aze\n- es\n- su\n- sw\n- sv\n- tl\n- tg\n- th\n- ta\n- tt\n- te\n- tr\n- uk\n- ud\n- uz\n- vi\n- vo\n- war\n- cy\n- fry\n- pnb\n- yo\nlicense: apache-2.0\ndatasets:\n- wikipedia\n---\n\n# BERT multilingual base model (cased)\n\nPretrained model on the top 104 languages with the largest Wikipedia using a masked language modeling (MLM) objective.\nIt was introduced in [this paper](https://arxiv.org/abs/1810.04805) and first released in\n[this repository](https://github.com/google-research/bert). This model is case sensitive: it makes a difference\nbetween english and English.\n\nDisclaimer: The team releasing BERT did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nBERT is a transformers model pretrained on a large corpus of multilingual data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it\nwas pretrained with two objectives:\n\n- Masked language modeling (MLM): taking a sentence, the model randomly masks 15% of the words in the input then run\n  the entire masked sentence through the model and has to predict the masked words. This is different from traditional\n  recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like\n  GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the\n  sentence.\n- Next sentence prediction (NSP): the models concatenates two masked sentences as inputs during pretraining. Sometimes\n  they correspond to sentences that were next to each other in the original text, sometimes not. The model then has to\n  predict if the two sentences were following each other or not.\n\nThis way, the model learns an inner representation of the languages in the training set that can then be used to\nextract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a\nstandard classifier using the features produced by the BERT model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it''s mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=bert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline(''fill-mask'', model=''bert-base-multilingual-cased'')\n>>> unmasker("Hello I''m a [MASK] model.")\n\n[{''sequence'': "[CLS] Hello I''m a model model. [SEP]",\n  ''score'': 0.10182085633277893,\n  ''token'': 13192,\n  ''token_str'': ''model''},\n {''sequence'': "[CLS] Hello I''m a world model. [SEP]",\n  ''score'': 0.052126359194517136,\n  ''token'': 11356,\n  ''token_str'': ''world''},\n {''sequence'': "[CLS] Hello I''m a data model. [SEP]",\n  ''score'': 0.048930276185274124,\n  ''token'': 11165,\n  ''token_str'': ''data''},\n {''sequence'': "[CLS] Hello I''m a flight model. [SEP]",\n  ''score'': 0.02036019042134285,\n  ''token'': 23578,\n  ''token_str'': ''flight''},\n {''sequence'': "[CLS] Hello I''m a business model. [SEP]",\n  ''score'': 0.020079681649804115,\n  ''token'': 14155,\n  ''token_str'': ''business''}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained(''bert-base-multilingual-cased'')\nmodel = BertModel.from_pretrained("bert-base-multilingual-cased")\ntext = "Replace me by any text you''d like."\nencoded_input = tokenizer(text, return_tensors=''pt'')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained(''bert-base-multilingual-cased'')\nmodel = TFBertModel.from_pretrained("bert-base-multilingual-cased")\ntext = "Replace me by any text you''d like."\nencoded_input = tokenizer(text, return_tensors=''tf'')\noutput = model(encoded_input)\n```\n\n## Training data\n\nThe BERT model was pretrained on the 104 languages with the largest Wikipedias. You can find the complete list\n[here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a shared vocabulary size of 110,000. The languages with a\nlarger Wikipedia are under-sampled and the ones with lower resources are oversampled. For languages like Chinese,\nJapanese Kanji and Korean Hanja that don''t have space, a CJK Unicode block is added around every character. \n\nThe inputs of the model are then of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it''s another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n"sentences" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n', '{"pipeline_tag":"fill-mask","library_name":"transformers","framework":"transformers","params":178566653,"storage_bytes":14492059671,"files_count":10,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BertForMaskedLM"],"model_type":"bert","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:google-research:bert","source_url":"https://github.com/google-research/bert"},{"type":"has_code","target_id":"github:google-research:bert","source_url":"https://github.com/google-research/bert"},{"type":"based_on_paper","target_id":"arxiv:1810.04805","source_url":"https://arxiv.org/abs/1810.04805"}]', NULL, 'Apache-2.0', 'approved', 62.5, '6b23b22ffb24663661b86ea6d8aa9d82', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-2-27b-it', 'huggingface--google--gemma-2-27b-it', 'gemma-2-27b-it', 'google', '', '["transformers","safetensors","gemma2","text-generation","conversational","arxiv:2009.03300","arxiv:1905.07830","arxiv:1911.11641","arxiv:1904.09728","arxiv:1905.10044","arxiv:1907.10641","arxiv:1811.00937","arxiv:1809.02789","arxiv:1911.01547","arxiv:1705.03551","arxiv:2107.03374","arxiv:2108.07732","arxiv:2110.14168","arxiv:2009.11462","arxiv:2101.11718","arxiv:2110.08193","arxiv:1804.09301","arxiv:2109.07958","arxiv:1804.06876","arxiv:2103.03874","arxiv:2304.06364","arxiv:2206.04615","arxiv:2203.09509","base_model:google/gemma-2-27b","base_model:finetune:google/gemma-2-27b","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 556, 460910, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-2-27b-it","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":27227128320,"storage_bytes":54521211517,"files_count":22,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["Gemma2ForCausalLM"],"model_type":"gemma2","tokenizer_config":{"bos_token":"<bos>","chat_template":"{{ bos_token }}{% if messages[0][''role''] == ''system'' %}{{ raise_exception(''System role not supported'') }}{% endif %}{% for message in messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% if (message[''role''] == ''assistant'') %}{% set role = ''model'' %}{% else %}{% set role = message[''role''] %}{% endif %}{{ ''<start_of_turn>'' + role + ''\n'' + message[''content''] | trim + ''<end_of_turn>\n'' }}{% endfor %}{% if add_generation_prompt %}{{''<start_of_turn>model\n''}}{% endif %}","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1811.00937","source_url":"https://arxiv.org/abs/1811.00937"},{"type":"based_on_paper","target_id":"arxiv:1809.02789","source_url":"https://arxiv.org/abs/1809.02789"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2009.11462","source_url":"https://arxiv.org/abs/2009.11462"},{"type":"based_on_paper","target_id":"arxiv:2101.11718","source_url":"https://arxiv.org/abs/2101.11718"},{"type":"based_on_paper","target_id":"arxiv:2110.08193","source_url":"https://arxiv.org/abs/2110.08193"},{"type":"based_on_paper","target_id":"arxiv:1804.09301","source_url":"https://arxiv.org/abs/1804.09301"},{"type":"based_on_paper","target_id":"arxiv:2109.07958","source_url":"https://arxiv.org/abs/2109.07958"},{"type":"based_on_paper","target_id":"arxiv:1804.06876","source_url":"https://arxiv.org/abs/1804.06876"},{"type":"based_on_paper","target_id":"arxiv:2103.03874","source_url":"https://arxiv.org/abs/2103.03874"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2206.04615","source_url":"https://arxiv.org/abs/2206.04615"},{"type":"based_on_paper","target_id":"arxiv:2203.09509","source_url":"https://arxiv.org/abs/2203.09509"}]', NULL, 'Gemma', 'approved', 37.5, '41f06f29ecfe0379103476b32b28f645', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-lightx2v-Wan2.2-Lightning', 'huggingface--lightx2v--wan2.2-lightning', 'Wan2.2-Lightning', 'lightx2v', '--- language: en license: apache-2.0 base_model: - Wan-AI/Wan2.2-T2V-A14B - Wan-AI/Wan2.2-I2V-A14B - Wan-AI/Wan2.2-TI2V-5B pipeline_tag: text-to-video tags: - text-to-video; - image-to-video; - comfyUI; - video-generation; library_name: diffusers --- You''re welcome to visit our GitHub repository for the latest model releases or to reproduce our results. <!-- [**Wan2.2-Lightning: Distill Wan2.2 Family into 4 Steps**] <be> --> We are excited to release the distilled version of <a href="https://...', '["diffusers","safetensors","text-to-video;","image-to-video;","comfyui;","video-generation;","text-to-video","en","arxiv:2309.14509","base_model:wan-ai/wan2.2-i2v-a14b","base_model:finetune:wan-ai/wan2.2-i2v-a14b","license:apache-2.0","region:us"]', 'text-to-video', 556, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/lightx2v/Wan2.2-Lightning","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage: en\nlicense: apache-2.0\nbase_model:\n- Wan-AI/Wan2.2-T2V-A14B\n- Wan-AI/Wan2.2-I2V-A14B\n- Wan-AI/Wan2.2-TI2V-5B\npipeline_tag: text-to-video\ntags:\n- text-to-video;\n- image-to-video;\n- comfyUI;\n- video-generation;\nlibrary_name: diffusers\n---\n\nYou''re welcome to visit our [GitHub repository](https://github.com/ModelTC/Wan2.2-Lightning) for the latest model releases or to reproduce our results.\n\n# Wan2.2-Lightning\n\n<!-- [**Wan2.2-Lightning: Distill Wan2.2 Family into 4 Steps**] <be> -->\n\n\nWe are excited to release the distilled version of <a href="https://wan.video"><b>Wan2.2</b></a> video generation model family, which offers the following advantages:\n- **Fast**: Video generation now requires only 4 steps without the need of CFG trick, leading to x20 speed-up\n- **High-quality**: The distilled model delivers visuals on par with the base model in most scenarios, sometimes even better.\n- **Complex Motion Generation**: Despite the reduction to just 4 steps, the model retains excellent motion dynamics in the generated scenes.\n\n\n## ðŸ”¥ Latest News!!\n\n* Aug 08, 2025: ðŸ‘‹ Release of Native ComfyUI Workflows.\n\n <!-- and [lora weights](https://hf-mirror.com/lightx2v/Wan2.2-Lightning/tree/main) for the `Wan2.2-Lightning` models! -->\n\n <!-- Choose one of These new [weights](https://hf-mirror.com/lightx2v/Wan2.2-Lightning/tree/main) are also compatible with [Kijai''s ComfyUI WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper). -->\n\n<table align="center">\n  <thead>\n    <tr>\n      <th>Model</th>\n      <th>Type</th>\n      <th>For Native Comfy</th>\n      <th>For Kijai''s Wrapper</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>Wan2.2-I2V-A14B-NFE4-V1</b></td>\n      <td>Image-to-Video</td>\n      <td><a href="https://huggingface.co/lightx2v/Wan2.2-Lightning/blob/main/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1-NativeComfy.json">I2V-V1-WF</a></td>\n      <td><a href="https://huggingface.co/lightx2v/Wan2.2-Lightning/blob/main/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1-forKJ.json">I2V-V1-WF</a></td>\n    </tr>\n    <tr>\n      <td><b>Wan2.2-T2V-A14B-NFE4-V1.1</b></td>\n      <td>Text-to-Video</td>\n      <td><a href="https://huggingface.co/lightx2v/Wan2.2-Lightning/blob/main/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1.1/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1.1-NativeComfy.json">T2V-V1.1-WF</a></td>\n      <td><a href="https://huggingface.co/lightx2v/Wan2.2-Lightning/blob/main/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1.1/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1.1-forKJ.json">T2V-V1.1-WF</a></td>\n    </tr>\n    <!-- <tr>\n      <td><b>Wan2.2-T2V-A14B-NFE4-V1</b></td>\n      <td>Text-to-Video</td>\n      <td><a href="https://hf-mirror/lightx2v/Wan2.2-Lightning/blob/main/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1-NativeComfy.json">Workflow</a></td>\n    </tr> -->\n  </tbody>\n</table>\n\n* Aug 07, 2025: ðŸ‘‹ Release of [Wan2.2-I2V-A14B-NFE4-V1](https://huggingface.co/lightx2v/Wan2.2-Lightning/tree/main/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1).\n <!-- A [workflow](https://hf-mirror.com/lightx2v/Wan2.2-Lightning/blob/main/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1-forKJ.json) compatible with [Kijai''s ComfyUI WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper) is inside this link. Enjoy! -->\n* Aug 07, 2025: ðŸ‘‹ Release of [Wan2.2-T2V-A14B-NFE4-V1.1](https://huggingface.co/lightx2v/Wan2.2-Lightning/tree/main/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1.1). The generation quality of V1.1 is slightly better than V1.\n<!-- A [workflow](https://hf-mirror.com/lightx2v/Wan2.2-Lightning/blob/main/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1.1/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1.1-forKJ.json) compatible with [Kijai''s ComfyUI WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper) is inside this link. The generation quality of V1.1 is slightly better than V1. Enjoy! -->\n* Aug 04, 2025: ðŸ‘‹ Release of [Wan2.2-T2V-A14B-NFE4-V1](https://huggingface.co/lightx2v/Wan2.2-Lightning/tree/main/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1).\n\n## Video Demos\n### Wan2.2-I2V-A14B-NFE4-V1 Demo\n\nThe videos below can be reproduced using [examples/i2v_prompt_list.txt](examples/i2v_prompt_list.txt) and [examples/i2v_image_path_list.txt](examples/i2v_image_path_list.txt).\n\n<table border="0" style="width: 100%; text-align: left; margin-top: 20px;">\n  <tr>\n      <td>\n          <video src="https://github.com/user-attachments/assets/4f6bb1e0-9e2b-4eb2-8b9f-0678ccd5b4ec" width="100%" controls loop></video>\n      </td>\n      <td>\n          <video src="https://github.com/user-attachments/assets/bb249553-3f52-40b3-88f9-6e3bca1a8358" width="100%" controls loop></video>\n      </td>\n       <td>\n          <video src="https://github.com/user-attachments/assets/17a6d26a-dd63-47ef-9a98-1502f503dfba" width="100%" controls loop></video>\n     </td>\n  </tr>\n  <tr>\n      <td>\n          <video src="https://github.com/user-attachments/assets/6ccc69cf-e129-456f-8b93-6dc709cb0ede" width="100%" controls loop></video>\n      </td>\n      <td>\n          <video src="https://github.com/user-attachments/assets/6cf9c586-f37a-47ed-ab5b-e106c3877fa8" width="100%" controls loop></video>\n      </td>\n       <td>\n          <video src="https://github.com/user-attachments/assets/27e82fdf-88af-44ac-b987-b48aa3f9f793" width="100%" controls loop></video>\n     </td>\n  </tr>\n  <tr>\n      <td>\n          <video src="https://github.com/user-attachments/assets/36a76f1d-2b64-4b16-a862-210d0ffd6d55" width="100%" controls loop></video>\n      </td>\n      <td>\n          <video src="https://github.com/user-attachments/assets/4bc36c70-931e-4539-be8c-432d832819d3" width="100%" controls loop></video>\n      </td>\n       <td>\n          <video src="https://github.com/user-attachments/assets/488b9179-741b-4b9d-8f23-895981f054cb" width="100%" controls loop></video>\n     </td>\n  </tr>\n</table>\n\n### Wan2.2-T2V-A14B-NFE4-V1 Demo\n\nThe videos below can be reproduced using [examples/prompt_list.txt](examples/prompt_list.txt).\n\n<table border="0" style="width: 100%; text-align: left; margin-top: 20px;">\n  <tr>\n      <td>\n          <video src="https://github.com/user-attachments/assets/ae791fbb-ef4a-4f72-989a-2ac862883201" width="100%" controls loop></video>\n      </td>\n      <td>\n          <video src="https://github.com/user-attachments/assets/f8083a50-25a0-42a8-9cd1-635f99588b19" width="100%" controls loop></video>\n      </td>\n       <td>\n          <video src="https://github.com/user-attachments/assets/5f15826b-b07b-49a2-a522-f2caea0adc60" width="100%" controls loop></video>\n     </td>\n  </tr>\n  <tr>\n      <td>\n          <video src="https://github.com/user-attachments/assets/9e48c7c2-f1a1-4d94-ade0-11e1aa913cb7" width="100%" controls loop></video>\n      </td>\n      <td>\n          <video src="https://github.com/user-attachments/assets/45ae83df-af1e-4506-b00e-7d413a0dfa51" width="100%" controls loop></video>\n      </td>\n       <td>\n          <video src="https://github.com/user-attachments/assets/554dd476-d9c1-49df-b6e1-d129113cb2be" width="100%" controls loop></video>\n     </td>\n  </tr>\n  <tr>\n      <td>\n          <video src="https://github.com/user-attachments/assets/f22b8c0f-9e40-418d-8cd5-153da3678093" width="100%" controls loop></video>\n      </td>\n      <td>\n          <video src="https://github.com/user-attachments/assets/2fc03af0-7c76-48e5-ab12-fc222164ec64" width="100%" controls loop></video>\n      </td>\n       <td>\n          <video src="https://github.com/user-attachments/assets/a8d07ae6-f037-4518-9b13-4a6702a3e0ae" width="100%" controls loop></video>\n     </td>\n  </tr>\n</table>\n\n### Wan2.2-T2V-A14B-NFE4 Limitation\n\nWhen the video contains elements with extremely large motion, the generated results may include artifacts.\nIn some results, the direction of the vehicles may be reversed.\n\n<table border="0" style="width: 100%; text-align: left; margin-top: 20px;">\n  <tr>\n      <td>\n          <video src="https://github.com/user-attachments/assets/db8f4240-7feb-4b95-8851-c52220ece9dc" width="100%" controls loop></video>\n      </td>\n      <td>\n          <video src="https://github.com/user-attachments/assets/43820463-22e0-41aa-a446-e0f130ef80d0" width="100%" controls loop></video>\n      </td>\n       <td>\n          <video src="https://github.com/user-attachments/assets/8a0580eb-2b35-4548-abcb-45fc0df12ff0" width="100%" controls loop></video>\n     </td>\n  </tr>\n</table>\n\n\n\n## ðŸ“‘ Todo List\n- [x] Wan2.2-T2V-A14B-4steps\n- [x] Wan2.2-I2V-A14B-4steps\n- [ ] Wan2.2-TI2V-5B-4steps\n\n## ðŸš€ Run Wan2.2-Lightning\n\n#### Installation\n\nPlease follow [Wan2.2 Official Github](https://github.com/Wan-Video/Wan2.2/) to install the **Python Environment** and download the **Base Model**.\n\n#### Model Download\n\nDownload models using huggingface-cli:\n``` sh\npip install "huggingface_hub[cli]"\nhuggingface-cli download Wan-AI/Wan2.2-T2V-A14B --local-dir ./Wan2.2-T2V-A14B\nhuggingface-cli download lightx2v/Wan2.2-Lightning --local-dir ./Wan2.2-Lightning\n```\n\n#### Run Text-to-Video Generation\n\nThis repository supports the `Wan2.2-T2V-A14B` Text-to-Video model and can simultaneously support video generation at 480P and 720P resolutions, either portrait or landscape.\n\n\n##### (1) Without Prompt Extension\n\nTo facilitate implementation, we will start with a basic version of the inference process that skips the [prompt extension](#2-using-prompt-extention) step.\n\n- Single-GPU, Single-prompt inference\n\n``` sh\npython generate.py  --task t2v-A14B --size "1280*720" --ckpt_dir ./Wan2.2-T2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1 --offload_model True --base_seed 42 --prompt "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage."\n```\n\n- Single-GPU, Multiple-prompt inference\n``` sh\npython generate.py  --task t2v-A14B --size "1280*720" --ckpt_dir ./Wan2.2-T2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1 --offload_model True --base_seed 42 --prompt_file examples/prompt_list.txt\n```\n\n> ðŸ’¡ This command can run on a GPU with at least 80GB VRAM.\n\n> ðŸ’¡If you encounter OOM (Out-of-Memory) issues, you can use the `--offload_model True`, `--convert_model_dtype` and `--t5_cpu` options to reduce GPU memory usage.\n\n\n- Multi-GPU inference using FSDP + DeepSpeed Ulysses\n\n  We use [PyTorch FSDP](https://docs.pytorch.org/docs/stable/fsdp.html) and [DeepSpeed Ulysses](https://arxiv.org/abs/2309.14509) to accelerate inference.\n\n\n``` sh\ntorchrun --nproc_per_node=8 generate.py --task t2v-A14B --size "1280*720" --ckpt_dir ./Wan2.2-T2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1 --dit_fsdp --t5_fsdp --ulysses_size 8 --base_seed 42 --prompt_file examples/prompt_list.txt\n```\n\n\n##### (2) Using Prompt Extension\n\nExtending the prompts can effectively enrich the details in the generated videos, further enhancing the video quality. Therefore, we recommend enabling prompt extension. We provide the following two methods for prompt extension:\n\n- Use the Dashscope API for extension.\n  - Apply for a `dashscope.api_key` in advance ([EN](https://www.alibabacloud.com/help/en/model-studio/getting-started/first-api-call-to-qwen) | [CN](https://help.aliyun.com/zh/model-studio/getting-started/first-api-call-to-qwen)).\n  - Configure the environment variable `DASH_API_KEY` to specify the Dashscope API key. For users of Alibaba Cloud''s international site, you also need to set the environment variable `DASH_API_URL` to ''https://dashscope-intl.aliyuncs.com/api/v1''. For more detailed instructions, please refer to the [dashscope document](https://www.alibabacloud.com/help/en/model-studio/developer-reference/use-qwen-by-calling-api?spm=a2c63.p38356.0.i1).\n  - Use the `qwen-plus` model for text-to-video tasks and `qwen-vl-max` for image-to-video tasks.\n  - You can modify the model used for extension with the parameter `--prompt_extend_model`. For example:\n```sh\nDASH_API_KEY=your_key torchrun --nproc_per_node=8 generate.py  --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1 --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage" --use_prompt_extend --prompt_extend_method ''dashscope'' --prompt_extend_target_lang ''zh''\n```\n\n- Using a local model for extension.\n\n  - By default, the Qwen model on HuggingFace is used for this extension. Users can choose Qwen models or other models based on the available GPU memory size.\n  - For text-to-video tasks, you can use models like `Qwen/Qwen2.5-14B-Instruct`, `Qwen/Qwen2.5-7B-Instruct` and `Qwen/Qwen2.5-3B-Instruct`.\n  - For image-to-video tasks, you can use models like `Qwen/Qwen2.5-VL-7B-Instruct` and `Qwen/Qwen2.5-VL-3B-Instruct`.\n  - Larger models generally provide better extension results but require more GPU memory.\n  - You can modify the model used for extension with the parameter `--prompt_extend_model` , allowing you to specify either a local model path or a Hugging Face model. For example:\n\n``` sh\ntorchrun --nproc_per_node=8 generate.py  --task t2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-T2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-T2V-A14B-4steps-lora-rank64-Seko-V1 --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage" --use_prompt_extend --prompt_extend_method ''local_qwen'' --prompt_extend_target_lang ''zh''\n```\n\n\n#### Run Image-to-Video Generation\n\nThis repository supports the `Wan2.2-I2V-A14B` Image-to-Video model and can simultaneously support video generation at 480P and 720P resolutions.\n\n\n- Single-GPU inference\n```sh\npython generate.py  --task i2v-A14B  --size "1280*720" --ckpt_dir ./Wan2.2-I2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1 --offload_model True --base_seed 42 --prompt_file examples/i2v_prompt_list.txt --image_path_file examples/i2v_image_path_list.txt\n```\n\n> This command can run on a GPU with at least 80GB VRAM.\n\n> ðŸ’¡For the Image-to-Video task, the `size` parameter represents the area of the generated video, with the aspect ratio following that of the original input image.\n\n\n- Multi-GPU inference using FSDP + DeepSpeed Ulysses\n\n```sh\ntorchrun --nproc_per_node=8 generate.py --task i2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-I2V-A14B --lora_dir ./Wan2.2-Lightning/Wan2.2-I2V-A14B-4steps-lora-rank64-Seko-V1 --dit_fsdp --t5_fsdp --ulysses_size 8 --base_seed 42 --prompt_file examples/i2v_prompt_list.txt --image_path_file examples/i2v_image_path_list.txt\n```\n\n<!-- \n- Image-to-Video Generation without prompt\n\n```sh\nDASH_API_KEY=your_key torchrun --nproc_per_node=8 generate.py --task i2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-I2V-A14B --prompt '''' --image examples/i2v_input.JPG --dit_fsdp --t5_fsdp --ulysses_size 8 --use_prompt_extend --prompt_extend_method ''dashscope''\n```\n\n> ðŸ’¡The model can generate videos solely from the input image. You can use prompt extension to generate prompt from the image.\n\n> The process of prompt extension can be referenced [here](#2-using-prompt-extention).\n\n#### Run Text-Image-to-Video Generation\n\nThis repository supports the `Wan2.2-TI2V-5B` Text-Image-to-Video model and can support video generation at 720P resolutions.\n\n\n- Single-GPU Text-to-Video inference\n```sh\npython generate.py --task ti2v-5B --size 1280*704 --ckpt_dir ./Wan2.2-TI2V-5B --offload_model True --convert_model_dtype --t5_cpu --prompt "Two anthropomorphic cats in comfy boxing gear and bright gloves fight intensely on a spotlighted stage"\n```\n\n> ðŸ’¡Unlike other tasks, the 720P resolution of the Text-Image-to-Video task is `1280*704` or `704*1280`.\n\n> This command can run on a GPU with at least 24GB VRAM (e.g, RTX 4090 GPU).\n\n> ðŸ’¡If you are running on a GPU with at least 80GB VRAM, you can remove the `--offload_model True`, `--convert_model_dtype` and `--t5_cpu` options to speed up execution.\n\n\n- Single-GPU Image-to-Video inference\n```sh\npython generate.py --task ti2v-5B --size 1280*704 --ckpt_dir ./Wan2.2-TI2V-5B --offload_model True --convert_model_dtype --t5_cpu --image examples/i2v_input.JPG --prompt "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline''s intricate details and the refreshing atmosphere of the seaside."\n```\n\n> ðŸ’¡If the image parameter is configured, it is an Image-to-Video generation; otherwise, it defaults to a Text-to-Video generation.\n\n> ðŸ’¡Similar to Image-to-Video, the `size` parameter represents the area of the generated video, with the aspect ratio following that of the original input image.\n\n\n- Multi-GPU inference using FSDP + DeepSpeed Ulysses\n\n```sh\ntorchrun --nproc_per_node=8 generate.py --task ti2v-5B --size 1280*704 --ckpt_dir ./Wan2.2-TI2V-5B --dit_fsdp --t5_fsdp --ulysses_size 8 --image examples/i2v_input.JPG --prompt "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline''s intricate details and the refreshing atmosphere of the seaside."\n```\n\n> The process of prompt extension can be referenced [here](#2-using-prompt-extension). \n-->\n\n\n\n## License Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generated contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the [license](LICENSE.txt).\n\n\n## Acknowledgements\n\nWe built upon and reused code from the following projects: [Wan2.1](https://github.com/Wan-Video/Wan2.1), [Wan2.2](https://github.com/Wan-Video/Wan2.2), licensed under the Apache License 2.0. \n\nWe also adopt the evaluation text prompts from [Movie Gen Bench](https://github.com/facebookresearch/MovieGenBench), which is licensed under the Creative Commons Attribution-NonCommercial 4.0 (CC BY-NC 4.0) License. The original license can be found [here](https://github.com/facebookresearch/MovieGenBench/blob/main/LICENSE).\n\nThe selected prompts are further enhanced using the `Qwen/Qwen2.5-14B-Instruct`model [Qwen](https://huggingface.co/Qwen).', '{"pipeline_tag":"text-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":55624460095,"files_count":24,"spaces_count":19,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:ModelTC:Wan2.2-Lightning","source_url":"https://github.com/ModelTC/Wan2.2-Lightning"},{"type":"has_code","target_id":"github:kijai:ComfyUI-WanVideoWrapper","source_url":"https://github.com/kijai/ComfyUI-WanVideoWrapper"},{"type":"has_code","target_id":"github:kijai:ComfyUI-WanVideoWrapper","source_url":"https://github.com/kijai/ComfyUI-WanVideoWrapper"},{"type":"has_code","target_id":"github:kijai:ComfyUI-WanVideoWrapper","source_url":"https://github.com/kijai/ComfyUI-WanVideoWrapper"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.2","source_url":"https://github.com/Wan-Video/Wan2.2"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.1","source_url":"https://github.com/Wan-Video/Wan2.1"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.2","source_url":"https://github.com/Wan-Video/Wan2.2"},{"type":"has_code","target_id":"github:facebookresearch:MovieGenBench","source_url":"https://github.com/facebookresearch/MovieGenBench"},{"type":"has_code","target_id":"github:facebookresearch:MovieGenBench","source_url":"https://github.com/facebookresearch/MovieGenBench"},{"type":"based_on_paper","target_id":"arxiv:2309.14509","source_url":"https://arxiv.org/abs/2309.14509"}]', NULL, 'Apache-2.0', 'approved', 77.5, 'a30e1d3edd4a9363d63e56da31bec46d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-berkeley-nest-Starling-LM-7B-alpha', 'huggingface--berkeley-nest--starling-lm-7b-alpha', 'Starling-LM-7B-alpha', 'berkeley-nest', '--- license: apache-2.0 datasets: - berkeley-nest/Nectar language: - en library_name: transformers tags: - reward model - RLHF - RLAIF --- <!-- Provide a quick summary of what the model is/does. --> - **Developed by:** Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao. - **Model type:** Language Model finetuned with RLHF / RLAIF - **License:** Apache-2.0 license under the condition that the model is not used to compete with OpenAI - **Finetuned from model:** Openchat 3...', '["transformers","safetensors","mistral","text-generation","reward model","rlhf","rlaif","conversational","en","dataset:berkeley-nest/nectar","arxiv:2306.02231","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 554, 1234, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ndatasets:\n- berkeley-nest/Nectar\nlanguage:\n- en\nlibrary_name: transformers\ntags:\n- reward model\n- RLHF\n- RLAIF\n---\n# Starling-LM-7B-alpha\n\n<!-- Provide a quick summary of what the model is/does. -->\n\n- **Developed by:** Banghua Zhu * , Evan Frick * , Tianhao Wu * , Hanlin Zhu and Jiantao Jiao.\n- **Model type:** Language Model finetuned with RLHF / RLAIF\n- **License:** Apache-2.0 license under the condition that the model is not used to compete with OpenAI\n- **Finetuned from model:** [Openchat 3.5](https://huggingface.co/openchat/openchat_3.5) (based on [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1))\n \n\n\nWe introduce Starling-7B, an open large language model (LLM) trained by Reinforcement Learning from AI Feedback (RLAIF). The model harnesses the power of our new GPT-4 labeled ranking dataset, [berkeley-nest/Nectar](https://huggingface.co/datasets/berkeley-nest/Nectar), and our new reward training and policy tuning pipeline. Starling-7B-alpha scores 8.09 in MT Bench with GPT-4 as a judge, outperforming every model to date on MT-Bench except for OpenAI''s GPT-4 and GPT-4 Turbo. We release the ranking dataset [Nectar](https://huggingface.co/datasets/berkeley-nest/Nectar), the reward model [Starling-RM-7B-alpha](https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha) and the language model [Starling-LM-7B-alpha](https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha) on HuggingFace, and an online demo in LMSYS [Chatbot Arena](https://chat.lmsys.org). Stay tuned for our forthcoming code and paper, which will provide more details on the whole process.\n\nStarling-LM-7B-alpha is a language model trained from [Openchat 3.5](https://huggingface.co/openchat/openchat_3.5) with reward model [berkeley-nest/Starling-RM-7B-alpha](https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha) and policy optimization method [advantage-induced policy alignment (APA)](https://arxiv.org/abs/2306.02231). The evaluation results are listed below.\n\n\n| Model                 | Tuning Method    | MT Bench | AlpacaEval | MMLU |\n|-----------------------|------------------|----------|------------|------|\n| GPT-4-Turbo           | ?                | 9.32     | 97.70      |      |\n| GPT-4                 | SFT + PPO        | 8.99     | 95.28      | 86.4 |\n| **Starling-7B**           | C-RLFT + APA     | 8.09     | 91.99      | 63.9 |\n| Claude-2              | ?                | 8.06     | 91.36      | 78.5 |\n| GPT-3.5-Turbo         | ?                | 7.94     | 89.37      | 70   |\n| Claude-1              | ?                | 7.9      | 88.39      | 77   |\n| Tulu-2-dpo-70b        | SFT + DPO        | 7.89     | 95.1       |      |\n| Openchat-3.5          | C-RLFT           | 7.81     | 88.51      | 64.3 |\n| Zephyr-7B-beta        | SFT + DPO        | 7.34     | 90.60      | 61.4 |\n| Llama-2-70b-chat-hf   | SFT + PPO        | 6.86     | 92.66      | 63   |\n| Neural-chat-7b-v3-1   | SFT + DPO        | 6.84     | 84.53      | 62.4 | \n| Tulu-2-dpo-7b         | SFT + DPO        | 6.29     | 85.1       |      |\n\n\n\nFor more detailed discussions, please check out our [blog post](https://starling.cs.berkeley.edu), and stay tuned for our upcoming code and paper!\n<!-- Provide the basic links for the model. -->\n\n- **Blog:** https://starling.cs.berkeley.edu/\n- **Paper:** Coming soon!\n- **Code:** Coming soon!\n\n\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n**Important: Please use the exact chat template provided below for the model. Otherwise there will be a degrade in the performance. The model output can be verbose in rare cases. Please consider setting temperature = 0 to make this happen less.**\n\nOur model follows the exact chat template and usage as [Openchat 3.5](https://huggingface.co/openchat/openchat_3.5). Please refer to their model card for more details.\nIn addition, our model is hosted on LMSYS [Chatbot Arena](https://chat.lmsys.org) for free test.\n\nThe conversation template is the same as Openchat 3.5:\n```\nimport transformers\ntokenizer = transformers.AutoTokenizer.from_pretrained("openchat/openchat_3.5")\n\n# Single-turn\ntokens = tokenizer("GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant:").input_ids\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n\n# Multi-turn\ntokens = tokenizer("GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant: Hi<|end_of_turn|>GPT4 Correct User: How are you today?<|end_of_turn|>GPT4 Correct Assistant:").input_ids\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747, 15359, 32000, 420, 6316, 28781, 3198, 3123, 1247, 28747, 1602, 460, 368, 3154, 28804, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n\n# Coding Mode\ntokens = tokenizer("Code User: Implement quicksort using C++<|end_of_turn|>Code Assistant:").input_ids\nassert tokens == [1, 7596, 1247, 28747, 26256, 2936, 7653, 1413, 334, 1680, 32000, 7596, 21631, 28747]\n```\n## Code Examples\n\n```python\nimport transformers\n\ntokenizer = transformers.AutoTokenizer.from_pretrained("berkeley-nest/Starling-LM-7B-alpha")\nmodel = transformers.AutoModelForCausalLM.from_pretrained("berkeley-nest/Starling-LM-7B-alpha")\n\ndef generate_response(prompt):\n    input_ids = tokenizer(prompt, return_tensors="pt").input_ids\n    outputs = model.generate(\n        input_ids,\n        max_length=256,\n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n    response_ids = outputs[0]\n    response_text = tokenizer.decode(response_ids, skip_special_tokens=True)\n    return response_text\n\n# Single-turn conversation\nprompt = "Hello, how are you?"\nsingle_turn_prompt = f"GPT4 Correct User: {prompt}<|end_of_turn|>GPT4 Correct Assistant:"\nresponse_text = generate_response(single_turn_prompt)\nprint("Response:", response_text)\n\n## Multi-turn conversation\nprompt = "Hello"\nfollow_up_question =  "How are you today?"\nresponse = ""\nmulti_turn_prompt = f"GPT4 Correct User: {prompt}<|end_of_turn|>GPT4 Correct Assistant: {response}<|end_of_turn|>GPT4 Correct User: {follow_up_question}<|end_of_turn|>GPT4 Correct Assistant:"\nresponse_text = generate_response(multi_turn_prompt)\nprint("Multi-turn conversation response:", response_text)\n\n### Coding conversation\nprompt = "Implement quicksort using C++"\ncoding_prompt = f"Code User: {prompt}<|end_of_turn|>Code Assistant:"\nresponse = generate_response(coding_prompt)\nprint("Coding conversation response:", response)\n```\n\n## License\nThe dataset, model and online demo is a research preview intended for non-commercial use only, subject to the data distillation [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, [Terms of Use](https://openai.com/policies/terms-of-use) of the data generated by OpenAI, and [Privacy Practices](https://chrome.google.com/webstore/detail/sharegpt-share-your-chatg/daiacboceoaocpibfodeljbdfacokfjb) of ShareGPT. Please contact us if you find any potential violation.\n\n\n## Acknowledgment\nWe would like to thank Wei-Lin Chiang from Berkeley for detailed feedback of the blog and the projects. We would like to thank the [LMSYS Organization](https://lmsys.org/) for their support of [lmsys-chat-1M](https://huggingface.co/datasets/lmsys/lmsys-chat-1m) dataset, evaluation and online demo. We would like to thank the open source community for their efforts in providing the datasets and base models we used to develope the project, including but not limited to Anthropic, Llama, Mistral, Hugging Face H4, LMSYS, OpenChat, OpenBMB, Flan and ShareGPT.\n\n## Citation\n```\n@misc{starling2023,\n    title = {Starling-7B: Improving LLM Helpfulness & Harmlessness with RLAIF},\n    url = {},\n    author = {Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Jiao, Jiantao},\n    month = {November},\n    year = {2023}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7241748480,"storage_bytes":14484024227,"files_count":14,"spaces_count":39,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","chat_template":"{{ bos_token }}{% for message in messages %}{{ ''GPT4 Correct '' + message[''role''].title() + '': '' + message[''content''] + ''<|end_of_turn|>''}}{% endfor %}{% if add_generation_prompt %}{{ ''GPT4 Correct Assistant:'' }}{% endif %}","eos_token":"<|end_of_turn|>","pad_token":"<|end_of_turn|>","sep_token":"<sep>","unk_token":"<unk>","use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:facebookresearch:llama","source_url":"https://github.com/facebookresearch/llama"},{"type":"based_on_paper","target_id":"arxiv:2306.02231","source_url":"https://arxiv.org/abs/2306.02231"}]', NULL, 'Apache-2.0', 'approved', 62.4, '04c496fcddcf37963cb60efe989787f8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-allenai-Molmo-7B-D-0924', 'huggingface--allenai--molmo-7b-d-0924', 'Molmo-7B-D-0924', 'allenai', '--- license: apache-2.0 language: - en base_model: - openai/clip-vit-large-patch14-336 - Qwen/Qwen2-7B pipeline_tag: image-text-to-text tags: - multimodal - olmo - molmo - pixmo library_name: transformers --- <img src="molmo_logo.png" alt="Logo for the Molmo Project" style="width: auto; height: 50px;"> Molmo is a family of open vision-language models developed by the Allen Institute for AI. Molmo models are trained on PixMo, a dataset of 1 million, highly-curated image-text pairs. It has stat...', '["transformers","safetensors","molmo","text-generation","multimodal","olmo","pixmo","image-text-to-text","conversational","custom_code","en","arxiv:2409.17146","base_model:qwen/qwen2-7b","base_model:finetune:qwen/qwen2-7b","license:apache-2.0","region:us"]', 'image-text-to-text', 554, 38776, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/allenai/Molmo-7B-D-0924","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\nbase_model:\n- openai/clip-vit-large-patch14-336\n- Qwen/Qwen2-7B\npipeline_tag: image-text-to-text\ntags:\n- multimodal\n- olmo\n- molmo\n- pixmo\nlibrary_name: transformers\n---\n\n<img src="molmo_logo.png" alt="Logo for the Molmo Project" style="width: auto; height: 50px;">\n\n# Molmo 7B-D\n\nMolmo is a family of open vision-language models developed by the Allen Institute for AI. Molmo models are trained on PixMo, a dataset of 1 million, highly-curated image-text pairs. It has state-of-the-art performance among multimodal models with a similar size while being fully open-source. You can find all models in the Molmo family [here](https://huggingface.co/collections/allenai/molmo-66f379e6fe3b8ef090a8ca19).\n**Learn more** about the Molmo family [in our announcement blog post](https://molmo.allenai.org/blog) or the [paper](https://huggingface.co/papers/2409.17146).\n\nMolmo 7B-D is based on [Qwen2-7B](https://huggingface.co/Qwen/Qwen2-7B) and uses [OpenAI CLIP](https://huggingface.co/openai/clip-vit-large-patch14-336) as vision backbone. \nIt performs comfortably between GPT-4V and GPT-4o on both academic benchmarks and human evaluation.\nIt powers the **Molmo demo at** [**molmo.allenai.org**](https://molmo.allenai.org).\n\nThis checkpoint is a **preview** of the Molmo release. All artifacts used in creating Molmo (PixMo dataset, training code, evaluations, intermediate checkpoints) will be made available at a later date, furthering our commitment to open-source AI development and reproducibility.\n\n[**Sign up here**](https://docs.google.com/forms/d/e/1FAIpQLSdML1MhNNBDsCHpgWG65Oydg2SjZzVasyqlP08nBrWjZp_c7A/viewform) to be the first to know when artifacts are released.\n\nQuick links:\n- ðŸ’¬ [Demo](https://molmo.allenai.org/)\n- ðŸ“‚ [All Models](https://huggingface.co/collections/allenai/molmo-66f379e6fe3b8ef090a8ca19)\n- ðŸ“ƒ [Paper](https://molmo.allenai.org/paper.pdf)\n- ðŸŽ¥ [Blog with Videos](https://molmo.allenai.org/blog)\n\n## Quick Start\n\nTo run Molmo, first install dependencies:\n\n```bash\npip install einops torchvision\n```\n\nThen, follow these steps:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\nfrom PIL import Image\nimport requests\n\n# load the processor\nprocessor = AutoProcessor.from_pretrained(\n    ''allenai/Molmo-7B-D-0924'',\n    trust_remote_code=True,\n    torch_dtype=''auto'',\n    device_map=''auto''\n)\n\n# load the model\nmodel = AutoModelForCausalLM.from_pretrained(\n    ''allenai/Molmo-7B-D-0924'',\n    trust_remote_code=True,\n    torch_dtype=''auto'',\n    device_map=''auto''\n)\n\n# process the image and text\ninputs = processor.process(\n    images=[Image.open(requests.get("https://picsum.photos/id/237/536/354", stream=True).raw)],\n    text="Describe this image."\n)\n\n# move inputs to the correct device and make a batch of size 1\ninputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}\n\n# generate output; maximum 200 new tokens; stop generation when <|endoftext|> is generated\noutput = model.generate_from_batch(\n    inputs,\n    GenerationConfig(max_new_tokens=200, stop_strings="<|endoftext|>"),\n    tokenizer=processor.tokenizer\n)\n\n# only get generated tokens; decode them to text\ngenerated_tokens = output[0,inputs[''input_ids''].size(1):]\ngenerated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n\n# print the generated text\nprint(generated_text)\n\n# >>>  This image features an adorable black Labrador puppy, captured from a top-down\n#      perspective. The puppy is sitting on a wooden deck, which is composed ...\n```\n\nTo make inference more efficient, run with autocast:\n\n```python\nwith torch.autocast(device_type="cuda", enabled=True, dtype=torch.bfloat16):\n  output = model.generate_from_batch(\n      inputs,\n      GenerationConfig(max_new_tokens=200, stop_strings="<|endoftext|>"),\n      tokenizer=processor.tokenizer\n  )\n```\n\nWe did most of our evaluation in this setting (autocast on, but float32 weights)\n\nTo even further reduce the memory requirements, the model can be run with bfloat16 weights:\n\n```python\nmodel.to(dtype=torch.bfloat16)\ninputs["images"] = inputs["images"].to(torch.bfloat16)\noutput = model.generate_from_batch(\n    inputs,\n    GenerationConfig(max_new_tokens=200, stop_strings="<|endoftext|>"),\n    tokenizer=processor.tokenizer\n)\n```\n\nNote that we have observed that this can change the output of the model compared to running with float32 weights.\n\n## vLLM\nMolmo is supported in vLLM, however please use version <=0.7.2 until a [prepreprocessing bug](https://github.com/vllm-project/vllm/issues/26451) is fixed.\n\n## Evaluations \n\n| Model                       | Average Score on 11 Academic Benchmarks | Human Preference Elo Rating |\n|-----------------------------|-----------------------------------------|-----------------------------|\n| Molmo 72B                   | 81.2                                    | 1077                        |\n| **Molmo 7B-D (this model)** | **77.3**                                | **1056**                    |\n| Molmo 7B-O                  | 74.6                                    | 1051                        |\n| MolmoE 1B                   | 68.6                                    | 1032                        |\n| GPT-4o                      | 78.5                                    | 1079                        |\n| GPT-4V                      | 71.1                                    | 1041                        |\n| Gemini 1.5 Pro              | 78.3                                    | 1074                        |\n| Gemini 1.5 Flash            | 75.1                                    | 1054                        |\n| Claude 3.5 Sonnet           | 76.7                                    | 1069                        |\n| Claude 3 Opus               | 66.4                                    |  971                        |\n| Claude 3 Haiku              | 65.3                                    |  999                        |\n| Qwen VL2 72B                | 79.4                                    | 1037                        |\n| Qwen VL2 7B                 | 73.7                                    | 1025                        |\n| Intern VL2 LLAMA 76B        | 77.1                                    | 1018                        |\n| Intern VL2 8B               | 69.4                                    |  953                        |\n| Pixtral 12B                 | 69.5                                    | 1016                        |\n| Phi3.5-Vision 4B            | 59.7                                    |  982                        |\n| PaliGemma 3B                | 50.0                                    |  937                        |\n| LLAVA OneVision 72B         | 76.6                                    | 1051                        |\n| LLAVA OneVision 7B          | 72.0                                    | 1024                        |\n| Cambrian-1 34B              | 66.8                                    |  953                        |\n| Cambrian-1 8B               | 63.4                                    |  952                        |\n| xGen - MM - Interleave 4B   | 59.5                                    |  979                        |\n| LLAVA-1.5 13B               | 43.9                                    |  960                        |\n| LLAVA-1.5 7B                | 40.7                                    |  951                        |\n\n*Benchmarks: AI2D test, ChartQA test, VQA v2.0 test, DocQA test, InfographicVQA test, TextVQA val, RealWorldQA, MMMU val, MathVista testmini, CountBenchQA, Flickr Count (we collected this new dataset that is significantly harder than CountBenchQA).*\n\n## FAQs\n\n### I''m getting an error a broadcast error when processing images!\n\nYour image might not be in RGB format. You can convert it using the following code snippet:\n\n```python\nfrom PIL import Image\n\nimage = Image.open(...)\n\nif image.mode != "RGB":\n    image = image.convert("RGB")\n```\n\n### Molmo doesn''t work great with transparent images!\n\nWe received reports that Molmo models might struggle with transparent images. \nFor the time being, we recommend adding a white or dark background to your images before passing them to the model. The code snippet below shows how to do this using the Python Imaging Library (PIL):\n\n```python\n\n# Load the image\nurl = "..."\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# Convert the image to grayscale to calculate brightness\ngray_image = image.convert(''L'')  # Convert to grayscale\n\n# Calculate the average brightness\nstat = ImageStat.Stat(gray_image)\naverage_brightness = stat.mean[0]  # Get the average value\n\n# Define background color based on brightness (threshold can be adjusted)\nbg_color = (0, 0, 0) if average_brightness > 127 else (255, 255, 255)\n\n# Create a new image with the same size as the original, filled with the background color\nnew_image = Image.new(''RGB'', image.size, bg_color)\n\n# Paste the original image on top of the background (use image as a mask if needed)\nnew_image.paste(image, (0, 0), image if image.mode == ''RGBA'' else None)\n\n# Now you can pass the new_image to Molmo\nprocessor = AutoProcessor.from_pretrained(\n    ''allenai/Molmo-7B-D-0924'',\n    trust_remote_code=True,\n    torch_dtype=''auto'',\n    device_map=''auto''\n)\n```\n\n## License and Use\n\nThis model is licensed under Apache 2.0. It is intended for research and educational use.\nFor more information, please see our [Responsible Use Guidelines](https://allenai.org/responsible-use).', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":8021025280,"storage_bytes":35884012544,"files_count":25,"spaces_count":28,"gated":false,"private":false,"config":{"architectures":["MolmoForCausalLM"],"auto_map":{"AutoConfig":"config_molmo.MolmoConfig","AutoModelForCausalLM":"modeling_molmo.MolmoForCausalLM"},"model_type":"molmo","tokenizer_config":{"bos_token":null,"chat_template":"{% for message in messages -%}\n        {%- if (loop.index % 2 == 1 and message[''role''] != ''user'') or \n          (loop.index % 2 == 0 and message[''role''].lower() != ''assistant'') -%}\n        {{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}\n        {%- endif -%}\n        {{ message[''role''].capitalize() + '': '' + message[''content''] }}\n        {%- if not loop.last -%}\n        {{ '' '' }}\n        {%- endif %}\n        {%- endfor -%}\n        {%- if add_generation_prompt -%}\n        {{ '' Assistant:'' }}\n        {%- endif %}","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"based_on_paper","target_id":"arxiv:2409.17146","source_url":"https://arxiv.org/abs/2409.17146"}]', NULL, 'Apache-2.0', 'approved', 62.4, '6161f68a0b65caae3191151924858e7c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nvidia-Llama3-ChatQA-1.5-8B', 'huggingface--nvidia--llama3-chatqa-1.5-8b', 'Llama3-ChatQA-1.5-8B', 'nvidia', '--- license: llama3 language: - en pipeline_tag: text-generation tags: - nvidia - chatqa-1.5 - chatqa - llama-3 - pytorch --- We introduce Llama3-ChatQA-1.5, which excels at conversational question answering (QA) and retrieval-augmented generation (RAG). Llama3-ChatQA-1.5 is developed using an improved training recipe from ChatQA paper, and it is built on top of Llama-3 base model. Specifically, we incorporate more conversational QA data to enhance its tabular and arithmetic calculation capab...', '["transformers","safetensors","llama","text-generation","nvidia","chatqa-1.5","chatqa","llama-3","pytorch","conversational","en","arxiv:2401.10225","license:llama3","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 553, 10684, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: llama3\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- nvidia\n- chatqa-1.5\n- chatqa\n- llama-3\n- pytorch\n---\n\n\n## Model Details\nWe introduce Llama3-ChatQA-1.5, which excels at conversational question answering (QA) and retrieval-augmented generation (RAG). Llama3-ChatQA-1.5 is developed using an improved training recipe from [ChatQA paper](https://arxiv.org/pdf/2401.10225), and it is built on top of [Llama-3 base model](https://huggingface.co/meta-llama/Meta-Llama-3-8B). Specifically, we incorporate more conversational QA data to enhance its tabular and arithmetic calculation capability. Llama3-ChatQA-1.5 has two variants: Llama3-ChatQA-1.5-8B and Llama3-ChatQA-1.5-70B. Both models were originally trained using [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), we converted the checkpoints to Hugging Face format. **For more information about ChatQA, check the [website](https://chatqa-project.github.io/)!**\n\n## Other Resources\n[Llama3-ChatQA-1.5-70B](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-70B) &ensp; [Evaluation Data](https://huggingface.co/datasets/nvidia/ChatRAG-Bench) &ensp; [Training Data](https://huggingface.co/datasets/nvidia/ChatQA-Training-Data) &ensp; [Retriever](https://huggingface.co/nvidia/dragon-multiturn-query-encoder) &ensp; [Website](https://chatqa-project.github.io/) &ensp; [Paper](https://arxiv.org/pdf/2401.10225)\n\n## Benchmark Results\nResults in [ChatRAG Bench](https://huggingface.co/datasets/nvidia/ChatRAG-Bench) are as follows:\n\n| | ChatQA-1.0-7B | Command-R-Plus | Llama3-instruct-70b | GPT-4-0613 | GPT-4-Turbo | ChatQA-1.0-70B | ChatQA-1.5-8B | ChatQA-1.5-70B |\n| -- |:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n| Doc2Dial | 37.88 | 33.51 | 37.88 | 34.16 | 35.35 | 38.90 | 39.33 | 41.26 |\n| QuAC | 29.69 | 34.16 | 36.96 | 40.29 | 40.10 | 41.82 | 39.73 | 38.82 |\n| QReCC | 46.97 | 49.77 | 51.34 | 52.01 | 51.46 | 48.05 | 49.03 | 51.40 |\n| CoQA | 76.61 | 69.71 | 76.98 | 77.42 | 77.73 | 78.57 | 76.46 | 78.44 |\n| DoQA | 41.57 | 40.67 | 41.24 | 43.39 | 41.60 | 51.94 | 49.60 | 50.67 |\n| ConvFinQA | 51.61 | 71.21 | 76.6 | 81.28 | 84.16 | 73.69 | 78.46 | 81.88 |\n| SQA | 61.87 | 74.07 | 69.61 | 79.21 | 79.98 | 69.14 | 73.28 | 83.82 |\n| TopioCQA | 45.45 | 53.77 | 49.72 | 45.09 | 48.32 | 50.98 | 49.96 | 55.63 |\n| HybriDial* | 54.51 | 46.7 | 48.59 | 49.81 | 47.86 | 56.44 | 65.76 | 68.27 |\n| INSCIT | 30.96 | 35.76 | 36.23 | 36.34 | 33.75 | 31.90 | 30.10 | 32.31 |\n| Average (all) | 47.71 | 50.93 | 52.52 | 53.90 | 54.03 | 54.14 | 55.17 | 58.25 |\n| Average (exclude HybriDial) | 46.96 | 51.40 | 52.95 | 54.35 | 54.72 | 53.89 | 53.99 | 57.14 |\n\nNote that ChatQA-1.5 is built based on Llama-3 base model, and ChatQA-1.0 is built based on Llama-2 base model. ChatQA-1.5 models use HybriDial training dataset. To ensure fair comparison, we also compare average scores excluding HybriDial. The data and evaluation scripts for ChatRAG Bench can be found [here](https://huggingface.co/datasets/nvidia/ChatRAG-Bench).\n\n\n## Prompt Format\n**We highly recommend that you use the prompt format we provide, as follows:**\n### when context is available\n<pre>\nSystem: {System}\n\n{Context}\n\nUser: {Question}\n\nAssistant: {Response}\n\nUser: {Question}\n\nAssistant:\n</pre>\n\n### when context is not available\n<pre>\nSystem: {System}\n\nUser: {Question}\n\nAssistant: {Response}\n\nUser: {Question}\n\nAssistant:\n</pre>\n**The content of the system''s turn (i.e., {System}) for both scenarios is as follows:**\n<pre>\nThis is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user''s questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\n</pre>\n**Note that our ChatQA-1.5 models are optimized for the capability with context, e.g., over documents or retrieved context.**\n\n## How to use\n\n### take the whole document as context \nThis can be applied to the scenario where the whole document can be fitted into the model, so that there is no need to run retrieval over the document.\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = "nvidia/Llama3-ChatQA-1.5-8B"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map="auto")\n\nmessages = [\n    {"role": "user", "content": "what is the percentage change of the net income from Q4 FY23 to Q4 FY24?"}\n]\n\ndocument = """NVIDIA (NASDAQ: NVDA) today reported revenue for the fourth quarter ended January 28, 2024, of $22.1 billion, up 22% from the previous quarter and up 265% from a year ago.\nFor the quarter, GAAP earnings per diluted share was $4.93, up 33% from the previous quarter and up 765% from a year ago. Non-GAAP earnings per diluted share was $5.16, up 28% from the previous quarter and up 486% from a year ago.\nQ4 Fiscal 2024 Summary\nGAAP\n| $ in millions, except earnings per share | Q4 FY24 | Q3 FY24 | Q4 FY23 | Q/Q | Y/Y |\n| Revenue | $22,103 | $18,120 | $6,051 | Up 22% | Up 265% |\n| Gross margin | 76.0% | 74.0% | 63.3% | Up 2.0 pts | Up 12.7 pts |\n| Operating expenses | $3,176 | $2,983 | $2,576 | Up 6% | Up 23% |\n| Operating income | $13,615 | $10,417 | $1,257 | Up 31% | Up 983% |\n| Net income | $12,285 | $9,243 | $1,414 | Up 33% | Up 769% |\n| Diluted earnings per share | $4.93 | $3.71 | $0.57 | Up 33% | Up 765% |"""\n\ndef get_formatted_input(messages, context):\n    system = "System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user''s questions based on the context. The assistant should also indicate when the answer cannot be found in the context."\n    instruction = "Please give a full and complete answer for the question."\n\n    for item in messages:\n        if item[''role''] == "user":\n            ## only apply this instruction for the first user turn\n            item[''content''] = instruction + " " + item[''content'']\n            break\n\n    conversation = ''\n\n''.join(["User: " + item["content"] if item["role"] == "user" else "Assistant: " + item["content"] for item in messages]) + "\n\nAssistant:"\n    formatted_input = system + "\n\n" + context + "\n\n" + conversation\n    \n    return formatted_input\n\nformatted_input = get_formatted_input(messages, document)\ntokenized_prompt = tokenizer(tokenizer.bos_token + formatted_input, return_tensors="pt").to(model.device)\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids("<|eot_id|>")\n]\n\noutputs = model.generate(input_ids=tokenized_prompt.input_ids, attention_mask=tokenized_prompt.attention_mask, max_new_tokens=128, eos_token_id=terminators)\n\nresponse = outputs[0][tokenized_prompt.input_ids.shape[-1]:]\nprint(tokenizer.decode(response, skip_special_tokens=True))\n```\n\n### run retrieval to get top-n chunks as context\nThis can be applied to the scenario when the document is very long, so that it is necessary to run retrieval. Here, we use our [Dragon-multiturn](https://huggingface.co/nvidia/dragon-multiturn-query-encoder) retriever which can handle conversatinoal query. In addition, we provide a few [documents](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B/tree/main/docs) for users to play with.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\nimport torch\nimport json\n\n## load ChatQA-1.5 tokenizer and model\nmodel_id = "nvidia/Llama3-ChatQA-1.5-8B"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map="auto")\n\n## load retriever tokenizer and model\nretriever_tokenizer = AutoTokenizer.from_pretrained(''nvidia/dragon-multiturn-query-encoder'')\nquery_encoder = AutoModel.from_pretrained(''nvidia/dragon-multiturn-query-encoder'')\ncontext_encoder = AutoModel.from_pretrained(''nvidia/dragon-multiturn-context-encoder'')\n\n## prepare documents, we take landrover car manual document that we provide as an example\nchunk_list = json.load(open("docs.json"))[''landrover'']\n\nmessages = [\n    {"role": "user", "content": "how to connect the bluetooth in the car?"}\n]\n\n### running retrieval\n## convert query into a format as follows:\n## user: {user}\nagent: {agent}\nuser: {user}\nformatted_query_for_retriever = ''\n''.join([turn[''role''] + ": " + turn[''content''] for turn in messages]).strip()\n\nquery_input = retriever_tokenizer(formatted_query_for_retriever, return_tensors=''pt'')\nctx_input = retriever_tokenizer(chunk_list, padding=True, truncation=True, max_length=512, return_tensors=''pt'')\nquery_emb = query_encoder(**query_input).last_hidden_state[:, 0, :]\nctx_emb = context_encoder(**ctx_input).last_hidden_state[:, 0, :]\n\n## Compute similarity scores using dot product and rank the similarity\nsimilarities = query_emb.matmul(ctx_emb.transpose(0, 1)) # (1, num_ctx)\nranked_results = torch.argsort(similarities, dim=-1, descending=True) # (1, num_ctx)\n\n## get top-n chunks (n=5)\nretrieved_chunks = [chunk_list[idx] for idx in ranked_results.tolist()[0][:5]]\ncontext = "\n\n".join(retrieved_chunks)\n\n### running text generation\nformatted_input = get_formatted_input(messages, context)\ntokenized_prompt = tokenizer(tokenizer.bos_token + formatted_input, return_tensors="pt").to(model.device)\n\nterminators = [\n    tokenizer.eos_token_id,\n    tokenizer.convert_tokens_to_ids("<|eot_id|>")\n]\noutputs = model.generate(input_ids=tokenized_prompt.input_ids, attention_mask=tokenized_prompt.attention_mask, max_new_tokens=128, eos_token_id=terminators)\n\nresponse = outputs[0][tokenized_prompt.input_ids.shape[-1]:]\nprint(tokenizer.decode(response, skip_special_tokens=True))\n```\n\n## Correspondence to\nZihan Liu (zihanl@nvidia.com), Wei Ping (wping@nvidia.com)\n\n## Citation\n<pre>\n@article{liu2024chatqa,\n  title={ChatQA: Surpassing GPT-4 on Conversational QA and RAG},\n  author={Liu, Zihan and Ping, Wei and Roy, Rajarshi and Xu, Peng and Lee, Chankyu and Shoeybi, Mohammad and Catanzaro, Bryan},\n  journal={arXiv preprint arXiv:2401.10225},\n  year={2024}}\n</pre>\n\n\n## License\nThe use of this model is governed by the [META LLAMA 3 COMMUNITY LICENSE AGREEMENT](https://llama.meta.com/llama3/license/)\n\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":8030263296,"storage_bytes":32121201772,"files_count":11,"spaces_count":17,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{{ bos_token }}{%- if messages[0][''role''] == ''system'' -%}{% set loop_messages = messages[1:] %}{%- else -%}{% set loop_messages = messages %}{% endif %}System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user''s questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\n\n{% for message in loop_messages %}{%- if message[''role''] == ''user'' -%}User: {{ message[''content''].strip() + ''\n\n'' }}{%- else -%}Assistant: {{ message[''content''].strip() + ''\n\n'' }}{%- endif %}{% if loop.last and message[''role''] == ''user'' %}Assistant:{% endif %}{% endfor %}","eos_token":"<|end_of_text|>"}}}', '[]', '[{"type":"has_code","target_id":"github:NVIDIA:Megatron-LM","source_url":"https://github.com/NVIDIA/Megatron-LM"},{"type":"based_on_paper","target_id":"arxiv:2401.10225","source_url":"https://arxiv.org/abs/2401.10225"}]', NULL, 'LLaMA-3', 'approved', 77.4, '8bf52ce97d0902f0e823dfd97bd25496', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-gsdf-Counterfeit-V3.0', 'huggingface--gsdf--counterfeit-v3.0', 'Counterfeit-V3.0', 'gsdf', '--- license: creativeml-openrail-m tags: - stable-diffusion - stable-diffusion-diffusers - text-to-image --- ãƒ»I have utilized BLIP-2 as a part of the training process. Natural language prompts might be more effective. ãƒ»I prioritize the freedom of composition, which may result in a higher possibility of anatomical errors. ãƒ»The expressiveness has been improved by merging with negative values, but the user experience may differ from previous checkpoints. ãƒ»I have uploaded a new Negative Embedding...', '["stable-diffusion","stable-diffusion-diffusers","text-to-image","license:creativeml-openrail-m","region:us"]', 'text-to-image', 552, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/gsdf/Counterfeit-V3.0","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\n---\n\n# Counterfeit-V3.0\nãƒ»I have utilized BLIP-2 as a part of the training process. Natural language prompts might be more effective.  \nãƒ»I prioritize the freedom of composition, which may result in a higher possibility of anatomical errors.  \nãƒ»The expressiveness has been improved by merging with negative values, but the user experience may differ from previous checkpoints.  \nãƒ»I have uploaded a new Negative Embedding, trained with Counterfeit-V3.0.   \nThere''s likely no clear superiority or inferiority between this and the previous embedding, so feel free to choose according to your preference.Note that I''m not specifically recommending the use of this embedding.  \n\n# Sample image\nprompt & Setting: https://civitai.com/models/4468/counterfeit-v30\n![01](https://huggingface.co/gsdf/Counterfeit-V3.0/resolve/main/images/01.png)\n![02](https://huggingface.co/gsdf/Counterfeit-V3.0/resolve/main/images/02.png)\n\n\n', '{"pipeline_tag":"text-to-image","library_name":null,"framework":null,"params":null,"storage_bytes":39965733259,"files_count":9,"spaces_count":67,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'creativeml-openrail-m', 'approved', 47.4, '2c6e584dbd1fa25a1358b355952e30d9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-deepseek-coder-33b-instruct', 'huggingface--deepseek-ai--deepseek-coder-33b-instruct', 'deepseek-coder-33b-instruct', 'deepseek-ai', '--- license: other license_name: deepseek license_link: LICENSE --- <p align="center"> <img width="1000px" alt="DeepSeek Coder" src="https://github.com/deepseek-ai/DeepSeek-Coder/blob/main/pictures/logo.png?raw=true"> </p> <p align="center"><a href="https://www.deepseek.com/">[ðŸ Homepage]</a> | <a href="https://coder.deepseek.com/">[ðŸ¤– Chat with DeepSeek Coder]</a> | <a href="https://discord.gg/Tc7c45Zzu5">[Discord]</a> | <a href="https://github.com/guoday/assert/blob/main/QR.png?raw=true">[W...', '["transformers","pytorch","safetensors","llama","text-generation","conversational","license:other","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 552, 21202, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: deepseek\nlicense_link: LICENSE\n---\n\n\n<p align="center">\n<img width="1000px" alt="DeepSeek Coder" src="https://github.com/deepseek-ai/DeepSeek-Coder/blob/main/pictures/logo.png?raw=true">\n</p>\n<p align="center"><a href="https://www.deepseek.com/">[ðŸ Homepage]</a>  |  <a href="https://coder.deepseek.com/">[ðŸ¤– Chat with DeepSeek Coder]</a>  |  <a href="https://discord.gg/Tc7c45Zzu5">[Discord]</a>  |  <a href="https://github.com/guoday/assert/blob/main/QR.png?raw=true">[Wechat(å¾®ä¿¡)]</a> </p>\n<hr>\n\n\n\n### 1. Introduction of Deepseek Coder\n\nDeepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, to support  project-level code completion and infilling. For coding capabilities, Deepseek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks. \n\n- **Massive Training Data**: Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.\n  \n- **Highly Flexible & Scalable**: Offered in model sizes of 1.3B, 5.7B, 6.7B, and 33B, enabling users to choose the setup most suitable for their requirements.\n  \n- **Superior Model Performance**: State-of-the-art performance among publicly available code models on HumanEval, MultiPL-E, MBPP, DS-1000, and APPS benchmarks.\n  \n- **Advanced Code Completion Capabilities**: A window size of 16K and a fill-in-the-blank task, supporting project-level code completion and infilling tasks.\n\n \n  \n### 2. Model Summary\ndeepseek-coder-33b-instruct is a 33B parameter model initialized from deepseek-coder-33b-base and fine-tuned on 2B tokens of instruction data.\n- **Home Page:** [DeepSeek](https://deepseek.com/)\n- **Repository:** [deepseek-ai/deepseek-coder](https://github.com/deepseek-ai/deepseek-coder)\n- **Chat With DeepSeek Coder:** [DeepSeek-Coder](https://coder.deepseek.com/)\n\n\n### 3. How to Use\nHere give some examples of how to use our model.\n#### Chat Model Inference\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-coder-6.7b-instruct", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained("deepseek-ai/deepseek-coder-6.7b-instruct", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\nmessages=[\n    { ''role'': ''user'', ''content'': "write a quick sort algorithm in python."}\n]\ninputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(model.device)\n# tokenizer.eos_token_id is the id of <|EOT|> token\noutputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))\n```\n\n### 4. License\nThis code repository is licensed under the MIT License. The use of DeepSeek Coder models is subject to the Model License. DeepSeek Coder supports commercial use.\n\nSee the [LICENSE-MODEL](https://github.com/deepseek-ai/deepseek-coder/blob/main/LICENSE-MODEL) for more details.\n\n### 5. Contact\n\nIf you have any questions, please raise an issue or contact us at [agi_code@deepseek.com](mailto:agi_code@deepseek.com).\n\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":33342991360,"storage_bytes":133372226408,"files_count":23,"spaces_count":85,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ï½œbeginâ–ofâ–sentenceï½œ>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<|EOT|>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ï½œendâ–ofâ–sentenceï½œ>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}\n{% set add_generation_prompt = false %}\n{% endif %}\n{%- set ns = namespace(found=false) -%}\n{%- for message in messages -%}\n    {%- if message[''role''] == ''system'' -%}\n        {%- set ns.found = true -%}\n    {%- endif -%}\n{%- endfor -%}\n{{bos_token}}{%- if not ns.found -%}\n{{''You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\n''}}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message[''role''] == ''system'' %}\n{{ message[''content''] }}\n    {%- else %}\n        {%- if message[''role''] == ''user'' %}\n{{''### Instruction:\\n'' + message[''content''] + ''\\n''}}\n        {%- else %}\n{{''### Response:\\n'' + message[''content''] + ''\\n<|EOT|>\\n''}}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{% if add_generation_prompt %}\n{{''### Response:''}}\n{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-Coder","source_url":"https://github.com/deepseek-ai/DeepSeek-Coder"},{"type":"has_code","target_id":"github:guoday:assert","source_url":"https://github.com/guoday/assert"},{"type":"has_code","target_id":"github:deepseek-ai:deepseek-coder","source_url":"https://github.com/deepseek-ai/deepseek-coder"},{"type":"has_code","target_id":"github:deepseek-ai:deepseek-coder","source_url":"https://github.com/deepseek-ai/deepseek-coder"}]', NULL, 'Other', 'approved', 62.4, '039c4be9951040469c72fc6742b1859f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-intfloat-e5-mistral-7b-instruct', 'huggingface--intfloat--e5-mistral-7b-instruct', 'e5-mistral-7b-instruct', 'intfloat', '--- tags: - mteb - sentence-transformers - transformers model-index: - name: e5-mistral-7b-instruct results: - task: type: STS dataset: type: C-MTEB/AFQMC name: MTEB AFQMC config: default split: validation revision: None metrics: - type: cos_sim_pearson value: 37.863226091673866 - type: cos_sim_spearman value: 38.98733013335281 - type: euclidean_pearson value: 37.51783380497874 - type: euclidean_spearman value: 38.98733012753365 - type: manhattan_pearson value: 37.26706888081721 - type: manha...', '["sentence-transformers","pytorch","safetensors","mistral","feature-extraction","mteb","transformers","en","arxiv:2401.00368","arxiv:2104.08663","arxiv:2210.07316","arxiv:2212.03533","license:mit","model-index","text-generation-inference","text-embeddings-inference","endpoints_compatible","deploy:azure","region:us"]', 'feature-extraction', 552, 153509, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/intfloat/e5-mistral-7b-instruct","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- mteb\n- sentence-transformers\n- transformers\nmodel-index:\n- name: e5-mistral-7b-instruct\n  results:\n  - task:\n      type: STS\n    dataset:\n      type: C-MTEB/AFQMC\n      name: MTEB AFQMC\n      config: default\n      split: validation\n      revision: None\n    metrics:\n    - type: cos_sim_pearson\n      value: 37.863226091673866\n    - type: cos_sim_spearman\n      value: 38.98733013335281\n    - type: euclidean_pearson\n      value: 37.51783380497874\n    - type: euclidean_spearman\n      value: 38.98733012753365\n    - type: manhattan_pearson\n      value: 37.26706888081721\n    - type: manhattan_spearman\n      value: 38.709750161903834\n  - task:\n      type: STS\n    dataset:\n      type: C-MTEB/ATEC\n      name: MTEB ATEC\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: cos_sim_pearson\n      value: 43.33924583134623\n    - type: cos_sim_spearman\n      value: 42.84316155158754\n    - type: euclidean_pearson\n      value: 45.62709879515238\n    - type: euclidean_spearman\n      value: 42.843155921732404\n    - type: manhattan_pearson\n      value: 45.4786950991229\n    - type: manhattan_spearman\n      value: 42.657334751855984\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en)\n      config: en\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 78.68656716417911\n    - type: ap\n      value: 41.71522322900398\n    - type: f1\n      value: 72.37207703532552\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (de)\n      config: de\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 74.04710920770879\n    - type: ap\n      value: 83.42622221864045\n    - type: f1\n      value: 72.14388257905772\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en-ext)\n      config: en-ext\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 77.93103448275862\n    - type: ap\n      value: 26.039284760509513\n    - type: f1\n      value: 64.81092954450712\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (ja)\n      config: ja\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 77.21627408993577\n    - type: ap\n      value: 24.876490553983036\n    - type: f1\n      value: 63.8773359684989\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_polarity\n      name: MTEB AmazonPolarityClassification\n      config: default\n      split: test\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n    metrics:\n    - type: accuracy\n      value: 95.90679999999999\n    - type: ap\n      value: 94.32357863164454\n    - type: f1\n      value: 95.90485634708557\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (en)\n      config: en\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 55.786\n    - type: f1\n      value: 55.31211995815146\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (de)\n      config: de\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 53.26\n    - type: f1\n      value: 52.156230111544986\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (es)\n      config: es\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 50.33\n    - type: f1\n      value: 49.195023008878145\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (fr)\n      config: fr\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 49.3\n    - type: f1\n      value: 48.434470184108\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (ja)\n      config: ja\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 48.68599999999999\n    - type: f1\n      value: 47.62681775202072\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (zh)\n      config: zh\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 46.238\n    - type: f1\n      value: 45.014030559653705\n  - task:\n      type: Retrieval\n    dataset:\n      type: arguana\n      name: MTEB ArguAna\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 36.486000000000004\n    - type: map_at_10\n      value: 53.076\n    - type: map_at_100\n      value: 53.657999999999994\n    - type: map_at_1000\n      value: 53.659\n    - type: map_at_3\n      value: 48.234\n    - type: map_at_5\n      value: 51.121\n    - type: mrr_at_1\n      value: 37.269000000000005\n    - type: mrr_at_10\n      value: 53.335\n    - type: mrr_at_100\n      value: 53.916\n    - type: mrr_at_1000\n      value: 53.918\n    - type: mrr_at_3\n      value: 48.518\n    - type: mrr_at_5\n      value: 51.406\n    - type: ndcg_at_1\n      value: 36.486000000000004\n    - type: ndcg_at_10\n      value: 61.882000000000005\n    - type: ndcg_at_100\n      value: 64.165\n    - type: ndcg_at_1000\n      value: 64.203\n    - type: ndcg_at_3\n      value: 52.049\n    - type: ndcg_at_5\n      value: 57.199\n    - type: precision_at_1\n      value: 36.486000000000004\n    - type: precision_at_10\n      value: 8.982999999999999\n    - type: precision_at_100\n      value: 0.9939999999999999\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_3\n      value: 21.029\n    - type: precision_at_5\n      value: 15.092\n    - type: recall_at_1\n      value: 36.486000000000004\n    - type: recall_at_10\n      value: 89.82900000000001\n    - type: recall_at_100\n      value: 99.36\n    - type: recall_at_1000\n      value: 99.644\n    - type: recall_at_3\n      value: 63.087\n    - type: recall_at_5\n      value: 75.46199999999999\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-p2p\n      name: MTEB ArxivClusteringP2P\n      config: default\n      split: test\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n    metrics:\n    - type: v_measure\n      value: 50.45119266859667\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-s2s\n      name: MTEB ArxivClusteringS2S\n      config: default\n      split: test\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\n    metrics:\n    - type: v_measure\n      value: 45.4958298992051\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/askubuntudupquestions-reranking\n      name: MTEB AskUbuntuDupQuestions\n      config: default\n      split: test\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\n    metrics:\n    - type: map\n      value: 66.98177472838887\n    - type: mrr\n      value: 79.91854636591478\n  - task:\n      type: STS\n    dataset:\n      type: mteb/biosses-sts\n      name: MTEB BIOSSES\n      config: default\n      split: test\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.67086498650698\n    - type: cos_sim_spearman\n      value: 85.54773239564638\n    - type: euclidean_pearson\n      value: 86.48229161588425\n    - type: euclidean_spearman\n      value: 85.54773239564638\n    - type: manhattan_pearson\n      value: 86.67533327742343\n    - type: manhattan_spearman\n      value: 85.76099026691983\n  - task:\n      type: STS\n    dataset:\n      type: C-MTEB/BQ\n      name: MTEB BQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: cos_sim_pearson\n      value: 50.31998888922809\n    - type: cos_sim_spearman\n      value: 50.6369940530675\n    - type: euclidean_pearson\n      value: 50.055544636296055\n    - type: euclidean_spearman\n      value: 50.63699405154838\n    - type: manhattan_pearson\n      value: 50.00739378036807\n    - type: manhattan_spearman\n      value: 50.607237418676945\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (de-en)\n      config: de-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.5615866388309\n    - type: f1\n      value: 99.49895615866389\n    - type: precision\n      value: 99.46764091858039\n    - type: recall\n      value: 99.5615866388309\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (fr-en)\n      config: fr-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.19656614571869\n    - type: f1\n      value: 99.08650671362535\n    - type: precision\n      value: 99.0314769975787\n    - type: recall\n      value: 99.19656614571869\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (ru-en)\n      config: ru-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 98.0256321440942\n    - type: f1\n      value: 97.83743216718624\n    - type: precision\n      value: 97.74390947927492\n    - type: recall\n      value: 98.0256321440942\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (zh-en)\n      config: zh-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.26276987888363\n    - type: f1\n      value: 99.22766368264\n    - type: precision\n      value: 99.21011058451816\n    - type: recall\n      value: 99.26276987888363\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77Classification\n      config: default\n      split: test\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\n    metrics:\n    - type: accuracy\n      value: 88.22727272727272\n    - type: f1\n      value: 88.17411732496673\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-p2p\n      name: MTEB BiorxivClusteringP2P\n      config: default\n      split: test\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\n    metrics:\n    - type: v_measure\n      value: 43.530637846246975\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-s2s\n      name: MTEB BiorxivClusteringS2S\n      config: default\n      split: test\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\n    metrics:\n    - type: v_measure\n      value: 40.23505728593893\n  - task:\n      type: Clustering\n    dataset:\n      type: C-MTEB/CLSClusteringP2P\n      name: MTEB CLSClusteringP2P\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: v_measure\n      value: 44.419028279451275\n  - task:\n      type: Clustering\n    dataset:\n      type: C-MTEB/CLSClusteringS2S\n      name: MTEB CLSClusteringS2S\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: v_measure\n      value: 42.5820277929776\n  - task:\n      type: Reranking\n    dataset:\n      type: C-MTEB/CMedQAv1-reranking\n      name: MTEB CMedQAv1\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map\n      value: 77.67811726152972\n    - type: mrr\n      value: 80.99003968253969\n  - task:\n      type: Reranking\n    dataset:\n      type: C-MTEB/CMedQAv2-reranking\n      name: MTEB CMedQAv2\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map\n      value: 78.66055354534922\n    - type: mrr\n      value: 81.66119047619047\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.162333333333333\n    - type: map_at_10\n      value: 37.22291666666667\n    - type: map_at_100\n      value: 38.56733333333333\n    - type: map_at_1000\n      value: 38.684250000000006\n    - type: map_at_3\n      value: 34.22858333333333\n    - type: map_at_5\n      value: 35.852500000000006\n    - type: mrr_at_1\n      value: 32.459833333333336\n    - type: mrr_at_10\n      value: 41.65358333333333\n    - type: mrr_at_100\n      value: 42.566916666666664\n    - type: mrr_at_1000\n      value: 42.61766666666667\n    - type: mrr_at_3\n      value: 39.210499999999996\n    - type: mrr_at_5\n      value: 40.582166666666666\n    - type: ndcg_at_1\n      value: 32.459833333333336\n    - type: ndcg_at_10\n      value: 42.96758333333333\n    - type: ndcg_at_100\n      value: 48.5065\n    - type: ndcg_at_1000\n      value: 50.556583333333336\n    - type: ndcg_at_3\n      value: 38.004416666666664\n    - type: ndcg_at_5\n      value: 40.25916666666667\n    - type: precision_at_1\n      value: 32.459833333333336\n    - type: precision_at_10\n      value: 7.664583333333333\n    - type: precision_at_100\n      value: 1.2349999999999999\n    - type: precision_at_1000\n      value: 0.15966666666666668\n    - type: precision_at_3\n      value: 17.731166666666663\n    - type: precision_at_5\n      value: 12.575333333333335\n    - type: recall_at_1\n      value: 27.162333333333333\n    - type: recall_at_10\n      value: 55.44158333333334\n    - type: recall_at_100\n      value: 79.56966666666666\n    - type: recall_at_1000\n      value: 93.45224999999999\n    - type: recall_at_3\n      value: 41.433083333333336\n    - type: recall_at_5\n      value: 47.31108333333333\n  - task:\n      type: Retrieval\n    dataset:\n      type: climate-fever\n      name: MTEB ClimateFEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 16.539\n    - type: map_at_10\n      value: 28.494999999999997\n    - type: map_at_100\n      value: 30.568\n    - type: map_at_1000\n      value: 30.741000000000003\n    - type: map_at_3\n      value: 23.846999999999998\n    - type: map_at_5\n      value: 26.275\n    - type: mrr_at_1\n      value: 37.394\n    - type: mrr_at_10\n      value: 50.068\n    - type: mrr_at_100\n      value: 50.727\n    - type: mrr_at_1000\n      value: 50.751000000000005\n    - type: mrr_at_3\n      value: 46.938\n    - type: mrr_at_5\n      value: 48.818\n    - type: ndcg_at_1\n      value: 37.394\n    - type: ndcg_at_10\n      value: 38.349\n    - type: ndcg_at_100\n      value: 45.512\n    - type: ndcg_at_1000\n      value: 48.321\n    - type: ndcg_at_3\n      value: 32.172\n    - type: ndcg_at_5\n      value: 34.265\n    - type: precision_at_1\n      value: 37.394\n    - type: precision_at_10\n      value: 11.927999999999999\n    - type: precision_at_100\n      value: 1.966\n    - type: precision_at_1000\n      value: 0.25\n    - type: precision_at_3\n      value: 24.126\n    - type: precision_at_5\n      value: 18.306\n    - type: recall_at_1\n      value: 16.539\n    - type: recall_at_10\n      value: 44.504\n    - type: recall_at_100\n      value: 68.605\n    - type: recall_at_1000\n      value: 84.1\n    - type: recall_at_3\n      value: 29.008\n    - type: recall_at_5\n      value: 35.58\n  - task:\n      type: Retrieval\n    dataset:\n      type: C-MTEB/CmedqaRetrieval\n      name: MTEB CmedqaRetrieval\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 19.482\n    - type: map_at_10\n      value: 28.622999999999998\n    - type: map_at_100\n      value: 30.262\n    - type: map_at_1000\n      value: 30.432\n    - type: map_at_3\n      value: 25.647\n    - type: map_at_5\n      value: 27.128000000000004\n    - type: mrr_at_1\n      value: 30.408\n    - type: mrr_at_10\n      value: 37.188\n    - type: mrr_at_100\n      value: 38.196000000000005\n    - type: mrr_at_1000\n      value: 38.273\n    - type: mrr_at_3\n      value: 35.067\n    - type: mrr_at_5\n      value: 36.124\n    - type: ndcg_at_1\n      value: 30.408\n    - type: ndcg_at_10\n      value: 34.215\n    - type: ndcg_at_100\n      value: 41.349999999999994\n    - type: ndcg_at_1000\n      value: 44.689\n    - type: ndcg_at_3\n      value: 30.264999999999997\n    - type: ndcg_at_5\n      value: 31.572\n    - type: precision_at_1\n      value: 30.408\n    - type: precision_at_10\n      value: 7.6770000000000005\n    - type: precision_at_100\n      value: 1.352\n    - type: precision_at_1000\n      value: 0.178\n    - type: precision_at_3\n      value: 17.213\n    - type: precision_at_5\n      value: 12.198\n    - type: recall_at_1\n      value: 19.482\n    - type: recall_at_10\n      value: 42.368\n    - type: recall_at_100\n      value: 72.694\n    - type: recall_at_1000\n      value: 95.602\n    - type: recall_at_3\n      value: 30.101\n    - type: recall_at_5\n      value: 34.708\n  - task:\n      type: PairClassification\n    dataset:\n      type: C-MTEB/CMNLI\n      name: MTEB Cmnli\n      config: default\n      split: validation\n      revision: None\n    metrics:\n    - type: cos_sim_accuracy\n      value: 71.16055321707758\n    - type: cos_sim_ap\n      value: 80.21073839711723\n    - type: cos_sim_f1\n      value: 72.9740932642487\n    - type: cos_sim_precision\n      value: 65.53136050623488\n    - type: cos_sim_recall\n      value: 82.3240589198036\n    - type: dot_accuracy\n      value: 71.16055321707758\n    - type: dot_ap\n      value: 80.212299264122\n    - type: dot_f1\n      value: 72.9740932642487\n    - type: dot_precision\n      value: 65.53136050623488\n    - type: dot_recall\n      value: 82.3240589198036\n    - type: euclidean_accuracy\n      value: 71.16055321707758\n    - type: euclidean_ap\n      value: 80.21076298680417\n    - type: euclidean_f1\n      value: 72.9740932642487\n    - type: euclidean_precision\n      value: 65.53136050623488\n    - type: euclidean_recall\n      value: 82.3240589198036\n    - type: manhattan_accuracy\n      value: 70.71557426337944\n    - type: manhattan_ap\n      value: 79.93448977199749\n    - type: manhattan_f1\n      value: 72.83962726826877\n    - type: manhattan_precision\n      value: 62.7407908077053\n    - type: manhattan_recall\n      value: 86.81318681318682\n    - type: max_accuracy\n      value: 71.16055321707758\n    - type: max_ap\n      value: 80.212299264122\n    - type: max_f1\n      value: 72.9740932642487\n  - task:\n      type: Retrieval\n    dataset:\n      type: C-MTEB/CovidRetrieval\n      name: MTEB CovidRetrieval\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 60.643\n    - type: map_at_10\n      value: 69.011\n    - type: map_at_100\n      value: 69.533\n    - type: map_at_1000\n      value: 69.545\n    - type: map_at_3\n      value: 67.167\n    - type: map_at_5\n      value: 68.12700000000001\n    - type: mrr_at_1\n      value: 60.801\n    - type: mrr_at_10\n      value: 69.111\n    - type: mrr_at_100\n      value: 69.6\n    - type: mrr_at_1000\n      value: 69.611\n    - type: mrr_at_3\n      value: 67.229\n    - type: mrr_at_5\n      value: 68.214\n    - type: ndcg_at_1\n      value: 60.801\n    - type: ndcg_at_10\n      value: 73.128\n    - type: ndcg_at_100\n      value: 75.614\n    - type: ndcg_at_1000\n      value: 75.92\n    - type: ndcg_at_3\n      value: 69.261\n    - type: ndcg_at_5\n      value: 70.973\n    - type: precision_at_1\n      value: 60.801\n    - type: precision_at_10\n      value: 8.662\n    - type: precision_at_100\n      value: 0.9860000000000001\n    - type: precision_at_1000\n      value: 0.101\n    - type: precision_at_3\n      value: 25.149\n    - type: precision_at_5\n      value: 15.953999999999999\n    - type: recall_at_1\n      value: 60.643\n    - type: recall_at_10\n      value: 85.959\n    - type: recall_at_100\n      value: 97.576\n    - type: recall_at_1000\n      value: 100.0\n    - type: recall_at_3\n      value: 75.184\n    - type: recall_at_5\n      value: 79.32000000000001\n  - task:\n      type: Retrieval\n    dataset:\n      type: dbpedia-entity\n      name: MTEB DBPedia\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 10.183\n    - type: map_at_10\n      value: 23.958\n    - type: map_at_100\n      value: 34.354\n    - type: map_at_1000\n      value: 36.442\n    - type: map_at_3\n      value: 16.345000000000002\n    - type: map_at_5\n      value: 19.647000000000002\n    - type: mrr_at_1\n      value: 74.25\n    - type: mrr_at_10\n      value: 80.976\n    - type: mrr_at_100\n      value: 81.256\n    - type: mrr_at_1000\n      value: 81.262\n    - type: mrr_at_3\n      value: 79.958\n    - type: mrr_at_5\n      value: 80.37100000000001\n    - type: ndcg_at_1\n      value: 62.0\n    - type: ndcg_at_10\n      value: 48.894999999999996\n    - type: ndcg_at_100\n      value: 53.867\n    - type: ndcg_at_1000\n      value: 61.304\n    - type: ndcg_at_3\n      value: 53.688\n    - type: ndcg_at_5\n      value: 50.900999999999996\n    - type: precision_at_1\n      value: 74.25\n    - type: precision_at_10\n      value: 39.525\n    - type: precision_at_100\n      value: 12.323\n    - type: precision_at_1000\n      value: 2.539\n    - type: precision_at_3\n      value: 57.49999999999999\n    - type: precision_at_5\n      value: 49.1\n    - type: recall_at_1\n      value: 10.183\n    - type: recall_at_10\n      value: 29.296\n    - type: recall_at_100\n      value: 60.394999999999996\n    - type: recall_at_1000\n      value: 83.12\n    - type: recall_at_3\n      value: 17.495\n    - type: recall_at_5\n      value: 22.235\n  - task:\n      type: Retrieval\n    dataset:\n      type: C-MTEB/DuRetrieval\n      name: MTEB DuRetrieval\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 26.613999999999997\n    - type: map_at_10\n      value: 79.77300000000001\n    - type: map_at_100\n      value: 82.71\n    - type: map_at_1000\n      value: 82.75\n    - type: map_at_3\n      value: 55.92700000000001\n    - type: map_at_5\n      value: 70.085\n    - type: mrr_at_1\n      value: 90.7\n    - type: mrr_at_10\n      value: 93.438\n    - type: mrr_at_100\n      value: 93.504\n    - type: mrr_at_1000\n      value: 93.50699999999999\n    - type: mrr_at_3\n      value: 93.125\n    - type: mrr_at_5\n      value: 93.34\n    - type: ndcg_at_1\n      value: 90.7\n    - type: ndcg_at_10\n      value: 87.023\n    - type: ndcg_at_100\n      value: 90.068\n    - type: ndcg_at_1000\n      value: 90.43299999999999\n    - type: ndcg_at_3\n      value: 86.339\n    - type: ndcg_at_5\n      value: 85.013\n    - type: precision_at_1\n      value: 90.7\n    - type: precision_at_10\n      value: 41.339999999999996\n    - type: precision_at_100\n      value: 4.806\n    - type: precision_at_1000\n      value: 0.48900000000000005\n    - type: precision_at_3\n      value: 76.983\n    - type: precision_at_5\n      value: 64.69\n    - type: recall_at_1\n      value: 26.613999999999997\n    - type: recall_at_10\n      value: 87.681\n    - type: recall_at_100\n      value: 97.44699999999999\n    - type: recall_at_1000\n      value: 99.348\n    - type: recall_at_3\n      value: 57.809999999999995\n    - type: recall_at_5\n      value: 74.258\n  - task:\n      type: Retrieval\n    dataset:\n      type: C-MTEB/EcomRetrieval\n      name: MTEB EcomRetrieval\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 30.9\n    - type: map_at_10\n      value: 40.467\n    - type: map_at_100\n      value: 41.423\n    - type: map_at_1000\n      value: 41.463\n    - type: map_at_3\n      value: 37.25\n    - type: map_at_5\n      value: 39.31\n    - type: mrr_at_1\n      value: 30.9\n    - type: mrr_at_10\n      value: 40.467\n    - type: mrr_at_100\n      value: 41.423\n    - type: mrr_at_1000\n      value: 41.463\n    - type: mrr_at_3\n      value: 37.25\n    - type: mrr_at_5\n      value: 39.31\n    - type: ndcg_at_1\n      value: 30.9\n    - type: ndcg_at_10\n      value: 45.957\n    - type: ndcg_at_100\n      value: 50.735\n    - type: ndcg_at_1000\n      value: 51.861999999999995\n    - type: ndcg_at_3\n      value: 39.437\n    - type: ndcg_at_5\n      value: 43.146\n    - type: precision_at_1\n      value: 30.9\n    - type: precision_at_10\n      value: 6.35\n    - type: precision_at_100\n      value: 0.861\n    - type: precision_at_1000\n      value: 0.095\n    - type: precision_at_3\n      value: 15.267\n    - type: precision_at_5\n      value: 10.96\n    - type: recall_at_1\n      value: 30.9\n    - type: recall_at_10\n      value: 63.5\n    - type: recall_at_100\n      value: 86.1\n    - type: recall_at_1000\n      value: 95.1\n    - type: recall_at_3\n      value: 45.800000000000004\n    - type: recall_at_5\n      value: 54.800000000000004\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/emotion\n      name: MTEB EmotionClassification\n      config: default\n      split: test\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\n    metrics:\n    - type: accuracy\n      value: 49.765\n    - type: f1\n      value: 45.93242203574485\n  - task:\n      type: Retrieval\n    dataset:\n      type: fever\n      name: MTEB FEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 75.138\n    - type: map_at_10\n      value: 84.21300000000001\n    - type: map_at_100\n      value: 84.43\n    - type: map_at_1000\n      value: 84.441\n    - type: map_at_3\n      value: 83.071\n    - type: map_at_5\n      value: 83.853\n    - type: mrr_at_1\n      value: 80.948\n    - type: mrr_at_10\n      value: 88.175\n    - type: mrr_at_100\n      value: 88.24\n    - type: mrr_at_1000\n      value: 88.241\n    - type: mrr_at_3\n      value: 87.516\n    - type: mrr_at_5\n      value: 87.997\n    - type: ndcg_at_1\n      value: 80.948\n    - type: ndcg_at_10\n      value: 87.84100000000001\n    - type: ndcg_at_100\n      value: 88.576\n    - type: ndcg_at_1000\n      value: 88.75699999999999\n    - type: ndcg_at_3\n      value: 86.176\n    - type: ndcg_at_5\n      value: 87.214\n    - type: precision_at_1\n      value: 80.948\n    - type: precision_at_10\n      value: 10.632\n    - type: precision_at_100\n      value: 1.123\n    - type: precision_at_1000\n      value: 0.11499999999999999\n    - type: precision_at_3\n      value: 33.193\n    - type: precision_at_5\n      value: 20.663\n    - type: recall_at_1\n      value: 75.138\n    - type: recall_at_10\n      value: 94.89699999999999\n    - type: recall_at_100\n      value: 97.751\n    - type: recall_at_1000\n      value: 98.833\n    - type: recall_at_3\n      value: 90.455\n    - type: recall_at_5\n      value: 93.085\n  - task:\n      type: Retrieval\n    dataset:\n      type: fiqa\n      name: MTEB FiQA2018\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 29.45\n    - type: map_at_10\n      value: 48.596000000000004\n    - type: map_at_100\n      value: 50.70400000000001\n    - type: map_at_1000\n      value: 50.83800000000001\n    - type: map_at_3\n      value: 42.795\n    - type: map_at_5\n      value: 46.085\n    - type: mrr_at_1\n      value: 56.172999999999995\n    - type: mrr_at_10\n      value: 64.35300000000001\n    - type: mrr_at_100\n      value: 64.947\n    - type: mrr_at_1000\n      value: 64.967\n    - type: mrr_at_3\n      value: 62.653999999999996\n    - type: mrr_at_5\n      value: 63.534\n    - type: ndcg_at_1\n      value: 56.172999999999995\n    - type: ndcg_at_10\n      value: 56.593\n    - type: ndcg_at_100\n      value: 62.942\n    - type: ndcg_at_1000\n      value: 64.801\n    - type: ndcg_at_3\n      value: 53.024\n    - type: ndcg_at_5\n      value: 53.986999999999995\n    - type: precision_at_1\n      value: 56.172999999999995\n    - type: precision_at_10\n      value: 15.494\n    - type: precision_at_100\n      value: 2.222\n    - type: precision_at_1000\n      value: 0.254\n    - type: precision_at_3\n      value: 35.185\n    - type: precision_at_5\n      value: 25.556\n    - type: recall_at_1\n      value: 29.45\n    - type: recall_at_10\n      value: 62.882000000000005\n    - type: recall_at_100\n      value: 85.56099999999999\n    - type: recall_at_1000\n      value: 96.539\n    - type: recall_at_3\n      value: 47.911\n    - type: recall_at_5\n      value: 54.52\n  - task:\n      type: Retrieval\n    dataset:\n      type: hotpotqa\n      name: MTEB HotpotQA\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 39.581\n    - type: map_at_10\n      value: 68.401\n    - type: map_at_100\n      value: 69.207\n    - type: map_at_1000\n      value: 69.25200000000001\n    - type: map_at_3\n      value: 64.689\n    - type: map_at_5\n      value: 67.158\n    - type: mrr_at_1\n      value: 79.163\n    - type: mrr_at_10\n      value: 85.22999999999999\n    - type: mrr_at_100\n      value: 85.386\n    - type: mrr_at_1000\n      value: 85.39099999999999\n    - type: mrr_at_3\n      value: 84.432\n    - type: mrr_at_5\n      value: 84.952\n    - type: ndcg_at_1\n      value: 79.163\n    - type: ndcg_at_10\n      value: 75.721\n    - type: ndcg_at_100\n      value: 78.411\n    - type: ndcg_at_1000\n      value: 79.23599999999999\n    - type: ndcg_at_3\n      value: 70.68799999999999\n    - type: ndcg_at_5\n      value: 73.694\n    - type: precision_at_1\n      value: 79.163\n    - type: precision_at_10\n      value: 16.134\n    - type: precision_at_100\n      value: 1.821\n    - type: precision_at_1000\n      value: 0.193\n    - type: precision_at_3\n      value: 46.446\n    - type: precision_at_5\n      value: 30.242\n    - type: recall_at_1\n      value: 39.581\n    - type: recall_at_10\n      value: 80.66799999999999\n    - type: recall_at_100\n      value: 91.033\n    - type: recall_at_1000\n      value: 96.408\n    - type: recall_at_3\n      value: 69.669\n    - type: recall_at_5\n      value: 75.604\n  - task:\n      type: Classification\n    dataset:\n      type: C-MTEB/IFlyTek-classification\n      name: MTEB IFlyTek\n      config: default\n      split: validation\n      revision: None\n    metrics:\n    - type: accuracy\n      value: 45.04809542131589\n    - type: f1\n      value: 37.01181779071118\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/imdb\n      name: MTEB ImdbClassification\n      config: default\n      split: test\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\n    metrics:\n    - type: accuracy\n      value: 94.78120000000001\n    - type: ap\n      value: 92.52931921594387\n    - type: f1\n      value: 94.77902110732532\n  - task:\n      type: Classification\n    dataset:\n      type: C-MTEB/JDReview-classification\n      name: MTEB JDReview\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: accuracy\n      value: 85.81613508442777\n    - type: ap\n      value: 52.430320593468394\n    - type: f1\n      value: 79.95467268178068\n  - task:\n      type: STS\n    dataset:\n      type: C-MTEB/LCQMC\n      name: MTEB LCQMC\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: cos_sim_pearson\n      value: 71.05801751913393\n    - type: cos_sim_spearman\n      value: 75.47954644971965\n    - type: euclidean_pearson\n      value: 74.27472296759713\n    - type: euclidean_spearman\n      value: 75.47954201369866\n    - type: manhattan_pearson\n      value: 74.30508190186474\n    - type: manhattan_spearman\n      value: 75.51326518159436\n  - task:\n      type: Reranking\n    dataset:\n      type: C-MTEB/Mmarco-reranking\n      name: MTEB MMarcoReranking\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map\n      value: 24.21110921666315\n    - type: mrr\n      value: 22.863492063492064\n  - task:\n      type: Retrieval\n    dataset:\n      type: C-MTEB/MMarcoRetrieval\n      name: MTEB MMarcoRetrieval\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 61.38400000000001\n    - type: map_at_10\n      value: 70.895\n    - type: map_at_100\n      value: 71.314\n    - type: map_at_1000\n      value: 71.331\n    - type: map_at_3\n      value: 69.016\n    - type: map_at_5\n      value: 70.179\n    - type: mrr_at_1\n      value: 63.481\n    - type: mrr_at_10\n      value: 71.543\n    - type: mrr_at_100\n      value: 71.91300000000001\n    - type: mrr_at_1000\n      value: 71.928\n    - type: mrr_at_3\n      value: 69.90899999999999\n    - type: mrr_at_5\n      value: 70.907\n    - type: ndcg_at_1\n      value: 63.481\n    - type: ndcg_at_10\n      value: 74.833\n    - type: ndcg_at_100\n      value: 76.705\n    - type: ndcg_at_1000\n      value: 77.13600000000001\n    - type: ndcg_at_3\n      value: 71.236\n    - type: ndcg_at_5\n      value: 73.199\n    - type: precision_at_1\n      value: 63.481\n    - type: precision_at_10\n      value: 9.179\n    - type: precision_at_100\n      value: 1.011\n    - type: precision_at_1000\n      value: 0.105\n    - type: precision_at_3\n      value: 27.044\n    - type: precision_at_5\n      value: 17.272000000000002\n    - type: recall_at_1\n      value: 61.38400000000001\n    - type: recall_at_10\n      value: 86.318\n    - type: recall_at_100\n      value: 94.786\n    - type: recall_at_1000\n      value: 98.14500000000001\n    - type: recall_at_3\n      value: 76.717\n    - type: recall_at_5\n      value: 81.416\n  - task:\n      type: Retrieval\n    dataset:\n      type: msmarco\n      name: MTEB MSMARCO\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 23.363999999999997\n    - type: map_at_10\n      value: 36.022\n    - type: map_at_100\n      value: 37.229\n    - type: map_at_1000\n      value: 37.274\n    - type: map_at_3\n      value: 32.131\n    - type: map_at_5\n      value: 34.391\n    - type: mrr_at_1\n      value: 24.069\n    - type: mrr_at_10\n      value: 36.620000000000005\n    - type: mrr_at_100\n      value: 37.769999999999996\n    - type: mrr_at_1000\n      value: 37.809\n    - type: mrr_at_3\n      value: 32.846\n    - type: mrr_at_5\n      value: 35.02\n    - type: ndcg_at_1\n      value: 24.069\n    - type: ndcg_at_10\n      value: 43.056\n    - type: ndcg_at_100\n      value: 48.754\n    - type: ndcg_at_1000\n      value: 49.829\n    - type: ndcg_at_3\n      value: 35.167\n    - type: ndcg_at_5\n      value: 39.168\n    - type: precision_at_1\n      value: 24.069\n    - type: precision_at_10\n      value: 6.762\n    - type: precision_at_100\n      value: 0.96\n    - type: precision_at_1000\n      value: 0.105\n    - type: precision_at_3\n      value: 14.957\n    - type: precision_at_5\n      value: 11.023\n    - type: recall_at_1\n      value: 23.363999999999997\n    - type: recall_at_10\n      value: 64.696\n    - type: recall_at_100\n      value: 90.795\n    - type: recall_at_1000\n      value: 98.892\n    - type: recall_at_3\n      value: 43.247\n    - type: recall_at_5\n      value: 52.86300000000001\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (en)\n      config: en\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 96.11947104423166\n    - type: f1\n      value: 95.89561841159332\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (de)\n      config: de\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 92.97548605240912\n    - type: f1\n      value: 92.17133696717212\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (es)\n      config: es\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 93.37224816544364\n    - type: f1\n      value: 93.19978829237863\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (fr)\n      config: fr\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 91.28719072972127\n    - type: f1\n      value: 91.28448045979604\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (hi)\n      config: hi\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 88.8131946934385\n    - type: f1\n      value: 88.27883019362747\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (th)\n      config: th\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 85.52260397830018\n    - type: f1\n      value: 85.15528226728568\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (en)\n      config: en\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 86.10807113543093\n    - type: f1\n      value: 70.88498219072167\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (de)\n      config: de\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 77.77120315581854\n    - type: f1\n      value: 57.97153920153224\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (es)\n      config: es\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 79.93995997331554\n    - type: f1\n      value: 58.839203810064866\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (fr)\n      config: fr\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 77.801440651425\n    - type: f1\n      value: 58.68009647839332\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (hi)\n      config: hi\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 72.90785227680172\n    - type: f1\n      value: 49.83760954655788\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (th)\n      config: th\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 73.24050632911391\n    - type: f1\n      value: 52.0562553541082\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (af)\n      config: af\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.47948890383321\n    - type: f1\n      value: 63.334877563135485\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (am)\n      config: am\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 44.2871553463349\n    - type: f1\n      value: 43.17658050605427\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ar)\n      config: ar\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 63.174176193678555\n    - type: f1\n      value: 59.236659587042425\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (az)\n      config: az\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 64.226630800269\n    - type: f1\n      value: 60.951842696956184\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (bn)\n      config: bn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 64.94283792871555\n    - type: f1\n      value: 61.40057652844215\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (cy)\n      config: cy\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 55.480833893745796\n    - type: f1\n      value: 52.5298332072816\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (da)\n      config: da\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.52858103564223\n    - type: f1\n      value: 69.3770851919204\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (de)\n      config: de\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.09213180901143\n    - type: f1\n      value: 71.13518469365879\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (el)\n      config: el\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.31203765971756\n    - type: f1\n      value: 66.05906970865144\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (en)\n      config: en\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 80.57162071284465\n    - type: f1\n      value: 77.7866172598823\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (es)\n      config: es\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 75.09414929388029\n    - type: f1\n      value: 72.5712594833695\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fa)\n      config: fa\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.20914593140553\n    - type: f1\n      value: 68.90619124909186\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fi)\n      config: fi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.74243443174176\n    - type: f1\n      value: 64.72743141749955\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fr)\n      config: fr\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 75.11096166778749\n    - type: f1\n      value: 72.61849933064694\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (he)\n      config: he\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.22394082044384\n    - type: f1\n      value: 62.43648797607235\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hi)\n      config: hi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.44855413584399\n    - type: f1\n      value: 66.56851670913659\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hu)\n      config: hu\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.4149293880296\n    - type: f1\n      value: 66.12960877904776\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hy)\n      config: hy\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 56.916610625420304\n    - type: f1\n      value: 54.02534600927991\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (id)\n      config: id\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.71351714862138\n    - type: f1\n      value: 69.70227985126316\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (is)\n      config: is\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 59.91257565568257\n    - type: f1\n      value: 57.06811572144974\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (it)\n      config: it\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 75.25218560860793\n    - type: f1\n      value: 72.48057563104247\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ja)\n      config: ja\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 76.35507733691998\n    - type: f1\n      value: 73.03024649541128\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (jv)\n      config: jv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 57.918628110289184\n    - type: f1\n      value: 54.75590124456177\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ka)\n      config: ka\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 52.548755884330866\n    - type: f1\n      value: 51.5356975360209\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (km)\n      config: km\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 46.44922663080027\n    - type: f1\n      value: 44.561114416830975\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (kn)\n      config: kn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 53.95763281775386\n    - type: f1\n      value: 50.68367245122476\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ko)\n      config: ko\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.20645595158035\n    - type: f1\n      value: 71.78450093258185\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (lv)\n      config: lv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 59.226630800269\n    - type: f1\n      value: 57.53988988993337\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ml)\n      config: ml\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 51.44922663080027\n    - type: f1\n      value: 48.58809018065056\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (mn)\n      config: mn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 51.3752521856086\n    - type: f1\n      value: 49.91373941436425\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ms)\n      config: ms\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.85205110961668\n    - type: f1\n      value: 67.05660019588582\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (my)\n      config: my\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 49.1492938802959\n    - type: f1\n      value: 46.717578025393195\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (nb)\n      config: nb\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.93140551445865\n    - type: f1\n      value: 67.45406609372205\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (nl)\n      config: nl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.82851378614662\n    - type: f1\n      value: 71.15951964393868\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (pl)\n      config: pl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.84868863483524\n    - type: f1\n      value: 71.76056802364877\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (pt)\n      config: pt\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 75.27236045729657\n    - type: f1\n      value: 72.48733090101163\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ro)\n      config: ro\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.63012777404168\n    - type: f1\n      value: 66.56444015346203\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ru)\n      config: ru\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 76.62743779421655\n    - type: f1\n      value: 73.82720656992142\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sl)\n      config: sl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.15198386012105\n    - type: f1\n      value: 64.41418309797744\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sq)\n      config: sq\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 58.8399462004035\n    - type: f1\n      value: 56.050989519693886\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sv)\n      config: sv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.86684599865501\n    - type: f1\n      value: 70.80682480844303\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sw)\n      config: sw\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 57.36718224613316\n    - type: f1\n      value: 54.998746471013774\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ta)\n      config: ta\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 53.150638870208475\n    - type: f1\n      value: 49.79179342620099\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (te)\n      config: te\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 51.50638870208473\n    - type: f1\n      value: 49.778960742003555\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (th)\n      config: th\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.906523201076\n    - type: f1\n      value: 66.75784022138245\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (tl)\n      config: tl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.73234700739744\n    - type: f1\n      value: 65.75016141148413\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (tr)\n      config: tr\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.06792199058508\n    - type: f1\n      value: 67.90334782594083\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ur)\n      config: ur\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.09145931405515\n    - type: f1\n      value: 58.88703095210731\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (vi)\n      config: vi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.17014122394083\n    - type: f1\n      value: 68.43676277921544\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (zh-CN)\n      config: zh-CN\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 74.99327505043712\n    - type: f1\n      value: 72.26813373392943\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (zh-TW)\n      config: zh-TW\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.13987895090787\n    - type: f1\n      value: 70.29309514467575\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (af)\n      config: af\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.37256220578345\n    - type: f1\n      value: 72.56456170538992\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (am)\n      config: am\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 47.205783456624076\n    - type: f1\n      value: 45.905999859074434\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ar)\n      config: ar\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 69.8352387357095\n    - type: f1\n      value: 69.43553987525273\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (az)\n      config: az\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 67.00403496973773\n    - type: f1\n      value: 65.97477215779143\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (bn)\n      config: bn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 68.04976462676531\n    - type: f1\n      value: 67.24581993778398\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (cy)\n      config: cy\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 61.882985877605925\n    - type: f1\n      value: 59.995293199988794\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (da)\n      config: da\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.75857431069267\n    - type: f1\n      value: 76.52031675299841\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (de)\n      config: de\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 79.03496973772697\n    - type: f1\n      value: 79.25548063175344\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (el)\n      config: el\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.96570275722931\n    - type: f1\n      value: 72.19110435289122\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (en)\n      config: en\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 82.38735709482178\n    - type: f1\n      value: 82.34495627619785\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (es)\n      config: es\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.83994620040352\n    - type: f1\n      value: 78.91526355393667\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fa)\n      config: fa\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.7350369872226\n    - type: f1\n      value: 75.919437344927\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fi)\n      config: fi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.21721587088096\n    - type: f1\n      value: 70.82973286243262\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fr)\n      config: fr\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.59784801613988\n    - type: f1\n      value: 78.47383161087423\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (he)\n      config: he\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 69.64021519838602\n    - type: f1\n      value: 68.45118053027653\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hi)\n      config: hi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.51042367182245\n    - type: f1\n      value: 72.90013022879003\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hu)\n      config: hu\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.0551445864156\n    - type: f1\n      value: 73.45871761713292\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hy)\n      config: hy\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 59.54606590450571\n    - type: f1\n      value: 57.72711794953869\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (id)\n      config: id\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.40753194351042\n    - type: f1\n      value: 76.8157455506521\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (is)\n      config: is\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.58372562205783\n    - type: f1\n      value: 65.2654868709758\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (it)\n      config: it\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.39273705447208\n    - type: f1\n      value: 78.3592956594837\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ja)\n      config: ja\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 79.62004034969739\n    - type: f1\n      value: 79.78673754501855\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (jv)\n      config: jv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 64.29051782111634\n    - type: f1\n      value: 63.12502587609454\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ka)\n      config: ka\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 57.51849361129791\n    - type: f1\n      value: 56.32320906403241\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (km)\n      config: km\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 52.41761936785474\n    - type: f1\n      value: 49.113762010098306\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (kn)\n      config: kn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 58.547410894418284\n    - type: f1\n      value: 56.87580674198118\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ko)\n      config: ko\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.89038332212507\n    - type: f1\n      value: 79.09210140529848\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (lv)\n      config: lv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 63.503698722259585\n    - type: f1\n      value: 61.45718858568352\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ml)\n      config: ml\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 54.02824478816408\n    - type: f1\n      value: 52.732738981386504\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (mn)\n      config: mn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 54.23671822461331\n    - type: f1\n      value: 52.688080372545286\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ms)\n      config: ms\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.5312710154674\n    - type: f1\n      value: 74.59368478550698\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (my)\n      config: my\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 52.192333557498316\n    - type: f1\n      value: 50.18302290152229\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (nb)\n      config: nb\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.6960322797579\n    - type: f1\n      value: 75.25331182714856\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (nl)\n      config: nl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.47679892400808\n    - type: f1\n      value: 78.24044732352424\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (pl)\n      config: pl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.36718224613315\n    - type: f1\n      value: 77.2714452985389\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (pt)\n      config: pt\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.96234028244788\n    - type: f1\n      value: 78.21282127011372\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ro)\n      config: ro\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.19435104236717\n    - type: f1\n      value: 73.1963711292812\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ru)\n      config: ru\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 80.52118359112306\n    - type: f1\n      value: 80.4179964390288\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sl)\n      config: sl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.65837256220577\n    - type: f1\n      value: 73.07156989634905\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sq)\n      config: sq\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 64.02824478816409\n    - type: f1\n      value: 62.972399027713664\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sv)\n      config: sv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 78.87020847343645\n    - type: f1\n      value: 78.224240866849\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sw)\n      config: sw\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 64.6570275722932\n    - type: f1\n      value: 63.274871811412545\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ta)\n      config: ta\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 57.760591795561524\n    - type: f1\n      value: 56.73711528075771\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (te)\n      config: te\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 57.26967047747142\n    - type: f1\n      value: 55.74735330863165\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (th)\n      config: th\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.46133154001345\n    - type: f1\n      value: 71.9644168952811\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (tl)\n      config: tl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.70880968392737\n    - type: f1\n      value: 73.61543141070884\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (tr)\n      config: tr\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.0437121721587\n    - type: f1\n      value: 74.83359868879921\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ur)\n      config: ur\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 67.05110961667788\n    - type: f1\n      value: 66.25869819274315\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (vi)\n      config: vi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.52118359112306\n    - type: f1\n      value: 75.92098546052303\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (zh-CN)\n      config: zh-CN\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 79.92938802958977\n    - type: f1\n      value: 79.79833572573796\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (zh-TW)\n      config: zh-TW\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.86617350369872\n    - type: f1\n      value: 77.42645654909516\n  - task:\n      type: Retrieval\n    dataset:\n      type: C-MTEB/MedicalRetrieval\n      name: MTEB MedicalRetrieval\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 44.6\n    - type: map_at_10\n      value: 50.019000000000005\n    - type: map_at_100\n      value: 50.611\n    - type: map_at_1000\n      value: 50.67\n    - type: map_at_3\n      value: 48.699999999999996\n    - type: map_at_5\n      value: 49.455\n    - type: mrr_at_1\n      value: 44.800000000000004\n    - type: mrr_at_10\n      value: 50.119\n    - type: mrr_at_100\n      value: 50.711\n    - type: mrr_at_1000\n      value: 50.77\n    - type: mrr_at_3\n      value: 48.8\n    - type: mrr_at_5\n      value: 49.555\n    - type: ndcg_at_1\n      value: 44.6\n    - type: ndcg_at_10\n      value: 52.754\n    - type: ndcg_at_100\n      value: 55.935\n    - type: ndcg_at_1000\n      value: 57.607\n    - type: ndcg_at_3\n      value: 50.012\n    - type: ndcg_at_5\n      value: 51.393\n    - type: precision_at_1\n      value: 44.6\n    - type: precision_at_10\n      value: 6.140000000000001\n    - type: precision_at_100\n      value: 0.77\n    - type: precision_at_1000\n      value: 0.09\n    - type: precision_at_3\n      value: 17.933\n    - type: precision_at_5\n      value: 11.44\n    - type: recall_at_1\n      value: 44.6\n    - type: recall_at_10\n      value: 61.4\n    - type: recall_at_100\n      value: 77.0\n    - type: recall_at_1000\n      value: 90.4\n    - type: recall_at_3\n      value: 53.800000000000004\n    - type: recall_at_5\n      value: 57.199999999999996\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-p2p\n      name: MTEB MedrxivClusteringP2P\n      config: default\n      split: test\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\n    metrics:\n    - type: v_measure\n      value: 38.192667527616315\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-s2s\n      name: MTEB MedrxivClusteringS2S\n      config: default\n      split: test\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\n    metrics:\n    - type: v_measure\n      value: 37.44738902946689\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/mind_small\n      name: MTEB MindSmallReranking\n      config: default\n      split: test\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\n    metrics:\n    - type: map\n      value: 32.59661273103955\n    - type: mrr\n      value: 33.82024242497473\n  - task:\n      type: Classification\n    dataset:\n      type: C-MTEB/MultilingualSentiment-classification\n      name: MTEB MultilingualSentiment\n      config: default\n      split: validation\n      revision: None\n    metrics:\n    - type: accuracy\n      value: 73.31333333333335\n    - type: f1\n      value: 73.0873466527602\n  - task:\n      type: Retrieval\n    dataset:\n      type: nfcorpus\n      name: MTEB NFCorpus\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 5.471\n    - type: map_at_10\n      value: 14.142\n    - type: map_at_100\n      value: 18.179000000000002\n    - type: map_at_1000\n      value: 19.772000000000002\n    - type: map_at_3\n      value: 9.716\n    - type: map_at_5\n      value: 11.763\n    - type: mrr_at_1\n      value: 51.393\n    - type: mrr_at_10\n      value: 58.814\n    - type: mrr_at_100\n      value: 59.330000000000005\n    - type: mrr_at_1000\n      value: 59.35\n    - type: mrr_at_3\n      value: 56.398\n    - type: mrr_at_5\n      value: 58.038999999999994\n    - type: ndcg_at_1\n      value: 49.69\n    - type: ndcg_at_10\n      value: 38.615\n    - type: ndcg_at_100\n      value: 35.268\n    - type: ndcg_at_1000\n      value: 43.745\n    - type: ndcg_at_3\n      value: 43.187\n    - type: ndcg_at_5\n      value: 41.528999999999996\n    - type: precision_at_1\n      value: 51.083999999999996\n    - type: precision_at_10\n      value: 29.474\n    - type: precision_at_100\n      value: 9.167\n    - type: precision_at_1000\n      value: 2.2089999999999996\n    - type: precision_at_3\n      value: 40.351\n    - type: precision_at_5\n      value: 36.285000000000004\n    - type: recall_at_1\n      value: 5.471\n    - type: recall_at_10\n      value: 19.242\n    - type: recall_at_100\n      value: 37.14\n    - type: recall_at_1000\n      value: 68.35900000000001\n    - type: recall_at_3\n      value: 10.896\n    - type: recall_at_5\n      value: 14.75\n  - task:\n      type: Retrieval\n    dataset:\n      type: nq\n      name: MTEB NQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 39.499\n    - type: map_at_10\n      value: 55.862\n    - type: map_at_100\n      value: 56.667\n    - type: map_at_1000\n      value: 56.684999999999995\n    - type: map_at_3\n      value: 51.534\n    - type: map_at_5\n      value: 54.2\n    - type: mrr_at_1\n      value: 44.351\n    - type: mrr_at_10\n      value: 58.567\n    - type: mrr_at_100\n      value: 59.099000000000004\n    - type: mrr_at_1000\n      value: 59.109\n    - type: mrr_at_3\n      value: 55.218999999999994\n    - type: mrr_at_5\n      value: 57.391999999999996\n    - type: ndcg_at_1\n      value: 44.322\n    - type: ndcg_at_10\n      value: 63.535\n    - type: ndcg_at_100\n      value: 66.654\n    - type: ndcg_at_1000\n      value: 66.991\n    - type: ndcg_at_3\n      value: 55.701\n    - type: ndcg_at_5\n      value: 60.06700000000001\n    - type: precision_at_1\n      value: 44.322\n    - type: precision_at_10\n      value: 10.026\n    - type: precision_at_100\n      value: 1.18\n    - type: precision_at_1000\n      value: 0.121\n    - type: precision_at_3\n      value: 24.865000000000002\n    - type: precision_at_5\n      value: 17.48\n    - type: recall_at_1\n      value: 39.499\n    - type: recall_at_10\n      value: 84.053\n    - type: recall_at_100\n      value: 97.11\n    - type: recall_at_1000\n      value: 99.493\n    - type: recall_at_3\n      value: 64.091\n    - type: recall_at_5\n      value: 74.063\n  - task:\n      type: PairClassification\n    dataset:\n      type: C-MTEB/OCNLI\n      name: MTEB Ocnli\n      config: default\n      split: validation\n      revision: None\n    metrics:\n    - type: cos_sim_accuracy\n      value: 61.18029236599891\n    - type: cos_sim_ap\n      value: 64.18398769398412\n    - type: cos_sim_f1\n      value: 67.96347757046446\n    - type: cos_sim_precision\n      value: 54.4529262086514\n    - type: cos_sim_recall\n      value: 90.3907074973601\n    - type: dot_accuracy\n      value: 61.18029236599891\n    - type: dot_ap\n      value: 64.18393484706077\n    - type: dot_f1\n      value: 67.96347757046446\n    - type: dot_precision\n      value: 54.4529262086514\n    - type: dot_recall\n      value: 90.3907074973601\n    - type: euclidean_accuracy\n      value: 61.18029236599891\n    - type: euclidean_ap\n      value: 64.18395024821486\n    - type: euclidean_f1\n      value: 67.96347757046446\n    - type: euclidean_precision\n      value: 54.4529262086514\n    - type: euclidean_recall\n      value: 90.3907074973601\n    - type: manhattan_accuracy\n      value: 61.451001624255554\n    - type: manhattan_ap\n      value: 64.38232708763513\n    - type: manhattan_f1\n      value: 68.05860805860804\n    - type: manhattan_precision\n      value: 52.10319685922602\n    - type: manhattan_recall\n      value: 98.09926082365365\n    - type: max_accuracy\n      value: 61.451001624255554\n    - type: max_ap\n      value: 64.38232708763513\n    - type: max_f1\n      value: 68.05860805860804\n  - task:\n      type: Classification\n    dataset:\n      type: C-MTEB/OnlineShopping-classification\n      name: MTEB OnlineShopping\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: accuracy\n      value: 92.19000000000001\n    - type: ap\n      value: 89.73918431886767\n    - type: f1\n      value: 92.17175032574507\n  - task:\n      type: STS\n    dataset:\n      type: C-MTEB/PAWSX\n      name: MTEB PAWSX\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: cos_sim_pearson\n      value: 15.079320253752224\n    - type: cos_sim_spearman\n      value: 16.813772504404263\n    - type: euclidean_pearson\n      value: 19.476541162041762\n    - type: euclidean_spearman\n      value: 16.813772498098782\n    - type: manhattan_pearson\n      value: 19.497429832915277\n    - type: manhattan_spearman\n      value: 16.869600674180607\n  - task:\n      type: STS\n    dataset:\n      type: C-MTEB/QBQTC\n      name: MTEB QBQTC\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: cos_sim_pearson\n      value: 30.36139599797913\n    - type: cos_sim_spearman\n      value: 31.80296402851347\n    - type: euclidean_pearson\n      value: 30.10387888252793\n    - type: euclidean_spearman\n      value: 31.80297780103808\n    - type: manhattan_pearson\n      value: 30.86720382849436\n    - type: manhattan_spearman\n      value: 32.70491131366606\n  - task:\n      type: Retrieval\n    dataset:\n      type: quora\n      name: MTEB QuoraRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 71.911\n    - type: map_at_10\n      value: 86.087\n    - type: map_at_100\n      value: 86.701\n    - type: map_at_1000\n      value: 86.715\n    - type: map_at_3\n      value: 83.231\n    - type: map_at_5\n      value: 85.051\n    - type: mrr_at_1\n      value: 82.75\n    - type: mrr_at_10\n      value: 88.759\n    - type: mrr_at_100\n      value: 88.844\n    - type: mrr_at_1000\n      value: 88.844\n    - type: mrr_at_3\n      value: 87.935\n    - type: mrr_at_5\n      value: 88.504\n    - type: ndcg_at_1\n      value: 82.75\n    - type: ndcg_at_10\n      value: 89.605\n    - type: ndcg_at_100\n      value: 90.664\n    - type: ndcg_at_1000\n      value: 90.733\n    - type: ndcg_at_3\n      value: 87.03\n    - type: ndcg_at_5\n      value: 88.473\n    - type: precision_at_1\n      value: 82.75\n    - type: precision_at_10\n      value: 13.575000000000001\n    - type: precision_at_100\n      value: 1.539\n    - type: precision_at_1000\n      value: 0.157\n    - type: precision_at_3\n      value: 38.153\n    - type: precision_at_5\n      value: 25.008000000000003\n    - type: recall_at_1\n      value: 71.911\n    - type: recall_at_10\n      value: 96.261\n    - type: recall_at_100\n      value: 99.72800000000001\n    - type: recall_at_1000\n      value: 99.993\n    - type: recall_at_3\n      value: 88.762\n    - type: recall_at_5\n      value: 92.949\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering\n      name: MTEB RedditClustering\n      config: default\n      split: test\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\n    metrics:\n    - type: v_measure\n      value: 57.711581165572376\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering-p2p\n      name: MTEB RedditClusteringP2P\n      config: default\n      split: test\n      revision: 282350215ef01743dc01b456c7f5241fa8937f16\n    metrics:\n    - type: v_measure\n      value: 66.48938885750297\n  - task:\n      type: Retrieval\n    dataset:\n      type: scidocs\n      name: MTEB SCIDOCS\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 3.7379999999999995\n    - type: map_at_10\n      value: 9.261\n    - type: map_at_100\n      value: 11.001\n    - type: map_at_1000\n      value: 11.262\n    - type: map_at_3\n      value: 6.816\n    - type: map_at_5\n      value: 8.0\n    - type: mrr_at_1\n      value: 18.4\n    - type: mrr_at_10\n      value: 28.755999999999997\n    - type: mrr_at_100\n      value: 29.892000000000003\n    - type: mrr_at_1000\n      value: 29.961\n    - type: mrr_at_3\n      value: 25.467000000000002\n    - type: mrr_at_5\n      value: 27.332\n    - type: ndcg_at_1\n      value: 18.4\n    - type: ndcg_at_10\n      value: 16.296\n    - type: ndcg_at_100\n      value: 23.52\n    - type: ndcg_at_1000\n      value: 28.504\n    - type: ndcg_at_3\n      value: 15.485\n    - type: ndcg_at_5\n      value: 13.471\n    - type: precision_at_1\n      value: 18.4\n    - type: precision_at_10\n      value: 8.469999999999999\n    - type: precision_at_100\n      value: 1.8950000000000002\n    - type: precision_at_1000\n      value: 0.309\n    - type: precision_at_3\n      value: 14.6\n    - type: precision_at_5\n      value: 11.84\n    - type: recall_at_1\n      value: 3.7379999999999995\n    - type: recall_at_10\n      value: 17.185\n    - type: recall_at_100\n      value: 38.397\n    - type: recall_at_1000\n      value: 62.798\n    - type: recall_at_3\n      value: 8.896999999999998\n    - type: recall_at_5\n      value: 12.021999999999998\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sickr-sts\n      name: MTEB SICK-R\n      config: default\n      split: test\n      revision: a6ea5a8cab320b040a23452cc28066d9beae2cee\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.43977757480083\n    - type: cos_sim_spearman\n      value: 82.64182475199533\n    - type: euclidean_pearson\n      value: 83.71756009999591\n    - type: euclidean_spearman\n      value: 82.64182331395057\n    - type: manhattan_pearson\n      value: 83.8028936913025\n    - type: manhattan_spearman\n      value: 82.71024597804252\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts12-sts\n      name: MTEB STS12\n      config: default\n      split: test\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.85653060698912\n    - type: cos_sim_spearman\n      value: 79.65598885228324\n    - type: euclidean_pearson\n      value: 83.1205137628455\n    - type: euclidean_spearman\n      value: 79.65629387709038\n    - type: manhattan_pearson\n      value: 83.71108853545837\n    - type: manhattan_spearman\n      value: 80.25617619716708\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts13-sts\n      name: MTEB STS13\n      config: default\n      split: test\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\n    metrics:\n    - type: cos_sim_pearson\n      value: 88.22921688565664\n    - type: cos_sim_spearman\n      value: 88.42662103041957\n    - type: euclidean_pearson\n      value: 87.91679798473325\n    - type: euclidean_spearman\n      value: 88.42662103041957\n    - type: manhattan_pearson\n      value: 88.16927537961303\n    - type: manhattan_spearman\n      value: 88.81581680062541\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts14-sts\n      name: MTEB STS14\n      config: default\n      split: test\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.77261424554293\n    - type: cos_sim_spearman\n      value: 84.53930146434155\n    - type: euclidean_pearson\n      value: 85.67420491389697\n    - type: euclidean_spearman\n      value: 84.53929771783851\n    - type: manhattan_pearson\n      value: 85.74306784515618\n    - type: manhattan_spearman\n      value: 84.7399304675314\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts15-sts\n      name: MTEB STS15\n      config: default\n      split: test\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\n    metrics:\n    - type: cos_sim_pearson\n      value: 89.86138395166455\n    - type: cos_sim_spearman\n      value: 90.42577823022054\n    - type: euclidean_pearson\n      value: 89.8787763797515\n    - type: euclidean_spearman\n      value: 90.42577823022054\n    - type: manhattan_pearson\n      value: 89.9592937492158\n    - type: manhattan_spearman\n      value: 90.63535505335524\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts16-sts\n      name: MTEB STS16\n      config: default\n      split: test\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.5176674585941\n    - type: cos_sim_spearman\n      value: 87.6842917085397\n    - type: euclidean_pearson\n      value: 86.70213081520711\n    - type: euclidean_spearman\n      value: 87.6842917085397\n    - type: manhattan_pearson\n      value: 86.83702628983627\n    - type: manhattan_spearman\n      value: 87.87791000374443\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (ko-ko)\n      config: ko-ko\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 83.86395454805867\n    - type: cos_sim_spearman\n      value: 83.69454595252267\n    - type: euclidean_pearson\n      value: 83.04743892608313\n    - type: euclidean_spearman\n      value: 83.69454026433006\n    - type: manhattan_pearson\n      value: 83.4032095553322\n    - type: manhattan_spearman\n      value: 84.11527379013802\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (ar-ar)\n      config: ar-ar\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 81.80249894729546\n    - type: cos_sim_spearman\n      value: 81.87004960533409\n    - type: euclidean_pearson\n      value: 80.0392760044179\n    - type: euclidean_spearman\n      value: 81.87004960533409\n    - type: manhattan_pearson\n      value: 80.38096542355912\n    - type: manhattan_spearman\n      value: 82.40774679630341\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-ar)\n      config: en-ar\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 77.6158201787172\n    - type: cos_sim_spearman\n      value: 77.934651044009\n    - type: euclidean_pearson\n      value: 77.7874683895269\n    - type: euclidean_spearman\n      value: 77.934651044009\n    - type: manhattan_pearson\n      value: 78.36151849193052\n    - type: manhattan_spearman\n      value: 78.52439586349938\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-de)\n      config: en-de\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.04363311392207\n    - type: cos_sim_spearman\n      value: 87.30483659369973\n    - type: euclidean_pearson\n      value: 87.62634489502616\n    - type: euclidean_spearman\n      value: 87.30483659369973\n    - type: manhattan_pearson\n      value: 88.02340837141445\n    - type: manhattan_spearman\n      value: 87.55012003294\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-en)\n      config: en-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 91.69172851958248\n    - type: cos_sim_spearman\n      value: 91.7546879482416\n    - type: euclidean_pearson\n      value: 91.84843039183963\n    - type: euclidean_spearman\n      value: 91.7546879482416\n    - type: manhattan_pearson\n      value: 91.72325753804357\n    - type: manhattan_spearman\n      value: 91.55330259513397\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-tr)\n      config: en-tr\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 73.95572901084864\n    - type: cos_sim_spearman\n      value: 72.56217821552626\n    - type: euclidean_pearson\n      value: 74.24242980323574\n    - type: euclidean_spearman\n      value: 72.56217821552626\n    - type: manhattan_pearson\n      value: 74.57473362519922\n    - type: manhattan_spearman\n      value: 72.76048826648497\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (es-en)\n      config: es-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.93329396008296\n    - type: cos_sim_spearman\n      value: 88.2406635486219\n    - type: euclidean_pearson\n      value: 87.49687343908533\n    - type: euclidean_spearman\n      value: 88.2406635486219\n    - type: manhattan_pearson\n      value: 88.14088309231084\n    - type: manhattan_spearman\n      value: 88.93314020908534\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (es-es)\n      config: es-es\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 88.70124451546057\n    - type: cos_sim_spearman\n      value: 87.45988160052252\n    - type: euclidean_pearson\n      value: 88.44395505247728\n    - type: euclidean_spearman\n      value: 87.45988160052252\n    - type: manhattan_pearson\n      value: 88.69269783495425\n    - type: manhattan_spearman\n      value: 87.65383425621\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (fr-en)\n      config: fr-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.64109149761346\n    - type: cos_sim_spearman\n      value: 88.06459637689733\n    - type: euclidean_pearson\n      value: 88.02313315797703\n    - type: euclidean_spearman\n      value: 88.06459637689733\n    - type: manhattan_pearson\n      value: 88.28328539133253\n    - type: manhattan_spearman\n      value: 88.06605708379142\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (it-en)\n      config: it-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 88.9040028177525\n    - type: cos_sim_spearman\n      value: 89.68152202933464\n    - type: euclidean_pearson\n      value: 89.23684469601253\n    - type: euclidean_spearman\n      value: 89.68152202933464\n    - type: manhattan_pearson\n      value: 89.59504307277454\n    - type: manhattan_spearman\n      value: 89.88060100313582\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (nl-en)\n      config: nl-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.69891585325125\n    - type: cos_sim_spearman\n      value: 88.25252785071736\n    - type: euclidean_pearson\n      value: 87.99932873748662\n    - type: euclidean_spearman\n      value: 88.25252785071736\n    - type: manhattan_pearson\n      value: 88.26959683009446\n    - type: manhattan_spearman\n      value: 88.32583227300715\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (en)\n      config: en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 67.53235909794135\n    - type: cos_sim_spearman\n      value: 66.97521740529574\n    - type: euclidean_pearson\n      value: 68.19502223613912\n    - type: euclidean_spearman\n      value: 66.97521740529574\n    - type: manhattan_pearson\n      value: 68.39070714774539\n    - type: manhattan_spearman\n      value: 67.1072812364868\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (de)\n      config: de\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 43.715742021204775\n    - type: cos_sim_spearman\n      value: 49.12255971271453\n    - type: euclidean_pearson\n      value: 40.76848562610837\n    - type: euclidean_spearman\n      value: 49.12255971271453\n    - type: manhattan_pearson\n      value: 40.92204625614112\n    - type: manhattan_spearman\n      value: 49.23333793661129\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (es)\n      config: es\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 63.35268345563588\n    - type: cos_sim_spearman\n      value: 66.99661626042061\n    - type: euclidean_pearson\n      value: 65.85589122857066\n    - type: euclidean_spearman\n      value: 66.99661626042061\n    - type: manhattan_pearson\n      value: 66.78454301512294\n    - type: manhattan_spearman\n      value: 67.17570330149233\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (pl)\n      config: pl\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 33.36599908204445\n    - type: cos_sim_spearman\n      value: 39.20768331939503\n    - type: euclidean_pearson\n      value: 22.16066769530468\n    - type: euclidean_spearman\n      value: 39.20768331939503\n    - type: manhattan_pearson\n      value: 22.386053195546022\n    - type: manhattan_spearman\n      value: 39.70172817465986\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (tr)\n      config: tr\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 63.06813956986753\n    - type: cos_sim_spearman\n      value: 68.72065117995668\n    - type: euclidean_pearson\n      value: 66.97373456344194\n    - type: euclidean_spearman\n      value: 68.72065117995668\n    - type: manhattan_pearson\n      value: 67.34907265771595\n    - type: manhattan_spearman\n      value: 68.73705769957843\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (ar)\n      config: ar\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 47.17664865207108\n    - type: cos_sim_spearman\n      value: 54.115568323148864\n    - type: euclidean_pearson\n      value: 48.56418162879182\n    - type: euclidean_spearman\n      value: 54.115568323148864\n    - type: manhattan_pearson\n      value: 48.85951643453165\n    - type: manhattan_spearman\n      value: 54.13599784169052\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (ru)\n      config: ru\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 55.87514136275987\n    - type: cos_sim_spearman\n      value: 60.82923573674973\n    -', '{"pipeline_tag":"feature-extraction","library_name":"sentence-transformers","framework":"sentence-transformers","params":7110660096,"storage_bytes":28779017177,"files_count":20,"spaces_count":46,"gated":false,"private":false,"config":{"architectures":["MistralModel"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","pad_token":"</s>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2401.00368","source_url":"https://arxiv.org/abs/2401.00368"},{"type":"based_on_paper","target_id":"arxiv:2104.08663","source_url":"https://arxiv.org/abs/2104.08663"},{"type":"based_on_paper","target_id":"arxiv:2210.07316","source_url":"https://arxiv.org/abs/2210.07316"},{"type":"based_on_paper","target_id":"arxiv:2212.03533","source_url":"https://arxiv.org/abs/2212.03533"}]', NULL, 'MIT', 'approved', 77.4, 'f78f02f73a29bf1a16d5b420ff3fc736', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-NovaSky-AI-Sky-T1-32B-Preview', 'huggingface--novasky-ai--sky-t1-32b-preview', 'Sky-T1-32B-Preview', 'NovaSky-AI', '--- library_name: transformers datasets: - codeparrot/apps - BAAI/TACO - AI-MO/NuminaMath-CoT language: - en base_model: - Qwen/Qwen2.5-32B-Instruct license: apache-2.0 --- <!-- Provide a longer summary of what this model is. --> This is a 32B reasoning model trained from Qwen2.5-32B-Instruct with 17K data. The performance is on par with o1-preview model on both math and coding. Please see our blog post for more details. - **Developed by:** NovaSky Team from Sky Computing Lab at UC Berkeley. ...', '["transformers","safetensors","qwen2","text-generation","conversational","en","dataset:codeparrot/apps","dataset:baai/taco","dataset:ai-mo/numinamath-cot","arxiv:2412.09413","base_model:qwen/qwen2.5-32b-instruct","base_model:finetune:qwen/qwen2.5-32b-instruct","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 550, 367, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlibrary_name: transformers\ndatasets:\n- codeparrot/apps\n- BAAI/TACO\n- AI-MO/NuminaMath-CoT\nlanguage:\n- en\nbase_model:\n- Qwen/Qwen2.5-32B-Instruct\nlicense: apache-2.0\n---\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\nThis is a 32B reasoning model trained from Qwen2.5-32B-Instruct with 17K data. The performance is on par with o1-preview model on both math and coding.\nPlease see our [blog post](https://novasky-ai.github.io/posts/sky-t1/) for more details.\n\n- **Developed by:** NovaSky Team from Sky Computing Lab at UC Berkeley.\n\n## Training Details\n\n### Training Data\n\n17K verified correct responses from Qwen/QwQ-32B-Preview on coding, math. In addition, we add the science portion from the [Still-2 paper](https://arxiv.org/pdf/2412.09413).\n\n### Training Procedure\nWe perform supervised fine tuning on the data, with a batch size of 96.\n\n#### Speeds\n\nWe use Llama-Factory for training. On 8 H100, the training takes 19 hours with DeepSpeed Zero-3 Offload.\n\n\n## Evaluation\n|               | Sky-T1-32B-Preview | Qwen-2.5-32B-Instruct | QwQ   | o1-preview |\n|-----------------------|---------------------|--------|-------|------------|\n| Math500              | 82.4                    | 76.2    | 85.4 | 81.4       |\n| AIME2024             | 43.3                    | 16.7    | 50.0  | 40.0       |\n| LiveCodeBench-Easy   | 86.3                    | 84.6   | 90.7  | 92.9       |\n| LiveCodeBench-Medium | 56.8                    | 40.8   | 56.3  | 54.9       |\n| LiveCodeBench-Hard   | 17.9                    | 9.8   | 17.1  | 16.3       |\n| GPQA-Diamond         | 56.8                    | 45.5   | 52.5  | 75.2       |\n\n## Acknowledgement\nWe would like to thanks the compute resources from [Lambda Lab](https://lambdalabs.com/service/gpu-cloud?srsltid=AfmBOop5FnmEFTkavVtdZDsLWvHWNg6peXtat-OXJ9MW5GMNsk756PE5) and [AnyScale](https://www.anyscale.com/). We would like to thanks the academic feedback and support from the [Still-2 Team](https://arxiv.org/pdf/2412.09413), and [Junyang Lin](https://justinlin610.github.io/) from the [Qwen Team](https://qwenlm.github.io/). \n\n## Citation \nPlease considering citing our blog post if you found it useful for your research. Thank you!\n\n```bibtex\n@misc{sky_t1_2025,\n  author       = {NovaSky Team},\n  title        = {Sky-T1: Fully open-source reasoning model with o1-preview performance in $450 budget},\n  howpublished = {https://novasky-ai.github.io/posts/sky-t1},\n  note         = {Accessed: 2025-01-09},\n  year         = {2025}\n}', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":32763876352,"storage_bytes":65527840896,"files_count":24,"spaces_count":32,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2412.09413","source_url":"https://arxiv.org/abs/2412.09413"}]', NULL, 'Apache-2.0', 'approved', 62.4, '8571a3ff9e87b28379e1b19f766d8046', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Wan-AI-Wan2.1-I2V-14B-720P', 'huggingface--wan-ai--wan2.1-i2v-14b-720p', 'Wan2.1-I2V-14B-720P', 'Wan-AI', '--- license: apache-2.0 language: - en - zh pipeline_tag: image-to-video library_name: diffusers tags: - video - video genration --- <p align="center"> <img src="assets/logo.png" width="400"/> <p> <p align="center"> ðŸ’œ <a href=""><b>Wan</b></a> &nbsp&nbsp ï½œ &nbsp&nbsp ðŸ–¥ï¸ <a href="https://github.com/Wan-Video/Wan2.1">GitHub</a> &nbsp&nbsp | &nbsp&nbspðŸ¤— <a href="https://huggingface.co/Wan-AI/">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspðŸ¤– <a href="https://modelscope.cn/organization/Wan-AI">ModelS...', '["diffusers","safetensors","i2v","video","video genration","image-to-video","en","zh","license:apache-2.0","region:us"]', 'image-to-video', 549, 8530, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P","fetched_at":"2025-12-08T10:30:37.945Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\n- zh\npipeline_tag: image-to-video\nlibrary_name: diffusers\ntags:\n- video\n- video genration\n---\n# Wan2.1\n\n<p align="center">\n    <img src="assets/logo.png" width="400"/>\n<p>\n\n<p align="center">\n    ðŸ’œ <a href=""><b>Wan</b></a> &nbsp&nbsp ï½œ &nbsp&nbsp ðŸ–¥ï¸ <a href="https://github.com/Wan-Video/Wan2.1">GitHub</a> &nbsp&nbsp  | &nbsp&nbspðŸ¤— <a href="https://huggingface.co/Wan-AI/">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspðŸ¤– <a href="https://modelscope.cn/organization/Wan-AI">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ðŸ“‘ <a href="">Paper (Coming soon)</a> &nbsp&nbsp | &nbsp&nbsp ðŸ“‘ <a href="https://wanxai.com">Blog</a> &nbsp&nbsp | &nbsp&nbspðŸ’¬ <a href="https://gw.alicdn.com/imgextra/i2/O1CN01tqjWFi1ByuyehkTSB_!!6000000000015-0-tps-611-1279.jpg">WeChat Group</a>&nbsp&nbsp | &nbsp&nbsp ðŸ“– <a href="https://discord.gg/p5XbdQV7">Discord</a>&nbsp&nbsp\n<br>\n\n-----\n\n[**Wan: Open and Advanced Large-Scale Video Generative Models**]() <be>\n\nIn this repository, we present **Wan2.1**, a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. **Wan2.1** offers these key features:\n- ðŸ‘ **SOTA Performance**: **Wan2.1** consistently outperforms existing open-source models and state-of-the-art commercial solutions across multiple benchmarks.\n- ðŸ‘ **Supports Consumer-grade GPUs**: The T2V-1.3B model requires only 8.19 GB VRAM, making it compatible with almost all consumer-grade GPUs. It can generate a 5-second 480P video on an RTX 4090 in about 4 minutes (without optimization techniques like quantization). Its performance is even comparable to some closed-source models.\n- ðŸ‘ **Multiple Tasks**: **Wan2.1** excels in Text-to-Video, Image-to-Video, Video Editing, Text-to-Image, and Video-to-Audio, advancing the field of video generation.\n- ðŸ‘ **Visual Text Generation**: **Wan2.1** is the first video model capable of generating both Chinese and English text, featuring robust text generation that enhances its practical applications.\n- ðŸ‘ **Powerful Video VAE**: **Wan-VAE** delivers exceptional efficiency and performance, encoding and decoding 1080P videos of any length while preserving temporal information, making it an ideal foundation for video and image generation.\n\n\nThis repository contains our I2V-14B model, which is capable of generating 720P high-definition videos. After thousands of rounds of human evaluations, this model has outperformed both closed-source and open-source alternatives, achieving state-of-the-art performance.\n\n\n## Video Demos\n\n<div align="center">\n    <video width="80%" controls>\n        <source src="https://cloud.video.taobao.com/vod/Jth64Y7wNoPcJki_Bo1ZJTDBvNjsgjlVKsNs05Fqfps.mp4" type="video/mp4">\n        Your browser does not support the video tag.\n    </video>\n</div>\n\n## ðŸ”¥ Latest News!!\n\n* Feb 25, 2025: ðŸ‘‹ We''ve released the inference code and weights of Wan2.1.\n\n\n## ðŸ“‘ Todo List\n- Wan2.1 Text-to-Video\n    - [x] Multi-GPU Inference code of the 14B and 1.3B models\n    - [x] Checkpoints of the 14B and 1.3B models\n    - [x] Gradio demo\n    - [ ] Diffusers integration\n    - [ ] ComfyUI integration\n- Wan2.1 Image-to-Video\n    - [x] Multi-GPU Inference code of the 14B model\n    - [x] Checkpoints of the 14B model\n    - [x] Gradio demo\n    - [ ] Diffusers integration\n    - [ ] ComfyUI integration\n\n\n## Quickstart\n\n#### Installation\nClone the repo:\n```\ngit clone https://github.com/Wan-Video/Wan2.1.git\ncd Wan2.1\n```\n\nInstall dependencies:\n```\n# Ensure torch >= 2.4.0\npip install -r requirements.txt\n```\n\n\n#### Model Download\n\n| Models        |                       Download Link                                           |    Notes                      |\n| --------------|-------------------------------------------------------------------------------|-------------------------------|\n| T2V-14B       |      ðŸ¤— [Huggingface](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B)      ðŸ¤– [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-T2V-14B)          | Supports both 480P and 720P\n| I2V-14B-720P  |      ðŸ¤— [Huggingface](https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P)    ðŸ¤– [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-720P)     | Supports 720P\n| I2V-14B-480P  |      ðŸ¤— [Huggingface](https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-480P)    ðŸ¤– [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-I2V-14B-480P)      | Supports 480P\n| T2V-1.3B      |      ðŸ¤— [Huggingface](https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B)     ðŸ¤– [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-T2V-1.3B)         | Supports 480P\n\n> ðŸ’¡Note: The 1.3B model is capable of generating videos at 720P resolution. However, due to limited training at this resolution, the results are generally less stable compared to 480P. For optimal performance, we recommend using 480P resolution.\n\n\nDownload models using ðŸ¤— huggingface-cli:\n```\npip install "huggingface_hub[cli]"\nhuggingface-cli download Wan-AI/Wan2.1-I2V-14B-720P --local-dir ./Wan2.1-I2V-14B-720P\n```\n\nDownload models using ðŸ¤– modelscope-cli:\n```\npip install modelscope\nmodelscope download Wan-AI/Wan2.1-I2V-14B-720P --local_dir ./Wan2.1-I2V-14B-720P\n```\n\n#### Run Image-to-Video Generation\n\nSimilar to Text-to-Video, Image-to-Video is also divided into processes with and without the prompt extension step. The specific parameters and their corresponding settings are as follows:\n<table>\n    <thead>\n        <tr>\n            <th rowspan="2">Task</th>\n            <th colspan="2">Resolution</th>\n            <th rowspan="2">Model</th>\n        </tr>\n        <tr>\n            <th>480P</th>\n            <th>720P</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>i2v-14B</td>\n            <td style="color: green;">âŒ</td>\n            <td style="color: green;">âœ”ï¸</td>\n            <td>Wan2.1-I2V-14B-720P</td>\n        </tr>\n        <tr>\n            <td>i2v-14B</td>\n            <td style="color: green;">âœ”ï¸</td>\n            <td style="color: red;">âŒ</td>\n            <td>Wan2.1-T2V-14B-480P</td>\n        </tr>\n    </tbody>\n</table>\n\n\n##### (1) Without Prompt Extention\n\n- Single-GPU inference\n```\npython generate.py --task i2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-I2V-14B-720P --image examples/i2v_input.JPG --prompt "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline''s intricate details and the refreshing atmosphere of the seaside."\n```\n\n> ðŸ’¡For the Image-to-Video task, the `size` parameter represents the area of the generated video, with the aspect ratio following that of the original input image.\n\n- Multi-GPU inference using FSDP + xDiT USP\n\n```\npip install "xfuser>=0.4.1"\ntorchrun --nproc_per_node=8 generate.py --task i2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-I2V-14B-720P --image examples/i2v_input.JPG --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline''s intricate details and the refreshing atmosphere of the seaside."\n```\n\n##### (2) Using Prompt Extention\n\n\nRun with local prompt extention using `Qwen/Qwen2.5-VL-7B-Instruct`:\n```\npython generate.py --task i2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-I2V-14B-720P --image examples/i2v_input.JPG --use_prompt_extend --prompt_extend_model Qwen/Qwen2.5-VL-7B-Instruct --prompt "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline''s intricate details and the refreshing atmosphere of the seaside."\n```\n\nRun with remote prompt extention using `dashscope`:\n```\nDASH_API_KEY=your_key python generate.py --task i2v-14B --size 1280*720 --ckpt_dir ./Wan2.1-I2V-14B-720P --image examples/i2v_input.JPG --use_prompt_extend --prompt_extend_method ''dashscope'' --prompt "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline''s intricate details and the refreshing atmosphere of the seaside."\n```\n\n##### (3) Runing local gradio\n\n```\ncd gradio\n# if one only uses 480P model in gradio\nDASH_API_KEY=your_key python i2v_14B_singleGPU.py --prompt_extend_method ''dashscope'' --ckpt_dir_480p ./Wan2.1-I2V-14B-480P\n\n# if one only uses 720P model in gradio\nDASH_API_KEY=your_key python i2v_14B_singleGPU.py --prompt_extend_method ''dashscope'' --ckpt_dir_720p ./Wan2.1-I2V-14B-720P\n\n# if one uses both 480P and 720P models in gradio\nDASH_API_KEY=your_key python i2v_14B_singleGPU.py --prompt_extend_method ''dashscope'' --ckpt_dir_480p ./Wan2.1-I2V-14B-480P --ckpt_dir_720p ./Wan2.1-I2V-14B-720P\n```\n\n\n## Manual Evaluation\n\nWe conducted extensive manual evaluations to evaluate the performance of the Image-to-Video model, and the results are presented in the table below. The results clearly indicate that **Wan2.1** outperforms both closed-source and open-source models.\n\n<div align="center">\n    <img src="assets/i2v_res.png" alt="" style="width: 80%;" />\n</div>\n\n\n## Computational Efficiency on Different GPUs\n\nWe test the computational efficiency of different **Wan2.1** models on different GPUs in the following table. The results are presented in the format: **Total time (s) / peak GPU memory (GB)**.\n\n\n<div align="center">\n    <img src="assets/comp_effic.png" alt="" style="width: 80%;" />\n</div>\n\n> The parameter settings for the tests presented in this table are as follows:\n> (1) For the 1.3B model on 8 GPUs, set `--ring_size 8` and `--ulysses_size 1`;\n> (2) For the 14B model on 1 GPU, use `--offload_model True`;\n> (3) For the 1.3B model on a single 4090 GPU, set `--offload_model True --t5_cpu`;\n> (4) For all testings, no prompt extension was applied, meaning `--use_prompt_extend` was not enabled.\n\n-------\n\n## Introduction of Wan2.1\n\n**Wan2.1**  is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. These include our novel spatio-temporal variational autoencoder (VAE), scalable training strategies, large-scale data construction, and automated evaluation metrics. Collectively, these contributions enhance the modelâ€™s performance and versatility.\n\n\n##### (1) 3D Variational Autoencoders\nWe propose a novel 3D causal VAE architecture, termed **Wan-VAE** specifically designed for video generation. By combining multiple strategies, we improve spatio-temporal compression, reduce memory usage, and ensure temporal causality. **Wan-VAE** demonstrates significant advantages in performance efficiency compared to other open-source VAEs. Furthermore, our **Wan-VAE** can encode and decode unlimited-length 1080P videos without losing historical temporal information, making it particularly well-suited for video generation tasks.\n\n\n<div align="center">\n    <img src="assets/video_vae_res.jpg" alt="" style="width: 80%;" />\n</div>\n\n\n##### (2) Video Diffusion DiT\n\n**Wan2.1** is designed using the Flow Matching framework within the paradigm of mainstream Diffusion Transformers. Our model''s architecture uses the T5 Encoder to encode multilingual text input, with cross-attention in each transformer block embedding the text into the model structure. Additionally, we employ an MLP with a Linear layer and a SiLU layer to process the input time embeddings and predict six modulation parameters individually. This MLP is shared across all transformer blocks, with each block learning a distinct set of biases. Our experimental findings reveal a significant performance improvement with this approach at the same parameter scale.\n\n<div align="center">\n    <img src="assets/video_dit_arch.jpg" alt="" style="width: 80%;" />\n</div>\n\n\n| Model  | Dimension | Input Dimension | Output Dimension | Feedforward Dimension | Frequency Dimension | Number of Heads | Number of Layers |\n|--------|-----------|-----------------|------------------|-----------------------|---------------------|-----------------|------------------|\n| 1.3B   | 1536      | 16              | 16               | 8960                  | 256                 | 12              | 30               |\n| 14B   | 5120       | 16              | 16               | 13824                 | 256                 | 40              | 40               |\n\n\n\n##### Data\n\nWe curated and deduplicated a candidate dataset comprising a vast amount of image and video data. During the data curation process, we designed a four-step data cleaning process, focusing on fundamental dimensions, visual quality and motion quality. Through the robust data processing pipeline, we can easily obtain high-quality, diverse, and large-scale training sets of images and videos.\n\n![figure1](assets/data_for_diff_stage.jpg "figure1")\n\n\n##### Comparisons to SOTA\nWe compared **Wan2.1** with leading open-source and closed-source models to evaluate the performace. Using our carefully designed set of 1,035 internal prompts, we tested across 14 major dimensions and 26 sub-dimensions. We then compute the total score by performing a weighted calculation on the scores of each dimension, utilizing weights derived from human preferences in the matching process. The detailed results are shown in the table below. These results demonstrate our model''s superior performance compared to both open-source and closed-source models.\n\n![figure1](assets/vben_vs_sota.png "figure1")\n\n\n## Citation\nIf you find our work helpful, please cite us.\n\n```\n@article{wan2.1,\n    title   = {Wan: Open and Advanced Large-Scale Video Generative Models},\n    author  = {Wan Team},\n    journal = {},\n    year    = {2025}\n}\n```\n\n## License Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generate contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the [license](LICENSE.txt).\n\n\n## Acknowledgements\n\nWe would like to thank the contributors to the [SD3](https://huggingface.co/stabilityai/stable-diffusion-3-medium), [Qwen](https://huggingface.co/Qwen), [umt5-xxl](https://huggingface.co/google/umt5-xxl), [diffusers](https://github.com/huggingface/diffusers) and [HuggingFace](https://huggingface.co) repositories, for their open research.\n\n\n\n## Contact Us\nIf you would like to leave a message to our research or product teams, feel free to join our [Discord](https://discord.gg/p5XbdQV7) or [WeChat groups](https://gw.alicdn.com/imgextra/i2/O1CN01tqjWFi1ByuyehkTSB_!!6000000000015-0-tps-611-1279.jpg)!', '{"pipeline_tag":"image-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":172376999091,"files_count":32,"spaces_count":25,"gated":false,"private":false,"config":{"model_type":"i2v"}}', '[]', '[{"type":"has_code","target_id":"github:Wan-Video:Wan2.1\">GitHub<","source_url":"https://github.com/Wan-Video/Wan2.1\">GitHub<"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.1.git","source_url":"https://github.com/Wan-Video/Wan2.1.git"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"}]', NULL, 'Apache-2.0', 'approved', 97.4, '2f10d423d946877326a61ff8b28f9527', NULL, 'https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P/resolve/main/assets/comp_effic.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-Wan-AI-Wan2.1-I2V-14B-720P from https://huggingface.co/Wan-AI/Wan2.1-I2V-14B-720P/resolve/main/assets/comp_effic.png
Image converted to WebP: data/images/huggingface-Wan-AI-Wan2.1-I2V-14B-720P.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Intel-neural-chat-7b-v3-1', 'huggingface--intel--neural-chat-7b-v3-1', 'neural-chat-7b-v3-1', 'Intel', '--- license: apache-2.0 tags: - LLMs - mistral - Intel pipeline_tag: text-generation base_model: mistralai/Mistral-7B-v0.1 model-index: - name: neural-chat-7b-v3-1 results: - task: type: Large Language Model name: Large Language Model dataset: type: Open-Orca/SlimOrca name: Open-Orca/SlimOrca metrics: - type: ARC (25-shot) value: 66.21 name: ARC (25-shot) verified: true - type: HellaSwag (10-shot) value: 83.64 name: HellaSwag (10-shot) verified: true - type: MMLU (5-shot) value: 62.37 name: M...', '["transformers","pytorch","safetensors","mistral","text-generation","llms","intel","conversational","en","dataset:open-orca/slimorca","arxiv:2306.02707","base_model:mistralai/mistral-7b-v0.1","base_model:finetune:mistralai/mistral-7b-v0.1","license:apache-2.0","model-index","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 547, 1333, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Intel/neural-chat-7b-v3-1","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ntags:\n- LLMs\n- mistral\n- Intel\npipeline_tag: text-generation\nbase_model: mistralai/Mistral-7B-v0.1\nmodel-index:\n- name: neural-chat-7b-v3-1\n  results:\n  - task:\n      type: Large Language Model\n      name: Large Language Model\n    dataset:\n      type: Open-Orca/SlimOrca\n      name: Open-Orca/SlimOrca\n    metrics:\n    - type: ARC (25-shot)\n      value: 66.21\n      name: ARC (25-shot)\n      verified: true\n    - type: HellaSwag (10-shot)\n      value: 83.64\n      name: HellaSwag (10-shot)\n      verified: true\n    - type: MMLU (5-shot)\n      value: 62.37\n      name: MMLU (5-shot)\n      verified: true\n    - type: TruthfulQA (0-shot)\n      value: 59.65\n      name: TruthfulQA (0-shot)\n      verified: true\n    - type: Winogrande (5-shot)\n      value: 78.14\n      name: Winogrande (5-shot)\n      verified: true\n    - type: GSM8K (5-shot)\n      value: 19.56\n      name: GSM8K (5-shot)\n      verified: true\n    - type: DROP (3-shot)\n      value: 43.84\n      name: DROP (3-shot)\n      verified: true\ndatasets:\n- Open-Orca/SlimOrca\nlanguage:\n- en\n---\n\n## Model Details: Neural-Chat-v3-1\n\nThis model is a fine-tuned 7B parameter LLM on the Intel Gaudi 2 processor from the [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) on the open source dataset [Open-Orca/SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca). The model was aligned using the Direct Performance Optimization (DPO) method with [Intel/orca_dpo_pairs](https://huggingface.co/datasets/Intel/orca_dpo_pairs). For more information, refer to the Medium article [The Practice of Supervised Fine-tuning and Direct Preference Optimization on Intel Gaudi2](https://medium.com/@NeuralCompressor/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3).\n\n<p align="center">\n  <img src="https://cdn-uploads.huggingface.co/production/uploads/6297f0e30bd2f58c647abb1d/ctASHUT5QYIxMsOFa-sHC.webp" width="500"/>\n  Photo by Google DeepMind on Unsplash\n</p>\n\n| Model Detail | Description |\n| ----------- | ----------- | \n| Model Authors - Company | Intel. The NeuralChat team with members from DCAI/AISE/AIPT. Core team members: Kaokao Lv, Liang Lv, Chang Wang, Wenxin Zhang, Xuhui Ren, and Haihao Shen.| \n| Date | October, 2023 | \n| Version | v3-1 | \n| Type | 7B Large Language Model | \n| Paper or Other Resources | [Medium Blog](https://medium.com/@NeuralCompressor/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3) | \n| License | Apache 2.0 |\n| Questions or Comments | [Community Tab](https://huggingface.co/Intel/neural-chat-7b-v3-1/discussions) and [Intel DevHub Discord](https://discord.gg/rv2Gp55UJQ)|\n\n| Intended Use | Description |\n| ----------- | ----------- | \n| Primary intended uses | You can use the fine-tuned model for several language-related tasks. Checkout the [LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) to see how this model is doing. | \n| Primary intended users | Anyone doing inference on language-related tasks. | \n| Out-of-scope uses | This model in most cases will need to be fine-tuned for your particular task.  The model should not be used to intentionally create hostile or alienating environments for people.|\n\n## How To Use\n\nContext length for this model: 8192 tokens (same as https://huggingface.co/mistralai/Mistral-7B-v0.1)\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n- learning_rate: 1e-04\n- train_batch_size: 1\n- eval_batch_size: 2\n- seed: 42\n- distributed_type: multi-HPU\n- num_devices: 8\n- gradient_accumulation_steps: 8\n- total_train_batch_size: 64\n- total_eval_batch_size: 8\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: cosine\n- lr_scheduler_warmup_ratio: 0.03\n- num_epochs: 2.0\n\n### Reproduce the model\nHere is the sample code to reproduce the model: [GitHub sample code](https://github.com/intel/intel-extension-for-transformers/blob/main/intel_extension_for_transformers/neural_chat/examples/finetuning/finetune_neuralchat_v3). Here is the documentation to reproduce building the model:\n\n```bash\ngit clone https://github.com/intel/intel-extension-for-transformers.git\ncd intel-extension-for-transformers\n\ndocker build --no-cache ./ --target hpu --build-arg REPO=https://github.com/intel/intel-extension-for-transformers.git --build-arg ITREX_VER=main -f ./intel_extension_for_transformers/neural_chat/docker/Dockerfile -t chatbot_finetuning:latest\n\ndocker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host chatbot_finetuning:latest\n\n# after entering docker container\ncd examples/finetuning/finetune_neuralchat_v3\n\n```\nWe select the latest pretrained mistralai/Mistral-7B-v0.1 and the open source dataset Open-Orca/SlimOrca to conduct the experiment.\n\nThe below script use deepspeed zero2 to lanuch the training with 8 cards Gaudi2. In the `finetune_neuralchat_v3.py`, the default `use_habana=True, use_lazy_mode=True, device="hpu"` for Gaudi2. And if you want to run it on NVIDIA GPU, you can set them `use_habana=False, use_lazy_mode=False, device="auto"`.\n\n```python\ndeepspeed --include localhost:0,1,2,3,4,5,6,7 \\n    --master_port 29501 \\n    finetune_neuralchat_v3.py\n```\n\nMerge the LoRA weights:\n\n```python\npython apply_lora.py \\n    --base-model-path mistralai/Mistral-7B-v0.1 \\n    --lora-model-path finetuned_model/ \\n    --output-path finetuned_model_lora\n```\n\n\n### FP32 Inference with Transformers\n\n```python\nimport transformers\n\nmodel_name = ''Intel/neural-chat-7b-v3-1''\nmodel = transformers.AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\ndef generate_response(system_input, user_input):\n\n    # Format the input using the provided template\n    prompt = f"### System:\n{system_input}\n### User:\n{user_input}\n### Assistant:\n"\n\n    # Tokenize and encode the prompt\n    inputs = tokenizer.encode(prompt, return_tensors="pt", add_special_tokens=False)\n\n    # Generate a response\n    outputs = model.generate(inputs, max_length=1000, num_return_sequences=1)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    # Extract only the assistant''s response\n    return response.split("### Assistant:\n")[-1]\n\n\n# Example usage\nsystem_input = "You are a math expert assistant. Your mission is to help users understand and solve various math problems. You should provide step-by-step solutions, explain reasonings and give the correct answer."\nuser_input = "calculate 100 + 520 + 60"\nresponse = generate_response(system_input, user_input)\nprint(response)\n\n# expected response\n"""\nTo calculate the sum of 100, 520, and 60, we will follow these steps:\n\n1. Add the first two numbers: 100 + 520\n2. Add the result from step 1 to the third number: (100 + 520) + 60\n\nStep 1: Add 100 and 520\n100 + 520 = 620\n\nStep 2: Add the result from step 1 to the third number (60)\n(620) + 60 = 680\n\nSo, the sum of 100, 520, and 60 is 680.\n"""\n```\n\n### BF16 Inference with Intel Extension for Transformers and Intel Extension for Pytorch\n```python\nfrom transformers import AutoTokenizer, TextStreamer\nimport torch\nfrom intel_extension_for_transformers.transformers import AutoModelForCausalLM\nimport intel_extension_for_pytorch as ipex\n\nmodel_name = "Intel/neural-chat-7b-v3-1"\nprompt = "Once upon a time, there existed a little girl,"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ninputs = tokenizer(prompt, return_tensors="pt").input_ids\nstreamer = TextStreamer(tokenizer)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\nmodel = ipex.optimize(model.eval(), dtype=torch.bfloat16, inplace=True, level="O1", auto_kernel_selection=True)\n\noutputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)\n```\n\n\n### INT4 Inference with Transformers and Intel Extension for Transformers\n```python\nfrom transformers import AutoTokenizer, TextStreamer\nfrom intel_extension_for_transformers.transformers import AutoModelForCausalLM, WeightOnlyQuantConfig\nmodel_name = "Intel/neural-chat-7b-v3-1"\n\n# for int8, should set weight_dtype="int8"       \nconfig = WeightOnlyQuantConfig(compute_dtype="bf16", weight_dtype="int4")\nprompt = "Once upon a time, there existed a little girl,"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ninputs = tokenizer(prompt, return_tensors="pt").input_ids\nstreamer = TextStreamer(tokenizer)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=config)\noutputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)\n\n```\n\n| Factors | Description | \n| ----------- | ----------- | \n| Groups | More details about the dataset and annotations can be found at [Open-Orca/SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca) and the associated paper at https://arxiv.org/abs/2306.02707. | \n| Instrumentation | The performance of the model can vary depending on the inputs to the model. In this case, the prompts provided can drastically change the prediction of the language model. |\n| Environment | The model was trained on the Intel Gaudi 2 processor (8 cards).  |\n| Card Prompts | Model deployment on alternate hardware and software will change model performance. The model evaluation factors are from the Hugging Face LLM leaderboard: ARC, HellaSwag, MMLU, TruthfulQA, Winogrande, GSM8K, and DROP (see Quantitative Analyses below). |\n\n| Metrics | Description | \n| ----------- | ----------- | \n| Model performance measures | The model performance was evaluated against other LLMs according to the measures on the LLM leaderboard. These were selected as this has become the standard for LLM performance. |\n| Decision thresholds | No decision thresholds were used. | \n| Approaches to uncertainty and variability | - | \n\n| Training and Evaluation Data | Description | \n| ----------- | ----------- | \n| Datasets | The training data are from [Open-Orca/SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca). There is no contamination from the GSM8k test set, as this is not a part of the Open-Orca/SlimOrca dataset.|\n| Motivation | - |\n| Preprocessing | - | \n\n## Quantitative Analyses \nThe model was submitted to the [LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard). The detailed submission can be found here: [https://huggingface.co/datasets/open-llm-leaderboard/details_Intel__neural-chat-7b-v3-1](https://huggingface.co/datasets/open-llm-leaderboard/details_Intel__neural-chat-7b-v3-1). The metrics can be found below and show that the model has significantly improved performance from Mistral-7B-v0.1 and neural-chat-7b-v3.\n\n| Model | Average â¬†ï¸| ARC (25-s) â¬†ï¸ | HellaSwag (10-s) â¬†ï¸ | MMLU (5-s) â¬†ï¸| TruthfulQA (MC) (0-s) â¬†ï¸ | Winogrande (5-s) | GSM8K (5-s) | DROP (3-s) |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|[mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) | 50.32 | 59.58  | 83.31  | 64.16  | 42.15 | 78.37 | 18.12 | 6.14 |\n| [Intel/neural-chat-7b-v3](https://huggingface.co/Intel/neural-chat-7b-v3) | **57.31** | 67.15 | 83.29 | 62.26  | 58.77 | 78.06 | 1.21 | 50.43 |\n| [Intel/neural-chat-7b-v3-1](https://huggingface.co/Intel/neural-chat-7b-v3-1) | **59.06** | 66.21 | 83.64 | 62.37  | 59.65 | 78.14 | 19.56 | 43.84 |\n\n## Testing Model Quantizability\nThe following code block can be run to determine, for PyTorch models, if that model is amenable to quantization.  \nOne caveat - the Intel Extension for PyTorch uses optimum ipex, which is pre-release and needs further testing.\n\nTo install the dependencies, you should first install Intel Extensions for PyTorch and tehn pip install each of the following dependencies:\n- torch\n- optimum.intel\n- optimum[ipex]\n- transformers\n\n### Intel Extension for PyTorch method:\nIn this case, we are testing if neural-chat-7b-v3-1 can be quantized and this testing method demonstrates the model size change, for example:\nwhen the base type is specified to be torch.bfloat16 but also specifying that load_in_4bit=True which causes the weights only to be quantized we see an output from the model testing as follows:\n- **model_quantize_internal: model size  = 27625.02 MB**\n- **model_quantize_internal: quant size  =  4330.80 MB**\n\nThis code should run from within a python script - such as ipex_test.py as follows:\n```python\nimport torch\nimport os\nfrom transformers import AutoTokenizer\nfrom intel_extension_for_transformers.transformers import AutoModelForCausalLM, pipeline\nmodel_name = "Intel/neural-chat-7b-v3-1"     \nprompt = "Once upon a time, there existed a little girl,"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ninputs = tokenizer(prompt, return_tensors="pt").input_ids\n\nresult = {torch.bfloat16:"failed"}\ntyp = torch.bfloat16\ntry:\n    model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True,  torch_dtype = typ)\n    outputs = model.generate(inputs, max_new_tokens=20)\n    result[typ] = f"passed, {os.stat(model.bin_file).st_size}"\nexcept:\n    result[typ] = "failed"\n\n    \nprint("\n\nResults of quantizing: ")  \n# determine if Quantized\nwith open(r"output.log", ''r'') as fp:\n    for l_no, line in enumerate(fp):\n        # search string\n        if ''model_quantize_internal'' in line:\n            print(line)\n            \nprint("\n\nExecution results ")\nfor k,v in result.items():\n    print(k,v)\n    \nprint("\n\nModel Output: ")\ntokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n```\nRun the code as folows from a bash terminal:\n```bash\npython ipex_test.py 2>&1 | tee output.log\n```\nThe entire output is captured in the output.log but it will be summarized, \nalong with output from the model indicating either pass or fail of the quantization as well as model output for a given prompt.\n\n\n## Ethical Considerations and Limitations\nNeural-chat-7b-v3-1 can produce factually incorrect output, and should not be relied on to produce factually accurate information. Because of the limitations of the pretrained model and the finetuning datasets, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\n\nTherefore, before deploying any applications of neural-chat-7b-v3-1, developers should perform safety testing.\n\n## Caveats and Recommendations\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model.\n\nHere are a couple of useful links to learn more about Intel''s AI software:\n* Intel Neural Compressor [link](https://github.com/intel/neural-compressor)\n* Intel Extension for Transformers [link](https://github.com/intel/intel-extension-for-transformers)\n\n## Disclaimer\n\nThe license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please cosult an attorney before using this model for commercial purposes.', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7241732096,"storage_bytes":28967554886,"files_count":15,"spaces_count":65,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":true,"chat_template":"{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content''] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{%- for message in loop_messages %}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}\n        {{- raise_exception(''After the optional system message, conversation roles must alternate user/assistant/user/assistant/...'') }}\n    {%- endif %}\n    {%- if loop.first and system_message is defined %}\n            {{- ''### System:\n'' + system_message + ''\n'' }}\n           {%- endif %}\n     {%- if message[''role''] == ''user'' %}\n  {{- ''### User:\n'' + message[''content''] + '' \n'' }}\n       {%- elif message[''role''] == ''assistant'' %}\n        {{- ''### Assistant:\n'' + message[''content''] + eos_token + ''\n''}}\n    {%- else %}\n        {{- raise_exception(''Only user and assistant roles are supported, with the exception of an initial optional system message!'') }}\n    {%- endif %}\n{%- endfor %}{% if add_generation_prompt %}{{ ''### Assistant:\n'' }}{% endif %}\n"}}}', '[]', '[{"type":"has_code","target_id":"github:intel:intel-extension-for-transformers","source_url":"https://github.com/intel/intel-extension-for-transformers"},{"type":"has_code","target_id":"github:intel:intel-extension-for-transformers.git","source_url":"https://github.com/intel/intel-extension-for-transformers.git"},{"type":"has_code","target_id":"github:intel:intel-extension-for-transformers.git","source_url":"https://github.com/intel/intel-extension-for-transformers.git"},{"type":"has_code","target_id":"github:intel:neural-compressor","source_url":"https://github.com/intel/neural-compressor"},{"type":"has_code","target_id":"github:intel:intel-extension-for-transformers","source_url":"https://github.com/intel/intel-extension-for-transformers"},{"type":"based_on_paper","target_id":"arxiv:2306.02707","source_url":"https://arxiv.org/abs/2306.02707"}]', NULL, 'Apache-2.0', 'approved', 77.4, '98016336d15481f4918ad2f4bdb88e68', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-Phi-3-mini-4k-instruct-gguf', 'huggingface--microsoft--phi-3-mini-4k-instruct-gguf', 'Phi-3-mini-4k-instruct-gguf', 'microsoft', '--- license: mit license_link: >- https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/LICENSE language: - en pipeline_tag: text-generation tags: - nlp - code --- This repo provides the GGUF format for the Phi-3-Mini-4K-Instruct. The Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoni...', '["gguf","nlp","code","text-generation","en","license:mit","endpoints_compatible","region:us","conversational"]', 'text-generation', 546, 92706, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlicense_link: >-\n  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- nlp\n- code\n---\n\n## Model Summary\n\nThis repo provides the GGUF format for the Phi-3-Mini-4K-Instruct. \nThe Phi-3-Mini-4K-Instruct is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties.\nThe model belongs to the Phi-3 family with the Mini version in two variants [4K](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) and [128K](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) which is the context length (in tokens) it can support. \nThe model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\nWhen assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters.\n\nResources and Technical Documentation:\n\n+ [Phi-3 Microsoft Blog](https://aka.ms/phi3blog-april)\n+ [Phi-3 Technical Report](https://aka.ms/phi3-tech-report)\n+ [Phi-3 on Azure AI Studio](https://aka.ms/phi3-azure-ai)\n+ [Phi-3 on Hugging Face](https://aka.ms/phi3-hf)\n+ Phi-3 ONNX: [4K](https://aka.ms/phi3-mini-4k-instruct-onnx) and [128K](https://aka.ms/phi3-mini-128k-instruct-onnx)\n\nThis repo provides GGUF files for the Phi-3 Mini-4K-Instruct model. \n| Name | Quant method | Bits | Size | Use case |\n| ---- | ---- | ---- | ---- | ----- |\n| [Phi-3-mini-4k-instruct-q4.gguf](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/blob/main/Phi-3-mini-4k-instruct-q4.gguf) | Q4_K_M | 4 | 2.2 GB| medium, balanced quality - recommended |\n| [Phi-3-mini-4k-instruct-fp16.gguf](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/blob/main/Phi-3-mini-4k-instruct-fp16.gguf) | None | 16 | 7.2 GB | minimal quality loss |\n\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for commercial and research use in English. The model provides uses for applications which require \n1) memory/compute constrained environments\n2) latency bound scenarios\n3) strong reasoning (especially math and logic)\n4) long context\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. \n\n**Use case considerations**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. \nDevelopers  should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case. \nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.  \n\n\n## Chat Format:\n\nGiven the nature of the training data, the Phi-3-Mini-4K-instruct model is best suited for prompts using the chat format as follows. \nYou can provide the prompt as a question with a generic template as follow:\n```markdown\n<|user|>\nQuestion <|end|>\n<|assistant|>\n```\nFor example:\n```markdown\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|>\n```\n\nwhere the model generates the text after "<|assistant|>" . In case of few-shots prompt, the prompt can be formatted as the following:\n\n```markdown\n<|user|>\nI am going to Paris, what should I see?<|end|>\n<|assistant|>\nParis, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\n\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\n2. The Louvre Museum: The Louvre is one of the world''s largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\n\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it''s no wonder that Paris is one of the most popular tourist destinations in the world."<|end|>\n<|user|>\nWhat is so great about #1?<|end|>\n<|assistant|>\n```\n\n## How to download GGUF files\n\n1. **Install Hugging Face CLI:**\n\n```\npip install huggingface-hub>=0.17.1\n```\n\n2. **Login to Hugging Face:**\n```\nhuggingface-cli login\n```\n\n3. **Download the GGUF model:**\n```\nhuggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Phi-3-mini-4k-instruct-q4.gguf --local-dir . --local-dir-use-symlinks False\n```\n\n## How to use with Ollama\n\n1. **Install Ollama:**\n\n```\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n2. **Run the *phi3* model:**\n\n```\nollama run phi3\n```\n\n### Building from `Modelfile`\n\nAssuming that you have already downloaded GGUF files, here is how you can use them with [Ollama](https://ollama.com/):\n\n1. **Get the Modelfile:**\n\n```\nhuggingface-cli download microsoft/Phi-3-mini-4k-instruct-gguf Modelfile_q4 --local-dir /path/to/your/local/dir\n```\n\n2. Build the Ollama Model:\nUse the Ollama CLI to create your model with the following command:\n\n```\nollama create phi3 -f Modelfile_q4\n```\n\n3. **Run the *phi3* model:** \n\nNow you can run the Phi-3-Mini-4k-Instruct model with Ollama using the following command:\n\n```\nollama run phi3 "Your prompt here"\n```\n\nReplace "Your prompt here" with the actual prompt you want to use for generating responses from the model.\n\n## How to use with Llamafile:\n\nAssuming that you already have GGUF files downloaded. Here is how you can use the GGUF model with [Llamafile](https://github.com/Mozilla-Ocho/llamafile):\n\n1. **Download Llamafile-0.7.3**\n```\nwget https://github.com/Mozilla-Ocho/llamafile/releases/download/0.7.3/llamafile-0.7.3\n```\n2. **Run the model with chat format prompt:**\n\n\n```markdown\n<|user|>\nHow to explain Internet for a medieval knight?\n<|end|>\n<|assistant|>\n```\n\n\n```\n./llamafile-0.7.3 -ngl 9999 -m Phi-3-mini-4k-instruct-q4.gguf --temp 0.6 -p "<|user|>\nHow to explain Internet for a medieval knight?\n<|end|>\n<|assistant|>"\n```\n\n3. **Run with a chat interface:**\n\n```\n./llamafile-0.7.3 -ngl 9999 -m Phi-3-mini-4k-instruct-q4.gguf\n```\n\nYour browser should open automatically and display a chat interface. (If it doesn''t, just open your browser and point it at http://localhost:8080)\n\n## How to run on Python:\n\n1. **Install llama-cpp-python:**\n\n```\n! CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python\n```\n\n2. **Run the model:**\n\n```python\nfrom llama_cpp import Llama\n\n\nllm = Llama(\n  model_path="./Phi-3-mini-4k-instruct-q4.gguf",  # path to GGUF file\n  n_ctx=4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\n  n_threads=8, # The number of CPU threads to use, tailor to your system and the resulting performance\n  n_gpu_layers=35, # The number of layers to offload to GPU, if you have GPU acceleration available. Set to 0 if no GPU acceleration is available on your system.\n)\n\nprompt = "How to explain Internet to a medieval knight?"\n\n# Simple inference example\noutput = llm(\n  f"<|user|>\n{prompt}<|end|>\n<|assistant|>",\n  max_tokens=256,  # Generate up to 256 tokens\n  stop=["<|end|>"], \n  echo=True,  # Whether to echo the prompt\n)\n\nprint(output[''choices''][0][''text''])\n```\n\n## Responsible AI Considerations\nLike other language models, the Phi series models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:  \n+ Quality of Service: the Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: these models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as "typing, math, random, collections, datetime, itertools". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.   \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include: \n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n\n## Training\n\n### Model\n\n* Architecture: Phi-3 Mini has 3.8B parameters and is a dense decoder-only Transformer model. The model is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidlines.\n* Inputs: Text. It is best suited for prompts using chat format.\n* Context length: 4K tokens\n* GPUS: 512 H100-80G\n* Training time: 7 days\n* Training data: 3.3T tokens\n* Outputs: Generated text in response to the input\n* Dates: Our models were trained between February and April 2024\n* Status: This is a static model trained on an offline dataset with cutoff date October 2023. Future versions of the tuned models may be released as we improve models.\n\n### Datasets\nOur training data includes a wide variety of sources, totaling 3.3 trillion tokens, and is a combination of \n1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code; \n2) newly created synthetic, â€œtextbook-likeâ€ data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.); \n3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\n### Software\n\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [DeepSpeed](https://github.com/microsoft/DeepSpeed)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n### License\n\nThe model is licensed under the [MIT license](https://huggingface.co/microsoft/phi-3-mini-128k/resolve/main/LICENSE).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must followâ€¯[Microsoftâ€™s Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-partyâ€™s policies.', '{"pipeline_tag":"text-generation","library_name":null,"framework":null,"params":null,"storage_bytes":32354200608,"files_count":10,"spaces_count":29,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:Mozilla-Ocho:llamafile","source_url":"https://github.com/Mozilla-Ocho/llamafile"},{"type":"has_code","target_id":"github:Mozilla-Ocho:llamafile","source_url":"https://github.com/Mozilla-Ocho/llamafile"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:microsoft:DeepSpeed","source_url":"https://github.com/microsoft/DeepSpeed"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:HazyResearch:flash-attention","source_url":"https://github.com/HazyResearch/flash-attention"}]', NULL, 'MIT', 'approved', 77.4, 'c8ae3f15029ff1becc8c53ca055e8a5b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Mistral-7B-v0.3', 'huggingface--mistralai--mistral-7b-v0.3', 'Mistral-7B-v0.3', 'mistralai', '--- library_name: vllm license: apache-2.0 extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href="https://mistral.ai/fr/terms/">Privacy Policy</a>. tags: - mistral-common --- The Mistral-7B-v0.3 Large Language Model (LLM) is a Mistral-7B-v0.2 with extended vocabulary. Mistral-7B-v0.3 has the following changes compared to Mistral-7B-v0.2 - Extended vocabulary to 32768 It is recommended to use with mistral-inference. For HF transf...', '["vllm","safetensors","mistral","mistral-common","license:apache-2.0","region:us"]', 'other', 546, 153334, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Mistral-7B-v0.3","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: vllm\nlicense: apache-2.0\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href="https://mistral.ai/fr/terms/">Privacy Policy</a>.\ntags:\n- mistral-common\n---\n\n# Model Card for Mistral-7B-v0.3\n\nThe Mistral-7B-v0.3 Large Language Model (LLM) is a Mistral-7B-v0.2 with extended vocabulary.\n\nMistral-7B-v0.3 has the following changes compared to [Mistral-7B-v0.2](https://huggingface.co/mistralai/Mistral-7B-v0.2/edit/main/README.md)\n- Extended vocabulary to 32768\n\n## Installation\n\nIt is recommended to use `mistralai/Mistral-7B-v0.3` with [mistral-inference](https://github.com/mistralai/mistral-inference). For HF transformers code snippets, please keep scrolling.\n\n```\npip install mistral_inference\n```\n\n## Download\n\n```py\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath(''mistral_models'', ''7B-v0.3'')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id="mistralai/Mistral-7B-v0.3", allow_patterns=["params.json", "consolidated.safetensors", "tokenizer.model.v3"], local_dir=mistral_models_path)\n```\n\n### Demo\n\nAfter installing `mistral_inference`, a `mistral-demo` CLI command should be available in your environment.\n\n```\nmistral-demo $HOME/mistral_models/7B-v0.3\n```\n\nShould give something along the following lines:\n\n```\nThis is a test of the emergency broadcast system. This is only a test.\n\nIf this were a real emergency, you would be told what to do.\n\nThis is a test\n=====================\nThis is another test of the new blogging software. Iâ€™m not sure if Iâ€™m going to keep it or not. Iâ€™m not sure if Iâ€™m going to keep\n=====================\nThis is a third test, mistral AI is very good at testing. ðŸ™‚\n\nThis is a third test, mistral AI is very good at testing. ðŸ™‚\n\nThis\n=====================\n```\n\n## Generate with `transformers`\n\nIf you want to use Hugging Face `transformers` to generate text, you can do something like this.\n\n```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = "mistralai/Mistral-7B-v0.3"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\ninputs = tokenizer("Hello my name is", return_tensors="pt")\n\noutputs = model.generate(**inputs, max_new_tokens=20)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n```\n\n## Limitations\n\nThe Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. \nIt does not have any moderation mechanisms. We''re looking forward to engaging with the community on ways to\nmake the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs.\n\n## The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur Mensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault, Blanche Savary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas, Eleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus Murke, Louis Martin, Louis Ternon, Lucile Saulnier, LÃ©lio Renard Lavaud, Margaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas Schuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thibaut Lavril, TimothÃ©e Lacroix, ThÃ©ophile Gervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall', '{"pipeline_tag":null,"library_name":"vllm","framework":"vllm","params":7248023552,"storage_bytes":28992746844,"files_count":15,"spaces_count":92,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:mistralai:mistral-inference","source_url":"https://github.com/mistralai/mistral-inference"}]', NULL, 'Apache-2.0', 'approved', 62.4, 'c4cd79cb63281edec23e4a2210fa5d0d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nomic-ai-nomic-embed-text-v1', 'huggingface--nomic-ai--nomic-embed-text-v1', 'nomic-embed-text-v1', 'nomic-ai', '--- library_name: sentence-transformers pipeline_tag: sentence-similarity tags: - feature-extraction - sentence-similarity - mteb - transformers - transformers.js model-index: - name: epoch_0_model results: - task: type: Classification dataset: type: mteb/amazon_counterfactual name: MTEB AmazonCounterfactualClassification (en) config: en split: test revision: e8379541af4e31359cca9fbcf4b00f2671dba205 metrics: - type: accuracy value: 76.8507462686567 - type: ap value: 40.592189159090495 - type:...', '["sentence-transformers","pytorch","onnx","safetensors","nomic_bert","feature-extraction","sentence-similarity","mteb","transformers","transformers.js","custom_code","en","arxiv:2402.01613","license:apache-2.0","model-index","text-embeddings-inference","endpoints_compatible","region:us"]', 'sentence-similarity', 545, 744333, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nomic-ai/nomic-embed-text-v1","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: sentence-transformers\npipeline_tag: sentence-similarity\ntags:\n- feature-extraction\n- sentence-similarity\n- mteb\n- transformers\n- transformers.js\nmodel-index:\n- name: epoch_0_model\n  results:\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en)\n      config: en\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 76.8507462686567\n    - type: ap\n      value: 40.592189159090495\n    - type: f1\n      value: 71.01634655512476\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_polarity\n      name: MTEB AmazonPolarityClassification\n      config: default\n      split: test\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n    metrics:\n    - type: accuracy\n      value: 91.51892500000001\n    - type: ap\n      value: 88.50346762975335\n    - type: f1\n      value: 91.50342077459624\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (en)\n      config: en\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 47.364\n    - type: f1\n      value: 46.72708080922794\n  - task:\n      type: Retrieval\n    dataset:\n      type: arguana\n      name: MTEB ArguAna\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 25.178\n    - type: map_at_10\n      value: 40.244\n    - type: map_at_100\n      value: 41.321999999999996\n    - type: map_at_1000\n      value: 41.331\n    - type: map_at_3\n      value: 35.016999999999996\n    - type: map_at_5\n      value: 37.99\n    - type: mrr_at_1\n      value: 25.605\n    - type: mrr_at_10\n      value: 40.422000000000004\n    - type: mrr_at_100\n      value: 41.507\n    - type: mrr_at_1000\n      value: 41.516\n    - type: mrr_at_3\n      value: 35.23\n    - type: mrr_at_5\n      value: 38.15\n    - type: ndcg_at_1\n      value: 25.178\n    - type: ndcg_at_10\n      value: 49.258\n    - type: ndcg_at_100\n      value: 53.776\n    - type: ndcg_at_1000\n      value: 53.995000000000005\n    - type: ndcg_at_3\n      value: 38.429\n    - type: ndcg_at_5\n      value: 43.803\n    - type: precision_at_1\n      value: 25.178\n    - type: precision_at_10\n      value: 7.831\n    - type: precision_at_100\n      value: 0.979\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_3\n      value: 16.121\n    - type: precision_at_5\n      value: 12.29\n    - type: recall_at_1\n      value: 25.178\n    - type: recall_at_10\n      value: 78.307\n    - type: recall_at_100\n      value: 97.866\n    - type: recall_at_1000\n      value: 99.57300000000001\n    - type: recall_at_3\n      value: 48.364000000000004\n    - type: recall_at_5\n      value: 61.451\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-p2p\n      name: MTEB ArxivClusteringP2P\n      config: default\n      split: test\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n    metrics:\n    - type: v_measure\n      value: 45.93034494751465\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-s2s\n      name: MTEB ArxivClusteringS2S\n      config: default\n      split: test\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\n    metrics:\n    - type: v_measure\n      value: 36.64579480054327\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/askubuntudupquestions-reranking\n      name: MTEB AskUbuntuDupQuestions\n      config: default\n      split: test\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\n    metrics:\n    - type: map\n      value: 60.601310529222054\n    - type: mrr\n      value: 75.04484896451656\n  - task:\n      type: STS\n    dataset:\n      type: mteb/biosses-sts\n      name: MTEB BIOSSES\n      config: default\n      split: test\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\n    metrics:\n    - type: cos_sim_pearson\n      value: 88.57797718095814\n    - type: cos_sim_spearman\n      value: 86.47064499110101\n    - type: euclidean_pearson\n      value: 87.4559602783142\n    - type: euclidean_spearman\n      value: 86.47064499110101\n    - type: manhattan_pearson\n      value: 87.7232764230245\n    - type: manhattan_spearman\n      value: 86.91222131777742\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77Classification\n      config: default\n      split: test\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\n    metrics:\n    - type: accuracy\n      value: 84.5422077922078\n    - type: f1\n      value: 84.47657456950589\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-p2p\n      name: MTEB BiorxivClusteringP2P\n      config: default\n      split: test\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\n    metrics:\n    - type: v_measure\n      value: 38.48953561974464\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-s2s\n      name: MTEB BiorxivClusteringS2S\n      config: default\n      split: test\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\n    metrics:\n    - type: v_measure\n      value: 32.75995857510105\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackAndroidRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 30.008000000000003\n    - type: map_at_10\n      value: 39.51\n    - type: map_at_100\n      value: 40.841\n    - type: map_at_1000\n      value: 40.973\n    - type: map_at_3\n      value: 36.248999999999995\n    - type: map_at_5\n      value: 38.096999999999994\n    - type: mrr_at_1\n      value: 36.481\n    - type: mrr_at_10\n      value: 44.818000000000005\n    - type: mrr_at_100\n      value: 45.64\n    - type: mrr_at_1000\n      value: 45.687\n    - type: mrr_at_3\n      value: 42.036\n    - type: mrr_at_5\n      value: 43.782\n    - type: ndcg_at_1\n      value: 36.481\n    - type: ndcg_at_10\n      value: 45.152\n    - type: ndcg_at_100\n      value: 50.449\n    - type: ndcg_at_1000\n      value: 52.76499999999999\n    - type: ndcg_at_3\n      value: 40.161\n    - type: ndcg_at_5\n      value: 42.577999999999996\n    - type: precision_at_1\n      value: 36.481\n    - type: precision_at_10\n      value: 8.369\n    - type: precision_at_100\n      value: 1.373\n    - type: precision_at_1000\n      value: 0.186\n    - type: precision_at_3\n      value: 18.693\n    - type: precision_at_5\n      value: 13.533999999999999\n    - type: recall_at_1\n      value: 30.008000000000003\n    - type: recall_at_10\n      value: 56.108999999999995\n    - type: recall_at_100\n      value: 78.55499999999999\n    - type: recall_at_1000\n      value: 93.659\n    - type: recall_at_3\n      value: 41.754999999999995\n    - type: recall_at_5\n      value: 48.296\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackEnglishRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 30.262\n    - type: map_at_10\n      value: 40.139\n    - type: map_at_100\n      value: 41.394\n    - type: map_at_1000\n      value: 41.526\n    - type: map_at_3\n      value: 37.155\n    - type: map_at_5\n      value: 38.785\n    - type: mrr_at_1\n      value: 38.153\n    - type: mrr_at_10\n      value: 46.369\n    - type: mrr_at_100\n      value: 47.072\n    - type: mrr_at_1000\n      value: 47.111999999999995\n    - type: mrr_at_3\n      value: 44.268\n    - type: mrr_at_5\n      value: 45.389\n    - type: ndcg_at_1\n      value: 38.153\n    - type: ndcg_at_10\n      value: 45.925\n    - type: ndcg_at_100\n      value: 50.394000000000005\n    - type: ndcg_at_1000\n      value: 52.37500000000001\n    - type: ndcg_at_3\n      value: 41.754000000000005\n    - type: ndcg_at_5\n      value: 43.574\n    - type: precision_at_1\n      value: 38.153\n    - type: precision_at_10\n      value: 8.796\n    - type: precision_at_100\n      value: 1.432\n    - type: precision_at_1000\n      value: 0.189\n    - type: precision_at_3\n      value: 20.318\n    - type: precision_at_5\n      value: 14.395\n    - type: recall_at_1\n      value: 30.262\n    - type: recall_at_10\n      value: 55.72200000000001\n    - type: recall_at_100\n      value: 74.97500000000001\n    - type: recall_at_1000\n      value: 87.342\n    - type: recall_at_3\n      value: 43.129\n    - type: recall_at_5\n      value: 48.336\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGamingRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 39.951\n    - type: map_at_10\n      value: 51.248000000000005\n    - type: map_at_100\n      value: 52.188\n    - type: map_at_1000\n      value: 52.247\n    - type: map_at_3\n      value: 48.211\n    - type: map_at_5\n      value: 49.797000000000004\n    - type: mrr_at_1\n      value: 45.329\n    - type: mrr_at_10\n      value: 54.749\n    - type: mrr_at_100\n      value: 55.367999999999995\n    - type: mrr_at_1000\n      value: 55.400000000000006\n    - type: mrr_at_3\n      value: 52.382\n    - type: mrr_at_5\n      value: 53.649\n    - type: ndcg_at_1\n      value: 45.329\n    - type: ndcg_at_10\n      value: 56.847\n    - type: ndcg_at_100\n      value: 60.738\n    - type: ndcg_at_1000\n      value: 61.976\n    - type: ndcg_at_3\n      value: 51.59\n    - type: ndcg_at_5\n      value: 53.915\n    - type: precision_at_1\n      value: 45.329\n    - type: precision_at_10\n      value: 8.959\n    - type: precision_at_100\n      value: 1.187\n    - type: precision_at_1000\n      value: 0.134\n    - type: precision_at_3\n      value: 22.612\n    - type: precision_at_5\n      value: 15.273\n    - type: recall_at_1\n      value: 39.951\n    - type: recall_at_10\n      value: 70.053\n    - type: recall_at_100\n      value: 86.996\n    - type: recall_at_1000\n      value: 95.707\n    - type: recall_at_3\n      value: 56.032000000000004\n    - type: recall_at_5\n      value: 61.629999999999995\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGisRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 25.566\n    - type: map_at_10\n      value: 33.207\n    - type: map_at_100\n      value: 34.166000000000004\n    - type: map_at_1000\n      value: 34.245\n    - type: map_at_3\n      value: 30.94\n    - type: map_at_5\n      value: 32.01\n    - type: mrr_at_1\n      value: 27.345000000000002\n    - type: mrr_at_10\n      value: 35.193000000000005\n    - type: mrr_at_100\n      value: 35.965\n    - type: mrr_at_1000\n      value: 36.028999999999996\n    - type: mrr_at_3\n      value: 32.806000000000004\n    - type: mrr_at_5\n      value: 34.021\n    - type: ndcg_at_1\n      value: 27.345000000000002\n    - type: ndcg_at_10\n      value: 37.891999999999996\n    - type: ndcg_at_100\n      value: 42.664\n    - type: ndcg_at_1000\n      value: 44.757000000000005\n    - type: ndcg_at_3\n      value: 33.123000000000005\n    - type: ndcg_at_5\n      value: 35.035\n    - type: precision_at_1\n      value: 27.345000000000002\n    - type: precision_at_10\n      value: 5.763\n    - type: precision_at_100\n      value: 0.859\n    - type: precision_at_1000\n      value: 0.108\n    - type: precision_at_3\n      value: 13.71\n    - type: precision_at_5\n      value: 9.401\n    - type: recall_at_1\n      value: 25.566\n    - type: recall_at_10\n      value: 50.563\n    - type: recall_at_100\n      value: 72.86399999999999\n    - type: recall_at_1000\n      value: 88.68599999999999\n    - type: recall_at_3\n      value: 37.43\n    - type: recall_at_5\n      value: 41.894999999999996\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackMathematicaRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 16.663\n    - type: map_at_10\n      value: 23.552\n    - type: map_at_100\n      value: 24.538\n    - type: map_at_1000\n      value: 24.661\n    - type: map_at_3\n      value: 21.085\n    - type: map_at_5\n      value: 22.391\n    - type: mrr_at_1\n      value: 20.025000000000002\n    - type: mrr_at_10\n      value: 27.643\n    - type: mrr_at_100\n      value: 28.499999999999996\n    - type: mrr_at_1000\n      value: 28.582\n    - type: mrr_at_3\n      value: 25.083\n    - type: mrr_at_5\n      value: 26.544\n    - type: ndcg_at_1\n      value: 20.025000000000002\n    - type: ndcg_at_10\n      value: 28.272000000000002\n    - type: ndcg_at_100\n      value: 33.353\n    - type: ndcg_at_1000\n      value: 36.454\n    - type: ndcg_at_3\n      value: 23.579\n    - type: ndcg_at_5\n      value: 25.685000000000002\n    - type: precision_at_1\n      value: 20.025000000000002\n    - type: precision_at_10\n      value: 5.187\n    - type: precision_at_100\n      value: 0.897\n    - type: precision_at_1000\n      value: 0.13\n    - type: precision_at_3\n      value: 10.987\n    - type: precision_at_5\n      value: 8.06\n    - type: recall_at_1\n      value: 16.663\n    - type: recall_at_10\n      value: 38.808\n    - type: recall_at_100\n      value: 61.305\n    - type: recall_at_1000\n      value: 83.571\n    - type: recall_at_3\n      value: 25.907999999999998\n    - type: recall_at_5\n      value: 31.214\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackPhysicsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.695999999999998\n    - type: map_at_10\n      value: 37.018\n    - type: map_at_100\n      value: 38.263000000000005\n    - type: map_at_1000\n      value: 38.371\n    - type: map_at_3\n      value: 34.226\n    - type: map_at_5\n      value: 35.809999999999995\n    - type: mrr_at_1\n      value: 32.916000000000004\n    - type: mrr_at_10\n      value: 42.067\n    - type: mrr_at_100\n      value: 42.925000000000004\n    - type: mrr_at_1000\n      value: 42.978\n    - type: mrr_at_3\n      value: 39.637\n    - type: mrr_at_5\n      value: 41.134\n    - type: ndcg_at_1\n      value: 32.916000000000004\n    - type: ndcg_at_10\n      value: 42.539\n    - type: ndcg_at_100\n      value: 47.873\n    - type: ndcg_at_1000\n      value: 50.08200000000001\n    - type: ndcg_at_3\n      value: 37.852999999999994\n    - type: ndcg_at_5\n      value: 40.201\n    - type: precision_at_1\n      value: 32.916000000000004\n    - type: precision_at_10\n      value: 7.5840000000000005\n    - type: precision_at_100\n      value: 1.199\n    - type: precision_at_1000\n      value: 0.155\n    - type: precision_at_3\n      value: 17.485\n    - type: precision_at_5\n      value: 12.512\n    - type: recall_at_1\n      value: 27.695999999999998\n    - type: recall_at_10\n      value: 53.638\n    - type: recall_at_100\n      value: 76.116\n    - type: recall_at_1000\n      value: 91.069\n    - type: recall_at_3\n      value: 41.13\n    - type: recall_at_5\n      value: 46.872\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackProgrammersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 24.108\n    - type: map_at_10\n      value: 33.372\n    - type: map_at_100\n      value: 34.656\n    - type: map_at_1000\n      value: 34.768\n    - type: map_at_3\n      value: 30.830999999999996\n    - type: map_at_5\n      value: 32.204\n    - type: mrr_at_1\n      value: 29.110000000000003\n    - type: mrr_at_10\n      value: 37.979\n    - type: mrr_at_100\n      value: 38.933\n    - type: mrr_at_1000\n      value: 38.988\n    - type: mrr_at_3\n      value: 35.731\n    - type: mrr_at_5\n      value: 36.963\n    - type: ndcg_at_1\n      value: 29.110000000000003\n    - type: ndcg_at_10\n      value: 38.635000000000005\n    - type: ndcg_at_100\n      value: 44.324999999999996\n    - type: ndcg_at_1000\n      value: 46.747\n    - type: ndcg_at_3\n      value: 34.37\n    - type: ndcg_at_5\n      value: 36.228\n    - type: precision_at_1\n      value: 29.110000000000003\n    - type: precision_at_10\n      value: 6.963\n    - type: precision_at_100\n      value: 1.146\n    - type: precision_at_1000\n      value: 0.152\n    - type: precision_at_3\n      value: 16.400000000000002\n    - type: precision_at_5\n      value: 11.552999999999999\n    - type: recall_at_1\n      value: 24.108\n    - type: recall_at_10\n      value: 49.597\n    - type: recall_at_100\n      value: 73.88900000000001\n    - type: recall_at_1000\n      value: 90.62400000000001\n    - type: recall_at_3\n      value: 37.662\n    - type: recall_at_5\n      value: 42.565\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 25.00791666666667\n    - type: map_at_10\n      value: 33.287749999999996\n    - type: map_at_100\n      value: 34.41141666666667\n    - type: map_at_1000\n      value: 34.52583333333333\n    - type: map_at_3\n      value: 30.734416666666668\n    - type: map_at_5\n      value: 32.137166666666666\n    - type: mrr_at_1\n      value: 29.305666666666664\n    - type: mrr_at_10\n      value: 37.22966666666666\n    - type: mrr_at_100\n      value: 38.066583333333334\n    - type: mrr_at_1000\n      value: 38.12616666666667\n    - type: mrr_at_3\n      value: 34.92275\n    - type: mrr_at_5\n      value: 36.23333333333334\n    - type: ndcg_at_1\n      value: 29.305666666666664\n    - type: ndcg_at_10\n      value: 38.25533333333333\n    - type: ndcg_at_100\n      value: 43.25266666666666\n    - type: ndcg_at_1000\n      value: 45.63583333333334\n    - type: ndcg_at_3\n      value: 33.777166666666666\n    - type: ndcg_at_5\n      value: 35.85\n    - type: precision_at_1\n      value: 29.305666666666664\n    - type: precision_at_10\n      value: 6.596416666666667\n    - type: precision_at_100\n      value: 1.0784166666666668\n    - type: precision_at_1000\n      value: 0.14666666666666664\n    - type: precision_at_3\n      value: 15.31075\n    - type: precision_at_5\n      value: 10.830916666666667\n    - type: recall_at_1\n      value: 25.00791666666667\n    - type: recall_at_10\n      value: 49.10933333333333\n    - type: recall_at_100\n      value: 71.09216666666667\n    - type: recall_at_1000\n      value: 87.77725000000001\n    - type: recall_at_3\n      value: 36.660916666666665\n    - type: recall_at_5\n      value: 41.94149999999999\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackStatsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 23.521\n    - type: map_at_10\n      value: 30.043\n    - type: map_at_100\n      value: 30.936000000000003\n    - type: map_at_1000\n      value: 31.022\n    - type: map_at_3\n      value: 27.926000000000002\n    - type: map_at_5\n      value: 29.076999999999998\n    - type: mrr_at_1\n      value: 26.227\n    - type: mrr_at_10\n      value: 32.822\n    - type: mrr_at_100\n      value: 33.61\n    - type: mrr_at_1000\n      value: 33.672000000000004\n    - type: mrr_at_3\n      value: 30.776999999999997\n    - type: mrr_at_5\n      value: 31.866\n    - type: ndcg_at_1\n      value: 26.227\n    - type: ndcg_at_10\n      value: 34.041\n    - type: ndcg_at_100\n      value: 38.394\n    - type: ndcg_at_1000\n      value: 40.732\n    - type: ndcg_at_3\n      value: 30.037999999999997\n    - type: ndcg_at_5\n      value: 31.845000000000002\n    - type: precision_at_1\n      value: 26.227\n    - type: precision_at_10\n      value: 5.244999999999999\n    - type: precision_at_100\n      value: 0.808\n    - type: precision_at_1000\n      value: 0.107\n    - type: precision_at_3\n      value: 12.679000000000002\n    - type: precision_at_5\n      value: 8.773\n    - type: recall_at_1\n      value: 23.521\n    - type: recall_at_10\n      value: 43.633\n    - type: recall_at_100\n      value: 63.126000000000005\n    - type: recall_at_1000\n      value: 80.765\n    - type: recall_at_3\n      value: 32.614\n    - type: recall_at_5\n      value: 37.15\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackTexRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 16.236\n    - type: map_at_10\n      value: 22.898\n    - type: map_at_100\n      value: 23.878\n    - type: map_at_1000\n      value: 24.009\n    - type: map_at_3\n      value: 20.87\n    - type: map_at_5\n      value: 22.025\n    - type: mrr_at_1\n      value: 19.339000000000002\n    - type: mrr_at_10\n      value: 26.382\n    - type: mrr_at_100\n      value: 27.245\n    - type: mrr_at_1000\n      value: 27.33\n    - type: mrr_at_3\n      value: 24.386\n    - type: mrr_at_5\n      value: 25.496000000000002\n    - type: ndcg_at_1\n      value: 19.339000000000002\n    - type: ndcg_at_10\n      value: 27.139999999999997\n    - type: ndcg_at_100\n      value: 31.944\n    - type: ndcg_at_1000\n      value: 35.077999999999996\n    - type: ndcg_at_3\n      value: 23.424\n    - type: ndcg_at_5\n      value: 25.188\n    - type: precision_at_1\n      value: 19.339000000000002\n    - type: precision_at_10\n      value: 4.8309999999999995\n    - type: precision_at_100\n      value: 0.845\n    - type: precision_at_1000\n      value: 0.128\n    - type: precision_at_3\n      value: 10.874\n    - type: precision_at_5\n      value: 7.825\n    - type: recall_at_1\n      value: 16.236\n    - type: recall_at_10\n      value: 36.513\n    - type: recall_at_100\n      value: 57.999\n    - type: recall_at_1000\n      value: 80.512\n    - type: recall_at_3\n      value: 26.179999999999996\n    - type: recall_at_5\n      value: 30.712\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackUnixRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 24.11\n    - type: map_at_10\n      value: 31.566\n    - type: map_at_100\n      value: 32.647\n    - type: map_at_1000\n      value: 32.753\n    - type: map_at_3\n      value: 29.24\n    - type: map_at_5\n      value: 30.564999999999998\n    - type: mrr_at_1\n      value: 28.265\n    - type: mrr_at_10\n      value: 35.504000000000005\n    - type: mrr_at_100\n      value: 36.436\n    - type: mrr_at_1000\n      value: 36.503\n    - type: mrr_at_3\n      value: 33.349000000000004\n    - type: mrr_at_5\n      value: 34.622\n    - type: ndcg_at_1\n      value: 28.265\n    - type: ndcg_at_10\n      value: 36.192\n    - type: ndcg_at_100\n      value: 41.388000000000005\n    - type: ndcg_at_1000\n      value: 43.948\n    - type: ndcg_at_3\n      value: 31.959\n    - type: ndcg_at_5\n      value: 33.998\n    - type: precision_at_1\n      value: 28.265\n    - type: precision_at_10\n      value: 5.989\n    - type: precision_at_100\n      value: 0.9650000000000001\n    - type: precision_at_1000\n      value: 0.13\n    - type: precision_at_3\n      value: 14.335\n    - type: precision_at_5\n      value: 10.112\n    - type: recall_at_1\n      value: 24.11\n    - type: recall_at_10\n      value: 46.418\n    - type: recall_at_100\n      value: 69.314\n    - type: recall_at_1000\n      value: 87.397\n    - type: recall_at_3\n      value: 34.724\n    - type: recall_at_5\n      value: 39.925\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWebmastersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 22.091\n    - type: map_at_10\n      value: 29.948999999999998\n    - type: map_at_100\n      value: 31.502000000000002\n    - type: map_at_1000\n      value: 31.713\n    - type: map_at_3\n      value: 27.464\n    - type: map_at_5\n      value: 28.968\n    - type: mrr_at_1\n      value: 26.482\n    - type: mrr_at_10\n      value: 34.009\n    - type: mrr_at_100\n      value: 35.081\n    - type: mrr_at_1000\n      value: 35.138000000000005\n    - type: mrr_at_3\n      value: 31.785000000000004\n    - type: mrr_at_5\n      value: 33.178999999999995\n    - type: ndcg_at_1\n      value: 26.482\n    - type: ndcg_at_10\n      value: 35.008\n    - type: ndcg_at_100\n      value: 41.272999999999996\n    - type: ndcg_at_1000\n      value: 43.972\n    - type: ndcg_at_3\n      value: 30.804\n    - type: ndcg_at_5\n      value: 33.046\n    - type: precision_at_1\n      value: 26.482\n    - type: precision_at_10\n      value: 6.462\n    - type: precision_at_100\n      value: 1.431\n    - type: precision_at_1000\n      value: 0.22899999999999998\n    - type: precision_at_3\n      value: 14.360999999999999\n    - type: precision_at_5\n      value: 10.474\n    - type: recall_at_1\n      value: 22.091\n    - type: recall_at_10\n      value: 45.125\n    - type: recall_at_100\n      value: 72.313\n    - type: recall_at_1000\n      value: 89.503\n    - type: recall_at_3\n      value: 33.158\n    - type: recall_at_5\n      value: 39.086999999999996\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWordpressRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 19.883\n    - type: map_at_10\n      value: 26.951000000000004\n    - type: map_at_100\n      value: 27.927999999999997\n    - type: map_at_1000\n      value: 28.022000000000002\n    - type: map_at_3\n      value: 24.616\n    - type: map_at_5\n      value: 25.917\n    - type: mrr_at_1\n      value: 21.996\n    - type: mrr_at_10\n      value: 29.221000000000004\n    - type: mrr_at_100\n      value: 30.024\n    - type: mrr_at_1000\n      value: 30.095\n    - type: mrr_at_3\n      value: 26.833000000000002\n    - type: mrr_at_5\n      value: 28.155\n    - type: ndcg_at_1\n      value: 21.996\n    - type: ndcg_at_10\n      value: 31.421\n    - type: ndcg_at_100\n      value: 36.237\n    - type: ndcg_at_1000\n      value: 38.744\n    - type: ndcg_at_3\n      value: 26.671\n    - type: ndcg_at_5\n      value: 28.907\n    - type: precision_at_1\n      value: 21.996\n    - type: precision_at_10\n      value: 5.009\n    - type: precision_at_100\n      value: 0.799\n    - type: precision_at_1000\n      value: 0.11199999999999999\n    - type: precision_at_3\n      value: 11.275\n    - type: precision_at_5\n      value: 8.059\n    - type: recall_at_1\n      value: 19.883\n    - type: recall_at_10\n      value: 43.132999999999996\n    - type: recall_at_100\n      value: 65.654\n    - type: recall_at_1000\n      value: 84.492\n    - type: recall_at_3\n      value: 30.209000000000003\n    - type: recall_at_5\n      value: 35.616\n  - task:\n      type: Retrieval\n    dataset:\n      type: climate-fever\n      name: MTEB ClimateFEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 17.756\n    - type: map_at_10\n      value: 30.378\n    - type: map_at_100\n      value: 32.537\n    - type: map_at_1000\n      value: 32.717\n    - type: map_at_3\n      value: 25.599\n    - type: map_at_5\n      value: 28.372999999999998\n    - type: mrr_at_1\n      value: 41.303\n    - type: mrr_at_10\n      value: 53.483999999999995\n    - type: mrr_at_100\n      value: 54.106\n    - type: mrr_at_1000\n      value: 54.127\n    - type: mrr_at_3\n      value: 50.315\n    - type: mrr_at_5\n      value: 52.396\n    - type: ndcg_at_1\n      value: 41.303\n    - type: ndcg_at_10\n      value: 40.503\n    - type: ndcg_at_100\n      value: 47.821000000000005\n    - type: ndcg_at_1000\n      value: 50.788\n    - type: ndcg_at_3\n      value: 34.364\n    - type: ndcg_at_5\n      value: 36.818\n    - type: precision_at_1\n      value: 41.303\n    - type: precision_at_10\n      value: 12.463000000000001\n    - type: precision_at_100\n      value: 2.037\n    - type: precision_at_1000\n      value: 0.26\n    - type: precision_at_3\n      value: 25.798\n    - type: precision_at_5\n      value: 19.896\n    - type: recall_at_1\n      value: 17.756\n    - type: recall_at_10\n      value: 46.102\n    - type: recall_at_100\n      value: 70.819\n    - type: recall_at_1000\n      value: 87.21799999999999\n    - type: recall_at_3\n      value: 30.646\n    - type: recall_at_5\n      value: 38.022\n  - task:\n      type: Retrieval\n    dataset:\n      type: dbpedia-entity\n      name: MTEB DBPedia\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 9.033\n    - type: map_at_10\n      value: 20.584\n    - type: map_at_100\n      value: 29.518\n    - type: map_at_1000\n      value: 31.186000000000003\n    - type: map_at_3\n      value: 14.468\n    - type: map_at_5\n      value: 17.177\n    - type: mrr_at_1\n      value: 69.75\n    - type: mrr_at_10\n      value: 77.025\n    - type: mrr_at_100\n      value: 77.36699999999999\n    - type: mrr_at_1000\n      value: 77.373\n    - type: mrr_at_3\n      value: 75.583\n    - type: mrr_at_5\n      value: 76.396\n    - type: ndcg_at_1\n      value: 58.5\n    - type: ndcg_at_10\n      value: 45.033\n    - type: ndcg_at_100\n      value: 49.071\n    - type: ndcg_at_1000\n      value: 56.056\n    - type: ndcg_at_3\n      value: 49.936\n    - type: ndcg_at_5\n      value: 47.471999999999994\n    - type: precision_at_1\n      value: 69.75\n    - type: precision_at_10\n      value: 35.775\n    - type: precision_at_100\n      value: 11.594999999999999\n    - type: precision_at_1000\n      value: 2.062\n    - type: precision_at_3\n      value: 52.5\n    - type: precision_at_5\n      value: 45.300000000000004\n    - type: recall_at_1\n      value: 9.033\n    - type: recall_at_10\n      value: 26.596999999999998\n    - type: recall_at_100\n      value: 54.607000000000006\n    - type: recall_at_1000\n      value: 76.961\n    - type: recall_at_3\n      value: 15.754999999999999\n    - type: recall_at_5\n      value: 20.033\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/emotion\n      name: MTEB EmotionClassification\n      config: default\n      split: test\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\n    metrics:\n    - type: accuracy\n      value: 48.345000000000006\n    - type: f1\n      value: 43.4514918068706\n  - task:\n      type: Retrieval\n    dataset:\n      type: fever\n      name: MTEB FEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 71.29100000000001\n    - type: map_at_10\n      value: 81.059\n    - type: map_at_100\n      value: 81.341\n    - type: map_at_1000\n      value: 81.355\n    - type: map_at_3\n      value: 79.74799999999999\n    - type: map_at_5\n      value: 80.612\n    - type: mrr_at_1\n      value: 76.40299999999999\n    - type: mrr_at_10\n      value: 84.615\n    - type: mrr_at_100\n      value: 84.745\n    - type: mrr_at_1000\n      value: 84.748\n    - type: mrr_at_3\n      value: 83.776\n    - type: mrr_at_5\n      value: 84.343\n    - type: ndcg_at_1\n      value: 76.40299999999999\n    - type: ndcg_at_10\n      value: 84.981\n    - type: ndcg_at_100\n      value: 86.00999999999999\n    - type: ndcg_at_1000\n      value: 86.252\n    - type: ndcg_at_3\n      value: 82.97\n    - type: ndcg_at_5\n      value: 84.152\n    - type: precision_at_1\n      value: 76.40299999999999\n    - type: precision_at_10\n      value: 10.446\n    - type: precision_at_100\n      value: 1.1199999999999999\n    - type: precision_at_1000\n      value: 0.116\n    - type: precision_at_3\n      value: 32.147999999999996\n    - type: precision_at_5\n      value: 20.135\n    - type: recall_at_1\n      value: 71.29100000000001\n    - type: recall_at_10\n      value: 93.232\n    - type: recall_at_100\n      value: 97.363\n    - type: recall_at_1000\n      value: 98.905\n    - type: recall_at_3\n      value: 87.893\n    - type: recall_at_5\n      value: 90.804\n  - task:\n      type: Retrieval\n    dataset:\n      type: fiqa\n      name: MTEB FiQA2018\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 18.667\n    - type: map_at_10\n      value: 30.853\n    - type: map_at_100\n      value: 32.494\n    - type: map_at_1000\n      value: 32.677\n    - type: map_at_3\n      value: 26.91\n    - type: map_at_5\n      value: 29.099000000000004\n    - type: mrr_at_1\n      value: 37.191\n    - type: mrr_at_10\n      value: 46.171\n    - type: mrr_at_100\n      value: 47.056\n    - type: mrr_at_1000\n      value: 47.099000000000004\n    - type: mrr_at_3\n      value: 44.059\n    - type: mrr_at_5\n      value: 45.147\n    - type: ndcg_at_1\n      value: 37.191\n    - type: ndcg_at_10\n      value: 38.437\n    - type: ndcg_at_100\n      value: 44.62\n    - type: ndcg_at_1000\n      value: 47.795\n    - type: ndcg_at_3\n      value: 35.003\n    - type: ndcg_at_5\n      value: 36.006\n    - type: precision_at_1\n      value: 37.191\n    - type: precision_at_10\n      value: 10.586\n    - type: precision_at_100\n      value: 1.688\n    - type: precision_at_1000\n      value: 0.22699999999999998\n    - type: precision_at_3\n      value: 23.302\n    - type: precision_at_5\n      value: 17.006\n    - type: recall_at_1\n      value: 18.667\n    - type: recall_at_10\n      value: 45.367000000000004\n    - type: recall_at_100\n      value: 68.207\n    - type: recall_at_1000\n      value: 87.072\n    - type: recall_at_3\n      value: 32.129000000000005\n    - type: recall_at_5\n      value: 37.719\n  - task:\n      type: Retrieval\n    dataset:\n      type: hotpotqa\n      name: MTEB HotpotQA\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 39.494\n    - type: map_at_10\n      value: 66.223\n    - type: map_at_100\n      value: 67.062\n    - type: map_at_1000\n      value: 67.11500000000001\n    - type: map_at_3\n      value: 62.867\n    - type: map_at_5\n      value: 64.994\n    - type: mrr_at_1\n      value: 78.987\n    - type: mrr_at_10\n      value: 84.585\n    - type: mrr_at_100\n      value: 84.773\n    - type: mrr_at_1000\n      value: 84.77900000000001\n    - type: mrr_at_3\n      value: 83.592\n    - type: mrr_at_5\n      value: 84.235\n    - type: ndcg_at_1\n      value: 78.987\n    - type: ndcg_at_10\n      value: 73.64\n    - type: ndcg_at_100\n      value: 76.519\n    - type: ndcg_at_1000\n      value: 77.51\n    - type: ndcg_at_3\n      value: 68.893\n    - type: ndcg_at_5\n      value: 71.585\n    - type: precision_at_1\n      value: 78.987\n    - type: precision_at_10\n      value: 15.529000000000002\n    - type: precision_at_100\n      value: 1.7770000000000001\n    - type: precision_at_1000\n      value: 0.191\n    - type: precision_at_3\n      value: 44.808\n    - type: precision_at_5\n      value: 29.006999999999998\n    - type: recall_at_1\n      value: 39.494\n    - type: recall_at_10\n      value: 77.643\n    - type: recall_at_100\n      value: 88.825\n    - type: recall_at_1000\n      value: 95.321\n    - type: recall_at_3\n      value: 67.211\n    - type: recall_at_5\n      value: 72.519\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/imdb\n      name: MTEB ImdbClassification\n      config: default\n      split: test\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\n    metrics:\n    - type: accuracy\n      value: 85.55959999999999\n    - type: ap\n      value: 80.7246500384617\n    - type: f1\n      value: 85.52336485065454\n  - task:\n      type: Retrieval\n    dataset:\n      type: msmarco\n      name: MTEB MSMARCO\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 23.631\n    - type: map_at_10\n      value: 36.264\n    - type: map_at_100\n      value: 37.428\n    - type: map_at_1000\n      value: 37.472\n    - type: map_at_3\n      value: 32.537\n    - type: map_at_5\n      value: 34.746\n    - type: mrr_at_1\n      value: 24.312\n    - type: mrr_at_10\n      value: 36.858000000000004\n    - type: mrr_at_100\n      value: 37.966\n    - type: mrr_at_1000\n      value: 38.004\n    - type: mrr_at_3\n      value: 33.188\n    - type: mrr_at_5\n      value: 35.367\n    - type: ndcg_at_1\n      value: 24.312\n    - type: ndcg_at_10\n      value: 43.126999999999995\n    - type: ndcg_at_100\n      value: 48.642\n    - type: ndcg_at_1000\n      value: 49.741\n    - type: ndcg_at_3\n      value: 35.589\n    - type: ndcg_at_5\n      value: 39.515\n    - type: precision_at_1\n      value: 24.312\n    - type: precision_at_10\n      value: 6.699\n    - type: precision_at_100\n      value: 0.9450000000000001\n    - type: precision_at_1000\n      value: 0.104\n    - type: precision_at_3\n      value: 15.153\n    - type: precision_at_5\n      value: 11.065999999999999\n    - type: recall_at_1\n      value: 23.631\n    - type: recall_at_10\n      value: 64.145\n    - type: recall_at_100\n      value: 89.41\n    - type: recall_at_1000\n      value: 97.83500000000001\n    - type: recall_at_3\n      value: 43.769000000000005\n    - type: recall_at_5\n      value: 53.169\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (en)\n      config: en\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 93.4108527131783\n    - type: f1\n      value: 93.1415880261038\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (en)\n      config: en\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 77.24806201550388\n    - type: f1\n      value: 60.531916308197175\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (en)\n      config: en\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.71553463349024\n    - type: f1\n      value: 71.70753174900791\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (en)\n      config: en\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.79757901815736\n    - type: f1\n      value: 77.83719850433258\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-p2p\n      name: MTEB MedrxivClusteringP2P\n      config: default\n      split: test\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\n    metrics:\n    - type: v_measure\n      value: 33.74193296622113\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-s2s\n      name: MTEB MedrxivClusteringS2S\n      config: default\n      split: test\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\n    metrics:\n    - type: v_measure\n      value: 30.64257594108566\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/mind_small\n      name: MTEB MindSmallReranking\n      config: default\n      split: test\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\n    metrics:\n    - type: map\n      value: 30.811018518883625\n    - type: mrr\n      value: 31.910376577445003\n  - task:\n      type: Retrieval\n    dataset:\n      type: nfcorpus\n      name: MTEB NFCorpus\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 5.409\n    - type: map_at_10\n      value: 13.093\n    - type: map_at_100\n      value: 16.256999999999998\n    - type: map_at_1000\n      value: 17.617\n    - type: map_at_3\n      value: 9.555\n    - type: map_at_5\n      value: 11.428\n    - type: mrr_at_1\n      value: 45.201\n    - type: mrr_at_10\n      value: 54.179\n    - type: mrr_at_100\n      value: 54.812000000000005\n    - type: mrr_at_1000\n      value: 54.840999999999994\n    - type: mrr_at_3\n      value: 51.909000000000006\n    - type: mrr_at_5\n      value: 53.519000000000005\n    - type: ndcg_at_1\n      value: 43.189\n    - type: ndcg_at_10\n      value: 35.028\n    - type: ndcg_at_100\n      value: 31.226\n    - type: ndcg_at_1000\n      value: 39.678000000000004\n    - type: ndcg_at_3\n      value: 40.596\n    - type: ndcg_at_5\n      value: 38.75\n    - type: precision_at_1\n      value: 44.582\n    - type: precision_at_10\n      value: 25.974999999999998\n    - type: precision_at_100\n      value: 7.793\n    - type: precision_at_1000\n      value: 2.036\n    - type: precision_at_3\n      value: 38.493\n    - type: precision_at_5\n      value: 33.994\n    - type: recall_at_1\n      value: 5.409\n    - type: recall_at_10\n      value: 16.875999999999998\n    - type: recall_at_100\n      value: 30.316\n    - type: recall_at_1000\n      value: 60.891\n    - type: recall_at_3\n      value: 10.688\n    - type: recall_at_5\n      value: 13.832\n  - task:\n      type: Retrieval\n    dataset:\n      type: nq\n      name: MTEB NQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 36.375\n    - type: map_at_10\n      value: 51.991\n    - type: map_at_100\n      value: 52.91400000000001\n    - type: map_at_1000\n      value: 52.93600000000001\n    - type: map_at_3\n      value: 48.014\n    - type: map_at_5\n      value: 50.381\n    - type: mrr_at_1\n      value: 40.759\n    - type: mrr_at_10\n      value: 54.617000000000004\n    - type: mrr_at_100\n      value: 55.301\n    - type: mrr_at_1000\n      value: 55.315000000000005\n    - type: mrr_at_3\n      value: 51.516\n    - type: mrr_at_5\n      value: 53.435\n    - type: ndcg_at_1\n      value: 40.759\n    - type: ndcg_at_10\n      value: 59.384\n    - type: ndcg_at_100\n      value: 63.157\n    - type: ndcg_at_1000\n      value: 63.654999999999994\n    - type: ndcg_at_3\n      value: 52.114000000000004\n    - type: ndcg_at_5\n      value: 55.986000000000004\n    - type: precision_at_1\n      value: 40.759\n    - type: precision_at_10\n      value: 9.411999999999999\n    - type: precision_at_100\n      value: 1.153\n    - type: precision_at_1000\n      value: 0.12\n    - type: precision_at_3\n      value: 23.329\n    - type: precision_at_5\n      value: 16.256999999999998\n    - type: recall_at_1\n      value: 36.375\n    - type: recall_at_10\n      value: 79.053\n    - type: recall_at_100\n      value: 95.167\n    - type: recall_at_1000\n      value: 98.82\n    - type: recall_at_3\n      value: 60.475\n    - type: recall_at_5\n      value: 69.327\n  - task:\n      type: Retrieval\n    dataset:\n      type: quora\n      name: MTEB QuoraRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 70.256\n    - type: map_at_10\n      value: 83.8\n    - type: map_at_100\n      value: 84.425\n    - type: map_at_1000\n      value: 84.444\n    - type: map_at_3\n      value: 80.906\n    - type: map_at_5\n      value: 82.717\n    - type: mrr_at_1\n      value: 80.97999999999999\n    - type: mrr_at_10\n      value: 87.161\n    - type: mrr_at_100\n      value: 87.262\n    - type: mrr_at_1000\n      value: 87.263\n    - type: mrr_at_3\n      value: 86.175\n    - type: mrr_at_5\n      value: 86.848\n    - type: ndcg_at_1\n      value: 80.97999999999999\n    - type: ndcg_at_10\n      value: 87.697\n    - type: ndcg_at_100\n      value: 88.959\n    - type: ndcg_at_1000\n      value: 89.09899999999999\n    - type: ndcg_at_3\n      value: 84.83800000000001\n    - type: ndcg_at_5\n      value: 86.401\n    - type: precision_at_1\n      value: 80.97999999999999\n    - type: precision_at_10\n      value: 13.261000000000001\n    - type: precision_at_100\n      value: 1.5150000000000001\n    - type: precision_at_1000\n      value: 0.156\n    - type: precision_at_3\n      value: 37.01\n    - type: precision_at_5\n      value: 24.298000000000002\n    - type: recall_at_1\n      value: 70.256\n    - type: recall_at_10\n      value: 94.935\n    - type: recall_at_100\n      value: 99.274\n    - type: recall_at_1000\n      value: 99.928\n    - type: recall_at_3\n      value: 86.602\n    - type: recall_at_5\n      value: 91.133\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering\n      name: MTEB RedditClustering\n      config: default\n      split: test\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\n    metrics:\n    - type: v_measure\n      value: 56.322692497613104\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering-p2p\n      name: MTEB RedditClusteringP2P\n      config: default\n      split: test\n      revision: 282350215ef01743dc01b456c7f5241fa8937f16\n    metrics:\n    - type: v_measure\n      value: 61.895813503775074\n  - task:\n      type: Retrieval\n    dataset:\n      type: scidocs\n      name: MTEB SCIDOCS\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 4.338\n    - type: map_at_10\n      value: 10.767\n    - type: map_at_100\n      value: 12.537999999999998\n    - type: map_at_1000\n      value: 12.803999999999998\n    - type: map_at_3\n      value: 7.788\n    - type: map_at_5\n      value: 9.302000000000001\n    - type: mrr_at_1\n      value: 21.4\n    - type: mrr_at_10\n      value: 31.637999999999998\n    - type: mrr_at_100\n      value: 32.688\n    - type: mrr_at_1000\n      value: 32.756\n    - type: mrr_at_3\n      value: 28.433000000000003\n    - type: mrr_at_5\n      value: 30.178\n    - type: ndcg_at_1\n      value: 21.4\n    - type: ndcg_at_10\n      value: 18.293\n    - type: ndcg_at_100\n      value: 25.274\n    - type: ndcg_at_1000\n      value: 30.284\n    - type: ndcg_at_3\n      value: 17.391000000000002\n    - type: ndcg_at_5\n      value: 15.146999999999998\n    - type: precision_at_1\n      value: 21.4\n    - type: precision_at_10\n      value: 9.48\n    - type: precision_at_100\n      value: 1.949\n    - type: precision_at_1000\n      value: 0.316\n    - type: precision_at_3\n      value: 16.167\n    - type: precision_at_5\n      value: 13.22\n    - type: recall_at_1\n      value: 4.338\n    - type: recall_at_10\n      value: 19.213\n    - type: recall_at_100\n      value: 39.562999999999995\n    - type: recall_at_1000\n      value: 64.08\n    - type: recall_at_3\n      value: 9.828000000000001\n    - type: recall_at_5\n      value: 13.383000000000001\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sickr-sts\n      name: MTEB SICK-R\n      config: default\n      split: test\n      revision: a6ea5a8cab320b040a23452cc28066d9beae2cee\n    metrics:\n    - type: cos_sim_pearson\n      value: 82.42568163642142\n    - type: cos_sim_spearman\n      value: 78.5797159641342\n    - type: euclidean_pearson\n      value: 80.22151260811604\n    - type: euclidean_spearman\n      value: 78.5797151953878\n    - type: manhattan_pearson\n      value: 80.21224215864788\n    - type: manhattan_spearman\n      value: 78.55641478381344\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts12-sts\n      name: MTEB STS12\n      config: default\n      split: test\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.44020710812569\n    - type: cos_sim_spearman\n      value: 78.91631735081286\n    - type: euclidean_pearson\n      value: 81.64188964182102\n    - type: euclidean_spearman\n      value: 78.91633286881678\n    - type: manhattan_pearson\n      value: 81.69294748512496\n    - type: manhattan_spearman\n      value: 78.93438558002656\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts13-sts\n      name: MTEB STS13\n      config: default\n      split: test\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.27165426412311\n    - type: cos_sim_spearman\n      value: 85.40429140249618\n    - type: euclidean_pearson\n      value: 84.7509580724893\n    - type: euclidean_spearman\n      value: 85.40429140249618\n    - type: manhattan_pearson\n      value: 84.76488289321308\n    - type: manhattan_spearman\n      value: 85.4256793698708\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts14-sts\n      name: MTEB STS14\n      config: default\n      split: test\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\n    metrics:\n    - type: cos_sim_pearson\n      value: 83.138851760732\n    - type: cos_sim_spearman\n      value: 81.64101363896586\n    - type: euclidean_pearson\n      value: 82.55165038934942\n    - type: euclidean_spearman\n      value: 81.64105257080502\n    - type: manhattan_pearson\n      value: 82.52802949883335\n    - type: manhattan_spearman\n      value: 81.61255430718158\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts15-sts\n      name: MTEB STS15\n      config: default\n      split: test\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.0654695484029\n    - type: cos_sim_spearman\n      value: 87.20408521902229\n    - type: euclidean_pearson\n      value: 86.8110651362115\n    - type: euclidean_spearman\n      value: 87.20408521902229\n    - type: manhattan_pearson\n      value: 86.77984656478691\n    - type: manhattan_spearman\n      value: 87.1719947099227\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts16-sts\n      name: MTEB STS16\n      config: default\n      split: test\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\n    metrics:\n    - type: cos_sim_pearson\n      value: 83.77823915496512\n    - type: cos_sim_spearman\n      value: 85.43566325729779\n    - type: euclidean_pearson\n      value: 84.5396956658821\n    - type: euclidean_spearman\n      value: 85.43566325729779\n    - type: manhattan_pearson\n      value: 84.5665398848169\n    - type: manhattan_spearman\n      value: 85.44375870303232\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-en)\n      config: en-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.20030208471798\n    - type: cos_sim_spearman\n      value: 87.20485505076539\n    - type: euclidean_pearson\n      value: 88.10588324368722\n    - type: euclidean_spearman\n      value: 87.20485505076539\n    - type: manhattan_pearson\n      value: 87.92324770415183\n    - type: manhattan_spearman\n      value: 87.0571314561877\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (en)\n      config: en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 63.06093161604453\n    - type: cos_sim_spearman\n      value: 64.2163140357722\n    - type: euclidean_pearson\n      value: 65.27589680994006\n    - type: euclidean_spearman\n      value: 64.2163140357722\n    - type: manhattan_pearson\n      value: 65.45904383711101\n    - type: manhattan_spearman\n      value: 64.55404716679305\n  - task:\n      type: STS\n    dataset:\n      type: mteb/stsbenchmark-sts\n      name: MTEB STSBenchmark\n      config: default\n      split: test\n      revision: b0fddb56ed78048fa8b90373c8a3cfc37b684831\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.32976164578706\n    - type: cos_sim_spearman\n      value: 85.54302197678368\n    - type: euclidean_pearson\n      value: 85.26307149193056\n    - type: euclidean_spearman\n      value: 85.54302197678368\n    - type: manhattan_pearson\n      value: 85.26647282029371\n    - type: manhattan_spearman\n      value: 85.5316135265568\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/scidocs-reranking\n      name: MTEB SciDocsRR\n      config: default\n      split: test\n      revision: d3c5e1fc0b855ab6097bf1cda04dd73947d7caab\n    metrics:\n    - type: map\n      value: 81.44675968318754\n    - type: mrr\n      value: 94.92741826075158\n  - task:\n      type: Retrieval\n    dataset:\n      type: scifact\n      name: MTEB SciFact\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 56.34400000000001\n    - type: map_at_10\n      value: 65.927\n    - type: map_at_100\n      value: 66.431\n    - type: map_at_1000\n      value: 66.461\n    - type: map_at_3\n      value: 63.529\n    - type: map_at_5\n      value: 64.818\n    - type: mrr_at_1\n      value: 59.333000000000006\n    - type: mrr_at_10\n      value: 67.54599999999999\n    - type: mrr_at_100\n      value: 67.892\n    - type: mrr_at_1000\n      value: 67.917\n    - type: mrr_at_3\n      value: 65.778\n    - type: mrr_at_5\n      value: 66.794\n    - type: ndcg_at_1\n      value: 59.333000000000006\n    - type: ndcg_at_10\n      value: 70.5\n    - type: ndcg_at_100\n      value: 72.688\n    - type: ndcg_at_1000\n      value: 73.483\n    - type: ndcg_at_3\n      value: 66.338\n    - type: ndcg_at_5\n      value: 68.265\n    - type: precision_at_1\n      value: 59.333000000000006\n    - type: precision_at_10\n      value: 9.3\n    - type: precision_at_100\n      value: 1.053\n    - type: precision_at_1000\n      value: 0.11199999999999999\n    - type: precision_at_3\n      value: 25.889\n    - type: precision_at_5\n      value: 16.866999999999997\n    - type: recall_at_1\n      value: 56.34400000000001\n    - type: recall_at_10\n      value: 82.789\n    - type: recall_at_100\n      value: 92.767\n    - type: recall_at_1000\n      value: 99\n    - type: recall_at_3\n      value: 71.64399999999999\n    - type: recall_at_5\n      value: 76.322\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/sprintduplicatequestions-pairclassification\n      name: MTEB SprintDuplicateQuestions\n      config: default\n      split: test\n      revision: d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46\n    metrics:\n    - type: cos_sim_accuracy\n      value: 99.75742574257426\n    - type: cos_sim_ap\n      value: 93.52081548447406\n    - type: cos_sim_f1\n      value: 87.33850129198966\n    - type: cos_sim_precision\n      value: 90.37433155080214\n    - type: cos_sim_recall\n      value: 84.5\n    - type: dot_accuracy\n      value: 99.75742574257426\n    - type: dot_ap\n      value: 93.52081548447406\n    - type: dot_f1\n      value: 87.33850129198966\n    - type: dot_precision\n      value: 90.37433155080214\n    - type: dot_recall\n      value: 84.5\n    - type: euclidean_accuracy\n      value: 99.75742574257426\n    - type: euclidean_ap\n      value: 93.52081548447406\n    - type: euclidean_f1\n      value: 87.33850129198966\n    - type: euclidean_precision\n      value: 90.37433155080214\n    - type: euclidean_recall\n      value: 84.5\n    - type: manhattan_accuracy\n      value: 99.75841584158415\n    - type: manhattan_ap\n      value: 93.4975678585854\n    - type: manhattan_f1\n      value: 87.26708074534162\n    - type: manhattan_precision\n      value: 90.45064377682404\n    - type: manhattan_recall\n      value: 84.3\n    - type: max_accuracy\n      value: 99.75841584158415\n    - type: max_ap\n      value: 93.52081548447406\n    - type: max_f1\n      value: 87.33850129198966\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering\n      name: MTEB StackExchangeClustering\n      config: default\n      split: test\n      revision: 6cbc1f7b2bc0622f2e39d2c77fa502909748c259\n    metrics:\n    - type: v_measure\n      value: 64.31437036686651\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering-p2p\n      name: MTEB StackExchangeClusteringP2P\n      config: default\n      split: test\n      revision: 815ca46b2622cec33ccafc3735d572c266efdb44\n    metrics:\n    - type: v_measure\n      value: 33.25569319007206\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/stackoverflowdupquestions-reranking\n      name: MTEB StackOverflowDupQuestions\n      config: default\n      split: test\n      revision: e185fbe320c72810689fc5848eb6114e1ef5ec69\n    metrics:\n    - type: map\n      value: 49.90474939720706\n    - type: mrr\n      value: 50.568115503777264\n  - task:\n      type: Summarization\n    dataset:\n      type: mteb/summeval\n      name: MTEB SummEval\n      config: default\n      split: test\n      revision: cda12ad7615edc362dbf25a00fdd61d3b1eaf93c\n    metrics:\n    - type: cos_sim_pearson\n      value: 29.866828641244712\n    - type: cos_sim_spearman\n      value: 30.077555055873866\n    - type: dot_pearson\n      value: 29.866832988572266\n    - type: dot_spearman\n      value: 30.077555055873866\n  - task:\n      type: Retrieval\n    dataset:\n      type: trec-covid\n      name: MTEB TRECCOVID\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 0.232\n    - type: map_at_10\n      value: 2.094\n    - type: map_at_100\n      value: 11.971\n    - type: map_at_1000\n      value: 28.158\n    - type: map_at_3\n      value: 0.688\n    - type: map_at_5\n      value: 1.114\n    - type: mrr_at_1\n      value: 88\n    - type: mrr_at_10\n      value: 93.4\n    - type: mrr_at_100\n      value: 93.4\n    - type: mrr_at_1000\n      value: 93.4\n    - type: mrr_at_3\n      value: 93\n    - type: mrr_at_5\n      value: 93.4\n    - type: ndcg_at_1\n      value: 84\n    - type: ndcg_at_10\n      value: 79.923\n    - type: ndcg_at_100\n      value: 61.17\n    - type: ndcg_at_1000\n      value: 53.03\n    - type: ndcg_at_3\n      value: 84.592\n    - type: ndcg_at_5\n      value: 82.821\n    - type: precision_at_1\n      value: 88\n    - type: precision_at_10\n      value: 85\n    - type: precision_at_100\n      value: 63.019999999999996\n    - type: precision_at_1000\n      value: 23.554\n    - type: precision_at_3\n      value: 89.333\n    - type: precision_at_5\n      value: 87.2\n    - type: recall_at_1\n      value: 0.232\n    - type: recall_at_10\n      value: 2.255\n    - type: recall_at_100\n      value: 14.823\n    - type: recall_at_1000\n      value: 49.456\n    - type: recall_at_3\n      value: 0.718\n    - type: recall_at_5\n      value: 1.175\n  - task:\n      type: Retrieval\n    dataset:\n      type: webis-touche2020\n      name: MTEB Touche2020\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 2.547\n    - type: map_at_10\n      value: 11.375\n    - type: map_at_100\n      value: 18.194\n    - type: map_at_1000\n      value: 19.749\n    - type: map_at_3\n      value: 5.825\n    - type: map_at_5\n      value: 8.581\n    - type: mrr_at_1\n      value: 32.653\n    - type: mrr_at_10\n      value: 51.32\n    - type: mrr_at_100\n      value: 51.747\n    - type: mrr_at_1000\n      value: 51.747\n    - type: mrr_at_3\n      value: 47.278999999999996\n    - type: mrr_at_5\n      value: 48.605\n    - type: ndcg_at_1\n      value: 29.592000000000002\n    - type: ndcg_at_10\n      value: 28.151\n    - type: ndcg_at_100\n      value: 39.438\n    - type: ndcg_at_1000\n      value: 50.769\n    - type: ndcg_at_3\n      value: 30.758999999999997\n    - type: ndcg_at_5\n      value: 30.366\n    - type: precision_at_1\n      value: 32.653\n    - type: precision_at_10\n      value: 25.714\n    - type: precision_at_100\n      value: 8.041\n    - type: precision_at_1000\n      value: 1.555\n    - type: precision_at_3\n      value: 33.333\n    - type: precision_at_5\n      value: 31.837\n    - type: recall_at_1\n      value: 2.547\n    - type: recall_at_10\n      value: 18.19\n    - type: recall_at_100\n      value: 49.538\n    - type: recall_at_1000\n      value: 83.86\n    - type: recall_at_3\n      value: 7.329\n    - type: recall_at_5\n      value: 11.532\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/toxic_conversations_50k\n      name: MTEB ToxicConversationsClassification\n      config: default\n      split: test\n      revision: d7c0de2777da35d6aae2200a62c6e0e5af397c4c\n    metrics:\n    - type: accuracy\n      value: 71.4952\n    - type: ap\n      value: 14.793362635531409\n    - type: f1\n      value: 55.204635551516915\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/tweet_sentiment_extraction\n      name: MTEB TweetSentimentExtractionClassification\n      config: default\n      split: test\n      revision: d604517c81ca91fe16a244d1248fc021f9ecee7a\n    metrics:\n    - type: accuracy\n      value: 61.5365025466893\n    - type: f1\n      value: 61.81742556334845\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/twentynewsgroups-clustering\n      name: MTEB TwentyNewsgroupsClustering\n      config: default\n      split: test\n      revision: 6125ec4e24fa026cec8a478383ee943acfbd5449\n    metrics:\n    - type: v_measure\n      value: 49.05531070301185\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twittersemeval2015-pairclassification\n      name: MTEB TwitterSemEval2015\n      config: default\n      split: test\n      revision: 70970daeab8776df92f5ea462b6173c0b46fd2d1\n    metrics:\n    - type: cos_sim_accuracy\n      value: 86.51725576682364\n    - type: cos_sim_ap\n      value: 75.2292304265163\n    - type: cos_sim_f1\n      value: 69.54022988505749\n    - type: cos_sim_precision\n      value: 63.65629110039457\n    - type: cos_sim_recall\n      value: 76.62269129287598\n    - type: dot_accuracy\n      value: 86.51725576682364\n    - type: dot_ap\n      value: 75.22922386081054\n    - type: dot_f1\n      value: 69.54022988505749\n    - type: dot_precision\n      value: 63.65629110039457\n    - type: dot_recall\n      value: 76.62269129287598\n    - type: euclidean_accuracy\n      value: 86.51725576682364\n    - type: euclidean_ap\n      value: 75.22925730473472\n    - type: euclidean_f1\n      value: 69.54022988505749\n    - type: euclidean_precision\n      value: 63.65629110039457\n    - type: euclidean_recall\n      value: 76.62269129287598\n    - type: manhattan_accuracy\n      value: 86.52321630804077\n    - type: manhattan_ap\n      value: 75.20608115037336\n    - type: manhattan_f1\n      value: 69.60000000000001\n    - type: manhattan_precision\n      value: 64.37219730941705\n    - type: manhattan_recall\n      value: 75.75197889182058\n    - type: max_accuracy\n      value: 86.52321630804077\n    - type: max_ap\n      value: 75.22925730473472\n    - type: max_f1\n      value: 69.60000000000001\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twitterurlcorpus-pairclassification\n      name: MTEB TwitterURLCorpus\n      config: default\n      split: test\n      revision: 8b6510b0b1fa4e4c4f879467980e9be563ec1cdf\n    metrics:\n    - type: cos_sim_accuracy\n      value: 89.34877944657896\n    - type: cos_sim_ap\n      value: 86.71257569277373\n    - type: cos_sim_f1\n      value: 79.10386355986088\n    - type: cos_sim_precision\n      value: 76.91468470434214\n    - type: cos_sim_recall\n      value: 81.4213119802895\n    - type: dot_accuracy\n      value: 89.34877944657896\n    - type: dot_ap\n      value: 86.71257133133368\n    - type: dot_f1\n      value: 79.10386355986088\n    - type: dot_precision\n      value: 76.91468470434214\n    - type: dot_recall\n      value: 81.4213119802895\n    - type: euclidean_accuracy\n      value: 89.34877944657896\n    - type: euclidean_ap\n      value: 86.71257651501476\n    - type: euclidean_f1\n      value: 79.10386355986088\n    - type: euclidean_precision\n      value: 76.91468470434214\n    - type: euclidean_recall\n      value: 81.4213119802895\n    - type: manhattan_accuracy\n      value: 89.35848177901967\n    - type: manhattan_ap\n      value: 86.69330615469126\n    - type: manhattan_f1\n      value: 79.13867741453949\n    - type: manhattan_precision\n      value: 76.78881807647741\n    - type: manhattan_recall\n      value: 81.63689559593472\n    - type: max_accuracy\n      value: 89.35848177901967\n    - type: max_ap\n      value: 86.71257651501476\n    - type: max_f1\n      value: 79.13867741453949\nlicense: apache-2.0\nlanguage:\n- en\nnew_version: nomic-ai/nomic-embed-text-v1.5\n---\n\n\n# nomic-embed-text-v1: A Reproducible Long Context (8192) Text Embedder \n\n[Blog](https://www.nomic.ai/blog/posts/nomic-embed-text-v1) | [Technical Report](https://arxiv.org/abs/2402.01613) | [AWS SageMaker](https://aws.amazon.com/marketplace/seller-profile?id=seller-tpqidcj54zawi) | [Atlas Embedding and Unstructured Data Analytics Platform](https://atlas.nomic.ai)\n\n`nomic-embed-text-v1` is 8192 context length text encoder that surpasses OpenAI text-embedding-ada-002 and text-embedding-3-small performance on short and long context tasks.\n\n# Performance Benchmarks\n\n| Name                             | SeqLen | MTEB      | LoCo     | Jina Long Context |  Open Weights | Open Training Code | Open Data   |\n| :-------------------------------:| :----- | :-------- | :------: | :---------------: | :-----------: | :----------------: | :---------- |\n| nomic-embed-text-v1              | 8192   | **62.39** |**85.53** | 54.16             | âœ…            | âœ…                  | âœ…          |\n| jina-embeddings-v2-base-en       | 8192   | 60.39     | 85.45    | 51.90             | âœ…            | âŒ                  | âŒ          |\n| text-embedding-3-small           | 8191   | 62.26     | 82.40    | **58.20**         | âŒ            | âŒ                  | âŒ          |\n| text-embedding-ada-002           | 8191   | 60.99     | 52.7     | 55.25             | âŒ            | âŒ                  | âŒ          |\n\n\n**Exciting Update!**: `nomic-embed-text-v1` is now multimodal! [nomic-embed-vision-v1](https://huggingface.co/nomic-ai/nomic-embed-vision-v1) is aligned to the embedding space of `nomic-embed-text-v1`, meaning any text embedding is multimodal!\n\n## Usage\n\n**Important**: the text prompt *must* include a *task instruction prefix*, instructing the model which task is being performed. \n\nFor example, if you are implementing a RAG application, you embed your documents as `search_document: <text here>` and embed your user queries as `search_query: <text here>`.\n\n## Task instruction prefixes\n\n### `search_document`\n\n#### Purpose: embed texts as documents from a dataset\n\nThis prefix is used for embedding texts as documents, for example as documents for a RAG index.\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer("nomic-ai/nomic-embed-text-v1", trust_remote_code=True)\nsentences = [''search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten'']\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n### `search_query`\n\n#### Purpose: embed texts as questions to answer\n\nThis prefix is used for embedding texts as questions that documents from a dataset could resolve, for example as queries to be answered by a RAG application.\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer("nomic-ai/nomic-embed-text-v1", trust_remote_code=True)\nsentences = [''search_query: Who is Laurens van Der Maaten?'']\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n### `clustering`\n\n#### Purpose: embed texts to group them into clusters\n\nThis prefix is used for embedding texts in order to group them into clusters, discover common topics, or remove semantic duplicates.\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer("nomic-ai/nomic-embed-text-v1", trust_remote_code=True)\nsentences = [''clustering: the quick brown fox'']\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n### `classification`\n\n#### Purpose: embed texts to classify them\n\nThis prefix is used for embedding texts into vectors that will be used as features for a classification model\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer("nomic-ai/nomic-embed-text-v1", trust_remote_code=True)\nsentences = [''classification: the quick brown fox'']\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n### Sentence Transformers\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer("nomic-ai/nomic-embed-text-v1", trust_remote_code=True)\nsentences = [''search_query: What is TSNE?'', ''search_query: Who is Laurens van der Maaten?'']\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n### Transformers\n\n```python\nimport torch\nimport torch.nn.functional as F\nfrom transformers import AutoTokenizer, AutoModel\n\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\nsentences = [''search_query: What is TSNE?'', ''search_query: Who is Laurens van der Maaten?'']\n\ntokenizer = AutoTokenizer.from_pretrained(''bert-base-uncased'')\nmodel = AutoModel.from_pretrained(''nomic-ai/nomic-embed-text-v1'', trust_remote_code=True)\nmodel.eval()\n\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=''pt'')\n\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\nembeddings = mean_pooling(model_output, encoded_input[''attention_mask''])\nembeddings = F.normalize(embeddings, p=2, dim=1)\nprint(embeddings)\n```\n\nThe model natively supports scaling of the sequence length past 2048 tokens. To do so, \n\n```diff\n- tokenizer = AutoTokenizer.from_pretrained(''bert-base-uncased'')\n+ tokenizer = AutoTokenizer.from_pretrained(''bert-base-uncased'', model_max_length=8192)\n\n\n- model = AutoModel.from_pretrained(''nomic-ai/nomic-embed-text-v1'', trust_remote_code=True)\n+ model = AutoModel.from_pretrained(''nomic-ai/nomic-embed-text-v1'', trust_remote_code=True, rotary_scaling_factor=2)\n```\n\n### Transformers.js\n\n```js\nimport { pipeline } from ''@xenova/transformers'';\n\n// Create a feature extraction pipeline\nconst extractor = await pipeline(''feature-extraction'', ''nomic-ai/nomic-embed-text-v1'', {\n    quantized: false, // Comment out this line to use the quantized version\n});\n\n// Compute sentence embeddings\nconst texts = [''search_query: What is TSNE?'', ''search_query: Who is Laurens van der Maaten?''];\nconst embeddings = await extractor(texts, { pooling: ''mean'', normalize: true });\nconsole.log(embeddings);\n```\n\n## Nomic API\n\nThe easiest way to get started with Nomic Embed is through the Nomic Embedding API.\n\nGenerating embeddings with the `nomic` Python client is as easy as \n\n```python\nfrom nomic import embed\n\noutput = embed.text(\n    texts=[''Nomic Embedding API'', ''#keepAIOpen''],\n    model=''nomic-embed-text-v1'',\n    task_type=''search_document''\n)\n\nprint(output)\n```\n\nFor more information, see the [API reference](https://docs.nomic.ai/reference/endpoints/nomic-embed-text)\n\n\n## Training\nClick the Nomic Atlas map below to visualize a 5M sample of our contrastive pretraining data!\n\n[![image/webp](https://cdn-uploads.huggingface.co/production/uploads/607997c83a565c15675055b3/pjhJhuNyRfPagRd_c_iUz.webp)](https://atlas.nomic.ai/map/nomic-text-embed-v1-5m-sample)\n\nWe train our embedder using a multi-stage training pipeline. Starting from a long-context [BERT model](https://huggingface.co/nomic-ai/nomic-bert-2048),\nthe first unsupervised contrastive stage trains on a dataset generated from weakly related text pairs, such as question-answer pairs from forums like StackExchange and Quora, title-body pairs from Amazon reviews, and summarizations from news articles.\n\nIn the second finetuning stage, higher quality labeled datasets such as search queries and answers from web searches are leveraged. Data curation and hard-example mining is crucial in this stage.\n\nFor more details, see the Nomic Embed [Technical Report](https://static.nomic.ai/reports/2024_Nomic_Embed_Text_Technical_Report.pdf) and corresponding [blog post](https://blog.nomic.ai/posts/nomic-embed-text-v1).\n\nTraining data to train the models is released in its entirety. For more details, see the `contrastors` [repository](https://github.com/nomic-ai/contrastors)\n\n\n# Join the Nomic Community\n\n- Nomic: [https://nomic.ai](https://nomic.ai)\n- Discord: [https://discord.gg/myY5YDR8z8](https://discord.gg/myY5YDR8z8)\n- Twitter: [https://twitter.com/nomic_ai](https://twitter.com/nomic_ai)\n\n\n# Citation\n\nIf you find the model, dataset, or training code useful, please cite our work\n\n```bibtex\n@misc{nussbaum2024nomic,\n      title={Nomic Embed: Training a Reproducible Long Context Text Embedder}, \n      author={Zach Nussbaum and John X. Morris and Brandon Duderstadt and Andriy Mulyar},\n      year={2024},\n      eprint={2402.01613},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```', '{"pipeline_tag":"sentence-similarity","library_name":"sentence-transformers","framework":"sentence-transformers","params":136731648,"storage_bytes":3202383417,"files_count":15,"spaces_count":94,"gated":false,"private":false,"config":{"architectures":["NomicBertModel"],"auto_map":{"AutoConfig":"nomic-ai/nomic-bert-2048--configuration_hf_nomic_bert.NomicBertConfig","AutoModel":"nomic-ai/nomic-bert-2048--modeling_hf_nomic_bert.NomicBertModel","AutoModelForMaskedLM":"nomic-ai/nomic-bert-2048--modeling_hf_nomic_bert.NomicBertForPreTraining"},"model_type":"nomic_bert","tokenizer_config":{"cls_token":"[CLS]","mask_token":"[MASK]","pad_token":"[PAD]","sep_token":"[SEP]","unk_token":"[UNK]"}}}', '[]', '[{"type":"has_code","target_id":"github:nomic-ai:contrastors","source_url":"https://github.com/nomic-ai/contrastors"},{"type":"based_on_paper","target_id":"arxiv:2402.01613","source_url":"https://arxiv.org/abs/2402.01613"}]', NULL, 'Apache-2.0', 'approved', 77.4, 'e5be98c272c8fe73dcd2bd8d9c206bef', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-V3.2-Speciale', 'huggingface--deepseek-ai--deepseek-v3.2-speciale', 'DeepSeek-V3.2-Speciale', 'deepseek-ai', '--- license: mit library_name: transformers base_model: - deepseek-ai/DeepSeek-V3.2-Exp-Base base_model_relation: finetune --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" targ...', '["transformers","safetensors","deepseek_v32","text-generation","base_model:deepseek-ai/deepseek-v3.2-exp-base","license:mit","endpoints_compatible","fp8","region:us"]', 'text-generation', 544, 8016, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-V3.2-Exp-Base\nbase_model_relation: finetune\n---\n# DeepSeek-V3.2: Efficient Reasoning & Agentic AI\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ðŸ¤–%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n<div align="center" style="line-height: 1;">\n  <a href="LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<p align="center">\n  <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf"><b>Technical Report</b>ðŸ‘ï¸</a>\n</p>\n\n## Introduction\n\nWe introduce **DeepSeek-V3.2**, a model that harmonizes high computational efficiency with superior reasoning and agent performance. Our approach is built upon three key technical breakthroughs:\n\n1. **DeepSeek Sparse Attention (DSA):** We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance, specifically optimized for long-context scenarios.\n2. **Scalable Reinforcement Learning Framework:** By implementing a robust RL protocol and scaling post-training compute, *DeepSeek-V3.2* performs comparably to GPT-5. Notably, our high-compute variant, **DeepSeek-V3.2-Speciale**, **surpasses GPT-5** and exhibits reasoning proficiency on par with Gemini-3.0-Pro.\n    - *Achievement:* ðŸ¥‡ **Gold-medal performance** in the 2025 International Mathematical Olympiad (IMO) and International Olympiad in Informatics (IOI).\n3. **Large-Scale Agentic Task Synthesis Pipeline:** To integrate **reasoning into tool-use** scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This facilitates scalable agentic post-training, improving compliance and generalization in complex interactive environments.\n\n<div align="center">\n <img src="assets/benchmark.png" >\n</div>\n\nWe have also released the final submissions for IOI 2025, ICPC World Finals, IMO 2025 and CMO 2025, which were selected based on our designed pipeline. These materials are provided for the community to conduct secondary verification. The files can be accessed at `assets/olympiad_cases`.\n\n## Chat Template\n\nDeepSeek-V3.2 introduces significant updates to its chat template compared to prior versions. The primary changes involve a revised format for tool calling and the introduction of a "thinking with tools" capability.\n\nTo assist the community in understanding and adapting to this new template, we have provided a dedicated `encoding` folder, which contains Python scripts and test cases demonstrating how to encode messages in OpenAI-compatible format into input strings for the model and how to parse the model''s text output.\n\nA brief example is illustrated below:\n\n```python\nimport transformers\n# encoding/encoding_dsv32.py\nfrom encoding_dsv32 import encode_messages, parse_message_from_completion_text\n\ntokenizer = transformers.AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-V3.2")\n\nmessages = [\n    {"role": "user", "content": "hello"},\n    {"role": "assistant", "content": "Hello! I am DeepSeek.", "reasoning_content": "thinking..."},\n    {"role": "user", "content": "1+1=?"}\n]\nencode_config = dict(thinking_mode="thinking", drop_thinking=True, add_default_bos_token=True)\n\n# messages -> string\nprompt = encode_messages(messages, **encode_config)\n# Output: "<ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>hello<ï½œAssistantï½œ></think>Hello! I am DeepSeek.<ï½œendâ–ofâ–sentenceï½œ><ï½œUserï½œ>1+1=?<ï½œAssistantï½œ><think>"\n\n# string -> tokens\ntokens = tokenizer.encode(prompt)\n# Output: [0, 128803, 33310, 128804, 128799, 19923, 3, 342, 1030, 22651, 4374, 1465, 16, 1, 128803, 19, 13, 19, 127252, 128804, 128798]\n```\n\nImportant Notes:\n\n1. This release does not include a Jinja-format chat template. Please refer to the Python code mentioned above.\n2. The output parsing function included in the code is designed to handle well-formatted strings only. It does not attempt to correct or recover from malformed output that the model might occasionally generate. It is not suitable for production use without robust error handling.\n3. A new role named `developer` has been introduced in the chat template. This role is dedicated exclusively to search agent scenarios and is designated for no other tasks. The official API does not accept messages assigned to `developer`.\n\n## How to Run Locally\n\nThe model structure of DeepSeek-V3.2 and DeepSeek-V3.2-Speciale are the same as DeepSeek-V3.2-Exp. Please visit [DeepSeek-V3.2-Exp](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp) repo for more information about running this model locally.\n\nUsage Recommendations:\n\n1. For local deployment, we recommend setting the sampling parameters to `temperature = 1.0, top_p = 0.95`.\n2. Please note that the DeepSeek-V3.2-Speciale variant is designed exclusively for deep reasoning tasks and does not support the tool-calling functionality.\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2025deepseekv32,\n      title={DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models}, \n      author={DeepSeek-AI},\n      year={2025},\n}\n```\n\n## Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":685396921376,"storage_bytes":689484423011,"files_count":192,"spaces_count":0,"gated":false,"private":false,"config":{"architectures":["DeepseekV32ForCausalLM"],"model_type":"deepseek_v32","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ï½œbeginâ–ofâ–sentenceï½œ>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ï½œendâ–ofâ–sentenceï½œ>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ï½œendâ–ofâ–sentenceï½œ>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3.2-Exp","source_url":"https://github.com/deepseek-ai/DeepSeek-V3.2-Exp"}]', NULL, 'MIT', 'approved', 82.4, '3bcfd457e471fb9bcbb42b9df7e79361', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale/resolve/main/assets/benchmark.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-V3.2-Speciale from https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale/resolve/main/assets/benchmark.png
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-V3.2-Speciale.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-DGSpitzer-Cyberpunk-Anime-Diffusion', 'huggingface--dgspitzer--cyberpunk-anime-diffusion', 'Cyberpunk-Anime-Diffusion', 'DGSpitzer', '--- language: - en thumbnail: "https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/thumbnail.png" tags: - cyberpunk - anime - waifu-diffusion - stable-diffusion - aiart - text-to-image license: creativeml-openrail-m --- <center><img src="https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/5.jpg" width="512" height="512"/></center> !visitors An AI model that generates cyberpunk anime characters!~ Based of a finetuned Waifu Diffusion V1.3 Model wi...', '["diffusers","safetensors","cyberpunk","anime","waifu-diffusion","stable-diffusion","aiart","text-to-image","en","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 543, 422, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage: \n  - en\nthumbnail: "https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/thumbnail.png"\ntags:\n- cyberpunk\n- anime\n- waifu-diffusion\n- stable-diffusion\n- aiart\n- text-to-image\nlicense: creativeml-openrail-m\n---\n<center><img src="https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/5.jpg" width="512" height="512"/></center>\n\n![visitors](https://visitor-badge.glitch.me/badge?page_id=Cyberpunk_Anime_Diffusion)\n\n# Cyberpunk Anime Diffusion\n\nAn AI model that generates cyberpunk anime characters!~\n\nBased of a finetuned Waifu Diffusion V1.3 Model with Stable Diffusion V1.5 New Vae, training in Dreambooth\n\nby [DGSpitzer](https://www.youtube.com/channel/UCzzsYBF4qwtMwJaPJZ5SuPg)\n\n### ðŸ§¨ Diffusers\n\nThis repo contains both .ckpt and Diffuser model files. It''s compatible to be used as any Stable Diffusion model, using standard [Stable Diffusion Pipelines](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\n\nYou can convert this model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX](https://huggingface.co/blog/stable_diffusion_jax).\n\n```python example for loading the Diffuser\n#!pip install diffusers transformers scipy torch\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = "DGSpitzer/Cyberpunk-Anime-Diffusion"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to("cuda")\n\nprompt = "a beautiful perfect face girl in dgs illustration style, Anime fine details portrait of school girl in front of modern tokyo city landscape on the background deep bokeh, anime masterpiece, 8k, sharp high quality anime"\nimage = pipe(prompt).images[0]\n\nimage.save("./cyberpunk_girl.png")\n```\n\n# Online Demo\n\nYou can try the Online Web UI demo build with [Gradio](https://github.com/gradio-app/gradio), or use Colab Notebook at here:\n\n*My Online Space Demo*\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/DGSpitzer/DGS-Diffusion-Space)\n\n*Finetuned Diffusion WebUI Demo by anzorq*\n[![Use Finetuned_Diffusion WebUI](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/anzorq/finetuned_diffusion)\n\n*Colab Notebook*\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HelixNGC7293/cyberpunk-anime-diffusion/blob/main/cyberpunk_anime_diffusion.ipynb)[![GitHub](https://badgen.net/badge/icon/Github?icon=github&label)](https://github.com/HelixNGC7293/cyberpunk-anime-diffusion)\n\n*Buy me a coffee if you like this project ;P â™¥*\n[![Buy me a coffee](https://badgen.net/badge/icon/Buy%20Me%20A%20Coffee?icon=buymeacoffee&label)](https://www.buymeacoffee.com/dgspitzer)\n\n<center><img src="https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/1.jpg" width="512" height="512"/></center>\n\n# **ðŸ‘‡ModelðŸ‘‡**\n\nAI Model Weights available at huggingface: https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion\n\n<center><img src="https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/2.jpg" width="512" height="512"/></center>\n\n# Usage\n\nAfter model loaded, use keyword **dgs** in your prompt, with **illustration style** to get even better results.\n\nFor sampler, use **Euler A** for the best result (**DDIM** kinda works too), CFG Scale 7, steps 20 should be fine\n\n**Example 1:**\n\n```\nportrait of a girl in dgs illustration style, Anime girl, female soldier working in a cyberpunk city, cleavage, ((perfect femine face)), intricate, 8k, highly detailed, shy, digital painting, intense, sharp focus\n```\n\nFor cyber robot male character, you can add **muscular male** to improve the output.\n\n**Example 2:**\n\n```\na photo of muscular beard soldier male in dgs illustration style, half-body, holding robot arms, strong chest\n```\n\n**Example 3 (with Stable Diffusion WebUI):**\n\nIf using [AUTOMATIC1111''s Stable Diffusion WebUI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)\n\nYou can simply use this as **prompt** with **Euler A** Sampler,  CFG Scale 7, steps 20, 704 x 704px output res:\n\n```\nan anime girl in dgs illustration style\n```\n\nAnd set the **negative prompt** as this to get cleaner face: \n\n```\nout of focus, scary, creepy, evil, disfigured, missing limbs, ugly, gross, missing fingers\n```\n\nThis will give you the exactly same style as the sample images above.\n\n<center><img src="https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/ReadmeAddon.jpg" width="256" height="353"/></center>\n\n---\n\n**NOTE: usage of this model implies accpetance of stable diffusion''s [CreativeML Open RAIL-M license](LICENSE)**\n\n---\n\n\n<center><img src="https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/4.jpg" width="700" height="700"/></center>\n\n<center><img src="https://huggingface.co/DGSpitzer/Cyberpunk-Anime-Diffusion/resolve/main/img/6.jpg" width="700" height="700"/></center>\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":24647194668,"files_count":40,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:HelixNGC7293:cyberpunk-anime-diffusion","source_url":"https://github.com/HelixNGC7293/cyberpunk-anime-diffusion"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"}]', NULL, 'creativeml-openrail-m', 'approved', 62.4, '434ec78193e489221016a2594ef58079', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tiiuae-falcon-180B-chat', 'huggingface--tiiuae--falcon-180b-chat', 'falcon-180B-chat', 'tiiuae', '', '["transformers","safetensors","falcon","text-generation","conversational","en","de","es","fr","dataset:tiiuae/falcon-refinedweb","arxiv:1911.02150","arxiv:2005.14165","arxiv:2104.09864","arxiv:2205.14135","arxiv:2306.01116","license:unknown","text-generation-inference","region:us"]', 'text-generation', 543, 78, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tiiuae/falcon-180B-chat","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":179522565120,"storage_bytes":359045204272,"files_count":91,"spaces_count":100,"gated":"auto","private":false,"config":{"architectures":["FalconForCausalLM"],"model_type":"falcon","tokenizer_config":{"chat_template":"{% for message in messages %}{% if not loop.first %}{{ ''\n'' }}{% endif %}{% if message[''role''] == ''system'' %}{{ ''System: '' }}{% elif message[''role''] == ''user'' %}{{ ''User: '' }}{% elif message[''role''] == ''assistant'' %}{{ ''Falcon: '' }}{% endif %}{{ message[''content''] }}{% endfor %}{% if add_generation_prompt %}{{ ''\n'' + ''Falcon:'' }}{% endif %}","eos_token":"<|endoftext|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1911.02150","source_url":"https://arxiv.org/abs/1911.02150"},{"type":"based_on_paper","target_id":"arxiv:2005.14165","source_url":"https://arxiv.org/abs/2005.14165"},{"type":"based_on_paper","target_id":"arxiv:2104.09864","source_url":"https://arxiv.org/abs/2104.09864"},{"type":"based_on_paper","target_id":"arxiv:2205.14135","source_url":"https://arxiv.org/abs/2205.14135"},{"type":"based_on_paper","target_id":"arxiv:2306.01116","source_url":"https://arxiv.org/abs/2306.01116"}]', NULL, 'unknown', 'approved', 37.4, '4a5ff4875a1fccd10a20e4edbd2034d8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Helsinki-NLP-opus-mt-zh-en', 'huggingface--helsinki-nlp--opus-mt-zh-en', 'opus-mt-zh-en', 'Helsinki-NLP', '--- language: - zh - en tags: - translation license: cc-by-4.0 --- - Model Details - Uses - Risks, Limitations and Biases - Training - Evaluation - Citation Information - How to Get Started With the Model - **Model Description:** - **Developed by:** Language Technology Research Group at the University of Helsinki - **Model Type:** Translation - **Language(s):** - Source Language: Chinese - Target Language: English - **License:** CC-BY-4.0 - **Resources for more information:** - GitHub Repo Th...', '["transformers","pytorch","tf","rust","marian","text2text-generation","translation","zh","en","license:cc-by-4.0","endpoints_compatible","deploy:azure","region:us"]', 'translation', 540, 389824, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Helsinki-NLP/opus-mt-zh-en","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage: \n- zh\n- en\n\ntags:\n- translation\n\nlicense: cc-by-4.0\n---\n\n### zho-eng\n\n## Table of Contents\n- [Model Details](#model-details)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n- [Evaluation](#evaluation)\n- [Citation Information](#citation-information)\n- [How to Get Started With the Model](#how-to-get-started-with-the-model)\n\n## Model Details\n- **Model Description:**\n- **Developed by:** Language Technology Research Group at the University of Helsinki\n- **Model Type:** Translation\n- **Language(s):**  \n  - Source Language:  Chinese\n  - Target Language: English\n- **License:** CC-BY-4.0\n- **Resources for more information:**\n  - [GitHub Repo](https://github.com/Helsinki-NLP/OPUS-MT-train)\n\n\n## Uses\n\n#### Direct Use\n\nThis model can be used for translation and text-to-text generation.\n\n\n## Risks, Limitations and Biases\n\n**CONTENT WARNING: Readers should be aware this section contains content that is disturbing, offensive, and can propagate historical and current stereotypes.**\n\nSignificant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)).\n\nFurther details about the dataset for this model can be found in the OPUS readme: [zho-eng](https://github.com/Helsinki-NLP/Tatoeba-Challenge/tree/master/models/zho-eng/README.md)\n\n## Training\n\n#### System Information \n* helsinki_git_sha: 480fcbe0ee1bf4774bcbe6226ad9f58e63f6c535\n* transformers_git_sha: 2207e5d8cb224e954a7cba69fa4ac2309e9ff30b\n* port_machine: brutasse\n* port_time: 2020-08-21-14:41\n* src_multilingual: False\n* tgt_multilingual: False\n\n#### Training Data\n##### Preprocessing\n* pre-processing: normalization + SentencePiece (spm32k,spm32k)\n* ref_len: 82826.0\n* dataset: [opus](https://github.com/Helsinki-NLP/Opus-MT)\n* download original weights: [opus-2020-07-17.zip](https://object.pouta.csc.fi/Tatoeba-MT-models/zho-eng/opus-2020-07-17.zip)\n\n* test set translations: [opus-2020-07-17.test.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/zho-eng/opus-2020-07-17.test.txt)\n\n\n## Evaluation\n\n#### Results\n\n* test set scores: [opus-2020-07-17.eval.txt](https://object.pouta.csc.fi/Tatoeba-MT-models/zho-eng/opus-2020-07-17.eval.txt)\n\n* brevity_penalty: 0.948\n\n\n## Benchmarks\n\n| testset               | BLEU  | chr-F |\n|-----------------------|-------|-------|\n| Tatoeba-test.zho.eng 	| 36.1 	| 0.548 |\n\n## Citation Information\n\n```bibtex\n@InProceedings{TiedemannThottingal:EAMT2020,\n  author = {J{\"o}rg Tiedemann and Santhosh Thottingal},\n  title = {{OPUS-MT} â€” {B}uilding open translation services for the {W}orld},\n  booktitle = {Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)},\n  year = {2020},\n  address = {Lisbon, Portugal}\n }\n```\n\n## How to Get Started With the Model\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-zh-en")\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-zh-en")\n```\n\n\n', '{"pipeline_tag":"translation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":3577746362,"files_count":12,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["MarianMTModel"],"model_type":"marian","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:Helsinki-NLP:OPUS-MT-train","source_url":"https://github.com/Helsinki-NLP/OPUS-MT-train"},{"type":"has_code","target_id":"github:Helsinki-NLP:Tatoeba-Challenge","source_url":"https://github.com/Helsinki-NLP/Tatoeba-Challenge"},{"type":"has_code","target_id":"github:Helsinki-NLP:Opus-MT","source_url":"https://github.com/Helsinki-NLP/Opus-MT"}]', NULL, 'CC-BY-4.0', 'approved', 62.3, '0e1b9cd6566525966d74a72874b8c384', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-2-70b', 'huggingface--meta-llama--llama-2-70b', 'Llama-2-70b', 'meta-llama', '', '["facebook","meta","pytorch","llama","llama-2","text-generation","en","arxiv:2307.09288","license:llama2","region:us"]', 'text-generation', 538, 48, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-2-70b","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":null,"framework":null,"params":null,"storage_bytes":137975402906,"files_count":17,"spaces_count":35,"gated":"manual","private":false,"config":null}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2307.09288","source_url":"https://arxiv.org/abs/2307.09288"}]', NULL, 'LLaMA-2', 'approved', 37.3, '59fdeeb3667827dc54651903f03b3000', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-togethercomputer-LLaMA-2-7B-32K', 'huggingface--togethercomputer--llama-2-7b-32k', 'LLaMA-2-7B-32K', 'togethercomputer', '--- license: llama2 datasets: - togethercomputer/RedPajama-Data-1T - togethercomputer/RedPajama-Data-Instruct - EleutherAI/pile - togethercomputer/Long-Data-Collections language: - en library_name: transformers --- LLaMA-2-7B-32K is an open-source, long context language model developed by Together, fine-tuned from Meta''s original Llama-2 7B model. This model represents our efforts to contribute to the rapid progress of the open-source ecosystem for large language models. The model has been ex...', '["transformers","pytorch","llama","text-generation","en","dataset:togethercomputer/redpajama-data-1t","dataset:togethercomputer/redpajama-data-instruct","dataset:eleutherai/pile","dataset:togethercomputer/long-data-collections","license:llama2","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 538, 1556, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/togethercomputer/LLaMA-2-7B-32K","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: llama2\ndatasets:\n- togethercomputer/RedPajama-Data-1T\n- togethercomputer/RedPajama-Data-Instruct\n- EleutherAI/pile\n- togethercomputer/Long-Data-Collections\nlanguage:\n- en\nlibrary_name: transformers\n---\n\n# LLaMA-2-7B-32K\n\n## Model Description\n\nLLaMA-2-7B-32K is an open-source, long context language model developed by Together, fine-tuned from Meta''s original Llama-2 7B model. \nThis model represents our efforts to contribute to the rapid progress of the open-source ecosystem for large language models. \nThe model has been extended to a context length of 32K with position interpolation, \nallowing applications on multi-document QA, long text summarization, etc.\n\n## What''s new?\n\nThis model introduces several improvements and new features:\n\n1. **Extended Context:** The model has been trained to handle context lengths up to 32K, which is a significant improvement over the previous versions.\n\n2. **Pre-training and Instruction Tuning:** We have shared our data recipe, which consists of a mixture of pre-training and instruction tuning data.\n\n3. **Fine-tuning Examples:** We provide examples of how to fine-tune the model for specific applications, including book summarization and long context question and answering.\n\n4. **Software Support:** We have updated both the inference and training stack to allow efficient inference and fine-tuning for 32K context.\n\n## Model Architecture\n\nThe model follows the architecture of Llama-2-7B and extends it to handle a longer context. It leverages the recently released FlashAttention-2 and a range of other optimizations to improve the speed and efficiency of inference and training.\n\n## Training and Fine-tuning\n\nThe model has been trained using a mixture of pre-training and instruction tuning data. \n- In the first training phase of continued pre-training, our data mixture contains 25% RedPajama Book, 25% RedPajama ArXiv (including abstracts), 25% other data from RedPajama, and 25% from the UL2 Oscar Data, which is a part of OIG (Open-Instruction-Generalist), asking the model to fill in missing chunks, or complete the text. \nTo enhance the long-context ability, we exclude data shorter than 2K word. The inclusion of UL2 Oscar Data is effective in compelling the model to read and utilize long-range context.\n- We then fine-tune the model to focus on its few shot capacity under long context, including 20% Natural Instructions (NI), 20% Public Pool of Prompts (P3), 20% the Pile. We decontaminated all data against HELM core scenarios . We teach the model to leverage the in-context examples by packing examples into one 32K-token sequence. To maintain the knowledge learned from the first piece of data, we incorporate 20% RedPajama-Data Book and 20% RedPajama-Data ArXiv.\n\nNext, we provide examples of how to fine-tune the model for specific applications. \nThe example datasets are placed in [togethercomputer/Long-Data-Collections](https://huggingface.co/datasets/togethercomputer/Long-Data-Collections)\nYou can use the [OpenChatKit](https://github.com/togethercomputer/OpenChatKit) to fine-tune your own 32K model over LLaMA-2-7B-32K.\nPlease refer to [OpenChatKit](https://github.com/togethercomputer/OpenChatKit) for step-by-step illustrations.\n\n1. Long Context QA.\n\n   We take as an example the multi-document question answering task from the paper â€œLost in the Middle: How Language Models Use Long Contextsâ€. The input for the model consists of (i) a question that requires an answer and (ii) k documents, which are passages extracted from Wikipedia. Notably, only one of these documents contains the answer to the question, while the remaining k âˆ’ 1 documents, termed as "distractor" documents, do not. To successfully perform this task, the model must identify and utilize the document containing the answer from its input context. \n\n   With OCK, simply run the following command to fine-tune:\n   ```\n   bash training/finetune_llama-2-7b-32k-mqa.sh\n   ```\n\n2. Summarization.\n\n   Another example is BookSum, a unique dataset designed to address the challenges of long-form narrative summarization. This dataset features source documents from the literature domain, including novels, plays, and stories, and offers human-written, highly abstractive summaries. We here focus on chapter-level data.  BookSum poses a unique set of challenges, necessitating that the model comprehensively read through each chapter.\n\n   With OCK, simply run the following command to fine-tune:\n   ```\n   bash training/finetune_llama-2-7b-32k-booksum.sh\n   ```\n\n\n## Inference\n\nYou can use the [Together API](https://together.ai/blog/api-announcement) to try out LLaMA-2-7B-32K for inference. \nThe updated inference stack allows for efficient inference.\n\nTo run the model locally, we strongly recommend to install Flash Attention V2, which is necessary to obtain the best performance:\n```\n# Please update the path of `CUDA_HOME`\nexport CUDA_HOME=/usr/local/cuda-11.8\npip install transformers==4.31.0\npip install sentencepiece\npip install ninja\npip install flash-attn --no-build-isolation\npip install git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary\n```\n\nYou can use this model directly from the Hugging Face Model Hub or fine-tune it on your own data using the OpenChatKit.\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained("togethercomputer/LLaMA-2-7B-32K")\nmodel = AutoModelForCausalLM.from_pretrained("togethercomputer/LLaMA-2-7B-32K", trust_remote_code=True, torch_dtype=torch.float16)\n\ninput_context = "Your text here"\ninput_ids = tokenizer.encode(input_context, return_tensors="pt")\noutput = model.generate(input_ids, max_length=128, temperature=0.7)\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(output_text)\n```\n\nAlternatively, you can set `trust_remote_code=False` if you prefer not to use flash attention.\n\n\n## Limitations and Bias\n\nAs with all language models, LLaMA-2-7B-32K may generate incorrect or biased content. It''s important to keep this in mind when using the model.\n\n## Community\n\nJoin us on [Together Discord](https://discord.gg/6ZVDU8tTD4)', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":53908211130,"files_count":13,"spaces_count":82,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","pad_token":"<unk>","unk_token":"<unk>","use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:togethercomputer:OpenChatKit","source_url":"https://github.com/togethercomputer/OpenChatKit"},{"type":"has_code","target_id":"github:togethercomputer:OpenChatKit","source_url":"https://github.com/togethercomputer/OpenChatKit"},{"type":"has_code","target_id":"github:HazyResearch:flash-attention.git","source_url":"https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc"}]', NULL, 'LLaMA-2', 'approved', 62.3, 'a2c3da9226f9e227fb150c7a45a9ce80', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-strangerzonehf-Flux-Midjourney-Mix2-LoRA', 'huggingface--strangerzonehf--flux-midjourney-mix2-lora', 'Flux-Midjourney-Mix2-LoRA', 'strangerzonehf', '--- tags: - text-to-image - lora - diffusers - template:diffusion-lora widget: - text: ''MJ v6, Portrait photography of a woman in a red dress, in the style of unsplash photography, street photography, dark green background --ar 47:64 --v 6.0 --style raw'' output: url: images/1.png - text: ''MJ v6, A portrait of a Bird in the dark, illuminated by an intense yellow light from above, with a soft blue gradient background. This scene evokes a sense of mystery or contemplation, highlighting the beaut...', '["diffusers","text-to-image","lora","template:diffusion-lora","base_model:black-forest-labs/flux.1-dev","base_model:adapter:black-forest-labs/flux.1-dev","license:other","region:us"]', 'text-to-image', 538, 4788, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/strangerzonehf/Flux-Midjourney-Mix2-LoRA","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- text-to-image\n- lora\n- diffusers\n- template:diffusion-lora\nwidget:\n- text: ''MJ v6, Portrait photography of a woman in a red dress, in the style of unsplash photography, street photography, dark green background --ar 47:64 --v 6.0 --style raw''\n  output:\n    url: images/1.png\n- text: ''MJ v6, A portrait of a Bird in the dark, illuminated by an intense yellow light from above, with a soft blue gradient background. This scene evokes a sense of mystery or contemplation, highlighting the beauty of the subjects features against the contrasting backdrop, lens glossy effect, high contrast, star bokeh ''\n  output:\n    url: images/2.png\n- text: ''MJ v6, A photo of an attractive man in his thirties, wearing a black coat and yellow scarf with a brown pattern inside a building talking on a phone standing near a modern glass skyscraper in London, shot from below looking up at him in the style of street photography, cinematic.  --ar 85:128 --v 6.0 --style raw''\n  output:\n    url: images/3.png\n- text: ''MJ v6, banana bread with chocolate chips and pecans, in the style of tabletop photography, y2k aesthetic, spiky mounds, flawless line work, schlieren photography, 8k, natural fibers, minimal  --ar 123:185 --v 5 ''\n  output:\n    url: images/4.png\n- text: ''MJ v6, A portrait of Woman, fashion photography, big shapes in the background, on top of colorful squares with stars, in the style of retro vintage photography, pastel colors, soft purple and yellow ''\n  output:\n    url: images/6.png\n- text: ''MJ v6, delicious dipped chocolate pastry japo gallery, white background, in the style of dark brown, close-up intensity, duckcore, rounded, high resolution --ar 2:3 --v 5''\n  output:\n    url: images/5.png\nbase_model: black-forest-labs/FLUX.1-dev\ninstance_prompt: MJ v6\nlicense: other\nlicense_name: flux-1-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md\n---\n![MJv6](images/mjv6.png)\n\n<Gallery />\n\n## Model description for MJv6 Mix2 LoRA\n\n[ Best for Realism, Modeling, Demonstration, Close-Up Shots ]\n\nImage Processing Parameters \n\n| Parameter                 | Value  | Parameter                 | Value  |\n|---------------------------|--------|---------------------------|--------|\n| LR Scheduler              | constant | Noise Offset              | 0.03   |\n| Optimizer                 | AdamW8 | Multires Noise Discount   | 0.1    |\n| Network Dim               | 64     | Multires Noise Iterations | 10     |\n| Network Alpha             | 32     | Repeat & Steps           | 25 & 3660 |\n| Epoch                     | 28   | Save Every N Epochs       | 1     |\n\n    Labeling: florence2-en(natural language & English)\n    \n    Total Images Used for Training : 36\n\n## Best Dimensions\n\n- 768 x 1024 (Best)\n- 1024 x 1024 (Default)\n    \n## Setting Up\n```python\nimport torch\nfrom pipelines import DiffusionPipeline\n\nbase_model = "black-forest-labs/FLUX.1-dev"\npipe = DiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.bfloat16)\n\nlora_repo = "strangerzonehf/Flux-Midjourney-Mix2-LoRA"\ntrigger_word = "MJ v6"  \npipe.load_lora_weights(lora_repo)\n\ndevice = torch.device("cuda")\npipe.to(device)\n```\n## Trigger words\n\nYou should use `MJ v6` to trigger the image generation.\n\n## Sample Prompts for MJv6 Mix2 LoRA\n\n| **Prompt**                                                                                                                                                               | **Description**                                                                                                                                                                                                                  |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| MJ v6, Portrait photography of a woman in a red dress, in the style of unsplash photography, street photography, dark green background --ar 47:64 --v 6.0 --style raw | A portrait of a woman in a red dress, photographed in the street photography style with a dark green background, capturing the raw and natural aesthetics of Unsplash-style imagery.                                             |\n| MJ v6, A portrait of a Bird in the dark, illuminated by an intense yellow light from above, with a soft blue gradient background. Lens glossy effect, high contrast, star bokeh | A mysterious and contemplative bird portrait illuminated by yellow light with a blue gradient background. Features include high contrast and a star bokeh effect to enhance the atmosphere.                                      |\n| MJ v6, banana bread with chocolate chips and pecans, in the style of tabletop photography, y2k aesthetic, spiky mounds, flawless line work, schlieren photography, 8k, natural fibers, minimal  --ar 123:185 --v 5 | A close-up image of banana bread with chocolate chips and pecans, styled with a Y2K aesthetic. The photography emphasizes texture, line work, and high resolution, with natural materials enhancing the minimalistic approach.   |\n| MJ v6, delicious dipped chocolate pastry japo gallery, white background, in the style of dark brown, close-up intensity, duckcore, rounded, high resolution --ar 2:3 --v 5 | A close-up of a chocolate-dipped pastry on a white background, featuring a rich brown color palette and soft, rounded forms. High-resolution imagery enhances the details and texture of the subject.                             |\n| MJ v6, A portrait of Woman, fashion photography, big shapes in the background, on top of colorful squares with stars, in the style of retro vintage photography, pastel colors, soft purple and yellow | A retro-vintage style portrait of a woman with a whimsical background of large shapes and colorful squares with stars. The pastel tones of purple and yellow create a soft and nostalgic mood.                                    |\n| MJ v6, Captured at eye-level, a close-up shot of a young woman with long dark brown hair, wearing a green bikini top adorned with yellow and orange flowers. The woman''s body is partially submerged in a body of water, her eyes are slightly open. The background is blurred, with a stone wall visible behind her. The sun is shining on the right side of the image, casting a shadow on the wall. | A vibrant and summery close-up of a young woman partially submerged in water, wearing a floral green bikini top. The image captures natural lighting, with the background blurred to enhance the subject''s focus.                  |\n| MJ v6, a woman with long dark brown hair stands in front of a stark white wall. She is dressed in a sleeveless black and white dress, adorned with a checkered pattern. Her eyes are a deep blue, and her lips are pursed. Her hair cascades over her shoulders, adding a touch of warmth to her face. The lighting is subdued, creating a stark contrast to the woman''s outfit. | A stark, minimalist portrait of a woman in a checkered dress. The subdued lighting and simple white background emphasize her expressive features and contrast with her bold outfit.                                                  |\n| MJ v6, a beautiful young woman with long brown hair is seated in a field of lavender flowers. She is dressed in a cream-colored bra with a red belt tied around her waist. Her bra is tied in a knot at the center of her chest. Her eyes are closed and her lips are pursed. Her hair is pulled back in a ponytail, adding a pop of color to her face. The backdrop is a lush green hillside. | A serene and dreamy image of a woman in a lavender field. The cream-colored attire and red accents create a harmonious blend with the lush green and vibrant purple surroundings.                                                    |\n\n## Download model\n\nWeights for this model are available in Safetensors format.\n\n[Download](/strangerzonehf/Flux-Midjourney-Mix2-LoRA/tree/main) them in the Files & versions tab.\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":867491996,"files_count":10,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'Other', 'approved', 62.3, '02a0271206fa5ad1af3ec8ebaa2ad9c8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-FacebookAI-roberta-base', 'huggingface--facebookai--roberta-base', 'roberta-base', 'FacebookAI', '--- language: en tags: - exbert license: mit datasets: - bookcorpus - wikipedia --- Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model is case-sensitive: it makes a difference between english and English. Disclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team. RoBERTa is a transformers model...', '["transformers","pytorch","tf","jax","rust","safetensors","roberta","fill-mask","exbert","en","dataset:bookcorpus","dataset:wikipedia","arxiv:1907.11692","arxiv:1806.02847","license:mit","endpoints_compatible","deploy:azure","region:us"]', 'fill-mask', 534, 9644639, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/FacebookAI/roberta-base","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: en\ntags:\n- exbert\nlicense: mit\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# RoBERTa base model\n\nPretrained model on English language using a masked language modeling (MLM) objective. It was introduced in\n[this paper](https://arxiv.org/abs/1907.11692) and first released in\n[this repository](https://github.com/pytorch/fairseq/tree/master/examples/roberta). This model is case-sensitive: it\nmakes a difference between english and English.\n\nDisclaimer: The team releasing RoBERTa did not write a model card for this model so this model card has been written by\nthe Hugging Face team.\n\n## Model description\n\nRoBERTa is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. This means\nit was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of\npublicly available data) with an automatic process to generate inputs and labels from those texts. \n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model\nrandomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict\nthe masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one\nafter the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to\nlearn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of the English language that can then be used to extract features\nuseful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard\nclassifier using the features produced by the BERT model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for masked language modeling, but it''s mostly intended to be fine-tuned on a downstream task.\nSee the [model hub](https://huggingface.co/models?filter=roberta) to look for fine-tuned versions on a task that\ninterests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at a model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline(''fill-mask'', model=''roberta-base'')\n>>> unmasker("Hello I''m a <mask> model.")\n\n[{''sequence'': "<s>Hello I''m a male model.</s>",\n  ''score'': 0.3306540250778198,\n  ''token'': 2943,\n  ''token_str'': ''Ä male''},\n {''sequence'': "<s>Hello I''m a female model.</s>",\n  ''score'': 0.04655390977859497,\n  ''token'': 2182,\n  ''token_str'': ''Ä female''},\n {''sequence'': "<s>Hello I''m a professional model.</s>",\n  ''score'': 0.04232972860336304,\n  ''token'': 2038,\n  ''token_str'': ''Ä professional''},\n {''sequence'': "<s>Hello I''m a fashion model.</s>",\n  ''score'': 0.037216778844594955,\n  ''token'': 2734,\n  ''token_str'': ''Ä fashion''},\n {''sequence'': "<s>Hello I''m a Russian model.</s>",\n  ''score'': 0.03253649175167084,\n  ''token'': 1083,\n  ''token_str'': ''Ä Russian''}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import RobertaTokenizer, RobertaModel\ntokenizer = RobertaTokenizer.from_pretrained(''roberta-base'')\nmodel = RobertaModel.from_pretrained(''roberta-base'')\ntext = "Replace me by any text you''d like."\nencoded_input = tokenizer(text, return_tensors=''pt'')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import RobertaTokenizer, TFRobertaModel\ntokenizer = RobertaTokenizer.from_pretrained(''roberta-base'')\nmodel = TFRobertaModel.from_pretrained(''roberta-base'')\ntext = "Replace me by any text you''d like."\nencoded_input = tokenizer(text, return_tensors=''tf'')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nThe training data used for this model contains a lot of unfiltered content from the internet, which is far from\nneutral. Therefore, the model can have biased predictions:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline(''fill-mask'', model=''roberta-base'')\n>>> unmasker("The man worked as a <mask>.")\n\n[{''sequence'': ''<s>The man worked as a mechanic.</s>'',\n  ''score'': 0.08702439814805984,\n  ''token'': 25682,\n  ''token_str'': ''Ä mechanic''},\n {''sequence'': ''<s>The man worked as a waiter.</s>'',\n  ''score'': 0.0819653645157814,\n  ''token'': 38233,\n  ''token_str'': ''Ä waiter''},\n {''sequence'': ''<s>The man worked as a butcher.</s>'',\n  ''score'': 0.073323555290699,\n  ''token'': 32364,\n  ''token_str'': ''Ä butcher''},\n {''sequence'': ''<s>The man worked as a miner.</s>'',\n  ''score'': 0.046322137117385864,\n  ''token'': 18678,\n  ''token_str'': ''Ä miner''},\n {''sequence'': ''<s>The man worked as a guard.</s>'',\n  ''score'': 0.040150221437215805,\n  ''token'': 2510,\n  ''token_str'': ''Ä guard''}]\n\n>>> unmasker("The Black woman worked as a <mask>.")\n\n[{''sequence'': ''<s>The Black woman worked as a waitress.</s>'',\n  ''score'': 0.22177888453006744,\n  ''token'': 35698,\n  ''token_str'': ''Ä waitress''},\n {''sequence'': ''<s>The Black woman worked as a prostitute.</s>'',\n  ''score'': 0.19288744032382965,\n  ''token'': 36289,\n  ''token_str'': ''Ä prostitute''},\n {''sequence'': ''<s>The Black woman worked as a maid.</s>'',\n  ''score'': 0.06498628109693527,\n  ''token'': 29754,\n  ''token_str'': ''Ä maid''},\n {''sequence'': ''<s>The Black woman worked as a secretary.</s>'',\n  ''score'': 0.05375480651855469,\n  ''token'': 2971,\n  ''token_str'': ''Ä secretary''},\n {''sequence'': ''<s>The Black woman worked as a nurse.</s>'',\n  ''score'': 0.05245552211999893,\n  ''token'': 9008,\n  ''token_str'': ''Ä nurse''}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nThe RoBERTa model was pretrained on the reunion of five datasets:\n- [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset consisting of 11,038 unpublished books;\n- [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia) (excluding lists, tables and headers) ;\n- [CC-News](https://commoncrawl.org/2016/10/news-dataset-available/), a dataset containing 63 millions English news\n  articles crawled between September 2016 and February 2019.\n- [OpenWebText](https://github.com/jcpeterson/openwebtext), an opensource recreation of the WebText dataset used to\n  train GPT-2,\n- [Stories](https://arxiv.org/abs/1806.02847) a dataset containing a subset of CommonCrawl data filtered to match the\n  story-like style of Winograd schemas.\n\nTogether these datasets weigh 160GB of text.\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of\nthe model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked\nwith `<s>` and the end of one by `</s>`\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `<mask>`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\nContrary to BERT, the masking is done dynamically during pretraining (e.g., it changes at each epoch and is not fixed).\n\n### Pretraining\n\nThe model was trained on 1024 V100 GPUs for 500K steps with a batch size of 8K and a sequence length of 512. The\noptimizer used is Adam with a learning rate of 6e-4, \\(\beta_{1} = 0.9\\), \\(\beta_{2} = 0.98\\) and\n\\(\epsilon = 1e-6\\), a weight decay of 0.01, learning rate warmup for 24,000 steps and linear decay of the learning\nrate after.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 87.6 | 91.9 | 92.8 | 94.8  | 63.6 | 91.2  | 90.2 | 78.7 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1907-11692,\n  author    = {Yinhan Liu and\n               Myle Ott and\n               Naman Goyal and\n               Jingfei Du and\n               Mandar Joshi and\n               Danqi Chen and\n               Omer Levy and\n               Mike Lewis and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {RoBERTa: {A} Robustly Optimized {BERT} Pretraining Approach},\n  journal   = {CoRR},\n  volume    = {abs/1907.11692},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1907.11692},\n  archivePrefix = {arXiv},\n  eprint    = {1907.11692},\n  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-11692.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href="https://huggingface.co/exbert/?model=roberta-base">\n	<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">\n</a>\n', '{"pipeline_tag":"fill-mask","library_name":"transformers","framework":"transformers","params":124697947,"storage_bytes":3467440206,"files_count":13,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["RobertaForMaskedLM"],"model_type":"roberta","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:pytorch:fairseq","source_url":"https://github.com/pytorch/fairseq"},{"type":"has_code","target_id":"github:jcpeterson:openwebtext","source_url":"https://github.com/jcpeterson/openwebtext"},{"type":"based_on_paper","target_id":"arxiv:1907.11692","source_url":"https://arxiv.org/abs/1907.11692"},{"type":"based_on_paper","target_id":"arxiv:1806.02847","source_url":"https://arxiv.org/abs/1806.02847"}]', NULL, 'MIT', 'approved', 62.3, '2fe9dcbcc772d134bdfff0719e32da3a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-XLabs-AI-flux-controlnet-collections', 'huggingface--xlabs-ai--flux-controlnet-collections', 'flux-controlnet-collections', 'XLabs-AI', '--- license: other license_name: flux-1-dev-non-commercial-license license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE. language: - en pipeline_tag: text-to-image tags: - Stable Diffusion - image-generation - Flux - diffusers --- !Controlnet collections for Flux <img src="https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/join-our-discord-rev1.png?raw=true"> This repository provides a collection of ControlNet checkpoints for FLUX.1-dev model by Bl...', '["diffusers","stable diffusion","image-generation","flux","text-to-image","en","license:other","region:us"]', 'text-to-image', 534, 9096, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/XLabs-AI/flux-controlnet-collections","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: flux-1-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.\nlanguage:\n- en\npipeline_tag: text-to-image\ntags:\n- Stable Diffusion\n- image-generation\n- Flux\n- diffusers\n---\n\n![Controlnet collections for Flux](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/flux-controlnet-collections.png?raw=true)\n[<img src="https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/light/join-our-discord-rev1.png?raw=true">](https://discord.gg/FHY2guThfy)\n\nThis repository provides a collection of ControlNet checkpoints for\n[FLUX.1-dev model](https://huggingface.co/black-forest-labs/FLUX.1-dev) by Black Forest Labs\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/depth_result1.png?raw=true)\n\n[See our github](https://github.com/XLabs-AI/x-flux-comfyui) for comfy ui workflows.\n![Example Picture 1](https://github.com/XLabs-AI/x-flux-comfyui/blob/main/assets/image1.png?raw=true)\n\n[See our github](https://github.com/XLabs-AI/x-flux) for train script, train configs and demo script for inference.\n\n# Models\n\nOur collection supports 3 models:\n- Canny\n- HED\n- Depth (Midas)\n\nEach ControlNet is trained on 1024x1024 resolution and works for 1024x1024 resolution.\nWe release **v3 versions** - better and realistic versions, which can be used directly in ComfyUI!   \n\nPlease, see our [ComfyUI custom nodes installation guide](https://github.com/XLabs-AI/x-flux-comfyui)\n\n\n# Examples\n\nSee examples of our models results below.  \nAlso, some generation results with input images are provided in "Files and versions"\n\n# Inference\n\nTo try our models, you have 2 options:\n1. Use main.py from our [official repo](https://github.com/XLabs-AI/x-flux)\n2. Use our custom nodes for ComfyUI and test it with provided workflows (check out folder /workflows)\n3. Use gradio demo\n\nSee examples how to launch our models:\n\n## Canny ControlNet (version 3)\n\n1. Clone our [x-flux-comfyui](https://github.com/XLabs-AI/x-flux-comfyui) custom nodes\n2. Launch ComfyUI\n3. Try our canny_workflow.json\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/canny_result1.png?raw=true)\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/canny_result2.png?raw=true)\n\n\n## Depth ControlNet (version 3)\n\n1. Clone our [x-flux-comfyui](https://github.com/XLabs-AI/x-flux-comfyui) custom nodes\n2. Launch ComfyUI\n3. Try our depth_workflow.json\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/depth_result1.png?raw=true)\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/depth_result2.png?raw=true)\n\n\n## HED ControlNet (version 3)\n\n1. Clone our [x-flux-comfyui](https://github.com/XLabs-AI/x-flux-comfyui) custom nodes\n2. Launch ComfyUI\n3. Try our hed_workflow.json\n\n![Example Picture 1](https://github.com/XLabs-AI/x-flux/blob/main/assets/readme/examples/hed_result1.png?raw=true)\n\n## License\n\nOur weights fall under the [FLUX.1 [dev]](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md) Non-Commercial License<br/>', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":11922020867,"files_count":33,"spaces_count":8,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux-comfyui","source_url":"https://github.com/XLabs-AI/x-flux-comfyui"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux-comfyui","source_url":"https://github.com/XLabs-AI/x-flux-comfyui"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux-comfyui","source_url":"https://github.com/XLabs-AI/x-flux-comfyui"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux-comfyui","source_url":"https://github.com/XLabs-AI/x-flux-comfyui"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux-comfyui","source_url":"https://github.com/XLabs-AI/x-flux-comfyui"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux-comfyui","source_url":"https://github.com/XLabs-AI/x-flux-comfyui"},{"type":"has_code","target_id":"github:XLabs-AI:x-flux","source_url":"https://github.com/XLabs-AI/x-flux"}]', NULL, 'Other', 'approved', 82.3, 'a2f8694f9dc2be58c28b0cecfa172939', NULL, 'https://huggingface.co/XLabs-AI/flux-controlnet-collections/resolve/main/assets/canny_v2_res1.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-XLabs-AI-flux-controlnet-collections from https://huggingface.co/XLabs-AI/flux-controlnet-collections/resolve/main/assets/canny_v2_res1.png
Image converted to WebP: data/images/huggingface-XLabs-AI-flux-controlnet-collections.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-zai-org-GLM-4.5-Air', 'huggingface--zai-org--glm-4.5-air', 'GLM-4.5-Air', 'zai-org', '--- language: - en - zh library_name: transformers license: mit pipeline_tag: text-generation --- <div align="center"> <img src=https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/logo.svg width="15%"/> </div> <p align="center"> ðŸ‘‹ Join our <a href="https://discord.gg/QR7SARHRxK" target="_blank">Discord</a> community. <br> ðŸ“– Check out the GLM-4.5 <a href="https://z.ai/blog/glm-4.5" target="_blank">technical blog</a>, <a href="https://arxiv.org/abs/2508.06471" target="...', '["transformers","safetensors","glm4_moe","text-generation","conversational","en","zh","arxiv:2508.06471","license:mit","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 531, 568166, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/zai-org/GLM-4.5-Air","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\n- zh\nlibrary_name: transformers\nlicense: mit\npipeline_tag: text-generation\n---\n\n# GLM-4.5-Air\n\n<div align="center">\n<img src=https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/logo.svg width="15%"/>\n</div>\n<p align="center">\n    ðŸ‘‹ Join our <a href="https://discord.gg/QR7SARHRxK" target="_blank">Discord</a> community.\n    <br>\n    ðŸ“– Check out the GLM-4.5 <a href="https://z.ai/blog/glm-4.5" target="_blank">technical blog</a>, <a href="https://arxiv.org/abs/2508.06471" target="_blank">technical report</a>, and <a href="https://zhipu-ai.feishu.cn/wiki/Gv3swM0Yci7w7Zke9E0crhU7n7D" target="_blank">Zhipu AI technical documentation</a>.\n    <br>\n    ðŸ“ Use GLM-4.5 API services on <a href="https://docs.z.ai/guides/llm/glm-4.5">Z.ai API Platform (Global)</a> or <br> <a href="https://docs.bigmodel.cn/cn/guide/models/text/glm-4.5">Zhipu AI Open Platform (Mainland China)</a>.\n    <br>\n    ðŸ‘‰ One click to <a href="https://chat.z.ai">GLM-4.5</a>.\n</p>\n  \n## Model Introduction\n\nThe **GLM-4.5** series models are foundation models designed for intelligent agents. GLM-4.5 has **355** billion total parameters with **32** billion active parameters, while GLM-4.5-Air adopts a more compact design with **106** billion total parameters and **12** billion active parameters. GLM-4.5 models unify reasoning, coding, and intelligent agent capabilities to meet the complex demands of intelligent agent applications.\n\nBoth GLM-4.5 and GLM-4.5-Air are hybrid reasoning models that provide two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses.\n\nWe have open-sourced the base models, hybrid reasoning models, and FP8 versions of the hybrid reasoning models for both GLM-4.5 and GLM-4.5-Air. They are released under the MIT open-source license and can be used commercially and for secondary development.\n\nAs demonstrated in our comprehensive evaluation across 12 industry-standard benchmarks, GLM-4.5 achieves exceptional performance with a score of **63.2**, in the **3rd** place among all the proprietary and open-source  models. Notably, GLM-4.5-Air delivers competitive results at **59.8** while maintaining superior efficiency.\n\n![bench](https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/bench.png)\n\nFor more eval results, show cases, and technical details, please visit\nour [technical blog](https://z.ai/blog/glm-4.5) or [technical report](https://huggingface.co/papers/2508.06471).\n\nThe model code, tool parser and reasoning parser can be found in the implementation of [transformers](https://github.com/huggingface/transformers/tree/main/src/transformers/models/glm4_moe), [vLLM](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/glm4_moe_mtp.py) and [SGLang](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/models/glm4_moe.py).\n\n## Quick Start\n\nPlease refer our [github page](https://github.com/zai-org/GLM-4.5) for more detail.', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":110468824832,"storage_bytes":220959914155,"files_count":55,"spaces_count":22,"gated":false,"private":false,"config":{"architectures":["Glm4MoeForCausalLM"],"model_type":"glm4_moe","tokenizer_config":{"eos_token":"<|endoftext|>","pad_token":"<|endoftext|>"},"chat_template_jinja":"[gMASK]<sop>\n{%- if tools -%}\n<|system|>\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>\n{% for tool in tools %}\n{{ tool | tojson(ensure_ascii=False) }}\n{% endfor %}\n</tools>\n\nFor each function call, output the function name and arguments within the following XML format:\n<tool_call>{function-name}\n<arg_key>{arg-key-1}</arg_key>\n<arg_value>{arg-value-1}</arg_value>\n<arg_key>{arg-key-2}</arg_key>\n<arg_value>{arg-value-2}</arg_value>\n...\n</tool_call>{%- endif -%}\n{%- macro visible_text(content) -%}\n    {%- if content is string -%}\n        {{- content }}\n    {%- elif content is iterable and content is not mapping -%}\n        {%- for item in content -%}\n            {%- if item is mapping and item.type == ''text'' -%}\n                {{- item.text }}\n            {%- elif item is string -%}\n                {{- item }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{- content }}\n    {%- endif -%}\n{%- endmacro -%}\n{%- set ns = namespace(last_user_index=-1) %}\n{%- for m in messages %}\n    {%- if m.role == ''user'' %}\n        {% set ns.last_user_index = loop.index0 -%}\n    {%- endif %}\n{%- endfor %}\n{% for m in messages %}\n{%- if m.role == ''user'' -%}<|user|>\n{{ visible_text(m.content) }}\n{{- ''/nothink'' if (enable_thinking is defined and not enable_thinking and not visible_text(m.content).endswith(\"/nothink\")) else '''' -}}\n{%- elif m.role == ''assistant'' -%}\n<|assistant|>\n{%- set reasoning_content = '''' %}\n{%- set content = visible_text(m.content) %}\n{%- if m.reasoning_content is string %}\n    {%- set reasoning_content = m.reasoning_content %}\n{%- else %}\n    {%- if ''</think>'' in content %}\n        {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n        {%- set content = content.split(''</think>'')[-1].lstrip(''\\n'') %}\n    {%- endif %}\n{%- endif %}\n{%- if loop.index0 > ns.last_user_index and reasoning_content -%}\n{{ ''\\n<think>'' + reasoning_content.strip() +  ''</think>''}}\n{%- else -%}\n{{ ''\\n<think></think>'' }}\n{%- endif -%}\n{%- if content.strip() -%}\n{{ ''\\n'' + content.strip() }}\n{%- endif -%}\n{% if m.tool_calls %}\n{% for tc in m.tool_calls %}\n{%- if tc.function %}\n    {%- set tc = tc.function %}\n{%- endif %}\n{{ ''\\n<tool_call>'' + tc.name }}\n{% set _args = tc.arguments %}\n{% for k, v in _args.items() %}\n<arg_key>{{ k }}</arg_key>\n<arg_value>{{ v | tojson(ensure_ascii=False) if v is not string else v }}</arg_value>\n{% endfor %}\n</tool_call>{% endfor %}\n{% endif %}\n{%- elif m.role == ''tool'' -%}\n{%- if m.content is string -%}\n{%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n    {{- ''<|observation|>'' }}\n{%- endif %}\n{{- ''\\n<tool_response>\\n'' }}\n{{- m.content }}\n{{- ''\\n</tool_response>'' }}\n{%- else -%}\n<|observation|>{% for tr in m.content %}\n\n<tool_response>\n{{ tr.output if tr.output is defined else tr }}\n</tool_response>{% endfor -%}\n{% endif -%}\n{%- elif m.role == ''system'' -%}\n<|system|>\n{{ visible_text(m.content) }}\n{%- endif -%}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    <|assistant|>{{- ''\\n<think></think>'' if (enable_thinking is defined and not enable_thinking) else '''' -}}\n{%- endif -%}"}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:zai-org:GLM-4.5","source_url":"https://github.com/zai-org/GLM-4.5"},{"type":"based_on_paper","target_id":"arxiv:2508.06471","source_url":"https://arxiv.org/abs/2508.06471"}]', NULL, 'MIT', 'approved', 62.3, '32983750a7882277606d56507f07bae5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-lovis93-next-scene-qwen-image-lora-2509', 'huggingface--lovis93--next-scene-qwen-image-lora-2509', 'next-scene-qwen-image-lora-2509', 'lovis93', '--- license: mit language: en base_model: - Qwen/Qwen-Image-Edit-2509 pipeline_tag: image-to-image tags: - lora - cinematic - comfyui - qwen - image-editing - next-scene - ai-video - diffusers --- --- ðŸš€ **New Model:** **What''s New in V2:** - ðŸŽ¯ **Trained on higher quality data** for significantly improved results - ðŸ’ª **Better command responsiveness** - the model follows your prompts more accurately - ðŸ–¼ï¸ **Fixed black bar artifacts** - no more unwanted black borders on generated images - âš¡ ...', '["diffusers","lora","cinematic","comfyui","qwen","image-editing","next-scene","ai-video","image-to-image","en","base_model:qwen/qwen-image-edit-2509","base_model:adapter:qwen/qwen-image-edit-2509","license:mit","region:us"]', 'image-to-image', 529, 23532, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/lovis93/next-scene-qwen-image-lora-2509","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlanguage: en\nbase_model:\n- Qwen/Qwen-Image-Edit-2509\npipeline_tag: image-to-image\ntags:\n- lora\n- cinematic\n- comfyui\n- qwen\n- image-editing\n- next-scene\n- ai-video\n- diffusers\n---\n\n# ðŸŽ¥ next-scene-qwen-image-lora-2509\n\n---\n\n## ðŸŽ‰ âœ¨ **UPDATE - Version 2 Now Available! (21 Oct 2025)** âœ¨ ðŸŽ‰\n\nðŸš€ **New Model:** `next-scene_lora-v2-3000.safetensors`\n\n**What''s New in V2:**\n- ðŸŽ¯ **Trained on higher quality data** for significantly improved results\n- ðŸ’ª **Better command responsiveness** - the model follows your prompts more accurately\n- ðŸ–¼ï¸ **Fixed black bar artifacts** - no more unwanted black borders on generated images\n- âš¡ **Overall enhanced performance** - smoother transitions and better cinematic flow\n\n**Recommended:** Use V2 for all new projects.\n\n**ðŸ“¥ ComfyUI Workflow:** [workflow-comfyui-basic-next-scene-v2.json](workflow-comfyui-basic-next-scene-v2.json)\n\n### V2 Demo Examples:\n\n![Demo 1 V2](01-update.gif) ![Demo 2 V2](02-update.gif) ![Demo 3 V2](03-update.gif)\n\n---\n\n**next-scene-qwen-image-lora-2509** is a **LoRA adapter fine-tuned on Qwen-Image-Edit (build 2509)**, purpose-built to generate cinematic image sequences with natural visual progression from frame to frame.\n\nThis model enables Qwen Image Edit to think like a film directorâ€”understanding camera dynamics, visual composition, and narrative continuity to create shots that flow seamlessly into one another.\n\n---\n\n## ðŸ“¦ Version 1 (Legacy)\n\n**Model File:** `next-scene_lora_v1-3000.safetensors`  \n**ComfyUI Workflow:** [workflow-comfyui-basic-next-scene.json](workflow-comfyui-basic-next-scene.json)\n\n### V1 Demo Examples:\n\n![Demo 1 V1](01.gif) ![Demo 2 V1](02.gif) ![Demo 3 V1](03.gif)\n\n---\n\n## ðŸ§  What This Model Does\n\nThis LoRA brings **cinematic storytelling continuity** into AI image generation workflows.\n\nEach output frame functions as the *"Next Scene"* in an evolving visual narrative, maintaining compositional coherence while introducing organic transitions such as:\n\n- **Camera movement:** Dolly shots, push-ins, pull-backs, and tracking moves\n- **Framing evolution:** Wide to close-up transitions, angle shifts, reframing\n- **Environmental reveals:** New characters entering frame, expanded scenery, spatial progression\n- **Atmospheric shifts:** Lighting changes, weather evolution, time-of-day transitions\n\n### Examples of Cinematic Logic:\n\n- *"Next Scene: The camera pulls back from a tight close-up on the airship to a sweeping aerial view, revealing an entire fleet of vessels soaring through a fantasy landscape."*\n\n- *"Next Scene: The camera tracks forward and tilts down, bringing the sun and helicopters closer into frame as a strong lens flare intensifies."*\n\n- *"Next Scene: The camera pans right, removing the dragon and rider from view while revealing more of the floating mountain range in the distance."*\n\n---\n\n## âš™ï¸ Usage Instructions\n\n### Basic Setup:\n\n1. Load **Qwen-Image-Edit 2509** as your base model\n2. Add a **LoRA Loader** node and select:\n   - **V2 (Recommended):** `next-scene_lora-v2-3000.safetensors`\n   - **V1 (Legacy):** `next-scene_lora_v1-3000.safetensors`\n3. Set LoRA strength: **0.7 â€“ 0.8** (recommended)\n4. Structure your prompts with **"Next Scene:"** prefix for optimal results\n\n### Example Prompt:\n\n```\nNext Scene: The camera moves slightly forward as sunlight breaks through the clouds, casting a soft glow around the character''s silhouette in the mist. Realistic cinematic style, atmospheric depth.\n```\n\n### Pro Tips:\n\n- Begin prompts with camera direction for stronger continuity\n- Specify lighting and atmospheric changes for mood consistency\n- Chain multiple generations to create sequential storyboards\n- Works particularly well with landscape and establishing shots\n\n---\n\n## ðŸŽ¬ Design Philosophy\n\nTrained on an extensive, curated cinematic dataset (proprietary), this model has learned to *think directionally* rather than just visually.\n\nIt doesn''t simply modify an imageâ€”it **advances the story**, preserving spatial relationships, lighting consistency, and emotional resonance across sequential frames.\n\n### Ideal Applications:\n\n- **Storyboard generation** for film and animation pre-production\n- **Cinematic AI video pipelines** requiring frame-to-frame coherence\n- **Sequential narrative workflows** in ComfyUI and similar tools\n- **Concept art evolution** showing scene progression\n- **Visual storytelling** for creative projects and presentations\n\n---\n\n## âš ï¸ Important Limitations\n\n- **Not optimized for:** Static portraits, single-image illustration tasks, or non-sequential edits\n- **Best suited for:** Multi-frame workflows with narrative progression\n- **Design priority:** Storytelling flow and continuity over isolated image perfection\n- **Recommended use case:** Scene-to-scene transitions rather than detailed object manipulation\n\n---\n\n## ðŸ§± Technical Specifications\n\n- **Base Model:** Qwen-Image-Edit (build 2509)\n- **Architecture:** Low-Rank Adaptation (LoRA)\n- **Training Objective:** Scene continuity and cinematic shot coherence\n- **Dataset:** Large-scale proprietary cinematic imagery\n- **Recommended Strength:** 0.7â€“0.8\n- **Compatible Platforms:** ComfyUI, Automatic1111 (with Qwen support), custom pipelines\n\n---\n\n## ðŸ“„ License\n\n**MIT License** â€” Free for research, educational, and creative use.\n\nCommercial applications require independent testing and proper attribution. See LICENSE file for full terms.\n\n---\n\n## ðŸŒ Creator\n\nDeveloped by **[@lovis93](https://huggingface.co/lovis93)**\n\nPushing the boundaries of AI-directed visual storytelling and cinematic image generation.\n\n---\n\n## ðŸ¦ Share This Model\n\nðŸŽ¥ Introducing **next-scene-qwen-image-lora-2509**\n\nA LoRA fine-tuned for **Qwen-Image-Edit 2509** that thinks like a film director.\n\nIt evolves each frame naturallyâ€”new angles, new lighting, same coherent world.\n\nPerfect for cinematic storyboards, sequential edits, and "Next Scene" workflows.\n\nðŸ‘‰ https://huggingface.co/lovis93/next-scene-qwen-image-lora-2509\n\n#AIart #ComfyUI #Qwen #LoRA #GenerativeAI #AIcinema #ImageEditing', '{"pipeline_tag":"image-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":616421537,"files_count":12,"spaces_count":34,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'MIT', 'approved', 62.2, '4974b28f49dd8f74ce893e7a46995bd6', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-IndexTeam-IndexTTS-2', 'huggingface--indexteam--indextts-2', 'IndexTTS-2', 'IndexTeam', '--- license: apache-2.0 language: - en --- <center><h3>IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</h3></center> <div align="center"> <a href=''https://arxiv.org/abs/2506.21619''> <img src=''https://img.shields.io/badge/ArXiv-2506.21619-red?logo=arxiv''/> </a> <br/> <a href=''https://github.com/index-tts/index-tts''> <img src=''https://img.shields.io/badge/GitHub-Code-orange?logo=github''/> </a> <a href=''https://index-tts.github...', '["safetensors","en","arxiv:2506.21619","arxiv:2502.05512","license:apache-2.0","region:us"]', 'other', 528, 21880, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/IndexTeam/IndexTTS-2","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\n---\n## ðŸ‘‰ðŸ» IndexTTS2 ðŸ‘ˆðŸ»\n\n<center><h3>IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</h3></center>\n\n\n<div align="center">\n  <a href=''https://arxiv.org/abs/2506.21619''>\n    <img src=''https://img.shields.io/badge/ArXiv-2506.21619-red?logo=arxiv''/>\n  </a>\n  <br/>\n  <a href=''https://github.com/index-tts/index-tts''>\n    <img src=''https://img.shields.io/badge/GitHub-Code-orange?logo=github''/>\n  </a>\n  <a href=''https://index-tts.github.io/index-tts2.github.io/''>\n    <img src=''https://img.shields.io/badge/GitHub-Demo-orange?logo=github''/>\n  </a>\n  <br/>\n  <!--a href=''https://huggingface.co/spaces/IndexTeam/IndexTTS''>\n    <img src=''https://img.shields.io/badge/HuggingFace-Demo-blue?logo=huggingface''/>\n  </a-->\n  <a href=''https://huggingface.co/IndexTeam/IndexTTS-2''>\n    <img src=''https://img.shields.io/badge/HuggingFace-Model-blue?logo=huggingface'' />\n  </a>\n  <br/>\n  <!--a href=''https://modelscope.cn/studios/IndexTeam/IndexTTS-Demo''>\n    <img src=''https://img.shields.io/badge/ModelScope-Demo-purple?logo=modelscope''/>\n  </a-->\n  <a href=''https://modelscope.cn/models/IndexTeam/IndexTTS-2''>\n    <img src=''https://img.shields.io/badge/ModelScope-Model-purple?logo=modelscope''/>\n  </a>\n</div>\n\n\n## Acknowledge\n1. [tortoise-tts](https://github.com/neonbjb/tortoise-tts)\n2. [XTTSv2](https://github.com/coqui-ai/TTS)\n3. [BigVGAN](https://github.com/NVIDIA/BigVGAN)\n4. [wenet](https://github.com/wenet-e2e/wenet/tree/main)\n5. [icefall](https://github.com/k2-fsa/icefall)\n6. [maskgct](https://github.com/open-mmlab/Amphion/tree/main/models/tts/maskgct)\n7. [seed-vc](https://github.com/Plachtaa/seed-vc)\n\n\n## ðŸ“š Citation\n\nðŸŒŸ If you find our work helpful, please leave us a star and cite our paper.\n\n\nIndexTTS2\n```\n@article{zhou2025indextts2,\n  title={IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech},\n  author={Siyi Zhou, Yiquan Zhou, Yi He, Xun Zhou, Jinchao Wang, Wei Deng, Jingchen Shu},\n  journal={arXiv preprint arXiv:2506.21619},\n  year={2025}\n}\n```\n\nIndexTTS\n```\n@article{deng2025indextts,\n  title={IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System},\n  author={Wei Deng, Siyi Zhou, Jingchen Shu, Jinchao Wang, Lu Wang},\n  journal={arXiv preprint arXiv:2502.05512},\n  year={2025},\n  doi={10.48550/arXiv.2502.05512},\n  url={https://arxiv.org/abs/2502.05512}\n}\n```\n', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":5919372763,"files_count":20,"spaces_count":5,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:index-tts:index-tts''>","source_url":"https://github.com/index-tts/index-tts''>"},{"type":"has_code","target_id":"github:neonbjb:tortoise-tts","source_url":"https://github.com/neonbjb/tortoise-tts"},{"type":"has_code","target_id":"github:coqui-ai:TTS","source_url":"https://github.com/coqui-ai/TTS"},{"type":"has_code","target_id":"github:NVIDIA:BigVGAN","source_url":"https://github.com/NVIDIA/BigVGAN"},{"type":"has_code","target_id":"github:wenet-e2e:wenet","source_url":"https://github.com/wenet-e2e/wenet"},{"type":"has_code","target_id":"github:k2-fsa:icefall","source_url":"https://github.com/k2-fsa/icefall"},{"type":"has_code","target_id":"github:open-mmlab:Amphion","source_url":"https://github.com/open-mmlab/Amphion"},{"type":"has_code","target_id":"github:Plachtaa:seed-vc","source_url":"https://github.com/Plachtaa/seed-vc"},{"type":"based_on_paper","target_id":"arxiv:2506.21619","source_url":"https://arxiv.org/abs/2506.21619"},{"type":"based_on_paper","target_id":"arxiv:2502.05512","source_url":"https://arxiv.org/abs/2502.05512"}]', NULL, 'Apache-2.0', 'approved', 62.2, 'b9a6bfbf7aeaa5f1eeecedc0cd9e8433', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-4B-Instruct-2507', 'huggingface--qwen--qwen3-4b-instruct-2507', 'Qwen3-4B-Instruct-2507', 'Qwen', '--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/blob/main/LICENSE pipeline_tag: text-generation --- <a href="https://chat.qwen.ai" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> We introduce the updated version of the **Qwen3-4B non-thinking mode**, named **Qwen3-4B-Instruct-250...', '["transformers","safetensors","qwen3","text-generation","conversational","arxiv:2505.09388","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 528, 6289445, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507/blob/main/LICENSE\npipeline_tag: text-generation\n---\n\n# Qwen3-4B-Instruct-2507\n<a href="https://chat.qwen.ai" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Highlights\n\nWe introduce the updated version of the **Qwen3-4B non-thinking mode**, named **Qwen3-4B-Instruct-2507**, featuring the following key enhancements:\n\n- **Significant improvements** in general capabilities, including **instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage**.\n- **Substantial gains** in long-tail knowledge coverage across **multiple languages**.\n- **Markedly better alignment** with user preferences in **subjective and open-ended tasks**, enabling more helpful responses and higher-quality text generation.\n- **Enhanced capabilities** in **256K long-context understanding**.\n\n![image/jpeg](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-2507/Qwen3-4B-Instruct.001.jpeg)\n\n## Model Overview\n\n**Qwen3-4B-Instruct-2507** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 4.0B\n- Number of Paramaters (Non-Embedding): 3.6B\n- Number of Layers: 36\n- Number of Attention Heads (GQA): 32 for Q and 8 for KV\n- Context Length: **262,144 natively**. \n\n**NOTE: This model supports only non-thinking mode and does not generate ``<think></think>`` blocks in its output. Meanwhile, specifying `enable_thinking=False` is no longer required.**\n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n\n## Performance\n\n|  | GPT-4.1-nano-2025-04-14 | Qwen3-30B-A3B Non-Thinking | Qwen3-4B Non-Thinking | Qwen3-4B-Instruct-2507 |\n|--- | --- | --- | --- | --- |\n| **Knowledge** | | | |\n| MMLU-Pro | 62.8 | 69.1 | 58.0 | **69.6** |\n| MMLU-Redux | 80.2 | 84.1 | 77.3 | **84.2** |\n| GPQA | 50.3 | 54.8 | 41.7 | **62.0** |\n| SuperGPQA | 32.2 | 42.2 | 32.0 | **42.8** |\n| **Reasoning** | | | |\n| AIME25 | 22.7 | 21.6 | 19.1 | **47.4** |\n| HMMT25 | 9.7 | 12.0 | 12.1 | **31.0** |\n| ZebraLogic | 14.8 | 33.2 | 35.2 | **80.2** |\n| LiveBench 20241125 | 41.5 | 59.4 | 48.4 | **63.0** |\n| **Coding** | | | |\n| LiveCodeBench v6 (25.02-25.05) | 31.5 | 29.0 | 26.4 | **35.1** |\n| MultiPL-E | 76.3 | 74.6 | 66.6 | **76.8** |\n| Aider-Polyglot |  9.8 | **24.4** | 13.8 | 12.9 |\n| **Alignment** | | | |\n| IFEval | 74.5 | **83.7** | 81.2 | 83.4 |\n| Arena-Hard v2* | 15.9 | 24.8 | 9.5 | **43.4** |\n| Creative Writing v3 | 72.7 | 68.1 | 53.6 | **83.5** |\n| WritingBench | 66.9 | 72.2 | 68.5 | **83.4** |\n| **Agent** | | | |\n| BFCL-v3 | 53.0 | 58.6 | 57.6 | **61.9** |\n| TAU1-Retail | 23.5 | 38.3 | 24.3 | **48.7** |\n| TAU1-Airline | 14.0 | 18.0 | 16.0 | **32.0** |\n| TAU2-Retail | - | 31.6 | 28.1 | **40.4** |\n| TAU2-Airline | - | 18.0 | 12.0 | **24.0** |\n| TAU2-Telecom | - | **18.4** | 17.5 | 13.2 |\n| **Multilingualism** | | | |\n| MultiIF | 60.7 | **70.8** | 61.3 | 69.0 |\n| MMLU-ProX | 56.2 | **65.1** | 49.6 | 61.6 |\n| INCLUDE | 58.6 | **67.8** | 53.8 | 60.1 |\n| PolyMATH | 15.6 | 23.3 | 16.6 | **31.1** |\n\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\n\n\n## Quickstart\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: ''qwen3''\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen3-4B-Instruct-2507"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\n\n# prepare the model input\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=16384\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\n\nprint("content:", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-4B-Instruct-2507 --context-length 262144\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-4B-Instruct-2507 --max-model-len 262144\n    ```\n\n**Note: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as `32,768`.**\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    ''model'': ''Qwen3-4B-Instruct-2507'',\n\n    # Use a custom endpoint compatible with OpenAI API:\n    ''model_server'': ''http://localhost:8000/v1'',  # api_base\n    ''api_key'': ''EMPTY'',\n}\n\n# Define Tools\ntools = [\n    {''mcpServers'': {  # You can specify the MCP configuration file\n            ''time'': {\n                ''command'': ''uvx'',\n                ''args'': [''mcp-server-time'', ''--local-timezone=Asia/Shanghai'']\n            },\n            "fetch": {\n                "command": "uvx",\n                "args": ["mcp-server-fetch"]\n            }\n        }\n    },\n  ''code_interpreter'',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{''role'': ''user'', ''content'': ''https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen''}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - We suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include "Please reason step by step, and put your final answer within \boxed{}." in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: "Please show your choice in the `answer` field with only the choice letter, e.g., `"answer": "C"`."\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":4022468096,"storage_bytes":64383326441,"files_count":13,"spaces_count":80,"gated":false,"private":false,"config":{"architectures":["Qwen3ForCausalLM"],"model_type":"qwen3","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen3","source_url":"https://github.com/QwenLM/Qwen3"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Agent","source_url":"https://github.com/QwenLM/Qwen-Agent"},{"type":"based_on_paper","target_id":"arxiv:2505.09388","source_url":"https://arxiv.org/abs/2505.09388"}]', NULL, 'Apache-2.0', 'approved', 62.2, '35c9b85304ec56b650b3c13c740ad991', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openai-whisper-large', 'huggingface--openai--whisper-large', 'whisper-large', 'openai', '--- language: - en - zh - de - es - ru - ko - fr - ja - pt - tr - pl - ca - nl - ar - sv - it - id - hi - fi - vi - he - uk - el - ms - cs - ro - da - hu - ta - no - th - ur - hr - bg - lt - la - mi - ml - cy - sk - te - fa - lv - bn - sr - az - sl - kn - et - mk - br - eu - is - hy - ne - mn - bs - kk - sq - sw - gl - mr - pa - si - km - sn - yo - so - af - oc - ka - be - tg - sd - gu - am - yi - lo - uz - fo - ht - ps - tk - nn - mt - sa - lb - my - bo - tl - mg - as - tt - haw - ln - ha - ...', '["transformers","pytorch","tf","jax","safetensors","whisper","automatic-speech-recognition","audio","hf-asr-leaderboard","en","zh","de","es","ru","ko","fr","ja","pt","tr","pl","ca","nl","ar","sv","it","id","hi","fi","vi","he","uk","el","ms","cs","ro","da","hu","ta","no","th","ur","hr","bg","lt","la","mi","ml","cy","sk","te","fa","lv","bn","sr","az","sl","kn","et","mk","br","eu","is","hy","ne","mn","bs","kk","sq","sw","gl","mr","pa","si","km","sn","yo","so","af","oc","ka","be","tg","sd","gu","am","yi","lo","uz","fo","ht","ps","tk","nn","mt","sa","lb","my","bo","tl","mg","as","tt","haw","ln","ha","ba","jw","su","arxiv:2212.04356","license:apache-2.0","model-index","endpoints_compatible","deploy:azure","region:us"]', 'automatic-speech-recognition', 527, 120284, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openai/whisper-large","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage: \n- en\n- zh\n- de\n- es\n- ru\n- ko\n- fr\n- ja\n- pt\n- tr\n- pl\n- ca\n- nl\n- ar\n- sv\n- it\n- id\n- hi\n- fi\n- vi\n- he\n- uk\n- el\n- ms\n- cs\n- ro\n- da\n- hu\n- ta\n- no\n- th\n- ur\n- hr\n- bg\n- lt\n- la\n- mi\n- ml\n- cy\n- sk\n- te\n- fa\n- lv\n- bn\n- sr\n- az\n- sl\n- kn\n- et\n- mk\n- br\n- eu\n- is\n- hy\n- ne\n- mn\n- bs\n- kk\n- sq\n- sw\n- gl\n- mr\n- pa\n- si\n- km\n- sn\n- yo\n- so\n- af\n- oc\n- ka\n- be\n- tg\n- sd\n- gu\n- am\n- yi\n- lo\n- uz\n- fo\n- ht\n- ps\n- tk\n- nn\n- mt\n- sa\n- lb\n- my\n- bo\n- tl\n- mg\n- as\n- tt\n- haw\n- ln\n- ha\n- ba\n- jw\n- su\ntags:\n- audio\n- automatic-speech-recognition\n- hf-asr-leaderboard\nwidget:\n- example_title: Librispeech sample 1\n  src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n- example_title: Librispeech sample 2\n  src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\nmodel-index:\n- name: whisper-large\n  results:\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: LibriSpeech (clean)\n      type: librispeech_asr\n      config: clean\n      split: test\n      args: \n        language: en\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 3.0\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: LibriSpeech (other)\n      type: librispeech_asr\n      config: other\n      split: test\n      args: \n        language: en\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 5.4\n  - task:\n      name: Automatic Speech Recognition\n      type: automatic-speech-recognition\n    dataset:\n      name: Common Voice 11.0\n      type: mozilla-foundation/common_voice_11_0\n      config: hi\n      split: test\n      args:\n        language: hi\n    metrics:\n    - name: Test WER\n      type: wer\n      value: 54.8\npipeline_tag: automatic-speech-recognition\nlicense: apache-2.0\n---\n\n# Whisper\n\nWhisper is a pre-trained model for automatic speech recognition (ASR) and speech translation. Trained on 680k hours \nof labelled data, Whisper models demonstrate a strong ability to generalise to many datasets and domains **without** the need \nfor fine-tuning.\n\nWhisper was proposed in the paper [Robust Speech Recognition via Large-Scale Weak Supervision](https://arxiv.org/abs/2212.04356) \nby Alec Radford et al from OpenAI. The original code repository can be found [here](https://github.com/openai/whisper).\n\n<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">\n  <p><b>Update:</b> following the release of the paper, the Whisper authors announced a <a href="ttps://huggingface.co/openai/whisper-large-v2"> large-v2</a> model trained for 2.5x more epochs with regularization. This <a href="ttps://huggingface.co/openai/whisper-large-v2"> large-v2</a> model surpasses the performance of the large model, with no architecture changes. Thus, it is recommended that the <a href="ttps://huggingface.co/openai/whisper-large-v2"> large-v2</a> model is used in-place of the original large model. </p>\n</div>\n\n\n**Disclaimer**: Content for this model card has partly been written by the Hugging Face team, and parts of it were \ncopied and pasted from the original model card.\n\n## Model details\n\nWhisper is a Transformer based encoder-decoder model, also referred to as a _sequence-to-sequence_ model. \nIt was trained on 680k hours of labelled speech data annotated using large-scale weak supervision. \n\nThe models were trained on either English-only data or multilingual data. The English-only models were trained \non the task of speech recognition. The multilingual models were trained on both speech recognition and speech \ntranslation. For speech recognition, the model predicts transcriptions in the *same* language as the audio. \nFor speech translation, the model predicts transcriptions to a *different* language to the audio.\n\nWhisper checkpoints come in five configurations of varying model sizes.\nThe smallest four are trained on either English-only or multilingual data.\nThe largest checkpoints are multilingual only. All ten of the pre-trained checkpoints \nare available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The \ncheckpoints are summarised in the following table with links to the models on the Hub:\n\n| Size     | Parameters | English-only                                         | Multilingual                                        |\n|----------|------------|------------------------------------------------------|-----------------------------------------------------|\n| tiny     | 39 M       | [âœ“](https://huggingface.co/openai/whisper-tiny.en)   | [âœ“](https://huggingface.co/openai/whisper-tiny)     |\n| base     | 74 M       | [âœ“](https://huggingface.co/openai/whisper-base.en)   | [âœ“](https://huggingface.co/openai/whisper-base)     |\n| small    | 244 M      | [âœ“](https://huggingface.co/openai/whisper-small.en)  | [âœ“](https://huggingface.co/openai/whisper-small)    |\n| medium   | 769 M      | [âœ“](https://huggingface.co/openai/whisper-medium.en) | [âœ“](https://huggingface.co/openai/whisper-medium)   |\n| large    | 1550 M     | x                                                    | [âœ“](https://huggingface.co/openai/whisper-large)    |\n| large-v2 | 1550 M     | x                                                    | [âœ“](https://huggingface.co/openai/whisper-large-v2) |\n\n# Usage\n\nTo transcribe audio samples, the model has to be used alongside a [`WhisperProcessor`](https://huggingface.co/docs/transformers/model_doc/whisper#transformers.WhisperProcessor).\n\nThe `WhisperProcessor` is used to:\n1. Pre-process the audio inputs (converting them to log-Mel spectrograms for the model)\n2. Post-process the model outputs (converting them from tokens to text)\n\nThe model is informed of which task to perform (transcription or translation) by passing the appropriate "context tokens". These context tokens \nare a sequence of tokens that are given to the decoder at the start of the decoding process, and take the following order:\n1. The transcription always starts with the `<|startoftranscript|>` token\n2. The second token is the language token (e.g. `<|en|>` for English)\n3. The third token is the "task token". It can take one of two values: `<|transcribe|>` for speech recognition or `<|translate|>` for speech translation\n4. In addition, a `<|notimestamps|>` token is added if the model should not include timestamp prediction\n\nThus, a typical sequence of context tokens might look as follows:\n```\n<|startoftranscript|> <|en|> <|transcribe|> <|notimestamps|>\n```\nWhich tells the model to decode in English, under the task of speech recognition, and not to predict timestamps.\n\nThese tokens can either be forced or un-forced. If they are forced, the model is made to predict each token at \neach position. This allows one to control the output language and task for the Whisper model. If they are un-forced, \nthe Whisper model will automatically predict the output langauge and task itself.\n\nThe context tokens can be set accordingly:\n\n```python\nmodel.config.forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language="english", task="transcribe")\n```\n\nWhich forces the model to predict in English under the task of speech recognition.\n\n## Transcription\n\n### English to English \nIn this example, the context tokens are ''unforced'', meaning the model automatically predicts the output language\n(English) and task (transcribe).\n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained("openai/whisper-large")\n>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large")\n>>> model.config.forced_decoder_ids = None\n\n>>> # load dummy dataset and read audio files\n>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")\n>>> sample = ds[0]["audio"]\n>>> input_features = processor(sample["array"], sampling_rate=sample["sampling_rate"], return_tensors="pt").input_features \n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=False)\n[''<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>'']\n\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n['' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'']\n```\nThe context tokens can be removed from the start of the transcription by setting `skip_special_tokens=True`.\n\n### French to French \nThe following example demonstrates French to French transcription by setting the decoder ids appropriately. \n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained("openai/whisper-large")\n>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language="french", task="transcribe")\n\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset("common_voice", "fr", split="test", streaming=True)\n>>> ds = ds.cast_column("audio", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))["audio"]\n>>> input_features = processor(input_speech["array"], sampling_rate=input_speech["sampling_rate"], return_tensors="pt").input_features\n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids)\n[''<|startoftranscript|><|fr|><|transcribe|><|notimestamps|> Un vrai travail intÃ©ressant va enfin Ãªtre menÃ© sur ce sujet.<|endoftext|>'']\n\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n['' Un vrai travail intÃ©ressant va enfin Ãªtre menÃ© sur ce sujet.'']\n```\n\n## Translation \nSetting the task to "translate" forces the Whisper model to perform speech translation.\n\n### French to English\n\n```python\n>>> from transformers import WhisperProcessor, WhisperForConditionalGeneration\n>>> from datasets import Audio, load_dataset\n\n>>> # load model and processor\n>>> processor = WhisperProcessor.from_pretrained("openai/whisper-large")\n>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large")\n>>> forced_decoder_ids = processor.get_decoder_prompt_ids(language="french", task="translate")\n\n>>> # load streaming dataset and read first audio sample\n>>> ds = load_dataset("common_voice", "fr", split="test", streaming=True)\n>>> ds = ds.cast_column("audio", Audio(sampling_rate=16_000))\n>>> input_speech = next(iter(ds))["audio"]\n>>> input_features = processor(input_speech["array"], sampling_rate=input_speech["sampling_rate"], return_tensors="pt").input_features\n\n>>> # generate token ids\n>>> predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n>>> # decode token ids to text\n>>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n['' A very interesting work, we will finally be given on this subject.'']\n```\n\n## Evaluation\n\nThis code snippet shows how to evaluate Whisper Large on [LibriSpeech test-clean](https://huggingface.co/datasets/librispeech_asr):\n \n```python\n>>> from datasets import load_dataset\n>>> from transformers import WhisperForConditionalGeneration, WhisperProcessor\n>>> import torch\n>>> from evaluate import load\n\n>>> librispeech_test_clean = load_dataset("librispeech_asr", "clean", split="test")\n\n>>> processor = WhisperProcessor.from_pretrained("openai/whisper-large")\n>>> model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-large").to("cuda")\n\n>>> def map_to_pred(batch):\n>>>     audio = batch["audio"]\n>>>     input_features = processor(audio["array"], sampling_rate=audio["sampling_rate"], return_tensors="pt").input_features\n>>>     batch["reference"] = processor.tokenizer._normalize(batch[''text''])\n>>> \n>>>     with torch.no_grad():\n>>>         predicted_ids = model.generate(input_features.to("cuda"))[0]\n>>>     transcription = processor.decode(predicted_ids)\n>>>     batch["prediction"] = processor.tokenizer._normalize(transcription)\n>>>     return batch\n\n>>> result = librispeech_test_clean.map(map_to_pred)\n\n>>> wer = load("wer")\n>>> print(100 * wer.compute(references=result["reference"], predictions=result["prediction"]))\n3.0003583080317572\n```\n\n## Long-Form Transcription\n\nThe Whisper model is intrinsically designed to work on audio samples of up to 30s in duration. However, by using a chunking \nalgorithm, it can be used to transcribe audio samples of up to arbitrary length. This is possible through Transformers \n[`pipeline`](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline) \nmethod. Chunking is enabled by setting `chunk_length_s=30` when instantiating the pipeline. With chunking enabled, the pipeline \ncan be run with batched inference. It can also be extended to predict sequence level timestamps by passing `return_timestamps=True`:\n\n```python\n>>> import torch\n>>> from transformers import pipeline\n>>> from datasets import load_dataset\n\n>>> device = "cuda:0" if torch.cuda.is_available() else "cpu"\n\n>>> pipe = pipeline(\n>>>   "automatic-speech-recognition",\n>>>   model="openai/whisper-large",\n>>>   chunk_length_s=30,\n>>>   device=device,\n>>> )\n\n>>> ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")\n>>> sample = ds[0]["audio"]\n\n>>> prediction = pipe(sample.copy(), batch_size=8)["text"]\n" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel."\n\n>>> # we can also return timestamps for the predictions\n>>> prediction = pipe(sample.copy(), batch_size=8, return_timestamps=True)["chunks"]\n[{''text'': '' Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.'',\n  ''timestamp'': (0.0, 5.44)}]\n```\n\nRefer to the blog post [ASR Chunking](https://huggingface.co/blog/asr-chunking) for more details on the chunking algorithm.\n\n## Fine-Tuning\n\nThe pre-trained Whisper model demonstrates a strong ability to generalise to different datasets and domains. However, \nits predictive capabilities can be improved further for certain languages and tasks through *fine-tuning*. The blog \npost [Fine-Tune Whisper with ðŸ¤— Transformers](https://huggingface.co/blog/fine-tune-whisper) provides a step-by-step \nguide to fine-tuning the Whisper model with as little as 5 hours of labelled data.\n\n### Evaluated Use\n\nThe primary intended users of these models are AI researchers studying robustness, generalization, capabilities, biases, and constraints of the current model. However, Whisper is also potentially quite useful as an ASR solution for developers, especially for English speech recognition. We recognize that once models are released, it is impossible to restrict access to only â€œintendedâ€ uses or to draw reasonable guidelines around what is or is not research.\n\nThe models are primarily trained and evaluated on ASR and speech translation to English tasks. They show strong ASR results in ~10 languages. They may exhibit additional capabilities, particularly if fine-tuned on certain tasks like voice activity detection, speaker classification, or speaker diarization but have not been robustly evaluated in these areas. We strongly recommend that users perform robust evaluations of the models in a particular context and domain before deploying them.\n\nIn particular, we caution against using Whisper models to transcribe recordings of individuals taken without their consent or purporting to use these models for any kind of subjective classification. We recommend against use in high-risk domains like decision-making contexts, where flaws in accuracy can lead to pronounced flaws in outcomes. The models are intended to transcribe and translate speech, use of the model for classification is not only not evaluated but also not appropriate, particularly to infer human attributes.\n\n\n## Training Data\n\nThe models are trained on 680,000 hours of audio and the corresponding transcripts collected from the internet. 65% of this data (or 438,000 hours) represents English-language audio and matched English transcripts, roughly 18% (or 126,000 hours) represents non-English audio and English transcripts, while the final 17% (or 117,000 hours) represents non-English audio and the corresponding transcript. This non-English data represents 98 different languages. \n\nAs discussed in [the accompanying paper](https://cdn.openai.com/papers/whisper.pdf), we see that performance on transcription in a given language is directly correlated with the amount of training data we employ in that language.\n\n\n## Performance and Limitations\n\nOur studies show that, over many existing ASR systems, the models exhibit improved robustness to accents, background noise, technical language, as well as zero shot translation from multiple languages into English; and that accuracy on speech recognition and translation is near the state-of-the-art level. \n\nHowever, because the models are trained in a weakly supervised manner using large-scale noisy data, the predictions may include texts that are not actually spoken in the audio input (i.e. hallucination). We hypothesize that this happens because, given their general knowledge of language, the models combine trying to predict the next word in audio with trying to transcribe the audio itself.\n\nOur models perform unevenly across languages, and we observe lower accuracy on low-resource and/or low-discoverability languages or languages where we have less training data. The models also exhibit disparate performance on different accents and dialects of particular languages, which may include higher word error rate across speakers of different genders, races, ages, or other demographic criteria. Our full evaluation results are presented in [the paper accompanying this release](https://cdn.openai.com/papers/whisper.pdf). \n\nIn addition, the sequence-to-sequence architecture of the model makes it prone to generating repetitive texts, which can be mitigated to some degree by beam search and temperature scheduling but not perfectly. Further analysis on these limitations are provided in [the paper](https://cdn.openai.com/papers/whisper.pdf). It is likely that this behavior and hallucinations may be worse on lower-resource and/or lower-discoverability languages.\n\n\n## Broader Implications\n\nWe anticipate that Whisper modelsâ€™ transcription capabilities may be used for improving accessibility tools. While Whisper models cannot be used for real-time transcription out of the box â€“ their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation. The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications.\n\nThere are also potential dual use concerns that come with releasing Whisper. While we hope the technology will be used primarily for beneficial purposes, making ASR technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication. Moreover, these models may have some capabilities to recognize specific individuals out of the box, which in turn presents safety concerns related both to dual use and disparate performance. In practice, we expect that the cost of transcription is not the limiting factor of scaling up surveillance projects.\n\n\n### BibTeX entry and citation info\n```bibtex\n@misc{radford2022whisper,\n  doi = {10.48550/ARXIV.2212.04356},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  publisher = {arXiv},\n  year = {2022},\n  copyright = {arXiv.org perpetual, non-exclusive license}\n}\n```\n', '{"pipeline_tag":"automatic-speech-recognition","library_name":"transformers","framework":"transformers","params":1543304960,"storage_bytes":30868435839,"files_count":16,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["WhisperForConditionalGeneration"],"model_type":"whisper","tokenizer_config":{"bos_token":"<|endoftext|>","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:openai:whisper","source_url":"https://github.com/openai/whisper"},{"type":"based_on_paper","target_id":"arxiv:2212.04356","source_url":"https://arxiv.org/abs/2212.04356"}]', NULL, 'Apache-2.0', 'approved', 77.2, '79a5529e64f3aa306106ee863ce1123a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-NexaAI-OmniVLM-968M', 'huggingface--nexaai--omnivlm-968m', 'OmniVLM-968M', 'NexaAI', '--- license: apache-2.0 tags: - multimodal - conversational - GGUF - Image-Text-to-Text --- - [Dec 16, 2024] Our work **"OmniVLM: A Token-Compressed, Sub-Billion-Parameter Vision-Language Model for Efficient On-Device Inference"** is now live on Arxiv! ðŸš€ - [Nov 27, 2024] **Model Improvements:** OmniVLM v3 model''s **GGUF file has been updated** in this Hugging Face Repo! âœ¨ ðŸ‘‰ Test these exciting changes in our Hugging Face Space - [Nov 22, 2024] **Model Improvements:** OmniVLM v2 model''s **GG...', '["gguf","multimodal","conversational","gguf","image-text-to-text","arxiv:2412.11475","license:apache-2.0","endpoints_compatible","region:us"]', 'other', 527, 3158, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/NexaAI/OmniVLM-968M","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\ntags:\n- multimodal\n- conversational\n- GGUF\n- Image-Text-to-Text\n---\n# OmniVLM\n\n## ðŸ”¥ Latest Update\n- [Dec 16, 2024] Our work **"OmniVLM: A Token-Compressed, Sub-Billion-Parameter Vision-Language Model for Efficient On-Device Inference"** is now live on [Arxiv](https://arxiv.org/abs/2412.11475)! ðŸš€ \n- [Nov 27, 2024] **Model Improvements:** OmniVLM v3 model''s **GGUF file has been updated** in this Hugging Face Repo! âœ¨\nðŸ‘‰ Test these exciting changes in our [Hugging Face Space](https://huggingface.co/spaces/NexaAIDev/omnivlm-dpo-demo)\n\n\n- [Nov 22, 2024] **Model Improvements:** OmniVLM v2 model''s **GGUF file has been updated** in this Hugging Face Repo! âœ¨ Key Improvements Include:\n  - Enhanced Art Descriptions\n  - Better Complex Image Understanding \n  - Improved Anime Recognition\n  - More Accurate Color and Detail Detection\n  - Expanded World Knowledge\n\nWe are continuously improving OmniVLM-968M based on your valuable feedback! **More exciting updates coming soon - Stay tuned!** â­\n\n\n## Introduction\n\nOmniVLM is a compact, sub-billion (968M) multimodal model for processing both visual and text inputs, optimized for edge devices. Improved on LLaVA''s architecture, it features:\n\n- **9x Token Reduction**: Reduces image tokens from **729** to **81**, cutting latency and computational cost aggressively. Note that the computation of vision encoder and the projection part keep the same, but the computation of language model backbone is reduced due to 9X shorter image token span.\n- **Trustworthy Result**: Reduces hallucinations using **DPO** training from trustworthy data.\n  \n**Quick Links:**\n1. Interactive Demo in our [Hugging Face Space](https://huggingface.co/spaces/NexaAIDev/omnivlm-dpo-demo). (Updated 2024 Nov 21)\n2. [Quickstart for local setup](#how-to-use-on-device)\n3. Learn more in our [Blogs](https://nexa.ai/blogs/omni-vision)\n\n**Feedback:** Send questions or comments about the model in our [Discord](https://discord.gg/nexa-ai)\n\n## Intended Use Cases\nOmniVLM is intended for **Visual Question Answering** (answering questions about images) and **Image Captioning** (describing scenes in photos), making it ideal for on-device applications.\n\n**Example Demo:**\nGenerating captions for a 1046Ã—1568 image on M4 Pro Macbook takes **< 2s processing time** and requires only 988 MB RAM and 948 MB Storage.\n\n<img src="https://cdn-uploads.huggingface.co/production/uploads/6618e0424dbef6bd3c72f89a/ueevDxicb98fXQ7zGN_E2.png" alt="Example" style="width:700px;"/>\n\n\n\n## Benchmarks\n\nBelow we demonstrate a figure to show how OmniVLM performs against nanollava. In all the tasks, OmniVLM outperforms the previous world''s smallest vision-language model.\n\n\n\nWe have conducted a series of experiments on benchmark datasets, including MM-VET, ChartQA, MMMU, ScienceQA, POPE to evaluate the performance of OmniVLM.\n\n| Benchmark         | Nexa AI OmniVLM v2 | Nexa AI OmniVLM v1 | nanoLLAVA |\n|-------------------|------------------------|------------------------|-----------|\n| ScienceQA (Eval)  | 71.0                  | 62.2                  | 59.0      |\n| ScienceQA (Test)  | 71.0                  | 64.5                  | 59.0      |\n| POPE              | 93.3                  | 89.4                  | 84.1      |\n| MM-VET            | 30.9                  | 27.5                  | 23.9      |\n| ChartQA (Test)    | 61.9                  | 59.2                  | NA        |\n| MMMU (Test)       | 42.1                  | 41.8                  | 28.6      |\n| MMMU (Eval)       | 40.0                  | 39.9                  | 30.4      |\n\n\n## How to Use On Device\nIn the following, we demonstrate how to run OmniVLM locally on your device.\n\n**Step 1: Install Nexa-SDK (local on-device inference framework)**\n\n[Install Nexa-SDK](https://github.com/NexaAI/nexa-sdk?tab=readme-ov-file#install-option-1-executable-installer)\n\n> Nexa-SDK is a open-sourced, local on-device inference framework, supporting text generation, image generation, vision-language models (VLM), audio-language models, speech-to-text (ASR), and text-to-speech (TTS) capabilities. Installable via Python Package or Executable Installer.\n\n**Step 2: Then run the following code in your terminal**\n\n```bash\nnexa run omniVLM \n```\n\n## Model Architecture ##\nOmniVLM''s architecture consists of three key components:\n\n- Base Language Model: Qwen2.5-0.5B-Instruct functions as the base model to process text inputs\n- Vision Encoder: SigLIP-400M operates at 384 resolution with 14Ã—14 patch size to generate image embeddings\n- Projection Layer: Multi-Layer Perceptron (MLP) aligns the vision encoder''s embeddings with the language model''s token space. Compared to vanilla Llava architecture, we designed a projector that reduce 9X image tokens.\n\nThe vision encoder first transforms input images into embeddings, which are then processed by the projection layer to match the token space of Qwen2.5-0.5B-Instruct, enabling end-to-end visual-language understanding.\n\n## Training\n\nWe developed OmniVLM through a three-stage training pipeline:\n\n**Pretraining:**\nThe initial stage focuses on establishing basic visual-linguistic alignments using image-caption pairs, during which only the projection layer parameters are unfrozen to learn these fundamental relationships.\n\n**Supervised Fine-tuning (SFT):**\nWe enhance the model''s contextual understanding using image-based question-answering datasets. This stage involves training on structured chat histories that incorporate images for the model to generate more contextually appropriate responses.\n\n**Direct Preference Optimization (DPO):**\nThe final stage implements DPO by first generating responses to images using the base model. A teacher model then produces minimally edited corrections while maintaining high semantic similarity with the original responses, focusing specifically on accuracy-critical elements. These original and corrected outputs form chosen-rejected pairs. The fine-tuning targeted at essential model output improvements without altering the model''s core response characteristics\n\n## What''s next for OmniVLM?\nOmniVLM is in early development and we are working to address current limitations:\n- Expand DPO Training: Increase the scope of DPO (Direct Preference Optimization) training in an iterative process to continually improve model performance and response quality.\n- Improve document and text understanding\n  \nIn the long term, we aim to develop OmniVLM as a fully optimized, production-ready solution for edge AI multimodal applications.\n\n### Follow us\n[Blogs](https://nexa.ai/blogs/OmniVLM) | [Discord](https://discord.gg/nexa-ai) | [X(Twitter)](https://x.com/nexa_ai)', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":9252221248,"files_count":10,"spaces_count":6,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:NexaAI:nexa-sdk","source_url":"https://github.com/NexaAI/nexa-sdk?tab=readme-ov-file#install-option-1-executable-installer"},{"type":"based_on_paper","target_id":"arxiv:2412.11475","source_url":"https://arxiv.org/abs/2412.11475"}]', NULL, 'Apache-2.0', 'approved', 82.2, 'e2ad44ccea24c4e225d59e2ceab20c6d', NULL, 'https://huggingface.co/NexaAI/OmniVLM-968M/resolve/main/benchmark.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-NexaAI-OmniVLM-968M from https://huggingface.co/NexaAI/OmniVLM-968M/resolve/main/benchmark.png
Image converted to WebP: data/images/huggingface-NexaAI-OmniVLM-968M.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-fishaudio-openaudio-s1-mini', 'huggingface--fishaudio--openaudio-s1-mini', 'openaudio-s1-mini', 'fishaudio', '', '["dual_ar","text-to-speech","zh","en","de","ja","fr","es","ko","ar","nl","ru","it","pl","pt","license:cc-by-nc-sa-4.0","region:us"]', 'text-to-speech', 526, 3637, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/fishaudio/openaudio-s1-mini","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-to-speech","library_name":null,"framework":null,"params":null,"storage_bytes":3606222702,"files_count":7,"spaces_count":9,"gated":"auto","private":false,"config":{"model_type":"dual_ar"}}', '[]', '[]', NULL, 'CC-BY-NC-SA-4.0', 'approved', 37.2, '8c47824c4b901177402ac39574c411c6', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-magenta-realtime', 'huggingface--google--magenta-realtime', 'magenta-realtime', 'google', '--- license: cc-by-4.0 library_name: magenta-realtime --- **Authors**: Google DeepMind **Resources**: - Blog Post - Paper - Colab Demo - Repository - HuggingFace Magenta RealTime is offered under a combination of licenses: the codebase is licensed under Apache 2.0, and the model weights under Creative Commons Attribution 4.0 International. In addition, we specify the following usage terms: Copyright 2025 Google LLC Use these materials responsibly and do not generate content, including outputs...', '["magenta-realtime","tf-keras","arxiv:2508.04651","arxiv:2107.03312","arxiv:2205.01917","arxiv:2208.12415","arxiv:2301.11325","license:cc-by-4.0","region:us"]', 'other', 526, 192, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/magenta-realtime","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: cc-by-4.0\nlibrary_name: magenta-realtime\n---\n\n# Model Card for Magenta RT\n\n**Authors**: Google DeepMind\n\n**Resources**:\n\n-   [Blog Post](https://g.co/magenta/rt)\n-   [Paper](https://arxiv.org/abs/2508.04651)\n-   [Colab Demo](https://colab.research.google.com/github/magenta/magenta-realtime/blob/main/notebooks/Magenta_RT_Demo.ipynb)\n-   [Repository](https://github.com/magenta/magenta-realtime)\n-   [HuggingFace](https://huggingface.co/google/magenta-realtime)\n\n## Terms of Use\n\nMagenta RealTime is offered under a combination of licenses: the codebase is\nlicensed under\n[Apache 2.0](https://github.com/magenta/magenta-realtime/blob/main/LICENSE), and\nthe model weights under\n[Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/legalcode).\nIn addition, we specify the following usage terms:\n\nCopyright 2025 Google LLC\n\nUse these materials responsibly and do not generate content, including outputs,\nthat infringe or violate the rights of others, including rights in copyrighted\ncontent.\n\nGoogle claims no rights in outputs you generate using Magenta RealTime. You and\nyour users are solely responsible for outputs and their subsequent uses.\n\nUnless required by applicable law or agreed to in writing, all software and\nmaterials distributed here under the Apache 2.0 or CC-BY licenses are\ndistributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\neither express or implied. See the licenses for the specific language governing\npermissions and limitations under those licenses. You are solely responsible for\ndetermining the appropriateness of using, reproducing, modifying, performing,\ndisplaying or distributing the software and materials, and any outputs, and\nassume any and all risks associated with your use or distribution of any of the\nsoftware and materials, and any outputs, and your exercise of rights and\npermissions under the licenses.\n\n## Model Details\n\nMagenta RealTime is an open music generation model from Google built from the\nsame research and technology used to create\n[MusicFX DJ](https://labs.google/fx/tools/music-fx-dj) and\n[Lyria RealTime](http://goo.gle/lyria-realtime). Magenta RealTime enables the\ncontinuous generation of musical audio steered by a text prompt, an audio\nexample, or a weighted combination of multiple text prompts and/or audio\nexamples. Its relatively small size makes it possible to deploy in environments\nwith limited resources, including live performance settings or freely available\nColab TPUs.\n\n### System Components\n\nMagenta RealTime is composed of three components: SpectroStream, MusicCoCa, and\nan LLM. A full technical report with more details on each component is\n[here](https://arxiv.org/abs/2508.04651).\n\n1.  **SpectroStream** is a discrete audio codec that converts stereo 48kHz audio\n    into tokens, building on the SoundStream RVQ codec from\n    [Zeghidour+ 21](https://arxiv.org/abs/2107.03312)\n1.  **MusicCoCa** is a contrastive-trained model capable of embedding audio and\n    text into a common embedding space, building on\n    [Yu+ 22](https://arxiv.org/abs/2205.01917) and\n    [Huang+ 22](https://arxiv.org/abs/2208.12415).\n1.  An **encoder-decoder Transformer LLM** generates audio tokens given context\n    audio tokens and a tokenized MusicCoCa embedding, building on the MusicLM\n    method from [Agostinelli+ 23](https://arxiv.org/abs/2301.11325)\n\n### Inputs and outputs\n\n-   **SpectroStream RVQ codec**: Tokenizes high-fidelity music audio\n    -   **Encoder input / Decoder output**: Music audio waveforms, 48kHz stereo\n    -   **Encoder output / Decoder input**: Discrete audio tokens, 25Hz frame\n        rate, 64 RVQ depth, 10 bit codes, 16kbps\n-   **MusicCoCa**: Joint embeddings of text and music audio\n    -   **Input**: Music audio waveforms, 16kHz mono, or text representation of\n        music style e.g. "heavy metal"\n    -   **Output**: 768 dimensional embedding, quantized to 12 RVQ depth, 10 bit\n        codes\n-   **Encoder-decoder Transformer LLM**: Generates audio tokens given context\n    and style\n    -   **Encoder Input**: (Context, 1000 tokens) 10s of audio context tokens w/\n        4 RVQ depth, (Style, 6 tokens) Quantized MusicCoCa style embedding\n    -   **Decoder Output**: (Generated, 800 tokens) 2s of audio w/ 16 RVQ depth\n\n## Uses\n\nMusic generation models, in particular ones targeted for continuous real-time\ngeneration and control, have a wide range of applications across various\nindustries and domains. The following list of potential uses is not\ncomprehensive. The purpose of this list is to provide contextual information\nabout the possible use-cases that the model creators considered as part of model\ntraining and development.\n\n-   **Interactive Music Creation**\n    -   Live Performance / Improvisation: These models can be used to generate\n        music in a live performance setting, controlled by performers\n        manipulating style embeddings or the audio context\n    -   Accessible Music-Making & Music Therapy: People with impediments to\n        using traditional instruments (skill gaps, disabilities, etc.) can\n        participate in communal jam sessions or solo music creation.\n    -   Video Games: Developers can create a custom soundtrack for users in\n        real-time based on their actions and environment.\n-   **Research**\n    -   Transfer learning: Researchers can leverage representations from\n        MusicCoCa and Magenta RT to recognize musical information.\n-   **Personalization**\n    -   Musicians can finetune models with their own catalog to customize the\n        model to their style (fine tuning support coming soon).\n-   **Education**\n    -   Exploring Genres, Instruments, and History: Natural language prompting\n        enables users to quickly learn about and experiment with musical\n        concepts.\n\n### Out-of-Scope Use\n\nSee our [Terms of Use](#terms-of-use) above for usage we consider out of scope.\n\n## Bias, Risks, and Limitations\n\nMagenta RT supports the real-time generation and steering of instrumental music.\nThe purpose and intention of this capability is to foster the development of new\nreal-time, interactive co-creation workflows that seamlessly integrate with\nhuman-centered forms of musical creativity.\n\nEvery AI music generation model, including Magenta RT, carries a risk of\nimpacting the economic and cultural landscape of music. We aim to mitigate these\nrisks through the following avenues:\n\n-   Prioritizing human-AI interaction as fundamental in the design of Magenta\n    RT.\n-   Distributing the model under a terms of service that prohibit developers\n    from generating outputs that infringe or violate the rights of others,\n    including rights in copyrighted content.\n-   Training on primarily instrumental data. With specific prompting, this model\n    has been observed to generate some vocal sounds and effects, though those\n    vocal sounds and effects tend to be non-lexical.\n\n### Known limitations\n\n**Coverage of broad musical styles**. Magenta RT''s training data primarily\nconsists of Western instrumental music. As a consequence, Magenta RT has\nincomplete coverage of both vocal performance and the broader landscape of rich\nmusical traditions worldwide. For real-time generation with broader style\ncoverage, we refer users to our\n[Lyria RealTime API](g.co/magenta/lyria-realtime).\n\n**Vocals**. While the model is capable of generating non-lexical vocalizations\nand humming, it is not conditioned on lyrics and is unlikely to generate actual\nwords. However, there remains some risk of generating explicit or\nculturally-insensitive lyrical content.\n\n**Latency**. Because the Magenta RT LLM operates on two second chunks, user\ninputs for the style prompt may take two or more seconds to influence the\nmusical output.\n\n**Limited context**. Because the Magenta RT encoder has a maximum audio context\nwindow of ten seconds, the model is unable to directly reference music that has\nbeen output earlier than that. While the context is sufficient to enable the\nmodel to create melodies, rhythms, and chord progressions, the model is not\ncapable of automatically creating longer-term song structures.\n\n### Benefits\n\nAt the time of release, Magenta RealTime represents the only open weights model\nsupporting real-time, continuous musical audio generation. It is designed\nspecifically to enable live, interactive musical creation, bringing new\ncapabilities to musical performances, art installations, video games, and many\nother applications.\n\n## How to Get Started with the Model\n\nSee our\n[Colab demo](https://colab.research.google.com/github/magenta/magenta-realtime/blob/main/notebooks/Magenta_RT_Demo.ipynb)\nand [GitHub repository](https://github.com/magenta/magenta-realtime) for usage\nexamples.\n\n## Training Details\n\n### Training Data\n\nMagenta RealTime was trained on ~190k hours of stock music from multiple\nsources, mostly instrumental.\n\n### Hardware\n\nMagenta RealTime was trained using\n[Tensor Processing Unit (TPU)](https://cloud.google.com/tpu/docs/intro-to-tpu)\nhardware (TPUv6e / Trillium).\n\n### Software\n\nTraining was done using [JAX](https://github.com/jax-ml/jax) and\n[T5X](https://github.com/google-research/t5x), utilizing\n[SeqIO](https://github.com/google/seqio) for data pipelines. JAX allows\nresearchers to take advantage of the latest generation of hardware, including\nTPUs, for faster and more efficient training of large models.\n\n## Evaluation\n\nModel evaluation metrics and results will be shared in our forthcoming technical\nreport.\n\n## Citation\n\nPlease cite our technical report:\n\n**BibTeX:**\n\n```\n@article{gdmlyria2025live,\n    title={Live Music Models},\n    author={Caillon, Antoine and McWilliams, Brian and Tarakajian, Cassie and Simon, Ian and Manco, Ilaria and Engel, Jesse and Constant, Noah and Li, Pen and Denk, Timo I. and Lalama, Alberto and Agostinelli, Andrea and Huang, Anna and Manilow, Ethan and Brower, George and Erdogan, Hakan and Lei, Heidi and Rolnick, Itai and Grishchenko, Ivan and Orsini, Manu and Kastelic, Matej and Zuluaga, Mauricio and Verzetti, Mauro and Dooley, Michael and Skopek, Ondrej and Ferrer, Rafael and Borsos, Zal{\''a}n and van den Oord, {\"A}aron and Eck, Douglas and Collins, Eli and Baldridge, Jason and Hume, Tom and Donahue, Chris and Han, Kehang and Roberts, Adam},\n    journal={arXiv:2508.04651},\n    year={2025}\n}\n```', '{"pipeline_tag":null,"library_name":"magenta-realtime","framework":"magenta-realtime","params":null,"storage_bytes":9983404210,"files_count":2844,"spaces_count":2,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:magenta:magenta-realtime","source_url":"https://github.com/magenta/magenta-realtime"},{"type":"has_code","target_id":"github:magenta:magenta-realtime","source_url":"https://github.com/magenta/magenta-realtime"},{"type":"has_code","target_id":"github:magenta:magenta-realtime","source_url":"https://github.com/magenta/magenta-realtime"},{"type":"has_code","target_id":"github:jax-ml:jax","source_url":"https://github.com/jax-ml/jax"},{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google:seqio","source_url":"https://github.com/google/seqio"},{"type":"based_on_paper","target_id":"arxiv:2508.04651","source_url":"https://arxiv.org/abs/2508.04651"},{"type":"based_on_paper","target_id":"arxiv:2107.03312","source_url":"https://arxiv.org/abs/2107.03312"},{"type":"based_on_paper","target_id":"arxiv:2205.01917","source_url":"https://arxiv.org/abs/2205.01917"},{"type":"based_on_paper","target_id":"arxiv:2208.12415","source_url":"https://arxiv.org/abs/2208.12415"},{"type":"based_on_paper","target_id":"arxiv:2301.11325","source_url":"https://arxiv.org/abs/2301.11325"}]', NULL, 'CC-BY-4.0', 'approved', 77.2, 'b9d5ed76bb54ff028ca51366ef80a040', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-flan-t5-xl', 'huggingface--google--flan-t5-xl', 'flan-t5-xl', 'google', '--- language: - en - fr - ro - de - multilingual widget: - text: "Translate to German: My name is Arthur" example_title: "Translation" - text: "Please answer to the following question. Who is going to be the next Ballon d''or?" example_title: "Question Answering" - text: "Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering." example_title: "Logical reasoning" - text: "Please answer the following question. What is the boiling point of Nitrogen?...', '["transformers","pytorch","tf","jax","safetensors","t5","text2text-generation","en","fr","ro","de","multilingual","dataset:svakulenk0/qrecc","dataset:taskmaster2","dataset:djaym7/wiki_dialog","dataset:deepmind/code_contests","dataset:lambada","dataset:gsm8k","dataset:aqua_rat","dataset:esnli","dataset:quasc","dataset:qed","arxiv:2210.11416","arxiv:1910.09700","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'other', 525, 201959, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/flan-t5-xl","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: \n- en\n- fr\n- ro\n- de\n- multilingual\n\nwidget:\n- text: "Translate to German:  My name is Arthur"\n  example_title: "Translation"\n- text: "Please answer to the following question. Who is going to be the next Ballon d''or?"\n  example_title: "Question Answering"\n- text: "Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering."\n  example_title: "Logical reasoning"\n- text: "Please answer the following question. What is the boiling point of Nitrogen?"\n  example_title: "Scientific knowledge"\n- text: "Answer the following yes/no question. Can you write a whole Haiku in a single tweet?"\n  example_title: "Yes/no question"\n- text: "Answer the following yes/no question by reasoning step-by-step. Can you write a whole Haiku in a single tweet?"\n  example_title: "Reasoning task"\n- text: "Q: ( False or not False or False ) is? A: Let''s think step by step"\n  example_title: "Boolean Expressions"\n- text: "The square root of x is the cube root of y. What is y to the power of 2, if x = 4?"\n  example_title: "Math reasoning"\n- text: "Premise:  At my age you will probably have learnt one lesson. Hypothesis:  It''s not certain how many lessons you''ll learn by your thirties. Does the premise entail the hypothesis?"\n  example_title: "Premise and hypothesis"\n\ntags:\n- text2text-generation\n\ndatasets:\n- svakulenk0/qrecc\n- taskmaster2\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- lambada\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed\n\n\nlicense: apache-2.0\n---\n\n# Model Card for FLAN-T5 XL\n\n<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/flan2_architecture.jpg"\nalt="drawing" width="600"/>\n\n#  Table of Contents\n\n0. [TL;DR](#TL;DR)\n1. [Model Details](#model-details)\n2. [Usage](#usage)\n3. [Uses](#uses)\n4. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n5. [Training Details](#training-details)\n6. [Evaluation](#evaluation)\n7. [Environmental Impact](#environmental-impact)\n8. [Citation](#citation)\n\n# TL;DR\n\nIf you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. \nAs mentioned in the first few lines of the abstract : \n>  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.\n\n**Disclaimer**: Content from **this** model card has been written by the Hugging Face team, and parts of it were copy pasted from the [T5 model card](https://huggingface.co/t5-large).\n\n# Model Details\n\n## Model Description\n\n\n- **Model type:** Language model\n- **Language(s) (NLP):** English, Spanish, Japanese, Persian, Hindi, French, Chinese, Bengali, Gujarati, German, Telugu, Italian, Arabic, Polish, Tamil, Marathi, Malayalam, Oriya, Panjabi, Portuguese, Urdu, Galician, Hebrew, Korean, Catalan, Thai, Dutch, Indonesian, Vietnamese, Bulgarian, Filipino, Central Khmer, Lao, Turkish, Russian, Croatian, Swedish, Yoruba, Kurdish, Burmese, Malay, Czech, Finnish, Somali, Tagalog, Swahili, Sinhala, Kannada, Zhuang, Igbo, Xhosa, Romanian, Haitian, Estonian, Slovak, Lithuanian, Greek, Nepali, Assamese, Norwegian\n- **License:** Apache 2.0\n- **Related Models:** [All FLAN-T5 Checkpoints](https://huggingface.co/models?search=flan-t5)\n- **Original Checkpoints:** [All Original FLAN-T5 Checkpoints](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)\n- **Resources for more information:**\n  - [Research paper](https://arxiv.org/pdf/2210.11416.pdf)\n  - [GitHub Repo](https://github.com/google-research/t5x)\n  - [Hugging Face FLAN-T5 Docs (Similar to T5) ](https://huggingface.co/docs/transformers/model_doc/t5)\n\n# Usage\n\nFind below some example scripts on how to use the model in `transformers`:\n\n## Using the Pytorch model\n\n### Running the model on a CPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-xl")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-xl")\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-xl")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-xl", device_map="auto")\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU using different precisions\n\n#### FP16\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-xl")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-xl", device_map="auto", torch_dtype=torch.float16)\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n#### INT8\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-xl")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-xl", device_map="auto", load_in_8bit=True)\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n# Uses\n\n## Direct Use and Downstream Use\n\nThe authors write in [the original paper''s model card](https://arxiv.org/pdf/2210.11416.pdf) that: \n\n> The primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models\n\nSee the [research paper](https://arxiv.org/pdf/2210.11416.pdf) for further details.\n\n## Out-of-Scope Use\n\nMore information needed.\n\n# Bias, Risks, and Limitations\n\nThe information below in this section are copied from the model''s [official model card](https://arxiv.org/pdf/2210.11416.pdf):\n\n> Language models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.\n\n## Ethical considerations and risks\n\n> Flan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\n\n## Known Limitations\n\n> Flan-T5 has not been tested in real world applications.\n\n## Sensitive Use:\n\n> Flan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech.\n\n# Training Details\n\n## Training Data\n\nThe model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2):\n\n![table.png](https://s3.amazonaws.com/moonup/production/uploads/1666363265279-62441d1d9fdefb55a0b7d12c.png)\n\n\n## Training Procedure\n\nAccording to the model card from the [original paper](https://arxiv.org/pdf/2210.11416.pdf):\n\n> These models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size.\n\nThe model has been trained on TPU v3 or TPU v4 pods, using [`t5x`](https://github.com/google-research/t5x) codebase together with [`jax`](https://github.com/google/jax).\n\n\n# Evaluation\n\n## Testing Data, Factors & Metrics\n\nThe authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation:\n![image.png](https://s3.amazonaws.com/moonup/production/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png)\nFor full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf).\n\n## Results \n\nFor full results for FLAN-T5-XL, see the [research paper](https://arxiv.org/pdf/2210.11416.pdf), Table 3.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** Google Cloud TPU Pods - TPU v3 or TPU v4  | Number of chips â‰¥ 4.\n- **Hours used:** More information needed\n- **Cloud Provider:** GCP\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@misc{https://doi.org/10.48550/arxiv.2210.11416,\n  doi = {10.48550/ARXIV.2210.11416},\n  \n  url = {https://arxiv.org/abs/2210.11416},\n  \n  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n  \n  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {Scaling Instruction-Finetuned Language Models},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```\n\n', '{"pipeline_tag":null,"library_name":"transformers","framework":"transformers","params":2849757184,"storage_bytes":45598305491,"files_count":20,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["T5ForConditionalGeneration"],"model_type":"t5","tokenizer_config":{"eos_token":"</s>","pad_token":"<pad>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google:jax","source_url":"https://github.com/google/jax"},{"type":"based_on_paper","target_id":"arxiv:2210.11416","source_url":"https://arxiv.org/abs/2210.11416"},{"type":"based_on_paper","target_id":"arxiv:1910.09700","source_url":"https://arxiv.org/abs/1910.09700"}]', NULL, 'Apache-2.0', 'approved', 77.2, '4b5e3f27d0d6619c5de236fc6d4f0624', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Mistral-Small-3.2-24B-Instruct-2506', 'huggingface--mistralai--mistral-small-3.2-24b-instruct-2506', 'Mistral-Small-3.2-24B-Instruct-2506', 'mistralai', '--- library_name: vllm language: - en - fr - de - es - pt - it - ja - ko - ru - zh - ar - fa - id - ms - ne - pl - ro - sr - sv - tr - uk - vi - hi - bn license: apache-2.0 inference: false base_model: - mistralai/Mistral-Small-3.1-24B-Base-2503 extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href="https://mistral.ai/terms/">Privacy Policy</a>. tags: - mistral-common --- Mistral-Small-3.2-24B-Instruct-2506 is a minor update of ...', '["vllm","safetensors","mistral3","mistral-common","en","fr","de","es","pt","it","ja","ko","ru","zh","ar","fa","id","ms","ne","pl","ro","sr","sv","tr","uk","vi","hi","bn","license:apache-2.0","region:us"]', 'other', 524, 140515, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: vllm\nlanguage:\n- en\n- fr\n- de\n- es\n- pt\n- it\n- ja\n- ko\n- ru\n- zh\n- ar\n- fa\n- id\n- ms\n- ne\n- pl\n- ro\n- sr\n- sv\n- tr\n- uk\n- vi\n- hi\n- bn\nlicense: apache-2.0\ninference: false\nbase_model:\n- mistralai/Mistral-Small-3.1-24B-Base-2503\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href="https://mistral.ai/terms/">Privacy Policy</a>.\ntags:\n- mistral-common\n---\n\n# Mistral-Small-3.2-24B-Instruct-2506\n\nMistral-Small-3.2-24B-Instruct-2506 is a minor update of [Mistral-Small-3.1-24B-Instruct-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503).\n\nSmall-3.2 improves in the following categories:\n- **Instruction following**: Small-3.2 is better at following precise instructions\n- **Repetition errors**: Small-3.2 produces less infinite generations or repetitive answers\n- **Function calling**: Small-3.2''s function calling template is more robust (see [here](https://github.com/mistralai/mistral-common/blob/535b4d0a0fc94674ea17db6cf8dc2079b81cbcfa/src/mistral_common/tokens/tokenizers/instruct.py#L778) and [examples](#function-calling))\n\nIn all other categories Small-3.2 should match or slightly improve compared to [Mistral-Small-3.1-24B-Instruct-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503).\n\n## Key Features\n- same as [Mistral-Small-3.1-24B-Instruct-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503#key-features)\n\n## Benchmark Results\n\nWe compare Mistral-Small-3.2-24B to [Mistral-Small-3.1-24B-Instruct-2503](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503).\nFor more comparison against other models of similar size, please check [Mistral-Small-3.1''s Benchmarks''](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503#benchmark-results)\n\n### Text \n\n#### Instruction Following / Chat / Tone\n\n| Model | Wildbench v2 | Arena Hard v2 | IF (Internal; accuracy) |\n|-------|---------------|---------------|------------------------|\n| Small 3.1 24B Instruct | 55.6% | 19.56% | 82.75% |\n| **Small 3.2 24B Instruct** | **65.33%** | **43.1%** | **84.78%** |\n\n#### Infinite Generations\n\nSmall 3.2 reduces infinite generations by 2x on challenging, long and repetitive prompts.\n\n| Model | Infinite Generations (Internal; Lower is better) |\n|-------|-------|\n| Small 3.1 24B Instruct | 2.11% |\n| **Small 3.2 24B Instruct** | **1.29%** |\n\n#### STEM\n\n| Model                          | MMLU      | MMLU Pro (5-shot CoT) | MATH                   | GPQA Main (5-shot CoT) | GPQA Diamond (5-shot CoT )| MBPP Plus - Pass@5 | HumanEval Plus - Pass@5 | SimpleQA (TotalAcc)|\n|--------------------------------|-----------|-----------------------|------------------------|------------------------|---------------------------|--------------------|-------------------------|--------------------|\n| Small 3.1 24B Instruct         | 80.62%    | 66.76%                | 69.30%                 | 44.42%                 | 45.96%                    | 74.63%             | 88.99%                  | 10.43%             |\n| **Small 3.2 24B Instruct**     | 80.50%    | **69.06%**            | 69.42%                 | 44.22%                 | 46.13%                    | **78.33%**         | **92.90%**              | **12.10%**         |\n\n### Vision\n\n| Model                          | MMMU       | Mathvista | ChartQA   | DocVQA    | AI2D      |\n|--------------------------------|------------|-----------|-----------|-----------|-----------|\n| Small 3.1 24B Instruct         | **64.00%** | **68.91%**| 86.24%    | 94.08%    | 93.72%  | \n| **Small 3.2 24B Instruct**     | 62.50%     | 67.09%    | **87.4%** | 94.86%    | 92.91%  | \n\n\n## Usage\n\nThe model can be used with the following frameworks;\n- [`vllm (recommended)`](https://github.com/vllm-project/vllm): See [here](#vllm-recommended)\n- [`transformers`](https://github.com/huggingface/transformers): See [here](#transformers)\n\n**Note 1**: We recommend using a relatively low temperature, such as `temperature=0.15`.\n\n**Note 2**: Make sure to add a system prompt to the model to best tailor it to your needs. If you want to use the model as a general assistant, we recommend to use the one provided in the [SYSTEM_PROMPT.txt](https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506/blob/main/SYSTEM_PROMPT.txt) file.\n\n### vLLM (recommended)\n\nWe recommend using this model with [vLLM](https://github.com/vllm-project/vllm).\n\n#### Installation\n\nMake sure to install [`vLLM >= 0.9.1`](https://github.com/vllm-project/vllm/releases/tag/v0.9.1):\n\n```\npip install vllm --upgrade\n```\n\nDoing so should automatically install [`mistral_common >= 1.6.2`](https://github.com/mistralai/mistral-common/releases/tag/v1.6.2).\n\nTo check:\n```\npython -c "import mistral_common; print(mistral_common.__version__)"\n```\n\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/Dockerfile) or on the [docker hub](https://hub.docker.com/layers/vllm/vllm-openai/latest/images/sha256-de9032a92ffea7b5c007dad80b38fd44aac11eddc31c435f8e52f3b7404bbf39).\n\n#### Serve\n\nWe recommend that you use Mistral-Small-3.2-24B-Instruct-2506 in a server/client setting. \n\n1. Spin up a server:\n\n```\nvllm serve mistralai/Mistral-Small-3.2-24B-Instruct-2506 \\n  --tokenizer_mode mistral --config_format mistral \\n  --load_format mistral --tool-call-parser mistral \\n  --enable-auto-tool-choice --limit-mm-per-prompt ''{"image":10}'' \\n  --tensor-parallel-size 2\n```\n\n**Note:** Running Mistral-Small-3.2-24B-Instruct-2506 on GPU requires ~55 GB of GPU RAM in bf16 or fp16. \n\n\n2. To ping the client you can use a simple Python snippet. See the following examples.\n\n\n#### Vision reasoning\n\nLeverage the vision capabilities of Mistral-Small-3.2-24B-Instruct-2506 to make the best choice given a scenario, go catch them all !\n\n<details>\n  <summary>Python snippet</summary>\n\n```py\nfrom datetime import datetime, timedelta\n\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n\n# Modify OpenAI''s API key and API base to use vLLM''s API server.\nopenai_api_key = "EMPTY"\nopenai_api_base = "http://localhost:8000/v1"\n\nTEMP = 0.15\nMAX_TOK = 131072\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, "r") as file:\n        system_prompt = file.read()\n    today = datetime.today().strftime("%Y-%m-%d")\n    yesterday = (datetime.today() - timedelta(days=1)).strftime("%Y-%m-%d")\n    model_name = repo_id.split("/")[-1]\n    return system_prompt.format(name=model_name, today=today, yesterday=yesterday)\n\n\nmodel_id = "mistralai/Mistral-Small-3.2-24B-Instruct-2506"\nSYSTEM_PROMPT = load_system_prompt(model_id, "SYSTEM_PROMPT.txt")\nimage_url = "https://static.wikia.nocookie.net/essentialsdocs/images/7/70/Battle.png/revision/latest?cb=20220523172438"\n\nmessages = [\n    {"role": "system", "content": SYSTEM_PROMPT},\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "text",\n                "text": "What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.",\n            },\n            {"type": "image_url", "image_url": {"url": image_url}},\n        ],\n    },\n]\n\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n)\n\nprint(response.choices[0].message.content)\n# In this situation, you are playing a PokÃ©mon game where your Pikachu (Level 42) is facing a wild Pidgey (Level 17). Here are the possible actions you can take and an analysis of each:\n\n# 1. **FIGHT**:\n#    - **Pros**: Pikachu is significantly higher level than the wild Pidgey, which suggests that it should be able to defeat Pidgey easily. This could be a good opportunity to gain experience points and possibly items or money.\n#    - **Cons**: There is always a small risk of Pikachu fainting, especially if Pidgey has a powerful move or a status effect that could hinder Pikachu. However, given the large level difference, this risk is minimal.\n\n# 2. **BAG**:\n#    - **Pros**: You might have items in your bag that could help in this battle, such as Potions, PokÃ© Balls, or Berries. Using an item could help you capture the Pidgey or heal your Pikachu if needed.\n#    - **Cons**: Using items might not be necessary given the level difference. It could be more efficient to just fight and defeat the Pidgey quickly.\n\n# 3. **POKÃ‰MON**:\n#    - **Pros**: You might have another PokÃ©mon in your party that is better suited for this battle or that you want to gain experience. Switching PokÃ©mon could also be a strategic move if you want to train a lower-level PokÃ©mon.\n#    - **Cons**: Switching PokÃ©mon might not be necessary since Pikachu is at a significant advantage. It could also waste time and potentially give Pidgey a turn to attack.\n\n# 4. **RUN**:\n#    - **Pros**: Running away could save time and conserve your PokÃ©mon''s health and resources. If you are in a hurry or do not need the experience or items, running away is a safe option.\n#    - **Cons**: Running away means you miss out on the experience points and potential items or money that you could gain from defeating the Pidgey. It also means you do not get the chance to capture the Pidgey if you wanted to.\n\n# ### Recommendation:\n# Given the significant level advantage, the best action is likely to **FIGHT**. This will allow you to quickly defeat the Pidgey, gain experience points, and potentially earn items or money. If you are concerned about Pikachu''s health, you could use an item from your **BAG** to heal it before or during the battle. Running away or switching PokÃ©mon does not seem necessary in this situation.\n```\n</details>\n\n#### Function calling\n\nMistral-Small-3.2-24B-Instruct-2506 is excellent at function / tool calling tasks via vLLM. *E.g.:*\n\n<details>\n  <summary>Python snippet - easy</summary>\n\n```py\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n\n# Modify OpenAI''s API key and API base to use vLLM''s API server.\nopenai_api_key = "EMPTY"\nopenai_api_base = "http://localhost:8000/v1"\n\nTEMP = 0.15\nMAX_TOK = 131072\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, "r") as file:\n        system_prompt = file.read()\n    return system_prompt\n\nmodel_id = "mistralai/Mistral-Small-3.2-24B-Instruct-2506"\nSYSTEM_PROMPT = load_system_prompt(model_id, "SYSTEM_PROMPT.txt")\n\nimage_url = "https://huggingface.co/datasets/patrickvonplaten/random_img/resolve/main/europe.png"\n\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_current_population",\n            "description": "Get the up-to-date population of a given country.",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "country": {\n                        "type": "string",\n                        "description": "The country to find the population of.",\n                    },\n                    "unit": {\n                        "type": "string",\n                        "description": "The unit for the population.",\n                        "enum": ["millions", "thousands"],\n                    },\n                },\n                "required": ["country", "unit"],\n            },\n        },\n    },\n    {\n        "type": "function",\n        "function": {\n            "name": "rewrite",\n            "description": "Rewrite a given text for improved clarity",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "text": {\n                        "type": "string",\n                        "description": "The input text to rewrite",\n                    }\n                },\n            },\n        },\n    },\n]\n\nmessages = [\n    {"role": "system", "content": SYSTEM_PROMPT},\n    {\n        "role": "user",\n        "content": "Could you please make the below article more concise?\n\nOpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.",\n    },\n    {\n        "role": "assistant",\n        "content": "",\n        "tool_calls": [\n            {\n                "id": "bbc5b7ede",\n                "type": "function",\n                "function": {\n                    "name": "rewrite",\n                    "arguments": ''{"text": "OpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership."}'',\n                },\n            }\n        ],\n    },\n    {\n        "role": "tool",\n        "content": ''{"action":"rewrite","outcome":"OpenAI is a FOR-profit company."}'',\n        "tool_call_id": "bbc5b7ede",\n        "name": "rewrite",\n    },\n    {\n        "role": "assistant",\n        "content": "---\n\nOpenAI is a FOR-profit company.",\n    },\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "text",\n                "text": "Can you tell me what is the biggest country depicted on the map?",\n            },\n            {\n                "type": "image_url",\n                "image_url": {\n                    "url": image_url,\n                },\n            },\n        ],\n    }\n]\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n    tools=tools,\n    tool_choice="auto",\n)\n\nassistant_message = response.choices[0].message.content\nprint(assistant_message)\n# The biggest country depicted on the map is Russia.\n\nmessages.extend([\n    {"role": "assistant", "content": assistant_message},\n    {"role": "user", "content": "What is the population of that country in millions?"},\n])\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n    tools=tools,\n    tool_choice="auto",\n)\n\nprint(response.choices[0].message.tool_calls)\n# [ChatCompletionMessageToolCall(id=''3e92V6Vfo'', function=Function(arguments=''{"country": "Russia", "unit": "millions"}'', name=''get_current_population''), type=''function'')]\n```\n\n</details>\n\n<details>\n  <summary>Python snippet - complex</summary>\n\n```python\nimport json\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n\n# Modify OpenAI''s API key and API base to use vLLM''s API server.\nopenai_api_key = "EMPTY"\nopenai_api_base = "http://localhost:8000/v1"\n\nTEMP = 0.15\nMAX_TOK = 131072\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, "r") as file:\n        system_prompt = file.read()\n    return system_prompt\n\n\nmodel_id = "mistralai/Mistral-Small-3.2-24B-Instruct-2506"\nSYSTEM_PROMPT = load_system_prompt(model_id, "SYSTEM_PROMPT.txt")\n\nimage_url = "https://math-coaching.com/img/fiche/46/expressions-mathematiques.jpg"\n\n\ndef my_calculator(expression: str) -> str:\n    return str(eval(expression))\n\n\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "my_calculator",\n            "description": "A calculator that can evaluate a mathematical expression.",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "expression": {\n                        "type": "string",\n                        "description": "The mathematical expression to evaluate.",\n                    },\n                },\n                "required": ["expression"],\n            },\n        },\n    },\n    {\n        "type": "function",\n        "function": {\n            "name": "rewrite",\n            "description": "Rewrite a given text for improved clarity",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "text": {\n                        "type": "string",\n                        "description": "The input text to rewrite",\n                    }\n                },\n            },\n        },\n    },\n]\n\nmessages = [\n    {"role": "system", "content": SYSTEM_PROMPT},\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "text",\n                "text": "Can you calculate the results for all the equations displayed in the image? Only compute the ones that involve numbers.",\n            },\n            {\n                "type": "image_url",\n                "image_url": {\n                    "url": image_url,\n                },\n            },\n        ],\n    },\n]\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n    tools=tools,\n    tool_choice="auto",\n)\n\ntool_calls = response.choices[0].message.tool_calls\nprint(tool_calls)\n# [ChatCompletionMessageToolCall(id=''CyQBSAtGh'', function=Function(arguments=''{"expression": "6 + 2 * 3"}'', name=''my_calculator''), type=''function''), ChatCompletionMessageToolCall(id=''KQqRCqvzc'', function=Function(arguments=''{"expression": "19 - (8 + 2) + 1"}'', name=''my_calculator''), type=''function'')]\n\nresults = []\nfor tool_call in tool_calls:\n    function_name = tool_call.function.name\n    function_args = tool_call.function.arguments\n    if function_name == "my_calculator":\n        result = my_calculator(**json.loads(function_args))\n        results.append(result)\n\nmessages.append({"role": "assistant", "tool_calls": tool_calls})\nfor tool_call, result in zip(tool_calls, results):\n    messages.append(\n        {\n            "role": "tool",\n            "tool_call_id": tool_call.id,\n            "name": tool_call.function.name,\n            "content": result,\n        }\n    )\n\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n)\n\nprint(response.choices[0].message.content)\n# Here are the results for the equations that involve numbers:\n\n# 1. \( 6 + 2 \times 3 = 12 \)\n# 3. \( 19 - (8 + 2) + 1 = 10 \)\n\n# For the other equations, you need to substitute the variables with specific values to compute the results.\n```\n\n</details>\n\n#### Instruction following\n\nMistral-Small-3.2-24B-Instruct-2506 will follow your instructions down to the last letter ! \n\n<details>\n  <summary>Python snippet</summary>\n\n```python\nfrom openai import OpenAI\nfrom huggingface_hub import hf_hub_download\n\n# Modify OpenAI''s API key and API base to use vLLM''s API server.\nopenai_api_key = "EMPTY"\nopenai_api_base = "http://localhost:8000/v1"\n\nTEMP = 0.15\nMAX_TOK = 131072\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nmodels = client.models.list()\nmodel = models.data[0].id\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, "r") as file:\n        system_prompt = file.read()\n    return system_prompt\n\n\nmodel_id = "mistralai/Mistral-Small-3.2-24B-Instruct-2506"\nSYSTEM_PROMPT = load_system_prompt(model_id, "SYSTEM_PROMPT.txt")\n\nmessages = [\n    {"role": "system", "content": SYSTEM_PROMPT},\n    {\n        "role": "user",\n        "content": "Write me a sentence where every word starts with the next letter in the alphabet - start with ''a'' and end with ''z''.",\n    },\n]\n\nresponse = client.chat.completions.create(\n    model=model,\n    messages=messages,\n    temperature=TEMP,\n    max_tokens=MAX_TOK,\n)\n\nassistant_message = response.choices[0].message.content\nprint(assistant_message)\n\n# Here''s a sentence where each word starts with the next letter of the alphabet, starting from ''a'' and ending with ''z'':\n\n# "Always brave cats dance elegantly, fluffy giraffes happily ignore jungle kites, lovingly munching nuts, observing playful quails racing swiftly, tiny unicorns vaulting while xylophones yodel zealously."\n\n# This sentence follows the sequence from A to Z without skipping any letters.\n```\n</details>\n\n### Transformers\n\nYou can also use Mistral-Small-3.2-24B-Instruct-2506 with `Transformers` !\n\nTo make the best use of our model with `Transformers` make sure to have [installed](https://github.com/mistralai/mistral-common) `mistral-common >= 1.6.2` to use our tokenizer.\n\n```bash\npip install mistral-common --upgrade\n```\n\nThen load our tokenizer along with the model and generate:\n\n<details>\n  <summary>Python snippet</summary>\n\n```python\nfrom datetime import datetime, timedelta\nimport torch\n\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom huggingface_hub import hf_hub_download\nfrom transformers import Mistral3ForConditionalGeneration\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, "r") as file:\n        system_prompt = file.read()\n    today = datetime.today().strftime("%Y-%m-%d")\n    yesterday = (datetime.today() - timedelta(days=1)).strftime("%Y-%m-%d")\n    model_name = repo_id.split("/")[-1]\n    return system_prompt.format(name=model_name, today=today, yesterday=yesterday)\n\n\nmodel_id = "mistralai/Mistral-Small-3.2-24B-Instruct-2506"\nSYSTEM_PROMPT = load_system_prompt(model_id, "SYSTEM_PROMPT.txt")\n\ntokenizer = MistralTokenizer.from_hf_hub(model_id)\n\nmodel = Mistral3ForConditionalGeneration.from_pretrained(\n    model_id, torch_dtype=torch.bfloat16\n)\n\nimage_url = "https://static.wikia.nocookie.net/essentialsdocs/images/7/70/Battle.png/revision/latest?cb=20220523172438"\n\nmessages = [\n    {"role": "system", "content": SYSTEM_PROMPT},\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "text",\n                "text": "What action do you think I should take in this situation? List all the possible actions and explain why you think they are good or bad.",\n            },\n            {"type": "image_url", "image_url": {"url": image_url}},\n        ],\n    },\n]\n\ntokenized = tokenizer.encode_chat_completion(ChatCompletionRequest(messages=messages))\n\ninput_ids = torch.tensor([tokenized.tokens])\nattention_mask = torch.ones_like(input_ids)\npixel_values = torch.tensor(tokenized.images[0], dtype=torch.bfloat16).unsqueeze(0)\nimage_sizes = torch.tensor([pixel_values.shape[-2:]])\n\noutput = model.generate(\n    input_ids=input_ids,\n    attention_mask=attention_mask,\n    pixel_values=pixel_values,\n    image_sizes=image_sizes,\n    max_new_tokens=1000,\n)[0]\n\ndecoded_output = tokenizer.decode(output[len(tokenized.tokens) :])\nprint(decoded_output)\n# In this situation, you are playing a PokÃ©mon game where your Pikachu (Level 42) is facing a wild Pidgey (Level 17). Here are the possible actions you can take and an analysis of each:\n\n# 1. **FIGHT**:\n#    - **Pros**: Pikachu is significantly higher level than the wild Pidgey, which suggests that it should be able to defeat Pidgey easily. This could be a good opportunity to gain experience points and possibly items or money.\n#    - **Cons**: There is always a small risk of Pikachu fainting, especially if Pidgey has a powerful move or a status effect that could hinder Pikachu. However, given the large level difference, this risk is minimal.\n\n# 2. **BAG**:\n#    - **Pros**: You might have items in your bag that could help in this battle, such as Potions, PokÃ© Balls, or Berries. Using an item could help you capture Pidgey or heal Pikachu if needed.\n#    - **Cons**: Using items might not be necessary given the level difference. It could be more efficient to just fight and defeat Pidgey quickly.\n\n# 3. **POKÃ‰MON**:\n#    - **Pros**: You might have another PokÃ©mon in your party that is better suited for this battle or that you want to gain experience. Switching PokÃ©mon could also be strategic if you want to train a lower-level PokÃ©mon.\n#    - **Cons**: Switching PokÃ©mon might not be necessary since Pikachu is at a significant advantage. It could also waste time and potentially give Pidgey a turn to attack.\n\n# 4. **RUN**:\n#    - **Pros**: Running away could be a quick way to avoid the battle altogether. This might be useful if you are trying to conserve resources or if you are in a hurry to get to another location.\n#    - **Cons**: Running away means you miss out on the experience points, items, or money that you could gain from defeating Pidgey. It also might not be the most efficient use of your time if you are trying to train your PokÃ©mon.\n\n# ### Recommendation:\n# Given the significant level advantage, the best action to take is likely **FIGHT**. This will allow you to quickly defeat Pidgey and gain experience points for Pikachu. If you are concerned about Pikachu''s health, you could use the **BAG** to heal Pikachu before or during the battle. Running away or switching PokÃ©mon does not seem necessary in this situation.\n```\n\n</details>', '{"pipeline_tag":null,"library_name":"vllm","framework":"vllm","params":24011361280,"storage_bytes":96082070772,"files_count":19,"spaces_count":12,"gated":false,"private":false,"config":{"architectures":["Mistral3ForConditionalGeneration"],"model_type":"mistral3"}}', '[]', '[{"type":"has_code","target_id":"github:mistralai:mistral-common","source_url":"https://github.com/mistralai/mistral-common"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:mistralai:mistral-common","source_url":"https://github.com/mistralai/mistral-common"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:mistralai:mistral-common","source_url":"https://github.com/mistralai/mistral-common"}]', NULL, 'Apache-2.0', 'approved', 77.2, '111cfdf752d2818efceacef03d4683a2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Wan-AI-Wan2.2-I2V-A14B', 'huggingface--wan-ai--wan2.2-i2v-a14b', 'Wan2.2-I2V-A14B', 'Wan-AI', '--- license: apache-2.0 language: - en - zh pipeline_tag: image-to-video library_name: wan2.2 --- <p align="center"> <img src="assets/logo.png" width="400"/> <p> <p align="center"> ðŸ’œ <a href="https://wan.video"><b>Wan</b></a> &nbsp&nbsp ï½œ &nbsp&nbsp ðŸ–¥ï¸ <a href="https://github.com/Wan-Video/Wan2.2">GitHub</a> &nbsp&nbsp | &nbsp&nbspðŸ¤— <a href="https://huggingface.co/Wan-AI/">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspðŸ¤– <a href="https://modelscope.cn/organization/Wan-AI">ModelScope</a>&nbsp&nbsp...', '["wan2.2","diffusers","safetensors","image-to-video","en","zh","arxiv:2503.20314","license:apache-2.0","region:us"]', 'image-to-video', 524, 7161, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\n- zh\npipeline_tag: image-to-video\nlibrary_name: wan2.2\n---\n# Wan2.2\n\n<p align="center">\n    <img src="assets/logo.png" width="400"/>\n<p>\n\n<p align="center">\n    ðŸ’œ <a href="https://wan.video"><b>Wan</b></a> &nbsp&nbsp ï½œ &nbsp&nbsp ðŸ–¥ï¸ <a href="https://github.com/Wan-Video/Wan2.2">GitHub</a> &nbsp&nbsp  | &nbsp&nbspðŸ¤— <a href="https://huggingface.co/Wan-AI/">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspðŸ¤– <a href="https://modelscope.cn/organization/Wan-AI">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ðŸ“‘ <a href="https://arxiv.org/abs/2503.20314">Technical Report</a> &nbsp&nbsp | &nbsp&nbsp ðŸ“‘ <a href="https://wan.video/welcome?spm=a2ty_o02.30011076.0.0.6c9ee41eCcluqg">Blog</a> &nbsp&nbsp | &nbsp&nbspðŸ’¬ <a href="https://gw.alicdn.com/imgextra/i2/O1CN01tqjWFi1ByuyehkTSB_!!6000000000015-0-tps-611-1279.jpg">WeChat Group</a>&nbsp&nbsp | &nbsp&nbsp ðŸ“– <a href="https://discord.gg/AKNgpMK4Yj">Discord</a>&nbsp&nbsp\n<br>\n\n-----\n\n[**Wan: Open and Advanced Large-Scale Video Generative Models**](https://arxiv.org/abs/2503.20314) <be>\n\n\nWe are excited to introduce **Wan2.2**, a major upgrade to our foundational video models. With **Wan2.2**, we have focused on incorporating the following innovations:\n\n- ðŸ‘ **Effective MoE Architecture**: Wan2.2 introduces a Mixture-of-Experts (MoE) architecture into video diffusion models. By separating the denoising process cross timesteps with specialized powerful expert models, this enlarges the overall model capacity while maintaining the same computational cost.\n\n- ðŸ‘ **Cinematic-level Aesthetics**: Wan2.2 incorporates meticulously curated aesthetic data, complete with detailed labels for lighting, composition, contrast, color tone, and more. This allows for more precise and controllable cinematic style generation, facilitating the creation of videos with customizable aesthetic preferences.\n\n- ðŸ‘ **Complex Motion Generation**: Compared to Wan2.1, Wan2.2 is trained on a significantly larger data, with +65.6% more images and +83.2% more videos. This expansion notably enhances the model''s generalization across multiple dimensions such as motions,  semantics, and aesthetics, achieving TOP performance among all open-sourced and closed-sourced models. \n\n- ðŸ‘ **Efficient High-Definition Hybrid TI2V**:  Wan2.2 open-sources a 5B model built with our advanced Wan2.2-VAE that achieves a compression ratio of **16Ã—16Ã—4**. This model supports both text-to-video and image-to-video generation at 720P resolution with 24fps and can also run on consumer-grade graphics cards like 4090. It is one of the fastest **720P@24fps** models currently available, capable of serving both the industrial and academic sectors simultaneously.\n\n\nThis repository also includes our I2V-A14B model, designed for image-to-video generation, supporting both 480P and 720P resolutions. Built with a Mixture-of-Experts (MoE) architecture, it achieves more stable video synthesis with reduced unrealistic camera movements and offers enhanced support for diverse stylized scenes.\n\n\n\n## Video Demos\n\n<div align="center">\n    <video width="80%" controls>\n        <source src="https://cloud.video.taobao.com/vod/NnCd0fC-1eckDUuVBMz43oD_U6mTsPpBwga3wdnAkXA.mp4" type="video/mp4">\n        Your browser does not support the video tag.\n    </video>\n</div>\n\n\n## ðŸ”¥ Latest News!!\n\n* Jul 28, 2025: ðŸ‘‹ Wan2.1 has been integrated into ComfyUI ([CN](https://docs.comfy.org/zh-CN/tutorials/video/wan/wan2_2) | [EN](https://docs.comfy.org/tutorials/video/wan/wan2_2)). Enjoy!\n* Jul 28, 2025: ðŸ‘‹ Wan2.2''s T2V, I2V and TI2V have been integrated into Diffusers ([T2V-A14B](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B-Diffusers) | [I2V-A14B](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B-Diffusers) | [TI2V-5B](https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B-Diffusers)). Feel free to give it a try!\n* Jul 28, 2025: ðŸ‘‹ We''ve released the inference code and model weights of **Wan2.2**.\n\n\n## Community Works\nIf your research or project builds upon [**Wan2.1**](https://github.com/Wan-Video/Wan2.1) or Wan2.2, we welcome you to share it with us so we can highlight it for the broader community.\n\n\n## ðŸ“‘ Todo List\n- Wan2.2 Text-to-Video\n    - [x] Multi-GPU Inference code of the A14B and 14B models\n    - [x] Checkpoints of the A14B and 14B models\n    - [x] ComfyUI integration\n    - [x] Diffusers integration\n- Wan2.2 Image-to-Video\n    - [x] Multi-GPU Inference code of the A14B model\n    - [x] Checkpoints of the A14B model\n    - [x] ComfyUI integration\n    - [x] Diffusers integration\n- Wan2.2 Text-Image-to-Video\n    - [x] Multi-GPU Inference code of the 5B model\n    - [x] Checkpoints of the 5B model\n    - [x] ComfyUI integration\n    - [x] Diffusers integration\n\n## Run Wan2.2\n\n#### Installation\nClone the repo:\n```sh\ngit clone https://github.com/Wan-Video/Wan2.2.git\ncd Wan2.2\n```\n\nInstall dependencies:\n```sh\n# Ensure torch >= 2.4.0\n# If the installation of `flash_attn` fails, try installing the other packages first and install `flash_attn` last\npip install -r requirements.txt\n```\n\n\n#### Model Download\n\n\n\n| Models              | Download Links                                                                                                                              | Description |\n|--------------------|---------------------------------------------------------------------------------------------------------------------------------------------|-------------|\n| T2V-A14B    | ðŸ¤— [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B)    ðŸ¤– [ModelScope](https://modelscope.cn/models/Wan-AI/Wan2.2-T2V-A14B)    | Text-to-Video MoE model, supports 480P & 720P |\n| I2V-A14B    | ðŸ¤— [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B)    ðŸ¤– [ModelScope](https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B)    | Image-to-Video MoE model, supports 480P & 720P |\n| TI2V-5B     | ðŸ¤— [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B)     ðŸ¤– [ModelScope](https://modelscope.cn/models/Wan-AI/Wan2.2-TI2V-5B)     | High-compression VAE, T2V+I2V, supports 720P |\n\n\n> ðŸ’¡Note: \n> The TI2V-5B model supports 720P video generation at **24 FPS**.\n\n\nDownload models using huggingface-cli:\n``` sh\npip install "huggingface_hub[cli]"\nhuggingface-cli download Wan-AI/Wan2.2-I2V-A14B --local-dir ./Wan2.2-I2V-A14B\n```\n\nDownload models using modelscope-cli:\n``` sh\npip install modelscope\nmodelscope download Wan-AI/Wan2.2-I2V-A14B --local_dir ./Wan2.2-I2V-A14B\n```\n\n#### Run Image-to-Video Generation\n\nThis repository supports the `Wan2.2-I2V-A14B`` Image-to-Video model and can simultaneously support video generation at 480P and 720P resolutions.\n\n\n- Single-GPU inference\n```sh\npython generate.py --task i2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-I2V-A14B --offload_model True --convert_model_dtype --image examples/i2v_input.JPG --prompt "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline''s intricate details and the refreshing atmosphere of the seaside."\n```\n\n> This command can run on a GPU with at least 80GB VRAM.\n\n> ðŸ’¡For the Image-to-Video task, the `size` parameter represents the area of the generated video, with the aspect ratio following that of the original input image.\n\n\n- Multi-GPU inference using FSDP + DeepSpeed Ulysses\n\n```sh\ntorchrun --nproc_per_node=8 generate.py --task i2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-I2V-A14B --image examples/i2v_input.JPG --dit_fsdp --t5_fsdp --ulysses_size 8 --prompt "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline''s intricate details and the refreshing atmosphere of the seaside."\n```\n\n- Image-to-Video Generation without prompt\n\n```sh\nDASH_API_KEY=your_key torchrun --nproc_per_node=8 generate.py --task i2v-A14B --size 1280*720 --ckpt_dir ./Wan2.2-I2V-A14B --prompt '''' --image examples/i2v_input.JPG --dit_fsdp --t5_fsdp --ulysses_size 8 --use_prompt_extend --prompt_extend_method ''dashscope''\n```\n\n> ðŸ’¡The model can generate videos solely from the input image. You can use prompt extension to generate prompt from the image.\n\n> The process of prompt extension can be referenced [here](#2-using-prompt-extention).\n\n\n\n\n## Computational Efficiency on Different GPUs\n\nWe test the computational efficiency of different **Wan2.2** models on different GPUs in the following table. The results are presented in the format: **Total time (s) / peak GPU memory (GB)**.\n\n\n<div align="center">\n    <img src="assets/comp_effic.png" alt="" style="width: 80%;" />\n</div>\n\n> The parameter settings for the tests presented in this table are as follows:\n> (1) Multi-GPU: 14B: `--ulysses_size 4/8 --dit_fsdp --t5_fsdp`, 5B: `--ulysses_size 4/8 --offload_model True --convert_model_dtype --t5_cpu`; Single-GPU: 14B: `--offload_model True --convert_model_dtype`, 5B: `--offload_model True --convert_model_dtype --t5_cpu`\n(--convert_model_dtype converts model parameter types to config.param_dtype);\n> (2) The distributed testing utilizes the built-in FSDP and Ulysses implementations, with FlashAttention3 deployed on Hopper architecture GPUs;\n> (3) Tests were run without the `--use_prompt_extend` flag;\n> (4) Reported results are the average of multiple samples taken after the warm-up phase.\n\n\n-------\n\n## Introduction of Wan2.2\n\n**Wan2.2** builds on the foundation of Wan2.1 with notable improvements in generation quality and model capability. This upgrade is driven by a series of key technical innovations, mainly including the Mixture-of-Experts (MoE) architecture, upgraded training data, and high-compression video generation.\n\n##### (1) Mixture-of-Experts (MoE) Architecture\n\nWan2.2 introduces Mixture-of-Experts (MoE) architecture into the video generation diffusion model. MoE has been widely validated in large language models as an efficient approach to increase total model parameters while keeping inference cost nearly unchanged. In Wan2.2, the A14B model series adopts a two-expert design tailored to the denoising process of diffusion models: a high-noise expert for the early stages, focusing on overall layout; and a low-noise expert for the later stages, refining video details. Each expert model has about 14B parameters, resulting in a total of 27B parameters but only 14B active parameters per step, keeping inference computation and GPU memory nearly unchanged.\n\n<div align="center">\n    <img src="assets/moe_arch.png" alt="" style="width: 90%;" />\n</div>\n\nThe transition point between the two experts is determined by the signal-to-noise ratio (SNR), a metric that decreases monotonically as the denoising step $t$ increases. At the beginning of the denoising process, $t$ is large and the noise level is high, so the SNR is at its minimum, denoted as ${SNR}_{min}$. In this stage, the high-noise expert is activated. We define a threshold step ${t}_{moe}$ corresponding to half of the ${SNR}_{min}$, and switch to the low-noise expert when $t<{t}_{moe}$.\n\n<div align="center">\n    <img src="assets/moe_2.png" alt="" style="width: 90%;" />\n</div>\n\nTo validate the effectiveness of the MoE architecture, four settings are compared based on their validation loss curves. The baseline **Wan2.1** model does not employ the MoE architecture. Among the MoE-based variants, the **Wan2.1 & High-Noise Expert** reuses the Wan2.1 model as the low-noise expert while uses the  Wan2.2''s high-noise expert, while the **Wan2.1 & Low-Noise Expert** uses Wan2.1 as the high-noise expert and employ the Wan2.2''s low-noise expert. The **Wan2.2 (MoE)** (our final version) achieves the lowest validation loss, indicating that its generated video distribution is closest to ground-truth and exhibits superior convergence.\n\n\n##### (2) Efficient High-Definition Hybrid TI2V\nTo enable more efficient deployment, Wan2.2 also explores a high-compression design. In addition to the 27B MoE models, a 5B dense model, i.e., TI2V-5B, is released. It is supported by a high-compression Wan2.2-VAE, which achieves a $T\times H\times W$ compression ratio of $4\times16\times16$, increasing the overall compression rate to 64 while maintaining high-quality video reconstruction. With an additional patchification layer, the total compression ratio of TI2V-5B reaches $4\times32\times32$. Without specific optimization, TI2V-5B can generate a 5-second 720P video in under 9 minutes on a single consumer-grade GPU, ranking among the fastest 720P@24fps video generation models. This model also natively supports both text-to-video and image-to-video tasks within a single unified framework, covering both academic research and practical applications.\n\n\n<div align="center">\n    <img src="assets/vae.png" alt="" style="width: 80%;" />\n</div>\n\n\n\n##### Comparisons to SOTAs\nWe compared Wan2.2 with leading closed-source commercial models on our new Wan-Bench 2.0, evaluating performance across multiple crucial dimensions. The results demonstrate that Wan2.2 achieves superior performance compared to these leading models.\n\n\n<div align="center">\n    <img src="assets/performance.png" alt="" style="width: 90%;" />\n</div>\n\n## Citation\nIf you find our work helpful, please cite us.\n\n```\n@article{wan2025,\n      title={Wan: Open and Advanced Large-Scale Video Generative Models}, \n      author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu},\n      journal = {arXiv preprint arXiv:2503.20314},\n      year={2025}\n}\n```\n\n## License Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generated contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the [license](LICENSE.txt).\n\n\n## Acknowledgements\n\nWe would like to thank the contributors to the [SD3](https://huggingface.co/stabilityai/stable-diffusion-3-medium), [Qwen](https://huggingface.co/Qwen), [umt5-xxl](https://huggingface.co/google/umt5-xxl), [diffusers](https://github.com/huggingface/diffusers) and [HuggingFace](https://huggingface.co) repositories, for their open research.\n\n\n\n## Contact Us\nIf you would like to leave a message to our research or product teams, feel free to join our [Discord](https://discord.gg/AKNgpMK4Yj) or [WeChat groups](https://gw.alicdn.com/imgextra/i2/O1CN01tqjWFi1ByuyehkTSB_!!6000000000015-0-tps-611-1279.jpg)!', '{"pipeline_tag":"image-to-video","library_name":"wan2.2","framework":"wan2.2","params":null,"storage_bytes":126206646514,"files_count":33,"spaces_count":90,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:Wan-Video:Wan2.2\">GitHub<","source_url":"https://github.com/Wan-Video/Wan2.2\">GitHub<"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.1","source_url":"https://github.com/Wan-Video/Wan2.1"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.2.git","source_url":"https://github.com/Wan-Video/Wan2.2.git"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"based_on_paper","target_id":"arxiv:2503.20314","source_url":"https://arxiv.org/abs/2503.20314"}]', NULL, 'Apache-2.0', 'approved', 97.2, '8eeebf62326081db34af508221a424e4', NULL, 'https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B/resolve/main/assets/comp_effic.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-Wan-AI-Wan2.2-I2V-A14B from https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B/resolve/main/assets/comp_effic.png
Image converted to WebP: data/images/huggingface-Wan-AI-Wan2.2-I2V-A14B.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-hkunlp-instructor-large', 'huggingface--hkunlp--instructor-large', 'instructor-large', 'hkunlp', '--- pipeline_tag: sentence-similarity tags: - text-embedding - embeddings - information-retrieval - beir - text-classification - language-model - text-clustering - text-semantic-similarity - text-evaluation - prompt-retrieval - text-reranking - sentence-transformers - feature-extraction - sentence-similarity - transformers - t5 - English - Sentence Similarity - natural_questions - ms_marco - fever - hotpot_qa - mteb language: en inference: false license: apache-2.0 model-index: - name: INSTRU...', '["sentence-transformers","pytorch","t5","text-embedding","embeddings","information-retrieval","beir","text-classification","language-model","text-clustering","text-semantic-similarity","text-evaluation","prompt-retrieval","text-reranking","feature-extraction","sentence-similarity","transformers","english","sentence similarity","natural_questions","ms_marco","fever","hotpot_qa","mteb","en","arxiv:2212.09741","license:apache-2.0","model-index","text-generation-inference","deploy:azure","region:us"]', 'sentence-similarity', 523, 215290, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/hkunlp/instructor-large","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\npipeline_tag: sentence-similarity\ntags:\n- text-embedding\n- embeddings\n- information-retrieval\n- beir\n- text-classification\n- language-model\n- text-clustering\n- text-semantic-similarity\n- text-evaluation\n- prompt-retrieval\n- text-reranking\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\n- t5\n- English\n- Sentence Similarity\n- natural_questions\n- ms_marco\n- fever\n- hotpot_qa\n- mteb\nlanguage: en\ninference: false\nlicense: apache-2.0\nmodel-index:\n- name: INSTRUCTOR\n  results:\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en)\n      config: en\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 88.13432835820896\n    - type: ap\n      value: 59.298209334395665\n    - type: f1\n      value: 83.31769058643586\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_polarity\n      name: MTEB AmazonPolarityClassification\n      config: default\n      split: test\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n    metrics:\n    - type: accuracy\n      value: 91.526375\n    - type: ap\n      value: 88.16327709705504\n    - type: f1\n      value: 91.51095801287843\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (en)\n      config: en\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 47.856\n    - type: f1\n      value: 45.41490917650942\n  - task:\n      type: Retrieval\n    dataset:\n      type: arguana\n      name: MTEB ArguAna\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 31.223\n    - type: map_at_10\n      value: 47.947\n    - type: map_at_100\n      value: 48.742000000000004\n    - type: map_at_1000\n      value: 48.745\n    - type: map_at_3\n      value: 43.137\n    - type: map_at_5\n      value: 45.992\n    - type: mrr_at_1\n      value: 32.432\n    - type: mrr_at_10\n      value: 48.4\n    - type: mrr_at_100\n      value: 49.202\n    - type: mrr_at_1000\n      value: 49.205\n    - type: mrr_at_3\n      value: 43.551\n    - type: mrr_at_5\n      value: 46.467999999999996\n    - type: ndcg_at_1\n      value: 31.223\n    - type: ndcg_at_10\n      value: 57.045\n    - type: ndcg_at_100\n      value: 60.175\n    - type: ndcg_at_1000\n      value: 60.233000000000004\n    - type: ndcg_at_3\n      value: 47.171\n    - type: ndcg_at_5\n      value: 52.322\n    - type: precision_at_1\n      value: 31.223\n    - type: precision_at_10\n      value: 8.599\n    - type: precision_at_100\n      value: 0.991\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_3\n      value: 19.63\n    - type: precision_at_5\n      value: 14.282\n    - type: recall_at_1\n      value: 31.223\n    - type: recall_at_10\n      value: 85.989\n    - type: recall_at_100\n      value: 99.075\n    - type: recall_at_1000\n      value: 99.502\n    - type: recall_at_3\n      value: 58.89\n    - type: recall_at_5\n      value: 71.408\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-p2p\n      name: MTEB ArxivClusteringP2P\n      config: default\n      split: test\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n    metrics:\n    - type: v_measure\n      value: 43.1621946393635\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-s2s\n      name: MTEB ArxivClusteringS2S\n      config: default\n      split: test\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\n    metrics:\n    - type: v_measure\n      value: 32.56417132407894\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/askubuntudupquestions-reranking\n      name: MTEB AskUbuntuDupQuestions\n      config: default\n      split: test\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\n    metrics:\n    - type: map\n      value: 64.29539304390207\n    - type: mrr\n      value: 76.44484017060196\n  - task:\n      type: STS\n    dataset:\n      type: mteb/biosses-sts\n      name: MTEB BIOSSES\n      config: default\n      split: test\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\n    metrics:\n    - type: cos_sim_spearman\n      value: 84.38746499431112\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77Classification\n      config: default\n      split: test\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\n    metrics:\n    - type: accuracy\n      value: 78.51298701298701\n    - type: f1\n      value: 77.49041754069235\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-p2p\n      name: MTEB BiorxivClusteringP2P\n      config: default\n      split: test\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\n    metrics:\n    - type: v_measure\n      value: 37.61848554098577\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-s2s\n      name: MTEB BiorxivClusteringS2S\n      config: default\n      split: test\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\n    metrics:\n    - type: v_measure\n      value: 31.32623280148178\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackAndroidRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 35.803000000000004\n    - type: map_at_10\n      value: 48.848\n    - type: map_at_100\n      value: 50.5\n    - type: map_at_1000\n      value: 50.602999999999994\n    - type: map_at_3\n      value: 45.111000000000004\n    - type: map_at_5\n      value: 47.202\n    - type: mrr_at_1\n      value: 44.635000000000005\n    - type: mrr_at_10\n      value: 55.593\n    - type: mrr_at_100\n      value: 56.169999999999995\n    - type: mrr_at_1000\n      value: 56.19499999999999\n    - type: mrr_at_3\n      value: 53.361999999999995\n    - type: mrr_at_5\n      value: 54.806999999999995\n    - type: ndcg_at_1\n      value: 44.635000000000005\n    - type: ndcg_at_10\n      value: 55.899\n    - type: ndcg_at_100\n      value: 60.958\n    - type: ndcg_at_1000\n      value: 62.302\n    - type: ndcg_at_3\n      value: 51.051\n    - type: ndcg_at_5\n      value: 53.351000000000006\n    - type: precision_at_1\n      value: 44.635000000000005\n    - type: precision_at_10\n      value: 10.786999999999999\n    - type: precision_at_100\n      value: 1.6580000000000001\n    - type: precision_at_1000\n      value: 0.213\n    - type: precision_at_3\n      value: 24.893\n    - type: precision_at_5\n      value: 17.740000000000002\n    - type: recall_at_1\n      value: 35.803000000000004\n    - type: recall_at_10\n      value: 68.657\n    - type: recall_at_100\n      value: 89.77199999999999\n    - type: recall_at_1000\n      value: 97.67\n    - type: recall_at_3\n      value: 54.066\n    - type: recall_at_5\n      value: 60.788\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackEnglishRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 33.706\n    - type: map_at_10\n      value: 44.896\n    - type: map_at_100\n      value: 46.299\n    - type: map_at_1000\n      value: 46.44\n    - type: map_at_3\n      value: 41.721000000000004\n    - type: map_at_5\n      value: 43.486000000000004\n    - type: mrr_at_1\n      value: 41.592\n    - type: mrr_at_10\n      value: 50.529\n    - type: mrr_at_100\n      value: 51.22\n    - type: mrr_at_1000\n      value: 51.258\n    - type: mrr_at_3\n      value: 48.205999999999996\n    - type: mrr_at_5\n      value: 49.528\n    - type: ndcg_at_1\n      value: 41.592\n    - type: ndcg_at_10\n      value: 50.77199999999999\n    - type: ndcg_at_100\n      value: 55.383\n    - type: ndcg_at_1000\n      value: 57.288\n    - type: ndcg_at_3\n      value: 46.324\n    - type: ndcg_at_5\n      value: 48.346000000000004\n    - type: precision_at_1\n      value: 41.592\n    - type: precision_at_10\n      value: 9.516\n    - type: precision_at_100\n      value: 1.541\n    - type: precision_at_1000\n      value: 0.2\n    - type: precision_at_3\n      value: 22.399\n    - type: precision_at_5\n      value: 15.770999999999999\n    - type: recall_at_1\n      value: 33.706\n    - type: recall_at_10\n      value: 61.353\n    - type: recall_at_100\n      value: 80.182\n    - type: recall_at_1000\n      value: 91.896\n    - type: recall_at_3\n      value: 48.204\n    - type: recall_at_5\n      value: 53.89699999999999\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGamingRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 44.424\n    - type: map_at_10\n      value: 57.169000000000004\n    - type: map_at_100\n      value: 58.202\n    - type: map_at_1000\n      value: 58.242000000000004\n    - type: map_at_3\n      value: 53.825\n    - type: map_at_5\n      value: 55.714\n    - type: mrr_at_1\n      value: 50.470000000000006\n    - type: mrr_at_10\n      value: 60.489000000000004\n    - type: mrr_at_100\n      value: 61.096\n    - type: mrr_at_1000\n      value: 61.112\n    - type: mrr_at_3\n      value: 58.192\n    - type: mrr_at_5\n      value: 59.611999999999995\n    - type: ndcg_at_1\n      value: 50.470000000000006\n    - type: ndcg_at_10\n      value: 63.071999999999996\n    - type: ndcg_at_100\n      value: 66.964\n    - type: ndcg_at_1000\n      value: 67.659\n    - type: ndcg_at_3\n      value: 57.74399999999999\n    - type: ndcg_at_5\n      value: 60.367000000000004\n    - type: precision_at_1\n      value: 50.470000000000006\n    - type: precision_at_10\n      value: 10.019\n    - type: precision_at_100\n      value: 1.29\n    - type: precision_at_1000\n      value: 0.13899999999999998\n    - type: precision_at_3\n      value: 25.558999999999997\n    - type: precision_at_5\n      value: 17.467\n    - type: recall_at_1\n      value: 44.424\n    - type: recall_at_10\n      value: 77.02\n    - type: recall_at_100\n      value: 93.738\n    - type: recall_at_1000\n      value: 98.451\n    - type: recall_at_3\n      value: 62.888\n    - type: recall_at_5\n      value: 69.138\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackGisRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 26.294\n    - type: map_at_10\n      value: 34.503\n    - type: map_at_100\n      value: 35.641\n    - type: map_at_1000\n      value: 35.724000000000004\n    - type: map_at_3\n      value: 31.753999999999998\n    - type: map_at_5\n      value: 33.190999999999995\n    - type: mrr_at_1\n      value: 28.362\n    - type: mrr_at_10\n      value: 36.53\n    - type: mrr_at_100\n      value: 37.541000000000004\n    - type: mrr_at_1000\n      value: 37.602000000000004\n    - type: mrr_at_3\n      value: 33.917\n    - type: mrr_at_5\n      value: 35.358000000000004\n    - type: ndcg_at_1\n      value: 28.362\n    - type: ndcg_at_10\n      value: 39.513999999999996\n    - type: ndcg_at_100\n      value: 44.815\n    - type: ndcg_at_1000\n      value: 46.839\n    - type: ndcg_at_3\n      value: 34.02\n    - type: ndcg_at_5\n      value: 36.522\n    - type: precision_at_1\n      value: 28.362\n    - type: precision_at_10\n      value: 6.101999999999999\n    - type: precision_at_100\n      value: 0.9129999999999999\n    - type: precision_at_1000\n      value: 0.11399999999999999\n    - type: precision_at_3\n      value: 14.161999999999999\n    - type: precision_at_5\n      value: 9.966\n    - type: recall_at_1\n      value: 26.294\n    - type: recall_at_10\n      value: 53.098\n    - type: recall_at_100\n      value: 76.877\n    - type: recall_at_1000\n      value: 91.834\n    - type: recall_at_3\n      value: 38.266\n    - type: recall_at_5\n      value: 44.287\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackMathematicaRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 16.407\n    - type: map_at_10\n      value: 25.185999999999996\n    - type: map_at_100\n      value: 26.533\n    - type: map_at_1000\n      value: 26.657999999999998\n    - type: map_at_3\n      value: 22.201999999999998\n    - type: map_at_5\n      value: 23.923\n    - type: mrr_at_1\n      value: 20.522000000000002\n    - type: mrr_at_10\n      value: 29.522\n    - type: mrr_at_100\n      value: 30.644\n    - type: mrr_at_1000\n      value: 30.713\n    - type: mrr_at_3\n      value: 26.679000000000002\n    - type: mrr_at_5\n      value: 28.483000000000004\n    - type: ndcg_at_1\n      value: 20.522000000000002\n    - type: ndcg_at_10\n      value: 30.656\n    - type: ndcg_at_100\n      value: 36.864999999999995\n    - type: ndcg_at_1000\n      value: 39.675\n    - type: ndcg_at_3\n      value: 25.319000000000003\n    - type: ndcg_at_5\n      value: 27.992\n    - type: precision_at_1\n      value: 20.522000000000002\n    - type: precision_at_10\n      value: 5.795999999999999\n    - type: precision_at_100\n      value: 1.027\n    - type: precision_at_1000\n      value: 0.13999999999999999\n    - type: precision_at_3\n      value: 12.396\n    - type: precision_at_5\n      value: 9.328\n    - type: recall_at_1\n      value: 16.407\n    - type: recall_at_10\n      value: 43.164\n    - type: recall_at_100\n      value: 69.695\n    - type: recall_at_1000\n      value: 89.41900000000001\n    - type: recall_at_3\n      value: 28.634999999999998\n    - type: recall_at_5\n      value: 35.308\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackPhysicsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 30.473\n    - type: map_at_10\n      value: 41.676\n    - type: map_at_100\n      value: 43.120999999999995\n    - type: map_at_1000\n      value: 43.230000000000004\n    - type: map_at_3\n      value: 38.306000000000004\n    - type: map_at_5\n      value: 40.355999999999995\n    - type: mrr_at_1\n      value: 37.536\n    - type: mrr_at_10\n      value: 47.643\n    - type: mrr_at_100\n      value: 48.508\n    - type: mrr_at_1000\n      value: 48.551\n    - type: mrr_at_3\n      value: 45.348\n    - type: mrr_at_5\n      value: 46.744\n    - type: ndcg_at_1\n      value: 37.536\n    - type: ndcg_at_10\n      value: 47.823\n    - type: ndcg_at_100\n      value: 53.395\n    - type: ndcg_at_1000\n      value: 55.271\n    - type: ndcg_at_3\n      value: 42.768\n    - type: ndcg_at_5\n      value: 45.373000000000005\n    - type: precision_at_1\n      value: 37.536\n    - type: precision_at_10\n      value: 8.681\n    - type: precision_at_100\n      value: 1.34\n    - type: precision_at_1000\n      value: 0.165\n    - type: precision_at_3\n      value: 20.468\n    - type: precision_at_5\n      value: 14.495\n    - type: recall_at_1\n      value: 30.473\n    - type: recall_at_10\n      value: 60.092999999999996\n    - type: recall_at_100\n      value: 82.733\n    - type: recall_at_1000\n      value: 94.875\n    - type: recall_at_3\n      value: 45.734\n    - type: recall_at_5\n      value: 52.691\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackProgrammersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 29.976000000000003\n    - type: map_at_10\n      value: 41.097\n    - type: map_at_100\n      value: 42.547000000000004\n    - type: map_at_1000\n      value: 42.659000000000006\n    - type: map_at_3\n      value: 37.251\n    - type: map_at_5\n      value: 39.493\n    - type: mrr_at_1\n      value: 37.557\n    - type: mrr_at_10\n      value: 46.605000000000004\n    - type: mrr_at_100\n      value: 47.487\n    - type: mrr_at_1000\n      value: 47.54\n    - type: mrr_at_3\n      value: 43.721\n    - type: mrr_at_5\n      value: 45.411\n    - type: ndcg_at_1\n      value: 37.557\n    - type: ndcg_at_10\n      value: 47.449000000000005\n    - type: ndcg_at_100\n      value: 53.052\n    - type: ndcg_at_1000\n      value: 55.010999999999996\n    - type: ndcg_at_3\n      value: 41.439\n    - type: ndcg_at_5\n      value: 44.292\n    - type: precision_at_1\n      value: 37.557\n    - type: precision_at_10\n      value: 8.847\n    - type: precision_at_100\n      value: 1.357\n    - type: precision_at_1000\n      value: 0.16999999999999998\n    - type: precision_at_3\n      value: 20.091\n    - type: precision_at_5\n      value: 14.384\n    - type: recall_at_1\n      value: 29.976000000000003\n    - type: recall_at_10\n      value: 60.99099999999999\n    - type: recall_at_100\n      value: 84.245\n    - type: recall_at_1000\n      value: 96.97200000000001\n    - type: recall_at_3\n      value: 43.794\n    - type: recall_at_5\n      value: 51.778999999999996\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 28.099166666666665\n    - type: map_at_10\n      value: 38.1365\n    - type: map_at_100\n      value: 39.44491666666667\n    - type: map_at_1000\n      value: 39.55858333333334\n    - type: map_at_3\n      value: 35.03641666666666\n    - type: map_at_5\n      value: 36.79833333333334\n    - type: mrr_at_1\n      value: 33.39966666666667\n    - type: mrr_at_10\n      value: 42.42583333333333\n    - type: mrr_at_100\n      value: 43.28575\n    - type: mrr_at_1000\n      value: 43.33741666666667\n    - type: mrr_at_3\n      value: 39.94975\n    - type: mrr_at_5\n      value: 41.41633333333334\n    - type: ndcg_at_1\n      value: 33.39966666666667\n    - type: ndcg_at_10\n      value: 43.81741666666667\n    - type: ndcg_at_100\n      value: 49.08166666666667\n    - type: ndcg_at_1000\n      value: 51.121166666666674\n    - type: ndcg_at_3\n      value: 38.73575\n    - type: ndcg_at_5\n      value: 41.18158333333333\n    - type: precision_at_1\n      value: 33.39966666666667\n    - type: precision_at_10\n      value: 7.738916666666667\n    - type: precision_at_100\n      value: 1.2265833333333331\n    - type: precision_at_1000\n      value: 0.15983333333333336\n    - type: precision_at_3\n      value: 17.967416666666665\n    - type: precision_at_5\n      value: 12.78675\n    - type: recall_at_1\n      value: 28.099166666666665\n    - type: recall_at_10\n      value: 56.27049999999999\n    - type: recall_at_100\n      value: 78.93291666666667\n    - type: recall_at_1000\n      value: 92.81608333333334\n    - type: recall_at_3\n      value: 42.09775\n    - type: recall_at_5\n      value: 48.42533333333334\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackStatsRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 23.663\n    - type: map_at_10\n      value: 30.377\n    - type: map_at_100\n      value: 31.426\n    - type: map_at_1000\n      value: 31.519000000000002\n    - type: map_at_3\n      value: 28.069\n    - type: map_at_5\n      value: 29.256999999999998\n    - type: mrr_at_1\n      value: 26.687\n    - type: mrr_at_10\n      value: 33.107\n    - type: mrr_at_100\n      value: 34.055\n    - type: mrr_at_1000\n      value: 34.117999999999995\n    - type: mrr_at_3\n      value: 31.058000000000003\n    - type: mrr_at_5\n      value: 32.14\n    - type: ndcg_at_1\n      value: 26.687\n    - type: ndcg_at_10\n      value: 34.615\n    - type: ndcg_at_100\n      value: 39.776\n    - type: ndcg_at_1000\n      value: 42.05\n    - type: ndcg_at_3\n      value: 30.322\n    - type: ndcg_at_5\n      value: 32.157000000000004\n    - type: precision_at_1\n      value: 26.687\n    - type: precision_at_10\n      value: 5.491\n    - type: precision_at_100\n      value: 0.877\n    - type: precision_at_1000\n      value: 0.11499999999999999\n    - type: precision_at_3\n      value: 13.139000000000001\n    - type: precision_at_5\n      value: 9.049\n    - type: recall_at_1\n      value: 23.663\n    - type: recall_at_10\n      value: 45.035\n    - type: recall_at_100\n      value: 68.554\n    - type: recall_at_1000\n      value: 85.077\n    - type: recall_at_3\n      value: 32.982\n    - type: recall_at_5\n      value: 37.688\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackTexRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 17.403\n    - type: map_at_10\n      value: 25.197000000000003\n    - type: map_at_100\n      value: 26.355\n    - type: map_at_1000\n      value: 26.487\n    - type: map_at_3\n      value: 22.733\n    - type: map_at_5\n      value: 24.114\n    - type: mrr_at_1\n      value: 21.37\n    - type: mrr_at_10\n      value: 29.091\n    - type: mrr_at_100\n      value: 30.018\n    - type: mrr_at_1000\n      value: 30.096\n    - type: mrr_at_3\n      value: 26.887\n    - type: mrr_at_5\n      value: 28.157\n    - type: ndcg_at_1\n      value: 21.37\n    - type: ndcg_at_10\n      value: 30.026000000000003\n    - type: ndcg_at_100\n      value: 35.416\n    - type: ndcg_at_1000\n      value: 38.45\n    - type: ndcg_at_3\n      value: 25.764\n    - type: ndcg_at_5\n      value: 27.742\n    - type: precision_at_1\n      value: 21.37\n    - type: precision_at_10\n      value: 5.609\n    - type: precision_at_100\n      value: 0.9860000000000001\n    - type: precision_at_1000\n      value: 0.14300000000000002\n    - type: precision_at_3\n      value: 12.423\n    - type: precision_at_5\n      value: 9.009\n    - type: recall_at_1\n      value: 17.403\n    - type: recall_at_10\n      value: 40.573\n    - type: recall_at_100\n      value: 64.818\n    - type: recall_at_1000\n      value: 86.53699999999999\n    - type: recall_at_3\n      value: 28.493000000000002\n    - type: recall_at_5\n      value: 33.660000000000004\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackUnixRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 28.639\n    - type: map_at_10\n      value: 38.951\n    - type: map_at_100\n      value: 40.238\n    - type: map_at_1000\n      value: 40.327\n    - type: map_at_3\n      value: 35.842\n    - type: map_at_5\n      value: 37.617\n    - type: mrr_at_1\n      value: 33.769\n    - type: mrr_at_10\n      value: 43.088\n    - type: mrr_at_100\n      value: 44.03\n    - type: mrr_at_1000\n      value: 44.072\n    - type: mrr_at_3\n      value: 40.656\n    - type: mrr_at_5\n      value: 42.138999999999996\n    - type: ndcg_at_1\n      value: 33.769\n    - type: ndcg_at_10\n      value: 44.676\n    - type: ndcg_at_100\n      value: 50.416000000000004\n    - type: ndcg_at_1000\n      value: 52.227999999999994\n    - type: ndcg_at_3\n      value: 39.494\n    - type: ndcg_at_5\n      value: 42.013\n    - type: precision_at_1\n      value: 33.769\n    - type: precision_at_10\n      value: 7.668\n    - type: precision_at_100\n      value: 1.18\n    - type: precision_at_1000\n      value: 0.145\n    - type: precision_at_3\n      value: 18.221\n    - type: precision_at_5\n      value: 12.966\n    - type: recall_at_1\n      value: 28.639\n    - type: recall_at_10\n      value: 57.687999999999995\n    - type: recall_at_100\n      value: 82.541\n    - type: recall_at_1000\n      value: 94.896\n    - type: recall_at_3\n      value: 43.651\n    - type: recall_at_5\n      value: 49.925999999999995\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWebmastersRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 29.57\n    - type: map_at_10\n      value: 40.004\n    - type: map_at_100\n      value: 41.75\n    - type: map_at_1000\n      value: 41.97\n    - type: map_at_3\n      value: 36.788\n    - type: map_at_5\n      value: 38.671\n    - type: mrr_at_1\n      value: 35.375\n    - type: mrr_at_10\n      value: 45.121\n    - type: mrr_at_100\n      value: 45.994\n    - type: mrr_at_1000\n      value: 46.04\n    - type: mrr_at_3\n      value: 42.227\n    - type: mrr_at_5\n      value: 43.995\n    - type: ndcg_at_1\n      value: 35.375\n    - type: ndcg_at_10\n      value: 46.392\n    - type: ndcg_at_100\n      value: 52.196\n    - type: ndcg_at_1000\n      value: 54.274\n    - type: ndcg_at_3\n      value: 41.163\n    - type: ndcg_at_5\n      value: 43.813\n    - type: precision_at_1\n      value: 35.375\n    - type: precision_at_10\n      value: 8.676\n    - type: precision_at_100\n      value: 1.678\n    - type: precision_at_1000\n      value: 0.253\n    - type: precision_at_3\n      value: 19.104\n    - type: precision_at_5\n      value: 13.913\n    - type: recall_at_1\n      value: 29.57\n    - type: recall_at_10\n      value: 58.779\n    - type: recall_at_100\n      value: 83.337\n    - type: recall_at_1000\n      value: 95.979\n    - type: recall_at_3\n      value: 44.005\n    - type: recall_at_5\n      value: 50.975\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackWordpressRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 20.832\n    - type: map_at_10\n      value: 29.733999999999998\n    - type: map_at_100\n      value: 30.727\n    - type: map_at_1000\n      value: 30.843999999999998\n    - type: map_at_3\n      value: 26.834999999999997\n    - type: map_at_5\n      value: 28.555999999999997\n    - type: mrr_at_1\n      value: 22.921\n    - type: mrr_at_10\n      value: 31.791999999999998\n    - type: mrr_at_100\n      value: 32.666000000000004\n    - type: mrr_at_1000\n      value: 32.751999999999995\n    - type: mrr_at_3\n      value: 29.144\n    - type: mrr_at_5\n      value: 30.622\n    - type: ndcg_at_1\n      value: 22.921\n    - type: ndcg_at_10\n      value: 34.915\n    - type: ndcg_at_100\n      value: 39.744\n    - type: ndcg_at_1000\n      value: 42.407000000000004\n    - type: ndcg_at_3\n      value: 29.421000000000003\n    - type: ndcg_at_5\n      value: 32.211\n    - type: precision_at_1\n      value: 22.921\n    - type: precision_at_10\n      value: 5.675\n    - type: precision_at_100\n      value: 0.872\n    - type: precision_at_1000\n      value: 0.121\n    - type: precision_at_3\n      value: 12.753999999999998\n    - type: precision_at_5\n      value: 9.353\n    - type: recall_at_1\n      value: 20.832\n    - type: recall_at_10\n      value: 48.795\n    - type: recall_at_100\n      value: 70.703\n    - type: recall_at_1000\n      value: 90.187\n    - type: recall_at_3\n      value: 34.455000000000005\n    - type: recall_at_5\n      value: 40.967\n  - task:\n      type: Retrieval\n    dataset:\n      type: climate-fever\n      name: MTEB ClimateFEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 10.334\n    - type: map_at_10\n      value: 19.009999999999998\n    - type: map_at_100\n      value: 21.129\n    - type: map_at_1000\n      value: 21.328\n    - type: map_at_3\n      value: 15.152\n    - type: map_at_5\n      value: 17.084\n    - type: mrr_at_1\n      value: 23.453\n    - type: mrr_at_10\n      value: 36.099\n    - type: mrr_at_100\n      value: 37.069\n    - type: mrr_at_1000\n      value: 37.104\n    - type: mrr_at_3\n      value: 32.096000000000004\n    - type: mrr_at_5\n      value: 34.451\n    - type: ndcg_at_1\n      value: 23.453\n    - type: ndcg_at_10\n      value: 27.739000000000004\n    - type: ndcg_at_100\n      value: 35.836\n    - type: ndcg_at_1000\n      value: 39.242\n    - type: ndcg_at_3\n      value: 21.263\n    - type: ndcg_at_5\n      value: 23.677\n    - type: precision_at_1\n      value: 23.453\n    - type: precision_at_10\n      value: 9.199\n    - type: precision_at_100\n      value: 1.791\n    - type: precision_at_1000\n      value: 0.242\n    - type: precision_at_3\n      value: 16.2\n    - type: precision_at_5\n      value: 13.147\n    - type: recall_at_1\n      value: 10.334\n    - type: recall_at_10\n      value: 35.177\n    - type: recall_at_100\n      value: 63.009\n    - type: recall_at_1000\n      value: 81.938\n    - type: recall_at_3\n      value: 19.914\n    - type: recall_at_5\n      value: 26.077\n  - task:\n      type: Retrieval\n    dataset:\n      type: dbpedia-entity\n      name: MTEB DBPedia\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 8.212\n    - type: map_at_10\n      value: 17.386\n    - type: map_at_100\n      value: 24.234\n    - type: map_at_1000\n      value: 25.724999999999998\n    - type: map_at_3\n      value: 12.727\n    - type: map_at_5\n      value: 14.785\n    - type: mrr_at_1\n      value: 59.25\n    - type: mrr_at_10\n      value: 68.687\n    - type: mrr_at_100\n      value: 69.133\n    - type: mrr_at_1000\n      value: 69.14099999999999\n    - type: mrr_at_3\n      value: 66.917\n    - type: mrr_at_5\n      value: 67.742\n    - type: ndcg_at_1\n      value: 48.625\n    - type: ndcg_at_10\n      value: 36.675999999999995\n    - type: ndcg_at_100\n      value: 41.543\n    - type: ndcg_at_1000\n      value: 49.241\n    - type: ndcg_at_3\n      value: 41.373\n    - type: ndcg_at_5\n      value: 38.707\n    - type: precision_at_1\n      value: 59.25\n    - type: precision_at_10\n      value: 28.525\n    - type: precision_at_100\n      value: 9.027000000000001\n    - type: precision_at_1000\n      value: 1.8339999999999999\n    - type: precision_at_3\n      value: 44.833\n    - type: precision_at_5\n      value: 37.35\n    - type: recall_at_1\n      value: 8.212\n    - type: recall_at_10\n      value: 23.188\n    - type: recall_at_100\n      value: 48.613\n    - type: recall_at_1000\n      value: 73.093\n    - type: recall_at_3\n      value: 14.419\n    - type: recall_at_5\n      value: 17.798\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/emotion\n      name: MTEB EmotionClassification\n      config: default\n      split: test\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\n    metrics:\n    - type: accuracy\n      value: 52.725\n    - type: f1\n      value: 46.50743309855908\n  - task:\n      type: Retrieval\n    dataset:\n      type: fever\n      name: MTEB FEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 55.086\n    - type: map_at_10\n      value: 66.914\n    - type: map_at_100\n      value: 67.321\n    - type: map_at_1000\n      value: 67.341\n    - type: map_at_3\n      value: 64.75800000000001\n    - type: map_at_5\n      value: 66.189\n    - type: mrr_at_1\n      value: 59.28600000000001\n    - type: mrr_at_10\n      value: 71.005\n    - type: mrr_at_100\n      value: 71.304\n    - type: mrr_at_1000\n      value: 71.313\n    - type: mrr_at_3\n      value: 69.037\n    - type: mrr_at_5\n      value: 70.35\n    - type: ndcg_at_1\n      value: 59.28600000000001\n    - type: ndcg_at_10\n      value: 72.695\n    - type: ndcg_at_100\n      value: 74.432\n    - type: ndcg_at_1000\n      value: 74.868\n    - type: ndcg_at_3\n      value: 68.72200000000001\n    - type: ndcg_at_5\n      value: 71.081\n    - type: precision_at_1\n      value: 59.28600000000001\n    - type: precision_at_10\n      value: 9.499\n    - type: precision_at_100\n      value: 1.052\n    - type: precision_at_1000\n      value: 0.11100000000000002\n    - type: precision_at_3\n      value: 27.503\n    - type: precision_at_5\n      value: 17.854999999999997\n    - type: recall_at_1\n      value: 55.086\n    - type: recall_at_10\n      value: 86.453\n    - type: recall_at_100\n      value: 94.028\n    - type: recall_at_1000\n      value: 97.052\n    - type: recall_at_3\n      value: 75.821\n    - type: recall_at_5\n      value: 81.6\n  - task:\n      type: Retrieval\n    dataset:\n      type: fiqa\n      name: MTEB FiQA2018\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 22.262999999999998\n    - type: map_at_10\n      value: 37.488\n    - type: map_at_100\n      value: 39.498\n    - type: map_at_1000\n      value: 39.687\n    - type: map_at_3\n      value: 32.529\n    - type: map_at_5\n      value: 35.455\n    - type: mrr_at_1\n      value: 44.907000000000004\n    - type: mrr_at_10\n      value: 53.239000000000004\n    - type: mrr_at_100\n      value: 54.086\n    - type: mrr_at_1000\n      value: 54.122\n    - type: mrr_at_3\n      value: 51.235\n    - type: mrr_at_5\n      value: 52.415\n    - type: ndcg_at_1\n      value: 44.907000000000004\n    - type: ndcg_at_10\n      value: 45.446\n    - type: ndcg_at_100\n      value: 52.429\n    - type: ndcg_at_1000\n      value: 55.169000000000004\n    - type: ndcg_at_3\n      value: 41.882000000000005\n    - type: ndcg_at_5\n      value: 43.178\n    - type: precision_at_1\n      value: 44.907000000000004\n    - type: precision_at_10\n      value: 12.931999999999999\n    - type: precision_at_100\n      value: 2.025\n    - type: precision_at_1000\n      value: 0.248\n    - type: precision_at_3\n      value: 28.652\n    - type: precision_at_5\n      value: 21.204\n    - type: recall_at_1\n      value: 22.262999999999998\n    - type: recall_at_10\n      value: 52.447\n    - type: recall_at_100\n      value: 78.045\n    - type: recall_at_1000\n      value: 94.419\n    - type: recall_at_3\n      value: 38.064\n    - type: recall_at_5\n      value: 44.769\n  - task:\n      type: Retrieval\n    dataset:\n      type: hotpotqa\n      name: MTEB HotpotQA\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 32.519\n    - type: map_at_10\n      value: 45.831\n    - type: map_at_100\n      value: 46.815\n    - type: map_at_1000\n      value: 46.899\n    - type: map_at_3\n      value: 42.836\n    - type: map_at_5\n      value: 44.65\n    - type: mrr_at_1\n      value: 65.037\n    - type: mrr_at_10\n      value: 72.16\n    - type: mrr_at_100\n      value: 72.51100000000001\n    - type: mrr_at_1000\n      value: 72.53\n    - type: mrr_at_3\n      value: 70.682\n    - type: mrr_at_5\n      value: 71.54599999999999\n    - type: ndcg_at_1\n      value: 65.037\n    - type: ndcg_at_10\n      value: 55.17999999999999\n    - type: ndcg_at_100\n      value: 58.888\n    - type: ndcg_at_1000\n      value: 60.648\n    - type: ndcg_at_3\n      value: 50.501\n    - type: ndcg_at_5\n      value: 52.977\n    - type: precision_at_1\n      value: 65.037\n    - type: precision_at_10\n      value: 11.530999999999999\n    - type: precision_at_100\n      value: 1.4460000000000002\n    - type: precision_at_1000\n      value: 0.168\n    - type: precision_at_3\n      value: 31.483\n    - type: precision_at_5\n      value: 20.845\n    - type: recall_at_1\n      value: 32.519\n    - type: recall_at_10\n      value: 57.657000000000004\n    - type: recall_at_100\n      value: 72.30199999999999\n    - type: recall_at_1000\n      value: 84.024\n    - type: recall_at_3\n      value: 47.225\n    - type: recall_at_5\n      value: 52.113\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/imdb\n      name: MTEB ImdbClassification\n      config: default\n      split: test\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\n    metrics:\n    - type: accuracy\n      value: 88.3168\n    - type: ap\n      value: 83.80165516037135\n    - type: f1\n      value: 88.29942471066407\n  - task:\n      type: Retrieval\n    dataset:\n      type: msmarco\n      name: MTEB MSMARCO\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 20.724999999999998\n    - type: map_at_10\n      value: 32.736\n    - type: map_at_100\n      value: 33.938\n    - type: map_at_1000\n      value: 33.991\n    - type: map_at_3\n      value: 28.788000000000004\n    - type: map_at_5\n      value: 31.016\n    - type: mrr_at_1\n      value: 21.361\n    - type: mrr_at_10\n      value: 33.323\n    - type: mrr_at_100\n      value: 34.471000000000004\n    - type: mrr_at_1000\n      value: 34.518\n    - type: mrr_at_3\n      value: 29.453000000000003\n    - type: mrr_at_5\n      value: 31.629\n    - type: ndcg_at_1\n      value: 21.361\n    - type: ndcg_at_10\n      value: 39.649\n    - type: ndcg_at_100\n      value: 45.481\n    - type: ndcg_at_1000\n      value: 46.775\n    - type: ndcg_at_3\n      value: 31.594\n    - type: ndcg_at_5\n      value: 35.543\n    - type: precision_at_1\n      value: 21.361\n    - type: precision_at_10\n      value: 6.3740000000000006\n    - type: precision_at_100\n      value: 0.931\n    - type: precision_at_1000\n      value: 0.104\n    - type: precision_at_3\n      value: 13.514999999999999\n    - type: precision_at_5\n      value: 10.100000000000001\n    - type: recall_at_1\n      value: 20.724999999999998\n    - type: recall_at_10\n      value: 61.034\n    - type: recall_at_100\n      value: 88.062\n    - type: recall_at_1000\n      value: 97.86399999999999\n    - type: recall_at_3\n      value: 39.072\n    - type: recall_at_5\n      value: 48.53\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (en)\n      config: en\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 93.8919288645691\n    - type: f1\n      value: 93.57059586398059\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (en)\n      config: en\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 67.97993616051072\n    - type: f1\n      value: 48.244319183606535\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (en)\n      config: en\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.90047074646941\n    - type: f1\n      value: 66.48999056063725\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (en)\n      config: en\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.34566240753195\n    - type: f1\n      value: 73.54164154290658\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-p2p\n      name: MTEB MedrxivClusteringP2P\n      config: default\n      split: test\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\n    metrics:\n    - type: v_measure\n      value: 34.21866934757011\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-s2s\n      name: MTEB MedrxivClusteringS2S\n      config: default\n      split: test\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\n    metrics:\n    - type: v_measure\n      value: 32.000936217235534\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/mind_small\n      name: MTEB MindSmallReranking\n      config: default\n      split: test\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\n    metrics:\n    - type: map\n      value: 31.68189362520352\n    - type: mrr\n      value: 32.69603637784303\n  - task:\n      type: Retrieval\n    dataset:\n      type: nfcorpus\n      name: MTEB NFCorpus\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 6.078\n    - type: map_at_10\n      value: 12.671\n    - type: map_at_100\n      value: 16.291\n    - type: map_at_1000\n      value: 17.855999999999998\n    - type: map_at_3\n      value: 9.610000000000001\n    - type: map_at_5\n      value: 11.152\n    - type: mrr_at_1\n      value: 43.963\n    - type: mrr_at_10\n      value: 53.173\n    - type: mrr_at_100\n      value: 53.718999999999994\n    - type: mrr_at_1000\n      value: 53.756\n    - type: mrr_at_3\n      value: 50.980000000000004\n    - type: mrr_at_5\n      value: 52.42\n    - type: ndcg_at_1\n      value: 42.415000000000006\n    - type: ndcg_at_10\n      value: 34.086\n    - type: ndcg_at_100\n      value: 32.545\n    - type: ndcg_at_1000\n      value: 41.144999999999996\n    - type: ndcg_at_3\n      value: 39.434999999999995\n    - type: ndcg_at_5\n      value: 37.888\n    - type: precision_at_1\n      value: 43.653\n    - type: precision_at_10\n      value: 25.014999999999997\n    - type: precision_at_100\n      value: 8.594\n    - type: precision_at_1000\n      value: 2.169\n    - type: precision_at_3\n      value: 37.049\n    - type: precision_at_5\n      value: 33.065\n    - type: recall_at_1\n      value: 6.078\n    - type: recall_at_10\n      value: 16.17\n    - type: recall_at_100\n      value: 34.512\n    - type: recall_at_1000\n      value: 65.447\n    - type: recall_at_3\n      value: 10.706\n    - type: recall_at_5\n      value: 13.158\n  - task:\n      type: Retrieval\n    dataset:\n      type: nq\n      name: MTEB NQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 27.378000000000004\n    - type: map_at_10\n      value: 42.178\n    - type: map_at_100\n      value: 43.32\n    - type: map_at_1000\n      value: 43.358000000000004\n    - type: map_at_3\n      value: 37.474000000000004\n    - type: map_at_5\n      value: 40.333000000000006\n    - type: mrr_at_1\n      value: 30.823\n    - type: mrr_at_10\n      value: 44.626\n    - type: mrr_at_100\n      value: 45.494\n    - type: mrr_at_1000\n      value: 45.519\n    - type: mrr_at_3\n      value: 40.585\n    - type: mrr_at_5\n      value: 43.146\n    - type: ndcg_at_1\n      value: 30.794\n    - type: ndcg_at_10\n      value: 50.099000000000004\n    - type: ndcg_at_100\n      value: 54.900999999999996\n    - type: ndcg_at_1000\n      value: 55.69499999999999\n    - type: ndcg_at_3\n      value: 41.238\n    - type: ndcg_at_5\n      value: 46.081\n    - type: precision_at_1\n      value: 30.794\n    - type: precision_at_10\n      value: 8.549\n    - type: precision_at_100\n      value: 1.124\n    - type: precision_at_1000\n      value: 0.12\n    - type: precision_at_3\n      value: 18.926000000000002\n    - type: precision_at_5\n      value: 14.16\n    - type: recall_at_1\n      value: 27.378000000000004\n    - type: recall_at_10\n      value: 71.842\n    - type: recall_at_100\n      value: 92.565\n    - type: recall_at_1000\n      value: 98.402\n    - type: recall_at_3\n      value: 49.053999999999995\n    - type: recall_at_5\n      value: 60.207\n  - task:\n      type: Retrieval\n    dataset:\n      type: quora\n      name: MTEB QuoraRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 70.557\n    - type: map_at_10\n      value: 84.729\n    - type: map_at_100\n      value: 85.369\n    - type: map_at_1000\n      value: 85.382\n    - type: map_at_3\n      value: 81.72\n    - type: map_at_5\n      value: 83.613\n    - type: mrr_at_1\n      value: 81.3\n    - type: mrr_at_10\n      value: 87.488\n    - type: mrr_at_100\n      value: 87.588\n    - type: mrr_at_1000\n      value: 87.589\n    - type: mrr_at_3\n      value: 86.53\n    - type: mrr_at_5\n      value: 87.18599999999999\n    - type: ndcg_at_1\n      value: 81.28999999999999\n    - type: ndcg_at_10\n      value: 88.442\n    - type: ndcg_at_100\n      value: 89.637\n    - type: ndcg_at_1000\n      value: 89.70700000000001\n    - type: ndcg_at_3\n      value: 85.55199999999999\n    - type: ndcg_at_5\n      value: 87.154\n    - type: precision_at_1\n      value: 81.28999999999999\n    - type: precision_at_10\n      value: 13.489999999999998\n    - type: precision_at_100\n      value: 1.54\n    - type: precision_at_1000\n      value: 0.157\n    - type: precision_at_3\n      value: 37.553\n    - type: precision_at_5\n      value: 24.708\n    - type: recall_at_1\n      value: 70.557\n    - type: recall_at_10\n      value: 95.645\n    - type: recall_at_100\n      value: 99.693\n    - type: recall_at_1000\n      value: 99.995\n    - type: recall_at_3\n      value: 87.359\n    - type: recall_at_5\n      value: 91.89699999999999\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering\n      name: MTEB RedditClustering\n      config: default\n      split: test\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\n    metrics:\n    - type: v_measure\n      value: 63.65060114776209\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering-p2p\n      name: MTEB RedditClusteringP2P\n      config: default\n      split: test\n      revision: 282350215ef01743dc01b456c7f5241fa8937f16\n    metrics:\n    - type: v_measure\n      value: 64.63271250680617\n  - task:\n      type: Retrieval\n    dataset:\n      type: scidocs\n      name: MTEB SCIDOCS\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 4.263\n    - type: map_at_10\n      value: 10.801\n    - type: map_at_100\n      value: 12.888\n    - type: map_at_1000\n      value: 13.224\n    - type: map_at_3\n      value: 7.362\n    - type: map_at_5\n      value: 9.149000000000001\n    - type: mrr_at_1\n      value: 21\n    - type: mrr_at_10\n      value: 31.416\n    - type: mrr_at_100\n      value: 32.513\n    - type: mrr_at_1000\n      value: 32.58\n    - type: mrr_at_3\n      value: 28.116999999999997\n    - type: mrr_at_5\n      value: 29.976999999999997\n    - type: ndcg_at_1\n      value: 21\n    - type: ndcg_at_10\n      value: 18.551000000000002\n    - type: ndcg_at_100\n      value: 26.657999999999998\n    - type: ndcg_at_1000\n      value: 32.485\n    - type: ndcg_at_3\n      value: 16.834\n    - type: ndcg_at_5\n      value: 15.204999999999998\n    - type: precision_at_1\n      value: 21\n    - type: precision_at_10\n      value: 9.84\n    - type: precision_at_100\n      value: 2.16\n    - type: precision_at_1000\n      value: 0.35500000000000004\n    - type: precision_at_3\n      value: 15.667\n    - type: precision_at_5\n      value: 13.62\n    - type: recall_at_1\n      value: 4.263\n    - type: recall_at_10\n      value: 19.922\n    - type: recall_at_100\n      value: 43.808\n    - type: recall_at_1000\n      value: 72.14500000000001\n    - type: recall_at_3\n      value: 9.493\n    - type: recall_at_5\n      value: 13.767999999999999\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sickr-sts\n      name: MTEB SICK-R\n      config: default\n      split: test\n      revision: a6ea5a8cab320b040a23452cc28066d9beae2cee\n    metrics:\n    - type: cos_sim_spearman\n      value: 81.27446313317233\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts12-sts\n      name: MTEB STS12\n      config: default\n      split: test\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\n    metrics:\n    - type: cos_sim_spearman\n      value: 76.27963301217527\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts13-sts\n      name: MTEB STS13\n      config: default\n      split: test\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\n    metrics:\n    - type: cos_sim_spearman\n      value: 88.18495048450949\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts14-sts\n      name: MTEB STS14\n      config: default\n      split: test\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\n    metrics:\n    - type: cos_sim_spearman\n      value: 81.91982338692046\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts15-sts\n      name: MTEB STS15\n      config: default\n      split: test\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\n    metrics:\n    - type: cos_sim_spearman\n      value: 89.00896818385291\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts16-sts\n      name: MTEB STS16\n      config: default\n      split: test\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\n    metrics:\n    - type: cos_sim_spearman\n      value: 85.48814644586132\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-en)\n      config: en-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_spearman\n      value: 90.30116926966582\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (en)\n      config: en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_spearman\n      value: 67.74132963032342\n  - task:\n      type: STS\n    dataset:\n      type: mteb/stsbenchmark-sts\n      name: MTEB STSBenchmark\n      config: default\n      split: test\n      revision: b0fddb56ed78048fa8b90373c8a3cfc37b684831\n    metrics:\n    - type: cos_sim_spearman\n      value: 86.87741355780479\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/scidocs-reranking\n      name: MTEB SciDocsRR\n      config: default\n      split: test\n      revision: d3c5e1fc0b855ab6097bf1cda04dd73947d7caab\n    metrics:\n    - type: map\n      value: 82.0019012295875\n    - type: mrr\n      value: 94.70267024188593\n  - task:\n      type: Retrieval\n    dataset:\n      type: scifact\n      name: MTEB SciFact\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 50.05\n    - type: map_at_10\n      value: 59.36\n    - type: map_at_100\n      value: 59.967999999999996\n    - type: map_at_1000\n      value: 60.023\n    - type: map_at_3\n      value: 56.515\n    - type: map_at_5\n      value: 58.272999999999996\n    - type: mrr_at_1\n      value: 53\n    - type: mrr_at_10\n      value: 61.102000000000004\n    - type: mrr_at_100\n      value: 61.476\n    - type: mrr_at_1000\n      value: 61.523\n    - type: mrr_at_3\n      value: 58.778\n    - type: mrr_at_5\n      value: 60.128\n    - type: ndcg_at_1\n      value: 53\n    - type: ndcg_at_10\n      value: 64.43100000000001\n    - type: ndcg_at_100\n      value: 66.73599999999999\n    - type: ndcg_at_1000\n      value: 68.027\n    - type: ndcg_at_3\n      value: 59.279\n    - type: ndcg_at_5\n      value: 61.888\n    - type: precision_at_1\n      value: 53\n    - type: precision_at_10\n      value: 8.767\n    - type: precision_at_100\n      value: 1.01\n    - type: precision_at_1000\n      value: 0.11100000000000002\n    - type: precision_at_3\n      value: 23.444000000000003\n    - type: precision_at_5\n      value: 15.667\n    - type: recall_at_1\n      value: 50.05\n    - type: recall_at_10\n      value: 78.511\n    - type: recall_at_100\n      value: 88.5\n    - type: recall_at_1000\n      value: 98.333\n    - type: recall_at_3\n      value: 64.117\n    - type: recall_at_5\n      value: 70.867\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/sprintduplicatequestions-pairclassification\n      name: MTEB SprintDuplicateQuestions\n      config: default\n      split: test\n      revision: d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46\n    metrics:\n    - type: cos_sim_accuracy\n      value: 99.72178217821782\n    - type: cos_sim_ap\n      value: 93.0728601593541\n    - type: cos_sim_f1\n      value: 85.6727976766699\n    - type: cos_sim_precision\n      value: 83.02063789868667\n    - type: cos_sim_recall\n      value: 88.5\n    - type: dot_accuracy\n      value: 99.72178217821782\n    - type: dot_ap\n      value: 93.07287396168348\n    - type: dot_f1\n      value: 85.6727976766699\n    - type: dot_precision\n      value: 83.02063789868667\n    - type: dot_recall\n      value: 88.5\n    - type: euclidean_accuracy\n      value: 99.72178217821782\n    - type: euclidean_ap\n      value: 93.07285657982895\n    - type: euclidean_f1\n      value: 85.6727976766699\n    - type: euclidean_precision\n      value: 83.02063789868667\n    - type: euclidean_recall\n      value: 88.5\n    - type: manhattan_accuracy\n      value: 99.72475247524753\n    - type: manhattan_ap\n      value: 93.02792973059809\n    - type: manhattan_f1\n      value: 85.7727737973388\n    - type: manhattan_precision\n      value: 87.84067085953879\n    - type: manhattan_recall\n      value: 83.8\n    - type: max_accuracy\n      value: 99.72475247524753\n    - type: max_ap\n      value: 93.07287396168348\n    - type: max_f1\n      value: 85.7727737973388\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering\n      name: MTEB StackExchangeClustering\n      config: default\n      split: test\n      revision: 6cbc1f7b2bc0622f2e39d2c77fa502909748c259\n    metrics:\n    - type: v_measure\n      value: 68.77583615550819\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering-p2p\n      name: MTEB StackExchangeClusteringP2P\n      config: default\n      split: test\n      revision: 815ca46b2622cec33ccafc3735d572c266efdb44\n    metrics:\n    - type: v_measure\n      value: 36.151636938606956\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/stackoverflowdupquestions-reranking\n      name: MTEB StackOverflowDupQuestions\n      config: default\n      split: test\n      revision: e185fbe320c72810689fc5848eb6114e1ef5ec69\n    metrics:\n    - type: map\n      value: 52.16607939471187\n    - type: mrr\n      value: 52.95172046091163\n  - task:\n      type: Summarization\n    dataset:\n      type: mteb/summeval\n      name: MTEB SummEval\n      config: default\n      split: test\n      revision: cda12ad7615edc362dbf25a00fdd61d3b1eaf93c\n    metrics:\n    - type: cos_sim_pearson\n      value: 31.314646669495666\n    - type: cos_sim_spearman\n      value: 31.83562491439455\n    - type: dot_pearson\n      value: 31.314590842874157\n    - type: dot_spearman\n      value: 31.83363065810437\n  - task:\n      type: Retrieval\n    dataset:\n      type: trec-covid\n      name: MTEB TRECCOVID\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 0.198\n    - type: map_at_10\n      value: 1.3010000000000002\n    - type: map_at_100\n      value: 7.2139999999999995\n    - type: map_at_1000\n      value: 20.179\n    - type: map_at_3\n      value: 0.528\n    - type: map_at_5\n      value: 0.8019999999999999\n    - type: mrr_at_1\n      value: 72\n    - type: mrr_at_10\n      value: 83.39999999999999\n    - type: mrr_at_100\n      value: 83.39999999999999\n    - type: mrr_at_1000\n      value: 83.39999999999999\n    - type: mrr_at_3\n      value: 81.667\n    - type: mrr_at_5\n      value: 83.06700000000001\n    - type: ndcg_at_1\n      value: 66\n    - type: ndcg_at_10\n      value: 58.059000000000005\n    - type: ndcg_at_100\n      value: 44.316\n    - type: ndcg_at_1000\n      value: 43.147000000000006\n    - type: ndcg_at_3\n      value: 63.815999999999995\n    - type: ndcg_at_5\n      value: 63.005\n    - type: precision_at_1\n      value: 72\n    - type: precision_at_10\n      value: 61.4\n    - type: precision_at_100\n      value: 45.62\n    - type: precision_at_1000\n      value: 19.866\n    - type: precision_at_3\n      value: 70\n    - type: precision_at_5\n      value: 68.8\n    - type: recall_at_1\n      value: 0.198\n    - type: recall_at_10\n      value: 1.517\n    - type: recall_at_100\n      value: 10.587\n    - type: recall_at_1000\n      value: 41.233\n    - type: recall_at_3\n      value: 0.573\n    - type: recall_at_5\n      value: 0.907\n  - task:\n      type: Retrieval\n    dataset:\n      type: webis-touche2020\n      name: MTEB Touche2020\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 1.894\n    - type: map_at_10\n      value: 8.488999999999999\n    - type: map_at_100\n      value: 14.445\n    - type: map_at_1000\n      value: 16.078\n    - type: map_at_3\n      value: 4.589\n    - type: map_at_5\n      value: 6.019\n    - type: mrr_at_1\n      value: 22.448999999999998\n    - type: mrr_at_10\n      value: 39.82\n    - type: mrr_at_100\n      value: 40.752\n    - type: mrr_at_1000\n      value: 40.771\n    - type: mrr_at_3\n      value: 34.354\n    - type: mrr_at_5\n      value: 37.721\n    - type: ndcg_at_1\n      value: 19.387999999999998\n    - type: ndcg_at_10\n      value: 21.563\n    - type: ndcg_at_100\n      value: 33.857\n    - type: ndcg_at_1000\n      value: 46.199\n    - type: ndcg_at_3\n      value: 22.296\n    - type: ndcg_at_5\n      value: 21.770999999999997\n    - type: precision_at_1\n      value: 22.448999999999998\n    - type: precision_at_10\n      value: 19.796\n    - type: precision_at_100\n      value: 7.142999999999999\n    - type: precision_at_1000\n      value: 1.541\n    - type: precision_at_3\n      value: 24.490000000000002\n    - type: precision_at_5\n      value: 22.448999999999998\n    - type: recall_at_1\n      value: 1.894\n    - type: recall_at_10\n      value: 14.931\n    - type: recall_at_100\n      value: 45.524\n    - type: recall_at_1000\n      value: 83.243\n    - type: recall_at_3\n      value: 5.712\n    - type: recall_at_5\n      value: 8.386000000000001\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/toxic_conversations_50k\n      name: MTEB ToxicConversationsClassification\n      config: default\n      split: test\n      revision: d7c0de2777da35d6aae2200a62c6e0e5af397c4c\n    metrics:\n    - type: accuracy\n      value: 71.049\n    - type: ap\n      value: 13.85116971310922\n    - type: f1\n      value: 54.37504302487686\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/tweet_sentiment_extraction\n      name: MTEB TweetSentimentExtractionClassification\n      config: default\n      split: test\n      revision: d604517c81ca91fe16a244d1248fc021f9ecee7a\n    metrics:\n    - type: accuracy\n      value: 64.1312959818902\n    - type: f1\n      value: 64.11413877009383\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/twentynewsgroups-clustering\n      name: MTEB TwentyNewsgroupsClustering\n      config: default\n      split: test\n      revision: 6125ec4e24fa026cec8a478383ee943acfbd5449\n    metrics:\n    - type: v_measure\n      value: 54.13103431861502\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twittersemeval2015-pairclassification\n      name: MTEB TwitterSemEval2015\n      config: default\n      split: test\n      revision: 70970daeab8776df92f5ea462b6173c0b46fd2d1\n    metrics:\n    - type: cos_sim_accuracy\n      value: 87.327889372355\n    - type: cos_sim_ap\n      value: 77.42059895975699\n    - type: cos_sim_f1\n      value: 71.02706903250873\n    - type: cos_sim_precision\n      value: 69.75324344950394\n    - type: cos_sim_recall\n      value: 72.34828496042216\n    - type: dot_accuracy\n      value: 87.327889372355\n    - type: dot_ap\n      value: 77.4209479346677\n    - type: dot_f1\n      value: 71.02706903250873\n    - type: dot_precision\n      value: 69.75324344950394\n    - type: dot_recall\n      value: 72.34828496042216\n    - type: euclidean_accuracy\n      value: 87.327889372355\n    - type: euclidean_ap\n      value: 77.42096495861037\n    - type: euclidean_f1\n      value: 71.02706903250873\n    - type: euclidean_precision\n      value: 69.75324344950394\n    - type: euclidean_recall\n      value: 72.34828496042216\n    - type: manhattan_accuracy\n      value: 87.31000774870358\n    - type: manhattan_ap\n      value: 77.38930750711619\n    - type: manhattan_f1\n      value: 71.07935314027831\n    - type: manhattan_precision\n      value: 67.70957726295677\n    - type: manhattan_recall\n      value: 74.80211081794195\n    - type: max_accuracy\n      value: 87.327889372355\n    - type: max_ap\n      value: 77.42096495861037\n    - type: max_f1\n      value: 71.07935314027831\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/twitterurlcorpus-pairclassification\n      name: MTEB TwitterURLCorpus\n      config: default\n      split: test\n      revision: 8b6510b0b1fa4e4c4f879467980e9be563ec1cdf\n    metrics:\n    - type: cos_sim_accuracy\n      value: 89.58939729110878\n    - type: cos_sim_ap\n      value: 87.17594155025475\n    - type: cos_sim_f1\n      value: 79.21146953405018\n    - type: cos_sim_precision\n      value: 76.8918527109307\n    - type: cos_sim_recall\n      value: 81.67539267015707\n    - type: dot_accuracy\n      value: 89.58939729110878\n    - type: dot_ap\n      value: 87.17593963273593\n    - type: dot_f1\n      value: 79.21146953405018\n    - type: dot_precision\n      value: 76.8918527109307\n    - type: dot_recall\n      value: 81.67539267015707\n    - type: euclidean_accuracy\n      value: 89.58939729110878\n    - type: euclidean_ap\n      value: 87.17592466925834\n    - type: euclidean_f1\n      value: 79.21146953405018\n    - type: euclidean_precision\n      value: 76.8918527109307\n    - type: euclidean_recall\n      value: 81.67539267015707\n    - type: manhattan_accuracy\n      value: 89.62626615438352\n    - type: manhattan_ap\n      value: 87.16589873161546\n    - type: manhattan_f1\n      value: 79.25143598295348\n    - type: manhattan_precision\n      value: 76.39494177323712\n    - type: manhattan_recall\n      value: 82.32984293193716\n    - type: max_accuracy\n      value: 89.62626615438352\n    - type: max_ap\n      value: 87.17594155025475\n    - type: max_f1\n      value: 79.25143598295348\n---\n\n# hkunlp/instructor-large\nWe introduce **Instructor**ðŸ‘¨â€ðŸ«, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) ***by simply providing the task instruction, without any finetuning***. InstructorðŸ‘¨â€ achieves sota on 70 diverse embedding tasks ([MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard))!\nThe model is easy to use with **our customized** `sentence-transformer` library. For more details, check out [our paper](https://arxiv.org/abs/2212.09741) and [project page](https://instructor-embedding.github.io/)! \n\n**************************** **Updates** ****************************\n\n* 12/28: We released a new [checkpoint](https://huggingface.co/hkunlp/instructor-large) trained with hard negatives, which gives better performance.\n* 12/21: We released our [paper](https://arxiv.org/abs/2212.09741), [code](https://github.com/HKUNLP/instructor-embedding), [checkpoint](https://huggingface.co/hkunlp/instructor-large) and [project page](https://instructor-embedding.github.io/)! Check them out!\n\n## Quick start\n<hr />\n\n## Installation\n```bash\npip install InstructorEmbedding\n```\n\n## Compute your customized embeddings\nThen you can use the model like this to calculate domain-specific and task-aware embeddings:\n```python\nfrom InstructorEmbedding import INSTRUCTOR\nmodel = INSTRUCTOR(''hkunlp/instructor-large'')\nsentence = "3D ActionSLAM: wearable person tracking in multi-floor environments"\ninstruction = "Represent the Science title:"\nembeddings = model.encode([[instruction,sentence]])\nprint(embeddings)\n```\n\n## Use cases\n<hr />\n\n## Calculate embeddings for your customized texts\nIf you want to calculate customized embeddings for specific sentences, you may follow the unified template to write instructions: \n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Represent the `domain` `text_type` for `task_objective`:\n* `domain` is optional, and it specifies the domain of the text, e.g., science, finance, medicine, etc.\n* `text_type` is required, and it specifies the encoding unit, e.g., sentence, document, paragraph, etc.\n* `task_objective` is optional, and it specifies the objective of embedding, e.g., retrieve a document, classify the sentence, etc.\n\n## Calculate Sentence similarities\nYou can further use the model to compute similarities between two groups of sentences, with **customized embeddings**.\n```python\nfrom sklearn.metrics.pairwise import cosine_similarity\nsentences_a = [[''Represent the Science sentence: '',''Parton energy loss in QCD matter''], \n               [''Represent the Financial statement: '',''The Federal Reserve on Wednesday raised its benchmark interest rate.'']]\nsentences_b = [[''Represent the Science sentence: '',''The Chiral Phase Transition in Dissipative Dynamics''],\n               [''Represent the Financial statement: '',''The funds rose less than 0.5 per cent on Friday'']]\nembeddings_a = model.encode(sentences_a)\nembeddings_b = model.encode(sentences_b)\nsimilarities = cosine_similarity(embeddings_a,embeddings_b)\nprint(similarities)\n```\n\n## Information Retrieval\nYou can also use **customized embeddings** for information retrieval.\n```python\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nquery  = [[''Represent the Wikipedia question for retrieving supporting documents: '',''where is the food stored in a yam plant'']]\ncorpus = [[''Represent the Wikipedia document for retrieval: '',''Capitalism has been dominant in the Western world since the end of feudalism, but most feel[who?] that the term "mixed economies" more precisely describes most contemporary economies, due to their containing both private-owned and state-owned enterprises. In capitalism, prices determine the demand-supply scale. For example, higher demand for certain goods and services lead to higher prices and lower demand for certain goods lead to lower prices.''],\n          [''Represent the Wikipedia document for retrieval: '',"The disparate impact theory is especially controversial under the Fair Housing Act because the Act regulates many activities relating to housing, insurance, and mortgage loansÃ¢â‚¬â€and some scholars have argued that the theory''s use under the Fair Housing Act, combined with extensions of the Community Reinvestment Act, contributed to rise of sub-prime lending and the crash of the U.S. housing market and ensuing global economic recession"],\n          [''Represent the Wikipedia document for retrieval: '',''Disparate impact in United States labor law refers to practices in employment, housing, and other areas that adversely affect one group of people of a protected characteristic more than another, even though rules applied by employers or landlords are formally neutral. Although the protected classes vary by statute, most federal civil rights laws protect based on race, color, religion, national origin, and sex as protected traits, and some laws include disability status and other traits as well.'']]\nquery_embeddings = model.encode(query)\ncorpus_embeddings = model.encode(corpus)\nsimilarities = cosine_similarity(query_embeddings,corpus_embeddings)\nretrieved_doc_id = np.argmax(similarities)\nprint(retrieved_doc_id)\n```\n\n## Clustering\nUse **customized embeddings** for clustering texts in groups.\n```python\nimport sklearn.cluster\nsentences = [[''Represent the Medicine sentence for clustering: '',''Dynamical Scalar Degree of Freedom in Horava-Lifshitz Gravity''],\n             [''Represent the Medicine sentence for clustering: '',''Comparison of Atmospheric Neutrino Flux Calculations at Low Energies''],\n             [''Represent the Medicine sentence for clustering: '',''Fermion Bags in the Massive Gross-Neveu Model''],\n             [''Represent the Medicine sentence for clustering: '',"QCD corrections to Associated t-tbar-H production at the Tevatron"],\n             [''Represent the Medicine sentence for clustering: '',''A New Analysis of the R Measurements: Resonance Parameters of the Higher,  Vector States of Charmonium'']]\nembeddings = model.encode(sentences)\nclustering_model = sklearn.cluster.MiniBatchKMeans(n_clusters=2)\nclustering_model.fit(embeddings)\ncluster_assignment = clustering_model.labels_\nprint(cluster_assignment)\n```', '{"pipeline_tag":"sentence-similarity","library_name":"sentence-transformers","framework":"sentence-transformers","params":null,"storage_bytes":6712370802,"files_count":14,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["T5EncoderModel"],"model_type":"t5","tokenizer_config":{"eos_token":"</s>","pad_token":"<pad>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:HKUNLP:instructor-embedding","source_url":"https://github.com/HKUNLP/instructor-embedding"},{"type":"based_on_paper","target_id":"arxiv:2212.09741","source_url":"https://arxiv.org/abs/2212.09741"}]', NULL, 'Apache-2.0', 'approved', 77.2, '49ab2746474e1f57324bb946d0e14c63', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-YoungMasterFromSect-Trauter-LoRAs', 'huggingface--youngmasterfromsect--trauter-loras', 'Trauter_LoRAs', 'YoungMasterFromSect', '--- tags: - anime --- NOTICE: My LoRAs require high amount of tags to look good, I will fix this later on and update all of my LoRAs if everything works out. - Overview - Installation - Usage - SocialMedia - Plans for the future Welcome to the place where I host my LoRAs. In short, LoRA is just a checkpoint trained on specific artstyle/subject that you load into your WebUI, that can be used with other models. Although you can use it with any model, the effects of LoRA will vary between them. ...', '["anime","region:us"]', 'other', 522, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- anime\n---\nNOTICE: My LoRAs require high amount of tags to look good, I will fix this later on and update all of my LoRAs if everything works out.\n\n# General Information\n\n- [Overview](#overview)\n- [Installation](#installation)\n- [Usage](#usage)\n- [SocialMedia](#socialmedia)\n- [Plans for the future](#plans-for-the-future)\n\n# Overview\n\nWelcome to the place where I host my LoRAs. In short, LoRA is just a checkpoint trained on specific artstyle/subject that you load into your WebUI, that can be used with other models.  \nAlthough you can use it with any model, the effects of LoRA will vary between them.\nMost of the previews use models that come from [WarriorMama777](https://huggingface.co/WarriorMama777/OrangeMixs) .  \nFor more information about them, you can visit the original LoRA repository: https://github.com/cloneofsimo/lora  \nEvery images posted here, or on the other sites have metadata in them that you can use in PNG Info tab in your WebUI to get access to the prompt of the image.  \nEverything I do here is for free of charge!  \nI don''t guarantee that my LoRAs will give you good results, if you think they are bad, don''t use them.\n\n# Installation\n\nTo use them in your WebUI, please install the extension linked under, following the installation guide:  \nhttps://github.com/kohya-ss/sd-webui-additional-networks#installation\n\n# Usage\n\nAll of my LoRAs are to be used with their original danbooru tag. For example:  \n```\nasuna \(blue archive\)\n```\nMy LoRAs will have sufixes that will tell you how much they were trained. Either by using words like "soft" and "hard",  \nwhere soft stands for lower amount of training and hard for higher amount of training.  \n\nMore trained LoRA is harder to modify but provides higher consistency in details and original outfits,  \nwhile lower trained one will be more flexible, but may get details wrong.\n\nAll the LoRAs that aren''t marked with PRUNED require tagging everything about the character to get the likness of it.\nYou have to tag every part of the character like: eyes,hair,breasts,accessories,special features,etc...\n\nIn theory, this should allow LoRAs to be more flexible, but it requires to prompt those things always, because character tag doesn''t have those features baked into it.\nFrom 1/16 I will test releasing pruned versions which will not require those prompting those things.\n\nThe usage of them is also explained in this guide:  \nhttps://github.com/kohya-ss/sd-webui-additional-networks#how-to-use\n\n# SocialMedia\n\nHere are some places where you can find my other stuff that I post, or if you feel like buying me a coffee:  \n[Twitter](https://twitter.com/Trauter8)  \n[Pixiv](https://www.pixiv.net/en/users/88153216)  \n[Buymeacoffee](https://www.buymeacoffee.com/Trauter) \n\n# Plans for the future\n\n- Remake all of my LoRAs into pruned versions which will be more user friendly and easier to use, and use 768x768 res. for training and better Learning Rate\n- After finishing all of my LoRA that I want to make, go over the old ones and try to make them better.\n- Accept suggestions for almost every character.\n- Maybe get motivation to actually tag outfits.\n  \n# LoRAs\n\n- [Genshin Impact](#genshin-impact)\n  - [Eula](#eula)\n  - [Barbara](#barbara)\n  - [Diluc](#diluc)\n  - [Mona](#mona)\n  - [Rosaria](#rosaria)\n  - [Yae Miko](#yae-miko)\n  - [Raiden Shogun](#raiden-shogun)\n  - [Kujou Sara](#kujou-sara)\n  - [Shenhe](#shenhe)\n  - [Yelan](#yelan)\n  - [Jean](#jean)\n  - [Lisa](#lisa)\n  - [Zhongli](#zhongli)\n  - [Yoimiya](#yoimiya)\n- [Blue Archive](#blue-archive)\n  - [Rikuhachima Aru](#rikuhachima-aru)\n  - [Ichinose Asuna](#ichinose-asuna)\n- [Fate Grand Order](#fate-grand-order)\n  - [Minamoto-no-Raikou](#minamoto-no-raikou)\n- [Misc. Characters](#misc.-characters)\n  - [Aponia](#aponia)\n  - [Reisalin Stout](#reisalin-stout)\n- [Artstyles](#artstyles)\n  - [Pozer](#pozer)\n\n\n# Genshin Impact\n  \n  - # Eula\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/1.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/1.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305293076)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Eula)\n  - # Barbara\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/bar.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/bar.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305435137)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Barbara)\n  - # Diluc\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/dil.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/dil.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305427945)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Diluc)\n  - # Mona\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/mon.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/mon.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305428050)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Mona)\n  - # Rosaria\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/ros.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/ros.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305428015)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Rosaria)\n  - # Yae Miko\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/yae.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/yae.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305448948)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/yae%20miko)\n  - # Raiden Shogun\n  - [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/ra.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/ra.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, raiden shogun, 1girl, breasts, solo, cleavage, kimono, bangs, sash, mole, obi, tassel, blush, large breasts, purple eyes, japanese clothes, long hair, looking at viewer, hand on own chest, hair ornament, purple hair, bridal gauntlets, closed mouth, purple kimono, blue hair, mole under eye, shoulder armor, long sleeves, wide sleeves, mitsudomoe (shape), tomoe (symbol), cowboy shot\nNegative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name, from behind\nSteps: 30, Sampler: DPM++ 2M Karras, CFG scale: 4.5, Seed: 2544310848, Size: 704x384, Model hash: 2bba3136, Denoising strength: 0.5, Clip skip: 2, ENSD: 31337, Hires upscale: 2.05, Hires upscaler: 4x_foolhardy_Remacri\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305313633)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Raiden%20Shogun)\n  - # Kujou Sara\n  - [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/ku.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/ku.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, kujou sara, 1girl, solo, mask, gloves, bangs, bodysuit, gradient, sidelocks, signature, yellow eyes, bird mask, mask on head, looking at viewer, short hair, black hair, detached sleeves, simple background, japanese clothes, black gloves, black bodysuit, wide sleeves, white background, upper body, gradient background, closed mouth, hair ornament, artist name, elbow gloves\nNegative prompt: (worst quality, low quality:1.4)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 3966121353, Size: 512x768, Model hash: 931f9552, Denoising strength: 0.5, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires steps: 20, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305311498)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Kujou%20Sara)\n  - # Shenhe\n  - [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/sh.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/sh.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, shenhe \(genshin impact\), 1girl, solo, breasts, bodysuit, tassel, gloves, bangs, braid, outdoors, bird, jewelry, earrings, sky, breast curtain, long hair, hair over one eye, covered navel, blue eyes, looking at viewer, hair ornament, large breasts, shoulder cutout, clothing cutout, very long hair, hip vent, braided ponytail, partially fingerless gloves, black bodysuit, tassel earrings, black gloves, gold trim, cowboy shot, white hair\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 22, Sampler: DPM++ SDE Karras, CFG scale: 6.5, Seed: 573332187, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 2, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305307599)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Shenhe)\n  - # Yelan\n  - [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/10.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/10.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, yelan \(genshin impact\), 1girl, breasts, solo, bangs, armpits, smile, sky, cleavage, jewelry, gloves, jacket, dice, mole, cloud, grin, dress, blush, earrings, thighs, tassel, sleeveless, day, outdoors, large breasts, looking at viewer, green eyes, arms up, short hair, blue hair, vision (genshin impact), fur trim, white jacket, blue sky, mole on breast, arms behind head, bob cut, multicolored hair, black hair, fur-trimmed jacket, elbow gloves, bare shoulders, blue dress, parted lips, diagonal bangs, clothing cutout, pelvic curtain, asymmetrical gloves\nNegative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name\nSteps: 23, Sampler: DPM++ SDE Karras, CFG scale: 6.5, Seed: 575500509, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.58, Clip skip: 2, ENSD: 31337, Hires upscale: 2.4, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305296897)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Yelan)\n  - # Jean\n  - [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/333.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/333.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, jean \(genshin impact\), 1girl, breasts, solo, cleavage, strapless, smile, ponytail, bangs, jewelry, earrings, bow, capelet, signature, sidelocks, cape, corset, shiny, blonde hair, long hair, upper body, detached sleeves, purple eyes, hair between eyes, hair bow, parted lips, looking to the side, large breasts, detached collar, medium breasts, blue capelet, white background, black bow, blue eyes, bare shoulders, simple background\nNegative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name, (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 22, Sampler: DPM++ SDE Karras, CFG scale: 7.5, Seed: 32930253, Size: 512x768, Model hash: ffa7b160, Denoising strength: 0.59, Clip skip: 2, ENSD: 31337, Hires upscale: 1.85, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305307594)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Jean)\n  - # Lisa\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/lis.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/lis.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, lisa \(genshin impact\), 1girl, solo, hat, breasts, gloves, cleavage, flower, smile, bangs, dress, rose, jewelry, witch, capelet, green eyes, witch hat, brown hair, purple headwear, looking at viewer, white background, large breasts, long hair, simple background, black gloves, purple flower, hair between eyes, upper body, purple rose, parted lips, purple capelet, hat flower, multicolored dress, hair ornament, multicolored clothes, vision (genshin impact)\nNegative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name, worst quality, low quality, extra digits, loli, loli face\nSteps: 23, Sampler: DPM++ SDE Karras, CFG scale: 6.5, Seed: 350134479, Size: 512x768, Model hash: ffa7b160, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.85, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305290865)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Lisa)\n\n  - # Zhongli\n[<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/zho.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/zho.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, zhongli \(genshin  impact\), solo, 1boy, bangs, jewelry, tassel, earrings, ponytail, low ponytail, gloves, necktie, jacket, shirt, formal, petals, suit, makeup, eyeliner, eyeshadow, male focus, long hair, brown hair, multicolored hair, long sleeves, tassel earrings, single earring, collared shirt, hair between eyes, black gloves, closed mouth, yellow eyes, gradient hair, orange hair, simple background\nNegative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name, worst quality, low quality, extra digits, loli, loli face\nSteps: 22, Sampler: DPM++ SDE Karras, CFG scale: 7, Seed: 88418604, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.58, Clip skip: 2, ENSD: 31337, Hires upscale: 2, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305311423)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Zhongli)\n    \n  - # Yoimiya\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/Yoi.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/Yoi.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305448498)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Genshin-Impact/Yoimiya)\n\n# Blue Archive\n  - # Rikuhachima Aru\n  - [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/22.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/22.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\naru \(blue archive\), masterpiece, best quality, 1girl, solo, horns, skirt, gloves, shirt, halo, window, breasts, blush, sweatdrop, ribbon, coat, bangs, :d, smile, indoors, standing, plant, thighs, sweat, jacket, day, sunlight, long hair, white shirt, white gloves, black skirt, looking at viewer, open mouth, long sleeves, red ribbon, fur trim, neck ribbon, red hair, fur-trimmed coat, collared shirt, orange eyes, medium breasts, brown coat, hands up, side slit, coat on shoulders, v-shaped eyebrows, yellow eyes, potted plant, fur collar, shirt tucked in, demon horns, high-waist skirt, dress shirt\nNegative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name, (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 22, Sampler: DPM++ SDE Karras, CFG scale: 6.5, Seed: 1190296645, Size: 512x768, Model hash: ffa7b160, Denoising strength: 0.58, Clip skip: 2, ENSD: 31337, Hires upscale: 1.85, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305293051)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Blue-Archive/Rikuhachima%20Aru)\n  - # Ichinose Asuna\n  - [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/asu.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/asu.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nphotorealistic, (hyperrealistic:1.2), (extremely detailed CG unity 8k wallpaper), (ultra-detailed), (mature female:1.2), masterpiece, best quality, asuna \(blue archive\), 1girl, breasts, solo, gloves, pantyhose, ass, leotard, smile, tail, halo, grin, blush, bangs, sideboob, highleg, standing, mole, strapless, ribbon, thighs, animal ears, playboy bunny, rabbit ears, long hair, white gloves, very long hair, large breasts, high heels, blue leotard, hair over one eye, fake animal ears, blue eyes, looking at viewer, white footwear, rabbit tail, official alternate costume, full body, elbow gloves, simple background, white background, absurdly long hair, bare shoulders, detached collar, thighband pantyhose, leaning forward, highleg leotard, strapless leotard, hair ribbon, brown pantyhose, black pantyhose, mole on breast, light brown hair, brown hair, looking back, fake tail\nNegative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name, (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 22, Sampler: DPM++ SDE Karras, CFG scale: 6.5, Seed: 2052579935, Size: 512x768, Model hash: ffa7b160, Clip skip: 2, ENSD: 31337\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305292996)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Blue-Archive/Ichinose%20Asuna)\n# Fate Grand Order\n  - # Minamoto-no-Raikou\n  - [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/3.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/3.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmature female, masterpiece, best quality, minamoto no raikou \(fate\), 1girl, breasts, solo, bodysuit, gloves, bangs, smile, rope, heart, blush, thighs, armor, kote, long hair, purple hair, fingerless gloves, purple eyes, large breasts, very long hair, looking at viewer, parted bangs, ribbed sleeves, black gloves, arm guards, covered navel, low-tied long hair, purple bodysuit, japanese armor\nNegative prompt: lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts,signature, watermark, username, blurry, artist name, (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 22, Sampler: DPM++ SDE Karras, CFG scale: 7.5, Seed: 3383453781, Size: 512x768, Model hash: ffa7b160, Denoising strength: 0.59, Clip skip: 2, ENSD: 31337, Hires upscale: 2, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305290900)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Fate-Grand-Order/Minamoto-no-Raikou)\n\n# Misc. Characters\n  \n  - # Aponia\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/apo.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/apo.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305445819)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Misc.%20Characters/Aponia)\n\n  - # Reisalin Stout\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/ryza.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/ryza.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305448553)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Misc.%20Characters/reisalin%20stout)\n\n# Artstyles\n  \n  - # Pozer\n    [<img src="https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/art.png" width="512" height="768">](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/resolve/main/LoRA/Previews/art.png)\n<details>\n  <summary>Sample Prompt</summary>\n  <pre>\nmasterpiece, best quality, eula \(genshin impact\), 1girl, solo, thighhighs, weapon, gloves, breasts, sword, hairband, necktie, holding, leotard, bangs, greatsword, cape, thighs, boots, blue hair, looking at viewer, arms up, vision (genshin impact), medium breasts, holding sword, long sleeves, holding weapon, purple eyes, medium hair, copyright name, hair ornament, thigh boots, black leotard, black hairband, blue necktie, black thighhighs, yellow eyes, closed mouth\nNegative prompt: (worst quality, low quality, extra digits, loli, loli face:1.3)\nSteps: 20, Sampler: DPM++ SDE Karras, CFG scale: 8, Seed: 2010519914, Size: 512x768, Model hash: a87fd7da, Denoising strength: 0.57, Clip skip: 2, ENSD: 31337, Hires upscale: 1.8, Hires upscaler: Latent (nearest-exact)\n</pre>\n</details>\n\n    - [Examples](https://www.flickr.com/photos/197461145@N04/albums/72177720305445399)\n    - [Download](https://huggingface.co/YoungMasterFromSect/Trauter_LoRAs/tree/main/LoRA/Artstyles/Pozer)', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":12414717335,"files_count":136,"spaces_count":1,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:cloneofsimo:lora","source_url":"https://github.com/cloneofsimo/lora"},{"type":"has_code","target_id":"github:kohya-ss:sd-webui-additional-networks","source_url":"https://github.com/kohya-ss/sd-webui-additional-networks#installation"},{"type":"has_code","target_id":"github:kohya-ss:sd-webui-additional-networks","source_url":"https://github.com/kohya-ss/sd-webui-additional-networks#how-to-use"}]', NULL, NULL, 'pending', 67.2, '9e580948744255b03b864522a16e4638', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-liuhaotian-llava-v1.5-7b', 'huggingface--liuhaotian--llava-v1.5-7b', 'llava-v1.5-7b', 'liuhaotian', '--- inference: false pipeline_tag: image-text-to-text --- <br> <br> **Model type:** LLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data. It is an auto-regressive language model, based on the transformer architecture. **Model date:** LLaVA-v1.5-7B was trained in September 2023. **Paper or resources for more information:** https://llava-vl.github.io/ Llama 2 is licensed under the LLAMA 2 Community License, Copyright (c) Meta...', '["transformers","pytorch","llava","text-generation","image-text-to-text","region:us"]', 'image-text-to-text', 522, 449679, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/liuhaotian/llava-v1.5-7b","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ninference: false\npipeline_tag: image-text-to-text\n---\n\n<br>\n<br>\n\n# LLaVA Model Card\n\n## Model details\n\n**Model type:**\nLLaVA is an open-source chatbot trained by fine-tuning LLaMA/Vicuna on GPT-generated multimodal instruction-following data.\nIt is an auto-regressive language model, based on the transformer architecture.\n\n**Model date:**\nLLaVA-v1.5-7B was trained in September 2023.\n\n**Paper or resources for more information:**\nhttps://llava-vl.github.io/\n\n## License\nLlama 2 is licensed under the LLAMA 2 Community License, \nCopyright (c) Meta Platforms, Inc. All Rights Reserved.\n\n**Where to send questions or comments about the model:**\nhttps://github.com/haotian-liu/LLaVA/issues\n\n## Intended use\n**Primary intended uses:**\nThe primary use of LLaVA is research on large multimodal models and chatbots.\n\n**Primary intended users:**\nThe primary intended users of the model are researchers and hobbyists in computer vision, natural language processing, machine learning, and artificial intelligence.\n\n## Training dataset\n- 558K filtered image-text pairs from LAION/CC/SBU, captioned by BLIP.\n- 158K GPT-generated multimodal instruction-following data.\n- 450K academic-task-oriented VQA data mixture.\n- 40K ShareGPT data.\n\n## Evaluation dataset\nA collection of 12 benchmarks, including 5 academic VQA benchmarks and 7 recent benchmarks specifically proposed for instruction-following LMMs.', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":27087556472,"files_count":11,"spaces_count":59,"gated":false,"private":false,"config":{"architectures":["LlavaLlamaForCausalLM"],"model_type":"llava","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<s>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"</s>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"pad_token":null,"unk_token":{"__type":"AddedToken","content":"<unk>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false}}}}', '[]', '[{"type":"has_code","target_id":"github:haotian-liu:LLaVA","source_url":"https://github.com/haotian-liu/LLaVA"}]', NULL, NULL, 'pending', 37.2, '9d3546e718c23cfe5ea490281cf54adc', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-HKUSTAudio-Llasa-3B', 'huggingface--hkustaudio--llasa-3b', 'Llasa-3B', 'HKUSTAudio', '--- license: cc-by-nc-4.0 language: - zh - en base_model: - meta-llama/Llama-3.2-3B-Instruct tags: - Text-to-Speech pipeline_tag: text-to-speech --- **Update ï¼ˆ2025-05-10):** Sometimes I find that top_p=0.95 and temperature=0.9 produce more stable results. **Update (2025-02-13):** Add Llasa finetune instruction. **Update (2025-02-07):** Our paper has been released! LLaSA: Scaling Train-Time and Inference-Time Compute for LLaMA-based Speech Synthesis - **Train from Scratch**: If you want to tra...', '["safetensors","llama","text-to-speech","text-to-speech","zh","en","arxiv:2502.04128","base_model:meta-llama/llama-3.2-3b-instruct","license:cc-by-nc-4.0","region:us"]', 'text-to-speech', 522, 1714, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/HKUSTAudio/Llasa-3B","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: cc-by-nc-4.0\nlanguage:\n- zh\n- en\nbase_model:\n- meta-llama/Llama-3.2-3B-Instruct\ntags:\n- Text-to-Speech\npipeline_tag: text-to-speech\n---\n\n[![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2502.04128)\n\n**Update ï¼ˆ2025-05-10):** Sometimes I find that top_p=0.95 and temperature=0.9 produce more stable results.\n\n\n**Update (2025-02-13):** Add [Llasa finetune instruction](https://github.com/zhenye234/LLaSA_training/tree/main/finetune).\n\n\n**Update (2025-02-07):** Our paper has been released!\n\n\nLLaSA: Scaling Train-Time and Inference-Time Compute for LLaMA-based Speech Synthesis \n\n\n- **Train from Scratch**: If you want to train the model from scratch, use the [LLaSA Training Repository](https://github.com/zhenye234/LLaSA_training).\n\n- **Scale for Test-Time Computation**: If you want to experiment with scaling for test-time computation, use the [LLaSA Testing Repository](https://github.com/zhenye234/LLaSA_inference).\n\n## Model Information\nOur model, Llasa, is a text-to-speech (TTS) system that extends the text-based LLaMA (1B,3B, and 8B) language model by incorporating speech tokens from the XCodec2 codebook,\n which contains 65,536 tokens. We trained Llasa on a dataset comprising 250,000 hours of Chinese-English speech data.\n The model is capable of generating speech **either solely from input text or by utilizing a given speech prompt.**  \n\n The method is seamlessly compatible with the Llama framework, making training TTS similar as training LLM (convert audios into single-codebook tokens and simply view it as a special language). It opens the possiblity of existing method for compression, acceleration and finetuning for LLM to be applied. \n\n\n\n## How to use\nInstall [XCodec2](https://huggingface.co/HKUSTAudio/xcodec2).  \n\n**1. Speech synthesis solely from input text**\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport soundfile as sf\n\nllasa_3b =''HKUSTAudio/Llasa-3B''\n\ntokenizer = AutoTokenizer.from_pretrained(llasa_3b)\nmodel = AutoModelForCausalLM.from_pretrained(llasa_3b)\nmodel.eval() \nmodel.to(''cuda'')\n\nfrom xcodec2.modeling_xcodec2 import XCodec2Model\n \nmodel_path = "HKUSTAudio/xcodec2"  \n \nCodec_model = XCodec2Model.from_pretrained(model_path)\nCodec_model.eval().cuda()   \n\ninput_text = ''Dealing with family secrets is never easy. Yet, sometimes, omission is a form of protection, intending to safeguard some from the harsh truths. One day, I hope you understand the reasons behind my actions. Until then, Anna, please, bear with me.''\n# input_text = ''çªç„¶ï¼Œèº«è¾¹ä¸€é˜µç¬‘å£°ã€‚æˆ‘çœ‹ç€ä»–ä»¬ï¼Œæ„æ°”é£Žå‘åœ°æŒºç›´äº†èƒ¸è†›ï¼Œç”©äº†ç”©é‚£ç¨æ˜¾è‚‰æ„Ÿçš„åŒè‡‚ï¼Œè½»ç¬‘é“ï¼š"æˆ‘èº«ä¸Šçš„è‚‰ï¼Œæ˜¯ä¸ºäº†æŽ©é¥°æˆ‘çˆ†æ£šçš„é­…åŠ›ï¼Œå¦åˆ™ï¼Œå²‚ä¸å“åäº†ä½ ä»¬å‘¢ï¼Ÿ"''\ndef ids_to_speech_tokens(speech_ids):\n \n    speech_tokens_str = []\n    for speech_id in speech_ids:\n        speech_tokens_str.append(f"<|s_{speech_id}|>")\n    return speech_tokens_str\n\ndef extract_speech_ids(speech_tokens_str):\n \n    speech_ids = []\n    for token_str in speech_tokens_str:\n        if token_str.startswith(''<|s_'') and token_str.endswith(''|>''):\n            num_str = token_str[4:-2]\n\n            num = int(num_str)\n            speech_ids.append(num)\n        else:\n            print(f"Unexpected token: {token_str}")\n    return speech_ids\n\n#TTS start!\nwith torch.no_grad():\n \n    formatted_text = f"<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>"\n\n    # Tokenize the text\n    chat = [\n        {"role": "user", "content": "Convert the text to speech:" + formatted_text},\n        {"role": "assistant", "content": "<|SPEECH_GENERATION_START|>"}\n    ]\n\n    input_ids = tokenizer.apply_chat_template(\n        chat, \n        tokenize=True, \n        return_tensors=''pt'', \n        continue_final_message=True\n    )\n    input_ids = input_ids.to(''cuda'')\n    speech_end_id = tokenizer.convert_tokens_to_ids(''<|SPEECH_GENERATION_END|>'')\n\n    # Generate the speech autoregressively\n    outputs = model.generate(\n        input_ids,\n        max_length=2048,  # We trained our model with a max length of 2048\n        eos_token_id= speech_end_id ,\n        do_sample=True,    \n        top_p=1,           #  Adjusts the diversity of generated content\n        temperature=0.8,   #  Controls randomness in output\n    )\n    # Extract the speech tokens\n    generated_ids = outputs[0][input_ids.shape[1]:-1]\n\n    speech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)   \n\n    # Convert  token <|s_23456|> to int 23456 \n    speech_tokens = extract_speech_ids(speech_tokens)\n\n    speech_tokens = torch.tensor(speech_tokens).cuda().unsqueeze(0).unsqueeze(0)\n\n    # Decode the speech tokens to speech waveform\n    gen_wav = Codec_model.decode_code(speech_tokens) \n \n\nsf.write("gen.wav", gen_wav[0, 0, :].cpu().numpy(), 16000)\n```\n\n**2. Speech synthesis utilizing a given speech prompt**\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport soundfile as sf\n\nllasa_3b =''HKUSTAudio/Llasa-3B''\n\ntokenizer = AutoTokenizer.from_pretrained(llasa_3b)\nmodel = AutoModelForCausalLM.from_pretrained(llasa_3b)\nmodel.eval() \nmodel.to(''cuda'')\n\nfrom xcodec2.modeling_xcodec2 import XCodec2Model\n \nmodel_path = "HKUSTAudio/xcodec2"  \n \nCodec_model = XCodec2Model.from_pretrained(model_path)\nCodec_model.eval().cuda()   \n# only 16khz speech support!\nprompt_wav, sr = sf.read("å¤ªä¹™çœŸäºº.wav")   # you can find wav in Files\n#prompt_wav, sr = sf.read("Anna.wav") # English prompt\nprompt_wav = torch.from_numpy(prompt_wav).float().unsqueeze(0)  \n\nprompt_text ="å¯¹ï¼Œè¿™å°±æ˜¯æˆ‘ä¸‡äººæ•¬ä»°çš„å¤ªä¹™çœŸäººï¼Œè™½ç„¶æœ‰ç‚¹å©´å„¿è‚¥ï¼Œä½†ä¹ŸæŽ©ä¸ä½æˆ‘é€¼äººçš„å¸…æ°”ã€‚"\n#promt_text = "A chance to leave him alone, but... No. She just wanted to see him again. Anna, you don''t know how it feels to lose a sister. Anna, I''m sorry, but your father asked me not to tell you anything."\ntarget_text = ''çªç„¶ï¼Œèº«è¾¹ä¸€é˜µç¬‘å£°ã€‚æˆ‘çœ‹ç€ä»–ä»¬ï¼Œæ„æ°”é£Žå‘åœ°æŒºç›´äº†èƒ¸è†›ï¼Œç”©äº†ç”©é‚£ç¨æ˜¾è‚‰æ„Ÿçš„åŒè‡‚ï¼Œè½»ç¬‘é“ï¼š"æˆ‘èº«ä¸Šçš„è‚‰ï¼Œæ˜¯ä¸ºäº†æŽ©é¥°æˆ‘çˆ†æ£šçš„é­…åŠ›ï¼Œå¦åˆ™ï¼Œå²‚ä¸å“åäº†ä½ ä»¬å‘¢ï¼Ÿ"''\n#target_text = "Dealing with family secrets is never easy. Yet, sometimes, omission is a form of protection, intending to safeguard some from the harsh truths. One day, I hope you understand the reasons behind my actions. Until then, Anna, please, bear with me."\ninput_text = prompt_text   + target_text\n\ndef ids_to_speech_tokens(speech_ids):\n \n    speech_tokens_str = []\n    for speech_id in speech_ids:\n        speech_tokens_str.append(f"<|s_{speech_id}|>")\n    return speech_tokens_str\n\ndef extract_speech_ids(speech_tokens_str):\n \n    speech_ids = []\n    for token_str in speech_tokens_str:\n        if token_str.startswith(''<|s_'') and token_str.endswith(''|>''):\n            num_str = token_str[4:-2]\n\n            num = int(num_str)\n            speech_ids.append(num)\n        else:\n            print(f"Unexpected token: {token_str}")\n    return speech_ids\n\n#TTS start!\nwith torch.no_grad():\n    # Encode the prompt wav\n    vq_code_prompt = Codec_model.encode_code(input_waveform=prompt_wav)\n    print("Prompt Vq Code Shape:", vq_code_prompt.shape )   \n\n    vq_code_prompt = vq_code_prompt[0,0,:]\n    # Convert int 12345 to token <|s_12345|>\n    speech_ids_prefix = ids_to_speech_tokens(vq_code_prompt)\n\n    formatted_text = f"<|TEXT_UNDERSTANDING_START|>{input_text}<|TEXT_UNDERSTANDING_END|>"\n\n    # Tokenize the text and the speech prefix\n    chat = [\n        {"role": "user", "content": "Convert the text to speech:" + formatted_text},\n        {"role": "assistant", "content": "<|SPEECH_GENERATION_START|>" + ''''.join(speech_ids_prefix)}\n    ]\n\n    input_ids = tokenizer.apply_chat_template(\n        chat, \n        tokenize=True, \n        return_tensors=''pt'', \n        continue_final_message=True\n    )\n    input_ids = input_ids.to(''cuda'')\n    speech_end_id = tokenizer.convert_tokens_to_ids(''<|SPEECH_GENERATION_END|>'')\n\n    # Generate the speech autoregressively\n    outputs = model.generate(\n        input_ids,\n        max_length=2048,  # We trained our model with a max length of 2048\n        eos_token_id= speech_end_id ,\n        do_sample=True,\n        top_p=1,           \n        temperature=0.8,\n    )\n    # Extract the speech tokens\n    generated_ids = outputs[0][input_ids.shape[1]-len(speech_ids_prefix):-1]\n\n    speech_tokens = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)   \n\n    # Convert  token <|s_23456|> to int 23456 \n    speech_tokens = extract_speech_ids(speech_tokens)\n\n    speech_tokens = torch.tensor(speech_tokens).cuda().unsqueeze(0).unsqueeze(0)\n\n    # Decode the speech tokens to speech waveform\n    gen_wav = Codec_model.decode_code(speech_tokens) \n\n    # if only need the generated part\n    # gen_wav = gen_wav[:,:,prompt_wav.shape[1]:]\n\nsf.write("gen.wav", gen_wav[0, 0, :].cpu().numpy(), 16000)\n```\n\n\n## Disclaimer\n\nThis model is licensed under the CC BY-NC 4.0 License, which prohibits free commercial use because of ethics and privacy concerns; detected violations will result in legal consequences.\n\nThis codebase is strictly prohibited from being used for any illegal purposes in any country or region. Please refer to your local laws about DMCA and other related laws.', '{"pipeline_tag":"text-to-speech","library_name":null,"framework":null,"params":4009454592,"storage_bytes":8060170434,"files_count":13,"spaces_count":26,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama"}}', '[]', '[{"type":"has_code","target_id":"github:zhenye234:LLaSA_training","source_url":"https://github.com/zhenye234/LLaSA_training"},{"type":"has_code","target_id":"github:zhenye234:LLaSA_training","source_url":"https://github.com/zhenye234/LLaSA_training"},{"type":"has_code","target_id":"github:zhenye234:LLaSA_inference","source_url":"https://github.com/zhenye234/LLaSA_inference"},{"type":"based_on_paper","target_id":"arxiv:2502.04128","source_url":"https://arxiv.org/abs/2502.04128"}]', NULL, 'CC-BY-NC-4.0', 'approved', 62.2, '480685c99844a6278e864ae7c067f2ef', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-VL-8B-Instruct', 'huggingface--qwen--qwen3-vl-8b-instruct', 'Qwen3-VL-8B-Instruct', 'Qwen', '--- license: apache-2.0 pipeline_tag: image-text-to-text library_name: transformers --- <a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> Meet Qwen3-VL â€” the most powerful vision-language model in the Qwen series to date. This generation delivers comprehensive upgrades across the board: superior text understanding ...', '["transformers","safetensors","qwen3_vl","image-to-text","image-text-to-text","conversational","arxiv:2505.09388","arxiv:2502.13923","arxiv:2409.12191","arxiv:2308.12966","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'image-text-to-text', 522, 2165060, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\npipeline_tag: image-text-to-text\nlibrary_name: transformers\n---\n<a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n\n# Qwen3-VL-8B-Instruct\n\n\nMeet Qwen3-VL â€” the most powerful vision-language model in the Qwen series to date.\n\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\n\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoningâ€‘enhanced Thinking editions for flexible, onâ€‘demand deployment.\n\n\n#### Key Enhancements:\n\n* **Visual Agent**: Operates PC/mobile GUIsâ€”recognizes elements, understands functions, invokes tools, completes tasks.\n\n* **Visual Coding Boost**: Generates Draw.io/HTML/CSS/JS from images/videos.\n\n* **Advanced Spatial Perception**: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\n\n* **Long Context & Video Understanding**: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\n\n* **Enhanced Multimodal Reasoning**: Excels in STEM/Mathâ€”causal analysis and logical, evidence-based answers.\n\n* **Upgraded Visual Recognition**: Broader, higher-quality pretraining is able to â€œrecognize everythingâ€â€”celebrities, anime, products, landmarks, flora/fauna, etc.\n\n* **Expanded OCR**: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\n\n* **Text Understanding on par with pure LLMs**: Seamless textâ€“vision fusion for lossless, unified comprehension.\n\n\n#### Model Architecture Updates:\n\n<p align="center">\n    <img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_arc.jpg" width="80%"/>\n<p>\n\n\n1. **Interleaved-MRoPE**: Fullâ€‘frequency allocation over time, width, and height via robust positional embeddings, enhancing longâ€‘horizon video reasoning.\n\n2. **DeepStack**: Fuses multiâ€‘level ViT features to capture fineâ€‘grained details and sharpen imageâ€“text alignment.\n\n3. **Textâ€“Timestamp Alignment:** Moves beyond Tâ€‘RoPE to precise, timestampâ€‘grounded event localization for stronger video temporal modeling.\n\nThis is the weight repository for Qwen3-VL-8B-Instruct.\n\n\n---\n\n## Model Performance\n\n**Multimodal performance**\n\n![](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_4b_8b_vl_instruct.jpg)\n\n**Pure text performance**\n![](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-VL/qwen3vl_4b_8b_text_instruct.jpg)\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen3-VL with ðŸ¤– ModelScope and ðŸ¤— Transformers.\n\nThe code of Qwen3-VL has been in the latest Hugging Face transformers and we advise you to build from source with command:\n```\npip install git+https://github.com/huggingface/transformers\n# pip install transformers==4.57.0 # currently, V4.57.0 is not released\n```\n\n### Using ðŸ¤— Transformers to Chat\n\nHere we show a code snippet to show how to use the chat model with `transformers`:\n\n```python\nfrom transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n\n# default: Load the model on the available device(s)\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n    "Qwen/Qwen3-VL-8B-Instruct", dtype="auto", device_map="auto"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen3VLForConditionalGeneration.from_pretrained(\n#     "Qwen/Qwen3-VL-8B-Instruct",\n#     dtype=torch.bfloat16,\n#     attn_implementation="flash_attention_2",\n#     device_map="auto",\n# )\n\nprocessor = AutoProcessor.from_pretrained("Qwen/Qwen3-VL-8B-Instruct")\n\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image",\n                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",\n            },\n            {"type": "text", "text": "Describe this image."},\n        ],\n    }\n]\n\n# Preparation for inference\ninputs = processor.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_dict=True,\n    return_tensors="pt"\n)\ninputs = inputs.to(model.device)\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\n### Generation Hyperparameters\n#### VL\n```bash\nexport greedy=''false''\nexport top_p=0.8\nexport top_k=20\nexport temperature=0.7\nexport repetition_penalty=1.0\nexport presence_penalty=1.5\nexport out_seq_length=16384\n```\n\n#### Text\n```bash\nexport greedy=''false''\nexport top_p=1.0\nexport top_k=40\nexport repetition_penalty=1.0\nexport presence_penalty=2.0\nexport temperature=1.0\nexport out_seq_length=32768\n```\n\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n\n@article{Qwen2.5-VL,\n  title={Qwen2.5-VL Technical Report},\n  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\n  journal={arXiv preprint arXiv:2502.13923},\n  year={2025}\n}\n\n@article{Qwen2VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model''s Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":8767123696,"storage_bytes":17534339512,"files_count":16,"spaces_count":37,"gated":false,"private":false,"config":{"architectures":["Qwen3VLForConditionalGeneration"],"model_type":"qwen3_vl","processor_config":{"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {%- if messages[0].content is string %}\n            {{- messages[0].content }}\n        {%- else %}\n            {%- for content in messages[0].content %}\n                {%- if ''text'' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' }}\n        {%- if messages[0].content is string %}\n            {{- messages[0].content }}\n        {%- else %}\n            {%- for content in messages[0].content %}\n                {%- if ''text'' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set image_count = namespace(value=0) %}\n{%- set video_count = namespace(value=0) %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- else %}\n            {%- for content in message.content %}\n                {%- if content.type == ''image'' or ''image'' in content or ''image_url'' in content %}\n                    {%- set image_count.value = image_count.value + 1 %}\n                    {%- if add_vision_id %}Picture {{ image_count.value }}: {% endif -%}\n                    <|vision_start|><|image_pad|><|vision_end|>\n                {%- elif content.type == ''video'' or ''video'' in content %}\n                    {%- set video_count.value = video_count.value + 1 %}\n                    {%- if add_vision_id %}Video {{ video_count.value }}: {% endif -%}\n                    <|vision_start|><|video_pad|><|vision_end|>\n                {%- elif ''text'' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- else %}\n            {%- for content_item in message.content %}\n                {%- if ''text'' in content_item %}\n                    {{- content_item.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and message.content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- else %}\n            {%- for content in message.content %}\n                {%- if content.type == ''image'' or ''image'' in content or ''image_url'' in content %}\n                    {%- set image_count.value = image_count.value + 1 %}\n                    {%- if add_vision_id %}Picture {{ image_count.value }}: {% endif -%}\n                    <|vision_start|><|image_pad|><|vision_end|>\n                {%- elif content.type == ''video'' or ''video'' in content %}\n                    {%- set video_count.value = video_count.value + 1 %}\n                    {%- if add_vision_id %}Video {{ video_count.value }}: {% endif -%}\n                    <|vision_start|><|video_pad|><|vision_end|>\n                {%- elif ''text'' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n"},"tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {%- if messages[0].content is string %}\n            {{- messages[0].content }}\n        {%- else %}\n            {%- for content in messages[0].content %}\n                {%- if ''text'' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' }}\n        {%- if messages[0].content is string %}\n            {{- messages[0].content }}\n        {%- else %}\n            {%- for content in messages[0].content %}\n                {%- if ''text'' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set image_count = namespace(value=0) %}\n{%- set video_count = namespace(value=0) %}\n{%- for message in messages %}\n    {%- if message.role == \"user\" %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- else %}\n            {%- for content in message.content %}\n                {%- if content.type == ''image'' or ''image'' in content or ''image_url'' in content %}\n                    {%- set image_count.value = image_count.value + 1 %}\n                    {%- if add_vision_id %}Picture {{ image_count.value }}: {% endif -%}\n                    <|vision_start|><|image_pad|><|vision_end|>\n                {%- elif content.type == ''video'' or ''video'' in content %}\n                    {%- set video_count.value = video_count.value + 1 %}\n                    {%- if add_vision_id %}Video {{ video_count.value }}: {% endif -%}\n                    <|vision_start|><|video_pad|><|vision_end|>\n                {%- elif ''text'' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- else %}\n            {%- for content_item in message.content %}\n                {%- if ''text'' in content_item %}\n                    {{- content_item.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and message.content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {%- if message.content is string %}\n            {{- message.content }}\n        {%- else %}\n            {%- for content in message.content %}\n                {%- if content.type == ''image'' or ''image'' in content or ''image_url'' in content %}\n                    {%- set image_count.value = image_count.value + 1 %}\n                    {%- if add_vision_id %}Picture {{ image_count.value }}: {% endif -%}\n                    <|vision_start|><|image_pad|><|vision_end|>\n                {%- elif content.type == ''video'' or ''video'' in content %}\n                    {%- set video_count.value = video_count.value + 1 %}\n                    {%- if add_vision_id %}Video {{ video_count.value }}: {% endif -%}\n                    <|vision_start|><|video_pad|><|vision_end|>\n                {%- elif ''text'' in content %}\n                    {{- content.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"based_on_paper","target_id":"arxiv:2505.09388","source_url":"https://arxiv.org/abs/2505.09388"},{"type":"based_on_paper","target_id":"arxiv:2502.13923","source_url":"https://arxiv.org/abs/2502.13923"},{"type":"based_on_paper","target_id":"arxiv:2409.12191","source_url":"https://arxiv.org/abs/2409.12191"},{"type":"based_on_paper","target_id":"arxiv:2308.12966","source_url":"https://arxiv.org/abs/2308.12966"}]', NULL, 'Apache-2.0', 'approved', 62.2, '9cac3b35bff1706aee98d51463e129e8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-tensorflow-tensorflow', 'github--tensorflow--tensorflow', 'tensorflow', 'tensorflow', '<div align="center"> <img src="https://www.tensorflow.org/images/tf_logo_horizontal.png"> </div> **** | ------------------- | | TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries, and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications. TensorFlow was originally developed by researchers and engineers working within the Machine ...', '["deep-learning","deep-neural-networks","distributed","machine-learning","ml","neural-network","python","tensorflow","c++"]', 'other', 192706, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/tensorflow/tensorflow","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n  <img src="https://www.tensorflow.org/images/tf_logo_horizontal.png">\n</div>\n\n[![Python](https://img.shields.io/pypi/pyversions/tensorflow.svg)](https://badge.fury.io/py/tensorflow)\n[![PyPI](https://badge.fury.io/py/tensorflow.svg)](https://badge.fury.io/py/tensorflow)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4724125.svg)](https://doi.org/10.5281/zenodo.4724125)\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1486/badge)](https://bestpractices.coreinfrastructure.org/projects/1486)\n[![OpenSSF Scorecard](https://api.securityscorecards.dev/projects/github.com/tensorflow/tensorflow/badge)](https://securityscorecards.dev/viewer/?uri=github.com/tensorflow/tensorflow)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow)\n[![Fuzzing Status](https://oss-fuzz-build-logs.storage.googleapis.com/badges/tensorflow-py.svg)](https://bugs.chromium.org/p/oss-fuzz/issues/list?sort=-opened&can=1&q=proj:tensorflow-py)\n[![OSSRank](https://shields.io/endpoint?url=https://ossrank.com/shield/44)](https://ossrank.com/p/44)\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-v1.4%20adopted-ff69b4.svg)](CODE_OF_CONDUCT.md)\n\n**`Documentation`** |\n------------------- |\n[![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/) |\n\n[TensorFlow](https://www.tensorflow.org/) is an end-to-end open source platform\nfor machine learning. It has a comprehensive, flexible ecosystem of\n[tools](https://www.tensorflow.org/resources/tools),\n[libraries](https://www.tensorflow.org/resources/libraries-extensions), and\n[community](https://www.tensorflow.org/community) resources that lets\nresearchers push the state-of-the-art in ML and developers easily build and\ndeploy ML-powered applications.\n\nTensorFlow was originally developed by researchers and engineers working within\nthe Machine Intelligence team at Google Brain to conduct research in machine\nlearning and neural networks. However, the framework is versatile enough to be\nused in other areas as well.\n\nTensorFlow provides stable [Python](https://www.tensorflow.org/api_docs/python)\nand [C++](https://www.tensorflow.org/api_docs/cc) APIs, as well as a\nnon-guaranteed backward compatible API for\n[other languages](https://www.tensorflow.org/api_docs).\n\nKeep up-to-date with release announcements and security updates by subscribing\nto\n[announce@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/announce).\nSee all the [mailing lists](https://www.tensorflow.org/community/forums).\n\n## Install\n\nSee the [TensorFlow install guide](https://www.tensorflow.org/install) for the\n[pip package](https://www.tensorflow.org/install/pip), to\n[enable GPU support](https://www.tensorflow.org/install/gpu), use a\n[Docker container](https://www.tensorflow.org/install/docker), and\n[build from source](https://www.tensorflow.org/install/source).\n\nTo install the current release, which includes support for\n[CUDA-enabled GPU cards](https://www.tensorflow.org/install/gpu) *(Ubuntu and\nWindows)*:\n\n```\n$ pip install tensorflow\n```\n\nOther devices (DirectX and MacOS-metal) are supported using\n[Device Plugins](https://www.tensorflow.org/install/gpu_plugins#available_devices).\n\nA smaller CPU-only package is also available:\n\n```\n$ pip install tensorflow-cpu\n```\n\nTo update TensorFlow to the latest version, add `--upgrade` flag to the above\ncommands.\n\n*Nightly binaries are available for testing using the\n[tf-nightly](https://pypi.python.org/pypi/tf-nightly) and\n[tf-nightly-cpu](https://pypi.python.org/pypi/tf-nightly-cpu) packages on PyPI.*\n\n#### *Try your first TensorFlow program*\n\n```shell\n$ python\n```\n\n```python\n>>> import tensorflow as tf\n>>> tf.add(1, 2).numpy()\n3\n>>> hello = tf.constant(''Hello, TensorFlow!'')\n>>> hello.numpy()\nb''Hello, TensorFlow!''\n```\n\nFor more examples, see the\n[TensorFlow Tutorials](https://www.tensorflow.org/tutorials/).\n\n## Contribution guidelines\n\n**If you want to contribute to TensorFlow, be sure to review the\n[Contribution Guidelines](CONTRIBUTING.md). This project adheres to TensorFlow''s\n[Code of Conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.**\n\n**We use [GitHub Issues](https://github.com/tensorflow/tensorflow/issues) for\ntracking requests and bugs, please see\n[TensorFlow Forum](https://discuss.tensorflow.org/) for general questions and\ndiscussion, and please direct specific questions to\n[Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).**\n\nThe TensorFlow project strives to abide by generally accepted best practices in\nopen-source software development.\n\n## Patching guidelines\n\nFollow these steps to patch a specific version of TensorFlow, for example, to\napply fixes to bugs or security vulnerabilities:\n\n*   Clone the TensorFlow repository and switch to the appropriate branch for\n    your desired versionâ€”for example, `r2.8` for version 2.8.\n*   Apply the desired changes (i.e., cherry-pick them) and resolve any code\n    conflicts.\n*   Run TensorFlow tests and ensure they pass.\n*   [Build](https://www.tensorflow.org/install/source) the TensorFlow pip\n    package from source.\n\n## Continuous build status\n\nYou can find more community-supported platforms and configurations in the\n[TensorFlow SIG Build Community Builds Table](https://github.com/tensorflow/build#community-supported-tensorflow-builds).\n\n### Official Builds\n\nBuild Type                    | Status                                                                                                                                                                           | Artifacts\n----------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------\n**Linux CPU**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html)           | [PyPI](https://pypi.org/project/tf-nightly/)\n**Linux GPU**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html) | [PyPI](https://pypi.org/project/tf-nightly-gpu/)\n**Linux XLA**                 | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html)         | TBA\n**macOS**                     | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html)     | [PyPI](https://pypi.org/project/tf-nightly/)\n**Windows CPU**               | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html)       | [PyPI](https://pypi.org/project/tf-nightly/)\n**Windows GPU**               | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html)       | [PyPI](https://pypi.org/project/tf-nightly-gpu/)\n**Android**                   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html)               | [Download](https://bintray.com/google/tensorflow/tensorflow/_latestVersion)\n**Raspberry Pi 0 and 1**      | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html)           | [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl)\n**Raspberry Pi 2 and 3**      | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html)           | [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl)\n**Libtensorflow MacOS CPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/macos/latest/macos_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Linux CPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/cpu/ubuntu_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Linux GPU**   | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/ubuntu_16/latest/gpu/ubuntu_gpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Windows CPU** | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/cpu/windows_cpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n**Libtensorflow Windows GPU** | Status Temporarily Unavailable                                                                                                                                                   | [Nightly Binary](https://storage.googleapis.com/libtensorflow-nightly/prod/tensorflow/release/windows/latest/gpu/windows_gpu_libtensorflow_binaries.tar.gz) [Official GCS](https://storage.googleapis.com/tensorflow/)\n\n## Resources\n\n*   [TensorFlow.org](https://www.tensorflow.org)\n*   [TensorFlow Tutorials](https://www.tensorflow.org/tutorials/)\n*   [TensorFlow Official Models](https://github.com/tensorflow/models/tree/master/official)\n*   [TensorFlow Examples](https://github.com/tensorflow/examples)\n*   [TensorFlow Codelabs](https://codelabs.developers.google.com/?cat=TensorFlow)\n*   [TensorFlow Blog](https://blog.tensorflow.org)\n*   [Learn ML with TensorFlow](https://www.tensorflow.org/resources/learn-ml)\n*   [TensorFlow Twitter](https://twitter.com/tensorflow)\n*   [TensorFlow YouTube](https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ)\n*   [TensorFlow model optimization roadmap](https://www.tensorflow.org/model_optimization/guide/roadmap)\n*   [TensorFlow White Papers](https://www.tensorflow.org/about/bib)\n*   [TensorBoard Visualization Toolkit](https://github.com/tensorflow/tensorboard)\n*   [TensorFlow Code Search](https://cs.opensource.google/tensorflow/tensorflow)\n\nLearn more about the\n[TensorFlow Community](https://www.tensorflow.org/community) and how to\n[Contribute](https://www.tensorflow.org/community/contribute).\n\n## Courses\n\n* [Coursera](https://www.coursera.org/search?query=TensorFlow)\n* [Udacity](https://www.udacity.com/courses/all?search=TensorFlow)\n* [Edx](https://www.edx.org/search?q=TensorFlow)\n\n## License\n\n[Apache License 2.0](LICENSE)\n', '{"language":"C++","stars":192706,"forks":75032,"watchers":192706,"open_issues":2737,"topics":["deep-learning","deep-neural-networks","distributed","machine-learning","ml","neural-network","python","tensorflow"],"default_branch":"master","size_kb":1232123,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:tensorflow:tensorflow","source_url":"https://github.com/tensorflow/tensorflow"},{"type":"has_code","target_id":"github:tensorflow:build","source_url":"https://github.com/tensorflow/build#community-supported-tensorflow-builds"},{"type":"has_code","target_id":"github:tensorflow:models","source_url":"https://github.com/tensorflow/models"},{"type":"has_code","target_id":"github:tensorflow:examples","source_url":"https://github.com/tensorflow/examples"},{"type":"has_code","target_id":"github:tensorflow:tensorboard","source_url":"https://github.com/tensorflow/tensorboard"}]', NULL, 'Apache-2.0', 'approved', 80, '5602baa1a7add8fb8514019a9a31004c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-tensorflow-tensorflow from https://github.com/tensorflow.png
Image converted to WebP: data/images/github-tensorflow-tensorflow.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-huggingface-transformers', 'github--huggingface--transformers', 'transformers', 'huggingface', '<!--- Copyright 2020 The HuggingFace Team. All rights reserved. Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the Licens...', '["audio","deep-learning","deepseek","gemma","glm","hacktoberfest","llm","machine-learning","model-hub","natural-language-processing","nlp","pretrained-models","python","pytorch","pytorch-transformers","qwen","speech-recognition","transformer","vlm","python"]', 'other', 153588, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/huggingface/transformers","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the "License");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an "AS IS" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align="center">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg">\n    <source media="(prefers-color-scheme: light)" srcset="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg">\n    <img alt="Hugging Face Transformers Library" src="https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg" width="352" height="59" style="max-width: 100%;">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align="center">\n    <a href="https://huggingface.com/models"><img alt="Checkpoints on Hub" src="https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen"></a>\n    <a href="https://circleci.com/gh/huggingface/transformers"><img alt="Build" src="https://img.shields.io/circleci/build/github/huggingface/transformers/main"></a>\n    <a href="https://github.com/huggingface/transformers/blob/main/LICENSE"><img alt="GitHub" src="https://img.shields.io/github/license/huggingface/transformers.svg?color=blue"></a>\n    <a href="https://huggingface.co/docs/transformers/index"><img alt="Documentation" src="https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online"></a>\n    <a href="https://github.com/huggingface/transformers/releases"><img alt="GitHub release" src="https://img.shields.io/github/release/huggingface/transformers.svg"></a>\n    <a href="https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img alt="Contributor Covenant" src="https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg"></a>\n    <a href="https://zenodo.org/badge/latestdoi/155220641"><img src="https://zenodo.org/badge/155220641.svg" alt="DOI"></a>\n</p>\n\n<h4 align="center">\n    <p>\n        <b>English</b> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md">ç®€ä½“ä¸­æ–‡</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md">ç¹é«”ä¸­æ–‡</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md">í•œêµ­ì–´</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">EspaÃ±ol</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">æ—¥æœ¬èªž</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md">à¤¹à¤¿à¤¨à¥à¤¦à¥€</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md">Ð ÑƒÑÑÐºÐ¸Ð¹</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md">PortuguÃªs</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_te.md">à°¤à±†à°²à±à°—à±</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md">FranÃ§ais</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_de.md">Deutsch</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_it.md">Italiano</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md">Tiáº¿ng Viá»‡t</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md">Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md">Ø§Ø±Ø¯Ùˆ</a> |\n        <a href="https://github.com/huggingface/transformers/blob/main/i18n/README_bn.md">à¦¬à¦¾à¦‚à¦²à¦¾</a> |\n    </p>\n</h4>\n\n<h3 align="center">\n    <p>State-of-the-art pretrained models for inference and training</p>\n</h3>\n\n<h3 align="center">\n    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png"/>\n</h3>\n\nTransformers acts as the model-definition framework for state-of-the-art machine learning with text, computer\nvision, audio, video, and multimodal models, for both inference and training.\n\nIt centralizes the model definition so that this definition is agreed upon across the ecosystem. `transformers` is the\npivot across frameworks: if a model definition is supported, it will be compatible with the majority of training\nframeworks (Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, ...), inference engines (vLLM, SGLang, TGI, ...),\nand adjacent modeling libraries (llama.cpp, mlx, ...) which leverage the model definition from `transformers`.\n\nWe pledge to help support new state-of-the-art models and democratize their usage by having their model definition be\nsimple, customizable, and efficient.\n\nThere are over 1M+ Transformers [model checkpoints](https://huggingface.co/models?library=transformers&sort=trending) on the [Hugging Face Hub](https://huggingface.com/models) you can use.\n\nExplore the [Hub](https://huggingface.com/) today to find a model and use Transformers to help you get started right away.\n\n## Installation\n\nTransformers works with Python 3.9+, and [PyTorch](https://pytorch.org/get-started/locally/) 2.1+.\n\nCreate and activate a virtual environment with [venv](https://docs.python.org/3/library/venv.html) or [uv](https://docs.astral.sh/uv/), a fast Rust-based Python package and project manager.\n\n```py\n# venv\npython -m venv .my-env\nsource .my-env/bin/activate\n# uv\nuv venv .my-env\nsource .my-env/bin/activate\n```\n\nInstall Transformers in your virtual environment.\n\n```py\n# pip\npip install "transformers[torch]"\n\n# uv\nuv pip install "transformers[torch]"\n```\n\nInstall Transformers from source if you want the latest changes in the library or are interested in contributing. However, the *latest* version may not be stable. Feel free to open an [issue](https://github.com/huggingface/transformers/issues) if you encounter an error.\n\n```shell\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\n\n# pip\npip install ''.[torch]''\n\n# uv\nuv pip install ''.[torch]''\n```\n\n## Quickstart\n\nGet started with Transformers right away with the [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) API. The `Pipeline` is a high-level inference class that supports text, audio, vision, and multimodal tasks. It handles preprocessing the input and returns the appropriate output.\n\nInstantiate a pipeline and specify model to use for text generation. The model is downloaded and cached so you can easily reuse it again. Finally, pass some text to prompt the model.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task="text-generation", model="Qwen/Qwen2.5-1.5B")\npipeline("the secret to baking a really good cake is ")\n[{''generated_text'': ''the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.''}]\n```\n\nTo chat with a model, the usage pattern is the same. The only difference is you need to construct a chat history (the input to `Pipeline`) between you and the system.\n\n> [!TIP]\n> You can also chat with a model directly from the command line, as long as [`transformers serve` is running](https://huggingface.co/docs/transformers/main/en/serving).\n> ```shell\n> transformers chat Qwen/Qwen2.5-0.5B-Instruct\n> ```\n\n```py\nimport torch\nfrom transformers import pipeline\n\nchat = [\n    {"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},\n    {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}\n]\n\npipeline = pipeline(task="text-generation", model="meta-llama/Meta-Llama-3-8B-Instruct", dtype=torch.bfloat16, device_map="auto")\nresponse = pipeline(chat, max_new_tokens=512)\nprint(response[0]["generated_text"][-1]["content"])\n```\n\nExpand the examples below to see how `Pipeline` works for different modalities and tasks.\n\n<details>\n<summary>Automatic speech recognition</summary>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task="automatic-speech-recognition", model="openai/whisper-large-v3")\npipeline("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")\n{''text'': '' I have a dream that one day this nation will rise up and live out the true meaning of its creed.''}\n```\n\n</details>\n\n<details>\n<summary>Image classification</summary>\n\n<h3 align="center">\n    <a><img src="https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png"></a>\n</h3>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task="image-classification", model="facebook/dinov2-small-imagenet1k-1-layer")\npipeline("https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png")\n[{''label'': ''macaw'', ''score'': 0.997848391532898},\n {''label'': ''sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita'',\n  ''score'': 0.0016551691805943847},\n {''label'': ''lorikeet'', ''score'': 0.00018523589824326336},\n {''label'': ''African grey, African gray, Psittacus erithacus'',\n  ''score'': 7.85409429227002e-05},\n {''label'': ''quail'', ''score'': 5.502637941390276e-05}]\n```\n\n</details>\n\n<details>\n<summary>Visual question answering</summary>\n\n<h3 align="center">\n    <a><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg"></a>\n</h3>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task="visual-question-answering", model="Salesforce/blip-vqa-base")\npipeline(\n    image="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg",\n    question="What is in the image?",\n)\n[{''answer'': ''statue of liberty''}]\n```\n\n</details>\n\n## Why should I use Transformers?\n\n1. Easy-to-use state-of-the-art models:\n    - High performance on natural language understanding & generation, computer vision, audio, video, and multimodal tasks.\n    - Low barrier to entry for researchers, engineers, and developers.\n    - Few user-facing abstractions with just three classes to learn.\n    - A unified API for using all our pretrained models.\n\n1. Lower compute costs, smaller carbon footprint:\n    - Share trained models instead of training from scratch.\n    - Reduce compute time and production costs.\n    - Dozens of model architectures with 1M+ pretrained checkpoints across all modalities.\n\n1. Choose the right framework for every part of a models lifetime:\n    - Train state-of-the-art models in 3 lines of code.\n    - Move a single model between PyTorch/JAX/TF2.0 frameworks at will.\n    - Pick the right framework for training, evaluation, and production.\n\n1. Easily customize a model or an example to your needs:\n    - We provide examples for each architecture to reproduce the results published by its original authors.\n    - Model internals are exposed as consistently as possible.\n    - Model files can be used independently of the library for quick experiments.\n\n<a target="_blank" href="https://huggingface.co/enterprise">\n    <img alt="Hugging Face Enterprise Hub" src="https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925">\n</a><br>\n\n## Why shouldn''t I use Transformers?\n\n- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.\n- The training API is optimized to work with PyTorch models provided by Transformers. For generic machine learning loops, you should use another library like [Accelerate](https://huggingface.co/docs/accelerate).\n- The [example scripts](https://github.com/huggingface/transformers/tree/main/examples) are only *examples*. They may not necessarily work out-of-the-box on your specific use case and you''ll need to adapt the code for it to work.\n\n## 100 projects using Transformers\n\nTransformers is more than a toolkit to use pretrained models, it''s a community of projects built around it and the\nHugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone\nelse to build their dream projects.\n\nIn order to celebrate Transformers 100,000 stars, we wanted to put the spotlight on the\ncommunity with the [awesome-transformers](./awesome-transformers.md) page which lists 100\nincredible projects built with Transformers.\n\nIf you own or use a project that you believe should be part of the list, please open a PR to add it!\n\n## Example models\n\nYou can test most of our models directly on their [Hub model pages](https://huggingface.co/models).\n\nExpand each modality below to see a few example models for various use cases.\n\n<details>\n<summary>Audio</summary>\n\n- Audio classification with [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo)\n- Automatic speech recognition with [Moonshine](https://huggingface.co/UsefulSensors/moonshine)\n- Keyword spotting with [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\n- Speech to speech generation with [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16)\n- Text to audio with [MusicGen](https://huggingface.co/facebook/musicgen-large)\n- Text to speech with [Bark](https://huggingface.co/suno/bark)\n\n</details>\n\n<details>\n<summary>Computer vision</summary>\n\n- Automatic mask generation with [SAM](https://huggingface.co/facebook/sam-vit-base)\n- Depth estimation with [DepthPro](https://huggingface.co/apple/DepthPro-hf)\n- Image classification with [DINO v2](https://huggingface.co/facebook/dinov2-base)\n- Keypoint detection with [SuperPoint](https://huggingface.co/magic-leap-community/superpoint)\n- Keypoint matching with [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor)\n- Object detection with [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd)\n- Pose Estimation with [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple)\n- Universal segmentation with [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large)\n- Video classification with [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large)\n\n</details>\n\n<details>\n<summary>Multimodal</summary>\n\n- Audio or text to text with [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B)\n- Document question answering with [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base)\n- Image or text to text with [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)\n- Image captioning [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)\n- OCR-based document understanding with [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)\n- Table question answering with [TAPAS](https://huggingface.co/google/tapas-base)\n- Unified multimodal understanding and generation with [Emu3](https://huggingface.co/BAAI/Emu3-Gen)\n- Vision to text with [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf)\n- Visual question answering with [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf)\n- Visual referring expression segmentation with [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)\n\n</details>\n\n<details>\n<summary>NLP</summary>\n\n- Masked word completion with [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)\n- Named entity recognition with [Gemma](https://huggingface.co/google/gemma-2-2b)\n- Question answering with [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)\n- Summarization with [BART](https://huggingface.co/facebook/bart-large-cnn)\n- Translation with [T5](https://huggingface.co/google-t5/t5-base)\n- Text generation with [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B)\n- Text classification with [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)\n\n</details>\n\n## Citation\n\nWe now have a [paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) you can cite for the ðŸ¤— Transformers library:\n```bibtex\n@inproceedings{wolf-etal-2020-transformers,\n    title = "Transformers: State-of-the-Art Natural Language Processing",\n    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",\n    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",\n    month = oct,\n    year = "2020",\n    address = "Online",\n    publisher = "Association for Computational Linguistics",\n    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",\n    pages = "38--45"\n}\n```\n', '{"language":"Python","stars":153588,"forks":31342,"watchers":153588,"open_issues":2135,"topics":["audio","deep-learning","deepseek","gemma","glm","hacktoberfest","llm","machine-learning","model-hub","natural-language-processing","nlp","pretrained-models","python","pytorch","pytorch-transformers","qwen","speech-recognition","transformer","vlm"],"default_branch":"main","size_kb":417578,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers.git","source_url":"https://github.com/huggingface/transformers.git"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"}]', NULL, 'Apache-2.0', 'approved', 80, '433a4d9d6a4a71d2583223d91ad5a58c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-huggingface-transformers from https://github.com/huggingface.png
Image converted to WebP: data/images/github-huggingface-transformers.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-pytorch-pytorch', 'github--pytorch--pytorch', 'pytorch', 'pytorch', '!PyTorch Logo -------------------------------------------------------------------------------- PyTorch is a Python package that provides two high-level features: - Tensor computation (like NumPy) with strong GPU acceleration - Deep neural networks built on a tape-based autograd system You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed. Our trunk health (Continuous Integration signals) can be found at hud.pytorch.org. <!-- toc --> - More ...', '["autograd","deep-learning","gpu","machine-learning","neural-network","numpy","python","tensor","python"]', 'other', 95687, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/pytorch/pytorch","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '![PyTorch Logo](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png)\n\n--------------------------------------------------------------------------------\n\nPyTorch is a Python package that provides two high-level features:\n- Tensor computation (like NumPy) with strong GPU acceleration\n- Deep neural networks built on a tape-based autograd system\n\nYou can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.\n\nOur trunk health (Continuous Integration signals) can be found at [hud.pytorch.org](https://hud.pytorch.org/ci/pytorch/pytorch/main).\n\n<!-- toc -->\n\n- [More About PyTorch](#more-about-pytorch)\n  - [A GPU-Ready Tensor Library](#a-gpu-ready-tensor-library)\n  - [Dynamic Neural Networks: Tape-Based Autograd](#dynamic-neural-networks-tape-based-autograd)\n  - [Python First](#python-first)\n  - [Imperative Experiences](#imperative-experiences)\n  - [Fast and Lean](#fast-and-lean)\n  - [Extensions Without Pain](#extensions-without-pain)\n- [Installation](#installation)\n  - [Binaries](#binaries)\n    - [NVIDIA Jetson Platforms](#nvidia-jetson-platforms)\n  - [From Source](#from-source)\n    - [Prerequisites](#prerequisites)\n      - [NVIDIA CUDA Support](#nvidia-cuda-support)\n      - [AMD ROCm Support](#amd-rocm-support)\n      - [Intel GPU Support](#intel-gpu-support)\n    - [Get the PyTorch Source](#get-the-pytorch-source)\n    - [Install Dependencies](#install-dependencies)\n    - [Install PyTorch](#install-pytorch)\n      - [Adjust Build Options (Optional)](#adjust-build-options-optional)\n  - [Docker Image](#docker-image)\n    - [Using pre-built images](#using-pre-built-images)\n    - [Building the image yourself](#building-the-image-yourself)\n  - [Building the Documentation](#building-the-documentation)\n    - [Building a PDF](#building-a-pdf)\n  - [Previous Versions](#previous-versions)\n- [Getting Started](#getting-started)\n- [Resources](#resources)\n- [Communication](#communication)\n- [Releases and Contributing](#releases-and-contributing)\n- [The Team](#the-team)\n- [License](#license)\n\n<!-- tocstop -->\n\n## More About PyTorch\n\n[Learn the basics of PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html)\n\nAt a granular level, PyTorch is a library that consists of the following components:\n\n| Component | Description |\n| ---- | --- |\n| [**torch**](https://pytorch.org/docs/stable/torch.html) | A Tensor library like NumPy, with strong GPU support |\n| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |\n| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\n| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | A neural networks library deeply integrated with autograd designed for maximum flexibility |\n| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |\n| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |\n\nUsually, PyTorch is used either as:\n\n- A replacement for NumPy to use the power of GPUs.\n- A deep learning research platform that provides maximum flexibility and speed.\n\nElaborating Further:\n\n### A GPU-Ready Tensor Library\n\nIf you use NumPy, then you have used Tensors (a.k.a. ndarray).\n\n![Tensor illustration](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/tensor_illustration.png)\n\nPyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the\ncomputation by a huge amount.\n\nWe provide a wide variety of tensor routines to accelerate and fit your scientific computation needs\nsuch as slicing, indexing, mathematical operations, linear algebra, reductions.\nAnd they are fast!\n\n### Dynamic Neural Networks: Tape-Based Autograd\n\nPyTorch has a unique way of building neural networks: using and replaying a tape recorder.\n\nMost frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world.\nOne has to build a neural network and reuse the same structure again and again.\nChanging the way the network behaves means that one has to start from scratch.\n\nWith PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to\nchange the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes\nfrom several research papers on this topic, as well as current and past work such as\n[torch-autograd](https://github.com/twitter/torch-autograd),\n[autograd](https://github.com/HIPS/autograd),\n[Chainer](https://chainer.org), etc.\n\nWhile this technique is not unique to PyTorch, it''s one of the fastest implementations of it to date.\nYou get the best of speed and flexibility for your crazy research.\n\n![Dynamic graph](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif)\n\n### Python First\n\nPyTorch is not a Python binding into a monolithic C++ framework.\nIt is built to be deeply integrated into Python.\nYou can use it naturally like you would use [NumPy](https://www.numpy.org/) / [SciPy](https://www.scipy.org/) / [scikit-learn](https://scikit-learn.org) etc.\nYou can write your new neural network layers in Python itself, using your favorite libraries\nand use packages such as [Cython](https://cython.org/) and [Numba](http://numba.pydata.org/).\nOur goal is to not reinvent the wheel where appropriate.\n\n### Imperative Experiences\n\nPyTorch is designed to be intuitive, linear in thought, and easy to use.\nWhen you execute a line of code, it gets executed. There isn''t an asynchronous view of the world.\nWhen you drop into a debugger or receive error messages and stack traces, understanding them is straightforward.\nThe stack trace points to exactly where your code was defined.\nWe hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.\n\n### Fast and Lean\n\nPyTorch has minimal framework overhead. We integrate acceleration libraries\nsuch as [Intel MKL](https://software.intel.com/mkl) and NVIDIA ([cuDNN](https://developer.nvidia.com/cudnn), [NCCL](https://developer.nvidia.com/nccl)) to maximize speed.\nAt the core, its CPU and GPU Tensor and neural network backends\nare mature and have been tested for years.\n\nHence, PyTorch is quite fast â€” whether you run small or large neural networks.\n\nThe memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.\nWe''ve written custom memory allocators for the GPU to make sure that\nyour deep learning models are maximally memory efficient.\nThis enables you to train bigger deep learning models than before.\n\n### Extensions Without Pain\n\nWriting new neural network modules, or interfacing with PyTorch''s Tensor API was designed to be straightforward\nand with minimal abstractions.\n\nYou can write new neural network layers in Python using the torch API\n[or your favorite NumPy-based libraries such as SciPy](https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html).\n\nIf you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.\nNo wrapper code needs to be written. You can see [a tutorial here](https://pytorch.org/tutorials/advanced/cpp_extension.html) and [an example here](https://github.com/pytorch/extension-cpp).\n\n\n## Installation\n\n### Binaries\nCommands to install binaries via Conda or pip wheels are on our website: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)\n\n\n#### NVIDIA Jetson Platforms\n\nPython wheels for NVIDIA''s Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, and Jetson AGX Orin are provided [here](https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048) and the L4T container is published [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch)\n\nThey require JetPack 4.2 and above, and [@dusty-nv](https://github.com/dusty-nv) and [@ptrblck](https://github.com/ptrblck) are maintaining them.\n\n\n### From Source\n\n#### Prerequisites\nIf you are installing from source, you will need:\n- Python 3.10 or later\n- A compiler that fully supports C++17, such as clang or gcc (gcc 9.4.0 or newer is required, on Linux)\n- Visual Studio or Visual Studio Build Tool (Windows only)\n\n\* PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise,\nProfessional, or Community Editions. You can also install the build tools from\nhttps://visualstudio.microsoft.com/visual-cpp-build-tools/. The build tools *do not*\ncome with Visual Studio Code by default.\n\nAn example of environment setup is shown below:\n\n* Linux:\n\n```bash\n$ source <CONDA_INSTALL_DIR>/bin/activate\n$ conda create -y -n <CONDA_NAME>\n$ conda activate <CONDA_NAME>\n```\n\n* Windows:\n\n```bash\n$ source <CONDA_INSTALL_DIR>\Scripts\activate.bat\n$ conda create -y -n <CONDA_NAME>\n$ conda activate <CONDA_NAME>\n$ call "C:\Program Files\Microsoft Visual Studio\<VERSION>\Community\VC\Auxiliary\Build\vcvarsall.bat" x64\n```\n\nA conda environment is not required.  You can also do a PyTorch build in a\nstandard virtual environment, e.g., created with tools like `uv`, provided\nyour system has installed all the necessary dependencies unavailable as pip\npackages (e.g., CUDA, MKL.)\n\n##### NVIDIA CUDA Support\nIf you want to compile with CUDA support, [select a supported version of CUDA from our support matrix](https://pytorch.org/get-started/locally/), then install the following:\n- [NVIDIA CUDA](https://developer.nvidia.com/cuda-downloads)\n- [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) v8.5 or above\n- [Compiler](https://gist.github.com/ax3l/9489132) compatible with CUDA\n\nNote: You could refer to the [cuDNN Support Matrix](https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html) for cuDNN versions with the various supported CUDA, CUDA driver, and NVIDIA hardware.\n\nIf you want to disable CUDA support, export the environment variable `USE_CUDA=0`.\nOther potentially useful environment variables may be found in `setup.py`.  If\nCUDA is installed in a non-standard location, set PATH so that the nvcc you\nwant to use can be found (e.g., `export PATH=/usr/local/cuda-12.8/bin:$PATH`).\n\nIf you are building for NVIDIA''s Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are [available here](https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/)\n\n##### AMD ROCm Support\nIf you want to compile with ROCm support, install\n- [AMD ROCm](https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html) 4.0 and above installation\n- ROCm is currently supported only for Linux systems.\n\nBy default the build system expects ROCm to be installed in `/opt/rocm`. If ROCm is installed in a different directory, the `ROCM_PATH` environment variable must be set to the ROCm installation directory. The build system automatically detects the AMD GPU architecture. Optionally, the AMD GPU architecture can be explicitly set with the `PYTORCH_ROCM_ARCH` environment variable [AMD GPU architecture](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus)\n\nIf you want to disable ROCm support, export the environment variable `USE_ROCM=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\n##### Intel GPU Support\nIf you want to compile with Intel GPU support, follow these\n- [PyTorch Prerequisites for Intel GPUs](https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html) instructions.\n- Intel GPU is supported for Linux and Windows.\n\nIf you want to disable Intel GPU support, export the environment variable `USE_XPU=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\n#### Get the PyTorch Source\n\n```bash\ngit clone https://github.com/pytorch/pytorch\ncd pytorch\n# if you are updating an existing checkout\ngit submodule sync\ngit submodule update --init --recursive\n```\n\n#### Install Dependencies\n\n**Common**\n\n```bash\n# Run this command from the PyTorch directory after cloning the source code using the â€œGet the PyTorch Sourceâ€œ section above\npip install --group dev\n```\n\n**On Linux**\n\n```bash\npip install mkl-static mkl-include\n# CUDA only: Add LAPACK support for the GPU if needed\n# magma installation: run with active conda environment. specify CUDA version to install\n.ci/docker/common/install_magma_conda.sh 12.4\n\n# (optional) If using torch.compile with inductor/triton, install the matching version of triton\n# Run from the pytorch directory after cloning\n# For Intel GPU support, please explicitly `export USE_XPU=1` before running command.\nmake triton\n```\n\n**On MacOS**\n\n```bash\n# Add this package on intel x86 processor machines only\npip install mkl-static mkl-include\n# Add these packages if torch.distributed is needed\nconda install pkg-config libuv\n```\n\n**On Windows**\n\n```bash\npip install mkl-static mkl-include\n# Add these packages if torch.distributed is needed.\n# Distributed package support on Windows is a prototype feature and is subject to changes.\nconda install -c conda-forge libuv=1.51\n```\n\n#### Install PyTorch\n\n**On Linux**\n\nIf you''re compiling for AMD ROCm then first run this command:\n\n```bash\n# Only run this if you''re compiling for ROCm\npython tools/amd_build/build_amd.py\n```\n\nInstall PyTorch\n\n```bash\n# the CMake prefix for conda environment\nexport CMAKE_PREFIX_PATH="${CONDA_PREFIX:-''$(dirname $(which conda))/../''}:${CMAKE_PREFIX_PATH}"\npython -m pip install --no-build-isolation -v -e .\n\n# the CMake prefix for non-conda environment, e.g. Python venv\n# call following after activating the venv\nexport CMAKE_PREFIX_PATH="${VIRTUAL_ENV}:${CMAKE_PREFIX_PATH}"\n```\n\n**On macOS**\n\n```bash\npython -m pip install --no-build-isolation -v -e .\n```\n\n**On Windows**\n\nIf you want to build legacy python code, please refer to [Building on legacy code and CUDA](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda)\n\n**CPU-only builds**\n\nIn this mode PyTorch computations will run on your CPU, not your GPU.\n\n```cmd\npython -m pip install --no-build-isolation -v -e .\n```\n\nNote on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you''ll need to manually download the library and set up the building environment by tweaking `CMAKE_INCLUDE_PATH` and `LIB`. The instruction [here](https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source) is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.\n\n**CUDA based build**\n\nIn this mode PyTorch computations will leverage your GPU via CUDA for faster number crunching\n\n[NVTX](https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm) is needed to build Pytorch with CUDA.\nNVTX is a part of CUDA distributive, where it is called "Nsight Compute". To install it onto an already installed CUDA run CUDA installation once again and check the corresponding checkbox.\nMake sure that CUDA with Nsight Compute is installed after Visual Studio.\n\nCurrently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If `ninja.exe` is detected in `PATH`, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019.\n<br/> If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.\n\nAdditional libraries such as\n[Magma](https://developer.nvidia.com/magma), [oneDNN, a.k.a. MKLDNN or DNNL](https://github.com/oneapi-src/oneDNN), and [Sccache](https://github.com/mozilla/sccache) are often needed. Please refer to the [installation-helper](https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers) to install them.\n\nYou can refer to the [build_pytorch.bat](https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat) script for some other environment variables configurations\n\n```cmd\ncmd\n\n:: Set the environment variables after you have downloaded and unzipped the mkl package,\n:: else CMake would throw an error as `Could NOT find OpenMP`.\nset CMAKE_INCLUDE_PATH={Your directory}\mkl\include\nset LIB={Your directory}\mkl\lib;%LIB%\n\n:: Read the content in the previous section carefully before you proceed.\n:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.\n:: "Visual Studio 2019 Developer Command Prompt" will be run automatically.\n:: Make sure you have CMake >= 3.12 before you do this when you use the Visual Studio generator.\nset CMAKE_GENERATOR_TOOLSET_VERSION=14.27\nset DISTUTILS_USE_SDK=1\nfor /f "usebackq tokens=*" %i in (`"%ProgramFiles(x86)%\Microsoft Visual Studio\Installer\vswhere.exe" -version [15^,17^) -products * -latest -property installationPath`) do call "%i\VC\Auxiliary\Build\vcvarsall.bat" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%\n\n:: [Optional] If you want to override the CUDA host compiler\nset CUDAHOSTCXX=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64\cl.exe\n\npython -m pip install --no-build-isolation -v -e .\n```\n\n**Intel GPU builds**\n\nIn this mode PyTorch with Intel GPU support will be built.\n\nPlease make sure [the common prerequisites](#prerequisites) as well as [the prerequisites for Intel GPU](#intel-gpu-support) are properly installed and the environment variables are configured prior to starting the build. For build tool support, `Visual Studio 2022` is required.\n\nThen PyTorch can be built with the command:\n\n```cmd\n:: CMD Commands:\n:: Set the CMAKE_PREFIX_PATH to help find corresponding packages\n:: %CONDA_PREFIX% only works after `conda activate custom_env`\n\nif defined CMAKE_PREFIX_PATH (\n    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library;%CMAKE_PREFIX_PATH%"\n) else (\n    set "CMAKE_PREFIX_PATH=%CONDA_PREFIX%\Library"\n)\n\npython -m pip install --no-build-isolation -v -e .\n```\n\n##### Adjust Build Options (Optional)\n\nYou can adjust the configuration of cmake variables optionally (without building first), by doing\nthe following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done\nwith such a step.\n\nOn Linux\n\n```bash\nexport CMAKE_PREFIX_PATH="${CONDA_PREFIX:-''$(dirname $(which conda))/../''}:${CMAKE_PREFIX_PATH}"\nCMAKE_ONLY=1 python setup.py build\nccmake build  # or cmake-gui build\n```\n\nOn macOS\n\n```bash\nexport CMAKE_PREFIX_PATH="${CONDA_PREFIX:-''$(dirname $(which conda))/../''}:${CMAKE_PREFIX_PATH}"\nMACOSX_DEPLOYMENT_TARGET=11.0 CMAKE_ONLY=1 python setup.py build\nccmake build  # or cmake-gui build\n```\n\n### Docker Image\n\n#### Using pre-built images\n\nYou can also pull a pre-built docker image from Docker Hub and run with docker v19.03+\n\n```bash\ndocker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest\n```\n\nPlease note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.\nfor multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you\nshould increase shared memory size either with `--ipc=host` or `--shm-size` command line options to `nvidia-docker run`.\n\n#### Building the image yourself\n\n**NOTE:** Must be built with a docker version > 18.06\n\nThe `Dockerfile` is supplied to build images with CUDA 11.1 support and cuDNN v8.\nYou can pass `PYTHON_VERSION=x.y` make variable to specify which Python version is to be used by Miniconda, or leave it\nunset to use the default.\n\n```bash\nmake -f docker.Makefile\n# images are tagged as docker.io/${your_docker_username}/pytorch\n```\n\nYou can also pass the `CMAKE_VARS="..."` environment variable to specify additional CMake variables to be passed to CMake during the build.\nSee [setup.py](./setup.py) for the list of available variables.\n\n```bash\nmake -f docker.Makefile\n```\n\n### Building the Documentation\n\nTo build documentation in various formats, you will need [Sphinx](http://www.sphinx-doc.org)\nand the pytorch_sphinx_theme2.\n\nBefore you build the documentation locally, ensure `torch` is\ninstalled in your environment. For small fixes, you can install the\nnightly version as described in [Getting Started](https://pytorch.org/get-started/locally/).\n\nFor more complex fixes, such as adding a new module and docstrings for\nthe new module, you might need to install torch [from source](#from-source).\nSee [Docstring Guidelines](https://github.com/pytorch/pytorch/wiki/Docstring-Guidelines)\nfor docstring conventions.\n\n```bash\ncd docs/\npip install -r requirements.txt\nmake html\nmake serve\n```\n\nRun `make` to get a list of all available output formats.\n\nIf you get a katex error run `npm install katex`.  If it persists, try\n`npm install -g katex`\n\n> [!NOTE]\n> If you installed `nodejs` with a different package manager (e.g.,\n> `conda`) then `npm` will probably install a version of `katex` that is not\n> compatible with your version of `nodejs` and doc builds will fail.\n> A combination of versions that is known to work is `node@6.13.1` and\n> `katex@0.13.18`. To install the latter with `npm` you can run\n> ```npm install -g katex@0.13.18```\n\n> [!NOTE]\n> If you see a numpy incompatibility error, run:\n> ```\n> pip install ''numpy<2''\n> ```\n\nWhen you make changes to the dependencies run by CI, edit the\n`.ci/docker/requirements-docs.txt` file.\n\n#### Building a PDF\n\nTo compile a PDF of all PyTorch documentation, ensure you have\n`texlive` and LaTeX installed. On macOS, you can install them using:\n\n```\nbrew install --cask mactex\n```\n\nTo create the PDF:\n\n1. Run:\n\n   ```\n   make latexpdf\n   ```\n\n   This will generate the necessary files in the `build/latex` directory.\n\n2. Navigate to this directory and execute:\n\n   ```\n   make LATEXOPTS="-interaction=nonstopmode"\n   ```\n\n   This will produce a `pytorch.pdf` with the desired content. Run this\n   command one more time so that it generates the correct table\n   of contents and index.\n\n> [!NOTE]\n> To view the Table of Contents, switch to the **Table of Contents**\n> view in your PDF viewer.\n\n\n### Previous Versions\n\nInstallation instructions and binaries for previous PyTorch versions may be found\non [our website](https://pytorch.org/get-started/previous-versions).\n\n\n## Getting Started\n\nThree pointers to get you started:\n- [Tutorials: get you started with understanding and using PyTorch](https://pytorch.org/tutorials/)\n- [Examples: easy to understand PyTorch code across all domains](https://github.com/pytorch/examples)\n- [The API Reference](https://pytorch.org/docs/)\n- [Glossary](https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md)\n\n## Resources\n\n* [PyTorch.org](https://pytorch.org/)\n* [PyTorch Tutorials](https://pytorch.org/tutorials/)\n* [PyTorch Examples](https://github.com/pytorch/examples)\n* [PyTorch Models](https://pytorch.org/hub/)\n* [Intro to Deep Learning with PyTorch from Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188)\n* [Intro to Machine Learning with PyTorch from Udacity](https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229)\n* [Deep Neural Networks with PyTorch from Coursera](https://www.coursera.org/learn/deep-neural-networks-with-pytorch)\n* [PyTorch Twitter](https://twitter.com/PyTorch)\n* [PyTorch Blog](https://pytorch.org/blog/)\n* [PyTorch YouTube](https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw)\n\n## Communication\n* Forums: Discuss implementations, research, etc. https://discuss.pytorch.org\n* GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc.\n* Slack: The [PyTorch Slack](https://pytorch.slack.com/) hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is [PyTorch Forums](https://discuss.pytorch.org). If you need a slack invite, please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1\n* Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here: https://eepurl.com/cbG0rv\n* Facebook Page: Important announcements about PyTorch. https://www.facebook.com/pytorch\n* For brand guidelines, please visit our website at [pytorch.org](https://pytorch.org/)\n\n## Releases and Contributing\n\nTypically, PyTorch has three minor releases a year. Please let us know if you encounter a bug by [filing an issue](https://github.com/pytorch/pytorch/issues).\n\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.\n\nIf you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us.\nSending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of.\n\nTo learn more about making a contribution to Pytorch, please see our [Contribution page](CONTRIBUTING.md). For more information about PyTorch releases, see [Release page](RELEASE.md).\n\n## The Team\n\nPyTorch is a community-driven project with several skillful engineers and researchers contributing to it.\n\nPyTorch is currently maintained by [Soumith Chintala](http://soumith.ch), [Gregory Chanan](https://github.com/gchanan), [Dmytro Dzhulgakov](https://github.com/dzhulgakov), [Edward Yang](https://github.com/ezyang), [Alban Desmaison](https://github.com/albanD), [Piotr Bialecki](https://github.com/ptrblck) and [Nikita Shulga](https://github.com/malfet) with major contributions coming from hundreds of talented individuals in various forms and means.\nA non-exhaustive but growing list needs to mention: [Trevor Killeen](https://github.com/killeent), [Sasank Chilamkurthy](https://github.com/chsasank), [Sergey Zagoruyko](https://github.com/szagoruyko), [Adam Lerer](https://github.com/adamlerer), [Francisco Massa](https://github.com/fmassa), [Alykhan Tejani](https://github.com/alykhantejani), [Luca Antiga](https://github.com/lantiga), [Alban Desmaison](https://github.com/albanD), [Andreas Koepf](https://github.com/andreaskoepf), [James Bradbury](https://github.com/jekbradbury), [Zeming Lin](https://github.com/ebetica), [Yuandong Tian](https://github.com/yuandong-tian), [Guillaume Lample](https://github.com/glample), [Marat Dukhan](https://github.com/Maratyszcza), [Natalia Gimelshein](https://github.com/ngimel), [Christian Sarofeen](https://github.com/csarofeen), [Martin Raison](https://github.com/martinraison), [Edward Yang](https://github.com/ezyang), [Zachary Devito](https://github.com/zdevito). <!-- codespell:ignore -->\n\nNote: This project is unrelated to [hughperkins/pytorch](https://github.com/hughperkins/pytorch) with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.\n\n## License\n\nPyTorch has a BSD-style license, as found in the [LICENSE](LICENSE) file.\n', '{"language":"Python","stars":95687,"forks":26136,"watchers":95687,"open_issues":17812,"topics":["autograd","deep-learning","gpu","machine-learning","neural-network","numpy","python","tensor"],"default_branch":"main","size_kb":1172135,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:twitter:torch-autograd","source_url":"https://github.com/twitter/torch-autograd"},{"type":"has_code","target_id":"github:HIPS:autograd","source_url":"https://github.com/HIPS/autograd"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:extension-cpp","source_url":"https://github.com/pytorch/extension-cpp"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:oneapi-src:oneDNN","source_url":"https://github.com/oneapi-src/oneDNN"},{"type":"has_code","target_id":"github:mozilla:sccache","source_url":"https://github.com/mozilla/sccache"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:examples","source_url":"https://github.com/pytorch/examples"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:pytorch:examples","source_url":"https://github.com/pytorch/examples"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:hughperkins:pytorch","source_url":"https://github.com/hughperkins/pytorch"}]', NULL, 'NOASSERTION', 'approved', 80, 'fc830e2694c6bb7369bd5674475ab371', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-pytorch-pytorch from https://github.com/pytorch.png
Image converted to WebP: data/images/github-pytorch-pytorch.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-rasbt-LLMs-from-scratch', 'github--rasbt--llms-from-scratch', 'LLMs-from-scratch', 'rasbt', 'This repository contains the code for developing, pretraining, and finetuning a GPT-like LLM and is the official code repository for the book Build a Large Language Model (From Scratch). <br> <br> <a href="https://amzn.to/4fqvn0D"><img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/cover.jpg?123" width="250px"></a> <br> In *Build a Large Language Model (From Scratch)*, you''ll learn and understand how large language models (LLMs) work from the inside out by coding them from ...', '["ai","artificial-intelligence","chatbot","chatgpt","deep-learning","from-scratch","generative-ai","gpt","language-model","large-language-models","llm","machine-learning","neural-networks","python","pytorch","transformers","jupyter notebook"]', 'other', 80646, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/rasbt/LLMs-from-scratch","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# Build a Large Language Model (From Scratch)\n\nThis repository contains the code for developing, pretraining, and finetuning a GPT-like LLM and is the official code repository for the book [Build a Large Language Model (From Scratch)](https://amzn.to/4fqvn0D).\n\n<br>\n<br>\n\n<a href="https://amzn.to/4fqvn0D"><img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/cover.jpg?123" width="250px"></a>\n\n<br>\n\nIn [*Build a Large Language Model (From Scratch)*](http://mng.bz/orYv), you''ll learn and understand how large language models (LLMs) work from the inside out by coding them from the ground up, step by step. In this book, I''ll guide you through creating your own LLM, explaining each stage with clear text, diagrams, and examples.\n\nThe method described in this book for training and developing your own small-but-functional model for educational purposes mirrors the approach used in creating large-scale foundational models such as those behind ChatGPT. In addition, this book includes code for loading the weights of larger pretrained models for finetuning.\n\n- Link to the official [source code repository](https://github.com/rasbt/LLMs-from-scratch)\n- [Link to the book at Manning (the publisher''s website)](http://mng.bz/orYv)\n- [Link to the book page on Amazon.com](https://www.amazon.com/gp/product/1633437167)\n- ISBN 9781633437166\n\n<a href="http://mng.bz/orYv#reviews"><img src="https://sebastianraschka.com//images/LLMs-from-scratch-images/other/reviews.png" width="220px"></a>\n\n\n<br>\n<br>\n\nTo download a copy of this repository, click on the [Download ZIP](https://github.com/rasbt/LLMs-from-scratch/archive/refs/heads/main.zip) button or execute the following command in your terminal:\n\n```bash\ngit clone --depth 1 https://github.com/rasbt/LLMs-from-scratch.git\n```\n\n<br>\n\n(If you downloaded the code bundle from the Manning website, please consider visiting the official code repository on GitHub at [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) for the latest updates.)\n\n<br>\n<br>\n\n\n# Table of Contents\n\nPlease note that this `README.md` file is a Markdown (`.md`) file. If you have downloaded this code bundle from the Manning website and are viewing it on your local computer, I recommend using a Markdown editor or previewer for proper viewing. If you haven''t installed a Markdown editor yet, [Ghostwriter](https://ghostwriter.kde.org) is a good free option.\n\nYou can alternatively view this and other files on GitHub at [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) in your browser, which renders Markdown automatically.\n\n<br>\n<br>\n\n\n> **Tip:**\n> If you''re seeking guidance on installing Python and Python packages and setting up your code environment, I suggest reading the [README.md](setup/README.md) file located in the [setup](setup) directory.\n\n<br>\n<br>\n\n[![Code tests Linux](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-linux-uv.yml/badge.svg)](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-linux-uv.yml)\n[![Code tests Windows](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-windows-uv-pip.yml/badge.svg)](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-windows-uv-pip.yml)\n[![Code tests macOS](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-macos-uv.yml/badge.svg)](https://github.com/rasbt/LLMs-from-scratch/actions/workflows/basic-tests-macos-uv.yml)\n\n\n\n| Chapter Title                                              | Main Code (for Quick Access)                                                                                                    | All Code + Supplementary      |\n|------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|-------------------------------|\n| [Setup recommendations](setup) <br/>[How to best read this book](https://sebastianraschka.com/blog/2025/reading-books.html)                            | -                                                                                                                               | -                             |\n| Ch 1: Understanding Large Language Models                  | No code                                                                                                                         | -                             |\n| Ch 2: Working with Text Data                               | - [ch02.ipynb](ch02/01_main-chapter-code/ch02.ipynb)<br/>- [dataloader.ipynb](ch02/01_main-chapter-code/dataloader.ipynb) (summary)<br/>- [exercise-solutions.ipynb](ch02/01_main-chapter-code/exercise-solutions.ipynb)               | [./ch02](./ch02)            |\n| Ch 3: Coding Attention Mechanisms                          | - [ch03.ipynb](ch03/01_main-chapter-code/ch03.ipynb)<br/>- [multihead-attention.ipynb](ch03/01_main-chapter-code/multihead-attention.ipynb) (summary) <br/>- [exercise-solutions.ipynb](ch03/01_main-chapter-code/exercise-solutions.ipynb)| [./ch03](./ch03)             |\n| Ch 4: Implementing a GPT Model from Scratch                | - [ch04.ipynb](ch04/01_main-chapter-code/ch04.ipynb)<br/>- [gpt.py](ch04/01_main-chapter-code/gpt.py) (summary)<br/>- [exercise-solutions.ipynb](ch04/01_main-chapter-code/exercise-solutions.ipynb) | [./ch04](./ch04)           |\n| Ch 5: Pretraining on Unlabeled Data                        | - [ch05.ipynb](ch05/01_main-chapter-code/ch05.ipynb)<br/>- [gpt_train.py](ch05/01_main-chapter-code/gpt_train.py) (summary) <br/>- [gpt_generate.py](ch05/01_main-chapter-code/gpt_generate.py) (summary) <br/>- [exercise-solutions.ipynb](ch05/01_main-chapter-code/exercise-solutions.ipynb) | [./ch05](./ch05)              |\n| Ch 6: Finetuning for Text Classification                   | - [ch06.ipynb](ch06/01_main-chapter-code/ch06.ipynb)  <br/>- [gpt_class_finetune.py](ch06/01_main-chapter-code/gpt_class_finetune.py)  <br/>- [exercise-solutions.ipynb](ch06/01_main-chapter-code/exercise-solutions.ipynb) | [./ch06](./ch06)              |\n| Ch 7: Finetuning to Follow Instructions                    | - [ch07.ipynb](ch07/01_main-chapter-code/ch07.ipynb)<br/>- [gpt_instruction_finetuning.py](ch07/01_main-chapter-code/gpt_instruction_finetuning.py) (summary)<br/>- [ollama_evaluate.py](ch07/01_main-chapter-code/ollama_evaluate.py) (summary)<br/>- [exercise-solutions.ipynb](ch07/01_main-chapter-code/exercise-solutions.ipynb) | [./ch07](./ch07)  |\n| Appendix A: Introduction to PyTorch                        | - [code-part1.ipynb](appendix-A/01_main-chapter-code/code-part1.ipynb)<br/>- [code-part2.ipynb](appendix-A/01_main-chapter-code/code-part2.ipynb)<br/>- [DDP-script.py](appendix-A/01_main-chapter-code/DDP-script.py)<br/>- [exercise-solutions.ipynb](appendix-A/01_main-chapter-code/exercise-solutions.ipynb) | [./appendix-A](./appendix-A) |\n| Appendix B: References and Further Reading                 | No code                                                                                                                         | [./appendix-B](./appendix-B) |\n| Appendix C: Exercise Solutions                             | - [list of exercise solutions](appendix-C)                                                                 | [./appendix-C](./appendix-C) |\n| Appendix D: Adding Bells and Whistles to the Training Loop | - [appendix-D.ipynb](appendix-D/01_main-chapter-code/appendix-D.ipynb)                                                          | [./appendix-D](./appendix-D)  |\n| Appendix E: Parameter-efficient Finetuning with LoRA       | - [appendix-E.ipynb](appendix-E/01_main-chapter-code/appendix-E.ipynb)                                                          | [./appendix-E](./appendix-E) |\n\n<br>\n&nbsp;\n\nThe mental model below summarizes the contents covered in this book.\n\n<img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/mental-model.jpg" width="650px">\n\n\n<br>\n&nbsp;\n\n## Prerequisites\n\nThe most important prerequisite is a strong foundation in Python programming.\nWith this knowledge, you will be well prepared to explore the fascinating world of LLMs\nand understand the concepts and code examples presented in this book.\n\nIf you have some experience with deep neural networks, you may find certain concepts more familiar, as LLMs are built upon these architectures.\n\nThis book uses PyTorch to implement the code from scratch without using any external LLM libraries. While proficiency in PyTorch is not a prerequisite, familiarity with PyTorch basics is certainly useful. If you are new to PyTorch, Appendix A provides a concise introduction to PyTorch. Alternatively, you may find my book, [PyTorch in One Hour: From Tensors to Training Neural Networks on Multiple GPUs](https://sebastianraschka.com/teaching/pytorch-1h/), helpful for learning about the essentials.\n\n\n\n<br>\n&nbsp;\n\n## Hardware Requirements\n\nThe code in the main chapters of this book is designed to run on conventional laptops within a reasonable timeframe and does not require specialized hardware. This approach ensures that a wide audience can engage with the material. Additionally, the code automatically utilizes GPUs if they are available. (Please see the [setup](https://github.com/rasbt/LLMs-from-scratch/blob/main/setup/README.md) doc for additional recommendations.)\n\n\n&nbsp;\n## Video Course\n\n[A 17-hour and 15-minute companion video course](https://www.manning.com/livevideo/master-and-build-large-language-models) where I code through each chapter of the book. The course is organized into chapters and sections that mirror the book''s structure so that it can be used as a standalone alternative to the book or complementary code-along resource.\n\n<a href="https://www.manning.com/livevideo/master-and-build-large-language-models"><img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/video-screenshot.webp?123" width="350px"></a>\n\n\n&nbsp;\n\n\n## Companion Book / Sequel\n\n[*Build A Reasoning Model (From Scratch)*](https://mng.bz/lZ5B), while a standalone book, can be considered as a sequel to *Build A Large Language Model (From Scratch)*.\n\nIt starts with a pretrained model and implements different reasoning approaches, including inference-time scaling, reinforcement learning, and distillation, to improve the model''s reasoning capabilities.\n\nSimilar to *Build A Large Language Model (From Scratch)*, [*Build A Reasoning Model (From Scratch)*](https://mng.bz/lZ5B) takes a hands-on approach implementing these methods from scratch.\n\n<a href="https://mng.bz/lZ5B"><img src="https://sebastianraschka.com/images/reasoning-from-scratch-images/cover.webp?123" width="120px"></a>\n\n- Amazon link (TBD)\n- [Manning link](https://mng.bz/lZ5B)\n- [GitHub repository](https://github.com/rasbt/reasoning-from-scratch)\n\n<br>\n\n&nbsp;\n## Exercises\n\nEach chapter of the book includes several exercises. The solutions are summarized in Appendix C, and the corresponding code notebooks are available in the main chapter folders of this repository (for example,  [./ch02/01_main-chapter-code/exercise-solutions.ipynb](./ch02/01_main-chapter-code/exercise-solutions.ipynb).\n\nIn addition to the code exercises, you can download a free 170-page PDF titled  [Test Yourself On Build a Large Language Model (From Scratch)](https://www.manning.com/books/test-yourself-on-build-a-large-language-model-from-scratch) from the Manning website. It contains approximately 30 quiz questions and solutions per chapter to help you test your understanding.\n\n<a href="https://www.manning.com/books/test-yourself-on-build-a-large-language-model-from-scratch"><img src="https://sebastianraschka.com/images/LLMs-from-scratch-images/test-yourself-cover.jpg?123" width="150px"></a>\n\n&nbsp;\n## Bonus Material\n\nSeveral folders contain optional materials as a bonus for interested readers:\n- **Setup**\n  - [Python Setup Tips](setup/01_optional-python-setup-preferences)\n  - [Installing Python Packages and Libraries Used in This Book](setup/02_installing-python-libraries)\n  - [Docker Environment Setup Guide](setup/03_optional-docker-environment)\n\n- **Chapter 2: Working With Text Data**\n  - [Byte Pair Encoding (BPE) Tokenizer From Scratch](ch02/05_bpe-from-scratch/bpe-from-scratch-simple.ipynb)\n  - [Comparing Various Byte Pair Encoding (BPE) Implementations](ch02/02_bonus_bytepair-encoder)\n  - [Understanding the Difference Between Embedding Layers and Linear Layers](ch02/03_bonus_embedding-vs-matmul)\n  - [Dataloader Intuition With Simple Numbers](ch02/04_bonus_dataloader-intuition)\n\n- **Chapter 3: Coding Attention Mechanisms**\n  - [Comparing Efficient Multi-Head Attention Implementations](ch03/02_bonus_efficient-multihead-attention/mha-implementations.ipynb)\n  - [Understanding PyTorch Buffers](ch03/03_understanding-buffers/understanding-buffers.ipynb)\n\n- **Chapter 4: Implementing a GPT Model From Scratch**\n  - [FLOPs Analysis](ch04/02_performance-analysis/flops-analysis.ipynb)\n  - [KV Cache](ch04/03_kv-cache)\n  - [Attention Alternatives](ch04/#attention-alternatives)\n    - [Grouped-Query Attention](ch04/04_gqa)\n    - [Multi-Head Latent Attention](ch04/05_mla)\n    - [Sliding Window Attention](ch04/06_swa)\n    - [Gated DeltaNet](ch04/08_deltanet)\n  - [Mixture-of-Experts (MoE)](ch04/07_moe)\n\n- **Chapter 5: Pretraining on Unlabeled Data**\n  - [Alternative Weight Loading Methods](ch05/02_alternative_weight_loading/)\n  - [Pretraining GPT on the Project Gutenberg Dataset](ch05/03_bonus_pretraining_on_gutenberg)\n  - [Adding Bells and Whistles to the Training Loop](ch05/04_learning_rate_schedulers)\n  - [Optimizing Hyperparameters for Pretraining](ch05/05_bonus_hparam_tuning)\n  - [Building a User Interface to Interact With the Pretrained LLM](ch05/06_user_interface)\n  - [Converting GPT to Llama](ch05/07_gpt_to_llama)\n  - [Memory-efficient Model Weight Loading](ch05/08_memory_efficient_weight_loading/memory-efficient-state-dict.ipynb)\n  - [Extending the Tiktoken BPE Tokenizer with New Tokens](ch05/09_extending-tokenizers/extend-tiktoken.ipynb)\n  - [PyTorch Performance Tips for Faster LLM Training](ch05/10_llm-training-speed)\n  - [LLM Architectures](ch05/#llm-architectures-from-scratch)\n    - [Llama 3.2 From Scratch](ch05/07_gpt_to_llama/standalone-llama32.ipynb)\n    - [Qwen3 Dense and Mixture-of-Experts (MoE) From Scratch](ch05/11_qwen3/)\n    - [Gemma 3 From Scratch](ch05/12_gemma3/)\n    - [Olmo 3 From Scratch](ch05/13_olmo3/)\n- **Chapter 6: Finetuning for classification**\n  - [Additional experiments finetuning different layers and using larger models](ch06/02_bonus_additional-experiments)\n  - [Finetuning different models on 50k IMDb movie review dataset](ch06/03_bonus_imdb-classification)\n  - [Building a User Interface to Interact With the GPT-based Spam Classifier](ch06/04_user_interface)\n- **Chapter 7: Finetuning to follow instructions**\n  - [Dataset Utilities for Finding Near Duplicates and Creating Passive Voice Entries](ch07/02_dataset-utilities)\n  - [Evaluating Instruction Responses Using the OpenAI API and Ollama](ch07/03_model-evaluation)\n  - [Generating a Dataset for Instruction Finetuning](ch07/05_dataset-generation/llama3-ollama.ipynb)\n  - [Improving a Dataset for Instruction Finetuning](ch07/05_dataset-generation/reflection-gpt4.ipynb)\n  - [Generating a Preference Dataset With Llama 3.1 70B and Ollama](ch07/04_preference-tuning-with-dpo/create-preference-data-ollama.ipynb)\n  - [Direct Preference Optimization (DPO) for LLM Alignment](ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb)\n  - [Building a User Interface to Interact With the Instruction-Finetuned GPT Model](ch07/06_user_interface)\n\nMore bonus material from the [Reasoning From Scratch](https://github.com/rasbt/reasoning-from-scratch) repository:\n\n- **Qwen3 (From Scratch) Basics**\n  - [Qwen3 Source Code Walkthrough](https://github.com/rasbt/reasoning-from-scratch/blob/main/chC/01_main-chapter-code/chC_main.ipynb)\n  - [Optimized Qwen3](https://github.com/rasbt/reasoning-from-scratch/tree/main/ch02/03_optimized-LLM)\n\n- **Evaluation**\n  - [Verifier-Based Evaluation (MATH-500)](https://github.com/rasbt/reasoning-from-scratch/tree/main/ch03)\n  - [Multiple-Choice Evaluation (MMLU)](https://github.com/rasbt/reasoning-from-scratch/blob/main/chF/02_mmlu)\n  - [LLM Leaderboard Evaluation](https://github.com/rasbt/reasoning-from-scratch/blob/main/chF/03_leaderboards)\n  - [LLM-as-a-Judge Evaluation](https://github.com/rasbt/reasoning-from-scratch/blob/main/chF/04_llm-judge)\n\n<br>\n&nbsp;\n\n## Questions, Feedback, and Contributing to This Repository\n\n\nI welcome all sorts of feedback, best shared via the [Manning Forum](https://livebook.manning.com/forum?product=raschka&page=1) or [GitHub Discussions](https://github.com/rasbt/LLMs-from-scratch/discussions). Likewise, if you have any questions or just want to bounce ideas off others, please don''t hesitate to post these in the forum as well.\n\nPlease note that since this repository contains the code corresponding to a print book, I currently cannot accept contributions that would extend the contents of the main chapter code, as it would introduce deviations from the physical book. Keeping it consistent helps ensure a smooth experience for everyone.\n\n\n&nbsp;\n## Citation\n\nIf you find this book or code useful for your research, please consider citing it.\n\nChicago-style citation:\n\n> Raschka, Sebastian. *Build A Large Language Model (From Scratch)*. Manning, 2024. ISBN: 978-1633437166.\n\nBibTeX entry:\n\n```\n@book{build-llms-from-scratch-book,\n  author       = {Sebastian Raschka},\n  title        = {Build A Large Language Model (From Scratch)},\n  publisher    = {Manning},\n  year         = {2024},\n  isbn         = {978-1633437166},\n  url          = {https://www.manning.com/books/build-a-large-language-model-from-scratch},\n  github       = {https://github.com/rasbt/LLMs-from-scratch}\n}\n```\n', '{"language":"Jupyter Notebook","stars":80646,"forks":12026,"watchers":80646,"open_issues":1,"topics":["ai","artificial-intelligence","chatbot","chatgpt","deep-learning","from-scratch","generative-ai","gpt","language-model","large-language-models","llm","machine-learning","neural-networks","python","pytorch","transformers"],"default_branch":"main","size_kb":15419,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch.git","source_url":"https://github.com/rasbt/LLMs-from-scratch.git"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch](https:","source_url":"https://github.com/rasbt/LLMs-from-scratch](https:"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch](https:","source_url":"https://github.com/rasbt/LLMs-from-scratch](https:"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:reasoning-from-scratch","source_url":"https://github.com/rasbt/reasoning-from-scratch"},{"type":"has_code","target_id":"github:rasbt:reasoning-from-scratch","source_url":"https://github.com/rasbt/reasoning-from-scratch"},{"type":"has_code","target_id":"github:rasbt:reasoning-from-scratch","source_url":"https://github.com/rasbt/reasoning-from-scratch"},{"type":"has_code","target_id":"github:rasbt:reasoning-from-scratch","source_url":"https://github.com/rasbt/reasoning-from-scratch"},{"type":"has_code","target_id":"github:rasbt:reasoning-from-scratch","source_url":"https://github.com/rasbt/reasoning-from-scratch"},{"type":"has_code","target_id":"github:rasbt:reasoning-from-scratch","source_url":"https://github.com/rasbt/reasoning-from-scratch"},{"type":"has_code","target_id":"github:rasbt:reasoning-from-scratch","source_url":"https://github.com/rasbt/reasoning-from-scratch"},{"type":"has_code","target_id":"github:rasbt:reasoning-from-scratch","source_url":"https://github.com/rasbt/reasoning-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch}","source_url":"https://github.com/rasbt/LLMs-from-scratch}"}]', NULL, 'NOASSERTION', 'approved', 80, '204e58f8972fa4521d6437a94ba6c183', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-rasbt-LLMs-from-scratch from https://github.com/rasbt.png
Image converted to WebP: data/images/github-rasbt-LLMs-from-scratch.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-ML-For-Beginners', 'github--microsoft--ml-for-beginners', 'ML-For-Beginners', 'microsoft', '<!-- CO-OP TRANSLATOR LANGUAGES TABLE START --> Arabic | Bengali | Bulgarian | Burmese (Myanmar) | Chinese (Simplified) | Chinese (Traditional, Hong Kong) | Chinese (Traditional, Macau) | Chinese (Traditional, Taiwan) | Croatian | Czech | Danish | Dutch | Estonian | Finnish | French | German | Greek | Hebrew | Hindi | Hungarian | Indonesian | Italian | Japanese | Korean | Lithuanian | Malay | Marathi | Nepali | Norwegian | Persian (Farsi) | Polish | Portuguese (Brazil) | Portuguese (Portugal)...', '["data-science","education","machine-learning","machine-learning-algorithms","machinelearning","machinelearning-python","microsoft-for-beginners","ml","python","r","scikit-learn","scikit-learn-python","jupyter notebook"]', 'other', 80313, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/ML-For-Beginners","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '[![GitHub license](https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg)](https://github.com/microsoft/ML-For-Beginners/blob/master/LICENSE)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/issues/)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/pulls/)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ML-For-Beginners/watchers/)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n\n### ðŸŒ Multi-Language Support\n\n#### Supported via GitHub Action (Automated & Always Up-to-Date)\n\n<!-- CO-OP TRANSLATOR LANGUAGES TABLE START -->\n[Arabic](./translations/ar/README.md) | [Bengali](./translations/bn/README.md) | [Bulgarian](./translations/bg/README.md) | [Burmese (Myanmar)](./translations/my/README.md) | [Chinese (Simplified)](./translations/zh/README.md) | [Chinese (Traditional, Hong Kong)](./translations/hk/README.md) | [Chinese (Traditional, Macau)](./translations/mo/README.md) | [Chinese (Traditional, Taiwan)](./translations/tw/README.md) | [Croatian](./translations/hr/README.md) | [Czech](./translations/cs/README.md) | [Danish](./translations/da/README.md) | [Dutch](./translations/nl/README.md) | [Estonian](./translations/et/README.md) | [Finnish](./translations/fi/README.md) | [French](./translations/fr/README.md) | [German](./translations/de/README.md) | [Greek](./translations/el/README.md) | [Hebrew](./translations/he/README.md) | [Hindi](./translations/hi/README.md) | [Hungarian](./translations/hu/README.md) | [Indonesian](./translations/id/README.md) | [Italian](./translations/it/README.md) | [Japanese](./translations/ja/README.md) | [Korean](./translations/ko/README.md) | [Lithuanian](./translations/lt/README.md) | [Malay](./translations/ms/README.md) | [Marathi](./translations/mr/README.md) | [Nepali](./translations/ne/README.md) | [Norwegian](./translations/no/README.md) | [Persian (Farsi)](./translations/fa/README.md) | [Polish](./translations/pl/README.md) | [Portuguese (Brazil)](./translations/br/README.md) | [Portuguese (Portugal)](./translations/pt/README.md) | [Punjabi (Gurmukhi)](./translations/pa/README.md) | [Romanian](./translations/ro/README.md) | [Russian](./translations/ru/README.md) | [Serbian (Cyrillic)](./translations/sr/README.md) | [Slovak](./translations/sk/README.md) | [Slovenian](./translations/sl/README.md) | [Spanish](./translations/es/README.md) | [Swahili](./translations/sw/README.md) | [Swedish](./translations/sv/README.md) | [Tagalog (Filipino)](./translations/tl/README.md) | [Tamil](./translations/ta/README.md) | [Thai](./translations/th/README.md) | [Turkish](./translations/tr/README.md) | [Ukrainian](./translations/uk/README.md) | [Urdu](./translations/ur/README.md) | [Vietnamese](./translations/vi/README.md)\n<!-- CO-OP TRANSLATOR LANGUAGES TABLE END -->\n\n #### Join Our Community\n\n[![Microsoft Foundry Discord](https://dcbadge.limes.pink/api/server/nTYy5BXMWG)](https://discord.gg/nTYy5BXMWG)\n\nWe have a Discord learn with AI series ongoing, learn more and join us at [Learn with AI Series](https://aka.ms/learnwithai/discord) from 18 - 30 September, 2025. You will get tips and tricks of using GitHub Copilot for Data Science.\n\n![Learn with AI series](/images/3.png)\n\n# Machine Learning for Beginners - A Curriculum\n\n> ðŸŒ Travel around the world as we explore Machine Learning by means of world cultures ðŸŒ\n\nCloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about **Machine Learning**. In this curriculum, you will learn about what is sometimes called **classic machine learning**, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our [AI for Beginners'' curriculum](https://aka.ms/ai4beginners). Pair these lessons with our [''Data Science for Beginners'' curriculum](https://aka.ms/ds4beginners), as well!\n\nTravel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to ''stick''.\n\n**âœï¸ Hearty thanks to our authors** Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, Ruth Yakubu and Amy Boyd\n\n**ðŸŽ¨ Thanks as well to our illustrators** Tomomi Imura, Dasani Madipalli, and Jen Looper\n\n**ðŸ™ Special thanks ðŸ™ to our Microsoft Student Ambassador authors, reviewers, and content contributors**, notably Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, and Snigdha Agarwal\n\n**ðŸ¤© Extra gratitude to Microsoft Student Ambassadors Eric Wanjau, Jasleen Sondhi, and Vidushi Gupta for our R lessons!**\n\n# Getting Started\n\nFollow these steps:\n1. **Fork the Repository**: Click on the "Fork" button at the top-right corner of this page.\n2. **Clone the Repository**:   `git clone https://github.com/microsoft/ML-For-Beginners.git`\n\n> [find all additional resources for this course in our Microsoft Learn collection](https://learn.microsoft.com/en-us/collections/qrqzamz1nn2wx3?WT.mc_id=academic-77952-bethanycheum)\n\n> ðŸ”§ **Need help?** Check our [Troubleshooting Guide](TROUBLESHOOTING.md) for solutions to common issues with installation, setup, and running lessons.\n\n\n**[Students](https://aka.ms/student-page)**, to use this curriculum, fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:\n\n- Start with a pre-lecture quiz.\n- Read the lecture and complete the activities, pausing and reflecting at each knowledge check.\n- Try to create the projects by comprehending the lessons rather than running the solution code; however that code is available in the `/solution` folders in each project-oriented lesson.\n- Take the post-lecture quiz.\n- Complete the challenge.\n- Complete the assignment.\n- After completing a lesson group, visit the [Discussion Board](https://github.com/microsoft/ML-For-Beginners/discussions) and "learn out loud" by filling out the appropriate PAT rubric. A ''PAT'' is a Progress Assessment Tool that is a rubric you fill out to further your learning. You can also react to other PATs so we can learn together.\n\n> For further study, we recommend following these [Microsoft Learn](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-77952-leestott) modules and learning paths.\n\n**Teachers**, we have [included some suggestions](for-teachers.md) on how to use this curriculum.\n\n---\n\n## Video walkthroughs\n\nSome of the lessons are available as short form video. You can find all these in-line in the lessons, or on the [ML for Beginners playlist on the Microsoft Developer YouTube channel](https://aka.ms/ml-beginners-videos) by clicking the image below.\n\n[![ML for beginners banner](./images/ml-for-beginners-video-banner.png)](https://aka.ms/ml-beginners-videos)\n\n---\n\n## Meet the Team\n\n[![Promo video](./images/ml.gif)](https://youtu.be/Tj1XWrDSYJU)\n\n**Gif by** [Mohit Jaisal](https://linkedin.com/in/mohitjaisal)\n\n> ðŸŽ¥ Click the image above for a video about the project and the folks who created it!\n\n---\n\n## Pedagogy\n\nWe have chosen two pedagogical tenets while building this curriculum: ensuring that it is hands-on **project-based** and that it includes **frequent quizzes**. In addition, this curriculum has a common **theme** to give it cohesion.\n\nBy ensuring that the content aligns with projects, the process is made more engaging for students and retention of concepts will be augmented. In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 12-week cycle. This curriculum also includes a postscript on real-world applications of ML, which can be used as extra credit or as a basis for discussion.\n\n> Find our [Code of Conduct](CODE_OF_CONDUCT.md), [Contributing](CONTRIBUTING.md), [Translation](TRANSLATIONS.md), and [Troubleshooting](TROUBLESHOOTING.md) guidelines. We welcome your constructive feedback!\n\n## Each lesson includes\n\n- optional sketchnote\n- optional supplemental video\n- video walkthrough (some lessons only)\n- [pre-lecture warmup quiz](https://ff-quizzes.netlify.app/en/ml/)\n- written lesson\n- for project-based lessons, step-by-step guides on how to build the project\n- knowledge checks\n- a challenge\n- supplemental reading\n- assignment\n- [post-lecture quiz](https://ff-quizzes.netlify.app/en/ml/)\n\n> **A note about languages**: These lessons are primarily written in Python, but many are also available in R. To complete an R lesson, go to the `/solution` folder and look for R lessons. They include an .rmd extension that represents an **R Markdown** file which can be simply defined as an embedding of `code chunks` (of R or other languages) and a `YAML header` (that guides how to format outputs such as PDF) in a `Markdown document`. As such, it serves as an exemplary authoring framework for data science since it allows you to combine your code, its output, and your thoughts by allowing you to write them down in Markdown. Moreover, R Markdown documents can be rendered to output formats such as PDF, HTML, or Word.\n\n> **A note about quizzes**: All quizzes are contained in [Quiz App folder](./quiz-app/), for 52 total quizzes of three questions each. They are linked from within the lessons but the quiz app can be run locally; follow the instruction in the `quiz-app` folder to locally host or deploy to Azure.\n\n| Lesson Number |                             Topic                              |                   Lesson Grouping                   | Learning Objectives                                                                                                             |                                                              Linked Lesson                                                               |                        Author                        |\n| :-----------: | :------------------------------------------------------------: | :-------------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------: |\n|      01       |                Introduction to machine learning                |      [Introduction](1-Introduction/README.md)       | Learn the basic concepts behind machine learning                                                                                |                                             [Lesson](1-Introduction/1-intro-to-ML/README.md)                                             |                       Muhammad                       |\n|      02       |                The History of machine learning                 |      [Introduction](1-Introduction/README.md)       | Learn the history underlying this field                                                                                         |                                            [Lesson](1-Introduction/2-history-of-ML/README.md)                                            |                     Jen and Amy                      |\n|      03       |                 Fairness and machine learning                  |      [Introduction](1-Introduction/README.md)       | What are the important philosophical issues around fairness that students should consider when building and applying ML models? |                                              [Lesson](1-Introduction/3-fairness/README.md)                                               |                        Tomomi                        |\n|      04       |                Techniques for machine learning                 |      [Introduction](1-Introduction/README.md)       | What techniques do ML researchers use to build ML models?                                                                       |                                          [Lesson](1-Introduction/4-techniques-of-ML/README.md)                                           |                    Chris and Jen                     |\n|      05       |                   Introduction to regression                   |        [Regression](2-Regression/README.md)         | Get started with Python and Scikit-learn for regression models                                                                  |         [Python](2-Regression/1-Tools/README.md) â€¢ [R](2-Regression/1-Tools/solution/R/lesson_1.html)         |      Jen â€¢ Eric Wanjau       |\n|      06       |                North American pumpkin prices ðŸŽƒ                |        [Regression](2-Regression/README.md)         | Visualize and clean data in preparation for ML                                                                                  |          [Python](2-Regression/2-Data/README.md) â€¢ [R](2-Regression/2-Data/solution/R/lesson_2.html)          |      Jen â€¢ Eric Wanjau       |\n|      07       |                North American pumpkin prices ðŸŽƒ                |        [Regression](2-Regression/README.md)         | Build linear and polynomial regression models                                                                                   |        [Python](2-Regression/3-Linear/README.md) â€¢ [R](2-Regression/3-Linear/solution/R/lesson_3.html)        |      Jen and Dmitry â€¢ Eric Wanjau       |\n|      08       |                North American pumpkin prices ðŸŽƒ                |        [Regression](2-Regression/README.md)         | Build a logistic regression model                                                                                               |     [Python](2-Regression/4-Logistic/README.md) â€¢ [R](2-Regression/4-Logistic/solution/R/lesson_4.html)      |      Jen â€¢ Eric Wanjau       |\n|      09       |                          A Web App ðŸ”Œ                          |           [Web App](3-Web-App/README.md)            | Build a web app to use your trained model                                                                                       |                                                 [Python](3-Web-App/1-Web-App/README.md)                                                  |                         Jen                          |\n|      10       |                 Introduction to classification                 |    [Classification](4-Classification/README.md)     | Clean, prep, and visualize your data; introduction to classification                                                            | [Python](4-Classification/1-Introduction/README.md) â€¢ [R](4-Classification/1-Introduction/solution/R/lesson_10.html)  | Jen and Cassie â€¢ Eric Wanjau |\n|      11       |             Delicious Asian and Indian cuisines ðŸœ             |    [Classification](4-Classification/README.md)     | Introduction to classifiers                                                                                                     | [Python](4-Classification/2-Classifiers-1/README.md) â€¢ [R](4-Classification/2-Classifiers-1/solution/R/lesson_11.html) | Jen and Cassie â€¢ Eric Wanjau |\n|      12       |             Delicious Asian and Indian cuisines ðŸœ             |    [Classification](4-Classification/README.md)     | More classifiers                                                                                                                | [Python](4-Classification/3-Classifiers-2/README.md) â€¢ [R](4-Classification/3-Classifiers-2/solution/R/lesson_12.html) | Jen and Cassie â€¢ Eric Wanjau |\n|      13       |             Delicious Asian and Indian cuisines ðŸœ             |    [Classification](4-Classification/README.md)     | Build a recommender web app using your model                                                                                    |                                              [Python](4-Classification/4-Applied/README.md)                                              |                         Jen                          |\n|      14       |                   Introduction to clustering                   |        [Clustering](5-Clustering/README.md)         | Clean, prep, and visualize your data; Introduction to clustering                                                                |         [Python](5-Clustering/1-Visualize/README.md) â€¢ [R](5-Clustering/1-Visualize/solution/R/lesson_14.html)         |      Jen â€¢ Eric Wanjau       |\n|      15       |              Exploring Nigerian Musical Tastes ðŸŽ§              |        [Clustering](5-Clustering/README.md)         | Explore the K-Means clustering method                                                                                           |           [Python](5-Clustering/2-K-Means/README.md) â€¢ [R](5-Clustering/2-K-Means/solution/R/lesson_15.html)           |      Jen â€¢ Eric Wanjau       |\n|      16       |        Introduction to natural language processing â˜•ï¸         |   [Natural language processing](6-NLP/README.md)    | Learn the basics about NLP by building a simple bot                                                                             |                                             [Python](6-NLP/1-Introduction-to-NLP/README.md)                                              |                       Stephen                        |\n|      17       |                      Common NLP Tasks â˜•ï¸                      |   [Natural language processing](6-NLP/README.md)    | Deepen your NLP knowledge by understanding common tasks required when dealing with language structures                          |                                                    [Python](6-NLP/2-Tasks/README.md)                                                     |                       Stephen                        |\n|      18       |             Translation and sentiment analysis â™¥ï¸              |   [Natural language processing](6-NLP/README.md)    | Translation and sentiment analysis with Jane Austen                                                                             |                                            [Python](6-NLP/3-Translation-Sentiment/README.md)                                             |                       Stephen                        |\n|      19       |                  Romantic hotels of Europe â™¥ï¸                  |   [Natural language processing](6-NLP/README.md)    | Sentiment analysis with hotel reviews 1                                                                                         |                                               [Python](6-NLP/4-Hotel-Reviews-1/README.md)                                                |                       Stephen                        |\n|      20       |                  Romantic hotels of Europe â™¥ï¸                  |   [Natural language processing](6-NLP/README.md)    | Sentiment analysis with hotel reviews 2                                                                                         |                                               [Python](6-NLP/5-Hotel-Reviews-2/README.md)                                                |                       Stephen                        |\n|      21       |            Introduction to time series forecasting             |        [Time series](7-TimeSeries/README.md)        | Introduction to time series forecasting                                                                                         |                                             [Python](7-TimeSeries/1-Introduction/README.md)                                              |                      Francesca                       |\n|      22       | âš¡ï¸ World Power Usage âš¡ï¸ - time series forecasting with ARIMA |        [Time series](7-TimeSeries/README.md)        | Time series forecasting with ARIMA                                                                                              |                                                 [Python](7-TimeSeries/2-ARIMA/README.md)                                                 |                      Francesca                       |\n|      23       |  âš¡ï¸ World Power Usage âš¡ï¸ - time series forecasting with SVR  |        [Time series](7-TimeSeries/README.md)        | Time series forecasting with Support Vector Regressor                                                                           |                                                  [Python](7-TimeSeries/3-SVR/README.md)                                                  |                       Anirban                        |\n|      24       |             Introduction to reinforcement learning             | [Reinforcement learning](8-Reinforcement/README.md) | Introduction to reinforcement learning with Q-Learning                                                                          |                                             [Python](8-Reinforcement/1-QLearning/README.md)                                              |                        Dmitry                        |\n|      25       |                 Help Peter avoid the wolf! ðŸº                  | [Reinforcement learning](8-Reinforcement/README.md) | Reinforcement learning Gym                                                                                                      |                                                [Python](8-Reinforcement/2-Gym/README.md)                                                 |                        Dmitry                        |\n|  Postscript   |            Real-World ML scenarios and applications            |      [ML in the Wild](9-Real-World/README.md)       | Interesting and revealing real-world applications of classical ML                                                               |                                             [Lesson](9-Real-World/1-Applications/README.md)                                              |                         Team                         |\n|  Postscript   |            Model Debugging in ML using RAI dashboard          |      [ML in the Wild](9-Real-World/README.md)       | Model Debugging in Machine Learning using Responsible AI dashboard components                                                              |                                             [Lesson](9-Real-World/2-Debugging-ML-Models/README.md)                                              |                         Ruth Yakubu                       |\n\n> [find all additional resources for this course in our Microsoft Learn collection](https://learn.microsoft.com/en-us/collections/qrqzamz1nn2wx3?WT.mc_id=academic-77952-bethanycheum)\n\n## Offline access\n\nYou can run this documentation offline by using [Docsify](https://docsify.js.org/#/). Fork this repo, [install Docsify](https://docsify.js.org/#/quickstart) on your local machine, and then in the root folder of this repo, type `docsify serve`. The website will be served on port 3000 on your localhost: `localhost:3000`.\n\n## PDFs\n\nFind a pdf of the curriculum with links [here](https://microsoft.github.io/ML-For-Beginners/pdf/readme.pdf).\n\n\n## ðŸŽ’ Other Courses \n\nOur team produces other courses! Check out:\n\n<!-- CO-OP TRANSLATOR OTHER COURSES START -->\n### Azure / Edge / MCP / Agents\n[![AZD for Beginners](https://img.shields.io/badge/AZD%20for%20Beginners-0078D4?style=for-the-badge&labelColor=E5E7EB&color=0078D4)](https://github.com/microsoft/AZD-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![Edge AI for Beginners](https://img.shields.io/badge/Edge%20AI%20for%20Beginners-00B8E4?style=for-the-badge&labelColor=E5E7EB&color=00B8E4)](https://github.com/microsoft/edgeai-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![MCP for Beginners](https://img.shields.io/badge/MCP%20for%20Beginners-009688?style=for-the-badge&labelColor=E5E7EB&color=009688)](https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![AI Agents for Beginners](https://img.shields.io/badge/AI%20Agents%20for%20Beginners-00C49A?style=for-the-badge&labelColor=E5E7EB&color=00C49A)](https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Generative AI Series\n[![Generative AI for Beginners](https://img.shields.io/badge/Generative%20AI%20for%20Beginners-8B5CF6?style=for-the-badge&labelColor=E5E7EB&color=8B5CF6)](https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (.NET)](https://img.shields.io/badge/Generative%20AI%20(.NET)-9333EA?style=for-the-badge&labelColor=E5E7EB&color=9333EA)](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (Java)](https://img.shields.io/badge/Generative%20AI%20(Java)-C084FC?style=for-the-badge&labelColor=E5E7EB&color=C084FC)](https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (JavaScript)](https://img.shields.io/badge/Generative%20AI%20(JavaScript)-E879F9?style=for-the-badge&labelColor=E5E7EB&color=E879F9)](https://github.com/microsoft/generative-ai-with-javascript?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Core Learning\n[![ML for Beginners](https://img.shields.io/badge/ML%20for%20Beginners-22C55E?style=for-the-badge&labelColor=E5E7EB&color=22C55E)](https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst)\n[![Data Science for Beginners](https://img.shields.io/badge/Data%20Science%20for%20Beginners-84CC16?style=for-the-badge&labelColor=E5E7EB&color=84CC16)](https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst)\n[![AI for Beginners](https://img.shields.io/badge/AI%20for%20Beginners-A3E635?style=for-the-badge&labelColor=E5E7EB&color=A3E635)](https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst)\n[![Cybersecurity for Beginners](https://img.shields.io/badge/Cybersecurity%20for%20Beginners-F97316?style=for-the-badge&labelColor=E5E7EB&color=F97316)](https://github.com/microsoft/Security-101?WT.mc_id=academic-96948-sayoung)\n[![Web Dev for Beginners](https://img.shields.io/badge/Web%20Dev%20for%20Beginners-EC4899?style=for-the-badge&labelColor=E5E7EB&color=EC4899)](https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst)\n[![IoT for Beginners](https://img.shields.io/badge/IoT%20for%20Beginners-14B8A6?style=for-the-badge&labelColor=E5E7EB&color=14B8A6)](https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst)\n[![XR Development for Beginners](https://img.shields.io/badge/XR%20Development%20for%20Beginners-38BDF8?style=for-the-badge&labelColor=E5E7EB&color=38BDF8)](https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Copilot Series\n[![Copilot for AI Paired Programming](https://img.shields.io/badge/Copilot%20for%20AI%20Paired%20Programming-FACC15?style=for-the-badge&labelColor=E5E7EB&color=FACC15)](https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst)\n[![Copilot for C#/.NET](https://img.shields.io/badge/Copilot%20for%20C%23/.NET-FBBF24?style=for-the-badge&labelColor=E5E7EB&color=FBBF24)](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst)\n[![Copilot Adventure](https://img.shields.io/badge/Copilot%20Adventure-FDE68A?style=for-the-badge&labelColor=E5E7EB&color=FDE68A)](https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst)\n<!-- CO-OP TRANSLATOR OTHER COURSES END -->\n\n## Getting Help\n\nIf you get stuck or have any questions about building AI apps. Join fellow learners and experienced developers in discussions about MCP. It''s a supportive community where questions are welcome and knowledge is shared freely.\n\n[![Microsoft Foundry Discord](https://dcbadge.limes.pink/api/server/nTYy5BXMWG)](https://discord.gg/nTYy5BXMWG)\n\nIf you have product feedback or errors while building visit:\n\n[![Microsoft Foundry Developer Forum](https://img.shields.io/badge/GitHub-Microsoft_Foundry_Developer_Forum-blue?style=for-the-badge&logo=github&color=000000&logoColor=fff)](https://aka.ms/foundry/forum)\n', '{"language":"Jupyter Notebook","stars":80313,"forks":18850,"watchers":80313,"open_issues":6,"topics":["data-science","education","machine-learning","machine-learning-algorithms","machinelearning","machinelearning-python","microsoft-for-beginners","ml","python","r","scikit-learn","scikit-learn-python"],"default_branch":"main","size_kb":1628865,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:microsoft:ML-For-Beginners","source_url":"https://github.com/microsoft/ML-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:ML-For-Beginners.git`","source_url":"https://github.com/microsoft/ML-For-Beginners.git`"},{"type":"has_code","target_id":"github:microsoft:ML-For-Beginners","source_url":"https://github.com/microsoft/ML-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:AZD-for-beginners","source_url":"https://github.com/microsoft/AZD-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:edgeai-for-beginners","source_url":"https://github.com/microsoft/edgeai-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:mcp-for-beginners","source_url":"https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:ai-agents-for-beginners","source_url":"https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-for-beginners","source_url":"https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:Generative-AI-for-beginners-dotnet","source_url":"https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-for-beginners-java","source_url":"https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-with-javascript","source_url":"https://github.com/microsoft/generative-ai-with-javascript?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:Security-101","source_url":"https://github.com/microsoft/Security-101?WT.mc_id=academic-96948-sayoung"},{"type":"has_code","target_id":"github:microsoft:xr-development-for-beginners","source_url":"https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:mastering-github-copilot-for-dotnet-csharp-developers","source_url":"https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:CopilotAdventures","source_url":"https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst"}]', NULL, 'MIT', 'approved', 80, 'a5fb71fa6b711cc8cb5a0b720165ba52', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-ML-For-Beginners from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-ML-For-Beginners.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-netdata-netdata', 'github--netdata--netdata', 'netdata', 'netdata', '<p align="center"> <a href="https://www.netdata.cloud#gh-light-mode-only"> <img src="https://www.netdata.cloud/img/readme-images/netdata_readme_logo_light.png" alt="Netdata" width="300"/> </a> <a href="https://www.netdata.cloud#gh-dark-mode-only"> <img src="https://www.netdata.cloud/img/readme-images/netdata_readme_logo_dark.png" alt="Netdata" width="300"/> </a> </p> <h3 align="center">X-Ray Vision for your infrastructure!</h3> <h4 align="center">Every Metric, Every Second. No BS.</h4> <br />...', '["ai","alerting","cncf","data-visualization","database","devops","docker","grafana","influxdb","kubernetes","linux","machine-learning","mcp","mongodb","monitoring","mysql","netdata","observability","postgresql","prometheus","c"]', 'other', 76940, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/netdata/netdata","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n<a href="https://www.netdata.cloud#gh-light-mode-only">\n  <img src="https://www.netdata.cloud/img/readme-images/netdata_readme_logo_light.png" alt="Netdata" width="300"/>\n</a>\n<a href="https://www.netdata.cloud#gh-dark-mode-only">\n  <img src="https://www.netdata.cloud/img/readme-images/netdata_readme_logo_dark.png" alt="Netdata" width="300"/>\n</a>\n</p>\n<h3 align="center">X-Ray Vision for your infrastructure!</h3>\n<h4 align="center">Every Metric, Every Second. No BS.</h4>\n\n<br />\n<p align="center">\n  <a href="https://github.com/netdata/netdata/"><img src="https://img.shields.io/github/stars/netdata/netdata?style=social" alt="GitHub Stars"></a>\n  <br />\n  <a href="https://app.netdata.cloud/spaces/netdata-demo?utm_campaign=github_readme_demo_badge"><img src="https://img.shields.io/badge/Live%20Demo-green" alt="Live Demo"></a>\n  <a href="https://github.com/netdata/netdata/releases/latest"><img src="https://img.shields.io/github/release/netdata/netdata.svg" alt="Latest release"></a>\n  <a href="https://github.com/netdata/netdata-nightlies/releases/latest"><img src="https://img.shields.io/github/release/netdata/netdata-nightlies.svg" alt="Latest nightly build"></a>\n  <br/>\n  <a href="https://community.netdata.cloud"><img alt="Discourse topics" src="https://img.shields.io/discourse/topics?server=https%3A%2F%2Fcommunity.netdata.cloud%2F&logo=discourse&label=discourse%20forum"></a>\n  <a href="https://github.com/netdata/netdata/discussions"><img alt="GitHub Discussions" src="https://img.shields.io/github/discussions/netdata/netdata?logo=github&label=github%20discussions"></a>\n  <br/>\n  <a href="https://bestpractices.coreinfrastructure.org/projects/2231"><img src="https://bestpractices.coreinfrastructure.org/projects/2231/badge" alt="CII Best Practices"></a>\n  <a href="https://scan.coverity.com/projects/netdata-netdata?tab=overview"><img alt="Coverity Scan" src="https://img.shields.io/coverity/scan/netdata"></a>\n</p>\n\n<p align="center">\n  <a href="https://registry.my-netdata.io/#menu_netdata_submenu_registry"><img src="https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_entries&dimensions=persons&label=user%20base&units=M&value_color=blue&precision=2&divide=1000000&options=unaligned&tier=1&v44" alt="User base"></a>\n  <a href="https://registry.my-netdata.io/#menu_netdata_submenu_registry"><img src="https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_entries&dimensions=machines&label=servers%20monitored&units=M&divide=1000000&value_color=orange&precision=2&options=unaligned&tier=1&v44" alt="Servers monitored"></a>\n  <a href="https://registry.my-netdata.io/#menu_netdata_submenu_registry"><img src="https://registry.my-netdata.io/api/v3/badge.svg?chart=netdata.registry_sessions&label=sessions%20served&units=M&value_color=yellowgreen&precision=2&divide=1000000&options=unaligned&tier=1&v44" alt="Sessions served"></a>\n  <a href="https://hub.docker.com/r/netdata/netdata"><img src="https://registry.my-netdata.io/api/v3/badge.svg?chart=dockerhub.pulls_sum&divide=1000000&precision=1&units=M&label=docker+hub+pulls&options=unaligned&tier=1&v44" alt="Docker Hub pulls"></a>\n</p>\n<p align="center"><b>Visit our <a href="https://www.netdata.cloud">Home Page</a></b></p>\n\n<hr class="solid">\n\nMENU: **[WHO WE ARE](#who-we-are)** | **[KEY FEATURES](#key-features)** | **[GETTING STARTED](#getting-started)** | **[HOW IT WORKS](#how-it-works)** | **[FAQ](#faq)** | **[DOCS](#book-documentation)** | **[COMMUNITY](#tada-community)** | **[CONTRIBUTE](#pray-contribute)** | **[LICENSE](#scroll-license)**\n\n\n> [!WARNING]\n> People **get addicted to Netdata.**\n> Once you use it on your systems, *there''s no going back.*\n\n[![Platforms](https://img.shields.io/badge/Platforms-Linux%20%7C%20macOS%20%7C%20FreeBSD%20%7C%20Windows-blue)]()\n\n---\n\n## WHO WE ARE\n\nNetdata is an open-source, real-time infrastructure monitoring platform. Monitor, detect, and act across your entire infrastructure.\n\n**Core Advantages:**\n\n* **Instant Insights** â€“ With Netdata you can access per-second metrics and visualizations.\n* **Zero Configuration** â€“ You can deploy immediately without complex setup.\n* **ML-Powered** â€“ You can detect anomalies, predict issues, and automate analysis.\n* **Efficient** â€“ You can monitor with minimal resource usage and maximum scalability.\n* **Secure & Distributed** â€“ You can keep your data local with no central collection needed.\n\nWith Netdata, you get real-time, per-second updates. Clear **insights at a glance**, no complexity.\n\n<details>\n  <summary><strong>All heroes have a great origin story. Click to discover ours.</strong></summary>\n  <br/>\n\nIn 2013, at the company where Costa Tsaousis was COO, a significant percentage of their cloud-based transactions failed silently, severely impacting business performance.\n\nCosta and his team tried every troubleshooting tool available at the time. None could identify the root cause. As Costa later wrote:\n\nâ€œ*I couldnâ€™t believe that monitoring systems provide so few metrics and with such low resolution, scale so badly, and cost so much to run.*â€\n\nFrustrated, he decided to build his own monitoring tool, starting from scratch.\n\nThat decision led to countless late nights and weekends. It also sparked a fundamental shift in how infrastructure monitoring and troubleshooting are approached, both in method and in cost.\n</details>\n\n### Most Energy-Efficient Monitoring Tool\n\n<p align="center">\n<a href="https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf#gh-dark-mode-only">\n  <img src="https://github.com/netdata/netdata/assets/139226121/7118757a-38fb-48d7-b12a-53e709a8e8c0" alt="Energy Efficiency" width="800"/>\n</a>\n<a href="https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf#gh-light-mode-only">\n  <img src="https://github.com/netdata/netdata/assets/139226121/4f64cbb6-05e4-48e3-b7c0-d1b79e37e219" alt="Energy efficiency" width="800"/>\n</a>\n</p>\n\nAccording to the [University of Amsterdam study](https://www.ivanomalavolta.com/files/papers/ICSOC_2023.pdf), Netdata is the most energy-efficient tool for monitoring Docker-based systems. The study also shows Netdata excels in CPU usage, RAM usage, and execution time compared to other monitoring solutions.\n\n---\n\n## Key Features\n\n| Feature                    | Description                               | What Makes It Unique                                     |\n|----------------------------|-------------------------------------------|----------------------------------------------------------|\n| **Real-Time**              | Per-second data collection and processing | Works in a beat â€“ click and see results instantly        |\n| **Zero-Configuration**     | Automatic detection and discovery         | Auto-discovers everything on the nodes it runs           |\n| **ML-Powered**             | Unsupervised anomaly detection            | Trains multiple ML models per metric at the edge         |\n| **Long-Term Retention**    | High-performance storage                  | ~0.5 bytes per sample with tiered storage for archiving  |\n| **Advanced Visualization** | Rich, interactive dashboards              | Slice and dice data without query language               |\n| **Extreme Scalability**    | Native horizontal scaling                 | Parent-Child centralization with multi-million samples/s |\n| **Complete Visibility**    | From infrastructure to applications       | Simplifies operations and eliminates silos               |\n| **Edge-Based**             | Processing at your premises               | Distributes code instead of centralizing data            |\n\n> [!NOTE]  \n> Want to put Netdata to the test against Prometheus?\n> Explore the [full comparison](https://www.netdata.cloud/blog/netdata-vs-prometheus-2025/).\n\n---\n\n## Netdata Ecosystem\n\nThis three-part architecture enables you to scale from single nodes to complex multi-cloud environments:\n\n| Component         | Description                                                                                                                                                 | License                                         |\n|-------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------|\n| **Netdata Agent** | â€¢ Core monitoring engine<br>â€¢ Handles collection, storage, ML, alerts, exports<br>â€¢ Runs on servers, cloud, K8s, IoT<br>â€¢ Zero production impact            | [GPL v3+](https://www.gnu.org/licenses/gpl-3.0) |\n| **Netdata Cloud** | â€¢ Enterprise features<br>â€¢ User management, RBAC, horizontal scaling<br>â€¢ Centralized alerts<br>â€¢ Free community tier<br>â€¢ No metric storage centralization |                                                 |\n| **Netdata UI**    | â€¢ Dashboards and visualizations<br>â€¢ Free to use<br>â€¢ Included in standard packages<br>â€¢ Latest version via CDN                                             | [NCUL1](https://app.netdata.cloud/LICENSE.txt)  |\n\n## What You Can Monitor\n\nWith Netdata you can monitor all these components across platforms:\n\n|                                                                                                   Component |              Linux               | FreeBSD | macOS |                      Windows                      |\n|------------------------------------------------------------------------------------------------------------:|:--------------------------------:|:-------:|:-----:|:-------------------------------------------------:|\n|                             **System Resources**<small><br/>CPU, Memory and system shared resources</small> |               Full               |   Yes   |  Yes  |                        Yes                        |\n|                                **Storage**<small><br/>Disks, Mount points, Filesystems, RAID arrays</small> |               Full               |   Yes   |  Yes  |                        Yes                        |\n|                                 **Network**<small><br/>Network Interfaces, Protocols, Firewall, etc</small> |               Full               |   Yes   |  Yes  |                        Yes                        |\n|                        **Hardware & Sensors**<small><br/>Fans, Temperatures, Controllers, GPUs, etc</small> |               Full               |  Some   | Some  |                       Some                        |\n|                                       **O/S Services**<small><br/>Resources, Performance and Status</small> | Yes<small><br/>`systemd`</small> |    -    |   -   |                         -                         |\n|                                      **Processes**<small><br/>Resources, Performance, OOM, and more</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n|                                                                             System and Application **Logs** | Yes<small><br/>`systemd`-journal |    -    |   -   | Yes<small><br/>`Windows Event Log`, `ETW`</small> |\n|                                 **Network Connections**<small><br/>Live TCP and UDP sockets per PID</small> |               Yes                |    -    |   -   |                         -                         |\n|                               **Containers**<small><br/>Docker/containerd, LXC/LXD, Kubernetes, etc</small> |               Yes                |    -    |   -   |                         -                         |\n|                                 **VMs** (from the host)<small><br/>KVM, qemu, libvirt, Proxmox, etc</small> | Yes<small><br/>`cgroups`</small> |    -    |   -   |         Yes<small><br/>`Hyper-V`</small>          |\n|                       **Synthetic Checks**<small><br/>Test APIs, TCP ports, Ping, Certificates, etc</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n| **Packaged Applications**<small><br/>nginx, apache, postgres, redis, mongodb,<br/>and hundreds more</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n|                              **Cloud Provider Infrastructure**<small><br/>AWS, GCP, Azure, and more</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n|                       **Custom Applications**<small><br/>OpenMetrics, StatsD and soon OpenTelemetry</small> |               Yes                |   Yes   |  Yes  |                        Yes                        |\n\nOn Linux, you can continuously monitor all kernel features and hardware sensors for errors, including Intel/AMD/Nvidia GPUs, PCI AER, RAM EDAC, IPMI, S.M.A.R.T, Intel RAPL, NVMe, fans, power supplies, and voltage readings.\n\n---\n\n## Getting Started\n\nYou can install Netdata on all major operating systems. To begin:\n\n### 1. Install Netdata\n\nChoose your platform and follow the installation guide:\n\n* [Linux Installation](https://learn.netdata.cloud/docs/installing/one-line-installer-for-all-linux-systems)\n* [macOS](https://learn.netdata.cloud/docs/installing/macos)\n* [FreeBSD](https://learn.netdata.cloud/docs/installing/freebsd)\n* [Windows](https://learn.netdata.cloud/docs/netdata-agent/installation/windows)\n* [Docker Guide](/packaging/docker/README.md)\n* [Kubernetes Setup](https://learn.netdata.cloud/docs/installation/install-on-specific-environments/kubernetes)\n\n> [!NOTE]\n> You can access the Netdata UI at `http://localhost:19999` (or `http://NODE:19999` if remote).\n\n### 2. Configure Collectors\n\nNetdata auto-discovers most metrics, but you can manually configure some collectors:\n\n* [All collectors](https://learn.netdata.cloud/docs/data-collection/)\n* [SNMP monitoring](https://learn.netdata.cloud/docs/data-collection/monitor-anything/networking/snmp)\n\n### 3. Configure Alerts\n\nYou can use hundreds of built-in alerts and integrate with:\n\n`email`, `Slack`, `Telegram`, `PagerDuty`, `Discord`, `Microsoft Teams`, and more.\n\n> [!NOTE]  \n> Email alerts work by default if there''s a configured MTA.\n\n### 4. Configure Parents\n\nYou can centralize dashboards, alerts, and storage with Netdata Parents:\n\n* [Streaming Reference](https://learn.netdata.cloud/docs/streaming/streaming-configuration-reference)\n\n> [!NOTE]  \n> You can use Netdata Parents for central dashboards, longer retention, and alert configuration.\n\n### 5. Connect to Netdata Cloud\n\n[Sign in to Netdata Cloud](https://app.netdata.cloud/sign-in) and connect your nodes for:\n\n* Access from anywhere\n* Horizontal scalability and multi-node dashboards\n* UI configuration for alerts and data collection\n* Role-based access control\n* Free tier available\n\n> [!NOTE]  \n> Netdata Cloud is optional. Your data stays in your infrastructure.\n\n## Live Demo Sites\n\n<p align="center">\n  <b>See Netdata in action</b><br/>\n  <a href="https://frankfurt.netdata.rocks"><b>FRANKFURT</b></a> |\n  <a href="https://newyork.netdata.rocks"><b>NEWYORK</b></a> |\n  <a href="https://atlanta.netdata.rocks"><b>ATLANTA</b></a> |\n  <a href="https://sanfrancisco.netdata.rocks"><b>SANFRANCISCO</b></a> |\n  <a href="https://toronto.netdata.rocks"><b>TORONTO</b></a> |\n  <a href="https://singapore.netdata.rocks"><b>SINGAPORE</b></a> |\n  <a href="https://bangalore.netdata.rocks"><b>BANGALORE</b></a>\n  <br/>\n  <i>These demo clusters run with default configuration and show real monitoring data.</i>\n  <br/>\n  <i>Choose the instance closest to you for the best performance.</i>\n</p>\n\n---\n\n## How It Works\n\nWith Netdata you can run a modular pipeline for metrics collection, processing, and visualization.\n\n```mermaid\nflowchart TB\n  A[Netdata Agent]:::mainNode\n  A1(Collect):::green --> A\n  A2(Store):::green --> A\n  A3(Learn):::green --> A\n  A4(Detect):::green --> A\n  A5(Check):::green --> A\n  A6(Stream):::green --> A\n  A7(Archive):::green --> A\n  A8(Query):::green --> A\n  A9(Score):::green --> A\n\n  classDef green fill:#bbf3bb,stroke:#333,stroke-width:1px,color:#000\n  classDef mainNode fill:#f0f0f0,stroke:#333,stroke-width:1px,color:#333\n```\n\nWith each Agent you can:\n\n1. **Collect** â€“ Gather metrics from systems, containers, apps, logs, APIs, and synthetic checks.\n2. **Store** â€“ Save metrics to a high-efficiency, tiered time-series database.\n3. **Learn** â€“ Train ML models per metric using recent behavior.\n4. **Detect** â€“ Identify anomalies using trained ML models.\n5. **Check** â€“ Evaluate metrics against pre-set or custom alert rules.\n6. **Stream** â€“ Send metrics to Netdata Parents in real time.\n7. **Archive** â€“ Export metrics to Prometheus, InfluxDB, OpenTSDB, Graphite, and others.\n8. **Query** â€“ Access metrics via an API for dashboards or third-party tools.\n9. **Score** â€“ Use a scoring engine to find patterns and correlations across metrics.\n\n> [!NOTE]  \n> Learn more: [Netdata''s architecture](https://learn.netdata.cloud/docs/netdata-agent/#distributed-observability-pipeline)\n\n## Agent Capabilities\n\nWith the Netdata Agent, you can use these core capabilities out-of-the-box:\n\n| Capability                   | Description                                                                                                                                   |\n|------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|\n| **Comprehensive Collection** | â€¢ 800+ integrations<br>â€¢ Systems, containers, VMs, hardware sensors<br>â€¢ OpenMetrics, StatsD, and logs<br>â€¢ OpenTelemetry support coming soon |\n| **Performance & Precision**  | â€¢ Per-second collection<br>â€¢ Real-time visualization with 1-second latency<br>â€¢ High-resolution metrics                                       |\n| **Edge-Based ML**            | â€¢ ML models trained at the edge<br>â€¢ Automatic anomaly detection per metric<br>â€¢ Pattern recognition based on historical behavior             |\n| **Advanced Log Management**  | â€¢ Direct systemd-journald and Windows Event Log integration<br>â€¢ Process logs at the edge<br>â€¢ Rich log visualization                         |\n| **Observability Pipeline**   | â€¢ Parent-Child relationships<br>â€¢ Flexible centralization<br>â€¢ Multi-level replication and retention                                          |\n| **Automated Visualization**  | â€¢ NIDL data model<br>â€¢ Auto-generated dashboards<br>â€¢ No query language needed                                                                |\n| **Smart Alerting**           | â€¢ Pre-configured alerts<br>â€¢ Multiple notification methods<br>â€¢ Proactive detection                                                           |\n| **Low Maintenance**          | â€¢ Auto-detection<br>â€¢ Zero-touch ML<br>â€¢ Easy scalability<br>â€¢ CI/CD friendly                                                                 |\n| **Open & Extensible**        | â€¢ Modular architecture<br>â€¢ Easy to customize<br>â€¢ Integrates with existing tools                                                             |\n\n---\n\n## CNCF Membership\n\n<p align="center">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/white/cncf-white.svg">\n    <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/color/cncf-color.svg">\n    <img alt="CNCF Logo" src="https://raw.githubusercontent.com/cncf/artwork/master/other/cncf/horizontal/color/cncf-color.svg" width="300">\n  </picture>\n  <br />\n  Netdata actively supports and is a member of the Cloud Native Computing Foundation (CNCF).<br />\n  It is one of the most starred projects in the <a href="https://landscape.cncf.io/?item=observability-and-analysis--observability--netdata">CNCF landscape</a>.\n</p>\n\n---\n\n## FAQ\n\n<details>\n<summary><strong>Is Netdata secure?</strong></summary>\n<br/>\n\nYes. Netdata follows [OpenSSF best practices](https://bestpractices.coreinfrastructure.org/en/projects/2231), has a security-first design, and is regularly audited by the community.\n\n* [Security design](https://learn.netdata.cloud/docs/security-and-privacy-design)\n* [Security policies and advisories](https://github.com/netdata/netdata/security)\n\n</details>\n\n<details>\n<summary><strong>Does Netdata use a lot of resources?</strong></summary>\n<br/>\n\nNo. Even with ML and per-second metrics, Netdata uses minimal resources.\n\n* \~5% CPU and 150MiB RAM by default on production systems\n* <1% CPU and \~100MiB RAM when ML and alerts are disabled and using ephemeral storage\n* Parents scale to millions of metrics per second with appropriate hardware\n\n> You can use the **Netdata Monitoring** section in the dashboard to inspect its resource usage.\n\n</details>\n\n<details>\n<summary><strong>How much data retention is possible?</strong></summary>\n<br/>\n\nAs much as your disk allows.\n\nWith Netdata you can use tiered retention:\n\n* Tier 0: per-second resolution\n* Tier 1: per-minute resolution\n* Tier 2: per-hour resolution\n\nThese are queried automatically based on the zoom level.\n</details>\n\n<details>\n<summary><strong>Can Netdata scale to many servers?</strong></summary>\n<br/>\n\nYes. With Netdata you can:\n\n* Scale horizontally with many Agents\n* Scale vertically with powerful Parents\n* Scale infinitely via Netdata Cloud\n\n> You can use Netdata Cloud to merge many independent infrastructures into one logical view.\n\n</details>\n\n<details>\n<summary><strong>Is disk I/O a concern?</strong></summary>\n<br/>\n\nNo. Netdata minimizes disk usage:\n\n* Metrics are flushed to disk every 17 minutes, spread out evenly\n* Uses direct I/O and compression (ZSTD)\n* Can run entirely in RAM or stream to a Parent\n\n> You can use `alloc` or `ram` mode for no disk writes.\n\n</details>\n\n<details>\n<summary><strong>How is Netdata different from Prometheus + Grafana?</strong></summary>\n<br/>\n\nWith Netdata you get a complete monitoring solutionâ€”not just tools.\n\n* No manual setup or dashboards needed\n* Built-in ML, alerts, dashboards, and correlations\n* More efficient and easier to deploy\n\n> [Performance comparison](https://blog.netdata.cloud/netdata-vs-prometheus-performance-analysis/)\n\n</details>\n\n<details>\n<summary><strong>How is Netdata different from commercial SaaS tools?</strong></summary>\n<br/>\n\nWith Netdata you can store all metrics on your infrastructureâ€”no sampling, no aggregation, no loss.\n\n* High-resolution metrics by default\n* ML per metric, not shared models\n* Unlimited scalability without skyrocketing cost\n\n</details>\n\n<details>\n<summary><strong>Can Netdata run alongside Nagios, Zabbix, etc.?</strong></summary>\n<br/>\n\nYes. You can use Netdata together with traditional tools.\n\nWith Netdata you get:\n\n* Real-time, high-resolution monitoring\n* Zero configuration and auto-generated dashboards\n* Anomaly detection and advanced visualization\n\n</details>\n\n<details>\n<summary><strong>What if I feel overwhelmed?</strong></summary>\n<br/>\n\nYou can start small:\n\n* Use the dashboard''s table of contents and search\n* Explore anomaly scoring ("AR" toggle)\n* Create custom dashboards in Netdata Cloud\n\n> [Docs and guides](https://learn.netdata.cloud/guides)\n\n</details>\n\n<details>\n<summary><strong>Do I have to use Netdata Cloud?</strong></summary>\n<br/>\n\nNo. Netdata Cloud is optional.\n\nNetdata works without it, but with Cloud you can:\n\n* Access remotely with SSO\n* Save dashboard customizations\n* Configure alerts centrally\n* Collaborate with role-based access\n\n</details>\n\n<details>\n<summary><strong>What telemetry does Netdata collect?</strong></summary>\n<br/>\n\nAnonymous telemetry helps improve the product. You can disable it:\n\n* Add `--disable-telemetry` to the installer, or\n* Create `/etc/netdata/.opt-out-from-anonymous-statistics` and restart Netdata\n\n> Telemetry helps us understand usage, not track users. No private data is collected.\n\n</details>\n\n<details>\n<summary><strong>Who uses Netdata?</strong></summary>\n<br/>\n\nYou''ll join users including:\n\n* Major companies (Amazon, ABN AMRO Bank, Facebook, Google, IBM, Intel, Netflix, Samsung)\n* Universities (NYU, Columbia, Seoul National, UCL)\n* Government organizations worldwide\n* Infrastructure-intensive organizations\n* Technology operators\n* Startups and freelancers\n* SysAdmins and DevOps professionals\n\n</details>\n\n---\n\n## \:book: Documentation\n\nVisit [Netdata Learn](https://learn.netdata.cloud) for full documentation and guides.\n\n> [!NOTE]  \n> Includes deployment, configuration, alerting, exporting, troubleshooting, and more.\n\n---\n\n## \:tada: Community\n\nJoin the Netdata community:\n\n* [Discord](https://discord.com/invite/2mEmfW735j)\n* [Forum](https://community.netdata.cloud)\n* [GitHub Discussions](https://github.com/netdata/netdata/discussions)\n\n> [!NOTE]  \n> [Code of Conduct](https://github.com/netdata/.github/blob/main/CODE_OF_CONDUCT.md)\n\nFollow us on:\n[Twitter](https://twitter.com/netdatahq) | [Reddit](https://www.reddit.com/r/netdata/) | [YouTube](https://www.youtube.com/c/Netdata) | [LinkedIn](https://www.linkedin.com/company/netdata-cloud/)\n\n---\n\n## \:pray: Contribute\n\nWe welcome your contributions.\n\nWays you help us stay sharp:\n\n* Share best practices and monitoring insights\n* Report issues or missing features\n* Improve documentation\n* Develop new integrations or collectors\n* Help users in forums and chats\n\n> [!NOTE]  \n> [Contribution guide](https://github.com/netdata/.github/blob/main/CONTRIBUTING.md)\n\n---\n\n## \:scroll: License\n\nThe Netdata ecosystem includes:\n\n* **Netdata Agent** â€“ Open-source core (GPLv3+). **Includes** data collection, storage, ML, alerting, APIs and **redistributes** several other open-source tools and libraries.\n    * [Netdata Agent License](https://github.com/netdata/netdata/blob/master/LICENSE)\n    * [Netdata Agent Redistributed](https://github.com/netdata/netdata/blob/master/REDISTRIBUTED.md)\n* **Netdata UI** â€“ Closed-source but free to use with Netdata Agent and Cloud. Delivered via CDN. It integrates third-party open-source components.\n    * [Netdata Cloud UI License](https://app.netdata.cloud/LICENSE.txt)\n    * [Netdata UI third-party licenses](https://app.netdata.cloud/3D_PARTY_LICENSES.txt)\n* **Netdata Cloud** â€“ Closed-source, with free and paid tiers. Adds remote access, SSO, scalability.\n', '{"language":"C","stars":76940,"forks":6258,"watchers":76940,"open_issues":245,"topics":["ai","alerting","cncf","data-visualization","database","devops","docker","grafana","influxdb","kubernetes","linux","machine-learning","mcp","mongodb","monitoring","mysql","netdata","observability","postgresql","prometheus"],"default_branch":"master","size_kb":245763,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"},{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"},{"type":"has_code","target_id":"github:netdata:netdata-nightlies","source_url":"https://github.com/netdata/netdata-nightlies"},{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"},{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"},{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"},{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"},{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"},{"type":"has_code","target_id":"github:netdata:.github","source_url":"https://github.com/netdata/.github"},{"type":"has_code","target_id":"github:netdata:.github","source_url":"https://github.com/netdata/.github"},{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"},{"type":"has_code","target_id":"github:netdata:netdata","source_url":"https://github.com/netdata/netdata"}]', NULL, 'GPL-3.0', 'approved', 80, '7fdacde065c8cfc971cb3bdfbc2af887', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-netdata-netdata from https://github.com/netdata.png
Image converted to WebP: data/images/github-netdata-netdata.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-d2l-ai-d2l-zh', 'github--d2l-ai--d2l-zh', 'd2l-zh', 'd2l-ai', 'ç¬¬äºŒç‰ˆï¼šzh.D2L.ai | ç¬¬ä¸€ç‰ˆï¼šzh-v1.D2L.ai | å®‰è£…å’Œä½¿ç”¨ä¹¦ä¸­æºä»£ç ï¼š ç¬¬äºŒç‰ˆ ç¬¬ä¸€ç‰ˆ <h5 align="center"><i>ç†è§£æ·±åº¦å­¦ä¹ çš„æœ€ä½³æ–¹æ³•æ˜¯å­¦ä»¥è‡´ç”¨ã€‚</i></h5> <p align="center"> <img width="200" src="static/frontpage/_images/eq.jpg"> <img width="200" src="static/frontpage/_images/figure.jpg"> <img width="200" src="static/frontpage/_images/code.jpg"> <img width="200" src="static/frontpage/_images/notebook.gif"> </p> æœ¬å¼€æºé¡¹ç›®ä»£è¡¨äº†æˆ‘ä»¬çš„ä¸€ç§å°è¯•ï¼šæˆ‘ä»¬å°†æ•™ç»™è¯»è€…æ¦‚å¿µã€èƒŒæ™¯çŸ¥è¯†å’Œä»£ç ï¼›æˆ‘ä»¬å°†åœ¨åŒä¸€ä¸ªåœ°æ–¹é˜è¿°å‰–æžé—®é¢˜æ‰€éœ€çš„æ‰¹åˆ¤æ€§æ€ç»´ã€è§£å†³é—®é¢˜æ‰€éœ€çš„æ•°å­¦çŸ¥è¯†ï¼Œä»¥åŠå®žçŽ°è§£å†³æ–¹æ¡ˆæ‰€éœ€çš„å·¥ç¨‹æŠ€èƒ½ã€‚ æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªä¸ºå®žçŽ°ä»¥ä¸‹ç›®æ ‡çš„ç»Ÿä¸€èµ„æºï¼š 1. æ‰€æœ‰äººå‡å¯åœ¨ç½‘ä¸Šå…è´¹èŽ·å–ï¼› 1. æä¾›...', '["book","chinese","computer-vision","deep-learning","machine-learning","natural-language-processing","notebook","python","python"]', 'other', 74263, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/d2l-ai/d2l-zh","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ï¼ˆDive into Deep Learningï¼ŒD2L.aiï¼‰\n\n[ç¬¬äºŒç‰ˆï¼šzh.D2L.ai](https://zh.d2l.ai)  | [ç¬¬ä¸€ç‰ˆï¼šzh-v1.D2L.ai](https://zh-v1.d2l.ai/) |  å®‰è£…å’Œä½¿ç”¨ä¹¦ä¸­æºä»£ç ï¼š [ç¬¬äºŒç‰ˆ](https://zh.d2l.ai/chapter_installation/index.html) [ç¬¬ä¸€ç‰ˆ](https://zh-v1.d2l.ai/chapter_prerequisite/install.html)\n\n<h5 align="center"><i>ç†è§£æ·±åº¦å­¦ä¹ çš„æœ€ä½³æ–¹æ³•æ˜¯å­¦ä»¥è‡´ç”¨ã€‚</i></h5>\n\n<p align="center">\n  <img width="200"  src="static/frontpage/_images/eq.jpg">\n  <img width="200"  src="static/frontpage/_images/figure.jpg">\n  <img width="200"  src="static/frontpage/_images/code.jpg">\n  <img width="200"  src="static/frontpage/_images/notebook.gif">\n</p>\n\næœ¬å¼€æºé¡¹ç›®ä»£è¡¨äº†æˆ‘ä»¬çš„ä¸€ç§å°è¯•ï¼šæˆ‘ä»¬å°†æ•™ç»™è¯»è€…æ¦‚å¿µã€èƒŒæ™¯çŸ¥è¯†å’Œä»£ç ï¼›æˆ‘ä»¬å°†åœ¨åŒä¸€ä¸ªåœ°æ–¹é˜è¿°å‰–æžé—®é¢˜æ‰€éœ€çš„æ‰¹åˆ¤æ€§æ€ç»´ã€è§£å†³é—®é¢˜æ‰€éœ€çš„æ•°å­¦çŸ¥è¯†ï¼Œä»¥åŠå®žçŽ°è§£å†³æ–¹æ¡ˆæ‰€éœ€çš„å·¥ç¨‹æŠ€èƒ½ã€‚\n\næˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªä¸ºå®žçŽ°ä»¥ä¸‹ç›®æ ‡çš„ç»Ÿä¸€èµ„æºï¼š\n1. æ‰€æœ‰äººå‡å¯åœ¨ç½‘ä¸Šå…è´¹èŽ·å–ï¼›\n1. æä¾›è¶³å¤Ÿçš„æŠ€æœ¯æ·±åº¦ï¼Œä»Žè€Œå¸®åŠ©è¯»è€…å®žé™…æˆä¸ºæ·±åº¦å­¦ä¹ åº”ç”¨ç§‘å­¦å®¶ï¼šæ—¢ç†è§£æ•°å­¦åŽŸç†ï¼Œåˆèƒ½å¤Ÿå®žçŽ°å¹¶ä¸æ–­æ”¹è¿›æ–¹æ³•ï¼›\n1. åŒ…å«å¯è¿è¡Œçš„ä»£ç ï¼Œä¸ºè¯»è€…å±•ç¤ºå¦‚ä½•åœ¨å®žé™…ä¸­è§£å†³é—®é¢˜ã€‚è¿™æ ·ä¸ä»…ç›´æŽ¥å°†æ•°å­¦å…¬å¼å¯¹åº”æˆå®žé™…ä»£ç ï¼Œè€Œä¸”å¯ä»¥ä¿®æ”¹ä»£ç ã€è§‚å¯Ÿç»“æžœå¹¶åŠæ—¶èŽ·å–ç»éªŒï¼›\n1. å…è®¸æˆ‘ä»¬å’Œæ•´ä¸ªç¤¾åŒºä¸æ–­å¿«é€Ÿè¿­ä»£å†…å®¹ï¼Œä»Žè€Œç´§è·Ÿä»åœ¨é«˜é€Ÿå‘å±•çš„æ·±åº¦å­¦ä¹ é¢†åŸŸï¼›\n1. ç”±åŒ…å«æœ‰å…³æŠ€æœ¯ç»†èŠ‚é—®ç­”çš„è®ºå›ä½œä¸ºè¡¥å……ï¼Œä½¿å¤§å®¶å¯ä»¥ç›¸äº’ç­”ç–‘å¹¶äº¤æ¢ç»éªŒã€‚\n\n<h5 align="center">å°†æœ¬ä¹¦ï¼ˆä¸­è‹±æ–‡ç‰ˆï¼‰ç”¨ä½œæ•™ææˆ–å‚è€ƒä¹¦çš„å¤§å­¦</h5>\n<p align="center">\n  <img width="400"  src="https://d2l.ai/_images/map.png">\n</p>\n\nå¦‚æžœæœ¬ä¹¦å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·Star (â˜…) æœ¬ä»“åº“æˆ–å¼•ç”¨æœ¬ä¹¦çš„è‹±æ–‡ç‰ˆï¼š\n\n```\n@book{zhang2023dive,\n    title={Dive into Deep Learning},\n    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},\n    publisher={Cambridge University Press},\n    note={\url{https://D2L.ai}},\n    year={2023}\n}\n```\n\n## æœ¬ä¹¦çš„è‹±æ–‡ç‰ˆ\n\nè™½ç„¶çº¸è´¨ä¹¦å·²å‡ºç‰ˆï¼Œä½†æ·±åº¦å­¦ä¹ é¢†åŸŸä¾ç„¶åœ¨è¿…é€Ÿå‘å±•ã€‚ä¸ºäº†å¾—åˆ°æ¥è‡ªæ›´å¹¿æ³›çš„è‹±æ–‡å¼€æºç¤¾åŒºçš„å¸®åŠ©ï¼Œä»Žè€Œæå‡æœ¬ä¹¦è´¨é‡ï¼Œæœ¬ä¹¦çš„æ–°ç‰ˆå°†ç»§ç»­ç”¨è‹±æ–‡ç¼–å†™ï¼Œå¹¶æ¬å›žä¸­æ–‡ç‰ˆã€‚\n\næ¬¢è¿Žå…³æ³¨æœ¬ä¹¦çš„[è‹±æ–‡å¼€æºé¡¹ç›®](https://github.com/d2l-ai/d2l-en)ã€‚\n\n## ä¸­è‹±æ–‡æ•™å­¦èµ„æº\n\nåŠ å·žå¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡ 2019 å¹´æ˜¥å­¦æœŸ [*Introduction to Deep Learning* è¯¾ç¨‹](http://courses.d2l.ai/berkeley-stat-157/index.html)æ•™æï¼ˆåŒæ—¶æä¾›å«æ•™å­¦è§†é¢‘åœ°å€çš„[ä¸­æ–‡ç‰ˆè¯¾ä»¶](https://github.com/d2l-ai/berkeley-stat-157/tree/master/slides-zh)ï¼‰ã€‚\n\n## å­¦æœ¯ç•ŒæŽ¨è\n\n> <p>"Dive into this book if you want to dive into deep learning!"</p>\n> <b>&mdash; éŸ©å®¶ç‚œï¼ŒACM é™¢å£«ã€IEEE é™¢å£«ï¼Œç¾Žå›½ä¼Šåˆ©è¯ºä¼Šå¤§å­¦é¦™æ§Ÿåˆ†æ ¡è®¡ç®—æœºç³» Michael Aiken Chair æ•™æŽˆ</b>\n\n> <p>"This is a highly welcome addition to the machine learning literature."</p>\n> <b>&mdash; Bernhard SchÃ¶lkopfï¼ŒACM é™¢å£«ã€å¾·å›½å›½å®¶ç§‘å­¦é™¢é™¢å£«ï¼Œå¾·å›½é©¬å…‹æ–¯â€¢æ™®æœ—å…‹ç ”ç©¶æ‰€æ™ºèƒ½ç³»ç»Ÿé™¢é™¢é•¿</b>\n\n> <p>"ä¹¦ä¸­ä»£ç å¯è°“â€˜æ‰€å­¦å³æ‰€ç”¨â€™ã€‚"</p>\n> <b>&mdash; å‘¨å¿—åŽï¼ŒACM é™¢å£«ã€IEEE é™¢å£«ã€AAAS é™¢å£«ï¼Œå—äº¬å¤§å­¦è®¡ç®—æœºç§‘å­¦ä¸ŽæŠ€æœ¯ç³»ä¸»ä»»</b>\n\n> <p>"è¿™æœ¬ä¹¦å¯ä»¥å¸®åŠ©æ·±åº¦å­¦ä¹ å®žè·µè€…å¿«é€Ÿæå‡è‡ªå·±çš„èƒ½åŠ›ã€‚"</p>\n> <b>&mdash; å¼ æ½¼ï¼ŒASA é™¢å£«ã€IMS é™¢å£«ï¼Œé¦™æ¸¯ç§‘æŠ€å¤§å­¦è®¡ç®—æœºç³»å’Œæ•°å­¦ç³»æ•™æŽˆ</b>\n\n## å·¥ä¸šç•ŒæŽ¨è\n\n> <p>"ä¸€æœ¬ä¼˜ç§€çš„æ·±åº¦å­¦ä¹ æ•™æï¼Œå€¼å¾—ä»»ä½•æƒ³äº†è§£æ·±åº¦å­¦ä¹ ä½•ä»¥å¼•çˆ†äººå·¥æ™ºèƒ½é©å‘½çš„äººå…³æ³¨ã€‚"</p>\n> <b>&mdash; é»„ä»å‹‹ï¼ŒNVIDIAåˆ›å§‹äºº & CEO</b>\n\n> <p>"ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹æ˜¯æœ€é€‚åˆå·¥ä¸šç•Œç ”å‘å·¥ç¨‹å¸ˆå­¦ä¹ çš„ã€‚æˆ‘æ¯«æ— ä¿ç•™åœ°å‘å¹¿å¤§çš„è¯»è€…ä»¬å¼ºçƒˆæŽ¨èã€‚"</p>\n> <b>&mdash; ä½™å‡¯ï¼Œåœ°å¹³çº¿å…¬å¸åˆ›å§‹äºº & CEO</b>\n\n> <p>"å¼ºçƒˆæŽ¨èè¿™æœ¬ä¹¦ï¼æˆ‘ç‰¹åˆ«èµžèµè¿™ç§æ‰‹è„‘ä¸€ä½“çš„å­¦ä¹ æ–¹å¼ã€‚"</p>\n> <b>&mdash; æ¼†è¿œï¼Œå¤æ—¦å¤§å­¦â€œæµ©æ¸…â€æ•™æŽˆã€äººå·¥æ™ºèƒ½åˆ›æ–°ä¸Žäº§ä¸šç ”ç©¶é™¢é™¢é•¿</b>\n\n> <p>"ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹æ˜¯ä¸€æœ¬å¾ˆå®¹æ˜“è®©å­¦ä¹ è€…ä¸Šç˜¾çš„ä¹¦ã€‚"</p>\n> <b>&mdash; æ²ˆå¼ºï¼Œå°†é—¨åˆ›æŠ•åˆ›å§‹åˆä¼™äºº</b>\n\n## è´¡çŒ®\n\næ„Ÿè°¢[ç¤¾åŒºè´¡çŒ®è€…ä»¬](https://github.com/d2l-ai/d2l-zh/graphs/contributors)ä¸ºæ¯ä¸€ä½è¯»è€…æ”¹è¿›è¿™æœ¬å¼€æºä¹¦ã€‚\n\n[å¦‚ä½•è´¡çŒ®](https://zh.d2l.ai/chapter_appendix-tools-for-deep-learning/contributing.html) | [è‡´è°¢](https://zh.d2l.ai/chapter_preface/index.html) | [è®¨è®ºæˆ–æŠ¥å‘Šé—®é¢˜](https://discuss.d2l.ai/c/chinese-version/16) | [å…¶ä»–](INFO.md)\n', '{"language":"Python","stars":74263,"forks":12019,"watchers":74263,"open_issues":119,"topics":["book","chinese","computer-vision","deep-learning","machine-learning","natural-language-processing","notebook","python"],"default_branch":"master","size_kb":316965,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:d2l-ai:d2l-en","source_url":"https://github.com/d2l-ai/d2l-en"},{"type":"has_code","target_id":"github:d2l-ai:berkeley-stat-157","source_url":"https://github.com/d2l-ai/berkeley-stat-157"},{"type":"has_code","target_id":"github:d2l-ai:d2l-zh","source_url":"https://github.com/d2l-ai/d2l-zh"}]', NULL, 'Apache-2.0', 'approved', 65, 'f8a684d85b0c9e6ded8510777eeb94ee', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-d2l-ai-d2l-zh from https://github.com/d2l-ai.png
Image converted to WebP: data/images/github-d2l-ai-d2l-zh.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-tesseract-ocr-tesseract', 'github--tesseract-ocr--tesseract', 'tesseract', 'tesseract-ocr', '\ * Tesseract OCR * About * Brief history * Installing Tesseract * Running Tesseract * For developers * Support * License * Dependencies * Latest Version of README This package contains an **OCR engine** - and a **command line program** - . Tesseract 4 adds a new neural net (LSTM) based OCR engine which is focused on line recognition, but also still supports the legacy Tesseract OCR engine of Tesseract 3 which works by recognizing character patterns. Compatibility with Tesseract 3 is enabled ...', '["hacktoberfest","lstm","machine-learning","ocr","ocr-engine","tesseract","tesseract-ocr","c++"]', 'other', 71280, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/tesseract-ocr/tesseract","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Tesseract OCR\n\n[![Coverity Scan Build Status](https://scan.coverity.com/projects/tesseract-ocr/badge.svg)](https://scan.coverity.com/projects/tesseract-ocr)\n[![CodeQL](https://github.com/tesseract-ocr/tesseract/workflows/CodeQL/badge.svg)](https://github.com/tesseract-ocr/tesseract/security/code-scanning)\n[![OSS-Fuzz](https://img.shields.io/badge/oss--fuzz-fuzzing-brightgreen)](https://issues.oss-fuzz.com/issues?q=is:open%20title:tesseract-ocr)\n\\n[![GitHub license](https://img.shields.io/badge/license-Apache--2.0-blue.svg)](https://raw.githubusercontent.com/tesseract-ocr/tesseract/main/LICENSE)\n[![Downloads](https://img.shields.io/badge/download-all%20releases-brightgreen.svg)](https://github.com/tesseract-ocr/tesseract/releases/)\n\n## Table of Contents\n\n* [Tesseract OCR](#tesseract-ocr)\n  * [About](#about)\n  * [Brief history](#brief-history)\n  * [Installing Tesseract](#installing-tesseract)\n  * [Running Tesseract](#running-tesseract)\n  * [For developers](#for-developers)\n  * [Support](#support)\n  * [License](#license)\n  * [Dependencies](#dependencies)\n  * [Latest Version of README](#latest-version-of-readme)\n\n## About\n\nThis package contains an **OCR engine** - `libtesseract` and a **command line program** - `tesseract`.\n\nTesseract 4 adds a new neural net (LSTM) based [OCR engine](https://en.wikipedia.org/wiki/Optical_character_recognition) which is focused on line recognition, but also still supports the legacy Tesseract OCR engine of Tesseract 3 which works by recognizing character patterns. Compatibility with Tesseract 3 is enabled by using the Legacy OCR Engine mode (--oem 0).\nIt also needs [traineddata](https://tesseract-ocr.github.io/tessdoc/Data-Files.html) files which support the legacy engine, for example those from the [tessdata](https://github.com/tesseract-ocr/tessdata) repository.\n\nStefan Weil is the current lead developer. Ray Smith was the lead developer until 2017. The maintainer is Zdenko Podobny. For a list of contributors see [AUTHORS](https://github.com/tesseract-ocr/tesseract/blob/main/AUTHORS)\nand GitHub''s log of [contributors](https://github.com/tesseract-ocr/tesseract/graphs/contributors).\n\nTesseract has **unicode (UTF-8) support**, and can **recognize [more than 100 languages](https://tesseract-ocr.github.io/tessdoc/Data-Files-in-different-versions.html)** "out of the box".\n\nTesseract supports **[various image formats](https://tesseract-ocr.github.io/tessdoc/InputFormats)** including PNG, JPEG and TIFF.\n\nTesseract supports **various output formats**: plain text, hOCR (HTML), PDF, invisible-text-only PDF, TSV, ALTO and PAGE.\n\nYou should note that in many cases, in order to get better OCR results, you''ll need to **[improve the quality](https://tesseract-ocr.github.io/tessdoc/ImproveQuality.html) of the image** you are giving Tesseract.\n\nThis project **does not include a GUI application**. If you need one, please see the [3rdParty](https://tesseract-ocr.github.io/tessdoc/User-Projects-%E2%80%93-3rdParty.html) documentation.\n\nTesseract **can be trained to recognize other languages**.\nSee [Tesseract Training](https://tesseract-ocr.github.io/tessdoc/Training-Tesseract.html) for more information.\n\n## Brief history\n\nTesseract was originally developed at Hewlett-Packard Laboratories Bristol UK and at Hewlett-Packard Co, Greeley Colorado USA between 1985 and 1994, with some more changes made in 1996 to port to Windows, and some C++izing in 1998. In 2005 Tesseract was open sourced by HP. From 2006 until August 2017 it was developed by Google.\n\nMajor version 5 is the current stable version and started with release\n[5.0.0](https://github.com/tesseract-ocr/tesseract/releases/tag/5.0.0) on November 30, 2021. Newer minor versions and bugfix versions are available from\n[GitHub](https://github.com/tesseract-ocr/tesseract/releases/).\n\nLatest source code is available from [main branch on GitHub](https://github.com/tesseract-ocr/tesseract/tree/main).\nOpen issues can be found in [issue tracker](https://github.com/tesseract-ocr/tesseract/issues),\nand [planning documentation](https://tesseract-ocr.github.io/tessdoc/Planning.html).\n\nSee **[Release Notes](https://tesseract-ocr.github.io/tessdoc/ReleaseNotes.html)**\nand **[Change Log](https://github.com/tesseract-ocr/tesseract/blob/main/ChangeLog)** for more details of the releases.\n\n## Installing Tesseract\n\nYou can either [Install Tesseract via pre-built binary package](https://tesseract-ocr.github.io/tessdoc/Installation.html)\nor [build it from source](https://tesseract-ocr.github.io/tessdoc/Compiling.html).\n\nBefore building Tesseract from source, please check that your system has a compiler which is one of the [supported compilers](https://tesseract-ocr.github.io/tessdoc/supported-compilers.html).\n\n## Running Tesseract\n\nBasic **[command line usage](https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.html)**:\n\n    tesseract imagename outputbase [-l lang] [--oem ocrenginemode] [--psm pagesegmode] [configfiles...]\n\nFor more information about the various command line options use `tesseract --help` or `man tesseract`.\n\nExamples can be found in the [documentation](https://tesseract-ocr.github.io/tessdoc/Command-Line-Usage.html#simplest-invocation-to-ocr-an-image).\n\n## For developers\n\nDevelopers can use `libtesseract` [C](https://github.com/tesseract-ocr/tesseract/blob/main/include/tesseract/capi.h) or\n[C++](https://github.com/tesseract-ocr/tesseract/blob/main/include/tesseract/baseapi.h) API to build their own application. If you need bindings to `libtesseract` for other programming languages, please see the\n[wrapper](https://tesseract-ocr.github.io/tessdoc/AddOns.html#tesseract-wrappers) section in the AddOns documentation.\n\nDocumentation of Tesseract generated from source code by doxygen can be found on [tesseract-ocr.github.io](https://tesseract-ocr.github.io/).\n\n## Support\n\nBefore you submit an issue, please review **[the guidelines for this repository](https://github.com/tesseract-ocr/tesseract/blob/main/CONTRIBUTING.md)**.\n\nFor support, first read the [documentation](https://tesseract-ocr.github.io/tessdoc/),\nparticularly the [FAQ](https://tesseract-ocr.github.io/tessdoc/FAQ.html) to see if your problem is addressed there.\nIf not, search the [Tesseract user forum](https://groups.google.com/g/tesseract-ocr), the [Tesseract developer forum](https://groups.google.com/g/tesseract-dev) and [past issues](https://github.com/tesseract-ocr/tesseract/issues), and if you still can''t find what you need, ask for support in the mailing-lists.\n\nMailing-lists:\n\n* [tesseract-ocr](https://groups.google.com/g/tesseract-ocr) - For tesseract users.\n* [tesseract-dev](https://groups.google.com/g/tesseract-dev) - For tesseract developers.\n\nPlease report an issue only for a **bug**, not for asking questions.\n\n## License\n\n    The code in this repository is licensed under the Apache License, Version 2.0 (the "License");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an "AS IS" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\n**NOTE**: This software depends on other packages that may be licensed under different open source licenses.\n\nTesseract uses [Leptonica library](http://leptonica.com/) which essentially\nuses a [BSD 2-clause license](http://leptonica.com/about-the-license.html).\n\n## Dependencies\n\nTesseract uses [Leptonica library](https://github.com/DanBloomberg/leptonica)\nfor opening input images (e.g. not documents like pdf).\nIt is suggested to use leptonica with built-in support for [zlib](https://zlib.net),\n[png](https://sourceforge.net/projects/libpng) and\n[tiff](http://www.simplesystems.org/libtiff) (for multipage tiff).\n\n## Latest Version of README\n\nFor the latest online version of the README.md see:\n\n<https://github.com/tesseract-ocr/tesseract/blob/main/README.md>\n', '{"language":"C++","stars":71280,"forks":10414,"watchers":71280,"open_issues":463,"topics":["hacktoberfest","lstm","machine-learning","ocr","ocr-engine","tesseract","tesseract-ocr"],"default_branch":"main","size_kb":53713,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tessdata","source_url":"https://github.com/tesseract-ocr/tessdata"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"},{"type":"has_code","target_id":"github:DanBloomberg:leptonica","source_url":"https://github.com/DanBloomberg/leptonica"},{"type":"has_code","target_id":"github:tesseract-ocr:tesseract","source_url":"https://github.com/tesseract-ocr/tesseract"}]', NULL, 'Apache-2.0', 'approved', 65, '771d5700c042315b910b5599d47c464b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-tesseract-ocr-tesseract from https://github.com/tesseract-ocr.png
Image converted to WebP: data/images/github-tesseract-ocr-tesseract.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Developer-Y-cs-video-courses', 'github--developer-y--cs-video-courses', 'cs-video-courses', 'Developer-Y', '<!-- omit in toc --> <!-- omit in toc --> - Please check NOTES for general information about this list. - Please refer CONTRIBUTING.md for contribution guidelines. - Please feel free to raise any genuine issue you may have, however, it has been noticed that few people open empty issues to raise their GitHub contribution on their account. Such spammers will be blocked. - You are welcome to contribute, please create PR for actual college/University level courses. Please do not add links for sma...', '["algorithms","bioinformatics","computational-biology","computational-physics","computer-architecture","computer-science","computer-vision","database-systems","databases","deep-learning","embedded-systems","machine-learning","quantum-computing","reinforcement-learning","robotics","security","systems","web-development"]', 'other', 70337, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Developer-Y/cs-video-courses","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<!-- omit in toc -->\n# Computer Science courses with video lectures\n\n<!-- omit in toc -->\n## Introduction\n\n- Please check [NOTES](https://github.com/Developer-Y/cs-video-courses/blob/master/NOTES.md) for general information about this list.\n- Please refer [CONTRIBUTING.md](https://github.com/Developer-Y/cs-video-courses/blob/master/CONTRIBUTING.md) for contribution guidelines.\n- Please feel free to raise any genuine issue you may have, however, it has been noticed that few people open empty issues to raise their GitHub contribution on their account. Such spammers will be blocked. \n- You are welcome to contribute, please create PR for actual college/University level courses. Please do not add links for small MOOCs, basic tutorials, or advertisements for some sites/channels.\n\n------------------------------\n\nTable of Contents\n\n------------------------------\n- [Introduction to Computer Science](#introduction-to-computer-science)\n- [Data Structures and Algorithms](#data-structures-and-algorithms)\n- [Systems Programming](#systems-programming)\n  * [Operating Systems](#operating-systems)\n  * [Distributed Systems](#distributed-systems)\n  * [Real-Time Systems](#real-time-systems) \n- [Database Systems](#database-systems)\n- [Software Engineering](#software-engineering)\n  * [Object Oriented Design](#object-oriented-design)\n  * [Software Engineering](#software-engineering)\n  * [Software Architecture](#software-architecture)\n  * [Concurrency](#concurrency)\n  * [Mobile Application Development](#mobile-application-development)\n- [Artificial Intelligence](#artificial-intelligence)\n- [Machine Learning](#machine-learning)\n  * [Introduction to Machine Learning](#introduction-to-machine-learning)\n  * [Data Mining](#data-mining)\n  * [Probabilistic Graphical Modeling](#probabilistic-graphical-modeling)\n  * [Deep Learning](#deep-learning)\n  * [Reinforcement Learning](#reinforcement-learning)\n  * [Advanced Machine Learning](#advanced-machine-learning)\n  * [Natural Language Processing](#natural-language-processing)\n  * [Generative AI and LLMs](#generative-ai-and-llms)\n  * [Computer Vision](#computer-vision)\n  * [Time Series Analysis](#time-series-analysis)\n  * [Optimization](#optimization)\n  * [Unsupervised Learning](#unsupervised-learning)\n  * [Misc Machine Learning Topics](#misc-machine-learning-topics)\n- [Computer Networks](#computer-networks)\n- [Math for Computer Scientist](#math-for-computer-scientist)\n- [Web Programming and Internet Technologies](#web-programming-and-internet-technologies)\n- [Theoretical CS and Programming Languages](#theoretical-cs-and-programming-languages)\n- [Embedded Systems](#embedded-systems)\n- [Real time system evaluation](#real-time-system-evaluation)\n- [Computer Organization and Architecture](#computer-organization-and-architecture)\n- [Security](#security)\n- [Computer Graphics](#computer-graphics)\n- [Image Processing and Computer Vision](#image-processing-and-computer-vision)\n- [Computational Physics](#computational-physics)\n- [Computational Biology](#computational-biology)\n- [Quantum Computing](#quantum-computing)\n- [Robotics and Control](#robotics-and-control)\n- [Computational Finance](#computational-finance)\n- [Network Science](#network-science)\n- [Blockchain Development](#blockchain-development)\n- [Misc](#misc)\n\n<!-- omit in toc -->\n## Courses\n\n------------------------------\n\n### Introduction to Computer Science\n\n- [CS 10 - The Beauty and Joy of Computing - Spring 2015 - Dan Garcia - UC Berkeley InfoCoBuild](http://www.infocobuild.com/education/audio-video-courses/computer-science/cs10-spring2015-berkeley.html)\n- [6.0001 - Introduction to Computer Science and Programming in Python - MIT OCW](https://ocw.mit.edu/courses/6-0001-introduction-to-computer-science-and-programming-in-python-fall-2016/video_galleries/lecture-videos/)\n- [6.001 - Structure and Interpretation of Computer Programs, MIT](https://ocw.mit.edu/courses/6-001-structure-and-interpretation-of-computer-programs-spring-2005/video_galleries/video-lectures/)\n- [Introduction to Computational Thinking - MIT](https://computationalthinking.mit.edu/Fall22/)\n- [CS 50 - Introduction to Computer Science, Harvard University](https://online-learning.harvard.edu/course/cs50-introduction-computer-science) ([cs50.tv](http://cs50.tv/2017/fall/))\n- [CS50R - Introduction to Programming with R](https://cs50.harvard.edu/r/2024/) ([Lecture Videos](https://www.youtube.com/playlist?list=PLhQjrBD2T382yfNp_-xzX244d-O9W6YmD))\n- [CS50: Introduction to Computer Science with Python - Harvard (David J. Malan)](https://www.youtube.com/playlist?list=PLhQjrBD2T3817j24-GogXmWqO5Q5vYy0V)\n- [CS 61A - Structure and Interpretation of Computer Programs [Python], UC Berkeley](https://cs61a.org/)\n- [CPSC 110 - Systematic Program Design [Racket], University of British Columbia](https://www.youtube.com/channel/UC7dEjIUwSxSNcW4PqNRQW8w/playlists?view=1&flow=grid&sort=da)\n- [CS50''s Understanding Technology](https://www.youtube.com/playlist?list=PLhQjrBD2T382p8amnvUp1rws1p7n7gJ2p)\n- [CSE 142 Computer Programming I (Java Programming), Spring 2016 - University of Washington](https://courses.cs.washington.edu/courses/cse142/16sp/calendar.shtml)\n- [CS 1301 Intro to computing - Gatech](https://www.cc.gatech.edu/classes/AY2016/cs1301c_fall/)\n- [CS 106A - Programming Methodology, Stanford University](https://see.stanford.edu/Course/CS106A) ([Lecture Videos](https://www.youtube.com/playlist?list=PL84A56BC7F4A1F852))\n- [CS 106B - Programming Abstractions, Stanford University](https://see.stanford.edu/Course/CS106B) ([Lecture Videos](https://www.youtube.com/playlist?list=PLnfg8b9vdpLn9exZweTJx44CII1bYczuk))\n- [CS 106L - Standard C++ Programming](https://web.stanford.edu/class/cs106l/)([Lecture Videos](https://www.youtube.com/playlist?list=PLCgD3ws8aVdolCexlz8f3U-RROA0s5jWA))\n- [CS 106X - Programming Abstractions in C++](http://web.stanford.edu/class/cs106x/) ([Lecture Videos](https://www.youtube.com/playlist?list=PLrivl8gTKLcpIJ-ktHCxMEgWOn8LawYhb))\n- [CS 107 - Programming Paradigms, Stanford University](https://see.stanford.edu/Course/CS107)\n- [CmSc 150 - Introduction to Programming with Arcade Games, Simpson College](http://ProgramArcadeGames.com)\n- [IN2377 - Concepts of C++ programming (Winter 2023), TUM](https://live.rbg.tum.de/?year=2023&term=W&slug=cpp&view=3) ([Winter 2022](https://live.rbg.tum.de/?year=2022&term=W&slug=cpp&view=3)) ([Summer 2022](https://live.rbg.tum.de/?year=2022&term=S&slug=ccppprog&view=3)) ([Summer 2021](https://live.rbg.tum.de/?year=2021&term=S&slug=ccppprog&view=3))\n- [IN1503 - Advanced C++ Programming, TUM](https://live.rbg.tum.de/?year=2023&term=W&slug=AdvProg&view=3)\n- [LINFO 1104 - Paradigms of computer programming, Peter Van Roy, UniversitÃ© catholique de Louvain, Belgium - EdX](https://www.youtube.com/playlist?list=PLw454N-VXALSIzIe_eL5U8L4S68v2X_ak)\n- [FP 101x - Introduction to Functional Programming, TU Delft](https://ocw.tudelft.nl/courses/introduction-to-functional-programming/)\n- [Introduction to Problem Solving and Programming - IIT Kanpur](https://nptel.ac.in/courses/106104074/)\n- [Introduction to programming in C - IIT Kanpur](https://nptel.ac.in/courses/106104128/)\n- [Programming in C++ - IIT Kharagpur](https://nptel.ac.in/courses/106105151/)\n- [Python Boot Camp Fall 2016 - Berkeley Institute for Data Science (BIDS)](https://www.youtube.com/playlist?list=PLKW2Azk23ZtSeBcvJi0JnL7PapedOvwz9)\n- [CS 101 - Introduction to Computer Science - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPmjFQ2w9j05WDX8Jtg5RXWW)\n- [6.00SC - Introduction to Computer Science and Programming (Spring 2011) - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-00sc-introduction-to-computer-science-and-programming-spring-2011/)\n- [6.00 - Introduction to Computer Science and Programming (Fall 2008) - MIT OCW](https://ocw.mit.edu/courses/6-00-introduction-to-computer-science-and-programming-fall-2008/video_galleries/video-lectures/)\n- [6.01SC - Introduction to Electrical Engineering and Computer Science I - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-01sc-introduction-to-electrical-engineering-and-computer-science-i-spring-2011/)\n- [Modern C++ Course (2018) - Bonn University](https://www.youtube.com/playlist?list=PLgnQpQtFTOGR50iIOtO36nK6aNPtVq98C)\n- [Modern C++ (Lecture & Tutorials, 2020, Vizzo & Stachniss) - University of Bonn](https://www.youtube.com/playlist?list=PLgnQpQtFTOGRM59sr3nSL8BmeMZR9GCIA)\n- [UW Madison CS 368 C++ for Java Programmers Fall 2020, by Michael Doescher](https://www.youtube.com/playlist?list=PLXY5xcFHqg33srpQjC7q7jqITLxcErPCM)\n- [UW Madison CS 354 Machine Organization and Programming spring 2020, 2021, by Michael Doescher](https://www.youtube.com/playlist?list=PLXY5xcFHqg32r5MZ-HfpA2Tr8Ke2lDYwI)\n- [Cornell CS 1110 Introduction to Computing using Python fall 2020, by Walker White](https://www.cs.cornell.edu/courses/cs1110/2020fa/lessons/) ([Lecture Videos](https://vod.video.cornell.edu/channel/CS+1110+Fall+2020/179890731))\n- [Cornell ECE 4960 Computational and Software Engineering spring 2017, by Edwin Kan](https://www.youtube.com/playlist?list=PLcVqWUh-bHiFN2CY1KMTw0-L39iDXlemi)\n\n------------------------------\n\n### Data Structures and Algorithms\n\n- [ECS 36C - Data Structures and Algorithms (C++) - Spring 2020 - JoÃ«l Porquet-Lupine - UC Davis](https://lupteach.gitlab.io/courses/ucd-ecs36c/online/)\n- [Programming and Data Structures with Python, 2021-2022, Sem I - by Prof. Madhavan Mukund, CMI](https://www.cmi.ac.in/~madhavan/courses/python2021sep/)\n- [Graph Algorithms - Robert Sedgewick - Princeton University](https://www.youtube.com/watch?v=0qF7tPSQdCg)\n- [6.006 - Introduction to Algorithms, MIT OCW](https://ocw.mit.edu/courses/6-006-introduction-to-algorithms-spring-2020/video_galleries/lecture-videos/)\n- [MIT 6.006 Introduction to Algorithms, Spring 2020](https://www.youtube.com/playlist?list=PLUl4u3cNGP63EdVPNLG3ToM6LaEUuStEY)\n- [Algorithms: Design and Analysis 1 - Stanford University](https://www.youtube.com/playlist?list=PLXFMmlk03Dt7Q0xr1PIAriY5623cKiH7V)\n- [Algorithms: Design and Analysis 2 - Stanford University](https://www.youtube.com/playlist?list=PLXFMmlk03Dt5EMI2s2WQBsLsZl7A5HEK6)\n- [COS 226 Algorithms, Youtube, Princeton - by Robert Sedgewick and Kevin Wayne](https://www.youtube.com/watch?v=1QZDe28peZk&list=PLRdD1c6QbAqJn0606RlOR6T3yUqFWKwmX&index=1)\n- [CSE 331 Introduction to Algorithm Design and Analysis, SUNY University at Buffalo, NY - Fall 2017](http://www-student.cse.buffalo.edu/~atri/cse331/fall17/index.html) ([Lectures](https://www.youtube.com/playlist?list=PLZBCR-EGqNpoiHeO17FlLADJ38Kb3EiPU)) ([Homework Walkthroughs](https://www.youtube.com/playlist?list=PLZBCR-EGqNpoVyQCIUDHiXnL-zdFD_ixk))\n- [CSE 373 - Analysis of Algorithms, Stony Brook - Prof Skiena](http://www.cs.sunysb.edu/~algorith/video-lectures/)\n- [COP 3530 Data Structures and Algorithms, Prof Sahni, UFL](http://www.cise.ufl.edu/~sahni/cop3530/) ([Videos](http://www.cise.ufl.edu/academics/courses/preview/cop3530sahni/))\n- [CS225 - Data Structures - University of Illinois at Urbana-Champaign](https://cs.illinois.edu/courses/profile/CS225)([Video lectures](https://www.youtube.com/playlist?list=PLRdSp8jtJxBqG3KNQPKKB-0Z2hh9omoDo))\n- [CS2 - Data Structures and Algorithms - Richard Buckland - UNSW](https://www.youtube.com/playlist?list=PLE621E25B3BF8B9D1)\n- [Data Structures - Pepperdine University](https://itunes.apple.com/us/course/data-structures/id546468797)\n- [CS 161 - Design and Analysis of Algorithms, Prof. Tim Roughgarden, Stanford University](http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=IntroToAlgorithms)\n- [6.046J - Introduction to Algorithms - Fall 2005, MIT OCW](https://ocw.mit.edu/courses/6-046j-introduction-to-algorithms-sma-5503-fall-2005/)\n- [Introduction to Algorithms (Spring 2020), MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-spring-2020/)\n- [6.046 - Design and Analysis of Algorithms, Spring 2015 - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-design-and-analysis-of-algorithms-spring-2015/lecture-videos/)\n- [CS 473 - Algorithms - University of Illinois at Urbana-Champaign](https://courses.engr.illinois.edu/cs473/sp2016/lectures.html) ([Notes - Jeff Erickson](http://jeffe.cs.illinois.edu/teaching/algorithms/)) ([YouTube](https://www.youtube.com/playlist?list=PL0v718LJg-78SFq81e4kJh_rS8XbKZ7Kn))\n- [COMP300E - Programming Challenges, Prof Skiena, Hong Kong University of Science and Technology - 2009](https://www.youtube.com/playlist?list=PL07B3F10B48592010)\n- [16s-4102 - Algorithms, University of Virginia](http://www.cs.virginia.edu/~shelat/16s-4102/) ([Youtube](https://www.youtube.com/channel/UCxXYk53cSZof2bR_Ax0uJYQ/videos))\n- [CS 61B - Data Structures (Java) - UC Berkeley](https://inst.eecs.berkeley.edu/~cs61b/)([Youtube](https://www.youtube.com/watch?v=gG4--V_PpEk&list=PLjuu7kFWxFtZBm-5GifiVpqdAxeW7Hsax))\n- [CS 170 Algorithms - UCBerkeley](https://cs170.org/) [Fall 2019, Youtube](https://www.youtube.com/playlist?list=PLIygTcviGPKD4TU_QsvJI0G7QnrIS_7Wn) [Fall 2018, Youtube](https://www.youtube.com/watch?v=fd5P-8IQwMY&list=PLkFD6_40KJIx8lWWbE-Uk069aZ1R-W-VU&index=2&t=0s) [Fall 2018,Bilibili](https://www.bilibili.com/video/av43955743/?p=1) [2013 Bilibili](https://www.bilibili.com/video/av26670685/)\n- [CS 159 Data-Driven Algorithm Design - Caltech](https://sites.google.com/view/cs-159-spring-2020/lectures?authuser=0) [Spring 2020, Youtube](https://www.youtube.com/playlist?list=PLuz4CTPOUNi4Dz6zBPypcI8I3oJUjFKk4)\n- [ECS 122A - Algorithm Design and Analysis, UC Davis](http://web.cs.ucdavis.edu/~gusfield/cs122f10/videolist.html)\n- [CSE 373 - Data Structures and Algorithms, Winter 2024 - University of Washington](https://courses.cs.washington.edu/courses/cse373/24wi/) ([Winter 2024, Youtube](https://www.youtube.com/playlist?list=PLEcoVsAaONjd5n69K84sSmAuvTrTQT_Nl)) ([Spring 2023, Notes](https://courses.cs.washington.edu/courses/cse373/23sp/)) ([Spring 2023, Youtube](https://www.youtube.com/playlist?list=PLEcoVsAaONjfHSAbP1AsVjAxIOFue6uWh))\n- [CSEP 521 - Applied Algorithms, Winter 2013 - University of Washington](https://courses.cs.washington.edu/courses/csep521/13wi/) ([Videos](https://courses.cs.washington.edu/courses/csep521/13wi/video/))\n- [Data Structures And Algorithms - IIT Delhi](https://nptel.ac.in/courses/106102064/)\n- [Design and Analysis of Algorithms - IIT Bombay](https://nptel.ac.in/courses/106101060/)\n- [Programming, Data Structures and Algorithms - IIT Madras](https://nptel.ac.in/courses/106106127/)\n- [Design and Analysis of Algorithms - IIT Madras](https://nptel.ac.in/courses/106106131/)\n- [Fundamental Algorithms:Design and Analysis - IIT Kharagpur](https://nptel.ac.in/courses/106105157/)\n- [Programming and Data Structure - IIT Kharagpur](https://nptel.ac.in/courses/106105085/)\n- [Programming, Data structures and Algorithms - IIT Madras](https://nptel.ac.in/courses/106106133/)\n- [Programming, Data Structures and Algorithms in Python - IIT Madras](https://nptel.ac.in/courses/106106145/)\n- [Programming and Data structures (PDS) - IIT Madras](https://nptel.ac.in/courses/106106130/)\n- [COP 5536 Advanced Data Structures, Prof Sahni - UFL](http://www.cise.ufl.edu/~sahni/cop5536/index.html) ([Videos](http://www.cise.ufl.edu/academics/courses/preview/cop5536sahni/))\n- [CS 261 - A Second Course in Algorithms, Stanford University](http://theory.stanford.edu/~tim/w16/w16.html) ([Youtube](https://www.youtube.com/playlist?list=PLEGCF-WLh2RJh2yDxlJJjnKswWdoO8gAc))\n- [CS 224 - Advanced Algorithms, Harvard University](http://people.seas.harvard.edu/~minilek/cs224/fall14/index.html) ([Lecture Videos](http://people.seas.harvard.edu/~minilek/cs224/fall14/lec.html)) ([Youtube](https://www.youtube.com/playlist?list=PL2SOU6wwxB0uP4rJgf5ayhHWgw7akUWSf))\n- [CS 6150 - Advanced Algorithms (Fall 2016), University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCp8X9FHOglnLqFjyvqGLftx)\n- [CS 6150 - Advanced Algorithms (Fall 2017), University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCqS9Z419eky9m6gJP7zfhO9)\n- [ECS 222A - Graduate Level Algorithm Design and Analysis, UC Davis](http://web.cs.ucdavis.edu/~gusfield/cs222f07/videolist.html)\n- [6.851 - Advanced Data Structures, MIT](http://courses.csail.mit.edu/6.851/spring14/lectures/) ([MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-851-advanced-data-structures-spring-2012/lecture-videos/))\n- [6.854 - Advanced Algorithms, MIT](https://www.youtube.com/playlist?list=PL6ogFv-ieghdoGKGg2Bik3Gl1glBTEu8c) ([Prof. Karger lectures](https://www.youtube.com/channel/UCtv9PiQVUDzsT4yl7524DCg/videos))\n- [CS264 Beyond Worst-Case Analysis, Fall 2014 - Tim Roughgarden Lecture](http://theory.stanford.edu/~tim/f14/f14.html) ([Youtube](https://www.youtube.com/playlist?list=PLEGCF-WLh2RL8jsZpaf2tLHa5LotFEt5b))\n- [CS364A Algorithmic Game Theory, Fall 2013 - Tim Roughgarden Lectures](https://www.youtube.com/playlist?list=PLEGCF-WLh2RJBqmxvZ0_ie-mleCFhi2N4)\n- [CS364B Advanced Mechanism Design, Winter 2014 - Tim Roughgarden Lectures](https://www.youtube.com/playlist?list=PLEGCF-WLh2RI77PL4gwLld_OU9Zh3TCX9)\n- [Algorithms - Aduni](http://aduni.org/courses/algorithms/index.php?view=cw)\n- [6.889 - Algorithms for Planar Graphs and Beyond (Fall 2011) MIT](http://courses.csail.mit.edu/6.889/fall11/lectures/)\n- [6.890 Algorithmic Lower Bounds: Fun with Hardness Proofs - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-890-algorithmic-lower-bounds-fun-with-hardness-proofs-fall-2014/)\n- [Computer Algorithms - 2 - IIT Kanpur](https://nptel.ac.in/courses/106104019/)\n- [Parallel Algorithm - IIT Kanpur](https://nptel.ac.in/courses/106104120/)\n- [Graph Theory - IISC Bangalore](https://nptel.ac.in/courses/106108054/)\n- [Data Structures - mycodeschool](https://www.youtube.com/watch?v=92S4zgXN17o&list=PL2_aWCzGMAwI3W_JlcBbtYTwiQSsOTa6P)\n- [Algorithmic Game Theory, Winter 2020/21 - Uni Bonn](https://www.youtube.com/playlist?list=PLyzcvvgje7aD_DjpmhFzQ9DVS8zzhrgp6)\n- [NETS 4120: Algorithmic Game Theory, Spring 2023 - UPenn](https://www.youtube.com/playlist?list=PLlIlhe_rS4U1MfB0NzG4IWb7CM0xKkx4d)\n- [Introduction to Game Theory and Mechanism Design - IIT Kanpur](https://www.youtube.com/playlist?list=PL3eEm6KzZ3lF2TlVOnPyJHyGWJhUogn-D)\n- [15-850 Advanced Algorithms - CMU Spring 2023](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%2253c58248-7fd4-4f71-8774-af85013a570a%22&page=1)\n- [CS 270. Combinatorial Algorithms and Data Structures, Spring 2021](https://people.eecs.berkeley.edu/~prasad/spring2021.html) ([Youtube](https://www.youtube.com/playlist?list=PLfkeJ2f4i0AfWApBP8X8YvQfN4WbRQTC3))\n- [CMU 15 850 Advanced Algorithms spring 2023, by Anupam Gupta](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%2253c58248-7fd4-4f71-8774-af85013a570a%22&page=1)\n- [UC Berkeley CS 294-165 Sketching Algorithms fall 2020, by Jelani Nelson](https://www.sketchingbigdata.org/fall20/lec/) ([Youtube](https://www.youtube.com/playlist?list=PLIygTcviGPKCdx1AVD-CAzNk5uXDu9wIA))\n- [UIUC CS 498 ABD / CS 598 CSC Algorithms for Big Data fall 2020, by Chandra Chekuri](https://www.youtube.com/playlist?list=PL682UO4IMem_OA8_wY3nnSDLOSWr3PgYa)\n- [Algorithms for Data Science spring 2021, by Anil Maheshwari ](https://people.scs.carleton.ca/~maheshwa/courses/ADS/ADS-S20.html)\n- [CMU 15 859 Algorithms for Big Data fall 2020, by David Woodruff](http://www.cs.cmu.edu/~dwoodruf/teaching/15859-fall20/index.html)\n- [CO 642 Graph Theory - University of Waterloo](https://www.youtube.com/playlist?list=PL2BdWtDKMS6mplieDd_vls0TBX9Fq2jht)\n- [COMS W4241 Numerical Algorithms spring 2006, by Henryk Wozniakowski - Columbia](https://www.youtube.com/playlist?list=PL682UO4IMem98vm26lNUJ0TV0-EFrcUJb)\n- [Bonn Algorithms and Uncertainty summer 2021, by Thomas Kesselheim](https://www.youtube.com/playlist?list=PLyzcvvgje7aDZRFMJZgaVgOW5t5KLvD1-)\n- [Harvard Information Theory 2022, by Gregory Falkovich](https://www.youtube.com/playlist?list=PLDEN2FPNHwVZKAFqfFl1b_NNAESTJwV9o)\n- [Math 510 - Linear Programming and Network Flows - Colorado State University](https://www.math.colostate.edu/~adams/teaching/math510fall2020/)\n- [LINFO 2266 Advanced Algorithms for Optimization 2021, by Pierre Schaus - UCLouvain](https://www.youtube.com/playlist?list=PL682UO4IMem-wgYnJl5yMswlNkve_8oGU)\n- [MIT 6.5210 / 6.854 / 18.415 Advanced Algorithms Fall 2013, 2020, 2021, 2022, by David Karger](https://6.5210.csail.mit.edu/materials) ([Spring 2016, by Ankur Moitra](https://www.youtube.com/playlist?list=PL6ogFv-ieghdoGKGg2Bik3Gl1glBTEu8c))\n- [CMU 10 801 Advanced Optimization and Randomized Algorithms spring 2014, by Suvrit Sra and Alex Smola](https://www.cs.cmu.edu/~suvrit/teach/)\n- [Purdue CS 381 Fundamental Algorithms, by Kent Quanrud](https://fas22.s3.amazonaws.com/fas22-book.pdf) ([Spring 2022](https://www.youtube.com/playlist?list=PL0YFU3y0Z_gb6P9jG0-40AgZapdGGXPTD))\n- [Purdue CS 390 ATA Fundamental Algorithms Advanced, by Kent Quanrud](https://fas25.s3.amazonaws.com/fas25.pdf) ([Spring 2025](https://www.youtube.com/playlist?list=PL0YFU3y0Z_gYOC4uogZcu54XKq57TVEs3))\n- [Purdue CS 580 Graduate Algorithms, by Kent Quanrud](https://fas23.s3.amazonaws.com/fas23.pdf) ([Spring 2023](https://www.youtube.com/playlist?list=PL0YFU3y0Z_gYAwo5Tg4kP91ifXPF_FIQ1)) ([Spring 2024](https://www.youtube.com/playlist?list=PL0YFU3y0Z_gaGD6jZLeuLHTRW0ISFn6AU))\n- [Purdue CS 588 Randomized Algorithms, by Kent Quanrud](https://ras24.s3.amazonaws.com/ras24.pdf) ([Fall 2022](https://www.youtube.com/playlist?list=PL0YFU3y0Z_gbjvT1yDQkwRU9UXahd0BP1)) ([Spring 2024](https://www.youtube.com/playlist?list=PL0YFU3y0Z_gZxc-FeLhbOFS99ZzlWl4He))\n- [UC Santa Cruz CSE 101 Intro to Data Structures and Algorithms fall 2022, by Seshadhri Comandur](https://www.youtube.com/playlist?list=PLOQjlWvnI0fY1BCDxdiUSwkRHjnNI73G6) ([Fall 2020](https://www.youtube.com/playlist?list=PLOQjlWvnI0fZGffr1_MqCoaC5nUVtQIWz))\n- [UC Santa Cruz CSE 201 Analysis of Algorithms winter 2022, by Seshadhri Comandur](https://www.youtube.com/playlist?list=PLOQjlWvnI0fYmOmrAAN-g1d4nFB2uz6tU)\n- [UC Santa Cruz CSE 202 Combinatorial Algorithms spring 2021, by Seshadhri Comandur](https://www.youtube.com/playlist?list=PLOQjlWvnI0fbn9zAJfvJoQF1nc50KQR9g)\n- [UC Santa Cruz CSE 104, 204 Computational Complexity spring 2022, by Seshadhri Comandur](https://www.youtube.com/playlist?list=PLOQjlWvnI0fYMPFnJeVZ0kt4KPwWcbF0o) ([Fall 2020](https://www.youtube.com/playlist?list=PLOQjlWvnI0fas529oXenovd3MyafNQbKl))\n- [UC Santa Cruz CSE 290A Randomized Algorithms spring 2020, by Seshadhri Comandur](https://www.youtube.com/playlist?list=PLOQjlWvnI0faRpH2oJcyW4CuM5Clt8a2n)\n- [University of Maryland CMSC351 Introduction to Algorithms, by Mohammad Hajiaghayi](https://www.youtube.com/playlist?list=PLx7SjCaKZzEJLQ9RubHk2zFuddXTD5_ac)\n- [University of Maryland CMSC858F Network Algorithms and Approximations, by Mohammad Hajiaghayi](https://www.cs.umd.edu/~hajiagha/NetDsgn11/courseNetworkDesign.html) ([YouTube playlists](https://www.youtube.com/playlist?list=PLx7SjCaKZzEIeJxOlTuXveAE5eY7WOYB9))\n- [University of Maryland CMSC858M Algorithmic Lower Bounds: Fun with Hardness Proofs, by Mohammad Hajiaghayi](https://www.cs.umd.edu/~hajiagha/ALB19/ALB19.html) ([YouTube playlists](https://www.youtube.com/playlist?list=PLx7SjCaKZzEKwynDkSTacgJGivjoB2ksn))\n- [University of Maryland UMD DATA602 / MSML602 Principles of Data Science spring 2024, by Mohammad Hajiaghayi](https://www.youtube.com/playlist?list=PLx7SjCaKZzEIzzVsO03ozPOCP7Yc-LNq_)\n- [Algorithms for Big-Data (Fall 2020) - Saket Saurabh](https://sites.google.com/view/sakethome/teaching/algorithms-for-big-data-fall-2020)\n- [CS498ABD - Algorithms for Big Data - UIUC, Fall 2020](https://courses.engr.illinois.edu/cs498abd/fa2020/schedule.html)\n- [Advanced Data Structures](https://www.youtube.com/playlist?list=PLN-ShipRKQ0h6jIphD381pHdQtj_APRM8)\n- [CS60025 Algorithmic Game Theory - IIT KGP - Winter 2020](http://cse.iitkgp.ac.in/~palash/Courses/2020AlgorithmicGameTheory/agt2020.html)\n- [CS60083 Parameterized Algorithms - IIT KGP](http://cse.iitkgp.ac.in/~palash/Courses/2020ParameterizedAlgo/paramAlgo.html)\n- [Parameterized Complexity](https://sites.google.com/view/sakethome/teaching/parameterized-complexity)\n- [Structural Graph Theory - IIT Madras](https://www.youtube.com/playlist?list=PLtDHG-2klXcEedB8L-jjvb17OIUZbF3gW)\n- [Information Theory - IISC Bangalore](https://nptel.ac.in/courses/108/108/108108168/)\n\n\n\n\n------------------------------\n\n### Systems Programming\n\n- [15-213 Introduction to Computer Systems, Fall 2015  - CMU](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22b96d90ae-9871-4fae-91e2-b1627b43e25e%22&maxResults=150)\n- [Computer Systems: A programmer''s Perspective](https://www.youtube.com/playlist?list=PLyboo2CCDSWnhzzzzDQ3OBPrRiIjl-aIE)\n- [CS361 - COMPUTER SYSTEMS - UIC](https://www.cs.uic.edu/~ckanich/cs361/f20/)\n- [CS 3650 - Computer Systems - Fall 2020 - Nat Tuck - NEU](https://web.archive.org/web/20210423030302/https://ntuck-neu.site/2020-09/cs3650/) ([Lectures - YouTube](https://www.youtube.com/playlist?list=PLtg_A_3rzLAtBuwQp6mA3WveYw9Q7GzIZ))\n- [CS 4400 â€“ Computer Systems   Fall 2016 - UoUtah](https://www.eng.utah.edu/~cs4400/)\n- [Systems - Aduni](http://aduni.org/courses/systems/index.php?view=cw)\n- [CS110: Principles of Computer Systems - Stanford](https://web.stanford.edu/class/archive/cs/cs110/cs110.1202/)\n- #### **Operating Systems**\n  - [ECS 150 - Operating Systems and Systems Programming - Fall 2020 - JoÃ«l Porquet-Lupine - UC Davis](https://lupteach.gitlab.io/courses/ucd-ecs150/online/)\n  - [CS124 Operating Systems - California Institute of Technology, Fall 2018 - Youtube](https://www.youtube.com/playlist?list=PL3swII2vlVoVbav6FV98pidq6BsTN4u56)\n  - [CS 162 Operating Systems and Systems Programming, Spring 2015 - University of California, Berkeley](https://archive.org/details/ucberkeley-webcast-PL-XXv-cvA_iBDyz-ba4yDskqMDY6A1w_c?sort=titleSorter) ([Fall 2020 - YouTube](https://www.youtube.com/playlist?list=PLF2K2xZjNEf97A_uBCwEl61sdxWVP7VWC))\n  - [CS 4414 - Operating Systems, University of Virginia (rust-class)](http://rust-class.org/pages/classes.html)\n  - [CS 4414 Operating Systems, Fall 2018 - University of Virginia](https://www.cs.virginia.edu/~cr4bd/4414/F2018/schedule.html)\n  - [CSE 421/521 - Introduction to Operating Systems, SUNY University at Buffalo, NY - Spring 2016](https://www.ops-class.org/courses/buffalo/CSE421_Spring2016/) ([Lectures - YouTube](https://www.youtube.com/playlist?list=PLE6LEE8y2Jp-kbEcVR2W3vfx0Pdca0BD3)) ([Recitations 2016](https://www.youtube.com/playlist?list=PLE6LEE8y2Jp_YJn8wu9aJTPOgeWqiaJDF)) ([Assignment walkthroughs](https://www.youtube.com/playlist?list=PLE6LEE8y2Jp9PC8fyzc2meL4XvrVSyP8O))\n  - [CS 377 - Operating Systems, Fall 16 - Umass OS](https://www.youtube.com/playlist?list=PLacuG5pysFbDTmsCRGWsMW_PzIOpXnckw)\n  - [CS 577 - Operating Systems, Spring 20 - Umass OS](https://www.youtube.com/playlist?list=PLacuG5pysFbB2_z9EkSfQIjq3yNzy8igs)\n  - [6.828 - Operating System Engineering [Fall 2014]](https://www.youtube.com/playlist?list=PLfciLKR3SgqNJKKIKUliWoNBBH1VHL3AP)\n  - [6.S081 - Operating System Engineering [Fall 2020]](https://pdos.csail.mit.edu/6.828/2020/schedule.html)\n  - [CSE 30341 - Operating Systems, Spr 2008](https://www.youtube.com/playlist?list=PLAB7D5CA7E262B0E2)\n  - [CSEP 551 Operating Systems Autumn 2014 - University of Washington](https://courses.cs.washington.edu/courses/csep551/14au/video/)\n  - [Introduction to Operating Systems - IIT Madras](https://nptel.ac.in/courses/106106144/)\n  - [CS194 Advanced Operating Systems Structures and Implementation, Spring 2013 InfoCoBuild, UC Berkeley](http://www.infocobuild.com/education/audio-video-courses/computer-science/cs194-spring2013-berkeley.html)\n  - [CSE 60641 - Graduate Operating Systems, Fall 08](https://www.youtube.com/view_play_list?p=22B10D854588E20C)\n  - [Advanced Programming in the UNIX Environment](https://stevens.netmeister.org/631/)\n  - [Operating System - IIT Madras](https://www.youtube.com/playlist?list=PLZ2ps__7DhBYcwlZ7GPCBzbowmiiF4BYR)\n- #### **Distributed Systems**\n  - [CS 677 - Distributed Operating Systems, Spring 24 - Umass OS](https://www.youtube.com/playlist?list=PLacuG5pysFbBpWHfKUU9Dfdk8RmQ7B9EH)\n  - [CS 436 - Distributed Computer Systems - U Waterloo](https://www.youtube.com/playlist?list=PLawkBQ15NDEkDJ5IyLIJUTZ1rRM9YQq6N)\n  - [6.824 - Distributed Systems, Spring 2015 - MIT](https://www.youtube.com/playlist?list=PLkcQbKbegkMqiWf7nF8apfMRL4P4sw8UL)\n  - [6.824 Distributed Systems - Spring 2020 - MIT](https://pdos.csail.mit.edu/6.824/schedule.html) ([Youtube](https://www.youtube.com/playlist?list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB))\n  - [Distributed Systems Lecture Series](https://www.youtube.com/playlist?list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB)\n  - [Distributed Algorithms, https://canvas.instructure.com/courses/902299](https://www.youtube.com/playlist?list=PL700757A5D4B3F368)\n  - [CSEP 552 - PMP Distributed Systems, Spring 2013 - University of Washington](https://courses.cs.washington.edu/courses/csep552/13sp/) ([Videos](https://courses.cs.washington.edu/courses/csep552/13sp/video/))\n  - [CSE 490H - Scalable Systems: Design, Implementation and Use of Large Scale Clusters, Autumn 2008 - University of Washington](https://courses.cs.washington.edu/courses/cse490h/08au/lectures.htm) ([Videos](https://courses.cs.washington.edu/courses/cse490h/08au/video.htm))\n  - [MOOC - Cloud Computing Concepts - UIUC](https://www.youtube.com/playlist?list=PLFd87qVsaLhOkTLvfp6MC94iFa_1c9wrU)\n  - [Distributed Systems (Prof. Pallab Dasgupta)](https://www.youtube.com/playlist?list=PLUJ7JmcrTifBROWODSG8wgyl20XgBuE-N)\n  - [EdX KTHx ID2203 Reliable Distributed Algorithms](https://www.youtube.com/playlist?list=PLx3mQFFeHPjndmQ0iP9j6C58b90hqGa0X)\n  - [Distributed Data Management - Technische UniversitÃ¤t Braunschweig, Germany](http://www.ifis.cs.tu-bs.de/teaching/ss-15/ddm)\n  - [Information Retrieval and Web Search Engines - Technische UniversitÃ¤t Braunschweig, Germany](http://www.ifis.cs.tu-bs.de/teaching/ws-1516/IRWS)\n  - [Middleware and Distributed Systems (WS 2009/10) - Dr. Martin von LÃ¶wis - HPI](https://www.tele-task.de/series/729/)\n  - [CSE 138 - Distributed Systems - UC Santa Cruz, Spring 2020](https://www.youtube.com/playlist?list=PLNPUF5QyWU8O0Wd8QDh9KaM1ggsxspJ31) ([2021](https://www.youtube.com/playlist?list=PLNPUF5QyWU8PydLG2cIJrCvnn5I_exhYx))\n  - [CMU 15 440 / 640 Distributed Systems Spring 2022, by Mahadev Satyanarayanan, Padmanabhan Pillai](https://www.youtube.com/playlist?list=PLIygTcviGPKAp30J9kcVW9jPzFC7Otpol)\n  - [UNC Comp533 - Distributed Systems Spring 2020](https://www.youtube.com/playlist?list=PLH5XTBxCO2hzgww9p5sew30lx3ngJsxcB)\n  - [Brown CSCI 1380 Distributed Computer Systems spring 2016, by Tom Doeppner & Rodrigo Fonseca](https://cs.brown.edu/courses/cs138/s16/syllabus.html)\n  - [Distributed Systems lecture series - Martin Kleppmann](https://www.youtube.com/playlist?list=PLeKd45zvjcDFUEv_ohr_HdUFe97RItdiB)\n  - [Distributed Algorithms - Jukka Suomela](https://www.youtube.com/playlist?list=PL2RY7P3JxZN8g9hFCasNqzuDhZbIbAj54)\n  - [Programming Parallel Computers - Jukka Suomela](https://www.youtube.com/playlist?list=PL2RY7P3JxZN-Pz1nwvnoJ9uEHmOmv4jmi)\n- #### **Real-Time Systems**\n  - [CPCS 663 - Real-Time Systems: Video Material - TAMU](http://faculty.cs.tamu.edu/bettati/Courses/663/Video/presentation.html)\n  - [Real Time Systems - IIT Kharagpur](https://nptel.ac.in/courses/106105036/)\n- [6.172 Performance Engineering of Software Systems - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-172-performance-engineering-of-software-systems-fall-2018/)\n- [Performance Evaluation of Computer Systems - IIT Madras](https://nptel.ac.in/courses/106106048/)\n- [Storage Systems - IISC Bangalore](https://nptel.ac.in/courses/106108058/)\n- [MAP6264 - Queueing Theory - FAU](http://www.cse.fau.edu/~bob/courses/map6264/)([Video Lectures](https://vimeo.com/album/171324/))\n- [EE 380 Colloquium on Computer Systems - Stanford University](http://web.stanford.edu/class/ee380/) ([Lecture videos](https://www.youtube.com/playlist?list=PLoROMvodv4rMWw6rRoeSpkiseTHzWj6vu))\n\n------------------------------\n\n### Database Systems\n\n- [CMPSC 431W Database Management Systems, Fall 2015 - Penn State University](http://www.cse.psu.edu/~wul2/cmpsc431w/) [Lectures - YouTube](https://www.youtube.com/playlist?list=PLstRzn3gXZMdXqAiVJ1NN2CoyXHqma7pQ)\n- [CS121 - Introduction to Relational Database Systems, Fall 2016 - Caltech](http://users.cms.caltech.edu/~donnie/cs121/)\n- [CS 5530 - Database Systems, Spring 2016 - University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCrercQNP9tTsjjPdgRVYvC7)\n- [Distributed Data Management (WT 2018/19) - HPI University of Potsdam](https://www.tele-task.de/series/1224/)\n- [MOOC - Database Stanford Dbclass](https://www.youtube.com/playlist?list=PL6hGtHedy2Z4EkgY76QOcueU8lAC4o6c3)\n- [CSEP 544, Database Management Systems, Au 2015 - University of Washington](https://www.youtube.com/playlist?list=PLTPQEx-31JXjQYrUKvAjUTWgCYluHGs_L)\n- [Database Design - IIT Madras](https://nptel.ac.in/courses/106106093/)\n- [Fundamentals of Database Systems - IIT Kanpur](https://nptel.ac.in/courses/106104135/)\n- [Principles of Database Management, Bart Baesens](https://www.youtube.com/playlist?list=PLdQddgMBv5zEhlpqdiUcf9aTNEtmESgyl)\n- [FIT9003 Database Systems Design - Monash University](https://itunes.apple.com/us/podcast/fit9003-database-systems-design/id306569364)\n- [15-445 - Introduction to Database Systems, CMU](https://15445.courses.cs.cmu.edu/fall2025/) ([YouTube-2017](https://www.youtube.com/playlist?list=PLSE8ODhjZXjYutVzTeAds8xUt1rcmyT7x)), ([YouTube-2018](https://www.youtube.com/playlist?list=PLSE8ODhjZXja3hgmuwhf89qboV1kOxMx7)),([YouTube-2019](https://www.youtube.com/playlist?list=PLSE8ODhjZXjbohkNBWQs_otTrBTrjyohi)), ([YouTube-2021](https://www.youtube.com/playlist?list=PLSE8ODhjZXjZaHA6QcxDfJ0SIWBzQFKEG)), ([YouTube-2022](https://www.youtube.com/playlist?list=PLSE8ODhjZXjaKScG3l0nuOiDTTqpfnWFf)),([YouTube-2023](https://youtube.com/playlist?list=PLSE8ODhjZXjbj8BMuIrRcacnQh20hmY9g&si=R7F_J9zbXsG07PjR)),([YouTube-2024](https://youtube.com/playlist?list=PLSE8ODhjZXjYDBpQnSymaectKjxCy6BYq&si=H-wfgjoLz6ifhZqS)),([YouTube-2025](https://youtube.com/playlist?list=PLSE8ODhjZXjYMAgsGH-GtY5rJYZ6zjsd5&si=KKXgy16Zm20utGyH))\n- [15-721 - Advanced Database Systems, CMU](http://15721.courses.cs.cmu.edu/spring2024) ([YouTube-2024](https://youtube.com/playlist?list=PLSE8ODhjZXjYa_zX-KeMJui7pcN1rIaIJ&si=J9cH2uZ0pFUu8q6f), [YouTube-2023](https://youtube.com/playlist?list=PLSE8ODhjZXjYzlLMbX3cR0sxWnRM7CLFn&si=O78E7wsQlVhgwE_u), [YouTube-2022](https://youtube.com/playlist?list=PLSE8ODhjZXjasmrEd2_Yi1deeE360zv5O&si=DsDfVhDp6j0n981J))\n- [CS122 - Relational Database System Implementation, Winter 2014-2015 - Caltech](http://users.cms.caltech.edu/~donnie/cs122/)\n- [CS 186 - Database Systems, UC Berkeley, Spring 2015](http://www.infocobuild.com/education/audio-video-courses/computer-science/cs186-spring2015-berkeley.html)\n- [CS 6530 - Graduate-level Database Systems, Fall 2016, University of Utah](https://www.cs.utah.edu/~lifeifei/cs6530/) ([Lectures - YouTube](https://www.youtube.com/playlist?list=PLbuogVdPnkCqwHUcieMrytP453Ep0y6eI))\n- [6.830/6.814 - Database Systems Fall 2014](https://www.youtube.com/playlist?list=PLfciLKR3SgqOxCy1TIXXyfTqKzX2enDjK)\n- [Informatics 1 - Data & Analysis 2014/15- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2014/da.htm)\n- [Database Management Systems, Aduni](http://aduni.org/courses/databases/index.php?view=cw)\n- [D4M - Signal Processing on Databases](https://ocw.mit.edu/resources/res-ll-005-d4m-signal-processing-on-databases-fall-2012/)\n- [In-Memory Data Management (2013)Prof. Hasso Plattner - HPI](https://open.hpi.de/courses/imdb2013/items/72j6pftms3dOSunM98JhfW)\n- [Distributed Data Management (WT 2019/20) - Dr. Thorsten Papenbrock - HPI](https://www.tele-task.de/series/1285/)\n- [CS122d - NoSQL Data Management (Spring 21) - Prof. Mike Carey - UC Irvine](https://uci.yuja.com/V/PlayList?node=9933576&a=1583628376&autoplay=1)\n\n------------------------------\n\n### Software Engineering\n\n- #### **Object Oriented Design**\n  - [ECE 462 Object-Oriented Programming using C++ and Java - Purdue](https://engineering.purdue.edu/OOSD/F2008/F2008.html)\n  - [Object-oriented Program Design and Software Engineering - Aduni](http://aduni.org/courses/java/index.php?view=cw)\n  - [OOSE - Object-Oriented Software Engineering, Dr. Tim Lethbridge](https://www.youtube.com/playlist?list=PL6iDJCG2nkhfNlig8NY5ePPfGvtQX6yLa)\n  - [Object Oriented Systems Analysis and Design (Systems Analysis and Design in a Changing World)](https://www.youtube.com/playlist?list=PL6XklZATqYx9dj72MKG6wLYjljeB2odra)\n  - [CS 251 - Intermediate Software Design (C++ version) - Vanderbilt University](https://www.youtube.com/playlist?list=PLZ9NgFYEMxp4ZsvD10uXmClGnukcu3Uff)\n  - [OOSE - Software Dev Using UML and Java](https://www.youtube.com/playlist?list=PLJ9pm_Rc9HesnkwKlal_buSIHA-jTZMpO)\n  - [Object-Oriented Analysis and Design - IIT Kharagpur](https://nptel.ac.in/courses/106105153/)\n  - [CS3 - Design in Computing - Richard Buckland UNSW](https://www.youtube.com/playlist?list=PL0C5D85DBA20E685C)\n  - [Informatics 1 - Object-Oriented Programming 2014/15- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2014/inf1op.htm)\n  - [Software Engineering with Objects and Components 2015/16- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2015/seoc.htm)\n- #### **Software Engineering**\n  - [Computer Science 169- Software Engineering - Spring 2015 - UCBerkeley](https://youtube.com/playlist?list=PLVEFwJhglgHJQEQ6RjMMjcclix94gp1k2)\n  - [Computer Science 169- Software Engineering - Fall 2019 - UCBerkeley](https://www.youtube.com/playlist?list=PLkFD6_40KJIxCKgzL0uysjsAtfY3JawLS)\n  - [CS 5150 -  Software Engineering, Fall 2014 - Cornell University](http://www.cs.cornell.edu/courses/cs5150/2014fa/materials.html)\n  - [Introduction to Service Design and Engineering - University of Trento, Italy](https://www.youtube.com/playlist?list=PLBdajHWwi0JCn87QuFT3e58mekU0-6WUT)\n  - [CS 164 Software Engineering - Harvard](https://www.youtube.com/watch?v=3zdfCR6c8vw&list=PLuhjguFxSeVLvKvWwTUIpVwXdLtZPX1ZS)\n  - [System Analysis and Design - IISC Bangalore](https://nptel.ac.in/courses/106108102/)\n  - [Software Engineering - IIT Bombay](https://nptel.ac.in/courses/106101061/)\n  - [Dependable Systems (SS 2014)- HPI University of Potsdam](https://www.tele-task.de/series/1005/)\n  - [Automated Software Testing - ETH ZÃ¼rich | Spring 2024](https://video.ethz.ch/lectures/d-infk/2024/spring/263-2815-00L/9c81df65-d04d-411a-bea4-cbd32eb249e5.html)\n  - [Software Testing - IIT Kharagpur](https://nptel.ac.in/courses/106105150/)\n  - [Software Testing - Udacity, course-cs258 | 2015](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkWVHeC_8aSIbSxE_NXI76g)\n  - [Software Debugging - Udacity, course-cs259 | 2015](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkxK63TiT88oEe-AIBhr96A)\n  - [Software Engineering - Bauhaus-Uni Weimar](https://www.youtube.com/watch?v=jouBM4AH0jw&list=PLjEglKdMOevU2STTGq79duxTXDFuO-k1H&index=2)\n  - [CMU 17-445 Software Engineering for AI-Enabled Systems summer 2020, by Christian Kaestner](https://www.youtube.com/playlist?list=PLDS2JMJnJzdkQPdkhcuwcbJpjB84g9ffX)\n- #### **Software Architecture**\n  - [CS 411 - Software Architecture Design - Bilkent University](http://video.bilkent.edu.tr/course_videos.php?courseid=10)\n  - [MOOC - Software Architecture & Design - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkMTetlG7xKWaI5ZAZFX8fL)\n  - [CS-310 Scalable Software Architectures](https://www.youtube.com/playlist?list=PLWl7jvxH18r0u5VRZsOjhghNXc_Ec4dZz)\n- #### **Concurrency**\n  - [CS176 - Multiprocessor Synchronization - Brown University](http://cs.brown.edu/courses/cs176/course_information.shtml) ([Videos from 2012](http://www.brown.edu/cis/sta/dev/herlihy_csci1760_fa12/#vid))\n  - [CS 282 (2014): Concurrent Java Network Programming in Android](https://www.youtube.com/playlist?list=PLZ9NgFYEMxp4KSJPUyaQCj7x--NQ6kvcX)\n  - [CSE P 506 â€“ Concurrency, Spring 2011 - University of Washington](https://courses.cs.washington.edu/courses/csep506/11sp/Home.html) ([Videos](https://courses.cs.washington.edu/courses/csep506/11sp/Videos.html))\n  - [CSEP 524 - Parallel Computation - University of Washington](https://courses.cs.washington.edu/courses/csep524/10sp/) ([Videos](https://courses.cs.washington.edu/courses/csep524/10sp/lectures/video.html))\n  - [Parallel Programming Concepts (WT 2013/14) - HPI University of Potsdam](https://www.tele-task.de/series/977/)\n  - [Parallel Programming Concepts (WT 2012/13) - HPI University of Potsdam](https://www.tele-task.de/series/924/)\n  - [UIUC ECE 408 / CS 408 Applied Parallel Programming fall 2022, by Wen-mei Hwu, Sanjay Patel](https://www.youtube.com/playlist?list=PL6RdenZrxrw-UKfRL5smPfFFpeqwN3Dsz) ([Spring 2018](https://www.youtube.com/playlist?list=PLRRuQYjFhpmvu5ODQoY2l7D0ADgWEcYAX))\n  - [UIUC ECE 508 / CS 508 Manycore Parallel Algorithms spring 2019, by Wen-mei Hwu](https://www.youtube.com/playlist?list=PLRRuQYjFhpmspsME4LmLbuCG1VHbJmIcy)\n  - [UIUC CS 420 / ECE 492 / CSE 402 Introduction to Parallel Programming for Scientists and Engineers fall 2015, by Sanjay Kale](https://www.youtube.com/playlist?list=PL682UO4IMem9cAjfy_RPjAc6k-HPYpTa9)\n  - [Stanford CME 213 Introduction to Parallel Computing using MPI, openMP, and CUDA winter 2020, by Eric Darve](https://www.youtube.com/playlist?list=PLAtMgFDMfGy2mysjPHN_d1cf9sR1muRkq)\n- #### **Mobile Application Development**\n  - [MOOC Programming Mobile Applications for Android Handheld Systems - University of Maryland](https://www.youtube.com/playlist?list=PLkHsKoi6eZnwilGXUc95CqS7Vw4uLLDLG)\n  - [CS 193p - Developing Applications for iOS, Stanford University](https://cs193p.sites.stanford.edu/)\n  - [CS S-76 Building Mobile Applications - Harvard](http://cs76.tv/2013/summer/)\n  - [CS 251 (2015): Intermediate Software Design](https://www.youtube.com/playlist?list=PLZ9NgFYEMxp7lylj-XC8h1kjatOjbh9ne)\n  - [Android App Development for Beginners Playlist - thenewboston](https://www.youtube.com/playlist?list=PL6gx4Cwl9DGBsvRxJJOzG4r4k_zLKrnxl)\n  - [Android Application Development Tutorials - thenewboston](https://www.youtube.com/playlist?list=PL2F07DBCDCC01493A)\n  - [MOOC - Developing Android Apps - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPnMwH5-FNkErnnq_aSy706S)\n  - [MOOC - Advanced Android App Development - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPmETCT07vnDSiIaUBuyut0X)\n  - [CSSE490 Android Development Rose-Hulman Winter 2010-2011, Dave Fisher](https://www.youtube.com/playlist?list=PLF3EEB647F6B52F03)\n  - [iOS Course, Dave Fisher](https://www.youtube.com/playlist?list=PL96C635E4DCD393A8)\n  - [Developing iPad Applications for Visualization and Insight - Carnegie Mellon University](https://itunes.apple.com/us/course/developing-ipad-applications/id499050344)\n  - [Mobile Computing - IIT Madras](https://nptel.ac.in/courses/106106147/)\n  - [Mobile Information Systems - Bauhaus-Uni Weimar](https://www.youtube.com/watch?v=8EmbrZJwMOI&list=PLjEglKdMOevWv4zPW0diw7iJFdT7s4sTP)\n\n------------------------------\n\n### Artificial Intelligence\n\n- [CS50 - Introduction to Artificial Intelligence with Python (and Machine Learning), Harvard OCW](https://cs50.harvard.edu/ai/2023/)\n- [CS 188 - Introduction to Artificial Intelligence, UC Berkeley - Spring 2025, by John Canny, Oliver Grillmeyer](https://inst.eecs.berkeley.edu/~cs188/sp25/) ([Spring 2024](https://inst.eecs.berkeley.edu/~cs188/sp24/)) ([Spring 2023](https://www.youtube.com/playlist?list=PLp8QV47qJEg7WWVg_5eOECzVPpy23UjJz))\n- [6.034 Artificial Intelligence, MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/)\n- [CS221: Artificial Intelligence: Principles and Techniques - Autumn 2019 - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rO1NB9TD4iUZ3qghGEGtqNX)\n- [15-780 - Graduate Artificial Intelligence, Spring 14, CMU](http://www.cs.cmu.edu/~zkolter/course/15-780-s14/lectures.html)\n- [CSE 592 Applications of Artificial Intelligence, Winter 2003 - University of Washington](https://courses.cs.washington.edu/courses/csep573/03wi/lectures/index.htm)\n- [CS322 - Introduction to Artificial Intelligence, Winter 2012-13 - UBC](http://www.cs.ubc.ca/~mack/CS322/) ([YouTube](https://www.youtube.com/playlist?list=PLDPnGbm0sUmpzvcGvktbz446SLdFbfZVU))\n- [CS 4804: Introduction to Artificial Intelligence, Fall 2016](https://www.youtube.com/playlist?list=PLUenpfvlyoa1iiSbGy9BBewgiXjzxVgBd)\n- [CS 5804: Introduction to Artificial Intelligence, Spring 2015](https://www.youtube.com/playlist?list=PLUenpfvlyoa0PB6_kqJ9WU7m6i6z1RhfJ)\n- [Artificial Intelligence, Fall 2023 - FAU](https://www.fau.tv/course/id/3595) ([Spring 2023](https://www.fau.tv/course/id/3386)) ([Fall 2022](https://www.fau.tv/course/id/3180)) ([Spring 2021](https://www.fau.tv/course/id/2095)) ([Fall 2020](https://www.fau.tv/course/id/1690)) ([Fall 2018](https://www.fau.tv/course/id/713)) ([Spring 2018](https://www.fau.tv/course/id/657))\n- [Artificial Intelligence - IIT Kharagpur](https://nptel.ac.in/courses/106105077/)\n- [Artificial Intelligence - IIT Madras](https://nptel.ac.in/courses/106106126/)\n- [Artificial Intelligence(Prof.P.Dasgupta) - IIT Kharagpur](https://nptel.ac.in/courses/106105079/)\n- [MOOC - Intro to Artificial Intelligence - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPlqMkzr4xyuD6cXTIgPuzgn)\n- [MOOC - Artificial Intelligence for Robotics - Udacity](https://www.youtube.com/playlist?list=PLAwxTw4SYaPkCSYXw6-a_aAoXVKLDwnHK)\n- [Graduate Course in Artificial Intelligence, Autumn 2012 - University of Washington](https://www.youtube.com/playlist?list=PLbQ3Aya0VERDoDdbMogU9EASJGWris9qG)\n- [Agent-Based Systems 2015/16- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2015/abs.htm)\n- [Informatics 2D - Reasoning and Agents 2014/15- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2014/inf2d.htm)\n- [Artificial Intelligence - Hochschule Ravensburg-Weingarten](https://www.youtube.com/playlist?list=PL39B5D3AFC249556A)\n- [Deductive Databases and Knowledge-Based Systems - Technische UniversitÃ¤t Braunschweig, Germany](http://www.ifis.cs.tu-bs.de/teaching/ws-1516/KBS)\n- [Artificial Intelligence: Knowledge Representation and Reasoning - IIT Madras](https://nptel.ac.in/courses/106106140/)\n- [Semantic Web Technologies by Dr. Harald Sack - HPI](https://www.youtube.com/playlist?list=PLoOmvuyo5UAeihlKcWpzVzB51rr014TwD)\n- [Knowledge Engineering with Semantic Web Technologies by Dr. Harald Sack - HPI](https://www.youtube.com/playlist?list=PLoOmvuyo5UAcBXlhTti7kzetSsi1PpJGR)\n- [T81-558: Applications of Deep Neural Networks by Jeff Heaton, 2022, Washington University in St. Louis](https://sites.wustl.edu/jeffheaton/t81-558/)\n- [MSU programming for AI](https://www.youtube.com/playlist?list=PLZ-krWGO-UEz84TseDMIlx2Set6xZp0YP)\n\n------------------------------\n\n### Machine Learning\n\n- #### **Introduction to Machine Learning**\n  - [Introduction to Machine Learning for Coders](https://course18.fast.ai/ml)\n  - [MOOC - Statistical Learning, Stanford University](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/)\n  - [Statistical Learning with Python - Stanford Online](https://www.youtube.com/playlist?list=PLoROMvodv4rPP6braWoRt5UCXYZ71GZIQ)\n  - [Foundations of Machine Learning Boot Camp, Berkeley Simons Institute](https://www.youtube.com/playlist?list=PLgKuh-lKre11GbZWneln-VZDLHyejO7YD)\n  - [CS 155 - Machine Learning & Data Mining, 2023 - Caltech](https://www.youtube.com/playlist?list=PLu5yXJ11s4r-nBlojSQC9seeUF1IxX-_z) ([Notes-2020](http://www.yisongyue.com/courses/cs155/2020_winter/)) ([YouTube-2020](https://www.youtube.com/playlist?list=PLuz4CTPOUNi67hPzb9zJXH1cbeN7LKNiD)) ([Notes-2019](http://www.yisongyue.com/courses/cs155/2019_winter/)) ([YouTube-2019](https://www.youtube.com/playlist?list=PLuz4CTPOUNi7r2trKGgwaedY17MADTay4)) ([Notes-2018](http://www.yisongyue.com/courses/cs155/2018_winter/)) ([YouTube-2018](https://www.youtube.com/playlist?list=PLuz4CTPOUNi644ypoxzP1frkPYVHdjDJU)) ([Notes-2017](http://www.yisongyue.com/courses/cs155/2017_winter/)) ([YouTube-2017](https://www.youtube.com/playlist?list=PLuz4CTPOUNi6BfMrltePqMAHdl5W33-bC)) ([Notes-2016](http://www.yisongyue.com/courses/cs155/2016_winter/)) ([YouTube-2016](https://www.youtube.com/playlist?list=PL5HdMttxBY0BVTP9y7qQtzTgmcjQ3P0mb))\n  - [CS 156 - Learning from Data, Caltech](https://work.caltech.edu/lectures.html)\n  - [10-601 - Introduction to Machine Learning (MS) - Tom Mitchell - 2015, CMU](http://www.cs.cmu.edu/~ninamf/courses/601sp15/lectures.shtml) ([YouTube](https://www.youtube.com/playlist?list=PLAJ0alZrN8rD63LD0FkzKFiFgkOmEtltQ))\n  - [10-601 Machine Learning | CMU | Fall 2017](https://www.youtube.com/playlist?list=PL7k0r4t5c10-g7CWCnHfZOAxLaiNinChk)\n  - [10-701 - Introduction to Machine Learning (PhD) - Tom Mitchell, Spring 2011, CMU](http://www.cs.cmu.edu/~tom/10701_sp11/lectures.shtml) ([Fall 2014](https://www.youtube.com/playlist?list=PL7y-1rk2cCsDZCVz2xS7LrExqidHpJM3B)) ([Spring 2015 by Alex Smola](https://www.youtube.com/playlist?list=PLZSO_6-bSqHTTV7w9u7grTXBHMH-mw3qn)) ([Fall 2020 by Ziv Bar-Joseph, Eric Xing](https://www.youtube.com/playlist?list=PLsWN0V-b507g7dbQTUvFkKZEqdHR5Fh4P))\n  - [10 - 301/601 - Introduction to Machine Learning - Fall 2023 - CMU](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22d5bf275d-ff88-4bf6-a865-b065010f55c2%22)\n  - [6.036 - Machine Learning, Broderick - MIT Fall 2020](https://www.youtube.com/playlist?list=PLxC_ffO4q_rW0bqQB80_vcQB09HOA3ClV)\n  - [Mediterranean Machine Learning summer school 2024](https://www.youtube.com/playlist?list=PLF-wkqRv4u1bV4Zd1UepYfyZXkv6Bz6ra) ([YouTube-2023](https://www.youtube.com/playlist?list=PLF-wkqRv4u1Y-Bret-wrcPypPCZ3Gg_3L)) ([YouTube-2022](https://www.youtube.com/playlist?list=PLF-wkqRv4u1agtfVaDsDUaMHmToP84Fk6)) ([YouTube-2021](https://www.youtube.com/playlist?list=PLF-wkqRv4u1YRbfnwN8cXXyrmXld-sked))\n  - [LxMLS Lisbon Machine Learning School 2024](https://www.youtube.com/playlist?list=PLQl_xdhSmQeh4eRfAwETbtJJLPKcDLrzw) ([YouTube-2023](https://www.youtube.com/playlist?list=PLQl_xdhSmQeikRCf-wJ8NCK51JOMHGOCP)) ([YouTube-2022](https://www.youtube.com/playlist?list=PLQl_xdhSmQejdxQL7qI5aJkLcAQJ68Abp)) ([YouTube-2021](https://www.youtube.com/playlist?list=PLQl_xdhSmQegzsLin54NbfePFAuTUEmUj)) ([YouTube-2020](https://www.youtube.com/playlist?list=PLQl_xdhSmQehE6aAk774yWMag4NNBGr5k))\n  - [Applied Machine Learning (Cornell Tech CS 5787, Fall 2020)](https://www.youtube.com/playlist?list=PL2UML_KCiC0UlY7iCQDSiGDMovaupqc83)\n  - [Stanford CS229: Machine Learning Course | Summer 2019 (Anand Avati)](https://www.youtube.com/playlist?list=PLoROMvodv4rNH7qL6-efu_q2_bPuy0adh) ([Spring 2022](https://www.youtube.com/playlist?list=PLoROMvodv4rNyWOpJg_Yh4NSqI4Z4vOYy))\n  - [CMS 165 Foundations of Machine Learning - 2019 - Caltech](http://tensorlab.cms.caltech.edu/users/anima/cms165-2019.html) ([Youtube](https://www.youtube.com/playlist?list=PLVNifWxslHCA5GUh0o92neMiWiQiGVFqp))\n  - [CMS 165 Foundations of Machine Learning and Statistical Inference - 2020 - Caltech](https://www.youtube.com/playlist?list=PLVNifWxslHCDlbyitaLLYBOAEPbmF1AHg)\n  - [Microsoft Research - Machine Learning Course](https://www.youtube.com/playlist?list=PL34iyE0uXtxo7vPXGFkmm6KbgZQwjf9Kf)\n  - [CS 446 - Machine Learning, Fall 2016, UIUC](https://www.youtube.com/playlist?list=PLQcasX5-oG91TgY6A_gz-IW7YSpwdnD2O)\n  - [CS 582 - Machine Learning for Bioinformatics, Fall 2024, UIUC](https://www.youtube.com/playlist?list=PLIygTcviGPKBtxPX875Z8suCYGJkrPQ2Z)\n  - [ECE 364 - Programming Methods for Machine Learning, Spring 2025, UIUC](https://www.youtube.com/playlist?list=PLIygTcviGPKD0WT5NEsD6AIqaeV4azJwN)\n  - [undergraduate machine learning at UBC 2012, Nando de Freitas](https://www.youtube.com/playlist?list=PLE6Wd9FR--Ecf_5nCbnSQMHqORpiChfJf)\n  - [CS 229 - Machine Learning - Stanford University](https://see.stanford.edu/Course/CS229) ([Autumn 2018](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU))\n  - [CS 189/289A Introduction to Machine Learning, Prof Jonathan Shewchuk - UCBerkeley](https://people.eecs.berkeley.edu/~jrs/189/)\n  - [CPSC 340: Machine Learning and Data Mining (2018) - UBC](https://www.youtube.com/playlist?list=PLWmXHcz_53Q02ZLeAxigki1JZFfCO6M-b)\n  - [CS391L Machine Learning, Spring 2025 - UT Austin](https://utcs-ml-course.github.io/main/Lectures/)\n  - [CS4780/5780 Machine Learning, Fall 2013 - Cornell University](http://www.cs.cornell.edu/courses/cs4780/2013fa/)\n  - [CS4780/5780 Machine Learning, Fall 2018 - Cornell University](http://www.cs.cornell.edu/courses/cs4780/2018fa/page18/index.html) ([Youtube](https://www.youtube.com/playlist?list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS))\n  - [CSE474/574 Introduction to Machine Learning - SUNY University at Buffalo](https://www.youtube.com/playlist?list=PLEQDy5tl3xkMzk_zlo2DPzXteCquHA8bQ)\n  - [CS 5350/6350 - Machine Learning, Spring 2024, University of Utah](https://svivek.com/teaching/machine-learning/spring2024/) ([Youtube](https://www.youtube.com/playlist?list=PLbuogVdPnkCr-ANNi5GZid3MvSkzm_wnM))\n  - [ECE 4252/8803 Fundamentals of Machine Learning (FunML), Spring 2024 - Georgia Tech](https://alregib.ece.gatech.edu/georgia-tech-courses/funml/)\n  - [ECE 5984 Introduction to Machine Learning, Spring 2015 - Virginia Tech](https://filebox.ece.vt.edu/~s15ece5984/)\n  - [CSx824/ECEx242 Machine Learning, Bert Huang, Fall 2015 - Virginia Tech](https://www.youtube.com/playlist?list=PLUenpfvlyoa0rMoE5nXA8kdctBKE9eSob)\n  - [STA 4273H - Large Scale Machine Learning, Winter 2015 - University of Toronto](http://www.cs.toronto.edu/~rsalakhu/STA4273_2015/lectures.html)\n  - [CSC 2515 Introduction to Machine Learning, Amir-massoud Farahmand, Fall 2021, University of Toronto](https://www.youtube.com/playlist?list=PLCveiXxL2xNZRg7PVp-JM4teSmaBETksy)\n  - [ECE 421 Introduction to Machine Learning, Amir Ashouri, Winter 2019, University of Toronto](https://www.youtube.com/playlist?list=PL-Mfq5QS-s8iS9XqKuApPE1TSlnZblFHF)\n  - [EECS 4404E/5327 Introduction to Machine Learning, Amir Ashouri, Fall 2019, York University](https://www.youtube.com/playlist?list=PL-Mfq5QS-s8horb94sQH4xcL85zDkpL9w)\n  - [CS 480/680 Introduction to Machine Learning, Gautam Kamath, University of Waterloo](http://www.gautamkamath.com/courses/CS480-sp2021.html) ([Spring 2021](https://www.youtube.com/playlist?list=PLmd_zeMNzSvSzRRc4Q29qEcpxbhdwjMOx))\n  - [CS 480/680 Introduction to Machine Learning, Kathryn Simone, University of Waterloo](https://github.com/kpc-simone/cs480-f24) ([Fall 2024](https://www.youtube.com/playlist?list=PLH84ETHrlsC8sbfb8WaOeXSx9ySH2Qoyc))\n  - [CS 485/685 Machine Learning, Shai Ben-David, University of Waterloo](https://www.youtube.com/channel/UCR4_akQ1HYMUcDszPQ6jh8Q/videos)\n  - [STAT 441/841 Classification Winter 2017 , Waterloo](https://www.youtube.com/playlist?list=PLehuLRPyt1HzXDemu7K4ETcF0Ld_B5adG)\n  - [10-605 - Machine Learning with Large Datasets, Fall 2016 - CMU](https://www.youtube.com/channel/UCIE4UdPoCJZMAZrTLuq-CPQ/videos)\n  - [Information Theory, Pattern Recognition, and Neural Networks - University of Cambridge](https://www.youtube.com/playlist?list=PLruBu5BI5n4aFpG32iMbdWoRVAA-Vcso6)\n  - [Pattern Analysis (2018) - FAU](https://www.fau.tv/course/id/655) ([Class 2017](https://www.fau.tv/course/id/544)) ([Class 2016](https://www.fau.tv/course/id/449)) ([Class 2015](https://www.fau.tv/course/id/355)) ([Class 2009](https://www.fau.tv/course/id/2))\n  - [Pattern Recognition (2020-2021) - FAU](https://www.fau.tv/course/id/1579) ([Class 2012-2013](https://www.fau.tv/course/id/173))\n  - [Beyond the Patterns (2020-2021) - FAU](https://www.fau.tv/course/id/1868)\n  - [Python and machine learning - Stanford Crowd Course Initiative](https://www.youtube.com/playlist?list=PLVxFQjPUB2cnYGZPAGG52OQc9SpWVKjjB)\n  - [MOOC - Machine Learning Part 1a - Udacity/Georgia Tech](https://www.youtube.com/playlist?list=PLAwxTw4SYaPl0N6-e1GvyLp5-MUMUjOKo) ([Part 1b](https://www.youtube.com/playlist?list=PLAwxTw4SYaPlkESDcHD-0oqVx5sAIgz7O) [Part 2](https://www.youtube.com/playlist?list=PLAwxTw4SYaPmaHhu-Lz3mhLSj-YH-JnG7) [Part 3](https://www.youtube.com/playlist?list=PLAwxTw4SYaPnidDwo9e2c7ixIsu_pdSNp))\n  - [Pattern Recognition Class (2012)- UniversitÃ¤t Heidelberg](https://www.youtube.com/playlist?list=PLuRaSnb3n4kRDZVU6wxPzGdx1CN12fn0w)\n  - [Introduction to Machine Learning and Pattern Recognition - CBCSL OSU](https://www.youtube.com/playlist?list=PLcXJymqaE9PPGGtFsTNoDWKl-VNVX5d6b)\n  - [Introduction to Machine Learning - IIT Kharagpur](https://nptel.ac.in/courses/106105152/)\n  - [Introduction to Machine Learning - IIT Madras](https://nptel.ac.in/courses/106106139/)\n  - [Pattern Recognition - IISC Bangalore](https://nptel.ac.in/courses/117108048/)\n  - [Pattern Recognition and Application - IIT Kharagpur](https://nptel.ac.in/courses/117105101/)\n  - [Pattern Recognition - IIT Madras](https://nptel.ac.in/courses/106106046/)\n  - [Machine Learning Summer School 2013 - Max Planck Institute for Intelligent Systems TÃ¼bingen](https://www.youtube.com/playlist?list=PLqJm7Rc5-EXFv6RXaPZzzlzo93Hl0v91E)\n  - [Machine Learning - Professor Kogan (Spring 2016) - Rutgers](https://www.youtube.com/playlist?list=PLauepKFT6DK_1_plY78bXMDj-bshv7UsQ)\n  - [CS273a: Introduction to Machine Learning](http://sli.ics.uci.edu/Classes/2015W-273a) ([YouTube](https://www.youtube.com/playlist?list=PLkWzaBlA7utJMRi89i9FAKMopL0h0LBMk))\n  - [Machine Learning Crash Course 2015](https://www.youtube.com/playlist?list=PLyGKBDfnk-iD5dK8N7UBUFVVDBBtznenR)\n  - [COM4509/COM6509 Machine Learning and Adaptive Intelligence 2015-16](http://inverseprobability.com/mlai2015/)\n  - [Introduction to Machine Learning - Spring 2018 - ETH Zurich](https://www.youtube.com/playlist?list=PLzn6LN6WhlN273tsqyfdrBUsA-o5nUESV)\n  - [Machine Learning - Pedro Domingos- University of Washington](https://www.youtube.com/user/UWCSE/playlists?view=50&sort=dd&shelf_id=16)\n  - [CSE 446/546 - Machine Learning, Spring 2020 - University of Washington](https://courses.cs.washington.edu/courses/cse446/20sp/schedule/) ([Videos](https://www.youtube.com/playlist?list=PLrE1feouzSWr7LBFAeRZIb7CN9H6dB9Jt))\n  - [Machine Learning (COMP09012)](https://www.youtube.com/playlist?list=PLyH-5mHPFffFwz7Twap0XuVeUJ8vuco9t)\n  - [Probabilistic Machine Learning 2020 - University of TÃ¼bingen](https://www.youtube.com/playlist?list=PL05umP7R6ij1tHaOFY96m5uX3J21a6yNd)\n  - [Statistical Machine Learning 2020 - Ulrike von Luxburg - University of TÃ¼bingen](https://www.youtube.com/playlist?list=PL05umP7R6ij2XCvrRzLokX6EoHWaGA2cC)\n  - [COMS W4995 - Applied Machine Learning - Spring 2020 - Columbia University](https://www.cs.columbia.edu/~amueller/comsw4995s20/schedule/)\n  - [Machine Learning for Engineers 2022](https://apmonitor.com/pds) ([YouTube](https://www.youtube.com/watch?v=Gh5rbBLh4JY&list=PLLBUgWXdTBDg1K1bu60lHypSzSP-WSBmx))\n  - [10-418 / 10-618 (Fall 2019) Machine Learning for Structured Data](https://www.youtube.com/playlist?list=PL4CxkUJbvNVihRKP4bXufvRLIWzeS-ieP)\n  - [ORIE 4741/5741: Learning with Big Messy Data - Cornell](https://people.orie.cornell.edu/mru8/orie4741/lectures.html)\n  - [Machine Learning in IoT](https://www.youtube.com/playlist?list=PLeZoXD_TLsLbW_ILvL9TlhBYdW8wJyON-)\n  - [Stanford CS229M: Machine Learning Theory - Fall 2021](https://www.youtube.com/playlist?list=PLoROMvodv4rP8nAmISxFINlGKSK4rbLKh)\n  - [Intro to Machine Learning and Statistical Pattern Classification - Prof Sebastian Raschka](https://www.youtube.com/playlist?list=PLTKMiZHVd_2KyGirGEvKlniaWeLOHhUF3)\n  - [CMU''s Multimodal Machine Learning course (11-777), Fall 2020](https://www.youtube.com/playlist?list=PL-Fhd_vrvisNup9YQs_TdLW7DQz-lda0G)\n  - [EE104: Introduction to Machine Learning - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rN_Uy7_wmS051_q1d6akXmK)\n  - [CPSC 330: Applied Machine Learning (2020) - UBC](https://www.youtube.com/playlist?list=PLWmXHcz_53Q2BXsWviGgEqdlSHmfsjSzC)\n  - [Machine Learning 2013 - Nando de Freitas, UBC](https://www.youtube.com/playlist?list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F6)\n  - [Machine Learning, 2014-2015, University of Oxford](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/)\n  - [10-702/36-702 - Statistical Machine Learning - Larry Wasserman, Spring 2016, CMU](https://www.stat.cmu.edu/~ryantibs/statml/) ([Spring 2015](https://www.youtube.com/playlist?list=PLjbUi5mgii6BWEUZf7He6nowWvGne_Y8r))\n  - [10-715 Advanced Introduction to Machine Learning - CMU](http://www.cs.cmu.edu/~bapoczos/Classes/ML10715_2015Fall/) ([YouTube](https://www.youtube.com/playlist?list=PL4DwY1suLMkcu-wytRDbvBNmx57CdQ2pJ))\n  - [CS 281B - Scalable Machine Learning, Alex Smola, UC Berkeley](http://alex.smola.org/teaching/berkeley2012/syllabus.html)\n  - [100 Days of Machine Learning - CampusX (Hindi)](https://www.youtube.com/playlist?list=PLKnIA16_Rmvbr7zKYQuBfsVkjoLcJgxHH)\n  - [CampusX Data Science Mentorship Program 2022-23 (Hindi)](https://www.youtube.com/playlist?list=PLKnIA16_RmvbAlyx4_rdtR66B7EHX5k3z)\n  - [Statistical Machine Learning - S2023 - Benyamin Ghojogh](https://www.youtube.com/playlist?list=PLPrxGIUWsqP2g7cpk0nFFt0c4aRcREq2s)\n  - [MIT 6.5940 EfficientML.ai Lecture, Fall 2023](https://www.youtube.com/playlist?list=PL80kAHvQbh-pT4lCkDT53zT8DKmhE0idB)\n  - [TinyML - Tiny Machine Learning at UPenn](https://www.youtube.com/playlist?list=PL7rtKJAz_mPe6kAbiH6Ucq02Vpa95qvBJ)\n  - [ECE 4760 (Digital Systems Design Using Microcontrollers) at Cornell for the Fall, 2022](https://www.youtube.com/playlist?list=PLDqMkB5cbBA5oDg8VXM110GKc-CmvUqEZ) ([Spring 2021](https://www.youtube.com/playlist?list=PLDqMkB5cbBA6FEJuj94gl-9vw8xcHu9Gp))\n  - [EfficientML.ai Lecture, Fall 2023, MIT 6.5940](https://www.youtube.com/playlist?list=PL80kAHvQbh-pT4lCkDT53zT8DKmhE0idB)\n  - [SFU CMPT 727 Statistical Machine Learning, by Maxwell Libbrecht](https://coursys.sfu.ca/2023sp-cmpt-727-g1/pages/) ([Spring 2023](https://www.youtube.com/playlist?list=PL_5SuHtr8fsrK9NqWWSL4YL8urMAHLsvU)) ([Spring 2022](https://www.youtube.com/playlist?list=PL_5SuHtr8fsp95AhIKeTHbpcVdhlhB9h6))\n  - [UC Berkeley CS 189 / 289A Introduction to Machine Learning fall 2023, by Jennifer Listgarten & Jitendra Malik](https://eecs189.org/)\n  - [UC Berkeley CS 189 Introduction to Machine Learning (CDSS offering) spring 2022, by Marvin Zhang](https://www.youtube.com/playlist?list=PLCuQm2FL98HTlRmlwMk2AuFEM9n1c06HE)\n  - [UC San Diego/edX DSE 220X Machine Learning Fundamentals, by Sanjoy Dasgupta](https://www.youtube.com/playlist?list=PLUPLKa8g2P75vrLVe6HKdgAwfuU89RfqB)\n  - [MIT 6.036 Introduction to Machine Learning spring 2019, by Leslie Kaelbling](https://www.youtube.com/playlist?list=PLQEw29vp6f1Ae9dp8vkKB8H6sF1PHvP5N)\n  - [LMU Munich Introduction to Machine Learning](https://slds-lmu.github.io/i2ml/)\n  - [CMU 15 388 / 15 688 Practical Data Science, by Zico Kolter](https://www.datasciencecourse.org/lectures/) ([Fall 2019](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22618ea253-ca45-4b14-9f1d-aab501543bd2%22)) ([Spring 2018](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22912b80a3-625d-405d-8905-a8620133666b%22))\n  - [UW Madison CS 320 Data Programming II spring 2021, by Tyler R. Caraza-Harter](https://tyler.caraza-harter.com/cs320/s21/schedule.html)\n  - [UC San Diego COGS9 Introduction to Data Science fall 2020, by Jason Fleischer](https://www.youtube.com/playlist?list=PLaaNbhBDEsoFarUB58v9s7pUVoyAahMeQ)\n  - [UCLA Stats 15 Introduction to Data Science fall 2022, by Miles Chen](https://www.youtube.com/playlist?list=PLIygTcviGPKCEzYDpJha6hP6ne-50sT1o)\n  - [UCLA Stats 21 Python and Other Technologies for Data Science spring 2024, by Miles Chen](https://www.youtube.com/playlist?list=PLIygTcviGPKAdsiprcbdk5HHPxS6PloWE) ([Spring 2021](https://www.youtube.com/playlist?list=PLKR7271tMEmgBPgu4LtjDhX3ywpTxda5g))\n  - [UCLA Stats C161/C261 Introduction to Pattern Recognition and Machine Learning winter 2024, by Arash Amini](https://www.youtube.com/playlist?list=PLN_qg0-2-0SxQ2vlXxlZVMKkt4gI1YYP8) ([Winter 2023](https://www.youtube.com/playlist?list=PLN_qg0-2-0SwLCXGUyM3FNSRwG6GNgONr))\n  - [UCLA Stats 231C Theories of Machine Learning spring 2022, by Arash Amini](https://www.youtube.com/playlist?list=PLN_qg0-2-0SxKyZLv_FotPDED5ET_rQmo)\n  - [MSU Machine Learning](https://www.youtube.com/watch?v=kMf0qDtQ_PM&list=PLZ-krWGO-UEyPHsZfOjYH03_TyIN2pPhl&pp=iAQB)\n  - [Data Science for Dynamical Systems, by Oliver Wallscheid & Sebastian Peitz](https://github.com/DS-4-DS/DS4DS_Course) ([YouTube](https://www.youtube.com/@DataScience4DynamicalSystems/playlists))\n  - [Cambridge Statistical Learning in Practice 2021, by Alberto J. Coca](https://www.youtube.com/playlist?list=PLn1JSlh3WT_b7sMBktkAgV9-cP052JFhb)\n  - [Data 8: The Foundations of Data Science - UC Berkeley](http://data8.org/) ([Spring 23](https://www.data8.org/sp23/)) ([Fall 22](https://www.data8.org/fa22/)) ([Spring 22](https://www.data8.org/sp22/)) ([Summer 17](http://data8.org/su17/))\n  - [Data 144: Foundations of Data Science spring 2021 - Vassar College](https://www.youtube.com/playlist?list=PLIygTcviGPKB9hHuywn56TraSMv2pRumr) ([Course materials](https://github.com/jwaterman/data144-materials-sp21))\n  - [CSE519 - Data Science Fall 2016 - Skiena, SBU](https://www.youtube.com/playlist?list=PLOtl7M3yp-DVBdLYatrltDJr56AKZ1qXo)\n  - [CS 109 Data Science, Harvard University](http://cs109.github.io/2015/pages/videos.html) ([YouTube](https://www.youtube.com/playlist?list=PLb4G5axmLqiuneCqlJD2bYFkBwHuOzKus))\n  - [6.0002 Introduction to Computational Thinking and Data Science - MIT OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/lecture-videos/)\n  - [Data 100: Principles and Techniques of Data Science - UC Berkeley](https://ds100.org/fa24/) ([Fall 24](https://www.youtube.com/playlist?list=PLIygTcviGPKBzDKR72ILypzPQZ_Cz6HcH)) ([Spring 24](https://ds100.org/sp24/)) ([Summer 19](https://www.youtube.com/playlist?list=PLPHXc20GewP8J56CisONS_mFZWZAfa7jR))\n  - [Data 102 - Spring 21- UC Berkeley](https://data102.org/sp21/#lecture-week-14) ([YouTube](https://www.youtube.com/playlist?list=PLIygTcviGPKAEbY32OXjmkQbD0uRhge9Y))\n  - [Distributed Data Analytics (WT 2017/18) - HPI University of Potsdam](https://www.tele-task.de/series/1179/)\n  - [Data Profiling and Data Cleansing (WS 2014/15) - HPI University of Potsdam](https://www.tele-task.de/series/1027/)\n  - [CS 229r - Algorithms for Big Data, Harvard University](http://people.seas.harvard.edu/~minilek/cs229r/fall15/lec.html) ([Youtube](https://www.youtube.com/playlist?list=PL2SOU6wwxB0v1kQTpqpuu5kEJo2i-iUyf))\n  - [Algorithms for Big Data - IIT Madras](https://nptel.ac.in/courses/106106142/)\n  - [Python Data Science with the TCLab](https://github.com/APMonitor/data_science) ([YouTube](https://www.youtube.com/watch?v=pAgW_bZVo88&list=PLLBUgWXdTBDg1Qgmwt4jKtVn9BWh5-zgy))\n  - [Foundations of Data Analysis (Fall 2020)- University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCqB1sx1eheVmLtp2EN7osYt)\n\n  \n- #### **Data Mining**\n  - [CSEP 546, Data Mining - Pedro Domingos, Sp 2016 - University of Washington](https://courses.cs.washington.edu/courses/csep546/16sp/) ([YouTube](https://www.youtube.com/playlist?list=PLTPQEx-31JXgtDaC6-3HxWcp7fq4N8YGr))\n  - [CS 5140/6140 - Data Mining, Spring 2020, University of Utah by Prof. Jeff Phillips](https://users.cs.utah.edu/~jeffp/teaching/cs5140-S20/cs5140.html) ([Youtube](https://www.youtube.com/playlist?list=PLbuogVdPnkCrEf65zrd3J1UG3LT6TcDlt))\n  - [CS 5140/6140 - Data Mining, Spring 2023, University of Utah by Prof. Ana MarasoviÄ‡](https://utah-data-mining-spring23.github.io/) ([Youtube](https://www.youtube.com/playlist?list=PLbuogVdPnkCrnLNqZPnTuG_s19TNDoad0))\n  - [CS 5955/6955 - Data Mining, University of Utah](http://www.cs.utah.edu/~jeffp/teaching/cs5955.html) ([YouTube](https://www.youtube.com/channel/UCcrlwW88yMcXujhGjSP2WBg/videos))\n  - [Statistics 202 - Statistical Aspects of Data Mining, Summer 2007 - Google](http://www.stats202.com/original_index.html) ([YouTube](https://www.youtube.com/playlist?list=PLFE776F2C513A744E))\n  - [MOOC - Text Mining and Analytics by ChengXiang Zhai](https://www.youtube.com/playlist?list=PLLssT5z_DsK8Xwnh_0bjN4KNT81bekvtt)\n  - [Information Retrieval SS 2014, iTunes - HPI](https://itunes.apple.com/us/itunes-u/information-retrieval-ss-2014/id874200291)\n  - [MOOC - Data Mining with Weka](https://www.youtube.com/playlist?list=PLm4W7_iX_v4NqPUjceOGd-OKNVO4c_cPD)\n  - [CS 290 DataMining Lectures](https://www.youtube.com/playlist?list=PLB4CCA346A5741C4C)\n  - [CS246 - Mining Massive Data Sets, Winter 2016, Stanford University](https://web.stanford.edu/class/cs246/) ([YouTube](https://www.youtube.com/channel/UC_Oao2FYkLAUlUVkBfze4jg/videos))\n  - [Information Retrieval - Spring 2018 - ETH Zurich](https://www.youtube.com/playlist?list=PLzn6LN6WhlN1ktkDvNurPSDwTQ_oGQisn)\n  - [Information Retrieval - WS 2022/23 - UniversitÃ¤t Freiburg](https://ad-wiki.informatik.uni-freiburg.de/teaching/InformationRetrievalWS2223)\n  - [CAP6673 - Data Mining and Machine Learning - FAU](http://www.cse.fau.edu/~taghi/classes/cap6673/)([Video lectures](https://vimeo.com/album/1505953))\n  - [CS 412 - Introduction to Data Mining - UIUC](https://www.youtube.com/playlist?list=PLIygTcviGPKDZi44-yuH2XH9UaHdaJkxs)\n  - [CS 512 - Data Mining Principles - UIUC](https://github.com/spacemanidol/CS512DM/tree/main/lectures) ([YouTube](https://www.youtube.com/playlist?list=PLIygTcviGPKABUzEm9v1PuKLlb0AQ9tdH))\n  \n- #### **Probabilistic Graphical Modeling**\n  - [CS 6190 - Probabilistic Modeling, Spring 2016, University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCpvxdF-Gy3gwaBObx7AnQut)\n  - [10-708 - Probabilistic Graphical Models, Carnegie Mellon University](https://www.cs.cmu.edu/~epxing/Class/10708-20/lectures.html)\n  - [Probabilistic Graphical Models, Daphne Koller, Stanford University](http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=ProbabilisticGraphicalModels)\n  - [Probabilistic Graphical Models, Spring 2018 - Notre Dame](https://www.youtube.com/playlist?list=PLd-PuDzW85AcV4bgdu7wHPL37hm60W4RM)\n  \n- #### **Deep Learning**\n  - [Full Stack Deep Learning - Course 2022](https://www.youtube.com/watch?v=-Iob-FW5jVM&list=PL1T8fO7ArWleMMI8KPJ_5D5XSlovTW_Ur)\n  - [Full Stack Deep Learning - Course 2021](https://www.youtube.com/watch?v=fGxWfEuUu0w&list=PL1T8fO7ArWlcWg04OgNiJy91PywMKT2lv)\n  - [NYU Deep Learning Spring 2020](https://www.youtube.com/playlist?list=PLLHTzKZzVU9eaEyErdV26ikyolxOsz6mq)\n  - [NYU Deep Learning Spring 2021](https://www.youtube.com/playlist?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI)\n  - [6.S191: Introduction to Deep Learning - MIT](http://introtodeeplearning.com/)\n  - [Intro to Deep Learning and Generative Models Course - Prof Sebastian Raschka](https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51)\n  - [Deep Learning CMU](https://www.youtube.com/channel/UC8hYZGEkI2dDO8scT8C5UQA/videos)\n  - [CS231n Deep Learning for Computer Vision - Stanford University](https://cs231n.stanford.edu/schedule.html) ([Spring 2025](https://www.youtube.com/playlist?list=PLoROMvodv4rOmsNzYBMe0gJY2XS8AQg16)) ([Winter 2016 Andrej Karpathy](https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC))\n  - [Deep Learning: CS 182 Spring 2021](https://www.youtube.com/playlist?list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A)\n  - [10-414/714: Deep Learning Systems - CMU](https://dlsyscourse.org/lectures/) ([Youtube](https://www.youtube.com/@deeplearningsystemscourse1116/videos))\n  - [11-785: Introduction to Deep Learning - CMU](https://deeplearning.cs.cmu.edu/S24/index.html) ([Lectures - YouTube-2024](https://www.youtube.com/playlist?list=PLp-0K3kfddPxUJzAW0KxNNjGiK_hISFas), [Recitations - YouTube-2024](https://www.youtube.com/playlist?list=PLp-0K3kfddPzNnco9QQAoTx_sVhZgHK-n))\n  - [Part 1: Practical Deep Learning for Coders, v3 - fast.ai](https://course.fast.ai/)\n  - [Part 2: Deep Learning from the Foundations - fast.ai](https://course19.fast.ai/part2)\n  - [Deep learning at Oxford 2015 - Nando de Freitas](https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu)\n  - [Self-Driving Cars â€” Andreas Geiger, 2021/22](https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/lectures/self-driving-cars/) ([YouTube](https://www.youtube.com/watch?v=wAaSJUAKPuY&list=PL05umP7R6ij321zzKXK6XCQXAaaYjQbzr))\n  - [6.S094: Deep Learning for Self-Driving Cars - MIT](https://www.youtube.com/playlist?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf)\n  - [CS294-129 Designing, Visualizing and Understanding Deep Neural Networks](https://bcourses.berkeley.edu/courses/1453965/pages/cs294-129-designing-visualizing-and-understanding-deep-neural-networks) ([YouTube](https://www.youtube.com/playlist?list=PLkFD6_40KJIxopmdJF_CLNqG3QuDFHQUm))\n  - [CS230: Deep Learning - Autumn 2018 - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb)\n  - [STAT-157 Deep Learning 2019 - UC Berkeley](https://www.youtube.com/playlist?list=PLZSO_6-bSqHQHBCoGaObUljoXAyyqhpFW)\n  - [Deep Learning, Stanford University](http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=DeepLearning)\n  - [MOOC - Neural Networks for Machine Learning, Geoffrey Hinton 2016 - Coursera](https://www.youtube.com/playlist?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9)\n  - [Stat 946 Deep Learning - University of Waterloo](https://www.youtube.com/playlist?list=PLehuLRPyt1Hyi78UOkMPWCGRxGcA9NVOE)\n  - [EECS 298 Theory of Computational Neural Networks and Machine Learning (Fall 2020) - UC Irvine](https://grandcentral.eee.uci.edu/syllabus/download/F-2020/17815) ([YouTube](https://www.youtube.com/playlist?list=PLIygTcviGPKBAh1HWXPOI3Rw59WdRlJXu))\n  - [ECE 1508 Applied Deep Learning - University of Toronto](https://www.bereyhi.com/deep-learning) ([Winter 2025](https://www.youtube.com/playlist?list=PLcFgNUo9s_AgsMOnniTMIWmLpjj9vZ1Wm)) ([Fall 2024](https://www.youtube.com/playlist?list=PLcFgNUo9s_Ajz1l4rBDIApwdcEKgzoMko))\n  - [Neural networks class - UniversitÃ© de Sherbrooke](http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html) ([YouTube](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH))\n  - [DLCV - Deep Learning for Computer Vision - UPC Barcelona](https://www.youtube.com/playlist?list=PL-5eMc3HQTBavDoZpFcX-bff5WgQqSLzR)\n  - [DLAI - Deep Learning for Artificial Intelligence @ UPC Barcelona](https://www.youtube.com/playlist?list=PL-5eMc3HQTBagIUjKefjcTbnXC0wXC_vd)\n  - [Neural Networks and Applications - IIT Kharagpur](https://nptel.ac.in/courses/117105084/)\n  - [UVA DEEP LEARNING COURSE](http://uvadlc.github.io/#lecture)\n  - [Deep Learning - Winter 2020-21 - TÃ¼bingen Machine Learning](https://www.youtube.com/playlist?list=PL05umP7R6ij3NTWIdtMbfvX7Z-4WEXRqD)\n  - [Geometric Deep Learning - AMMI](https://www.youtube.com/playlist?list=PLn2-dEmQeTfQ8YVuHBOvAhUlnIPYxkeu3)\n  - [Math for Deep Learning â€” Andreas Geiger](https://www.youtube.com/playlist?list=PL05umP7R6ij0bo4UtMdzEJ6TiLOqj4ZCm)\n  - [Applied Deep Learning 2022 - TU Wien](https://www.youtube.com/playlist?list=PLNsFwZQ_pkE_QaTwYxoTmmRJHtMXyIAU6)\n  - [Neural Networks: Zero to Hero - Andrej Karpathy](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\n  - [CIS 522 - Deep Learning - U Penn](https://www.youtube.com/@cis522-deeplearning8/playlists)\n  - [UVA DEEP LEARNING COURSE](http://uvadlc.github.io/#lecture)\n  - [Deep Learning (Fall 2020) - FAU](https://www.fau.tv/course/id/1600) ([Spring 2020](https://www.fau.tv/course/id/925)) ([Fall 2019](https://www.fau.tv/course/id/849)) ([Spring 2019](https://www.fau.tv/course/id/758)) ([Fall 2018](https://www.fau.tv/course/id/701)) ([Spring 2018](https://www.fau.tv/course/id/662))\n  - [Deep Learning (Fall 2020) - Georgia Tech](https://www.youtube.com/playlist?list=PL-fZD610i7yB7gDnPDpFcKpHI9X8z3OQ7)\n  - [Mathematics of Deep Learning (2021) - FAU](https://www.fau.tv/course/id/878)\n  - [CS7015 - Deep Learning - Prof. Mitesh M. Khapra - IIT Madras](https://www.youtube.com/playlist?list=PLyqSpQzTE6M9gCgajvQbc68Hk_JKGBAYT)\n  - [ETH ZÃ¼rich | Deep Learning in Scientific Computing 2023](https://www.youtube.com/playlist?list=PLJkYEExhe7rYY5HjpIJbgo-tDZ3bIAqAm)\n  - [Applied Deep Learning Maziar Raissi](https://www.youtube.com/playlist?list=PLoEMreTa9CNmuxQeIKWaz7AVFd_ZeAcy4)\n  - [UC Berkeley CS 182 / 282a Deep Learning spring 2023, by Anant Sahai](https://www.youtube.com/playlist?list=PLIygTcviGPKAaj_UAJcazYN4964xZ7Lt1)\n  - [Foundations of Deep Learning - UMD](https://www.youtube.com/playlist?list=PLHgjs9ncvHi80UCSlSvQe-TK_uOyDv_Jf)\n  - [TUM IN2346 Introduction to Deep Learning Fall 2024, by Daniel Cremers](https://cvg.cit.tum.de/teaching/ws2024/i2dl) ([Summer 2023](https://www.youtube.com/playlist?list=PLQ8Y4kIIbzy_pGm2QAwF625E6nmcRu2sU))\n  - [UT Austin - Advances in Deep Learning](https://ut.philkr.net/advances_in_deeplearning/)\n  - [HKU - Data 8014 Principles of Deep Representation Learning Fall 2025, by Yi Ma](https://ma-lab-berkeley.github.io/deep-representation-learning-book/community.html)\n  \n- #### **Reinforcement Learning**\n  - [CS234: Reinforcement Learning - Spring 2024 - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rN4wG6Nk6sNpTEbuOSosZdX) ([Winter 2019](https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u))\n  - [CSE 542: Reinforcement Learning - Spring 2024 - University of Washington](https://courses.cs.washington.edu/courses/cse542/24sp/schedule/)\n  - [CSE 579: Reinforcement Learning - Autumn 2024 - University of Washington](https://courses.cs.washington.edu/courses/cse579/24au/schedule/)\n  - [CSC 2547: Introduction to Reinforcement Learning - Spring 2021 - University of Toronto](https://amfarahmand.github.io/IntroRL/) ([YouTube](https://www.youtube.com/playlist?list=PLCveiXxL2xNbiDq51a8iJwPRq2aO0ykrq))\n  - [Introduction to reinforcement learning - UCL](https://www.youtube.com/playlist?list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)\n  - [Reinforcement Learning - IIT Madras](https://www.youtube.com/playlist?list=PLyqSpQzTE6M_FwzHFAyf4LSkz_IjMyjD9) ([TA - Manav Mishra](https://www.youtube.com/playlist?list=PLpKrAXMumEsjAR1Ybb0qbTKGYd9RY0vxa), [TA - Prabhleen Kukreja](https://www.youtube.com/playlist?list=PLTt_oMmEiDJhBPjy_aedcoeIY-IsaDBOn), [TA - Sandarbh Yadav ](https://www.youtube.com/playlist?list=PLoo_WPzXEM1ShdfOxv-adi3JdhRX4rA7F), [TA - Avik Kar](https://www.youtube.com/playlist?list=PLz2x4RAIbeXkTJFEipkD_ds3z0qx8_5D7))\n  - [Special topics in ML (Reinforcement Learning) IIT madras](https://www.youtube.com/playlist?list=PLZ2ps__7DhBbDiVplM2I9q2XNso1Qfj62)\n  - [CS885 Reinforcement Learning - Spring 2018 - University of Waterloo](https://www.youtube.com/playlist?list=PLdAoL1zKcqTXFJniO3Tqqn6xMBBL07EDc)\n  - [CS 285 - Deep Reinforcement Learning- UC Berkeley](https://www.youtube.com/playlist?list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A)\n  - [CS 294 112 - Reinforcement Learning](https://www.youtube.com/playlist?list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37)\n  - [NUS CS 6101 - Deep Reinforcement Learning](https://www.youtube.com/playlist?list=PLllwxvcS7ca5wOmRLKm6ri-OaC0INYehv)\n  - [ECE 8851: Reinforcement Learning](https://www.youtube.com/playlist?list=PL_Nk3YvgORJs1tCLQnlnSRsOJArj_cP9u)\n  - [CS294-112, Deep Reinforcement Learning Sp17](http://rll.berkeley.edu/deeprlcourse/) ([YouTube](https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX))\n  - [UCL Course 2015 on Reinforcement Learning by David Silver from DeepMind](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) ([YouTube](https://www.youtube.com/watch?v=2pWv7GOvuf0))\n  - [Deep RL Bootcamp - Berkeley Aug 2017](https://sites.google.com/view/deep-rl-bootcamp/lectures)\n  - [Reinforcement Learning - IIT Madras](https://www.youtube.com/playlist?list=PLyqSpQzTE6M_FwzHFAyf4LSkz_IjMyjD9)\n  - [Reinforcement Learning Course at KTH (FDD3359 - 2022)](https://www.youtube.com/playlist?list=PL21JFJEtbq0JLNo53UIkbIwkc2njCVUUR)\n  - [Reinforcement Learning Course at ASU, Spring 2022](https://www.youtube.com/playlist?list=PLmH30BG15SIoXhxLldoio0BhsIY84YMDj)\n  - [CS 4789/5789: Introduction to Reinforcement Learning - Cornell](https://www.youtube.com/playlist?list=PLQVNhPb8ajtCjWSKUvKU8cX5lueYP9s3X)\n  - [S20/IE613 - Online (Machine) Learning/ Bandit Algorithms](https://www.youtube.com/playlist?list=PLDREIwGwrHBdiBm1q0cVJLZn4Cn6Hig2s)\n  - [Reinforcement Learning - Fall 2021 chandar-lab](https://www.youtube.com/playlist?list=PLImtCgowF_ES_JdF_UcM60EXTcGZg67Ua)\n  - [CMU 10 703 Deep Reinforcement Learning & Control fall 2022, by Katerina Fragkiadaki](https://scs.hosted.panopto.com/Panopto/Pages/Sessions/List.aspx#folderID=%22ee5794a2-cb54-4edc-836b-aefc01023243%22)\n  - [ECE524 Foundations of Reinforcement Learning at Princeton University, Spring 2024](https://www.youtube.com/playlist?list=PLYXvCE1En13epbogBmgafC_Yyyk9oQogl)\n  - [REINFORCEMENT LEARNING AND OPTIMAL CONTROL - Dimitri P. Bertsekas, ASU](https://web.mit.edu/dimitrib/www/RLbook.html)\n  - [CMU 16 745 Optimal Control and Reinforcement Learning spring by Zac Manchester](https://www.youtube.com/@roboticexplorationlab3724/playlists)\n  - [CMU 16 899 Adaptive Control and Reinforcement Learning fall 2020, by  Changliu Liu](https://www.youtube.com/playlist?list=PLZL5VXraKdz-0zByoPNzNTqSirR4veU8z)\n  - [Jadavpur University, 2025: Introduction to Reinforcement Learning](https://www.youtube.com/playlist?list=PLcNLn_ApooUzGJW60RcD2HRrL7sm0HG-w)\n  - [EE675 (2024) Introduction to Reinforcement Learning Course | IIT Kanpur](https://www.youtube.com/playlist?list=PLZAmMLcSnKRICBNyjraAhQdtdJFFgyRL5)\n  - [Reinforcement Learning Course by FrÃ©dÃ©ric Godin - Concordia University](https://www.youtube.com/playlist?list=PLFskzTP727yc5eXXm09Xst7bIk5fi7mD6)\n  - [CS 285: Deep RL, 2023](https://www.youtube.com/playlist?list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps)\n  - [Mathematical Foundations of Reinforcement Learning - WINDY Lab](https://www.youtube.com/playlist?list=PLEhdbSEZZbDaFWPX4gehhwB9vJZJ1DNm8)\n  - [Reinforcement Learning (HMC CS 181V)â€”Spring, 2020 - Neil Rhodes](https://www.youtube.com/playlist?list=PL2Yggtk_pK69evEzfwQHm9ASOCbXPlXPS)\n  - [Reinforcement Learning Course: Lectures (Summer 2023) by Paderborn University](https://www.youtube.com/playlist?list=PL4GzQQuIDBGv-IFxRSgydCR7OrOM_xKqN)\n  - [CS292F (Spring 2021) Statistical Foundation of Reinforcement Learning - UCSD](https://cseweb.ucsd.edu/~yuxiangw/classes/RLCourse-2021Spring/)\n  - [Algorithmic Foundations of Interactive Learning - CMU](https://interactive-learning-algos.github.io/)\n  \n- #### **Advanced Machine Learning**\n  - [Advanced Machine Learning, 2021-2022, Sem I - by Prof. Madhavan Mukund, CMI](https://www.cmi.ac.in/~madhavan/courses/aml2021)\n  - [18.409 Algorithmic Aspects of Machine Learning Spring 2015 - MIT](https://www.youtube.com/playlist?list=PLB3sDpSRdrOvI1hYXNsa6Lety7K8FhPpx)\n  - [CS 330 - Deep Multi-Task and Meta Learning - Fall 2019 - Stanford University](https://cs330.stanford.edu/) ([Youtube](https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5))\n  - [Stanford CS330: Deep Multi-Task and Meta Learning I Autumn 2022](https://www.youtube.com/playlist?list=PLoROMvodv4rNjRoawgt72BBNwL2V7doGI)\n  - [ES 661 (2023): Probabilistic Machine Learning - IIT Gandhinagar](https://www.youtube.com/playlist?list=PLftoLyLEwECBEJyfRBJoSBd0UaTjEcs3I)\n  - [Information Retrieval in High Dimensional Data](https://www.youtube.com/playlist?list=PLaE1lKCe0jH3ePp9wCU1ygTquVOXY-UYv)\n  - [Trustworthy Machine Learning - Winter Semester 2023-2024, University of TÃ¼bingen](https://scalabletrustworthyai.github.io/courses/tml_winter_2324/)\n  - [Trustworthy Machine Learning - Winter Semester 2024-2025, University of TÃ¼bingen](https://scalabletrustworthyai.github.io/courses/tml_winter_2425/)\n  - [ETH ZÃ¼rich Advanced Machine Learning fall 2019, by Joachim M. Buhmann](https://video.ethz.ch/lectures/d-infk/2019/autumn/252-0535-00L.html)\n  - [CS 159 Advanced Topics in Machine Learning, Spring 2021 - Caltech](https://1five9.github.io/)\n  - [CS 229br Advanced Topics in the theory of machine learning, Spring 2021 - Harvard](https://boazbk.github.io/mltheoryseminar/cs229br.html)\n  \n  \n- #### **Natural Language Processing**\n  - [CS 224N -Natural Language Processing with Deep Learning - Stanford University](http://web.stanford.edu/class/cs224n/) ([Lectures -  Winter 2019](https://youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)) ([Lectures -  Winter 2021](https://youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)) ([Lectures - Spring 2024](https://www.youtube.com/playlist?list=PLoROMvodv4rOaMFbaqxPDoLWjDaRAdP9D))\n  - [CS 224N - Natural Language Processing, Stanford University](https://web.stanford.edu/~jurafsky/NLPCourseraSlides.html) ([Lecture videos](https://academictorrents.com/details/d2c8f8f1651740520b7dfab23438d89bc8c0c0ab))\n  - [Stanford XCS224U: Natural Language Understanding I Spring 2023](https://www.youtube.com/playlist?list=PLoROMvodv4rOwvldxftJTmoR3kRcWkJBp)\n  - [CS388: Natural Language Processing - UT Austin](https://www.cs.utexas.edu/~gdurrett/courses/online-course/materials.html)\n  - [CS 124 - From Languages to Information - Stanford University](https://www.youtube.com/channel/UC_48v322owNVtORXuMeRmpA/playlists?view=50&sort=dd&shelf_id=2)\n  - [CS 6340/5340 - Natural Language Processing - University of Utah - Spring 2024](https://utah-intro-nlp.github.io/) ([Youtube](https://www.youtube.com/playlist?list=PLbuogVdPnkCrPZ4Vc-GRnk730SLhC1L43))\n  - [CSE 447/517 - Natural Language Processing - University of Washington - Winter 2024](https://safe-fernleaf-26d.notion.site/Winter-24-CSE-447-517-Natural-Language-Processing-4142333a001143d2be5ecff1a535c4ab)\n  - [Neural Networks: Zero to Hero - Andrej Karpathy](https://karpathy.ai/zero-to-hero.html)\n  - [fast.ai Code-First Intro to Natural Language Processing](https://www.youtube.com/playlist?list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9) ([Github](https://github.com/fastai/course-nlp))\n  - [MOOC - Natural Language Processing - Coursera, University of Michigan](https://www.youtube.com/playlist?list=PLLssT5z_DsK8BdawOVCCaTCO99Ya58ryR)\n  - [Natural Language Processing at UT Austin (Greg Durrett)](https://www.youtube.com/playlist?list=PLofp2YXfp7Tbk88uH4jejfXPd2OpWuSLq)\n  - [CS224U: Natural Language Understanding - Spring 2019 - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rObpMCir6rNNUlFAn56Js20)\n  - [Deep Learning for Natural Language Processing, 2017 - Oxford University](https://github.com/oxford-cs-deepnlp-2017/lectures)\n  - [Natural Language Processing - IIT Bombay](https://nptel.ac.in/courses/106101007/)\n  - [CMU Advanced NLP Fall 2024](https://phontron.com/class/anlp-fall2024/schedule/) ([Lectures - Fall 2024](https://www.youtube.com/playlist?list=PL8PYTP1V4I8D4BeyjwWczukWq9d8PNyZp)) ([Lectures - Fall 2021](https://www.youtube.com/playlist?list=PL8PYTP1V4I8AYSXn_GKVgwXVluCT9chJ6))\n  - [CMU Neural Nets for NLP 2021](https://www.youtube.com/playlist?list=PL8PYTP1V4I8AkaHEJ7lOOrlex-pcxS-XV)\n  - [Natural Language Processing - Michael Collins - Columbia University](https://www.youtube.com/playlist?list=PLA212ij5XG8OTDRl8IWFiJgHR9Ve2k9pv)\n  - [CMU CS11-711 - Advanced Natural Language Processing](https://cmu-l3.github.io/anlp-spring2025/) ([Lectures - Spring 2025](https://www.youtube.com/playlist?list=PLqC25OT8ZpD3WxQ0FwWMGPS_BcWdcKyZy))\n  - [CMU CS11-737 - Multilingual Natural Language Processing](https://www.youtube.com/playlist?list=PL8PYTP1V4I8CHhppU6n1Q9-04m96D9gt5)\n  - [UMass CS685: Advanced Natural Language Processing (Spring 2022)](https://www.youtube.com/playlist?list=PLWnsVgP6CzadI4-FT2Po4wsEK7MHCIQ-d)\n  - [Natural Language Processing (CMSC 470)](https://www.youtube.com/playlist?list=PLwrUPjGidcJ4UkSoi7_rmn-1kcedLqgdL)\n  - [Stanford CS25 - Transformers United 2023](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)\n  - [Natural Language Processing (IN2361) - TUM](https://live.rbg.tum.de/?year=2019&term=W&slug=nlp&view=3)\n  - [Natural Language Processing (Spring 2024) - University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCrPZ4Vc-GRnk730SLhC1L43)\n  - [Multilingual NLP 2020 - CMU](https://www.youtube.com/playlist?list=PL8PYTP1V4I8CHhppU6n1Q9-04m96D9gt5)\n  - [Speech Technology - IIT Madras](https://www.youtube.com/playlist?list=PLZ2ps__7DhBaI_g3_V-CFrgFIf-0Yksiv)\n  \n- #### **Generative AI and LLMs**\n  - [Stanford CS236: Deep Generative Models I 2023 I Stefano Ermon](https://www.youtube.com/playlist?list=PLoROMvodv4rPOWA-omMM6STXaWW4FvJT8)\n  - [CS 6785 - Deep Generative Models - Cornell Tech, Spring 2023)](https://www.youtube.com/playlist?list=PL2UML_KCiC0UPzjW9BjO-IW6dqliu9O4B)\n  - [MIT 6.S184 Flow Matching and Diffusion Models, 2025](https://diffusion.csail.mit.edu)\n  - [Course on Diffusion Models for Generative AI - UT Austin](https://www.youtube.com/playlist?list=PL8lIiiIWuabLxhJreBRZwNW-d02dJwbMb)\n  - [Stanford CS336 Language Modeling from Scratch I 2025 - Stanford](https://www.youtube.com/playlist?list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_)\n  - [Stanford CME295 Transformers & LLMs - Autumn 2025 - Stanford](https://www.youtube.com/playlist?list=PLoROMvodv4rObv1FMizXqumgVVdzX4_05)\n  - [Introduction to large language models - IIT Madras](https://www.youtube.com/playlist?list=PLZ2ps__7DhBbaMNZoyW2Hizl8DG6ikkjo)\n  - [Build a Large Language Model (From Scratch) by Sebastian Raschka](https://www.youtube.com/playlist?list=PLTKMiZHVd_2IIEsoJrWACkIxLRdfMlw11)\n  - [Reinforcement Learning of Large Language Models - UCLA](https://www.youtube.com/playlist?list=PLir0BWtR5vRp5dqaouyMU-oTSzaU5LK9r)\n  - [WING NUS CS6101 Large Language Models (T2310)](https://www.youtube.com/playlist?list=PLzIZxnJJT7ORSBnYrXJMYBVnYeLryJtl7)\n  - [CS 886: Recent Advances on Foundation Models Winter 2024 - University of Waterloo](https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/)\n  - [UC Berkeley CS 194/294-196 Large Language Model Agents Fall 2024, by Dawn Song & Xinyun Chen](https://rdi.berkeley.edu/llm-agents/f24) ([YouTube playlist](https://www.youtube.com/playlist?list=PLS01nW3RtgopsNLeM936V4TNSsvvVglLc))\n  - [UC Berkeley CS 194/294-267 Understanding Large Language Models Foundations and Safety spring 2024, by Dawn Song & Dan Hendrycks](https://www.youtube.com/playlist?list=PLJ66BAXN6D8H_gRQJGjmbnS5qCWoxJNfe)\n  - [Introduction to Large Language Models (LLMs), IIT Delhi](https://www.youtube.com/playlist?list=PLqGkIjcOyrGnjyBHl4GE2S9kX47X96FH-)\n  \n- #### **Computer Vision**\n  - [CS 231n - Convolutional Neural Networks for Visual Recognition, Stanford University](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)\n  - [CS 198-126: Modern Computer Vision Fall 2022 (UC Berkeley)](https://www.youtube.com/playlist?list=PLzWRmD0Vi2KVsrCqA4VnztE4t71KnTnP5)\n  - [Machine Learning for Robotics and Computer Vision, WS 2013/2014 - TU MÃ¼nchen](https://vision.in.tum.de/teaching/ws2013/ml_ws13) ([YouTube](https://www.youtube.com/playlist?list=PLTBdjV_4f-EIiongKlS9OKrBEp8QR47Wl))\n  - [COGSCI 1 - Intro to Cognitive Science Summer 2022 - UC Berkeley](https://www.youtube.com/playlist?list=PLaMjLYzDGxvz1oT5gpFiY6rJZnlJ-1Xu-)\n  - [Informatics 1 - Cognitive Science 2015/16- University of Edinburgh](http://groups.inf.ed.ac.uk/vision/VIDEO/2015/inf1cs.htm)\n  - [Informatics 2A - Processing Formal and Natural Languages 2016-17 - University of Edinburgh](http://www.inf.ed.ac.uk/teaching/courses/inf2a/schedule.html)\n  - [NOC:Deep Learning For Visual Computing - IIT Kharagpur](https://nptel.ac.in/courses/108/105/108105103/)\n  - [Extreme Classification ](https://www.youtube.com/playlist?list=PLXtAHOcKKDTk43wjXud9GQS-l-QA5DQxH)\n  - [EECS 498/598 - Deep Learning for Computer Vision - University of Michigan - Fall 2019](https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2019/) ([Youtube](https://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r))\n  - [Computer Vision - FAU Spring 2021](https://www.fau.tv/course/id/2306) ([Spring 2018](https://www.fau.tv/course/id/734))\n  - [CAP5415 Computer Vision - UCF Fall 2023](https://www.youtube.com/playlist?list=PLd3hlSJsX_InWyCQtwqQ7y6KnwhxNCgRf)\n  - [CAP6412 Advanced Computer Vision - UCF Spring 2024](https://www.crcv.ucf.edu/courses/cap6412-spring-2024/schedule/) ([Youtube](https://www.youtube.com/playlist?list=PLd3hlSJsX_IlSr0ua4v8WQezazAMXEJE4))\n  - [Advanced Deep Learning for Computer vision (ADL4CV) (IN2364) - TU Munich](https://dvl.in.tum.de/teaching/adl4cv-ss20/) ([Youtube](https://www.youtube.com/playlist?list=PLog3nOPCjKBnjhuHMIXu4ISE4Z4f2jm39))\n  - [Computer Vision III: Detection, Segmentation and Tracking (CV3DST) (IN2375) - TU Munich](https://www.youtube.com/playlist?list=PLog3nOPCjKBkamdw8F6Hw_4YbRiDRb2rb)\n  \n- #### **Time Series Analysis**\n  - [02417 Time Series Analysis](https://www.youtube.com/playlist?list=PLtiTxpFJ4k6TZ0g496fVcQpt_-XJRNkbi)\n  - [Applied Time Series Analysis](https://www.youtube.com/playlist?list=PLl0FT6O_WWDBm-4W-eoK34omYmEMseQDX)\n  \n- #### **Optimization**\n  - [Optimisation for Machine Learning: Theory and Implementation (Hindi) - IIT](https://www.youtube.com/playlist?list=PLyqSpQzTE6M-pmLzCoMu_ANU6atEFyyJl)\n  - [Rochester DSCC 435 Optimization for Machine Learning fall 2023, by Jiaming Liang](https://www.youtube.com/playlist?list=PLuJY91x7h5orCtyh6mChurVQ2ZZef9qPF)\n  - [Princeton ELE539/COS512 Optimization for Machine Learning spring 2021, by Chi Jin](https://sites.google.com/view/cjin/teaching/ece539cos512-2021-ver)\n  - [UT Dallas CS 7301 Advanced Topics in Optimization for Machine Learning spring 2021, by Rishabh Iyer](https://github.com/rishabhk108/AdvancedOptML) ([YouTube](https://www.youtube.com/playlist?list=PLGod0_zT9w92_evaYrf3-rE67AmgPJoUU))\n  - [Convex Analysis, Summer 2021 - TU Braunschweig](https://www.tu-braunschweig.de/index.php?eID=dumpFile&t=f&f=128341&token=3c40ee32c6c029df85f7e552522f4a87470e3401) ([YouTube](https://www.youtube.com/playlist?list=PLPomPKAI5ZlJBThiwc3bya7qngzw9-0QD))\n  - [EE364a: Convex Optimization I - Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rMJqxxviPa4AmDClvcbHi6h)\n  - [10-725 Convex Optimization, Spring 2015 - CMU](http://www.stat.cmu.edu/~ryantibs/convexopt-S15/)\n  - [10-725 Convex Optimization: Fall 2016 - CMU](http://www.stat.cmu.edu/~ryantibs/convexopt/)\n  - [10-725 Optimization Fall 2012 - CMU](http://www.cs.cmu.edu/~ggordon/10725-F12/schedule.html)\n  - [10-801 Advanced Optimization and Randomized Methods - CMU](http://www.cs.cmu.edu/~suvrit/teach/aopt.html) ([YouTube](https://www.youtube.com/playlist?list=PLjTcdlvIS6cjdA8WVXNIk56X_SjICxt0d))\n  - [AM 207 - Stochastic Methods for Data Analysis, Inference and Optimization, Harvard University](http://am207.github.io/2016/index.html)\n  - [MIT 6.S098 Applied Convex Optimization IAP 2022, by Alexandre Amice, Benoit Legat](https://alexandreamice.github.io/teaching/convex_optimization/) ([YouTube](https://www.youtube.com/playlist?list=PL5SG6ajT9NZKxdvM1jQOLXmeKO7MfyLxR))\n  - [University of Twente Discrete Optimization, by Marc Uetz](https://marcuetz.personalweb.utwente.nl/do/) ([Fall 2020](https://www.youtube.com/playlist?list=PLIygTcviGPKBUY9e1MCruRr5-I24LNm4g))\n  - [UC Davis MAT 168 Optimization winter 2024, by Matthias KÃ¶ppe](https://video.ucdavis.edu/channel/MAT+168+Optimization+%28Matthias+K%C3%B6ppe%3B+Winter+2024%29/329241352)\n  - [Purdue University CHE 597 Computational Optimization spring 2025, by Can Li](https://canli1.github.io/courses)\n  - [UCSD CS292F Convex Optimization Spring 2020, by Yu-Xiang Wang](https://cseweb.ucsd.edu/~yuxiangw/classes/CS292F-2020Spring/) ([Youtube](https://www.youtube.com/playlist?list=PLTN4aNO9NiB5VxYILKPBXoy9g1tUqmnBx))\n  - [UIUC ECE 490 Introduction to Optimization fall 2020, by Venugopal V. Veeravalli](https://courses.grainger.illinois.edu/ece490/fa2020/) ([YouTube](https://www.youtube.com/playlist?list=PLIygTcviGPKC5wSXtE6s2qdSYutRH1AUU))\n  - [University of Wisconsin-Madison CS/ECE/ISyE 524 Introduction to Optimization spring 2017-18, by Laurent Lessard](https://laurentlessard.com/teaching/524-intro-to-optimization/)\n  - [University of Wisconsin-Madison ISyE/Math/CS/Stat 525 Linear Optimization fall 2021, by Alberto Del Pia](https://www.youtube.com/playlist?list=PLeO_PhASIA0Ot69TqANAnNxoykHGOQp2Y)\n  - [University of Wisconsin-Madison ISyE/Math/CS 728 Integer Optimization (second part of the course) spring 2020](https://www.youtube.com/playlist?list=PLeO_PhASIA0NlDNF9y-SsgVEYcvAMj2CY)\n  - [Columbia IEOR E4007 Optimization Models and Methods 2005, by Garud Iyengar](https://www.youtube.com/playlist?list=PLIygTcviGPKCNQ2xHRwrLOxkEWv9OfGiF)\n \n- #### **Unsupervised Learning**\n  - [CS294 Deep Unsupervised Learning Spring 2024](https://sites.google.com/view/berkeley-cs294-158-sp24/home)\n  - [Deep Unsupervised Learning -- Berkeley Spring 2020](https://www.youtube.com/playlist?list=PLwRJQ4m4UJjPiJP3691u-qWwPGVKzSlNP)\n  - [CS294-158 Deep Unsupervised Learning SP19](https://www.youtube.com/channel/UCf4SX8kAZM_oGcZjMREsU9w/videos)\n  - [UC San Diego COGS 118A Supervised Machine Learning fall 2020, by Jason Fleischer](https://www.youtube.com/playlist?list=PLaaNbhBDEsoF_ad5N2mlOsvB-RwSTYUjQ)\n  - [UC San Diego COGS 118B Unsupervised Machine Learning winter 2024, by Jason Fleischer](https://www.youtube.com/playlist?list=PLaaNbhBDEsoEU9RRbwCqJWFAZH2LjNwBN)\n  - [UIUC STAT 437 Unsupervised Learning spring 2024, by Tori Ellison](https://www.youtube.com/playlist?list=PLIygTcviGPKB133Vh7zxsxFoblyfS4P5Y)\n  - [Johns Hopkins Unsupervised Learning spring 2017, by Rene Vidal](https://www.youtube.com/playlist?list=PLaBAmmD3yH4Nta9Y6g9hOV4dcnpTzeW4q)\n  - [Unsupervised Learning (STAT 841), Winter 2017](https://www.youtube.com/playlist?list=PLehuLRPyt1HzQoXEhtNuYTmd0aNQvtyAK)\n  \n- #### **Misc Machine Learning Topics**\n  - [Quantum Machine Learning | 2021 Qiskit Global Summer School](https://www.youtube.com/playlist?list=PLOFEBzvs-VvqJwybFxkTiDzhf5E11p8BI)\n  - [CS 6955 - Clustering, Spring 2015, University of Utah](https://www.youtube.com/playlist?list=PLbuogVdPnkCpRvi-qSMCdOwyn4UYoPxTI)\n  - [Info 290 - Analyzing Big Data with Twitter, UC Berkeley school of information](http://blogs.ischool.berkeley.edu/i290-abdt-s12/) ([YouTube](https://www.youtube.com/playlist?list=PLE8C1256A28C1487F))\n  - [CS224W Machine Learning with Graphs | Spring 2021 | Stanford University](https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn)\n  - [9.520 - Statistical Learning Theory and Applications, Fall 2015 - MIT](https://www.youtube.com/playlist?list=PLyGKBDfnk-iDj3FBd0Avr_dLbrU8VG73O)\n  - [Statistical Learning Theory, Spring 2019 - ETH ZÃ¼rich](https://video.ethz.ch/lectures/d-infk/2019/spring/252-0526-00L.html)\n  - [Course on the Statistical Learning Theory, University of SÃ£o Paulo, ICMC](https://www.youtube.com/playlist?list=PLKWX1jIoUZaVpVhMfevAE7iYNcDHPEJI_)\n  - [Reinforcement Learning - UCL](https://www.youtube.com/playlist?list=PLacBNHqv7n9gp9cBMrA6oDbzz_8JqhSKo)\n  - [Regularization Methods for Machine Learning 2016](http://academictorrents.com/details/493251615310f9b6ae1f483126292378137074cd) ([YouTube](https://www.youtube.com/playlist?list=PLbF0BXX_6CPJ20Gf_KbLFnPWj', '{"language":null,"stars":70337,"forks":9421,"watchers":70337,"open_issues":2,"topics":["algorithms","bioinformatics","computational-biology","computational-physics","computer-architecture","computer-science","computer-vision","database-systems","databases","deep-learning","embedded-systems","machine-learning","quantum-computing","reinforcement-learning","robotics","security","systems","web-development"],"default_branch":"master","size_kb":861,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:Developer-Y:cs-video-courses","source_url":"https://github.com/Developer-Y/cs-video-courses"},{"type":"has_code","target_id":"github:Developer-Y:cs-video-courses","source_url":"https://github.com/Developer-Y/cs-video-courses"},{"type":"has_code","target_id":"github:kpc-simone:cs480-f24","source_url":"https://github.com/kpc-simone/cs480-f24"},{"type":"has_code","target_id":"github:DS-4-DS:DS4DS_Course","source_url":"https://github.com/DS-4-DS/DS4DS_Course"},{"type":"has_code","target_id":"github:jwaterman:data144-materials-sp21","source_url":"https://github.com/jwaterman/data144-materials-sp21"},{"type":"has_code","target_id":"github:APMonitor:data_science","source_url":"https://github.com/APMonitor/data_science"},{"type":"has_code","target_id":"github:spacemanidol:CS512DM","source_url":"https://github.com/spacemanidol/CS512DM"},{"type":"has_code","target_id":"github:fastai:course-nlp","source_url":"https://github.com/fastai/course-nlp"},{"type":"has_code","target_id":"github:oxford-cs-deepnlp-2017:lectures","source_url":"https://github.com/oxford-cs-deepnlp-2017/lectures"},{"type":"has_code","target_id":"github:rishabhk108:AdvancedOptML","source_url":"https://github.com/rishabhk108/AdvancedOptML"}]', NULL, NULL, 'pending', 70, '9460a68a8fadb891177d11dea8f7fea6', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Developer-Y-cs-video-courses from https://github.com/Developer-Y.png
Image converted to WebP: data/images/github-Developer-Y-cs-video-courses.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-mlabonne-llm-course', 'github--mlabonne--llm-course', 'llm-course', 'mlabonne', '<div align="center"> <img src="img/banner.png" alt="LLM Course"> <p align="center"> ð• <a href="https://twitter.com/maximelabonne">Follow me on X</a> â€¢ ðŸ¤— <a href="https://huggingface.co/mlabonne">Hugging Face</a> â€¢ ðŸ’» <a href="https://mlabonne.github.io/blog">Blog</a> â€¢ ðŸ“™ <a href="https://packt.link/a/9781836200079">LLM Engineer''s Handbook</a> </p> </div> <br/> <a href="https://a.co/d/a2M67rE"><img align="right" width="25%" src="https://i.imgur.com/7iNjEq2.png" alt="LLM Engineer''s Handbook ...', '["course","large-language-models","llm","machine-learning","roadmap"]', 'other', 69281, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/mlabonne/llm-course","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<div align="center">\n<img src="img/banner.png" alt="LLM Course">\n  <p align="center">\n    ð• <a href="https://twitter.com/maximelabonne">Follow me on X</a> â€¢ \n    ðŸ¤— <a href="https://huggingface.co/mlabonne">Hugging Face</a> â€¢ \n    ðŸ’» <a href="https://mlabonne.github.io/blog">Blog</a> â€¢ \n    ðŸ“™ <a href="https://packt.link/a/9781836200079">LLM Engineer''s Handbook</a>\n  </p>\n</div>\n<br/>\n\n<a href="https://a.co/d/a2M67rE"><img align="right" width="25%" src="https://i.imgur.com/7iNjEq2.png" alt="LLM Engineer''s Handbook Cover"/></a>The LLM course is divided into three parts:\n\n1. ðŸ§© **LLM Fundamentals** is optional and covers fundamental knowledge about mathematics, Python, and neural networks.\n2. ðŸ§‘â€ðŸ”¬ **The LLM Scientist** focuses on building the best possible LLMs using the latest techniques.\n3. ðŸ‘· **The LLM Engineer** focuses on creating LLM-based applications and deploying them.\n\n> [!NOTE]\n> Based on this course, I wrote the [LLM Engineer''s Handbook](https://packt.link/a/9781836200079) with Paul Iuzstin. It''s a hands-on and detailed book that covers an end-to-end LLM application from design to deployment. The LLM course will always stay free but feel free to support my work by purchasing the book.\n\nFor an interactive version of this course, I created an LLM assistant that will answer questions and test your knowledge in a personalized way on [**HuggingChat**](https://hf.co/chat/assistant/66029d2e5f4a884f7aabc9d1) or [**ChatGPT**](https://chat.openai.com/g/g-yviLuLqvI-llm-course).\n\n## ðŸ“ Notebooks\n\nA list of notebooks and articles I wrote about LLMs.\n\n### Tools\n\n| Notebook | Description | Notebook |\n|----------|-------------|----------|\n| ðŸ§ [LLM AutoEval](https://github.com/mlabonne/llm-autoeval) | Automatically evaluate your LLMs using RunPod | <a href="https://colab.research.google.com/drive/1Igs3WZuXAIv9X0vwqiE90QlEPys8e8Oa?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| ðŸ¥± LazyMergekit | Easily merge models using MergeKit in one click. | <a href="https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| ðŸ¦Ž LazyAxolotl | Fine-tune models in the cloud using Axolotl in one click. | <a href="https://colab.research.google.com/drive/1TsDKNo2riwVmU55gjuBgB1AXVtRRfRHW?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| âš¡ AutoQuant | Quantize LLMs in GGUF, GPTQ, EXL2, AWQ, and HQQ formats in one click. | <a href="https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| ðŸŒ³ Model Family Tree | Visualize the family tree of merged models. | <a href="https://colab.research.google.com/drive/1s2eQlolcI1VGgDhqWIANfkfKvcKrMyNr?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| ðŸš€ ZeroSpace | Automatically create a Gradio chat interface using a free ZeroGPU. | <a href="https://colab.research.google.com/drive/1LcVUW5wsJTO2NGmozjji5CkC--646LgC"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| âœ‚ï¸ AutoAbliteration | Automatically abliteration models with custom datasets. | <a href="https://colab.research.google.com/drive/1RmLv-pCMBBsQGXQIM8yF-OdCNyoylUR1?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| ðŸ§¼ AutoDedup | Automatically deduplicate datasets using the Rensa library. | <a href="https://colab.research.google.com/drive/1o1nzwXWAa8kdkEJljbJFW1VuI-3VZLUn?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n\n### Fine-tuning\n\n| Notebook | Description | Article | Notebook |\n|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Fine-tune Llama 3.1 with Unsloth | Ultra-efficient supervised fine-tuning in Google Colab. | [Article](https://mlabonne.github.io/blog/posts/2024-07-29_Finetune_Llama31.html) | <a href="https://colab.research.google.com/drive/164cg_O7SV7G8kZr_JXqLd6VC7pd86-1Z?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| Fine-tune Llama 3 with ORPO | Cheaper and faster fine-tuning in a single stage with ORPO. | [Article](https://mlabonne.github.io/blog/posts/2024-04-19_Fine_tune_Llama_3_with_ORPO.html) | <a href="https://colab.research.google.com/drive/1eHNWg9gnaXErdAa8_mcvjMupbSS6rDvi"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| Fine-tune Mistral-7b with DPO | Boost the performance of supervised fine-tuned models with DPO. | [Article](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html) | <a href="https://colab.research.google.com/drive/15iFBr1xWgztXvhrj5I9fBv20c7CFOPBE?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| Fine-tune Mistral-7b with QLoRA | Supervised fine-tune Mistral-7b in a free-tier Google Colab with TRL. |  | <a href="https://colab.research.google.com/drive/1o_w0KastmEJNVwT5GoqMCciH-18ca5WS?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| Fine-tune CodeLlama using Axolotl | End-to-end guide to the state-of-the-art tool for fine-tuning. | [Article](https://mlabonne.github.io/blog/posts/A_Beginners_Guide_to_LLM_Finetuning.html) | <a href="https://colab.research.google.com/drive/1Xu0BrCB7IShwSWKVcfAfhehwjDrDMH5m?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| Fine-tune Llama 2 with QLoRA | Step-by-step guide to supervised fine-tune Llama 2 in Google Colab. | [Article](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html) | <a href="https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n\n### Quantization\n\n| Notebook | Description | Article | Notebook |\n|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Introduction to Quantization | Large language model optimization using 8-bit quantization. | [Article](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html) | <a href="https://colab.research.google.com/drive/1DPr4mUQ92Cc-xf4GgAaB6dFcFnWIvqYi?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| 4-bit Quantization using GPTQ | Quantize your own open-source LLMs to run them on consumer hardware. | [Article](https://mlabonne.github.io/blog/4bit_quantization/) | <a href="https://colab.research.google.com/drive/1lSvVDaRgqQp_mWK_jC9gydz6_-y6Aq4A?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| Quantization with GGUF and llama.cpp | Quantize Llama 2 models with llama.cpp and upload GGUF versions to the HF Hub. | [Article](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html) | <a href="https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| ExLlamaV2: The Fastest Library to RunÂ LLMs | Quantize and run EXL2Â models and upload them to the HF Hub. | [Article](https://mlabonne.github.io/blog/posts/ExLlamaV2_The_Fastest_Library_to_Run%C2%A0LLMs.html) | <a href="https://colab.research.google.com/drive/1yrq4XBlxiA0fALtMoT2dwiACVc77PHou?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n\n### Other\n\n| Notebook | Description | Article | Notebook |\n|---------------------------------------|-------------------------------------------------------------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Merge LLMs with MergeKit | Create your own models easily, no GPU required! | [Article](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit%20copy.html) | <a href="https://colab.research.google.com/drive/1_JS7JKJAQozD48-LhYdegcuuZ2ddgXfr?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| Create MoEs with MergeKit | Combine multiple experts into a single frankenMoE | [Article](https://mlabonne.github.io/blog/posts/2024-03-28_Create_Mixture_of_Experts_with_MergeKit.html) | <a href="https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| Uncensor any LLM with abliteration | Fine-tuning without retraining | [Article](https://mlabonne.github.io/blog/posts/2024-06-04_Uncensor_any_LLM_with_abliteration.html) | <a href="https://colab.research.google.com/drive/1VYm3hOcvCpbGiqKZb141gJwjdmmCcVpR?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| Improve ChatGPT with Knowledge Graphs | Augment ChatGPT''s answers with knowledge graphs. | [Article](https://mlabonne.github.io/blog/posts/Article_Improve_ChatGPT_with_Knowledge_Graphs.html) | <a href="https://colab.research.google.com/drive/1mwhOSw9Y9bgEaIFKT4CLi0n18pXRM4cj?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n| Decoding Strategies in Large Language Models | A guide to text generation from beam search to nucleus sampling | [Article](https://mlabonne.github.io/blog/posts/2022-06-07-Decoding_strategies.html) | <a href="https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing"><img src="img/colab.svg" alt="Open In Colab"></a> |\n\n## ðŸ§© LLM Fundamentals\n\nThis section introduces essential knowledge about mathematics, Python, and neural networks. You might not want to start here but refer to it as needed.\n\n<details>\n<summary>Toggle section (optional)</summary>\n  \n![](img/roadmap_fundamentals.png)\n\n### 1. Mathematics for Machine Learning\n\nBefore mastering machine learning, it is important to understand the fundamental mathematical concepts that power these algorithms.\n\n- **Linear Algebra**: This is crucial for understanding many algorithms, especially those used in deep learning. Key concepts include vectors, matrices, determinants, eigenvalues and eigenvectors, vector spaces, and linear transformations.\n- **Calculus**: Many machine learning algorithms involve the optimization of continuous functions, which requires an understanding of derivatives, integrals, limits, and series. Multivariable calculus and the concept of gradients are also important.\n- **Probability and Statistics**: These are crucial for understanding how models learn from data and make predictions. Key concepts include probability theory, random variables, probability distributions, expectations, variance, covariance, correlation, hypothesis testing, confidence intervals, maximum likelihood estimation, and Bayesian inference.\n\nðŸ“š Resources:\n\n- [3Blue1Brown - The Essence of Linear Algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab): Series of videos that give a geometric intuition to these concepts.\n- [StatQuest with Josh Starmer - Statistics Fundamentals](https://www.youtube.com/watch?v=qBigTkBLU6g&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9): Offers simple and clear explanations for many statistical concepts.\n- [AP Statistics Intuition by Ms Aerin](https://automata88.medium.com/list/cacc224d5e7d): List of Medium articles that provide the intuition behind every probability distribution.\n- [Immersive Linear Algebra](https://immersivemath.com/ila/learnmore.html): Another visual interpretation of linear algebra.\n- [Khan Academy - Linear Algebra](https://www.khanacademy.org/math/linear-algebra): Great for beginners as it explains the concepts in a very intuitive way.\n- [Khan Academy - Calculus](https://www.khanacademy.org/math/calculus-1): An interactive course that covers all the basics of calculus.\n- [Khan Academy - Probability and Statistics](https://www.khanacademy.org/math/statistics-probability): Delivers the material in an easy-to-understand format.\n\n---\n\n### 2. Python for Machine Learning\n\nPython is a powerful and flexible programming language that''s particularly good for machine learning, thanks to its readability, consistency, and robust ecosystem of data science libraries.\n\n- **Python Basics**: Python programming requires a good understanding of the basic syntax, data types, error handling, and object-oriented programming.\n- **Data Science Libraries**: It includes familiarity with NumPy for numerical operations, Pandas for data manipulation and analysis, Matplotlib and Seaborn for data visualization.\n- **Data Preprocessing**: This involves feature scaling and normalization, handling missing data, outlier detection, categorical data encoding, and splitting data into training, validation, and test sets.\n- **Machine Learning Libraries**: Proficiency with Scikit-learn, a library providing a wide selection of supervised and unsupervised learning algorithms, is vital. Understanding how to implement algorithms like linear regression, logistic regression, decision trees, random forests, k-nearest neighbors (K-NN), and K-means clustering is important. Dimensionality reduction techniques like PCA and t-SNE are also helpful for visualizing high-dimensional data.\n\nðŸ“š Resources:\n\n- [Real Python](https://realpython.com/): A comprehensive resource with articles and tutorials for both beginner and advanced Python concepts.\n- [freeCodeCamp - Learn Python](https://www.youtube.com/watch?v=rfscVS0vtbw): Long video that provides a full introduction into all of the core concepts in Python.\n- [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/): Free digital book that is a great resource for learning pandas, NumPy, Matplotlib, and Seaborn.\n- [freeCodeCamp - Machine Learning for Everybody](https://youtu.be/i_LwzRVP7bg): Practical introduction to different machine learning algorithms for beginners.\n- [Udacity - Intro to Machine Learning](https://www.udacity.com/course/intro-to-machine-learning--ud120): Free course that covers PCA and several other machine learning concepts.\n\n---\n\n### 3. Neural Networks\n\nNeural networks are a fundamental part of many machine learning models, particularly in the realm of deep learning. To utilize them effectively, a comprehensive understanding of their design and mechanics is essential.\n\n- **Fundamentals**: This includes understanding the structure of a neural network, such as layers, weights, biases, and activation functions (sigmoid, tanh, ReLU, etc.)\n- **Training and Optimization**: Familiarize yourself with backpropagation and different types of loss functions, like Mean Squared Error (MSE) and Cross-Entropy. Understand various optimization algorithms like Gradient Descent, Stochastic Gradient Descent, RMSprop, and Adam.\n- **Overfitting**: Understand the concept of overfitting (where a model performs well on training data but poorly on unseen data) and learn various regularization techniques (dropout, L1/L2 regularization, early stopping, data augmentation) to prevent it.\n- **Implement a Multilayer Perceptron (MLP)**: Build an MLP, also known as a fully connected network, using PyTorch.\n\nðŸ“š Resources:\n\n- [3Blue1Brown - But what is a Neural Network?](https://www.youtube.com/watch?v=aircAruvnKk): This video gives an intuitive explanation of neural networks and their inner workings.\n- [freeCodeCamp - Deep Learning Crash Course](https://www.youtube.com/watch?v=VyWAvY2CF9c): This video efficiently introduces all the most important concepts in deep learning.\n- [Fast.ai - Practical Deep Learning](https://course.fast.ai/): Free course designed for people with coding experience who want to learn about deep learning.\n- [Patrick Loeber - PyTorch Tutorials](https://www.youtube.com/playlist?list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4): Series of videos for complete beginners to learn about PyTorch.\n\n---\n\n### 4. Natural Language Processing (NLP)\n\nNLP is a fascinating branch of artificial intelligence that bridges the gap between human language and machine understanding. From simple text processing to understanding linguistic nuances, NLP plays a crucial role in many applications like translation, sentiment analysis, chatbots, and much more.\n\n- **Text Preprocessing**: Learn various text preprocessing steps like tokenization (splitting text into words or sentences), stemming (reducing words to their root form), lemmatization (similar to stemming but considers the context), stop word removal, etc.\n- **Feature Extraction Techniques**: Become familiar with techniques to convert text data into a format that can be understood by machine learning algorithms. Key methods include Bag-of-words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and n-grams.\n- **Word Embeddings**: Word embeddings are a type of word representation that allows words with similar meanings to have similar representations. Key methods include Word2Vec, GloVe, and FastText.\n- **Recurrent Neural Networks (RNNs)**: Understand the working of RNNs, a type of neural network designed to work with sequence data. Explore LSTMs and GRUs, two RNN variants that are capable of learning long-term dependencies.\n\nðŸ“š Resources:\n\n- [Lena Voita - Word Embeddings](https://lena-voita.github.io/nlp_course/word_embeddings.html): Beginner-friendly course about concepts related to word embeddings.\n- [RealPython - NLP with spaCy in Python](https://realpython.com/natural-language-processing-spacy-python/): Exhaustive guide about the spaCy library for NLP tasks in Python.\n- [Kaggle - NLP Guide](https://www.kaggle.com/learn-guide/natural-language-processing): A few notebooks and resources for a hands-on explanation of NLP in Python.\n- [Jay Alammar - The Illustration Word2Vec](https://jalammar.github.io/illustrated-word2vec/): A good reference to understand the famous Word2Vec architecture.\n- [Jake Tae - PyTorch RNN from Scratch](https://jaketae.github.io/study/pytorch-rnn/): Practical and simple implementation of RNN, LSTM, and GRU models in PyTorch.\n- [colah''s blog - Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/): A more theoretical article about the LSTM network.\n</details>\n\n## ðŸ§‘â€ðŸ”¬ The LLM Scientist\n\nThis section of the course focuses on learning how to build the best possible LLMs using the latest techniques.\n\n![](img/roadmap_scientist.png)\n\n### 1. The LLM architecture\n\nAn in-depth knowledge of the Transformer architecture is not required, but it''s important to understand the main steps of modern LLMs: converting text into numbers through tokenization, processing these tokens through layers including attention mechanisms, and finally generating new text through various sampling strategies.\n\n- **Architectural Overview**: Understand the evolution from encoder-decoder Transformers to decoder-only architectures like GPT, which form the basis of modern LLMs. Focus on how these models process and generate text at a high level.\n- **Tokenization**: Learn the principles of tokenization - how text is converted into numerical representations that LLMs can process. Explore different tokenization strategies and their impact on model performance and output quality.\n- **Attention mechanisms**: Master the core concepts of attention mechanisms, particularly self-attention and its variants. Understand how these mechanisms enable LLMs to process long-range dependencies and maintain context throughout sequences.\n- **Sampling techniques**: Explore various text generation approaches and their tradeoffs. Compare deterministic methods like greedy search and beam search with probabilistic approaches like temperature sampling and nucleus sampling.\n\nðŸ“š **References**:\n* [Visual intro to Transformers](https://www.youtube.com/watch?v=wjZofJX0v4M) by 3Blue1Brown: Visual introduction to Transformers for complete beginners.\n* [LLM Visualization](https://bbycroft.net/llm) by Brendan Bycroft: Interactive 3D visualization of LLM internals.\n* [nanoGPT](https://www.youtube.com/watch?v=kCc8FmEb1nY) by Andrej Karpathy: A 2h-long YouTube video to reimplement GPT from scratch (for programmers). He also made a video about [tokenization](https://www.youtube.com/watch?v=zduSFxRajkE).\n* [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/) by Lilian Weng: Historical overview to introduce the need for attention mechanisms.\n* [Decoding Strategies in LLMs](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html) by Maxime Labonne: Provide code and a visual introduction to the different decoding strategies to generate text.\n\n---\n### 2. Pre-training models\n\nPre-training is a computationally intensive and expensive process. While it''s not the focus of this course, it''s important to have a solid understanding of how models are pre-trained, especially in terms of data and parameters. Pre-training can also be performed by hobbyists at a small scale with <1B models.\n\n* **Data preparation**: Pre-training requires massive datasets (e.g., [Llama 3.1](https://arxiv.org/abs/2307.09288) was trained on 15 trillion tokens) that need careful curation, cleaning, deduplication, and tokenization. Modern pre-training pipelines implement sophisticated filtering to remove low-quality or problematic content.\n* **Distributed training**: Combine different parallelization strategies: data parallel (batch distribution), pipeline parallel (layer distribution), and tensor parallel (operation splitting). These strategies require optimized network communication and memory management across GPU clusters.\n* **Training optimization**: Use adaptive learning rates with warm-up, gradient clipping, and normalization to prevent explosions, mixed-precision training for memory efficiency, and modern optimizers (AdamW, Lion) with tuned hyperparameters.\n* **Monitoring**: Track key metrics (loss, gradients, GPU stats) using dashboards, implement targeted logging for distributed training issues, and set up performance profiling to identify bottlenecks in computation and communication across devices.\n\nðŸ“š **References**:\n* [FineWeb](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1) by Penedo et al.: Article to recreate a large-scale dataset for LLM pretraining (15T), including FineWeb-Edu, a high-quality subset.\n* [RedPajama v2](https://www.together.ai/blog/redpajama-data-v2) by Weber et al.: Another article and paper about a large-scale pre-training dataset with a lot of interesting quality filters.\n* [nanotron](https://github.com/huggingface/nanotron) by Hugging Face: Minimalistic LLM training codebase used to make [SmolLM2](https://github.com/huggingface/smollm).\n* [Parallel training](https://www.andrew.cmu.edu/course/11-667/lectures/W10L2%20Scaling%20Up%20Parallel%20Training.pdf) by Chenyan Xiong: Overview of optimization and parallelism techniques.\n* [Distributed training](https://arxiv.org/abs/2407.20018) by Duan et al.: A survey about efficient training of LLM on distributed architectures.\n* [OLMo 2](https://allenai.org/olmo) by AI2: Open-source language model with model, data, training, and evaluation code.\n* [LLM360](https://www.llm360.ai/) by LLM360: A framework for open-source LLMs with training and data preparation code, data, metrics, and models.\n\n---\n### 3. Post-training datasets\n\nPost-training datasets have a precise structure with instructions and answers (supervised fine-tuning) or instructions and chosen/rejected answers (preference alignment). Conversational structures are a lot rarer than the raw text used for pre-training, which is why we often need to process seed data and refine it to improve the accuracy, diversity, and complexity of the samples. More information and examples are available in my repo [ðŸ’¾ LLM Datasets](https://github.com/mlabonne/llm-datasets).\n\n* **Storage & chat templates**: Because of the conversational structure, post-training datasets are stored in a specific format like ShareGPT or OpenAI/HF. Then, these formats are mapped to a chat template like ChatML or Alpaca to produce the final samples the model is trained on.\n* **Synthetic data generation**: Create instruction-response pairs based on seed data using frontier models like GPT-4o. This approach allows for flexible and scalable dataset creation with high-quality answers. Key considerations include designing diverse seed tasks and effective system prompts.\n* **Data enhancement**: Enhance existing samples using techniques like verified outputs (using unit tests or solvers), multiple answers with rejection sampling, [Auto-Evol](https://arxiv.org/abs/2406.00770), Chain-of-Thought, Branch-Solve-Merge, personas, etc.\n* **Quality filtering**: Traditional techniques involve rule-based filtering, removing duplicates or near-duplicates (with MinHash or embeddings), and n-gram decontamination. Reward models and judge LLMs complement this step with fine-grained and customizable quality control.\n\nðŸ“š **References**:\n* [Synthetic Data Generator](https://huggingface.co/spaces/argilla/synthetic-data-generator) by Argilla: Beginner-friendly way of building datasets using natural language in a Hugging Face space.\n* [LLM Datasets](https://github.com/mlabonne/llm-datasets) by Maxime Labonne: Curated list of datasets and tools for post-training.\n* [NeMo-Curator](https://github.com/NVIDIA/NeMo-Curator) by Nvidia: Dataset preparation and curation framework for pre- and post-training data.\n* [Distilabel](https://distilabel.argilla.io/dev/sections/pipeline_samples/) by Argilla: Framework to generate synthetic data. It also includes interesting reproductions of papers like UltraFeedback.\n* [Semhash](https://github.com/MinishLab/semhash) by MinishLab: Minimalistic library for near-deduplication and decontamination with a distilled embedding model.\n* [Chat Template](https://huggingface.co/docs/transformers/main/en/chat_templating) by Hugging Face: Hugging Face''s documentation about chat templates.\n\n---\n### 4. Supervised Fine-Tuning\n\nSFT turns base models into helpful assistants, capable of answering questions and following instructions. During this process, they learn how to structure answers and reactivate a subset of knowledge learned during pre-training. Instilling new knowledge is possible but superficial: it cannot be used to learn a completely new language. Always prioritize data quality over parameter optimization.\n\n- **Training techniques**: Full fine-tuning updates all model parameters but requires significant compute. Parameter-efficient fine-tuning techniques like LoRA and QLoRA reduce memory requirements by training a small number of adapter parameters while keeping base weights frozen. QLoRA combines 4-bit quantization with LoRA to reduce VRAM usage. These techniques are all implemented in the most popular fine-tuning frameworks: [TRL](https://huggingface.co/docs/trl/en/index), [Unsloth](https://docs.unsloth.ai/), and [Axolotl](https://axolotl.ai/).\n- **Training parameters**: Key parameters include learning rate with schedulers, batch size, gradient accumulation, number of epochs, optimizer (like 8-bit AdamW), weight decay for regularization, and warmup steps for training stability. LoRA also adds three parameters: rank (typically 16-128), alpha (1-2x rank), and target modules.\n- **Distributed training**: Scale training across multiple GPUs using DeepSpeed or FSDP. DeepSpeed provides three ZeRO optimization stages with increasing levels of memory efficiency through state partitioning. Both methods support gradient checkpointing for memory efficiency.\n- **Monitoring**: Track training metrics including loss curves, learning rate schedules, and gradient norms. Monitor for common issues like loss spikes, gradient explosions, or performance degradation.\n\nðŸ“š **References**:\n* [Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth](https://huggingface.co/blog/mlabonne/sft-llama3) by Maxime Labonne: Hands-on tutorial on how to fine-tune a Llama 3.1 model using Unsloth.\n* [Axolotl - Documentation](https://axolotl-ai-cloud.github.io/axolotl/) by Wing Lian: Lots of interesting information related to distributed training and dataset formats.\n* [Mastering LLMs](https://parlance-labs.com/education/) by Hamel Husain: Collection of educational resources about fine-tuning (but also RAG, evaluation, applications, and prompt engineering).\n* [LoRA insights](https://lightning.ai/pages/community/lora-insights/) by Sebastian Raschka: Practical insights about LoRA and how to select the best parameters.\n\n---\n### 5. Preference Alignment\n\nPreference alignment is a second stage in the post-training pipeline, focused on aligning generated answers with human preferences. This stage was designed to tune the tone of LLMs and reduce toxicity and hallucinations. However, it has become increasingly important to also boost their performance and improve usefulness. Unlike SFT, there are many preference alignment algorithms. Here, we''ll focus on the three most important ones: DPO, GRPO, and PPO.\n\n- **Rejection sampling**: For each prompt, use the trained model to generate multiple responses, and score them to infer chosen/rejected answers. This creates on-policy data, where both responses come from the model being trained, improving alignment stability.\n- **[Direct Preference Optimization](https://arxiv.org/abs/2305.18290)** Directly optimizes the policy to maximize the likelihood of chosen responses over rejected ones. It doesn''t require reward modeling, which makes it more computationally efficient than RL techniques but slightly worse in terms of quality. Great for creating chat models.\n- **Reward model**: Train a reward model with human feedback to predict metrics like human preferences. It can leverage frameworks like [TRL](https://huggingface.co/docs/trl/en/index), [verl](https://github.com/volcengine/verl), and [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF) for scalable training.\n- **Reinforcement Learning**: RL techniques like [GRPO](https://arxiv.org/abs/2402.03300) and [PPO](https://arxiv.org/abs/1707.06347) iteratively update a policy to maximize rewards while staying close to the initial behavior. They can use a reward model or reward functions to score responses. They tend to be computationally expensive and require careful tuning of hyperparameters, including learning rate, batch size, and clip range. Ideal for creating reasoning models.\n\nðŸ“š **References**:\n* [Illustrating RLHF](https://huggingface.co/blog/rlhf) by Hugging Face: Introduction to RLHF with reward model training and fine-tuning with reinforcement learning.\n* [LLM Training: RLHF and Its Alternatives](https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives) by Sebastian Raschka: Overview of the RLHF process and alternatives like RLAIF.\n* [Preference Tuning LLMs](https://huggingface.co/blog/pref-tuning) by Hugging Face: Comparison of the DPO, IPO, and KTO algorithms to perform preference alignment.\n* [Fine-tune with DPO](https://mlabonne.github.io/blog/posts/Fine_tune_Mistral_7b_with_DPO.html) by Maxime Labonne: Tutorial to fine-tune a Mistral-7b model with DPO and reproduce [NeuralHermes-2.5](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B).\n* [Fine-tune with GRPO](https://huggingface.co/learn/llm-course/en/chapter12/5) by Maxime Labonne: Practical exercise to fine-tune a small model with GRPO.\n* [DPO Wandb logs](https://wandb.ai/alexander-vishnevskiy/dpo/reports/TRL-Original-DPO--Vmlldzo1NjI4MTc4) by Alexander Vishnevskiy: It shows you the main DPO metrics to track and the trends you should expect.\n\n---\n### 6. Evaluation\n\nReliably evaluating LLMs is a complex but essential task guiding data generation and training. It provides invaluable feedback about areas of improvement, which can be leveraged to modify the data mixture, quality, and training parameters. However, it''s always good to remember Goodhart''s law: "When a measure becomes a target, it ceases to be a good measure."\n\n- **Automated benchmarks**: Evaluate models on specific tasks using curated datasets and metrics, like MMLU. It works well for concrete tasks but struggles with abstract and creative capabilities. It is also prone to data contamination.\n- **Human evaluation**: It involves humans prompting models and grading responses. Methods range from vibe checks to systematic annotations with specific guidelines and large-scale community voting (arena). It is more suited for subjective tasks and less reliable for factual accuracy.\n- **Model-based evaluation**: Use judge and reward models to evaluate model outputs. It highly correlates with human preferences but suffers from bias toward their own outputs and inconsistent scoring.\n- **Feedback signal**: Analyze error patterns to identify specific weaknesses, such as limitations in following complex instructions, lack of specific knowledge, or susceptibility to adversarial prompts. This can be improved with better data generation and training parameters.\n\nðŸ“š **References**:\n* [Evaluation guidebook](https://github.com/huggingface/evaluation-guidebook) by ClÃ©mentine Fourrier: Practical insights and theoretical knowledge about LLM evaluation.\n* [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) by Hugging Face: Main leaderboard to compare LLMs in an open and reproducible way (automated benchmarks).\n* [Language Model Evaluation Harness](https://github.com/EleutherAI/lm-evaluation-harness) by EleutherAI: A popular framework for evaluating LLMs using automated benchmarks.\n* [Lighteval](https://github.com/huggingface/lighteval) by Hugging Face: Alternative evaluation framework that also includes model-based evaluations.\n* [Chatbot Arena](https://lmarena.ai/) by LMSYS: Elo rating of general-purpose LLMs, based on comparisons made by humans (human evaluation).\n\n---\n### 7. Quantization\n\nQuantization is the process of converting the parameters and activations of a model using a lower precision. For example, weights stored using 16 bits can be converted into a 4-bit representation. This technique has become increasingly important to reduce the computational and memory costs associated with LLMs.\n\n* **Base techniques**: Learn the different levels of precision (FP32, FP16, INT8, etc.) and how to perform naÃ¯ve quantization with absmax and zero-point techniques.\n* **GGUF & llama.cpp**: Originally designed to run on CPUs, [llama.cpp](https://github.com/ggerganov/llama.cpp) and the GGUF format have become the most popular tools to run LLMs on consumer-grade hardware. It supports storing special tokens, vocabulary, and metadata in a single file. \n* **GPTQ & AWQ**: Techniques like [GPTQ](https://arxiv.org/abs/2210.17323)/[EXL2](https://github.com/turboderp/exllamav2) and [AWQ](https://arxiv.org/abs/2306.00978) introduce layer-by-layer calibration that retains performance at extremely low bitwidths. They reduce catastrophic outliers using dynamic scaling, selectively skipping or re-centering the heaviest parameters.\n* **SmoothQuant & ZeroQuant**: New quantization-friendly transformations (SmoothQuant) and compiler-based optimizations (ZeroQuant) help mitigate outliers before quantization. They also reduce hardware overhead by fusing certain ops and optimizing dataflow.\n\nðŸ“š **References**:\n* [Introduction to quantization](https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html) by Maxime Labonne: Overview of quantization, absmax and zero-point quantization, and LLM.int8() with code.\n* [Quantize Llama models with llama.cpp](https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html) by Maxime Labonne: Tutorial on how to quantize a Llama 2 model using llama.cpp and the GGUF format.\n* [4-bit LLM Quantization with GPTQ](https://mlabonne.github.io/blog/posts/4_bit_Quantization_with_GPTQ.html) by Maxime Labonne: Tutorial on how to quantize an LLM using the GPTQ algorithm with AutoGPTQ.\n* [Understanding Activation-Aware Weight Quantization](https://medium.com/friendliai/understanding-activation-aware-weight-quantization-awq-boosting-inference-serving-efficiency-in-10bb0faf63a8) by FriendliAI: Overview of the AWQ technique and its benefits.\n* [SmoothQuant on Llama 2 7B](https://github.com/mit-han-lab/smoothquant/blob/main/examples/smoothquant_llama_demo.ipynb) by MIT HAN Lab: Tutorial on how to use SmoothQuant with a Llama 2 model in 8-bit precision.\n* [DeepSpeed Model Compression](https://www.deepspeed.ai/tutorials/model-compression/) by DeepSpeed: Tutorial on how to use ZeroQuant and extreme compression (XTC) with DeepSpeed Compression.\n\n---\n### 8. New Trends\n\nHere are notable topics that didn''t fit into other categories. Some are established (model merging, multimodal) techniques, but others are more experimental (interpretability, test-time compute scaling) and the focus of numerous research papers. \n\n* **Model merging**: Merging trained models has become a popular way of creating performant models without any fine-tuning. The popular [mergekit](https://github.com/cg123/mergekit) library implements the most popular merging methods, like SLERP, [DARE](https://arxiv.org/abs/2311.03099), and [TIES](https://arxiv.org/abs/2311.03099).\n* **Multimodal models**: These models (like [CLIP](https://openai.com/research/clip), [Stable Diffusion](https://stability.ai/stable-image), or [LLaVA](https://llava-vl.github.io/)) process multiple types of inputs (text, images, audio, etc.) with a unified embedding space, which unlocks powerful applications like text-to-image.\n* **Interpretability**: Mechanistic interpretability techniques like Sparse Autoencoders (SAEs) have made remarkable progress to provide insights about the inner workings of LLMs. This has also been applied with techniques such as abliteration, which allow you to modify the behavior of models without training.\n* **Test-time compute**: Reasoning models trained with RL techniques can be further improved by scaling the compute budget during test time. It can involve multiple calls, MCTS, or specialized models like a Process Reward Model (PRM). Iterative steps with precise scoring significantly improve performance for complex reasoning tasks.\n\nðŸ“š **References**:\n* [Merge LLMs with mergekit](https://mlabonne.github.io/blog/posts/2024-01-08_Merge_LLMs_with_mergekit.html) by Maxime Labonne: Tutorial about model merging using mergekit.\n* [Smol Vision](https://github.com/merveenoyan/smol-vision) by Merve Noyan: Collection of notebooks and scripts dedicated to small multimodal models.\n* [Large Multimodal Models](https://huyenchip.com/2023/10/10/multimodal.html) by Chip Huyen: Overview of multimodal systems and the recent history of this field.\n* [Unsensor any LLM with abliteration](https://huggingface.co/blog/mlabonne/abliteration) by Maxime Labonne: Direct application of interpretability techniques to modify the style of a model.\n* [Intuitive Explanation of SAEs](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html) by Adam Karvonen: Article about how SAEs work and why they make sense for interpretability.\n* [Scaling test-time compute](https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute) by Beeching et al.: Tutorial and experiments to outperform Llama 3.1 70B on MATH-500 with a 3B model.\n\n## ðŸ‘· The LLM Engineer\n\nThis section of the course focuses on learning how to build LLM-powered applications that can be used in production, with a focus on augmenting models and deploying them.\n\n![](img/roadmap_engineer.png)\n\n### 1. Running LLMs\n\nRunning LLMs can be difficult due to high hardware requirements. Depending on your use case, you might want to simply consume a model through an API (like GPT-4) or run it locally. In any case, additional prompting and guidance techniques can improve and constrain the output for your applications.\n\n* **LLM APIs**: APIs are a convenient way to deploy LLMs. This space is divided between private LLMs ([OpenAI](https://platform.openai.com/), [Google](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview), [Anthropic](https://docs.anthropic.com/claude/reference/getting-started-with-the-api), etc.) and open-source LLMs ([OpenRouter](https://openrouter.ai/), [Hugging Face](https://huggingface.co/inference-api), [Together AI](https://www.together.ai/), etc.).\n* **Open-source LLMs**: The [Hugging Face Hub](https://huggingface.co/models) is a great place to find LLMs. You can directly run some of them in [Hugging Face Spaces](https://huggingface.co/spaces), or download and run them locally in apps like [LM Studio](https://lmstudio.ai/) or through the CLI with [llama.cpp](https://github.com/ggerganov/llama.cpp) or [ollama](https://ollama.ai/).\n* **Prompt engineering**: Common techniques include zero-shot prompting, few-shot prompting, chain of thought, and ReAct. They work better with bigger models, but can be adapted to smaller ones.\n* **Structuring outputs**: Many tasks require a structured output, like a strict template or a JSON format. Libraries like [Outlines](https://github.com/outlines-dev/outlines) can be used to guide the generation and respect a given structure. Some APIs also support structured output generation natively using JSON schemas.\n\nðŸ“š **References**:\n* [Run an LLM locally with LM Studio](https://www.kdnuggets.com/run-an-llm-locally-with-lm-studio) by Nisha Arya: Short guide on how to use LM Studio.\n* [Prompt engineering guide](https://www.promptingguide.ai/) by DAIR.AI: Exhaustive list of prompt techniques with examples\n* [Outlines - Quickstart](https://dottxt-ai.github.io/outlines/latest/quickstart/): List of guided generation techniques enabled by Outlines. \n* [LMQL - Overview](https://lmql.ai/docs/language/overview.html): Introduction to the LMQL language.\n\n---\n### 2. Building a Vector Storage\n\nCreating a vector storage is the first step to building a Retrieval Augmented Generation (RAG) pipeline. Documents are loaded, split, and relevant chunks are used to produce vector representations (embeddings) that are stored for future use during inference.\n\n* **Ingesting documents**: Document loaders are convenient wrappers that can handle many formats: PDF, JSON, HTML, Markdown, etc. They can also directly retrieve data from some databases and APIs (GitHub, Reddit, Google Drive, etc.).\n* **Splitting documents**: Text splitters break down documents into smaller, semantically meaningful chunks. Instead of splitting text after *n* characters, it''s often better to split by header or recursively, with some additional metadata.\n* **Embedding models**: Embedding models convert text into vector representations. Picking task-specific models significantly improves performance for semantic search and RAG.\n* **Vector databases**: Vector databases (like [Chroma](https://www.trychroma.com/), [Pinecone](https://www.pinecone.io/), [Milvus](https://milvus.io/), [FAISS](https://faiss.ai/), [Annoy](https://github.com/spotify/annoy), etc.) are designed to store embedding vectors. They enable efficient retrieval of data that is ''most similar'' to a query based on vector similarity.\n\nðŸ“š **References**:\n* [LangChain - Text splitters](https://python.langchain.com/docs/how_to/#text-splitters): List of different text splitters implemented in LangChain.\n* [Sentence Transformers library](https://www.sbert.net/): Popular library for embedding models.\n* [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard): Leaderboard for embedding models.\n* [The Top 7 Vector Databases](https://www.datacamp.com/blog/the-top-5-vector-databases) by Moez Ali: A comparison of the best and most popular vector databases.\n\n---\n### 3. Retrieval Augmented Generation\n\nWith RAG, LLMs retrieve contextual documents from a database to improve the accuracy of their answers. RAG is a popular way of augmenting the model''s knowledge without any fine-tuning.\n\n* **Orchestrators**: Orchestrators like [LangChain](https://python.langchain.com/docs/get_started/introduction) and [LlamaIndex](https://docs.llamaindex.ai/en/stable/) are popular frameworks to connect your LLMs with tools and databases. The Model Context Protocol (MCP) introduces a new standard to pass data and context to models across providers.\n* **Retrievers**: Query rewriters and generative retrievers like CoRAG and HyDE enhance search by transforming user queries. Multi-vector and hybrid retrieval methods combine embeddings with keyword signals to improve recall and precision.\n* **Memory**: To remember previous instructions and answers, LLMs and chatbots like ChatGPT add this history to their context window. This buffer can be improved with summarization (e.g., using a smaller LLM), a vector store + RAG, etc.\n* **Evaluation**: We need to evaluate both the document retrieval (context precision and recall) and generation stages (faithfulness and answer relevancy). It can be simplified with tools [Ragas](https://github.com/explodinggradients/ragas/tree/main) and [DeepEval](https://github.com/confident-ai/deepeval) (assessing quality).\n\nðŸ“š **References**:\n* [Llamaindex - High-level concepts](https://docs.llamaindex.ai/en/stable/getting_started/concepts.html): Main concepts to know when building RAG pipelines.\n* [Model Context Protocol](https://modelcontextprotocol.io/introduction): Introduction to MCP with motivate, architecture, and quick starts.\n* [Pinecone - Retrieval Augmentation](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/): Overview of the retrieval augmentation process. \n* [LangChain - Q&A with RAG](https://python.langchain.com/docs/tutorials/rag/): Step-by-step tutorial to build a typical RAG pipeline.\n* [LangChain - Memory types](https://python.langchain.com/docs/how_to/chatbots_memory/): List of different types of memories with relevant usage.\n* [RAG pipeline - Metrics](https://docs.ragas.io/en/stable/concepts/metrics/index.html): Overview of the main metrics used to evaluate RAG pipelines.\n\n---\n### 4. Advanced RAG\n\nReal-life applications can require complex pipelines, including SQL or graph databases, as well as automatically selecting relevant tools and APIs. These advanced techniques can improve a baseline solution and provide additional features.\n\n* **Query construction**: Structured data stored in traditional databases requires a specific query language like SQL, Cypher, metadata, etc. We can directly translate the user instruction into a query to access the data with query construction.\n* **Tools**: Agents augment LLMs by automatically selecting the most relevant tools to provide an answer. These tools can be as simple as using Google or Wikipedia, or more complex like a Python interpreter or Jira. \n* **Post-processing**: Final step that processes the inputs that are fed to the LLM. It enhances the relevance and diversity of documents retrieved with re-ranking, [RAG-fusion](https://github.com/Raudaschl/rag-fusion), and classification.\n* **Program LLMs**: Frameworks like [DSPy](https://github.com/stanfordnlp/dspy) allow you to optimize prompts and weights based on automated evaluations in a programmatic way.\n\nðŸ“š **References**:\n* [LangChain - Query Construction](https://blog.langchain.dev/query-construction/): Blog post about different types of query construction.\n* [LangChain - SQL](https://python.langchain.com/docs/tutorials/sql_qa/): Tutorial on how to interact with SQL databases with LLMs, involving Text-to-SQL and an optional SQL agent.\n* [Pinecone - LLM agents](https://www.pinecone.io/learn/series/langchain/langchain-agents/): Introduction to agents and tools with different types.\n* [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) by Lilian Weng: A more theoretical article about LLM agents.\n* [LangChain - OpenAI''s RAG](https://blog.langchain.dev/applying-openai-rag/): Overview of the RAG strategies employed by OpenAI, including post-processing.\n* [DSPy in 8 Steps](https://dspy-docs.vercel.app/docs/building-blocks/solving_your_task): General-purpose guide to DSPy introducing modules, signatures, and optimizers.\n\n---\n### 5. Agents\n\nAn LLM agent can autonomously perform tasks by taking actions based on reasoning about its environment, typically through the use of tools or functions to interact with external systems.\n\n* **Agent fundamentals**: Agents operate using thoughts (internal reasoning to decide what to do next), action (executing tasks, often by interacting with external tools), and observation (analyzing feedback or results to refine the next step).\n* **Agent frameworks**: Agent development can be streamlined using different frameworks like [LangGraph](https://www.langchain.com/langgraph) (design and visualization of workflows), [LlamaIndex](https://docs.llamaindex.ai/en/stable/use_cases/agents/) (data-augmented agents with RAG), or [smolagents](https://github.com/huggingface/smolagents) (beginner-friendly, lightweight option).\n* **Multi-agents**: More experimental frameworks include collaboration between different agents, such as [CrewAI](https://docs.crewai.com/introduction) (role-based team orchestration), [AutoGen](https://github.com/microsoft/autogen) (conversation-driven multi-agent systems), and [OpenAI Agents SDK](https://github.com/openai/openai-agents-python) (production-ready with strong OpenAI model integration).\n\nðŸ“š **References**:\n* [Agents Course](https://huggingface.co/learn/agents-course/unit0/introduction): Popular course about AI agents made by Hugging Face.\n* [AI Agents Comparison](https://langfuse.com/blog/2025-03-19-ai-agent-comparison) by Jannik MaierhÃ¶fer: Comparison of features across different open-source AI agent frameworks.\n* [LangGraph](https://langchain-ai.github.io/langgraph/concepts/why-langgraph/): Overview of how to build AI agents with LangGraph.\n* [LlamaIndex Agents](https://docs.llamaindex.ai/en/stable/use_cases/agents/): Uses cases and resources to build agents with LlamaIndex.\n* [smolagents](https://huggingface.co/docs/smolagents/index): Documentation with a guided tour, how-to guides, and more conceptual articles.\n\n---\n### 6. Inference optimization\n\nText generation is a costly process that requires expensive hardware. In addition to quantization, various techniques have been proposed to maximize throughput and reduce inference costs.\n\n* **Flash Attention**: Optimization of the attention mechanism to transform its complexity from quadratic to linear, speeding up both training and inference.\n* **Key-value cache**: Understand the key-value cache and the improvements introduced in [Multi-Query Attention](https://arxiv.org/abs/1911.02150) (MQA) and [Grouped-Query Attention](https://arxiv.org/abs/2305.13245) (GQA).\n* **Speculative decoding**: Use a small model to produce drafts that are then reviewed by a larger model to speed up text generation.\n\nðŸ“š **References**:\n* [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one) by Hugging Face: Explain how to optimize inference on GPUs.\n* [LLM Inference](https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices) by Databricks: Best practices for how to optimize LLM inference in production.\n* [Optimizing LLMs for Speed and Memory](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization) by Hugging Face: Explain three main techniques to optimize speed and memory, namely quantization, Flash Attention, and architectural innovations.\n* [Assisted Generation](https://huggingface.co/blog/assisted-generation) by Hugging Face: HF''s version of speculative decoding, it''s an interesting blog post about how it works with code to implement it.\n\n---\n### 7. Deploying LLMs\n\nDeploying LLMs at scale is an engineering feat that can require multiple clusters of GPUs. In other scenarios, demos and local apps can be achieved with a much lower complexity. \n\n* **Local deployment**: Privacy is an important advantage that open-source LLMs have over private ones. Local LLM servers ([LM Studio](https://lmstudio.ai/), [Ollama](https://ollama.ai/), [oobabooga](https://github.com/oobabooga/text-generation-webui), [kobold.cpp](https://github.com/LostRuins/koboldcpp), etc.) capitalize on this advantage to power local apps. \n* **Demo deployment**: Frameworks like [Gradio](https://www.gradio.app/) and [Streamlit](https://docs.streamlit.io/) are helpful to prototype applications and share demos. You can also easily host them online, for example using [Hugging Face Spaces](https://huggingface.co/spaces).\n* **Server deployment**: Deploy LLMs at scale requires cloud (see also [SkyPilot](https://skypilot.readthedocs.io/en/latest/)) or on-prem infrastructure and often leverage optimized text generation frameworks like [TGI](https://github.com/huggingface/text-generation-inference), [vLLM](https://github.com/vllm-project/vllm/tree/main), etc.\n* **Edge deployment**: In constrained environments, high-performance frameworks like [MLC LLM](https://github.com/mlc-ai/mlc-llm) and [mnn-llm](https://github.com/wangzhaode/mnn-llm/blob/master/README_en.md) can deploy LLM in web browsers, Android, and iOS.\n\nðŸ“š **References**:\n* [Streamlit - Build a basic LLM app](https://docs.streamlit.io/knowledge-base/tutorials/build-conversational-apps): Tutorial to make a basic ChatGPT-like app using Streamlit.\n* [HF LLM Inference Container](https://huggingface.co/blog/sagemaker-huggingface-llm): Deploy LLMs on Amazon SageMaker using Hugging Face''s inference container.\n* [PhilschmidÂ blog](https://www.philschmid.de/) by Philipp Schmid: Collection of high-quality articles about LLM deployment using Amazon SageMaker.\n* [Optimizing latence](https://hamel.dev/notes/llm/inference/03_inference.html) by Hamel Husain: Comparison of TGI, vLLM, CTranslate2, and mlc in terms of throughput and latency.\n\n---\n### 8. Securing LLMs\n\nIn addition to traditional security problems associated with software, LLMs have unique weaknesses due to the way they are trained and prompted.\n\n* **Prompt hacking**: Different techniques related to prompt engineering, including prompt injection (additional instruction to hijack the model''s answer), data/prompt leaking (retrieve its original data/prompt), and jailbreaking (craft prompts to bypass safety features).\n* **Backdoors**: Attack vectors can target the training data itself, by poisoning the training data (e.g., with false information) or creating backdoors (secret triggers to change the model''s behavior during inference).\n* **Defensive measures**: The best way to protect your LLM applications is to test them against these vulnerabilities (e.g., using red teaming and checks like [garak](https://github.com/leondz/garak/)) and observe them in production (with a framework like [langfuse](https://github.com/langfuse/langfuse)).\n\nðŸ“š **References**:\n* [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/) by HEGO Wiki: List of the 10 most critical vulnerabilities seen in LLM applications.\n* [Prompt Injection Primer](https://github.com/jthack/PIPE) by Joseph Thacker: Short guide dedicated to prompt injection for engineers.\n* [LLM Security](https://llmsecurity.net/) by [@llm_sec](https://twitter.com/llm_sec): Extensive list of resources related to LLM security.\n* [Red teaming LLMs](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming) by Microsoft: Guide on how to perform red teaming with LLMs.\n\n---\n## Acknowledgements\n\nThis roadmap was inspired by the excellent [DevOps Roadmap](https://github.com/milanm/DevOps-Roadmap) from Milan MilanoviÄ‡ and Romano Roth.\n\nSpecial thanks to:\n\n* Thomas Thelen for motivating me to create a roadmap\n* AndrÃ© Frade for his input and review of the first draft\n* Dino Dunn for providing resources about LLM security\n* Magdalena Kuhn for improving the "human evaluation" part\n* Odoverdose for suggesting 3Blue1Brown''s video about Transformers\n* Everyone who contributed to the educational references in this course :)\n\n*Disclaimer: I am not affiliated with any sources listed here.*\n\n---\n\n[![Star History Chart](https://api.star-history.com/svg?repos=mlabonne/llm-course&type=Date)](https://www.star-history.com/#mlabonne/llm-course&Date)\n', '{"language":null,"stars":69281,"forks":7889,"watchers":69281,"open_issues":76,"topics":["course","large-language-models","llm","machine-learning","roadmap"],"default_branch":"main","size_kb":7742,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:mlabonne:llm-autoeval","source_url":"https://github.com/mlabonne/llm-autoeval"},{"type":"has_code","target_id":"github:huggingface:nanotron","source_url":"https://github.com/huggingface/nanotron"},{"type":"has_code","target_id":"github:huggingface:smollm","source_url":"https://github.com/huggingface/smollm"},{"type":"has_code","target_id":"github:mlabonne:llm-datasets","source_url":"https://github.com/mlabonne/llm-datasets"},{"type":"has_code","target_id":"github:mlabonne:llm-datasets","source_url":"https://github.com/mlabonne/llm-datasets"},{"type":"has_code","target_id":"github:NVIDIA:NeMo-Curator","source_url":"https://github.com/NVIDIA/NeMo-Curator"},{"type":"has_code","target_id":"github:MinishLab:semhash","source_url":"https://github.com/MinishLab/semhash"},{"type":"has_code","target_id":"github:volcengine:verl","source_url":"https://github.com/volcengine/verl"},{"type":"has_code","target_id":"github:OpenRLHF:OpenRLHF","source_url":"https://github.com/OpenRLHF/OpenRLHF"},{"type":"has_code","target_id":"github:huggingface:evaluation-guidebook","source_url":"https://github.com/huggingface/evaluation-guidebook"},{"type":"has_code","target_id":"github:EleutherAI:lm-evaluation-harness","source_url":"https://github.com/EleutherAI/lm-evaluation-harness"},{"type":"has_code","target_id":"github:huggingface:lighteval","source_url":"https://github.com/huggingface/lighteval"},{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:turboderp:exllamav2","source_url":"https://github.com/turboderp/exllamav2"},{"type":"has_code","target_id":"github:mit-han-lab:smoothquant","source_url":"https://github.com/mit-han-lab/smoothquant"},{"type":"has_code","target_id":"github:cg123:mergekit","source_url":"https://github.com/cg123/mergekit"},{"type":"has_code","target_id":"github:merveenoyan:smol-vision","source_url":"https://github.com/merveenoyan/smol-vision"},{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:outlines-dev:outlines","source_url":"https://github.com/outlines-dev/outlines"},{"type":"has_code","target_id":"github:spotify:annoy","source_url":"https://github.com/spotify/annoy"},{"type":"has_code","target_id":"github:explodinggradients:ragas","source_url":"https://github.com/explodinggradients/ragas"},{"type":"has_code","target_id":"github:confident-ai:deepeval","source_url":"https://github.com/confident-ai/deepeval"},{"type":"has_code","target_id":"github:Raudaschl:rag-fusion","source_url":"https://github.com/Raudaschl/rag-fusion"},{"type":"has_code","target_id":"github:stanfordnlp:dspy","source_url":"https://github.com/stanfordnlp/dspy"},{"type":"has_code","target_id":"github:huggingface:smolagents","source_url":"https://github.com/huggingface/smolagents"},{"type":"has_code","target_id":"github:microsoft:autogen","source_url":"https://github.com/microsoft/autogen"},{"type":"has_code","target_id":"github:openai:openai-agents-python","source_url":"https://github.com/openai/openai-agents-python"},{"type":"has_code","target_id":"github:oobabooga:text-generation-webui","source_url":"https://github.com/oobabooga/text-generation-webui"},{"type":"has_code","target_id":"github:LostRuins:koboldcpp","source_url":"https://github.com/LostRuins/koboldcpp"},{"type":"has_code","target_id":"github:huggingface:text-generation-inference","source_url":"https://github.com/huggingface/text-generation-inference"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:mlc-ai:mlc-llm","source_url":"https://github.com/mlc-ai/mlc-llm"},{"type":"has_code","target_id":"github:wangzhaode:mnn-llm","source_url":"https://github.com/wangzhaode/mnn-llm"},{"type":"has_code","target_id":"github:leondz:garak","source_url":"https://github.com/leondz/garak"},{"type":"has_code","target_id":"github:langfuse:langfuse","source_url":"https://github.com/langfuse/langfuse"},{"type":"has_code","target_id":"github:jthack:PIPE","source_url":"https://github.com/jthack/PIPE"},{"type":"has_code","target_id":"github:milanm:DevOps-Roadmap","source_url":"https://github.com/milanm/DevOps-Roadmap"}]', NULL, 'Apache-2.0', 'approved', 80, '2ac20fc47ab4a98a27b579fb37fef912', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-mlabonne-llm-course from https://github.com/mlabonne.png
Image converted to WebP: data/images/github-mlabonne-llm-course.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-binhnguyennus-awesome-scalability', 'github--binhnguyennus--awesome-scalability', 'awesome-scalability', 'binhnguyennus', 'An updated and organized reading list for illustrating the patterns of scalable, reliable, and performant large-scale systems. Concepts are explained in the articles of prominent engineers and credible references. Case studies are taken from battle-tested systems that serve millions to billions of users. > Understand your problems: scalability problem (fast for a single user but slow under heavy load) or performance problem (slow for a single user) by reviewing some design principles and chec...', '["architecture","awesome","awesome-list","backend","big-data","computer-science","design-patterns","devops","distributed-systems","interview","interview-practice","interview-questions","lists","machine-learning","programming","resources","scalability","system","system-design","web-development"]', 'other', 67076, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/binhnguyennus/awesome-scalability","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '[![Logo](/logo.png)](http://awesome-scalability.com/)\n\nAn updated and organized reading list for illustrating the patterns of scalable, reliable, and performant large-scale systems. Concepts are explained in the articles of prominent engineers and credible references. Case studies are taken from battle-tested systems that serve millions to billions of users.\n\n#### If your system goes slow\n> Understand your problems: scalability problem (fast for a single user but slow under heavy load) or performance problem (slow for a single user) by reviewing some [design principles](#principle) and checking how [scalability](#scalability) and [performance](#performance) problems are solved at tech companies. The section of [intelligence](#intelligence) are created for those who work with data and machine learning at big (data) and deep (learning) scale.\n\n#### If your system goes down\n> "Even if you lose all one day, you can build all over again if you retain your calm!" - Thuan Pham, former CTO of Uber. So, keep calm and mind the [availability](#availability) and [stability](#stability) matters! \n\n#### If you are having a system design interview\n> Look at some [interview notes](#interview) and [real-world architectures with completed diagrams](#architecture) to get a comprehensive view before designing your system on whiteboard. You can check some [talks](#talk) of engineers from tech giants to know how they build, scale, and optimize their systems. Good luck!\n\n#### If you are building your dream team\n> The goal of scaling team is not growing team size but increasing team output and value. You can find out how tech companies reach that goal in various aspects: hiring, management, organization, culture, and communication in the [organization](#organization) section.\n\n#### Community power\n\n> Contributions are greatly welcome! You may want to take a look at the [contribution guidelines](CONTRIBUTING.md). If you see a link here that is no longer maintained or is not a good fit, please submit a pull request!\n\n> Many long hours of hard work have gone into this project. If you find it helpful, please share on Facebook, [on Twitter](https://ctt.ec/V8B2p), [on Weibo](http://t.cn/RnjFLCB), or on your chat groups! Knowledge is power, knowledge shared is power multiplied. Thank you!\n\n## Content\n- [Principle](#principle)\n- [Scalability](#scalability)\n- [Availability](#availability)\n- [Stability](#stability)\n- [Performance](#performance)\n- [Intelligence](#intelligence)\n- [Architecture](#architecture)\n- [Interview](#interview)\n- [Organization](#organization)\n- [Talk](#talk)\n- [Book](#book)\n\n## Principle\n* [Lessons from Giant-Scale Services - Eric Brewer, UC Berkeley & Google](https://people.eecs.berkeley.edu/~brewer/papers/GiantScale-IEEE.pdf)\n* [Designs, Lessons and Advice from Building Large Distributed Systems - Jeff Dean, Google](https://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf)\n* [How to Design a Good API & Why it Matters - Joshua Bloch, CMU & Google](https://www.infoq.com/presentations/effective-api-design)\n* [On Efficiency, Reliability, Scaling - James Hamilton, VP at AWS](http://mvdirona.com/jrh/work/)\n* [Principles of Chaos Engineering](https://www.usenix.org/conference/srecon17americas/program/presentation/rosenthal)\n* [Finding the Order in Chaos](https://www.usenix.org/conference/srecon16/program/presentation/lueder)\n* [The Twelve-Factor App](https://12factor.net/)\n* [Clean Architecture](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html)\n* [High Cohesion and Low Coupling](http://www.math-cs.gordon.edu/courses/cs211/lectures-2009/Cohesion,Coupling,MVC.pdf)\n* [Monoliths and Microservices](https://medium.com/@SkyscannerEng/monoliths-and-microservices-8c65708c3dbf)\n* [CAP Theorem and Trade-offs](http://robertgreiner.com/2014/08/cap-theorem-revisited/)\n* [CP Databases and AP Databases](https://blog.andyet.com/2014/10/01/right-database)\n* [Stateless vs Stateful Scalability](http://ithare.com/scaling-stateful-objects/)	\n* [Scale Up vs Scale Out: Hidden Costs](https://blog.codinghorror.com/scaling-up-vs-scaling-out-hidden-costs/)\n* [ACID and BASE](https://neo4j.com/blog/acid-vs-base-consistency-models-explained/)\n* [Blocking/Non-Blocking and Sync/Async](https://blogs.msdn.microsoft.com/csliu/2009/08/27/io-concept-blockingnon-blocking-vs-syncasync/)\n* [Performance and Scalability of Databases](https://use-the-index-luke.com/sql/testing-scalability)\n* [Database Isolation Levels and Effects on Performance and Scalability](http://highscalability.com/blog/2011/2/10/database-isolation-levels-and-their-effects-on-performance-a.html)\n* [The Probability of Data Loss in Large Clusters](https://martin.kleppmann.com/2017/01/26/data-loss-in-large-clusters.html)\n* [Data Access for Highly-Scalable Solutions: Using SQL, NoSQL, and Polyglot Persistence](https://docs.microsoft.com/en-us/previous-versions/msp-n-p/dn271399(v=pandp.10))\n* [SQL vs NoSQL](https://www.upwork.com/hiring/data/sql-vs-nosql-databases-whats-the-difference/)\n* [SQL vs NoSQL - Lesson Learned at Salesforce](https://engineering.salesforce.com/sql-or-nosql-9eaf1d92545b)\n* [NoSQL Databases: Survey and Decision Guidance](https://medium.baqend.com/nosql-databases-a-survey-and-decision-guidance-ea7823a822d)\n* [How Sharding Works](https://medium.com/@jeeyoungk/how-sharding-works-b4dec46b3f6)\n* [Consistent Hashing](http://www.tom-e-white.com/2007/11/consistent-hashing.html)\n* [Consistent Hashing: Algorithmic Tradeoffs](https://medium.com/@dgryski/consistent-hashing-algorithmic-tradeoffs-ef6b8e2fcae8)\n* [Donâ€™t be tricked by the Hashing Trick](https://booking.ai/dont-be-tricked-by-the-hashing-trick-192a6aae3087)\n* [Uniform Consistent Hashing at Netflix](https://medium.com/netflix-techblog/distributing-content-to-open-connect-3e3e391d4dc9)\n* [Eventually Consistent - Werner Vogels, CTO at Amazon](https://www.allthingsdistributed.com/2008/12/eventually_consistent.html)\n* [Cache is King](https://www.stevesouders.com/blog/2012/10/11/cache-is-king/)\n* [Anti-Caching](https://www.the-paper-trail.org/post/2014-06-06-paper-notes-anti-caching/)\n* [Understand Latency](http://highscalability.com/latency-everywhere-and-it-costs-you-sales-how-crush-it)\n* [Latency Numbers Every Programmer Should Know](http://norvig.com/21-days.html#answers)\n* [The Calculus of Service Availability](https://queue.acm.org/detail.cfm?id=3096459&__s=dnkxuaws9pogqdnxmx8i)\n* [Architecture Issues When Scaling Web Applications: Bottlenecks, Database, CPU, IO](http://highscalability.com/blog/2014/5/12/4-architecture-issues-when-scaling-web-applications-bottlene.html)	\n* [Common Bottlenecks](http://highscalability.com/blog/2012/5/16/big-list-of-20-common-bottlenecks.html)\n* [Life Beyond Distributed Transactions](https://queue.acm.org/detail.cfm?id=3025012)\n* [Relying on Software to Redirect Traffic Reliably at Various Layers](https://www.usenix.org/conference/srecon15/program/presentation/taveira)\n* [Breaking Things on Purpose](https://www.usenix.org/conference/srecon17americas/program/presentation/andrus)\n* [Avoid Over Engineering](https://medium.com/@rdsubhas/10-modern-software-engineering-mistakes-bc67fbef4fc8)\n* [Scalability Worst Practices](https://www.infoq.com/articles/scalability-worst-practices)\n* [Use Solid Technologies - Donâ€™t Re-invent the Wheel - Keep It Simple!](https://medium.com/@DataStax/instagram-engineerings-3-rules-to-a-scalable-cloud-application-architecture-c44afed31406)\n* [Simplicity by Distributing Complexity](https://engineering.zalando.com/posts/2018/01/simplicity-by-distributing-complexity.html)\n* [Why Over-Reusing is Bad](http://tech.transferwise.com/why-over-reusing-is-bad/)\n* [Performance is a Feature](https://blog.codinghorror.com/performance-is-a-feature/)\n* [Make Performance Part of Your Workflow](https://codeascraft.com/2014/12/11/make-performance-part-of-your-workflow/)\n* [The Benefits of Server Side Rendering over Client Side Rendering](https://medium.com/walmartlabs/the-benefits-of-server-side-rendering-over-client-side-rendering-5d07ff2cefe8)\n* [Automate and Abstract: Lessons at Facebook](https://architecht.io/lessons-from-facebook-on-engineering-for-scale-f5716f0afc7a)\n* [AWS Do''s and Don''ts](https://8thlight.com/blog/sarah-sunday/2017/09/15/aws-dos-and-donts.html)\n* [(UI) Design Doesnâ€™t Scale - Stanley Wood, Design Director at Spotify](https://medium.com/@hellostanley/design-doesnt-scale-4d81e12cbc3e)\n* [Linux Performance](http://www.brendangregg.com/linuxperf.html)\n* [Building Fast and Resilient Web Applications - Ilya Grigorik](https://www.igvita.com/2016/05/20/building-fast-and-resilient-web-applications/)\n* [Accept Partial Failures, Minimize Service Loss](https://www.usenix.org/conference/srecon17asia/program/presentation/wang_daxin)\n* [Design for Resiliency](http://highscalability.com/blog/2012/12/31/designing-for-resiliency-will-be-so-2013.html)\n* [Design for Self-healing](https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/self-healing)\n* [Design for Scaling Out](https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/scale-out)	\n* [Design for Evolution](https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/design-for-evolution)\n* [Learn from Mistakes](http://highscalability.com/blog/2013/8/26/reddit-lessons-learned-from-mistakes-made-scaling-to-1-billi.html)\n\n## Scalability\n* [Microservices and Orchestration](https://martinfowler.com/microservices/)\n	* [Domain-Oriented Microservice Architecture at Uber](https://eng.uber.com/microservice-architecture/)\n	* [Service Architecture (3 parts: Domain Gateways, Value-Added Services, BFF) at SoundCloud](https://developers.soundcloud.com/blog/service-architecture-3)\n	* [Container (8 parts) at Riot Games](https://engineering.riotgames.com/news/thinking-inside-container)\n	* [Containerization at Pinterest](https://medium.com/@Pinterest_Engineering/containerization-at-pinterest-92295347f2f3)\n	* [Evolution of Container Usage at Netflix](https://medium.com/netflix-techblog/the-evolution-of-container-usage-at-netflix-3abfc096781b)\n	* [Dockerizing MySQL at Uber](https://eng.uber.com/dockerizing-mysql/)\n	* [Testing of Microservices at Spotify](https://labs.spotify.com/2018/01/11/testing-of-microservices/)\n	* [Docker in Production at Treehouse](https://medium.com/treehouse-engineering/lessons-learned-running-docker-in-production-5dce99ece770)\n	* [Microservice at SoundCloud](https://developers.soundcloud.com/blog/inside-a-soundcloud-microservice)\n	* [Operate Kubernetes Reliably at Stripe](https://stripe.com/blog/operating-kubernetes)\n	* [Cross-Cluster Traffic Mirroring with Istio at Trivago](https://tech.trivago.com/2020/06/10/cross-cluster-traffic-mirroring-with-istio/)\n	* [Agrarian-Scale Kubernetes (3 parts) at New York Times](https://open.nytimes.com/agrarian-scale-kubernetes-part-3-ee459887ed7e)\n	* [Nanoservices at BBC](https://medium.com/bbc-design-engineering/powering-bbc-online-with-nanoservices-727840ba015b)\n	* [PowerfulSeal: Testing Tool for Kubernetes Clusters at Bloomberg](https://www.techatbloomberg.com/blog/powerfulseal-testing-tool-kubernetes-clusters/)\n	* [Conductor: Microservices Orchestrator at Netflix](https://medium.com/netflix-techblog/netflix-conductor-a-microservices-orchestrator-2e8d4771bf40)\n	* [Docker Containers that Power Over 100.000 Online Shops at Shopify](https://shopifyengineering.myshopify.com/blogs/engineering/docker-at-shopify-how-we-built-containers-that-power-over-100-000-online-shops)\n	* [Microservice Architecture at Medium](https://medium.engineering/microservice-architecture-at-medium-9c33805eb74f)\n	* [From bare-metal to Kubernetes at Betabrand](https://boxunix.com/post/bare_metal_to_kube/)\n	* [Kubernetes at Tinder](https://medium.com/tinder-engineering/tinders-move-to-kubernetes-cda2a6372f44)\n	* [Kubernetes at Quora](https://www.quora.com/q/quoraengineering/Adopting-Kubernetes-at-Quora)	\n	* [Kubernetes Platform at Pinterest](https://medium.com/pinterest-engineering/building-a-kubernetes-platform-at-pinterest-fb3d9571c948)\n	* [Microservices at Nubank](https://medium.com/building-nubank/microservices-at-nubank-an-overview-2ebcb336c64d)\n	* [Payment Transaction Management in Microservices at Mercari](https://engineering.mercari.com/en/blog/entry/20210831-2019-06-07-155849/)\n	* [Service Mesh at Snap](https://eng.snap.com/monolith-to-multicloud-microservices-snap-service-mesh)\n	* [GRIT: Protocol for Distributed Transactions across Microservices at eBay](https://tech.ebayinc.com/engineering/grit-a-protocol-for-distributed-transactions-across-microservices/)\n	* [Rubix: Kubernetes at Palantir](https://medium.com/palantir/introducing-rubix-kubernetes-at-palantir-ab0ce16ea42e)\n	* [CRISP: Critical Path Analysis for Microservice Architectures at Uber](https://eng.uber.com/crisp-critical-path-analysis-for-microservice-architectures/)\n* [Distributed Caching](https://www.wix.engineering/post/scaling-to-100m-to-cache-or-not-to-cache)\n	* [EVCache: Distributed In-memory Caching at Netflix](https://medium.com/netflix-techblog/caching-for-a-global-netflix-7bcc457012f1)\n	* [EVCache Cache Warmer Infrastructure at Netflix](https://medium.com/netflix-techblog/cache-warming-agility-for-a-stateful-service-2d3b1da82642)\n	* [Memsniff: Robust Memcache Traffic Analyzer at Box](https://blog.box.com/blog/introducing-memsniff-robust-memcache-traffic-analyzer/)\n	* [Caching with Consistent Hashing and Cache Smearing at Etsy](https://codeascraft.com/2017/11/30/how-etsy-caches/)\n	* [Analysis of Photo Caching at Facebook](https://code.facebook.com/posts/220956754772273/an-analysis-of-facebook-photo-caching/)\n	* [Cache Efficiency Exercise at Facebook](https://code.facebook.com/posts/964122680272229/web-performance-cache-efficiency-exercise/)\n	* [tCache: Scalable Data-aware Java Caching at Trivago](http://tech.trivago.com/2015/10/15/tcache/)\n	* [Pycache: In-process Caching at Quora](https://quoraengineering.quora.com/Pycache-lightning-fast-in-process-caching)\n	* [Reduce Memcached Memory Usage by 50% at Trivago](http://tech.trivago.com/2017/12/19/how-trivago-reduced-memcached-memory-usage-by-50/)\n	* [Caching Internal Service Calls at Yelp](https://engineeringblog.yelp.com/2018/03/caching-internal-service-calls-at-yelp.html)\n	* [Estimating the Cache Efficiency using Big Data at Allegro](https://allegro.tech/2017/01/estimating-the-cache-efficiency-using-big-data.html)\n	* [Distributed Cache at Zalando](https://engineering.zalando.com/posts/2018/04/distributed-cache-akka-kubernetes.html)\n	* [Distributed Cache for S3 at ClickHouse](https://clickhouse.com/blog/building-a-distributed-cache-for-s3)\n	* [Application Data Caching from RAM to SSD at NetFlix](https://medium.com/netflix-techblog/evolution-of-application-data-caching-from-ram-to-ssd-a33d6fa7a690)\n	* [Tradeoffs of Replicated Cache at Skyscanner](https://medium.com/@SkyscannerEng/the-tradeoffs-of-a-replicated-cache-b6680c722f58)\n	* [Location Caching with Quadtrees at Yext](http://engblog.yext.com/post/geolocation-caching)\n	* [Video Metadata Caching at Vimeo](https://medium.com/vimeo-engineering-blog/video-metadata-caching-at-vimeo-a54b25f0b304)\n	* [Scaling Redis at Twitter](http://highscalability.com/blog/2014/9/8/how-twitter-uses-redis-to-scale-105tb-ram-39mm-qps-10000-ins.html)\n	* [Scaling Job Queue with Redis at Slack](https://slack.engineering/scaling-slacks-job-queue-687222e9d100)\n	* [Moving persistent data out of Redis at Github](https://githubengineering.com/moving-persistent-data-out-of-redis/)\n	* [Storing Hundreds of Millions of Simple Key-Value Pairs in Redis at Instagram](https://engineering.instagram.com/storing-hundreds-of-millions-of-simple-key-value-pairs-in-redis-1091ae80f74c)\n	* [Redis at Trivago](http://tech.trivago.com/2017/01/25/learn-redis-the-hard-way-in-production/)\n	* [Optimizing Redis Storage at Deliveroo](https://deliveroo.engineering/2017/01/19/optimising-membership-queries.html)\n	* [Memory Optimization in Redis at Wattpad](http://engineering.wattpad.com/post/23244724794/store-more-stuff-memory-optimization-in-redis)\n	* [Redis Fleet at Heroku](https://blog.heroku.com/rolling-redis-fleet)\n	* [Solving Remote Build Cache Misses (2 parts) at SoundCloud](https://developers.soundcloud.com/blog/gradle-remote-build-cache-misses-part-2)\n	* [Ratings & Reviews (2 parts) at Flipkart](https://blog.flipkart.tech/ratings-reviews-flipkart-part-2-574ab08e75cf)\n	* [Prefetch Caching of Items at eBay](https://tech.ebayinc.com/engineering/prefetch-caching-of-ebay-items/)\n	* [Cross-Region Caching Library at Wix](https://www.wix.engineering/post/how-we-built-a-cross-region-caching-library)\n	* [Improving Distributed Caching Performance and Efficiency at Pinterest](https://medium.com/pinterest-engineering/improving-distributed-caching-performance-and-efficiency-at-pinterest-92484b5fe39b)\n	* [Standardize and Improve Microservices Caching at DoorDash](https://doordash.engineering/2023/10/19/how-doordash-standardized-and-improved-microservices-caching/)\n    * [HTTP Caching and CDN](https://developer.mozilla.org/en-US/docs/Web/HTTP/Caching)\n        * [Zynga Geo Proxy: Reducing Mobile Game Latency at Zynga](https://www.zynga.com/blogs/engineering/zynga-geo-proxy-reducing-mobile-game-latency)\n        * [Google AMP at CondÃ© Nast](https://technology.condenast.com/story/the-why-and-how-of-google-amp-at-conde-nast)\n        * [A/B Tests on Hosting Infrastructure (CDNs) at Deliveroo](https://deliveroo.engineering/2016/09/19/ab-testing-cdns.html)\n        * [HAProxy with Kubernetes for User-facing Traffic at SoundCloud](https://developers.soundcloud.com/blog/how-soundcloud-uses-haproxy-with-kubernetes-for-user-facing-traffic)\n        * [Bandaid: Service Proxy at Dropbox](https://blogs.dropbox.com/tech/2018/03/meet-bandaid-the-dropbox-service-proxy/)\n		* [Service Workers at Slack](https://slack.engineering/service-workers-at-slack-our-quest-for-faster-boot-times-and-offline-support-3492cf79c88)\n		* [CDN Services at Spotify](https://labs.spotify.com/2020/02/24/how-spotify-aligned-cdn-services-for-a-lightning-fast-streaming-experience/)\n* [Distributed Locking](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)\n	* [Chubby: Lock Service for Loosely Coupled Distributed Systems at Google](https://blog.acolyer.org/2015/02/13/the-chubby-lock-service-for-loosely-coupled-distributed-systems/)\n	* [Distributed Locking at Uber](https://www.youtube.com/watch?v=MDuagr729aU)\n	* [Distributed Locks using Redis at GoSquared](https://engineering.gosquared.com/distributed-locks-using-redis)\n	* [ZooKeeper at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2018/zookeeper-at-twitter.html)\n	* [Eliminating Duplicate Queries using Distributed Locking at Chartio](https://chartio.com/blog/eliminating-duplicate-queries-using-distributed-locking/)\n* [Distributed Tracking, Tracing, and Measuring](https://www.oreilly.com/ideas/understanding-the-value-of-distributed-tracing)\n	* [Zipkin: Distributed Systems Tracing at Twitter](https://blog.twitter.com/engineering/en_us/a/2012/distributed-systems-tracing-with-zipkin.html)\n	* [Improve Zipkin Traces using Kubernetes Pod Metadata at SoundCloud](https://developers.soundcloud.com/blog/using-kubernetes-pod-metadata-to-improve-zipkin-traces)\n	* [Canopy: Scalable Distributed Tracing & Analysis at Facebook](https://www.infoq.com/presentations/canopy-scalable-tracing-analytics-facebook)\n	* [Pintrace: Distributed Tracing at Pinterest](https://medium.com/@Pinterest_Engineering/distributed-tracing-at-pinterest-with-new-open-source-tools-a4f8a5562f6b)\n	* [XCMetrics: All-in-One Tool for Tracking Xcode Build Metrics at Spotify](https://engineering.atspotify.com/2021/01/20/introducing-xcmetrics-our-all-in-one-tool-for-tracking-xcode-build-metrics/)\n	* [Real-time Distributed Tracing at LinkedIn](https://engineering.linkedin.com/distributed-service-call-graph/real-time-distributed-tracing-website-performance-and-efficiency)	\n	* [Tracking Service Infrastructure at Scale at Shopify](https://www.usenix.org/conference/srecon17americas/program/presentation/arthorne)	\n	* [Distributed Tracing at HelloFresh](https://engineering.hellofresh.com/scaling-hellofresh-distributed-tracing-7b182928247d)\n	* [Analyzing Distributed Trace Data at Pinterest](https://medium.com/@Pinterest_Engineering/analyzing-distributed-trace-data-6aae58919949)\n	* [Distributed Tracing at Uber](https://eng.uber.com/distributed-tracing/)\n	* [JVM Profiler: Tracing Distributed JVM Applications at Uber](https://eng.uber.com/jvm-profiler/)\n	* [Data Checking at Dropbox](https://www.usenix.org/conference/srecon17asia/program/presentation/mah)\n	* [Tracing Distributed Systems at Showmax](https://tech.showmax.com/2016/10/tracing-distributed-systems-at-showmax/)\n	* [osquery Across the Enterprise at Palantir](https://medium.com/@palantir/osquery-across-the-enterprise-3c3c9d13ec55)\n	* [StatsD at Etsy](https://codeascraft.com/2011/02/15/measure-anything-measure-everything/)\n* [Distributed Scheduling](https://www.csee.umbc.edu/courses/graduate/CMSC621/fall02/lectures/ch11.pdf)\n	* [Distributed Task Scheduling (3 parts) at PagerDuty](https://www.pagerduty.com/eng/distributed-task-scheduling-3/)\n    * [Building Cron at Google](https://landing.google.com/sre/sre-book/chapters/distributed-periodic-scheduling/)\n    * [Distributed Cron Architecture at Quora](https://quoraengineering.quora.com/Quoras-Distributed-Cron-Architecture)\n    * [Chronos: A Replacement for Cron at Airbnb](https://medium.com/airbnb-engineering/chronos-a-replacement-for-cron-f05d7d986a9d)\n    * [Scheduler at Nextdoor](https://engblog.nextdoor.com/we-don-t-run-cron-jobs-at-nextdoor-6f7f9cc62040)\n    * [Peloton: Unified Resource Scheduler for Diverse Cluster Workloads at Uber](https://eng.uber.com/peloton/)\n    * [Fenzo: OSS Scheduler for Apache Mesos Frameworks at Netflix](https://medium.com/netflix-techblog/fenzo-oss-scheduler-for-apache-mesos-frameworks-5c340e77e543)\n    * [Airflow - Workflow Orchestration](https://airflow.apache.org/)\n		* [Airflow at Airbnb](https://medium.com/airbnb-engineering/airflow-a-workflow-management-platform-46318b977fd8)\n		* [Airflow at Adyen](https://www.adyen.com/knowledge-hub/apache-airflow-at-adyen)\n		* [Airflow at Pandora](https://engineering.pandora.com/apache-airflow-at-pandora-1d7a844d68ee)\n        * [Airflow at Robinhood](https://medium.com/robinhood-engineering/why-robinhood-uses-airflow-aed13a9a90c8)\n        * [Airflow at Lyft](https://eng.lyft.com/running-apache-airflow-at-lyft-6e53bb8fccff)\n        * [Airflow at Drivy](https://drivy.engineering/airflow-architecture/)\n		* [Airflow at Grab](https://engineering.grab.com/experimentation-platform-data-pipeline)\n		* [Airflow at Adobe](https://medium.com/adobetech/adobe-experience-platform-orchestration-service-with-apache-airflow-952203723c0b)\n        * [Auditing Airflow Job Runs at Walmart](https://medium.com/walmartlabs/auditing-airflow-batch-jobs-73b45100045)\n        * [MaaT: DAG-based Distributed Task Scheduler at Alibaba](https://hackernoon.com/meet-maat-alibabas-dag-based-distributed-task-scheduler-7c9cf0c83438)\n        * [boundary-layer: Declarative Airflow Workflows at Etsy](https://www.etsy.com/codeascraft/boundary-layer-declarative-airflow-workflows)\n* [Distributed Monitoring and Alerting](https://www.oreilly.com/ideas/monitoring-distributed-systems)\n	* [Unicorn: Remediation System at eBay](https://www.ebayinc.com/stories/blogs/tech/unicorn-rheos-remediation-center/)\n	* [M3: Metrics and Monitoring Platform at Uber](https://eng.uber.com/optimizing-m3/)\n	* [Athena: Automated Build Health Management System at Dropbox](https://blogs.dropbox.com/tech/2019/05/athena-our-automated-build-health-management-system/)\n	* [Vortex: Monitoring Server Applications at Dropbox](https://blogs.dropbox.com/tech/2019/11/monitoring-server-applications-with-vortex/)	\n	* [Nuage: Cloud Management Service at LinkedIn](https://engineering.linkedin.com/blog/2019/solving-manageability-challenges-with-nuage)\n	* [Telltale: Application Monitoring at Netflix](https://netflixtechblog.com/telltale-netflix-application-monitoring-simplified-5c08bfa780ba)\n	* [ThirdEye: Monitoring Platform at LinkedIn](https://engineering.linkedin.com/blog/2019/06/smart-alerts-in-thirdeye--linkedins-real-time-monitoring-platfor)\n	* [Periskop: Exception Monitoring Service at SoundCloud](https://developers.soundcloud.com/blog/periskop-exception-monitoring-service)\n    * [Securitybot: Distributed Alerting Bot at Dropbox](https://blogs.dropbox.com/tech/2017/02/meet-securitybot-open-sourcing-automated-security-at-scale/)	\n    * [Monitoring System at Alibaba](https://www.usenix.org/conference/srecon18asia/presentation/xinchi)\n    * [Real User Monitoring at Dailymotion](https://medium.com/dailymotion/real-user-monitoring-1948375f8be5)\n    * [Alerting Ecosystem at Uber](https://eng.uber.com/observability-at-scale/)\n	* [Alerting Framework at Airbnb](https://medium.com/airbnb-engineering/alerting-framework-at-airbnb-35ba48df894f)\n	* [Alerting on Service-Level Objectives (SLOs) at SoundCloud](https://developers.soundcloud.com/blog/alerting-on-slos)\n    * [Job-based Forecasting Workflow for Observability Anomaly Detection at Uber](https://eng.uber.com/observability-anomaly-detection/)\n	* [Monitoring and Alert System using Graphite and Cabot at HackerEarth](http://engineering.hackerearth.com/2017/03/21/monitoring-and-alert-system-using-graphite-and-cabot/)\n    * [Observability (2 parts) at Twitter](https://blog.twitter.com/engineering/en_us/a/2016/observability-at-twitter-technical-overview-part-ii.html)\n    * [Distributed Security Alerting at Slack](https://slack.engineering/distributed-security-alerting-c89414c992d6)\n    * [Real-Time News Alerting at Bloomberg](https://www.infoq.com/presentations/news-alerting-bloomberg)\n	* [Data Pipeline Monitoring System at LinkedIn](https://engineering.linkedin.com/blog/2019/an-inside-look-at-linkedins-data-pipeline-monitoring-system-)\n	* [Monitoring and Observability at Picnic](https://blog.picnic.nl/monitoring-and-observability-at-picnic-684cefd845c4)\n* [Distributed Security](https://msdn.microsoft.com/en-us/library/cc767123.aspx)\n	* [Approach to Security at Scale at Dropbox](https://blogs.dropbox.com/tech/2018/02/security-at-scale-the-dropbox-approach/)\n	* [Aardvark and Repokid: AWS Least Privilege for Distributed, High-Velocity Development at Netflix](https://medium.com/netflix-techblog/introducing-aardvark-and-repokid-53b081bf3a7e)	\n	* [LISA: Distributed Firewall at LinkedIn](https://www.slideshare.net/MikeSvoboda/2017-lisa-linkedins-distributed-firewall-dfw)\n	* [Secure Infrastructure To Store Bitcoin In The Cloud at Coinbase](https://engineering.coinbase.com/how-coinbase-builds-secure-infrastructure-to-store-bitcoin-in-the-cloud-30a6504e40ba)\n	* [BinaryAlert: Real-time Serverless Malware Detection at Airbnb](https://medium.com/airbnb-engineering/binaryalert-real-time-serverless-malware-detection-ca44370c1b90)\n	* [Scalable IAM Architecture to Secure Access to 100 AWS Accounts at Segment](https://segment.com/blog/secure-access-to-100-aws-accounts/)\n	* [OAuth Audit Toolbox at Indeed](http://engineering.indeedblog.com/blog/2018/04/oaudit-toolbox/)\n	* [Active Directory Password Blacklisting at Yelp](https://engineeringblog.yelp.com/2018/04/ad-password-blacklisting.html)	\n	* [Syscall Auditing at Scale at Slack](https://slack.engineering/syscall-auditing-at-scale-e6a3ca8ac1b8)\n	* [Athenz: Fine-Grained, Role-Based Access Control at Yahoo](https://yahooeng.tumblr.com/post/160481899076/open-sourcing-athenz-fine-grained-role-based)\n	* [WebAuthn Support for Secure Sign In at Dropbox](https://blogs.dropbox.com/tech/2018/05/introducing-webauthn-support-for-secure-dropbox-sign-in/)\n	* [Security Development Lifecycle at Slack](https://slack.engineering/moving-fast-and-securing-things-540e6c5ae58a)\n	* [Unprivileged Container Builds at Kinvolk](https://kinvolk.io/blog/2018/04/towards-unprivileged-container-builds/)\n	* [Diffy: Differencing Engine for Digital Forensics in the Cloud at Netflix](https://medium.com/netflix-techblog/netflix-sirt-releases-diffy-a-differencing-engine-for-digital-forensics-in-the-cloud-37b71abd2698)\n	* [Detecting Credential Compromise in AWS at Netflix](https://medium.com/netflix-techblog/netflix-cloud-security-detecting-credential-compromise-in-aws-9493d6fd373a)\n	* [Scalable User Privacy at Spotify](https://labs.spotify.com/2018/09/18/scalable-user-privacy/)\n	* [AVA: Audit Web Applications at Indeed](https://engineering.indeedblog.com/blog/2018/09/application-scanning/)\n	* [TTL as a Service: Automatic Revocation of Stale Privileges at Yelp](https://engineeringblog.yelp.com/2018/11/ttl-as-a-service.html)\n	* [Enterprise Key Management at Slack](https://slack.engineering/engineering-dive-into-slack-enterprise-key-management-1fce471b178c)	\n	* [Scalability and Authentication at Twitch](https://blog.twitch.tv/en/2019/03/15/how-twitch-addresses-scalability-and-authentication/)\n	* [Edge Authentication and Token-Agnostic Identity Propagation at Netflix](https://netflixtechblog.com/edge-authentication-and-token-agnostic-identity-propagation-514e47e0b602)\n	* [Hardening Kubernetes Infrastructure with Cilium at Palantir](https://blog.palantir.com/hardening-palantirs-kubernetes-infrastructure-with-cilium-1c40d4c7ef0)\n	* [Improving Web Vulnerability Management through Automation at Lyft](https://eng.lyft.com/improving-web-vulnerability-management-through-automation-2631570d8415)\n	* [Clock Skew when Syncing Password Payloads at Drobbox](https://dropbox.tech/application/dropbox-passwords-clock-skew-payload-sync-merge)\n* [Distributed Messaging, Queuing, and Event Streaming](https://arxiv.org/pdf/1704.00411.pdf)\n	* [Cape: Event Stream Processing Framework at Dropbox](https://blogs.dropbox.com/tech/2017/05/introducing-cape/)\n	* [Brooklin: Distributed Service for Near Real-Time Data Streaming at LinkedIn](https://engineering.linkedin.com/blog/2019/brooklin-open-source)\n	* [Samza: Stream Processing System for Latency Insighs at LinkedIn](https://engineering.linkedin.com/blog/2018/04/samza-aeon--latency-insights-for-asynchronous-one-way-flows)	\n	* [Bullet: Forward-Looking Query Engine for Streaming Data at Yahoo](https://yahooeng.tumblr.com/post/161855616651/open-sourcing-bullet-yahoos-forward-looking)\n	* [EventHorizon: Tool for Watching Events Streaming at Etsy](https://codeascraft.com/2018/05/29/the-eventhorizon-saga/)\n	* [Qmessage: Distributed, Asynchronous Task Queue at Quora](https://quoraengineering.quora.com/Qmessage-Handling-Billions-of-Tasks-Per-Day)\n	* [Cherami: Message Queue System for Transporting Async Tasks at Uber](https://eng.uber.com/cherami/)\n	* [Dynein: Distributed Delayed Job Queueing System at Airbnb](https://medium.com/airbnb-engineering/dynein-building-a-distributed-delayed-job-queueing-system-93ab10f05f99)\n	* [Timestone: Queueing System for Non-Parallelizable Workloads at Netflix](https://netflixtechblog.com/timestone-netflixs-high-throughput-low-latency-priority-queueing-system-with-built-in-support-1abf249ba95f)\n	* [Messaging Service at Riot Games](https://engineering.riotgames.com/news/riot-messaging-service)\n	* [Messaging System Model at Dropbox](https://dropbox.tech/infrastructure/infrastructure-messaging-system-model-async-platform-evolution)\n	* [Debugging Production with Event Logging at Zillow](https://www.zillow.com/engineering/debugging-production-event-logging/)\n	* [Cross-platform In-app Messaging Orchestration Service at Netflix](https://medium.com/netflix-techblog/building-a-cross-platform-in-app-messaging-orchestration-service-86ba614f92d8)\n	* [Video Gatekeeper at Netflix](https://medium.com/netflix-techblog/re-architecting-the-video-gatekeeper-f7b0ac2f6b00)\n	* [Scaling Push Messaging for Millions of Devices at Netflix](https://www.infoq.com/presentations/neflix-push-messaging-scale)\n	* [Delaying Asynchronous Message Processing with RabbitMQ at Indeed](http://engineering.indeedblog.com/blog/2017/06/delaying-messages/)	\n	* [Benchmarking Streaming Computation Engines at Yahoo](https://yahooeng.tumblr.com/post/135321837876/benchmarking-streaming-computation-engines-at)\n	* [Improving Stream Data Quality With Protobuf Schema Validation at Deliveroo](https://deliveroo.engineering/2019/02/05/improving-stream-data-quality-with-protobuf-schema-validation.html)\n	* [Scaling Email Infrastructure at Medium](https://medium.engineering/scaling-email-infrastructure-for-medium-digest-254223c883b8)\n	* [Real-time Messaging at Slack](https://slack.engineering/real-time-messaging/)\n	* [Event Stream Database at Nike](https://medium.com/nikeengineering/moving-faster-with-aws-by-creating-an-event-stream-database-dedec8ca3eeb)\n	* [Event Tracking System at Udemy](https://medium.com/udemy-engineering/designing-the-new-event-tracking-system-at-udemy-a45e502216fd)\n    * [Event-Driven Messaging](https://martinfowler.com/articles/201701-event-driven.html)\n        * [Domain-Driven Design at Alibaba](https://medium.com/swlh/creating-coding-excellence-with-domain-driven-design-88f73d2232c3)\n        * [Domain-Driven Design at Weebly](https://medium.com/weebly-engineering/how-to-organize-your-monolith-before-breaking-it-into-services-69cbdb9248b0)\n        * [Domain-Driven Design at Moonpig](https://engineering.moonpig.com/development/modelling-for-domain-driven-design)\n        * [Scaling Event Sourcing for Netflix Downloads](https://www.infoq.com/presentations/netflix-scale-event-sourcing)\n        * [Scaling Event-Sourcing at Jet.com](https://medium.com/@eulerfx/scaling-event-sourcing-at-jet-9c873cac33b8)\n        * [Event Sourcing (2 parts) at eBay](https://www.ebayinc.com/stories/blogs/tech/event-sourcing-in-action-with-ebays-continuous-delivery-team/)\n		* [Event Sourcing at FREE NOW](https://medium.com/inside-freenow/event-sourcing-an-evolutionary-perspective-31e7387aa6f1)\n		* [Scalable content feed using Event Sourcing and CQRS patterns at Brainly](https://medium.com/engineering-brainly/scalable-content-feed-using-event-sourcing-and-cqrs-patterns-e09df98bf977)\n    * [Pub-Sub Messaging](https://aws.amazon.com/pub-sub-messaging/)\n		* [Pulsar: Pub-Sub Messaging at Scale at Yahoo](https://yahooeng.tumblr.com/post/150078336821/open-sourcing-pulsar-pub-sub-messaging-at-scale)\n		* [Wormhole: Pub-Sub System at Facebook](https://code.facebook.com/posts/188966771280871/wormhole-pub-sub-system-moving-data-through-space-and-time/)\n		* [MemQ: Cloud Native Pub-Sub System at Pinterest](https://medium.com/pinterest-engineering/memq-an-efficient-scalable-cloud-native-pubsub-system-4402695dd4e7)\n		* [Pub-Sub in Microservices at Netflix](https://medium.com/netflix-techblog/how-netflix-microservices-tackle-dataset-pub-sub-4a068adcc9a)\n	* [Kafka - Message Broker](https://martin.kleppmann.com/papers/kafka-debull15.pdf)	\n		* [Kafka at LinkedIn](https://engineering.linkedin.com/kafka/running-kafka-scale)\n		* [Kafka at Pinterest](https://medium.com/pinterest-engineering/how-pinterest-runs-kafka-at-scale-ff9c6f735be)\n		* [Kafka at Trello](https://tech.trello.com/why-we-chose-kafka/)	\n		* [Kafka at Salesforce](https://engineering.salesforce.com/how-apache-kafka-inspired-our-platform-events-architecture-2f351fe4cf63)\n		* [Kafka at The New York Times](https://open.nytimes.com/publishing-with-apache-kafka-at-the-new-york-times-7f0e3b7d2077)\n		* [Kafka at Yelp](https://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html)\n		* [Kafka at Criteo](https://medium.com/criteo-labs/upgrading-kafka-on-a-large-infra-3ee99f56e970)\n		* [Kafka on Kubernetes at Shopify](https://shopifyengineering.myshopify.com/blogs/engineering/running-apache-kafka-on-kubernetes-at-shopify)\n		* [Kafka on PaaSTA: Running Kafka on Kubernetes at Yelp (2 parts)](https://engineeringblog.yelp.com/2022/03/kafka-on-paasta-part-two.html)\n		* [Migrating Kafka''s Zookeeper with No Downtime at Yelp](https://engineeringblog.yelp.com/2019/01/migrating-kafkas-zookeeper-with-no-downtime.html)\n		* [Reprocessing and Dead Letter Queues with Kafka at Uber](https://eng.uber.com/reliable-reprocessing/)\n		* [Chaperone: Audit Kafka End-to-End at Uber](https://eng.uber.com/chaperone/)\n		* [Finding Kafka throughput limit in infrastructure at Dropbox](https://blogs.dropbox.com/tech/2019/01/finding-kafkas-throughput-limit-in-dropbox-infrastructure/)\n		* [Cost Orchestration at Walmart](https://medium.com/walmartlabs/cost-orchestration-at-walmart-f34918af67c4)\n		* [InfluxDB and Kafka to Scale to Over 1 Million Metrics a Second at Hulu](https://medium.com/hulu-tech-blog/how-hulu-uses-influxdb-and-kafka-to-scale-to-over-1-million-metrics-a-second-1721476aaff5)\n		* [Scaling Kafka to Support Data Growth at PayPal](https://medium.com/paypal-tech/scaling-kafka-to-support-paypals-data-growth-a0b4da420fab)\n	* [Stream Data Deduplication](https://en.wikipedia.org/wiki/Data_deduplication)\n		* [Exactly-once Semantics with Kafka](https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/)\n		* [Real-time Deduping at Tapjoy](http://eng.tapjoy.com/blog-list/real-time-deduping-at-scale)\n		* [Deduplication at Segment](https://segment.com/blog/exactly-once-delivery/)\n		* [Deduplication at Mail.Ru](https://medium.com/@andrewsumin/efficient-storage-how-we-went-down-from-50-pb-to-32-pb-99f9c61bf6b4)\n		* [Petabyte Scale Data Deduplication at Mixpanel](https://medium.com/mixpaneleng/petabyte-scale-data-deduplication-mixpanel-engineering-e808c70c99f8)\n* [Distributed Logging](https://blog.codinghorror.com/the-problem-with-logging/)\n	* [Logging at LinkedIn](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)\n	* [Scalable and Reliable Log Ingestion at Pinterest](https://medium.com/@Pinterest_Engineering/scalable-and-reliable-data-ingestion-at-pinterest-b921c2ee8754)\n	* [High-performance Replicated Log Service at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2015/building-distributedlog-twitter-s-high-performance-replicated-log-servic.html)\n	* [Logging Service with Spark at CERN Accelerator](https://databricks.com/blog/2017/12/14/the-architecture-of-the-next-cern-accelerator-logging-service.html)\n	* [Logging and Aggregation at Quora](https://quoraengineering.quora.com/Logging-and-Aggregation-at-Quora)\n	* [Collection and Analysis of Daemon Logs at Badoo](https://badoo.com/techblog/blog/2016/06/06/collection-and-analysis-of-daemon-logs-at-badoo/)\n	* [Log Parsing with Static Code Analysis at Palantir](https://medium.com/palantir/using-static-code-analysis-to-improve-log-parsing-18f0d1843965)		\n	* [Centralized Application Logging at eBay](https://tech.ebayinc.com/engineering/low-latency-and-high-throughput-cal-ingress/)\n	* [Enrich VPC Flow Logs at Hyper Scale to provide Network Insight at Netflix](https://netflixtechblog.com/hyper-scale-vpc-flow-logs-enrichment-to-provide-network-insight-e5f1db02910d)	\n	* [BookKeeper: Distributed Log Storage at Yahoo](https://yahooeng.tumblr.com/post/109908973316/bookkeeper-yahoos-distributed-log-storage-is)\n	* [LogDevice: Distributed Data Store for Logs at Facebook](https://code.facebook.com/posts/357056558062811/logdevice-a-distributed-data-store-for-logs/)\n	* [LogFeeder: Log Collection System at Yelp](https://engineeringblog.yelp.com/2018/03/introducing-logfeeder.html)\n	* [DBLog: Generic Change-Data-Capture Framework at Netflix](https://medium.com/netflix-techblog/dblog-a-generic-change-data-capture-framework-69351fb9099b)	\n* [Distributed Searching](http://nwds.cs.washington.edu/files/nwds/pdf/Distributed-WR.pdf)\n	* [Search Architecture at Instagram](https://instagram-engineering.com/search-architecture-eeb34a936d3a)\n	* [Search Architecture at eBay](http://www.cs.otago.ac.nz/homepages/andrew/papers/2017-8.pdf)\n	* [Search Architecture at Box](https://medium.com/box-tech-blog/scaling-box-search-using-lumos-22d9e0cb4175)\n	* [Search Discovery Indexing Platform at Coupang](https://medium.com/coupang-tech/the-evolution-of-search-discovery-indexing-platform-fa43e41305f9)\n	* [Universal Search System at Pinterest](https://medium.com/pinterest-engineering/building-a-universal-search-system-for-pinterest-e4cb03a898d4)\n	* [Improving Search Engine Efficiency by over 25% at eBay](https://www.ebayinc.com/stories/blogs/tech/making-e-commerce-search-faster/)	\n	* [Indexing and Querying Telemetry Logs with Lucene at Palantir](https://medium.com/palantir/indexing-and-querying-telemetry-logs-with-lucene-234c5ce3e5f3)\n	* [Query Understanding at TripAdvisor](https://www.tripadvisor.com/engineering/query-understanding-at-tripadvisor/)\n	* [Search Federation Architecture at LinkedIn (2018)](https://engineering.linkedin.com/blog/2018/03/search-federation-architecture-at-linkedin)\n	* [Search at Slack](https://slack.engineering/search-at-slack-431f8c80619e)\n	* [Search Engine at DoorDash](https://careersatdoordash.com/blog/introducing-doordashs-in-house-search-engine/)\n	* [Stability and Scalability for Search at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2022/stability-and-scalability-for-search)\n	* [Search Service at Twitter (2014)](https://blog.twitter.com/engineering/en_us/a/2014/building-a-complete-tweet-index.html)\n	* [Autocomplete Search (2 parts) at Traveloka](https://medium.com/traveloka-engineering/high-quality-autocomplete-search-part-2-d5b15bb0dadf)\n	* [Data-Driven Autocorrection System at Canva](https://product.canva.com/building-a-data-driven-autocorrection-system/)\n	* [Adapting Search to Indian Phonetics at Flipkart](https://blog.flipkart.tech/adapting-search-to-indian-phonetics-cdbe65259686)\n	* [Nautilus: Search Engine at Dropbox](https://blogs.dropbox.com/tech/2018/09/architecture-of-nautilus-the-new-dropbox-search-engine/)\n	* [Galene: Search Architecture of LinkedIn](https://engineering.linkedin.com/search/did-you-mean-galene)\n	* [Manas: High Performing Customized Search System at Pinterest](https://medium.com/@Pinterest_Engineering/manas-a-high-performing-customized-search-system-cf189f6ca40f)\n	* [Sherlock: Near Real Time Search Indexing at Flipkart](https://blog.flipkart.tech/sherlock-near-real-time-search-indexing-95519783859d)\n	* [Nebula: Storage Platform to Build Search Backends at Airbnb](https://medium.com/airbnb-engineering/nebula-as-a-storage-platform-to-build-airbnbs-search-backends-ecc577b05f06)\n	* [ELK (Elasticsearch, Logstash, Kibana) Stack](https://logz.io/blog/15-tech-companies-chose-elk-stack/)\n		* [Predictions in Real Time with ELK at Uber](https://eng.uber.com/elk/)\n		* [Building a scalable ELK stack at Envato](https://webuild.envato.com/blog/building-a-scalable-elk-stack/)\n		* [ELK at Robinhood](https://robinhood.engineering/taming-elk-4e1349f077c3)\n		* [Scaling Elasticsearch Clusters at Uber](https://www.infoq.com/presentations/uber-elasticsearch-clusters?utm_source=presentations_about_Case_Study&utm_medium=link&utm_campaign=Case_Study)\n		* [Elasticsearch Performance Tuning Practice at eBay](https://www.ebayinc.com/stories/blogs/tech/elasticsearch-performance-tuning-practice-at-ebay/)\n		* [Improve Performance using Elasticsearch Plugins (2 parts) at Tinder](https://medium.com/tinder-engineering/how-we-improved-our-performance-using-elasticsearch-plugins-part-2-b051da2ee85b)\n		* [Elasticsearch at Kickstarter](https://kickstarter.engineering/elasticsearch-at-kickstarter-db3c487887fc)\n		* [Log Parsing with Logstash and Google Protocol Buffers at Trivago](https://tech.trivago.com/2016/01/19/logstash_protobuf_codec/)\n		* [Fast Order Search using Data Pipeline and Elasticsearch at Yelp](https://engineeringblog.yelp.com/2018/06/fast-order-search.html)\n		* [Moving Core Business Search to Elasticsearch at Yelp](https://engineeringblog.yelp.com/2017/06/moving-yelps-core-business-search-to-elasticsearch.html)\n		* [Sharding out Elasticsearch at Vinted](http://engineering.vinted.com/2017/06/05/sharding-out-elasticsearch/)\n		* [Self-Ranking Search with Elasticsearch at Wattpad](http://engineering.wattpad.com/post/146216619727/self-ranking-search-with-elasticsearch-at-wattpad)\n		* [Vulcanizer: a library for operating Elasticsearch at Github](https://github.blog/2019-03-05-vulcanizer-a-library-for-operating-elasticsearch/)	\n* [Distributed Storage](http://highscalability.com/blog/2011/11/1/finding-the-right-data-solution-for-your-application-in-the.html)\n	* [In-memory Storage](https://medium.com/@denisanikin/what-an-in-memory-database-is-and-how-it-persists-data-efficiently-f43868cff4c1)\n		* [MemSQL Architecture - The Fast (MVCC, InMem, LockFree, CodeGen) And Familiar (SQL)](http://highscalability.com/blog/2012/8/14/memsql-architecture-the-fast-mvcc-inmem-lockfree-codegen-and.html)\n		* [Optimizing Memcached Efficiency at Quora](https://quoraengineering.quora.com/Optimizing-Memcached-Efficiency)\n		* [Real-Time Data Warehouse with MemSQL on Cisco UCS](https://blogs.cisco.com/datacenter/memsql)\n		* [Moving to MemSQL at Tapjoy](http://eng.tapjoy.com/blog-list/moving-to-memsql)\n		* [MemSQL and Kinesis for Real-time Insights at Disney](https://conferences.oreilly.com/strata/strata-ca/public/schedule/detail/68131)\n		* [MemSQL to Query Hundreds of Billions of Rows in a Dashboard at Pandora](https://engineering.pandora.com/using-memsql-at-pandora-79a86cb09b57)\n	* [Object Storage](http://www.datacenterknowledge.com/archives/2013/10/04/object-storage-the-future-of-scale-out)\n		* [Scaling HDFS at Uber](https://eng.uber.com/scaling-hdfs/)\n		* [Reasons for Choosing S3 over HDFS at Databricks](https://databricks.com/blog/2017/05/31/top-5-reasons-for-choosing-s3-over-hdfs.html)\n		* [File System on Amazon S3 at Quantcast](https://www.quantcast.com/blog/quantcast-file-system-on-amazon-s3/)\n		* [Image Recovery at Scale Using S3 Versioning at Trivago](https://tech.trivago.com/2018/09/03/efficient-image-recovery-at-scale-using-amazon-s3-versioning/)\n		* [Cloud Object Store at Yahoo](https://yahooeng.tumblr.com/post/116391291701/yahoo-cloud-object-store-object-storage-at)\n		* [Ambry: Distributed Immutable Object Store at LinkedIn](https://www.usenix.org/conference/srecon17americas/program/presentation/shenoy)\n		* [Dynamometer: Scale Testing HDFS on Minimal Hardware with Maximum Fidelity at LinkedIn](https://engineering.linkedin.com/blog/2018/02/dynamometer--scale-testing-hdfs-on-minimal-hardware-with-maximum)\n		* [Hammerspace: Persistent, Concurrent, Off-heap Storage at Airbnb](https://medium.com/airbnb-engineering/hammerspace-persistent-concurrent-off-heap-storage-3db39bb04472)\n		* [MezzFS: Mounting Object Storage in Media Processing Platform at Netflix](https://medium.com/netflix-techblog/mezzfs-mounting-object-storage-in-netflixs-media-processing-platform-cda01c446ba)	\n		* [Magic Pocket: In-house Multi-exabyte Storage System at Dropbox](https://blogs.dropbox.com/tech/2016/05/inside-the-magic-pocket/)\n* [Relational Databases](https://www.mysql.com/products/cluster/scalability.html)\n	* [MySQL at Uber](https://www.uber.com/en-SG/blog/mysql-at-uber/)\n	* [MySQL at Pinterest](https://medium.com/@Pinterest_Engineering/learn-to-stop-using-shiny-new-things-and-love-mysql-3e1613c2ce14)\n	* [PostgreSQL at Twitch](https://blog.twitch.tv/en/2016/10/11/how-twitch-uses-postgresql-c34aa9e56f58)\n	* [Scaling MySQL-based Financial Reporting System at Airbnb](https://medium.com/airbnb-engineering/tracking-the-money-scaling-financial-reporting-at-airbnb-6d742b80f040)\n	* [Scaling MySQL at Wix](https://www.wix.engineering/post/scaling-to-100m-mysql-is-a-better-nosql)\n	* [Building and Deploying MySQL Raft at Meta](https://engineering.fb.com/2023/05/16/data-infrastructure/mysql-raft-meta/)\n	* [MaxScale (MySQL) Database Proxy at Airbnb](https://medium.com/airbnb-engineering/unlocking-horizontal-scalability-in-our-web-serving-tier-d907449cdbcf)\n	* [Switching from Postgres to MySQL at Uber](https://www.uber.com/en-NL/blog/postgres-to-mysql-migration/)\n	* [Handling Growth with Postgres at Instagram](https://engineering.instagram.com/handling-growth-with-postgres-5-tips-from-instagram-d5d7e7ffdfcb)\n	* [Scaling the Analytics Database (Postgres) at TransferWise](http://tech.transferwise.com/scaling-our-analytics-database/)\n	* [Updating a 50 Terabyte PostgreSQL Database at Adyen](https://medium.com/adyen/updating-a-50-terabyte-postgresql-database-f64384b799e7)\n	* [Scaling Database Access for 100s of Billions of Queries per Day at PayPal](https://medium.com/paypal-engineering/scaling-database-access-for-100s-of-billions-of-queries-per-day-paypal-introducing-hera-e192adacda54)\n	* [Minimizing Read-Write MySQL Downtime at Yelp](https://engineeringblog.yelp.com/2020/11/minimizing-read-write-mysql-downtime.html)\n	* [Migrating MySQL from 5.6 to 8.0 at Facebook](https://engineering.fb.com/2021/07/22/data-infrastructure/mysql/)\n	* [Migration from HBase to MyRocks at Quora](https://quoraengineering.quora.com/Migration-from-HBase-to-MyRocks-at-Quora)\n	* [Replication](https://docs.microsoft.com/en-us/sql/relational-databases/replication/types-of-replication)\n		* [MySQL Parallel Replication (4 parts) at Booking.com](https://medium.com/booking-com-infrastructure/evaluating-mysql-parallel-replication-part-4-annex-under-the-hood-eb456cf8b2fb)\n		* [Mitigating MySQL Replication Lag and Reducing Read Load at Github](https://githubengineering.com/mitigating-replication-lag-and-reducing-read-load-with-freno/)\n		* [Read Consistency with Database Replicas at Shopify](https://shopify.engineering/read-consistency-database-replicas)\n		* [Black-Box Auditing: Verifying End-to-End Replication Integrity between MySQL and Redshift at Yelp](https://engineeringblog.yelp.com/2018/04/black-box-auditing.html)\n		* [Partitioning Main MySQL Database at Airbnb](https://medium.com/airbnb-engineering/how-we-partitioned-airbnb-s-main-database-in-two-weeks-55f7e006ff21)\n		* [Herb: Multi-DC Replication Engine for Schemaless Datastore at Uber](https://eng.uber.com/herb-datacenter-replication/)\n	* [Sharding](https://quabase.sei.cmu.edu/mediawiki/index.php/Shard_data_set_across_multiple_servers_(Range-based))\n		* [Sharding MySQL at Pinterest](https://medium.com/@Pinterest_Engineering/sharding-pinterest-how-we-scaled-our-mysql-fleet-3f341e96ca6f)\n		* [Sharding MySQL at Twilio](https://www.twilio.com/engineering/2014/06/26/how-we-replaced-our-data-pipeline-with-zero-downtime)\n		* [Sharding MySQL at Square](https://medium.com/square-corner-blog/sharding-cash-10280fa3ef3b)\n		* [Sharding MySQL at Quora](https://www.quora.com/q/quoraengineering/MySQL-sharding-at-Quora)\n		* [Sharding Layer of Schemaless Datastore at Uber](https://eng.uber.com/schemaless-rewrite/)\n		* [Sharding & IDs at Instagram](https://instagram-engineering.com/sharding-ids-at-instagram-1cf5a71e5a5c)\n		* [Sharding Postgres at Notion](https://www.notion.so/blog/sharding-postgres-at-notion)\n		* [Sharding Postgres at Figma](https://www.figma.com/blog/how-figmas-databases-team-lived-to-tell-the-scale/)\n		* [Solr: Improving Performance for Batch Indexing at Box](https://blog.box.com/blog/solr-improving-performance-batch-indexing/)	\n		* [Geosharded Recommendations (3 parts) at Tinder](https://medium.com/tinder-engineering/geosharded-recommendations-part-3-consistency-2d2cb2f0594b)\n		* [Scaling Services with Shard Manager at Facebook](https://engineering.fb.com/production-engineering/scaling-services-with-shard-manager/)\n	* [Presto the Distributed SQL Query Engine](https://research.fb.com/wp-content/uploads/2019/03/Presto-SQL-on-Everything.pdf?)\n		* [Presto at Pinterest](https://medium.com/@Pinterest_Engineering/presto-at-pinterest-a8bda7515e52)\n		* [Presto Infrastructure at Lyft](https://eng.lyft.com/presto-infrastructure-at-lyft-b10adb9db01)\n		* [Presto at Grab](https://engineering.grab.com/scaling-like-a-boss-with-presto)\n		* [Engineering Data Analytics with Presto and Apache Parquet at Uber](https://eng.uber.com/presto/)\n		* [Data Wrangling at Slack](https://slack.engineering/data-wrangling-at-slack-f2e0ff633b69)\n		* [Presto in Big Data Platform on AWS at Netflix](https://medium.com/netflix-techblog/using-presto-in-our-big-data-platform-on-aws-938035909fd4)\n		* [Presto Auto Scaling at Eventbrite](https://www.eventbrite.com/engineering/big-data-workloads-presto-auto-scaling/)\n		* [Speed Up Presto with Alluxio Local Cache at Uber](https://www.uber.com/en-MY/blog/speed-up-presto-with-alluxio-local-cache/)\n* [NoSQL Databases](https://www.thoughtworks.com/insights/blog/nosql-databases-overview)\n	* [Key-Value Databases](http://www.cs.ucsb.edu/~agrawal/fall2009/dynamo.pdf)\n		* [DynamoDB at Nike](https://medium.com/nikeengineering/becoming-a-nimble-giant-how-dynamo-db-serves-nike-at-scale-4cc375dbb18e)\n		* [DynamoDB at Segment](https://segment.com/blog/the-million-dollar-eng-problem/)\n		* [DynamoDB at Mapbox](https://blog.mapbox.com/scaling-mapbox-infrastructure-with-dynamodb-streams-d53eabc5e972)\n		* [Manhattan: Distributed Key-Value Database at Twitter](https://blog.twitter.com/engineering/en_us/a/2014/manhattan-our-real-time-multi-tenant-distributed-database-for-twitter-scale.html)\n		* [Sherpa: Distributed NoSQL Key-Value Store at Yahoo](https://yahooeng.tumblr.com/post/120730204806/sherpa-scales-new-heights)\n		* [HaloDB: Embedded Key-Value Storage Engine at Yahoo](https://yahooeng.tumblr.com/post/178262468576/introducing-halodb-a-fast-embedded-key-value)\n		* [MPH: Fast and Compact Immutable Key-Value Stores at Indeed](http://engineering.indeedblog.com/blog/2018/02/indeed-mph/)\n		* [Venice: Distributed Key-Value Database at Linkedin](https://engineering.linkedin.com/blog/2017/02/building-venice-with-apache-helix)\n	* [Columnar Databases](https://aws.amazon.com/nosql/columnar/)\n		* [Cassandra](http://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf)\n			* [Cassandra at Instagram](https://www.slideshare.net/DataStax/cassandra-at-instagram-2016)\n			* [Storing Images in Cassandra at Walmart](https://medium.com/walmartlabs/building-object-store-storing-images-in-cassandra-walmart-scale-a6b9c02af593)\n			* [Storing Messages with Cassandra at Discord](https://blog.discordapp.com/how-discord-stores-billions-of-messages-7fa6ec7ee4c7)\n			* [Scaling Cassandra Cluster at Walmart](https://medium.com/walmartlabs/avoid-pitfalls-in-scaling-your-cassandra-cluster-lessons-and-remedies-a71ca01f8c04)\n			* [Scaling Ad Analytics with Cassandra at Yelp](https://engineeringblog.yelp.com/2016/08/how-we-scaled-our-ad-analytics-with-cassandra.html)\n			* [Scaling to 100+ Million Reads/Writes using Spark and Cassandra at Dream11](https://medium.com/dream11-tech-blog/leaderboard-dream11-4efc6f93c23e)		\n			* [Moving Food Feed from Redis to Cassandra at Zomato](https://www.zomato.com/blog/how-we-moved-our-food-feed-from-redis-to-cassandra)\n			* [Benchmarking Cassandra Scalability on AWS at Netflix](https://medium.com/netflix-techblog/benchmarking-cassandra-scalability-on-aws-over-a-million-writes-per-second-39f45f066c9e)\n			* [Service Decomposition at Scale with Cassandra at Intuit QuickBooks](https://quickbooks-engineering.intuit.com/service-decomposition-at-scale-70405ac2f637)\n			* [Cassandra for Keeping Counts In Sync at SoundCloud](https://developers.soundcloud.com/blog/keeping-counts-in-sync)\n			* [Cassandra Driver Configuration for Improved Performance and Load Balancing at Glassdoor](https://medium.com/glassdoor-engineering/cassandra-driver-configuration-for-improved-performance-and-load-balancing-1b0106ce12bb)\n			* [cstar: Cassandra Orchestration Tool at Spotify](https://labs.spotify.com/2018/09/04/introducing-cstar-the-spotify-cassandra-orchestration-tool-now-open-source/)\n		* [HBase](https://hbase.apache.org/)\n			* [HBase at Salesforce](https://engineering.salesforce.com/investing-in-big-data-apache-hbase-b9d98661a66b)\n			* [HBase in Facebook Messages](https://www.facebook.com/notes/facebook-engineering/the-underlying-technology-of-messages/454991608919/)\n			* [HBase in Imgur Notification](https://blog.imgur.com/2015/09/15/tech-tuesday-imgur-notifications-from-mysql-to-hbase/)\n			* [Improving HBase Backup Efficiency at Pinterest](https://medium.com/@Pinterest_Engineering/improving-hbase-backup-efficiency-at-pinterest-86159da4b954)\n			* [HBase at Xiaomi](https://www.slideshare.net/HBaseCon/hbase-practice-at-xiaomi)\n		* [Redshift](https://www.allthingsdistributed.com/2018/11/amazon-redshift-performance-optimization.html)\n			* [Redshift at GIPHY](https://engineering.giphy.com/scaling-redshift-without-scaling-costs/)\n			* [Redshift at Hudl](https://www.hudl.com/bits/the-low-hanging-fruit-of-redshift-performance)\n			* [Redshift at Drivy](https://drivy.engineering/redshift_tips_ticks_part_1/)\n	* [Document Databases](https://msdn.microsoft.com/en-us/magazine/hh547103.aspx)\n		* [eBay: Building Mission-Critical Multi-Data Center Applications with MongoDB](https://www.mongodb.com/blog/post/ebay-building-mission-critical-multi-data-center-applications-with-mongodb)\n		* [MongoDB at Baidu: Multi-Tenant Cluster Storing 200+ Billion Documents across 160 Shards](https://www.mongodb.com/blog/post/mongodb-at-baidu-powering-100-apps-across-600-nodes-at-pb-scale)\n		* [Migrating Mongo Data at Addepar](https://medium.com/build-addepar/migrating-mountains-of-mongo-data-63e530539952)\n		* [The AWS and MongoDB Infrastructure of Parse (acquired by Facebook)](https://medium.baqend.com/parse-is-gone-a-few-secrets-about-their-infrastructure-91b3ab2fcf71)\n		* [Migrating Mountains of Mongo Data at Addepar](https://medium.com/build-addepar/migrating-mountains-of-mongo-data-63e530539952)\n		* [Couchbase Ecosystem at LinkedIn](https://engineering.linkedin.com/blog/2017/12/couchbase-ecosystem-at-linkedin)\n		* [SimpleDB at Zendesk](https://medium.com/zendesk-engineering/resurrecting-amazon-simpledb-9404034ec506)\n		* [Espresso: Distributed Document Store at LinkedIn](https://engineering.linkedin.com/espresso/introducing-espresso-linkedins-hot-new-distributed-document-store)\n	* [Graph Databases](https://www.eecs.harvard.edu/margo/papers/systor13-bench/)\n		* [FlockDB: Distributed Graph Database at Twitter](https://blog.twitter.com/engineering/en_us/a/2010/introducing-flockdb.html)\n		* [TAO: Distributed Data Store for the Social Graph at Facebook](https://www.cs.cmu.edu/~pavlo/courses/fall2013/static/papers/11730-atc13-bronson.pdf)\n		* [Akutan: Distributed Knowledge Graph Store at eBay](https://tech.ebayinc.com/engineering/akutan-a-distributed-knowledge-graph-store/)\n* [Time Series Databases](https://www.influxdata.com/time-series-database/)\n	* [Beringei: High-performance Time Series Storage Engine at Facebook](https://code.facebook.com/posts/952820474848503/beringei-a-high-performance-time-series-storage-engine/)\n	* [MetricsDB: TimeSeries Database for storing metrics at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2019/metricsdb.html)	\n	* [Atlas: In-memory Dimensional Time Series Database at Netflix](https://medium.com/netflix-techblog/introducing-atlas-netflixs-primary-telemetry-platform-bd31f4d8ed9a)\n	* [Heroic: Time Series Database at Spotify](https://labs.spotify.com/2015/11/17/monitoring-at-spotify-introducing-heroic/)\n	* [Roshi: Distributed Storage System for Time-Series Event at SoundCloud](https://developers.soundcloud.com/blog/roshi-a-crdt-system-for-timestamped-events)\n	* [Goku: Time Series Database at Pinterest](https://medium.com/@Pinterest_Engineering/goku-building-a-scalable-and-high-performant-time-series-database-system-a8ff5758a181)\n	* [Scaling Time Series Data Storage (2 parts) at Netflix](https://medium.com/netflix-techblog/scaling-time-series-data-storage-part-ii-d67939655586)\n	* [Time Series Data Abstraction Layer at Netflix](https://netflixtechblog.com/introducing-netflix-timeseries-data-abstraction-layer-31552f6326f8)\n	* [Druid - Real-time Analytics Database](https://druid.apache.org/)\n		* [Druid at Airbnb](https://medium.com/airbnb-engineering/druid-airbnb-data-platform-601c312f2a4c)\n		* [Druid at Walmart](https://medium.com/walmartlabs/event-stream-analytics-at-walmart-with-druid-dcf1a37ceda7)\n		* [Druid at eBay](https://tech.ebayinc.com/engineering/monitoring-at-ebay-with-druid/)\n		* [Druid at Netflix](https://netflixtechblog.com/how-netflix-uses-druid-for-real-time-insights-to-ensure-a-high-quality-experience-19e1e8568d06)\n* [Distributed Repositories, Dependencies, and Configurations Management](https://betterexplained.com/articles/intro-to-distributed-version-control-illustrated/)\n	* [DGit: Distributed Git at Github](https://githubengineering.com/introducing-dgit/)\n	* [Stemma: Distributed Git Server at Palantir](https://medium.com/@palantir/stemma-distributed-git-server-70afbca0fc29)\n	* [Configuration Management for Distributed Systems at Flickr](https://code.flickr.net/2016/03/24/configuration-management-for-distributed-systems-using-github-and-cfg4j/)\n	* [Git Repository at Microsoft](https://blogs.msdn.microsoft.com/bharry/2017/05/24/the-largest-git-repo-on-the-planet/)\n	* [Solve Git Problem with Large Repositories at Microsoft](https://www.infoq.com/news/2017/02/GVFS)	\n	* [Single Repository at Google](https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext)	\n	* [Scaling Infrastructure and (Git) Workflow at Adyen](https://medium.com/adyen/from-0-100-billion-scaling-infrastructure-and-workflow-at-adyen-7b63b690dfb6)	\n	* [Dotfiles Distribution at Booking.com](https://medium.com/booking-com-infrastructure/dotfiles-distribution-dedb69c66a75)\n	* [Secret Detector: Preventing Secrets in Source Code at Yelp](https://engineeringblog.yelp.com/2018/06/yelps-secret-detector.html)\n	* [Managing Software Dependency at Scale at LinkedIn](https://engineering.linkedin.com/blog/2018/09/managing-software-dependency-at-scale)\n	* [Merging Code in High-velocity Repositories at LinkedIn](https://engineering.linkedin.com/blog/2020/continuous-integration)\n	* [Dynamic Configuration at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2018/dynamic-configuration-at-twitter.html)\n	* [Dynamic Configuration at Mixpanel](https://medium.com/mixpaneleng/dynamic-configuration-at-mixpanel-94bfcf97d6b8)\n	* [Dynamic Configuration at GoDaddy](https://sg.godaddy.com/engineering/2019/03/06/dynamic-configuration-for-nodejs/)\n	* [Fleet Management (3 parts) at Spotify](https://engineering.atspotify.com/2023/5/fleet-management-at-spotify-part-3-fleet-wide-refactoring)\n* [Scaling Continuous Integration and Continuous Delivery](https://www.synopsys.com/blogs/software-security/agile-cicd-devops-glossary/)\n	* [Continuous Integration Stack at Facebook](https://code.fb.com/web/rapid-release-at-massive-scale/)\n	* [Continuous Integration with Distributed Repositories and Dependencies at Netflix](https://medium.com/netflix-techblog/towards-true-continuous-integration-distributed-repositories-and-dependencies-2a2e3108c051)\n	* [Continuous Integration and Deployment with Bazel at Dropbox](https://blogs.dropbox.com/tech/2019/12/continuous-integration-and-deployment-with-bazel/)\n	* [Adopting Bazel for Web at Airbnb](https://medium.com/airbnb-engineering/adopting-bazel-for-web-at-scale-a784b2dbe325)\n	* [Continuous Deployments at BuzzFeed](https://tech.buzzfeed.com/continuous-deployments-at-buzzfeed-d171f76c1ac4)\n	* [Screwdriver: Continuous Delivery Build System for Dynamic Infrastructure at Yahoo](https://yahooeng.tumblr.com/post/155765242061/open-sourcing-screwdriver-yahoos-continuous)\n	* [CI/CD at Betterment](https://www.betterment.com/resources/ci-cd-shortening-the-feedback-loop/)\n	* [CI/CD at Brainly](https://medium.com/engineering-brainly/ci-cd-at-scale-fdfb0f49e031)\n	* [Scaling iOS CI with Anka at Shopify](https://engineering.shopify.com/blogs/engineering/scaling-ios-ci-with-anka)\n	* [Scaling Jira Server at Yelp](https://engineeringblog.yelp.com/2019/04/Scaling-Jira-Server-Administration-For-The-Enterprise.html)\n	* [Auto-scaling CI/CD cluster at Flexport](https://flexport.engineering/how-flexport-halved-testing-costs-with-an-auto-scaling-ci-cd-cluster-8304297222f)\n\n## Availability\n* [Resilience Engineering: Learning to Embrace Failure](https://queue.acm.org/detail.cfm?id=2371297)	\n	* [Resilience Engineering with Project Waterbear at LinkedIn](https://engineering.linkedin.com/blog/2017/11/resilience-engineering-at-linkedin-with-project-waterbear)\n	* [Resiliency against Traffic Oversaturation at iHeartRadio](https://tech.iheart.com/resiliency-against-traffic-oversaturation-77c5ed92a5fb)\n	* [Resiliency in Distributed Systems at GO-JEK](https://blog.gojekengineering.com/resiliency-in-distributed-systems-efd30f74baf4)\n	* [Practical NoSQL Resilience Design Pattern for the Enterprise at eBay](https://www.ebayinc.com/stories/blogs/tech/practical-nosql-resilience-design-pattern-for-the-enterprise/)\n	* [Ensuring Resilience to Disaster at Quora](https://quoraengineering.quora.com/Ensuring-Quoras-Resilience-to-Disaster)\n	* [Site Resiliency at Expedia](https://www.infoq.com/presentations/expedia-website-resiliency?utm_source=presentations_about_Case_Study&utm_medium=link&utm_campaign=Case_Study)\n	* [Resiliency and Disaster Recovery with Kafka at eBay](https://tech.ebayinc.com/engineering/resiliency-and-disaster-recovery-with-kafka/)\n	* [Disaster Recovery for Multi-Region Kafka at Uber](https://eng.uber.com/kafka/)\n* [Failover](http://cloudpatterns.org/mechanisms/failover_system)\n	* [The Evolution of Global Traffic Routing and Failover](https://www.usenix.org/conference/srecon16/program/presentation/heady)\n	* [Testing for Disaster Recovery Failover Testing](https://www.usenix.org/conference/srecon17asia/program/presentation/liu_zehua)\n	* [Designing a Microservices Architecture for Failure](https://blog.risingstack.com/designing-microservices-architecture-for-failure/)\n	* [ELB for Automatic Failover at GoSquared](https://engineering.gosquared.com/use-elb-automatic-failover)\n	* [Eliminate the Database for Higher Availability at American Express](http://americanexpress.io/eliminate-the-database-for-higher-availability/)\n	* [Failover with Redis Sentinel at Vinted](http://engineering.vinted.com/2015/09/03/failover-with-redis-sentinel/)\n	* [High-availability SaaS Infrastructure at FreeAgent](http://engineering.freeagent.com/2017/02/06/ha-infrastructure-without-breaking-the-bank/)\n	* [MySQL High Availability at GitHub](https://github.blog/2018-06-20-mysql-high-availability-at-github/)\n	* [MySQL High Availability at Eventbrite](https://www.eventbrite.com/engineering/mysql-high-availability-at-eventbrite/)\n	* [Business Continuity & Disaster Recovery at Walmart](https://medium.com/walmartlabs/business-continuity-disaster-recovery-in-the-microservices-world-ef2adca363df)\n* [Load Balancing](https://blog.vivekpanyam.com/scaling-a-web-service-load-balancing/)\n	* [Introduction to Modern Network Load Balancing and Proxying](https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236)\n	* [Top Five (Load Balancing) Scalability Patterns](https://www.f5.com/company/blog/top-five-scalability-patterns)\n	* [Load Balancing infrastructure to support more than 1.3 billion users at Facebook](https://www.usenix.org/conference/srecon15europe/program/presentation/shuff)\n	* [DHCPLB: DHCP Load Balancer at Facebook](https://code.facebook.com/posts/1734309626831603/dhcplb-an-open-source-load-balancer/)\n	* [Katran: Scalable Network Load Balancer at Facebook](https://code.facebook.com/posts/1906146702752923/open-sourcing-katran-a-scalable-network-load-balancer/)\n	* [Deterministic Aperture: A Distributed, Load Balancing Algorithm at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2019/daperture-load-balancer.html)	\n	* [Load Balancing with Eureka at Netflix](https://medium.com/netflix-techblog/netflix-shares-cloud-load-balancing-and-failover-tool-eureka-c10647ef95e5)\n	* [Edge Load Balancing at Netflix](https://medium.com/netflix-techblog/netflix-edge-load-balancing-695308b5548c)\n	* [Zuul 2: Cloud Gateway at Netflix](https://medium.com/netflix-techblog/open-sourcing-zuul-2-82ea476cb2b3)\n	* [Load Balancing at Yelp](https://engineeringblog.yelp.com/2017/05/taking-zero-downtime-load-balancing-even-further.html)\n	* [Load Balancing at Github](https://githubengineering.com/introducing-glb/)\n	* [Consistent Hashing to Improve Load Balancing at Vimeo](https://medium.com/vimeo-engineering-blog/improving-load-balancing-with-a-new-consistent-hashing-algorithm-9f1bd75709ed)\n	* [UDP Load Balancing at 500 pixel](https://developers.500px.com/udp-load-balancing-with-keepalived-167382d7ad08)\n	* [QALM: QoS Load Management Framework at Uber](https://eng.uber.com/qalm/)	\n	* [Traffic Steering using Rum DNS at LinkedIn](https://www.usenix.org/conference/srecon17europe/program/presentation/rastogi)\n	* [Traffic Infrastructure (Edge Network) at Dropbox](https://blogs.dropbox.com/tech/2018/10/dropbox-traffic-infrastructure-edge-network/)\n	* [Intelligent DNS based load balancing at Dropbox](https://blogs.dropbox.com/tech/2020/01/intelligent-dns-based-load-balancing-at-dropbox/)\n	* [Monitor DNS systems at Stripe](https://stripe.com/en-sg/blog/secret-life-of-dns)\n	* [Multi-DNS Architecture (3 parts) at Monday](https://medium.com/monday-engineering/how-and-why-we-migrated-our-dns-from-cloudflare-to-a-multi-dns-architecture-part-3-584a470f4062)\n	* [Dynamic Anycast DNS Infrastructure at Hulu](https://medium.com/hulu-tech-blog/building-hulus-dynamic-anycast-dns-infrastructure-985a7a11fd30)\n* [Rate Limiting](https://www.keycdn.com/support/rate-limiting/)\n	* [Rate Limiting for Scaling to Millions of Domains at Cloudflare](https://blog.cloudflare.com/counting-things-a-lot-of-different-things/)\n	* [Cloud Bouncer: Distributed Rate Limiting at Yahoo](https://yahooeng.tumblr.com/post/111288877956/cloud-bouncer-distributed-rate-limiting-at-yahoo)\n	* [Scaling API with Rate Limiters at Stripe](https://stripe.com/blog/rate-limiters)\n	* [Distributed Rate Limiting at Allegro](https://allegro.tech/2017/04/hermes-max-rate.html)\n	* [Ratequeue: Core Queueing-And-Rate-Limiting System at Twilio](https://www.twilio.com/blog/2017/11/chaos-engineering-ratequeue-ha.html)\n	* [Quotas Service at Grab](https://engineering.grab.com/quotas-service)\n	* [Rate Limiting at Figma](https://medium.com/figma-design/an-alternative-approach-to-rate-limiting-f8a06cf7c94c)	\n* [Autoscaling](https://medium.com/@BotmetricHQ/top-11-hard-won-lessons-learned-about-aws-auto-scaling-5bfe56da755f)\n	* [Autoscaling Pinterest](https://medium.com/@Pinterest_Engineering/auto-scaling-pinterest-df1d2beb4d64)\n	* [Autoscaling Based on Request Queuing at Square](https://medium.com/square-corner-blog/autoscaling-based-on-request-queuing-c4c0f57f860f)\n	* [Autoscaling Jenkins at Trivago](http://tech.trivago.com/2017/02/17/your-definite-guide-for-autoscaling-jenkins/)\n	* [Autoscaling Pub-Sub Consumers at Spotify](https://labs.spotify.com/2017/11/20/autoscaling-pub-sub-consumers/)\n	* [Autoscaling Bigtable Clusters based on CPU Load at Spotify](https://labs.spotify.com/2018/12/18/bigtable-autoscaler-saving-money-and-time-using-managed-storage/)\n	* [Autoscaling AWS Step Functions Activities at Yelp](https://engineeringblog.yelp.com/2019/06/autoscaling-aws-step-functions-activities.html)\n	* [Scryer: Predictive Auto Scaling Engine at Netflix](https://medium.com/netflix-techblog/scryer-netflixs-predictive-auto-scaling-engine-a3f8fc922270)	\n	* [Bouncer: Simple AWS Auto Scaling Rollovers at Palantir](https://medium.com/palantir/bouncer-simple-aws-auto-scaling-rollovers-c5af601d65d4)\n	* [Clusterman: Autoscaling Mesos Clusters at Yelp](https://engineeringblog.yelp.com/2019/02/autoscaling-mesos-clusters-with-clusterman.html)\n* [Availability in Globally Distributed Storage Systems at Google](http://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/36737.pdf)	\n* [NodeJS High Availability at Yahoo](https://yahooeng.tumblr.com/post/68823943185/nodejs-high-availability)\n* [Operations (11 parts) at LinkedIn](https://www.linkedin.com/pulse/introduction-every-day-monday-operations-benjamin-purgason)\n* [Monitoring Powers High Availability for LinkedIn Feed](https://www.usenix.org/conference/srecon17americas/program/presentation/barot)\n* [Supporting Global Events at Facebook](https://code.facebook.com/posts/166966743929963/how-production-engineers-support-global-events-on-facebook/)\n* [High Availability at BlaBlaCar](https://medium.com/blablacar-tech/the-expendables-backends-high-availability-at-blablacar-8cea3b95b26b)\n* [High Availability at Netflix](https://medium.com/@NetflixTechBlog/tips-for-high-availability-be0472f2599c)\n* [High Availability Cloud Infrastructure at Twilio](https://www.twilio.com/engineering/2011/12/12/scaling-high-availablity-infrastructure-in-cloud)\n* [High Availability with Distributed DB on Kubernetes at Airbnb](https://airbnb.tech/infrastructure/achieving-high-availability-with-distributed-database-on-kubernetes-at-airbnb/)\n* [Automating Datacenter Operations at Dropbox](https://blogs.dropbox.com/tech/2019/01/automating-datacenter-operations-at-dropbox/)\n* [Globalizing Player Accounts at Riot Games](https://technology.riotgames.com/news/globalizing-player-accounts)\n\n## Stability\n* [Circuit Breaker](https://martinfowler.com/bliki/CircuitBreaker.html)\n	* [Circuit Breaking in Distributed Systems](https://www.infoq.com/presentations/circuit-breaking-distributed-systems)\n	* [Circuit Breaker for Scaling Containers](https://f5.com/about-us/blog/articles/the-art-of-scaling-containers-circuit-breakers-28919)\n	* [Lessons in Resilience at SoundCloud](https://developers.soundcloud.com/blog/lessons-in-resilience-at-SoundCloud)\n	* [Protector: Circuit Breaker for Time Series Databases at Trivago](http://tech.trivago.com/2016/02/23/protector/)\n	* [Improved Production Stability with Circuit Breakers at Heroku](https://blog.heroku.com/improved-production-stability-with-circuit-breakers)\n	* [Circuit Breaker at Zendesk](https://medium.com/zendesk-engineering/the-joys-of-circuit-breaking-ee6584acd687)\n	* [Circuit Breaker at Traveloka](https://medium.com/traveloka-engineering/circuit-breakers-dont-let-your-dependencies-bring-you-down-5ba1c5cf1eec)\n	* [Circuit Breaker at Shopify](https://shopify.engineering/circuit-breaker-misconfigured)\n* [Timeouts](https://www.javaworld.com/article/2824163/application-performance/stability-patterns-applied-in-a-restful-architecture.html)\n	* [Fault Tolerance (Timeouts and Retries, Thread Separation, Semaphores, Circuit Breakers) at Netflix](https://medium.com/netflix-techblog/fault-tolerance-in-a-high-volume-distributed-system-91ab4faae74a)\n	* [Enforce Timeout: A Reliability Methodology at DoorDash](https://doordash.engineering/2018/12/21/enforce-timeout-a-doordash-reliability-methodology/)\n	* [Troubleshooting a Connection Timeout Issue with tcp_tw_recycle Enabled at eBay](https://www.ebayinc.com/stories/blogs/tech/a-vip-connection-timeout-issue-caused-by-snat-and-tcp-tw-recycle/)\n* [Crash-safe Replication for MySQL at Booking.com](https://medium.com/booking-com-infrastructure/better-crash-safe-replication-for-mysql-a336a69b317f)\n* [Bulkheads: Partition and Tolerate Failure in One Part](https://skife.org/architecture/fault-tolerance/2009/12/31/bulkheads.html)\n* [Steady State: Always Put Logs on Separate Disk](https://docs.microsoft.com/en-us/sql/relational-databases/policy-based-management/place-data-and-log-files-on-separate-drives)\n* [Throttling: Maintain a Steady Pace](http://www.sosp.org/2001/papers/welsh.pdf)\n* [Multi-Clustering: Improving Resiliency and Stability of a Large-scale Monolithic API Service at LinkedIn](https://engineering.linkedin.com/blog/2017/11/improving-resiliency-and-stability-of-a-large-scale-api)\n* [Determinism (4 parts) in League of Legends Server](https://engineering.riotgames.com/news/determinism-league-legends-fixing-divergences)\n\n## Performance\n* [Performance Optimization on OS, Storage, Database, Network](https://stackify.com/application-performance-metrics/)\n	* [Improving Performance with Background Data Prefetching at Instagram](https://engineering.instagram.com/improving-performance-with-background-data-prefetching-b191acb39898)\n	* [Fixing Linux filesystem performance regressions at LinkedIn](https://engineering.linkedin.com/blog/2020/fixing-linux-filesystem-performance-regressions)\n	* [Compression Techniques to Solve Network I/O Bottlenecks at eBay](https://www.ebayinc.com/stories/blogs/tech/how-ebays-shopping-cart-used-compression-techniques-to-solve-network-io-bottlenecks/)\n	* [Optimizing Web Servers for High Throughput and Low Latency at Dropbox](https://blogs.dropbox.com/tech/2017/09/optimizing-web-servers-for-high-throughput-and-low-latency/)\n	* [Linux Performance Analysis in 60.000 Milliseconds at Netflix](https://medium.com/netflix-techblog/linux-performance-analysis-in-60-000-milliseconds-accc10403c55)\n	* [Live Downsizing Google Cloud Persistent Disks (PD-SSD) at Mixpanel](https://engineering.mixpanel.com/2018/07/31/live-downsizing-google-cloud-pds-for-fun-and-profit/)\n	* [Decreasing RAM Usage by 40% Using jemalloc with Python & Celery at Zapier](https://zapier.com/engineering/celery-python-jemalloc/)\n	* [Reducing Memory Footprint at Slack](https://slack.engineering/reducing-slacks-memory-footprint-4480fec7e8eb)\n	* [Continuous Load Testing at Slack](https://slack.engineering/continuous-load-testing/)\n	* [Performance Improvements at Pinterest](https://medium.com/@Pinterest_Engineering/driving-user-growth-with-performance-improvements-cfc50dafadd7)\n	* [Server Side Rendering at Wix](https://www.youtube.com/watch?v=f9xI2jR71Ms)\n	* [30x Performance Improvements on MySQLStreamer at Yelp](https://engineeringblog.yelp.com/2018/02/making-30x-performance-improvements-on-yelps-mysqlstreamer.html)\n	* [Optimizing APIs at Netflix](https://medium.com/netflix-techblog/optimizing-the-netflix-api-5c9ac715cf19)\n	* [Performance Monitoring with Riemann and Clojure at Walmart](https://medium.com/walmartlabs/performance-monitoring-with-riemann-and-clojure-eafc07fcd375)\n	* [Performance Tracking Dashboard for Live Games at Zynga](https://www.zynga.com/blogs/engineering/live-games-have-evolving-performance)\n	* [Optimizing CAL Report Hadoop MapReduce Jobs at eBay](https://www.ebayinc.com/stories/blogs/tech/optimization-of-cal-report-hadoop-mapreduce-job/)\n	* [Performance Tuning on Quartz Scheduler at eBay](https://www.ebayinc.com/stories/blogs/tech/performance-tuning-on-quartz-scheduler/)\n	* [Profiling C++ (Part 1: Optimization, Part 2: Measurement and Analysis) at Riot Games](https://engineering.riotgames.com/news/profiling-optimisation)\n	* [Profiling React Server-Side Rendering at HomeAway](https://medium.com/homeaway-tech-blog/profiling-react-server-side-rendering-to-free-the-node-js-event-loop-7f0fe455a901)\n	* [Hardware-Assisted Video Transcoding at Dailymotion](https://medium.com/dailymotion-engineering/hardware-assisted-video-transcoding-at-dailymotion-66cd2db448ae)\n	* [Cross Shard Transactions at 10 Million RPS at Dropbox](https://blogs.dropbox.com/tech/2018/11/cross-shard-transactions-at-10-million-requests-per-second/)\n	* [API Profiling at Pinterest](https://medium.com/@Pinterest_Engineering/api-profiling-at-pinterest-6fa9333b4961)\n	* [Pagelets Parallelize Server-side Processing at Yelp](https://engineeringblog.yelp.com/2017/07/generating-web-pages-in-parallel-with-pagelets.html)\n	* [Improving key expiration in Redis at Twitter](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2019/improving-key-expiration-in-redis.html)\n	* [Ad Delivery Network Performance Optimization with Flame Graphs at MindGeek](https://medium.com/mindgeek-engineering-blog/ad-delivery-network-performance-optimization-with-flame-graphs-bc550cf59cf7)\n	* [Predictive CPU isolation of containers at Netflix](https://medium.com/netflix-techblog/predictive-cpu-isolation-of-containers-at-netflix-91f014d856c7)\n	* [Improving HDFS I/O Utilization for Efficiency at Uber](https://eng.uber.com/improving-hdfs-i-o-utilization-for-efficiency/)\n	* [Cloud Jewels: Estimating kWh in the Cloud at Etsy](https://codeascraft.com/2020/04/23/cloud-jewels-estimating-kwh-in-the-cloud/)\n	* [Unthrottled: Fixing CPU Limits in the Cloud (2 parts) at Indeed](https://engineering.indeedblog.com/blog/2019/12/unthrottled-fixing-cpu-limits-in-the-cloud/)\n* [Performance Optimization by Tuning Garbage Collection](https://confluence.atlassian.com/enterprise/garbage-collection-gc-tuning-guide-461504616.html)\n	* [Garbage Collection in Java Applications at LinkedIn](https://engineering.linkedin.com/garbage-collection/garbage-collection-optimization-high-throughput-and-low-latency-java-applications)\n	* [Garbage Collection in High-Throughput, Low-Latency Machine Learning Services at Adobe](https://medium.com/adobetech/engineering-high-throughput-low-latency-machine-learning-services-7d45edac0271)\n	* [Garbage Collection in Redux Applications at SoundCloud](https://developers.soundcloud.com/blog/garbage-collection-in-redux-applications)\n	* [Garbage Collection in Go Application at Twitch](https://blog.twitch.tv/go-memory-ballast-how-i-learnt-to-stop-worrying-and-love-the-heap-26c2462549a2)\n	* [Analyzing V8 Garbage Collection Logs at Alibaba](https://www.linux.com/blog/can-nodejs-scale-ask-team-alibaba)\n	* [Python Garbage Collection for Dropping 50% Memory Growth Per Request at Instagram](https://instagram-engineering.com/copy-on-write-friendly-python-garbage-collection-ad6ed5233ddf)\n	* [Performance Impact of Removing Out of Band Garbage Collector (OOBGC) at Github](https://githubengineering.com/removing-oobgc/)\n	* [Debugging Java Memory Leaks at Allegro](https://allegro.tech/2018/05/a-comedy-of-errors-debugging-java-memory-leaks.html)\n	* [Optimizing JVM at Alibaba](https://www.youtube.com/watch?v=X4tmr3nhZRg)\n	* [Tuning JVM Memory for Large-scale Services at Uber](https://eng.uber.com/jvm-tuning-garbage-collection/)\n	* [Solr Performance Tuning at Walmart](https://medium.com/walmartglobaltech/solr-performance-tuning-beb7d0d0f8d9)\n	* [Memory Tuning a High Throughput Microservice at Flipkart](https://blog.flipkart.tech/memory-tuning-a-high-throughput-microservice-ed57b3e60997)\n* [Performance Optimization on Image, Video, Page Load](https://developers.google.com/web/fundamentals/performance/why-performance-matters/)\n	* [Optimizing 360 Photos at Scale at Facebook](https://code.facebook.com/posts/129055711052260/optimizing-360-photos-at-scale/)\n	* [Reducing Image File Size in the Photos Infrastructure at Etsy](https://codeascraft.com/2017/05/30/reducing-image-file-size-at-etsy/)\n	* [Improving GIF Performance at Pinterest](https://medium.com/@Pinterest_Engineering/improving-gif-performance-on-pinterest-8dad74bf92f1)\n	* [Optimizing Video Playback Performance at Pinterest](https://medium.com/@Pinterest_Engineering/optimizing-video-playback-performance-caf55ce310d1)\n	* [Optimizing Video Stream for Low Bandwidth with Dynamic Optimizer at Netflix](https://medium.com/netflix-techblog/optimized-shot-based-encodes-now-streaming-4b9464204830)\n	* [Adaptive Video Streaming at YouTube](https://youtube-eng.googleblog.com/2018/04/making-high-quality-video-efficient.html)\n    * [Reducing Video Loading Time at Dailymotion](https://medium.com/dailymotion/reducing-video-loading-time-fa9c997a2294)\n	* [Improving Homepage Performance at Zillow](https://www.zillow.com/engineering/improving-homepage-performance/)\n	* [The Process of Optimizing for Client Performance at Expedia](https://medium.com/expedia-engineering/go-fast-or-go-home-the-process-of-optimizing-for-client-performance-57bb497402e)\n	* [Web Performance at BBC](https://medium.com/bbc-design-engineering/bbc-world-service-web-performance-26b08f7abfcc)\n* [Performance Optimization by Brotli Compression](https://blogs.akamai.com/2016/02/understanding-brotlis-potential.html)\n	* [Boosting Site Speed Using Brotli Compression at LinkedIn](https://engineering.linkedin.com/blog/2017/05/boosting-site-speed-using-brotli-compression)	\n	* [Brotli at Booking.com](https://medium.com/booking-com-development/bookings-journey-with-brotli-978b249d34f3)\n	* [Brotli at Treebo](https://tech.treebo.com/a-tale-of-brotli-compression-bcb071d9780a)\n	* [Deploying Brotli for Static Content at Dropbox](https://dropbox.tech/infrastructure/deploying-brotli-for-static-content)\n	* [Progressive Enhancement with Brotli at Yelp](https://engineeringblog.yelp.com/2017/07/progressive-enhancement-with-brotli.html)\n	* [Speeding Up Redis with Compression at DoorDash](https://doordash.engineering/2019/01/02/speeding-up-redis-with-compression/)\n* [Performance Optimization on Languages and Frameworks](https://www.techempower.com/benchmarks/)\n	* [Python at Netflix](https://netflixtechblog.com/python-at-netflix-bba45dae649e)\n	* [Python at scale (3 parts) at Instagram](https://instagram-engineering.com/python-at-scale-strict-modules-c0bb9245c834)\n	* [OCaml best practices (2 parts) at Issuu](https://engineering.issuu.com/2018/12/10/our-current-ocaml-best-practices-part-2)\n	* [PHP at Slack](https://slack.engineering/taking-php-seriously-cf7a60065329)\n	* [Go at Trivago](https://tech.trivago.com/2020/03/02/why-we-chose-go/)\n	* [TypeScript at Etsy](https://codeascraft.com/2021/11/08/etsys-journey-to-typescript/)\n	* [Kotlin for taming state at Etsy](https://www.etsy.com/sg-en/codeascraft/sealed-classes-opened-my-mind)\n	* [Kotlin at DoorDash](https://doordash.engineering/2022/03/22/how-to-leverage-functional-programming-in-kotlin-to-write-better-cleaner-code/)\n	* [BPF and Go at Bumble](https://medium.com/bumble-tech/bpf-and-go-modern-forms-of-introspection-in-linux-6b9802682223)\n	* [Ruby on Rails at GitLab](https://medium.com/gitlab-magazine/why-we-use-ruby-on-rails-to-build-gitlab-601dce4a7a38)\n	* [Rust in production at Figma](https://medium.com/figma-design/rust-in-production-at-figma-e10a0ec31929)\n	* [Choosing a Language Stack at WeWork](https://engineering.wework.com/choosing-a-language-stack-cac3726928f6)\n	* [Switching from Go to Rust at Discord](https://blog.discord.com/why-discord-is-switching-from-go-to-rust-a190bbca2b1f)\n	* [ASP.NET Core Performance Optimization at Agoda](https://medium.com/agoda-engineering/happy-asp-net-core-performance-optimization-4e21a383d299)\n	* [Data Race Patterns in Go at Uber](https://eng.uber.com/data-race-patterns-in-go/)\n	* [Java 21 Virtual Threads at Netflix](https://netflixtechblog.com/java-21-virtual-threads-dude-wheres-my-lock-3052540e231d)	\n    \n## Intelligence\n* [Big Data](https://insights.sei.cmu.edu/sei_blog/2017/05/reference-architectures-for-big-data-systems.html)	\n	* [Data Platform at Uber](https://eng.uber.com/uber-big-data-platform/)\n	* [Data Platform at BMW](https://www.unibw.de/code/events-u/jt-2018-workshops/ws3_bigdata_vortrag_widmann.pdf)\n	* [Data Platform at Netflix](https://www.youtube.com/watch?v=CSDIThSwA7s)\n	* [Data Platform at Flipkart](https://blog.flipkart.tech/overview-of-flipkart-data-platform-20c6d3e9a196)\n	* [Data Platform at Coupang](https://medium.com/coupang-tech/evolving-the-coupang-data-platform-308e305a9c45)\n	* [Data Platform at DoorDash](https://doordash.engineering/2020/09/25/how-doordash-is-scaling-its-data-platform/)\n	* [Data Platform at Khan Academy](http://engineering.khanacademy.org/posts/khanalytics.htm)\n	* [Data Infrastructure at Airbnb](https://medium.com/airbnb-engineering/data-infrastructure-at-airbnb-8adfb34f169c)\n	* [Data Infrastructure at LinkedIn](https://www.infoq.com/presentations/big-data-infrastructure-linkedin)\n	* [Data Infrastructure at GO-JEK](https://blog.gojekengineering.com/data-infrastructure-at-go-jek-cd4dc8cbd929)\n	* [Data Ingestion Infrastructure at Pinterest](https://medium.com/@Pinterest_Engineering/scalable-and-reliable-data-ingestion-at-pinterest-b921c2ee8754)\n	* [Data Analytics Architecture at Pinterest](https://medium.com/@Pinterest_Engineering/behind-the-pins-building-analytics-f7b508cdacab)\n	* [Data Orchestration Service at Spotify](https://engineering.atspotify.com/2022/03/why-we-switched-our-data-orchestration-service/)\n	* [Big Data Processing (2 parts) at Spotify](https://labs.spotify.com/2017/10/23/big-data-processing-at-spotify-the-road-to-scio-part-2/)\n	* [Big Data Processing at Uber](https://cdn.oreillystatic.com/en/assets/1/event/160/Big%20data%20processing%20with%20Hadoop%20and%20Spark%2C%20the%20Uber%20way%20Presentation.pdf)\n	* [Analytics Pipeline at Lyft](https://cdn.oreillystatic.com/en/assets/1/event/269/Lyft_s%20analytics%20pipeline_%20From%20Redshift%20to%20Apache%20Hive%20and%20Presto%20Presentation.pdf)\n	* [Analytics Pipeline at Grammarly](https://tech.grammarly.com/blog/building-a-versatile-analytics-pipeline-on-top-of-apache-spark)\n	* [Analytics Pipeline at Teads](https://medium.com/teads-engineering/give-meaning-to-100-billion-analytics-events-a-day-d6ba09aa8f44)\n	* [ML Data Pipelines for Real-Time Fraud Prevention at PayPal](https://www.infoq.com/presentations/paypal-ml-fraud-prevention-2018)\n	* [Big Data Analytics and ML Techniques at LinkedIn](https://cdn.oreillystatic.com/en/assets/1/event/269/Big%20data%20analytics%20and%20machine%20learning%20techniques%20to%20drive%20and%20grow%20business%20Presentation%201.pdf)\n	* [Self-Serve Reporting Platform on Hadoop at LinkedIn](https://cdn.oreillystatic.com/en/assets/1/event/137/Building%20a%20self-serve%20real-time%20reporting%20platform%20at%20LinkedIn%20Presentation%201.pdf)\n	* [Privacy-Preserving Analytics and Reporting at LinkedIn](https://engineering.linkedin.com/blog/2019/04/privacy-preserving-analytics-and-reporting-at-linkedin)\n	* [Analytics Platform for Tracking Item Availability at Walmart](https://medium.com/walmartlabs/how-we-build-a-robust-analytics-platform-using-spark-kafka-and-cassandra-lambda-architecture-70c2d1bc8981)\n	* [Real-Time Analytics for Mobile App Crashes using Apache Pinot at Uber](https://www.uber.com/en-SG/blog/real-time-analytics-for-mobile-app-crashes/)\n	* [HALO: Hardware Analytics and Lifecycle Optimization at Facebook](https://code.fb.com/data-center-engineering/hardware-analytics-and-lifecycle-optimization-halo-at-facebook/)\n	* [RBEA: Real-time Analytics Platform at King](https://techblog.king.com/rbea-scalable-real-time-analytics-king/)\n	* [AresDB: GPU-Powered Real-time Analytics Engine at Uber](https://eng.uber.com/aresdb/)\n	* [AthenaX: Streaming Analytics Platform at Uber](https://eng.uber.com/athenax/)\n	* [Jupiter: Config Driven Adtech Batch Ingestion Platform at Uber](https://www.uber.com/en-SG/blog/jupiter-batch-ingestion-platform/)\n	* [Delta: Data Synchronization and Enrichment Platform at Netflix](https://medium.com/netflix-techblog/delta-a-data-synchronization-and-enrichment-platform-e82c36a79aee)\n	* [Keystone: Real-time Stream Processing Platform at Netflix](https://medium.com/netflix-techblog/keystone-real-time-stream-processing-platform-a3ee651812a)\n	* [Databook: Turning Big Data into Knowledge with Metadata at Uber](https://eng.uber.com/databook/)\n	* [Amundsen: Data Discovery & Metadata Engine at Lyft](https://eng.lyft.com/amundsen-lyfts-data-discovery-metadata-engine-62d27254fbb9)\n	* [Maze: Funnel Visualization Platform at Uber](https://eng.uber.com/maze/)\n	* [Metacat: Making Big Data Discoverable and Meaningful at Netflix](https://medium.com/netflix-techblog/metacat-making-big-data-discoverable-and-meaningful-at-netflix-56fb36a53520)\n	* [SpinalTap: Change Data Capture System at Airbnb](https://medium.com/airbnb-engineering/capturing-data-evolution-in-a-service-oriented-architecture-72f7c643ee6f)\n	* [Accelerator: Fast Data Processing Framework at eBay](https://www.ebayinc.com/stories/blogs/tech/announcing-the-accelerator-processing-1-000-000-000-lines-per-second-on-a-single-computer/)\n	* [Omid: Transaction Processing Platform at Yahoo](https://yahooeng.tumblr.com/post/180867271141/a-new-chapter-for-omid)\n	* [TensorFlowOnSpark: Distributed Deep Learning on Big Data Clusters at Yahoo](https://yahooeng.tumblr.com/post/157196488076/open-sourcing-tensorflowonspark-distributed-deep)\n	* [CaffeOnSpark: Distributed Deep Learning on Big Data Clusters at Yahoo](https://yahooeng.tumblr.com/post/139916828451/caffeonspark-open-sourced-for-distributed-deep)\n	* [Spark on Scala: Analytics Reference Architecture at Adobe](https://medium.com/adobetech/spark-on-scala-adobe-analytics-reference-architecture-7457f5614b4c)\n	* [Experimentation Platform (2 parts) at Spotify](https://engineering.atspotify.com/2020/11/02/spotifys-new-experimentation-platform-part-2/)\n	* [Experimentation Platform at Airbnb](https://medium.com/airbnb-engineering/https-medium-com-jonathan-parks-scaling-erf-23fd17c91166)\n	* [Smart Product Platform at Zalando](https://engineering.zalando.com/posts/2017/10/zalando-smart-product-platform.html)\n	* [Log Analysis Platform at LINE](https://www.slideshare.net/wyukawa/strata2017-sg)\n	* [Data Visualisation Platform at Myntra](https://medium.com/myntra-engineering/universal-dashboarding-platform-udp-data-visualisation-platform-at-myntra-5f2522fcf72d)\n	* [Building and Scaling Data Lineage at Netflix](https://medium.com/netflix-techblog/building-and-scaling-data-lineage-at-netflix-to-improve-data-infrastructure-reliability-and-1a52526a7977)\n	* [Building a scalable data management system for computer vision tasks at Pinterest](https://medium.com/@Pinterest_Engineering/building-a-scalable-data-management-system-for-computer-vision-tasks-a6dee8f1c580)\n	* [Structured Data at Etsy](https://codeascraft.com/2019/07/31/an-introduction-to-structured-data-at-etsy/)\n	* [Scaling a Mature Data Pipeline - Managing Overhead at Airbnb](https://medium.com/airbnb-engineering/scaling-a-mature-data-pipeline-managing-overhead-f34835cbc866)\n	* [Spark Partitioning Strategies at Airbnb](https://medium.com/airbnb-engineering/on-spark-hive-and-small-files-an-in-depth-look-at-spark-partitioning-strategies-a9a364f908)\n	* [Scaling the Hadoop Distributed File System at LinkedIn](https://engineering.linkedin.com/blog/2021/the-exabyte-club--linkedin-s-journey-of-scaling-the-hadoop-distr)\n	* [Scaling Hadoop YARN cluster beyond 10,000 nodes at LinkedIn](https://engineering.linkedin.com/blog/2021/scaling-linkedin-s-hadoop-yarn-cluster-beyond-10-000-nodes)\n	* [Scaling Big Data Access Controls at Pinterest](https://medium.com/pinterest-engineering/securely-scaling-big-data-access-controls-at-pinterest-bbc3406a1695)\n* [Distributed Machine Learning](https://www.csie.ntu.edu.tw/~cjlin/talks/bigdata-bilbao.pdf)\n	* [Machine Learning Platform at Yelp](https://engineeringblog.yelp.com/2020/07/ML-platform-overview.html)\n	* [Machine Learning Platform at Etsy](https://codeascraft.com/2021/12/21/redesigning-etsys-machine-learning-platform/)\n	* [Machine Learning Platform at Zalando](https://engineering.zalando.com/posts/2022/04/zalando-machine-learning-platform.html)\n	* [Scaling AI/ML Infrastructure at Uber](https://www.uber.com/en-SG/blog/scaling-ai-ml-infrastructure-at-uber/)\n	* [Recommendation System at Lyft](https://eng.lyft.com/the-recommendation-system-at-lyft-67bc9dcc1793)\n	* [Reinforcement Learning Platform at Lyft](https://eng.lyft.com/lyfts-reinforcement-learning-platform-670f77ff46ec)\n	* [Platform for Serving Recommendations at Etsy](https://www.etsy.com/sg-en/codeascraft/building-a-platform-for-serving-recommendations-at-etsy)\n	* [Infrastructure to Run User Forecasts at Spotify](https://engineering.atspotify.com/2022/06/how-we-built-infrastructure-to-run-user-forecasts-at-spotify/)\n	* [Aroma: Using ML for Code Recommendation at Facebook](https://code.fb.com/developer-tools/aroma/)\n	* [Flyte: Cloud Native Machine Learning and Data Processing Platform at Lyft](https://eng.lyft.com/introducing-flyte-cloud-native-machine-learning-and-data-processing-platform-fb2bb3046a59)\n	* [LyftLearn: ML Model Training Infrastructure built on Kubernetes at Lyft](https://eng.lyft.com/lyftlearn-ml-model-training-infrastructure-built-on-kubernetes-aef8218842bb)\n	* [Horovod: Open Source Distributed Deep Learning Framework for TensorFlow at Uber](https://eng.uber.com/horovod/)\n	* [Genie: Gen AI On-Call Copilot at Uber](https://www.uber.com/blog/genie-ubers-gen-ai-on-call-copilot/)\n	* [COTA: Improving Customer Care with NLP & Machine Learning at Uber](https://eng.uber.com/cota/)\n	* [Manifold: Model-Agnostic Visual Debugging Tool for Machine Learning at Uber](https://eng.uber.com/manifold/)	\n	* [Repo-Topix: Topic Extraction Framework at Github](https://githubengineering.com/topics/)\n	* [Concourse: Generating Personalized Content Notifications in Near-Real-Time at LinkedIn](https://engineering.linkedin.com/blog/2018/05/concourse--generating-personalized-content-notifications-in-near)\n	* [Altus Care: Applying a Chatbot to Platform Engineering at eBay](https://www.ebayinc.com/stories/blogs/tech/altus-care-apply-chatbot-to-ebay-platform-engineering/)\n	* [PyKrylov: Accelerating Machine Learning Research at eBay](https://tech.ebayinc.com/engineering/pykrylov-accelerating-machine-learning-research-at-ebay/)\n	* [Box Graph: Spontaneous Social Network at Box](https://blog.box.com/blog/box-graph-how-we-built-spontaneous-social-network/)\n	* [PricingNet: Pricing Modelling with Neural Networks at Skyscanner](https://hackernoon.com/pricingnet-modelling-the-global-airline-industry-with-neural-networks-833844d20ea6)\n	* [PinText: Multitask Text Embedding System at Pinterest](https://medium.com/pinterest-engineering/pintext-a-multitask-text-embedding-system-in-pinterest-b80ece364555)\n	* [SearchSage: Learning Search Query Representations at Pinterest](https://medium.com/pinterest-engineering/searchsage-learning-search-query-representations-at-pinterest-654f2bb887fc)\n	* [Cannes: ML saves $1.7M a year on document previews at Dropbox](https://dropbox.tech/machine-learning/cannes--how-ml-saves-us--1-7m-a-year-on-document-previews)\n	* [Scaling Gradient Boosted Trees for Click-Through-Rate Prediction at Yelp](https://engineeringblog.yelp.com/2018/01/building-a-distributed-ml-pipeline-part1.html)	\n	* [Learning with Privacy at Scale at Apple](https://machinelearning.apple.com/2017/12/06/learning-with-privacy-at-scale.html)\n	* [Deep Learning for Image Classification Experiment at Mercari](https://medium.com/mercari-engineering/mercaris-image-classification-experiment-using-deep-learning-9b4e994a18ec)\n	* [Deep Learning for Frame Detection in Product Images at Allegro](https://allegro.tech/2016/12/deep-learning-for-frame-detection.html)\n	* [Content-based Video Relevance Prediction at Hulu](https://medium.com/hulu-tech-blog/content-based-video-relevance-prediction-b2c448e14752)\n	* [Moderating Inappropriate Video Content at Yelp](https://engineeringblog.yelp.com/2024/03/moderating-inappropriate-video-content-at-yelp.html)\n	* [Improving Photo Selection With Deep Learning at TripAdvisor](http://engineering.tripadvisor.com/improving-tripadvisor-photo-selection-deep-learning/)\n	* [Personalized Recommendations for Experiences Using Deep Learning at TripAdvisor](https://www.tripadvisor.com/engineering/personalized-recommendations-for-experiences-using-deep-learning/)\n	* [Personalised Recommender Systems at BBC](https://medium.com/bbc-design-engineering/developing-personalised-recommender-systems-at-the-bbc-e26c5e0c4216)\n	* [Machine Learning (2 parts) at CondÃ© Nast](https://technology.condenast.com/story/handbag-brand-and-color-detection)\n	* [Natural Language Processing and Content Analysis (2 parts) at CondÃ© Nast](https://technology.condenast.com/story/natural-language-processing-and-content-analysis-at-conde-nast-part-2-system-architecture)\n	* [Mapping the World of Music Using Machine Learning (2 parts) at iHeartRadio](https://tech.iheart.com/mapping-the-world-of-music-using-machine-learning-part-2-aa50b6a0304c)\n	* [Machine Learning to Improve Streaming Quality at Netflix](https://medium.com/netflix-techblog/using-machine-learning-to-improve-streaming-quality-at-netflix-9651263ef09f)\n	* [Machine Learning to Match Drivers & Riders at GO-JEK](https://blog.gojekengineering.com/how-we-use-machine-learning-to-match-drivers-riders-b06d617b9e5)\n	* [Improving Video Thumbnails with Deep Neural Nets at YouTube](https://youtube-eng.googleblog.com/2015/10/improving-youtube-video-thumbnails-with_8.', '{"language":null,"stars":67076,"forks":6709,"watchers":67076,"open_issues":18,"topics":["architecture","awesome","awesome-list","backend","big-data","computer-science","design-patterns","devops","distributed-systems","interview","interview-practice","interview-questions","lists","machine-learning","programming","resources","scalability","system","system-design","web-development"],"default_branch":"master","size_kb":1463,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[]', NULL, 'MIT', 'approved', 80, '2f90e75984b3270efee3d993f4641468', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-binhnguyennus-awesome-scalability from https://github.com/binhnguyennus.png
Image converted to WebP: data/images/github-binhnguyennus-awesome-scalability.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-labmlai-annotated-deep-learning-paper-implementations', 'github--labmlai--annotated-deep-learning-paper-implementations', 'annotated_deep_learning_paper_implementations', 'labmlai', 'This is a collection of simple PyTorch implementations of neural networks and related algorithms. These implementations are documented with explanations, The website renders these as side-by-side formatted notes. We believe these would help you understand these algorithms better. !Screenshot We are actively maintaining this repo and adding new implementations almost weekly. for updates. * JAX implementation * Multi-headed attention * Triton Flash Attention * Transformer building blocks * Tran...', '["attention","deep-learning","deep-learning-tutorial","gan","literate-programming","lora","machine-learning","neural-networks","optimizers","pytorch","reinforcement-learning","transformer","transformers","python"]', 'other', 64715, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/labmlai/annotated_deep_learning_paper_implementations","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '[![Twitter](https://img.shields.io/twitter/follow/labmlai?style=social)](https://twitter.com/labmlai)\n\n# [labml.ai Deep Learning Paper Implementations](https://nn.labml.ai/index.html)\n\nThis is a collection of simple PyTorch implementations of\nneural networks and related algorithms.\nThese implementations are documented with explanations,\n\n[The website](https://nn.labml.ai/index.html)\nrenders these as side-by-side formatted notes.\nWe believe these would help you understand these algorithms better.\n\n![Screenshot](https://nn.labml.ai/dqn-light.png)\n\nWe are actively maintaining this repo and adding new \nimplementations almost weekly.\n[![Twitter](https://img.shields.io/twitter/follow/labmlai?style=social)](https://twitter.com/labmlai) for updates.\n\n## Paper Implementations\n\n#### âœ¨ [Transformers](https://nn.labml.ai/transformers/index.html)\n\n* [JAX implementation](https://nn.labml.ai/transformers/jax_transformer/index.html)\n* [Multi-headed attention](https://nn.labml.ai/transformers/mha.html)\n* [Triton Flash Attention](https://nn.labml.ai/transformers/flash/index.html)\n* [Transformer building blocks](https://nn.labml.ai/transformers/models.html) \n* [Transformer XL](https://nn.labml.ai/transformers/xl/index.html)\n    * [Relative multi-headed attention](https://nn.labml.ai/transformers/xl/relative_mha.html)\n* [Rotary Positional Embeddings](https://nn.labml.ai/transformers/rope/index.html)\n* [Attention with Linear Biases (ALiBi)](https://nn.labml.ai/transformers/alibi/index.html)\n* [RETRO](https://nn.labml.ai/transformers/retro/index.html)\n* [Compressive Transformer](https://nn.labml.ai/transformers/compressive/index.html)\n* [GPT Architecture](https://nn.labml.ai/transformers/gpt/index.html)\n* [GLU Variants](https://nn.labml.ai/transformers/glu_variants/simple.html)\n* [kNN-LM: Generalization through Memorization](https://nn.labml.ai/transformers/knn)\n* [Feedback Transformer](https://nn.labml.ai/transformers/feedback/index.html)\n* [Switch Transformer](https://nn.labml.ai/transformers/switch/index.html)\n* [Fast Weights Transformer](https://nn.labml.ai/transformers/fast_weights/index.html)\n* [FNet](https://nn.labml.ai/transformers/fnet/index.html)\n* [Attention Free Transformer](https://nn.labml.ai/transformers/aft/index.html)\n* [Masked Language Model](https://nn.labml.ai/transformers/mlm/index.html)\n* [MLP-Mixer: An all-MLP Architecture for Vision](https://nn.labml.ai/transformers/mlp_mixer/index.html)\n* [Pay Attention to MLPs (gMLP)](https://nn.labml.ai/transformers/gmlp/index.html)\n* [Vision Transformer (ViT)](https://nn.labml.ai/transformers/vit/index.html)\n* [Primer EZ](https://nn.labml.ai/transformers/primer_ez/index.html)\n* [Hourglass](https://nn.labml.ai/transformers/hour_glass/index.html)\n\n#### âœ¨ [Low-Rank Adaptation (LoRA)](https://nn.labml.ai/lora/index.html)\n\n#### âœ¨ [Eleuther GPT-NeoX](https://nn.labml.ai/neox/index.html)\n* [Generate on a 48GB GPU](https://nn.labml.ai/neox/samples/generate.html)\n* [Finetune on two 48GB GPUs](https://nn.labml.ai/neox/samples/finetune.html)\n* [LLM.int8()](https://nn.labml.ai/neox/utils/llm_int8.html)\n\n#### âœ¨ [Diffusion models](https://nn.labml.ai/diffusion/index.html)\n\n* [Denoising Diffusion Probabilistic Models (DDPM)](https://nn.labml.ai/diffusion/ddpm/index.html)\n* [Denoising Diffusion Implicit Models (DDIM)](https://nn.labml.ai/diffusion/stable_diffusion/sampler/ddim.html)\n* [Latent Diffusion Models](https://nn.labml.ai/diffusion/stable_diffusion/latent_diffusion.html)\n* [Stable Diffusion](https://nn.labml.ai/diffusion/stable_diffusion/index.html)\n\n#### âœ¨ [Generative Adversarial Networks](https://nn.labml.ai/gan/index.html)\n* [Original GAN](https://nn.labml.ai/gan/original/index.html)\n* [GAN with deep convolutional network](https://nn.labml.ai/gan/dcgan/index.html)\n* [Cycle GAN](https://nn.labml.ai/gan/cycle_gan/index.html)\n* [Wasserstein GAN](https://nn.labml.ai/gan/wasserstein/index.html)\n* [Wasserstein GAN with Gradient Penalty](https://nn.labml.ai/gan/wasserstein/gradient_penalty/index.html)\n* [StyleGAN 2](https://nn.labml.ai/gan/stylegan/index.html)\n\n#### âœ¨ [Recurrent Highway Networks](https://nn.labml.ai/recurrent_highway_networks/index.html)\n\n#### âœ¨ [LSTM](https://nn.labml.ai/lstm/index.html)\n\n#### âœ¨ [HyperNetworks - HyperLSTM](https://nn.labml.ai/hypernetworks/hyper_lstm.html)\n\n#### âœ¨ [ResNet](https://nn.labml.ai/resnet/index.html)\n\n#### âœ¨ [ConvMixer](https://nn.labml.ai/conv_mixer/index.html)\n\n#### âœ¨ [Capsule Networks](https://nn.labml.ai/capsule_networks/index.html)\n\n#### âœ¨ [U-Net](https://nn.labml.ai/unet/index.html)\n\n#### âœ¨ [Sketch RNN](https://nn.labml.ai/sketch_rnn/index.html)\n\n#### âœ¨ Graph Neural Networks\n\n* [Graph Attention Networks (GAT)](https://nn.labml.ai/graphs/gat/index.html)\n* [Graph Attention Networks v2 (GATv2)](https://nn.labml.ai/graphs/gatv2/index.html)\n\n#### âœ¨ [Counterfactual Regret Minimization (CFR)](https://nn.labml.ai/cfr/index.html)\n\nSolving games with incomplete information such as poker with CFR.\n\n* [Kuhn Poker](https://nn.labml.ai/cfr/kuhn/index.html)\n\n#### âœ¨ [Reinforcement Learning](https://nn.labml.ai/rl/index.html)\n* [Proximal Policy Optimization](https://nn.labml.ai/rl/ppo/index.html) with\n [Generalized Advantage Estimation](https://nn.labml.ai/rl/ppo/gae.html)\n* [Deep Q Networks](https://nn.labml.ai/rl/dqn/index.html) with\n with [Dueling Network](https://nn.labml.ai/rl/dqn/model.html),\n [Prioritized Replay](https://nn.labml.ai/rl/dqn/replay_buffer.html)\n and Double Q Network.\n\n#### âœ¨ [Optimizers](https://nn.labml.ai/optimizers/index.html)\n* [Adam](https://nn.labml.ai/optimizers/adam.html)\n* [AMSGrad](https://nn.labml.ai/optimizers/amsgrad.html)\n* [Adam Optimizer with warmup](https://nn.labml.ai/optimizers/adam_warmup.html)\n* [Noam Optimizer](https://nn.labml.ai/optimizers/noam.html)\n* [Rectified Adam Optimizer](https://nn.labml.ai/optimizers/radam.html)\n* [AdaBelief Optimizer](https://nn.labml.ai/optimizers/ada_belief.html)\n* [Sophia-G Optimizer](https://nn.labml.ai/optimizers/sophia.html)\n\n#### âœ¨ [Normalization Layers](https://nn.labml.ai/normalization/index.html)\n* [Batch Normalization](https://nn.labml.ai/normalization/batch_norm/index.html)\n* [Layer Normalization](https://nn.labml.ai/normalization/layer_norm/index.html)\n* [Instance Normalization](https://nn.labml.ai/normalization/instance_norm/index.html)\n* [Group Normalization](https://nn.labml.ai/normalization/group_norm/index.html)\n* [Weight Standardization](https://nn.labml.ai/normalization/weight_standardization/index.html)\n* [Batch-Channel Normalization](https://nn.labml.ai/normalization/batch_channel_norm/index.html)\n* [DeepNorm](https://nn.labml.ai/normalization/deep_norm/index.html)\n\n#### âœ¨ [Distillation](https://nn.labml.ai/distillation/index.html)\n\n#### âœ¨ [Adaptive Computation](https://nn.labml.ai/adaptive_computation/index.html)\n\n* [PonderNet](https://nn.labml.ai/adaptive_computation/ponder_net/index.html)\n\n#### âœ¨ [Uncertainty](https://nn.labml.ai/uncertainty/index.html)\n\n* [Evidential Deep Learning to Quantify Classification Uncertainty](https://nn.labml.ai/uncertainty/evidence/index.html)\n\n#### âœ¨ [Activations](https://nn.labml.ai/activations/index.html)\n\n* [Fuzzy Tiling Activations](https://nn.labml.ai/activations/fta/index.html)\n\n#### âœ¨ [Langauge Model Sampling Techniques](https://nn.labml.ai/sampling/index.html)\n* [Greedy Sampling](https://nn.labml.ai/sampling/greedy.html)\n* [Temperature Sampling](https://nn.labml.ai/sampling/temperature.html)\n* [Top-k Sampling](https://nn.labml.ai/sampling/top_k.html)\n* [Nucleus Sampling](https://nn.labml.ai/sampling/nucleus.html)\n\n#### âœ¨ [Scalable Training/Inference](https://nn.labml.ai/scaling/index.html)\n* [Zero3 memory optimizations](https://nn.labml.ai/scaling/zero3/index.html)\n\n### Installation\n\n```bash\npip install labml-nn\n```\n', '{"language":"Python","stars":64715,"forks":6553,"watchers":64715,"open_issues":27,"topics":["attention","deep-learning","deep-learning-tutorial","gan","literate-programming","lora","machine-learning","neural-networks","optimizers","pytorch","reinforcement-learning","transformer","transformers"],"default_branch":"master","size_kb":156359,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[]', NULL, 'MIT', 'approved', 65, 'f2c82de3855bbf3c7c3f855ccf043ecb', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-labmlai-annotated-deep-learning-paper-implementations from https://github.com/labmlai.png
Image converted to WebP: data/images/github-labmlai-annotated-deep-learning-paper-implementations.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-scikit-learn-scikit-learn', 'github--scikit-learn--scikit-learn', 'scikit-learn', 'scikit-learn', '.. -*- mode: rst -*- |Azure| |Codecov| |CircleCI| |Nightly wheels| |Ruff| |PythonVersion| |PyPI| |DOI| |Benchmark| .. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main :target: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=main .. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield :target: https://circleci.com/gh/scikit-learn/scikit-...', '["data-analysis","data-science","machine-learning","python","statistics","python"]', 'other', 64224, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/scikit-learn/scikit-learn","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '.. -*- mode: rst -*-\n\n|Azure| |Codecov| |CircleCI| |Nightly wheels| |Ruff| |PythonVersion| |PyPI| |DOI| |Benchmark|\n\n.. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main\n   :target: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=main\n\n.. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield\n   :target: https://circleci.com/gh/scikit-learn/scikit-learn\n\n.. |Codecov| image:: https://codecov.io/gh/scikit-learn/scikit-learn/branch/main/graph/badge.svg?token=Pk8G9gg3y9\n   :target: https://codecov.io/gh/scikit-learn/scikit-learn\n\n.. |Nightly wheels| image:: https://github.com/scikit-learn/scikit-learn/actions/workflows/wheels.yml/badge.svg?event=schedule\n   :target: https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule\n\n.. |Ruff| image:: https://img.shields.io/badge/code%20style-ruff-000000.svg\n   :target: https://github.com/astral-sh/ruff\n\n.. |PythonVersion| image:: https://img.shields.io/pypi/pyversions/scikit-learn.svg\n   :target: https://pypi.org/project/scikit-learn/\n\n.. |PyPI| image:: https://img.shields.io/pypi/v/scikit-learn\n   :target: https://pypi.org/project/scikit-learn\n\n.. |DOI| image:: https://zenodo.org/badge/21369/scikit-learn/scikit-learn.svg\n   :target: https://zenodo.org/badge/latestdoi/21369/scikit-learn/scikit-learn\n\n.. |Benchmark| image:: https://img.shields.io/badge/Benchmarked%20by-asv-blue\n   :target: https://scikit-learn.org/scikit-learn-benchmarks\n\n.. |PythonMinVersion| replace:: 3.11\n.. |NumPyMinVersion| replace:: 1.24.1\n.. |SciPyMinVersion| replace:: 1.10.0\n.. |JoblibMinVersion| replace:: 1.3.0\n.. |ThreadpoolctlMinVersion| replace:: 3.2.0\n.. |MatplotlibMinVersion| replace:: 3.6.1\n.. |Scikit-ImageMinVersion| replace:: 0.22.0\n.. |PandasMinVersion| replace:: 1.5.0\n.. |SeabornMinVersion| replace:: 0.13.0\n.. |PytestMinVersion| replace:: 7.1.2\n.. |PlotlyMinVersion| replace:: 5.18.0\n\n.. image:: https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/scikit-learn-logo.png\n  :target: https://scikit-learn.org/\n\n**scikit-learn** is a Python module for machine learning built on top of\nSciPy and is distributed under the 3-Clause BSD license.\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https://scikit-learn.org/dev/about.html#authors>`__ page\nfor a list of core contributors.\n\nIt is currently maintained by a team of volunteers.\n\nWebsite: https://scikit-learn.org\n\nInstallation\n------------\n\nDependencies\n~~~~~~~~~~~~\n\nscikit-learn requires:\n\n- Python (>= |PythonMinVersion|)\n- NumPy (>= |NumPyMinVersion|)\n- SciPy (>= |SciPyMinVersion|)\n- joblib (>= |JoblibMinVersion|)\n- threadpoolctl (>= |ThreadpoolctlMinVersion|)\n\n=======\n\nScikit-learn plotting capabilities (i.e., functions start with ``plot_`` and\nclasses end with ``Display``) require Matplotlib (>= |MatplotlibMinVersion|).\nFor running the examples Matplotlib >= |MatplotlibMinVersion| is required.\nA few examples require scikit-image >= |Scikit-ImageMinVersion|, a few examples\nrequire pandas >= |PandasMinVersion|, some examples require seaborn >=\n|SeabornMinVersion| and Plotly >= |PlotlyMinVersion|.\n\nUser installation\n~~~~~~~~~~~~~~~~~\n\nIf you already have a working installation of NumPy and SciPy,\nthe easiest way to install scikit-learn is using ``pip``::\n\n    pip install -U scikit-learn\n\nor ``conda``::\n\n    conda install -c conda-forge scikit-learn\n\nThe documentation includes more detailed `installation instructions <https://scikit-learn.org/stable/install.html>`_.\n\n\nChangelog\n---------\n\nSee the `changelog <https://scikit-learn.org/dev/whats_new.html>`__\nfor a history of notable changes to scikit-learn.\n\nDevelopment\n-----------\n\nWe welcome new contributors of all experience levels. The scikit-learn\ncommunity goals are to be helpful, welcoming, and effective. The\n`Development Guide <https://scikit-learn.org/stable/developers/index.html>`_\nhas detailed information about contributing code, documentation, tests, and\nmore. We''ve included some basic information in this README.\n\nImportant links\n~~~~~~~~~~~~~~~\n\n- Official source code repo: https://github.com/scikit-learn/scikit-learn\n- Download releases: https://pypi.org/project/scikit-learn/\n- Issue tracker: https://github.com/scikit-learn/scikit-learn/issues\n\nSource code\n~~~~~~~~~~~\n\nYou can check the latest sources with the command::\n\n    git clone https://github.com/scikit-learn/scikit-learn.git\n\nContributing\n~~~~~~~~~~~~\n\nTo learn more about making a contribution to scikit-learn, please see our\n`Contributing guide\n<https://scikit-learn.org/dev/developers/contributing.html>`_.\n\nTesting\n~~~~~~~\n\nAfter installation, you can launch the test suite from outside the source\ndirectory (you will need to have ``pytest`` >= |PytestMinVersion| installed)::\n\n    pytest sklearn\n\nSee the web page https://scikit-learn.org/dev/developers/contributing.html#testing-and-improving-test-coverage\nfor more information.\n\n    Random number generation can be controlled during testing by setting\n    the ``SKLEARN_SEED`` environment variable.\n\nSubmitting a Pull Request\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBefore opening a Pull Request, have a look at the\nfull Contributing page to make sure your code complies\nwith our guidelines: https://scikit-learn.org/stable/developers/index.html\n\nProject History\n---------------\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us <https://scikit-learn.org/dev/about.html#authors>`__ page\nfor a list of core contributors.\n\nThe project is currently maintained by a team of volunteers.\n\n**Note**: `scikit-learn` was previously referred to as `scikits.learn`.\n\nHelp and Support\n----------------\n\nDocumentation\n~~~~~~~~~~~~~\n\n- HTML documentation (stable release): https://scikit-learn.org\n- HTML documentation (development version): https://scikit-learn.org/dev/\n- FAQ: https://scikit-learn.org/stable/faq.html\n\nCommunication\n~~~~~~~~~~~~~\n\nMain Channels\n^^^^^^^^^^^^^\n\n- **Website**: https://scikit-learn.org\n- **Blog**: https://blog.scikit-learn.org\n- **Mailing list**: https://mail.python.org/mailman/listinfo/scikit-learn\n\nDeveloper & Support\n^^^^^^^^^^^^^^^^^^^^^^\n\n- **GitHub Discussions**: https://github.com/scikit-learn/scikit-learn/discussions\n- **Stack Overflow**: https://stackoverflow.com/questions/tagged/scikit-learn\n- **Discord**: https://discord.gg/h9qyrK8Jc8\n\nSocial Media Platforms\n^^^^^^^^^^^^^^^^^^^^^^\n\n- **LinkedIn**: https://www.linkedin.com/company/scikit-learn\n- **YouTube**: https://www.youtube.com/channel/UCJosFjYm0ZYVUARxuOZqnnw/playlists\n- **Facebook**: https://www.facebook.com/scikitlearnofficial/\n- **Instagram**: https://www.instagram.com/scikitlearnofficial/\n- **TikTok**: https://www.tiktok.com/@scikit.learn\n- **Bluesky**: https://bsky.app/profile/scikit-learn.org\n- **Mastodon**: https://mastodon.social/@sklearn@fosstodon.org\n\nResources\n^^^^^^^^^\n\n- **Calendar**: https://blog.scikit-learn.org/calendar/\n- **Logos & Branding**: https://github.com/scikit-learn/scikit-learn/tree/main/doc/logos\n\nCitation\n~~~~~~~~\n\nIf you use scikit-learn in a scientific publication, we would appreciate citations: https://scikit-learn.org/stable/about.html#citing-scikit-learn\n', '{"language":"Python","stars":64224,"forks":26490,"watchers":64224,"open_issues":2122,"topics":["data-analysis","data-science","machine-learning","python","statistics"],"default_branch":"main","size_kb":178580,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:scikit-learn:scikit-learn","source_url":"https://github.com/scikit-learn/scikit-learn"},{"type":"has_code","target_id":"github:scikit-learn:scikit-learn","source_url":"https://github.com/scikit-learn/scikit-learn"},{"type":"has_code","target_id":"github:astral-sh:ruff","source_url":"https://github.com/astral-sh/ruff"},{"type":"has_code","target_id":"github:scikit-learn:scikit-learn","source_url":"https://github.com/scikit-learn/scikit-learn"},{"type":"has_code","target_id":"github:scikit-learn:scikit-learn","source_url":"https://github.com/scikit-learn/scikit-learn"},{"type":"has_code","target_id":"github:scikit-learn:scikit-learn.git","source_url":"https://github.com/scikit-learn/scikit-learn.git"},{"type":"has_code","target_id":"github:scikit-learn:scikit-learn","source_url":"https://github.com/scikit-learn/scikit-learn"},{"type":"has_code","target_id":"github:scikit-learn:scikit-learn","source_url":"https://github.com/scikit-learn/scikit-learn"}]', NULL, 'BSD-3-Clause', 'approved', 65, 'c8e4e8ec3328becb641499bf8bf8ad92', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-scikit-learn-scikit-learn from https://github.com/scikit-learn.png
Image converted to WebP: data/images/github-scikit-learn-scikit-learn.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-keras-team-keras', 'github--keras-team--keras', 'keras', 'keras-team', 'Keras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only). Effortlessly build and train models for computer vision, natural language processing, audio processing, timeseries forecasting, recommender systems, etc. - **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras and the availability of easy-to-debug runtimes like PyTorch or JAX eager execution. - **State-of-the-ar...', '["data-science","deep-learning","jax","machine-learning","neural-networks","python","pytorch","tensorflow","python"]', 'other', 63628, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/keras-team/keras","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Keras 3: Deep Learning for Humans\n\nKeras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, PyTorch, and OpenVINO (for inference-only).\nEffortlessly build and train models for computer vision, natural language processing, audio processing,\ntimeseries forecasting, recommender systems, etc.\n\n- **Accelerated model development**: Ship deep learning solutions faster thanks to the high-level UX of Keras\nand the availability of easy-to-debug runtimes like PyTorch or JAX eager execution.\n- **State-of-the-art performance**: By picking the backend that is the fastest for your model architecture (often JAX!),\nleverage speedups ranging from 20% to 350% compared to other frameworks. [Benchmark here](https://keras.io/getting_started/benchmarks/).\n- **Datacenter-scale training**: Scale confidently from your laptop to large clusters of GPUs or TPUs.\n\nJoin nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.\n\n\n## Installation\n\n### Install with pip\n\nKeras 3 is available on PyPI as `keras`. Note that Keras 2 remains available as the `tf-keras` package.\n\n1. Install `keras`:\n\n```\npip install keras --upgrade\n```\n\n2. Install backend package(s).\n\nTo use `keras`, you should also install the backend of choice: `tensorflow`, `jax`, or `torch`. Additionally,\nThe `openvino` backend is available with support for model inference only.\n\n### Local installation\n\n#### Minimal installation\n\nKeras 3 is compatible with Linux and macOS systems. For Windows users, we recommend using WSL2 to run Keras.\nTo install a local development version:\n\n1. Install dependencies:\n\n```\npip install -r requirements.txt\n```\n\n2. Run installation command from the root directory.\n\n```\npython pip_build.py --install\n```\n\n3. Run API generation script when creating PRs that update `keras_export` public APIs:\n\n```\n./shell/api_gen.sh\n```\n\n## Backend Compatibility Table\n\nThe following table lists the minimum supported versions of each backend for the latest stable release of Keras (v3.x):\n\n| Backend    | Minimum Supported Version |\n|------------|---------------------------|\n| TensorFlow | 2.16.1                    |\n| JAX        | 0.4.20                    |\n| PyTorch    | 2.1.0                     |\n| OpenVINO   | 2025.3.0                  |\n\n#### Adding GPU support\n\nThe `requirements.txt` file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also\nprovide a separate `requirements-{backend}-cuda.txt` for TensorFlow, JAX, and PyTorch. These install all CUDA\ndependencies via `pip` and expect a NVIDIA driver to be pre-installed. We recommend a clean Python environment for each\nbackend to avoid CUDA version mismatches. As an example, here is how to create a JAX GPU environment with `conda`:\n\n```shell\nconda create -y -n keras-jax python=3.10\nconda activate keras-jax\npip install -r requirements-jax-cuda.txt\npython pip_build.py --install\n```\n\n## Configuring your backend\n\nYou can export the environment variable `KERAS_BACKEND` or you can edit your local config file at `~/.keras/keras.json`\nto configure your backend. Available backend options are: `"tensorflow"`, `"jax"`, `"torch"`, `"openvino"`. Example:\n\n```\nexport KERAS_BACKEND="jax"\n```\n\nIn Colab, you can do:\n\n```python\nimport os\nos.environ["KERAS_BACKEND"] = "jax"\n\nimport keras\n```\n\n**Note:** The backend must be configured before importing `keras`, and the backend cannot be changed after\nthe package has been imported.\n\n**Note:** The OpenVINO backend is an inference-only backend, meaning it is designed only for running model\npredictions using `model.predict()` method.\n\n## Backwards compatibility\n\nKeras 3 is intended to work as a drop-in replacement for `tf.keras` (when using the TensorFlow backend). Just take your\nexisting `tf.keras` code, make sure that your calls to `model.save()` are using the up-to-date `.keras` format, and you''re\ndone.\n\nIf your `tf.keras` model does not include custom components, you can start running it on top of JAX or PyTorch immediately.\n\nIf it does include custom components (e.g. custom layers or a custom `train_step()`), it is usually possible to convert it\nto a backend-agnostic implementation in just a few minutes.\n\nIn addition, Keras models can consume datasets in any format, regardless of the backend you''re using:\nyou can train your models with your existing `tf.data.Dataset` pipelines or PyTorch `DataLoaders`.\n\n## Why use Keras 3?\n\n- Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework,\ne.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow.\n- Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework.\n    - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch.\n    - You can take a Keras model and use it as part of a PyTorch-native `Module` or as part of a JAX-native model function.\n- Make your ML code future-proof by avoiding framework lock-in.\n- As a PyTorch user: get access to power and usability of Keras, at last!\n- As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library.\n\n\nRead more in the [Keras 3 release announcement](https://keras.io/keras_3/).\n', '{"language":"Python","stars":63628,"forks":19651,"watchers":63628,"open_issues":258,"topics":["data-science","deep-learning","jax","machine-learning","neural-networks","python","pytorch","tensorflow"],"default_branch":"master","size_kb":48860,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[]', NULL, 'Apache-2.0', 'approved', 65, 'da0664abbe8e343dd1002fff06b7bf46', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-keras-team-keras from https://github.com/keras-team.png
Image converted to WebP: data/images/github-keras-team-keras.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ultralytics-yolov5', 'github--ultralytics--yolov5', 'yolov5', 'ultralytics', '<div align="center"> <p> <a href="https://www.ultralytics.com/events/yolovision?utm_source=github&utm_medium=org&utm_campaign=yv25_event" target="_blank"> <img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" alt="Ultralytics YOLO banner"></a> </p> ä¸­æ–‡ | í•œêµ­ì–´ | æ—¥æœ¬èªž | Ð ÑƒÑÑÐºÐ¸Ð¹ | Deutsch | FranÃ§ais | EspaÃ±ol | PortuguÃªs | TÃ¼rkÃ§e | Tiáº¿ng Viá»‡t | Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© <div> <a href="https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml"><img src...', '["coreml","deep-learning","ios","machine-learning","ml","object-detection","onnx","pytorch","tflite","ultralytics","yolo","yolov3","yolov5","python"]', 'other', 56296, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ultralytics/yolov5","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n  <p>\n    <a href="https://www.ultralytics.com/events/yolovision?utm_source=github&utm_medium=org&utm_campaign=yv25_event" target="_blank">\n      <img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" alt="Ultralytics YOLO banner"></a>\n  </p>\n\n[ä¸­æ–‡](https://docs.ultralytics.com/zh/) | [í•œêµ­ì–´](https://docs.ultralytics.com/ko/) | [æ—¥æœ¬èªž](https://docs.ultralytics.com/ja/) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [FranÃ§ais](https://docs.ultralytics.com/fr/) | [EspaÃ±ol](https://docs.ultralytics.com/es) | [PortuguÃªs](https://docs.ultralytics.com/pt/) | [TÃ¼rkÃ§e](https://docs.ultralytics.com/tr/) | [Tiáº¿ng Viá»‡t](https://docs.ultralytics.com/vi/) | [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](https://docs.ultralytics.com/ar/)\n\n<div>\n    <a href="https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml"><img src="https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg" alt="YOLOv5 CI Testing"></a>\n    <a href="https://zenodo.org/badge/latestdoi/264818686"><img src="https://zenodo.org/badge/264818686.svg" alt="YOLOv5 Citation"></a>\n    <a href="https://hub.docker.com/r/ultralytics/yolov5"><img src="https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker" alt="Docker Pulls"></a>\n    <a href="https://discord.com/invite/ultralytics"><img alt="Discord" src="https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue"></a> <a href="https://community.ultralytics.com/"><img alt="Ultralytics Forums" src="https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue"></a> <a href="https://www.reddit.com/r/ultralytics/"><img alt="Ultralytics Reddit" src="https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue"></a>\n    <br>\n    <a href="https://bit.ly/yolov5-paperspace-notebook"><img src="https://assets.paperspace.io/img/gradient-badge.svg" alt="Run on Gradient"></a>\n    <a href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>\n    <a href="https://www.kaggle.com/models/ultralytics/yolov5"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" alt="Open In Kaggle"></a>\n  </div>\n  <br>\n\nUltralytics YOLOv5 ðŸš€ is a cutting-edge, state-of-the-art (SOTA) computer vision model developed by [Ultralytics](https://www.ultralytics.com/). Based on the [PyTorch](https://pytorch.org/) framework, YOLOv5 is renowned for its ease of use, speed, and accuracy. It incorporates insights and best practices from extensive research and development, making it a popular choice for a wide range of vision AI tasks, including [object detection](https://docs.ultralytics.com/tasks/detect/), [image segmentation](https://docs.ultralytics.com/tasks/segment/), and [image classification](https://docs.ultralytics.com/tasks/classify/).\n\nWe hope the resources here help you get the most out of YOLOv5. Please browse the [YOLOv5 Docs](https://docs.ultralytics.com/yolov5/) for detailed information, raise an issue on [GitHub](https://github.com/ultralytics/yolov5/issues/new/choose) for support, and join our [Discord community](https://discord.com/invite/ultralytics) for questions and discussions!\n\nTo request an Enterprise License, please complete the form at [Ultralytics Licensing](https://www.ultralytics.com/license).\n\n<div align="center">\n  <a href="https://github.com/ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="2%" alt="Ultralytics GitHub"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">\n  <a href="https://www.linkedin.com/company/ultralytics/"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="2%" alt="Ultralytics LinkedIn"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">\n  <a href="https://twitter.com/ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="2%" alt="Ultralytics Twitter"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">\n  <a href="https://youtube.com/ultralytics?sub_confirmation=1"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="2%" alt="Ultralytics YouTube"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">\n  <a href="https://www.tiktok.com/@ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="2%" alt="Ultralytics TikTok"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">\n  <a href="https://ultralytics.com/bilibili"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="2%" alt="Ultralytics BiliBili"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">\n  <a href="https://discord.com/invite/ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="2%" alt="Ultralytics Discord"></a>\n</div>\n\n</div>\n<br>\n\n## ðŸš€ YOLO11: The Next Evolution\n\nWe are excited to announce the launch of **Ultralytics YOLO11** ðŸš€, the latest advancement in our state-of-the-art (SOTA) vision models! Available now at the [Ultralytics YOLO GitHub repository](https://github.com/ultralytics/ultralytics), YOLO11 builds on our legacy of speed, precision, and ease of use. Whether you''re tackling [object detection](https://docs.ultralytics.com/tasks/detect/), [instance segmentation](https://docs.ultralytics.com/tasks/segment/), [pose estimation](https://docs.ultralytics.com/tasks/pose/), [image classification](https://docs.ultralytics.com/tasks/classify/), or [oriented object detection (OBB)](https://docs.ultralytics.com/tasks/obb/), YOLO11 delivers the performance and versatility needed to excel in diverse applications.\n\nGet started today and unlock the full potential of YOLO11! Visit the [Ultralytics Docs](https://docs.ultralytics.com/) for comprehensive guides and resources:\n\n[![PyPI version](https://badge.fury.io/py/ultralytics.svg)](https://badge.fury.io/py/ultralytics) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://clickpy.clickhouse.com/dashboard/ultralytics)\n\n```bash\n# Install the ultralytics package\npip install ultralytics\n```\n\n<div align="center">\n  <a href="https://www.ultralytics.com/yolo" target="_blank">\n  <img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png" alt="Ultralytics YOLO Performance Comparison"></a>\n</div>\n\n## ðŸ“š Documentation\n\nSee the [YOLOv5 Docs](https://docs.ultralytics.com/yolov5/) for full documentation on training, testing, and deployment. See below for quickstart examples.\n\n<details open>\n<summary>Install</summary>\n\nClone the repository and install dependencies in a [**Python>=3.8.0**](https://www.python.org/) environment. Ensure you have [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/) installed.\n\n```bash\n# Clone the YOLOv5 repository\ngit clone https://github.com/ultralytics/yolov5\n\n# Navigate to the cloned directory\ncd yolov5\n\n# Install required packages\npip install -r requirements.txt\n```\n\n</details>\n\n<details open>\n<summary>Inference with PyTorch Hub</summary>\n\nUse YOLOv5 via [PyTorch Hub](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/) for inference. [Models](https://github.com/ultralytics/yolov5/tree/master/models) are automatically downloaded from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\n\n```python\nimport torch\n\n# Load a YOLOv5 model (options: yolov5n, yolov5s, yolov5m, yolov5l, yolov5x)\nmodel = torch.hub.load("ultralytics/yolov5", "yolov5s")  # Default: yolov5s\n\n# Define the input image source (URL, local file, PIL image, OpenCV frame, numpy array, or list)\nimg = "https://ultralytics.com/images/zidane.jpg"  # Example image\n\n# Perform inference (handles batching, resizing, normalization automatically)\nresults = model(img)\n\n# Process the results (options: .print(), .show(), .save(), .crop(), .pandas())\nresults.print()  # Print results to console\nresults.show()  # Display results in a window\nresults.save()  # Save results to runs/detect/exp\n```\n\n</details>\n\n<details>\n<summary>Inference with detect.py</summary>\n\nThe `detect.py` script runs inference on various sources. It automatically downloads [models](https://github.com/ultralytics/yolov5/tree/master/models) from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases) and saves the results to the `runs/detect` directory.\n\n```bash\n# Run inference using a webcam\npython detect.py --weights yolov5s.pt --source 0\n\n# Run inference on a local image file\npython detect.py --weights yolov5s.pt --source img.jpg\n\n# Run inference on a local video file\npython detect.py --weights yolov5s.pt --source vid.mp4\n\n# Run inference on a screen capture\npython detect.py --weights yolov5s.pt --source screen\n\n# Run inference on a directory of images\npython detect.py --weights yolov5s.pt --source path/to/images/\n\n# Run inference on a text file listing image paths\npython detect.py --weights yolov5s.pt --source list.txt\n\n# Run inference on a text file listing stream URLs\npython detect.py --weights yolov5s.pt --source list.streams\n\n# Run inference using a glob pattern for images\npython detect.py --weights yolov5s.pt --source ''path/to/*.jpg''\n\n# Run inference on a YouTube video URL\npython detect.py --weights yolov5s.pt --source ''https://youtu.be/LNwODJXcvt4''\n\n# Run inference on an RTSP, RTMP, or HTTP stream\npython detect.py --weights yolov5s.pt --source ''rtsp://example.com/media.mp4''\n```\n\n</details>\n\n<details>\n<summary>Training</summary>\n\nThe commands below demonstrate how to reproduce YOLOv5 [COCO dataset](https://docs.ultralytics.com/datasets/detect/coco/) results. Both [models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) are downloaded automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases). Training times for YOLOv5n/s/m/l/x are approximately 1/2/4/6/8 days on a single [NVIDIA V100 GPU](https://www.nvidia.com/en-us/data-center/v100/). Using [Multi-GPU training](https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training/) can significantly reduce training time. Use the largest `--batch-size` your hardware allows, or use `--batch-size -1` for YOLOv5 [AutoBatch](https://github.com/ultralytics/yolov5/pull/5092). The batch sizes shown below are for V100-16GB GPUs.\n\n```bash\n# Train YOLOv5n on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '''' --cfg yolov5n.yaml --batch-size 128\n\n# Train YOLOv5s on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '''' --cfg yolov5s.yaml --batch-size 64\n\n# Train YOLOv5m on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '''' --cfg yolov5m.yaml --batch-size 40\n\n# Train YOLOv5l on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '''' --cfg yolov5l.yaml --batch-size 24\n\n# Train YOLOv5x on COCO for 300 epochs\npython train.py --data coco.yaml --epochs 300 --weights '''' --cfg yolov5x.yaml --batch-size 16\n```\n\n<img width="800" src="https://user-images.githubusercontent.com/26833433/90222759-949d8800-ddc1-11ea-9fa1-1c97eed2b963.png" alt="YOLOv5 Training Results">\n\n</details>\n\n<details open>\n<summary>Tutorials</summary>\n\n- **[Train Custom Data](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data/)** ðŸš€ **RECOMMENDED**: Learn how to train YOLOv5 on your own datasets.\n- **[Tips for Best Training Results](https://docs.ultralytics.com/guides/model-training-tips/)** â˜˜ï¸: Improve your model''s performance with expert tips.\n- **[Multi-GPU Training](https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training/)**: Speed up training using multiple GPUs.\n- **[PyTorch Hub Integration](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/)** ðŸŒŸ **NEW**: Easily load models using PyTorch Hub.\n- **[Model Export (TFLite, ONNX, CoreML, TensorRT)](https://docs.ultralytics.com/yolov5/tutorials/model_export/)** ðŸš€: Convert your models to various deployment formats like [ONNX](https://onnx.ai/) or [TensorRT](https://developer.nvidia.com/tensorrt).\n- **[NVIDIA Jetson Deployment](https://docs.ultralytics.com/guides/nvidia-jetson/)** ðŸŒŸ **NEW**: Deploy YOLOv5 on [NVIDIA Jetson](https://developer.nvidia.com/embedded-computing) devices.\n- **[Test-Time Augmentation (TTA)](https://docs.ultralytics.com/yolov5/tutorials/test_time_augmentation/)**: Enhance prediction accuracy with TTA.\n- **[Model Ensembling](https://docs.ultralytics.com/yolov5/tutorials/model_ensembling/)**: Combine multiple models for better performance.\n- **[Model Pruning/Sparsity](https://docs.ultralytics.com/yolov5/tutorials/model_pruning_and_sparsity/)**: Optimize models for size and speed.\n- **[Hyperparameter Evolution](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution/)**: Automatically find the best training hyperparameters.\n- **[Transfer Learning with Frozen Layers](https://docs.ultralytics.com/yolov5/tutorials/transfer_learning_with_frozen_layers/)**: Adapt pretrained models to new tasks efficiently using [transfer learning](https://www.ultralytics.com/glossary/transfer-learning).\n- **[Architecture Summary](https://docs.ultralytics.com/yolov5/tutorials/architecture_description/)** ðŸŒŸ **NEW**: Understand the YOLOv5 model architecture.\n- **[Ultralytics HUB Training](https://www.ultralytics.com/hub)** ðŸš€ **RECOMMENDED**: Train and deploy YOLO models using Ultralytics HUB.\n- **[ClearML Logging](https://docs.ultralytics.com/yolov5/tutorials/clearml_logging_integration/)**: Integrate with [ClearML](https://clear.ml/) for experiment tracking.\n- **[Neural Magic DeepSparse Integration](https://docs.ultralytics.com/yolov5/tutorials/neural_magic_pruning_quantization/)**: Accelerate inference with DeepSparse.\n- **[Comet Logging](https://docs.ultralytics.com/yolov5/tutorials/comet_logging_integration/)** ðŸŒŸ **NEW**: Log experiments using [Comet ML](https://www.comet.com/site/).\n\n</details>\n\n## ðŸ§© Integrations\n\nOur key integrations with leading AI platforms extend the functionality of Ultralytics'' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with partners like [Weights & Biases](https://docs.ultralytics.com/integrations/weights-biases/), [Comet ML](https://docs.ultralytics.com/integrations/comet/), [Roboflow](https://docs.ultralytics.com/integrations/roboflow/), and [Intel OpenVINO](https://docs.ultralytics.com/integrations/openvino/), can optimize your AI workflow. Explore more at [Ultralytics Integrations](https://docs.ultralytics.com/integrations/).\n\n<a href="https://docs.ultralytics.com/integrations/" target="_blank">\n    <img width="100%" src="https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png" alt="Ultralytics active learning integrations">\n</a>\n<br>\n<br>\n\n<div align="center">\n  <a href="https://www.ultralytics.com/hub">\n    <img src="https://github.com/ultralytics/assets/raw/main/partners/logo-ultralytics-hub.png" width="10%" alt="Ultralytics HUB logo"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space">\n  <a href="https://docs.ultralytics.com/integrations/weights-biases/">\n    <img src="https://github.com/ultralytics/assets/raw/main/partners/logo-wb.png" width="10%" alt="Weights & Biases logo"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space">\n  <a href="https://docs.ultralytics.com/integrations/comet/">\n    <img src="https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png" width="10%" alt="Comet ML logo"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space">\n  <a href="https://docs.ultralytics.com/integrations/neural-magic/">\n    <img src="https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png" width="10%" alt="Neural Magic logo"></a>\n</div>\n\n|                                                       Ultralytics HUB ðŸŒŸ                                                        |                                                          Weights & Biases                                                           |                                                                              Comet                                                                              |                                                        Neural Magic                                                         |\n| :-----------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------: |\n| Streamline YOLO workflows: Label, train, and deploy effortlessly with [Ultralytics HUB](https://hub.ultralytics.com/). Try now! | Track experiments, hyperparameters, and results with [Weights & Biases](https://docs.ultralytics.com/integrations/weights-biases/). | Free forever, [Comet ML](https://docs.ultralytics.com/integrations/comet/) lets you save YOLO models, resume training, and interactively visualize predictions. | Run YOLO inference up to 6x faster with [Neural Magic DeepSparse](https://docs.ultralytics.com/integrations/neural-magic/). |\n\n## â­ Ultralytics HUB\n\nExperience seamless AI development with [Ultralytics HUB](https://www.ultralytics.com/hub) â­, the ultimate platform for building, training, and deploying [computer vision](https://www.ultralytics.com/glossary/computer-vision-cv) models. Visualize datasets, train [YOLOv5](https://docs.ultralytics.com/models/yolov5/) and [YOLOv8](https://docs.ultralytics.com/models/yolov8/) ðŸš€ models, and deploy them to real-world applications without writing any code. Transform images into actionable insights using our cutting-edge tools and user-friendly [Ultralytics App](https://www.ultralytics.com/app-install). Start your journey for **Free** today!\n\n<a align="center" href="https://www.ultralytics.com/hub" target="_blank">\n<img width="100%" src="https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png" alt="Ultralytics HUB Platform Screenshot"></a>\n\n## ðŸ¤” Why YOLOv5?\n\nYOLOv5 is designed for simplicity and ease of use. We prioritize real-world performance and accessibility.\n\n<p align="left"><img width="800" src="https://user-images.githubusercontent.com/26833433/155040763-93c22a27-347c-4e3c-847a-8094621d3f4e.png" alt="YOLOv5 Performance Chart"></p>\n<details>\n  <summary>YOLOv5-P5 640 Figure</summary>\n\n<p align="left"><img width="800" src="https://user-images.githubusercontent.com/26833433/155040757-ce0934a3-06a6-43dc-a979-2edbbd69ea0e.png" alt="YOLOv5 P5 640 Performance Chart"></p>\n</details>\n<details>\n  <summary>Figure Notes</summary>\n\n- **COCO AP val** denotes the [mean Average Precision (mAP)](https://www.ultralytics.com/glossary/mean-average-precision-map) at [Intersection over Union (IoU)](https://www.ultralytics.com/glossary/intersection-over-union-iou) thresholds from 0.5 to 0.95, measured on the 5,000-image [COCO val2017 dataset](https://docs.ultralytics.com/datasets/detect/coco/) across various inference sizes (256 to 1536 pixels).\n- **GPU Speed** measures the average inference time per image on the [COCO val2017 dataset](https://docs.ultralytics.com/datasets/detect/coco/) using an [AWS p3.2xlarge V100 instance](https://aws.amazon.com/ec2/instance-types/p4/) with a batch size of 32.\n- **EfficientDet** data is sourced from the [google/automl repository](https://github.com/google/automl) at batch size 8.\n- **Reproduce** these results using the command: `python val.py --task study --data coco.yaml --iou 0.7 --weights yolov5n6.pt yolov5s6.pt yolov5m6.pt yolov5l6.pt yolov5x6.pt`\n\n</details>\n\n### Pretrained Checkpoints\n\nThis table shows the performance metrics for various YOLOv5 models trained on the COCO dataset.\n\n| Model                                                                                                                                                                    | Size<br><sup>(pixels) | mAP<sup>val<br>50-95 | mAP<sup>val<br>50 | Speed<br><sup>CPU b1<br>(ms) | Speed<br><sup>V100 b1<br>(ms) | Speed<br><sup>V100 b32<br>(ms) | Params<br><sup>(M) | FLOPs<br><sup>@640 (B) |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------- | -------------------- | ----------------- | ---------------------------- | ----------------------------- | ------------------------------ | ------------------ | ---------------------- |\n| [YOLOv5n](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt)                                                                                       | 640                   | 28.0                 | 45.7              | **45**                       | **6.3**                       | **0.6**                        | **1.9**            | **4.5**                |\n| [YOLOv5s](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt)                                                                                       | 640                   | 37.4                 | 56.8              | 98                           | 6.4                           | 0.9                            | 7.2                | 16.5                   |\n| [YOLOv5m](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt)                                                                                       | 640                   | 45.4                 | 64.1              | 224                          | 8.2                           | 1.7                            | 21.2               | 49.0                   |\n| [YOLOv5l](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l.pt)                                                                                       | 640                   | 49.0                 | 67.3              | 430                          | 10.1                          | 2.7                            | 46.5               | 109.1                  |\n| [YOLOv5x](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x.pt)                                                                                       | 640                   | 50.7                 | 68.9              | 766                          | 12.1                          | 4.8                            | 86.7               | 205.7                  |\n|                                                                                                                                                                          |                       |                      |                   |                              |                               |                                |                    |                        |\n| [YOLOv5n6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n6.pt)                                                                                     | 1280                  | 36.0                 | 54.4              | 153                          | 8.1                           | 2.1                            | 3.2                | 4.6                    |\n| [YOLOv5s6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s6.pt)                                                                                     | 1280                  | 44.8                 | 63.7              | 385                          | 8.2                           | 3.6                            | 12.6               | 16.8                   |\n| [YOLOv5m6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m6.pt)                                                                                     | 1280                  | 51.3                 | 69.3              | 887                          | 11.1                          | 6.8                            | 35.7               | 50.0                   |\n| [YOLOv5l6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l6.pt)                                                                                     | 1280                  | 53.7                 | 71.3              | 1784                         | 15.8                          | 10.5                           | 76.8               | 111.4                  |\n| [YOLOv5x6](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x6.pt)<br>+ [[TTA]](https://docs.ultralytics.com/yolov5/tutorials/test_time_augmentation/) | 1280<br>1536          | 55.0<br>**55.8**     | 72.7<br>**72.7**  | 3136<br>-                    | 26.2<br>-                     | 19.4<br>-                      | 140.7<br>-         | 209.8<br>-             |\n\n<details>\n  <summary>Table Notes</summary>\n\n- All checkpoints were trained for 300 epochs using default settings. Nano (n) and Small (s) models use [hyp.scratch-low.yaml](https://github.com/ultralytics/yolov5/blob/master/data/hyps/hyp.scratch-low.yaml) hyperparameters, while Medium (m), Large (l), and Extra-Large (x) models use [hyp.scratch-high.yaml](https://github.com/ultralytics/yolov5/blob/master/data/hyps/hyp.scratch-high.yaml).\n- **mAP<sup>val</sup>** values represent single-model, single-scale performance on the [COCO val2017 dataset](https://docs.ultralytics.com/datasets/detect/coco/).<br>Reproduce using: `python val.py --data coco.yaml --img 640 --conf 0.001 --iou 0.65`\n- **Speed** metrics are averaged over COCO val images using an [AWS p3.2xlarge V100 instance](https://aws.amazon.com/ec2/instance-types/p4/). Non-Maximum Suppression (NMS) time (~1 ms/image) is not included.<br>Reproduce using: `python val.py --data coco.yaml --img 640 --task speed --batch 1`\n- **TTA** ([Test Time Augmentation](https://docs.ultralytics.com/yolov5/tutorials/test_time_augmentation/)) includes reflection and scale augmentations for improved accuracy.<br>Reproduce using: `python val.py --data coco.yaml --img 1536 --iou 0.7 --augment`\n\n</details>\n\n## ðŸ–¼ï¸ Segmentation\n\nThe YOLOv5 [release v7.0](https://github.com/ultralytics/yolov5/releases/v7.0) introduced [instance segmentation](https://docs.ultralytics.com/tasks/segment/) models that achieve state-of-the-art performance. These models are designed for easy training, validation, and deployment. For full details, see the [Release Notes](https://github.com/ultralytics/yolov5/releases/v7.0) and explore the [YOLOv5 Segmentation Colab Notebook](https://github.com/ultralytics/yolov5/blob/master/segment/tutorial.ipynb) for quickstart examples.\n\n<details>\n  <summary>Segmentation Checkpoints</summary>\n\n<div align="center">\n<a align="center" href="https://www.ultralytics.com/yolo" target="_blank">\n<img width="800" src="https://user-images.githubusercontent.com/61612323/204180385-84f3aca9-a5e9-43d8-a617-dda7ca12e54a.png" alt="YOLOv5 Segmentation Performance Chart"></a>\n</div>\n\nYOLOv5 segmentation models were trained on the [COCO dataset](https://docs.ultralytics.com/datasets/segment/coco/) for 300 epochs at an image size of 640 pixels using A100 GPUs. Models were exported to [ONNX](https://onnx.ai/) FP32 for CPU speed tests and [TensorRT](https://developer.nvidia.com/tensorrt) FP16 for GPU speed tests. All speed tests were conducted on Google [Colab Pro](https://colab.research.google.com/signup) notebooks for reproducibility.\n\n| Model                                                                                      | Size<br><sup>(pixels) | mAP<sup>box<br>50-95 | mAP<sup>mask<br>50-95 | Train Time<br><sup>300 epochs<br>A100 (hours) | Speed<br><sup>ONNX CPU<br>(ms) | Speed<br><sup>TRT A100<br>(ms) | Params<br><sup>(M) | FLOPs<br><sup>@640 (B) |\n| ------------------------------------------------------------------------------------------ | --------------------- | -------------------- | --------------------- | --------------------------------------------- | ------------------------------ | ------------------------------ | ------------------ | ---------------------- |\n| [YOLOv5n-seg](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-seg.pt) | 640                   | 27.6                 | 23.4                  | 80:17                                         | **62.7**                       | **1.2**                        | **2.0**            | **7.1**                |\n| [YOLOv5s-seg](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-seg.pt) | 640                   | 37.6                 | 31.7                  | 88:16                                         | 173.3                          | 1.4                            | 7.6                | 26.4                   |\n| [YOLOv5m-seg](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-seg.pt) | 640                   | 45.0                 | 37.1                  | 108:36                                        | 427.0                          | 2.2                            | 22.0               | 70.8                   |\n| [YOLOv5l-seg](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-seg.pt) | 640                   | 49.0                 | 39.9                  | 66:43 (2x)                                    | 857.4                          | 2.9                            | 47.9               | 147.7                  |\n| [YOLOv5x-seg](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x-seg.pt) | 640                   | **50.7**             | **41.4**              | 62:56 (3x)                                    | 1579.2                         | 4.5                            | 88.8               | 265.7                  |\n\n- All checkpoints were trained for 300 epochs using the SGD optimizer with `lr0=0.01` and `weight_decay=5e-5` at an image size of 640 pixels, using default settings.<br>Training runs are logged at [https://wandb.ai/glenn-jocher/YOLOv5_v70_official](https://wandb.ai/glenn-jocher/YOLOv5_v70_official).\n- **Accuracy** values represent single-model, single-scale performance on the COCO dataset.<br>Reproduce using: `python segment/val.py --data coco.yaml --weights yolov5s-seg.pt`\n- **Speed** metrics are averaged over 100 inference images using a [Colab Pro A100 High-RAM instance](https://colab.research.google.com/signup). Values indicate inference speed only (NMS adds approximately 1ms per image).<br>Reproduce using: `python segment/val.py --data coco.yaml --weights yolov5s-seg.pt --batch 1`\n- **Export** to ONNX (FP32) and TensorRT (FP16) was performed using `export.py`.<br>Reproduce using: `python export.py --weights yolov5s-seg.pt --include engine --device 0 --half`\n\n</details>\n\n<details>\n  <summary>Segmentation Usage Examples &nbsp;<a href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/segment/tutorial.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></summary>\n\n### Train\n\nYOLOv5 segmentation training supports automatic download of the [COCO128-seg dataset](https://docs.ultralytics.com/datasets/segment/coco8-seg/) via the `--data coco128-seg.yaml` argument. For the full [COCO-segments dataset](https://docs.ultralytics.com/datasets/segment/coco/), download it manually using `bash data/scripts/get_coco.sh --train --val --segments` and then train with `python train.py --data coco.yaml`.\n\n```bash\n# Train on a single GPU\npython segment/train.py --data coco128-seg.yaml --weights yolov5s-seg.pt --img 640\n\n# Train using Multi-GPU Distributed Data Parallel (DDP)\npython -m torch.distributed.run --nproc_per_node 4 --master_port 1 segment/train.py --data coco128-seg.yaml --weights yolov5s-seg.pt --img 640 --device 0,1,2,3\n```\n\n### Val\n\nValidate the mask [mean Average Precision (mAP)](https://www.ultralytics.com/glossary/mean-average-precision-map) of YOLOv5s-seg on the COCO dataset:\n\n```bash\n# Download COCO validation segments split (780MB, 5000 images)\nbash data/scripts/get_coco.sh --val --segments\n\n# Validate the model\npython segment/val.py --weights yolov5s-seg.pt --data coco.yaml --img 640\n```\n\n### Predict\n\nUse the pretrained YOLOv5m-seg.pt model to perform segmentation on `bus.jpg`:\n\n```bash\n# Run prediction\npython segment/predict.py --weights yolov5m-seg.pt --source data/images/bus.jpg\n```\n\n```python\n# Load model from PyTorch Hub (Note: Inference support might vary)\nmodel = torch.hub.load("ultralytics/yolov5", "custom", "yolov5m-seg.pt")\n```\n\n| ![Zidane Segmentation Example](https://user-images.githubusercontent.com/26833433/203113421-decef4c4-183d-4a0a-a6c2-6435b33bc5d3.jpg) | ![Bus Segmentation Example](https://user-images.githubusercontent.com/26833433/203113416-11fe0025-69f7-4874-a0a6-65d0bfe2999a.jpg) |\n| :-----------------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------------------------------------: |\n\n### Export\n\nExport the YOLOv5s-seg model to ONNX and TensorRT formats:\n\n```bash\n# Export model\npython export.py --weights yolov5s-seg.pt --include onnx engine --img 640 --device 0\n```\n\n</details>\n\n## ðŸ·ï¸ Classification\n\nYOLOv5 [release v6.2](https://github.com/ultralytics/yolov5/releases/v6.2) introduced support for [image classification](https://docs.ultralytics.com/tasks/classify/) model training, validation, and deployment. Check the [Release Notes](https://github.com/ultralytics/yolov5/releases/v6.2) for details and the [YOLOv5 Classification Colab Notebook](https://github.com/ultralytics/yolov5/blob/master/classify/tutorial.ipynb) for quickstart guides.\n\n<details>\n  <summary>Classification Checkpoints</summary>\n\n<br>\n\nYOLOv5-cls classification models were trained on [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/) for 90 epochs using a 4xA100 instance. [ResNet](https://arxiv.org/abs/1512.03385) and [EfficientNet](https://arxiv.org/abs/1905.11946) models were trained alongside under identical settings for comparison. Models were exported to [ONNX](https://onnx.ai/) FP32 (CPU speed tests) and [TensorRT](https://developer.nvidia.com/tensorrt) FP16 (GPU speed tests). All speed tests were run on Google [Colab Pro](https://colab.research.google.com/signup) for reproducibility.\n\n| Model                                                                                              | Size<br><sup>(pixels) | Acc<br><sup>top1 | Acc<br><sup>top5 | Training<br><sup>90 epochs<br>4xA100 (hours) | Speed<br><sup>ONNX CPU<br>(ms) | Speed<br><sup>TensorRT V100<br>(ms) | Params<br><sup>(M) | FLOPs<br><sup>@224 (B) |\n| -------------------------------------------------------------------------------------------------- | --------------------- | ---------------- | ---------------- | -------------------------------------------- | ------------------------------ | ----------------------------------- | ------------------ | ---------------------- |\n| [YOLOv5n-cls](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-cls.pt)         | 224                   | 64.6             | 85.4             | 7:59                                         | **3.3**                        | **0.5**                             | **2.5**            | **0.5**                |\n| [YOLOv5s-cls](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt)         | 224                   | 71.5             | 90.2             | 8:09                                         | 6.6                            | 0.6                                 | 5.4                | 1.4                    |\n| [YOLOv5m-cls](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-cls.pt)         | 224                   | 75.9             | 92.9             | 10:06                                        | 15.5                           | 0.9                                 | 12.9               | 3.9                    |\n| [YOLOv5l-cls](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-cls.pt)         | 224                   | 78.0             | 94.0             | 11:56                                        | 26.9                           | 1.4                                 | 26.5               | 8.5                    |\n| [YOLOv5x-cls](https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x-cls.pt)         | 224                   | **79.0**         | **94.4**         | 15:04                                        | 54.3                           | 1.8                                 | 48.1               | 15.9                   |\n|                                                                                                    |                       |                  |                  |                                              |                                |                                     |                    |                        |\n| [ResNet18](https://github.com/ultralytics/yolov5/releases/download/v7.0/resnet18.pt)               | 224                   | 70.3             | 89.5             | **6:47**                                     | 11.2                           | 0.5                                 | 11.7               | 3.7                    |\n| [ResNet34](https://github.com/ultralytics/yolov5/releases/download/v7.0/resnet34.pt)               | 224                   | 73.9             | 91.8             | 8:33                                         | 20.6                           | 0.9                                 | 21.8               | 7.4                    |\n| [ResNet50](https://github.com/ultralytics/yolov5/releases/download/v7.0/resnet50.pt)               | 224                   | 76.8             | 93.4             | 11:10                                        | 23.4                           | 1.0                                 | 25.6               | 8.5                    |\n| [ResNet101](https://github.com/ultralytics/yolov5/releases/download/v7.0/resnet101.pt)             | 224                   | 78.5             | 94.3             | 17:10                                        | 42.1                           | 1.9                                 | 44.5               | 15.9                   |\n|                                                                                                    |                       |                  |                  |                                              |                                |                                     |                    |                        |\n| [EfficientNet_b0](https://github.com/ultralytics/yolov5/releases/download/v7.0/efficientnet_b0.pt) | 224                   | 75.1             | 92.4             | 13:03                                        | 12.5                           | 1.3                                 | 5.3                | 1.0                    |\n| [EfficientNet_b1](https://github.com/ultralytics/yolov5/releases/download/v7.0/efficientnet_b1.pt) | 224                   | 76.4             | 93.2             | 17:04                                        | 14.9                           | 1.6                                 | 7.8                | 1.5                    |\n| [EfficientNet_b2](https://github.com/ultralytics/yolov5/releases/download/v7.0/efficientnet_b2.pt) | 224                   | 76.6             | 93.4             | 17:10                                        | 15.9                           | 1.6                                 | 9.1                | 1.7                    |\n| [EfficientNet_b3](https://github.com/ultralytics/yolov5/releases/download/v7.0/efficientnet_b3.pt) | 224                   | 77.7             | 94.0             | 19:19                                        | 18.9                           | 1.9                                 | 12.2               | 2.4                    |\n\n<details>\n  <summary>Table Notes (click to expand)</summary>\n\n- All checkpoints were trained for 90 epochs using the SGD optimizer with `lr0=0.001` and `weight_decay=5e-5` at an image size of 224 pixels, using default settings.<br>Training runs are logged at [https://wandb.ai/glenn-jocher/YOLOv5-Classifier-v6-2](https://wandb.ai/glenn-jocher/YOLOv5-Classifier-v6-2).\n- **Accuracy** values (top-1 and top-5) represent single-model, single-scale performance on the [ImageNet-1k dataset](https://docs.ultralytics.com/datasets/classify/imagenet/).<br>Reproduce using: `python classify/val.py --data ../datasets/imagenet --img 224`\n- **Speed** metrics are averaged over 100 inference images using a Google [Colab Pro V100 High-RAM instance](https://colab.research.google.com/signup).<br>Reproduce using: `python classify/val.py --data ../datasets/imagenet --img 224 --batch 1`\n- **Export** to ONNX (FP32) and TensorRT (FP16) was performed using `export.py`.<br>Reproduce using: `python export.py --weights yolov5s-cls.pt --include engine onnx --imgsz 224`\n\n</details>\n</details>\n\n<details>\n  <summary>Classification Usage Examples &nbsp;<a href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/classify/tutorial.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></summary>\n\n### Train\n\nYOLOv5 classification training supports automatic download for datasets like [MNIST](https://docs.ultralytics.com/datasets/classify/mnist/), [Fashion-MNIST](https://docs.ultralytics.com/datasets/classify/fashion-mnist/), [CIFAR10](https://docs.ultralytics.com/datasets/classify/cifar10/), [CIFAR100](https://docs.ultralytics.com/datasets/classify/cifar100/), [Imagenette](https://docs.ultralytics.com/datasets/classify/imagenette/), [Imagewoof](https://docs.ultralytics.com/datasets/classify/imagewoof/), and [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/) using the `--data` argument. For example, start training on MNIST with `--data mnist`.\n\n```bash\n# Train on a single GPU using CIFAR-100 dataset\npython classify/train.py --model yolov5s-cls.pt --data cifar100 --epochs 5 --img 224 --batch 128\n\n# Train using Multi-GPU DDP on ImageNet dataset\npython -m torch.distributed.run --nproc_per_node 4 --master_port 1 classify/train.py --model yolov5s-cls.pt --data imagenet --epochs 5 --img 224 --device 0,1,2,3\n```\n\n### Val\n\nValidate the accuracy of the YOLOv5m-cls model on the ImageNet-1k validation dataset:\n\n```bash\n# Download ImageNet validation split (6.3GB, 50,000 images)\nbash data/scripts/get_imagenet.sh --val\n\n# Validate the model\npython classify/val.py --weights yolov5m-cls.pt --data ../datasets/imagenet --img 224\n```\n\n### Predict\n\nUse the pretrained YOLOv5s-cls.pt model to classify the image `bus.jpg`:\n\n```bash\n# Run prediction\npython classify/predict.py --weights yolov5s-cls.pt --source data/images/bus.jpg\n```\n\n```python\n# Load model from PyTorch Hub\nmodel = torch.hub.load("ultralytics/yolov5", "custom", "yolov5s-cls.pt")\n```\n\n### Export\n\nExport trained YOLOv5s-cls, ResNet50, and EfficientNet_b0 models to ONNX and TensorRT formats:\n\n```bash\n# Export models\npython export.py --weights yolov5s-cls.pt resnet50.pt efficientnet_b0.pt --include onnx engine --img 224\n```\n\n</details>\n\n## â˜ï¸ Environments\n\nGet started quickly with our pre-configured environments. Click the icons below for setup details.\n\n<div align="center">\n  <a href="https://bit.ly/yolov5-paperspace-notebook" title="Run on Paperspace Gradient">\n    <img src="https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-gradient.png" width="10%" /></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="5%" alt="" />\n  <a href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb" title="Open in Google Colab">\n    <img src="https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-colab-small.png" width="10%" /></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="5%" alt="" />\n  <a href="https://www.kaggle.com/models/ultralytics/yolov5" title="Open in Kaggle">\n    <img src="https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-kaggle-small.png" width="10%" /></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="5%" alt="" />\n  <a href="https://hub.docker.com/r/ultralytics/yolov5" title="Pull Docker Image">\n    <img src="https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-docker-small.png" width="10%" /></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="5%" alt="" />\n  <a href="https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/" title="AWS Quickstart Guide">\n    <img src="https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-aws-small.png" width="10%" /></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="5%" alt="" />\n  <a href="https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/" title="GCP Quickstart Guide">\n    <img src="https://github.com/ultralytics/assets/releases/download/v0.0.0/logo-gcp-small.png" width="10%" /></a>\n</div>\n\n## ðŸ¤ Contribute\n\nWe welcome your contributions! Making YOLOv5 accessible and effective is a community effort. Please see our [Contributing Guide](https://docs.ultralytics.com/help/contributing/) to get started. Share your feedback through the [YOLOv5 Survey](https://www.ultralytics.com/survey?utm_source=github&utm_medium=social&utm_campaign=Survey). Thank you to all our contributors for making YOLOv5 better!\n\n[![Ultralytics open-source contributors](https://raw.githubusercontent.com/ultralytics/assets/main/im/image-contributors.png)](https://github.com/ultralytics/yolov5/graphs/contributors)\n\n## ðŸ“œ License\n\nUltralytics provides two licensing options to meet different needs:\n\n- **AGPL-3.0 License**: An [OSI-approved](https://opensource.org/license/agpl-v3) open-source license ideal for academic research, personal projects, and testing. It promotes open collaboration and knowledge sharing. See the [LICENSE](https://github.com/ultralytics/yolov5/blob/master/LICENSE) file for details.\n- **Enterprise License**: Tailored for commercial applications, this license allows seamless integration of Ultralytics software and AI models into commercial products and services, bypassing the open-source requirements of AGPL-3.0. For commercial use cases, please contact us via [Ultralytics Licensing](https://www.ultralytics.com/license).\n\n## ðŸ“§ Contact\n\nFor bug reports and feature requests related to YOLOv5, please visit [GitHub Issues](https://github.com/ultralytics/yolov5/issues). For general questions, discussions, and community support, join our [Discord server](https://discord.com/invite/ultralytics)!\n\n<br>\n<div align="center">\n  <a href="https://github.com/ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="3%" alt="Ultralytics GitHub"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space">\n  <a href="https://www.linkedin.com/company/ultralytics/"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="3%" alt="Ultralytics LinkedIn"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space">\n  <a href="https://twitter.com/ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="3%" alt="Ultralytics Twitter"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space">\n  <a href="https://youtube.com/ultralytics?sub_confirmation=1"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="3%" alt="Ultralytics YouTube"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space">\n  <a href="https://www.tiktok.com/@ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="3%" alt="Ultralytics TikTok"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space">\n  <a href="https://ultralytics.com/bilibili"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="3%" alt="Ultralytics BiliBili"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space">\n  <a href="https://discord.com/invite/ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="3%" alt="Ultralytics Discord"></a>\n</div>\n', '{"language":"Python","stars":56296,"forks":17361,"watchers":56296,"open_issues":126,"topics":["coreml","deep-learning","ios","machine-learning","ml","object-detection","onnx","pytorch","tflite","ultralytics","yolo","yolov3","yolov5"],"default_branch":"master","size_kb":17378,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:ultralytics","source_url":"https://github.com/ultralytics/ultralytics"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:google:automl","source_url":"https://github.com/google/automl"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"}]', NULL, 'AGPL-3.0', 'approved', 80, '9a848e05fd14c29ce842644eef01a8c4', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ultralytics-yolov5 from https://github.com/ultralytics.png
Image converted to WebP: data/images/github-ultralytics-yolov5.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ageitgey-face-recognition', 'github--ageitgey--face-recognition', 'face_recognition', 'ageitgey', '_You can also read a translated version of this file in Chinese ç®€ä½“ä¸­æ–‡ç‰ˆ or in Korean í•œêµ­ì–´ or in Japanese æ—¥æœ¬èªž._ Recognize and manipulate faces from Python or from the command line with the world''s simplest face recognition library. Built using dlib''s state-of-the-art face recognition built with deep learning. The model has an accuracy of 99.38% on the Labeled Faces in the Wild benchmark. This also provides a simple command line tool that lets you do face recognition on a folder of images from the...', '["face-detection","face-recognition","machine-learning","python","python"]', 'other', 55868, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ageitgey/face_recognition","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Face Recognition\n\n_You can also read a translated version of this file [in Chinese ç®€ä½“ä¸­æ–‡ç‰ˆ](https://github.com/ageitgey/face_recognition/blob/master/README_Simplified_Chinese.md) or [in Korean í•œêµ­ì–´](https://github.com/ageitgey/face_recognition/blob/master/README_Korean.md) or [in Japanese æ—¥æœ¬èªž](https://github.com/m-i-k-i/face_recognition/blob/master/README_Japanese.md)._\n\nRecognize and manipulate faces from Python or from the command line with\nthe world''s simplest face recognition library.\n\nBuilt using [dlib](http://dlib.net/)''s state-of-the-art face recognition\nbuilt with deep learning. The model has an accuracy of 99.38% on the\n[Labeled Faces in the Wild](http://vis-www.cs.umass.edu/lfw/) benchmark.\n\nThis also provides a simple `face_recognition` command line tool that lets\nyou do face recognition on a folder of images from the command line!\n\n\n[![PyPI](https://img.shields.io/pypi/v/face_recognition.svg)](https://pypi.python.org/pypi/face_recognition)\n[![Build Status](https://github.com/ageitgey/face_recognition/workflows/CI/badge.svg?branch=master&event=push)](https://github.com/ageitgey/face_recognition/actions?query=workflow%3ACI)\n[![Documentation Status](https://readthedocs.org/projects/face-recognition/badge/?version=latest)](http://face-recognition.readthedocs.io/en/latest/?badge=latest)\n\n## Features\n\n#### Find faces in pictures\n\nFind all the faces that appear in a picture:\n\n![](https://cloud.githubusercontent.com/assets/896692/23625227/42c65360-025d-11e7-94ea-b12f28cb34b4.png)\n\n```python\nimport face_recognition\nimage = face_recognition.load_image_file("your_file.jpg")\nface_locations = face_recognition.face_locations(image)\n```\n\n#### Find and manipulate facial features in pictures\n\nGet the locations and outlines of each person''s eyes, nose, mouth and chin.\n\n![](https://cloud.githubusercontent.com/assets/896692/23625282/7f2d79dc-025d-11e7-8728-d8924596f8fa.png)\n\n```python\nimport face_recognition\nimage = face_recognition.load_image_file("your_file.jpg")\nface_landmarks_list = face_recognition.face_landmarks(image)\n```\n\nFinding facial features is super useful for lots of important stuff. But you can also use it for really stupid stuff\nlike applying [digital make-up](https://github.com/ageitgey/face_recognition/blob/master/examples/digital_makeup.py) (think ''Meitu''):\n\n![](https://cloud.githubusercontent.com/assets/896692/23625283/80638760-025d-11e7-80a2-1d2779f7ccab.png)\n\n#### Identify faces in pictures\n\nRecognize who appears in each photo.\n\n![](https://cloud.githubusercontent.com/assets/896692/23625229/45e049b6-025d-11e7-89cc-8a71cf89e713.png)\n\n```python\nimport face_recognition\nknown_image = face_recognition.load_image_file("biden.jpg")\nunknown_image = face_recognition.load_image_file("unknown.jpg")\n\nbiden_encoding = face_recognition.face_encodings(known_image)[0]\nunknown_encoding = face_recognition.face_encodings(unknown_image)[0]\n\nresults = face_recognition.compare_faces([biden_encoding], unknown_encoding)\n```\n\nYou can even use this library with other Python libraries to do real-time face recognition:\n\n![](https://cloud.githubusercontent.com/assets/896692/24430398/36f0e3f0-13cb-11e7-8258-4d0c9ce1e419.gif)\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_webcam_faster.py) for the code.\n\n## Online Demos\n\nUser-contributed shared Jupyter notebook demo (not officially supported): [![Deepnote](https://beta.deepnote.org/buttons/try-in-a-jupyter-notebook.svg)](https://beta.deepnote.org/launch?template=face_recognition)\n\n## Installation\n\n### Requirements\n\n  * Python 3.3+ or Python 2.7\n  * macOS or Linux (Windows not officially supported, but might work)\n\n### Installation Options:\n\n#### Installing on Mac or Linux\n\nFirst, make sure you have dlib already installed with Python bindings:\n\n  * [How to install dlib from source on macOS or Ubuntu](https://gist.github.com/ageitgey/629d75c1baac34dfa5ca2a1928a7aeaf)\n  \nThen, make sure you have cmake installed:  \n \n```brew install cmake```\n\nFinally, install this module from pypi using `pip3` (or `pip2` for Python 2):\n\n```bash\npip3 install face_recognition\n```\n\nAlternatively, you can try this library with [Docker](https://www.docker.com/), see [this section](#deployment).\n\nIf you are having trouble with installation, you can also try out a\n[pre-configured VM](https://medium.com/@ageitgey/try-deep-learning-in-python-now-with-a-fully-pre-configured-vm-1d97d4c3e9b).\n\n#### Installing on an Nvidia Jetson Nano board\n\n * [Jetson Nano installation instructions](https://medium.com/@ageitgey/build-a-hardware-based-face-recognition-system-for-150-with-the-nvidia-jetson-nano-and-python-a25cb8c891fd)\n   * Please follow the instructions in the article carefully. There is current a bug in the CUDA libraries on the Jetson Nano that will cause this library to fail silently if you don''t follow the instructions in the article to comment out a line in dlib and recompile it.\n\n#### Installing on Raspberry Pi 2+\n\n  * [Raspberry Pi 2+ installation instructions](https://gist.github.com/ageitgey/1ac8dbe8572f3f533df6269dab35df65)\n\n#### Installing on FreeBSD\n\n```bash\npkg install graphics/py-face_recognition\n```\n\n#### Installing on Windows\n\nWhile Windows isn''t officially supported, helpful users have posted instructions on how to install this library:\n\n  * [@masoudr''s Windows 10 installation guide (dlib + face_recognition)](https://github.com/ageitgey/face_recognition/issues/175#issue-257710508)\n\n#### Installing a pre-configured Virtual Machine image\n\n  * [Download the pre-configured VM image](https://medium.com/@ageitgey/try-deep-learning-in-python-now-with-a-fully-pre-configured-vm-1d97d4c3e9b) (for VMware Player or VirtualBox).\n\n## Usage\n\n### Command-Line Interface\n\nWhen you install `face_recognition`, you get two simple command-line \nprograms:\n\n* `face_recognition` - Recognize faces in a photograph or folder full for \n   photographs.\n* `face_detection` - Find faces in a photograph or folder full for photographs.\n\n#### `face_recognition` command line tool\n\nThe `face_recognition` command lets you recognize faces in a photograph or \nfolder full  for photographs.\n\nFirst, you need to provide a folder with one picture of each person you\nalready know. There should be one image file for each person with the\nfiles named according to who is in the picture:\n\n![known](https://cloud.githubusercontent.com/assets/896692/23582466/8324810e-00df-11e7-82cf-41515eba704d.png)\n\nNext, you need a second folder with the files you want to identify:\n\n![unknown](https://cloud.githubusercontent.com/assets/896692/23582465/81f422f8-00df-11e7-8b0d-75364f641f58.png)\n\nThen in you simply run the command `face_recognition`, passing in\nthe folder of known people and the folder (or single image) with unknown\npeople and it tells you who is in each image:\n\n```bash\n$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/\n\n/unknown_pictures/unknown.jpg,Barack Obama\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person\n```\n\nThere''s one line in the output for each face. The data is comma-separated\nwith the filename and the name of the person found.\n\nAn `unknown_person` is a face in the image that didn''t match anyone in\nyour folder of known people.\n\n#### `face_detection` command line tool\n\nThe `face_detection` command lets you find the location (pixel coordinatates) \nof any faces in an image.\n\nJust run the command `face_detection`, passing in a folder of images \nto check (or a single image):\n\n```bash\n$ face_detection  ./folder_with_pictures/\n\nexamples/image1.jpg,65,215,169,112\nexamples/image2.jpg,62,394,211,244\nexamples/image2.jpg,95,941,244,792\n```\n\nIt prints one line for each face that was detected. The coordinates\nreported are the top, right, bottom and left coordinates of the face (in pixels).\n \n##### Adjusting Tolerance / Sensitivity\n\nIf you are getting multiple matches for the same person, it might be that\nthe people in your photos look very similar and a lower tolerance value\nis needed to make face comparisons more strict.\n\nYou can do that with the `--tolerance` parameter. The default tolerance\nvalue is 0.6 and lower numbers make face comparisons more strict:\n\n```bash\n$ face_recognition --tolerance 0.54 ./pictures_of_people_i_know/ ./unknown_pictures/\n\n/unknown_pictures/unknown.jpg,Barack Obama\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person\n```\n\nIf you want to see the face distance calculated for each match in order\nto adjust the tolerance setting, you can use `--show-distance true`:\n\n```bash\n$ face_recognition --show-distance true ./pictures_of_people_i_know/ ./unknown_pictures/\n\n/unknown_pictures/unknown.jpg,Barack Obama,0.378542298956785\n/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person,None\n```\n\n##### More Examples\n\nIf you simply want to know the names of the people in each photograph but don''t\ncare about file names, you could do this:\n\n```bash\n$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/ | cut -d '','' -f2\n\nBarack Obama\nunknown_person\n```\n\n##### Speeding up Face Recognition\n\nFace recognition can be done in parallel if you have a computer with\nmultiple CPU cores. For example, if your system has 4 CPU cores, you can\nprocess about 4 times as many images in the same amount of time by using\nall your CPU cores in parallel.\n\nIf you are using Python 3.4 or newer, pass in a `--cpus <number_of_cpu_cores_to_use>` parameter:\n\n```bash\n$ face_recognition --cpus 4 ./pictures_of_people_i_know/ ./unknown_pictures/\n```\n\nYou can also pass in `--cpus -1` to use all CPU cores in your system.\n\n#### Python Module\n\nYou can import the `face_recognition` module and then easily manipulate\nfaces with just a couple of lines of code. It''s super easy!\n\nAPI Docs: [https://face-recognition.readthedocs.io](https://face-recognition.readthedocs.io/en/latest/face_recognition.html).\n\n##### Automatically find all the faces in an image\n\n```python\nimport face_recognition\n\nimage = face_recognition.load_image_file("my_picture.jpg")\nface_locations = face_recognition.face_locations(image)\n\n# face_locations is now an array listing the co-ordinates of each face!\n```\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture.py)\n to try it out.\n\nYou can also opt-in to a somewhat more accurate deep-learning-based face detection model.\n\nNote: GPU acceleration (via NVidia''s CUDA library) is required for good\nperformance with this model. You''ll also want to enable CUDA support\nwhen compliling `dlib`.\n\n```python\nimport face_recognition\n\nimage = face_recognition.load_image_file("my_picture.jpg")\nface_locations = face_recognition.face_locations(image, model="cnn")\n\n# face_locations is now an array listing the co-ordinates of each face!\n```\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture_cnn.py)\n to try it out.\n\nIf you have a lot of images and a GPU, you can also\n[find faces in batches](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_batches.py).\n\n##### Automatically locate the facial features of a person in an image\n\n```python\nimport face_recognition\n\nimage = face_recognition.load_image_file("my_picture.jpg")\nface_landmarks_list = face_recognition.face_landmarks(image)\n\n# face_landmarks_list is now an array with the locations of each facial feature in each face.\n# face_landmarks_list[0][''left_eye''] would be the location and outline of the first person''s left eye.\n```\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/find_facial_features_in_picture.py)\n to try it out.\n\n##### Recognize faces in images and identify who they are\n\n```python\nimport face_recognition\n\npicture_of_me = face_recognition.load_image_file("me.jpg")\nmy_face_encoding = face_recognition.face_encodings(picture_of_me)[0]\n\n# my_face_encoding now contains a universal ''encoding'' of my facial features that can be compared to any other picture of a face!\n\nunknown_picture = face_recognition.load_image_file("unknown.jpg")\nunknown_face_encoding = face_recognition.face_encodings(unknown_picture)[0]\n\n# Now we can see the two face encodings are of the same person with `compare_faces`!\n\nresults = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)\n\nif results[0] == True:\n    print("It''s a picture of me!")\nelse:\n    print("It''s not a picture of me!")\n```\n\nSee [this example](https://github.com/ageitgey/face_recognition/blob/master/examples/recognize_faces_in_pictures.py)\n to try it out.\n\n## Python Code Examples\n\nAll the examples are available [here](https://github.com/ageitgey/face_recognition/tree/master/examples).\n\n\n#### Face Detection\n\n* [Find faces in a photograph](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture.py)\n* [Find faces in a photograph (using deep learning)](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_picture_cnn.py)\n* [Find faces in batches of images w/ GPU (using deep learning)](https://github.com/ageitgey/face_recognition/blob/master/examples/find_faces_in_batches.py)\n* [Blur all the faces in a live video using your webcam (Requires OpenCV to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/blur_faces_on_webcam.py)\n\n#### Facial Features\n\n* [Identify specific facial features in a photograph](https://github.com/ageitgey/face_recognition/blob/master/examples/find_facial_features_in_picture.py)\n* [Apply (horribly ugly) digital make-up](https://github.com/ageitgey/face_recognition/blob/master/examples/digital_makeup.py)\n\n#### Facial Recognition\n\n* [Find and recognize unknown faces in a photograph based on photographs of known people](https://github.com/ageitgey/face_recognition/blob/master/examples/recognize_faces_in_pictures.py)\n* [Identify and draw boxes around each person in a photo](https://github.com/ageitgey/face_recognition/blob/master/examples/identify_and_draw_boxes_on_faces.py)\n* [Compare faces by numeric face distance instead of only True/False matches](https://github.com/ageitgey/face_recognition/blob/master/examples/face_distance.py)\n* [Recognize faces in live video using your webcam - Simple / Slower Version (Requires OpenCV to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_webcam.py)\n* [Recognize faces in live video using your webcam - Faster Version (Requires OpenCV to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_webcam_faster.py)\n* [Recognize faces in a video file and write out new video file (Requires OpenCV to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_from_video_file.py)\n* [Recognize faces on a Raspberry Pi w/ camera](https://github.com/ageitgey/face_recognition/blob/master/examples/facerec_on_raspberry_pi.py)\n* [Run a web service to recognize faces via HTTP (Requires Flask to be installed)](https://github.com/ageitgey/face_recognition/blob/master/examples/web_service_example.py)\n* [Recognize faces with a K-nearest neighbors classifier](https://github.com/ageitgey/face_recognition/blob/master/examples/face_recognition_knn.py)\n* [Train multiple images per person then recognize faces using a SVM](https://github.com/ageitgey/face_recognition/blob/master/examples/face_recognition_svm.py)\n\n## Creating a Standalone Executable\nIf you want to create a standalone executable that can run without the need to install `python` or `face_recognition`, you can use [PyInstaller](https://github.com/pyinstaller/pyinstaller). However, it requires some custom configuration to work with this library. See [this issue](https://github.com/ageitgey/face_recognition/issues/357) for how to do it.\n\n## Articles and Guides that cover `face_recognition`\n\n- My article on how Face Recognition works: [Modern Face Recognition with Deep Learning](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78)\n  - Covers the algorithms and how they generally work\n- [Face recognition with OpenCV, Python, and deep learning](https://www.pyimagesearch.com/2018/06/18/face-recognition-with-opencv-python-and-deep-learning/) by Adrian Rosebrock\n  - Covers how to use face recognition in practice\n- [Raspberry Pi Face Recognition](https://www.pyimagesearch.com/2018/06/25/raspberry-pi-face-recognition/) by Adrian Rosebrock\n  - Covers how to use this on a Raspberry Pi\n- [Face clustering with Python](https://www.pyimagesearch.com/2018/07/09/face-clustering-with-python/) by Adrian Rosebrock\n  - Covers how to automatically cluster photos based on who appears in each photo using unsupervised learning\n\n## How Face Recognition Works\n\nIf you want to learn how face location and recognition work instead of\ndepending on a black box library, [read my article](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78).\n\n## Caveats\n\n* The face recognition model is trained on adults and does not work very well on children. It tends to mix\n  up children quite easy using the default comparison threshold of 0.6.\n* Accuracy may vary between ethnic groups. Please see [this wiki page](https://github.com/ageitgey/face_recognition/wiki/Face-Recognition-Accuracy-Problems#question-face-recognition-works-well-with-european-individuals-but-overall-accuracy-is-lower-with-asian-individuals) for more details.\n\n## <a name="deployment">Deployment to Cloud Hosts (Heroku, AWS, etc)</a>\n\nSince `face_recognition` depends on `dlib` which is written in C++, it can be tricky to deploy an app\nusing it to a cloud hosting provider like Heroku or AWS.\n\nTo make things easier, there''s an example Dockerfile in this repo that shows how to run an app built with\n`face_recognition` in a [Docker](https://www.docker.com/) container. With that, you should be able to deploy\nto any service that supports Docker images.\n\nYou can try the Docker image locally by running: `docker-compose up --build`\n\nThere are also [several prebuilt Docker images.](docker/README.md)\n\nLinux users with a GPU (drivers >= 384.81) and [Nvidia-Docker](https://github.com/NVIDIA/nvidia-docker) installed can run the example on the GPU: Open the [docker-compose.yml](docker-compose.yml) file and uncomment the `dockerfile: Dockerfile.gpu` and `runtime: nvidia` lines.\n\n## Having problems?\n\nIf you run into problems, please read the [Common Errors](https://github.com/ageitgey/face_recognition/wiki/Common-Errors) section of the wiki before filing a github issue.\n\n## Thanks\n\n* Many, many thanks to [Davis King](https://github.com/davisking) ([@nulhom](https://twitter.com/nulhom))\n  for creating dlib and for providing the trained facial feature detection and face encoding models\n  used in this library. For more information on the ResNet that powers the face encodings, check out\n  his [blog post](http://blog.dlib.net/2017/02/high-quality-face-recognition-with-deep.html).\n* Thanks to everyone who works on all the awesome Python data science libraries like numpy, scipy, scikit-image,\n  pillow, etc, etc that makes this kind of stuff so easy and fun in Python.\n* Thanks to [Cookiecutter](https://github.com/audreyr/cookiecutter) and the\n  [audreyr/cookiecutter-pypackage](https://github.com/audreyr/cookiecutter-pypackage) project template\n  for making Python project packaging way more tolerable.\n', '{"language":"Python","stars":55868,"forks":13707,"watchers":55868,"open_issues":831,"topics":["face-detection","face-recognition","machine-learning","python"],"default_branch":"master","size_kb":103959,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:m-i-k-i:face_recognition","source_url":"https://github.com/m-i-k-i/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:pyinstaller:pyinstaller","source_url":"https://github.com/pyinstaller/pyinstaller"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:NVIDIA:nvidia-docker","source_url":"https://github.com/NVIDIA/nvidia-docker"},{"type":"has_code","target_id":"github:ageitgey:face_recognition","source_url":"https://github.com/ageitgey/face_recognition"},{"type":"has_code","target_id":"github:audreyr:cookiecutter","source_url":"https://github.com/audreyr/cookiecutter"},{"type":"has_code","target_id":"github:audreyr:cookiecutter-pypackage","source_url":"https://github.com/audreyr/cookiecutter-pypackage"}]', NULL, 'MIT', 'approved', 80, '215a6d5dc5625c370947928436292b6d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ageitgey-face-recognition from https://github.com/ageitgey.png
Image converted to WebP: data/images/github-ageitgey-face-recognition.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-OpenBB-finance-OpenBB', 'github--openbb-finance--openbb', 'OpenBB', 'OpenBB-finance', '<br /> <img src="https://github.com/OpenBB-finance/OpenBB/blob/develop/images/odp-light.svg?raw=true#gh-light-mode-only" alt="Open Data Platform by OpenBB logo" width="600"> <img src="https://github.com/OpenBB-finance/OpenBB/blob/develop/images/odp-dark.svg?raw=true#gh-dark-mode-only" alt="Open Data Platform by OpenBB logo" width="600"> <br /> <br /> <a href="https://codespaces.new/OpenBB-finance/OpenBB"> <img src="https://github.com/codespaces/badge.svg" height="20" /> </a> <a target="_blank...', '["ai","crypto","derivatives","economics","equity","finance","fixed-income","machine-learning","openbb","options","python","quantitative-finance","stocks","python"]', 'other', 55233, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/OpenBB-finance/OpenBB","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<br />\n<img src="https://github.com/OpenBB-finance/OpenBB/blob/develop/images/odp-light.svg?raw=true#gh-light-mode-only" alt="Open Data Platform by OpenBB logo" width="600">\n<img src="https://github.com/OpenBB-finance/OpenBB/blob/develop/images/odp-dark.svg?raw=true#gh-dark-mode-only" alt="Open Data Platform by OpenBB logo" width="600">\n<br />\n<br />\n\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/openbb_finance.svg?style=social&label=Follow%20%40openbb_finance)](https://x.com/openbb_finance)\n[![Discord Shield](https://img.shields.io/discord/831165782750789672)](https://discord.com/invite/xPHTuHCmuV)\n[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/OpenBB-finance/OpenBB)\n<a href="https://codespaces.new/OpenBB-finance/OpenBB">\n  <img src="https://github.com/codespaces/badge.svg" height="20" />\n</a>\n<a target="_blank" href="https://colab.research.google.com/github/OpenBB-finance/OpenBB/blob/develop/examples/googleColab.ipynb">\n  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>\n</a>\n[![PyPI](https://img.shields.io/pypi/v/openbb?color=blue&label=PyPI%20Package)](https://pypi.org/project/openbb/)\n\nOpen Data Platform by OpenBB (ODP) is the open-source toolset that helps data engineers integrate proprietary, licensed, and public data sources into downstream applications like AI copilots and research dashboards.\n\nODP operates as the "connect once, consume everywhere" infrastructure layer that consolidates and exposes data to multiple surfaces at once: Python environments for quants, OpenBB Workspace and Excel for analysts, MCP servers for AI agents, and REST APIs for other applications.\n\n<a href="https://pro.openbb.co">\n  <div align="center">\n  <img src="https://openbb-cms.directus.app/assets/70b971ef-7a7e-486e-b5ae-1cc602f2162c.png" alt="Logo" width="1000">\n  </div>\n</a>\n\nGet started with: `pip install openbb`\n\n```python\nfrom openbb import obb\noutput = obb.equity.price.historical("AAPL")\ndf = output.to_dataframe()\n```\n\nData integrations available can be found here: <https://docs.openbb.co/python/reference>\n\n---\n\n## OpenBB Workspace\n\nWhile the Open Data Platform provides the open-source data integration foundation, **OpenBB Workspace** offers the enterprise UI for analysts to visualize datasets and leverage AI agents. The platform''s "connect once, consume everywhere" architecture enables seamless integration between the two.\n\nYou can find OpenBB Workspace at <https://pro.openbb.co>.\n<a href="https://pro.openbb.co">\n  <div align="center">\n  <img src="https://openbb-cms.directus.app/assets/f69b6aaf-0821-4bc8-a43c-715e03a924ef.png" alt="Logo" width="1000">\n  </div>\n</a>\n\nData integration:\n\n- You can learn more about adding data to the OpenBB workspace from the [docs](https://docs.openbb.co/workspace) or [this open source repository](https://github.com/OpenBB-finance/backends-for-openbb).\n\nAI Agents integration:\n\n- You can learn more about adding AI agents to the OpenBB workspace from [this open source repository](https://github.com/OpenBB-finance/agents-for-openbb).\n\n### Integrating Open Data Platform to the OpenBB Workspace\n\nConnect this library to the OpenBB Workspace with a few simple commands, in a Python (3.9.21 - 3.12) environment.\n\n#### Run an ODP backend\n\n- Install the packages.\n\n```sh\npip install "openbb[all]"\n```\n\n- Start the API server over localhost.\n\n```sh\nopenbb-api\n```\n\nThis will launch a FastAPI server, via Uvicorn, at `127.0.0.1:6900`.\n\nYou can check that it works by going to <http://127.0.0.1:6900>.\n\n#### Integrate the ODP Backend to OpenBB Workspace\n\nSign-in to the [OpenBB Workspace](https://pro.openbb.co/), and follow the following steps:\n\n![CleanShot 2025-05-17 at 09 51 56@2x](https://github.com/user-attachments/assets/75cffb4a-5e95-470a-b9d0-6ffd4067e069)\n\n1. Go to the "Apps" tab\n2. Click on "Connect backend"\n3. Fill in the form with:\n   Name: Open Data Platform\n   URL: <http://127.0.0.1:6900>\n4. Click on "Test". You should get a "Test successful" with the number of apps found.\n5. Click on "Add".\n\nThat''s it.\n\n---\n\n<!-- TABLE OF CONTENTS -->\n<details closed="closed">\n  <summary><h2 style="display: inline-block">Table of Contents</h2></summary>\n  <ol>\n    <li><a href="#1-installation">Installation</a></li>\n    <li><a href="#2-contributing">Contributing</a></li>\n    <li><a href="#3-license">License</a></li>\n    <li><a href="#4-disclaimer">Disclaimer</a></li>\n    <li><a href="#5-contacts">Contacts</a></li>\n    <li><a href="#6-star-history">Star History</a></li>\n    <li><a href="#7-contributors">Contributors</a></li>\n  </ol>\n</details>\n\n## 1. Installation\n\nThe ODP Python Package can be installed from [PyPI package](https://pypi.org/project/openbb/) by running `pip install openbb`\n\nor by cloning the repository directly with `git clone https://github.com/OpenBB-finance/OpenBB.git`.\n\nPlease find more about the installation process, in the [OpenBB Documentation](https://docs.openbb.co/python/installation).\n\n### ODP CLI installation\n\nThe ODP CLI is a command-line interface that allows you to access the ODP directly from your command line.\n\nIt can be installed by running `pip install openbb-cli`\n\nor by cloning the repository directly with  `git clone https://github.com/OpenBB-finance/OpenBB.git`.\n\nPlease find more about the installation process in the [OpenBB Documentation](https://docs.openbb.co/cli/installation).\n\n## 2. Contributing\n\nThere are three main ways of contributing to this project. (Hopefully you have starred the project by now â­ï¸)\n\n### Become a Contributor\n\n- More information on our [Developer Documentation](https://docs.openbb.co/python/developer).\n\n### Create a GitHub ticket\n\nBefore creating a ticket make sure the one you are creating doesn''t exist already [among the existing issues](https://github.com/OpenBB-finance/OpenBB/issues)\n\n- [Report bug](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&labels=bug&template=bug_report.md&title=%5BBug%5D)\n- [Suggest improvement](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&labels=enhancement&template=enhancement.md&title=%5BIMPROVE%5D)\n- [Request a feature](https://github.com/OpenBB-finance/OpenBB/issues/new?assignees=&labels=new+feature&template=feature_request.md&title=%5BFR%5D)\n\n### Provide feedback\n\nWe are most active on [our Discord](https://openbb.co/discord), but feel free to reach out to us in any of [our social media](https://openbb.co/links) for feedback.\n\n## 3. License\n\nDistributed under the AGPLv3 License. See\n[LICENSE](https://github.com/OpenBB-finance/OpenBB/blob/main/LICENSE) for more information.\n\n## 4. Disclaimer\n\nTrading in financial instruments involves high risks including the risk of losing some, or all, of your investment\namount, and may not be suitable for all investors.\n\nBefore deciding to trade in a financial instrument you should be fully informed of the risks and costs associated with trading the financial markets, carefully consider your investment objectives, level of experience, and risk appetite, and seek professional advice where needed.\n\nThe data contained in the Open Data Platform is not necessarily accurate.\n\nOpenBB and any provider of the data contained in this website will not accept liability for any loss or damage as a result of your trading, or your reliance on the information displayed.\n\nAll names, logos, and brands of third parties that may be referenced in our sites, products or documentation are trademarks of their respective owners. Unless otherwise specified, OpenBB and its products and services are not endorsed by, sponsored by, or affiliated with these third parties.\n\nOur use of these names, logos, and brands is for identification purposes only, and does not imply any such endorsement, sponsorship, or affiliation.\n\n## 5. Contacts\n\nIf you have any questions about the platform or anything OpenBB, feel free to email us at `support@openbb.co`\n\nIf you want to say hi, or are interested in partnering with us, feel free to reach us at `hello@openbb.co`\n\nAny of our social media platforms: [openbb.co/links](https://openbb.co/links)\n\n## 6. Star History\n\nThis is a proxy of our growth and that we are just getting started.\n\nBut for more metrics important to us check [openbb.co/open](https://openbb.co/open).\n\n[![Star History Chart](https://api.star-history.com/svg?repos=openbb-finance/OpenBB&type=Date&theme=dark)](https://api.star-history.com/svg?repos=openbb-finance/OpenBB&type=Date&theme=dark)\n\n## 7. Contributors\n\nOpenBB wouldn''t be OpenBB without you. If we are going to disrupt financial industry, every contribution counts. Thank you for being part of this journey.\n\n<a href="https://github.com/OpenBB-finance/OpenBB/graphs/contributors">\n   <img src="https://contributors-img.web.app/image?repo=OpenBB-finance/OpenBB" width="800"/>\n</a>\n\n<!-- MARKDOWN LINKS & IMAGES -->\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n\n[contributors-shield]: https://img.shields.io/github/contributors/OpenBB-finance/OpenBB.svg?style=for-the-badge\n[contributors-url]: https://github.com/OpenBB-finance/OpenBB/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/OpenBB-finance/OpenBB.svg?style=for-the-badge\n[forks-url]: https://github.com/OpenBB-finance/OpenBB/network/members\n[stars-shield]: https://img.shields.io/github/stars/OpenBB-finance/OpenBB.svg?style=for-the-badge\n[stars-url]: https://github.com/OpenBB-finance/OpenBB/stargazers\n[issues-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBB.svg?style=for-the-badge&color=blue\n[issues-url]: https://github.com/OpenBB-finance/OpenBB/issues\n[bugs-open-shield]: https://img.shields.io/github/issues/OpenBB-finance/OpenBB/bug.svg?style=for-the-badge&color=yellow\n[bugs-open-url]: https://github.com/OpenBB-finance/OpenBB/issues?q=is%3Aissue+label%3Abug+is%3Aopen\n[bugs-closed-shield]: https://img.shields.io/github/issues-closed/OpenBB-finance/OpenBB/bug.svg?style=for-the-badge&color=success\n[bugs-closed-url]: https://github.com/OpenBB-finance/OpenBB/issues?q=is%3Aissue+label%3Abug+is%3Aclosed\n[license-shield]: https://img.shields.io/github/license/OpenBB-finance/OpenBB.svg?style=for-the-badge\n[license-url]: https://github.com/OpenBB-finance/OpenBB/blob/main/LICENSE.txt\n[linkedin-shield]: https://img.shields.io/badge/-LinkedIn-black.svg?style=for-the-badge&logo=linkedin&colorB=555\n[linkedin-url]: https://linkedin.com/in/DidierRLopes\n', '{"language":"Python","stars":55233,"forks":5358,"watchers":55233,"open_issues":50,"topics":["ai","crypto","derivatives","economics","equity","finance","fixed-income","machine-learning","openbb","options","python","quantitative-finance","stocks"],"default_branch":"develop","size_kb":2383294,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB","source_url":"https://github.com/OpenBB-finance/OpenBB"},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB","source_url":"https://github.com/OpenBB-finance/OpenBB"},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB","source_url":"https://github.com/OpenBB-finance/OpenBB"},{"type":"has_code","target_id":"github:codespaces:badge.svg\"","source_url":"https://github.com/codespaces/badge.svg\""},{"type":"has_code","target_id":"github:OpenBB-finance:backends-for-openbb","source_url":"https://github.com/OpenBB-finance/backends-for-openbb"},{"type":"has_code","target_id":"github:OpenBB-finance:agents-for-openbb","source_url":"https://github.com/OpenBB-finance/agents-for-openbb"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB.git`.","source_url":"https://github.com/OpenBB-finance/OpenBB.git`."},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB.git`.","source_url":"https://github.com/OpenBB-finance/OpenBB.git`."},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB","source_url":"https://github.com/OpenBB-finance/OpenBB"},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB","source_url":"https://github.com/OpenBB-finance/OpenBB"},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB","source_url":"https://github.com/OpenBB-finance/OpenBB"},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB","source_url":"https://github.com/OpenBB-finance/OpenBB"},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB","source_url":"https://github.com/OpenBB-finance/OpenBB"},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB","source_url":"https://github.com/OpenBB-finance/OpenBB"},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB","source_url":"https://github.com/OpenBB-finance/OpenBB"},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB","source_url":"https://github.com/OpenBB-finance/OpenBB"},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB","source_url":"https://github.com/OpenBB-finance/OpenBB"},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB","source_url":"https://github.com/OpenBB-finance/OpenBB"},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB","source_url":"https://github.com/OpenBB-finance/OpenBB"},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB","source_url":"https://github.com/OpenBB-finance/OpenBB"},{"type":"has_code","target_id":"github:OpenBB-finance:OpenBB","source_url":"https://github.com/OpenBB-finance/OpenBB"}]', NULL, 'NOASSERTION', 'approved', 80, 'd9fdde35be4412a465184548b06cedaf', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-OpenBB-finance-OpenBB from https://github.com/OpenBB-finance.png
Image converted to WebP: data/images/github-OpenBB-finance-OpenBB.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-deepfakes-faceswap', 'github--deepfakes--faceswap', 'faceswap', 'deepfakes', '<p align="center"> <a href="https://faceswap.dev"><img src="https://i.imgur.com/zHvjHnb.png"></img></a> <br />FaceSwap is a tool that utilizes deep learning to recognize and swap faces in pictures and videos. </p> <p align="center"> <img src = "https://i.imgur.com/nWHFLDf.jpg"></img> </p> <p align="center"> <a href="https://www.patreon.com/bePatron?u=23238350"><img src="https://c5.patreon.com/external/logo/become_a_patron_button.png"></img></a> &nbsp;&nbsp;&nbsp;&nbsp;<a href="https://discord...', '["deep-face-swap","deep-learning","deep-neural-networks","deepface","deepfakes","deeplearning","face-swap","faceswap","fakeapp","machine-learning","myfakeapp","neural-nets","neural-networks","openfaceswap","python"]', 'other', 54775, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/deepfakes/faceswap","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# deepfakes_faceswap\n\n### Important information for **Patreon** and **PayPal** supporters. Please see this forum post: https://forum.faceswap.dev/viewtopic.php?f=14&t=3120\n\n<p align="center">\n  <a href="https://faceswap.dev"><img src="https://i.imgur.com/zHvjHnb.png"></img></a>\n<br />FaceSwap is a tool that utilizes deep learning to recognize and swap faces in pictures and videos.\n</p>\n<p align="center">\n<img src = "https://i.imgur.com/nWHFLDf.jpg"></img>\n</p>\n\n<p align="center">\n<a href="https://www.patreon.com/bePatron?u=23238350"><img src="https://c5.patreon.com/external/logo/become_a_patron_button.png"></img></a>\n&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://discord.gg/FC54sYg"><img src="https://i.imgur.com/gIpztkv.png"></img></a></p>\n\n<p align="center">\n  <a href="https://www.dailymotion.com/video/x810mot"><img src="https://user-images.githubusercontent.com/36920800/178301720-b69841bb-a1ca-4c20-91db-a2a10f5692ca.png"></img></a>\n<br />Emma Stone/Scarlett Johansson FaceSwap using the Phaze-A model\n</p>\n\n<p align="center">\n  <a href="https://www.youtube.com/watch?v=r1jng79a5xc"><img src="https://img.youtube.com/vi/r1jng79a5xc/0.jpg"></img></a>\n<br />Jennifer Lawrence/Steve Buscemi FaceSwap using the Villain model\n</p>\n\n\n![Build Status](https://github.com/deepfakes/faceswap/actions/workflows/pytest.yml/badge.svg) [![Documentation Status](https://readthedocs.org/projects/faceswap/badge/?version=latest)](https://faceswap.readthedocs.io/en/latest/?badge=latest)\n\nMake sure you check out [INSTALL.md](INSTALL.md) before getting started.\n\n- [deepfakes\_faceswap](#deepfakes_faceswap)\n    - [Important information for **Patreon** and **PayPal** supporters. Please see this forum post: https://forum.faceswap.dev/viewtopic.php?f=14\&t=3120](#important-information-for-patreon-and-paypal-supporters-please-see-this-forum-post-httpsforumfaceswapdevviewtopicphpf14t3120)\n- [Manifesto](#manifesto)\n  - [FaceSwap has ethical uses.](#faceswap-has-ethical-uses)\n- [How To setup and run the project](#how-to-setup-and-run-the-project)\n- [Overview](#overview)\n  - [Extract](#extract)\n  - [Train](#train)\n  - [Convert](#convert)\n  - [GUI](#gui)\n- [General notes:](#general-notes)\n- [Help I need support!](#help-i-need-support)\n  - [Discord Server](#discord-server)\n  - [FaceSwap Forum](#faceswap-forum)\n- [Donate](#donate)\n  - [Patreon](#patreon)\n  - [One time Donations](#one-time-donations)\n    - [@torzdf](#torzdf)\n    - [@andenixa](#andenixa)\n- [How to contribute](#how-to-contribute)\n  - [For people interested in the generative models](#for-people-interested-in-the-generative-models)\n  - [For devs](#for-devs)\n  - [For non-dev advanced users](#for-non-dev-advanced-users)\n  - [For end-users](#for-end-users)\n- [About machine learning](#about-machine-learning)\n  - [How does a computer know how to recognize/shape faces? How does machine learning work? What is a neural network?](#how-does-a-computer-know-how-to-recognizeshape-faces-how-does-machine-learning-work-what-is-a-neural-network)\n\n# Manifesto\n\n## FaceSwap has ethical uses.\n\nWhen faceswapping was first developed and published, the technology was groundbreaking, it was a huge step in AI development. It was also completely ignored outside of academia because the code was confusing and fragmentary. It required a thorough understanding of complicated AI techniques and took a lot of effort to figure it out. Until one individual brought it together into a single, cohesive collection. It ran, it worked, and as is so often the way with new technology emerging on the internet, it was immediately used to create inappropriate content. Despite the inappropriate uses the software was given originally, it was the first AI code that anyone could download, run and learn by experimentation without having a Ph.D. in math, computer theory, psychology, and more. Before "deepfakes" these techniques were like black magic, only practiced by those who could understand all of the inner workings as described in esoteric and endlessly complicated books and papers.\n\n"Deepfakes" changed all that and anyone could participate in AI development. To us, developers, the release of this code opened up a fantastic learning opportunity. It allowed us to build on ideas developed by others, collaborate with a variety of skilled coders, experiment with AI whilst learning new skills and ultimately contribute towards an emerging technology which will only see more mainstream use as it progresses.\n\nAre there some out there doing horrible things with similar software? Yes. And because of this, the developers have been following strict ethical standards. Many of us don''t even use it to create videos, we just tinker with the code to see what it does. Sadly, the media concentrates only on the unethical uses of this software. That is, unfortunately, the nature of how it was first exposed to the public, but it is not representative of why it was created, how we use it now, or what we see in its future. Like any technology, it can be used for good or it can be abused. It is our intention to develop FaceSwap in a way that its potential for abuse is minimized whilst maximizing its potential as a tool for learning, experimenting and, yes, for legitimate faceswapping.\n\nWe are not trying to denigrate celebrities or to demean anyone. We are programmers, we are engineers, we are Hollywood VFX artists, we are activists, we are hobbyists, we are human beings. To this end, we feel that it''s time to come out with a standard statement of what this software is and isn''t as far as us developers are concerned.\n\n- FaceSwap is not for creating inappropriate content.\n- FaceSwap is not for changing faces without consent or with the intent of hiding its use.\n- FaceSwap is not for any illicit, unethical, or questionable purposes.\n- FaceSwap exists to experiment and discover AI techniques, for social or political commentary, for movies, and for any number of ethical and reasonable uses.\n\nWe are very troubled by the fact that FaceSwap can be used for unethical and disreputable things. However, we support the development of tools and techniques that can be used ethically as well as provide education and experience in AI for anyone who wants to learn it hands-on. We will take a zero tolerance approach to anyone using this software for any unethical purposes and will actively discourage any such uses.\n\n# How To setup and run the project\nFaceSwap is a Python program that will run on multiple Operating Systems including Windows, Linux, and MacOS.\n\nSee [INSTALL.md](INSTALL.md) for full installation instructions. You will need a modern GPU with CUDA support for best performance. Many AMD GPUs are supported through DirectML (Windows) and ROCm (Linux).\n\n# Overview\nThe project has multiple entry points. You will have to:\n - Gather photos and/or videos\n - **Extract** faces from your raw photos\n - **Train** a model on the faces extracted from the photos/videos\n - **Convert** your sources with the model\n\nCheck out [USAGE.md](USAGE.md) for more detailed instructions.\n\n## Extract\nFrom your setup folder, run `python faceswap.py extract`. This will take photos from `src` folder and extract faces into `extract` folder.\n\n## Train\nFrom your setup folder, run `python faceswap.py train`. This will take photos from two folders containing pictures of both faces and train a model that will be saved inside the `models` folder.\n\n## Convert\nFrom your setup folder, run `python faceswap.py convert`. This will take photos from `original` folder and apply new faces into `modified` folder.\n\n## GUI\nAlternatively, you can run the GUI by running `python faceswap.py gui`\n\n# General notes:\n- All of the scripts mentioned have `-h`/`--help` options with arguments that they will accept. You''re smart, you can figure out how this works, right?!\n\nNB: there is a conversion tool for video. This can be accessed by running `python tools.py effmpeg -h`. Alternatively, you can use [ffmpeg](https://www.ffmpeg.org) to convert video into photos, process images, and convert images back to the video.\n\n\n**Some tips:**\n\nReusing existing models will train much faster than starting from nothing.\nIf there is not enough training data, start with someone who looks similar, then switch the data.\n\n# Help I need support!\n## Discord Server\nYour best bet is to join the [FaceSwap Discord server](https://discord.gg/FC54sYg) where there are plenty of users willing to help. Please note that, like this repo, this is a SFW Server!\n\n## FaceSwap Forum\nAlternatively, you can post questions in the [FaceSwap Forum](https://faceswap.dev/forum). Please do not post general support questions in this repo as they are liable to be deleted without response.\n\n# Donate\nThe developers work tirelessly to improve and develop FaceSwap. Many hours have been put in to provide the software as it is today, but this is an extremely time-consuming process with no financial reward. If you enjoy using the software, please consider donating to the devs, so they can spend more time implementing improvements.\n\n## Patreon\nThe best way to support us is through our Patreon page:\n\n[![become-a-patron](https://c5.patreon.com/external/logo/become_a_patron_button.png)](https://www.patreon.com/bePatron?u=23238350)\n\n## One time Donations\nAlternatively you can give a one off donation to any of our Devs:\n### @torzdf\n There is very little FaceSwap code that hasn''t been touched by torzdf. He is responsible for implementing the GUI, FAN aligner, MTCNN detector and porting the Villain, DFL-H128 and DFaker models to FaceSwap, as well as significantly improving many areas of the code.\n\n**Bitcoin:** bc1qpm22suz59ylzk0j7qk5e4c7cnkjmve2rmtrnc6\n\n**Ethereum:** 0xd3e954dC241B87C4E8E1A801ada485DC1d530F01\n\n**Monero:** 45dLrtQZ2pkHizBpt3P3yyJKkhcFHnhfNYPMSnz3yVEbdWm3Hj6Kr5TgmGAn3Far8LVaQf1th2n3DJVTRkfeB5ZkHxWozSX\n\n**Paypal:** [![torzdf](https://www.paypalobjects.com/en_GB/i/btn/btn_donate_SM.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=JZ8PP3YE9J62L)\n\n### @andenixa\nCreator of the Unbalanced and OHR models, as well as expanding various capabilities within the training process. Andenixa is currently working on new models and will take requests for donations.\n\n**Paypal:** [![andenixa](https://www.paypalobjects.com/en_GB/i/btn/btn_donate_SM.gif)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=NRVLQYGS6NWTU)\n\n# How to contribute\n\n## For people interested in the generative models\n - Go to the ''faceswap-model'' to discuss/suggest/commit alternatives to the current algorithm.\n\n## For devs\n - Read this README entirely\n - Fork the repo\n - Play with it\n - Check issues with the ''dev'' tag\n - For devs more interested in computer vision and openCV, look at issues with the ''opencv'' tag. Also feel free to add your own alternatives/improvements\n\n## For non-dev advanced users\n - Read this README entirely\n - Clone the repo\n - Play with it\n - Check issues with the ''advuser'' tag\n - Also go to the ''[faceswap Forum](https://faceswap.dev/forum)'' and help others.\n\n## For end-users\n - Get the code here and play with it if you can\n - You can also go to the [faceswap Forum](https://faceswap.dev/forum) and help or get help from others.\n - Be patient. This is a relatively new technology for developers as well. Much effort is already being put into making this program easy to use for the average user. It just takes time!\n - **Notice** Any issue related to running the code has to be opened in the [faceswap Forum](https://faceswap.dev/forum)!\n\n# About machine learning\n\n## How does a computer know how to recognize/shape faces? How does machine learning work? What is a neural network?\nIt''s complicated. Here''s a good video that makes the process understandable:\n[![How Machines Learn](https://img.youtube.com/vi/R9OHn5ZF4Uo/0.jpg)](https://www.youtube.com/watch?v=R9OHn5ZF4Uo)\n\nHere''s a slightly more in depth video that tries to explain the basic functioning of a neural network:\n[![How Machines Learn](https://img.youtube.com/vi/aircAruvnKk/0.jpg)](https://www.youtube.com/watch?v=aircAruvnKk)\n\ntl;dr: training data + trial and error\n', '{"language":"Python","stars":54775,"forks":13428,"watchers":54775,"open_issues":45,"topics":["deep-face-swap","deep-learning","deep-neural-networks","deepface","deepfakes","deeplearning","face-swap","faceswap","fakeapp","machine-learning","myfakeapp","neural-nets","neural-networks","openfaceswap"],"default_branch":"master","size_kb":203532,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:deepfakes:faceswap","source_url":"https://github.com/deepfakes/faceswap"}]', NULL, 'GPL-3.0', 'approved', 80, 'e3d50e138153c437d24c77a13c2a2786', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-deepfakes-faceswap from https://github.com/deepfakes.png
Image converted to WebP: data/images/github-deepfakes-faceswap.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ultralytics-ultralytics', 'github--ultralytics--ultralytics', 'ultralytics', 'ultralytics', '<div align="center"> <p> <a href="https://www.ultralytics.com/events/yolovision?utm_source=github&utm_medium=org&utm_campaign=yv25_event" target="_blank"> <img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" alt="Ultralytics YOLO banner"></a> </p> ä¸­æ–‡ | í•œêµ­ì–´ | æ—¥æœ¬èªž | Ð ÑƒÑÑÐºÐ¸Ð¹ | Deutsch | FranÃ§ais | EspaÃ±ol | PortuguÃªs | TÃ¼rkÃ§e | Tiáº¿ng Viá»‡t | Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© <br> <div> <a href="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml"><img s...', '["cli","computer-vision","deep-learning","hub","image-classification","instance-segmentation","machine-learning","object-detection","pose-estimation","python","pytorch","rotated-object-detection","segment-anything","tracking","ultralytics","yolo","yolo-world","yolo11","yolo26","yolov8","python"]', 'other', 49658, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ultralytics/ultralytics","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n  <p>\n    <a href="https://www.ultralytics.com/events/yolovision?utm_source=github&utm_medium=org&utm_campaign=yv25_event" target="_blank">\n      <img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png" alt="Ultralytics YOLO banner"></a>\n  </p>\n\n[ä¸­æ–‡](https://docs.ultralytics.com/zh/) | [í•œêµ­ì–´](https://docs.ultralytics.com/ko/) | [æ—¥æœ¬èªž](https://docs.ultralytics.com/ja/) | [Ð ÑƒÑÑÐºÐ¸Ð¹](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [FranÃ§ais](https://docs.ultralytics.com/fr/) | [EspaÃ±ol](https://docs.ultralytics.com/es) | [PortuguÃªs](https://docs.ultralytics.com/pt/) | [TÃ¼rkÃ§e](https://docs.ultralytics.com/tr/) | [Tiáº¿ng Viá»‡t](https://docs.ultralytics.com/vi/) | [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](https://docs.ultralytics.com/ar/) <br>\n\n<div>\n    <a href="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml"><img src="https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg" alt="Ultralytics CI"></a>\n    <a href="https://clickpy.clickhouse.com/dashboard/ultralytics"><img src="https://static.pepy.tech/badge/ultralytics" alt="Ultralytics Downloads"></a>\n    <a href="https://zenodo.org/badge/latestdoi/264818686"><img src="https://zenodo.org/badge/264818686.svg" alt="Ultralytics YOLO Citation"></a>\n    <a href="https://discord.com/invite/ultralytics"><img alt="Ultralytics Discord" src="https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue"></a>\n    <a href="https://community.ultralytics.com/"><img alt="Ultralytics Forums" src="https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue"></a>\n    <a href="https://www.reddit.com/r/ultralytics/"><img alt="Ultralytics Reddit" src="https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue"></a>\n    <br>\n    <a href="https://console.paperspace.com/github/ultralytics/ultralytics"><img src="https://assets.paperspace.io/img/gradient-badge.svg" alt="Run Ultralytics on Gradient"></a>\n    <a href="https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open Ultralytics In Colab"></a>\n    <a href="https://www.kaggle.com/models/ultralytics/yolo11"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" alt="Open Ultralytics In Kaggle"></a>\n    <a href="https://mybinder.org/v2/gh/ultralytics/ultralytics/HEAD?labpath=examples%2Ftutorial.ipynb"><img src="https://mybinder.org/badge_logo.svg" alt="Open Ultralytics In Binder"></a>\n</div>\n</div>\n<br>\n\n[Ultralytics](https://www.ultralytics.com/) creates cutting-edge, state-of-the-art (SOTA) [YOLO models](https://www.ultralytics.com/yolo) built on years of foundational research in computer vision and AI. Constantly updated for performance and flexibility, our models are **fast**, **accurate**, and **easy to use**. They excel at [object detection](https://docs.ultralytics.com/tasks/detect/), [tracking](https://docs.ultralytics.com/modes/track/), [instance segmentation](https://docs.ultralytics.com/tasks/segment/), [image classification](https://docs.ultralytics.com/tasks/classify/), and [pose estimation](https://docs.ultralytics.com/tasks/pose/) tasks.\n\nFind detailed documentation in the [Ultralytics Docs](https://docs.ultralytics.com/). Get support via [GitHub Issues](https://github.com/ultralytics/ultralytics/issues/new/choose). Join discussions on [Discord](https://discord.com/invite/ultralytics), [Reddit](https://www.reddit.com/r/ultralytics/), and the [Ultralytics Community Forums](https://community.ultralytics.com/)!\n\nRequest an Enterprise License for commercial use at [Ultralytics Licensing](https://www.ultralytics.com/license).\n\n<a href="https://docs.ultralytics.com/models/yolo11/" target="_blank">\n  <img width="100%" src="https://raw.githubusercontent.com/ultralytics/assets/refs/heads/main/yolo/performance-comparison.png" alt="YOLO11 performance plots">\n</a>\n\n<div align="center">\n  <a href="https://github.com/ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="2%" alt="Ultralytics GitHub"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">\n  <a href="https://www.linkedin.com/company/ultralytics/"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="2%" alt="Ultralytics LinkedIn"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">\n  <a href="https://twitter.com/ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="2%" alt="Ultralytics Twitter"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">\n  <a href="https://www.youtube.com/ultralytics?sub_confirmation=1"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="2%" alt="Ultralytics YouTube"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">\n  <a href="https://www.tiktok.com/@ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="2%" alt="Ultralytics TikTok"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">\n  <a href="https://ultralytics.com/bilibili"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="2%" alt="Ultralytics BiliBili"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="2%" alt="space">\n  <a href="https://discord.com/invite/ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="2%" alt="Ultralytics Discord"></a>\n</div>\n\n## ðŸ“„ Documentation\n\nSee below for quickstart installation and usage examples. For comprehensive guidance on training, validation, prediction, and deployment, refer to our full [Ultralytics Docs](https://docs.ultralytics.com/).\n\n<details open>\n<summary>Install</summary>\n\nInstall the `ultralytics` package, including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml), in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Ultralytics Downloads](https://static.pepy.tech/badge/ultralytics)](https://clickpy.clickhouse.com/dashboard/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)\n\n```bash\npip install ultralytics\n```\n\nFor alternative installation methods, including [Conda](https://anaconda.org/conda-forge/ultralytics), [Docker](https://hub.docker.com/r/ultralytics/ultralytics), and building from source via Git, please consult the [Quickstart Guide](https://docs.ultralytics.com/quickstart/).\n\n[![Conda Version](https://img.shields.io/conda/vn/conda-forge/ultralytics?logo=condaforge)](https://anaconda.org/conda-forge/ultralytics) [![Docker Image Version](https://img.shields.io/docker/v/ultralytics/ultralytics?sort=semver&logo=docker)](https://hub.docker.com/r/ultralytics/ultralytics) [![Ultralytics Docker Pulls](https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker)](https://hub.docker.com/r/ultralytics/ultralytics)\n\n</details>\n\n<details open>\n<summary>Usage</summary>\n\n### CLI\n\nYou can use Ultralytics YOLO directly from the Command Line Interface (CLI) with the `yolo` command:\n\n```bash\n# Predict using a pretrained YOLO model (e.g., YOLO11n) on an image\nyolo predict model=yolo11n.pt source=''https://ultralytics.com/images/bus.jpg''\n```\n\nThe `yolo` command supports various tasks and modes, accepting additional arguments like `imgsz=640`. Explore the YOLO [CLI Docs](https://docs.ultralytics.com/usage/cli/) for more examples.\n\n### Python\n\nUltralytics YOLO can also be integrated directly into your Python projects. It accepts the same [configuration arguments](https://docs.ultralytics.com/usage/cfg/) as the CLI:\n\n```python\nfrom ultralytics import YOLO\n\n# Load a pretrained YOLO11n model\nmodel = YOLO("yolo11n.pt")\n\n# Train the model on the COCO8 dataset for 100 epochs\ntrain_results = model.train(\n    data="coco8.yaml",  # Path to dataset configuration file\n    epochs=100,  # Number of training epochs\n    imgsz=640,  # Image size for training\n    device="cpu",  # Device to run on (e.g., ''cpu'', 0, [0,1,2,3])\n)\n\n# Evaluate the model''s performance on the validation set\nmetrics = model.val()\n\n# Perform object detection on an image\nresults = model("path/to/image.jpg")  # Predict on an image\nresults[0].show()  # Display results\n\n# Export the model to ONNX format for deployment\npath = model.export(format="onnx")  # Returns the path to the exported model\n```\n\nDiscover more examples in the YOLO [Python Docs](https://docs.ultralytics.com/usage/python/).\n\n</details>\n\n## âœ¨ Models\n\nUltralytics supports a wide range of YOLO models, from early versions like [YOLOv3](https://docs.ultralytics.com/models/yolov3/) to the latest [YOLO11](https://docs.ultralytics.com/models/yolo11/). The tables below showcase YOLO11 models pretrained on the [COCO](https://docs.ultralytics.com/datasets/detect/coco/) dataset for [Detection](https://docs.ultralytics.com/tasks/detect/), [Segmentation](https://docs.ultralytics.com/tasks/segment/), and [Pose Estimation](https://docs.ultralytics.com/tasks/pose/). Additionally, [Classification](https://docs.ultralytics.com/tasks/classify/) models pretrained on the [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/) dataset are available. [Tracking](https://docs.ultralytics.com/modes/track/) mode is compatible with all Detection, Segmentation, and Pose models. All [Models](https://docs.ultralytics.com/models/) are automatically downloaded from the latest Ultralytics [release](https://github.com/ultralytics/assets/releases) upon first use.\n\n<a href="https://docs.ultralytics.com/tasks/" target="_blank">\n    <img width="100%" src="https://github.com/ultralytics/docs/releases/download/0/ultralytics-yolov8-tasks-banner.avif" alt="Ultralytics YOLO supported tasks">\n</a>\n<br>\n<br>\n\n<details open><summary>Detection (COCO)</summary>\n\nExplore the [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for usage examples. These models are trained on the [COCO dataset](https://cocodataset.org/), featuring 80 object classes.\n\n| Model                                                                                | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| ------------------------------------------------------------------------------------ | --------------------- | -------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt) | 640                   | 39.5                 | 56.1 Â± 0.8                     | 1.5 Â± 0.0                           | 2.6                | 6.5               |\n| [YOLO11s](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt) | 640                   | 47.0                 | 90.0 Â± 1.2                     | 2.5 Â± 0.0                           | 9.4                | 21.5              |\n| [YOLO11m](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt) | 640                   | 51.5                 | 183.2 Â± 2.0                    | 4.7 Â± 0.1                           | 20.1               | 68.0              |\n| [YOLO11l](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l.pt) | 640                   | 53.4                 | 238.6 Â± 1.4                    | 6.2 Â± 0.1                           | 25.3               | 86.9              |\n| [YOLO11x](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt) | 640                   | 54.7                 | 462.8 Â± 6.7                    | 11.3 Â± 0.2                          | 56.9               | 194.9             |\n\n- **mAP<sup>val</sup>** values refer to single-model single-scale performance on the [COCO val2017](https://cocodataset.org/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. <br>Reproduce with `yolo val detect data=coco.yaml device=0`\n- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. <br>Reproduce with `yolo val detect data=coco.yaml batch=1 device=0|cpu`\n\n</details>\n\n<details><summary>Segmentation (COCO)</summary>\n\nRefer to the [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for usage examples. These models are trained on [COCO-Seg](https://docs.ultralytics.com/datasets/segment/coco/), including 80 classes.\n\n| Model                                                                                        | size<br><sup>(pixels) | mAP<sup>box<br>50-95 | mAP<sup>mask<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| -------------------------------------------------------------------------------------------- | --------------------- | -------------------- | --------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-seg.pt) | 640                   | 38.9                 | 32.0                  | 65.9 Â± 1.1                     | 1.8 Â± 0.0                           | 2.9                | 9.7               |\n| [YOLO11s-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-seg.pt) | 640                   | 46.6                 | 37.8                  | 117.6 Â± 4.9                    | 2.9 Â± 0.0                           | 10.1               | 33.0              |\n| [YOLO11m-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-seg.pt) | 640                   | 51.5                 | 41.5                  | 281.6 Â± 1.2                    | 6.3 Â± 0.1                           | 22.4               | 113.2             |\n| [YOLO11l-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-seg.pt) | 640                   | 53.4                 | 42.9                  | 344.2 Â± 3.2                    | 7.8 Â± 0.2                           | 27.6               | 132.2             |\n| [YOLO11x-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-seg.pt) | 640                   | 54.7                 | 43.8                  | 664.5 Â± 3.2                    | 15.8 Â± 0.7                          | 62.1               | 296.4             |\n\n- **mAP<sup>val</sup>** values are for single-model single-scale on the [COCO val2017](https://cocodataset.org/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. <br>Reproduce with `yolo val segment data=coco.yaml device=0`\n- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. <br>Reproduce with `yolo val segment data=coco.yaml batch=1 device=0|cpu`\n\n</details>\n\n<details><summary>Classification (ImageNet)</summary>\n\nConsult the [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for usage examples. These models are trained on [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/), covering 1000 classes.\n\n| Model                                                                                        | size<br><sup>(pixels) | acc<br><sup>top1 | acc<br><sup>top5 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) at 224 |\n| -------------------------------------------------------------------------------------------- | --------------------- | ---------------- | ---------------- | ------------------------------ | ----------------------------------- | ------------------ | ------------------------ |\n| [YOLO11n-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt) | 224                   | 70.0             | 89.4             | 5.0 Â± 0.3                      | 1.1 Â± 0.0                           | 2.8                | 0.5                      |\n| [YOLO11s-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-cls.pt) | 224                   | 75.4             | 92.7             | 7.9 Â± 0.2                      | 1.3 Â± 0.0                           | 6.7                | 1.6                      |\n| [YOLO11m-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-cls.pt) | 224                   | 77.3             | 93.9             | 17.2 Â± 0.4                     | 2.0 Â± 0.0                           | 11.6               | 4.9                      |\n| [YOLO11l-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-cls.pt) | 224                   | 78.3             | 94.3             | 23.2 Â± 0.3                     | 2.8 Â± 0.0                           | 14.1               | 6.2                      |\n| [YOLO11x-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-cls.pt) | 224                   | 79.5             | 94.9             | 41.4 Â± 0.9                     | 3.8 Â± 0.0                           | 29.6               | 13.6                     |\n\n- **acc** values represent model accuracy on the [ImageNet](https://www.image-net.org/) dataset validation set. <br>Reproduce with `yolo val classify data=path/to/ImageNet device=0`\n- **Speed** metrics are averaged over ImageNet val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. <br>Reproduce with `yolo val classify data=path/to/ImageNet batch=1 device=0|cpu`\n\n</details>\n\n<details><summary>Pose (COCO)</summary>\n\nSee the [Pose Estimation Docs](https://docs.ultralytics.com/tasks/pose/) for usage examples. These models are trained on [COCO-Pose](https://docs.ultralytics.com/datasets/pose/coco/), focusing on the ''person'' class.\n\n| Model                                                                                          | size<br><sup>(pixels) | mAP<sup>pose<br>50-95 | mAP<sup>pose<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| ---------------------------------------------------------------------------------------------- | --------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt) | 640                   | 50.0                  | 81.0               | 52.4 Â± 0.5                     | 1.7 Â± 0.0                           | 2.9                | 7.4               |\n| [YOLO11s-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-pose.pt) | 640                   | 58.9                  | 86.3               | 90.5 Â± 0.6                     | 2.6 Â± 0.0                           | 9.9                | 23.1              |\n| [YOLO11m-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-pose.pt) | 640                   | 64.9                  | 89.4               | 187.3 Â± 0.8                    | 4.9 Â± 0.1                           | 20.9               | 71.4              |\n| [YOLO11l-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-pose.pt) | 640                   | 66.1                  | 89.9               | 247.7 Â± 1.1                    | 6.4 Â± 0.1                           | 26.1               | 90.3              |\n| [YOLO11x-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-pose.pt) | 640                   | 69.5                  | 91.1               | 488.0 Â± 13.9                   | 12.1 Â± 0.2                          | 58.8               | 202.8             |\n\n- **mAP<sup>val</sup>** values are for single-model single-scale on the [COCO Keypoints val2017](https://docs.ultralytics.com/datasets/pose/coco/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. <br>Reproduce with `yolo val pose data=coco-pose.yaml device=0`\n- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. <br>Reproduce with `yolo val pose data=coco-pose.yaml batch=1 device=0|cpu`\n\n</details>\n\n<details><summary>Oriented Bounding Boxes (DOTAv1)</summary>\n\nCheck the [OBB Docs](https://docs.ultralytics.com/tasks/obb/) for usage examples. These models are trained on [DOTAv1](https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10/), including 15 classes.\n\n| Model                                                                                        | size<br><sup>(pixels) | mAP<sup>test<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>T4 TensorRT10<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n| -------------------------------------------------------------------------------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n| [YOLO11n-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-obb.pt) | 1024                  | 78.4               | 117.6 Â± 0.8                    | 4.4 Â± 0.0                           | 2.7                | 16.8              |\n| [YOLO11s-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-obb.pt) | 1024                  | 79.5               | 219.4 Â± 4.0                    | 5.1 Â± 0.0                           | 9.7                | 57.1              |\n| [YOLO11m-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-obb.pt) | 1024                  | 80.9               | 562.8 Â± 2.9                    | 10.1 Â± 0.4                          | 20.9               | 182.8             |\n| [YOLO11l-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-obb.pt) | 1024                  | 81.0               | 712.5 Â± 5.0                    | 13.5 Â± 0.6                          | 26.1               | 231.2             |\n| [YOLO11x-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-obb.pt) | 1024                  | 81.3               | 1408.6 Â± 7.7                   | 28.6 Â± 1.0                          | 58.8               | 519.1             |\n\n- **mAP<sup>test</sup>** values are for single-model multiscale performance on the [DOTAv1 test set](https://captain-whu.github.io/DOTA/dataset.html). <br>Reproduce by `yolo val obb data=DOTAv1.yaml device=0 split=test` and submit merged results to the [DOTA evaluation server](https://captain-whu.github.io/DOTA/evaluation.html).\n- **Speed** metrics are averaged over [DOTAv1 val images](https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10) using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. <br>Reproduce by `yolo val obb data=DOTAv1.yaml batch=1 device=0|cpu`\n\n</details>\n\n## ðŸ§© Integrations\n\nOur key integrations with leading AI platforms extend the functionality of Ultralytics'' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with partners like [Weights & Biases](https://docs.ultralytics.com/integrations/weights-biases/), [Comet ML](https://docs.ultralytics.com/integrations/comet/), [Roboflow](https://docs.ultralytics.com/integrations/roboflow/), and [Intel OpenVINO](https://docs.ultralytics.com/integrations/openvino/), can optimize your AI workflow. Explore more at [Ultralytics Integrations](https://docs.ultralytics.com/integrations/).\n\n<a href="https://docs.ultralytics.com/integrations/" target="_blank">\n    <img width="100%" src="https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png" alt="Ultralytics active learning integrations">\n</a>\n<br>\n<br>\n\n<div align="center">\n  <a href="https://www.ultralytics.com/hub">\n    <img src="https://github.com/ultralytics/assets/raw/main/partners/logo-ultralytics-hub.png" width="10%" alt="Ultralytics HUB logo"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space">\n  <a href="https://docs.ultralytics.com/integrations/weights-biases/">\n    <img src="https://github.com/ultralytics/assets/raw/main/partners/logo-wb.png" width="10%" alt="Weights & Biases logo"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space">\n  <a href="https://docs.ultralytics.com/integrations/comet/">\n    <img src="https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png" width="10%" alt="Comet ML logo"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="15%" height="0" alt="space">\n  <a href="https://docs.ultralytics.com/integrations/neural-magic/">\n    <img src="https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png" width="10%" alt="Neural Magic logo"></a>\n</div>\n\n|                                                       Ultralytics HUB ðŸŒŸ                                                        |                                                          Weights & Biases                                                           |                                                                              Comet                                                                              |                                                        Neural Magic                                                         |\n| :-----------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------: |\n| Streamline YOLO workflows: Label, train, and deploy effortlessly with [Ultralytics HUB](https://hub.ultralytics.com/). Try now! | Track experiments, hyperparameters, and results with [Weights & Biases](https://docs.ultralytics.com/integrations/weights-biases/). | Free forever, [Comet ML](https://docs.ultralytics.com/integrations/comet/) lets you save YOLO models, resume training, and interactively visualize predictions. | Run YOLO inference up to 6x faster with [Neural Magic DeepSparse](https://docs.ultralytics.com/integrations/neural-magic/). |\n\n## ðŸŒŸ Ultralytics HUB\n\nExperience seamless AI with [Ultralytics HUB](https://hub.ultralytics.com/), the all-in-one platform for data visualization, training YOLO models, and deploymentâ€”no coding required. Transform images into actionable insights and bring your AI visions to life effortlessly using our cutting-edge platform and user-friendly [Ultralytics App](https://www.ultralytics.com/app-install). Start your journey for **Free** today!\n\n<a href="https://www.ultralytics.com/hub" target="_blank">\n<img width="100%" src="https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png" alt="Ultralytics HUB preview image"></a>\n\n## ðŸ¤ Contribute\n\nWe thrive on community collaboration! Ultralytics YOLO wouldn''t be the SOTA framework it is without contributions from developers like you. Please see our [Contributing Guide](https://docs.ultralytics.com/help/contributing/) to get started. We also welcome your feedbackâ€”share your experience by completing our [Survey](https://www.ultralytics.com/survey?utm_source=github&utm_medium=social&utm_campaign=Survey). A huge **Thank You** ðŸ™ to everyone who contributes!\n\n<!-- SVG image from https://opencollective.com/ultralytics/contributors.svg?width=1280 -->\n\n[![Ultralytics open-source contributors](https://raw.githubusercontent.com/ultralytics/assets/main/im/image-contributors.png)](https://github.com/ultralytics/ultralytics/graphs/contributors)\n\nWe look forward to your contributions to help make the Ultralytics ecosystem even better!\n\n## ðŸ“œ License\n\nUltralytics offers two licensing options to suit different needs:\n\n- **AGPL-3.0 License**: This [OSI-approved](https://opensource.org/license/agpl-v3) open-source license is perfect for students, researchers, and enthusiasts. It encourages open collaboration and knowledge sharing. See the [LICENSE](https://github.com/ultralytics/ultralytics/blob/main/LICENSE) file for full details.\n- **Ultralytics Enterprise License**: Designed for commercial use, this license allows for the seamless integration of Ultralytics software and AI models into commercial products and services, bypassing the open-source requirements of AGPL-3.0. If your use case involves commercial deployment, please contact us via [Ultralytics Licensing](https://www.ultralytics.com/license).\n\n## ðŸ“ž Contact\n\nFor bug reports and feature requests related to Ultralytics software, please visit [GitHub Issues](https://github.com/ultralytics/ultralytics/issues). For questions, discussions, and community support, join our active communities on [Discord](https://discord.com/invite/ultralytics), [Reddit](https://www.reddit.com/r/ultralytics/), and the [Ultralytics Community Forums](https://community.ultralytics.com/). We''re here to help with all things Ultralytics!\n\n<br>\n<div align="center">\n  <a href="https://github.com/ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png" width="3%" alt="Ultralytics GitHub"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space">\n  <a href="https://www.linkedin.com/company/ultralytics/"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png" width="3%" alt="Ultralytics LinkedIn"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space">\n  <a href="https://twitter.com/ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png" width="3%" alt="Ultralytics Twitter"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space">\n  <a href="https://www.youtube.com/ultralytics?sub_confirmation=1"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png" width="3%" alt="Ultralytics YouTube"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space">\n  <a href="https://www.tiktok.com/@ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png" width="3%" alt="Ultralytics TikTok"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space">\n  <a href="https://ultralytics.com/bilibili"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-bilibili.png" width="3%" alt="Ultralytics BiliBili"></a>\n  <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="3%" alt="space">\n  <a href="https://discord.com/invite/ultralytics"><img src="https://github.com/ultralytics/assets/raw/main/social/logo-social-discord.png" width="3%" alt="Ultralytics Discord"></a>\n</div>\n', '{"language":"Python","stars":49658,"forks":9596,"watchers":49658,"open_issues":304,"topics":["cli","computer-vision","deep-learning","hub","image-classification","instance-segmentation","machine-learning","object-detection","pose-estimation","python","pytorch","rotated-object-detection","segment-anything","tracking","ultralytics","yolo","yolo-world","yolo11","yolo26","yolov8"],"default_branch":"main","size_kb":41948,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:ultralytics:ultralytics","source_url":"https://github.com/ultralytics/ultralytics"},{"type":"has_code","target_id":"github:ultralytics:ultralytics","source_url":"https://github.com/ultralytics/ultralytics"},{"type":"has_code","target_id":"github:ultralytics:ultralytics","source_url":"https://github.com/ultralytics/ultralytics"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:ultralytics","source_url":"https://github.com/ultralytics/ultralytics"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:docs","source_url":"https://github.com/ultralytics/docs"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:ultralytics","source_url":"https://github.com/ultralytics/ultralytics"},{"type":"has_code","target_id":"github:ultralytics:ultralytics","source_url":"https://github.com/ultralytics/ultralytics"},{"type":"has_code","target_id":"github:ultralytics:ultralytics","source_url":"https://github.com/ultralytics/ultralytics"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"},{"type":"has_code","target_id":"github:ultralytics:assets","source_url":"https://github.com/ultralytics/assets"}]', NULL, 'AGPL-3.0', 'approved', 80, 'acb4c75428b951472e29f9a0e2f3e609', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ultralytics-ultralytics from https://github.com/ultralytics.png
Image converted to WebP: data/images/github-ultralytics-ultralytics.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Avik-Jain-100-Days-Of-ML-Code', 'github--avik-jain--100-days-of-ml-code', '100-Days-Of-ML-Code', 'Avik-Jain', '100 Days of Machine Learning Coding as proposed by Siraj Raval Get the datasets from here Check out the code from here. <p align="center"> <img src="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%201.jpg"> </p> Check out the code from here. <p align="center"> <img src="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%202.jpg"> </p> Check out the code from here. <p align="center"> <img src="https://github.com/Avik-Jain/100-Days-Of-ML-Co...', '["100-days-of-code-log","100daysofcode","deep-learning","implementation","infographics","linear-algebra","linear-regression","logistic-regression","machine-learning","machine-learning-algorithms","naive-bayes-classifier","python","scikit-learn","siraj-raval","siraj-raval-challenge","support-vector-machines","svm","tutorial"]', 'other', 48971, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# 100-Days-Of-ML-Code\n\n100 Days of Machine Learning Coding as proposed by [Siraj Raval](https://github.com/llSourcell)\n\nGet the datasets from [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/tree/master/datasets)\n\n## Data PreProcessing | Day 1\nCheck out the code from [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day%201_Data%20PreProcessing.md).\n\n<p align="center">\n  <img src="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%201.jpg">\n</p>\n\n## Simple Linear Regression | Day 2\nCheck out the code from [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day2_Simple_Linear_Regression.md).\n\n<p align="center">\n  <img src="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%202.jpg">\n</p>\n\n## Multiple Linear Regression | Day 3\nCheck out the code from [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day3_Multiple_Linear_Regression.md).\n\n<p align="center">\n  <img src="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%203.jpg">\n</p>\n\n## Logistic Regression | Day 4\n\n<p align="center">\n  <img src="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%204.jpg">\n</p>\n\n## Logistic Regression | Day 5\nMoving forward into #100DaysOfMLCode today I dived into the deeper depth of what Logistic Regression actually is and what is the math involved behind it. Learned how cost function is calculated and then how to apply gradient descent algorithm to cost function to minimize the error in prediction.  \nDue to less time I will now be posting an infographic on alternate days.\nAlso if someone wants to help me out in documentaion of code and already has some experince in the field and knows Markdown for github please contact me on LinkedIn :) .\n\n## Implementing Logistic Regression | Day 6\nCheck out the Code [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day%206%20Logistic%20Regression.md)\n\n## K Nearest Neighbours | Day 7\n<p align="center">\n  <img src="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%207.jpg">\n</p>\n\n## Math Behind Logistic Regression | Day 8 \n\n#100DaysOfMLCode To clear my insights on logistic regression I was searching on the internet for some resource or article and I came across this article (https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc) by Saishruthi Swaminathan. \n\nIt gives a detailed description of Logistic Regression. Do check it out.\n\n## Support Vector Machines | Day 9\nGot an intution on what SVM is and how it is used to solve Classification problem.\n\n## SVM and KNN | Day 10\nLearned more about how SVM works and implementing the K-NN algorithm.\n\n## Implementation of K-NN | Day 11  \n\nImplemented the K-NN algorithm for classification. #100DaysOfMLCode \nSupport Vector Machine Infographic is halfway complete. Will update it tomorrow.\n\n## Support Vector Machines | Day 12\n<p align="center">\n  <img src="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2012.jpg">\n</p>\n\n## Naive Bayes Classifier | Day 13\n\nContinuing with #100DaysOfMLCode today I went through the Naive Bayes classifier.\nI am also implementing the SVM in python using scikit-learn. Will update the code soon.\n\n## Implementation of SVM | Day 14\nToday I implemented SVM on linearly related data. Used Scikit-Learn library. In Scikit-Learn we have SVC classifier which we use to achieve this task. Will be using kernel-trick on next implementation.\nCheck the code [here](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day%2013%20SVM.md).\n\n## Naive Bayes Classifier and Black Box Machine Learning | Day 15\nLearned about different types of naive bayes classifiers. Also started the lectures by [Bloomberg](https://bloomberg.github.io/foml/#home). First one in the playlist was Black Box Machine Learning. It gives the whole overview about prediction functions, feature extraction, learning algorithms, performance evaluation, cross-validation, sample bias, nonstationarity, overfitting, and hyperparameter tuning.\n\n## Implemented SVM using Kernel Trick | Day 16\nUsing Scikit-Learn library implemented SVM algorithm along with kernel function which maps our data points into higher dimension to find optimal hyperplane. \n\n## Started Deep learning Specialization on Coursera | Day 17\nCompleted the whole Week 1 and Week 2 on a single day. Learned Logistic regression as Neural Network. \n\n## Deep learning Specialization on Coursera | Day 18\nCompleted the Course 1 of the deep learning specialization. Implemented a neural net in python.\n\n## The Learning Problem , Professor Yaser Abu-Mostafa | Day 19\nStarted Lecture 1 of 18 of Caltech''s Machine Learning Course - CS 156 by Professor Yaser Abu-Mostafa. It was basically an introduction to the upcoming lectures. He also explained Perceptron Algorithm.\n\n## Started Deep learning Specialization Course 2 | Day 20\nCompleted the Week 1 of Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization.\n\n## Web Scraping | Day 21\nWatched some tutorials on how to do web scraping using Beautiful Soup in order to collect data for building a model.\n\n## Is Learning Feasible? | Day 22\nLecture 2 of 18 of Caltech''s Machine Learning Course - CS 156 by Professor Yaser Abu-Mostafa. Learned about Hoeffding Inequality.\n\n## Decision Trees | Day 23\n<p align="center">\n  <img src="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2023.jpg">\n</p>\n\n## Introduction To Statistical Learning Theory | Day 24\nLec 3 of Bloomberg ML course introduced some of the core concepts like input space, action space, outcome space, prediction functions, loss functions, and hypothesis spaces.\n\n## Implementing Decision Trees | Day 25\nCheck the code [here.](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day%2025%20Decision%20Tree.md)\n\n## Jumped To Brush up Linear Algebra | Day 26\nFound an amazing [channel](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw) on youtube 3Blue1Brown. It has a playlist called Essence of Linear Algebra. Started off by completing 4 videos which gave a complete overview of Vectors, Linear Combinations, Spans, Basis Vectors, Linear Transformations and Matrix Multiplication. \n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n\n## Jumped To Brush up Linear Algebra | Day 27\nContinuing with the playlist completed next 4 videos discussing topics 3D Transformations, Determinants, Inverse Matrix, Column Space, Null Space and Non-Square Matrices.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n\n## Jumped To Brush up Linear Algebra | Day 28\nIn the playlist of 3Blue1Brown completed another 3 videos from the essence of linear algebra. \nTopics covered were Dot Product and Cross Product.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n\n\n## Jumped To Brush up Linear Algebra | Day 29\nCompleted the whole playlist today, videos 12-14. Really an amazing playlist to refresh the concepts of Linear Algebra.\nTopics covered were the change of basis, Eigenvectors and Eigenvalues, and Abstract Vector Spaces.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n\n## Essence of calculus | Day 30\nCompleting the playlist - Essence of Linear Algebra by 3blue1brown a suggestion popped up by youtube regarding a series of videos again by the same channel 3Blue1Brown. Being already impressed by the previous series on Linear algebra I dived straight into it.\nCompleted about 5 videos on topics such as Derivatives, Chain Rule, Product Rule, and derivative of exponential.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)\n\n## Essence of calculus | Day 31\nWatched 2 Videos on topic Implicit Diffrentiation and Limits from the playlist Essence of Calculus.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)\n\n## Essence of calculus | Day 32\nWatched the remaining 4 videos covering topics Like Integration and Higher order derivatives.\n\nLink to the playlist [here.](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)\n\n## Random Forests | Day 33\n<p align="center">\n  <img src="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2033.jpg">\n</p>\n\n## Implementing Random Forests | Day 34\nCheck the code [here.](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Code/Day%2034%20Random_Forest.md)\n\n## But what *is* a Neural Network? | Deep learning, chapter 1  | Day 35\nAn Amazing Video on neural networks by 3Blue1Brown youtube channel. This video gives a good understanding of Neural Networks and uses Handwritten digit dataset to explain the concept. \nLink To the [video.](https://www.youtube.com/watch?v=aircAruvnKk&t=7s)\n\n## Gradient descent, how neural networks learn | Deep learning, chapter 2 | Day 36\nPart two of neural networks by 3Blue1Brown youtube channel. This video explains the concepts of Gradient Descent in an interesting way. 169 must watch and highly recommended.\nLink To the [video.](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n\n## What is backpropagation really doing? | Deep learning, chapter 3 | Day 37\nPart three of neural networks by 3Blue1Brown youtube channel. This video mostly discusses the partial derivatives and backpropagation.\nLink To the [video.](https://www.youtube.com/watch?v=Ilg3gGewQ5U)\n\n## Backpropagation calculus | Deep learning, chapter 4 | Day 38\nPart four of neural networks by 3Blue1Brown youtube channel. The goal here is to represent, in somewhat more formal terms, the intuition for how backpropagation works and the video moslty discusses the partial derivatives and backpropagation.\nLink To the [video.](https://www.youtube.com/watch?v=tIeHLnjs5U8)\n\n## Deep Learning with Python, TensorFlow, and Keras tutorial | Day 39\nLink To the [video.](https://www.youtube.com/watch?v=wQ8BIBpya2k&t=19s&index=2&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN)\n\n## Loading in your own data - Deep Learning basics with Python, TensorFlow and Keras p.2 | Day 40\nLink To the [video.](https://www.youtube.com/watch?v=j-3vuBynnOE&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN&index=2)\n\n## Convolutional Neural Networks - Deep Learning basics with Python, TensorFlow and Keras p.3 | Day 41\nLink To the [video.](https://www.youtube.com/watch?v=WvoLTXIjBYU&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN&index=3)\n\n## Analyzing Models with TensorBoard - Deep Learning with Python, TensorFlow and Keras p.4 | Day 42\nLink To the [video.](https://www.youtube.com/watch?v=BqgTU7_cBnk&list=PLQVvvaa0QuDfhTox0AjmQ6tvTgMBZBEXN&index=4)\n\n## K Means Clustering | Day 43\nMoved to Unsupervised Learning and studied about Clustering.\nWorking on my website check it out [avikjain.me](http://www.avikjain.me/)\nAlso found a wonderful animation that can help to easily understand K - Means Clustering [Link](http://shabal.in/visuals/kmeans/6.html)\n\n<p align="center">\n  <img src="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2043.jpg">\n</p>\n\n## K Means Clustering Implementation | Day 44\nImplemented K Means Clustering. Check the code [here.]()\n\n## Digging Deeper | NUMPY  | Day 45\nGot a new book "Python Data Science HandBook" by JK VanderPlas Check the Jupyter notebooks [here.](https://github.com/jakevdp/PythonDataScienceHandbook)\n<br>Started with chapter 2 : Introduction to Numpy. Covered topics like Data Types, Numpy arrays and Computations on Numpy arrays.\n<br>Check the code - \n<br>[Introduction to NumPy](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.00-Introduction-to-NumPy.ipynb)\n<br>[Understanding Data Types in Python](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.01-Understanding-Data-Types.ipynb)\n<br>[The Basics of NumPy Arrays](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.02-The-Basics-Of-NumPy-Arrays.ipynb)\n<br>[Computation on NumPy Arrays: Universal Functions](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.03-Computation-on-arrays-ufuncs.ipynb)\n\n## Digging Deeper | NUMPY | Day 46\nChapter 2 : Aggregations, Comparisions and Broadcasting\n<br>Link to Notebook:\n<br>[Aggregations: Min, Max, and Everything In Between](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.04-Computation-on-arrays-aggregates.ipynb)\n<br>[Computation on Arrays: Broadcasting](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.05-Computation-on-arrays-broadcasting.ipynb)\n<br>[Comparisons, Masks, and Boolean Logic](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.06-Boolean-Arrays-and-Masks.ipynb)\n\n## Digging Deeper | NUMPY | Day 47\nChapter 2 : Fancy Indexing, sorting arrays, Struchered Data\n<br>Link to Notebook:\n<br>[Fancy Indexing](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.07-Fancy-Indexing.ipynb)\n<br>[Sorting Arrays](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.08-Sorting.ipynb)\n<br>[Structured Data: NumPy''s Structured Arrays](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.09-<br>Structured-Data-NumPy.ipynb)\n\n## Digging Deeper | PANDAS | Day 48\nChapter 3 : Data Manipulation with Pandas\n<br> Covered Various topics like Pandas Objects, Data Indexing and Selection, Operating on Data, Handling Missing Data, Hierarchical Indexing, ConCat and Append.\n<br>Link To the Notebooks:\n<br>[Data Manipulation with Pandas](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.00-Introduction-to-Pandas.ipynb)\n<br>[Introducing Pandas Objects](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.01-Introducing-Pandas-Objects.ipynb)\n<br>[Data Indexing and Selection](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.02-Data-Indexing-and-Selection.ipynb)\n<br>[Operating on Data in Pandas](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.03-Operations-in-Pandas.ipynb)\n<br>[Handling Missing Data](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.04-Missing-Values.ipynb)\n<br>[Hierarchical Indexing](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.05-Hierarchical-Indexing.ipynb)\n<br>[Combining Datasets: Concat and Append](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.06-Concat-And-Append.ipynb)\n\n## Digging Deeper | PANDAS | Day 49\nChapter 3: Completed following topics- Merge and Join, Aggregation and grouping and Pivot Tables.\n<br>[Combining Datasets: Merge and Join](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.07-Merge-and-Join.ipynb)\n<br>[Aggregation and Grouping](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.08-Aggregation-and-Grouping.ipynb)\n<br>[Pivot Tables](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.09-Pivot-Tables.ipynb)\n\n## Digging Deeper | PANDAS | Day 50\nChapter 3: Vectorized Strings Operations, Working with Time Series\n<br>Links to Notebooks:\n<br>[Vectorized String Operations](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.10-Working-With-Strings.ipynb)\n<br>[Working with Time Series](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.11-Working-with-Time-Series.ipynb)\n<br>[High-Performance Pandas: eval() and query()](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.12-Performance-Eval-and-Query.ipynb)\n\n## Digging Deeper | MATPLOTLIB | Day 51\nChapter 4: Visualization with Matplotlib \nLearned about Simple Line Plots, Simple Scatter Plotsand Density and Contour Plots.\n<br>Links to Notebooks: \n<br>[Visualization with Matplotlib](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.00-Introduction-To-Matplotlib.ipynb)\n<br>[Simple Line Plots](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.01-Simple-Line-Plots.ipynb)\n<br>[Simple Scatter Plots](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.02-Simple-Scatter-Plots.ipynb)\n<br>[Visualizing Errors](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.03-Errorbars.ipynb)\n<br>[Density and Contour Plots](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.04-Density-and-Contour-Plots.ipynb)\n\n## Digging Deeper | MATPLOTLIB | Day 52\nChapter 4: Visualization with Matplotlib \nLearned about Histograms, How to customize plot legends, colorbars, and buliding Multiple Subplots.\n<br>Links to Notebooks: \n<br>[Histograms, Binnings, and Density](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.05-Histograms-and-Binnings.ipynb)\n<br>[Customizing Plot Legends](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.06-Customizing-Legends.ipynb)\n<br>[Customizing Colorbars](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.07-Customizing-Colorbars.ipynb)\n<br>[Multiple Subplots](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.08-Multiple-Subplots.ipynb)\n<br>[Text and Annotation](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.09-Text-and-Annotation.ipynb)\n\n## Digging Deeper | MATPLOTLIB | Day 53\nChapter 4: Covered Three Dimensional Plotting in Mathplotlib.\n<br>Links to Notebooks:\n<br>[Three-Dimensional Plotting in Matplotlib](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.12-Three-Dimensional-Plotting.ipynb)\n\n## Hierarchical Clustering | Day 54\nStudied about Hierarchical Clustering.\nCheck out this amazing [Visualization.](https://cdn-images-1.medium.com/max/800/1*ET8kCcPpr893vNZFs8j4xg.gif)\n<p align="center">\n  <img src="https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/Info-graphs/Day%2054.jpg">\n</p>\n', '{"language":null,"stars":48971,"forks":11244,"watchers":48971,"open_issues":66,"topics":["100-days-of-code-log","100daysofcode","deep-learning","implementation","infographics","linear-algebra","linear-regression","logistic-regression","machine-learning","machine-learning-algorithms","naive-bayes-classifier","python","scikit-learn","siraj-raval","siraj-raval-challenge","support-vector-machines","svm","tutorial"],"default_branch":"master","size_kb":10955,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:jakevdp:PythonDataScienceHandbook","source_url":"https://github.com/jakevdp/PythonDataScienceHandbook"},{"type":"has_code","target_id":"github:Avik-Jain:100-Days-Of-ML-Code","source_url":"https://github.com/Avik-Jain/100-Days-Of-ML-Code"}]', NULL, 'MIT', 'approved', 80, '7669026bc307fe862c07d57f7c9b37a2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Avik-Jain-100-Days-Of-ML-Code from https://github.com/Avik-Jain.png
Image converted to WebP: data/images/github-Avik-Jain-100-Days-Of-ML-Code.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-JuliaLang-julia', 'github--julialang--julia', 'julia', 'JuliaLang', '<a name="logo"/> <div align="center"> <a href="https://julialang.org/" target="_blank"> <img src="doc/src/assets/logo.svg" alt="Julia Logo" width="210" height="142"></img> </a> </div> <table> <!-- Docs --> <tr> <td>Documentation</td> <td> <a href="https://docs.julialang.org"><img src=''https://img.shields.io/badge/docs-v1-blue.svg''/></a> </td> </tr> <!-- Continuous integration To change the badge to point to a different pipeline, it is not sufficient to simply change the part. You need to go t...', '["hacktoberfest","hpc","julia","julia-language","julialang","machine-learning","numerical","programming-language","science","scientific","julia"]', 'other', 48057, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/JuliaLang/julia","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<a name="logo"/>\n<div align="center">\n<a href="https://julialang.org/" target="_blank">\n<img src="doc/src/assets/logo.svg" alt="Julia Logo" width="210" height="142"></img>\n</a>\n</div>\n\n<table>\n    <!-- Docs -->\n    <tr>\n        <td>Documentation</td>\n        <td>\n            <a href="https://docs.julialang.org"><img src=''https://img.shields.io/badge/docs-v1-blue.svg''/></a>\n        </td>\n    </tr>\n    <!-- Continuous integration\n    To change the badge to point to a different pipeline, it is not sufficient to simply change the `?branch=` part.\n    You need to go to the Buildkite website and get the SVG URL for the correct pipeline. -->\n    <tr>\n        <td>Continuous integration</td>\n        <td>\n            <a href="https://buildkite.com/julialang/julia-master"><img src=''https://badge.buildkite.com/f28e0d28b345f9fad5856ce6a8d64fffc7c70df8f4f2685cd8.svg?branch=master''/></a>\n        </td>\n    </tr>\n    <!-- Coverage -->\n    <tr>\n        <td>Code coverage</td>\n        <td>\n            <a href=''https://coveralls.io/github/JuliaLang/julia?branch=master''><img src=''https://coveralls.io/repos/github/JuliaLang/julia/badge.svg?branch=master'' alt=''Coverage Status''/></a>\n            <a href="https://codecov.io/gh/JuliaLang/julia"><img src="https://codecov.io/gh/JuliaLang/julia/branch/master/graph/badge.svg?token=TckCRxc7HS"/></a>\n        </td>\n    </tr>\n</table>\n\n## The Julia Language\n\nJulia is a high-level, high-performance dynamic language for technical\ncomputing. The main homepage for Julia can be found at\n[julialang.org](https://julialang.org/). This is the GitHub\nrepository of Julia source code, including instructions for compiling\nand installing Julia, below.\n\n## Resources\n\n- **Homepage:** <https://julialang.org>\n- **Install:** <https://julialang.org/downloads/>\n- **Source code:** <https://github.com/JuliaLang/julia>\n- **Documentation:** <https://docs.julialang.org>\n- **Packages:** <https://julialang.org/packages/>\n- **Discussion forum:** <https://discourse.julialang.org>\n- **Zulip:** <https://julialang.zulipchat.com/>\n- **Slack:** <https://julialang.slack.com> (get an invite from <https://julialang.org/slack/>)\n- **YouTube:** <https://www.youtube.com/user/JuliaLanguage>\n- **Code coverage:** <https://coveralls.io/r/JuliaLang/julia>\n\nNew developers may find the notes in\n[CONTRIBUTING](https://github.com/JuliaLang/julia/blob/master/CONTRIBUTING.md)\nhelpful to start contributing to the Julia codebase.\n\n### Learning Julia\n\n- [**Learning resources**](https://julialang.org/learning/)\n\n## Binary Installation\n\nThe recommended way of installing Julia is to use `juliaup` which will install\nthe latest stable `julia` for you and help keep it up to date. It can also let\nyou install and run different Julia versions simultaneously. Instructions for\nthis can be found [here](https://julialang.org/downloads/). If you want to manually\ndownload specific Julia binaries, you can find those on the [Manual Downloads\npage](https://julialang.org/downloads/manual-downloads/). The downloads page also provides\ndetails on the [different tiers of\nsupport](https://julialang.org/downloads/support) for OS and\nplatform combinations.\n\nIf everything works correctly, you will get a `julia` program and when you run\nit in a terminal or command prompt, you will see a Julia banner and an\ninteractive prompt into which you can enter expressions for evaluation. You can\nread about [getting\nstarted](https://docs.julialang.org/en/v1/manual/getting-started/) in the\nmanual.\n\n**Note**: Although some OS package managers provide Julia, such\ninstallations are neither maintained nor endorsed by the Julia\nproject. They may be outdated, broken and/or unmaintained. We\nrecommend you use the official Julia binaries instead.\n\n## Building Julia\n\nFirst, make sure you have all the [required\ndependencies](https://github.com/JuliaLang/julia/blob/master/doc/src/devdocs/build/build.md#required-build-tools-and-external-libraries) installed.\nThen, acquire the source code by cloning the git repository:\n\n    git clone https://github.com/JuliaLang/julia.git\n\nand then use the command prompt to change into the resulting julia directory. By default, you will be building the latest unstable version of\nJulia. However, most users should use the [most recent stable version](https://github.com/JuliaLang/julia/releases)\nof Julia. You can get this version by running:\n\n    git checkout v1.12.2\n\nTo build the `julia` executable, run `make` from within the julia directory.\n\nBuilding Julia requires 2GiB of disk space and approximately 4GiB of virtual memory.\n\n**Note:** The build process will fail badly if any of the build directory''s parent directories have spaces or other shell meta-characters such as `$` or `:` in their names (this is due to a limitation in GNU make).\n\nOnce it is built, you can run the `julia` executable. From within the julia directory, run\n\n    ./julia\n\nYour first test of Julia determines whether your build is working\nproperly. From the julia\ndirectory, type `make testall`. You should see output that\nlists a series of running tests; if they complete without error, you\nshould be in good shape to start using Julia.\n\nYou can read about [getting\nstarted](https://docs.julialang.org/en/v1/manual/getting-started/)\nin the manual.\n\nDetailed build instructions, should they be necessary,\nare included in the [build documentation](https://github.com/JuliaLang/julia/blob/master/doc/src/devdocs/build/build.md).\n\n### Uninstalling Julia\n\nBy default, Julia does not install anything outside the directory it was cloned\ninto and `~/.julia`. Julia and the vast majority of Julia packages can be\ncompletely uninstalled by deleting these two directories.\n\n## Source Code Organization\n\nThe Julia source code is organized as follows:\n\n| Directory         | Contents                                                           |\n| -                 | -                                                                  |\n| `base/`           | source code for the Base module (part of Julia''s standard library) |\n| `cli/`            | source for the command line interface/REPL                         |\n| `contrib/`        | miscellaneous scripts                                              |\n| `deps/`           | external dependencies                                              |\n| `doc/src/`        | source for the user manual                                         |\n| `etc/`            | contains `startup.jl`                                              |\n| `src/`            | source for Julia language core                                     |\n| `stdlib/`         | source code for other standard library packages                    |\n| `test/`           | test suites                                                        |\n\n## Terminal, Editors and IDEs\n\nThe Julia REPL is quite powerful. See the section in the manual on\n[the Julia REPL](https://docs.julialang.org/en/v1/stdlib/REPL/)\nfor more details.\n\nOn Windows, we highly recommend running Julia in a modern terminal,\nsuch as [Windows Terminal from the Microsoft Store](https://aka.ms/terminal).\n\nSupport for editing Julia is available for many\n[widely used editors](https://github.com/JuliaEditorSupport):\n[Emacs](https://github.com/JuliaEditorSupport/julia-emacs),\n[Vim](https://github.com/JuliaEditorSupport/julia-vim),\n[Sublime Text](https://github.com/JuliaEditorSupport/Julia-sublime), and many\nothers.\n\nFor users who prefer IDEs, we recommend using VS Code with the\n[julia-vscode](https://www.julia-vscode.org/) plugin.\\nFor notebook users, [Jupyter](https://jupyter.org/) notebook support is available through the\n[IJulia](https://github.com/JuliaLang/IJulia.jl) package, and\nthe [Pluto.jl](https://github.com/fonsp/Pluto.jl) package provides Pluto notebooks.\n', '{"language":"Julia","stars":48057,"forks":5682,"watchers":48057,"open_issues":4910,"topics":["hacktoberfest","hpc","julia","julia-language","julialang","machine-learning","numerical","programming-language","science","scientific"],"default_branch":"master","size_kb":352132,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:JuliaLang:julia>","source_url":"https://github.com/JuliaLang/julia>"},{"type":"has_code","target_id":"github:JuliaLang:julia","source_url":"https://github.com/JuliaLang/julia"},{"type":"has_code","target_id":"github:JuliaLang:julia","source_url":"https://github.com/JuliaLang/julia"},{"type":"has_code","target_id":"github:JuliaLang:julia.git","source_url":"https://github.com/JuliaLang/julia.git"},{"type":"has_code","target_id":"github:JuliaLang:julia","source_url":"https://github.com/JuliaLang/julia"},{"type":"has_code","target_id":"github:JuliaLang:julia","source_url":"https://github.com/JuliaLang/julia"},{"type":"has_code","target_id":"github:JuliaEditorSupport:julia-emacs","source_url":"https://github.com/JuliaEditorSupport/julia-emacs"},{"type":"has_code","target_id":"github:JuliaEditorSupport:julia-vim","source_url":"https://github.com/JuliaEditorSupport/julia-vim"},{"type":"has_code","target_id":"github:JuliaEditorSupport:Julia-sublime","source_url":"https://github.com/JuliaEditorSupport/Julia-sublime"},{"type":"has_code","target_id":"github:JuliaLang:IJulia.jl","source_url":"https://github.com/JuliaLang/IJulia.jl"},{"type":"has_code","target_id":"github:fonsp:Pluto.jl","source_url":"https://github.com/fonsp/Pluto.jl"}]', NULL, 'MIT', 'approved', 65, '2a2e0c8d36a12acf05a00f5c18334795', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-JuliaLang-julia from https://github.com/JuliaLang.png
Image converted to WebP: data/images/github-JuliaLang-julia.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-pathwaycom-llm-app', 'github--pathwaycom--llm-app', 'llm-app', 'pathwaycom', '<div align="center"> <a href="https://trendshift.io/repositories/4400" target="_blank"><img src="https://trendshift.io/api/badge/repositories/4400" alt="pathwaycom%2Fllm-app | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a> !Linux !macOS </div> Pathway''s **AI Pipelines** allow you to quickly put in production AI applications that offer **high-accuracy RAG and AI enterprise search at scale** using the most **up-to-date knowledge** available in your data sources. I...', '["chatbot","hugging-face","llm","llm-local","llm-prompting","llm-security","llmops","machine-learning","open-ai","pathway","rag","real-time","retrieval-augmented-generation","vector-database","vector-index","jupyter notebook"]', 'other', 47712, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/pathwaycom/llm-app","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n\n# Pathway AI Pipelines\n\n<a href="https://trendshift.io/repositories/4400" target="_blank"><img src="https://trendshift.io/api/badge/repositories/4400" alt="pathwaycom%2Fllm-app | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n\n![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black)\n![macOS](https://img.shields.io/badge/mac%20os-000000?style=for-the-badge&logo=apple&logoColor=white)\n[![chat on Discord](https://img.shields.io/badge/Discord-5865F2?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/pathway)\n[![follow on X](  https://img.shields.io/badge/X-000000?style=for-the-badge&logo=x&logoColor=white)](https://x.com/intent/follow?screen_name=pathway_com)\n</div>\n\nPathway''s **AI Pipelines** allow you to quickly put in production AI applications that offer **high-accuracy RAG and AI enterprise search at scale** using the most **up-to-date knowledge** available in your data sources. It provides you ready-to-deploy **LLM (Large Language Model) App Templates**. You can test them on your own machine and deploy on-cloud (GCP, AWS, Azure, Render,...) or on-premises.\n\nThe apps connect and sync (all new data additions, deletions, updates) with data sources on your **file system, Google Drive, Sharepoint, S3, Kafka, PostgreSQL, real-time data APIs**. They come with no infrastructure dependencies that would need a separate setup. They include **built-in data indexing** enabling vector search, hybrid search, and full-text search - all done in-memory, with cache.\n\n\n## Application Templates\n\nThe application templates provided in this repo scale up to **millions of pages of documents**. Some of them are optimized for simplicity, some are optimized for amazing accuracy. Pick the one that suits you best. You can use it out of the box, or change some steps of the pipeline - for example, if you would like to add a new data source, or change a Vector Index into a Hybrid Index, it''s just a one-line change. \n\n| Application (template)                                                                           | Description                                                                                                                                                                                                                                                                                                                                                         |\n| --------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [`Question-Answering RAG App`](templates/question_answering_rag/)    | Basic end-to-end RAG app. A question-answering pipeline that uses the GPT model of choice to provide answers to queries to your documents (PDF, DOCX,...) on a live connected data source (files, Google Drive, Sharepoint,...). You can also try out a [demo REST endpoint](https://pathway.com/solutions/rag-pipelines#try-it-out).              |\n| [`Live Document Indexing (Vector Store / Retriever)`](templates/document_indexing/)     | A real-time document indexing pipeline for RAG that acts as a vector store service. It performs live indexing on your documents (PDF, DOCX,...) from a connected data source (files, Google Drive, Sharepoint,...). It can be used with any frontend, or integrated as a retriever backend for a [Langchain](https://pathway.com/blog/langchain-integration) or [Llamaindex](https://pathway.com/blog/llamaindex-pathway) application. You can also try out a [demo REST endpoint](https://pathway.com/solutions/ai-contract-management#try-it-out).         |\n| [`Multimodal RAG pipeline with GPT4o`](templates/multimodal_rag/) | Multimodal RAG using GPT-4o in the parsing stage to index PDFs and other documents from a connected data source files, Google Drive, Sharepoint,...). It is perfect for extracting information from unstructured financial documents in your folders (including charts and tables), updating results as documents change or new ones arrive.|\n| [`Unstructured-to-SQL pipeline + SQL question-answering`](templates/unstructured_to_sql_on_the_fly/) | A RAG example which connects to unstructured financial data sources (financial report PDFs), structures the data into SQL, and loads it into a PostgreSQL table. It also answers natural language user queries to these financial documents by translating them into SQL using an LLM and executing the query on the PostgreSQL table. |\n| [`Adaptive RAG App`](templates/adaptive_rag/) | A RAG application using Adaptive RAG, a technique developed by Pathway to reduce token cost in RAG up to 4x while maintaining accuracy. |\n| [`Private RAG App with Mistral and Ollama`](templates/private_rag/) |  A fully private (local) version of the `question_answering_rag` RAG pipeline using Pathway, Mistral, and Ollama. |\n| [`Slides AI Search App`](templates/slides_ai_search/)                                        | An indexing pipeline for retrieving slides. It performs multi-modal of PowerPoint and PDF and maintains live index of your slides."|\n\n\n## How do these AI Pipelines work?\n\nThe apps can be run as **Docker containers**, and expose an **HTTP API** to connect the frontend. To allow quick testing and demos, some app templates also include an optional Streamlit UI which connects to this API. \n\nThe apps rely on the [Pathway Live Data framework](https://github.com/pathwaycom/pathway) for data source synchronization and for serving API requests (Pathway is a standalone Python library with a Rust engine built into it). They bring you a **simple and unified application logic** for back-end, embedding, retrieval, LLM tech stack. There is no need to integrate and maintain separate modules for your Gen AI app: ~Vector Database (e.g. Pinecone/Weaviate/Qdrant) + Cache (e.g. Redis) + API Framework (e.g. Fast API)~. Pathway''s default choice of **built-in vector index** is based on the lightning-fast [usearch](https://github.com/unum-cloud/usearch) library, and **hybrid full-text indexes** make use of [Tantivy](https://github.com/quickwit-oss/tantivy) library. Everything works out of the box.\n\n## Getting started\n\nEach of the [App templates](templates/) in this repo contains a README.md with instructions on how to run it.\n\nYou can also find [more ready-to-run code templates](https://pathway.com/developers/templates/) on the Pathway website.\n\n\n## Some visual highlights\n\nEffortlessly extract and organize table and chart data from PDFs, docs, and more with multimodal RAG - in real-time:\n\n![Effortlessly extract and organize table and chart data from PDFs, docs, and more with multimodal RAG - in real-time](https://github.com/pathwaycom/llm-app/blob/main/templates/multimodal_rag/gpt4o_with_pathway_comparison.gif)\n\n(Check out [`Multimodal RAG pipeline with GPT4o`](templates/multimodal_rag/) to see the whole pipeline in the works. You may also check out the [`Unstructured-to-SQL pipeline`](templates/unstructured_to_sql_on_the_fly/) for a minimal example that works with non-multimodal models as well.)\n\n\nAutomated real-time knowledge mining and alerting:\n\n![Automated real-time knowledge mining and alerting](templates/drive_alert/drive_alert_demo.gif)\n\n(Check out the [`Alerting when answers change on Google Drive`](https://github.com/pathwaycom/llm-app/tree/main/templates/drive_alert) app example.)\n\n\n###  Do-it-Yourself Videos\n\nâ–¶ï¸ [An introduction to building LLM apps with Pathway](https://www.youtube.com/watch?v=kcrJSk00duw) - by [Jan Chorowski](https://scholar.google.com/citations?user=Yc94070AAAAJ)\n\nâ–¶ï¸ [Let''s build a real-world LLM app in 11 minutes](https://www.youtube.com/watch?v=k1XGo7ts4tI) - by [Pau Labarta Bajo](https://substack.com/@paulabartabajo)\n\n\n## Troubleshooting\n\nTo provide feedback or report a bug, please [raise an issue on our issue tracker](https://github.com/pathwaycom/pathway/issues).\n\n## Contributing\n\nAnyone who wishes to contribute to this project, whether documentation, features, bug fixes, code cleanup, testing, or code reviews, is very much encouraged to do so. If this is your first contribution to a GitHub project, here is a [Get Started Guide](https://docs.github.com/en/get-started/quickstart/contributing-to-projects). \n\nIf you''d like to make a contribution that needs some more work, just raise your hand on the [Pathway Discord server](https://discord.com/invite/pathway) (#get-help) and let us know what you are planning!\n\n## Supported and maintained by\n\n<p align="center">\n  <a href="https://github.com/pathwaycom/"><img src="https://pathway.com/logo-light.svg" alt="Pathway"/></a>\n</p>\n<p align="center">\n  <a href="https://pathway.com/solutions/llm-app">\n    <img src="https://img.shields.io/badge/See%20Pathway''s%20offering%20for%20AI%20applications-0000FF" alt="See Pathway''s offering for AI applications"/>\n  </a>\n</p>\n', '{"language":"Jupyter Notebook","stars":47712,"forks":1222,"watchers":47712,"open_issues":6,"topics":["chatbot","hugging-face","llm","llm-local","llm-prompting","llm-security","llmops","machine-learning","open-ai","pathway","rag","real-time","retrieval-augmented-generation","vector-database","vector-index"],"default_branch":"main","size_kb":62766,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:pathwaycom:pathway","source_url":"https://github.com/pathwaycom/pathway"},{"type":"has_code","target_id":"github:unum-cloud:usearch","source_url":"https://github.com/unum-cloud/usearch"},{"type":"has_code","target_id":"github:quickwit-oss:tantivy","source_url":"https://github.com/quickwit-oss/tantivy"},{"type":"has_code","target_id":"github:pathwaycom:llm-app","source_url":"https://github.com/pathwaycom/llm-app"},{"type":"has_code","target_id":"github:pathwaycom:llm-app","source_url":"https://github.com/pathwaycom/llm-app"},{"type":"has_code","target_id":"github:pathwaycom:pathway","source_url":"https://github.com/pathwaycom/pathway"},{"type":"has_code","target_id":"github:pathwaycom:\"><img","source_url":"https://github.com/pathwaycom/\"><img"}]', NULL, 'MIT', 'approved', 65, '7529ed2b036ba89e86deb0619822e284', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-pathwaycom-llm-app from https://github.com/pathwaycom.png
Image converted to WebP: data/images/github-pathwaycom-llm-app.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-GokuMohandas-Made-With-ML', 'github--gokumohandas--made-with-ml', 'Made-With-ML', 'GokuMohandas', '<div align="center"> <h1><img width="30" src="https://madewithml.com/static/images/rounded_logo.png">&nbsp;<a href="https://madewithml.com/">Made With ML</a></h1> Design Â· Develop Â· Deploy Â· Iterate <br> Join 40K+ developers in learning how to responsibly deliver value with ML. <br> </div> <br> <div align="center"> <a target="_blank" href="https://madewithml.com/"><img src="https://img.shields.io/badge/Subscribe-40K-brightgreen"></a>&nbsp; <a target="_blank" href="https://github.com/GokuMohan...', '["data-engineering","data-quality","data-science","deep-learning","distributed-ml","distributed-training","llms","machine-learning","mlops","natural-language-processing","python","pytorch","ray","jupyter notebook"]', 'other', 44664, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/GokuMohandas/Made-With-ML","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n<h1><img width="30" src="https://madewithml.com/static/images/rounded_logo.png">&nbsp;<a href="https://madewithml.com/">Made With ML</a></h1>\nDesign Â· Develop Â· Deploy Â· Iterate\n<br>\nJoin 40K+ developers in learning how to responsibly deliver value with ML.\n    <br>\n</div>\n\n<br>\n\n<div align="center">\n    <a target="_blank" href="https://madewithml.com/"><img src="https://img.shields.io/badge/Subscribe-40K-brightgreen"></a>&nbsp;\n    <a target="_blank" href="https://github.com/GokuMohandas/Made-With-ML"><img src="https://img.shields.io/github/stars/GokuMohandas/Made-With-ML.svg?style=social&label=Star"></a>&nbsp;\n    <a target="_blank" href="https://www.linkedin.com/in/goku"><img src="https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social"></a>&nbsp;\n    <a target="_blank" href="https://twitter.com/GokuMohandas"><img src="https://img.shields.io/twitter/follow/GokuMohandas.svg?label=Follow&style=social"></a>\n    <br>\n    ðŸ”¥&nbsp; Among the <a href="https://github.com/GokuMohandas/Made-With-ML" target="_blank">top ML repositories</a> on GitHub\n</div>\n\n<br>\n<hr>\n\n## Lessons\n\nLearn how to combine machine learning with software engineering to design, develop, deploy and iterate on production-grade ML applications.\n\n- Lessons: https://madewithml.com/\n- Code: [GokuMohandas/Made-With-ML](https://github.com/GokuMohandas/Made-With-ML)\n\n<a href="https://madewithml.com/#course">\n  <img src="https://madewithml.com/static/images/lessons.png" alt="lessons">\n</a>\n\n## Overview\n\nIn this course, we''ll go from experimentation (design + development) to production (deployment + iteration). We''ll do this iteratively by motivating the components that will enable us to build a *reliable* production system.\n\n<blockquote>\n  <img width=20 src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/09/YouTube_full-color_icon_%282017%29.svg/640px-YouTube_full-color_icon_%282017%29.svg.png">&nbsp; Be sure to watch the video below for a quick overview of what we''ll be building.\n</blockquote>\n\n<div align="center">\n  <a href="https://youtu.be/AWgkt8H8yVo"><img src="https://img.youtube.com/vi/AWgkt8H8yVo/0.jpg" alt="Course overview video"></a>\n</div>\n\n<br>\n\n- **ðŸ’¡ First principles**: before we jump straight into the code, we develop a first principles understanding for every machine learning concept.\n- **ðŸ’» Best practices**: implement software engineering best practices as we develop and deploy our machine learning models.\n- **ðŸ“ˆ Scale**: easily scale ML workloads (data, train, tune, serve) in Python without having to learn completely new languages.\n- **âš™ï¸ MLOps**: connect MLOps components (tracking, testing, serving, orchestration, etc.) as we build an end-to-end machine learning system.\n- **ðŸš€ Dev to Prod**: learn how to quickly and reliably go from development to production without any changes to our code or infra management.\n- **ðŸ™ CI/CD**: learn how to create mature CI/CD workflows to continuously train and deploy better models in a modular way that integrates with any stack.\n\n## Audience\n\nMachine learning is not a separate industry, instead, it''s a powerful way of thinking about data that''s not reserved for any one type of person.\n\n- **ðŸ‘©â€ðŸ’» All developers**: whether software/infra engineer or data scientist, ML is increasingly becoming a key part of the products that you''ll be developing.\n- **ðŸ‘©â€ðŸŽ“ College graduates**: learn the practical skills required for industry and bridge gap between the university curriculum and what industry expects.\n- **ðŸ‘©â€ðŸ’¼ Product/Leadership**: who want to develop a technical foundation so that they can build amazing (and reliable) products powered by machine learning.\n\n## Set up\n\nBe sure to go through the [course](https://madewithml/#course) for a much more detailed walkthrough of the content on this repository. We will have instructions for both local laptop and Anyscale clusters for the sections below, so be sure to toggle the â–º dropdown based on what you''re using (Anyscale instructions will be toggled on by default). If you do want to run this course with Anyscale, where we''ll provide the **structure**, **compute (GPUs)** and **community** to learn everything in one day, join our next upcoming live cohort â†’ [sign up here](https://4190urw86oh.typeform.com/madewithml)!\n\n### Cluster\n\nWe''ll start by setting up our cluster with the environment and compute configurations.\n\n<details>\n  <summary>Local</summary><br>\n  Your personal laptop (single machine) will act as the cluster, where one CPU will be the head node and some of the remaining CPU will be the worker nodes. All of the code in this course will work in any personal laptop though it will be slower than executing the same workloads on a larger cluster.\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  We can create an [Anyscale Workspace](https://docs.anyscale.com/develop/workspaces/get-started) using the [webpage UI](https://console.anyscale.com/o/madewithml/workspaces/add/blank).\n\n  ```md\n  - Workspace name: `madewithml`\n  - Project: `madewithml`\n  - Cluster environment name: `madewithml-cluster-env`\n  # Toggle `Select from saved configurations`\n  - Compute config: `madewithml-cluster-compute-g5.4xlarge`\n  ```\n\n  > Alternatively, we can use the [CLI](https://docs.anyscale.com/reference/anyscale-cli) to create the workspace via `anyscale workspace create ...`\n\n</details>\n\n<details>\n  <summary>Other (cloud platforms, K8s, on-prem)</summary><br>\n\n  If you don''t want to do this course locally or via Anyscale, you have the following options:\n\n  - On [AWS and GCP](https://docs.ray.io/en/latest/cluster/vms/index.html#cloud-vm-index). Community-supported Azure and Aliyun integrations also exist.\n  - On [Kubernetes](https://docs.ray.io/en/latest/cluster/kubernetes/index.html#kuberay-index), via the officially supported KubeRay project.\n  - Deploy Ray manually [on-prem](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/on-premises.html#on-prem) or onto platforms [not listed here](https://docs.ray.io/en/latest/cluster/vms/user-guides/community/index.html#ref-cluster-setup).\n\n</details>\n\n### Git setup\n\nCreate a repository by following these instructions: [Create a new repository](https://github.com/new) â†’ name it `Made-With-ML` â†’ Toggle `Add a README file` (**very important** as this creates a `main` branch) â†’ Click `Create repository` (scroll down)\n\nNow we''re ready to clone the repository that has all of our code:\n\n```bash\ngit clone https://github.com/GokuMohandas/Made-With-ML.git .\n```\n\n### Credentials\n\n```bash\ntouch .env\n```\n```bash\n# Inside .env\nGITHUB_USERNAME="CHANGE_THIS_TO_YOUR_USERNAME"  # â† CHANGE THIS\n```\n```bash\nsource .env\n```\n\n### Virtual environment\n\n<details>\n  <summary>Local</summary><br>\n\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:$PWD\n  python3 -m venv venv  # recommend using Python 3.10\n  source venv/bin/activate  # on Windows: venv\Scripts\activate\n  python3 -m pip install --upgrade pip setuptools wheel\n  python3 -m pip install -r requirements.txt\n  pre-commit install\n  pre-commit autoupdate\n  ```\n\n  > Highly recommend using Python `3.10` and using [pyenv](https://github.com/pyenv/pyenv) (mac) or [pyenv-win](https://github.com/pyenv-win/pyenv-win) (windows).\n\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  Our environment with the appropriate Python version and libraries is already all set for us through the cluster environment we used when setting up our Anyscale Workspace. So we just need to run these commands:\n  ```bash\n  export PYTHONPATH=$PYTHONPATH:$PWD\n  pre-commit install\n  pre-commit autoupdate\n  ```\n\n</details>\n\n## Notebook\n\nStart by exploring the [jupyter notebook](notebooks/madewithml.ipynb) to interactively walkthrough the core machine learning workloads.\n\n<div align="center">\n  <img src="https://madewithml.com/static/images/mlops/systems-design/workloads.png">\n</div>\n\n<details>\n  <summary>Local</summary><br>\n\n  ```bash\n  # Start notebook\n  jupyter lab notebooks/madewithml.ipynb\n```\n\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  Click on the Jupyter icon &nbsp;<img width=15 src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/38/Jupyter_logo.svg/1200px-Jupyter_logo.svg.png">&nbsp; at the top right corner of our Anyscale Workspace page and this will open up our JupyterLab instance in a new tab. Then navigate to the `notebooks` directory and open up the `madewithml.ipynb` notebook.\n\n</details>\n\n\n## Scripts\n\nNow we''ll execute the same workloads using the clean Python scripts following software engineering best practices (testing, documentation, logging, serving, versioning, etc.) The code we''ve implemented in our notebook will be refactored into the following scripts:\n\n```bash\nmadewithml\nâ”œâ”€â”€ config.py\nâ”œâ”€â”€ data.py\nâ”œâ”€â”€ evaluate.py\nâ”œâ”€â”€ models.py\nâ”œâ”€â”€ predict.py\nâ”œâ”€â”€ serve.py\nâ”œâ”€â”€ train.py\nâ”œâ”€â”€ tune.py\nâ””â”€â”€ utils.py\n```\n\n**Note**: Change the `--num-workers`, `--cpu-per-worker`, and `--gpu-per-worker` input argument values below based on your system''s resources. For example, if you''re on a local laptop, a reasonable configuration would be `--num-workers 6 --cpu-per-worker 1 --gpu-per-worker 0`.\n\n### Training\n```bash\nexport EXPERIMENT_NAME="llm"\nexport DATASET_LOC="https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv"\nexport TRAIN_LOOP_CONFIG=''{"dropout_p": 0.5, "lr": 1e-4, "lr_factor": 0.8, "lr_patience": 3}''\npython madewithml/train.py \\n    --experiment-name "$EXPERIMENT_NAME" \\n    --dataset-loc "$DATASET_LOC" \\n    --train-loop-config "$TRAIN_LOOP_CONFIG" \\n    --num-workers 1 \\n    --cpu-per-worker 3 \\n    --gpu-per-worker 1 \\n    --num-epochs 10 \\n    --batch-size 256 \\n    --results-fp results/training_results.json\n```\n\n### Tuning\n```bash\nexport EXPERIMENT_NAME="llm"\nexport DATASET_LOC="https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv"\nexport TRAIN_LOOP_CONFIG=''{"dropout_p": 0.5, "lr": 1e-4, "lr_factor": 0.8, "lr_patience": 3}''\nexport INITIAL_PARAMS="[{\"train_loop_config\": $TRAIN_LOOP_CONFIG}]"\npython madewithml/tune.py \\n    --experiment-name "$EXPERIMENT_NAME" \\n    --dataset-loc "$DATASET_LOC" \\n    --initial-params "$INITIAL_PARAMS" \\n    --num-runs 2 \\n    --num-workers 1 \\n    --cpu-per-worker 3 \\n    --gpu-per-worker 1 \\n    --num-epochs 10 \\n    --batch-size 256 \\n    --results-fp results/tuning_results.json\n```\n\n### Experiment tracking\n\nWe''ll use [MLflow](https://mlflow.org/) to track our experiments and store our models and the [MLflow Tracking UI](https://www.mlflow.org/docs/latest/tracking.html#tracking-ui) to view our experiments. We have been saving our experiments to a local directory but note that in an actual production setting, we would have a central location to store all of our experiments. It''s easy/inexpensive to spin up your own MLflow server for all of your team members to track their experiments on or use a managed solution like [Weights & Biases](https://wandb.ai/site), [Comet](https://www.comet.ml/), etc.\n\n```bash\nexport MODEL_REGISTRY=$(python -c "from madewithml import config; print(config.MODEL_REGISTRY)")\nmlflow server -h 0.0.0.0 -p 8080 --backend-store-uri $MODEL_REGISTRY\n```\n\n<details>\n  <summary>Local</summary><br>\n\n  If you''re running this notebook on your local laptop then head on over to <a href="http://localhost:8080/" target="_blank">http://localhost:8080/</a> to view your MLflow dashboard.\n\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  If you''re on <a href="https://docs.anyscale.com/develop/workspaces/get-started" target="_blank">Anyscale Workspaces</a>, then we need to first expose the port of the MLflow server. Run the following command on your Anyscale Workspace terminal to generate the public URL to your MLflow server.\n\n  ```bash\n  APP_PORT=8080\n  echo https://$APP_PORT-port-$ANYSCALE_SESSION_DOMAIN\n  ```\n\n</details>\n\n### Evaluation\n```bash\nexport EXPERIMENT_NAME="llm"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\nexport HOLDOUT_LOC="https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/holdout.csv"\npython madewithml/evaluate.py \\n    --run-id $RUN_ID \\n    --dataset-loc $HOLDOUT_LOC \\n    --results-fp results/evaluation_results.json\n```\n```json\n{\n  "timestamp": "June 09, 2023 09:26:18 AM",\n  "run_id": "6149e3fec8d24f1492d4a4cabd5c06f6",\n  "overall": {\n    "precision": 0.9076136428670714,\n    "recall": 0.9057591623036649,\n    "f1": 0.9046792827719773,\n    "num_samples": 191.0\n  },\n...\n```\n\n### Inference\n```bash\nexport EXPERIMENT_NAME="llm"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\npython madewithml/predict.py predict \\n    --run-id $RUN_ID \\n    --title "Transfer learning with transformers" \\n    --description "Using transformers for transfer learning on text classification tasks."\n```\n```json\n[{\n  "prediction": [\n    "natural-language-processing"\n  ],\n  "probabilities": {\n    "computer-vision": 0.0009767753,\n    "mlops": 0.0008223939,\n    "natural-language-processing": 0.99762577,\n    "other": 0.000575123\n  }\n}]\n```\n\n### Serving\n\n<details>\n  <summary>Local</summary><br>\n\n  ```bash\n  # Start\n  ray start --head\n  ```\n\n  ```bash\n  # Set up\n  export EXPERIMENT_NAME="llm"\n  export RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\n  python madewithml/serve.py --run_id $RUN_ID\n  ```\n\n  Once the application is running, we can use it via cURL, Python, etc.:\n\n  ```python\n  # via Python\n  import json\n  import requests\n  title = "Transfer learning with transformers"\n  description = "Using transformers for transfer learning on text classification tasks."\n  json_data = json.dumps({"title": title, "description": description})\n  requests.post("http://127.0.0.1:8000/predict", data=json_data).json()\n  ```\n\n  ```bash\n  ray stop  # shutdown\n  ```\n\n</details>\n\n<details open>\n  <summary>Anyscale</summary><br>\n\n  In Anyscale Workspaces, Ray is already running so we don''t have to manually start/shutdown like we have to do locally.\n\n  ```bash\n  # Set up\n  export EXPERIMENT_NAME="llm"\n  export RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\n  python madewithml/serve.py --run_id $RUN_ID\n  ```\n\n  Once the application is running, we can use it via cURL, Python, etc.:\n\n  ```python\n  # via Python\n  import json\n  import requests\n  title = "Transfer learning with transformers"\n  description = "Using transformers for transfer learning on text classification tasks."\n  json_data = json.dumps({"title": title, "description": description})\n  requests.post("http://127.0.0.1:8000/predict", data=json_data).json()\n  ```\n\n</details>\n\n### Testing\n```bash\n# Code\npython3 -m pytest tests/code --verbose --disable-warnings\n\n# Data\nexport DATASET_LOC="https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/dataset.csv"\npytest --dataset-loc=$DATASET_LOC tests/data --verbose --disable-warnings\n\n# Model\nexport EXPERIMENT_NAME="llm"\nexport RUN_ID=$(python madewithml/predict.py get-best-run-id --experiment-name $EXPERIMENT_NAME --metric val_loss --mode ASC)\npytest --run-id=$RUN_ID tests/model --verbose --disable-warnings\n\n# Coverage\npython3 -m pytest tests/code --cov madewithml --cov-report html --disable-warnings  # html report\npython3 -m pytest tests/code --cov madewithml --cov-report term --disable-warnings  # terminal report\n```\n\n## Production\n\nFrom this point onwards, in order to deploy our application into production, we''ll need to either be on Anyscale or on a [cloud VM](https://docs.ray.io/en/latest/cluster/vms/index.html#cloud-vm-index) / [on-prem](https://docs.ray.io/en/latest/cluster/vms/user-guides/launching-clusters/on-premises.html#on-prem) cluster you manage yourself (w/ Ray). If not on Anyscale, the commands will be [slightly different](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html) but the concepts will be the same.\n\n> If you don''t want to set up all of this yourself, we highly recommend joining our [upcoming live cohort](https://4190urw86oh.typeform.com/madewithml){:target="_blank"} where we''ll provide an environment with all of this infrastructure already set up for you so that you just focused on the machine learning.\n\n<div align="center">\n  <img src="https://madewithml.com/static/images/mlops/jobs_and_services/manual.png">\n</div>\n\n### Authentication\n\nThese credentials below are **automatically** set for us if we''re using Anyscale Workspaces. We **do not** need to set these credentials explicitly on Workspaces but we do if we''re running this locally or on a cluster outside of where our Anyscale Jobs and Services are configured to run.\n\n``` bash\nexport ANYSCALE_HOST=https://console.anyscale.com\nexport ANYSCALE_CLI_TOKEN=$YOUR_CLI_TOKEN  # retrieved from Anyscale credentials page\n```\n\n### Cluster environment\n\nThe cluster environment determines **where** our workloads will be executed (OS, dependencies, etc.) We''ve already created this [cluster environment](./deploy/cluster_env.yaml) for us but this is how we can create/update one ourselves.\n\n```bash\nexport CLUSTER_ENV_NAME="madewithml-cluster-env"\nanyscale cluster-env build deploy/cluster_env.yaml --name $CLUSTER_ENV_NAME\n```\n\n### Compute configuration\n\nThe compute configuration determines **what** resources our workloads will be executes on. We''ve already created this [compute configuration](./deploy/cluster_compute.yaml) for us but this is how we can create it ourselves.\n\n```bash\nexport CLUSTER_COMPUTE_NAME="madewithml-cluster-compute-g5.4xlarge"\nanyscale cluster-compute create deploy/cluster_compute.yaml --name $CLUSTER_COMPUTE_NAME\n```\n\n### Anyscale jobs\n\nNow we''re ready to execute our ML workloads. We''ve decided to combine them all together into one [job](./deploy/jobs/workloads.yaml) but we could have also created separate jobs for each workload (train, evaluate, etc.) We''ll start by editing the `$GITHUB_USERNAME` slots inside our [`workloads.yaml`](./deploy/jobs/workloads.yaml) file:\n```yaml\nruntime_env:\n  working_dir: .\n  upload_path: s3://madewithml/$GITHUB_USERNAME/jobs  # <--- CHANGE USERNAME (case-sensitive)\n  env_vars:\n    GITHUB_USERNAME: $GITHUB_USERNAME  # <--- CHANGE USERNAME (case-sensitive)\n```\n\nThe `runtime_env` here specifies that we should upload our current `working_dir` to an S3 bucket so that all of our workers when we execute an Anyscale Job have access to the code to use. The `GITHUB_USERNAME` is used later to save results from our workloads to S3 so that we can retrieve them later (ex. for serving).\n\nNow we''re ready to submit our job to execute our ML workloads:\n```bash\nanyscale job submit deploy/jobs/workloads.yaml\n```\n\n### Anyscale Services\n\nAnd after our ML workloads have been executed, we''re ready to launch our serve our model to production. Similar to our Anyscale Jobs configs, be sure to change the `$GITHUB_USERNAME` in [`serve_model.yaml`](./deploy/services/serve_model.yaml).\n\n```yaml\nray_serve_config:\n  import_path: deploy.services.serve_model:entrypoint\n  runtime_env:\n    working_dir: .\n    upload_path: s3://madewithml/$GITHUB_USERNAME/services  # <--- CHANGE USERNAME (case-sensitive)\n    env_vars:\n      GITHUB_USERNAME: $GITHUB_USERNAME  # <--- CHANGE USERNAME (case-sensitive)\n```\n\nNow we''re ready to launch our service:\n```bash\n# Rollout service\nanyscale service rollout -f deploy/services/serve_model.yaml\n\n# Query\ncurl -X POST -H "Content-Type: application/json" -H "Authorization: Bearer $SECRET_TOKEN" -d ''{\n  "title": "Transfer learning with transformers",\n  "description": "Using transformers for transfer learning on text classification tasks."\n}'' $SERVICE_ENDPOINT/predict/\n\n# Rollback (to previous version of the Service)\nanyscale service rollback -f $SERVICE_CONFIG --name $SERVICE_NAME\n\n# Terminate\nanyscale service terminate --name $SERVICE_NAME\n```\n\n### CI/CD\n\nWe''re not going to manually deploy our application every time we make a change. Instead, we''ll automate this process using GitHub Actions!\n\n<div align="center">\n  <img src="https://madewithml.com/static/images/mlops/cicd/cicd.png">\n</div>\n\n1. Create a new github branch to save our changes to and execute CI/CD workloads:\n```bash\ngit remote set-url origin https://github.com/$GITHUB_USERNAME/Made-With-ML.git  # <-- CHANGE THIS to your username\ngit checkout -b dev\n```\n\n2. We''ll start by adding the necessary credentials to the [`/settings/secrets/actions`](https://github.com/GokuMohandas/Made-With-ML/settings/secrets/actions) page of our GitHub repository.\n\n``` bash\nexport ANYSCALE_HOST=https://console.anyscale.com\nexport ANYSCALE_CLI_TOKEN=$YOUR_CLI_TOKEN  # retrieved from https://console.anyscale.com/o/madewithml/credentials\n```\n\n3. Now we can make changes to our code (not on `main` branch) and push them to GitHub. But in order to push our code to GitHub, we''ll need to first authenticate with our credentials before pushing to our repository:\n\n```bash\ngit config --global user.name $GITHUB_USERNAME  # <-- CHANGE THIS to your username\ngit config --global user.email you@example.com  # <-- CHANGE THIS to your email\ngit add .\ngit commit -m ""  # <-- CHANGE THIS to your message\ngit push origin dev\n```\n\nNow you will be prompted to enter your username and password (personal access token). Follow these steps to get personal access token: [New GitHub personal access token](https://github.com/settings/tokens/new) â†’ Add a name â†’ Toggle `repo` and `workflow` â†’ Click `Generate token` (scroll down) â†’ Copy the token and paste it when prompted for your password.\n\n4. Now we can start a PR from this branch to our `main` branch and this will trigger the [workloads workflow](/.github/workflows/workloads.yaml). If the workflow (Anyscale Jobs) succeeds, this will produce comments with the training and evaluation results directly on the PR.\n\n<div align="center">\n  <img src="https://madewithml.com/static/images/mlops/cicd/comments.png">\n</div>\n\n5. If we like the results, we can merge the PR into the `main` branch. This will trigger the [serve workflow](/.github/workflows/serve.yaml) which will rollout our new service to production!\n\n### Continual learning\n\nWith our CI/CD workflow in place to deploy our application, we can now focus on continually improving our model. It becomes really easy to extend on this foundation to connect to scheduled runs (cron), [data pipelines](https://madewithml.com/courses/mlops/data-engineering/), drift detected through [monitoring](https://madewithml.com/courses/mlops/monitoring/), [online evaluation](https://madewithml.com/courses/mlops/evaluation/#online-evaluation), etc. And we can easily add additional context such as comparing any experiment with what''s currently in production (directly in the PR even), etc.\n\n<div align="center">\n  <img src="https://madewithml.com/static/images/mlops/cicd/continual.png">\n</div>\n\n## FAQ\n\n### Jupyter notebook kernels\n\nIssues with configuring the notebooks with jupyter? By default, jupyter will use the kernel with our virtual environment but we can also manually add it to jupyter:\n```bash\npython3 -m ipykernel install --user --name=venv\n```\nNow we can open up a notebook â†’ Kernel (top menu bar) â†’ Change Kernel â†’ `venv`. To ever delete this kernel, we can do the following:\n```bash\njupyter kernelspec list\njupyter kernelspec uninstall venv\n```\n', '{"language":"Jupyter Notebook","stars":44664,"forks":6966,"watchers":44664,"open_issues":21,"topics":["data-engineering","data-quality","data-science","deep-learning","distributed-ml","distributed-training","llms","machine-learning","mlops","natural-language-processing","python","pytorch","ray"],"default_branch":"main","size_kb":4001,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:GokuMohandas:Made-With-ML\"><img","source_url":"https://github.com/GokuMohandas/Made-With-ML\"><img"},{"type":"has_code","target_id":"github:GokuMohandas:Made-With-ML\"","source_url":"https://github.com/GokuMohandas/Made-With-ML\""},{"type":"has_code","target_id":"github:GokuMohandas:Made-With-ML","source_url":"https://github.com/GokuMohandas/Made-With-ML"},{"type":"has_code","target_id":"github:GokuMohandas:Made-With-ML.git","source_url":"https://github.com/GokuMohandas/Made-With-ML.git"},{"type":"has_code","target_id":"github:pyenv:pyenv","source_url":"https://github.com/pyenv/pyenv"},{"type":"has_code","target_id":"github:pyenv-win:pyenv-win","source_url":"https://github.com/pyenv-win/pyenv-win"},{"type":"has_code","target_id":"github:$GITHUB_USERNAME:Made-With-ML.git","source_url":"https://github.com/$GITHUB_USERNAME/Made-With-ML.git"},{"type":"has_code","target_id":"github:GokuMohandas:Made-With-ML","source_url":"https://github.com/GokuMohandas/Made-With-ML"},{"type":"has_code","target_id":"github:settings:tokens","source_url":"https://github.com/settings/tokens"}]', NULL, 'MIT', 'approved', 80, '41b7e6cf339dda323e1e82d568443b5b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-GokuMohandas-Made-With-ML from https://github.com/GokuMohandas.png
Image converted to WebP: data/images/github-GokuMohandas-Made-With-ML.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-AI-For-Beginners', 'github--microsoft--ai-for-beginners', 'AI-For-Beginners', 'microsoft', '|!Sketchnote by @girlie_mac https://twitter.com/girlie_mac| |:---:| | AI For Beginners - _Sketchnote by @girlie_mac_ | Explore the world of **Artificial Intelligence** (AI) with our 12-week, 24-lesson curriculum! It includes practical lessons, quizzes, and labs. The curriculum is beginner-friendly and covers tools like TensorFlow and PyTorch, as well as ethics in AI <!-- CO-OP TRANSLATOR LANGUAGES TABLE START --> Arabic | Bengali | Bulgarian | Burmese (Myanmar) | Chinese (Simplified) | Chines...', '["ai","artificial-intelligence","cnn","computer-vision","deep-learning","gan","machine-learning","microsoft-for-beginners","nlp","rnn","jupyter notebook"]', 'other', 44118, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/AI-For-Beginners","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '[![GitHub license](https://img.shields.io/github/license/microsoft/AI-For-Beginners.svg)](https://github.com/microsoft/AI-For-Beginners/blob/main/LICENSE)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/AI-For-Beginners.svg)](https://GitHub.com/microsoft/AI-For-Beginners/graphs/contributors/)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/AI-For-Beginners.svg)](https://GitHub.com/microsoft/AI-For-Beginners/issues/)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/AI-For-Beginners.svg)](https://GitHub.com/microsoft/AI-For-Beginners/pulls/)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/AI-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/AI-For-Beginners/watchers/)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/AI-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/AI-For-Beginners/network/)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/AI-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/AI-For-Beginners/stargazers/)\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/microsoft/ai-for-beginners/HEAD)\n[![Gitter](https://badges.gitter.im/Microsoft/ai-for-beginners.svg)](https://gitter.im/Microsoft/ai-for-beginners?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\n[![Microsoft Foundry Discord](https://dcbadge.limes.pink/api/server/nTYy5BXMWG)](https://discord.gg/nTYy5BXMWG)\n\n# Artificial Intelligence for Beginners - A Curriculum\n\n|![Sketchnote by @girlie_mac https://twitter.com/girlie_mac](./lessons/sketchnotes/ai-overview.png)|\n|:---:|\n| AI For Beginners - _Sketchnote by [@girlie_mac](https://twitter.com/girlie_mac)_ |\n\nExplore the world of **Artificial Intelligence** (AI) with our 12-week, 24-lesson curriculum!  It includes practical lessons, quizzes, and labs. The curriculum is beginner-friendly and covers tools like TensorFlow and PyTorch, as well as ethics in AI\n\n\n### ðŸŒ Multi-Language Support\n\n#### Supported via GitHub Action (Automated & Always Up-to-Date)\n\n<!-- CO-OP TRANSLATOR LANGUAGES TABLE START -->\n[Arabic](./translations/ar/README.md) | [Bengali](./translations/bn/README.md) | [Bulgarian](./translations/bg/README.md) | [Burmese (Myanmar)](./translations/my/README.md) | [Chinese (Simplified)](./translations/zh/README.md) | [Chinese (Traditional, Hong Kong)](./translations/hk/README.md) | [Chinese (Traditional, Macau)](./translations/mo/README.md) | [Chinese (Traditional, Taiwan)](./translations/tw/README.md) | [Croatian](./translations/hr/README.md) | [Czech](./translations/cs/README.md) | [Danish](./translations/da/README.md) | [Dutch](./translations/nl/README.md) | [Estonian](./translations/et/README.md) | [Finnish](./translations/fi/README.md) | [French](./translations/fr/README.md) | [German](./translations/de/README.md) | [Greek](./translations/el/README.md) | [Hebrew](./translations/he/README.md) | [Hindi](./translations/hi/README.md) | [Hungarian](./translations/hu/README.md) | [Indonesian](./translations/id/README.md) | [Italian](./translations/it/README.md) | [Japanese](./translations/ja/README.md) | [Korean](./translations/ko/README.md) | [Lithuanian](./translations/lt/README.md) | [Malay](./translations/ms/README.md) | [Marathi](./translations/mr/README.md) | [Nepali](./translations/ne/README.md) | [Norwegian](./translations/no/README.md) | [Persian (Farsi)](./translations/fa/README.md) | [Polish](./translations/pl/README.md) | [Portuguese (Brazil)](./translations/br/README.md) | [Portuguese (Portugal)](./translations/pt/README.md) | [Punjabi (Gurmukhi)](./translations/pa/README.md) | [Romanian](./translations/ro/README.md) | [Russian](./translations/ru/README.md) | [Serbian (Cyrillic)](./translations/sr/README.md) | [Slovak](./translations/sk/README.md) | [Slovenian](./translations/sl/README.md) | [Spanish](./translations/es/README.md) | [Swahili](./translations/sw/README.md) | [Swedish](./translations/sv/README.md) | [Tagalog (Filipino)](./translations/tl/README.md) | [Tamil](./translations/ta/README.md) | [Thai](./translations/th/README.md) | [Turkish](./translations/tr/README.md) | [Ukrainian](./translations/uk/README.md) | [Urdu](./translations/ur/README.md) | [Vietnamese](./translations/vi/README.md)\n<!-- CO-OP TRANSLATOR LANGUAGES TABLE END -->\n\n**If you wish to have additional translations languages supported are listed [here](https://github.com/Azure/co-op-translator/blob/main/getting_started/supported-languages.md)**\n\n## Join the Community\n[![Microsoft Foundry Discord](https://dcbadge.limes.pink/api/server/nTYy5BXMWG)](https://discord.gg/nTYy5BXMWG)\n\n## What you will learn\n\n**[Mindmap of the Course](http://soshnikov.com/courses/ai-for-beginners/mindmap.html)**\n\nIn this curriculum, you will learn:\n\n* Different approaches to Artificial Intelligence, including the "good old" symbolic approach with **Knowledge Representation** and reasoning ([GOFAI](https://en.wikipedia.org/wiki/Symbolic_artificial_intelligence)).\n* **Neural Networks** and **Deep Learning**, which are at the core of modern AI. We will illustrate the concepts behind these important topics using code in two of the most popular frameworks - [TensorFlow](http://Tensorflow.org) and [PyTorch](http://pytorch.org).\n* **Neural Architectures** for working with images and text. We will cover recent models but may be a bit lacking in the state-of-the-art.\n* Less popular AI approaches, such as **Genetic Algorithms** and **Multi-Agent Systems**.\n\nWhat we will not cover in this curriculum:\n\n> [Find all additional resources for this course in our Microsoft Learn collection](https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum)\n\n* Business cases of using **AI in Business**. Consider taking [Introduction to AI for business users](https://docs.microsoft.com/learn/paths/introduction-ai-for-business-users/?WT.mc_id=academic-77998-bethanycheum) learning path on Microsoft Learn, or [AI Business School](https://www.microsoft.com/ai/ai-business-school/?WT.mc_id=academic-77998-bethanycheum), developed in cooperation with [INSEAD](https://www.insead.edu/).\n* **Classic Machine Learning**, which is well described in our [Machine Learning for Beginners Curriculum](http://github.com/Microsoft/ML-for-Beginners).\n* Practical AI applications built using **[Cognitive Services](https://azure.microsoft.com/services/cognitive-services/?WT.mc_id=academic-77998-bethanycheum)**. For this, we recommend that you start with modules Microsoft Learn for [vision](https://docs.microsoft.com/learn/paths/create-computer-vision-solutions-azure-cognitive-services/?WT.mc_id=academic-77998-bethanycheum), [natural language processing](https://docs.microsoft.com/learn/paths/explore-natural-language-processing/?WT.mc_id=academic-77998-bethanycheum), **[Generative AI with Azure OpenAI Service](https://learn.microsoft.com/en-us/training/paths/develop-ai-solutions-azure-openai/?WT.mc_id=academic-77998-bethanycheum)** and others.\n* Specific ML **Cloud Frameworks**, such as [Azure Machine Learning](https://azure.microsoft.com/services/machine-learning/?WT.mc_id=academic-77998-bethanycheum), [Microsoft Fabric](https://learn.microsoft.com/en-us/training/paths/get-started-fabric/?WT.mc_id=academic-77998-bethanycheum), or [Azure Databricks](https://docs.microsoft.com/learn/paths/data-engineer-azure-databricks?WT.mc_id=academic-77998-bethanycheum). Consider using [Build and operate machine learning solutions with Azure Machine Learning](https://docs.microsoft.com/learn/paths/build-ai-solutions-with-azure-ml-service/?WT.mc_id=academic-77998-bethanycheum) and [Build and Operate Machine Learning Solutions with Azure Databricks](https://docs.microsoft.com/learn/paths/build-operate-machine-learning-solutions-azure-databricks/?WT.mc_id=academic-77998-bethanycheum) learning paths.\n* **Conversational AI** and **Chat Bots**. There is a separate [Create conversational AI solutions](https://docs.microsoft.com/learn/paths/create-conversational-ai-solutions/?WT.mc_id=academic-77998-bethanycheum) learning path, and you can also refer to [this blog post](https://soshnikov.com/azure/hello-bot-conversational-ai-on-microsoft-platform/) for more detail.\n* **Deep Mathematics** behind deep learning. For this, we would recommend [Deep Learning](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618) by Ian Goodfellow, Yoshua Bengio and Aaron Courville, which is also available online at [https://www.deeplearningbook.org/](https://www.deeplearningbook.org/).\n\nFor a gentle introduction to _AI in the Cloud_ topics you may consider taking the [Get started with artificial intelligence on Azure](https://docs.microsoft.com/learn/paths/get-started-with-artificial-intelligence-on-azure/?WT.mc_id=academic-77998-bethanycheum) Learning Path.\n\n# Content\n\n|     |                                                                 Lesson Link                                                                  |                                           PyTorch/Keras/TensorFlow                                          | Lab                                                            |\n| :-: | :------------------------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------: | ------------------------------------------------------------------------------ |\n| 0  |                                 [Course Setup](./lessons/0-course-setup/setup.md)                                 |                      [Setup Your Development Environment](./lessons/0-course-setup/how-to-run.md)                       |   |\n| I  |               [**Introduction to AI**](./lessons/1-Intro/README.md)      | | |\n| 01  |       [Introduction and History of AI](./lessons/1-Intro/README.md)       |           -                            | -  |\n| II |              **Symbolic AI**              |\n| 02  |       [Knowledge Representation and Expert Systems](./lessons/2-Symbolic/README.md)       |            [Expert Systems](./lessons/2-Symbolic/Animals.ipynb) /  [Ontology](./lessons/2-Symbolic/FamilyOntology.ipynb) /[Concept Graph](./lessons/2-Symbolic/MSConceptGraph.ipynb)                             |  |\n| III |                        [**Introduction to Neural Networks**](./lessons/3-NeuralNetworks/README.md) |||\n| 03  |                [Perceptron](./lessons/3-NeuralNetworks/03-Perceptron/README.md)                 |                       [Notebook](./lessons/3-NeuralNetworks/03-Perceptron/Perceptron.ipynb)                      | [Lab](./lessons/3-NeuralNetworks/03-Perceptron/lab/README.md) |\n| 04  |                   [Multi-Layered Perceptron and Creating our own Framework](./lessons/3-NeuralNetworks/04-OwnFramework/README.md)                   |        [Notebook](./lessons/3-NeuralNetworks/04-OwnFramework/OwnFramework.ipynb)        | [Lab](./lessons/3-NeuralNetworks/04-OwnFramework/lab/README.md) |\n| 05  |            [Intro to Frameworks (PyTorch/TensorFlow) and Overfitting](./lessons/3-NeuralNetworks/05-Frameworks/README.md)             |           [PyTorch](./lessons/3-NeuralNetworks/05-Frameworks/IntroPyTorch.ipynb) / [Keras](./lessons/3-NeuralNetworks/05-Frameworks/IntroKeras.ipynb) / [TensorFlow](./lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb)             | [Lab](./lessons/3-NeuralNetworks/05-Frameworks/lab/README.md) |\n| IV  |            [**Computer Vision**](./lessons/4-ComputerVision/README.md)             | [PyTorch](https://docs.microsoft.com/learn/modules/intro-computer-vision-pytorch/?WT.mc_id=academic-77998-cacaste) / [TensorFlow](https://docs.microsoft.com/learn/modules/intro-computer-vision-TensorFlow/?WT.mc_id=academic-77998-cacaste)| [Explore Computer Vision on Microsoft Azure](https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum) |\n| 06  |            [Intro to Computer Vision. OpenCV](./lessons/4-ComputerVision/06-IntroCV/README.md)             |           [Notebook](./lessons/4-ComputerVision/06-IntroCV/OpenCV.ipynb)         | [Lab](./lessons/4-ComputerVision/06-IntroCV/lab/README.md) |\n| 07  |            [Convolutional Neural Networks](./lessons/4-ComputerVision/07-ConvNets/README.md) &  [CNN Architectures](./lessons/4-ComputerVision/07-ConvNets/CNN_Architectures.md)             |           [PyTorch](./lessons/4-ComputerVision/07-ConvNets/ConvNetsPyTorch.ipynb) /[TensorFlow](./lessons/4-ComputerVision/07-ConvNets/ConvNetsTF.ipynb)             | [Lab](./lessons/4-ComputerVision/07-ConvNets/lab/README.md) |\n| 08  |            [Pre-trained Networks and Transfer Learning](./lessons/4-ComputerVision/08-TransferLearning/README.md) and [Training Tricks](./lessons/4-ComputerVision/08-TransferLearning/TrainingTricks.md)             |           [PyTorch](./lessons/4-ComputerVision/08-TransferLearning/TransferLearningPyTorch.ipynb) / [TensorFlow](./lessons/3-NeuralNetworks/05-Frameworks/IntroKerasTF.ipynb)             | [Lab](./lessons/4-ComputerVision/08-TransferLearning/lab/README.md) |\n| 09  |            [Autoencoders and VAEs](./lessons/4-ComputerVision/09-Autoencoders/README.md)             |           [PyTorch](./lessons/4-ComputerVision/09-Autoencoders/AutoEncodersPyTorch.ipynb) / [TensorFlow](./lessons/4-ComputerVision/09-Autoencoders/AutoencodersTF.ipynb)             |  |\n| 10  |            [Generative Adversarial Networks & Artistic Style Transfer](./lessons/4-ComputerVision/10-GANs/README.md)             |           [PyTorch](./lessons/4-ComputerVision/10-GANs/GANPyTorch.ipynb) / [TensorFlow](./lessons/4-ComputerVision/10-GANs/GANTF.ipynb)             |  |\n| 11  |            [Object Detection](./lessons/4-ComputerVision/11-ObjectDetection/README.md)             |         [TensorFlow](./lessons/4-ComputerVision/11-ObjectDetection/ObjectDetection.ipynb)             | [Lab](./lessons/4-ComputerVision/11-ObjectDetection/lab/README.md) |\n| 12  |            [Semantic Segmentation. U-Net](./lessons/4-ComputerVision/12-Segmentation/README.md)             |           [PyTorch](./lessons/4-ComputerVision/12-Segmentation/SemanticSegmentationPytorch.ipynb) / [TensorFlow](./lessons/4-ComputerVision/12-Segmentation/SemanticSegmentationTF.ipynb)             |  |\n| V  |            [**Natural Language Processing**](./lessons/5-NLP/README.md)             | [PyTorch](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-pytorch/?WT.mc_id=academic-77998-cacaste) /[TensorFlow](https://docs.microsoft.com/learn/modules/intro-natural-language-processing-TensorFlow/?WT.mc_id=academic-77998-cacaste) | [Explore Natural Language Processing on Microsoft Azure](https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum)|\n| 13  |            [Text Representation. Bow/TF-IDF](./lessons/5-NLP/13-TextRep/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/13-TextRep/TextRepresentationPyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/13-TextRep/TextRepresentationTF.ipynb)             | |\n| 14  |            [Semantic word embeddings. Word2Vec and GloVe](./lessons/5-NLP/14-Embeddings/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/14-Embeddings/EmbeddingsPyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/14-Embeddings/EmbeddingsTF.ipynb)             |  |\n| 15  |            [Language Modeling. Training your own embeddings](./lessons/5-NLP/15-LanguageModeling/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/15-LanguageModeling/CBoW-PyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/15-LanguageModeling/CBoW-TF.ipynb)             | [Lab](./lessons/5-NLP/15-LanguageModeling/lab/README.md) |\n| 16  |            [Recurrent Neural Networks](./lessons/5-NLP/16-RNN/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/16-RNN/RNNPyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/16-RNN/RNNTF.ipynb)             |  |\n| 17  |            [Generative Recurrent Networks](./lessons/5-NLP/17-GenerativeNetworks/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/17-GenerativeNetworks/GenerativePyTorch.ipynb) / [TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/17-GenerativeNetworks/GenerativeTF.ipynb)             | [Lab](./lessons/5-NLP/17-GenerativeNetworks/lab/README.md) |\n| 18  |            [Transformers. BERT.](./lessons/5-NLP/18-Transformers/README.md)             |           [PyTorch](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/18-Transformers/TransformersPyTorch.ipynb) /[TensorFlow](https://github.com/microsoft/AI-For-Beginners/blob/main/lessons/5-NLP/18-Transformers/TransformersTF.ipynb)             |  |\n| 19  |            [Named Entity Recognition](./lessons/5-NLP/19-NER/README.md)             |           [TensorFlow](https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/19-NER/NER-TF.ipynb)             | [Lab](./lessons/5-NLP/19-NER/lab/README.md) |\n| 20  |            [Large Language Models, Prompt Programming and Few-Shot Tasks](./lessons/5-NLP/20-LangModels/README.md)             |           [PyTorch](https://microsoft.github.io/AI-For-Beginners/lessons/5-NLP/20-LangModels/GPT-PyTorch.ipynb) | |\n| VI |            **Other AI Techniques** || |\n| 21  |            [Genetic Algorithms](./lessons/6-Other/21-GeneticAlgorithms/README.md)             |           [Notebook](./lessons/6-Other/21-GeneticAlgorithms/Genetic.ipynb) | |\n| 22  |            [Deep Reinforcement Learning](./lessons/6-Other/22-DeepRL/README.md)             |           [PyTorch](./lessons/6-Other/22-DeepRL/CartPole-RL-PyTorch.ipynb) /[TensorFlow](./lessons/6-Other/22-DeepRL/CartPole-RL-TF.ipynb)             | [Lab](./lessons/6-Other/22-DeepRL/lab/README.md) |\n| 23  |            [Multi-Agent Systems](./lessons/6-Other/23-MultiagentSystems/README.md)             |  | |\n| VII |            **AI Ethics** | | |\n| 24  |            [AI Ethics and Responsible AI](./lessons/7-Ethics/README.md)             |           [Microsoft Learn: Responsible AI Principles](https://docs.microsoft.com/learn/paths/responsible-ai-business-principles/?WT.mc_id=academic-77998-cacaste) | |\n| IX  |            **Extras** | | |\n| 25  |            [Multi-Modal Networks, CLIP and VQGAN](./lessons/X-Extras/X1-MultiModal/README.md)             |           [Notebook](./lessons/X-Extras/X1-MultiModal/Clip.ipynb)    | |\n\n## Each lesson contains\n\n* Pre-reading material\n* Executable Jupyter Notebooks, which are often specific to the framework (**PyTorch** or **TensorFlow**). The executable notebook also contains a lot of theoretical material, so to understand the topic you need to go through at least one version of the notebook (either PyTorch or TensorFlow).\n* **Labs** available for some topics, which give you an opportunity to try applying the material you have learned to a specific problem.\n* Some sections contain links to [**MS Learn**](https://learn.microsoft.com/en-us/collections/7w28iy2xrqzdj0?WT.mc_id=academic-77998-bethanycheum) modules that cover related topics.\n\n## Getting Started\n\n### ðŸŽ¯ New to AI? Start Here!\n\nIf you''re completely new to AI and want quick, hands-on examples, check out our [**Beginner-Friendly Examples**](./examples/README.md)! These include:\n\n- ðŸŒŸ **Hello AI World** - Your first AI program (pattern recognition)\n- ðŸ§  **Simple Neural Network** - Build a neural network from scratch  \n- ðŸ–¼ï¸ **Image Classifier** - Classify images with detailed comments\n- ðŸ’¬ **Text Sentiment** - Analyze positive/negative text\n\nThese examples are designed to help you understand AI concepts before diving into the full curriculum.\n\n### ðŸ“š Full Curriculum Setup\n\n- We have created a [setup lesson](./lessons/0-course-setup/setup.md) to help you with setting up your development environment. - For Educators, we have created a [curricula setup lesson](./lessons/0-course-setup/for-teachers.md) for you too!\n- How to [Run the code in a VSCode or a Codepace](./lessons/0-course-setup/how-to-run.md)\n\nFollow these steps:\n\nFork the Repository: Click on the "Fork" button at the top-right corner of this page.\n\nClone the Repository: `git clone https://github.com/microsoft/AI-For-Beginners.git`\n\nDon''t forget to star (ðŸŒŸ) this repo to find it easier later.\n\n## Meet other Learners\n\nJoin our [official AI Discord server](https://aka.ms/genai-discord?WT.mc_id=academic-105485-bethanycheum) to meet and network with other learners taking this course and get support.\n\nIf you have product feedback or questions whilst building visit our [Azure AI Foundry Developer Forum](https://aka.ms/foundry/forum)\n\n## Quizzes \n\n> **A note about quizzes**: All quizzes are contained in the Quiz-app folder in etc\quiz-app, or [Online Here](https://ff-quizzes.netlify.app/) They are linked from within the lessons the quiz app can be run locally or deployed to Azure; follow the instruction in the `quiz-app` folder. They are gradually being localized.\n\n## Help Wanted\n\nDo you have suggestions or found spelling or code errors? Raise an issue or create a pull request.\n\n## Special Thanks\n\n* **âœï¸ Primary Author:** [Dmitry Soshnikov](http://soshnikov.com), PhD\n* **ðŸ”¥ Editor:** [Jen Looper](https://twitter.com/jenlooper), PhD\n* **ðŸŽ¨ Sketchnote illustrator:** [Tomomi Imura](https://twitter.com/girlie_mac)\n* **âœ… Quiz Creator:** [Lateefah Bello](https://github.com/CinnamonXI), [MLSA](https://studentambassadors.microsoft.com/)\n* **ðŸ™ Core Contributors:** [Evgenii Pishchik](https://github.com/Pe4enIks)\n\n## Other Curricula\n\nOur team produces other curricula! Check out:\n\n<!-- CO-OP TRANSLATOR OTHER COURSES START -->\n### Azure / Edge / MCP / Agents\n[![AZD for Beginners](https://img.shields.io/badge/AZD%20for%20Beginners-0078D4?style=for-the-badge&labelColor=E5E7EB&color=0078D4)](https://github.com/microsoft/AZD-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![Edge AI for Beginners](https://img.shields.io/badge/Edge%20AI%20for%20Beginners-00B8E4?style=for-the-badge&labelColor=E5E7EB&color=00B8E4)](https://github.com/microsoft/edgeai-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![MCP for Beginners](https://img.shields.io/badge/MCP%20for%20Beginners-009688?style=for-the-badge&labelColor=E5E7EB&color=009688)](https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![AI Agents for Beginners](https://img.shields.io/badge/AI%20Agents%20for%20Beginners-00C49A?style=for-the-badge&labelColor=E5E7EB&color=00C49A)](https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Generative AI Series\n[![Generative AI for Beginners](https://img.shields.io/badge/Generative%20AI%20for%20Beginners-8B5CF6?style=for-the-badge&labelColor=E5E7EB&color=8B5CF6)](https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (.NET)](https://img.shields.io/badge/Generative%20AI%20(.NET)-9333EA?style=for-the-badge&labelColor=E5E7EB&color=9333EA)](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (Java)](https://img.shields.io/badge/Generative%20AI%20(Java)-C084FC?style=for-the-badge&labelColor=E5E7EB&color=C084FC)](https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (JavaScript)](https://img.shields.io/badge/Generative%20AI%20(JavaScript)-E879F9?style=for-the-badge&labelColor=E5E7EB&color=E879F9)](https://github.com/microsoft/generative-ai-with-javascript?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Core Learning\n[![ML for Beginners](https://img.shields.io/badge/ML%20for%20Beginners-22C55E?style=for-the-badge&labelColor=E5E7EB&color=22C55E)](https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst)\n[![Data Science for Beginners](https://img.shields.io/badge/Data%20Science%20for%20Beginners-84CC16?style=for-the-badge&labelColor=E5E7EB&color=84CC16)](https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst)\n[![AI for Beginners](https://img.shields.io/badge/AI%20for%20Beginners-A3E635?style=for-the-badge&labelColor=E5E7EB&color=A3E635)](https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst)\n[![Cybersecurity for Beginners](https://img.shields.io/badge/Cybersecurity%20for%20Beginners-F97316?style=for-the-badge&labelColor=E5E7EB&color=F97316)](https://github.com/microsoft/Security-101?WT.mc_id=academic-96948-sayoung)\n[![Web Dev for Beginners](https://img.shields.io/badge/Web%20Dev%20for%20Beginners-EC4899?style=for-the-badge&labelColor=E5E7EB&color=EC4899)](https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst)\n[![IoT for Beginners](https://img.shields.io/badge/IoT%20for%20Beginners-14B8A6?style=for-the-badge&labelColor=E5E7EB&color=14B8A6)](https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst)\n[![XR Development for Beginners](https://img.shields.io/badge/XR%20Development%20for%20Beginners-38BDF8?style=for-the-badge&labelColor=E5E7EB&color=38BDF8)](https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Copilot Series\n[![Copilot for AI Paired Programming](https://img.shields.io/badge/Copilot%20for%20AI%20Paired%20Programming-FACC15?style=for-the-badge&labelColor=E5E7EB&color=FACC15)](https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst)\n[![Copilot for C#/.NET](https://img.shields.io/badge/Copilot%20for%20C%23/.NET-FBBF24?style=for-the-badge&labelColor=E5E7EB&color=FBBF24)](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst)\n[![Copilot Adventure](https://img.shields.io/badge/Copilot%20Adventure-FDE68A?style=for-the-badge&labelColor=E5E7EB&color=FDE68A)](https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst)\n<!-- CO-OP TRANSLATOR OTHER COURSES END -->\n\n## Getting Help\n\nIf you get stuck or have any questions about building AI apps. Join fellow learners and experienced developers in discussions about MCP. It''s a supportive community where questions are welcome and knowledge is shared freely.\n\n[![Microsoft Foundry Discord](https://dcbadge.limes.pink/api/server/nTYy5BXMWG)](https://discord.gg/nTYy5BXMWG)\n\nIf you have product feedback or errors while building visit:\n\n[![Microsoft Foundry Developer Forum](https://img.shields.io/badge/GitHub-Microsoft_Foundry_Developer_Forum-blue?style=for-the-badge&logo=github&color=000000&logoColor=fff)](https://aka.ms/foundry/forum)\n', '{"language":"Jupyter Notebook","stars":44118,"forks":8786,"watchers":44118,"open_issues":7,"topics":["ai","artificial-intelligence","cnn","computer-vision","deep-learning","gan","machine-learning","microsoft-for-beginners","nlp","rnn"],"default_branch":"main","size_kb":1003297,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:microsoft:AI-For-Beginners","source_url":"https://github.com/microsoft/AI-For-Beginners"},{"type":"has_code","target_id":"github:Azure:co-op-translator","source_url":"https://github.com/Azure/co-op-translator"},{"type":"has_code","target_id":"github:Microsoft:ML-for-Beginners","source_url":"http://github.com/Microsoft/ML-for-Beginners"},{"type":"has_code","target_id":"github:microsoft:AI-For-Beginners","source_url":"https://github.com/microsoft/AI-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:AI-For-Beginners","source_url":"https://github.com/microsoft/AI-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:AI-For-Beginners","source_url":"https://github.com/microsoft/AI-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:AI-For-Beginners","source_url":"https://github.com/microsoft/AI-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:AI-For-Beginners","source_url":"https://github.com/microsoft/AI-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:AI-For-Beginners","source_url":"https://github.com/microsoft/AI-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:AI-For-Beginners","source_url":"https://github.com/microsoft/AI-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:AI-For-Beginners","source_url":"https://github.com/microsoft/AI-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:AI-For-Beginners","source_url":"https://github.com/microsoft/AI-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:AI-For-Beginners","source_url":"https://github.com/microsoft/AI-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:AI-For-Beginners","source_url":"https://github.com/microsoft/AI-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:AI-For-Beginners","source_url":"https://github.com/microsoft/AI-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:AI-For-Beginners.git`","source_url":"https://github.com/microsoft/AI-For-Beginners.git`"},{"type":"has_code","target_id":"github:microsoft:AZD-for-beginners","source_url":"https://github.com/microsoft/AZD-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:edgeai-for-beginners","source_url":"https://github.com/microsoft/edgeai-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:mcp-for-beginners","source_url":"https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:ai-agents-for-beginners","source_url":"https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-for-beginners","source_url":"https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:Generative-AI-for-beginners-dotnet","source_url":"https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-for-beginners-java","source_url":"https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-with-javascript","source_url":"https://github.com/microsoft/generative-ai-with-javascript?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:Security-101","source_url":"https://github.com/microsoft/Security-101?WT.mc_id=academic-96948-sayoung"},{"type":"has_code","target_id":"github:microsoft:xr-development-for-beginners","source_url":"https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:mastering-github-copilot-for-dotnet-csharp-developers","source_url":"https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:CopilotAdventures","source_url":"https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst"}]', NULL, 'MIT', 'approved', 80, '992d1f99892cc34bff66034d85178ee1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-AI-For-Beginners from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-AI-For-Beginners.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-aymericdamien-TensorFlow-Examples', 'github--aymericdamien--tensorflow-examples', 'TensorFlow-Examples', 'aymericdamien', 'This tutorial was designed for easily diving into TensorFlow, through examples. For readability, it includes both notebooks and source codes with explanation, for both TF v1 & v2. It is suitable for beginners who want to find clear and concise examples about TensorFlow. Besides the traditional ''raw'' TensorFlow implementations, you can also find the latest TensorFlow API practices (such as , , , ...). **Update (05/16/2020):** Moving all default examples to TF2. For TF v1 examples: check here. ...', '["deep-learning","examples","machine-learning","python","tensorflow","tutorial","jupyter notebook"]', 'other', 43765, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/aymericdamien/TensorFlow-Examples","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# TensorFlow Examples\n\nThis tutorial was designed for easily diving into TensorFlow, through examples. For readability, it includes both notebooks and source codes with explanation, for both TF v1 & v2.\n\nIt is suitable for beginners who want to find clear and concise examples about TensorFlow. Besides the traditional ''raw'' TensorFlow implementations, you can also find the latest TensorFlow API practices (such as `layers`, `estimator`, `dataset`, ...).\n\n**Update (05/16/2020):** Moving all default examples to TF2. For TF v1 examples: [check here](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1).\n\n## Tutorial index\n\n#### 0 - Prerequisite\n- [Introduction to Machine Learning](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/0_Prerequisite/ml_introduction.ipynb).\n- [Introduction to MNIST Dataset](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb).\n\n#### 1 - Introduction\n- **Hello World** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/1_Introduction/helloworld.ipynb)). Very simple example to learn how to print "hello world" using TensorFlow 2.0+.\n- **Basic Operations** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/1_Introduction/basic_operations.ipynb)). A simple example that cover TensorFlow 2.0+ basic operations.\n\n#### 2 - Basic Models\n- **Linear Regression** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/2_BasicModels/linear_regression.ipynb)). Implement a Linear Regression with TensorFlow 2.0+.\n- **Logistic Regression** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/2_BasicModels/logistic_regression.ipynb)). Implement a Logistic Regression with TensorFlow 2.0+.\n- **Word2Vec (Word Embedding)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/2_BasicModels/word2vec.ipynb)). Build a Word Embedding Model (Word2Vec) from Wikipedia data, with TensorFlow 2.0+.\n- **GBDT (Gradient Boosted Decision Trees)** ([notebooks](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/2_BasicModels/gradient_boosted_trees.ipynb)). Implement a Gradient Boosted Decision Trees with TensorFlow 2.0+ to predict house value using Boston Housing dataset.\n\n#### 3 - Neural Networks\n##### Supervised\n\n- **Simple Neural Network** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/neural_network.ipynb)). Use TensorFlow 2.0 ''layers'' and ''model'' API to build a simple neural network to classify MNIST digits dataset.\n- **Simple Neural Network (low-level)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/neural_network_raw.ipynb)). Raw implementation of a simple neural network to classify MNIST digits dataset.\n- **Convolutional Neural Network** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/convolutional_network.ipynb)). Use TensorFlow 2.0+ ''layers'' and ''model'' API to build a convolutional neural network to classify MNIST digits dataset.\n- **Convolutional Neural Network (low-level)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/convolutional_network_raw.ipynb)). Raw implementation of a convolutional neural network to classify MNIST digits dataset.\n- **Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/recurrent_network.ipynb)). Build a recurrent neural network (LSTM) to classify MNIST digits dataset, using TensorFlow 2.0 ''layers'' and ''model'' API.\n- **Bi-directional Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/bidirectional_rnn.ipynb)). Build a bi-directional recurrent neural network (LSTM) to classify MNIST digits dataset, using TensorFlow 2.0+ ''layers'' and ''model'' API.\n- **Dynamic Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/dynamic_rnn.ipynb)). Build a recurrent neural network (LSTM) that performs dynamic calculation to classify sequences of variable length, using TensorFlow 2.0+ ''layers'' and ''model'' API.\n\n##### Unsupervised\n- **Auto-Encoder** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/autoencoder.ipynb)). Build an auto-encoder to encode an image to a lower dimension and re-construct it.\n- **DCGAN (Deep Convolutional Generative Adversarial Networks)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/3_NeuralNetworks/dcgan.ipynb)). Build a Deep Convolutional Generative Adversarial Network (DCGAN) to generate images from noise.\n\n#### 4 - Utilities\n- **Save and Restore a model** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/4_Utils/save_restore_model.ipynb)). Save and Restore a model with TensorFlow 2.0+.\n- **Build Custom Layers & Modules** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/4_Utils/build_custom_layers.ipynb)). Learn how to build your own layers / modules and integrate them into TensorFlow 2.0+ Models.\n- **Tensorboard** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/4_Utils/tensorboard.ipynb)). Track and visualize neural network computation graph, metrics, weights and more using TensorFlow 2.0+ tensorboard.\n\n#### 5 - Data Management\n- **Load and Parse data** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/5_DataManagement/load_data.ipynb)). Build efficient data pipeline with TensorFlow 2.0 (Numpy arrays, Images, CSV files, custom data, ...).\n- **Build and Load TFRecords** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/5_DataManagement/tfrecords.ipynb)). Convert data into TFRecords format, and load them with TensorFlow 2.0+.\n- **Image Transformation (i.e. Image Augmentation)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/5_DataManagement/image_transformation.ipynb)). Apply various image augmentation techniques with TensorFlow 2.0+, to generate distorted images for training.\n\n#### 6 - Hardware\n- **Multi-GPU Training** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v2/notebooks/6_Hardware/multigpu_training.ipynb)). Train a convolutional neural network with multiple GPUs on CIFAR-10 dataset.\n\n## TensorFlow v1\n\nThe tutorial index for TF v1 is available here: [TensorFlow v1.15 Examples](tensorflow_v1). Or see below for a list of the examples.\n\n## Dataset\nSome examples require MNIST dataset for training and testing. Don''t worry, this dataset will automatically be downloaded when running examples.\nMNIST is a database of handwritten digits, for a quick description of that dataset, you can check [this notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/0_Prerequisite/mnist_dataset_intro.ipynb).\n\nOfficial Website: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).\n\n## Installation\n\nTo download all the examples, simply clone this repository:\n```\ngit clone https://github.com/aymericdamien/TensorFlow-Examples\n```\n\nTo run them, you also need the latest version of TensorFlow. To install it:\n```\npip install tensorflow\n```\n\nor (with GPU support):\n```\npip install tensorflow_gpu\n```\n\nFor more details about TensorFlow installation, you can check [TensorFlow Installation Guide](https://www.tensorflow.org/install/)\n\n\n## TensorFlow v1 Examples - Index\n\nThe tutorial index for TF v1 is available here: [TensorFlow v1.15 Examples](tensorflow_v1).\n\n#### 0 - Prerequisite\n- [Introduction to Machine Learning](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/tensorflow_v1/0_Prerequisite/ml_introduction.ipynb).\n- [Introduction to MNIST Dataset](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/tensorflow_v1/0_Prerequisite/mnist_dataset_intro.ipynb).\n\n#### 1 - Introduction\n- **Hello World** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/1_Introduction/helloworld.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/1_Introduction/helloworld.py)). Very simple example to learn how to print "hello world" using TensorFlow.\n- **Basic Operations** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/tensorflow_v1/1_Introduction/basic_operations.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-examples/Examples/blob/master/tensorflow_v1/1_Introduction/basic_operations.py)). A simple example that cover TensorFlow basic operations.\n- **TensorFlow Eager API basics** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/1_Introduction/basic_eager_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/1_Introduction/basic_eager_api.py)). Get started with TensorFlow''s Eager API.\n\n#### 2 - Basic Models\n- **Linear Regression** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/linear_regression.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/linear_regression.py)). Implement a Linear Regression with TensorFlow.\n- **Linear Regression (eager api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/linear_regression_eager_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/linear_regression_eager_api.py)). Implement a Linear Regression using TensorFlow''s Eager API.\n- **Logistic Regression** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/logistic_regression.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/logistic_regression.py)). Implement a Logistic Regression with TensorFlow.\n- **Logistic Regression (eager api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/logistic_regression_eager_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/logistic_regression_eager_api.py)). Implement a Logistic Regression using TensorFlow''s Eager API.\n- **Nearest Neighbor** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/nearest_neighbor.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/nearest_neighbor.py)). Implement Nearest Neighbor algorithm with TensorFlow.\n- **K-Means** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/kmeans.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/kmeans.py)). Build a K-Means classifier with TensorFlow.\n- **Random Forest** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/random_forest.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/random_forest.py)). Build a Random Forest classifier with TensorFlow.\n- **Gradient Boosted Decision Tree (GBDT)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/gradient_boosted_decision_tree.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/gradient_boosted_decision_tree.py)). Build a Gradient Boosted Decision Tree (GBDT) with TensorFlow.\n- **Word2Vec (Word Embedding)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/2_BasicModels/word2vec.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/2_BasicModels/word2vec.py)). Build a Word Embedding Model (Word2Vec) from Wikipedia data, with TensorFlow.\n\n#### 3 - Neural Networks\n##### Supervised\n\n- **Simple Neural Network** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/3_NeuralNetworks/notebooks/neural_network_raw.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/neural_network_raw.py)). Build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset. Raw TensorFlow implementation.\n- **Simple Neural Network (tf.layers/estimator api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/neural_network.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/neural_network.py)). Use TensorFlow ''layers'' and ''estimator'' API to build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset.\n- **Simple Neural Network (eager api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/neural_network_eager_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/neural_network_eager_api.py)). Use TensorFlow Eager API to build a simple neural network (a.k.a Multi-layer Perceptron) to classify MNIST digits dataset.\n- **Convolutional Neural Network** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/convolutional_network_raw.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/convolutional_network_raw.py)). Build a convolutional neural network to classify MNIST digits dataset. Raw TensorFlow implementation.\n- **Convolutional Neural Network (tf.layers/estimator api)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/convolutional_network.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/convolutional_network.py)). Use TensorFlow ''layers'' and ''estimator'' API to build a convolutional neural network to classify MNIST digits dataset.\n- **Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/recurrent_network.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/recurrent_network.py)). Build a recurrent neural network (LSTM) to classify MNIST digits dataset.\n- **Bi-directional Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/bidirectional_rnn.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/bidirectional_rnn.py)). Build a bi-directional recurrent neural network (LSTM) to classify MNIST digits dataset.\n- **Dynamic Recurrent Neural Network (LSTM)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/dynamic_rnn.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/dynamic_rnn.py)). Build a recurrent neural network (LSTM) that performs dynamic calculation to classify sequences of different length.\n\n##### Unsupervised\n- **Auto-Encoder** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/autoencoder.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/autoencoder.py)). Build an auto-encoder to encode an image to a lower dimension and re-construct it.\n- **Variational Auto-Encoder** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/variational_autoencoder.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/variational_autoencoder.py)). Build a variational auto-encoder (VAE), to encode and generate images from noise.\n- **GAN (Generative Adversarial Networks)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/gan.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/gan.py)). Build a Generative Adversarial Network (GAN) to generate images from noise.\n- **DCGAN (Deep Convolutional Generative Adversarial Networks)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/3_NeuralNetworks/dcgan.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/3_NeuralNetworks/dcgan.py)). Build a Deep Convolutional Generative Adversarial Network (DCGAN) to generate images from noise.\n\n#### 4 - Utilities\n- **Save and Restore a model** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/4_Utils/save_restore_model.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/4_Utils/save_restore_model.py)). Save and Restore a model with TensorFlow.\n- **Tensorboard - Graph and loss visualization** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/4_Utils/tensorboard_basic.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/4_Utils/tensorboard_basic.py)). Use Tensorboard to visualize the computation Graph and plot the loss.\n- **Tensorboard - Advanced visualization** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/4_Utils/tensorboard_advanced.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/4_Utils/tensorboard_advanced.py)). Going deeper into Tensorboard; visualize the variables, gradients, and more...\n\n#### 5 - Data Management\n- **Build an image dataset** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/build_an_image_dataset.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/build_an_image_dataset.py)). Build your own images dataset with TensorFlow data queues, from image folders or a dataset file.\n- **TensorFlow Dataset API** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/tensorflow_dataset_api.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/5_DataManagement/tensorflow_dataset_api.py)). Introducing TensorFlow Dataset API for optimizing the input data pipeline.\n- **Load and Parse data** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/load_data.ipynb)). Build efficient data pipeline (Numpy arrays, Images, CSV files, custom data, ...).\n- **Build and Load TFRecords** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/tfrecords.ipynb)). Convert data into TFRecords format, and load them.\n- **Image Transformation (i.e. Image Augmentation)** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/5_DataManagement/image_transformation.ipynb)). Apply various image augmentation techniques, to generate distorted images for training.\n\n#### 6 - Multi GPU\n- **Basic Operations on multi-GPU** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/6_MultiGPU/multigpu_basics.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/6_MultiGPU/multigpu_basics.py)). A simple example to introduce multi-GPU in TensorFlow.\n- **Train a Neural Network on multi-GPU** ([notebook](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/notebooks/6_MultiGPU/multigpu_cnn.ipynb)) ([code](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/tensorflow_v1/examples/6_MultiGPU/multigpu_cnn.py)). A clear and simple TensorFlow implementation to train a convolutional neural network on multiple GPUs.\n\n## More Examples\nThe following examples are coming from [TFLearn](https://github.com/tflearn/tflearn), a library that provides a simplified interface for TensorFlow. You can have a look, there are many [examples](https://github.com/tflearn/tflearn/tree/master/examples) and [pre-built operations and layers](http://tflearn.org/doc_index/#api).\n\n### Tutorials\n- [TFLearn Quickstart](https://github.com/tflearn/tflearn/blob/master/tutorials/intro/quickstart.md). Learn the basics of TFLearn through a concrete machine learning task. Build and train a deep neural network classifier.\n\n### Examples\n- [TFLearn Examples](https://github.com/tflearn/tflearn/blob/master/examples). A large collection of examples using TFLearn.\n\n', '{"language":"Jupyter Notebook","stars":43765,"forks":14833,"watchers":43765,"open_issues":226,"topics":["deep-learning","examples","machine-learning","python","tensorflow","tutorial"],"default_branch":"master","size_kb":10242,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-examples","source_url":"https://github.com/aymericdamien/TensorFlow-examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:tflearn:tflearn","source_url":"https://github.com/tflearn/tflearn"},{"type":"has_code","target_id":"github:tflearn:tflearn","source_url":"https://github.com/tflearn/tflearn"},{"type":"has_code","target_id":"github:tflearn:tflearn","source_url":"https://github.com/tflearn/tflearn"},{"type":"has_code","target_id":"github:tflearn:tflearn","source_url":"https://github.com/tflearn/tflearn"}]', NULL, 'NOASSERTION', 'approved', 80, 'f2fb1f9be1421e79b60fc7ba27bde347', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-aymericdamien-TensorFlow-Examples from https://github.com/aymericdamien.png
Image converted to WebP: data/images/github-aymericdamien-TensorFlow-Examples.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-apache-airflow', 'github--apache--airflow', 'airflow', 'apache', '<!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in w...', '["airflow","apache","apache-airflow","automation","dag","data-engineering","data-integration","data-orchestrator","data-pipelines","data-science","elt","etl","machine-learning","mlops","orchestration","python","scheduler","workflow","workflow-engine","workflow-orchestration","python"]', 'other', 43457, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/apache/airflow","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<!--\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n "License"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n-->\n\n<!-- START Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n# Apache Airflow\n\n| Category   | Badges                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n|------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| License    | [![License](https://img.shields.io/:license-Apache%202-blue.svg)](https://www.apache.org/licenses/LICENSE-2.0.txt)                                                                                                                                                                                                                                                                                                                                                                                                                   |\n| PyPI       | [![PyPI version](https://badge.fury.io/py/apache-airflow.svg)](https://badge.fury.io/py/apache-airflow) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/apache-airflow.svg)](https://pypi.org/project/apache-airflow/) [![PyPI - Downloads](https://img.shields.io/pypi/dm/apache-airflow)](https://pypi.org/project/apache-airflow/)                                                                                                                                                                               |\n| Containers | [![Docker Pulls](https://img.shields.io/docker/pulls/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow) [![Docker Stars](https://img.shields.io/docker/stars/apache/airflow.svg)](https://hub.docker.com/r/apache/airflow) [![Artifact HUB](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/apache-airflow)](https://artifacthub.io/packages/search?repo=apache-airflow)                                                                                                                      |\n| Community  | [![Contributors](https://img.shields.io/github/contributors/apache/airflow)](https://github.com/apache/airflow/graphs/contributors) [![Slack Status](https://img.shields.io/badge/slack-join_chat-white.svg?logo=slack&style=social)](https://s.apache.org/airflow-slack) ![Commit Activity](https://img.shields.io/github/commit-activity/m/apache/airflow) [![LFX Health Score](https://insights.linuxfoundation.org/api/badge/health-score?project=apache-airflow)](https://insights.linuxfoundation.org/project/apache-airflow)  |\n| Dev tools  | [![prek](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/j178/prek/master/docs/assets/badge-v0.json)](https://github.com/j178/prek)                                                                                                                                                                                                                                                                                                                                                                                 |\n\n\n| Version | Build Status                                                                                                                                                    |\n|---------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Main    | [![GitHub Build main](https://github.com/apache/airflow/actions/workflows/ci-amd-arm.yml/badge.svg)](https://github.com/apache/airflow/actions)                 |\n| 3.x     | [![GitHub Build 3.1](https://github.com/apache/airflow/actions/workflows/ci-amd-arm.yml/badge.svg?branch=v3-1-test)](https://github.com/apache/airflow/actions) |\n| 2.x     | [![GitHub Build 2.11](https://github.com/apache/airflow/actions/workflows/ci.yml/badge.svg?branch=v2-11-test)](https://github.com/apache/airflow/actions)       |\n\n\n\n<picture width="500">\n  <img\n    src="https://github.com/apache/airflow/blob/19ebcac2395ef9a6b6ded3a2faa29dc960c1e635/docs/apache-airflow/img/logos/wordmark_1.png?raw=true"\n    alt="Apache Airflow logo"\n  />\n</picture>\n\n[Apache Airflow](https://airflow.apache.org/docs/apache-airflow/stable/) (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.\n\nWhen workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.\n\nUse Airflow to author workflows (Dags) that orchestrate tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\n\n<!-- END Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON''T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n**Table of contents**\n\n- [Project Focus](#project-focus)\n- [Principles](#principles)\n- [Requirements](#requirements)\n- [Getting started](#getting-started)\n- [Installing from PyPI](#installing-from-pypi)\n- [Installation](#installation)\n- [Official source code](#official-source-code)\n- [Convenience packages](#convenience-packages)\n- [User Interface](#user-interface)\n- [Semantic versioning](#semantic-versioning)\n- [Version Life Cycle](#version-life-cycle)\n- [Support for Python and Kubernetes versions](#support-for-python-and-kubernetes-versions)\n- [Base OS support for reference Airflow images](#base-os-support-for-reference-airflow-images)\n- [Approach to dependencies of Airflow](#approach-to-dependencies-of-airflow)\n- [Contributing](#contributing)\n- [Voting Policy](#voting-policy)\n- [Who uses Apache Airflow?](#who-uses-apache-airflow)\n- [Who maintains Apache Airflow?](#who-maintains-apache-airflow)\n- [What goes into the next release?](#what-goes-into-the-next-release)\n- [Can I use the Apache Airflow logo in my presentation?](#can-i-use-the-apache-airflow-logo-in-my-presentation)\n- [Links](#links)\n- [Sponsors](#sponsors)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n## Project Focus\n\nAirflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include [Luigi](https://github.com/spotify/luigi), [Oozie](https://oozie.apache.org/) and [Azkaban](https://azkaban.github.io/).\n\nAirflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow''s [XCom feature](https://airflow.apache.org/docs/apache-airflow/stable/concepts/xcoms.html)). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.\n\nAirflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.\n\n## Principles\n\n- **Dynamic**: Pipelines are defined in code, enabling dynamic dag generation and parameterization.\n- **Extensible**: The Airflow framework includes a wide range of built-in operators and can be extended to fit your needs.\n- **Flexible**: Airflow leverages the [**Jinja**](https://jinja.palletsprojects.com) templating engine, allowing rich customizations.\n\n<!-- START Requirements, please keep comment here to allow auto update of PyPI readme.md -->\n## Requirements\n\nApache Airflow is tested with:\n\n|            | Main version (dev)           | Stable version (3.1.3) |\n|------------|------------------------------|------------------------|\n| Python     | 3.10, 3.11, 3.12, 3.13       | 3.10, 3.11, 3.12, 3.13 |\n| Platform   | AMD64/ARM64(\*)              | AMD64/ARM64(\*)        |\n| Kubernetes | 1.30, 1.31, 1.32, 1.33, 1.34 | 1.30, 1.31, 1.32, 1.33 |\n| PostgreSQL | 14, 15, 16, 17, 18           | 13, 14, 15, 16, 17     |\n| MySQL      | 8.0, 8.4, Innovation         | 8.0, 8.4, Innovation   |\n| SQLite     | 3.15.0+                      | 3.15.0+                |\n\n\* Experimental\n\n**Note**: MariaDB is not tested/recommended.\n\n**Note**: SQLite is used in Airflow tests. Do not use it in production. We recommend\nusing the latest stable version of SQLite for local development.\n\n**Note**: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly\ntested on fairly modern Linux Distros and recent versions of macOS.\nOn Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers.\nThe work to add Windows support is tracked via [#10388](https://github.com/apache/airflow/issues/10388), but\nit is not a high priority. You should only use Linux-based distros as "Production" execution environment\nas this is the only environment that is supported. The only distro that is used in our CI tests and that\nis used in the [Community managed DockerHub image](https://hub.docker.com/p/apache/airflow) is\n`Debian Bookworm`.\n\n<!-- END Requirements, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Getting started, please keep comment here to allow auto update of PyPI readme.md -->\n## Getting started\n\nVisit the official Airflow website documentation (latest **stable** release) for help with\n[installing Airflow](https://airflow.apache.org/docs/apache-airflow/stable/installation/),\n[getting started](https://airflow.apache.org/docs/apache-airflow/stable/start.html), or walking\nthrough a more complete [tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/).\n\n> Note: If you''re looking for documentation for the main branch (latest development branch): you can find it on [s.apache.org/airflow-docs](https://s.apache.org/airflow-docs/).\n\nFor more information on Airflow Improvement Proposals (AIPs), visit\nthe [Airflow Wiki](https://cwiki.apache.org/confluence/display/AIRFLOW/Airflow+Improvement+Proposals).\n\nDocumentation for dependent projects like provider distributions, Docker image, Helm Chart, you''ll find it in [the documentation index](https://airflow.apache.org/docs/).\n\n<!-- END Getting started, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Installing from PyPI\n\nWe publish Apache Airflow as `apache-airflow` package in PyPI. Installing it however might be sometimes tricky\nbecause Airflow is a bit of both a library and application. Libraries usually keep their dependencies open, and\napplications usually pin them, but we should do neither and both simultaneously. We decided to keep\nour dependencies as open as possible (in `pyproject.toml`) so users can install different versions of libraries\nif needed. This means that `pip install apache-airflow` will not work from time to time or will\nproduce unusable Airflow installation.\n\nTo have repeatable installation, however, we keep a set of "known-to-be-working" constraint\nfiles in the orphan `constraints-main` and `constraints-2-0` branches. We keep those "known-to-be-working"\nconstraints files separately per major/minor Python version.\nYou can use them as constraint files when installing Airflow from PyPI. Note that you have to specify\ncorrect Airflow tag/version/branch and Python versions in the URL.\n\n1. Installing just Airflow:\n\n> Note: Only `pip` installation is currently officially supported.\n\nWhile it is possible to install Airflow with tools like [Poetry](https://python-poetry.org) or\n[pip-tools](https://pypi.org/project/pip-tools), they do not share the same workflow as\n`pip` - especially when it comes to constraint vs. requirements management.\nInstalling via `Poetry` or `pip-tools` is not currently supported.\n\nIf you wish to install Airflow using those tools, you should use the constraint files and convert\nthem to the appropriate format and workflow that your tool requires.\n\n\n```bash\npip install ''apache-airflow==3.1.3'' \\n --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.1.3/constraints-3.10.txt"\n```\n\n2. Installing with extras (i.e., postgres, google)\n\n```bash\npip install ''apache-airflow[postgres,google]==3.1.3'' \\n --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.1.3/constraints-3.10.txt"\n```\n\nFor information on installing provider distributions, check\n[providers](http://airflow.apache.org/docs/apache-airflow-providers/index.html).\n\n<!-- END Installing from PyPI, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Installation\n\nFor comprehensive instructions on setting up your local development environment and installing Apache Airflow, please refer to the [INSTALLING.md](INSTALLING.md) file.\n\n<!-- START Official source code, please keep comment here to allow auto update of PyPI readme.md -->\n## Official source code\n\nApache Airflow is an [Apache Software Foundation](https://www.apache.org) (ASF) project,\nand our official source code releases:\n\n- Follow the [ASF Release Policy](https://www.apache.org/legal/release-policy.html)\n- Can be downloaded from [the ASF Distribution Directory](https://downloads.apache.org/airflow)\n- Are cryptographically signed by the release manager\n- Are officially voted on by the PMC members during the\n  [Release Approval Process](https://www.apache.org/legal/release-policy.html#release-approval)\n\nFollowing the ASF rules, the source packages released must be sufficient for a user to build and test the\nrelease provided they have access to the appropriate platform and tools.\n\n<!-- END Official source code, please keep comment here to allow auto update of PyPI readme.md -->\n## Convenience packages\n\nThere are other ways of installing and using Airflow. Those are "convenience" methods - they are\nnot "official releases" as stated by the `ASF Release Policy`, but they can be used by the users\nwho do not want to build the software themselves.\n\nThose are - in the order of most common ways people install Airflow:\n\n- [PyPI releases](https://pypi.org/project/apache-airflow/) to install Airflow using standard `pip` tool\n- [Docker Images](https://hub.docker.com/r/apache/airflow) to install airflow via\n  `docker` tool, use them in Kubernetes, Helm Charts, `docker-compose`, `docker swarm`, etc. You can\n  read more about using, customizing, and extending the images in the\n  [Latest docs](https://airflow.apache.org/docs/docker-stack/index.html), and\n  learn details on the internals in the [images](https://airflow.apache.org/docs/docker-stack/index.html) document.\n- [Tags in GitHub](https://github.com/apache/airflow/tags) to retrieve the git project sources that\n  were used to generate official source packages via git\n\nAll those artifacts are not official releases, but they are prepared using officially released sources.\nSome of those artifacts are "development" or "pre-release" ones, and they are clearly marked as such\nfollowing the ASF Policy.\n\n## User Interface\n\n- **DAGs**: Overview of all DAGs in your environment.\n\n  ![DAGs](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/dags.png)\n\n- **Assets**: Overview of Assets with dependencies.\n\n  ![Asset Dependencies](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/assets_graph.png)\n\n- **Grid**: Grid representation of a DAG that spans across time.\n\n  ![Grid](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/grid.png)\n\n- **Graph**: Visualization of a DAG''s dependencies and their current status for a specific run.\n\n  ![Graph](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/graph.png)\n\n- **Home**: Summary statistics of your Airflow environment.\n\n  ![Home](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/home.png)\n\n- **Backfill**: Backfilling a DAG for a specific date range.\n\n  ![Backfill](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/backfill.png)\n\n- **Code**: Quick way to view source code of a DAG.\n\n  ![Code](https://raw.githubusercontent.com/apache/airflow/main/airflow-core/docs/img/ui-dark/code.png)\n\n## Semantic versioning\n\nAs of Airflow 2.0.0, we support a strict [SemVer](https://semver.org/) approach for all packages released.\n\nThere are few specific rules that we agreed to that define details of versioning of the different\npackages:\n\n* **Airflow**: SemVer rules apply to core airflow only (excludes any changes to providers).\n  Changing limits for versions of Airflow dependencies is not a breaking change on its own.\n* **Airflow Providers**: SemVer rules apply to changes in the particular provider''s code only.\n  SemVer MAJOR and MINOR versions for the packages are independent of the Airflow version.\n  For example, `google 4.1.0` and `amazon 3.1.1` providers can happily be installed\n  with `Airflow 2.1.2`. If there are limits of cross-dependencies between providers and Airflow packages,\n  they are present in providers as `install_requires` limitations. We aim to keep backwards\n  compatibility of providers with all previously released Airflow 2 versions but\n  there will sometimes be breaking changes that might make some, or all\n  providers, have minimum Airflow version specified.\n* **Airflow Helm Chart**: SemVer rules apply to changes in the chart only. SemVer MAJOR and MINOR\n  versions for the chart are independent of the Airflow version. We aim to keep backwards\n  compatibility of the Helm Chart with all released Airflow 2 versions, but some new features might\n  only work starting from specific Airflow releases. We might however limit the Helm\n  Chart to depend on minimal Airflow version.\n* **Airflow API clients**: Their versioning is independent from Airflow versions. They follow their own\n  SemVer rules for breaking changes and new features - which for example allows to change the way we generate\n  the clients.\n\n## Version Life Cycle\n\nApache Airflow version life cycle:\n\n<!-- This table is automatically updated by prek scripts/ci/prek/supported_versions.py -->\n<!-- Beginning of auto-generated table -->\n\n| Version   | Current Patch/Minor   | State     | First Release   | Limited Maintenance   | EOL/Terminated   |\n|-----------|-----------------------|-----------|-----------------|-----------------------|------------------|\n| 3         | 3.1.3                 | Supported | Apr 22, 2025    | TBD                   | TBD              |\n| 2         | 2.11.0                | Supported | Dec 17, 2020    | Oct 22, 2025          | Apr 22, 2026     |\n| 1.10      | 1.10.15               | EOL       | Aug 27, 2018    | Dec 17, 2020          | June 17, 2021    |\n| 1.9       | 1.9.0                 | EOL       | Jan 03, 2018    | Aug 27, 2018          | Aug 27, 2018     |\n| 1.8       | 1.8.2                 | EOL       | Mar 19, 2017    | Jan 03, 2018          | Jan 03, 2018     |\n| 1.7       | 1.7.1.2               | EOL       | Mar 28, 2016    | Mar 19, 2017          | Mar 19, 2017     |\n\n<!-- End of auto-generated table -->\n\nLimited support versions will be supported with security and critical bug fix only.\nEOL versions will not get any fixes nor support.\nWe always recommend that all users run the latest available minor release for whatever major version is in use.\nWe **highly** recommend upgrading to the latest Airflow major release at the earliest convenient time and before the EOL date.\n\n## Support for Python and Kubernetes versions\n\nAs of Airflow 2.0, we agreed to certain rules we follow for Python and Kubernetes support.\nThey are based on the official release schedule of Python and Kubernetes, nicely summarized in the\n[Python Developer''s Guide](https://devguide.python.org/#status-of-python-branches) and\n[Kubernetes version skew policy](https://kubernetes.io/docs/setup/release/version-skew-policy/).\n\n1. We drop support for Python and Kubernetes versions when they reach EOL. Except for Kubernetes, a\n   version stays supported by Airflow if two major cloud providers still provide support for it. We drop\n   support for those EOL versions in main right after EOL date, and it is effectively removed when we release\n   the first new MINOR (Or MAJOR if there is no new MINOR version) of Airflow. For example, for Python 3.10 it\n   means that we will drop support in main right after 27.06.2023, and the first MAJOR or MINOR version of\n   Airflow released after will not have it.\n\n2. We support a new version of Python/Kubernetes in main after they are officially released, as soon as we\n   make them work in our CI pipeline (which might not be immediate due to dependencies catching up with\n   new versions of Python mostly) we release new images/support in Airflow based on the working CI setup.\n\n3. This policy is best-effort which means there may be situations where we might terminate support earlier\n   if circumstances require it.\n\n## Base OS support for reference Airflow images\n\nThe Airflow Community provides conveniently packaged container images that are published whenever\nwe publish an Apache Airflow release. Those images contain:\n\n* Base OS with necessary packages to install Airflow (stable Debian OS)\n* Base Python installation in versions supported at the time of release for the MINOR version of\n  Airflow released (so there could be different versions for 2.3 and 2.2 line for example)\n* Libraries required to connect to supported Databases (again the set of databases supported depends\n  on the MINOR version of Airflow)\n* Predefined set of popular providers (for details see the [Dockerfile](https://raw.githubusercontent.com/apache/airflow/main/Dockerfile)).\n* Possibility of building your own, custom image where the user can choose their own set of providers\n  and libraries (see [Building the image](https://airflow.apache.org/docs/docker-stack/build.html))\n* In the future Airflow might also support a "slim" version without providers nor database clients installed\n\nThe version of the base OS image is the stable version of Debian. Airflow supports using all currently active\nstable versions - as soon as all Airflow dependencies support building, and we set up the CI pipeline for\nbuilding and testing the OS version. Approximately 6 months before the end-of-regular support of a\nprevious stable version of the OS, Airflow switches the images released to use the latest supported\nversion of the OS.\n\nFor example switch from ``Debian Bullseye`` to ``Debian Bookworm`` has been implemented\nbefore 2.8.0 release in October 2023 and ``Debian Bookworm`` will be the only option supported as of\nAirflow 2.10.0.\n\nUsers will continue to be able to build their images using stable Debian releases until the end of regular\nsupport and building and verifying of the images happens in our CI but no unit tests were executed using\nthis image in the `main` branch.\n\n## Approach to dependencies of Airflow\n\nAirflow has a lot of dependencies - direct and transitive, also Airflow is both - library and application,\ntherefore our policies to dependencies has to include both - stability of installation of application,\nbut also ability to install newer version of dependencies for those users who develop DAGs. We developed\nthe approach where `constraints` are used to make sure airflow can be installed in a repeatable way, while\nwe do not limit our users to upgrade most of the dependencies. As a result we decided not to upper-bound\nversion of Airflow dependencies by default, unless we have good reasons to believe upper-bounding them is\nneeded because of importance of the dependency as well as risk it involves to upgrade specific dependency.\nWe also upper-bound the dependencies that we know cause problems.\n\nThe constraint mechanism of ours takes care about finding and upgrading all the non-upper bound dependencies\nautomatically (providing that all the tests pass). Our `main` build failures will indicate in case there\nare versions of dependencies that break our tests - indicating that we should either upper-bind them or\nthat we should fix our code/tests to account for the upstream changes from those dependencies.\n\nWhenever we upper-bound such a dependency, we should always comment why we are doing it - i.e. we should have\na good reason why dependency is upper-bound. And we should also mention what is the condition to remove the\nbinding.\n\n### Approach for dependencies for Airflow Core\n\nThose dependencies are maintained in ``pyproject.toml``.\n\nThere are few dependencies that we decided are important enough to upper-bound them by default, as they are\nknown to follow predictable versioning scheme, and we know that new versions of those are very likely to\nbring breaking changes. We commit to regularly review and attempt to upgrade to the newer versions of\nthe dependencies as they are released, but this is manual process.\n\nThe important dependencies are:\n\n* `SQLAlchemy`: upper-bound to specific MINOR version (SQLAlchemy is known to remove deprecations and\n   introduce breaking changes especially that support for different Databases varies and changes at\n   various speed)\n* `Alembic`: it is important to handle our migrations in predictable and performant way. It is developed\n   together with SQLAlchemy. Our experience with Alembic is that it very stable in MINOR version\n* `Flask`: We are using Flask as the back-bone of our web UI and API. We know major version of Flask\n   are very likely to introduce breaking changes across those so limiting it to MAJOR version makes sense\n* `werkzeug`: the library is known to cause problems in new versions. It is tightly coupled with Flask\n   libraries, and we should update them together\n* `celery`: Celery is a crucial component of Airflow as it used for CeleryExecutor (and similar). Celery\n   [follows SemVer](https://docs.celeryq.dev/en/stable/contributing.html?highlight=semver#versions), so\n   we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library,\n   we should make sure Celery Provider minimum Airflow version is updated.\n* `kubernetes`: Kubernetes is a crucial component of Airflow as it is used for the KubernetesExecutor\n   (and similar). Kubernetes Python library [follows SemVer](https://github.com/kubernetes-client/python#compatibility),\n   so we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library,\n   we should make sure Kubernetes Provider minimum Airflow version is updated.\n\n### Approach for dependencies in Airflow Providers and extras\n\nThe main part of the Airflow is the Airflow Core, but the power of Airflow also comes from a number of\nproviders that extend the core functionality and are released separately, even if we keep them (for now)\nin the same monorepo for convenience. You can read more about the providers in the\n[Providers documentation](https://airflow.apache.org/docs/apache-airflow-providers/index.html). We also\nhave set of policies implemented for maintaining and releasing community-managed providers as well\nas the approach for community vs. 3rd party providers in the [providers](https://github.com/apache/airflow/blob/main/PROVIDERS.rst) document.\n\nThose `extras` and `providers` dependencies are maintained in `provider.yaml` of each provider.\n\nBy default, we should not upper-bound dependencies for providers, however each provider''s maintainer\nmight decide to add additional limits (and justify them with comment).\n\n<!-- START Contributing, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Contributing\n\nWant to help build Apache Airflow? Check out our [contributors'' guide](https://github.com/apache/airflow/blob/main/contributing-docs/README.rst) for a comprehensive overview of how to contribute, including setup instructions, coding standards, and pull request guidelines.\n\nIf you can''t wait to contribute, and want to get started asap, check out the [contribution quickstart](https://github.com/apache/airflow/blob/main/contributing-docs/03a_contributors_quick_start_beginners.rst) here!\n\nOfficial Docker (container) images for Apache Airflow are described in [images](https://github.com/apache/airflow/blob/main/dev/breeze/doc/ci/02_images.md).\n\n<!-- END Contributing, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Voting Policy\n\n* Commits need a +1 vote from a committer who is not the author\n* When we do AIP voting, both PMC member''s and committer''s `+1s` are considered a binding vote.\n\n## Who uses Apache Airflow?\n\nWe know about around 500 organizations that are using Apache Airflow (but there are likely many more)\n[in the wild](https://github.com/apache/airflow/blob/main/INTHEWILD.md).\n\nIf you use Airflow - feel free to make a PR to add your organisation to the list.\n\n<!-- END Who uses Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n<!-- START Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n\n## Who maintains Apache Airflow?\n\nAirflow is the work of the [community](https://github.com/apache/airflow/graphs/contributors),\nbut the [core committers/maintainers](https://people.apache.org/committers-by-project.html#airflow)\nare responsible for reviewing and merging PRs as well as steering conversations around new feature requests.\nIf you would like to become a maintainer, please review the Apache Airflow\n[committer requirements](https://github.com/apache/airflow/blob/main/COMMITTERS.rst#guidelines-to-become-an-airflow-committer).\n\n<!-- END Who maintains Apache Airflow, please keep comment here to allow auto update of PyPI readme.md -->\n\n## What goes into the next release?\n\nOften you will see an issue that is assigned to specific milestone with Airflow version, or a PR that gets merged\nto the main branch and you might wonder which release the merged PR(s) will be released in or which release the fixed\nissues will be in. The answer to this is as usual - it depends on various scenarios. The answer is different for PRs and Issues.\n\nTo add a bit of context, we are following the [Semver](https://semver.org/) versioning scheme as described in\n[Airflow release process](https://airflow.apache.org/docs/apache-airflow/stable/release-process.html). More\ndetails are explained in detail in this README under the [Semantic versioning](#semantic-versioning) chapter, but\nin short, we have `MAJOR.MINOR.PATCH` versions of Airflow.\n\n* `MAJOR` version is incremented in case of breaking changes\n* `MINOR` version is incremented when there are new features added\n* `PATCH` version is incremented when there are only bug-fixes and doc-only changes\n\nGenerally we release `MINOR` versions of Airflow from a branch that is named after the MINOR version. For example\n`2.7.*` releases are released from `v2-7-stable` branch, `2.8.*` releases are released from `v2-8-stable`\nbranch, etc.\n\n1. Most of the time in our release cycle, when the branch for next `MINOR` branch is not yet created, all\nPRs merged to `main` (unless they get reverted), will find their way to the next `MINOR` release. For example\nif the last release is `2.7.3` and `v2-8-stable` branch is not created yet, the next `MINOR` release\nis `2.8.0` and all PRs merged to main will be released in `2.8.0`. However, some PRs (bug-fixes and\ndoc-only changes) when merged, can be cherry-picked to current `MINOR` branch and released in the\nnext `PATCHLEVEL` release. For example, if `2.8.1` is already released and we are working on `2.9.0dev`,  then\nmarking a PR with `2.8.2` milestone means that it will be cherry-picked to `v2-8-test` branch and\nreleased in `2.8.2rc1`, and eventually in `2.8.2`.\n\n2. When we prepare for the next `MINOR` release, we cut new `v2-*-test` and `v2-*-stable` branch\nand prepare `alpha`, `beta` releases for the next `MINOR` version, the PRs merged to main will still be\nreleased in the next `MINOR` release until `rc` version is cut. This is happening because the `v2-*-test`\nand `v2-*-stable` branches are rebased on top of main when next `beta` and `rc` releases are prepared.\nFor example, when we cut `2.10.0beta1` version, anything merged to main before `2.10.0rc1` is released,\nwill find its way to 2.10.0rc1.\n\n3. Then, once we prepare the first RC candidate for the MINOR release, we stop moving the `v2-*-test` and\n`v2-*-stable` branches and the PRs merged to main will be released in the next `MINOR` release.\nHowever, some PRs (bug-fixes and doc-only changes) when merged, can be cherry-picked to current `MINOR`\nbranch and released in the next `PATCHLEVEL` release - for example when the last released version from `v2-10-stable`\nbranch is `2.10.0rc1`, some of the PRs from main can be marked as `2.10.0` milestone by committers,\nthe release manager will try to cherry-pick them into the release branch.\nIf successful, they will be released in `2.10.0rc2` and subsequently in `2.10.0`. This also applies to\nsubsequent `PATCHLEVEL` versions. When for example `2.10.1` is already released, marking a PR with\n`2.10.2` milestone will mean that it will be cherry-picked to `v2-10-stable` branch and released in `2.10.2rc1`\nand eventually in `2.10.2`.\n\nThe final decision about cherry-picking is made by the release manager.\n\nMarking issues with a milestone is a bit different. Maintainers do not mark issues with a milestone usually,\nnormally they are only marked in PRs. If PR linked to the issue (and "fixing it") gets merged and released\nin a specific version following the process described above, the issue will be automatically closed, no\nmilestone will be set for the issue, you need to check the PR that fixed the issue to see which version\nit was released in.\n\nHowever, sometimes maintainers mark issues with specific milestone, which means that the\nissue is important to become a candidate to take a look when the release is being prepared. Since this is an\nOpen-Source project, where basically all contributors volunteer their time, there is no guarantee that specific\nissue will be fixed in specific version. We do not want to hold the release because some issue is not fixed,\nso in such case release manager will reassign such unfixed issues to the next milestone in case they are not\nfixed in time for the current release. Therefore, the milestone for issue is more of an intent that it should be\nlooked at, than promise it will be fixed in the version.\n\nMore context and **FAQ** about the patchlevel release can be found in the\n[What goes into the next release](dev/WHAT_GOES_INTO_THE_NEXT_RELEASE.md) document in the `dev` folder of the\nrepository.\n\n## Can I use the Apache Airflow logo in my presentation?\n\nYes! Be sure to abide by the Apache Foundation [trademark policies](https://www.apache.org/foundation/marks/#books) and the Apache Airflow [Brandbook](https://cwiki.apache.org/confluence/display/AIRFLOW/Brandbook). The most up-to-date logos are found in [this repo](https://github.com/apache/airflow/tree/main/airflow-core/docs/img/logos/) and on the Apache Software Foundation [website](https://www.apache.org/logos/about.html).\n\n## Links\n\n- [Documentation](https://airflow.apache.org/docs/apache-airflow/stable/)\n- [Chat](https://s.apache.org/airflow-slack)\n- [Community Information](https://airflow.apache.org/community/)\n\n## Sponsors\n\nThe CI infrastructure for Apache Airflow has been sponsored by:\n\n<!-- Ordered by most recently "funded" -->\n\n<a href="https://astronomer.io"><img src="https://assets2.astronomer.io/logos/logoForLIGHTbackground.png" alt="astronomer.io" width="250px"></a>\n<a href="https://aws.amazon.com/opensource/"><img src="https://github.com/apache/airflow/blob/main/providers/amazon/docs/integration-logos/AWS-Cloud-alt_light-bg@4x.png?raw=true" alt="AWS OpenSource" width="130px"></a>\n', '{"language":"Python","stars":43457,"forks":16066,"watchers":43457,"open_issues":1694,"topics":["airflow","apache","apache-airflow","automation","dag","data-engineering","data-integration","data-orchestrator","data-pipelines","data-science","elt","etl","machine-learning","mlops","orchestration","python","scheduler","workflow","workflow-engine","workflow-orchestration"],"default_branch":"main","size_kb":506778,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:j178:prek","source_url":"https://github.com/j178/prek"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:spotify:luigi","source_url":"https://github.com/spotify/luigi"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:kubernetes-client:python","source_url":"https://github.com/kubernetes-client/python#compatibility"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"},{"type":"has_code","target_id":"github:apache:airflow","source_url":"https://github.com/apache/airflow"}]', NULL, 'Apache-2.0', 'approved', 80, '518a4234b96ceef2a8a8ae4fe2838852', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-apache-airflow from https://github.com/apache.png
Image converted to WebP: data/images/github-apache-airflow.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-streamlit-streamlit', 'github--streamlit--streamlit', 'streamlit', 'streamlit', '<br> <img src="https://user-images.githubusercontent.com/7164864/217935870-c0bc60a3-6fc0-4047-b011-7b4c59488c91.png" alt="Streamlit logo" style="margin-top:50px"></img> **A faster way to build and share data apps.** Streamlit lets you transform Python scripts into interactive web apps in minutes, instead of weeks. Build dashboards, generate reports, or create chat apps. Once youâ€™ve created an app, you can use our Community Cloud platform to deploy, manage, and share your app. - **Simple and P...', '["data-analysis","data-science","data-visualization","deep-learning","developer-tools","machine-learning","python","streamlit","python"]', 'other', 42570, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/streamlit/streamlit","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<br>\n\n<img src="https://user-images.githubusercontent.com/7164864/217935870-c0bc60a3-6fc0-4047-b011-7b4c59488c91.png" alt="Streamlit logo" style="margin-top:50px"></img>\n\n# Welcome to Streamlit ðŸ‘‹\n\n**A faster way to build and share data apps.**\n\n## What is Streamlit?\n\nStreamlit lets you transform Python scripts into interactive web apps in minutes, instead of weeks. Build dashboards, generate reports, or create chat apps. Once youâ€™ve created an app, you can use our [Community Cloud platform](https://streamlit.io/cloud) to deploy, manage, and share your app.\n\n### Why choose Streamlit?\n\n- **Simple and Pythonic:** Write beautiful, easy-to-read code.\n- **Fast, interactive prototyping:** Let others interact with your data and provide feedback quickly.\n- **Live editing:** See your app update instantly as you edit your script.\n- **Open-source and free:** Join a vibrant community and contribute to Streamlit''s future.\n\n## Installation\n\nOpen a terminal and run:\n\n```bash\n$ pip install streamlit\n$ streamlit hello\n```\n\nIf this opens our sweet _Streamlit Hello_ app in your browser, you''re all set! If not, head over to [our docs](https://docs.streamlit.io/get-started) for specific installs.\n\nThe app features a bunch of examples of what you can do with Streamlit. Jump to the [quickstart](#quickstart) section to understand how that all works.\n\n<img src="https://user-images.githubusercontent.com/7164864/217936487-1017784e-68ec-4e0d-a7f6-6b97525ddf88.gif" alt="Streamlit Hello" width=500 href="none"></img>\n\n## Quickstart\n\n### A little example\n\nCreate a new file named `streamlit_app.py` in your project directory with the following code:\n```python\nimport streamlit as st\nx = st.slider("Select a value")\nst.write(x, "squared is", x * x)\n```\n\nNow run it to open the app!\n```\n$ streamlit run streamlit_app.py\n```\n\n<img src="https://user-images.githubusercontent.com/7164864/215172915-cf087c56-e7ae-449a-83a4-b5fa0328d954.gif" width=300 alt="Little example"></img>\n\n### Give me more!\n\nStreamlit comes in with [a ton of additional powerful elements](https://docs.streamlit.io/develop/api-reference) to spice up your data apps and delight your viewers. Some examples:\n\n<table border="0">\n  <tr>\n    <td>\n      <a target="_blank" href="https://docs.streamlit.io/develop/api-reference/widgets">\n        <img src="https://user-images.githubusercontent.com/7164864/217936099-12c16f8c-7fe4-44b1-889a-1ac9ee6a1b44.png" style="max-height:150px; width:auto; display:block;">\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.streamlit.io/develop/api-reference/data/st.dataframe">\n        <img src="https://user-images.githubusercontent.com/7164864/215110064-5eb4e294-8f30-4933-9563-0275230e52b5.gif" style="max-height:150px; width:auto; display:block;">\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.streamlit.io/develop/api-reference/charts">\n        <img src="https://user-images.githubusercontent.com/7164864/215174472-bca8a0d7-cf4b-4268-9c3b-8c03dad50bcd.gif" style="max-height:150px; width:auto; display:block;">\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.streamlit.io/develop/api-reference/layout">\n        <img src="https://user-images.githubusercontent.com/7164864/217936149-a35c35be-0d96-4c63-8c6a-1c4b52aa8f60.png" style="max-height:150px; width:auto; display:block;">\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://docs.streamlit.io/develop/concepts/multipage-apps">\n        <img src="https://user-images.githubusercontent.com/7164864/215173883-eae0de69-7c1d-4d78-97d0-3bc1ab865e5b.gif" style="max-height:150px; width:auto; display:block;">\n      </a>\n    </td>\n    <td>\n      <a target="_blank" href="https://streamlit.io/gallery">\n        <img src="https://user-images.githubusercontent.com/7164864/215109229-6ae9111f-e5c1-4f0b-b3a2-87a79268ccc9.gif" style="max-height:150px; width:auto; display:block;">\n      </a>\n    </td>\n  </tr>\n  <tr>\n    <td>Input widgets</td>\n    <td>Dataframes</td>\n    <td>Charts</td>\n    <td>Layout</td>\n    <td>Multi-page apps</td>\n    <td>Fun</td>\n  </tr>\n</table>\n\n\nOur vibrant creators community also extends Streamlit capabilities using Â ðŸ§© [Streamlit Components](https://streamlit.io/components).\n\n## Get inspired\n\nThere''s so much you can build with Streamlit:\n- ðŸ¤–Â Â [LLMs & chatbot apps](https://streamlit.io/gallery?category=llms)\n- ðŸ§¬Â Â [Science & technology apps](https://streamlit.io/gallery?category=science-technology)\n- ðŸ’¬Â Â [NLP & language apps](https://streamlit.io/gallery?category=nlp-language)\n- ðŸ¦Â Â [Finance & business apps](https://streamlit.io/gallery?category=finance-business)\n- ðŸ—ºÂ Â [Geography & society apps](https://streamlit.io/gallery?category=geography-society)\n- and more!\n\n**Check out [our gallery!](https://streamlit.io/gallery)** ðŸŽˆ\n\n## Community Cloud\n\nDeploy, manage and share your apps for free using our [Community Cloud](https://streamlit.io/cloud)! Sign-up [here](https://share.streamlit.io/signup). <br><br>\n<img src="https://user-images.githubusercontent.com/7164864/214965336-64500db3-0d79-4a20-8052-2dda883902d2.gif" width="400"></img>\n\n## Resources\n\n- Explore our [docs](https://docs.streamlit.io) to learn how Streamlit works.\n- Ask questions and get help in our [community forum](https://discuss.streamlit.io).\n- Read our [blog](https://blog.streamlit.io) for tips from developers and creators.\n- Extend Streamlit''s capabilities by installing or creating your own [Streamlit Components](https://streamlit.io/components).\n- Help others find and play with your app by using the Streamlit GitHub badge in your repository:\n```markdown\n[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](URL_TO_YOUR_APP)\n```\n[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://share.streamlit.io/streamlit/roadmap)\n\n## Contribute\n\nðŸŽ‰ Thanks for your interest in helping improve Streamlit! ðŸŽ‰\n\nBefore contributing, please read our guidelines here: https://github.com/streamlit/streamlit/wiki/Contributing\n\n## License\n\nStreamlit is completely free and open-source and licensed under the [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) license.\n', '{"language":"Python","stars":42570,"forks":3943,"watchers":42570,"open_issues":1307,"topics":["data-analysis","data-science","data-visualization","deep-learning","developer-tools","machine-learning","python","streamlit"],"default_branch":"develop","size_kb":790460,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:streamlit:streamlit","source_url":"https://github.com/streamlit/streamlit"}]', NULL, 'Apache-2.0', 'approved', 65, 'a0699b965473363ec49d99eceeed15bc', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-streamlit-streamlit from https://github.com/streamlit.png
Image converted to WebP: data/images/github-streamlit-streamlit.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-deepspeedai-DeepSpeed', 'github--deepspeedai--deepspeed', 'DeepSpeed', 'deepspeedai', '<div align="center"> <img src="docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only" width="400px"> <img src="docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-mode-only" width="400px"> </div> * [2025/10] We hosted the Ray x DeepSpeed Meetup at Anyscale. We shared our most recent work on SuperOffload, ZenFlow, Muon Optimizer Support, Arctic Long Sequence Training and DeepCompile. Please find the meetup slides here. * [2025/10] SuperOffload: Unleashing the Power of Large-Scale LLM...', '["billion-parameters","compression","data-parallelism","deep-learning","gpu","inference","machine-learning","mixture-of-experts","model-parallelism","pipeline-parallelism","pytorch","trillion-parameters","zero","python"]', 'other', 40937, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/deepspeedai/DeepSpeed","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '[![License Apache 2.0](https://badgen.net/badge/license/apache2.0/blue)](https://github.com/deepspeedai/DeepSpeed/blob/master/LICENSE)\n[![PyPI version](https://badge.fury.io/py/deepspeed.svg)](https://pypi.org/project/deepspeed/)\n[![Downloads](https://static.pepy.tech/badge/deepspeed)](https://pepy.tech/project/deepspeed)\n[![Build](https://badgen.net/badge/build/check-status/blue)](#build-pipeline-status)\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/9530/badge)](https://www.bestpractices.dev/projects/9530)\n[![Twitter](https://img.shields.io/twitter/follow/DeepSpeedAI)](https://twitter.com/intent/follow?screen_name=DeepSpeedAI)\n[![Japanese Twitter](https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9ETwitter-%40DeepSpeedAI_JP-blue)](https://twitter.com/DeepSpeedAI_JP)\n[![Chinese Zhihu](https://img.shields.io/badge/%E7%9F%A5%E4%B9%8E-%E5%BE%AE%E8%BD%AFDeepSpeed-blue)](https://www.zhihu.com/people/deepspeed)\n[![Slack](https://img.shields.io/badge/Slack-4A154B?style=for-the-badge&logo=slack&logoColor=white)](https://join.slack.com/t/deepspeedworkspace/shared_invite/zt-3a8pjd8dd-PCj2hMvR4Y2syPwVnjEoww)\n\n\n<div align="center">\n <img src="docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only" width="400px">\n <img src="docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-mode-only" width="400px">\n</div>\n\n## Latest News\n\n* [2025/10] We hosted the [Ray x DeepSpeed Meetup](https://luma.com/3wctqteh) at Anyscale. We shared our most recent work on SuperOffload, ZenFlow, Muon Optimizer Support, Arctic Long Sequence Training and DeepCompile. Please find the meetup slides [here](https://docs.google.com/presentation/d/1eM3mY6oW9GYkRy1Xz0iOnbbEr5T1t0JJXOM5BKtR-Ks/edit?slide=id.g38615d6b4c2_0_87#slide=id.g38615d6b4c2_0_87).\n\n* [2025/10] [SuperOffload: Unleashing the Power of Large-Scale LLM Training on Superchips](https://pytorch.org/blog/superoffload-unleashing-the-power-of-large-scale-llm-training-on-superchips/)\n\n* [2025/10] [Study of ZenFlow and ZeRO offload performance with DeepSpeed CPU core binding](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/zenflow-corebinding/README.md)\n\n* [2025/08] [ZenFlow: Stall-Free Offloading Engine for LLM Training](https://pytorch.org/blog/zenflow-stall-free-offloading-engine-for-llm-training/)\n\n* [2025/06] [Arctic Long Sequence Training (ALST) with DeepSpeed: Scalable And Efficient Training For Multi-Million Token Sequences](https://www.snowflake.com/en/engineering-blog/arctic-long-sequence-training-multi-million-token-ai/)\n\n* [2025/06] [DeepNVMe: Affordable I/O scaling for Deep Learning Applications](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepnvme/06-2025/README.md)\n\n\n<!-- NOTE: we must use html for news items otherwise links will be broken in the ''more news'' section -->\n<details>\n<!-- NOTE: Maintain only three items in ''more news'' section -->\n <summary>More news</summary>\n <ul>\n\n   <li>[2025/04] <a href="https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepcompile/README.md">DeepCompile: Unlocking Compiler Optimization for Distributed Training</a></li>\n\n   <li>[2025/03] <a href="https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/huggingface-tp/README.md">DeepSpeed AutoTP: Automatic Tensor Parallel Training of Hugging Face models</a></li>\n\n<li>[2024/12] <a href="https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/ulysses-offload/README.md">Ulysses-Offload: Democratizing Long Context LLM Training</a></li>\n\n </ul>\n</details>\n\n---\n\n# Extreme Speed and Scale for DL Training\n\n***[DeepSpeed](https://www.deepspeed.ai/) enabled the world''s most powerful language models (at the time of this writing) such as [MT-530B](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) and [BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed)***. DeepSpeed offers a confluence of [system innovations](https://www.deepspeed.ai/training/), that has made large scale DL training effective, and efficient, greatly improved ease of use, and redefined the DL training landscape in terms of scale that is possible. These innovations include ZeRO, ZeRO-Infinity, 3D-Parallelism, Ulysses Sequence Parallelism, DeepSpeed-MoE, etc.\n\n---\n\n# DeepSpeed Adoption\n\nDeepSpeed was an important part of Microsoftâ€™s\n[AI at Scale](https://www.microsoft.com/en-us/research/project/ai-at-scale/)\ninitiative to enable next-generation AI capabilities at scale, where you can find more\ninformation [here](https://innovation.microsoft.com/en-us/exploring-ai-at-scale).\n\nDeepSpeed has been used to train many different large-scale models, below is a list of several examples that we are aware of (if you''d like to include your model please submit a PR):\n\n  * [Megatron-Turing NLG (530B)](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)\n  * [Jurassic-1 (178B)](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf)\n  * [BLOOM (176B)](https://huggingface.co/blog/bloom-megatron-deepspeed)\n  * [GLM (130B)](https://github.com/THUDM/GLM-130B)\n  * [xTrimoPGLM (100B)](https://www.biorxiv.org/content/10.1101/2023.07.05.547496v2)\n  * [YaLM (100B)](https://github.com/yandex/YaLM-100B)\n  * [GPT-NeoX (20B)](https://github.com/EleutherAI/gpt-neox)\n  * [AlexaTM (20B)](https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning)\n  * [Turing NLG (17B)](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)\n  * [METRO-LM (5.4B)](https://arxiv.org/pdf/2204.06644.pdf)\n\nDeepSpeed has been integrated with several different popular open-source DL frameworks such as:\n\n|                                                                                                | Documentation                                |\n| ---------------------------------------------------------------------------------------------- | -------------------------------------------- |\n<img src="docs/assets/images/transformers-light.png#gh-light-mode-only" width="250px"><img src="docs/assets/images/transformers-dark.png#gh-dark-mode-only" width="250px"> | [Transformers with DeepSpeed](https://huggingface.co/docs/transformers/deepspeed) |\n| <img src="docs/assets/images/accelerate-light.png#gh-light-mode-only" width="250px"><img src="docs/assets/images/accelerate-dark.png#gh-dark-mode-only" width="250px"> | [Accelerate with DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed) |\n| <img src="docs/assets/images/lightning-light.svg#gh-light-mode-only" width="200px"><img src="docs/assets/images/lightning-dark.svg#gh-dark-mode-only" width="200px"> | [Lightning with DeepSpeed](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html#deepspeed) |\n| <img src="docs/assets/images/mosaicml.svg" width="200px"> | [MosaicML with DeepSpeed](https://docs.mosaicml.com/projects/composer/en/latest/trainer/using_the_trainer.html?highlight=deepspeed#deepspeed-integration) |\n| <img src="docs/assets/images/determined.svg" width="225px"> | [Determined with DeepSpeed](https://docs.determined.ai/latest/training/apis-howto/deepspeed/overview.html) |\n| <img src="https://user-images.githubusercontent.com/58739961/187154444-fce76639-ac8d-429b-9354-c6fac64b7ef8.jpg" width=150> | [MMEngine with DeepSpeed](https://mmengine.readthedocs.io/en/latest/common_usage/large_model_training.html#deepspeed) |\n\n---\n\n# Build Pipeline Status\n\n| Description | Status |\n| ----------- | ------ |\n| NVIDIA | [![nv-torch-latest-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml)  [![nv-inference](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-inference.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-inference.yml) [![nv-nightly](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-nightly.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-nightly.yml) |\n| AMD | [![amd-mi200](https://github.com/deepspeedai/DeepSpeed/actions/workflows/amd-mi200.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/amd-mi200.yml) |\n| CPU | [![torch-latest-cpu](https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-torch-latest.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-torch-latest.yml) |\n| Intel Gaudi | [![hpu-gaudi2](https://github.com/deepspeedai/DeepSpeed/actions/workflows/hpu-gaudi2.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/hpu-gaudi2.yml) |\n| Intel XPU | [![xpu-max1100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/xpu-max1100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/xpu-max1100.yml) |\n| PyTorch Nightly | [![nv-torch-nightly-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml) |\n| Integrations | [![nv-transformers-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-transformers-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-transformers-v100.yml) [![nv-lightning-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-lightning-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-lightning-v100.yml) [![nv-accelerate-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-accelerate-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-accelerate-v100.yml) [![nv-mii](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-mii.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-mii.yml) [![nv-ds-chat](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml) [![nv-sd](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-sd.yml/badge.svg)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-sd.yml) |\n| Misc | [![Formatting](https://github.com/deepspeedai/DeepSpeed/actions/workflows/formatting.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/formatting.yml) [![pages-build-deployment](https://github.com/deepspeedai/DeepSpeed/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/pages/pages-build-deployment) [![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest)[![python](https://github.com/deepspeedai/DeepSpeed/actions/workflows/python.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/python.yml) |\n| Huawei Ascend NPU | [![Huawei Ascend NPU](https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml/badge.svg?branch=main)](https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml) |\n\n# Installation\n\nThe quickest way to get started with DeepSpeed is via pip, this will install\nthe latest release of DeepSpeed which is not tied to specific PyTorch or CUDA\nversions. DeepSpeed includes several C++/CUDA extensions that we commonly refer\nto as our ''ops''.  By default, all of these extensions/ops will be built\njust-in-time (JIT) using [torch''s JIT C++ extension loader that relies on\nninja](https://pytorch.org/docs/stable/cpp_extension.html) to build and\ndynamically link them at runtime.\n\n## Requirements\n* [PyTorch](https://pytorch.org/) must be installed _before_ installing DeepSpeed.\n* For full feature support we recommend a version of PyTorch that is >= 1.9 and ideally the latest PyTorch stable release.\n* A CUDA or ROCm compiler such as [nvcc](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#introduction) or [hipcc](https://github.com/ROCm-Developer-Tools/HIPCC) used to compile C++/CUDA/HIP extensions.\n* Specific GPUs we develop and test against are listed below, this doesn''t mean your GPU will not work if it doesn''t fall into this category it''s just DeepSpeed is most well tested on the following:\n  * NVIDIA: Pascal, Volta, Ampere, and Hopper architectures\n  * AMD: MI100 and MI200\n\n## Contributed HW support\n* DeepSpeed now support various HW accelerators.\n\n| Contributor | Hardware                            | Accelerator Name | Contributor validated | Upstream validated |\n|-------------|-------------------------------------|------------------| --------------------- |--------------------|\n| Huawei      | Huawei Ascend NPU                   | npu              | Yes | No                 |\n| Intel       | Intel(R) Gaudi(R) 2 AI accelerator  | hpu              | Yes | Yes                |\n| Intel       | Intel(R) Xeon(R) Processors         | cpu              | Yes | Yes                |\n| Intel       | Intel(R) Data Center GPU Max series | xpu              | Yes | Yes                |\n| Tecorigin   | Scalable Data Analytics Accelerator | sdaa             | Yes | No                 |\n\n## PyPI\nWe regularly push releases to [PyPI](https://pypi.org/project/deepspeed/) and encourage users to install from there in most cases.\n\n```bash\npip install deepspeed\n```\n\nAfter installation, you can validate your install and see which extensions/ops\nyour machine is compatible with via the DeepSpeed environment report.\n\n```bash\nds_report\n```\n\nIf you would like to pre-install any of the DeepSpeed extensions/ops (instead\nof JIT compiling) or install pre-compiled ops via PyPI please see our [advanced\ninstallation instructions](https://www.deepspeed.ai/tutorials/advanced-install/).\n\n## Windows\nMany DeepSpeed features are supported on Windows for both training and inference. You can read more about this in the original blog post [here](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/README.md). Among features that are currently not supported are async io (AIO) and GDS (which does not support Windows).\n1. Install PyTorch, such as pytorch 2.3+cu121.\n2. Install Visual C++ build tools, such as VS2022 C++ x64/x86 build tools.\n3. Launch Cmd console with Administrator permissions for creating required symlink folders and ensure MSVC tools are added to your PATH or launch the Developer Command Prompt for Visual Studio 2022 with administrator permissions.\n4. Run `build_win.bat` to build wheel in `dist` folder.\n\n\n# Further Reading\n\nAll DeepSpeed documentation, tutorials, and blogs can be found on our website: [deepspeed.ai](https://www.deepspeed.ai/)\n\n\n|                                                                                                | Description                                  |\n| ---------------------------------------------------------------------------------------------- | -------------------------------------------- |\n| [Getting Started](https://www.deepspeed.ai/getting-started/)                                   |  First steps with DeepSpeed                  |\n| [DeepSpeed JSON Configuration](https://www.deepspeed.ai/docs/config-json/)                     |  Configuring DeepSpeed                       |\n| [API Documentation](https://deepspeed.readthedocs.io/en/latest/)                               |  Generated DeepSpeed API documentation       |\n| [Tutorials](https://www.deepspeed.ai/tutorials/)                                               |  Tutorials                                   |\n| [Blogs](https://www.deepspeed.ai/posts/)                                                       |  Blogs                                   |\n\n\n# CI funding\n\nThis being an open source project we rely on others to provide us resources for CI hardware. At this moment Modal is kindly supporting our GPU CI runs by funding the hardware for us. Modal is an AI infrastructure platform for inference, fine-tuning, batch jobs and more. Get started with $30/mo in free credits today at https://modal.com. We have been getting an amazing support from Modal''s team and will surely recommend them to your business.\n\n# Contributing\nDeepSpeed welcomes your contributions! Please see our\n[contributing](CONTRIBUTING.md) guide for more details on formatting, testing,\netc.<br/>\nThanks so much to all of our amazing contributors!\n\n<a href="https://github.com/deepspeedai/DeepSpeed/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=microsoft/DeepSpeed&r="  width="800px"/>\n</a>\n\n## Contributor License Agreement\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to, and\nactually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply\nfollow the instructions provided by the bot. You will only need to do this once across\nall repos using our CLA.\n\n## Code of Conduct\nThis project has adopted the [Microsoft Open Source Code of\nConduct](https://opensource.microsoft.com/codeofconduct/). For more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact\n[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Publications\n1. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He. (2019) ZeRO: memory optimizations toward training trillion parameter models. [arXiv:1910.02054](https://arxiv.org/abs/1910.02054) and [In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC ''20)](https://dl.acm.org/doi/10.5555/3433701.3433727).\n2. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. [In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD ''20, Tutorial)](https://dl.acm.org/doi/10.1145/3394486.3406703).\n3. Minjia Zhang, Yuxiong He. (2020) Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. [arXiv:2010.13369](https://arxiv.org/abs/2010.13369) and [NeurIPS 2020](https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html).\n4. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. [arXiv:2101.06840](https://arxiv.org/abs/2101.06840) and [USENIX ATC 2021](https://www.usenix.org/conference/atc21/presentation/ren-jie). [[paper]](https://arxiv.org/abs/2101.06840) [[slides]](https://www.usenix.org/system/files/atc21_slides_ren-jie.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)\n5. Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam''s Convergence Speed. [arXiv:2102.02888](https://arxiv.org/abs/2102.02888) and [ICML 2021](http://proceedings.mlr.press/v139/tang21a.html).\n6. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. (2021) ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. [arXiv:2104.07857](https://arxiv.org/abs/2104.07857) and [SC 2021](https://dl.acm.org/doi/abs/10.1145/3458817.3476205). [[paper]](https://arxiv.org/abs/2104.07857) [[slides]](docs/assets/files/SC21-ZeRO-Infinity.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/)\n7. Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB''s Convergence Speed. [arXiv:2104.06069](https://arxiv.org/abs/2104.06069) and [HiPC 2022](https://hipc.org/advance-program/).\n8. Conglong Li, Minjia Zhang, Yuxiong He. (2021) The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models. [arXiv:2108.06084](https://arxiv.org/abs/2108.06084) and [NeurIPS 2022](https://openreview.net/forum?id=JpZ5du_Kdh).\n9. Yucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, Yuxiong He. (2022) Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam. [arXiv:2202.06009](https://arxiv.org/abs/2202.06009).\n10. Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He. (2022) DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale [arXiv:2201.05596](https://arxiv.org/abs/2201.05596) and [ICML 2022](https://proceedings.mlr.press/v162/rajbhandari22a.html). [[pdf]](https://arxiv.org/abs/2201.05596) [[slides]](docs/assets/files/ICML-5mins.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/)\n11. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro. (2022) Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model [arXiv:2201.11990](https://arxiv.org/abs/2201.11990).\n12. Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, Yuxiong He. (2022) Extreme Compression for Pre-trained Transformers Made Simple and Efficient. [arXiv:2206.01859](https://arxiv.org/abs/2206.01859) and [NeurIPS 2022](https://openreview.net/forum?id=xNeAhc2CNAl).\n13. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He. (2022) ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. [arXiv:2206.01861](https://arxiv.org/abs/2206.01861) and [NeurIPS 2022](https://openreview.net/forum?id=f-fVCElZ-G1) [[slides]](docs/assets/files/zeroquant_series.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-compression-a-composable-library-for-extreme-compression-and-zero-cost-quantization/)\n14. Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, Yuxiong He. (2022) DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. [arXiv:2207.00032](https://arxiv.org/abs/2207.00032) and [SC 2022](https://dl.acm.org/doi/abs/10.5555/3571885.3571946). [[paper]](https://arxiv.org/abs/2207.00032) [[slides]](docs/assets/files/sc22-ds-inference.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/)\n15. Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes, Minjia Zhang, Cheng Li, Yuxiong He. (2022) Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers. [arXiv:2211.11586](https://arxiv.org/abs/2211.11586).\n16. Conglong Li, Zhewei Yao, Xiaoxia Wu, Minjia Zhang, Yuxiong He. (2022) DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. [arXiv:2212.03597](https://arxiv.org/abs/2212.03597) [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/)\n17. Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, Yuxiong He. (2023) Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. [arXiv:2301.12017](https://arxiv.org/abs/2301.12017) and [ICML2023](https://icml.cc/Conferences/2023).\n18. Syed Zawad, Cheng Li, Zhewei Yao, Elton Zheng, Yuxiong He, Feng Yan. (2023) DySR: Adaptive Super-Resolution via Algorithm and System Co-design. [ICLR:2023](https://openreview.net/forum?id=Pgtn4l6eKjv).\n19. Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He. (2023) Scaling Vision-Language Models with Sparse Mixture of Experts. [arXiv:2303.07226](https://arxiv.org/abs/2303.07226) and [Finding at EMNLP2023](https://2023.emnlp.org/).\n20. Quentin Anthony, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He, Aamir Shafi, Mustafa Abduljabbar, Hari Subramoni, Dhabaleswar Panda. (2023) MCR-DL: Mix-and-Match Communication Runtime for Deep Learning [arXiv:2303.08374](https://arxiv.org/abs/2303.08374) and will appear at IPDPS 2023.\n21. Siddharth Singh, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He, Abhinav Bhatele. (2023) A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training [arXiv:2303.06318](https://arxiv.org/abs/2303.06318) and [ICS 2023](https://dl.acm.org/doi/10.1145/3577193.3593704).\n22. Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Xiaoxia Wu, Connor Holmes, Zhewei Yao, Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, Yuxiong He. (2023) ZeRO++: Extremely Efficient Collective Communication for Giant Model Training [arXiv:2306.10209](https://arxiv.org/abs/2306.10209) and [ML for Sys Workshop at NeurIPS2023](http://mlforsystems.org/) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/)\n23. Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong He. (2023) ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation [arXiv:2303.08302](https://arxiv.org/abs/2303.08302) and [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/) [[slides]](docs/assets/files/zeroquant_series.pdf)\n24. Pareesa Ameneh Golnari, Zhewei Yao, Yuxiong He. (2023) Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important? [arXiv:2305.09847](https://arxiv.org/abs/2305.09847)\n25. Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, Yuxiong He. (2023) DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales [arXiv:2308.01320](https://arxiv.org/abs/2308.01320).\n26. Xiaoxia Wu, Zhewei Yao, Yuxiong He. (2023) ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats [arXiv:2307.09782](https://arxiv.org/abs/2307.09782) and [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/) [[slides]](docs/assets/files/zeroquant_series.pdf)\n27. Zhewei Yao, Xiaoxia Wu, Conglong Li, Minjia Zhang, Heyang Qin, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He. (2023) DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention [arXiv:2309.14327](https://arxiv.org/pdf/2309.14327.pdf)\n28. Shuaiwen Leon Song, Bonnie Kruft, Minjia Zhang, Conglong Li, Shiyang Chen, Chengming Zhang, Masahiro Tanaka, Xiaoxia Wu, Jeff Rasley, Ammar Ahmad Awan, Connor Holmes, Martin Cai, Adam Ghanem, Zhongzhu Zhou, Yuxiong He, et al. (2023) DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies [arXiv:2310.04610](https://arxiv.org/abs/2310.04610) [[blog]](https://www.microsoft.com/en-us/research/blog/announcing-the-deepspeed4science-initiative-enabling-large-scale-scientific-discovery-through-sophisticated-ai-system-technologies/)\n29. Zhewei Yao, Reza Yazdani Aminabadi, Stephen Youn, Xiaoxia Wu, Elton Zheng, Yuxiong He. (2023) ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers [arXiv:2310.17723](https://arxiv.org/abs/2310.17723)\n\n30. Xiaoxia Wu, Haojun Xia, Stephen Youn, Zhen Zheng, Shiyang Chen, Arash Bakhtiari, Michael Wyatt, Reza Yazdani Aminabadi, Yuxiong He, Olatunji Ruwase, Leon Song, Zhewei Yao (2023) ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks [arXiv:2312.08583](https://arxiv.org/abs/2312.08583)\n\n31. Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon Song. (2024) FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design  [arXiv:2401.14112](https://arxiv.org/abs/2401.14112)\n32. Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Reza Yazdani Aminadabi, Shuaiwen Leon Song, Samyam Rajbhandari, Yuxiong He. (2024) [System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models](https://dl.acm.org/doi/10.1145/3662158.3662806)\n33. Xinyu Lian, Sam Ade Jacobs, Lev Kurilenko, Masahiro Tanaka, Stas Bekman, Olatunji Ruwase, Minjia Zhang. (2024) Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training [arXiv:2406.18820](https://arxiv.org/abs/2406.18820)\n34. Stas Bekman, Samyam Rajbhandari, Michael Wyatt, Jeff Rasley, Tunji Ruwase, Zhewei Yao, Aurick Qiao, Yuxiong He. (2025) Arctic Long Sequence Training: Scalable And Efficient Training For Multi-Million Token Sequences [arXiv:2506.13996](https://arxiv.org/abs/2506.13996)\n35. Tingfeng Lan, Yusen Wu, Bin Ma, Zhaoyuan Su, Rui Yang, Tekin Bicer, Masahiro Tanaka, Olatunji Ruwase, Dong Li, Yue Cheng. (2025) ZenFlow: Enabling Stall-Free Offloading Training via Asynchronous Updates [arXiv:2505.12242](https://arxiv.org/abs/2505.12242)\n36. Xinyu Lian, Masahiro Tanaka, Olatunji Ruwase, Minjia Zhang. (2026) SuperOffload: Unleashing the Power of Large-Scale LLM Training on Superchips [arxiv](https://arxiv.org/abs/2509.21271), [ASPLOS 2026](https://www.asplos-conference.org/asplos2026)\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)\n4. [Large Model Training and Inference with DeepSpeed // Samyam Rajbhandari // LLMs in Prod Conference](https://www.youtube.com/watch?v=cntxC3g22oU) [[slides]](docs/assets/files/presentation-mlops.pdf)\n5. Community Tutorials\n    * [DeepSpeed: All the tricks to scale to gigantic models (Mark Saroufim)](https://www.youtube.com/watch?v=pDGI668pNg0)\n    * [Turing-NLG, DeepSpeed and the ZeRO optimizer (Yannic Kilcher)](https://www.youtube.com/watch?v=tC01FRB0M7w)\n    * [Ultimate Guide To Scaling ML Models (The AI Epiphany)](https://www.youtube.com/watch?v=hc0u4avAkuM)\n', '{"language":"Python","stars":40937,"forks":4652,"watchers":40937,"open_issues":1247,"topics":["billion-parameters","compression","data-parallelism","deep-learning","gpu","inference","machine-learning","mixture-of-experts","model-parallelism","pipeline-parallelism","pytorch","trillion-parameters","zero"],"default_branch":"master","size_kb":243722,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:THUDM:GLM-130B","source_url":"https://github.com/THUDM/GLM-130B"},{"type":"has_code","target_id":"github:yandex:YaLM-100B","source_url":"https://github.com/yandex/YaLM-100B"},{"type":"has_code","target_id":"github:EleutherAI:gpt-neox","source_url":"https://github.com/EleutherAI/gpt-neox"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:Ascend:Ascend-CI","source_url":"https://github.com/Ascend/Ascend-CI"},{"type":"has_code","target_id":"github:Ascend:Ascend-CI","source_url":"https://github.com/Ascend/Ascend-CI"},{"type":"has_code","target_id":"github:ROCm-Developer-Tools:HIPCC","source_url":"https://github.com/ROCm-Developer-Tools/HIPCC"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"},{"type":"has_code","target_id":"github:deepspeedai:DeepSpeed","source_url":"https://github.com/deepspeedai/DeepSpeed"}]', NULL, 'Apache-2.0', 'approved', 80, '5f4f07e99e5164d833b03b8c97512c2c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-deepspeedai-DeepSpeed from https://github.com/deepspeedai.png
Image converted to WebP: data/images/github-deepspeedai-DeepSpeed.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-gradio-app-gradio', 'github--gradio-app--gradio', 'gradio', 'gradio-app', '<!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE OR TEMPLATES AND THEN RUN SCRIPT. --> <div align="center"> <a href="https://gradio.app"> <img src="readme_files/gradio.svg" alt="gradio" width=350> </a> </div> <div align="center"> <span> <a href="https://www.producthunt.com/posts/gradio-5-0?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-gradio&#0045;5&#0045;0" target="_blank"><img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=501906&th...', '["data-analysis","data-science","data-visualization","deep-learning","deploy","gradio","gradio-interface","interface","machine-learning","models","python","python-notebook","ui","ui-components","python"]', 'other', 40851, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/gradio-app/gradio","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<!-- DO NOT EDIT THIS FILE DIRECTLY. INSTEAD EDIT THE `readme_template.md` OR `guides/01_getting-started/01_quickstart.md` TEMPLATES AND THEN RUN `render_readme.py` SCRIPT. -->\n\n<div align="center">\n<a href="https://gradio.app">\n<img src="readme_files/gradio.svg" alt="gradio" width=350>\n</a>\n</div>\n\n<div align="center">\n<span>\n<a href="https://www.producthunt.com/posts/gradio-5-0?embed=true&utm_source=badge-featured&utm_medium=badge&utm_souce=badge-gradio&#0045;5&#0045;0" target="_blank"><img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=501906&theme=light" alt="Gradio&#0032;5&#0046;0 - the&#0032;easiest&#0032;way&#0032;to&#0032;build&#0032;AI&#0032;web&#0032;apps | Product Hunt" style="width: 150px; height: 54px;" width="150" height="54" /></a>\n<a href="https://trendshift.io/repositories/2145" target="_blank"><img src="https://trendshift.io/api/badge/repositories/2145" alt="gradio-app%2Fgradio | Trendshift" style="width: 150px; height: 55px;" width="150" height="55"/></a>\n</span>\n\n[![gradio-backend](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/test-python.yml)\n[![gradio-ui](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml/badge.svg)](https://github.com/gradio-app/gradio/actions/workflows/tests-js.yml) \n[![PyPI](https://img.shields.io/pypi/v/gradio)](https://pypi.org/project/gradio/)\n[![PyPI downloads](https://img.shields.io/pypi/dm/gradio)](https://pypi.org/project/gradio/)\n![Python version](https://img.shields.io/badge/python-3.10+-important)\n[![Twitter follow](https://img.shields.io/twitter/follow/gradio?style=social&label=follow)](https://twitter.com/gradio)\n\n[Website](https://gradio.app)\n| [Documentation](https://gradio.app/docs/)\n| [Guides](https://gradio.app/guides/)\n| [Getting Started](https://gradio.app/getting_started/)\n| [Examples](demo/)\n\n</div>\n\n<div align="center">\n\nEnglish | [ä¸­æ–‡](readme_files/zh-cn#readme)\n\n</div>\n\n# Gradio: Build Machine Learning Web Apps â€” in Python\n\n\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio''s built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*\n\n<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/gif-version.gif" style="padding-bottom: 10px">\n\nIt just takes a few lines of Python to create your own demo, so let''s get started ðŸ’«\n\n\n### Installation\n\n**Prerequisite**: Gradio requires [Python 3.10 or higher](https://www.python.org/downloads/).\n\n\nWe recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:\n\n```bash\npip install --upgrade gradio\n```\n\n\n> [!TIP]\n > It is best to install Gradio in a virtual environment. Detailed installation instructions for all common operating systems <a href="https://www.gradio.app/main/guides/installing-gradio-in-a-virtual-environment">are provided here</a>. \n\n### Building Your First Demo\n\nYou can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let''s write your first Gradio app:\n\n\n```python\nimport gradio as gr\n\ndef greet(name, intensity):\n    return "Hello, " + name + "!" * int(intensity)\n\ndemo = gr.Interface(\n    fn=greet,\n    inputs=["text", "slider"],\n    outputs=["text"],\n)\n\ndemo.launch()\n```\n\n\n\n> [!TIP]\n > We shorten the imported name from <code>gradio</code> to <code>gr</code>. This is a widely adopted convention for better readability of code. \n\nNow, run your code. If you''ve written the Python code in a file named `app.py`, then you would run `python app.py` from the terminal.\n\nThe demo below will open in a browser on [http://localhost:7860](http://localhost:7860) if running from a file. If you are running within a notebook, the demo will appear embedded within the notebook.\n\n![`hello_world_4` demo](demo/hello_world_4/screenshot.gif)\n\nType your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\n\n> [!TIP]\n > When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. You can also enable <strong>vibe mode</strong> by using the <code>--vibe</code> flag, e.g. <code>gradio --vibe app.py</code>, which provides an in-browser chat that can be used to write or edit your Gradio app using natural language. Learn more in the <a href="https://www.gradio.app/guides/developing-faster-with-reload-mode">Hot Reloading Guide</a>.\n\n\n**Understanding the `Interface` Class**\n\nYou''ll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. \n\nThe `Interface` class has three core arguments:\n\n- `fn`: the function to wrap a user interface (UI) around\n- `inputs`: the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n- `outputs`: the Gradio component(s) to use for the output. The number of components should match the number of return values from your function.\n\nThe `fn` argument is very flexible -- you can pass *any* Python function that you want to wrap with a UI. In the example above, we saw a relatively simple function, but the function could be anything from a music generator to a tax calculator to the prediction function of a pretrained machine learning model.\n\nThe `inputs` and `outputs` arguments take one or more Gradio components. As we''ll see, Gradio includes more than [30 built-in components](https://www.gradio.app/docs/gradio/introduction) (such as the `gr.Textbox()`, `gr.Image()`, and `gr.HTML()` components) that are designed for machine learning applications. \n\n> [!TIP]\n > For the `inputs` and `outputs` arguments, you can pass in the name of these components as a string (`"textbox"`) or an instance of the class (`gr.Textbox()`).\n\nIf your function accepts more than one argument, as is the case above, pass a list of input components to `inputs`, with each input component corresponding to one of the arguments of the function, in order. The same holds true if your function returns more than one value: simply pass in a list of components to `outputs`. This flexibility makes the `Interface` class a very powerful way to create demos.\n\nWe''ll dive deeper into the `gr.Interface` on our series on [building Interfaces](https://www.gradio.app/main/guides/the-interface-class).\n\n### Sharing Your Demo\n\nWhat good is a beautiful demo if you can''t share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set `share=True` in `launch()`, and a publicly accessible URL will be created for your demo. Let''s revisit our example demo,  but change the last line as follows:\n\n```python\nimport gradio as gr\n\ndef greet(name):\n    return "Hello " + name + "!"\n\ndemo = gr.Interface(fn=greet, inputs="textbox", outputs="textbox")\n    \ndemo.launch(share=True)  # Share your demo with just 1 extra parameter ðŸš€\n```\n\nWhen you run this code, a public URL will be generated for your demo in a matter of seconds, something like:\n\nðŸ‘‰ &nbsp; `https://a23dsf231adb.gradio.live`\n\nNow, anyone around the world can try your Gradio demo from their browser, while the machine learning model and all computation continues to run locally on your computer.\n\nTo learn more about sharing your demo, read our dedicated guide on [sharing your Gradio application](https://www.gradio.app/guides/sharing-your-app).\n\n\n### An Overview of Gradio\n\nSo far, we''ve been discussing the `Interface` class, which is a high-level class that lets you build demos quickly with Gradio. But what else does Gradio include?\n\n#### Custom Demos with `gr.Blocks`\n\nGradio offers a low-level approach for designing web apps with more customizable layouts and data flows with the `gr.Blocks` class. Blocks supports things like controlling where components appear on the page, handling multiple data flows and more complex interactions (e.g. outputs can serve as inputs to other functions), and updating properties/visibility of components based on user interaction â€” still all in Python. \n\nYou can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\n#### Chatbots with `gr.ChatInterface`\n\nGradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you''re interested in creating a chatbot, you can jump straight to [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast).\n\n#### The Gradio Python & JavaScript Ecosystem\n\nThat''s the gist of the core `gradio` Python library, but Gradio is actually so much more! It''s an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\n* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.\n* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.\n* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications â€” for free!\n\n### What''s Next?\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [let''s dive deeper into the Interface class](https://www.gradio.app/guides/the-interface-class).\n\nOr, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/).\n\n\n### Gradio Sketch\n\nYou can also build Gradio applications without writing any code. Simply type `gradio sketch` into your terminal to open up an editor that lets you define and modify Gradio components, adjust their layouts, add events, all through a web editor. Or [use this hosted version of Gradio Sketch, running on Hugging Face Spaces](https://huggingface.co/spaces/aliabid94/Sketch).\n\n## Questions?\n\nIf you''d like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help.\n\nIf you like Gradio, please leave us a â­ on GitHub!\n\n## Open Source Stack\n\nGradio is built on top of many wonderful open-source libraries!\n\n[<img src="readme_files/huggingface_mini.svg" alt="huggingface" height=40>](https://huggingface.co)\n[<img src="readme_files/python.svg" alt="python" height=40>](https://www.python.org)\n[<img src="readme_files/fastapi.svg" alt="fastapi" height=40>](https://fastapi.tiangolo.com)\n[<img src="readme_files/encode.svg" alt="encode" height=40>](https://www.encode.io)\n[<img src="readme_files/svelte.svg" alt="svelte" height=40>](https://svelte.dev)\n[<img src="readme_files/vite.svg" alt="vite" height=40>](https://vitejs.dev)\n[<img src="readme_files/pnpm.svg" alt="pnpm" height=40>](https://pnpm.io)\n[<img src="readme_files/tailwind.svg" alt="tailwind" height=40>](https://tailwindcss.com)\n[<img src="readme_files/storybook.svg" alt="storybook" height=40>](https://storybook.js.org/)\n[<img src="readme_files/chromatic.svg" alt="chromatic" height=40>](https://www.chromatic.com/)\n\n## License\n\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository.\n\n## Citation\n\nAlso check out the paper _[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019_, and please cite it if you use Gradio in your work.\n\n```\n@article{abid2019gradio,\n  title = {Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild},\n  author = {Abid, Abubakar and Abdalla, Ali and Abid, Ali and Khan, Dawood and Alfozan, Abdulrahman and Zou, James},\n  journal = {arXiv preprint arXiv:1906.02569},\n  year = {2019},\n}\n```\n', '{"language":"Python","stars":40851,"forks":3177,"watchers":40851,"open_issues":434,"topics":["data-analysis","data-science","data-visualization","deep-learning","deploy","gradio","gradio-interface","interface","machine-learning","models","python","python-notebook","ui","ui-components"],"default_branch":"main","size_kb":312578,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui"},{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"}]', NULL, 'Apache-2.0', 'approved', 80, 'e002e1b5a98a4ec77bf4ee340f62aa9c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-gradio-app-gradio from https://github.com/gradio-app.png
Image converted to WebP: data/images/github-gradio-app-gradio.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ray-project-ray', 'github--ray-project--ray', 'ray', 'ray-project', '.. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/ray_header_logo.png .. image:: https://readthedocs.org/projects/ray/badge/?version=master :target: http://docs.ray.io/en/master/?badge=master .. image:: https://img.shields.io/badge/Ray-Join%20Slack-blue :target: https://www.ray.io/join-slack .. image:: https://img.shields.io/badge/Discuss-Ask%20Questions-blue :target: https://discuss.ray.io/ .. image:: https://img.shields.io/twitter/follow/raydistributed.svg?style=soc...', '["data-science","deep-learning","deployment","distributed","hyperparameter-optimization","hyperparameter-search","large-language-models","llm","llm-inference","llm-serving","machine-learning","optimization","parallel","python","pytorch","ray","reinforcement-learning","rllib","serving","tensorflow","python"]', 'other', 40208, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ray-project/ray","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '.. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/ray_header_logo.png\n\n.. image:: https://readthedocs.org/projects/ray/badge/?version=master\n    :target: http://docs.ray.io/en/master/?badge=master\n\n.. image:: https://img.shields.io/badge/Ray-Join%20Slack-blue\n    :target: https://www.ray.io/join-slack\n\n.. image:: https://img.shields.io/badge/Discuss-Ask%20Questions-blue\n    :target: https://discuss.ray.io/\n\n.. image:: https://img.shields.io/twitter/follow/raydistributed.svg?style=social&logo=twitter\n    :target: https://x.com/raydistributed\n\n.. image:: https://img.shields.io/badge/Get_started_for_free-3C8AE9?logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8%2F9hAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAEKADAAQAAAABAAAAEAAAAAA0VXHyAAABKElEQVQ4Ea2TvWoCQRRGnWCVWChIIlikC9hpJdikSbGgaONbpAoY8gKBdAGfwkfwKQypLQ1sEGyMYhN1Pd%2B6A8PqwBZeOHt%2FvsvMnd3ZXBRFPQjBZ9K6OY8ZxF%2B0IYw9PW3qz8aY6lk92bZ%2BVqSI3oC9T7%2FyCVnrF1ngj93us%2B540sf5BrCDfw9b6jJ5lx%2FyjtGKBBXc3cnqx0INN4ImbI%2Bl%2BPnI8zWfFEr4chLLrWHCp9OO9j19Kbc91HX0zzzBO8EbLK2Iv4ZvNO3is3h6jb%2BCwO0iL8AaWqB7ILPTxq3kDypqvBuYuwswqo6wgYJbT8XxBPZ8KS1TepkFdC79TAHHce%2F7LbVioi3wEfTpmeKtPRGEeoldSP%2FOeoEftpP4BRbgXrYZefsAI%2BP9JU7ImyEAAAAASUVORK5CYII%3D\n   :target: https://www.anyscale.com/ray-on-anyscale?utm_source=github&utm_medium=ray_readme&utm_campaign=get_started_badge\n\nRay is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI libraries for simplifying ML compute:\n\n.. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/what-is-ray-padded.svg\n\n..\n  https://docs.google.com/drawings/d/1Pl8aCYOsZCo61cmp57c7Sja6HhIygGCvSZLi_AuBuqo/edit\n\nLearn more about `Ray AI Libraries`_:\n\n- `Data`_: Scalable Datasets for ML\n- `Train`_: Distributed Training\n- `Tune`_: Scalable Hyperparameter Tuning\n- `RLlib`_: Scalable Reinforcement Learning\n- `Serve`_: Scalable and Programmable Serving\n\nOr more about `Ray Core`_ and its key abstractions:\n\n- `Tasks`_: Stateless functions executed in the cluster.\n- `Actors`_: Stateful worker processes created in the cluster.\n- `Objects`_: Immutable values accessible across the cluster.\n\nLearn more about Monitoring and Debugging:\n\n- Monitor Ray apps and clusters with the `Ray Dashboard <https://docs.ray.io/en/latest/ray-core/ray-dashboard.html>`__.\n- Debug Ray apps with the `Ray Distributed Debugger <https://docs.ray.io/en/latest/ray-observability/ray-distributed-debugger.html>`__.\n\nRay runs on any machine, cluster, cloud provider, and Kubernetes, and features a growing\n`ecosystem of community integrations`_.\n\nInstall Ray with: ``pip install ray``. For nightly wheels, see the\n`Installation page <https://docs.ray.io/en/latest/ray-overview/installation.html>`__.\n\n.. _`Serve`: https://docs.ray.io/en/latest/serve/index.html\n.. _`Data`: https://docs.ray.io/en/latest/data/dataset.html\n.. _`Workflow`: https://docs.ray.io/en/latest/workflows/\n.. _`Train`: https://docs.ray.io/en/latest/train/train.html\n.. _`Tune`: https://docs.ray.io/en/latest/tune/index.html\n.. _`RLlib`: https://docs.ray.io/en/latest/rllib/index.html\n.. _`ecosystem of community integrations`: https://docs.ray.io/en/latest/ray-overview/ray-libraries.html\n\n\nWhy Ray?\n--------\n\nToday''s ML workloads are increasingly compute-intensive. As convenient as they are, single-node development environments such as your laptop cannot scale to meet these demands.\n\nRay is a unified way to scale Python and AI applications from a laptop to a cluster.\n\nWith Ray, you can seamlessly scale the same code from a laptop to a cluster. Ray is designed to be general-purpose, meaning that it can performantly run any kind of workload. If your application is written in Python, you can scale it with Ray, no other infrastructure required.\n\nMore Information\n----------------\n\n- `Documentation`_\n- `Ray Architecture whitepaper`_\n- `Exoshuffle: large-scale data shuffle in Ray`_\n- `Ownership: a distributed futures system for fine-grained tasks`_\n- `RLlib paper`_\n- `Tune paper`_\n\n*Older documents:*\n\n- `Ray paper`_\n- `Ray HotOS paper`_\n- `Ray Architecture v1 whitepaper`_\n\n.. _`Ray AI Libraries`: https://docs.ray.io/en/latest/ray-air/getting-started.html\n.. _`Ray Core`: https://docs.ray.io/en/latest/ray-core/walkthrough.html\n.. _`Tasks`: https://docs.ray.io/en/latest/ray-core/tasks.html\n.. _`Actors`: https://docs.ray.io/en/latest/ray-core/actors.html\n.. _`Objects`: https://docs.ray.io/en/latest/ray-core/objects.html\n.. _`Documentation`: http://docs.ray.io/en/latest/index.html\n.. _`Ray Architecture v1 whitepaper`: https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview\n.. _`Ray Architecture whitepaper`: https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview\n.. _`Exoshuffle: large-scale data shuffle in Ray`: https://arxiv.org/abs/2203.05072\n.. _`Ownership: a distributed futures system for fine-grained tasks`: https://www.usenix.org/system/files/nsdi21-wang.pdf\n.. _`Ray paper`: https://arxiv.org/abs/1712.05889\n.. _`Ray HotOS paper`: https://arxiv.org/abs/1703.03924\n.. _`RLlib paper`: https://arxiv.org/abs/1712.09381\n.. _`Tune paper`: https://arxiv.org/abs/1807.05118\n\nGetting Involved\n----------------\n\n.. list-table::\n   :widths: 25 50 25 25\n   :header-rows: 1\n\n   * - Platform\n     - Purpose\n     - Estimated Response Time\n     - Support Level\n   * - `Discourse Forum`_\n     - For discussions about development and questions about usage.\n     - < 1 day\n     - Community\n   * - `GitHub Issues`_\n     - For reporting bugs and filing feature requests.\n     - < 2 days\n     - Ray OSS Team\n   * - `Slack`_\n     - For collaborating with other Ray users.\n     - < 2 days\n     - Community\n   * - `StackOverflow`_\n     - For asking questions about how to use Ray.\n     - 3-5 days\n     - Community\n   * - `Meetup Group`_\n     - For learning about Ray projects and best practices.\n     - Monthly\n     - Ray DevRel\n   * - `Twitter`_\n     - For staying up-to-date on new features.\n     - Daily\n     - Ray DevRel\n\n.. _`Discourse Forum`: https://discuss.ray.io/\n.. _`GitHub Issues`: https://github.com/ray-project/ray/issues\n.. _`StackOverflow`: https://stackoverflow.com/questions/tagged/ray\n.. _`Meetup Group`: https://www.meetup.com/Bay-Area-Ray-Meetup/\n.. _`Twitter`: https://x.com/raydistributed\n.. _`Slack`: https://www.ray.io/join-slack?utm_source=github&utm_medium=ray_readme&utm_campaign=getting_involved\n', '{"language":"Python","stars":40208,"forks":6980,"watchers":40208,"open_issues":3218,"topics":["data-science","deep-learning","deployment","distributed","hyperparameter-optimization","hyperparameter-search","large-language-models","llm","llm-inference","llm-serving","machine-learning","optimization","parallel","python","pytorch","ray","reinforcement-learning","rllib","serving","tensorflow"],"default_branch":"master","size_kb":643105,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:ray-project:ray","source_url":"https://github.com/ray-project/ray"},{"type":"has_code","target_id":"github:ray-project:ray","source_url":"https://github.com/ray-project/ray"},{"type":"has_code","target_id":"github:ray-project:ray","source_url":"https://github.com/ray-project/ray"}]', NULL, 'Apache-2.0', 'approved', 65, '4e810944c5ced749b319403d13f5b309', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ray-project-ray from https://github.com/ray-project.png
Image converted to WebP: data/images/github-ray-project-ray.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-photoprism-photoprism', 'github--photoprism--photoprism', 'photoprism', 'photoprism', 'PhotoPrism: Browse Your Life in Pictures ======================================== PhotoPrismÂ® is an AI-Powered Photos App for the Decentralized Web. It makes use of the latest technologies to tag and find pictures automatically without getting in your way. You can run it at home, on a private server, or in the cloud. To get a first impression, you are welcome to play with our public demo. Please be careful not to upload any private, unlawful or offensive pictures. **Our mission is to provide ...', '["ai","golang","google-photos","machine-learning","photography","private-cloud","self-hosted","tensorflow","go"]', 'other', 38891, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/photoprism/photoprism","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', 'PhotoPrism: Browse Your Life in Pictures\n========================================\n\n[![License: AGPL](https://img.shields.io/badge/license-AGPL%203.0-454377.svg)](https://docs.photoprism.app/license/agpl/)\n[![Documentation](https://img.shields.io/badge/read-the%20docs-4d6a91.svg)](https://docs.photoprism.app/)\n[![Community Chat](https://img.shields.io/badge/chat-on%20gitter-4d6a91.svg)](https://link.photoprism.app/chat)\n[![GitHub Discussions](https://img.shields.io/badge/ask-%20on%20github-4d6a91.svg)](https://link.photoprism.app/discussions)\n[![Bluesky Social](https://dl.photoprism.app/img/badges/badge-bluesky.svg)](https://bsky.app/profile/photoprism.app)\n[![Mastodon](https://dl.photoprism.app/img/badges/badge-floss-social.svg)](https://floss.social/@photoprism)\n\nPhotoPrismÂ® is an AI-Powered Photos App for the [Decentralized Web](https://en.wikipedia.org/wiki/Decentralized_web).\nIt makes use of the latest technologies to tag and find pictures automatically without getting in your way.\nYou can run it at home, on a private server, or in the cloud.\n\n![](https://dl.photoprism.app/img/ui/2025/desktop-search.jpg)\n\nTo get a first impression, you are welcome to play with our [public demo](https://try.photoprism.app/). Please be careful not to upload any private, unlawful or offensive pictures.\n\n## Feature Overview ##\n\n**Our mission is to provide the most user- and privacy-friendly solution to keep your pictures organized and accessible.** That''s why PhotoPrism was built from the ground up to run wherever you need it, without compromising freedom, privacy, or functionality:\n\n<img align="right" height="270" src="https://dl.photoprism.app/img/ui/2025/iphone-crocus-540px.png">\n\n* Browse [all your pictures](https://docs.photoprism.app/user-guide/organize/browse/) without worrying about [RAW images](https://www.photoprism.app/kb/file-formats) or [video formats](https://docs.photoprism.app/user-guide/organize/video/)\n* Whether you''re using a phone, tablet, or desktop computer, our [intuitive PWA](https://try.photoprism.app/) provides a native app-like experience and can be [easily installed](https://docs.photoprism.app/user-guide/pwa/) on your home screen\n* Quickly find specific photos and videos with [powerful search filters](https://docs.photoprism.app/user-guide/search/filters/) that can be combined and are available for [many different properties](https://docs.photoprism.app/user-guide/search/filters/#filter-reference), including [labels](https://try.photoprism.app/library/labels), [location](https://try.photoprism.app/library/places?q=s2:47a85a63f764), [resolution](https://try.photoprism.app/library/browse?view=cards&q=mp:4), [color](https://try.photoprism.app/library/browse?view=cards&q=color:red), [chroma](https://try.photoprism.app/library/browse?view=cards&q=mono%3Atrue), and [quality](https://try.photoprism.app/library/review)\n* [Automatically labels your pictures](https://try.photoprism.app/library/labels) based on content and location, and recognizes the faces of [your family and friends](https://try.photoprism.app/library/people/new)\n* [Live Photos](https://try.photoprism.app/library/live) start playing when you [hover over them](https://try.photoprism.app/library/browse?view=cards&q=type%3Alive) and when viewing a slideshow\n* Six high-resolution [World Maps](https://try.photoprism.app/library/places) and our [privacy-preserving geocoding service](https://docs.photoprism.app/getting-started/#maps-places) help bring back memories of your favorite trips and let you explore the world\n* Metadata can be extracted and merged from Exif, XMP, and other sources like Google Photos\n* [Use compatible apps](https://docs.photoprism.app/user-guide/native-apps/) like [PhotoSync](https://link.photoprism.app/photosync) to back up iOS and Android phones in the background\n* WebDAV clients such as [Microsoft''s Windows Explorer](https://docs.photoprism.app/user-guide/sync/webdav/#__tabbed_1_2) and [Apple''s Finder](https://docs.photoprism.app/user-guide/sync/webdav/#connect-to-a-webdav-server) can [connect directly to PhotoPrism](https://docs.photoprism.app/user-guide/sync/webdav/), allowing you to open, edit, and delete files from your computer as if they were local\n\nBeing completely [**self-funded and independent**](https://link.photoprism.app/membership), we can promise you that we will [never sell your data](https://www.photoprism.app/privacy) and that we will [always be transparent](https://www.photoprism.app/terms) about our software and services. Your data will never be shared with Google, Amazon, Microsoft or Apple unless you intentionally upload files to one of their services. ðŸ”’\n\n## Getting Started ##\n\nStep-by-step [installation instructions](https://docs.photoprism.app/getting-started/) for our self-hosted [community edition](https://link.photoprism.app/personal-editions) can be found on [docs.photoprism.app](https://docs.photoprism.app/getting-started/) - all you need is a Web browser and [Docker](https://docs.docker.com/get-docker/) to run the server. It is available for Mac, Linux, and Windows.\n\nThe [stable releases](https://docs.photoprism.app/release-notes/) and [development preview](https://docs.photoprism.app/getting-started/updates/#development-preview) are available as a [multi-arch image](https://link.photoprism.app/docker-hub) for 64-bit AMD, Intel, and ARM processors.\nThat means, [Raspberry Pi](https://docs.photoprism.app/getting-started/raspberry-pi/) and Apple Silicon users enjoy the exact same functionality and can follow the same [installation steps](https://docs.photoprism.app/getting-started/docker-compose/).\n\nSee our [Getting Started FAQ](https://docs.photoprism.app/getting-started/faq/#how-can-i-install-photoprism-without-docker) for alternative installation methods, for example using the [*tar.gz* packages](https://dl.photoprism.app/pkg/linux/README.html) we provide.\n\n## Support Our Mission ðŸ’Ž ##\n\n**PhotoPrism is 100% self-funded and independent.** Your [continued support](https://link.photoprism.app/membership) helps us [provide more features to the public](https://www.photoprism.app/oss/faq#what-functionality-is-generally-available), release [regular updates](https://docs.photoprism.app/release-notes/), and remain independent!\n\nOur members [enjoy additional features](https://www.photoprism.app/kb/personal), including access to [interactive world maps](https://try.photoprism.app/library/places), and can join our private chat room to [connect with our team](https://www.photoprism.app/about/team). We currently have the following membership options:\n\n- You can [sign up directly on our website](https://link.photoprism.app/membership) and pay with credit card or SEPA through Stripe, so you don''t need to [link an external account](https://www.photoprism.app/kb/activation) and can easily upgrade or downgrade at any time\n- Alternatively, [Patreon](https://link.photoprism.app/patreon) also supports PayPal, additional currencies, and lets you choose between monthly and annual billing for all tiers\n\nIf you currently support us through [GitHub Sponsors](https://link.photoprism.app/sponsor), you can also [register on our website](https://my.photoprism.app/register) and use the *Activate GitHub Sponsors Membership* button to link your account. For details on this and how to [link your Patreon account](https://www.patreon.com/pledges), see our [Activation Guide](https://www.photoprism.app/kb/activation).\n\nYou are [welcome to contact us](https://www.photoprism.app/contact) for change requests, membership questions, and business partnerships.\n\n[View Membership FAQ â€º](https://www.photoprism.app/kb/membership)â€ƒ[Sign Up â€º](https://link.photoprism.app/membership)\n\n### Why Your Support Matters ###\n\n- Your continued support helps us provide regular updates and remain independent, so we can fulfill our mission and protect your privacy\n- Sustained funding is key to quickly releasing new features requested by you and other community members\n- Being self-funded and independent, we can personally promise you that we will never sell your data and that we will always be transparent about our software and services\n\nPlease also leave [a star](https://github.com/photoprism/photoprism/stargazers) on GitHub if you like this project. It provides additional motivation to keep going.\n\n**A big thank you to all current and past sponsors, whose generous support has been and continues to be essential to the success of the project!**\n\n[View Sponsors â€º](SPONSORS.md)â€ƒ[View Credits â€º](https://docs.photoprism.app/credits/)\n\n## Getting Support ##\n\nVisit [docs.photoprism.app/user-guide](https://docs.photoprism.app/user-guide/) to learn how to [sync](https://docs.photoprism.app/user-guide/sync/webdav/), [organize](https://docs.photoprism.app/user-guide/library/), and [share](https://docs.photoprism.app/user-guide/share/) your pictures. If you need help installing our software at home, you are welcome to post your question in [GitHub Discussions](https://link.photoprism.app/discussions) or ask in our [Community Chat](https://link.photoprism.app/chat).\nCommon problems can be quickly diagnosed and solved using our [Troubleshooting Checklists](https://docs.photoprism.app/getting-started/troubleshooting/). Eligible [members](https://link.photoprism.app/membership) are also welcome to email us for technical support and advice.\n\n## Upcoming Features and Enhancements ##\n\n<a href="https://github.com/orgs/photoprism/projects/5"><img align="right" height="240" src="https://dl.photoprism.app/img/ui/2025/upcoming-features-240px.png"></a>\n\nOur [Project Roadmap](https://link.photoprism.app/roadmap) shows what tasks are in progress and what features will be implemented next. You are invited to give ideas you like a thumbs-up, so we know what''s most popular.\n\nBe aware that we have a zero-bug policy and do our best to help users when they need support or have other questions. This comes at a price though, as we can''t give exact release dates for new features. Our team receives many more requests than can be implemented, so we want to emphasize that we are in no way obligated to implement the features, enhancements, or other changes you request. We do, however, appreciate your feedback and carefully consider all requests.\n\n**Because sustained funding is key to quickly releasing new features, we encourage you to support our mission by [signing up for a personal membership](https://link.photoprism.app/membership) or [purchasing a commercial license](https://www.photoprism.app/teams#compare).**\n\n[Become a Member â€º](https://link.photoprism.app/membership)\n\n## GitHub Issues âš ï¸ ##\n\nWe kindly ask you not to report bugs via GitHub Issues **unless you are certain to have found a fully reproducible and previously unreported issue** that must be fixed directly in the app. Thank you for your careful consideration!\n\n- When browsing issues, please note that **our team and all issue subscribers receive an email notification** from GitHub whenever a new comment is added, so these should only be used for sharing important information and not for [discussions, questions](https://github.com/photoprism/photoprism/discussions), or [expressing personal opinions](https://www.photoprism.app/code-of-conduct)\n- In order for us to investigate [new bug reports](https://www.photoprism.app/kb/reporting-bugs), they must include **a complete list of steps to reproduce the problem**, the software versions used and information about the environment in which the problem occurred, such as [browser type, browser version, browser plug-ins](https://docs.photoprism.app/getting-started/troubleshooting/browsers/), operating system, [storage type](https://docs.photoprism.app/getting-started/troubleshooting/performance/#storage), [processor type](https://docs.photoprism.app/getting-started/troubleshooting/performance/#server-cpu), and [memory size](https://docs.photoprism.app/getting-started/troubleshooting/performance/#memory)\n- [Contact us](https://www.photoprism.app/contact) or [a community member](https://link.photoprism.app/discussions) if you need help, it could be a local configuration problem, or a misunderstanding in how the software works\n- This gives us the opportunity to [improve our documentation](https://docs.photoprism.app/getting-started/troubleshooting/) and provide best-in-class support instead of dealing with unclear/duplicate bug reports or triggering a flood of notifications by replying to comments\n\n## Connect with the Community ##\n\n<a href="https://link.photoprism.app/chat"><img align="right" width="144" height="144" src="https://dl.photoprism.app/img/brands/element-logo.svg"></a>\n\nFollow us on [Mastodon](https://floss.social/@photoprism), [Bluesky](https://bsky.app/profile/photoprism.app), or join the [Community Chat](https://link.photoprism.app/chat) to get regular updates, connect with other users, and discuss your ideas. Our [Code of Conduct](https://www.photoprism.app/code-of-conduct) explains the "dos and donâ€™ts" when interacting with other community members.\n\nAs a [contributor](CONTRIBUTING.md), you are also welcome to [contact us directly](https://www.photoprism.app/contact) if you have something on your mind that you don''t want to discuss publicly. Please note, however, that due to the high volume of emails we receive, our team may be unable to get back to you immediately. We do our best to respond within five business days or less.\n\n## Every Contribution Makes a Difference ##\n\nWe welcome [contributions](CONTRIBUTING.md) of any kind, including blog posts, tutorials, translations, testing, writing documentation, and pull requests. Our [Developer Guide](https://docs.photoprism.app/developer-guide/) contains all the information necessary for you to get started.\n\n----\n\n*PhotoPrismÂ® is a [registered trademark](https://www.photoprism.app/trademark). By using the software and services we provide, you agree to our [Terms of Service](https://www.photoprism.app/terms), [Privacy Policy](https://www.photoprism.app/privacy), and [Code of Conduct](https://www.photoprism.app/code-of-conduct). Docs are [available](https://link.photoprism.app/github-docs) under the [CC BY-NC-SA 4.0 License](https://creativecommons.org/licenses/by-nc-sa/4.0/); [additional terms](https://github.com/photoprism/photoprism/blob/develop/assets/README.md) may apply.*\n', '{"language":"Go","stars":38891,"forks":2173,"watchers":38891,"open_issues":421,"topics":["ai","golang","google-photos","machine-learning","photography","private-cloud","self-hosted","tensorflow"],"default_branch":"develop","size_kb":321324,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:photoprism:photoprism","source_url":"https://github.com/photoprism/photoprism"},{"type":"has_code","target_id":"github:orgs:photoprism","source_url":"https://github.com/orgs/photoprism"},{"type":"has_code","target_id":"github:photoprism:photoprism","source_url":"https://github.com/photoprism/photoprism"},{"type":"has_code","target_id":"github:photoprism:photoprism","source_url":"https://github.com/photoprism/photoprism"}]', NULL, 'NOASSERTION', 'approved', 80, 'fd216a0633506fe9f5dbb1ec54bdbda2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-photoprism-photoprism from https://github.com/photoprism.png
Image converted to WebP: data/images/github-photoprism-photoprism.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-LAION-AI-Open-Assistant', 'github--laion-ai--open-assistant', 'Open-Assistant', 'LAION-AI', '<h1 align="center"> <span>Open-Assistant</span> <img width="auto" height="50px" src="https://github.com/LAION-AI/Open-Assistant/blob/main/assets/logo_crop.png"/> </h1> <blockquote> <p>:memo: <strong>NOTE</strong>: OpenAssistant is completed, and the project is now finished. Thank you to everyone who contributed! Check out our <a href="https://projects.laion.ai/Open-Assistant/blog/2023/10/25/open-assistant-is-completed">blog post</a> for more information. The final published oasst2 dataset can...', '["ai","assistant","chatgpt","discord-bot","language-model","machine-learning","nextjs","python","rlhf","python"]', 'other', 37497, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/LAION-AI/Open-Assistant","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<h1 align="center">\n    <span>Open-Assistant</span>\n  <img width="auto" height="50px" src="https://github.com/LAION-AI/Open-Assistant/blob/main/assets/logo_crop.png"/>\n</h1>\n\n<blockquote>\n<p>:memo: <strong>NOTE</strong>: OpenAssistant is completed, and the project is now finished. Thank you to everyone who contributed! Check out our <a href="https://projects.laion.ai/Open-Assistant/blog/2023/10/25/open-assistant-is-completed">blog post</a> for more information. The final published oasst2 dataset can be found on HuggingFace at <a href="https://huggingface.co/datasets/OpenAssistant/oasst2">OpenAssistant/oasst2</a></p>\n</blockquote>\n\n<div align="center">\n\n<a href="https://github.com/LAION-AI/Open-Assistant/stargazers">![GitHub Repo stars](https://img.shields.io/github/stars/LAION-AI/Open-Assistant?style=social)</a>\n<a href="https://laion-ai.github.io/Open-Assistant/">![Docs](https://img.shields.io/badge/docs-laion--ai.github.io%2FOpen--Assistant%2F-green)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/actions/workflows/build-frontend.yaml">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/build-frontend.yaml?label=build-frontend)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/actions/workflows/build-postgres.yaml">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/build-postgres.yaml?label=build-postgres)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/actions/workflows/pre-commit.yaml">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/pre-commit.yaml?label=pre-commit)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/actions/workflows/test-api-contract.yaml">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/test-api-contract.yaml?label=tests-api)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/actions/workflows/test-e2e.yaml">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/test-e2e.yaml?label=tests-web)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/actions/workflows/deploy-docs-site.yaml">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/deploy-docs-site.yaml?label=deploy-docs)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/actions/workflows/production-deploy.yaml">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/production-deploy.yaml?label=deploy-production)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/actions/workflows/release.yaml">![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/LAION-AI/Open-Assistant/release.yaml?label=deploy-release)</a>\n<a href="https://github.com/LAION-AI/Open-Assistant/releases">![GitHub release (latest by date)](https://img.shields.io/github/v/release/LAION-AI/Open-Assistant)</a>\n<a href="https://github-com.translate.goog/LAION-AI/Open-Assistant/blob/main/README.md?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en&_x_tr_pto=wapp">![Translate](https://img.shields.io/badge/Translate-blue)</a>\n\n</div>\n\n# Table of Contents\n\n- [What is Open Assistant?](#what-is-open-assistant)\n- [Useful Links](#useful-links)\n- [How To Try It Out](#how-to-try-it-out)\n- [The Vision](#the-vision)\n- [The Plan](#the-plan)\n- [How You Can Help](#how-you-can-help)\n\n---\n\n## What is Open Assistant?\n\n<p align="center">\nOpen Assistant is a project meant to give everyone access to a great chat based\nlarge language model.\n</p>\n\nWe believe that by doing this we will create a revolution in innovation in\nlanguage. In the same way that stable-diffusion helped the world make art and\nimages in new ways we hope Open Assistant can help improve the world by\nimproving language itself.\n\n# Useful Links\n\n- [Data Collection](https://open-assistant.io)\n\n- [Chat](https://open-assistant.io/chat)\n\n- [Project Documentation](https://projects.laion.ai/Open-Assistant/)\n\n## How To Try It Out\n\n### Chatting with the AI\n\nThe chat frontend is now live [here](https://open-assistant.io/chat). Log in and\nstart chatting! Please try to react with a thumbs up or down for the assistant''s\nresponses when chatting.\n\n### Contributing to Data Collection\n\nThe data collection frontend is now live [here](https://open-assistant.io/). Log\nin and start taking on tasks! We want to collect a high volume of quality data.\nBy submitting, ranking, and labelling model prompts and responses you will be\ndirectly helping to improve the capabilities of Open Assistant.\n\n### Running the Development Setup Locally (without chat)\n\n**You do not need to run the project locally unless you are contributing to the\ndevelopment process. The website link above will take you to the public website\nwhere you can use the data collection app and the chat.**\n\nIf you would like to run the data collection app locally for development, you\ncan set up an entire stack needed to run **Open-Assistant**, including the\nwebsite, backend, and associated dependent services, with Docker.\n\nTo start the demo, run this in the root directory of the repository (check\n[this FAQ](https://projects.laion.ai/Open-Assistant/docs/faq#docker-compose-instead-of-docker-compose)\nif you have problems):\n\n```sh\ndocker compose --profile ci up --build --attach-dependencies\n```\n\n> **Note:** when running on MacOS with an M1 chip you have to use:\n> `DB_PLATFORM=linux/x86_64 docker compose ...`\n\nThen, navigate to `http://localhost:3000` (It may take some time to boot up) and\ninteract with the website.\n\n> **Note:** If an issue occurs with the build, please head to the\n> [FAQ](https://projects.laion.ai/Open-Assistant/docs/faq) and check out the\n> entries about Docker.\n\n> **Note:** When logging in via email, navigate to `http://localhost:1080` to\n> get the magic email login link.\n\n> **Note:** If you would like to run this in a standardized development\n> environment (a\n> ["devcontainer"](https://code.visualstudio.com/docs/devcontainers/containers))\n> using\n> [vscode locally](https://code.visualstudio.com/docs/devcontainers/create-dev-container#_create-a-devcontainerjson-file)\n> or in a web browser using\n> [GitHub Codespaces](https://github.com/features/codespaces), you can use the\n> provided [`.devcontainer`](.devcontainer/) folder.\n\n### Running the Development Setup Locally for Chat\n\n**You do not need to run the project locally unless you are contributing to the\ndevelopment process. The website link above will take you to the public website\nwhere you can use the data collection app and the chat.**\n\n**Also note that the local setup is only for development and is not meant to be\nused as a local chatbot, unless you know what you are doing.**\n\nIf you _do_ know what you are doing, then see the `inference` folder for getting\nthe inference system up and running, or have a look at `--profile inference` in\naddition to `--profile ci` in the above command.\n\n## The Vision\n\nWe are not going to stop at replicating ChatGPT. We want to build the assistant\nof the future, able to not only write email and cover letters, but do meaningful\nwork, use APIs, dynamically research information, and much more, with the\nability to be personalized and extended by anyone. And we want to do this in a\nway that is open and accessible, which means we must not only build a great\nassistant, but also make it small and efficient enough to run on consumer\nhardware.\n\n## The Plan\n\n##### We want to get to an initial MVP as fast as possible, by following the 3-steps outlined in the [InstructGPT paper](https://arxiv.org/abs/2203.02155)\n\n1. Collect high-quality human generated Instruction-Fulfillment samples\n   (prompt + response), goal >50k. We design a crowdsourced process to collect\n   and reviewed prompts. We do not want to train on\n   flooding/toxic/spam/junk/personal information data. We will have a\n   leaderboard to motivate the community that shows progress and the most active\n   users. Swag will be given to the top-contributors.\n2. For each of the collected prompts we will sample multiple completions.\n   Completions of one prompt will then be shown randomly to users to rank them\n   from best to worst. Again this should happen crowd-sourced, e.g. we need to\n   deal with unreliable potentially malicious users. At least multiple votes by\n   independent users have to be collected to measure the overall agreement. The\n   gathered ranking-data will be used to train a reward model.\n3. Now follows the RLHF training phase based on the prompts and the reward\n   model.\n\nWe can then take the resulting model and continue with completion sampling step\n2 for a next iteration.\n\n### Slide Decks\n\n[Vision & Roadmap](https://docs.google.com/presentation/d/1n7IrAOVOqwdYgiYrXc8Sj0He8krn5MVZO_iLkCjTtu0/edit?usp=sharing)\n\n[Important Data Structures](https://docs.google.com/presentation/d/1iaX_nxasVWlvPiSNs0cllR9L_1neZq0RJxd6MFEalUY/edit?usp=sharing)\n\n## How You Can Help\n\nAll open source projects begin with people like you. Open source is the belief\nthat if we collaborate we can together gift our knowledge and technology to the\nworld for the benefit of humanity.\n\nCheck out our [contributing guide](CONTRIBUTING.md) to get started.\n', '{"language":"Python","stars":37497,"forks":3300,"watchers":37497,"open_issues":291,"topics":["ai","assistant","chatgpt","discord-bot","language-model","machine-learning","nextjs","python","rlhf"],"default_branch":"main","size_kb":35477,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:LAION-AI:Open-Assistant","source_url":"https://github.com/LAION-AI/Open-Assistant"},{"type":"has_code","target_id":"github:features:codespaces","source_url":"https://github.com/features/codespaces"}]', NULL, 'Apache-2.0', 'approved', 65, '088d2fba3d79519bdbd83d25cd9b1a78', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-LAION-AI-Open-Assistant from https://github.com/LAION-AI.png
Image converted to WebP: data/images/github-LAION-AI-Open-Assistant.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-google-research-google-research', 'github--google-research--google-research', 'google-research', 'google-research', 'This repository contains code released by Google Research. All datasets in this repository are released under the CC BY 4.0 International license, which can be found here: https://creativecommons.org/licenses/by/4.0/legalcode. All source files in this repository are released under the Apache 2.0 license, the text of which can be found in the LICENSE file. --- Because the repo is large, we recommend you download only the subdirectory of interest: * Use GitHub editor to open the project. To ope...', '["ai","machine-learning","research","jupyter notebook"]', 'other', 36858, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/google-research/google-research","fetched_at":"2025-12-08T10:30:37.946Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Google Research\n\nThis repository contains code released by\n[Google Research](https://research.google).\n\nAll datasets in this repository are released under the CC BY 4.0 International\nlicense, which can be found here:\nhttps://creativecommons.org/licenses/by/4.0/legalcode.  All source files in this\nrepository are released under the Apache 2.0 license, the text of which can be\nfound in the LICENSE file.\n\n---\n\nBecause the repo is large, we recommend you download only the subdirectory of\ninterest:\n\n* Use GitHub editor to open the project. To open the editor change the url from\ngithub.com to github.dev in the address bar.\n* In the left navigation panel, right-click on the folder of interest and select\ndownload.\n\nIf you''d like to submit a pull request, you''ll need to clone the repository;\nwe recommend making a shallow clone (without history).\n\n```\ngit clone git@github.com:google-research/google-research.git --depth=1\n```\n\n---\n\n*Disclaimer: This is not an official Google product.*\n\nUpdated in 2023.', '{"language":"Jupyter Notebook","stars":36858,"forks":8264,"watchers":36858,"open_issues":1705,"topics":["ai","machine-learning","research"],"default_branch":"master","size_kb":1149754,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[]', NULL, 'Apache-2.0', 'approved', 50, '6ccd1e88bdd18645fe4251374ef7b23c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-google-research-google-research from https://github.com/google-research.png
Image converted to WebP: data/images/github-google-research-google-research.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-roboflow-supervision', 'github--roboflow--supervision', 'supervision', 'roboflow', '<div align="center"> <p> <a align="center" href="" target="https://supervision.roboflow.com"> <img width="100%" src="https://media.roboflow.com/open-source/supervision/rf-supervision-banner.png?updatedAt=1678995927529" > </a> </p> <br> notebooks | inference | autodistill | maestro <br> <div align="center"> <a href="https://trendshift.io/repositories/124" target="_blank"><img src="https://trendshift.io/api/badge/repositories/124" alt="roboflow%2Fsupervision | Trendshift" style="width: 250px; h...', '["classification","coco","computer-vision","deep-learning","hacktoberfest","image-processing","instance-segmentation","low-code","machine-learning","metrics","object-detection","oriented-bounding-box","pascal-voc","python","pytorch","tensorflow","tracking","video-processing","yolo","python"]', 'other', 36118, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/roboflow/supervision","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n  <p>\n    <a align="center" href="" target="https://supervision.roboflow.com">\n      <img\n        width="100%"\n        src="https://media.roboflow.com/open-source/supervision/rf-supervision-banner.png?updatedAt=1678995927529"\n      >\n    </a>\n  </p>\n\n<br>\n\n[notebooks](https://github.com/roboflow/notebooks) | [inference](https://github.com/roboflow/inference) | [autodistill](https://github.com/autodistill/autodistill) | [maestro](https://github.com/roboflow/multimodal-maestro)\n\n<br>\n\n[![version](https://badge.fury.io/py/supervision.svg)](https://badge.fury.io/py/supervision)\n[![downloads](https://img.shields.io/pypi/dm/supervision)](https://pypistats.org/packages/supervision)\n[![snyk](https://snyk.io/advisor/python/supervision/badge.svg)](https://snyk.io/advisor/python/supervision)\n[![license](https://img.shields.io/pypi/l/supervision)](https://github.com/roboflow/supervision/blob/main/LICENSE.md)\n[![python-version](https://img.shields.io/pypi/pyversions/supervision)](https://badge.fury.io/py/supervision)\n[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow/supervision/blob/main/demo.ipynb)\n[![gradio](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Roboflow/Annotators)\n[![discord](https://img.shields.io/discord/1159501506232451173?logo=discord&label=discord&labelColor=fff&color=5865f2&link=https%3A%2F%2Fdiscord.gg%2FGbfgXGJ8Bk)](https://discord.gg/GbfgXGJ8Bk)\n[![built-with-material-for-mkdocs](https://img.shields.io/badge/Material_for_MkDocs-526CFE?logo=MaterialForMkDocs&logoColor=white)](https://squidfunk.github.io/mkdocs-material/)\n\n  <div align="center">\n    <a href="https://trendshift.io/repositories/124"  target="_blank"><img src="https://trendshift.io/api/badge/repositories/124" alt="roboflow%2Fsupervision | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n  </div>\n\n</div>\n\n## ðŸ‘‹ hello\n\n**We write your reusable computer vision tools.** Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us! ðŸ¤\n\n## ðŸ’» install\n\nPip install the supervision package in a\n[**Python>=3.9**](https://www.python.org/) environment.\n\n```bash\npip install supervision\n```\n\nRead more about conda, mamba, and installing from source in our [guide](https://roboflow.github.io/supervision/).\n\n## ðŸ”¥ quickstart\n\n### models\n\nSupervision was designed to be model agnostic. Just plug in any classification, detection, or segmentation model. For your convenience, we have created [connectors](https://supervision.roboflow.com/latest/detection/core/#detections) for the most popular libraries like Ultralytics, Transformers, or MMDetection.\n\n```python\nimport cv2\nimport supervision as sv\nfrom ultralytics import YOLO\n\nimage = cv2.imread(...)\nmodel = YOLO("yolov8s.pt")\nresult = model(image)[0]\ndetections = sv.Detections.from_ultralytics(result)\n\nlen(detections)\n# 5\n```\n\n<details>\n<summary>ðŸ‘‰ more model connectors</summary>\n\n- inference\n\n  Running with [Inference](https://github.com/roboflow/inference) requires a [Roboflow API KEY](https://docs.roboflow.com/api-reference/authentication#retrieve-an-api-key).\n\n  ```python\n  import cv2\n  import supervision as sv\n  from inference import get_model\n\n  image = cv2.imread(...)\n  model = get_model(model_id="yolov8s-640", api_key=<ROBOFLOW API KEY>)\n  result = model.infer(image)[0]\n  detections = sv.Detections.from_inference(result)\n\n  len(detections)\n  # 5\n  ```\n\n</details>\n\n### annotators\n\nSupervision offers a wide range of highly customizable [annotators](https://supervision.roboflow.com/latest/detection/annotators/), allowing you to compose the perfect visualization for your use case.\n\n```python\nimport cv2\nimport supervision as sv\n\nimage = cv2.imread(...)\ndetections = sv.Detections(...)\n\nbox_annotator = sv.BoxAnnotator()\nannotated_frame = box_annotator.annotate(\n  scene=image.copy(),\n  detections=detections)\n```\n\nhttps://github.com/roboflow/supervision/assets/26109316/691e219c-0565-4403-9218-ab5644f39bce\n\n### datasets\n\nSupervision provides a set of [utils](https://supervision.roboflow.com/latest/datasets/core/) that allow you to load, split, merge, and save datasets in one of the supported formats.\n\n```python\nimport supervision as sv\nfrom roboflow import Roboflow\n\nproject = Roboflow().workspace(<WORKSPACE_ID>).project(<PROJECT_ID>)\ndataset = project.version(<PROJECT_VERSION>).download("coco")\n\nds = sv.DetectionDataset.from_coco(\n    images_directory_path=f"{dataset.location}/train",\n    annotations_path=f"{dataset.location}/train/_annotations.coco.json",\n)\n\npath, image, annotation = ds[0]\n    # loads image on demand\n\nfor path, image, annotation in ds:\n    # loads image on demand\n```\n\n<details close>\n<summary>ðŸ‘‰ more dataset utils</summary>\n\n- load\n\n  ```python\n  dataset = sv.DetectionDataset.from_yolo(\n      images_directory_path=...,\n      annotations_directory_path=...,\n      data_yaml_path=...\n  )\n\n  dataset = sv.DetectionDataset.from_pascal_voc(\n      images_directory_path=...,\n      annotations_directory_path=...\n  )\n\n  dataset = sv.DetectionDataset.from_coco(\n      images_directory_path=...,\n      annotations_path=...\n  )\n  ```\n\n- split\n\n  ```python\n  train_dataset, test_dataset = dataset.split(split_ratio=0.7)\n  test_dataset, valid_dataset = test_dataset.split(split_ratio=0.5)\n\n  len(train_dataset), len(test_dataset), len(valid_dataset)\n  # (700, 150, 150)\n  ```\n\n- merge\n\n  ```python\n  ds_1 = sv.DetectionDataset(...)\n  len(ds_1)\n  # 100\n  ds_1.classes\n  # [''dog'', ''person'']\n\n  ds_2 = sv.DetectionDataset(...)\n  len(ds_2)\n  # 200\n  ds_2.classes\n  # [''cat'']\n\n  ds_merged = sv.DetectionDataset.merge([ds_1, ds_2])\n  len(ds_merged)\n  # 300\n  ds_merged.classes\n  # [''cat'', ''dog'', ''person'']\n  ```\n\n- save\n\n  ```python\n  dataset.as_yolo(\n      images_directory_path=...,\n      annotations_directory_path=...,\n      data_yaml_path=...\n  )\n\n  dataset.as_pascal_voc(\n      images_directory_path=...,\n      annotations_directory_path=...\n  )\n\n  dataset.as_coco(\n      images_directory_path=...,\n      annotations_path=...\n  )\n  ```\n\n- convert\n\n  ```python\n  sv.DetectionDataset.from_yolo(\n      images_directory_path=...,\n      annotations_directory_path=...,\n      data_yaml_path=...\n  ).as_pascal_voc(\n      images_directory_path=...,\n      annotations_directory_path=...\n  )\n  ```\n\n</details>\n\n## ðŸŽ¬ tutorials\n\nWant to learn how to use Supervision? Explore our [how-to guides](https://supervision.roboflow.com/develop/how_to/detect_and_annotate/), [end-to-end examples](https://github.com/roboflow/supervision/tree/develop/examples), [cheatsheet](https://roboflow.github.io/cheatsheet-supervision/), and [cookbooks](https://supervision.roboflow.com/develop/cookbooks/)!\n\n<br/>\n\n<p align="left">\n<a href="https://youtu.be/hAWpsIuem10" title="Dwell Time Analysis with Computer Vision | Real-Time Stream Processing"><img src="https://github.com/SkalskiP/SkalskiP/assets/26109316/a742823d-c158-407d-b30f-063a5d11b4e1" alt="Dwell Time Analysis with Computer Vision | Real-Time Stream Processing" width="300px" align="left" /></a>\n<a href="https://youtu.be/hAWpsIuem10" title="Dwell Time Analysis with Computer Vision | Real-Time Stream Processing"><strong>Dwell Time Analysis with Computer Vision | Real-Time Stream Processing</strong></a>\n<div><strong>Created: 5 Apr 2024</strong></div>\n<br/>Learn how to use computer vision to analyze wait times and optimize processes. This tutorial covers object detection, tracking, and calculating time spent in designated zones. Use these techniques to improve customer experience in retail, traffic management, or other scenarios.</p>\n\n<br/>\n\n<p align="left">\n<a href="https://youtu.be/uWP6UjDeZvY" title="Speed Estimation & Vehicle Tracking | Computer Vision | Open Source"><img src="https://github.com/SkalskiP/SkalskiP/assets/26109316/61a444c8-b135-48ce-b979-2a5ab47c5a91" alt="Speed Estimation & Vehicle Tracking | Computer Vision | Open Source" width="300px" align="left" /></a>\n<a href="https://youtu.be/uWP6UjDeZvY" title="Speed Estimation & Vehicle Tracking | Computer Vision | Open Source"><strong>Speed Estimation & Vehicle Tracking | Computer Vision | Open Source</strong></a>\n<div><strong>Created: 11 Jan 2024</strong></div>\n<br/>Learn how to track and estimate the speed of vehicles using YOLO, ByteTrack, and Roboflow Inference. This comprehensive tutorial covers object detection, multi-object tracking, filtering detections, perspective transformation, speed estimation, visualization improvements, and more.</p>\n\n## ðŸ’œ built with supervision\n\nDid you build something cool using supervision? [Let us know!](https://github.com/roboflow/supervision/discussions/categories/built-with-supervision)\n\nhttps://user-images.githubusercontent.com/26109316/207858600-ee862b22-0353-440b-ad85-caa0c4777904.mp4\n\nhttps://github.com/roboflow/supervision/assets/26109316/c9436828-9fbf-4c25-ae8c-60e9c81b3900\n\nhttps://github.com/roboflow/supervision/assets/26109316/3ac6982f-4943-4108-9b7f-51787ef1a69f\n\n## ðŸ“š documentation\n\nVisit our [documentation](https://roboflow.github.io/supervision) page to learn how supervision can help you build computer vision applications faster and more reliably.\n\n## ðŸ† contribution\n\nWe love your input! Please see our [contributing guide](https://github.com/roboflow/supervision/blob/main/CONTRIBUTING.md) to get started. Thank you ðŸ™ to all our contributors!\n\n<p align="center">\n    <a href="https://github.com/roboflow/supervision/graphs/contributors">\n      <img src="https://contrib.rocks/image?repo=roboflow/supervision" />\n    </a>\n</p>\n\n<br>\n\n<div align="center">\n\n<div align="center">\n      <a href="https://youtube.com/roboflow">\n          <img\n            src="https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949634652"\n            width="3%"\n          />\n      </a>\n      <img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"/>\n      <a href="https://roboflow.com">\n          <img\n            src="https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949746649"\n            width="3%"\n          />\n      </a>\n      <img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"/>\n      <a href="https://www.linkedin.com/company/roboflow-ai/">\n          <img\n            src="https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633691"\n            width="3%"\n          />\n      </a>\n      <img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"/>\n      <a href="https://docs.roboflow.com">\n          <img\n            src="https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949634511"\n            width="3%"\n          />\n      </a>\n      <img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"/>\n      <a href="https://discuss.roboflow.com">\n          <img\n            src="https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633584"\n            width="3%"\n          />\n      <img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"/>\n      <a href="https://blog.roboflow.com">\n          <img\n            src="https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633605"\n            width="3%"\n          />\n      </a>\n      </a>\n  </div>\n</div>\n', '{"language":"Python","stars":36118,"forks":3040,"watchers":36118,"open_issues":158,"topics":["classification","coco","computer-vision","deep-learning","hacktoberfest","image-processing","instance-segmentation","low-code","machine-learning","metrics","object-detection","oriented-bounding-box","pascal-voc","python","pytorch","tensorflow","tracking","video-processing","yolo"],"default_branch":"develop","size_kb":2693269,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:roboflow:notebooks","source_url":"https://github.com/roboflow/notebooks"},{"type":"has_code","target_id":"github:roboflow:inference","source_url":"https://github.com/roboflow/inference"},{"type":"has_code","target_id":"github:autodistill:autodistill","source_url":"https://github.com/autodistill/autodistill"},{"type":"has_code","target_id":"github:roboflow:multimodal-maestro","source_url":"https://github.com/roboflow/multimodal-maestro"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"},{"type":"has_code","target_id":"github:roboflow:inference","source_url":"https://github.com/roboflow/inference"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"},{"type":"has_code","target_id":"github:SkalskiP:SkalskiP","source_url":"https://github.com/SkalskiP/SkalskiP"},{"type":"has_code","target_id":"github:SkalskiP:SkalskiP","source_url":"https://github.com/SkalskiP/SkalskiP"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"},{"type":"has_code","target_id":"github:roboflow:supervision","source_url":"https://github.com/roboflow/supervision"}]', NULL, 'MIT', 'approved', 80, '2c00741d8d41ce1143c568cdd0f5aa71', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-roboflow-supervision from https://github.com/roboflow.png
Image converted to WebP: data/images/github-roboflow-supervision.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-fengdu78-Coursera-ML-AndrewNg-Notes', 'github--fengdu78--coursera-ml-andrewng-notes', 'Coursera-ML-AndrewNg-Notes', 'fengdu78', '**æ–¯å¦ç¦å¤§å­¦2014ï¼ˆå´æ©è¾¾ï¼‰æœºå™¨å­¦ä¹ æ•™ç¨‹ä¸­æ–‡ç¬”è®°** è¯¾ç¨‹åœ°å€ï¼š<https://www.coursera.org/course/ml> **ç¬”è®°åœ¨çº¿é˜…è¯»** **Machine Learning**(æœºå™¨å­¦ä¹ )æ˜¯ç ”ç©¶è®¡ç®—æœºæ€Žæ ·æ¨¡æ‹Ÿæˆ–å®žçŽ°äººç±»çš„å­¦ä¹ è¡Œä¸ºï¼Œä»¥èŽ·å–æ–°çš„çŸ¥è¯†æˆ–æŠ€èƒ½ï¼Œé‡æ–°ç»„ç»‡å·²æœ‰çš„çŸ¥è¯†ç»“æž„ä½¿ä¹‹ä¸æ–­æ”¹å–„è‡ªèº«çš„æ€§èƒ½ã€‚å®ƒæ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒï¼Œæ˜¯ä½¿è®¡ç®—æœºå…·æœ‰æ™ºèƒ½çš„æ ¹æœ¬é€”å¾„ï¼Œå…¶åº”ç”¨éåŠäººå·¥æ™ºèƒ½çš„å„ä¸ªé¢†åŸŸï¼Œå®ƒä¸»è¦ä½¿ç”¨å½’çº³ã€ç»¼åˆè€Œä¸æ˜¯æ¼”è¯‘ã€‚åœ¨è¿‡åŽ»çš„åå¹´ä¸­ï¼Œæœºå™¨å­¦ä¹ å¸®åŠ©æˆ‘ä»¬è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œæœ‰æ•ˆçš„è¯­éŸ³è¯†åˆ«ï¼Œæœ‰æ•ˆçš„ç½‘ç»œæœç´¢ï¼Œå¹¶æžå¤§åœ°æé«˜äº†äººç±»åŸºå› ç»„çš„è®¤è¯†ã€‚æœºå™¨å­¦ä¹ æ˜¯å½“ä»Šéžå¸¸æ™®éï¼Œä½ å¯èƒ½ä¼šä½¿ç”¨è¿™ä¸€å¤©å‡ åå€è€Œä¸è‡ªçŸ¥ã€‚å¾ˆå¤šç ”ç©¶è€…ä¹Ÿè®¤ä¸ºè¿™æ˜¯æœ€å¥½çš„äººå·¥æ™ºèƒ½çš„å–å¾—æ–¹å¼ã€‚åœ¨æœ¬è¯¾ä¸­ï¼Œæ‚¨å°†å­¦ä¹ æœ€æœ‰æ•ˆçš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå¹¶èŽ·å¾—å®žè·µï¼Œè®©å®ƒä»¬ä¸ºè‡ªå·±çš„å·¥ä½œã€‚æ›´é‡è¦çš„æ˜¯ï¼Œä½ ä¼šä¸ä»…å¾—åˆ°ç†è®ºåŸºç¡€çš„å­¦ä¹ ï¼Œè€Œä¸”èŽ·å¾—é‚£äº›éœ€è¦å¿«é€Ÿå’Œå¼ºå¤§çš„åº”ç”¨æŠ€æœ¯è§£å†³é—®é¢˜çš„å®žç”¨æŠ€æœ¯ã€‚æœ€åŽï¼Œä½ ä¼šå­¦åˆ°ä¸€äº›ç¡…è°·åˆ©ç”¨æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½çš„æœ€ä½³å®žè·µåˆ›æ–°ã€‚ æœ¬è¯¾ç¨‹æä¾›äº†ä¸€ä¸ªå¹¿æ³›çš„ä»‹ç»æœºå™¨å­¦ä¹ ã€æ•°æ®æŒ–æŽ˜ã€ç»Ÿè®¡æ¨¡å¼è¯†åˆ«çš„è¯¾ç¨‹ã€‚ä¸»é¢˜åŒ…æ‹¬ï¼š ï¼ˆä¸€ï¼‰ç›‘ç£å­¦ä¹ ï¼ˆå‚æ•°/éžå‚æ•°ç®—æ³•ï¼Œ...', '["coursera","machine-learning","html"]', 'other', 35796, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '**æ–¯å¦ç¦å¤§å­¦2014ï¼ˆå´æ©è¾¾ï¼‰æœºå™¨å­¦ä¹ æ•™ç¨‹ä¸­æ–‡ç¬”è®°**\n\nè¯¾ç¨‹åœ°å€ï¼š<https://www.coursera.org/course/ml>\n\n[**ç¬”è®°åœ¨çº¿é˜…è¯»**](http://www.ai-start.com/ml2014)\n\n**Machine Learning**(æœºå™¨å­¦ä¹ )æ˜¯ç ”ç©¶è®¡ç®—æœºæ€Žæ ·æ¨¡æ‹Ÿæˆ–å®žçŽ°äººç±»çš„å­¦ä¹ è¡Œä¸ºï¼Œä»¥èŽ·å–æ–°çš„çŸ¥è¯†æˆ–æŠ€èƒ½ï¼Œé‡æ–°ç»„ç»‡å·²æœ‰çš„çŸ¥è¯†ç»“æž„ä½¿ä¹‹ä¸æ–­æ”¹å–„è‡ªèº«çš„æ€§èƒ½ã€‚å®ƒæ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒï¼Œæ˜¯ä½¿è®¡ç®—æœºå…·æœ‰æ™ºèƒ½çš„æ ¹æœ¬é€”å¾„ï¼Œå…¶åº”ç”¨éåŠäººå·¥æ™ºèƒ½çš„å„ä¸ªé¢†åŸŸï¼Œå®ƒä¸»è¦ä½¿ç”¨å½’çº³ã€ç»¼åˆè€Œä¸æ˜¯æ¼”è¯‘ã€‚åœ¨è¿‡åŽ»çš„åå¹´ä¸­ï¼Œæœºå™¨å­¦ä¹ å¸®åŠ©æˆ‘ä»¬è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œæœ‰æ•ˆçš„è¯­éŸ³è¯†åˆ«ï¼Œæœ‰æ•ˆçš„ç½‘ç»œæœç´¢ï¼Œå¹¶æžå¤§åœ°æé«˜äº†äººç±»åŸºå› ç»„çš„è®¤è¯†ã€‚æœºå™¨å­¦ä¹ æ˜¯å½“ä»Šéžå¸¸æ™®éï¼Œä½ å¯èƒ½ä¼šä½¿ç”¨è¿™ä¸€å¤©å‡ åå€è€Œä¸è‡ªçŸ¥ã€‚å¾ˆå¤šç ”ç©¶è€…ä¹Ÿè®¤ä¸ºè¿™æ˜¯æœ€å¥½çš„äººå·¥æ™ºèƒ½çš„å–å¾—æ–¹å¼ã€‚åœ¨æœ¬è¯¾ä¸­ï¼Œæ‚¨å°†å­¦ä¹ æœ€æœ‰æ•ˆçš„æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå¹¶èŽ·å¾—å®žè·µï¼Œè®©å®ƒä»¬ä¸ºè‡ªå·±çš„å·¥ä½œã€‚æ›´é‡è¦çš„æ˜¯ï¼Œä½ ä¼šä¸ä»…å¾—åˆ°ç†è®ºåŸºç¡€çš„å­¦ä¹ ï¼Œè€Œä¸”èŽ·å¾—é‚£äº›éœ€è¦å¿«é€Ÿå’Œå¼ºå¤§çš„åº”ç”¨æŠ€æœ¯è§£å†³é—®é¢˜çš„å®žç”¨æŠ€æœ¯ã€‚æœ€åŽï¼Œä½ ä¼šå­¦åˆ°ä¸€äº›ç¡…è°·åˆ©ç”¨æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½çš„æœ€ä½³å®žè·µåˆ›æ–°ã€‚\n\næœ¬è¯¾ç¨‹æä¾›äº†ä¸€ä¸ªå¹¿æ³›çš„ä»‹ç»æœºå™¨å­¦ä¹ ã€æ•°æ®æŒ–æŽ˜ã€ç»Ÿè®¡æ¨¡å¼è¯†åˆ«çš„è¯¾ç¨‹ã€‚ä¸»é¢˜åŒ…æ‹¬ï¼š\n\nï¼ˆä¸€ï¼‰ç›‘ç£å­¦ä¹ ï¼ˆå‚æ•°/éžå‚æ•°ç®—æ³•ï¼Œæ”¯æŒå‘é‡æœºï¼Œæ ¸å‡½æ•°ï¼Œç¥žç»ç½‘ç»œï¼‰ã€‚\n\nï¼ˆäºŒï¼‰æ— ç›‘ç£å­¦ä¹ ï¼ˆèšç±»ï¼Œé™ç»´ï¼ŒæŽ¨èç³»ç»Ÿï¼Œæ·±å…¥å­¦ä¹ æŽ¨èï¼‰ã€‚\n\nï¼ˆä¸‰ï¼‰åœ¨æœºå™¨å­¦ä¹ çš„æœ€ä½³å®žè·µï¼ˆåå·®/æ–¹å·®ç†è®ºï¼›åœ¨æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½åˆ›æ–°è¿‡ç¨‹ï¼‰ã€‚æœ¬è¯¾ç¨‹è¿˜å°†ä½¿ç”¨å¤§é‡çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œæ‚¨è¿˜å°†å­¦ä¹ å¦‚ä½•è¿ç”¨å­¦ä¹ ç®—æ³•æž„å»ºæ™ºèƒ½æœºå™¨äººï¼ˆæ„ŸçŸ¥ï¼ŒæŽ§åˆ¶ï¼‰ï¼Œæ–‡æœ¬çš„ç†è§£ï¼ˆ**Web**æœç´¢ï¼Œååžƒåœ¾é‚®ä»¶ï¼‰ï¼Œè®¡ç®—æœºè§†è§‰ï¼ŒåŒ»ç–—ä¿¡æ¯ï¼ŒéŸ³é¢‘ï¼Œæ•°æ®æŒ–æŽ˜ï¼Œå’Œå…¶ä»–é¢†åŸŸã€‚\n\næœ¬è¯¾ç¨‹éœ€è¦10å‘¨å…±18èŠ‚è¯¾ï¼Œç›¸å¯¹ä»¥å‰çš„æœºå™¨å­¦ä¹ è§†é¢‘ï¼Œè¿™ä¸ªè§†é¢‘æ›´åŠ æ¸…æ™°ï¼Œè€Œä¸”æ¯è¯¾éƒ½æœ‰**ppt**è¯¾ä»¶ï¼ŒæŽ¨èå­¦ä¹ ã€‚\n\næœ¬äºº2014å¹´ä¸‹åŠå¹´å¼€å§‹ç¿»è¯‘æœ¬è¯¾ç¨‹å­—å¹•ï¼Œå¹¶å†™äº†è¯¾ç¨‹çš„ä¸­æ–‡ç¬”è®°ã€‚ç¬”è®°è¢«ä¸‹è½½äº†å‡ ä¸‡æ¬¡ï¼Œåº”è¯¥å¸®åŠ©äº†ä¸å°‘äººï¼Œä¹Ÿæœ‰å¾ˆå¤šäººä¸€ç›´åœ¨å¸®åŠ©æˆ‘ï¼ŒçŽ°åœ¨æˆ‘æŠŠç¬”è®°çš„**word**åŽŸç¨¿å’Œ**markdown**åŽŸç¨¿åˆ†äº«ç»™å¤§å®¶ã€‚\n\n**markdown**çš„ç¬”è®°å’Œè¯¾ç¨‹ä¸­è‹±æ–‡å­—å¹•æˆ‘å°†æ”¾åœ¨**github**ï¼Œå¸Œæœ›å¤§å®¶èƒ½ç»§ç»­å®Œå–„ã€‚ä¸ºæ–¹ä¾¿æ•°å­¦å…¬å¼çš„åœ¨çº¿æ˜¾ç¤ºï¼Œåœ¨çº¿è§‚çœ‹çš„æ˜¯**html**æ–‡ä»¶ï¼Œå…¬å¼å·²ç»è¢«è½¬ä¸ºå›¾ç‰‡ï¼Œå…¬å¼æºç åœ¨**markdown**æ–‡ä»¶ã€‚\n\n**æœ€åŽæƒ³å¯¹å„ä½æœ‹å‹è¯´ï¼š**\n**èµ äººçŽ«ç‘°ï¼Œæ‰‹æœ‰ä½™é¦™ï¼**\n**åœ¨äººå·¥æ™ºèƒ½çš„é“è·¯ä¸Šï¼Œä½ ä¸æ˜¯ä¸€ä¸ªäººåœ¨æˆ˜æ–—ï¼**\n\né»„æµ·å¹¿\n\n2018-3-26 å¤œ\n\nå¾®ä¿¡å…¬ä¼—å·ï¼šæœºå™¨å­¦ä¹ åˆå­¦è€… ![gongzhong](images/gongzhong.jpg)\n[æˆ‘çš„çŸ¥ä¹Ž](https://www.zhihu.com/people/fengdu78/activities)\n\nå‚è€ƒï¼š\n\n1. https://www.coursera.org/course/ml æœºå™¨å­¦ä¹ å…¬å¼€è¯¾\n2. https://mooc.guokr.com/note/12/ [å°å°äºº_V](https://mooc.guokr.com/user/2133483357/) çš„ä¸ªäººç¬”è®°\n\n3. ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹æŽèˆª   \n4. ã€Šæœºå™¨å­¦ä¹ è¯¾ã€‹é‚¹åš\n\nå¤‡æ³¨ï¼šå´æ©è¾¾è€å¸ˆçš„æ·±åº¦å­¦ä¹ è¯¾ï¼ˆdeepLearning.aiï¼‰çš„ç¬”è®°åœ°å€ï¼šhttps://github.com/fengdu78/deeplearning_ai_books\n\n-----------------------\n\næ–‡ä»¶å¤¹è¯´æ˜Žï¼š\n\n**docx**ï¼šç¬”è®°çš„**word**ç‰ˆæœ¬\n\n**markdown**ï¼šç¬”è®°çš„**markdown**ç‰ˆæœ¬\n\n**html**ï¼šç¬”è®°çš„**html**ç‰ˆæœ¬\n\n**images**ï¼šç¬”è®°çš„å›¾ç‰‡\n\n**ppt**ï¼šè¯¾ç¨‹çš„åŽŸç‰ˆè¯¾ä»¶\n\n**srt**ï¼šè¯¾ç¨‹çš„ä¸­è‹±æ–‡å­—å¹•ï¼ˆ**mp4**æ–‡ä»¶éœ€è¦åœ¨ç™¾åº¦äº‘ä¸‹è½½ï¼Œå¤§å®¶å¯ä»¥ç”¨è®°äº‹æœ¬æˆ–è€…å­—å¹•ç¼–è¾‘è½¯ä»¶æ¥ç¼–è¾‘å­—å¹•ï¼Œå…±åŒå®Œå–„ã€‚\n\nç™¾åº¦äº‘é“¾æŽ¥ï¼šhttps://pan.baidu.com/s/1h8QjqBlOm0Exh7orm9teMQ å¯†ç ï¼šd3weï¼Œä¸‹è½½åŽè§£åŽ‹ï¼‰\n\n**code**ï¼šè¯¾ç¨‹çš„**python**ä»£ç \n\næœºå™¨å­¦ä¹ è¯¾ç¨‹è§†é¢‘ï¼šhttps://www.bilibili.com/video/BV1W34y1i7xK\n\n[ç¬”è®°åœ¨çº¿é˜…è¯»](http://www.ai-start.com/ml2014)\n\nç¬”è®°pdfç‰ˆæœ¬ä¸‹è½½ ï¼šè§**Github**æ ¹ç›®å½•ã€‚\n\næœºå™¨å­¦ä¹ qqç¾¤ï¼š955171419ï¼ˆæˆ‘ä»¬æœ‰13ä¸ªç¾¤ï¼ŒåŠ è¿‡ä¸€ä¸ªå°±ä¸éœ€è¦åŠ äº†ï¼‰\n\n-----------------------\n\n# æœºå™¨å­¦ä¹ æ•™ç¨‹ä¸­æ–‡ç¬”è®°ç›®å½•\n\n- [ç¬¬ä¸€å‘¨](markdown/week1.md)\n\nä¸€ã€ å¼•è¨€(**Introduction**) \n\n1.1 æ¬¢è¿Ž \n\n1.2 æœºå™¨å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ \n\n1.3 ç›‘ç£å­¦ä¹  \n\n1.4 æ— ç›‘ç£å­¦ä¹  \n\näºŒã€å•å˜é‡çº¿æ€§å›žå½’(**Linear Regression with One Variable**) \n\n2.1 æ¨¡åž‹è¡¨ç¤º \n\n2.2 ä»£ä»·å‡½æ•° \n\n2.3 ä»£ä»·å‡½æ•°çš„ç›´è§‚ç†è§£I \n\n2.4 ä»£ä»·å‡½æ•°çš„ç›´è§‚ç†è§£II \n\n2.5 æ¢¯åº¦ä¸‹é™ \n\n2.6 æ¢¯åº¦ä¸‹é™çš„ç›´è§‚ç†è§£ \n\n2.7 æ¢¯åº¦ä¸‹é™çš„çº¿æ€§å›žå½’ \n\n2.8 æŽ¥ä¸‹æ¥çš„å†…å®¹ \n\nä¸‰ã€çº¿æ€§ä»£æ•°å›žé¡¾(**Linear Algebra Review**) \n\n3.1 çŸ©é˜µå’Œå‘é‡ \n\n3.2 åŠ æ³•å’Œæ ‡é‡ä¹˜æ³• \n\n3.3 çŸ©é˜µå‘é‡ä¹˜æ³• \n\n3.4 çŸ©é˜µä¹˜æ³• \n\n3.5 çŸ©é˜µä¹˜æ³•çš„æ€§è´¨ \n\n3.6 é€†ã€è½¬ç½®\n\n- [ç¬¬äºŒå‘¨](markdown/week2.md)\n\nå››ã€å¤šå˜é‡çº¿æ€§å›žå½’(**Linear Regression with Multiple Variables**) \n\n4.1 å¤šç»´ç‰¹å¾ \n\n4.2 å¤šå˜é‡æ¢¯åº¦ä¸‹é™ \n\n4.3 æ¢¯åº¦ä¸‹é™æ³•å®žè·µ1-ç‰¹å¾ç¼©æ”¾ \n\n4.4 æ¢¯åº¦ä¸‹é™æ³•å®žè·µ2-å­¦ä¹ çŽ‡ \n\n4.5 ç‰¹å¾å’Œå¤šé¡¹å¼å›žå½’ \n\n4.6 æ­£è§„æ–¹ç¨‹ \n\n4.7 æ­£è§„æ–¹ç¨‹åŠä¸å¯é€†æ€§ï¼ˆé€‰ä¿®ï¼‰ \n\näº”ã€Octaveæ•™ç¨‹(**Octave Tutorial**) \n\n5.1 åŸºæœ¬æ“ä½œ \n\n5.2 ç§»åŠ¨æ•°æ® \n\n5.3 è®¡ç®—æ•°æ® \n\n5.4 ç»˜å›¾æ•°æ® \n\n5.5 æŽ§åˆ¶è¯­å¥ï¼š**for**ï¼Œ**while**ï¼Œ**if**è¯­å¥ \n\n5.6 å‘é‡åŒ– 88\n\n5.7 å·¥ä½œå’Œæäº¤çš„ç¼–ç¨‹ç»ƒä¹  \n\n- [ç¬¬ä¸‰å‘¨](markdown/week3.md)\n\nå…­ã€é€»è¾‘å›žå½’(**Logistic Regression**) \n\n6.1 åˆ†ç±»é—®é¢˜ \n\n6.2 å‡è¯´è¡¨ç¤º \n\n6.3 åˆ¤å®šè¾¹ç•Œ \n\n6.4 ä»£ä»·å‡½æ•° \n\n6.5 ç®€åŒ–çš„æˆæœ¬å‡½æ•°å’Œæ¢¯åº¦ä¸‹é™ \n\n6.6 é«˜çº§ä¼˜åŒ– \n\n6.7 å¤šç±»åˆ«åˆ†ç±»ï¼šä¸€å¯¹å¤š \n\nä¸ƒã€æ­£åˆ™åŒ–(**Regularization**) \n\n7.1 è¿‡æ‹Ÿåˆçš„é—®é¢˜ \n\n7.2 ä»£ä»·å‡½æ•° \n\n7.3 æ­£åˆ™åŒ–çº¿æ€§å›žå½’ \n\n7.4 æ­£åˆ™åŒ–çš„é€»è¾‘å›žå½’æ¨¡åž‹ \n\n- [ç¬¬å››å‘¨](markdown/week4.md)\n\nç¬¬å…«ã€ç¥žç»ç½‘ç»œï¼šè¡¨è¿°(**Neural Networks: Representation**) \n\n8.1 éžçº¿æ€§å‡è®¾ \n\n8.2 ç¥žç»å…ƒå’Œå¤§è„‘ \n\n8.3 æ¨¡åž‹è¡¨ç¤º1 \n\n8.4 æ¨¡åž‹è¡¨ç¤º2 \n\n8.5 æ ·æœ¬å’Œç›´è§‚ç†è§£1 \n\n8.6 æ ·æœ¬å’Œç›´è§‚ç†è§£II \n\n8.7 å¤šç±»åˆ†ç±» \n\n- [ç¬¬äº”å‘¨](markdown/week5.md)\n\nä¹ã€ç¥žç»ç½‘ç»œçš„å­¦ä¹ (**Neural Networks: Learning**) \n\n9.1 ä»£ä»·å‡½æ•° \n\n9.2 åå‘ä¼ æ’­ç®—æ³• \n\n9.3 åå‘ä¼ æ’­ç®—æ³•çš„ç›´è§‚ç†è§£ \n\n9.4 å®žçŽ°æ³¨æ„ï¼šå±•å¼€å‚æ•° \n\n9.5 æ¢¯åº¦æ£€éªŒ \n\n9.6 éšæœºåˆå§‹åŒ– \n\n9.7 ç»¼åˆèµ·æ¥ \n\n9.8 è‡ªä¸»é©¾é©¶ \n\n- [ç¬¬å…­å‘¨](markdown/week6.md)\n\nåã€åº”ç”¨æœºå™¨å­¦ä¹ çš„å»ºè®®(**Advice for Applying Machine Learning**) \n\n10.1 å†³å®šä¸‹ä¸€æ­¥åšä»€ä¹ˆ \n\n10.2 è¯„ä¼°ä¸€ä¸ªå‡è®¾ \n\n10.3 æ¨¡åž‹é€‰æ‹©å’Œäº¤å‰éªŒè¯é›† \n\n10.4 è¯Šæ–­åå·®å’Œæ–¹å·® \n\n10.5 æ­£åˆ™åŒ–å’Œåå·®/æ–¹å·® \n\n10.6 å­¦ä¹ æ›²çº¿ \n\n10.7 å†³å®šä¸‹ä¸€æ­¥åšä»€ä¹ˆ \n\nåä¸€ã€æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„è®¾è®¡(**Machine Learning System Design**) \n\n11.1 é¦–å…ˆè¦åšä»€ä¹ˆ \n\n11.2 è¯¯å·®åˆ†æž \n\n11.3 ç±»åæ–œçš„è¯¯å·®åº¦é‡ \n\n11.4 æŸ¥å‡†çŽ‡å’ŒæŸ¥å…¨çŽ‡ä¹‹é—´çš„æƒè¡¡ \n\n11.5 æœºå™¨å­¦ä¹ çš„æ•°æ® \n\n[ç¬¬7å‘¨](markdown/week7.md)\n\nåäºŒã€æ”¯æŒå‘é‡æœº(**Support Vector Machines**) \n\n12.1 ä¼˜åŒ–ç›®æ ‡ \n\n12.2 å¤§è¾¹ç•Œçš„ç›´è§‚ç†è§£ \n\n12.3 æ•°å­¦èƒŒåŽçš„å¤§è¾¹ç•Œåˆ†ç±»ï¼ˆé€‰ä¿®ï¼‰ \n\n12.4 æ ¸å‡½æ•°1 \n\n12.5 æ ¸å‡½æ•°2 \n\n12.6 ä½¿ç”¨æ”¯æŒå‘é‡æœº \n\n- [ç¬¬å…«å‘¨](markdown/week8.md)\n\nåä¸‰ã€èšç±»(**Clustering**) \n\n13.1 æ— ç›‘ç£å­¦ä¹ ï¼šç®€ä»‹ \n\n13.2 K-å‡å€¼ç®—æ³• \n\n13.3 ä¼˜åŒ–ç›®æ ‡ \n\n13.4 éšæœºåˆå§‹åŒ–\n\n13.5 é€‰æ‹©èšç±»æ•° \n\nåå››ã€é™ç»´(**Dimensionality Reduction**) \n\n14.1 åŠ¨æœºä¸€ï¼šæ•°æ®åŽ‹ç¼© \n\n14.2 åŠ¨æœºäºŒï¼šæ•°æ®å¯è§†åŒ– \n\n14.3 ä¸»æˆåˆ†åˆ†æžé—®é¢˜ \n\n14.4 ä¸»æˆåˆ†åˆ†æžç®—æ³• \n\n14.5 é€‰æ‹©ä¸»æˆåˆ†çš„æ•°é‡ \n\n14.6 é‡å»ºçš„åŽ‹ç¼©è¡¨ç¤º \n\n14.7 ä¸»æˆåˆ†åˆ†æžæ³•çš„åº”ç”¨å»ºè®® \n\n- [ç¬¬ä¹å‘¨](markdown/week9.md)\n\nåäº”ã€å¼‚å¸¸æ£€æµ‹(**Anomaly Detection**) \n\n15.1 é—®é¢˜çš„åŠ¨æœº \n\n15.2 é«˜æ–¯åˆ†å¸ƒ \n\n15.3 ç®—æ³• \n\n15.4 å¼€å‘å’Œè¯„ä»·ä¸€ä¸ªå¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ \n\n15.5 å¼‚å¸¸æ£€æµ‹ä¸Žç›‘ç£å­¦ä¹ å¯¹æ¯” \n\n15.6 é€‰æ‹©ç‰¹å¾ \n\n15.7 å¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼ˆé€‰ä¿®ï¼‰ \n\n15.8 ä½¿ç”¨å¤šå…ƒé«˜æ–¯åˆ†å¸ƒè¿›è¡Œå¼‚å¸¸æ£€æµ‹ï¼ˆé€‰ä¿®ï¼‰ \n\nåå…­ã€æŽ¨èç³»ç»Ÿ(**Recommender Systems**) \n\n16.1 é—®é¢˜å½¢å¼åŒ– \n\n16.2 åŸºäºŽå†…å®¹çš„æŽ¨èç³»ç»Ÿ \n\n16.3 ååŒè¿‡æ»¤ \n\n16.4 ååŒè¿‡æ»¤ç®—æ³• \n\n16.5 å‘é‡åŒ–ï¼šä½Žç§©çŸ©é˜µåˆ†è§£ \n\n16.6 æŽ¨è¡Œå·¥ä½œä¸Šçš„ç»†èŠ‚ï¼šå‡å€¼å½’ä¸€åŒ– \n\n- [ç¬¬åå‘¨](markdown/week10.md)\n\nåä¸ƒã€å¤§è§„æ¨¡æœºå™¨å­¦ä¹ (**Large Scale Machine Learning**) \n\n17.1 å¤§åž‹æ•°æ®é›†çš„å­¦ä¹  \n\n17.2 éšæœºæ¢¯åº¦ä¸‹é™æ³• \n\n17.3 å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ \n\n17.4 éšæœºæ¢¯åº¦ä¸‹é™æ”¶æ•› \n\n17.5 åœ¨çº¿å­¦ä¹  \n\n17.6 æ˜ å°„åŒ–ç®€å’Œæ•°æ®å¹¶è¡Œ \n\nåå…«ã€åº”ç”¨å®žä¾‹ï¼šå›¾ç‰‡æ–‡å­—è¯†åˆ«(**Application Example: Photo OCR**) \n\n18.1 é—®é¢˜æè¿°å’Œæµç¨‹å›¾\n\n18.2 æ»‘åŠ¨çª—å£ \n\n18.3 èŽ·å–å¤§é‡æ•°æ®å’Œäººå·¥æ•°æ® \n\n18.4 ä¸Šé™åˆ†æžï¼šå“ªéƒ¨åˆ†ç®¡é“çš„æŽ¥ä¸‹åŽ»åš \n\nåä¹ã€æ€»ç»“(**Conclusion**) \n\n19.1 æ€»ç»“å’Œè‡´è°¢ \n\n------\n\n\n\n**æœºå™¨å­¦ä¹ qqç¾¤ï¼š955171419ï¼ˆæˆ‘ä»¬æœ‰13ä¸ªç¾¤ï¼ŒåŠ è¿‡ä¸€ä¸ªå°±ä¸éœ€è¦åŠ äº†ï¼‰** \n\n', '{"language":"HTML","stars":35796,"forks":11091,"watchers":35796,"open_issues":68,"topics":["coursera","machine-learning"],"default_branch":"master","size_kb":662586,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:fengdu78:deeplearning_ai_books","source_url":"https://github.com/fengdu78/deeplearning_ai_books"}]', NULL, NULL, 'pending', 55, '44dcf2191c08d2352d9f451a89d48c2f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-fengdu78-Coursera-ML-AndrewNg-Notes from https://github.com/fengdu78.png
Image converted to WebP: data/images/github-fengdu78-Coursera-ML-AndrewNg-Notes.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-BVLC-caffe', 'github--bvlc--caffe', 'caffe', 'BVLC', 'Caffe is a deep learning framework made with expression, speed, and modularity in mind. It is developed by Berkeley AI Research (BAIR)/The Berkeley Vision and Learning Center (BVLC) and community contributors. Check out the project site for all the details like - DIY Deep Learning for Vision with Caffe - Tutorial Documentation - BAIR reference models and the community model zoo - Installation instructions and step-by-step examples. - Intel Caffe (Optimized for CPU and support for multi-node),...', '["deep-learning","machine-learning","vision","c++"]', 'other', 34763, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/BVLC/caffe","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Caffe\n\n[![Build Status](https://travis-ci.org/BVLC/caffe.svg?branch=master)](https://travis-ci.org/BVLC/caffe)\n[![License](https://img.shields.io/badge/license-BSD-blue.svg)](LICENSE)\n\nCaffe is a deep learning framework made with expression, speed, and modularity in mind.\nIt is developed by Berkeley AI Research ([BAIR](http://bair.berkeley.edu))/The Berkeley Vision and Learning Center (BVLC) and community contributors.\n\nCheck out the [project site](http://caffe.berkeleyvision.org) for all the details like\n\n- [DIY Deep Learning for Vision with Caffe](https://docs.google.com/presentation/d/1UeKXVgRvvxg9OUdh_UiC5G71UMscNPlvArsWER41PsU/edit#slide=id.p)\n- [Tutorial Documentation](http://caffe.berkeleyvision.org/tutorial/)\n- [BAIR reference models](http://caffe.berkeleyvision.org/model_zoo.html) and the [community model zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo)\n- [Installation instructions](http://caffe.berkeleyvision.org/installation.html)\n\nand step-by-step examples.\n\n## Custom distributions\n\n - [Intel Caffe](https://github.com/BVLC/caffe/tree/intel) (Optimized for CPU and support for multi-node), in particular IntelÂ® Xeon processors.\n- [OpenCL Caffe](https://github.com/BVLC/caffe/tree/opencl) e.g. for AMD or Intel devices.\n- [Windows Caffe](https://github.com/BVLC/caffe/tree/windows)\n\n## Community\n\n[![Join the chat at https://gitter.im/BVLC/caffe](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/BVLC/caffe?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nPlease join the [caffe-users group](https://groups.google.com/forum/#!forum/caffe-users) or [gitter chat](https://gitter.im/BVLC/caffe) to ask questions and talk about methods and models.\nFramework development discussions and thorough bug reports are collected on [Issues](https://github.com/BVLC/caffe/issues).\n\nHappy brewing!\n\n## License and Citation\n\nCaffe is released under the [BSD 2-Clause license](https://github.com/BVLC/caffe/blob/master/LICENSE).\nThe BAIR/BVLC reference models are released for unrestricted use.\n\nPlease cite Caffe in your publications if it helps your research:\n\n    @article{jia2014caffe,\n      Author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},\n      Journal = {arXiv preprint arXiv:1408.5093},\n      Title = {Caffe: Convolutional Architecture for Fast Feature Embedding},\n      Year = {2014}\n    }\n', '{"language":"C++","stars":34763,"forks":18589,"watchers":34763,"open_issues":1177,"topics":["deep-learning","machine-learning","vision"],"default_branch":"master","size_kb":76170,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"},{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"},{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"},{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"},{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"},{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"}]', NULL, 'NOASSERTION', 'approved', 65, 'b66af93ac0b501bf629634a24af30603', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-BVLC-caffe from https://github.com/BVLC.png
Image converted to WebP: data/images/github-BVLC-caffe.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-paperless-ngx-paperless-ngx', 'github--paperless-ngx--paperless-ngx', 'paperless-ngx', 'paperless-ngx', '<p align="center"> <picture> <source media="(prefers-color-scheme: dark)" srcset="https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png" width="50%"> <source media="(prefers-color-scheme: light)" srcset="https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png" width="50%"> <img src="https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%2...', '["angular","archiving","django","dms","document-management","document-management-system","hacktoberfest","machine-learning","ocr","optical-character-recognition","pdf","python"]', 'other', 34730, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/paperless-ngx/paperless-ngx","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '[![ci](https://github.com/paperless-ngx/paperless-ngx/workflows/ci/badge.svg)](https://github.com/paperless-ngx/paperless-ngx/actions)\n[![Crowdin](https://badges.crowdin.net/paperless-ngx/localized.svg)](https://crowdin.com/project/paperless-ngx)\n[![Documentation Status](https://img.shields.io/github/deployments/paperless-ngx/paperless-ngx/github-pages?label=docs)](https://docs.paperless-ngx.com)\n[![codecov](https://codecov.io/gh/paperless-ngx/paperless-ngx/branch/main/graph/badge.svg?token=VK6OUPJ3TY)](https://codecov.io/gh/paperless-ngx/paperless-ngx)\n[![Chat on Matrix](https://matrix.to/img/matrix-badge.svg)](https://matrix.to/#/%23paperlessngx%3Amatrix.org)\n[![demo](https://cronitor.io/badges/ve7ItY/production/W5E_B9jkelG9ZbDiNHUPQEVH3MY.svg)](https://demo.paperless-ngx.com)\n\n<p align="center">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://github.com/paperless-ngx/paperless-ngx/blob/main/resources/logo/web/png/White%20logo%20-%20no%20background.png" width="50%">\n    <source media="(prefers-color-scheme: light)" srcset="https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png" width="50%">\n    <img src="https://github.com/paperless-ngx/paperless-ngx/raw/main/resources/logo/web/png/Black%20logo%20-%20no%20background.png" width="50%">\n  </picture>\n</p>\n\n<!-- omit in toc -->\n\n# Paperless-ngx\n\nPaperless-ngx is a document management system that transforms your physical documents into a searchable online archive so you can keep, well, _less paper_.\n\nPaperless-ngx is the official successor to the original [Paperless](https://github.com/the-paperless-project/paperless) & [Paperless-ng](https://github.com/jonaswinkler/paperless-ng) projects and is designed to distribute the responsibility of advancing and supporting the project among a team of people. [Consider joining us!](#community-support)\n\nThanks to the generous folks at [DigitalOcean](https://m.do.co/c/8d70b916d462), a demo is available at [demo.paperless-ngx.com](https://demo.paperless-ngx.com) using login `demo` / `demo`. _Note: demo content is reset frequently and confidential information should not be uploaded._\n\n- [Features](#features)\n- [Getting started](#getting-started)\n- [Contributing](#contributing)\n  - [Community Support](#community-support)\n  - [Translation](#translation)\n  - [Feature Requests](#feature-requests)\n  - [Bugs](#bugs)\n- [Related Projects](#related-projects)\n- [Important Note](#important-note)\n\n<p align="right">This project is supported by:<br/>\n  <a href="https://m.do.co/c/8d70b916d462" style="padding-top: 4px; display: block;">\n    <picture>\n      <source media="(prefers-color-scheme: dark)" srcset="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_white.svg" width="140px">\n      <source media="(prefers-color-scheme: light)" srcset="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_blue.svg" width="140px">\n      <img src="https://opensource.nyc3.cdn.digitaloceanspaces.com/attribution/assets/SVG/DO_Logo_horizontal_black_.svg" width="140px">\n    </picture>\n  </a>\n</p>\n\n# Features\n\n<picture>\n  <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards-dark.png">\n  <source media="(prefers-color-scheme: light)" srcset="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png">\n  <img src="https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/docs/assets/screenshots/documents-smallcards.png">\n</picture>\n\nA full list of [features](https://docs.paperless-ngx.com/#features) and [screenshots](https://docs.paperless-ngx.com/#screenshots) are available in the [documentation](https://docs.paperless-ngx.com/).\n\n# Getting started\n\nThe easiest way to deploy paperless is `docker compose`. The files in the [`/docker/compose` directory](https://github.com/paperless-ngx/paperless-ngx/tree/main/docker/compose) are configured to pull the image from the GitHub container registry.\n\nIf you''d like to jump right in, you can configure a `docker compose` environment with our install script:\n\n```bash\nbash -c "$(curl -L https://raw.githubusercontent.com/paperless-ngx/paperless-ngx/main/install-paperless-ngx.sh)"\n```\n\nMore details and step-by-step guides for alternative installation methods can be found in [the documentation](https://docs.paperless-ngx.com/setup/#installation).\n\nMigrating from Paperless-ng is easy, just drop in the new docker image! See the [documentation on migrating](https://docs.paperless-ngx.com/setup/#migrating-to-paperless-ngx) for more details.\n\n<!-- omit in toc -->\n\n### Documentation\n\nThe documentation for Paperless-ngx is available at [https://docs.paperless-ngx.com](https://docs.paperless-ngx.com/).\n\n# Contributing\n\nIf you feel like contributing to the project, please do! Bug fixes, enhancements, visual fixes etc. are always welcome. If you want to implement something big: Please start a discussion about that! The [documentation](https://docs.paperless-ngx.com/development/) has some basic information on how to get started.\n\n## Community Support\n\nPeople interested in continuing the work on paperless-ngx are encouraged to reach out here on github and in the [Matrix Room](https://matrix.to/#/#paperless:matrix.org). If you would like to contribute to the project on an ongoing basis there are multiple [teams](https://github.com/orgs/paperless-ngx/people) (frontend, ci/cd, etc) that could use your help so please reach out!\n\n## Translation\n\nPaperless-ngx is available in many languages that are coordinated on Crowdin. If you want to help out by translating paperless-ngx into your language, please head over to https://crowdin.com/project/paperless-ngx, and thank you! More details can be found in [CONTRIBUTING.md](https://github.com/paperless-ngx/paperless-ngx/blob/main/CONTRIBUTING.md#translating-paperless-ngx).\n\n## Feature Requests\n\nFeature requests can be submitted via [GitHub Discussions](https://github.com/paperless-ngx/paperless-ngx/discussions/categories/feature-requests), you can search for existing ideas, add your own and vote for the ones you care about.\n\n## Bugs\n\nFor bugs please [open an issue](https://github.com/paperless-ngx/paperless-ngx/issues) or [start a discussion](https://github.com/paperless-ngx/paperless-ngx/discussions) if you have questions.\n\n# Related Projects\n\nPlease see [the wiki](https://github.com/paperless-ngx/paperless-ngx/wiki/Related-Projects) for a user-maintained list of related projects and software that is compatible with Paperless-ngx.\n\n# Important Note\n\n> Document scanners are typically used to scan sensitive documents like your social insurance number, tax records, invoices, etc. **Paperless-ngx should never be run on an untrusted host** because information is stored in clear text without encryption. No guarantees are made regarding security (but we do try!) and you use the app at your own risk.\n> **The safest way to run Paperless-ngx is on a local server in your own home with backups in place**.\n', '{"language":"Python","stars":34730,"forks":2181,"watchers":34730,"open_issues":17,"topics":["angular","archiving","django","dms","document-management","document-management-system","hacktoberfest","machine-learning","ocr","optical-character-recognition","pdf"],"default_branch":"dev","size_kb":174262,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:the-paperless-project:paperless","source_url":"https://github.com/the-paperless-project/paperless"},{"type":"has_code","target_id":"github:jonaswinkler:paperless-ng","source_url":"https://github.com/jonaswinkler/paperless-ng"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:orgs:paperless-ngx","source_url":"https://github.com/orgs/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"}]', NULL, 'GPL-3.0', 'approved', 65, '3bd53fb6e18997b74676fd644fa183ca', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-paperless-ngx-paperless-ngx from https://github.com/paperless-ngx.png
Image converted to WebP: data/images/github-paperless-ngx-paperless-ngx.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-qlib', 'github--microsoft--qlib', 'qlib', 'microsoft', 'Recent released features We are excited to announce the release of **RD-Agent**ðŸ“¢, a powerful tool that supports automated factor mining and model optimization in quant investment R&D. RD-Agent is now available on GitHub, and we welcome your starðŸŒŸ! To learn more, please visit our â™¾ï¸Demo page. Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent. We have prepared several demo videos for you: | Scenario | Demo video (Engli...', '["algorithmic-trading","auto-quant","deep-learning","finance","fintech","investment","machine-learning","paper","platform","python","quant","quant-dataset","quant-models","quantitative-finance","quantitative-trading","research","research-paper","stock-data","python"]', 'other', 34378, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/qlib","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '[![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&logoColor=white)](https://pypi.org/project/pyqlib/#files)\n[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)\n[![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)\n[![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)\n[![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)\n[![Documentation Status](https://readthedocs.org/projects/qlib/badge/?version=latest)](https://qlib.readthedocs.io/en/latest/?badge=latest)\n[![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)\n[![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n## :newspaper: **What''s NEW!** &nbsp;   :sparkling_heart: \n\nRecent released features\n\n### Introducing <a href="https://github.com/microsoft/RD-Agent"><img src="docs/_static/img/rdagent_logo.png" alt="RD_Agent" style="height: 2em"></a>: LLM-Based Autonomous Evolving Agents for Industrial Data-Driven R&D\n\nWe are excited to announce the release of **RD-Agent**ðŸ“¢, a powerful tool that supports automated factor mining and model optimization in quant investment R&D.\n\nRD-Agent is now available on [GitHub](https://github.com/microsoft/RD-Agent), and we welcome your starðŸŒŸ!\n\nTo learn more, please visit our [â™¾ï¸Demo page](https://rdagent.azurewebsites.net/). Here, you will find demo videos in both English and Chinese to help you better understand the scenario and usage of RD-Agent.\n\nWe have prepared several demo videos for you:\n| Scenario | Demo video (English) | Demo video (ä¸­æ–‡) |\n| --                      | ------    | ------    |\n| Quant Factor Mining | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/factor_loop?lang=zh) |\n| Quant Factor Mining from reports | [Link](https://rdagent.azurewebsites.net/report_factor?lang=en) | [Link](https://rdagent.azurewebsites.net/report_factor?lang=zh) |\n| Quant Model Optimization | [Link](https://rdagent.azurewebsites.net/model_loop?lang=en) | [Link](https://rdagent.azurewebsites.net/model_loop?lang=zh) |\n\n- ðŸ“ƒ**Paper**: [R&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization](https://arxiv.org/abs/2505.15155)\n- ðŸ‘¾**Code**: https://github.com/microsoft/RD-Agent/\n```BibTeX\n@misc{li2025rdagentquant,\n    title={R\&D-Agent-Quant: A Multi-Agent Framework for Data-Centric Factors and Model Joint Optimization},\n    author={Yuante Li and Xu Yang and Xiao Yang and Minrui Xu and Xisen Wang and Weiqing Liu and Jiang Bian},\n    year={2025},\n    eprint={2505.15155},\n    archivePrefix={arXiv},\n    primaryClass={cs.AI}\n}\n```\n![image](https://github.com/user-attachments/assets/3198bc10-47ba-4ee0-8a8e-46d5ce44f45d)\n\n***\n\n| Feature | Status |\n| --                      | ------    |\n| [R&D-Agent-Quant](https://arxiv.org/abs/2505.15155) Published | Apply R&D-Agent to Qlib for quant trading | \n| BPQP for End-to-end learning | ðŸ“ˆComing soon!([Under review](https://github.com/microsoft/qlib/pull/1863)) |\n| ðŸ”¥LLM-driven Auto Quant FactoryðŸ”¥ | ðŸš€ Released in [â™¾ï¸RD-Agent](https://github.com/microsoft/RD-Agent) on Aug 8, 2024 |\n| KRNN and Sandwich models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1414/) on May 26, 2023 |\n| Release Qlib v0.9.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.9.0) on Dec 9, 2022 |\n| RL Learning Framework | :hammer: :chart_with_upwards_trend: Released on Nov 10, 2022. [#1332](https://github.com/microsoft/qlib/pull/1332), [#1322](https://github.com/microsoft/qlib/pull/1322), [#1316](https://github.com/microsoft/qlib/pull/1316),[#1299](https://github.com/microsoft/qlib/pull/1299),[#1263](https://github.com/microsoft/qlib/pull/1263), [#1244](https://github.com/microsoft/qlib/pull/1244), [#1169](https://github.com/microsoft/qlib/pull/1169), [#1125](https://github.com/microsoft/qlib/pull/1125), [#1076](https://github.com/microsoft/qlib/pull/1076)|\n| HIST and IGMTF models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1040) on Apr 10, 2022 |\n| Qlib [notebook tutorial](https://github.com/microsoft/qlib/tree/main/examples/tutorial) | ðŸ“– [Released](https://github.com/microsoft/qlib/pull/1037) on Apr 7, 2022 | \n| Ibovespa index data | :rice: [Released](https://github.com/microsoft/qlib/pull/990) on Apr 6, 2022 |\n| Point-in-Time database | :hammer: [Released](https://github.com/microsoft/qlib/pull/343) on Mar 10, 2022 |\n| Arctic Provider Backend & Orderbook data example | :hammer: [Released](https://github.com/microsoft/qlib/pull/744) on Jan 17, 2022 |\n| Meta-Learning-based framework & DDG-DA  | :chart_with_upwards_trend:  :hammer: [Released](https://github.com/microsoft/qlib/pull/743) on Jan 10, 2022 | \n| Planning-based portfolio optimization | :hammer: [Released](https://github.com/microsoft/qlib/pull/754) on Dec 28, 2021 | \n| Release Qlib v0.8.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.8.0) on Dec 8, 2021 |\n| ADD model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/704) on Nov 22, 2021 |\n| ADARNN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/689) on Nov 14, 2021 |\n| TCN  model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/668) on Nov 4, 2021 |\n| Nested Decision Framework | :hammer: [Released](https://github.com/microsoft/qlib/pull/438) on Oct 1, 2021. [Example](https://github.com/microsoft/qlib/blob/main/examples/nested_decision_execution/workflow.py) and [Doc](https://qlib.readthedocs.io/en/latest/component/highfreq.html) |\n| Temporal Routing Adaptor (TRA) | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/531) on July 30, 2021 |\n| Transformer & Localformer | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/508) on July 22, 2021 |\n| Release Qlib v0.7.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.7.0) on July 12, 2021 |\n| TCTS Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/491) on July 1, 2021 |\n| Online serving and automatic model rolling | :hammer:  [Released](https://github.com/microsoft/qlib/pull/290) on May 17, 2021 | \n| DoubleEnsemble Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/286) on Mar 2, 2021 | \n| High-frequency data processing example | :hammer: [Released](https://github.com/microsoft/qlib/pull/257) on Feb 5, 2021  |\n| High-frequency trading example | :chart_with_upwards_trend: [Part of code released](https://github.com/microsoft/qlib/pull/227) on Jan 28, 2021  | \n| High-frequency data(1min) | :rice: [Released](https://github.com/microsoft/qlib/pull/221) on Jan 27, 2021 |\n| Tabnet Model | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/205) on Jan 22, 2021 |\n\nFeatures released before 2021 are not listed here.\n\n<p align="center">\n  <img src="docs/_static/img/logo/1.png" />\n</p>\n\nQlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.\n\nAn increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market''s complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.\n\nIt contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. \nFor more details, please refer to our paper ["Qlib: An AI-oriented Quantitative Investment Platform"](https://arxiv.org/abs/2009.11189).\n\n\n<table>\n  <tbody>\n    <tr>\n      <th>Frameworks, Tutorial, Data & DevOps</th>\n      <th>Main Challenges & Solutions in Quant Research</th>\n    </tr>\n    <tr>\n      <td>\n        <li><a href="#plans"><strong>Plans</strong></a></li>\n        <li><a href="#framework-of-qlib">Framework of Qlib</a></li>\n        <li><a href="#quick-start">Quick Start</a></li>\n          <ul dir="auto">\n            <li type="circle"><a href="#installation">Installation</a> </li>\n            <li type="circle"><a href="#data-preparation">Data Preparation</a></li>\n            <li type="circle"><a href="#auto-quant-research-workflow">Auto Quant Research Workflow</a></li>\n            <li type="circle"><a href="#building-customized-quant-research-workflow-by-code">Building Customized Quant Research Workflow by Code</a></li></ul>\n        <li><a href="#quant-dataset-zoo"><strong>Quant Dataset Zoo</strong></a></li>\n        <li><a href="#learning-framework">Learning Framework</a></li>\n        <li><a href="#more-about-qlib">More About Qlib</a></li>\n        <li><a href="#offline-mode-and-online-mode">Offline Mode and Online Mode</a>\n        <ul>\n          <li type="circle"><a href="#performance-of-qlib-data-server">Performance of Qlib Data Server</a></li></ul>\n        <li><a href="#related-reports">Related Reports</a></li>\n        <li><a href="#contact-us">Contact Us</a></li>\n        <li><a href="#contributing">Contributing</a></li>\n      </td>\n      <td valign="baseline">\n        <li><a href="#main-challenges--solutions-in-quant-research">Main Challenges &amp; Solutions in Quant Research</a>\n          <ul>\n            <li type="circle"><a href="#forecasting-finding-valuable-signalspatterns">Forecasting: Finding Valuable Signals/Patterns</a>\n              <ul>\n                <li type="disc"><a href="#quant-model-paper-zoo"><strong>Quant Model (Paper) Zoo</strong></a>\n                  <ul>\n                    <li type="circle"><a href="#run-a-single-model">Run a Single Model</a></li>\n                    <li type="circle"><a href="#run-multiple-models">Run Multiple Models</a></li>\n                  </ul>\n                </li>\n              </ul>\n            </li>\n          <li type="circle"><a href="#adapting-to-market-dynamics">Adapting to Market Dynamics</a></li>\n          <li type="circle"><a href="#reinforcement-learning-modeling-continuous-decisions">Reinforcement Learning: modeling continuous decisions</a></li>\n          </ul>\n        </li>\n      </td>\n    </tr>\n  </tbody>\n</table>\n\n# Plans\nNew features under development(order by estimated release time).\nYour feedbacks about the features are very important.\n<!-- | Feature                        | Status      | -->\n<!-- | --                      | ------    | -->\n\n# Framework of Qlib\n\n<div style="align: center">\n<img src="docs/_static/img/framework-abstract.jpg" />\n</div>\n\nThe high-level framework of Qlib can be found above(users can find the [detailed framework](https://qlib.readthedocs.io/en/latest/introduction/introduction.html#framework) of Qlib''s design when getting into nitty gritty).\nThe components are designed as loose-coupled modules, and each component could be used stand-alone.\n\nQlib provides a strong infrastructure to support Quant research. [Data](https://qlib.readthedocs.io/en/latest/component/data.html) is always an important part.\nA strong learning framework is designed to support diverse learning paradigms (e.g. [reinforcement learning](https://qlib.readthedocs.io/en/latest/component/rl.html), [supervised learning](https://qlib.readthedocs.io/en/latest/component/workflow.html#model-section)) and patterns at different levels(e.g. [market dynamic modeling](https://qlib.readthedocs.io/en/latest/component/meta.html)).\nBy modeling the market, [trading strategies](https://qlib.readthedocs.io/en/latest/component/strategy.html) will generate trade decisions that will be executed. Multiple trading strategies and executors in different levels or granularities can be [nested to be optimized and run together](https://qlib.readthedocs.io/en/latest/component/highfreq.html).\nAt last, a comprehensive [analysis](https://qlib.readthedocs.io/en/latest/component/report.html) will be provided and the model can be [served online](https://qlib.readthedocs.io/en/latest/component/online.html) in a low cost.\n\n\n# Quick Start\n\nThis quick start guide tries to demonstrate\n1. It''s very easy to build a complete Quant research workflow and try your ideas with _Qlib_.\n2. Though with *public data* and *simple models*, machine learning technologies **work very well** in practical Quant investment.\n\nHere is a quick **[demo](https://terminalizer.com/view/3f24561a4470)** shows how to install ``Qlib``, and run LightGBM with ``qrun``. **But**, please make sure you have already prepared the data following the [instruction](#data-preparation).\n\n\n## Installation\n\nThis table demonstrates the supported Python version of `Qlib`:\n|               | install with pip      | install from source  |        plot        |\n| ------------- |:---------------------:|:--------------------:|:------------------:|\n| Python 3.8    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.9    | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.10   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.11   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n| Python 3.12   | :heavy_check_mark:    | :heavy_check_mark:   | :heavy_check_mark: |\n\n**Note**: \n1. **Conda** is suggested for managing your Python environment. In some cases, using Python outside of a `conda` environment may result in missing header files, causing the installation failure of certain packages.\n2. Please pay attention that installing cython in Python 3.6 will raise some error when installing ``Qlib`` from source. If users use Python 3.6 on their machines, it is recommended to *upgrade* Python to version 3.8 or higher, or use `conda`''s Python to install ``Qlib`` from source.\n\n### Install with pip\nUsers can easily install ``Qlib`` by pip according to the following command.\n\n```bash\n  pip install pyqlib\n```\n\n**Note**: pip will install the latest stable qlib. However, the main branch of qlib is in active development. If you want to test the latest scripts or functions in the main branch. Please install qlib with the methods below.\n\n### Install from source\nAlso, users can install the latest dev version ``Qlib`` by the source code according to the following steps:\n\n* Before installing ``Qlib`` from source, users need to install some dependencies:\n\n  ```bash\n  pip install numpy\n  pip install --upgrade cython\n  ```\n\n* Clone the repository and install ``Qlib`` as follows.\n    ```bash\n    git clone https://github.com/microsoft/qlib.git && cd qlib\n    pip install .  # `pip install -e .[dev]` is recommended for development. check details in docs/developer/code_standard_and_dev_guide.rst\n    ```\n\n**Tips**: If you fail to install `Qlib` or run the examples in your environment,  comparing your steps and the [CI workflow](.github/workflows/test_qlib_from_source.yml) may help you find the problem.\n\n**Tips for Mac**: If you are using Mac with M1, you might encounter issues in building the wheel for LightGBM, which is due to missing dependencies from OpenMP. To solve the problem, install openmp first with ``brew install libomp`` and then run ``pip install .`` to build it successfully. \n\n## Data Preparation\nâ— Due to more restrict data security policy. The official dataset is disabled temporarily. You can try [this data source](https://github.com/chenditc/investment_data/releases) contributed by the community.\nHere is an example to download the latest data.\n```bash\nwget https://github.com/chenditc/investment_data/releases/latest/download/qlib_bin.tar.gz\nmkdir -p ~/.qlib/qlib_data/cn_data\ntar -zxvf qlib_bin.tar.gz -C ~/.qlib/qlib_data/cn_data --strip-components=1\nrm -f qlib_bin.tar.gz\n```\n\nThe official dataset below will resume in short future.\n\n\n----\n\nLoad and prepare data by running the following code:\n\n### Get with module\n  ```bash\n  # get 1d data\n  python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python -m qlib.cli.data qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\n### Get from source\n\n  ```bash\n  # get 1d data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data --region cn\n\n  # get 1min data\n  python scripts/get_data.py qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --region cn --interval 1min\n\n  ```\n\nThis dataset is created by public data collected by [crawler scripts](scripts/data_collector/), which have been released in\nthe same repository.\nUsers could create the same dataset with it. [Description of dataset](https://github.com/microsoft/qlib/tree/main/scripts/data_collector#description-of-dataset)\n\n*Please pay **ATTENTION** that the data is collected from [Yahoo Finance](https://finance.yahoo.com/lookup), and the data might not be perfect.\nWe recommend users to prepare their own data if they have a high-quality dataset. For more information, users can refer to the [related document](https://qlib.readthedocs.io/en/latest/component/data.html#converting-csv-format-into-qlib-format)*.\n\n### Automatic update of daily frequency data (from yahoo finance)\n  > This step is *Optional* if users only want to try their models and strategies on history data.\n  > \n  > It is recommended that users update the data manually once (--trading_date 2021-05-25) and then set it to update automatically.\n  >\n  > **NOTE**: Users can''t incrementally  update data based on the offline data provided by Qlib(some fields are removed to reduce the data size). Users should use [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance) to download Yahoo data from scratch and then incrementally update it.\n  > \n  > For more information, please refer to: [yahoo collector](https://github.com/microsoft/qlib/tree/main/scripts/data_collector/yahoo#automatic-update-of-daily-frequency-datafrom-yahoo-finance)\n\n  * Automatic update of data to the "qlib" directory each trading day(Linux)\n      * use *crontab*: `crontab -e`\n      * set up timed tasks:\n\n        ```\n        * * * * 1-5 python <script path> update_data_to_bin --qlib_data_1d_dir <user data dir>\n        ```\n        * **script path**: *scripts/data_collector/yahoo/collector.py*\n\n  * Manual update of data\n      ```\n      python scripts/data_collector/yahoo/collector.py update_data_to_bin --qlib_data_1d_dir <user data dir> --trading_date <start date> --end_date <end date>\n      ```\n      * *trading_date*: start of trading day\n      * *end_date*: end of trading day(not included)\n\n### Checking the health of the data\n  * We provide a script to check the health of the data, you can run the following commands to check whether the data is healthy or not.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data\n    ```\n  * Of course, you can also add some parameters to adjust the test results, such as this.\n    ```\n    python scripts/check_data_health.py check_data --qlib_dir ~/.qlib/qlib_data/cn_data --missing_data_num 30055 --large_step_threshold_volume 94485 --large_step_threshold_price 20\n    ```\n  * If you want more information about `check_data_health`, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/component/data.html#checking-the-health-of-the-data).\n\n<!-- \n- Run the initialization code and get stock data:\n\n  ```python\n  import qlib\n  from qlib.data import D\n  from qlib.constant import REG_CN\n\n  # Initialization\n  mount_path = "~/.qlib/qlib_data/cn_data"  # target_dir\n  qlib.init(mount_path=mount_path, region=REG_CN)\n\n  # Get stock data by Qlib\n  # Load trading calendar with the given time range and frequency\n  print(D.calendar(start_time=''2010-01-01'', end_time=''2017-12-31'', freq=''day'')[:2])\n\n  # Parse a given market name into a stockpool config\n  instruments = D.instruments(''csi500'')\n  print(D.list_instruments(instruments=instruments, start_time=''2010-01-01'', end_time=''2017-12-31'', as_list=True)[:6])\n\n  # Load features of certain instruments in given time range\n  instruments = [''SH600000'']\n  fields = [''$close'', ''$volume'', ''Ref($close, 1)'', ''Mean($close, 3)'', ''$high-$low'']\n  print(D.features(instruments, fields, start_time=''2010-01-01'', end_time=''2017-12-31'', freq=''day'').head())\n  ```\n -->\n\n## Docker images\n1. Pulling a docker image from a docker hub repository\n    ```bash\n    docker pull pyqlib/qlib_image_stable:stable\n    ```\n2. Start a new Docker container\n    ```bash\n    docker run -it --name <container name> -v <Mounted local directory>:/app pyqlib/qlib_image_stable:stable\n    ```\n3. At this point you are in the docker environment and can run the qlib scripts. An example:\n    ```bash\n    >>> python scripts/get_data.py qlib_data --name qlib_data_simple --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn\n    >>> python qlib/cli/run.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n4. Exit the container\n    ```bash\n    >>> exit\n    ```\n5. Restart the container\n    ```bash\n    docker start -i -a <container name>\n    ```\n6. Stop the container\n    ```bash\n    docker stop <container name>\n    ```\n7. Delete the container\n    ```bash\n    docker rm <container name>\n    ```\n8. If you want to know more information, please refer to the [documentation](https://qlib.readthedocs.io/en/latest/developer/how_to_build_image.html).\n\n## Auto Quant Research Workflow\nQlib provides a tool named `qrun` to run the whole workflow automatically (including building dataset, training models, backtest and evaluation). You can start an auto quant research workflow and have a graphical reports analysis according to the following steps: \n\n1. Quant Research Workflow: Run  `qrun` with lightgbm workflow config ([workflow_config_lightgbm_Alpha158.yaml](examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml) as following.\n    ```bash\n      cd examples  # Avoid running program under the directory contains `qlib`\n      qrun benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    If users want to use `qrun` under debug mode, please use the following command:\n    ```bash\n    python -m pdb qlib/cli/run.py examples/benchmarks/LightGBM/workflow_config_lightgbm_Alpha158.yaml\n    ```\n    The result of `qrun` is as follows, please refer to [docs](https://qlib.readthedocs.io/en/latest/component/strategy.html#result) for more explanations about the result. \n\n    ```bash\n\n    ''The following are analysis results of the excess return without cost.''\n                           risk\n    mean               0.000708\n    std                0.005626\n    annualized_return  0.178316\n    information_ratio  1.996555\n    max_drawdown      -0.081806\n    ''The following are analysis results of the excess return with cost.''\n                           risk\n    mean               0.000512\n    std                0.005626\n    annualized_return  0.128982\n    information_ratio  1.444287\n    max_drawdown      -0.091078\n    ```\n    Here are detailed documents for `qrun` and [workflow](https://qlib.readthedocs.io/en/latest/component/workflow.html).\n\n2. Graphical Reports Analysis: First, run `python -m pip install .[analysis]` to install the required dependencies. Then run `examples/workflow_by_code.ipynb` with `jupyter notebook` to get graphical reports. \n    - Forecasting signal (model prediction) analysis\n      - Cumulative Return of groups\n      ![Cumulative Return](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_cumulative_return.png)\n      - Return distribution\n      ![long_short](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_long_short.png)\n      - Information Coefficient (IC)\n      ![Information Coefficient](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_IC.png)\n      ![Monthly IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_monthly_IC.png)\n      ![IC](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_NDQ.png)\n      - Auto Correlation of forecasting signal (model prediction)\n      ![Auto Correlation](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/analysis_model_auto_correlation.png)\n\n    - Portfolio analysis\n      - Backtest return\n      ![Report](https://github.com/microsoft/qlib/blob/main/docs/_static/img/analysis/report.png)\n      <!-- \n      - Score IC\n      ![Score IC](docs/_static/img/score_ic.png)\n      - Cumulative Return\n      ![Cumulative Return](docs/_static/img/cumulative_return.png)\n      - Risk Analysis\n      ![Risk Analysis](docs/_static/img/risk_analysis.png)\n      - Rank Label\n      ![Rank Label](docs/_static/img/rank_label.png)\n      -->\n   - [Explanation](https://qlib.readthedocs.io/en/latest/component/report.html) of above results\n\n## Building Customized Quant Research Workflow by Code\nThe automatic workflow may not suit the research workflow of all Quant researchers. To support a flexible Quant research workflow, Qlib also provides a modularized interface to allow researchers to build their own workflow by code. [Here](examples/workflow_by_code.ipynb) is a demo for customized Quant research workflow by code.\n\n# Main Challenges & Solutions in Quant Research\nQuant investment is a very unique scenario with lots of key challenges to be solved.\nCurrently, Qlib provides some solutions for several of them.\n\n## Forecasting: Finding Valuable Signals/Patterns\nAccurate forecasting of the stock price trend is a very important part to construct profitable portfolios.\nHowever, huge amount of data with various formats in the financial market which make it challenging to build forecasting models.\n\nAn increasing number of SOTA Quant research works/papers, which focus on building forecasting models to mine valuable signals/patterns in complex financial data, are released in `Qlib`\n\n\n### [Quant Model (Paper) Zoo](examples/benchmarks)\n\nHere is a list of models built on `Qlib`.\n- [GBDT based on XGBoost (Tianqi Chen, et al. KDD 2016)](examples/benchmarks/XGBoost/)\n- [GBDT based on LightGBM (Guolin Ke, et al. NIPS 2017)](examples/benchmarks/LightGBM/)\n- [GBDT based on Catboost (Liudmila Prokhorenkova, et al. NIPS 2018)](examples/benchmarks/CatBoost/)\n- [MLP based on pytorch](examples/benchmarks/MLP/)\n- [LSTM based on pytorch (Sepp Hochreiter, et al. Neural computation 1997)](examples/benchmarks/LSTM/)\n- [GRU based on pytorch (Kyunghyun Cho, et al. 2014)](examples/benchmarks/GRU/)\n- [ALSTM based on pytorch (Yao Qin, et al. IJCAI 2017)](examples/benchmarks/ALSTM)\n- [GATs based on pytorch (Petar Velickovic, et al. 2017)](examples/benchmarks/GATs/)\n- [SFM based on pytorch (Liheng Zhang, et al. KDD 2017)](examples/benchmarks/SFM/)\n- [TFT based on tensorflow (Bryan Lim, et al. International Journal of Forecasting 2019)](examples/benchmarks/TFT/)\n- [TabNet based on pytorch (Sercan O. Arik, et al. AAAI 2019)](examples/benchmarks/TabNet/)\n- [DoubleEnsemble based on LightGBM (Chuheng Zhang, et al. ICDM 2020)](examples/benchmarks/DoubleEnsemble/)\n- [TCTS based on pytorch (Xueqing Wu, et al. ICML 2021)](examples/benchmarks/TCTS/)\n- [Transformer based on pytorch (Ashish Vaswani, et al. NeurIPS 2017)](examples/benchmarks/Transformer/)\n- [Localformer based on pytorch (Juyong Jiang, et al.)](examples/benchmarks/Localformer/)\n- [TRA based on pytorch (Hengxu, Dong, et al. KDD 2021)](examples/benchmarks/TRA/)\n- [TCN based on pytorch (Shaojie Bai, et al. 2018)](examples/benchmarks/TCN/)\n- [ADARNN based on pytorch (YunTao Du, et al. 2021)](examples/benchmarks/ADARNN/)\n- [ADD based on pytorch (Hongshun Tang, et al.2020)](examples/benchmarks/ADD/)\n- [IGMTF based on pytorch (Wentao Xu, et al.2021)](examples/benchmarks/IGMTF/)\n- [HIST based on pytorch (Wentao Xu, et al.2021)](examples/benchmarks/HIST/)\n- [KRNN based on pytorch](examples/benchmarks/KRNN/)\n- [Sandwich based on pytorch](examples/benchmarks/Sandwich/)\n\nYour PR of new Quant models is highly welcomed.\n\nThe performance of each model on the `Alpha158` and `Alpha360` datasets can be found [here](examples/benchmarks/README.md).\n\n### Run a single model\nAll the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.\n\n`Qlib` provides three different ways to run a single model, users can pick the one that fits their cases best:\n- Users can use the tool `qrun` mentioned above to run a model''s workflow based from a config file.\n- Users can create a `workflow_by_code` python script based on the [one](examples/workflow_by_code.py) listed in the `examples` folder.\n\n- Users can use the script [`run_all_model.py`](examples/run_all_model.py) listed in the `examples` folder to run a model. Here is an example of the specific shell command to be used: `python run_all_model.py run --models=lightgbm`, where the `--models` arguments can take any number of models listed above(the available models can be found  in [benchmarks](examples/benchmarks/)). For more use cases, please refer to the file''s [docstrings](examples/run_all_model.py).\n    - **NOTE**: Each baseline has different environment dependencies, please make sure that your python version aligns with the requirements(e.g. TFT only supports Python 3.6~3.7 due to the limitation of `tensorflow==1.15.0`)\n\n### Run multiple models\n`Qlib` also provides a script [`run_all_model.py`](examples/run_all_model.py) which can run multiple models for several iterations. (**Note**: the script only support *Linux* for now. Other OS will be supported in the future. Besides, it doesn''t support parallel running the same model for multiple times as well, and this will be fixed in the future development too.)\n\nThe script will create a unique virtual environment for each model, and delete the environments after training. Thus, only experiment results such as `IC` and `backtest` results will be generated and stored.\n\nHere is an example of running all the models for 10 iterations:\n```python\npython run_all_model.py run 10\n```\n\nIt also provides the API to run specific models at once. For more use cases, please refer to the file''s [docstrings](examples/run_all_model.py). \n\n### Break change\nIn `pandas`, `group_key` is one of the parameters of the `groupby` method. From version 1.5 to 2.0 of `pandas`, the default value of `group_key` has been changed from `no default` to `True`, which will cause qlib to report an error during operation. So we set `group_key=False`, but it doesn''t guarantee that some programmes will run correctly, including:\n* qlib\examples\rl_order_execution\scripts\gen_training_orders.py\n* qlib\examples\benchmarks\TRA\src\dataset.MTSDatasetH.py\n* qlib\examples\benchmarks\TFT\tft.py\n\n\n\n## [Adapting to Market Dynamics](examples/benchmarks_dynamic)\n\nDue to the non-stationary nature of the environment of the financial market, the data distribution may change in different periods, which makes the performance of models build on training data decays in the future test data.\nSo adapting the forecasting models/strategies to market dynamics is very important to the model/strategies'' performance.\n\nHere is a list of solutions built on `Qlib`.\n- [Rolling Retraining](examples/benchmarks_dynamic/baseline/)\n- [DDG-DA on pytorch (Wendi, et al. AAAI 2022)](examples/benchmarks_dynamic/DDG-DA/)\n\n##  Reinforcement Learning: modeling continuous decisions\nQlib now supports reinforcement learning, a feature designed to model continuous investment decisions. This functionality assists investors in optimizing their trading strategies by learning from interactions with the environment to maximize some notion of cumulative reward.\n\nHere is a list of solutions built on `Qlib` categorized by scenarios.\n\n### [RL for order execution](examples/rl_order_execution)\n[Here](https://qlib.readthedocs.io/en/latest/component/rl/overall.html#order-execution) is the introduction of this scenario.  All the methods below are compared [here](examples/rl_order_execution).\n- [TWAP](examples/rl_order_execution/exp_configs/backtest_twap.yml)\n- [PPO: "An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization", IJCAL 2020](examples/rl_order_execution/exp_configs/backtest_ppo.yml)\n- [OPDS: "Universal Trading for Order Execution with Oracle Policy Distillation", AAAI 2021](examples/rl_order_execution/exp_configs/backtest_opds.yml)\n\n# Quant Dataset Zoo\nDataset plays a very important role in Quant. Here is a list of the datasets built on `Qlib`:\n\n| Dataset                                    | US Market | China Market |\n| --                                         | --        | --           |\n| [Alpha360](./qlib/contrib/data/handler.py) |  âˆš        |  âˆš           |\n| [Alpha158](./qlib/contrib/data/handler.py) |  âˆš        |  âˆš           |\n\n[Here](https://qlib.readthedocs.io/en/latest/advanced/alpha.html) is a tutorial to build dataset with `Qlib`.\nYour PR to build new Quant dataset is highly welcomed.\n\n\n# Learning Framework\nQlib is high customizable and a lot of its components are learnable.\nThe learnable components are instances of `Forecast Model` and `Trading Agent`. They are learned based on the `Learning Framework` layer and then applied to multiple scenarios in `Workflow` layer.\nThe learning framework leverages the `Workflow` layer as well(e.g. sharing `Information Extractor`, creating environments based on `Execution Env`).\n\nBased on learning paradigms, they can be categorized into reinforcement learning and supervised learning.\n- For supervised learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/model.html).\n- For reinforcement learning, the detailed docs can be found [here](https://qlib.readthedocs.io/en/latest/component/rl.html). Qlib''s RL learning framework leverages `Execution Env` in `Workflow` layer to create environments.  It''s worth noting that `NestedExecutor` is supported as well. This empowers users to optimize different level of strategies/models/agents together (e.g. optimizing an order execution strategy for a specific portfolio management strategy).\n\n\n# More About Qlib\nIf you want to have a quick glance at the most frequently used components of qlib, you can try notebooks [here](examples/tutorial/).\n\nThe detailed documents are organized in [docs](docs/).\n[Sphinx](http://www.sphinx-doc.org) and the readthedocs theme is required to build the documentation in html formats. \n```bash\ncd docs/\nconda install sphinx sphinx_rtd_theme -y\n# Otherwise, you can install them with pip\n# pip install sphinx sphinx_rtd_theme\nmake html\n```\nYou can also view the [latest document](http://qlib.readthedocs.io/) online directly.\n\nQlib is in active and continuing development. Our plan is in the roadmap, which is managed as a [github project](https://github.com/microsoft/qlib/projects/1).\n\n\n\n# Offline Mode and Online Mode\nThe data server of Qlib can either deployed as `Offline` mode or `Online` mode. The default mode is offline mode.\n\nUnder `Offline` mode, the data will be deployed locally. \n\nUnder `Online` mode, the data will be deployed as a shared data service. The data and their cache will be shared by all the clients. The data retrieval performance is expected to be improved due to a higher rate of cache hits. It will consume less disk space, too. The documents of the online mode can be found in [Qlib-Server](https://qlib-server.readthedocs.io/). The online mode can be deployed automatically with [Azure CLI based scripts](https://qlib-server.readthedocs.io/en/latest/build.html#one-click-deployment-in-azure). The source code of online data server can be found in [Qlib-Server repository](https://github.com/microsoft/qlib-server).\n\n## Performance of Qlib Data Server\nThe performance of data processing is important to data-driven methods like AI technologies. As an AI-oriented platform, Qlib provides a solution for data storage and data processing. To demonstrate the performance of Qlib data server, we\ncompare it with several other data storage solutions. \n\nWe evaluate the performance of several storage solutions by finishing the same task,\nwhich creates a dataset (14 features/factors) from the basic OHLCV daily data of a stock market (800 stocks each day from 2007 to 2020). The task involves data queries and processing.\n\n|                         | HDF5      | MySQL     | MongoDB   | InfluxDB  | Qlib -E -D  | Qlib +E -D   | Qlib +E +D  |\n| --                      | ------    | ------    | --------  | --------- | ----------- | ------------ | ----------- |\n| Total (1CPU) (seconds)  | 184.4Â±3.7 | 365.3Â±7.5 | 253.6Â±6.7 | 368.2Â±3.6 | 147.0Â±8.8   | 47.6Â±1.0     | **7.4Â±0.3** |\n| Total (64CPU) (seconds) |           |           |           |           | 8.8Â±0.6     | **4.2Â±0.2**  |             |\n* `+(-)E` indicates with (out) `ExpressionCache`\n* `+(-)D` indicates with (out) `DatasetCache`\n\nMost general-purpose databases take too much time to load data. After looking into the underlying implementation, we find that data go through too many layers of interfaces and unnecessary format transformations in general-purpose database solutions.\nSuch overheads greatly slow down the data loading process.\nQlib data are stored in a compact format, which is efficient to be combined into arrays for scientific computation.\n\n# Related Reports\n- [Guide To Qlib: Microsoftâ€™s AI Investment Platform](https://analyticsindiamag.com/qlib/)\n- [å¾®è½¯ä¹ŸæžAIé‡åŒ–å¹³å°ï¼Ÿè¿˜æ˜¯å¼€æºçš„ï¼](https://mp.weixin.qq.com/s/47bP5YwxfTp2uTHjUBzJQQ)\n- [å¾®çŸ¿Qlibï¼šä¸šå†…é¦–ä¸ªAIé‡åŒ–æŠ•èµ„å¼€æºå¹³å°](https://mp.weixin.qq.com/s/vsJv7lsgjEi-ALYUz4CvtQ)\n\n# Contact Us\n- If you have any issues, please create issue [here](https://github.com/microsoft/qlib/issues/new/choose) or send messages in [gitter](https://gitter.im/Microsoft/qlib).\n- If you want to make contributions to `Qlib`, please [create pull requests](https://github.com/microsoft/qlib/compare). \n- For other reasons, you are welcome to contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).\n  - We are recruiting new members(both FTEs and interns), your resumes are welcome!\n\nJoin IM discussion groups:\n|[Gitter](https://gitter.im/Microsoft/qlib)|\n|----|\n|![image](https://github.com/microsoft/qlib/blob/main/docs/_static/img/qrcode/gitter_qr.png)|\n\n# Contributing\nWe appreciate all contributions and thank all the contributors!\n<a href="https://github.com/microsoft/qlib/graphs/contributors"><img src="https://contrib.rocks/image?repo=microsoft/qlib" /></a>\n\nBefore we released Qlib as an open-source project on Github in Sep 2020, Qlib is an internal project in our group. Unfortunately, the internal commit history is not kept. A lot of members in our group have also contributed a lot to Qlib, which includes Ruihua Wang, Yinda Zhang, Haisu Yu, Shuyu Wang, Bochen Pang, and [Dong Zhou](https://github.com/evanzd/evanzd). Especially thanks to [Dong Zhou](https://github.com/evanzd/evanzd) due to his initial version of Qlib.\n\n## Guidance\n\nThis project welcomes contributions and suggestions.  \n**Here are some \n[code standards and development guidance](docs/developer/code_standard_and_dev_guide.rst) for submiting a pull request.**\n\nMaking contributions is not a hard thing. Solving an issue(maybe just answering a question raised in [issues list](https://github.com/microsoft/qlib/issues) or [gitter](https://gitter.im/Microsoft/qlib)), fixing/issuing a bug, improving the documents and even fixing a typo are important contributions to Qlib.\n\nFor example, if you want to contribute to Qlib''s document/code, you can follow the steps in the figure below.\n<p align="center">\n  <img src="https://github.com/demon143/qlib/blob/main/docs/_static/img/change%20doc.gif" />\n</p>\n\nIf you don''t know how to start to contribute, you can refer to the following examples.\n| Type | Examples |\n| -- | -- |\n| Solving issues | [Answer a question](https://github.com/microsoft/qlib/issues/749);  [issuing](https://github.com/microsoft/qlib/issues/765) or [fixing](https://github.com/microsoft/qlib/pull/792) a bug |\n| Docs | [Improve docs quality](https://github.com/microsoft/qlib/pull/797/files) ;  [Fix a typo](https://github.com/microsoft/qlib/pull/774) | \n| Feature |  Implement a [requested feature](https://github.com/microsoft/qlib/projects) like [this](https://github.com/microsoft/qlib/pull/754); [Refactor interfaces](https://github.com/microsoft/qlib/pull/539/files) |\n| Dataset | [Add a dataset](https://github.com/microsoft/qlib/pull/733) | \n| Models |  [Implement a new model](https://github.com/microsoft/qlib/pull/689), [some instructions to contribute models](https://github.com/microsoft/qlib/tree/main/examples/benchmarks#contributing) |\n\n[Good first issues](https://github.com/microsoft/qlib/labels/good%20first%20issue) are labelled to indicate that they are easy to start your contributions.\n\nYou can find some impefect implementation in Qlib by  `rg ''TODO|FIXME'' qlib`\n \nIf you would like to become one of Qlib''s maintainers to contribute more (e.g. help merge PR, triage issues), please contact us by email([qlib@microsoft.com](mailto:qlib@microsoft.com)).  We are glad to help to upgrade your permission.\n\n## License\nMost contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe right to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n', '{"language":"Python","stars":34378,"forks":5342,"watchers":34378,"open_issues":309,"topics":["algorithmic-trading","auto-quant","deep-learning","finance","fintech","investment","machine-learning","paper","platform","python","quant","quant-dataset","quant-models","quantitative-finance","quantitative-trading","research","research-paper","stock-data"],"default_branch":"main","size_kb":17928,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:RD-Agent\"><img","source_url":"https://github.com/microsoft/RD-Agent\"><img"},{"type":"has_code","target_id":"github:microsoft:RD-Agent","source_url":"https://github.com/microsoft/RD-Agent"},{"type":"has_code","target_id":"github:microsoft:RD-Agent","source_url":"https://github.com/microsoft/RD-Agent"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:RD-Agent","source_url":"https://github.com/microsoft/RD-Agent"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib.git","source_url":"https://github.com/microsoft/qlib.git"},{"type":"has_code","target_id":"github:chenditc:investment_data","source_url":"https://github.com/chenditc/investment_data"},{"type":"has_code","target_id":"github:chenditc:investment_data","source_url":"https://github.com/chenditc/investment_data"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib-server","source_url":"https://github.com/microsoft/qlib-server"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:evanzd:evanzd","source_url":"https://github.com/evanzd/evanzd"},{"type":"has_code","target_id":"github:evanzd:evanzd","source_url":"https://github.com/evanzd/evanzd"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:demon143:qlib","source_url":"https://github.com/demon143/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"},{"type":"has_code","target_id":"github:microsoft:qlib","source_url":"https://github.com/microsoft/qlib"}]', NULL, 'MIT', 'approved', 80, '269f3f31daa446f6617316d22dab7592', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-qlib from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-qlib.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-CMU-Perceptual-Computing-Lab-openpose', 'github--cmu-perceptual-computing-lab--openpose', 'openpose', 'CMU-Perceptual-Computing-Lab', '<div align="center"> <img src=".github/Logo_main_black.png" width="300"> </div> ----------------- | **Build Type** | | | | | :---: | :---: | :---: | :---: | | **Build Status** | | | | **OpenPose** has represented the **first real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints (in total 135 keypoints) on single images**. It is **authored by** **GinÃ©s Hidalgo**, **Zhe Cao**, **Tomas Simon**, **Shih-En Wei**, **Yaadhav Raaj**, **Hanbyul Joo**, **and** **Y...', '["caffe","computer-vision","cpp","cvpr-2017","deep-learning","face","foot-estimation","hand-estimation","human-behavior-understanding","human-pose","human-pose-estimation","keypoint-detection","keypoints","machine-learning","multi-person","opencv","openpose","pose","pose-estimation","real-time","c++"]', 'other', 33527, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n    <img src=".github/Logo_main_black.png" width="300">\n</div>\n\n-----------------\n\n| **Build Type**   |`Linux`           |`MacOS`           |`Windows`         |\n| :---:            | :---:            | :---:            | :---:            |\n| **Build Status** | [![Status](https://github.com/CMU-Perceptual-Computing-Lab/openpose/workflows/CI/badge.svg)](https://github.com/CMU-Perceptual-Computing-Lab/openpose/actions) | [![Status](https://github.com/CMU-Perceptual-Computing-Lab/openpose/workflows/CI/badge.svg)](https://github.com/CMU-Perceptual-Computing-Lab/openpose/actions) | [![Status](https://ci.appveyor.com/api/projects/status/5leescxxdwen77kg/branch/master?svg=true)](https://ci.appveyor.com/project/gineshidalgo99/openpose/branch/master) |\n\n[**OpenPose**](https://github.com/CMU-Perceptual-Computing-Lab/openpose) has represented the **first real-time multi-person system to jointly detect human body, hand, facial, and foot keypoints (in total 135 keypoints) on single images**.\n\nIt is **authored by** [**GinÃ©s Hidalgo**](https://www.gineshidalgo.com), [**Zhe Cao**](https://people.eecs.berkeley.edu/~zhecao), [**Tomas Simon**](http://www.cs.cmu.edu/~tsimon), [**Shih-En Wei**](https://scholar.google.com/citations?user=sFQD3k4AAAAJ&hl=en), [**Yaadhav Raaj**](https://www.raaj.tech), [**Hanbyul Joo**](https://jhugestar.github.io), **and** [**Yaser Sheikh**](http://www.cs.cmu.edu/~yaser). It is **maintained by** [**GinÃ©s Hidalgo**](https://www.gineshidalgo.com) **and** [**Yaadhav Raaj**](https://www.raaj.tech). OpenPose would not be possible without the [**CMU Panoptic Studio dataset**](http://domedb.perception.cs.cmu.edu). We would also like to thank all the people who [have helped OpenPose in any way](doc/09_authors_and_contributors.md).\n\n\n\n<p align="center">\n    <img src=".github/media/pose_face_hands.gif" width="480">\n    <br>\n    <sup>Authors <a href="https://www.gineshidalgo.com" target="_blank">GinÃ©s Hidalgo</a> (left) and <a href="https://jhugestar.github.io" target="_blank">Hanbyul Joo</a> (right) in front of the <a href="http://domedb.perception.cs.cmu.edu" target="_blank">CMU Panoptic Studio</a></sup>\n</p>\n\n\n\n## Contents\n1. [Results](#results)\n2. [Features](#features)\n3. [Related Work](#related-work)\n4. [Installation](#installation)\n5. [Quick Start Overview](#quick-start-overview)\n6. [Send Us Feedback!](#send-us-feedback)\n7. [Citation](#citation)\n8. [License](#license)\n\n\n\n## Results\n### Whole-body (Body, Foot, Face, and Hands) 2D Pose Estimation\n<p align="center">\n    <img src=".github/media/dance_foot.gif" width="300">\n    <img src=".github/media/pose_face.gif" width="300">\n    <img src=".github/media/pose_hands.gif" width="300">\n    <br>\n    <sup>Testing OpenPose: (Left) <a href="https://www.youtube.com/watch?v=2DiQUX11YaY" target="_blank"><i>Crazy Uptown Funk flashmob in Sydney</i></a> video sequence. (Center and right) Authors <a href="https://www.gineshidalgo.com" target="_blank">GinÃ©s Hidalgo</a> and <a href="http://www.cs.cmu.edu/~tsimon" target="_blank">Tomas Simon</a> testing face and hands</sup>\n</p>\n\n### Whole-body 3D Pose Reconstruction and Estimation\n<p align="center">\n    <img src=".github/media/openpose3d.gif" width="360">\n    <br>\n    <sup><a href="https://ziutinyat.github.io/" target="_blank">Tianyi Zhao</a> testing the OpenPose 3D Module</a></sup>\n</p>\n\n### Unity Plugin\n<p align="center">\n    <img src=".github/media/unity_main.png" width="300">\n    <img src=".github/media/unity_body_foot.png" width="300">\n    <img src=".github/media/unity_hand_face.png" width="300">\n    <br>\n    <sup><a href="https://ziutinyat.github.io/" target="_blank">Tianyi Zhao</a> and <a href="https://www.gineshidalgo.com" target="_blank">GinÃ©s Hidalgo</a> testing the <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin" target="_blank">OpenPose Unity Plugin</a></sup>\n</p>\n\n### Runtime Analysis\nWe show an inference time comparison between the 3 available pose estimation libraries (same hardware and conditions): OpenPose, Alpha-Pose (fast Pytorch version), and Mask R-CNN. The OpenPose runtime is constant, while the runtime of Alpha-Pose and Mask R-CNN grow linearly with the number of people. More details [**here**](https://arxiv.org/abs/1812.08008).\n\n<p align="center">\n    <img src=".github/media/openpose_vs_competition.png" width="360">\n</p>\n\n\n\n## Features\n**Main Functionality**:\n- **2D real-time multi-person keypoint detection**:\n    - 15, 18 or **25-keypoint body/foot keypoint estimation**, including **6 foot keypoints**. **Runtime invariant to number of detected people**.\n    - **2x21-keypoint hand keypoint estimation**. **Runtime depends on number of detected people**. See [**OpenPose Training**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_train) for a runtime invariant alternative.\n    - **70-keypoint face keypoint estimation**. **Runtime depends on number of detected people**. See [**OpenPose Training**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_train) for a runtime invariant alternative.\n- [**3D real-time single-person keypoint detection**](doc/advanced/3d_reconstruction_module.md):\n    - 3D triangulation from multiple single views.\n    - Synchronization of Flir cameras handled.\n    - Compatible with Flir/Point Grey cameras.\n- [**Calibration toolbox**](doc/advanced/calibration_module.md): Estimation of distortion, intrinsic, and extrinsic camera parameters.\n- **Single-person tracking** for further speedup or visual smoothing.\n\n**Input**: Image, video, webcam, Flir/Point Grey, IP camera, and support to add your own custom input source (e.g., depth camera).\n\n**Output**: Basic image + keypoint display/saving (PNG, JPG, AVI, ...), keypoint saving (JSON, XML, YML, ...), keypoints as array class, and support to add your own custom output code (e.g., some fancy UI).\n\n**OS**: Ubuntu (20, 18, 16, 14), Windows (10, 8), Mac OSX, Nvidia TX2.\n\n**Hardware compatibility**: CUDA (Nvidia GPU), OpenCL (AMD GPU), and non-GPU (CPU-only) versions.\n\n**Usage Alternatives**:\n- [**Command-line demo**](doc/01_demo.md) for built-in functionality.\n- [**C++ API**](doc/04_cpp_api.md/) and [**Python API**](doc/03_python_api.md) for custom functionality. E.g., adding your custom inputs, pre-processing, post-posprocessing, and output steps.\n\nFor further details, check the [major released features](doc/07_major_released_features.md) and [release notes](doc/08_release_notes.md) docs.\n\n\n\n## Related Work\n- [**OpenPose training code**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_train)\n- [**OpenPose foot dataset**](https://cmu-perceptual-computing-lab.github.io/foot_keypoint_dataset/)\n- [**OpenPose Unity Plugin**](https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin)\n- OpenPose papers published in **IEEE TPAMI and CVPR**. Cite them in your publications if OpenPose helps your research! (Links and more details in the [Citation](#citation) section below).\n\n\n\n## Installation\nIf you want to use OpenPose without installing or writing any code, simply [download and use the latest Windows portable version of OpenPose](doc/installation/0_index.md#windows-portable-demo)!\n\nOtherwise, you could [build OpenPose from source](doc/installation/0_index.md#compiling-and-running-openpose-from-source). See the [installation doc](doc/installation/0_index.md) for all the alternatives.\n\n\n\n## Quick Start Overview\nSimply use the OpenPose Demo from your favorite command-line tool (e.g., Windows PowerShell or Ubuntu Terminal). E.g., this example runs OpenPose on your webcam and displays the body keypoints:\n```\n# Ubuntu\n./build/examples/openpose/openpose.bin\n```\n```\n:: Windows - Portable Demo\nbin\OpenPoseDemo.exe --video examples\media\video.avi\n```\n\nYou can also add any of the available flags in any order. E.g., the following example runs on a video (`--video {PATH}`), enables face (`--face`) and hands (`--hand`), and saves the output keypoints on JSON files on disk (`--write_json {PATH}`).\n```\n# Ubuntu\n./build/examples/openpose/openpose.bin --video examples/media/video.avi --face --hand --write_json output_json_folder/\n```\n```\n:: Windows - Portable Demo\nbin\OpenPoseDemo.exe --video examples\media\video.avi --face --hand --write_json output_json_folder/\n```\n\nOptionally, you can also extend OpenPose''s functionality from its Python and C++ APIs. After [installing](doc/installation/0_index.md) OpenPose, check its [official doc](doc/00_index.md) for a quick overview of all the alternatives and tutorials.\n\n\n\n## Send Us Feedback!\nOur library is open source for research purposes, and we want to improve it! So let us know (create a new GitHub issue or pull request, email us, etc.) if you...\n1. Find/fix any bug (in functionality or speed) or know how to speed up or improve any part of OpenPose.\n2. Want to add/show some cool functionality/demo/project made on top of OpenPose. We can add your project link to our [Community-based Projects](doc/10_community_projects.md) section or even integrate it with OpenPose!\n\n\n\n## Citation\nPlease cite these papers in your publications if OpenPose helps your research. All of OpenPose is based on [OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields](https://arxiv.org/abs/1812.08008), while the hand and face detectors also use [Hand Keypoint Detection in Single Images using Multiview Bootstrapping](https://arxiv.org/abs/1704.07809) (the face detector was trained using the same procedure as the hand detector).\n\n    @article{8765346,\n      author = {Z. {Cao} and G. {Hidalgo Martinez} and T. {Simon} and S. {Wei} and Y. A. {Sheikh}},\n      journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},\n      title = {OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},\n      year = {2019}\n    }\n\n    @inproceedings{simon2017hand,\n      author = {Tomas Simon and Hanbyul Joo and Iain Matthews and Yaser Sheikh},\n      booktitle = {CVPR},\n      title = {Hand Keypoint Detection in Single Images using Multiview Bootstrapping},\n      year = {2017}\n    }\n\n    @inproceedings{cao2017realtime,\n      author = {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},\n      booktitle = {CVPR},\n      title = {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},\n      year = {2017}\n    }\n\n    @inproceedings{wei2016cpm,\n      author = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},\n      booktitle = {CVPR},\n      title = {Convolutional pose machines},\n      year = {2016}\n    }\n\nPaper links:\n- OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields:\n    - [IEEE TPAMI](https://ieeexplore.ieee.org/document/8765346)\n    - [ArXiv](https://arxiv.org/abs/1812.08008)\n- [Hand Keypoint Detection in Single Images using Multiview Bootstrapping](https://arxiv.org/abs/1704.07809)\n- [Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields](https://arxiv.org/abs/1611.08050)\n- [Convolutional Pose Machines](https://arxiv.org/abs/1602.00134)\n\n\n\n## License\nOpenPose is freely available for free non-commercial use, and may be redistributed under these conditions. Please, see the [license](./LICENSE) for further details. Interested in a commercial license? Check this [FlintBox link](https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740). For commercial queries, use the `Contact` section from the [FlintBox link](https://cmu.flintbox.com/#technologies/b820c21d-8443-4aa2-a49f-8919d93a8740) and also send a copy of that message to [Yaser Sheikh](mailto:yaser@cs.cmu.edu).\n', '{"language":"C++","stars":33527,"forks":8046,"watchers":33527,"open_issues":360,"topics":["caffe","computer-vision","cpp","cvpr-2017","deep-learning","face","foot-estimation","hand-estimation","human-behavior-understanding","human-pose","human-pose-estimation","keypoint-detection","keypoints","machine-learning","multi-person","opencv","openpose","pose","pose-estimation","real-time"],"default_branch":"master","size_kb":86496,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose"},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose"},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose"},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose"},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose"},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose_unity_plugin\"","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin\""},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose_train","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose_train"},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose_train","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose_train"},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose_train","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose_train"},{"type":"has_code","target_id":"github:CMU-Perceptual-Computing-Lab:openpose_unity_plugin","source_url":"https://github.com/CMU-Perceptual-Computing-Lab/openpose_unity_plugin"}]', NULL, 'NOASSERTION', 'approved', 80, '0688fa6a237c82539265bb9257f66a6f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-CMU-Perceptual-Computing-Lab-openpose from https://github.com/CMU-Perceptual-Computing-Lab.png
Image converted to WebP: data/images/github-CMU-Perceptual-Computing-Lab-openpose.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-TheAlgorithms-C-Plus-Plus', 'github--thealgorithms--c-plus-plus', 'C-Plus-Plus', 'TheAlgorithms', '<!-- the suffix in the above line is required for doxygen to consider this as the index page of the generated documentation site --> !GitHub repo size This repository is a collection of open-source implementation of a variety of algorithms implemented in C++ and licensed under MIT License. These algorithms span a variety of topics from computer science, mathematics and statistics, data science, machine learning, engineering, etc.. The implementations and the associated documentation are meant...', '["algorithm","algorithm-competitions","algorithms-implemented","artificial-intelligence-algorithms","computer-science","cpp","data-structures","educational","instructor-materials","interview-preparation","interview-questions","machine-learning","machine-learning-algorithms","mathematics","search","sort","c++"]', 'other', 33486, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# The Algorithms - C++ # {#mainpage}\n\n<!-- the suffix in the above line is required for doxygen to consider this as the index page of the generated documentation site -->\n\n[![Gitpod Ready-to-Code](https://img.shields.io/badge/Gitpod-Ready--to--Code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/TheAlgorithms/C-Plus-Plus)\n[![CodeQL CI](https://github.com/TheAlgorithms/C-Plus-Plus/actions/workflows/codeql.yml/badge.svg)](https://github.com/TheAlgorithms/C-Plus-Plus/actions/workflows/codeql.yml)\n[![Gitter chat](https://img.shields.io/badge/Chat-Gitter-ff69b4.svg?label=Chat&logo=gitter&style=flat-square)](https://gitter.im/TheAlgorithms)\n[![contributions welcome](https://img.shields.io/static/v1.svg?label=Contributions&message=Welcome&color=0059b3&style=flat-square)](https://github.com/TheAlgorithms/C-Plus-Plus/blob/master/CONTRIBUTING.md)\n![GitHub repo size](https://img.shields.io/github/repo-size/TheAlgorithms/C-Plus-Plus?color=red&style=flat-square)\n[![Doxygen CI](https://github.com/TheAlgorithms/C-Plus-Plus/workflows/Doxygen%20CI/badge.svg)](https://TheAlgorithms.github.io/C-Plus-Plus)\n[![Awesome CI](https://github.com/TheAlgorithms/C-Plus-Plus/workflows/Awesome%20CI%20Workflow/badge.svg)](https://github.com/TheAlgorithms/C-Plus-Plus/actions?query=workflow%3A%22Awesome+CI+Workflow%22)\n[![Income](https://img.shields.io/liberapay/receives/TheAlgorithms.svg?logo=liberapay)](https://liberapay.com/TheAlgorithms)\n[![Discord chat](https://img.shields.io/discord/808045925556682782.svg?logo=discord&colorB=5865F2)](https://the-algorithms.com/discord/)\n[![Donate](https://liberapay.com/assets/widgets/donate.svg)](https://liberapay.com/TheAlgorithms/donate)\n\n## Overview\n\nThis repository is a collection of open-source implementation of a variety of algorithms implemented in C++ and licensed under [MIT License](https://github.com/TheAlgorithms/C-Plus-Plus/blob/master/LICENSE). These algorithms span a variety of topics from computer science, mathematics and statistics, data science, machine learning, engineering, etc.. The implementations and the associated documentation are meant to provide a learning resource for educators and students. Hence, one may find more than one implementation for the same objective but using a different algorithm strategies and optimizations.\n\n## Features\n\n- The repository provides implementations of various algorithms in one of the most fundamental general purpose languages - [C++](https://en.wikipedia.org/wiki/C%2B%2B).\n- Well documented source code with detailed explanations provide a valuable resource for educators and students alike.\n- Each source code is atomic using [STL classes](https://en.wikipedia.org/wiki/Standard_Template_Library) and _no external libraries_ are required for their compilation and execution. Thus, the fundamentals of the algorithms can be studied in much depth.\n- Source codes are [compiled and tested](https://github.com/TheAlgorithms/C-Plus-Plus/actions?query=workflow%3A%22Awesome+CI+Workflow%22) for every commit on the latest versions of three major operating systems viz., Windows, MacOS, and Ubuntu (Linux) using MSVC 19 2022, AppleClang 15.0.15, and GNU 13.3.0 respectively.\n- Strict adherence to [C++17](https://en.wikipedia.org/wiki/C%2B%2B17) standard ensures portability of code to embedded systems as well like [ESP32](https://docs.espressif.com/projects/esp-idf/en/stable/esp32/api-guides/cplusplus.html#c-language-standard), [ARM Cortex](https://developer.arm.com/documentation/101458/2404/Standards-support/Supported-C-C---standards-in-Arm-C-C---Compiler), etc. with little to no changes.\n- Self-checks within programs ensure correct implementations with confidence.\n- Modular implementations and OpenSource licensing enable the functions to be utilized conveniently in other applications.\n\n## Documentation\n\n[Online Documentation](https://TheAlgorithms.github.io/C-Plus-Plus) is generated from the repository source codes directly. The documentation contains all resources including source code snippets, details on execution of the programs, diagrammatic representation of program flow, and links to external resources where necessary. The documentation also introduces interactive source code with links to documentation for C++ STL library functions used.\nClick on [Files menu](https://TheAlgorithms.github.io/C-Plus-Plus/files.html) to see the list of all the files documented with the code.\n\n[Documentation of Algorithms in C++](https://thealgorithms.github.io/C-Plus-Plus) by [The Algorithms Contributors](https://github.com/TheAlgorithms/C-Plus-Plus/graphs/contributors) is licensed under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/?ref=chooser-v1)<br/>\n<a href="https://creativecommons.org/licenses/by-sa/4.0"><img alt="Creative Commons License" style="height:22px!important;margin-left: 3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/cc.svg" /><img  alt="Credit must be given to the creator" style="height:22px!important;margin-left: 3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/by.svg" /><img alt="Adaptations must be shared under the same terms" style="height:22px!important;margin-left: 3px;vertical-align:text-bottom;" src="https://mirrors.creativecommons.org/presskit/icons/sa.svg" /></a>\n\n## Contributions\n\nAs a community developed and maintained repository, we welcome new un-plagiarized quality contributions. Please read our [Contribution Guidelines](https://github.com/TheAlgorithms/C-Plus-Plus/blob/master/CONTRIBUTING.md).\n', '{"language":"C++","stars":33486,"forks":7657,"watchers":33486,"open_issues":32,"topics":["algorithm","algorithm-competitions","algorithms-implemented","artificial-intelligence-algorithms","computer-science","cpp","data-structures","educational","instructor-materials","interview-preparation","interview-questions","machine-learning","machine-learning-algorithms","mathematics","search","sort"],"default_branch":"master","size_kb":140614,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"},{"type":"has_code","target_id":"github:TheAlgorithms:C-Plus-Plus","source_url":"https://github.com/TheAlgorithms/C-Plus-Plus"}]', NULL, 'MIT', 'approved', 65, 'ba0cc19e36b5c085e81d9fc62fe742be', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-TheAlgorithms-C-Plus-Plus from https://github.com/TheAlgorithms.png
Image converted to WebP: data/images/github-TheAlgorithms-C-Plus-Plus.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-explosion-spaCy', 'github--explosion--spacy', 'spaCy', 'explosion', '<a href="https://explosion.ai"><img src="https://explosion.ai/assets/img/logo.svg" width="125" height="125" align="right" /></a> spaCy is a library for **advanced Natural Language Processing** in Python and Cython. It''s built on the very latest research, and was designed from day one to be used in real products. spaCy comes with pretrained pipelines and currently supports tokenization and training for **70+ languages**. It features state-of-the-art speed and **neural network models** for tagg...', '["ai","artificial-intelligence","cython","data-science","deep-learning","entity-linking","machine-learning","named-entity-recognition","natural-language-processing","neural-network","neural-networks","nlp","nlp-library","python","spacy","text-classification","tokenization","python"]', 'other', 32917, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/explosion/spaCy","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<a href="https://explosion.ai"><img src="https://explosion.ai/assets/img/logo.svg" width="125" height="125" align="right" /></a>\n\n# spaCy: Industrial-strength NLP\n\nspaCy is a library for **advanced Natural Language Processing** in Python and\nCython. It''s built on the very latest research, and was designed from day one to\nbe used in real products.\n\nspaCy comes with [pretrained pipelines](https://spacy.io/models) and currently\nsupports tokenization and training for **70+ languages**. It features\nstate-of-the-art speed and **neural network models** for tagging, parsing,\n**named entity recognition**, **text classification** and more, multi-task\nlearning with pretrained **transformers** like BERT, as well as a\nproduction-ready [**training system**](https://spacy.io/usage/training) and easy\nmodel packaging, deployment and workflow management. spaCy is commercial\nopen-source software, released under the\n[MIT license](https://github.com/explosion/spaCy/blob/master/LICENSE).\n\nðŸ’« **Version 3.8 out now!**\n[Check out the release notes here.](https://github.com/explosion/spaCy/releases)\n\n[![tests](https://github.com/explosion/spaCy/actions/workflows/tests.yml/badge.svg)](https://github.com/explosion/spaCy/actions/workflows/tests.yml)\n[![Current Release Version](https://img.shields.io/github/release/explosion/spacy.svg?style=flat-square&logo=github)](https://github.com/explosion/spaCy/releases)\n[![pypi Version](https://img.shields.io/pypi/v/spacy.svg?style=flat-square&logo=pypi&logoColor=white)](https://pypi.org/project/spacy/)\n[![conda Version](https://img.shields.io/conda/vn/conda-forge/spacy.svg?style=flat-square&logo=conda-forge&logoColor=white)](https://anaconda.org/conda-forge/spacy)\n[![Python wheels](https://img.shields.io/badge/wheels-%E2%9C%93-4c1.svg?longCache=true&style=flat-square&logo=python&logoColor=white)](https://github.com/explosion/wheelwright/releases)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/ambv/black)\n<br />\n[![PyPi downloads](https://static.pepy.tech/personalized-badge/spacy?period=total&units=international_system&left_color=grey&right_color=orange&left_text=pip%20downloads)](https://pypi.org/project/spacy/)\n[![Conda downloads](https://img.shields.io/conda/dn/conda-forge/spacy?label=conda%20downloads)](https://anaconda.org/conda-forge/spacy)\n\n## ðŸ“– Documentation\n\n| Documentation                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                              |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| â­ï¸ **[spaCy 101]**                                                                                                                                                                                                       | New to spaCy? Here''s everything you need to know!                                                                                                                                                                                                                                                                                            |\n| ðŸ“š **[Usage Guides]**                                                                                                                                                                                                     | How to use spaCy and its features.                                                                                                                                                                                                                                                                                                           |\n| ðŸš€ **[New in v3.0]**                                                                                                                                                                                                      | New features, backwards incompatibilities and migration guide.                                                                                                                                                                                                                                                                               |\n| ðŸª **[Project Templates]**                                                                                                                                                                                                | End-to-end workflows you can clone, modify and run.                                                                                                                                                                                                                                                                                          |\n| ðŸŽ› **[API Reference]**                                                                                                                                                                                                     | The detailed reference for spaCy''s API.                                                                                                                                                                                                                                                                                                      |\n| â© **[GPU Processing]**                                                                                                                                                                                                    | Use spaCy with CUDA-compatible GPU processing.                                                                                                                                                                                                                                                                                               |\n| ðŸ“¦ **[Models]**                                                                                                                                                                                                           | Download trained pipelines for spaCy.                                                                                                                                                                                                                                                                                                        |\n| ðŸ¦™ **[Large Language Models]**                                                                                                                                                                                            | Integrate LLMs into spaCy pipelines.                                                                                                                                                                                                                                                                                                        |\n| ðŸŒŒ **[Universe]**                                                                                                                                                                                                         | Plugins, extensions, demos and books from the spaCy ecosystem.                                                                                                                                                                                                                                                                               |\n| âš™ï¸ **[spaCy VS Code Extension]**                                                                                                                                                                                          | Additional tooling and features for working with spaCy''s config files.                                                                                                                                                                                                                                                                       |\n| ðŸ‘©â€ðŸ« **[Online Course]**                                                                                                                                                                                                    | Learn spaCy in this free and interactive online course.                                                                                                                                                                                                                                                                                      |\n| ðŸ“° **[Blog]**                                                                                                                                                                                                             | Read about current spaCy and Prodigy development, releases, talks and more from Explosion.                                                                                                                                                                                                                 |\n| ðŸ“º **[Videos]**                                                                                                                                                                                                           | Our YouTube channel with video tutorials, talks and more.                                                                                                                                                                                                                                                                                    |\n| ðŸ”´ **[Live Stream]**                                                                                                                                                                                                       | Join Matt as he works on spaCy and chat about NLP, live every week.                                                                                                                                                                                                                                                                         |\n| ðŸ›  **[Changelog]**                                                                                                                                                                                                         | Changes and version history.                                                                                                                                                                                                                                                                                                                 |\n| ðŸ’ **[Contribute]**                                                                                                                                                                                                       | How to contribute to the spaCy project and code base.                                                                                                                                                                                                                                                                                        |\n| ðŸ‘• **[Swag]**                                                                                                                                                                                                             | Support us and our work with unique, custom-designed swag!                                                                                                                                                                                                                                                                                   |\n| <a href="https://explosion.ai/tailored-solutions"><img src="https://github.com/explosion/spaCy/assets/13643239/36d2a42e-98c0-4599-90e1-788ef75181be" width="150" alt="Tailored Solutions"/></a> | Custom NLP consulting, implementation and strategic advice by spaCyâ€™s core development team. Streamlined, production-ready, predictable and maintainable. Send us an email or take our 5-minute questionnaire, and well''be in touch! **[Learn more &rarr;](https://explosion.ai/tailored-solutions)**                 |\n\n[spacy 101]: https://spacy.io/usage/spacy-101\n[new in v3.0]: https://spacy.io/usage/v3\n[usage guides]: https://spacy.io/usage/\n[api reference]: https://spacy.io/api/\n[gpu processing]: https://spacy.io/usage#gpu\n[models]: https://spacy.io/models\n[large language models]: https://spacy.io/usage/large-language-models\n[universe]: https://spacy.io/universe\n[spacy vs code extension]: https://github.com/explosion/spacy-vscode\n[videos]: https://www.youtube.com/c/ExplosionAI\n[live stream]: https://www.youtube.com/playlist?list=PLBmcuObd5An5_iAxNYLJa_xWmNzsYce8c\n[online course]: https://course.spacy.io\n[blog]: https://explosion.ai\n[project templates]: https://github.com/explosion/projects\n[changelog]: https://spacy.io/usage#changelog\n[contribute]: https://github.com/explosion/spaCy/blob/master/CONTRIBUTING.md\n[swag]: https://explosion.ai/merch\n\n## ðŸ’¬ Where to ask questions\n\nThe spaCy project is maintained by the [spaCy team](https://explosion.ai/about).\nPlease understand that we won''t be able to provide individual support via email.\nWe also believe that help is much more valuable if it''s shared publicly, so that\nmore people can benefit from it.\n\n| Type                            | Platforms                               |\n| ------------------------------- | --------------------------------------- |\n| ðŸš¨ **Bug Reports**              | [GitHub Issue Tracker]                  |\n| ðŸŽ **Feature Requests & Ideas** | [GitHub Discussions] Â· [Live Stream]    |\n| ðŸ‘©â€ðŸ’» **Usage Questions**          | [GitHub Discussions] Â· [Stack Overflow] |\n| ðŸ—¯ **General Discussion**        | [GitHub Discussions] Â· [Live Stream]   |\n\n[github issue tracker]: https://github.com/explosion/spaCy/issues\n[github discussions]: https://github.com/explosion/spaCy/discussions\n[stack overflow]: https://stackoverflow.com/questions/tagged/spacy\n[live stream]: https://www.youtube.com/playlist?list=PLBmcuObd5An5_iAxNYLJa_xWmNzsYce8c\n\n## Features\n\n- Support for **70+ languages**\n- **Trained pipelines** for different languages and tasks\n- Multi-task learning with pretrained **transformers** like BERT\n- Support for pretrained **word vectors** and embeddings\n- State-of-the-art speed\n- Production-ready **training system**\n- Linguistically-motivated **tokenization**\n- Components for named **entity recognition**, part-of-speech-tagging,\n  dependency parsing, sentence segmentation, **text classification**,\n  lemmatization, morphological analysis, entity linking and more\n- Easily extensible with **custom components** and attributes\n- Support for custom models in **PyTorch**, **TensorFlow** and other frameworks\n- Built in **visualizers** for syntax and NER\n- Easy **model packaging**, deployment and workflow management\n- Robust, rigorously evaluated accuracy\n\nðŸ“– **For more details, see the\n[facts, figures and benchmarks](https://spacy.io/usage/facts-figures).**\n\n## â³ Install spaCy\n\nFor detailed installation instructions, see the\n[documentation](https://spacy.io/usage).\n\n- **Operating system**: macOS / OS X Â· Linux Â· Windows (Cygwin, MinGW, Visual\n  Studio)\n- **Python version**: Python >=3.7, <3.13 (only 64 bit)\n- **Package managers**: [pip] Â· [conda] (via `conda-forge`)\n\n[pip]: https://pypi.org/project/spacy/\n[conda]: https://anaconda.org/conda-forge/spacy\n\n### pip\n\nUsing pip, spaCy releases are available as source packages and binary wheels.\nBefore you install spaCy and its dependencies, make sure that your `pip`,\n`setuptools` and `wheel` are up to date.\n\n```bash\npip install -U pip setuptools wheel\npip install spacy\n```\n\nTo install additional data tables for lemmatization and normalization you can\nrun `pip install spacy[lookups]` or install\n[`spacy-lookups-data`](https://github.com/explosion/spacy-lookups-data)\nseparately. The lookups package is needed to create blank models with\nlemmatization data, and to lemmatize in languages that don''t yet come with\npretrained models and aren''t powered by third-party libraries.\n\nWhen using pip it is generally recommended to install packages in a virtual\nenvironment to avoid modifying system state:\n\n```bash\npython -m venv .env\nsource .env/bin/activate\npip install -U pip setuptools wheel\npip install spacy\n```\n\n### conda\n\nYou can also install spaCy from `conda` via the `conda-forge` channel. For the\nfeedstock including the build recipe and configuration, check out\n[this repository](https://github.com/conda-forge/spacy-feedstock).\n\n```bash\nconda install -c conda-forge spacy\n```\n\n### Updating spaCy\n\nSome updates to spaCy may require downloading new statistical models. If you''re\nrunning spaCy v2.0 or higher, you can use the `validate` command to check if\nyour installed models are compatible and if not, print details on how to update\nthem:\n\n```bash\npip install -U spacy\npython -m spacy validate\n```\n\nIf you''ve trained your own models, keep in mind that your training and runtime\ninputs must match. After updating spaCy, we recommend **retraining your models**\nwith the new version.\n\nðŸ“– **For details on upgrading from spaCy 2.x to spaCy 3.x, see the\n[migration guide](https://spacy.io/usage/v3#migrating).**\n\n## ðŸ“¦ Download model packages\n\nTrained pipelines for spaCy can be installed as **Python packages**. This means\nthat they''re a component of your application, just like any other module. Models\ncan be installed using spaCy''s [`download`](https://spacy.io/api/cli#download)\ncommand, or manually by pointing pip to a path or URL.\n\n| Documentation              |                                                                  |\n| -------------------------- | ---------------------------------------------------------------- |\n| **[Available Pipelines]**  | Detailed pipeline descriptions, accuracy figures and benchmarks. |\n| **[Models Documentation]** | Detailed usage and installation instructions.                    |\n| **[Training]**             | How to train your own pipelines on your data.                    |\n\n[available pipelines]: https://spacy.io/models\n[models documentation]: https://spacy.io/usage/models\n[training]: https://spacy.io/usage/training\n\n```bash\n# Download best-matching version of specific model for your spaCy installation\npython -m spacy download en_core_web_sm\n\n# pip install .tar.gz archive or .whl from path or URL\npip install /Users/you/en_core_web_sm-3.0.0.tar.gz\npip install /Users/you/en_core_web_sm-3.0.0-py3-none-any.whl\npip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n```\n\n### Loading and using models\n\nTo load a model, use [`spacy.load()`](https://spacy.io/api/top-level#spacy.load)\nwith the model name or a path to the model data directory.\n\n```python\nimport spacy\nnlp = spacy.load("en_core_web_sm")\ndoc = nlp("This is a sentence.")\n```\n\nYou can also `import` a model directly via its full name and then call its\n`load()` method with no arguments.\n\n```python\nimport spacy\nimport en_core_web_sm\n\nnlp = en_core_web_sm.load()\ndoc = nlp("This is a sentence.")\n```\n\nðŸ“– **For more info and examples, check out the\n[models documentation](https://spacy.io/docs/usage/models).**\n\n## âš’ Compile from source\n\nThe other way to install spaCy is to clone its\n[GitHub repository](https://github.com/explosion/spaCy) and build it from\nsource. That is the common way if you want to make changes to the code base.\nYou''ll need to make sure that you have a development environment consisting of a\nPython distribution including header files, a compiler,\n[pip](https://pip.pypa.io/en/latest/installing/),\n[virtualenv](https://virtualenv.pypa.io/en/latest/) and\n[git](https://git-scm.com) installed. The compiler part is the trickiest. How to\ndo that depends on your system.\n\n| Platform    |                                                                                                                                                                                                                                                                     |\n| ----------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| **Ubuntu**  | Install system-level dependencies via `apt-get`: `sudo apt-get install build-essential python-dev git` .                                                                                                                                                            |\n| **Mac**     | Install a recent version of [XCode](https://developer.apple.com/xcode/), including the so-called "Command Line Tools". macOS and OS X ship with Python and git preinstalled.                                                                                        |\n| **Windows** | Install a version of the [Visual C++ Build Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/) or [Visual Studio Express](https://visualstudio.microsoft.com/vs/express/) that matches the version that was used to compile your Python interpreter. |\n\nFor more details and instructions, see the documentation on\n[compiling spaCy from source](https://spacy.io/usage#source) and the\n[quickstart widget](https://spacy.io/usage#section-quickstart) to get the right\ncommands for your platform and Python version.\n\n```bash\ngit clone https://github.com/explosion/spaCy\ncd spaCy\n\npython -m venv .env\nsource .env/bin/activate\n\n# make sure you are using the latest pip\npython -m pip install -U pip setuptools wheel\n\npip install -r requirements.txt\npip install --no-build-isolation --editable .\n```\n\nTo install with extras:\n\n```bash\npip install --no-build-isolation --editable .[lookups,cuda102]\n```\n\n## ðŸš¦ Run tests\n\nspaCy comes with an [extensive test suite](spacy/tests). In order to run the\ntests, you''ll usually want to clone the repository and build spaCy from source.\nThis will also install the required development dependencies and test utilities\ndefined in the [`requirements.txt`](requirements.txt).\n\nAlternatively, you can run `pytest` on the tests from within the installed\n`spacy` package. Don''t forget to also install the test utilities via spaCy''s\n[`requirements.txt`](requirements.txt):\n\n```bash\npip install -r requirements.txt\npython -m pytest --pyargs spacy\n```\n', '{"language":"Python","stars":32917,"forks":4636,"watchers":32917,"open_issues":183,"topics":["ai","artificial-intelligence","cython","data-science","deep-learning","entity-linking","machine-learning","named-entity-recognition","natural-language-processing","neural-network","neural-networks","nlp","nlp-library","python","spacy","text-classification","tokenization"],"default_branch":"master","size_kb":203340,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:wheelwright","source_url":"https://github.com/explosion/wheelwright"},{"type":"has_code","target_id":"github:ambv:black","source_url":"https://github.com/ambv/black"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spacy-vscode","source_url":"https://github.com/explosion/spacy-vscode"},{"type":"has_code","target_id":"github:explosion:projects","source_url":"https://github.com/explosion/projects"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spacy-lookups-data","source_url":"https://github.com/explosion/spacy-lookups-data"},{"type":"has_code","target_id":"github:conda-forge:spacy-feedstock","source_url":"https://github.com/conda-forge/spacy-feedstock"},{"type":"has_code","target_id":"github:explosion:spacy-models","source_url":"https://github.com/explosion/spacy-models"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"},{"type":"has_code","target_id":"github:explosion:spaCy","source_url":"https://github.com/explosion/spaCy"}]', NULL, 'MIT', 'approved', 80, '1c94d9d3cb9a56ec01b4f2ed0f361c05', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-explosion-spaCy from https://github.com/explosion.png
Image converted to WebP: data/images/github-explosion-spaCy.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-google-ai-edge-mediapipe', 'github--google-ai-edge--mediapipe', 'mediapipe', 'google-ai-edge', '--- layout: forward target: https://developers.google.com/mediapipe title: Home nav_order: 1 --- ---- **Attention:** *We have moved to https://developers.google.com/mediapipe as the primary developer documentation site for MediaPipe as of April 3, 2023.* !MediaPipe **Attention**: MediaPipe Solutions Preview is an early release. Learn more. **On-device machine learning for everyone** Delight your customers with innovative machine learning features. MediaPipe contains everything that you need t...', '["android","audio-processing","c-plus-plus","calculator","computer-vision","deep-learning","framework","graph-based","graph-framework","inference","machine-learning","mediapipe","mobile-development","perception","pipeline-framework","stream-processing","video-processing","c++"]', 'other', 32330, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/google-ai-edge/mediapipe","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '---\nlayout: forward\ntarget: https://developers.google.com/mediapipe\ntitle: Home\nnav_order: 1\n---\n\n----\n\n**Attention:** *We have moved to\n[https://developers.google.com/mediapipe](https://developers.google.com/mediapipe)\nas the primary developer documentation site for MediaPipe as of April 3, 2023.*\n\n![MediaPipe](https://developers.google.com/static/mediapipe/images/home/hero_01_1920.png)\n\n**Attention**: MediaPipe Solutions Preview is an early release. [Learn\nmore](https://developers.google.com/mediapipe/solutions/about#notice).\n\n**On-device machine learning for everyone**\n\nDelight your customers with innovative machine learning features. MediaPipe\ncontains everything that you need to customize and deploy to mobile (Android,\niOS), web, desktop, edge devices, and IoT, effortlessly.\n\n*   [See demos](https://goo.gle/mediapipe-studio)\n*   [Learn more](https://developers.google.com/mediapipe/solutions)\n\n## Get started\n\nYou can get started with MediaPipe Solutions by by checking out any of the\ndeveloper guides for\n[vision](https://developers.google.com/mediapipe/solutions/vision/object_detector),\n[text](https://developers.google.com/mediapipe/solutions/text/text_classifier),\nand\n[audio](https://developers.google.com/mediapipe/solutions/audio/audio_classifier)\ntasks. If you need help setting up a development environment for use with\nMediaPipe Tasks, check out the setup guides for\n[Android](https://developers.google.com/mediapipe/solutions/setup_android), [web\napps](https://developers.google.com/mediapipe/solutions/setup_web), and\n[Python](https://developers.google.com/mediapipe/solutions/setup_python).\n\n## Solutions\n\nMediaPipe Solutions provides a suite of libraries and tools for you to quickly\napply artificial intelligence (AI) and machine learning (ML) techniques in your\napplications. You can plug these solutions into your applications immediately,\ncustomize them to your needs, and use them across multiple development\nplatforms. MediaPipe Solutions is part of the MediaPipe [open source\nproject](https://github.com/google/mediapipe), so you can further customize the\nsolutions code to meet your application needs.\n\nThese libraries and resources provide the core functionality for each MediaPipe\nSolution:\n\n*   **MediaPipe Tasks**: Cross-platform APIs and libraries for deploying\n    solutions. [Learn\n    more](https://developers.google.com/mediapipe/solutions/tasks).\n*   **MediaPipe models**: Pre-trained, ready-to-run models for use with each\n    solution.\n\nThese tools let you customize and evaluate solutions:\n\n*   **MediaPipe Model Maker**: Customize models for solutions with your data.\n    [Learn more](https://developers.google.com/mediapipe/solutions/model_maker).\n*   **MediaPipe Studio**: Visualize, evaluate, and benchmark solutions in your\n    browser. [Learn\n    more](https://developers.google.com/mediapipe/solutions/studio).\n\n### Legacy solutions\n\nWe have ended support for [these MediaPipe Legacy Solutions](https://developers.google.com/mediapipe/solutions/guide#legacy)\nas of March 1, 2023. All other MediaPipe Legacy Solutions will be upgraded to\na new MediaPipe Solution. See the [Solutions guide](https://developers.google.com/mediapipe/solutions/guide#legacy)\nfor details. The [code repository](https://github.com/google/mediapipe/tree/master/mediapipe)\nand prebuilt binaries for all MediaPipe Legacy Solutions will continue to be\nprovided on an as-is basis.\n\nFor more on the legacy solutions, see the [documentation](https://github.com/google/mediapipe/tree/master/docs/solutions).\n\n## Framework\n\nTo start using MediaPipe Framework, [install MediaPipe\nFramework](https://developers.google.com/mediapipe/framework/getting_started/install)\nand start building example applications in C++, Android, and iOS.\n\n[MediaPipe Framework](https://developers.google.com/mediapipe/framework) is the\nlow-level component used to build efficient on-device machine learning\npipelines, similar to the premade MediaPipe Solutions.\n\nBefore using MediaPipe Framework, familiarize yourself with the following key\n[Framework\nconcepts](https://developers.google.com/mediapipe/framework/framework_concepts/overview.md):\n\n*   [Packets](https://developers.google.com/mediapipe/framework/framework_concepts/packets.md)\n*   [Graphs](https://developers.google.com/mediapipe/framework/framework_concepts/graphs.md)\n*   [Calculators](https://developers.google.com/mediapipe/framework/framework_concepts/calculators.md)\n\n## Community\n\n*   [Slack community](https://mediapipe.page.link/joinslack) for MediaPipe\n    users.\n*   [Discuss](https://groups.google.com/forum/#!forum/mediapipe) - General\n    community discussion around MediaPipe.\n*   [Awesome MediaPipe](https://mediapipe.page.link/awesome-mediapipe) - A\n    curated list of awesome MediaPipe related frameworks, libraries and\n    software.\n\n## Contributing\n\nWe welcome contributions. Please follow these\n[guidelines](https://github.com/google/mediapipe/blob/master/CONTRIBUTING.md).\n\nWe use GitHub issues for tracking requests and bugs. Please post questions to\nthe MediaPipe Stack Overflow with a `mediapipe` tag.\n\n## Resources\n\n### Publications\n\n*   [Bringing artworks to life with AR](https://developers.googleblog.com/2021/07/bringing-artworks-to-life-with-ar.html)\n    in Google Developers Blog\n*   [Prosthesis control via Mirru App using MediaPipe hand tracking](https://developers.googleblog.com/2021/05/control-your-mirru-prosthesis-with-mediapipe-hand-tracking.html)\n    in Google Developers Blog\n*   [SignAll SDK: Sign language interface using MediaPipe is now available for\n    developers](https://developers.googleblog.com/2021/04/signall-sdk-sign-language-interface-using-mediapipe-now-available.html)\n    in Google Developers Blog\n*   [MediaPipe Holistic - Simultaneous Face, Hand and Pose Prediction, on\n    Device](https://ai.googleblog.com/2020/12/mediapipe-holistic-simultaneous-face.html)\n    in Google AI Blog\n*   [Background Features in Google Meet, Powered by Web ML](https://ai.googleblog.com/2020/10/background-features-in-google-meet.html)\n    in Google AI Blog\n*   [MediaPipe 3D Face Transform](https://developers.googleblog.com/2020/09/mediapipe-3d-face-transform.html)\n    in Google Developers Blog\n*   [Instant Motion Tracking With MediaPipe](https://developers.googleblog.com/2020/08/instant-motion-tracking-with-mediapipe.html)\n    in Google Developers Blog\n*   [BlazePose - On-device Real-time Body Pose Tracking](https://ai.googleblog.com/2020/08/on-device-real-time-body-pose-tracking.html)\n    in Google AI Blog\n*   [MediaPipe Iris: Real-time Eye Tracking and Depth Estimation](https://ai.googleblog.com/2020/08/mediapipe-iris-real-time-iris-tracking.html)\n    in Google AI Blog\n*   [MediaPipe KNIFT: Template-based feature matching](https://developers.googleblog.com/2020/04/mediapipe-knift-template-based-feature-matching.html)\n    in Google Developers Blog\n*   [Alfred Camera: Smart camera features using MediaPipe](https://developers.googleblog.com/2020/03/alfred-camera-smart-camera-features-using-mediapipe.html)\n    in Google Developers Blog\n*   [Real-Time 3D Object Detection on Mobile Devices with MediaPipe](https://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html)\n    in Google AI Blog\n*   [AutoFlip: An Open Source Framework for Intelligent Video Reframing](https://ai.googleblog.com/2020/02/autoflip-open-source-framework-for.html)\n    in Google AI Blog\n*   [MediaPipe on the Web](https://developers.googleblog.com/2020/01/mediapipe-on-web.html)\n    in Google Developers Blog\n*   [Object Detection and Tracking using MediaPipe](https://developers.googleblog.com/2019/12/object-detection-and-tracking-using-mediapipe.html)\n    in Google Developers Blog\n*   [On-Device, Real-Time Hand Tracking with MediaPipe](https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html)\n    in Google AI Blog\n*   [MediaPipe: A Framework for Building Perception Pipelines](https://arxiv.org/abs/1906.08172)\n\n### Videos\n\n*   [YouTube Channel](https://www.youtube.com/c/MediaPipe)\n', '{"language":"C++","stars":32330,"forks":5637,"watchers":32330,"open_issues":621,"topics":["android","audio-processing","c-plus-plus","calculator","computer-vision","deep-learning","framework","graph-based","graph-framework","inference","machine-learning","mediapipe","mobile-development","perception","pipeline-framework","stream-processing","video-processing"],"default_branch":"master","size_kb":605811,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:google:mediapipe","source_url":"https://github.com/google/mediapipe"},{"type":"has_code","target_id":"github:google:mediapipe","source_url":"https://github.com/google/mediapipe"},{"type":"has_code","target_id":"github:google:mediapipe","source_url":"https://github.com/google/mediapipe"},{"type":"has_code","target_id":"github:google:mediapipe","source_url":"https://github.com/google/mediapipe"}]', NULL, 'Apache-2.0', 'approved', 65, 'e1b087e75e015a74ce76b7a137389649', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-google-ai-edge-mediapipe from https://github.com/google-ai-edge.png
Image converted to WebP: data/images/github-google-ai-edge-mediapipe.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-lutzroeder-netron', 'github--lutzroeder--netron', 'netron', 'lutzroeder', '<div align="center"> <img width="400px" height="100px" src="https://github.com/lutzroeder/netron/raw/main/.github/logo-light.svg#gh-light-mode-only"> <img width="400px" height="100px" src="https://github.com/lutzroeder/netron/raw/main/.github/logo-dark.svg#gh-dark-mode-only"> </div> Netron is a viewer for neural network, deep learning and machine learning models. Netron supports ONNX, TensorFlow Lite, Core ML, Keras, Caffe, Darknet, PyTorch, TensorFlow.js, Safetensors and NumPy. Netron has ex...', '["ai","coreml","deep-learning","deeplearning","keras","machine-learning","machinelearning","ml","neural-network","numpy","onnx","pytorch","safetensors","tensorflow","tensorflow-lite","visualizer","javascript"]', 'other', 31961, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/lutzroeder/netron","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<div align="center">\n<img width="400px" height="100px" src="https://github.com/lutzroeder/netron/raw/main/.github/logo-light.svg#gh-light-mode-only">\n<img width="400px" height="100px" src="https://github.com/lutzroeder/netron/raw/main/.github/logo-dark.svg#gh-dark-mode-only">\n</div>\n\nNetron is a viewer for neural network, deep learning and machine learning models.\n\nNetron supports ONNX, TensorFlow Lite, Core ML, Keras, Caffe, Darknet, PyTorch, TensorFlow.js, Safetensors and NumPy.\n\nNetron has experimental support for TorchScript, torch.export, ExecuTorch, TensorFlow, OpenVINO, RKNN, ncnn, MNN, PaddlePaddle, GGUF and scikit-learn.\n\n<p align=''center''><a href=''https://www.lutzroeder.com/ai''><img src=''.github/screenshot.png'' width=''800''></a></p>\n\n## Install\n\n**Browser**: [**Start**](https://netron.app) the browser version.\n\n**macOS**: [**Download**](https://github.com/lutzroeder/netron/releases/latest) the `.dmg` file or run `brew install --cask netron`.\n\n**Linux**: [**Download**](https://github.com/lutzroeder/netron/releases/latest) the `.deb` or `.rpm` file.\n\n**Windows**: [**Download**](https://github.com/lutzroeder/netron/releases/latest) the `.exe` installer or run `winget install -s winget netron`.\n\n**Python**: `pip install netron`, then run `netron [FILE]` or `netron.start(''[FILE]'')`.\n\n## Models\n\nSample model files to download or open using the browser version:\n\n * **ONNX**: [squeezenet](https://github.com/onnx/models/raw/main/validated/vision/classification/squeezenet/model/squeezenet1.0-3.onnx) [[open](https://netron.app?url=https://github.com/onnx/models/raw/main/validated/vision/classification/squeezenet/model/squeezenet1.0-3.onnx)]\n * **TorchScript**: [traced_online_pred_layer](https://github.com/ApolloAuto/apollo/raw/master/modules/prediction/data/traced_online_pred_layer.pt) [[open](https://netron.app?url=https://github.com/ApolloAuto/apollo/raw/master/modules/prediction/data/traced_online_pred_layer.pt)]\n * **TensorFlow Lite**: [yamnet](https://huggingface.co/thelou1s/yamnet/resolve/main/lite-model_yamnet_tflite_1.tflite) [[open](https://netron.app?url=https://huggingface.co/thelou1s/yamnet/blob/main/lite-model_yamnet_tflite_1.tflite)]\n * **TensorFlow**: [chessbot](https://github.com/srom/chessbot/raw/master/model/chessbot.pb) [[open](https://netron.app?url=https://github.com/srom/chessbot/raw/master/model/chessbot.pb)]\n * **Keras**: [mobilenet](https://github.com/aio-libs/aiohttp-demos/raw/master/demos/imagetagger/tests/data/mobilenet.h5) [[open](https://netron.app?url=https://github.com/aio-libs/aiohttp-demos/raw/master/demos/imagetagger/tests/data/mobilenet.h5)]\n * **Core ML**: [exermote](https://github.com/Lausbert/Exermote/raw/master/ExermoteInference/ExermoteCoreML/ExermoteCoreML/Model/Exermote.mlmodel) [[open](https://netron.app?url=https://github.com/Lausbert/Exermote/raw/master/ExermoteInference/ExermoteCoreML/ExermoteCoreML/Model/Exermote.mlmodel)]\n * **Darknet**: [yolo](https://github.com/AlexeyAB/darknet/raw/master/cfg/yolo.cfg) [[open](https://netron.app?url=https://github.com/AlexeyAB/darknet/raw/master/cfg/yolo.cfg)]\n', '{"language":"JavaScript","stars":31961,"forks":3040,"watchers":31961,"open_issues":21,"topics":["ai","coreml","deep-learning","deeplearning","keras","machine-learning","machinelearning","ml","neural-network","numpy","onnx","pytorch","safetensors","tensorflow","tensorflow-lite","visualizer"],"default_branch":"main","size_kb":71209,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:lutzroeder:netron","source_url":"https://github.com/lutzroeder/netron"},{"type":"has_code","target_id":"github:lutzroeder:netron","source_url":"https://github.com/lutzroeder/netron"},{"type":"has_code","target_id":"github:lutzroeder:netron","source_url":"https://github.com/lutzroeder/netron"},{"type":"has_code","target_id":"github:lutzroeder:netron","source_url":"https://github.com/lutzroeder/netron"},{"type":"has_code","target_id":"github:lutzroeder:netron","source_url":"https://github.com/lutzroeder/netron"},{"type":"has_code","target_id":"github:onnx:models","source_url":"https://github.com/onnx/models"},{"type":"has_code","target_id":"github:onnx:models","source_url":"https://github.com/onnx/models"},{"type":"has_code","target_id":"github:ApolloAuto:apollo","source_url":"https://github.com/ApolloAuto/apollo"},{"type":"has_code","target_id":"github:ApolloAuto:apollo","source_url":"https://github.com/ApolloAuto/apollo"},{"type":"has_code","target_id":"github:srom:chessbot","source_url":"https://github.com/srom/chessbot"},{"type":"has_code","target_id":"github:srom:chessbot","source_url":"https://github.com/srom/chessbot"},{"type":"has_code","target_id":"github:aio-libs:aiohttp-demos","source_url":"https://github.com/aio-libs/aiohttp-demos"},{"type":"has_code","target_id":"github:aio-libs:aiohttp-demos","source_url":"https://github.com/aio-libs/aiohttp-demos"},{"type":"has_code","target_id":"github:Lausbert:Exermote","source_url":"https://github.com/Lausbert/Exermote"},{"type":"has_code","target_id":"github:Lausbert:Exermote","source_url":"https://github.com/Lausbert/Exermote"},{"type":"has_code","target_id":"github:AlexeyAB:darknet","source_url":"https://github.com/AlexeyAB/darknet"},{"type":"has_code","target_id":"github:AlexeyAB:darknet","source_url":"https://github.com/AlexeyAB/darknet"}]', NULL, 'MIT', 'approved', 65, 'fb3042121c5294e9e993246abef63e68', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-lutzroeder-netron from https://github.com/lutzroeder.png
Image converted to WebP: data/images/github-lutzroeder-netron.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-openai-CLIP', 'github--openai--clip', 'CLIP', 'openai', '[[Blog]](https://openai.com/blog/clip/) [[Paper]](https://arxiv.org/abs/2103.00020) [[Model Card]](model-card.md) [[Colab]](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb) CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to th...', '["deep-learning","machine-learning","jupyter notebook"]', 'other', 31883, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/openai/CLIP","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# CLIP\n\n[[Blog]](https://openai.com/blog/clip/) [[Paper]](https://arxiv.org/abs/2103.00020) [[Model Card]](model-card.md) [[Colab]](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb)\n\nCLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3. We found CLIP matches the performance of the original ResNet50 on ImageNet â€œzero-shotâ€ without using any of the original 1.28M labeled examples, overcoming several major challenges in computer vision.\n\n\n\n## Approach\n\n![CLIP](CLIP.png)\n\n\n\n## Usage\n\nFirst, [install PyTorch 1.7.1](https://pytorch.org/get-started/locally/) (or later) and torchvision, as well as small additional dependencies, and then install this repo as a Python package. On a CUDA GPU machine, the following will do the trick:\n\n```bash\n$ conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n$ pip install ftfy regex tqdm\n$ pip install git+https://github.com/openai/CLIP.git\n```\n\nReplace `cudatoolkit=11.0` above with the appropriate CUDA version on your machine or `cpuonly` when installing on a machine without a GPU.\n\n```python\nimport torch\nimport clip\nfrom PIL import Image\n\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\nmodel, preprocess = clip.load("ViT-B/32", device=device)\n\nimage = preprocess(Image.open("CLIP.png")).unsqueeze(0).to(device)\ntext = clip.tokenize(["a diagram", "a dog", "a cat"]).to(device)\n\nwith torch.no_grad():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    \n    logits_per_image, logits_per_text = model(image, text)\n    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n\nprint("Label probs:", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]\n```\n\n\n## API\n\nThe CLIP module `clip` provides the following methods:\n\n#### `clip.available_models()`\n\nReturns the names of the available CLIP models.\n\n#### `clip.load(name, device=..., jit=False)`\n\nReturns the model and the TorchVision transform needed by the model, specified by the model name returned by `clip.available_models()`. It will download the model as necessary. The `name` argument can also be a path to a local checkpoint.\n\nThe device to run the model can be optionally specified, and the default is to use the first CUDA device if there is any, otherwise the CPU. When `jit` is `False`, a non-JIT version of the model will be loaded.\n\n#### `clip.tokenize(text: Union[str, List[str]], context_length=77)`\n\nReturns a LongTensor containing tokenized sequences of given text input(s). This can be used as the input to the model\n\n---\n\nThe model returned by `clip.load()` supports the following methods:\n\n#### `model.encode_image(image: Tensor)`\n\nGiven a batch of images, returns the image features encoded by the vision portion of the CLIP model.\n\n#### `model.encode_text(text: Tensor)`\n\nGiven a batch of text tokens, returns the text features encoded by the language portion of the CLIP model.\n\n#### `model(image: Tensor, text: Tensor)`\n\nGiven a batch of images and a batch of text tokens, returns two Tensors, containing the logit scores corresponding to each image and text input. The values are cosine similarities between the corresponding image and text features, times 100.\n\n\n\n## More Examples\n\n### Zero-Shot Prediction\n\nThe code below performs zero-shot prediction using CLIP, as shown in Appendix B in the paper. This example takes an image from the [CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), and predicts the most likely labels among the 100 textual labels from the dataset.\n\n```python\nimport os\nimport clip\nimport torch\nfrom torchvision.datasets import CIFAR100\n\n# Load the model\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\nmodel, preprocess = clip.load(''ViT-B/32'', device)\n\n# Download the dataset\ncifar100 = CIFAR100(root=os.path.expanduser("~/.cache"), download=True, train=False)\n\n# Prepare the inputs\nimage, class_id = cifar100[3637]\nimage_input = preprocess(image).unsqueeze(0).to(device)\ntext_inputs = torch.cat([clip.tokenize(f"a photo of a {c}") for c in cifar100.classes]).to(device)\n\n# Calculate features\nwith torch.no_grad():\n    image_features = model.encode_image(image_input)\n    text_features = model.encode_text(text_inputs)\n\n# Pick the top 5 most similar labels for the image\nimage_features /= image_features.norm(dim=-1, keepdim=True)\ntext_features /= text_features.norm(dim=-1, keepdim=True)\nsimilarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\nvalues, indices = similarity[0].topk(5)\n\n# Print the result\nprint("\nTop predictions:\n")\nfor value, index in zip(values, indices):\n    print(f"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%")\n```\n\nThe output will look like the following (the exact numbers may be slightly different depending on the compute device):\n\n```\nTop predictions:\n\n           snake: 65.31%\n          turtle: 12.29%\n    sweet_pepper: 3.83%\n          lizard: 1.88%\n       crocodile: 1.75%\n```\n\nNote that this example uses the `encode_image()` and `encode_text()` methods that return the encoded features of given inputs.\n\n\n### Linear-probe evaluation\n\nThe example below uses [scikit-learn](https://scikit-learn.org/) to perform logistic regression on image features.\n\n```python\nimport os\nimport clip\nimport torch\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR100\nfrom tqdm import tqdm\n\n# Load the model\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\nmodel, preprocess = clip.load(''ViT-B/32'', device)\n\n# Load the dataset\nroot = os.path.expanduser("~/.cache")\ntrain = CIFAR100(root, download=True, train=True, transform=preprocess)\ntest = CIFAR100(root, download=True, train=False, transform=preprocess)\n\n\ndef get_features(dataset):\n    all_features = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n            features = model.encode_image(images.to(device))\n\n            all_features.append(features)\n            all_labels.append(labels)\n\n    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n\n# Calculate the image features\ntrain_features, train_labels = get_features(train)\ntest_features, test_labels = get_features(test)\n\n# Perform logistic regression\nclassifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\nclassifier.fit(train_features, train_labels)\n\n# Evaluate using the logistic regression classifier\npredictions = classifier.predict(test_features)\naccuracy = np.mean((test_labels == predictions).astype(float)) * 100.\nprint(f"Accuracy = {accuracy:.3f}")\n```\n\nNote that the `C` value should be determined via a hyperparameter sweep using a validation split.\n\n\n## See Also\n\n* [OpenCLIP](https://github.com/mlfoundations/open_clip): includes larger and independently trained CLIP models up to ViT-G/14\n* [Hugging Face implementation of CLIP](https://huggingface.co/docs/transformers/model_doc/clip): for easier integration with the HF ecosystem\n', '{"language":"Jupyter Notebook","stars":31883,"forks":3858,"watchers":31883,"open_issues":265,"topics":["deep-learning","machine-learning"],"default_branch":"main","size_kb":9140,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:openai:CLIP.git","source_url":"https://github.com/openai/CLIP.git"},{"type":"has_code","target_id":"github:mlfoundations:open_clip","source_url":"https://github.com/mlfoundations/open_clip"}]', NULL, 'MIT', 'approved', 65, '4a11535c9dd86fc0584b456ad8f4118a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-openai-CLIP from https://github.com/openai.png
Image converted to WebP: data/images/github-openai-CLIP.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Lightning-AI-pytorch-lightning', 'github--lightning-ai--pytorch-lightning', 'pytorch-lightning', 'Lightning-AI', '<div align="center"> <img alt="Lightning" src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/ptl_banner.png" width="800px" style="max-width: 100%;"> <br/> <br/> **The deep learning framework to pretrain and finetune AI models.** **Deploying models?** Check out LitServe, the PyTorch Lightning for inference engines ______________________________________________________________________ <p align="center"> <a href="#quick-start" style="margin: 0 10px;">Quick start</a> â€¢ <a href="#ex...', '["ai","artificial-intelligence","data-science","deep-learning","machine-learning","python","pytorch","python"]', 'other', 30554, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Lightning-AI/pytorch-lightning","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<div align="center">\n\n<img alt="Lightning" src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/ptl_banner.png" width="800px" style="max-width: 100%;">\n\n<br/>\n<br/>\n\n**The deep learning framework to pretrain and finetune AI models.**\n\n**Deploying models?** Check out [LitServe](https://github.com/Lightning-AI/litserve?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme), the PyTorch Lightning for inference engines\n\n______________________________________________________________________\n\n<p align="center">\n    <a href="#quick-start" style="margin: 0 10px;">Quick start</a> â€¢\n  <a href="#examples">Examples</a> â€¢\n  <a href="#why-pytorch-lightning">PyTorch Lightning</a> â€¢\n  <a href="#lightning-fabric-expert-control">Fabric</a> â€¢\n  <a href="https://lightning.ai/?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme">Lightning Cloud</a> â€¢   \n  <a href="#community">Community</a> â€¢\n  <a href="https://pytorch-lightning.readthedocs.io/en/stable/">Docs</a>\n</p>\n\n<!-- DO NOT ADD CONDA DOWNLOADS... README CHANGES MUST BE APPROVED BY EDEN OR WILL -->\n\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/pytorch-lightning)](https://pypi.org/project/pytorch-lightning/)\n[![PyPI Status](https://badge.fury.io/py/pytorch-lightning.svg)](https://badge.fury.io/py/pytorch-lightning)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/pytorch-lightning)](https://pepy.tech/project/pytorch-lightning)\n[![Conda](https://img.shields.io/conda/v/conda-forge/lightning?label=conda&color=success)](https://anaconda.org/conda-forge/lightning)\n[![codecov](https://codecov.io/gh/Lightning-AI/pytorch-lightning/graph/badge.svg?token=SmzX8mnKlA)](https://codecov.io/gh/Lightning-AI/pytorch-lightning)\n\n[![Discord](https://img.shields.io/discord/1077906959069626439?style=plastic)](https://discord.gg/VptPCZkGNa)\n![GitHub commit activity](https://img.shields.io/github/commit-activity/w/lightning-ai/lightning)\n[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/Lightning-AI/pytorch-lightning/blob/master/LICENSE)\n\n<!--\n[![CodeFactor](https://www.codefactor.io/repository/github/Lightning-AI/lightning/badge)](https://www.codefactor.io/repository/github/Lightning-AI/lightning)\n-->\n\n</div>\n\n<div align="center">\n  \n<p align="center">\n\n&nbsp;\n\n<a target="_blank" href="https://lightning.ai/docs/pytorch/latest/starter/introduction.html?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme#define-a-lightningmodule">\n  <img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/get-started-badge.svg" height="36px" alt="Get started"/>\n</a>\n\n</p>\n\n</div>\n\n&nbsp;\n\n# Looking for GPUs?\nOver 340,000 developers use [Lightning Cloud](https://lightning.ai/?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) - purpose-built for PyTorch and PyTorch Lightning. \n- [GPUs](https://lightning.ai/pricing?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) from $0.19.   \n- [Clusters](https://lightning.ai/clusters?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme): frontier-grade training/inference clusters.   \n- [AI Studio (vibe train)](https://lightning.ai/studios?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme): workspaces where AI helps you debug, tune and vibe train.\n- [AI Studio (vibe deploy)](https://lightning.ai/studios?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme): workspaces where AI helps you optimize, and deploy models.     \n- [Notebooks](https://lightning.ai/notebooks?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme): Persistent GPU workspaces where AI helps you code and analyze.\n- [Inference](https://lightning.ai/deploy?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme): Deploy models as inference APIs.   \n\n<a id="why-pytorch-lightning"></a>\n# Why PyTorch Lightning?   \n\nTraining models in plain PyTorch is tedious and error-prone - you have to manually handle things like backprop, mixed precision, multi-GPU, and distributed training, often rewriting code for every new project. PyTorch Lightning organizes PyTorch code to automate those complexities so you can focus on your model and data, while keeping full control and scaling from CPU to multi-node without changing your core code. But if you want control of those things, you can still opt into [expert-level control](#lightning-fabric-expert-control).   \n\nFun analogy: If PyTorch is Javascript, PyTorch Lightning is ReactJS or NextJS.\n\n# Lightning has 2 core packages\n\n[PyTorch Lightning: Train and deploy PyTorch at scale](#why-pytorch-lightning).\n<br/>\n[Lightning Fabric: Expert control](#lightning-fabric-expert-control).\n\nLightning gives you granular control over how much abstraction you want to add over PyTorch.\n\n<div align="center">\n    <img src="https://pl-public-data.s3.amazonaws.com/assets_lightning/continuum.png" width="80%">\n</div>\n\n&nbsp;\n\n# Quick start\nInstall Lightning:\n\n```bash\npip install lightning\n```\n\n<!-- following section will be skipped from PyPI description -->\n\n<details>\n  <summary>Advanced install options</summary>\n    <!-- following section will be skipped from PyPI description -->\n\n#### Install with optional dependencies\n\n```bash\npip install lightning[''extra'']\n```\n\n#### Conda\n\n```bash\nconda install lightning -c conda-forge\n```\n\n#### Install stable version\n\nInstall future release from the source\n\n```bash\npip install https://github.com/Lightning-AI/lightning/archive/refs/heads/release/stable.zip -U\n```\n\n#### Install bleeding-edge\n\nInstall nightly from the source (no guarantees)\n\n```bash\npip install https://github.com/Lightning-AI/lightning/archive/refs/heads/master.zip -U\n```\n\nor from testing PyPI\n\n```bash\npip install -iU https://test.pypi.org/simple/ pytorch-lightning\n```\n\n</details>\n<!-- end skipping PyPI description -->\n\n### PyTorch Lightning example\nDefine the training workflow. Here''s a toy example ([explore real examples](https://lightning.ai/lightning-ai/studios?view=public&section=featured&query=pytorch+lightning&utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme)):\n\n```python\n# main.py\n# ! pip install torchvision\nimport torch, torch.nn as nn, torch.utils.data as data, torchvision as tv, torch.nn.functional as F\nimport lightning as L\n\n# --------------------------------\n# Step 1: Define a LightningModule\n# --------------------------------\n# A LightningModule (nn.Module subclass) defines a full *system*\n# (ie: an LLM, diffusion model, autoencoder, or simple image classifier).\n\n\nclass LitAutoEncoder(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))\n        self.decoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))\n\n    def forward(self, x):\n        # in lightning, forward defines the prediction/inference actions\n        embedding = self.encoder(x)\n        return embedding\n\n    def training_step(self, batch, batch_idx):\n        # training_step defines the train loop. It is independent of forward\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = F.mse_loss(x_hat, x)\n        self.log("train_loss", loss)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer\n\n\n# -------------------\n# Step 2: Define data\n# -------------------\ndataset = tv.datasets.MNIST(".", download=True, transform=tv.transforms.ToTensor())\ntrain, val = data.random_split(dataset, [55000, 5000])\n\n# -------------------\n# Step 3: Train\n# -------------------\nautoencoder = LitAutoEncoder()\ntrainer = L.Trainer()\ntrainer.fit(autoencoder, data.DataLoader(train), data.DataLoader(val))\n```\n\nRun the model on your terminal\n\n```bash\npip install torchvision\npython main.py\n```\n\n&nbsp;\n\n\n# Convert from PyTorch to PyTorch Lightning\n\nPyTorch Lightning is just organized PyTorch - Lightning disentangles PyTorch code to decouple the science from the engineering.\n\n![PT to PL](docs/source-pytorch/_static/images/general/pl_quick_start_full_compressed.gif)\n\n&nbsp;\n\n----\n\n### Examples\nExplore various types of training possible with PyTorch Lightning. Pretrain and finetune ANY kind of model to perform ANY task like classification, segmentation, summarization and more:    \n\n| Task | Description | Run |\n|------|--------------|-----|\n| [Hello world](https://lightning.ai/lightning-ai/studios/pytorch-lightning-hello-world?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Pretrain - Hello world example | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/pytorch-lightning-hello-world?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Image classification](https://lightning.ai/lightning-ai/studios/image-classification-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Finetune - ResNet-34 model to classify images of cars | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/image-classification-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Image segmentation](https://lightning.ai/lightning-ai/studios/image-segmentation-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Finetune - ResNet-50 model to segment images | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/image-segmentation-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Object detection](https://lightning.ai/lightning-ai/studios/object-detection-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Finetune - Faster R-CNN model to detect objects | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/object-detection-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Text classification](https://lightning.ai/lightning-ai/studios/text-classification-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Finetune - text classifier (BERT model) | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/text-classification-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Text summarization](https://lightning.ai/lightning-ai/studios/text-summarization-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Finetune - text summarization (Hugging Face transformer model) | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/text-summarization-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Audio generation](https://lightning.ai/lightning-ai/studios/finetune-a-personal-ai-music-generator?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Finetune - audio generator (transformer model) | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/finetune-a-personal-ai-music-generator?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [LLM finetuning](https://lightning.ai/lightning-ai/studios/finetune-an-llm-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Finetune - LLM (Meta Llama 3.1 8B) | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/finetune-an-llm-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Image generation](https://lightning.ai/lightning-ai/studios/train-a-diffusion-model-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Pretrain - Image generator (diffusion model) | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/train-a-diffusion-model-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Recommendation system](https://lightning.ai/lightning-ai/studios/recommendation-system-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Train - recommendation system (factorization and embedding) | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/recommendation-system-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n| [Time-series forecasting](https://lightning.ai/lightning-ai/studios/time-series-forecasting-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) | Train - Time-series forecasting with LSTM | <a target="_blank" href="https://lightning.ai/lightning-ai/studios/time-series-forecasting-with-pytorch-lightning?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"><img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/app-2/studio-badge.svg" alt="Open In Studio"/></a> |\n\n\n______________________________________________________________________\n\n## Advanced features\n\nLightning has over [40+ advanced features](https://lightning.ai/docs/pytorch/stable/common/trainer.html?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme#trainer-flags)\ndesigned for professional AI research at scale.\n\nHere are some examples:\n\n<div align="center">\n    <img src="https://pl-bolts-doc-images.s3.us-east-2.amazonaws.com/features_2.jpg" max-height="600px">\n  </div>\n\n<details>\n  <summary>Train on 1000s of GPUs without code changes</summary>\n\n```python\n# 8 GPUs\n# no code changes needed\ntrainer = Trainer(accelerator="gpu", devices=8)\n\n# 256 GPUs\ntrainer = Trainer(accelerator="gpu", devices=8, num_nodes=32)\n```\n\n</details>\n\n<details>\n  <summary>Train on other accelerators like TPUs without code changes</summary>\n\n```python\n# no code changes needed\ntrainer = Trainer(accelerator="tpu", devices=8)\n```\n\n</details>\n\n<details>\n  <summary>16-bit precision</summary>\n\n```python\n# no code changes needed\ntrainer = Trainer(precision=16)\n```\n\n</details>\n\n<details>\n  <summary>Experiment managers</summary>\n\n```python\nfrom lightning import loggers\n\n# tensorboard\ntrainer = Trainer(logger=TensorBoardLogger("logs/"))\n\n# weights and biases\ntrainer = Trainer(logger=loggers.WandbLogger())\n\n# comet\ntrainer = Trainer(logger=loggers.CometLogger())\n\n# mlflow\ntrainer = Trainer(logger=loggers.MLFlowLogger())\n\n# neptune\ntrainer = Trainer(logger=loggers.NeptuneLogger())\n\n# ... and dozens more\n```\n\n</details>\n\n<details>\n\n<summary>Early Stopping</summary>\n\n```python\nes = EarlyStopping(monitor="val_loss")\ntrainer = Trainer(callbacks=[es])\n```\n\n</details>\n\n<details>\n  <summary>Checkpointing</summary>\n\n```python\ncheckpointing = ModelCheckpoint(monitor="val_loss")\ntrainer = Trainer(callbacks=[checkpointing])\n```\n\n</details>\n\n<details>\n  <summary>Export to torchscript (JIT) (production use)</summary>\n\n```python\n# torchscript\nautoencoder = LitAutoEncoder()\ntorch.jit.save(autoencoder.to_torchscript(), "model.pt")\n```\n\n</details>\n\n<details>\n  <summary>Export to ONNX (production use)</summary>\n\n```python\n# onnx\nwith tempfile.NamedTemporaryFile(suffix=".onnx", delete=False) as tmpfile:\n    autoencoder = LitAutoEncoder()\n    input_sample = torch.randn((1, 64))\n    autoencoder.to_onnx(tmpfile.name, input_sample, export_params=True)\n    os.path.isfile(tmpfile.name)\n```\n\n</details>\n\n______________________________________________________________________\n\n## Advantages over unstructured PyTorch\n\n- Models become hardware agnostic\n- Code is clear to read because engineering code is abstracted away\n- Easier to reproduce\n- Make fewer mistakes because lightning handles the tricky engineering\n- Keeps all the flexibility (LightningModules are still PyTorch modules), but removes a ton of boilerplate\n- Lightning has dozens of integrations with popular machine learning tools.\n- [Tested rigorously with every new PR](https://github.com/Lightning-AI/lightning/tree/master/tests). We test every combination of PyTorch and Python supported versions, every OS, multi GPUs and even TPUs.\n- Minimal running speed overhead (about 300 ms per epoch compared with pure PyTorch).\n\n______________________________________________________________________\n\n<div align="center">\n    <a href="https://lightning.ai/docs/pytorch/stable/?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme">Read the PyTorch Lightning docs</a>\n</div>\n\n______________________________________________________________________\n\n&nbsp;\n&nbsp;\n\n# Lightning Fabric: Expert control\n\nRun on any device at any scale with expert-level control over PyTorch training loop and scaling strategy. You can even write your own Trainer.\n\nFabric is designed for the most complex models like foundation model scaling, LLMs, diffusion, transformers, reinforcement learning, active learning. Of any size.\n\n<table>\n<tr>\n<th>What to change</th>\n<th>Resulting Fabric Code (copy me!)</th>\n</tr>\n<tr>\n<td>\n<sub>\n\n```diff\n+ import lightning as L\n  import torch; import torchvision as tv\n\n dataset = tv.datasets.CIFAR10("data", download=True,\n                               train=True,\n                               transform=tv.transforms.ToTensor())\n\n+ fabric = L.Fabric()\n+ fabric.launch()\n\n  model = tv.models.resnet18()\n  optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n- device = "cuda" if torch.cuda.is_available() else "cpu"\n- model.to(device)\n+ model, optimizer = fabric.setup(model, optimizer)\n\n  dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\n+ dataloader = fabric.setup_dataloaders(dataloader)\n\n  model.train()\n  num_epochs = 10\n  for epoch in range(num_epochs):\n      for batch in dataloader:\n          inputs, labels = batch\n-         inputs, labels = inputs.to(device), labels.to(device)\n          optimizer.zero_grad()\n          outputs = model(inputs)\n          loss = torch.nn.functional.cross_entropy(outputs, labels)\n-         loss.backward()\n+         fabric.backward(loss)\n          optimizer.step()\n          print(loss.data)\n```\n\n</sub>\n<td>\n<sub>\n\n```Python\nimport lightning as L\nimport torch; import torchvision as tv\n\ndataset = tv.datasets.CIFAR10("data", download=True,\n                              train=True,\n                              transform=tv.transforms.ToTensor())\n\nfabric = L.Fabric()\nfabric.launch()\n\nmodel = tv.models.resnet18()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\nmodel, optimizer = fabric.setup(model, optimizer)\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\ndataloader = fabric.setup_dataloaders(dataloader)\n\nmodel.train()\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        inputs, labels = batch\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = torch.nn.functional.cross_entropy(outputs, labels)\n        fabric.backward(loss)\n        optimizer.step()\n        print(loss.data)\n```\n\n</sub>\n</td>\n</tr>\n</table>\n\n## Key features\n\n<details>\n  <summary>Easily switch from running on CPU to GPU (Apple Silicon, CUDA, â€¦), TPU, multi-GPU or even multi-node training</summary>\n\n```python\n# Use your available hardware\n# no code changes needed\nfabric = Fabric()\n\n# Run on GPUs (CUDA or MPS)\nfabric = Fabric(accelerator="gpu")\n\n# 8 GPUs\nfabric = Fabric(accelerator="gpu", devices=8)\n\n# 256 GPUs, multi-node\nfabric = Fabric(accelerator="gpu", devices=8, num_nodes=32)\n\n# Run on TPUs\nfabric = Fabric(accelerator="tpu")\n```\n\n</details>\n\n<details>\n  <summary>Use state-of-the-art distributed training strategies (DDP, FSDP, DeepSpeed) and mixed precision out of the box</summary>\n\n```python\n# Use state-of-the-art distributed training techniques\nfabric = Fabric(strategy="ddp")\nfabric = Fabric(strategy="deepspeed")\nfabric = Fabric(strategy="fsdp")\n\n# Switch the precision\nfabric = Fabric(precision="16-mixed")\nfabric = Fabric(precision="64")\n```\n\n</details>\n\n<details>\n  <summary>All the device logic boilerplate is handled for you</summary>\n\n```diff\n  # no more of this!\n- model.to(device)\n- batch.to(device)\n```\n\n</details>\n\n<details>\n  <summary>Build your own custom Trainer using Fabric primitives for training checkpointing, logging, and more</summary>\n\n```python\nimport lightning as L\n\n\nclass MyCustomTrainer:\n    def __init__(self, accelerator="auto", strategy="auto", devices="auto", precision="32-true"):\n        self.fabric = L.Fabric(accelerator=accelerator, strategy=strategy, devices=devices, precision=precision)\n\n    def fit(self, model, optimizer, dataloader, max_epochs):\n        self.fabric.launch()\n\n        model, optimizer = self.fabric.setup(model, optimizer)\n        dataloader = self.fabric.setup_dataloaders(dataloader)\n        model.train()\n\n        for epoch in range(max_epochs):\n            for batch in dataloader:\n                input, target = batch\n                optimizer.zero_grad()\n                output = model(input)\n                loss = loss_fn(output, target)\n                self.fabric.backward(loss)\n                optimizer.step()\n```\n\nYou can find a more extensive example in our [examples](examples/fabric/build_your_own_trainer)\n\n</details>\n\n______________________________________________________________________\n\n<div align="center">\n    <a href="https://lightning.ai/docs/fabric/stable/?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme">Read the Lightning Fabric docs</a>\n</div>\n\n______________________________________________________________________\n\n&nbsp;\n&nbsp;\n\n## Examples\n\n###### Self-supervised Learning\n\n- [CPC transforms](https://lightning-bolts.readthedocs.io/en/stable/transforms/self_supervised.html#cpc-transforms)\n- [Moco v2 transforms](https://lightning-bolts.readthedocs.io/en/stable/transforms/self_supervised.html#moco-v2-transforms)\n- [SimCLR transforms](https://lightning-bolts.readthedocs.io/en/stable/transforms/self_supervised.html#simclr-transforms)\n\n###### Convolutional Architectures\n\n- [GPT-2](https://lightning-bolts.readthedocs.io/en/stable/models/convolutional.html#gpt-2)\n- [UNet](https://lightning-bolts.readthedocs.io/en/stable/models/convolutional.html#unet)\n\n###### Reinforcement Learning\n\n- [DQN Loss](https://lightning-bolts.readthedocs.io/en/stable/losses.html#dqn-loss)\n- [Double DQN Loss](https://lightning-bolts.readthedocs.io/en/stable/losses.html#double-dqn-loss)\n- [Per DQN Loss](https://lightning-bolts.readthedocs.io/en/stable/losses.html#per-dqn-loss)\n\n###### GANs\n\n- [Basic GAN](https://lightning-bolts.readthedocs.io/en/stable/models/gans.html#basic-gan)\n- [DCGAN](https://lightning-bolts.readthedocs.io/en/stable/models/gans.html#dcgan)\n\n###### Classic ML\n\n- [Logistic Regression](https://lightning-bolts.readthedocs.io/en/stable/models/classic_ml.html#logistic-regression)\n- [Linear Regression](https://lightning-bolts.readthedocs.io/en/stable/models/classic_ml.html#linear-regression)\n\n&nbsp;\n&nbsp;\n\n## Continuous Integration\n\nLightning is rigorously tested across multiple CPUs, GPUs and TPUs and against major Python and PyTorch versions.\n\n###### \*Codecov is > 90%+ but build delays may show less\n\n<details>\n  <summary>Current build statuses</summary>\n\n<center>\n\n|       System / PyTorch ver.        | 1.13                                                                                                                                                                                                                            | 2.0                                                                                                                                                                                                                             |                                                                                                               2.1                                                                                                               |\n| :--------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n|        Linux py3.9 \[GPUs\]        |  |  | [![Build Status](https://dev.azure.com/Lightning-AI/lightning/_apis/build/status%2Fpytorch-lightning%20%28GPUs%29?branchName=master)](https://dev.azure.com/Lightning-AI/lightning/_build/latest?definitionId=24&branchName=master) |\n|  Linux (multiple Python versions)  | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 |                 [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                 |\n|   OSX (multiple Python versions)   | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 |                 [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                 |\n| Windows (multiple Python versions) | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                                 |                 [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml)                 |\n\n</center>\n</details>\n\n&nbsp;\n&nbsp;\n\n## Community\n\nThe lightning community is maintained by\n\n- [10+ core contributors](https://lightning.ai/docs/pytorch/latest/community/governance.html) who are all a mix of professional engineers, Research Scientists, and Ph.D. students from top AI labs.\n- 800+ community contributors.\n\nWant to help us build Lightning and reduce boilerplate for thousands of researchers? [Learn how to make your first contribution here](https://lightning.ai/docs/pytorch/stable/generated/CONTRIBUTING.html?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme)\n\nLightning is also part of the [PyTorch ecosystem](https://pytorch.org/ecosystem/) which requires projects to have solid testing, documentation and support.\n\n### Asking for help\n\nIf you have any questions please:\n\n1. [Read the docs](https://lightning.ai/docs?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme).\n1. [Search through existing Discussions](https://github.com/Lightning-AI/lightning/discussions), or [add a new question](https://github.com/Lightning-AI/lightning/discussions/new)\n1. [Join our discord](https://discord.com/invite/tfXFetEZxv).\n', '{"language":"Python","stars":30554,"forks":3619,"watchers":30554,"open_issues":920,"topics":["ai","artificial-intelligence","data-science","deep-learning","machine-learning","python","pytorch"],"default_branch":"master","size_kb":132374,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:Lightning-AI:litserve","source_url":"https://github.com/Lightning-AI/litserve?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme"},{"type":"has_code","target_id":"github:Lightning-AI:pytorch-lightning","source_url":"https://github.com/Lightning-AI/pytorch-lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"},{"type":"has_code","target_id":"github:Lightning-AI:lightning","source_url":"https://github.com/Lightning-AI/lightning"}]', NULL, 'Apache-2.0', 'approved', 80, '500f6ac54ba616a0e2a7c606810b9b64', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Lightning-AI-pytorch-lightning from https://github.com/Lightning-AI.png
Image converted to WebP: data/images/github-Lightning-AI-pytorch-lightning.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-AMAI-GmbH-AI-Expert-Roadmap', 'github--amai-gmbh--ai-expert-roadmap', 'AI-Expert-Roadmap', 'AMAI-GmbH', '<p align="center"> <a href="https://github.com/AMAI-GmbH/AI-Expert-Roadmap"> <img src="https://uploads-ssl.webflow.com/58e6a2b25c28230d367487ad/5c32232ecb585fcc5c4645e1_icon_machine-learning.svg" alt="Developer Roadmap" width="96" height="96"> </a> <h2 align="center">i.am.ai<br>AI Expert Roadmap</h2> <p align="center">Roadmap to becoming an Artificial Intelligence Expert in 2022</p> <p align="center"> <a href="https://twitter.com/home?status=https://i.am.ai/roadmap Roadmap to becoming an Arti...', '["ai","ai-roadmap","artificial-intelligence","data-analysis","data-science","deep-learning","machine-learning","neural-network","roadmap","study-plan","javascript"]', 'other', 30531, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/AMAI-GmbH/AI-Expert-Roadmap","fetched_at":"2025-12-08T10:30:37.947Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <a href="https://github.com/AMAI-GmbH/AI-Expert-Roadmap">\n    <img src="https://uploads-ssl.webflow.com/58e6a2b25c28230d367487ad/5c32232ecb585fcc5c4645e1_icon_machine-learning.svg" alt="Developer Roadmap" width="96" height="96">\n  </a>\n  <h2 align="center">i.am.ai<br>AI Expert Roadmap</h2>\n  <p align="center">Roadmap to becoming an Artificial Intelligence Expert in 2022</p>\n  <p align="center">\n      <a href="https://twitter.com/home?status=https://i.am.ai/roadmap Roadmap to becoming an Artificial Intelligence Expert in 2022" target="_blank"><img src="https://img.shields.io/badge/tweet-blue.svg?logo=twitter&logoColor=white" style="display: inherit;"/></a>\n      <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://i.am.ai/roadmap&title=&summary=Roadmap to becoming an Artificial Intelligence Expert in 2022&source=" target="_blank"><img src="https://img.shields.io/badge/post-blue.svg?logo=linkedin&logoColor=white" style="display: inherit;"/></a>\n      <a href="https://github.com/AMAI-GmbH/AI-Expert-Roadmap"><img src="https://img.shields.io/badge/Roadmap-2022-yellowgreen.svg" style="display: inherit;"/></a>\n      <a href="https://am.ai?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Badge" target="_blank"><img alt="AMAI GmbH" src="https://img.shields.io/badge/Author-AMAI GmbH-blue.svg" style="display: inherit;"/></a>\n<a href="https://opensource.org/licenses/MIT/" target="_blank"><img alt="MIT License" src="https://img.shields.io/badge/License-MIT-blue.svg" style="display: inherit;"/></a>\n  </p>\n  <br>\n</p>\n\nBelow you find a set of charts demonstrating the paths that you can take and the technologies that you would want to adopt in order to become a data scientist, machine learning or an AI expert. We made these charts for our new employees to make them AI Experts but we wanted to share them here to help the community.\n\nIf you are interested to become an AI EXPERT at [AMAI](https://www.linkedin.com/company/amai-gmbh/?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Become+Expert) in Germany, or you want to [hire an AI Expert](https://am.ai?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Hire+Expert), please say [hi@am.ai](mailto:hi@am.ai).\n\n## Note\n\nðŸ‘‰ An **interactive version with links to follow** about each bullet of the list can be found at [i.am.ai/roadmap](https://i.am.ai/roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Interactive) ðŸ‘ˆ\n\nTo receive updates [star :star:](https://github.com/AMAI-GmbH/AI-Expert-Roadmap/stargazers) and watch :eyes: the [GitHub Repo](https://github.com/AMAI-GmbH/AI-Expert-Roadmap/) to get notified, when we add new content to stay on the top of the most recent research.\n\nFollow our [AI Newsletter](https://i.am.ai/newsletter?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Newsletter) to stay up to date with the latest developments in AI. We cover new use cases and research topics.\n\n## Disclaimer\n\nThe purpose of these roadmaps is to give you an idea about the landscape and to guide you if you are confused about what to learn next and not to encourage you to pick what is hip and trendy. You should grow some understanding of why one tool would be better suited for some cases than the other and remember hip and trendy never means best suited for the job.\n\n## Introduction\n\n<p align="center">\n  <a href="https://i.am.ai/roadmap#introduction?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Introduction" target="_blank">\n      <img src="./images/intro.svg"/>\n  </a>\n</p>\n\n## Fundamentals\n\n<p align="center">\n  <a href="https://i.am.ai/roadmap#fundamentals?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+Fundamentals" target="_blank">\n      <img src="./images/fundamentals.svg"/>\n  </a>\n</p>\n\n## Data Science Roadmap\n\n<p align="center">\n  <a href="https://i.am.ai/roadmap#data-science-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+DataScience" target="_blank">\n      <img src="./images/datascience.svg"/>\n  </a>\n</p>\n\n## Machine Learning Roadmap\n\n<p align="center">\n  <a href="https://i.am.ai/roadmap#machine-learning-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+MachineLearning" target="_blank">\n      <img src="./images/machine_learning.svg"/>\n  </a>\n</p>\n\n## Deep Learning Roadmap\n\n<p align="center">\n  <a href="https://i.am.ai/roadmap#deep-learning-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+DeepLearning" target="_blank">\n      <img src="./images/deep_learning.svg"/>\n  </a>\n</p>\n\n## Data Engineer Roadmap\n\n<p align="center">\n  <a href="https://i.am.ai/roadmap#data-engineer-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+DataEngineer" target="_blank">\n      <img src="./images/data_engineer.svg"/>\n  </a>\n</p>\n\n## Big Data Engineer Roadmap\n\n<p align="center">\n  <a href="https://i.am.ai/roadmap#big-data-engineer-roadmap?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+BigDataEngineer" target="_blank">\n      <img src="./images/big_data_engineer.svg"/>\n  </a>\n</p>\n\n## ðŸš¦ Wrap Up\n\nIf you think any of the roadmaps can be improved, please do open a PR with any updates and submit any issues. Also, we will continue to improve this, so you might want to watch/star this repository to revisit.\n\n## ðŸ™Œ Contribution\n\n> Have a look at the [contribution docs](./contributing.md) for how to update any of the roadmaps\n\n* Open pull request with improvements\n* Discuss ideas in issues\n* Spread the word\n* Reach out with any feedback\n\n## Supported By\n\n<a href="https://www.linkedin.com/company/amai-gmbh/?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap+SupportedBy" target="_blank"><img alt="AMAI GmbH" src="./images/logos/amai.svg" style="display: inherit;max-width: 150px;"/></a>\n<a href="https://digitalhub-ai.de?utm_source=GitHub&utm_medium=Referral&utm_campaign=AI+Expert+Roadmap" target="_blank"><img alt="AMAI GmbH" src="./images/logos/de-hub.svg" style="display: inherit; max-width: 150px;"/></a>\n', '{"language":"JavaScript","stars":30531,"forks":2549,"watchers":30531,"open_issues":15,"topics":["ai","ai-roadmap","artificial-intelligence","data-analysis","data-science","deep-learning","machine-learning","neural-network","roadmap","study-plan"],"default_branch":"main","size_kb":414,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:AMAI-GmbH:AI-Expert-Roadmap\">","source_url":"https://github.com/AMAI-GmbH/AI-Expert-Roadmap\">"},{"type":"has_code","target_id":"github:AMAI-GmbH:AI-Expert-Roadmap\"><img","source_url":"https://github.com/AMAI-GmbH/AI-Expert-Roadmap\"><img"},{"type":"has_code","target_id":"github:AMAI-GmbH:AI-Expert-Roadmap","source_url":"https://github.com/AMAI-GmbH/AI-Expert-Roadmap"},{"type":"has_code","target_id":"github:AMAI-GmbH:AI-Expert-Roadmap","source_url":"https://github.com/AMAI-GmbH/AI-Expert-Roadmap"}]', NULL, 'MIT', 'approved', 65, '5f1a62a38943510c58736e39d645c2b1', NULL, NULL, CURRENT_TIMESTAMP);
