/* LOGS:
Downloading image for github-deezer-spleeter from https://github.com/deezer.png
Image converted to WebP: data/images/github-deezer-spleeter.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-svc-develop-team-so-vits-svc', 'github--svc-develop-team--so-vits-svc', 'so-vits-svc', 'svc-develop-team', '<div align="center"> <img alt="LOGO" src="https://avatars.githubusercontent.com/u/127122328?s=400&u=5395a98a4f945a3a50cb0cc96c2747505d190dbc&v=4" width="300" height="300" /> **English** | **ä¸­æ–‡ç®€ä½“** This round of limited time update is coming to an end, the warehouse will enter the Archieve state, please know </div> > âœ¨ A studio that contains visible f0 editor, speaker mix timeline editor and other features (Where the Onnx models are used) : MoeVoiceStudio > âœ¨ A fork with a greatly improved use...', '["ai","audio-analysis","deep-learning","flow","generative-adversarial-network","pytorch","singing-voice-conversion","so-vits-svc","sovits","speech","variational-inference","vc","vits","voice","voice-changer","voice-conversion","voiceconversion","python"]', 'other', 27842, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/svc-develop-team/so-vits-svc","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n<img alt="LOGO" src="https://avatars.githubusercontent.com/u/127122328?s=400&u=5395a98a4f945a3a50cb0cc96c2747505d190dbc&v=4" width="300" height="300" />\n  \n# SoftVC VITS Singing Voice Conversion\n\n[**English**](./README.md) | [**ä¸­æ–‡ç®€ä½“**](./README_zh_CN.md)\n\n[![Open In Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)](https://colab.research.google.com/github/svc-develop-team/so-vits-svc/blob/4.1-Stable/sovits4_for_colab.ipynb)\n[![Licence](https://img.shields.io/badge/LICENSE-AGPL3.0-green.svg?style=for-the-badge)](https://github.com/svc-develop-team/so-vits-svc/blob/4.1-Stable/LICENSE)\n\nThis round of limited time update is coming to an end, the warehouse will enter the Archieve state, please know\n\n</div>\n\n> âœ¨ A studio that contains visible f0 editor, speaker mix timeline editor and other features (Where the Onnx models are used) : [MoeVoiceStudio](https://github.com/NaruseMioShirakana/MoeVoiceStudio)\n\n> âœ¨ A fork with a greatly improved user interface: [34j/so-vits-svc-fork](https://github.com/34j/so-vits-svc-fork)\n\n> âœ¨ A client supports real-time conversion: [w-okada/voice-changer](https://github.com/w-okada/voice-changer)\n\n**This project differs fundamentally from VITS, as it focuses on Singing Voice Conversion (SVC) rather than Text-to-Speech (TTS). In this project, TTS functionality is not supported, and VITS is incapable of performing SVC tasks. It''s important to note that the models used in these two projects are not interchangeable or universally applicable.**\n\n## Announcement\n\nThe purpose of this project was to enable developers to have their beloved anime characters perform singing tasks. The developers'' intention was to focus solely on fictional characters and avoid any involvement of real individuals, anything related to real individuals deviates from the developer''s original intention.\n\n## Disclaimer\n\nThis project is an open-source, offline endeavor, and all members of SvcDevelopTeam, as well as other developers and maintainers involved (hereinafter referred to as contributors), have no control over the project. The contributors have never provided any form of assistance to any organization or individual, including but not limited to dataset extraction, dataset processing, computing support, training support, inference, and so on. The contributors do not and cannot be aware of the purposes for which users utilize the project. Therefore, any AI models and synthesized audio produced through the training of this project are unrelated to the contributors. Any issues or consequences arising from their use are the sole responsibility of the user.\n\nThis project is run completely offline and does not collect any user information or gather user input data. Therefore, contributors to this project are not aware of all user input and models and therefore are not responsible for any user input.\n\nThis project serves as a framework only and does not possess speech synthesis functionality by itself. All functionalities require users to train the models independently. Furthermore, this project does not come bundled with any models, and any secondary distributed projects are independent of the contributors of this project.\n\n## ğŸ“ Terms of Use\n\n# Warning: Please ensure that you address any authorization issues related to the dataset on your own. You bear full responsibility for any problems arising from the usage of non-authorized datasets for training, as well as any resulting consequences. The repository and its maintainer, svc develop team, disclaim any association with or liability for the consequences. \n\n1. This project is exclusively established for academic purposes, aiming to facilitate communication and learning. It is not intended for deployment in production environments.\n2. Any sovits-based video posted to a video platform must clearly specify in the introduction the input source vocals and audio used for the voice changer conversion, e.g., if you use someone else''s video/audio and convert it by separating the vocals as the input source, you must give a clear link to the original video or music; if you use your own vocals or a voice synthesized by another voice synthesis engine as the input source, you must also state this in your introduction.\n3. You are solely responsible for any infringement issues caused by the input source and all consequences. When using other commercial vocal synthesis software as an input source, please ensure that you comply with the regulations of that software, noting that the regulations of many vocal synthesis engines explicitly state that they cannot be used to convert input sources!\n4. Engaging in illegal activities, as well as religious and political activities, is strictly prohibited when using this project. The project developers vehemently oppose the aforementioned activities. If you disagree with this provision, the usage of the project is prohibited.\n5. If you continue to use the program, you will be deemed to have agreed to the terms and conditions set forth in README and README has discouraged you and is not responsible for any subsequent problems.\n6. If you intend to employ this project for any other purposes, kindly contact and inform the maintainers of this repository in advance.\n\n## ğŸ“ Model Introduction\n\nThe singing voice conversion model uses SoftVC content encoder to extract speech features from the source audio. These feature vectors are directly fed into VITS without the need for conversion to a text-based intermediate representation. As a result, the pitch and intonations of the original audio are preserved. Meanwhile, the vocoder was replaced with [NSF HiFiGAN](https://github.com/openvpi/DiffSinger/tree/refactor/modules/nsf_hifigan) to solve the problem of sound interruption.\n\n### ğŸ†• 4.1-Stable Version Update Content\n\n- Feature input is changed to the 12th Layer of [Content Vec](https://github.com/auspicious3000/contentvec) Transformer output, And compatible with 4.0 branches.\n- Update the shallow diffusion, you can use the shallow diffusion model to improve the sound quality.\n- Added Whisper-PPG encoder support\n- Added static/dynamic sound fusion\n- Added loudness embedding\n- Added Functionality of feature retrieval from [RVC](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI)\n  \n### ğŸ†• Questions about compatibility with the 4.0 model\n\n- To support the 4.0 model and incorporate the speech encoder, you can make modifications to the `config.json` file. Add the `speech_encoder` field to the "model" section as shown below:\n\n```\n  "model": {\n    .........\n    "ssl_dim": 256,\n    "n_speakers": 200,\n    "speech_encoder":"vec256l9"\n  }\n```\n\n### ğŸ†• Shallow diffusion\n![Diagram](shadowdiffusion.png)\n\n## ğŸ’¬ Python Version\n\nBased on our testing, we have determined that the project runs stable on `Python 3.8.9`.\n\n## ğŸ“¥ Pre-trained Model Files\n\n#### **Required**\n\n**You need to select one encoder from the list below**\n\n##### **1. If using contentvec as speech encoder(recommended)**\n\n`vec768l12` and `vec256l9` require the encoder\n\n- ContentVec: [checkpoint_best_legacy_500.pt](https://ibm.box.com/s/z1wgl1stco8ffooyatzdwsqn2psd9lrr)\n  - Place it under the `pretrain` directory\n\nOr download the following ContentVec, which is only 199MB in size but has the same effect:\n- ContentVec: [hubert_base.pt](https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt)\n  - Change the file name to `checkpoint_best_legacy_500.pt` and place it in the `pretrain` directory\n\n```shell\n# contentvec\nwget -P pretrain/ https://huggingface.co/lj1995/VoiceConversionWebUI/resolve/main/hubert_base.pt -O checkpoint_best_legacy_500.pt\n# Alternatively, you can manually download and place it in the hubert directory\n```\n\n##### **2. If hubertsoft is used as the speech encoder**\n- soft vc hubert: [hubert-soft-0d54a1f4.pt](https://github.com/bshall/hubert/releases/download/v0.1/hubert-soft-0d54a1f4.pt)\n  - Place it under the `pretrain` directory\n\n##### **3. If whisper-ppg as the encoder**\n- download model at [medium.pt](https://openaipublic.azureedge.net/main/whisper/models/345ae4da62f9b3d59415adc60127b97c714f32e89e936602e85993674d08dcb1/medium.pt), the model fits `whisper-ppg`\n- or download model at [large-v2.pt](https://openaipublic.azureedge.net/main/whisper/models/81f7c96c852ee8fc832187b0132e569d6c3065a3252ed18e56effd0b6a73e524/large-v2.pt), the model fits `whisper-ppg-large`\n  - Place it under the `pretrain` directory\n  \n##### **4. If cnhubertlarge as the encoder**\n- download model at [chinese-hubert-large-fairseq-ckpt.pt](https://huggingface.co/TencentGameMate/chinese-hubert-large/resolve/main/chinese-hubert-large-fairseq-ckpt.pt)\n  - Place it under the `pretrain` directory\n\n##### **5. If dphubert as the encoder**\n- download model at [DPHuBERT-sp0.75.pth](https://huggingface.co/pyf98/DPHuBERT/resolve/main/DPHuBERT-sp0.75.pth)\n  - Place it under the `pretrain` directory\n\n##### **6. If WavLM is used as the encoder**\n- download model at  [WavLM-Base+.pt](https://valle.blob.core.windows.net/share/wavlm/WavLM-Base+.pt?sv=2020-08-04&st=2023-03-01T07%3A51%3A05Z&se=2033-03-02T07%3A51%3A00Z&sr=c&sp=rl&sig=QJXmSJG9DbMKf48UDIU1MfzIro8HQOf3sqlNXiflY1I%3D), the model fits `wavlmbase+`\n  - Place it under the `pretrain` directory\n\n##### **7. If OnnxHubert/ContentVec as the encoder**\n- download model at [MoeSS-SUBModel](https://huggingface.co/NaruseMioShirakana/MoeSS-SUBModel/tree/main)\n  - Place it under the `pretrain` directory\n\n#### **List of Encoders**\n- "vec768l12"\n- "vec256l9"\n- "vec256l9-onnx"\n- "vec256l12-onnx"\n- "vec768l9-onnx"\n- "vec768l12-onnx"\n- "hubertsoft-onnx"\n- "hubertsoft"\n- "whisper-ppg"\n- "cnhubertlarge"\n- "dphubert"\n- "whisper-ppg-large"\n- "wavlmbase+"\n\n#### **Optional(Strongly recommend)**\n\n- Pre-trained model files: `G_0.pth` `D_0.pth`\n  - Place them under the `logs/44k` directory\n\n- Diffusion model pretraining base model file: `model_0.pt`\n  - Put it in the `logs/44k/diffusion` directory\n\nGet Sovits Pre-trained model from svc-develop-team(TBD) or anywhere else.\n\nDiffusion model references [Diffusion-SVC](https://github.com/CNChTu/Diffusion-SVC) diffusion model. The pre-trained diffusion model is universal with the DDSP-SVC''s. You can go to [Diffusion-SVC](https://github.com/CNChTu/Diffusion-SVC)''s repo to get the pre-trained diffusion model.\n\nWhile the pretrained model typically does not pose copyright concerns, it is essential to remain vigilant. It is advisable to consult with the author beforehand or carefully review the description to ascertain the permissible usage of the model. This helps ensure compliance with any specified guidelines or restrictions regarding its utilization.\n\n#### **Optional(Select as Required)**\n\n##### NSF-HIFIGAN\n\nIf you are using the `NSF-HIFIGAN enhancer` or `shallow diffusion`, you will need to download the pre-trained NSF-HIFIGAN model.\n\n- Pre-trained NSF-HIFIGAN Vocoder: [nsf_hifigan_20221211.zip](https://github.com/openvpi/vocoders/releases/download/nsf-hifigan-v1/nsf_hifigan_20221211.zip)\n  - Unzip and place the four files under the `pretrain/nsf_hifigan` directory\n\n```shell\n# nsf_hifigan\nwget -P pretrain/ https://github.com/openvpi/vocoders/releases/download/nsf-hifigan-v1/nsf_hifigan_20221211.zip\nunzip -od pretrain/nsf_hifigan pretrain/nsf_hifigan_20221211.zip\n# Alternatively, you can manually download and place it in the pretrain/nsf_hifigan directory\n# URL: https://github.com/openvpi/vocoders/releases/tag/nsf-hifigan-v1\n```\n\n##### RMVPE\n\nIf you are using the `rmvpe` F0 Predictor, you will need to download the pre-trained RMVPE model.\n\n+ download model at [rmvpe.zip](https://github.com/yxlllc/RMVPE/releases/download/230917/rmvpe.zip), this weight is recommended.\n  + unzip `rmvpe.zip`ï¼Œand rename the `model.pt` file to `rmvpe.pt` and place it under the `pretrain` directory.\n\n- ~~download model at [rmvpe.pt](https://huggingface.co/datasets/ylzz1997/rmvpe_pretrain_model/resolve/main/rmvpe.pt)~~\n  - ~~Place it under the `pretrain` directory~~\n\n##### FCPE(Preview version)\n\n[FCPE(Fast Context-base Pitch Estimator)](https://github.com/CNChTu/MelPE) is a dedicated F0 predictor designed for real-time voice conversion and will become the preferred F0 predictor for sovits real-time voice conversion in the future.(The paper is being written)\n\nIf you are using the `fcpe` F0 Predictor, you will need to download the pre-trained FCPE model.\n\n- download model at [fcpe.pt](https://huggingface.co/datasets/ylzz1997/rmvpe_pretrain_model/resolve/main/fcpe.pt)\n  - Place it under the `pretrain` directory\n\n## ğŸ“Š Dataset Preparation\n\nSimply place the dataset in the `dataset_raw` directory with the following file structure:\n\n```\ndataset_raw\nâ”œâ”€â”€â”€speaker0\nâ”‚   â”œâ”€â”€â”€xxx1-xxx1.wav\nâ”‚   â”œâ”€â”€â”€...\nâ”‚   â””â”€â”€â”€Lxx-0xx8.wav\nâ””â”€â”€â”€speaker1\n    â”œâ”€â”€â”€xx2-0xxx2.wav\n    â”œâ”€â”€â”€...\n    â””â”€â”€â”€xxx7-xxx007.wav\n```\nThere are no specific restrictions on the format of the name for each audio file (naming conventions such as `000001.wav` to `999999.wav` are also valid), but the file type must be `WAV``.\n\nYou can customize the speaker''s name as showed below:\n\n```\ndataset_raw\nâ””â”€â”€â”€suijiSUI\n    â”œâ”€â”€â”€1.wav\n    â”œâ”€â”€â”€...\n    â””â”€â”€â”€25788785-20221210-200143-856_01_(Vocals)_0_0.wav\n```\n\n## ğŸ› ï¸ Preprocessing\n\n### 0. Slice audio\n\nTo avoid video memory overflow during training or pre-processing, it is recommended to limit the length of audio clips. Cutting the audio to a length of "5s - 15s" is more recommended. Slightly longer times are acceptable, however, excessively long clips may cause problems such as `torch.cuda.OutOfMemoryError`.\n\nTo facilitate the slicing process, you can use [audio-slicer-GUI](https://github.com/flutydeer/audio-slicer) or [audio-slicer-CLI](https://github.com/openvpi/audio-slicer)\n\nIn general, only the `Minimum Interval` needs to be adjusted. For spoken audio, the default value usually suffices, while for singing audio, it can be adjusted to around `100` or even `50`, depending on the specific requirements.\n\nAfter slicing, it is recommended to remove any audio clips that are excessively long or too short.\n\n**If you are using whisper-ppg encoder for training, the audio clips must shorter than 30s.**\n\n### 1. Resample to 44100Hz and mono\n\n```shell\npython resample.py\n```\n\n#### Cautions\n\nAlthough this project has resample.py scripts for resampling, mono and loudness matching, the default loudness matching is to match to 0db. This can cause damage to the sound quality. While python''s loudness matching package pyloudnorm does not limit the level, this can lead to sonic boom. Therefore, it is recommended to consider using professional sound processing software, such as `adobe audition` for loudness matching. If you are already using other software for loudness matching, add the parameter `-skip_loudnorm` to the run command:\n\n```shell\npython resample.py --skip_loudnorm\n```\n\n### 2. Automatically split the dataset into training and validation sets, and generate configuration files.\n\n```shell\npython preprocess_flist_config.py --speech_encoder vec768l12\n```\n\nspeech_encoder has the following options\n\n```\nvec768l12\nvec256l9\nhubertsoft\nwhisper-ppg\ncnhubertlarge\ndphubert\nwhisper-ppg-large\nwavlmbase+\n```\n\nIf the speech_encoder argument is omitted, the default value is `vec768l12`\n\n**Use loudness embedding**\n\nAdd `--vol_aug` if you want to enable loudness embedding:\n\n```shell\npython preprocess_flist_config.py --speech_encoder vec768l12 --vol_aug\n```\n\nAfter enabling loudness embedding, the trained model will match the loudness of the input source; otherwise, it will match the loudness of the training set.\n\n#### You can modify some parameters in the generated config.json and diffusion.yaml\n\n* `keep_ckpts`: Keep the the the number of previous models during training. Set to `0` to keep them all. Default is `3`.\n\n* `all_in_mem`: Load all dataset to RAM. It can be enabled when the disk IO of some platforms is too low and the system memory is **much larger** than your dataset.\n  \n* `batch_size`: The amount of data loaded to the GPU for a single training session can be adjusted to a size lower than the GPU memory capacity.\n\n* `vocoder_name`: Select a vocoder. The default is `nsf-hifigan`.\n\n##### diffusion.yaml\n\n* `cache_all_data`: Load all dataset to RAM. It can be enabled when the disk IO of some platforms is too low and the system memory is **much larger** than your dataset.\n\n* `duration`: The duration of the audio slicing during training, can be adjusted according to the size of the video memory, **Note: this value must be less than the minimum time of the audio in the training set!**\n\n* `batch_size`: The amount of data loaded to the GPU for a single training session can be adjusted to a size lower than the video memory capacity.\n\n* `timesteps`: The total number of steps in the diffusion model, which defaults to 1000.\n\n* `k_step_max`: Training can only train `k_step_max` step diffusion to save training time, note that the value must be less than `timesteps`, 0 is to train the entire diffusion model, **Note: if you do not train the entire diffusion model will not be able to use only_diffusion!**\n\n##### **List of Vocoders**\n\n```\nnsf-hifigan\nnsf-snake-hifigan\n```\n\n### 3. Generate hubert and f0\n\n```shell\npython preprocess_hubert_f0.py --f0_predictor dio\n```\n\nf0_predictor has the following options\n\n```\ncrepe\ndio\npm\nharvest\nrmvpe\nfcpe\n```\n\nIf the training set is too noisy,it is recommended to use `crepe` to handle f0\n\nIf the f0_predictor parameter is omitted, the default value is `rmvpe`\n\nIf you want shallow diffusion (optional), you need to add the `--use_diff` parameter, for example:\n\n```shell\npython preprocess_hubert_f0.py --f0_predictor dio --use_diff\n```\n\n**Speed Up preprocess**\n\nIf your dataset is pretty large,you can increase the param `--num_processes` like that:\n\n```shell\npython preprocess_hubert_f0.py --f0_predictor dio --num_processes 8\n```\nAll the worker will be assigned to different GPU if you have more than one GPUs.\n\nAfter completing the above steps, the dataset directory will contain the preprocessed data, and the dataset_raw folder can be deleted.\n\n## ğŸ‹ï¸â€ Training\n\n### Sovits Model\n\n```shell\npython train.py -c configs/config.json -m 44k\n```\n\n### Diffusion Model (optional)\n\nIf the shallow diffusion function is needed, the diffusion model needs to be trained. The diffusion model training method is as follows:\n\n```shell\npython train_diff.py -c configs/diffusion.yaml\n```\n\nDuring training, the model files will be saved to `logs/44k`, and the diffusion model will be saved to `logs/44k/diffusion`\n\n## ğŸ¤– Inference\n\nUse [inference_main.py](https://github.com/svc-develop-team/so-vits-svc/blob/4.0/inference_main.py)\n\n```shell\n# Example\npython inference_main.py -m "logs/44k/G_30400.pth" -c "configs/config.json" -n "å›ã®çŸ¥ã‚‰ãªã„ç‰©èª-src.wav" -t 0 -s "nen"\n```\n\nRequired parameters:\n- `-m` | `--model_path`: path to the model.\n- `-c` | `--config_path`: path to the configuration file.\n- `-n` | `--clean_names`: a list of wav file names located in the `raw` folder.\n- `-t` | `--trans`: pitch shift, supports positive and negative (semitone) values.\n- `-s` | `--spk_list`: Select the speaker ID to use for conversion.\n- `-cl` | `--clip`: Forced audio clipping, set to 0 to disable(default), setting it to a non-zero value (duration in seconds) to enable.\n\nOptional parameters: see the next section\n- `-lg` | `--linear_gradient`: The cross fade length of two audio slices in seconds. If there is a discontinuous voice after forced slicing, you can adjust this value. Otherwise, it is recommended to use the default value of 0.\n- `-f0p` | `--f0_predictor`: Select a F0 predictor, options are `crepe`, `pm`, `dio`, `harvest`, `rmvpe`,`fcpe`, default value is `pm`(note: f0 mean pooling will be enable when using `crepe`)\n- `-a` | `--auto_predict_f0`: automatic pitch prediction, do not enable this when converting singing voices as it can cause serious pitch issues.\n- `-cm` | `--cluster_model_path`: Cluster model or feature retrieval index path, if left blank, it will be automatically set as the default path of these models. If there is no training cluster or feature retrieval, fill in at will.\n- `-cr` | `--cluster_infer_ratio`: The proportion of clustering scheme or feature retrieval ranges from 0 to 1. If there is no training clustering model or feature retrieval, the default is 0.\n- `-eh` | `--enhance`: Whether to use NSF_HIFIGAN enhancer, this option has certain effect on sound quality enhancement for some models with few training sets, but has negative effect on well-trained models, so it is disabled by default.\n- `-shd` | `--shallow_diffusion`: Whether to use shallow diffusion, which can solve some electrical sound problems after use. This option is disabled by default. When this option is enabled, NSF_HIFIGAN enhancer will be disabled\n- `-usm` | `--use_spk_mix`: whether to use dynamic voice fusion\n- `-lea` | `--loudness_envelope_adjustment`ï¼šThe adjustment of the input source''s loudness envelope in relation to the fusion ratio of the output loudness envelope. The closer to 1, the more the output loudness envelope is used\n- `-fr` | `--feature_retrieval`ï¼šWhether to use feature retrieval If clustering model is used, it will be disabled, and `cm` and `cr` parameters will become the index path and mixing ratio of feature retrieval\n  \nShallow diffusion settings:\n- `-dm` | `--diffusion_model_path`: Diffusion model path\n- `-dc` | `--diffusion_config_path`: Diffusion config file path\n- `-ks` | `--k_step`: The larger the number of k_steps, the closer it is to the result of the diffusion model. The default is 100\n- `-od` | `--only_diffusion`: Whether to use Only diffusion mode, which does not load the sovits model to only use diffusion model inference\n- `-se` | `--second_encoding`ï¼šwhich involves applying an additional encoding to the original audio before shallow diffusion. This option can yield varying results - sometimes positive and sometimes negative.\n\n### Cautions\n\nIf inferencing using `whisper-ppg` speech encoder, you need to set `--clip` to 25 and `-lg` to 1. Otherwise it will fail to infer properly.\n\n## ğŸ¤” Optional Settings\n\nIf you are satisfied with the previous results, or if you do not feel you understand what follows, you can skip it and it will have no effect on the use of the model. The impact of these optional settings mentioned is relatively small, and while they may have some impact on specific datasets, in most cases the difference may not be significant.\n\n### Automatic f0 prediction\n\nDuring the training of the 4.0 model, an f0 predictor is also trained, which enables automatic pitch prediction during voice conversion. However, if the results are not satisfactory, manual pitch prediction can be used instead. Please note that when converting singing voices, it is advised not to enable this feature as it may cause significant pitch shifting.\n\n- Set `auto_predict_f0` to `true` in `inference_main.py`.\n\n### Cluster-based timbre leakage control\n\nIntroduction: The clustering scheme implemented in this model aims to reduce timbre leakage and enhance the similarity of the trained model to the target''s timbre, although the effect may not be very pronounced. However, relying solely on clustering can reduce the model''s clarity and make it sound less distinct. Therefore, a fusion method is adopted in this model to control the balance between the clustering and non-clustering approaches. This allows manual adjustment of the trade-off between "sounding like the target''s timbre" and "have clear enunciation" to find an optimal balance.\n\nNo changes are required in the existing steps. Simply train an additional clustering model, which incurs relatively low training costs.\n\n- Training process:\n  - Train on a machine with good CPU performance. According to extant experience, it takes about 4 minutes to train each speaker on a Tencent Cloud machine with 6-core CPU.\n  - Execute `python cluster/train_cluster.py`. The output model will be saved in `logs/44k/kmeans_10000.pt`.\n  - The clustering model can currently be trained using the gpu by executing `python cluster/train_cluster.py --gpu`\n- Inference process:\n  - Specify `cluster_model_path` in `inference_main.py`. If not specified, the default is `logs/44k/kmeans_10000.pt`.\n  - Specify `cluster_infer_ratio` in `inference_main.py`, where `0` means not using clustering at all, `1` means only using clustering, and usually `0.5` is sufficient.\n\n### Feature retrieval\n\nIntroduction: As with the clustering scheme, the timbre leakage can be reduced, the enunciation is slightly better than clustering, but it will reduce the inference speed. By employing the fusion method, it becomes possible to linearly control the balance between feature retrieval and non-feature retrieval, allowing for fine-tuning of the desired proportion.\n\n- Training process: \n  First, it needs to be executed after generating hubert and f0: \n\n```shell\npython train_index.py -c configs/config.json\n```\n\nThe output of the model will be in `logs/44k/feature_and_index.pkl`\n\n- Inference process: \n  - The `--feature_retrieval` needs to be formulated first, and the clustering mode automatically switches to the feature retrieval mode.\n  - Specify `cluster_model_path` in `inference_main.py`. If not specified, the default is `logs/44k/feature_and_index.pkl`.\n  - Specify `cluster_infer_ratio` in `inference_main.py`, where `0` means not using feature retrieval at all, `1` means only using feature retrieval, and usually `0.5` is sufficient.\n\n## ğŸ—œï¸ Model compression\n\nThe generated model contains data that is needed for further training. If you confirm that the model is final and not be used in further training, it is safe to remove these data to get smaller file size (about 1/3).\n\n```shell\n# Example\npython compress_model.py -c="configs/config.json" -i="logs/44k/G_30400.pth" -o="logs/44k/release.pth"\n```\n\n## ğŸ‘¨â€ğŸ”§ Timbre mixing\n\n### Static Tone Mixing\n\n**Refer to `webUI.py` file for stable Timbre mixing of the gadget/lab feature.**\n\nIntroduction: This function can combine multiple models into one model (convex combination or linear combination of multiple model parameters) to create mixed voice that do not exist in reality\n\n**Note:**\n1. This feature is only supported for single-speaker models\n2. If you force a multi-speaker model, it is critical to make sure there are the same number of speakers in each model. This will ensure that sounds with the same SpeakerID can be mixed correctly.\n3. Ensure that the `model` fields in config.json of all models to be mixed are the same\n4. The mixed model can use any config.json file from the models being synthesized. However, the clustering model will not be functional after mixed.\n5. When batch uploading models, it is best to put the models into a folder and upload them together after selecting them\n6. It is suggested to adjust the mixing ratio between 0 and 100, or to other numbers, but unknown effects will occur in the linear combination mode\n7. After mixing, the file named output.pth will be saved in the root directory of the project\n8. Convex combination mode will perform Softmax to add the mix ratio to 1, while linear combination mode will not\n\n### Dynamic timbre mixing\n\n**Refer to the `spkmix.py` file for an introduction to dynamic timbre mixing**\n\nCharacter mix track writing rules:\n\nRole ID: \[\[Start time 1, end time 1, start value 1, start value 1], [Start time 2, end time 2, start value 2]]\n\nThe start time must be the same as the end time of the previous one. The first start time must be 0, and the last end time must be 1 (time ranges from 0 to 1).\n\nAll roles must be filled in. For unused roles, fill \[\[0., 1., 0., 0.]]\n\nThe fusion value can be filled in arbitrarily, and the linear change from the start value to the end value within the specified period of time. The \n\ninternal linear combination will be automatically guaranteed to be 1 (convex combination condition), so it can be used safely\n\nUse the `--use_spk_mix` parameter when reasoning to enable dynamic timbre mixing\n\n## ğŸ“¤ Exporting to Onnx\n\nUse [onnx_export.py](https://github.com/svc-develop-team/so-vits-svc/blob/4.0/onnx_export.py)\n\n- Create a folder named `checkpoints` and open it\n- Create a folder in the `checkpoints` folder as your project folder, naming it after your project, for example `aziplayer`\n- Rename your model as `model.pth`, the configuration file as `config.json`, and place them in the `aziplayer` folder you just created\n- Modify `"NyaruTaffy"` in `path = "NyaruTaffy"` in [onnx_export.py](https://github.com/svc-develop-team/so-vits-svc/blob/4.0/onnx_export.py) to your project name, `path = "aziplayer"`ï¼ˆonnx_export_speaker_mix makes you can mix speaker''s voiceï¼‰\n- Run [onnx_export.py](https://github.com/svc-develop-team/so-vits-svc/blob/4.0/onnx_export.py)\n- Wait for it to finish running. A `model.onnx` will be generated in your project folder, which is the exported model.\n\nNote: For Hubert Onnx models, please use the models provided by MoeSS. Currently, they cannot be exported on their own (Hubert in fairseq has many unsupported operators and things involving constants that can cause errors or result in problems with the input/output shape and results when exported.)\n\n\n## ğŸ“ Reference\n\n| URL | Designation | Title | Implementation Source |\n| --- | ----------- | ----- | --------------------- |\n|[2106.06103](https://arxiv.org/abs/2106.06103) | VITS (Synthesizer)| Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech | [jaywalnut310/vits](https://github.com/jaywalnut310/vits) |\n|[2111.02392](https://arxiv.org/abs/2111.02392) | SoftVC (Speech Encoder)| A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion | [bshall/hubert](https://github.com/bshall/hubert) |\n|[2204.09224](https://arxiv.org/abs/2204.09224) | ContentVec (Speech Encoder)| ContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers | [auspicious3000/contentvec](https://github.com/auspicious3000/contentvec) |\n|[2212.04356](https://arxiv.org/abs/2212.04356) | Whisper (Speech Encoder) | Robust Speech Recognition via Large-Scale Weak Supervision | [openai/whisper](https://github.com/openai/whisper) |\n|[2110.13900](https://arxiv.org/abs/2110.13900) | WavLM (Speech Encoder) | WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing | [microsoft/unilm/wavlm](https://github.com/microsoft/unilm/tree/master/wavlm) |\n|[2305.17651](https://arxiv.org/abs/2305.17651) | DPHubert (Speech Encoder) | DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models | [pyf98/DPHuBERT](https://github.com/pyf98/DPHuBERT) |\n|[DOI:10.21437/Interspeech.2017-68](http://dx.doi.org/10.21437/Interspeech.2017-68) | Harvest (F0 Predictor) | Harvest: A high-performance fundamental frequency estimator from speech signals | [mmorise/World/harvest](https://github.com/mmorise/World/blob/master/src/harvest.cpp) |\n|[aes35-000039](https://www.aes.org/e-lib/online/browse.cfm?elib=15165) | Dio (F0 Predictor) | Fast and reliable F0 estimation method based on the period extraction of vocal fold vibration of singing voice and speech | [mmorise/World/dio](https://github.com/mmorise/World/blob/master/src/dio.cpp) |\n|[8461329](https://ieeexplore.ieee.org/document/8461329) | Crepe (F0 Predictor) | Crepe: A Convolutional Representation for Pitch Estimation | [maxrmorrison/torchcrepe](https://github.com/maxrmorrison/torchcrepe) |\n|[DOI:10.1016/j.wocn.2018.07.001](https://doi.org/10.1016/j.wocn.2018.07.001) | Parselmouth (F0 Predictor) | Introducing Parselmouth: A Python interface to Praat | [YannickJadoul/Parselmouth](https://github.com/YannickJadoul/Parselmouth) |\n|[2306.15412v2](https://arxiv.org/abs/2306.15412v2) | RMVPE (F0 Predictor) | RMVPE: A Robust Model for Vocal Pitch Estimation in Polyphonic Music | [Dream-High/RMVPE](https://github.com/Dream-High/RMVPE) |\n|[2010.05646](https://arxiv.org/abs/2010.05646) | HIFIGAN (Vocoder) | HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis | [jik876/hifi-gan](https://github.com/jik876/hifi-gan) |\n|[1810.11946](https://arxiv.org/abs/1810.11946.pdf) | NSF (Vocoder) | Neural source-filter-based waveform model for statistical parametric speech synthesis | [openvpi/DiffSinger/modules/nsf_hifigan](https://github.com/openvpi/DiffSinger/tree/refactor/modules/nsf_hifigan)\n|[2006.08195](https://arxiv.org/abs/2006.08195) | Snake (Vocoder) | Neural Networks Fail to Learn Periodic Functions and How to Fix It | [EdwardDixon/snake](https://github.com/EdwardDixon/snake)\n|[2105.02446v3](https://arxiv.org/abs/2105.02446v3) | Shallow Diffusion (PostProcessing)| DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism | [CNChTu/Diffusion-SVC](https://github.com/CNChTu/Diffusion-SVC) |\n|[K-means](https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=01D65490BADCC216F350D06F84D721AD?doi=10.1.1.308.8619&rep=rep1&type=pdf) | Feature K-means Clustering (PreProcessing)| Some methods for classification and analysis of multivariate observations | This repo |\n| | Feature TopK Retrieval (PreProcessing)| Retrieval based Voice Conversion | [RVC-Project/Retrieval-based-Voice-Conversion-WebUI](https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI) |\n| | whisper ppg| whisper ppg | [PlayVoice/whisper_ppg](https://github.com/PlayVoice/whisper_ppg) |\n| | bigvgan| bigvgan | [PlayVoice/so-vits-svc-5.0](https://github.com/PlayVoice/so-vits-svc-5.0/tree/bigvgan-mix-v2/vits_decoder/alias) |\n\n\n## â˜€ï¸ Previous contributors\n\nFor some reason the author deleted the original repository. Because of the negligence of the organization members, the contributor list was cleared because all files were directly reuploaded to this repository at the beginning of the reconstruction of this repository. Now add a previous contributor list to README.md.\n\n*Some members have not listed according to their personal wishes.*\n\n<table>\n  <tr>\n    <td align="center"><a href="https://github.com/MistEO"><img src="https://avatars.githubusercontent.com/u/18511905?v=4" width="100px;" alt=""/><br /><sub><b>MistEO</b></sub></a><br /></td>\n    <td align="center"><a href="https://github.com/XiaoMiku01"><img src="https://avatars.githubusercontent.com/u/54094119?v=4" width="100px;" alt=""/><br /><sub><b>XiaoMiku01</b></sub></a><br /></td>\n    <td align="center"><a href="https://github.com/ForsakenRei"><img src="https://avatars.githubusercontent.com/u/23041178?v=4" width="100px;" alt=""/><br /><sub><b>ã—ãã‚Œ</b></sub></a><br /></td>\n    <td align="center"><a href="https://github.com/TomoGaSukunai"><img src="https://avatars.githubusercontent.com/u/25863522?v=4" width="100px;" alt=""/><br /><sub><b>TomoGaSukunai</b></sub></a><br /></td>\n    <td align="center"><a href="https://github.com/Plachtaa"><img src="https://avatars.githubusercontent.com/u/112609742?v=4" width="100px;" alt=""/><br /><sub><b>Plachtaa</b></sub></a><br /></td>\n    <td align="center"><a href="https://github.com/zdxiaoda"><img src="https://avatars.githubusercontent.com/u/45501959?v=4" width="100px;" alt=""/><br /><sub><b>zdå°è¾¾</b></sub></a><br /></td>\n    <td align="center"><a href="https://github.com/Archivoice"><img src="https://avatars.githubusercontent.com/u/107520869?v=4" width="100px;" alt=""/><br /><sub><b>å‡è²éŸ¿ä¸–</b></sub></a><br /></td>\n  </tr>\n</table>\n\n## ğŸ“š Some legal provisions for reference\n\n#### Any country, region, organization, or individual using this project must comply with the following laws.\n\n#### ã€Šæ°‘æ³•å…¸ã€‹\n\n##### ç¬¬ä¸€åƒé›¶ä¸€åä¹æ¡ \n\nä»»ä½•ç»„ç»‡æˆ–è€…ä¸ªäººä¸å¾—ä»¥ä¸‘åŒ–ã€æ±¡æŸï¼Œæˆ–è€…åˆ©ç”¨ä¿¡æ¯æŠ€æœ¯æ‰‹æ®µä¼ªé€ ç­‰æ–¹å¼ä¾µå®³ä»–äººçš„è‚–åƒæƒã€‚æœªç»è‚–åƒæƒäººåŒæ„ï¼Œä¸å¾—åˆ¶ä½œã€ä½¿ç”¨ã€å…¬å¼€è‚–åƒæƒäººçš„è‚–åƒï¼Œä½†æ˜¯æ³•å¾‹å¦æœ‰è§„å®šçš„é™¤å¤–ã€‚æœªç»è‚–åƒæƒäººåŒæ„ï¼Œè‚–åƒä½œå“æƒåˆ©äººä¸å¾—ä»¥å‘è¡¨ã€å¤åˆ¶ã€å‘è¡Œã€å‡ºç§Ÿã€å±•è§ˆç­‰æ–¹å¼ä½¿ç”¨æˆ–è€…å…¬å¼€è‚–åƒæƒäººçš„è‚–åƒã€‚å¯¹è‡ªç„¶äººå£°éŸ³çš„ä¿æŠ¤ï¼Œå‚ç…§é€‚ç”¨è‚–åƒæƒä¿æŠ¤çš„æœ‰å…³è§„å®šã€‚\n\n#####  ç¬¬ä¸€åƒé›¶äºŒåå››æ¡ \n\nã€åèª‰æƒã€‘æ°‘äº‹ä¸»ä½“äº«æœ‰åèª‰æƒã€‚ä»»ä½•ç»„ç»‡æˆ–è€…ä¸ªäººä¸å¾—ä»¥ä¾®è¾±ã€è¯½è°¤ç­‰æ–¹å¼ä¾µå®³ä»–äººçš„åèª‰æƒã€‚  \n\n#####  ç¬¬ä¸€åƒé›¶äºŒåä¸ƒæ¡\n\nã€ä½œå“ä¾µå®³åèª‰æƒã€‘è¡Œä¸ºäººå‘è¡¨çš„æ–‡å­¦ã€è‰ºæœ¯ä½œå“ä»¥çœŸäººçœŸäº‹æˆ–è€…ç‰¹å®šäººä¸ºæè¿°å¯¹è±¡ï¼Œå«æœ‰ä¾®è¾±ã€è¯½è°¤å†…å®¹ï¼Œä¾µå®³ä»–äººåèª‰æƒçš„ï¼Œå—å®³äººæœ‰æƒä¾æ³•è¯·æ±‚è¯¥è¡Œä¸ºäººæ‰¿æ‹…æ°‘äº‹è´£ä»»ã€‚è¡Œä¸ºäººå‘è¡¨çš„æ–‡å­¦ã€è‰ºæœ¯ä½œå“ä¸ä»¥ç‰¹å®šäººä¸ºæè¿°å¯¹è±¡ï¼Œä»…å…¶ä¸­çš„æƒ…èŠ‚ä¸è¯¥ç‰¹å®šäººçš„æƒ…å†µç›¸ä¼¼çš„ï¼Œä¸æ‰¿æ‹…æ°‘äº‹è´£ä»»ã€‚  \n\n#### ã€Š[ä¸­åäººæ°‘å…±å’Œå›½å®ªæ³•](http://www.gov.cn/guoqing/2018-03/22/content_5276318.htm)ã€‹\n\n#### ã€Š[ä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•](http://gongbao.court.gov.cn/Details/f8e30d0689b23f57bfc782d21035c3.html?sw=%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD%E5%88%91%E6%B3%95)ã€‹\n\n#### ã€Š[ä¸­åäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸](http://gongbao.court.gov.cn/Details/51eb6750b8361f79be8f90d09bc202.html)ã€‹\n\n#### ã€Š[ä¸­åäººæ°‘å…±å’Œå›½åˆåŒæ³•](http://www.npc.gov.cn/zgrdw/npc/lfzt/rlyw/2016-07/01/content_1992739.htm)ã€‹\n\n## ğŸ’ª Thanks to all contributors for their efforts\n<a href="https://github.com/svc-develop-team/so-vits-svc/graphs/contributors" target="_blank">\n  <img src="https://contrib.rocks/image?repo=svc-develop-team/so-vits-svc" />\n</a>\n', '{"language":"Python","stars":27842,"forks":5070,"watchers":27842,"open_issues":28,"topics":["ai","audio-analysis","deep-learning","flow","generative-adversarial-network","pytorch","singing-voice-conversion","so-vits-svc","sovits","speech","variational-inference","vc","vits","voice","voice-changer","voice-conversion","voiceconversion"],"default_branch":"4.1-Stable","size_kb":11145,"archived":true,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:svc-develop-team:so-vits-svc","source_url":"https://github.com/svc-develop-team/so-vits-svc"},{"type":"has_code","target_id":"github:NaruseMioShirakana:MoeVoiceStudio","source_url":"https://github.com/NaruseMioShirakana/MoeVoiceStudio"},{"type":"has_code","target_id":"github:34j:so-vits-svc-fork","source_url":"https://github.com/34j/so-vits-svc-fork"},{"type":"has_code","target_id":"github:w-okada:voice-changer","source_url":"https://github.com/w-okada/voice-changer"},{"type":"has_code","target_id":"github:openvpi:DiffSinger","source_url":"https://github.com/openvpi/DiffSinger"},{"type":"has_code","target_id":"github:auspicious3000:contentvec","source_url":"https://github.com/auspicious3000/contentvec"},{"type":"has_code","target_id":"github:RVC-Project:Retrieval-based-Voice-Conversion-WebUI","source_url":"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI"},{"type":"has_code","target_id":"github:bshall:hubert","source_url":"https://github.com/bshall/hubert"},{"type":"has_code","target_id":"github:CNChTu:Diffusion-SVC","source_url":"https://github.com/CNChTu/Diffusion-SVC"},{"type":"has_code","target_id":"github:CNChTu:Diffusion-SVC","source_url":"https://github.com/CNChTu/Diffusion-SVC"},{"type":"has_code","target_id":"github:openvpi:vocoders","source_url":"https://github.com/openvpi/vocoders"},{"type":"has_code","target_id":"github:openvpi:vocoders","source_url":"https://github.com/openvpi/vocoders"},{"type":"has_code","target_id":"github:openvpi:vocoders","source_url":"https://github.com/openvpi/vocoders"},{"type":"has_code","target_id":"github:yxlllc:RMVPE","source_url":"https://github.com/yxlllc/RMVPE"},{"type":"has_code","target_id":"github:CNChTu:MelPE","source_url":"https://github.com/CNChTu/MelPE"},{"type":"has_code","target_id":"github:flutydeer:audio-slicer","source_url":"https://github.com/flutydeer/audio-slicer"},{"type":"has_code","target_id":"github:openvpi:audio-slicer","source_url":"https://github.com/openvpi/audio-slicer"},{"type":"has_code","target_id":"github:svc-develop-team:so-vits-svc","source_url":"https://github.com/svc-develop-team/so-vits-svc"},{"type":"has_code","target_id":"github:svc-develop-team:so-vits-svc","source_url":"https://github.com/svc-develop-team/so-vits-svc"},{"type":"has_code","target_id":"github:svc-develop-team:so-vits-svc","source_url":"https://github.com/svc-develop-team/so-vits-svc"},{"type":"has_code","target_id":"github:svc-develop-team:so-vits-svc","source_url":"https://github.com/svc-develop-team/so-vits-svc"},{"type":"has_code","target_id":"github:jaywalnut310:vits","source_url":"https://github.com/jaywalnut310/vits"},{"type":"has_code","target_id":"github:bshall:hubert","source_url":"https://github.com/bshall/hubert"},{"type":"has_code","target_id":"github:auspicious3000:contentvec","source_url":"https://github.com/auspicious3000/contentvec"},{"type":"has_code","target_id":"github:openai:whisper","source_url":"https://github.com/openai/whisper"},{"type":"has_code","target_id":"github:microsoft:unilm","source_url":"https://github.com/microsoft/unilm"},{"type":"has_code","target_id":"github:pyf98:DPHuBERT","source_url":"https://github.com/pyf98/DPHuBERT"},{"type":"has_code","target_id":"github:mmorise:World","source_url":"https://github.com/mmorise/World"},{"type":"has_code","target_id":"github:mmorise:World","source_url":"https://github.com/mmorise/World"},{"type":"has_code","target_id":"github:maxrmorrison:torchcrepe","source_url":"https://github.com/maxrmorrison/torchcrepe"},{"type":"has_code","target_id":"github:YannickJadoul:Parselmouth","source_url":"https://github.com/YannickJadoul/Parselmouth"},{"type":"has_code","target_id":"github:Dream-High:RMVPE","source_url":"https://github.com/Dream-High/RMVPE"},{"type":"has_code","target_id":"github:jik876:hifi-gan","source_url":"https://github.com/jik876/hifi-gan"},{"type":"has_code","target_id":"github:openvpi:DiffSinger","source_url":"https://github.com/openvpi/DiffSinger"},{"type":"has_code","target_id":"github:EdwardDixon:snake","source_url":"https://github.com/EdwardDixon/snake"},{"type":"has_code","target_id":"github:CNChTu:Diffusion-SVC","source_url":"https://github.com/CNChTu/Diffusion-SVC"},{"type":"has_code","target_id":"github:RVC-Project:Retrieval-based-Voice-Conversion-WebUI","source_url":"https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI"},{"type":"has_code","target_id":"github:PlayVoice:whisper_ppg","source_url":"https://github.com/PlayVoice/whisper_ppg"},{"type":"has_code","target_id":"github:PlayVoice:so-vits-svc-5.0","source_url":"https://github.com/PlayVoice/so-vits-svc-5.0"},{"type":"has_code","target_id":"github:svc-develop-team:so-vits-svc","source_url":"https://github.com/svc-develop-team/so-vits-svc"}]', NULL, 'AGPL-3.0', 'approved', 80, '0e5d18d10c1f3ec6cb1a1a377f0ca841', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-svc-develop-team-so-vits-svc from https://github.com/svc-develop-team.png
Image converted to WebP: data/images/github-svc-develop-team-so-vits-svc.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-WZMIAOMIAO-deep-learning-for-image-processing', 'github--wzmiaomiao--deep-learning-for-image-processing', 'deep-learning-for-image-processing', 'WZMIAOMIAO', '* æœ¬æ•™ç¨‹æ˜¯å¯¹æœ¬äººç ”ç©¶ç”ŸæœŸé—´çš„ç ”ç©¶å†…å®¹è¿›è¡Œæ•´ç†æ€»ç»“ï¼Œæ€»ç»“çš„åŒæ—¶ä¹Ÿå¸Œæœ›èƒ½å¤Ÿå¸®åŠ©æ›´å¤šçš„å°ä¼™ä¼´ã€‚åæœŸå¦‚æœæœ‰å­¦ä¹ åˆ°æ–°çš„çŸ¥è¯†ä¹Ÿä¼šä¸å¤§å®¶ä¸€èµ·åˆ†äº«ã€‚ * æœ¬æ•™ç¨‹ä¼šä»¥è§†é¢‘çš„æ–¹å¼è¿›è¡Œåˆ†äº«ï¼Œæ•™å­¦æµç¨‹å¦‚ä¸‹ï¼š 1ï¼‰ä»‹ç»ç½‘ç»œçš„ç»“æ„ä¸åˆ›æ–°ç‚¹ 2ï¼‰ä½¿ç”¨Pytorchè¿›è¡Œç½‘ç»œçš„æ­å»ºä¸è®­ç»ƒ 3ï¼‰ä½¿ç”¨Tensorflowï¼ˆå†…éƒ¨çš„kerasæ¨¡å—ï¼‰è¿›è¡Œç½‘ç»œçš„æ­å»ºä¸è®­ç»ƒ * è¯¾ç¨‹ä¸­æ‰€æœ‰PPTéƒ½æ”¾åœ¨æ–‡ä»¶å¤¹ä¸‹ï¼Œéœ€è¦çš„è‡ªè¡Œä¸‹è½½ã€‚ * å›¾åƒåˆ†ç±» * LeNetï¼ˆå·²å®Œæˆï¼‰ * Pytorchå®˜æ–¹demo(Lenet) * Tensorflow2å®˜æ–¹demo * AlexNetï¼ˆå·²å®Œæˆï¼‰ * AlexNetç½‘ç»œè®²è§£ * Pytorchæ­å»ºAlexNet * Tensorflow2æ­å»ºAlexnet * VggNetï¼ˆå·²å®Œæˆï¼‰ * VggNetç½‘ç»œè®²è§£ * Pytorchæ­å»ºVGGç½‘ç»œ * Tensorflow2æ­å»ºVGGç½‘ç»œ * GoogLeNetï¼ˆå·²å®Œæˆï¼‰ * GoogLeNetç½‘ç»œè®²è§£ * Pytorchæ­å»ºGoogLeNetç½‘ç»œ * Tensorflow2æ­å»ºGoogLeNetç½‘ç»œ * ResNetï¼ˆå·²å®Œæˆï¼‰ * ResNet...', '["bilibili","classification","deep-learning","object-detection","pytorch","segmentation","tensorflow2","python"]', 'other', 25862, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/WZMIAOMIAO/deep-learning-for-image-processing","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# æ·±åº¦å­¦ä¹ åœ¨å›¾åƒå¤„ç†ä¸­çš„åº”ç”¨æ•™ç¨‹\n\n## å‰è¨€\n* æœ¬æ•™ç¨‹æ˜¯å¯¹æœ¬äººç ”ç©¶ç”ŸæœŸé—´çš„ç ”ç©¶å†…å®¹è¿›è¡Œæ•´ç†æ€»ç»“ï¼Œæ€»ç»“çš„åŒæ—¶ä¹Ÿå¸Œæœ›èƒ½å¤Ÿå¸®åŠ©æ›´å¤šçš„å°ä¼™ä¼´ã€‚åæœŸå¦‚æœæœ‰å­¦ä¹ åˆ°æ–°çš„çŸ¥è¯†ä¹Ÿä¼šä¸å¤§å®¶ä¸€èµ·åˆ†äº«ã€‚\n* æœ¬æ•™ç¨‹ä¼šä»¥è§†é¢‘çš„æ–¹å¼è¿›è¡Œåˆ†äº«ï¼Œæ•™å­¦æµç¨‹å¦‚ä¸‹ï¼š  \n1ï¼‰ä»‹ç»ç½‘ç»œçš„ç»“æ„ä¸åˆ›æ–°ç‚¹  \n2ï¼‰ä½¿ç”¨Pytorchè¿›è¡Œç½‘ç»œçš„æ­å»ºä¸è®­ç»ƒ  \n3ï¼‰ä½¿ç”¨Tensorflowï¼ˆå†…éƒ¨çš„kerasæ¨¡å—ï¼‰è¿›è¡Œç½‘ç»œçš„æ­å»ºä¸è®­ç»ƒ \n* è¯¾ç¨‹ä¸­æ‰€æœ‰PPTéƒ½æ”¾åœ¨`course_ppt`æ–‡ä»¶å¤¹ä¸‹ï¼Œéœ€è¦çš„è‡ªè¡Œä¸‹è½½ã€‚\n\n\n## æ•™ç¨‹ç›®å½•ï¼Œç‚¹å‡»è·³è½¬ç›¸åº”è§†é¢‘ï¼ˆåæœŸä¼šæ ¹æ®å­¦ä¹ å†…å®¹å¢åŠ ï¼‰\n\n* å›¾åƒåˆ†ç±»\n  * LeNetï¼ˆå·²å®Œæˆï¼‰\n    * [Pytorchå®˜æ–¹demo(Lenet)](https://www.bilibili.com/video/BV187411T7Ye)\n    * [Tensorflow2å®˜æ–¹demo](https://www.bilibili.com/video/BV1n7411T7o6)\n\n  * AlexNetï¼ˆå·²å®Œæˆï¼‰\n    * [AlexNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1p7411T7Pc)\n    * [Pytorchæ­å»ºAlexNet](https://www.bilibili.com/video/BV1W7411T7qc)\n    * [Tensorflow2æ­å»ºAlexnet](https://www.bilibili.com/video/BV1s7411T7vs)\n\n  * VggNetï¼ˆå·²å®Œæˆï¼‰\n    * [VggNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1q7411T7Y6)\n    * [Pytorchæ­å»ºVGGç½‘ç»œ](https://www.bilibili.com/video/BV1i7411T7ZN)\n    * [Tensorflow2æ­å»ºVGGç½‘ç»œ](https://www.bilibili.com/video/BV1q7411T76b)\n\n  * GoogLeNetï¼ˆå·²å®Œæˆï¼‰\n    * [GoogLeNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1z7411T7ie)\n    * [Pytorchæ­å»ºGoogLeNetç½‘ç»œ](https://www.bilibili.com/video/BV1r7411T7M5)\n    * [Tensorflow2æ­å»ºGoogLeNetç½‘ç»œ](https://www.bilibili.com/video/BV1a7411T7Ht)\n\n  * ResNetï¼ˆå·²å®Œæˆï¼‰\n    * [ResNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1T7411T7wa)\n    * [Pytorchæ­å»ºResNetç½‘ç»œ](https://www.bilibili.com/video/BV14E411H7Uw)\n    * [Tensorflow2æ­å»ºResNetç½‘ç»œ](https://www.bilibili.com/video/BV1WE41177Ya)\n\n  * ResNeXt (å·²å®Œæˆ)\n    * [ResNeXtç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1Ap4y1p71v/)\n    * [Pytorchæ­å»ºResNeXtç½‘ç»œ](https://www.bilibili.com/video/BV1rX4y1N7tE)\n\n  * MobileNet_V1_V2ï¼ˆå·²å®Œæˆï¼‰\n    * [MobileNet_V1_V2ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1yE411p7L7)\n    * [Pytorchæ­å»ºMobileNetV2ç½‘ç»œ](https://www.bilibili.com/video/BV1qE411T7qZ)\n    * [Tensorflow2æ­å»ºMobileNetV2ç½‘ç»œ](https://www.bilibili.com/video/BV1NE411K7tX)\n\n  * MobileNet_V3ï¼ˆå·²å®Œæˆï¼‰\n    * [MobileNet_V3ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1GK4y1p7uE)\n    * [Pytorchæ­å»ºMobileNetV3ç½‘ç»œ](https://www.bilibili.com/video/BV1zT4y1P7pd)\n    * [Tensorflow2æ­å»ºMobileNetV3ç½‘ç»œ](https://www.bilibili.com/video/BV1KA411g7wX)\n\n  * ShuffleNet_V1_V2 (å·²å®Œæˆ)\n    * [ShuffleNet_V1_V2ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV15y4y1Y7SY)\n    * [ä½¿ç”¨Pytorchæ­å»ºShuffleNetV2](https://www.bilibili.com/video/BV1dh411r76X)\n    * [ä½¿ç”¨Tensorflow2æ­å»ºShuffleNetV2](https://www.bilibili.com/video/BV1kr4y1N7bh)\n\n  * EfficientNet_V1ï¼ˆå·²å®Œæˆï¼‰\n    * [EfficientNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1XK4y1U7PX)\n    * [ä½¿ç”¨Pytorchæ­å»ºEfficientNet](https://www.bilibili.com/video/BV19z4y1179h/)\n    * [ä½¿ç”¨Tensorflow2æ­å»ºEfficientNet](https://www.bilibili.com/video/BV1PK4y1S7Jf)\n\n  * EfficientNet_V2 (å·²å®Œæˆ)\n    * [EfficientNetV2ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV19v41157AU)\n    * [ä½¿ç”¨Pytorchæ­å»ºEfficientNetV2](https://www.bilibili.com/video/BV1Xy4y1g74u)\n    * [ä½¿ç”¨Tensorflowæ­å»ºEfficientNetV2](https://www.bilibili.com/video/BV19K4y1g7m4)\n  \n  * RepVGGï¼ˆå·²å®Œæˆï¼‰\n    * [RepVGGç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV15f4y1o7QR)\n\n  * Vision Transformer(å·²å®Œæˆ)\n    * [Multi-Head Attentionè®²è§£](https://www.bilibili.com/video/BV15v411W78M)\n    * [Vision Transformerç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1Jh411Y7WQ)\n    * [ä½¿ç”¨Pytorchæ­å»ºVision Transformer](https://www.bilibili.com/video/BV1AL411W7dT)\n    * [ä½¿ç”¨tensorflow2æ­å»ºVision Transformer](https://www.bilibili.com/video/BV1q64y1X7GY)\n\n  * Swin Transformer(å·²å®Œæˆ)\n    * [Swin Transformerç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1pL4y1v7jC)\n    * [ä½¿ç”¨Pytorchæ­å»ºSwin Transformer](https://www.bilibili.com/video/BV1yg411K7Yc)\n    * [ä½¿ç”¨Tensorflow2æ­å»ºSwin Transformer](https://www.bilibili.com/video/BV1bR4y1t7qT)\n\n  * ConvNeXt(å·²å®Œæˆ)\n    * [ConvNeXtç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1SS4y157fu)\n    * [ä½¿ç”¨Pytorchæ­å»ºConvNeXt](https://www.bilibili.com/video/BV14S4y1L791)\n    * [ä½¿ç”¨Tensorflow2æ­å»ºConvNeXt](https://www.bilibili.com/video/BV1TS4y1V7Gz)\n\n  * MobileViT(å·²å®Œæˆ)\n    * [MobileViTç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1TG41137sb)\n    * [ä½¿ç”¨Pytorchæ­å»ºMobileViT](https://www.bilibili.com/video/BV1ae411L7Ki)\n\n* ç›®æ ‡æ£€æµ‹\n  * Faster-RCNN/FPNï¼ˆå·²å®Œæˆï¼‰\n    * [Faster-RCNNç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1af4y1m7iL)\n    * [FPNç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1dh411U7D9)\n    * [Faster-RCNNæºç è§£æ(Pytorch)](https://www.bilibili.com/video/BV1of4y1m7nj)\n\n  * SSD/RetinaNet (å·²å®Œæˆ)\n    * [SSDç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1fT4y1L7Gi)\n    * [RetinaNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1Q54y1L7sM)\n    * [SSDæºç è§£æ(Pytorch)](https://www.bilibili.com/video/BV1vK411H771)\n\n  * YOLO Series (å·²å®Œæˆ)\n    * [YOLOç³»åˆ—ç½‘ç»œè®²è§£(V1~V3)](https://www.bilibili.com/video/BV1yi4y1g7ro)\n    * [YOLOv3 SPPæºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/BV1t54y1C7ra)\n    * [YOLOV4ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1NF41147So)\n    * [YOLOV5ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1T3411p7zR)\n    * [YOLOX ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1JW4y1k76c)\n  \n  * FCOSï¼ˆå·²å®Œæˆï¼‰\n    * [FCOSç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1G5411X7jw)\n\n* è¯­ä¹‰åˆ†å‰² \n  * FCN (å·²å®Œæˆ)\n    * [FCNç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1J3411C7zd)\n    * [FCNæºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/BV19q4y1971Q)\n\n  * DeepLabV3 (å·²å®Œæˆ)\n    * [DeepLabV1ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1SU4y1N7Ao)\n    * [DeepLabV2ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1gP4y1G7TC)\n    * [DeepLabV3ç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1Jb4y1q7j7)\n    * [DeepLabV3æºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/BV1TD4y1c7Wx)\n\n  * LR-ASPP (å·²å®Œæˆ)\n    * [LR-ASPPç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1LS4y1M76E)\n    * [LR-ASPPæºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/bv13D4y1F7ML)\n  \n  * U-Net (å·²å®Œæˆ)\n    * [U-Netç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1Vq4y127fB/)\n    * [U-Netæºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/BV1Vq4y127fB)\n  \n  * U2Net (å·²å®Œæˆ)\n    * [U2Netç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1yB4y1z7mj)\n    * [U2Netæºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/BV1Kt4y137iS)\n\n* å®ä¾‹åˆ†å‰²\n  * Mask R-CNNï¼ˆå·²å®Œæˆï¼‰\n    * [Mask R-CNNç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1ZY411774T)\n    * [Mask R-CNNæºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/BV1hY411E7wD)\n\n* å…³é”®ç‚¹æ£€æµ‹\n  * DeepPoseï¼ˆå·²å®Œæˆï¼‰\n    * [DeepPoseç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1bm421g7aJ)\n    * [DeepPoseæºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/BV1bm421g7aJ)\n\n  * HRNetï¼ˆå·²å®Œæˆï¼‰\n    * [HRNetç½‘ç»œè®²è§£](https://www.bilibili.com/video/BV1bB4y1y7qP)\n    * [HRNetæºç è§£æ(Pytorchç‰ˆ)](https://www.bilibili.com/video/BV1ar4y157JM)\n\n**[æ›´å¤šç›¸å…³è§†é¢‘è¯·è¿›å…¥æˆ‘çš„bilibilié¢‘é“æŸ¥çœ‹](https://space.bilibili.com/18161609/channel/index)**\n\n---\n\næ¬¢è¿å¤§å®¶å…³æ³¨ä¸‹æˆ‘çš„å¾®ä¿¡å…¬ä¼—å·ï¼ˆ**é˜¿å–†å­¦ä¹ å°è®°**ï¼‰ï¼Œå¹³æ—¶ä¼šæ€»ç»“äº›ç›¸å…³å­¦ä¹ åšæ–‡ã€‚    \n\nå¦‚æœæœ‰ä»€ä¹ˆé—®é¢˜ï¼Œä¹Ÿå¯ä»¥åˆ°æˆ‘çš„CSDNä¸­ä¸€èµ·è®¨è®ºã€‚\n[https://blog.csdn.net/qq_37541097/article/details/103482003](https://blog.csdn.net/qq_37541097/article/details/103482003)\n\næˆ‘çš„bilibilié¢‘é“ï¼š\n[https://space.bilibili.com/18161609/channel/index](https://space.bilibili.com/18161609/channel/index)\n', '{"language":"Python","stars":25862,"forks":8245,"watchers":25862,"open_issues":102,"topics":["bilibili","classification","deep-learning","object-detection","pytorch","segmentation","tensorflow2"],"default_branch":"master","size_kb":25636,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[]', NULL, 'GPL-3.0', 'approved', 65, '70af8da5616d56cd31e714af5f66f91c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-WZMIAOMIAO-deep-learning-for-image-processing from https://github.com/WZMIAOMIAO.png
Image converted to WebP: data/images/github-WZMIAOMIAO-deep-learning-for-image-processing.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-HumanSignal-label-studio', 'github--humansignal--label-studio', 'label-studio', 'HumanSignal', '<img src="https://user-images.githubusercontent.com/12534576/192582340-4c9e4401-1fe6-4dbb-95bb-fdbba5493f61.png"/> !GitHub !label-studio:build !GitHub release Website â€¢ Docs â€¢ Join Slack Community <img src="https://app.heartex.ai/docs/images/slack-mini.png" width="18px"/> <!-- <a href="https://labelstud.io/blog/release-130.html"><img src="https://github.com/HumanSignal/label-studio/raw/master/docs/themes/htx/source/images/release-130/LS-Hits-v1.3.png" align="right" /></a> --> Label Studio is ...', '["annotation","annotation-tool","annotations","boundingbox","computer-vision","data-labeling","dataset","datasets","deep-learning","image-annotation","image-classification","image-labeling","image-labelling-tool","label-studio","labeling","labeling-tool","mlops","semantic-segmentation","text-annotation","yolo","typescript"]', 'other', 25735, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/HumanSignal/label-studio","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<img src="https://user-images.githubusercontent.com/12534576/192582340-4c9e4401-1fe6-4dbb-95bb-fdbba5493f61.png"/>\n\n![GitHub](https://img.shields.io/github/license/heartexlabs/label-studio?logo=heartex) ![label-studio:build](https://github.com/HumanSignal/label-studio/workflows/label-studio:build/badge.svg) ![GitHub release](https://img.shields.io/github/v/release/heartexlabs/label-studio?include_prereleases)\n\n[Website](https://labelstud.io/) â€¢ [Docs](https://labelstud.io/guide/) â€¢ [Join Slack Community <img src="https://app.heartex.ai/docs/images/slack-mini.png" width="18px"/>](https://slack.labelstud.io/?source=github-1)\n\n\n## What is Label Studio?\n\n<!-- <a href="https://labelstud.io/blog/release-130.html"><img src="https://github.com/HumanSignal/label-studio/raw/master/docs/themes/htx/source/images/release-130/LS-Hits-v1.3.png" align="right" /></a> -->\n\nLabel Studio is an open source data labeling tool. It lets you label data types like audio, text, images, videos, and time series with a simple and straightforward UI and export to various model formats. It can be used to prepare raw data or improve existing training data to get more accurate ML models.\n\n- [Try out Label Studio](#try-out-label-studio)\n- [What you get from Label Studio](#what-you-get-from-label-studio)\n- [Included templates for labeling data in Label Studio](#included-templates-for-labeling-data-in-label-studio)\n- [Set up machine learning models with Label Studio](#set-up-machine-learning-models-with-Label-Studio)\n- [Integrate Label Studio with your existing tools](#integrate-label-studio-with-your-existing-tools)\n\n![Gif of Label Studio annotating different types of data](/images/annotation_examples.gif)\n\nHave a custom dataset? You can customize Label Studio to fit your needs. Read an [introductory blog post](https://towardsdatascience.com/introducing-label-studio-a-swiss-army-knife-of-data-labeling-140c1be92881) to learn more. \n\n## Try out Label Studio\n\nInstall Label Studio locally or deploy it in a cloud instance. [Or sign up for a free trial of our Starter Cloud edition!](https://humansignal.com/platform/starter-cloud/) You can learn more about what each edition offers [here](https://labelstud.io/guide/label_studio_compare). \n\n- [Install locally with Docker](#install-locally-with-docker)\n- [Run with Docker Compose (Label Studio + Nginx + PostgreSQL)](#run-with-docker-compose)\n- [Install locally with pip](#install-locally-with-pip)\n- [Install locally with poetry](#install-locally-with-poetry)\n- [Install locally with Anaconda](#install-locally-with-anaconda)\n- [Install for local development](#install-for-local-development)\n- [Deploy in a cloud instance](#deploy-in-a-cloud-instance)\n\n### Install locally with Docker\nOfficial Label Studio docker image is [here](https://hub.docker.com/r/heartexlabs/label-studio) and it can be downloaded with `docker pull`. \nRun Label Studio in a Docker container and access it at `http://localhost:8080`.\n\n\n```bash\ndocker pull heartexlabs/label-studio:latest\ndocker run -it -p 8080:8080 -v $(pwd)/mydata:/label-studio/data heartexlabs/label-studio:latest\n```\nYou can find all the generated assets, including SQLite3 database storage `label_studio.sqlite3` and uploaded files, in the `./mydata` directory.\n\n#### Override default Docker install\nYou can override the default launch command by appending the new arguments:\n```bash\ndocker run -it -p 8080:8080 -v $(pwd)/mydata:/label-studio/data heartexlabs/label-studio:latest label-studio --log-level DEBUG\n```\n\n#### Build a local image with Docker\nIf you want to build a local image, run:\n```bash\ndocker build -t heartexlabs/label-studio:latest .\n```\n\n### Run with Docker Compose\nDocker Compose script provides production-ready stack consisting of the following components:\n\n- Label Studio\n- [Nginx](https://www.nginx.com/) - proxy web server used to load various static data, including uploaded audio, images, etc.\n- [PostgreSQL](https://www.postgresql.org/) - production-ready database that replaces less performant SQLite3.\n\nTo start using the app from `http://localhost` run this command:\n```bash\ndocker-compose up\n```\n\n### Run with Docker Compose + MinIO\nYou can also run it with an additional MinIO server for local S3 storage. This is particularly useful when you want to \ntest the behavior with S3 storage on your local system. To start Label Studio in this way, you need to run the following command:\n````bash\n# Add sudo on Linux if you are not a member of the docker group\ndocker compose -f docker-compose.yml -f docker-compose.minio.yml up -d\n````\nIf you do not have a static IP address, you must create an entry in your hosts file so that both Label Studio and your \nbrowser can access the MinIO server. For more detailed instructions, please refer to [our guide on storing data](docs/source/guide/storedata.md).\n\n\n### Install locally with pip\n\n```bash\n# Requires Python >=3.8\npip install label-studio\n\n# Start the server at http://localhost:8080\nlabel-studio\n```\n\n### Install locally with poetry\n\n```bash\n### install poetry\npip install poetry\n\n### set poetry environment\npoetry new my-label-studio\ncd my-label-studio\npoetry add label-studio\n\n### activate poetry environment\npoetry shell\n\n### Start the server at http://localhost:8080\nlabel-studio\n```\n\n### Install locally with Anaconda\n\n```bash\nconda create --name label-studio\nconda activate label-studio\nconda install psycopg2\npip install label-studio\n```\n\n### Install for local development\n\nYou can run the latest Label Studio version locally without installing the package from pypi. \n\n```bash\n# Install all package dependencies\npip install poetry\npoetry install\n# Run database migrations\npython label_studio/manage.py migrate\npython label_studio/manage.py collectstatic\n# Start the server in development mode at http://localhost:8080\npython label_studio/manage.py runserver\n```\n\n### Deploy in a cloud instance\n\nYou can deploy Label Studio with one click in Heroku, Microsoft Azure, or Google Cloud Platform: \n\n<a href="https://www.heroku.com/deploy?template=https://github.com/HumanSignal/label-studio/tree/heroku-persistent-pg"><img src="https://www.herokucdn.com/deploy/button.svg" alt="Deploy" height="30px"></a>\n[<img src="https://aka.ms/deploytoazurebutton" height="30px">](https://portal.azure.com/#create/Microsoft.Template/uri/https%3A%2F%2Fraw.githubusercontent.com%2Fhumansignal%2Flabel-studio%2Fdevelop%2Fazuredeploy.json)\n[<img src="https://deploy.cloud.run/button.svg" height="30px">](https://deploy.cloud.run)\n\n\n#### Apply frontend changes\n\nFor information about updating the frontend, see [label-studio/web/README.md](https://github.com/HumanSignal/label-studio/blob/develop/web/README.md#installation-instructions).\n\n\n#### Install dependencies on Windows \nTo run Label Studio on Windows, download and install the following wheel packages from [Gohlke builds](https://www.lfd.uci.edu/~gohlke/pythonlibs) to ensure you''re using the correct version of Python:\n- [lxml](https://www.lfd.uci.edu/~gohlke/pythonlibs/#lxml)\n\n```bash\n# Upgrade pip \npip install -U pip\n\n# If you''re running Win64 with Python 3.8, install the packages downloaded from Gohlke:\npip install lxmlâ€‘4.5.0â€‘cp38â€‘cp38â€‘win_amd64.whl\n\n# Install label studio\npip install label-studio\n```\n\n### Run test suite\nTo add the tests'' dependencies to your local install:\n\n```bash\npoetry install --with test\n```\n\nAlternatively, it is possible to run the unit tests from a Docker container in which the test dependencies are installed:\n\n\n```bash\nmake build-testing-image\nmake docker-testing-shell\n```\n\nIn either case, to run the unit tests:\n\n```bash\ncd label_studio\n\n# sqlite3\nDJANGO_DB=sqlite DJANGO_SETTINGS_MODULE=core.settings.label_studio pytest -vv\n\n# postgres (assumes default postgres user,db,pass. Will not work in Docker\n# testing container without additional configuration)\nDJANGO_DB=default DJANGO_SETTINGS_MODULE=core.settings.label_studio pytest -vv\n```\n \n## What you get from Label Studio\n\nhttps://github.com/user-attachments/assets/525ad5ff-6904-4398-b507-7e8954268d69\n\n- **Multi-user labeling** sign up and login, when you create an annotation it''s tied to your account.\n- **Multiple projects** to work on all your datasets in one instance.\n- **Streamlined design** helps you focus on your task, not how to use the software.\n- **Configurable label formats** let you customize the visual interface to meet your specific labeling needs.\n- **Support for multiple data types** including images, audio, text, HTML, time-series, and video. \n- **Import from files or from cloud storage** in Amazon AWS S3, Google Cloud Storage, or JSON, CSV, TSV, RAR, and ZIP archives. \n- **Integration with machine learning models** so that you can visualize and compare predictions from different models and perform pre-labeling.\n- **Embed it in your data pipeline** REST API makes it easy to make it a part of your pipeline\n\n## Included templates for labeling data in Label Studio \n\nLabel Studio includes a variety of templates to help you label your data, or you can create your own using specifically designed configuration language. The most common templates and use cases for labeling include the following cases:\n\n<img src="/images/template-types.png" />\n\n## Set up machine learning models with Label Studio\n\nConnect your favorite machine learning model using the Label Studio Machine Learning SDK. Follow these steps:\n\n1. Start your own machine learning backend server. See [more detailed instructions](https://github.com/HumanSignal/label-studio-ml-backend).\n2. Connect Label Studio to the server on the model page found in project settings.\n\nThis lets you:\n\n- **Pre-label** your data using model predictions. \n- Do **online learning** and retrain your model while new annotations are being created. \n- Do **active learning** by labeling only the most complex examples in your data.\n\n## Integrate Label Studio with your existing tools\n\nYou can use Label Studio as an independent part of your machine learning workflow or integrate the frontend or backend into your existing tools.  \n\n## Ecosystem\n\n| Project | Description |\n|-|-|\n| label-studio | Server, distributed as a pip package |\n| [Frontend library](web/libs/editor/) | The Label Studio frontend library. This uses React to build the UI and mobx-state-tree for state management. |  \n| [Data Manager library](web/libs/datamanager/) | A library for the Data Manager, our data exploration tool. | \n| [label-studio-converter](https://github.com/HumanSignal/label-studio-sdk/tree/master/src/label_studio_sdk/converter) | Encode labels in the format of your favorite machine learning library |\n| [label-studio-transformers](https://github.com/HumanSignal/label-studio-transformers) | Transformers library connected and configured for use with Label Studio |\n\n## Citation\n\nInclude a citation for Label Studio in the **References** section of your articles:\n\n```tex\n@misc{Label Studio,\n  title={{Label Studio}: Data labeling software},\n  url={https://github.com/HumanSignal/label-studio},\n  note={Open source software available from https://github.com/HumanSignal/label-studio},\n  author={\n    Maxim Tkachenko and\n    Mikhail Malyuk and\n    Andrey Holmanyuk and\n    Nikolai Liubimov},\n  year={2020-2025},\n}\n```\n\n## License\n\nThis software is licensed under the [Apache 2.0 LICENSE](/LICENSE) Â© [Heartex](https://www.heartex.com/). 2020-2025\n\n<img src="https://user-images.githubusercontent.com/12534576/192582529-cf628f58-abc5-479b-a0d4-8a3542a4b35e.png" title="Hey everyone!" width="180" />\n', '{"language":"TypeScript","stars":25735,"forks":3240,"watchers":25735,"open_issues":948,"topics":["annotation","annotation-tool","annotations","boundingbox","computer-vision","data-labeling","dataset","datasets","deep-learning","image-annotation","image-classification","image-labeling","image-labelling-tool","label-studio","labeling","labeling-tool","mlops","semantic-segmentation","text-annotation","yolo"],"default_branch":"develop","size_kb":2950135,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:HumanSignal:label-studio","source_url":"https://github.com/HumanSignal/label-studio"},{"type":"has_code","target_id":"github:HumanSignal:label-studio","source_url":"https://github.com/HumanSignal/label-studio"},{"type":"has_code","target_id":"github:HumanSignal:label-studio","source_url":"https://github.com/HumanSignal/label-studio"},{"type":"has_code","target_id":"github:HumanSignal:label-studio","source_url":"https://github.com/HumanSignal/label-studio"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:HumanSignal:label-studio-ml-backend","source_url":"https://github.com/HumanSignal/label-studio-ml-backend"},{"type":"has_code","target_id":"github:HumanSignal:label-studio-sdk","source_url":"https://github.com/HumanSignal/label-studio-sdk"},{"type":"has_code","target_id":"github:HumanSignal:label-studio-transformers","source_url":"https://github.com/HumanSignal/label-studio-transformers"},{"type":"has_code","target_id":"github:HumanSignal:label-studio},","source_url":"https://github.com/HumanSignal/label-studio},"},{"type":"has_code","target_id":"github:HumanSignal:label-studio},","source_url":"https://github.com/HumanSignal/label-studio},"}]', NULL, 'Apache-2.0', 'approved', 80, 'dd1cdcc1d05b052b7a5d7d295c400ad5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-HumanSignal-label-studio from https://github.com/HumanSignal.png
Image converted to WebP: data/images/github-HumanSignal-label-studio.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-junyanz-pytorch-CycleGAN-and-pix2pix', 'github--junyanz--pytorch-cyclegan-and-pix2pix', 'pytorch-CycleGAN-and-pix2pix', 'junyanz', '<img src=''imgs/horse2zebra.gif'' align="right" width=384> <br><br><br> **Udpate in 2025**: we recently updated the code to support Python 3.11 and PyTorch 2.4. It also supports DDP for single-machine multiple-GPU training. (Please use ) **New**: Please check out img2img-turbo repo that includes both pix2pix-turbo and CycleGAN-Turbo. Our new one-step image-to-image translation methods can support both paired and unpaired training and produce better results by leveraging the pre-trained StableDi...', '["computer-graphics","computer-vision","cyclegan","deep-learning","gan","gans","generative-adversarial-network","image-generation","image-manipulation","pix2pix","pytorch","python"]', 'other', 24769, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<img src=''imgs/horse2zebra.gif'' align="right" width=384>\n\n<br><br><br>\n\n# CycleGAN and pix2pix in PyTorch\n\n**Udpate in 2025**: we recently updated the code to support Python 3.11 and PyTorch 2.4. It also supports DDP for single-machine multiple-GPU training. (Please use `torchrun --nproc_per_node=4 train.py ...`)\n\n**New**: Please check out [img2img-turbo](https://github.com/GaParmar/img2img-turbo) repo that includes both pix2pix-turbo and CycleGAN-Turbo. Our new one-step image-to-image translation methods can support both paired and unpaired training and produce better results by leveraging the pre-trained StableDiffusion-Turbo model. The inference time for 512x512 image is 0.29 sec on A6000 and 0.11 sec on A100.\n\nPlease check out [contrastive-unpaired-translation](https://github.com/taesungp/contrastive-unpaired-translation) (CUT), our new unpaired image-to-image translation model that enables fast and memory-efficient training.\n\nWe provide PyTorch implementations for both unpaired and paired image-to-image translation.\n\nThe code was written by [Jun-Yan Zhu](https://github.com/junyanz) and [Taesung Park](https://github.com/taesungp), and supported by [Tongzhou Wang](https://github.com/SsnL).\n\nThis PyTorch implementation produces results comparable to or better than our original Torch software. If you would like to reproduce the same results as in the papers, check out the original [CycleGAN Torch](https://github.com/junyanz/CycleGAN) and [pix2pix Torch](https://github.com/phillipi/pix2pix) code in Lua/Torch.\n\n**Note**: The current software works well with PyTorch 2.4+. Check out the older [branch](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/tree/pytorch0.3.1) that supports PyTorch 0.1-0.3.\n\nYou may find useful information in [training/test tips](docs/tips.md) and [frequently asked questions](docs/qa.md). To implement custom models and datasets, check out our [templates](#custom-model-and-dataset). To help users better understand and adapt our codebase, we provide an [overview](docs/overview.md) of the code structure of this repository.\n\n**CycleGAN: [Project](https://junyanz.github.io/CycleGAN/) | [Paper](https://arxiv.org/pdf/1703.10593.pdf) | [Torch](https://github.com/junyanz/CycleGAN) |\n[Tensorflow Core Tutorial](https://www.tensorflow.org/tutorials/generative/cyclegan) | [PyTorch Colab](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb)**\n\n<img src="https://junyanz.github.io/CycleGAN/images/teaser_high_res.jpg" width="800"/>\n\n**Pix2pix: [Project](https://phillipi.github.io/pix2pix/) | [Paper](https://arxiv.org/pdf/1611.07004.pdf) | [Torch](https://github.com/phillipi/pix2pix) |\n[Tensorflow Core Tutorial](https://www.tensorflow.org/tutorials/generative/pix2pix) | [PyTorch Colab](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb)**\n\n<img src="https://phillipi.github.io/pix2pix/images/teaser_v3.png" width="800px"/>\n\n**[EdgesCats Demo](https://affinelayer.com/pixsrv/) | [pix2pix-tensorflow](https://github.com/affinelayer/pix2pix-tensorflow) | by [Christopher Hesse](https://twitter.com/christophrhesse)**\n\n<img src=''imgs/edges2cats.jpg'' width="400px"/>\n\nIf you use this code for your research, please cite:\n\nUnpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.<br>\n[Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/)\*, [Taesung Park](https://taesung.me/)\*, [Phillip Isola](https://people.eecs.berkeley.edu/~isola/), [Alexei A. Efros](https://people.eecs.berkeley.edu/~efros). In ICCV 2017. (\* equal contributions) [[Bibtex]](https://junyanz.github.io/CycleGAN/CycleGAN.txt)\n\nImage-to-Image Translation with Conditional Adversarial Networks.<br>\n[Phillip Isola](https://people.eecs.berkeley.edu/~isola), [Jun-Yan Zhu](https://www.cs.cmu.edu/~junyanz/), [Tinghui Zhou](https://people.eecs.berkeley.edu/~tinghuiz), [Alexei A. Efros](https://people.eecs.berkeley.edu/~efros). In CVPR 2017. [[Bibtex]](https://www.cs.cmu.edu/~junyanz/projects/pix2pix/pix2pix.bib)\n\n## Talks and Course\n\npix2pix slides: [keynote](http://efrosgans.eecs.berkeley.edu/CVPR18_slides/pix2pix.key) | [pdf](http://efrosgans.eecs.berkeley.edu/CVPR18_slides/pix2pix.pdf),\nCycleGAN slides: [pptx](http://efrosgans.eecs.berkeley.edu/CVPR18_slides/CycleGAN.pptx) | [pdf](http://efrosgans.eecs.berkeley.edu/CVPR18_slides/CycleGAN.pdf)\n\nCycleGAN course assignment [code](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-code.zip) and [handout](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-handout.pdf) designed by Prof. [Roger Grosse](http://www.cs.toronto.edu/~rgrosse/) for [CSC321](http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/) "Intro to Neural Networks and Machine Learning" at University of Toronto. Please contact the instructor if you would like to adopt it in your course.\n\n## Colab Notebook\n\nTensorFlow Core CycleGAN Tutorial: [Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb) | [Code](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb)\n\nTensorFlow Core pix2pix Tutorial: [Google Colab](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb) | [Code](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb)\n\nPyTorch Colab notebook: [CycleGAN](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb) and [pix2pix](https://colab.research.google.com/github/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/pix2pix.ipynb)\n\nZeroCostDL4Mic Colab notebook: [CycleGAN](https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks_Beta/CycleGAN_ZeroCostDL4Mic.ipynb) and [pix2pix](https://colab.research.google.com/github/HenriquesLab/ZeroCostDL4Mic/blob/master/Colab_notebooks_Beta/pix2pix_ZeroCostDL4Mic.ipynb)\n\n## Other implementations\n\n### CycleGAN\n\n<p><a href="https://github.com/leehomyc/cyclegan-1"> [Tensorflow]</a> (by Harry Yang),\n<a href="https://github.com/architrathore/CycleGAN/">[Tensorflow]</a> (by Archit Rathore),\n<a href="https://github.com/vanhuyz/CycleGAN-TensorFlow">[Tensorflow]</a> (by Van Huy),\n<a href="https://github.com/XHUJOY/CycleGAN-tensorflow">[Tensorflow]</a> (by Xiaowei Hu),\n<a href="https://github.com/LynnHo/CycleGAN-Tensorflow-2"> [Tensorflow2]</a> (by Zhenliang He),\n<a href="https://github.com/luoxier/CycleGAN_Tensorlayer"> [TensorLayer1.0]</a> (by luoxier),\n<a href="https://github.com/tensorlayer/cyclegan"> [TensorLayer2.0]</a> (by zsdonghao),\n<a href="https://github.com/Aixile/chainer-cyclegan">[Chainer]</a> (by Yanghua Jin),\n<a href="https://github.com/yunjey/mnist-svhn-transfer">[Minimal PyTorch]</a> (by yunjey),\n<a href="https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Mxnet-Scala/CycleGAN">[Mxnet]</a> (by Ldpe2G),\n<a href="https://github.com/tjwei/GANotebooks">[lasagne/Keras]</a> (by tjwei),\n<a href="https://github.com/simontomaskarlsson/CycleGAN-Keras">[Keras]</a> (by Simon Karlsson),\n<a href="https://github.com/Ldpe2G/DeepLearningForFun/tree/master/Oneflow-Python/CycleGAN">[OneFlow]</a> (by Ldpe2G)\n</p>\n</ul>\n\n### pix2pix\n\n<p><a href="https://github.com/affinelayer/pix2pix-tensorflow"> [Tensorflow]</a> (by Christopher Hesse),\n<a href="https://github.com/Eyyub/tensorflow-pix2pix">[Tensorflow]</a> (by EyyÃ¼b Sariu),\n<a href="https://github.com/datitran/face2face-demo"> [Tensorflow (face2face)]</a> (by Dat Tran),\n<a href="https://github.com/awjuliani/Pix2Pix-Film"> [Tensorflow (film)]</a> (by Arthur Juliani),\n<a href="https://github.com/kaonashi-tyc/zi2zi">[Tensorflow (zi2zi)]</a> (by Yuchen Tian),\n<a href="https://github.com/pfnet-research/chainer-pix2pix">[Chainer]</a> (by mattya),\n<a href="https://github.com/tjwei/GANotebooks">[tf/torch/keras/lasagne]</a> (by tjwei),\n<a href="https://github.com/taey16/pix2pixBEGAN.pytorch">[Pytorch]</a> (by taey16)\n</p>\n</ul>\n\n## Prerequisites\n\n- Linux or macOS\n- Python 3\n- CPU or NVIDIA GPU + CUDA CuDNN\n\n## Getting Started\n\n### Installation\n\n- Clone this repo:\n\n```bash\ngit clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\ncd pytorch-CycleGAN-and-pix2pix\n```\n\n- Install [PyTorch](http://pytorch.org) and other dependencies. For Conda users, you can create a new Conda environment by\n\n```bash\nconda env create -f environment.yml\n```\n\nand then activate the environment by\n\n```bash\nconda activate pytorch-img2img\n```\n\n- For Docker users, we provide the pre-built Docker image and Dockerfile. Please refer to our [Docker](docs/docker.md) page.\n- For Repl users, please click [![Run on Repl.it](https://repl.it/badge/github/junyanz/pytorch-CycleGAN-and-pix2pix)](https://repl.it/github/junyanz/pytorch-CycleGAN-and-pix2pix).\n\n### CycleGAN train/test\n\n- Download a CycleGAN dataset (e.g. maps):\n\n```bash\nbash ./datasets/download_cyclegan_dataset.sh maps\n```\n\n- To log training progress and test images to W&B dashboard, set the `--use_wandb` flag with training script\n- Train a model:\n\n```bash\n#!./scripts/train_cyclegan.sh\npython train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan --use_wandb\n```\n\nTo see more intermediate results, check out `./checkpoints/maps_cyclegan/web/index.html`.\n\n- Test the model:\n\n```bash\n#!./scripts/test_cyclegan.sh\npython test.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan\n```\n\n- The test results will be saved to a html file here: `./results/maps_cyclegan/latest_test/index.html`.\n\n### pix2pix train/test\n\n- Download a pix2pix dataset (e.g.[facades](http://cmp.felk.cvut.cz/~tylecr1/facade/)):\n\n```bash\nbash ./datasets/download_pix2pix_dataset.sh facades\n```\n\n- To log training progress and test images to W&B dashboard, set the `--use_wandb` flag with training script\n- Train a model:\n\n```bash\n#!./scripts/train_pix2pix.sh\npython train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA  --use_wandb\n```\n\nTo see more intermediate results, check out `./checkpoints/facades_pix2pix/web/index.html`.\n\n- Test the model (`bash ./scripts/test_pix2pix.sh`):\n\n```bash\n#!./scripts/test_pix2pix.sh\npython test.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA\n```\n\n- The test results will be saved to a html file here: `./results/facades_pix2pix/test_latest/index.html`. You can find more scripts at `scripts` directory.\n- To train and test pix2pix-based colorization models, please add `--model colorization` and `--dataset_mode colorization`. See our training [tips](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#notes-on-colorization) for more details.\n\n### Apply a pre-trained model (CycleGAN)\n\n- You can download a pretrained model (e.g. horse2zebra) with the following script:\n\n```bash\nbash ./scripts/download_cyclegan_model.sh horse2zebra\n```\n\n- The pretrained model is saved at `./checkpoints/{name}_pretrained/latest_net_G.pth`. Check [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/download_cyclegan_model.sh#L3) for all the available CycleGAN models.\n- To test the model, you also need to download the horse2zebra dataset:\n\n```bash\nbash ./datasets/download_cyclegan_dataset.sh horse2zebra\n```\n\n- Then generate the results using\n\n```bash\npython test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout\n```\n\n- The option `--model test` is used for generating results of CycleGAN only for one side. This option will automatically set `--dataset_mode single`, which only loads the images from one set. On the contrary, using `--model cycle_gan` requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at `./results/`. Use `--results_dir {directory_path_to_save_result}` to specify the results directory.\n\n- For pix2pix and your own models, you need to explicitly specify `--netG`, `--norm`, `--no_dropout` to match the generator architecture of the trained model. See this [FAQ](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md#runtimeerror-errors-in-loading-state_dict-812-671461-296) for more details.\n\n### Apply a pre-trained model (pix2pix)\n\nDownload a pre-trained model with `./scripts/download_pix2pix_model.sh`.\n\n- Check [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/scripts/download_pix2pix_model.sh#L3) for all the available pix2pix models. For example, if you would like to download label2photo model on the Facades dataset,\n\n```bash\nbash ./scripts/download_pix2pix_model.sh facades_label2photo\n```\n\n- Download the pix2pix facades datasets:\n\n```bash\nbash ./datasets/download_pix2pix_dataset.sh facades\n```\n\n- Then generate the results using\n\n```bash\npython test.py --dataroot ./datasets/facades/ --direction BtoA --model pix2pix --name facades_label2photo_pretrained\n```\n\n- Note that we specified `--direction BtoA` as Facades dataset''s A to B direction is photos to labels.\n\n- If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use `--model test` option. See `./scripts/test_single.sh` for how to apply a model to Facade label maps (stored in the directory `facades/testB`).\n\n- See a list of currently available models at `./scripts/download_pix2pix_model.sh`\n\n### Multi-GPU training\n\nTo train a model on multiple GPUs, please use `torchrun --nproc_per_node=4 train.py ...` instead of `python train.py ...`. We also need to use synchronized batchnorm by setting `--norm sync_batch` (or `--norm sync_instance` for instance normgalization). The `--norm batch` is not compatible with DDP.\n\n## [Docker](docs/docker.md)\n\nWe provide the pre-built Docker image and Dockerfile that can run this code repo. See [docker](docs/docker.md).\n\n## [Datasets](docs/datasets.md)\n\nDownload pix2pix/CycleGAN datasets and create your own datasets.\n\n## [Training/Test Tips](docs/tips.md)\n\nBest practice for training and testing your models.\n\n## [Frequently Asked Questions](docs/qa.md)\n\nBefore you post a new question, please first look at the above Q & A and existing GitHub issues.\n\n## Custom Model and Dataset\n\nIf you plan to implement custom models and dataset for your new applications, we provide a dataset [template](data/template_dataset.py) and a model [template](models/template_model.py) as a starting point.\n\n## [Code structure](docs/overview.md)\n\nTo help users better understand and use our code, we briefly overview the functionality and implementation of each package and each module.\n\n## Pull Request\n\nYou are always welcome to contribute to this repository by sending a [pull request](https://help.github.com/articles/about-pull-requests/).\nPlease run `flake8 --ignore E501 .` and `pytest scripts/test_before_push.py -v` before you commit the code. Please also update the code structure [overview](docs/overview.md) accordingly if you add or remove files.\n\n## Citation\n\nIf you use this code for your research, please cite our papers.\n\n```\n@inproceedings{CycleGAN2017,\n  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},\n  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},\n  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},\n  year={2017}\n}\n\n\n@inproceedings{isola2017image,\n  title={Image-to-Image Translation with Conditional Adversarial Networks},\n  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},\n  booktitle={Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},\n  year={2017}\n}\n```\n\n## Other Languages\n\n[Spanish](docs/README_es.md)\n\n## Related Projects\n\n[img2img-turbo](https://github.com/GaParmar/img2img-turbo)<br>\n[contrastive-unpaired-translation](https://github.com/taesungp/contrastive-unpaired-translation) (CUT)<br>\n[CycleGAN-Torch](https://github.com/junyanz/CycleGAN) |\n[pix2pix-Torch](https://github.com/phillipi/pix2pix) | [pix2pixHD](https://github.com/NVIDIA/pix2pixHD)|\n[BicycleGAN](https://github.com/junyanz/BicycleGAN) | [vid2vid](https://tcwang0509.github.io/vid2vid/) | [SPADE/GauGAN](https://github.com/NVlabs/SPADE)<br>\n[iGAN](https://github.com/junyanz/iGAN) | [GAN Dissection](https://github.com/CSAILVision/GANDissect) | [GAN Paint](http://ganpaint.io/)\n\n## Cat Paper Collection\n\nIf you love cats, and love reading cool graphics, vision, and learning papers, please check out the Cat Paper [Collection](https://github.com/junyanz/CatPapers).\n\n## Acknowledgments\n\nOur code is inspired by [pytorch-DCGAN](https://github.com/pytorch/examples/tree/master/dcgan).\n', '{"language":"Python","stars":24769,"forks":6546,"watchers":24769,"open_issues":586,"topics":["computer-graphics","computer-vision","cyclegan","deep-learning","gan","gans","generative-adversarial-network","image-generation","image-manipulation","pix2pix","pytorch"],"default_branch":"master","size_kb":8464,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:GaParmar:img2img-turbo","source_url":"https://github.com/GaParmar/img2img-turbo"},{"type":"has_code","target_id":"github:taesungp:contrastive-unpaired-translation","source_url":"https://github.com/taesungp/contrastive-unpaired-translation"},{"type":"has_code","target_id":"github:junyanz:CycleGAN","source_url":"https://github.com/junyanz/CycleGAN"},{"type":"has_code","target_id":"github:phillipi:pix2pix","source_url":"https://github.com/phillipi/pix2pix"},{"type":"has_code","target_id":"github:junyanz:pytorch-CycleGAN-and-pix2pix","source_url":"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"},{"type":"has_code","target_id":"github:junyanz:CycleGAN","source_url":"https://github.com/junyanz/CycleGAN"},{"type":"has_code","target_id":"github:phillipi:pix2pix","source_url":"https://github.com/phillipi/pix2pix"},{"type":"has_code","target_id":"github:affinelayer:pix2pix-tensorflow","source_url":"https://github.com/affinelayer/pix2pix-tensorflow"},{"type":"has_code","target_id":"github:tensorflow:docs","source_url":"https://github.com/tensorflow/docs"},{"type":"has_code","target_id":"github:tensorflow:docs","source_url":"https://github.com/tensorflow/docs"},{"type":"has_code","target_id":"github:leehomyc:cyclegan-1\">","source_url":"https://github.com/leehomyc/cyclegan-1\">"},{"type":"has_code","target_id":"github:architrathore:CycleGAN","source_url":"https://github.com/architrathore/CycleGAN"},{"type":"has_code","target_id":"github:vanhuyz:CycleGAN-TensorFlow\">[Tensorflow]<","source_url":"https://github.com/vanhuyz/CycleGAN-TensorFlow\">[Tensorflow]<"},{"type":"has_code","target_id":"github:XHUJOY:CycleGAN-tensorflow\">[Tensorflow]<","source_url":"https://github.com/XHUJOY/CycleGAN-tensorflow\">[Tensorflow]<"},{"type":"has_code","target_id":"github:LynnHo:CycleGAN-Tensorflow-2\">","source_url":"https://github.com/LynnHo/CycleGAN-Tensorflow-2\">"},{"type":"has_code","target_id":"github:luoxier:CycleGAN_Tensorlayer\">","source_url":"https://github.com/luoxier/CycleGAN_Tensorlayer\">"},{"type":"has_code","target_id":"github:tensorlayer:cyclegan\">","source_url":"https://github.com/tensorlayer/cyclegan\">"},{"type":"has_code","target_id":"github:Aixile:chainer-cyclegan\">[Chainer]<","source_url":"https://github.com/Aixile/chainer-cyclegan\">[Chainer]<"},{"type":"has_code","target_id":"github:yunjey:mnist-svhn-transfer\">[Minimal","source_url":"https://github.com/yunjey/mnist-svhn-transfer\">[Minimal"},{"type":"has_code","target_id":"github:Ldpe2G:DeepLearningForFun","source_url":"https://github.com/Ldpe2G/DeepLearningForFun"},{"type":"has_code","target_id":"github:tjwei:GANotebooks\">[lasagne","source_url":"https://github.com/tjwei/GANotebooks\">[lasagne"},{"type":"has_code","target_id":"github:simontomaskarlsson:CycleGAN-Keras\">[Keras]<","source_url":"https://github.com/simontomaskarlsson/CycleGAN-Keras\">[Keras]<"},{"type":"has_code","target_id":"github:Ldpe2G:DeepLearningForFun","source_url":"https://github.com/Ldpe2G/DeepLearningForFun"},{"type":"has_code","target_id":"github:affinelayer:pix2pix-tensorflow\">","source_url":"https://github.com/affinelayer/pix2pix-tensorflow\">"},{"type":"has_code","target_id":"github:Eyyub:tensorflow-pix2pix\">[Tensorflow]<","source_url":"https://github.com/Eyyub/tensorflow-pix2pix\">[Tensorflow]<"},{"type":"has_code","target_id":"github:datitran:face2face-demo\">","source_url":"https://github.com/datitran/face2face-demo\">"},{"type":"has_code","target_id":"github:awjuliani:Pix2Pix-Film\">","source_url":"https://github.com/awjuliani/Pix2Pix-Film\">"},{"type":"has_code","target_id":"github:kaonashi-tyc:zi2zi\">[Tensorflow","source_url":"https://github.com/kaonashi-tyc/zi2zi\">[Tensorflow"},{"type":"has_code","target_id":"github:pfnet-research:chainer-pix2pix\">[Chainer]<","source_url":"https://github.com/pfnet-research/chainer-pix2pix\">[Chainer]<"},{"type":"has_code","target_id":"github:tjwei:GANotebooks\">[tf","source_url":"https://github.com/tjwei/GANotebooks\">[tf"},{"type":"has_code","target_id":"github:taey16:pix2pixBEGAN.pytorch\">[Pytorch]<","source_url":"https://github.com/taey16/pix2pixBEGAN.pytorch\">[Pytorch]<"},{"type":"has_code","target_id":"github:junyanz:pytorch-CycleGAN-and-pix2pix","source_url":"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"},{"type":"has_code","target_id":"github:junyanz:pytorch-CycleGAN-and-pix2pix","source_url":"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"},{"type":"has_code","target_id":"github:junyanz:pytorch-CycleGAN-and-pix2pix","source_url":"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"},{"type":"has_code","target_id":"github:junyanz:pytorch-CycleGAN-and-pix2pix","source_url":"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"},{"type":"has_code","target_id":"github:junyanz:pytorch-CycleGAN-and-pix2pix","source_url":"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"},{"type":"has_code","target_id":"github:GaParmar:img2img-turbo","source_url":"https://github.com/GaParmar/img2img-turbo"},{"type":"has_code","target_id":"github:taesungp:contrastive-unpaired-translation","source_url":"https://github.com/taesungp/contrastive-unpaired-translation"},{"type":"has_code","target_id":"github:junyanz:CycleGAN","source_url":"https://github.com/junyanz/CycleGAN"},{"type":"has_code","target_id":"github:phillipi:pix2pix","source_url":"https://github.com/phillipi/pix2pix"},{"type":"has_code","target_id":"github:NVIDIA:pix2pixHD","source_url":"https://github.com/NVIDIA/pix2pixHD"},{"type":"has_code","target_id":"github:junyanz:BicycleGAN","source_url":"https://github.com/junyanz/BicycleGAN"},{"type":"has_code","target_id":"github:NVlabs:SPADE","source_url":"https://github.com/NVlabs/SPADE"},{"type":"has_code","target_id":"github:junyanz:iGAN","source_url":"https://github.com/junyanz/iGAN"},{"type":"has_code","target_id":"github:CSAILVision:GANDissect","source_url":"https://github.com/CSAILVision/GANDissect"},{"type":"has_code","target_id":"github:junyanz:CatPapers","source_url":"https://github.com/junyanz/CatPapers"},{"type":"has_code","target_id":"github:pytorch:examples","source_url":"https://github.com/pytorch/examples"}]', NULL, 'NOASSERTION', 'approved', 80, 'c5fe41bb2002943a730c061ed28f4e04', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-junyanz-pytorch-CycleGAN-and-pix2pix from https://github.com/junyanz.png
Image converted to WebP: data/images/github-junyanz-pytorch-CycleGAN-and-pix2pix.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-HumanSignal-labelImg', 'github--humansignal--labelimg', 'labelImg', 'HumanSignal', '.. image:: /readme/images/labelimg.png :target: https://github.com/heartexlabs/label-studio Label Studio is a modern, multi-modal data annotation tool ======= LabelImg, the popular image annotation tool created by Tzutalin with the help of dozens contributors, is no longer actively being developed and has become part of the Label Studio community. Check out __, the most flexible open source data labeling tool for images, text, hypertext, audio, video and time-series data. __ Label Studio and ...', '["annotations","deep-learning","detection","image-classification","imagenet","python2","python3","recognition","tools","python"]', 'other', 24551, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/HumanSignal/labelImg","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '.. image:: /readme/images/labelimg.png\n        :target: https://github.com/heartexlabs/label-studio\n\nLabel Studio is a modern, multi-modal data annotation tool\n=======\n\nLabelImg, the popular image annotation tool created by Tzutalin with the help of dozens contributors, is no longer actively being developed and has become part of the Label Studio community. Check out `Label Studio <https://github.com/heartexlabs/label-studio>`__, the most flexible open source data labeling tool for images, text, hypertext, audio, video and time-series data. `Install <https://labelstud.io/guide/install.html>`__ Label Studio and join the `slack community <https://label-studio.slack.com/>`__ to get started.\n\n.. image:: /readme/images/label-studio-1-6-player-screenshot.png\n        :target: https://github.com/heartexlabs/label-studio\n\nAbout LabelImg\n========\n\n.. image:: https://img.shields.io/pypi/v/labelimg.svg\n        :target: https://pypi.python.org/pypi/labelimg\n\n.. image:: https://img.shields.io/github/workflow/status/tzutalin/labelImg/Package?style=for-the-badge\n        :alt: GitHub Workflow Status\n\n.. image:: https://img.shields.io/badge/lang-en-blue.svg\n        :target: https://github.com/tzutalin/labelImg\n\n.. image:: https://img.shields.io/badge/lang-zh-green.svg\n        :target: https://github.com/tzutalin/labelImg/blob/master/readme/README.zh.rst\n\n.. image:: https://img.shields.io/badge/lang-jp-green.svg\n        :target: https://github.com/tzutalin/labelImg/blob/master/readme/README.jp.rst\n\nLabelImg is a graphical image annotation tool.\n\nIt is written in Python and uses Qt for its graphical interface.\n\nAnnotations are saved as XML files in PASCAL VOC format, the format used\nby `ImageNet <http://www.image-net.org/>`__.  Besides, it also supports YOLO and CreateML formats.\n\n.. image:: https://raw.githubusercontent.com/tzutalin/labelImg/master/demo/demo3.jpg\n     :alt: Demo Image\n\n.. image:: https://raw.githubusercontent.com/tzutalin/labelImg/master/demo/demo.jpg\n     :alt: Demo Image\n\n`Watch a demo video <https://youtu.be/p0nR2YsCY_U>`__\n\nInstallation\n------------------\n\nGet from PyPI but only python3.0 or above\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThis is the simplest (one-command) install method on modern Linux distributions such as Ubuntu and Fedora.\n\n.. code:: shell\n\n    pip3 install labelImg\n    labelImg\n    labelImg [IMAGE_PATH] [PRE-DEFINED CLASS FILE]\n\n\nBuild from source\n~~~~~~~~~~~~~~~~~\n\nLinux/Ubuntu/Mac requires at least `Python\n2.6 <https://www.python.org/getit/>`__ and has been tested with `PyQt\n4.8 <https://www.riverbankcomputing.com/software/pyqt/intro>`__. However, `Python\n3 or above <https://www.python.org/getit/>`__ and  `PyQt5 <https://pypi.org/project/PyQt5/>`__ are strongly recommended.\n\n\nUbuntu Linux\n^^^^^^^^^^^^\n\nPython 3 + Qt5\n\n.. code:: shell\n\n    sudo apt-get install pyqt5-dev-tools\n    sudo pip3 install -r requirements/requirements-linux-python3.txt\n    make qt5py3\n    python3 labelImg.py\n    python3 labelImg.py [IMAGE_PATH] [PRE-DEFINED CLASS FILE]\n\nmacOS\n^^^^^\n\nPython 3 + Qt5\n\n.. code:: shell\n\n    brew install qt  # Install qt-5.x.x by Homebrew\n    brew install libxml2\n\n    or using pip\n\n    pip3 install pyqt5 lxml # Install qt and lxml by pip\n\n    make qt5py3\n    python3 labelImg.py\n    python3 labelImg.py [IMAGE_PATH] [PRE-DEFINED CLASS FILE]\n\n\nPython 3 Virtualenv (Recommended)\n\nVirtualenv can avoid a lot of the QT / Python version issues\n\n.. code:: shell\n\n    brew install python3\n    pip3 install pipenv\n    pipenv run pip install pyqt5==5.15.2 lxml\n    pipenv run make qt5py3\n    pipenv run python3 labelImg.py\n    [Optional] rm -rf build dist; pipenv run python setup.py py2app -A;mv "dist/labelImg.app" /Applications\n\nNote: The Last command gives you a nice .app file with a new SVG Icon in your /Applications folder. You can consider using the script: build-tools/build-for-macos.sh\n\n\nWindows\n^^^^^^^\n\nInstall `Python <https://www.python.org/downloads/windows/>`__,\n`PyQt5 <https://www.riverbankcomputing.com/software/pyqt/download5>`__\nand `install lxml <http://lxml.de/installation.html>`__.\n\nOpen cmd and go to the `labelImg <#labelimg>`__ directory\n\n.. code:: shell\n\n    pyrcc4 -o libs/resources.py resources.qrc\n    For pyqt5, pyrcc5 -o libs/resources.py resources.qrc\n\n    python labelImg.py\n    python labelImg.py [IMAGE_PATH] [PRE-DEFINED CLASS FILE]\n\nIf you want to package it into a separate EXE file\n\n.. code:: shell\n\n    Install pyinstaller and execute:\n\n    pip install pyinstaller\n    pyinstaller --hidden-import=pyqt5 --hidden-import=lxml -F -n "labelImg" -c labelImg.py -p ./libs -p ./\n\nWindows + Anaconda\n^^^^^^^^^^^^^^^^^^\n\nDownload and install `Anaconda <https://www.anaconda.com/download/#download>`__ (Python 3+)\n\nOpen the Anaconda Prompt and go to the `labelImg <#labelimg>`__ directory\n\n.. code:: shell\n\n    conda install pyqt=5\n    conda install -c anaconda lxml\n    pyrcc5 -o libs/resources.py resources.qrc\n    python labelImg.py\n    python labelImg.py [IMAGE_PATH] [PRE-DEFINED CLASS FILE]\n\nUse Docker\n~~~~~~~~~~~~~~~~~\n.. code:: shell\n\n    docker run -it \\n    --user $(id -u) \\n    -e DISPLAY=unix$DISPLAY \\n    --workdir=$(pwd) \\n    --volume="/home/$USER:/home/$USER" \\n    --volume="/etc/group:/etc/group:ro" \\n    --volume="/etc/passwd:/etc/passwd:ro" \\n    --volume="/etc/shadow:/etc/shadow:ro" \\n    --volume="/etc/sudoers.d:/etc/sudoers.d:ro" \\n    -v /tmp/.X11-unix:/tmp/.X11-unix \\n    tzutalin/py2qt4\n\n    make qt4py2;./labelImg.py\n\nYou can pull the image which has all of the installed and required dependencies. `Watch a demo video <https://youtu.be/nw1GexJzbCI>`__\n\n\nUsage\n-----\n\nSteps (PascalVOC)\n~~~~~~~~~~~~~~~~~\n\n1. Build and launch using the instructions above.\n2. Click ''Change default saved annotation folder'' in Menu/File\n3. Click ''Open Dir''\n4. Click ''Create RectBox''\n5. Click and release left mouse to select a region to annotate the rect\n   box\n6. You can use right mouse to drag the rect box to copy or move it\n\nThe annotation will be saved to the folder you specify.\n\nYou can refer to the below hotkeys to speed up your workflow.\n\nSteps (YOLO)\n~~~~~~~~~~~~\n\n1. In ``data/predefined_classes.txt`` define the list of classes that will be used for your training.\n\n2. Build and launch using the instructions above.\n\n3. Right below "Save" button in the toolbar, click "PascalVOC" button to switch to YOLO format.\n\n4. You may use Open/OpenDIR to process single or multiple images. When finished with a single image, click save.\n\nA txt file of YOLO format will be saved in the same folder as your image with same name. A file named "classes.txt" is saved to that folder too. "classes.txt" defines the list of class names that your YOLO label refers to.\n\nNote:\n\n- Your label list shall not change in the middle of processing a list of images. When you save an image, classes.txt will also get updated, while previous annotations will not be updated.\n\n- You shouldn''t use "default class" function when saving to YOLO format, it will not be referred.\n\n- When saving as YOLO format, "difficult" flag is discarded.\n\nCreate pre-defined classes\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can edit the\n`data/predefined\_classes.txt <https://github.com/tzutalin/labelImg/blob/master/data/predefined_classes.txt>`__\nto load pre-defined classes\n\nAnnotation visualization\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n1. Copy the existing lables file to same folder with the images. The labels file name must be same with image file name.\n\n2. Click File and choose ''Open Dir'' then Open the image folder.\n\n3. Select image in File List, it will appear the bounding box and label for all objects in that image.\n\n(Choose Display Labels mode in View to show/hide lablels)\n\n\nHotkeys\n~~~~~~~\n\n+--------------------+--------------------------------------------+\n| Ctrl + u           | Load all of the images from a directory    |\n+--------------------+--------------------------------------------+\n| Ctrl + r           | Change the default annotation target dir   |\n+--------------------+--------------------------------------------+\n| Ctrl + s           | Save                                       |\n+--------------------+--------------------------------------------+\n| Ctrl + d           | Copy the current label and rect box        |\n+--------------------+--------------------------------------------+\n| Ctrl + Shift + d   | Delete the current image                   |\n+--------------------+--------------------------------------------+\n| Space              | Flag the current image as verified         |\n+--------------------+--------------------------------------------+\n| w                  | Create a rect box                          |\n+--------------------+--------------------------------------------+\n| d                  | Next image                                 |\n+--------------------+--------------------------------------------+\n| a                  | Previous image                             |\n+--------------------+--------------------------------------------+\n| del                | Delete the selected rect box               |\n+--------------------+--------------------------------------------+\n| Ctrl++             | Zoom in                                    |\n+--------------------+--------------------------------------------+\n| Ctrl--             | Zoom out                                   |\n+--------------------+--------------------------------------------+\n| â†‘â†’â†“â†               | Keyboard arrows to move selected rect box  |\n+--------------------+--------------------------------------------+\n\n**Verify Image:**\n\nWhen pressing space, the user can flag the image as verified, a green background will appear.\nThis is used when creating a dataset automatically, the user can then through all the pictures and flag them instead of annotate them.\n\n**Difficult:**\n\nThe difficult field is set to 1 indicates that the object has been annotated as "difficult", for example, an object which is clearly visible but difficult to recognize without substantial use of context.\nAccording to your deep neural network implementation, you can include or exclude difficult objects during training.\n\nHow to reset the settings\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn case there are issues with loading the classes, you can either:\n\n1. From the top menu of the labelimg click on Menu/File/Reset All\n2. Remove the `.labelImgSettings.pkl` from your home directory. In Linux and Mac you can do:\n    `rm ~/.labelImgSettings.pkl`\n\n\nHow to contribute\n~~~~~~~~~~~~~~~~~\n\nSend a pull request\n\nLicense\n~~~~~~~\n`Free software: MIT license <https://github.com/tzutalin/labelImg/blob/master/LICENSE>`_\n\nCitation: Tzutalin. LabelImg. Git code (2015). https://github.com/tzutalin/labelImg\n\nRelated and additional tools\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n1. `Label Studio <https://github.com/heartexlabs/label-studio>`__ to label images, text, audio, video and time-series data for machine learning and AI\n2. `ImageNet Utils <https://github.com/tzutalin/ImageNet_Utils>`__ to\n   download image, create a label text for machine learning, etc\n3. `Use Docker to run labelImg <https://hub.docker.com/r/tzutalin/py2qt4>`__\n4. `Generating the PASCAL VOC TFRecord files <https://github.com/tensorflow/models/blob/4f32535fe7040bb1e429ad0e3c948a492a89482d/research/object_detection/g3doc/preparing_inputs.md#generating-the-pascal-voc-tfrecord-files>`__\n5. `App Icon based on Icon by Nick Roach (GPL) <https://www.elegantthemes.com/>`__\n6. `Setup python development in vscode <https://tzutalin.blogspot.com/2019/04/set-up-visual-studio-code-for-python-in.html>`__\n7. `The link of this project on iHub platform <https://code.ihub.org.cn/projects/260/repository/labelImg>`__\n8. `Convert annotation files to CSV format or format for Google Cloud AutoML <https://github.com/tzutalin/labelImg/tree/master/tools>`__\n\n\n\nStargazers over time\n~~~~~~~~~~~~~~~~~~~~\n\n.. image:: https://starchart.cc/tzutalin/labelImg.svg\n\n', '{"language":"Python","stars":24551,"forks":6561,"watchers":24551,"open_issues":451,"topics":["annotations","deep-learning","detection","image-classification","imagenet","python2","python3","recognition","tools"],"default_branch":"master","size_kb":242833,"archived":true,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:heartexlabs:label-studio","source_url":"https://github.com/heartexlabs/label-studio"},{"type":"has_code","target_id":"github:heartexlabs:label-studio>`__,","source_url":"https://github.com/heartexlabs/label-studio>`__,"},{"type":"has_code","target_id":"github:heartexlabs:label-studio","source_url":"https://github.com/heartexlabs/label-studio"},{"type":"has_code","target_id":"github:tzutalin:labelImg","source_url":"https://github.com/tzutalin/labelImg"},{"type":"has_code","target_id":"github:tzutalin:labelImg","source_url":"https://github.com/tzutalin/labelImg"},{"type":"has_code","target_id":"github:tzutalin:labelImg","source_url":"https://github.com/tzutalin/labelImg"},{"type":"has_code","target_id":"github:tzutalin:labelImg","source_url":"https://github.com/tzutalin/labelImg"},{"type":"has_code","target_id":"github:tzutalin:labelImg","source_url":"https://github.com/tzutalin/labelImg"},{"type":"has_code","target_id":"github:tzutalin:labelImg","source_url":"https://github.com/tzutalin/labelImg"},{"type":"has_code","target_id":"github:heartexlabs:label-studio>`__","source_url":"https://github.com/heartexlabs/label-studio>`__"},{"type":"has_code","target_id":"github:tzutalin:ImageNet_Utils>`__","source_url":"https://github.com/tzutalin/ImageNet_Utils>`__"},{"type":"has_code","target_id":"github:tensorflow:models","source_url":"https://github.com/tensorflow/models"},{"type":"has_code","target_id":"github:tzutalin:labelImg","source_url":"https://github.com/tzutalin/labelImg"}]', NULL, 'MIT', 'approved', 80, 'e6a3a6a105688e7afcc5b58686204df6', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-HumanSignal-labelImg from https://github.com/HumanSignal.png
Image converted to WebP: data/images/github-HumanSignal-labelImg.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-JARVIS', 'github--microsoft--jarvis', 'JARVIS', 'microsoft', 'The mission of JARVIS is to explore artificial general intelligence (AGI) and deliver cutting-edge research to the whole community. + [2024.01.15] We release Easytool for easier tool usage. + The code and datasets are available at EasyTool. + The paper is available at EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction. + [2023.11.30] We release TaskBench for evaluating task automation capability of LLMs. + The code and datasets are available at TaskBench. + The paper is availa...', '["deep-learning","platform","pytorch","python"]', 'other', 24485, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/JARVIS","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# JARVIS\n\n\n[![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/abs/2303.17580)\n[![Open in Spaces](https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/microsoft/HuggingGPT)\n\nThe mission of JARVIS is to explore artificial general intelligence (AGI) and deliver cutting-edge research to the whole community.\n\n## What''s New\n\n+  [2024.01.15] We release Easytool for easier tool usage.\n   + The code and datasets are available at [EasyTool](/easytool).\n   + The paper is available at [EasyTool: Enhancing LLM-based Agents with Concise Tool Instruction](https://arxiv.org/abs/2401.06201).\n+  [2023.11.30] We release TaskBench for evaluating task automation capability of LLMs.\n   + The code and datasets are available at [TaskBench](/taskbench).\n   + The paper is available at [TaskBench: Benchmarking Large Language Models for Task Automation](https://arxiv.org/abs/2311.18760).\n+  [2023.07.28] We are now in the process of planning evaluation and project rebuilding. We will release a new version of Jarvis in the near future.\n+  [2023.07.24] We released a light langchain version of Jarvis. See <a href="https://github.com/langchain-ai/langchain/tree/master/libs/experimental/langchain_experimental/autonomous_agents/hugginggpt">here</a>.\n+  [2023.04.16] Jarvis now supports the OpenAI service on the Azure platform and the GPT-4 model.\n+  [2023.04.06] We added the Gradio demo and built the web API for `/tasks` and `/results` in `server` mode.\n   +  The Gradio demo is now hosted on Hugging Face Space. (Build with `inference_mode=hybrid` and `local_deployment=standard`)\n   +  The Web API `/tasks` and `/results` access intermediate results for `Stage #1`: task planning and `Stage #1-3`: model selection with execution results. See <a href="#Server">here</a>.\n+  [2023.04.03] We added the CLI mode and provided parameters for configuring the scale of local endpoints.\n   +  You can enjoy a lightweight experience with Jarvis without deploying the models locally. See <a href="#Configuration">here</a>.\n   +  Just run `python awesome_chat.py --config configs/config.lite.yaml` to experience it.\n+  [2023.04.01] We updated a version of code for building.\n\n\n### Overview\n\nLanguage serves as an interface for LLMs to connect numerous AI models for solving complicated AI tasks!\n\n<p align="center">\n<img width="100%" alt="image" src="./hugginggpt/assets/intro.png">    \n</p>\n\n\nSee our paper: [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace](http://arxiv.org/abs/2303.17580), Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu and Yueting Zhuang (the first two authors contribute equally)\n\nWe introduce a collaborative system that consists of **an LLM as the controller** and **numerous expert models as collaborative executors** (from HuggingFace Hub). The workflow of our system consists of four stages:\n+ **Task Planning**: Using ChatGPT to analyze the requests of users to understand their intention, and disassemble them into possible solvable tasks.\n+ **Model Selection**: To solve the planned tasks, ChatGPT selects expert models hosted on Hugging Face based on their descriptions.\n+ **Task Execution**: Invokes and executes each selected model, and return the results to ChatGPT.\n+ **Response Generation**: Finally, using ChatGPT to integrate the prediction of all models, and generate responses.\n\n<p align="center"><img src="./hugginggpt/assets/overview.jpg"></p>\n\n### System Requirements\n\n#### Default (Recommended)\n\nFor `configs/config.default.yaml`:\n\n+ Ubuntu 16.04 LTS\n+ VRAM >= 24GB\n+ RAM > 12GB (minimal), 16GB (standard), 80GB (full)\n+ Disk > 284GB \n  + 42GB for `damo-vilab/text-to-video-ms-1.7b`\n  + 126GB for `ControlNet`\n  + 66GB for `stable-diffusion-v1-5`\n  + 50GB for others\n  \n#### Minimum (Lite)\n\nFor `configs/config.lite.yaml`:\n\n+ Ubuntu 16.04 LTS\n+ Nothing else\n\nThe configuration `configs/config.lite.yaml` does not require any expert models to be downloaded and deployed locally. However, it means that Jarvis is restricted to models running stably on HuggingFace Inference Endpoints.\n\n### Quick Start\n\nFirst replace `openai.key` and `huggingface.token` in `server/configs/config.default.yaml` with **your personal OpenAI Key** and **your Hugging Face Token**, or put them in the environment variables `OPENAI_API_KEY` and `HUGGINGFACE_ACCESS_TOKEN` respectively. Then run the following commands:\n\n<span id="Server"></span>\n\n#### For Server:\n\n```bash\n# setup env\ncd server\nconda create -n jarvis python=3.8\nconda activate jarvis\nconda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia\npip install -r requirements.txt\n\n# download models. Make sure that `git-lfs` is installed.\ncd models\nbash download.sh # required when `inference_mode` is `local` or `hybrid`. \n\n# run server\ncd ..\npython models_server.py --config configs/config.default.yaml # required when `inference_mode` is `local` or `hybrid`\npython awesome_chat.py --config configs/config.default.yaml --mode server # for text-davinci-003\n```\n\nNow you can access Jarvis'' services by the Web API. \n\n+ `/hugginggpt` --method `POST`, access the full service.\n+ `/tasks` --method `POST`, access intermediate results for Stage #1.\n+ `/results` --method `POST`, access intermediate results for Stage #1-3.\n\nFor example:\n\n```bash\n# request\ncurl --location ''http://localhost:8004/tasks'' \\n--header ''Content-Type: application/json'' \\n--data ''{\n    "messages": [\n        {\n            "role": "user",\n            "content": "based on pose of /examples/d.jpg and content of /examples/e.jpg, please show me a new image"\n        }\n    ]\n}''\n\n# response\n[{"args":{"image":"/examples/d.jpg"},"dep":[-1],"id":0,"task":"openpose-control"},{"args":{"image":"/examples/e.jpg"},"dep":[-1],"id":1,"task":"image-to-text"},{"args":{"image":"<GENERATED>-0","text":"<GENERATED>-1"},"dep":[1,0],"id":2,"task":"openpose-text-to-image"}]\n```\n\n\n#### For Web:\n\nWe provide a user-friendly web page. After starting `awesome_chat.py` in a server mode, you can run the commands to communicate with Jarvis in your browser:\n \n- you need to install `nodejs` and `npm` first.\n- [ IMPORTANT ] if you are running the web client on another machine, you need set `http://{LAN_IP_of_the_server}:{port}/` to `HUGGINGGPT_BASE_URL` of `web/src/config/index.ts`.\n- if you want to use the video generation feature, you need to compile `ffmpeg` manually with H.264.\n- you can switch to ChatGPT by `double click` on the setting icon!\n\n```bash\ncd web\nnpm install\nnpm run dev\n```\n\n```bash\n# Optional: Install ffmpeg\n# This command need be executed without errors.\nLD_LIBRARY_PATH=/usr/local/lib /usr/local/bin/ffmpeg -i input.mp4 -vcodec libx264 output.mp4\n```\n\n<span id="Gradio"></span>\n\n#### For Gradio\n\nThe Gradio demo is now hosted on Hugging Face Space. You can also run the following commands to start the demo locally:\n\n```bash\npython models_server.py --config configs/config.gradio.yaml\npython run_gradio_demo.py --config configs/config.gradio.yaml\n\n# or run the HF Space as a Docker image (Build with `inference_mode=hibrid` and `local_deployment=standard`)\ndocker run -it -p 7860:7860 --platform=linux/amd64 registry.hf.space/microsoft-hugginggpt:latest python app.py\n```\n\n#### For CLI:\n\nYou can also run Jarvis more easily in CLI mode:\n\n```bash\ncd server\npython awesome_chat.py --config configs/config.default.yaml --mode cli\n```\n\nExamples of CLI mode:\n\n```\nWelcome to Jarvis! A collaborative system that consists of an LLM as the controller and numerous expert models as collaborative executors. Jarvis can plan tasks, schedule Hugging Face models, generate friendly responses based on your requests, and help you with many things. Please enter your request (`exit` to exit).\n[ User ]: Given a collection of image A: /examples/a.jpg, B: /examples/b.jpg, C: /examples/c.jpg, please tell me how many zebras in these picture?\n[ Jarvis ]: Based on your request, I have used several models to process the images you provided. For the image A: /examples/a.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text "a cat sitting on a window sill looking out". Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box ''/images/a3dc.jpg'', which contains two objects with labels ''potted plant'' and ''cat''. For the image B: /examples/b.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text "two zebras standing in a field of dry grass". Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box ''/images/f5df.jpg'', which contains three objects with labels ''zebra''. For the image C: /examples/c.jpg, I used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text "a zebra standing in a field of tall grass". Then I used the object-detection model facebook/detr-resnet-50 to generate the image with predicted box ''/images/69c7.jpg'', which contains one object with label ''zebra''. Finally, I used the canny-control model canny-control to generate the image ''/images/7e18.png'' based on the image /examples/f.jpg. Therefore, there are four zebras in these pictures. Is there anything else I can help you with?\n[ User ]: Please answer all the named entities in the sentence: Iron Man is a superhero appearing in American comic books published by Marvel Comics. The character was co-created by writer and editor Stan Lee, developed by scripter Larry Lieber, and designed by artists Don Heck and Jack Kirby.\n[ Jarvis ]: Sure, I understand your request. Based on the inference results of the models, I will answer your request step by step. Firstly, I used the token-classification model dslim/bert-base-NER to recognize the named entities in the sentence. The inference result is that the named entities are Iron Man, Stan Lee, Larry Lieber, Don Heck and Jack Kirby. Then, I used the question-answering model deepset/roberta-base-squad2 to answer your request. The inference result is that there is no output since the context cannot be empty. Therefore, I cannot make it. I hope this answer your request. Is there anything else I can help you with?\n[ User ]: Please generate a canny image based on /examples/f.jpg\n[ Jarvis ]: Sure. I have generated a canny image based on /examples/f.jpg. To do this, I first used the image-to-text model nlpconnect/vit-gpt2-image-captioning to generate the text description of the image, which is "a herd of giraffes and zebras grazing in a field". Then I used the canny-control model to generate the canny image of the original image, which is stored in the path "/images/1ea2.png". Finally, I used the canny-text-to-image model lllyasviel/sd-controlnet-canny to generate the canny image based on the text description and the canny image, which is stored in the path "/images/ba66.png". Do you need anything else?\n```\n\n\n### Configuration\n\nThe server-side configuration file is `server/configs/config.default.yaml`, and some parameters are presented as follows:\n\n+ `model`: LLM, currently supports `text-davinci-003`. We are working on integrating more open-source LLMs.\n+ `inference_mode`: mode of inference endpoints\n  + `local`: only use the local inference endpoints\n  + `huggingface`: only use the Hugging Face Inference Endpoints **(free of local inference endpoints)**\n  + `hybrid`: both of `local` and `huggingface`\n+ `local_deployment`: scale of locally deployed models, works under `local` or `hybrid` inference mode:\n  +  `minimal` (RAM>12GB, ControlNet only)\n  +  `standard` (RAM>16GB, ControlNet + Standard Pipelines)\n  +  `full` (RAM>42GB, All registered models)\n\nOn a personal laptop, we recommend the configuration of `inference_mode: hybrid `and `local_deployment: minimal`. But the available models under this setting may be limited due to the instability of remote Hugging Face Inference Endpoints.\n\n### NVIDIA Jetson Embedded Device Support\nA [Dockerfile](./Dockerfile.jetson) is included that provides experimental support for [NVIDIA Jetson embedded devices](https://developer.nvidia.com/embedded-computing).  This image provides accelerated ffmpeg, pytorch, torchaudio, and torchvision dependencies.  To build the docker image, [ensure that the default docker runtime is set to ''nvidia''](https://github.com/NVIDIA/nvidia-docker/wiki/Advanced-topics#default-runtime).  A pre-built image is provided at https://hub.docker.com/r/toolboc/nv-jarvis.\n\n```bash\n#Build the docker image\ndocker build --pull --rm -f "Dockerfile.jetson" -t toolboc/nv-jarvis:r35.2.1 \n```\n\nDue to to memory requirements, JARVIS is required to run on Jetson AGX Orin family devices (64G on-board RAM device preferred) with config options set to:\n* `inference_mode: local` \n* `local_deployment: standard`  \n\nModels and configs are recommended to be provided through a volume mount from the host to the container as shown in the `docker run` step below.  It is possible to uncomment the `# Download local models` section of the [Dockerfile](./Dockerfile.jetson) to build a container with models included.\n\n#### Start the model server, awesomechat, and web app on Jetson Orin AGX\n\n```bash\n# run the container which will automatically start the model server\ndocker run --name jarvis --net=host --gpus all -v ~/jarvis/configs:/app/server/configs -v ~/src/JARVIS/server/models:/app/server/models toolboc/nv-jarvis:r35.2.1\n\n# (wait for model server to complete initialization)\n\n# start awesome_chat.py \ndocker exec jarvis python3 awesome_chat.py --config configs/config.default.yaml --mode server\n\n#start the web application (application will be acessible at http://localhost:9999)\ndocker exec jarvis npm run dev --prefix=/app/web\n```\n\n### Screenshots\n\n<p align="center"><img src="./hugginggpt/assets/screenshot_q.jpg"><img src="./hugginggpt/assets/screenshot_a.jpg"></p>\n\n\n\n\n## Citation\nIf you find this work useful in your method, you can cite the paper as below:\n\n    @inproceedings{shen2023hugginggpt,\n      author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},\n      booktitle = {Advances in Neural Information Processing Systems},\n      title = {HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace},\n      year = {2023}\n    }\n\n>\n    @article{shen2023taskbench,\n      title   = {TaskBench: Benchmarking Large Language Models for Task Automation},\n      author  = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Zhang, Wenqi and Ren, Kan and Yuan, Siyu and Lu, Weiming and Li, Dongsheng and Zhuang, Yueting},\n      journal = {arXiv preprint arXiv:2311.18760},\n      year    = {2023}\n    }\n\n>\n    @article{yuan2024easytool,\n      title   = {EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction},\n      author  = {Siyu Yuan and Kaitao Song and Jiangjie Chen and Xu Tan and Yongliang Shen and Ren Kan and Dongsheng Li and Deqing Yang},\n      journal = {arXiv preprint arXiv:2401.06201},\n      year    = {2024}\n    }\n', '{"language":"Python","stars":24485,"forks":2057,"watchers":24485,"open_issues":521,"topics":["deep-learning","platform","pytorch"],"default_branch":"main","size_kb":41674,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:langchain-ai:langchain","source_url":"https://github.com/langchain-ai/langchain"},{"type":"has_code","target_id":"github:NVIDIA:nvidia-docker","source_url":"https://github.com/NVIDIA/nvidia-docker"}]', NULL, 'MIT', 'approved', 80, '369e288e7826121d407a77ded6f13ddb', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-JARVIS from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-JARVIS.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-pyg-team-pytorch-geometric', 'github--pyg-team--pytorch-geometric', 'pytorch_geometric', 'pyg-team', '<p align="center"> <img height="150" src="https://raw.githubusercontent.com/pyg-team/pyg_sphinx_theme/master/pyg_sphinx_theme/static/img/pyg_logo_text.svg?sanitize=true" /> </p> ______________________________________________________________________ <div align="center"> [![PyPI Version][pypi-image]][pypi-url] [![PyPI Download][pypi-download-image]][pypi-download-url] [![Slack][slack-image]][slack-url] [![Contributing][contributing-image]][contributing-url] **Documentation** | **PyG 1.0 Paper**...', '["deep-learning","geometric-deep-learning","graph-convolutional-networks","graph-neural-networks","pytorch","python"]', 'other', 23243, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/pyg-team/pytorch_geometric","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <img height="150" src="https://raw.githubusercontent.com/pyg-team/pyg_sphinx_theme/master/pyg_sphinx_theme/static/img/pyg_logo_text.svg?sanitize=true" />\n</p>\n\n______________________________________________________________________\n\n<div align="center">\n\n[![PyPI Version][pypi-image]][pypi-url]\n[![PyPI Download][pypi-download-image]][pypi-download-url]\n[![Slack][slack-image]][slack-url]\n[![Contributing][contributing-image]][contributing-url]\n\n**[Documentation](https://pytorch-geometric.readthedocs.io)** |\n**[PyG 1.0 Paper](https://arxiv.org/abs/1903.02428)** |\n**[PyG 2.0 Paper](https://arxiv.org/abs/2507.16991)** |\n**[Colab Notebooks](https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html)** |\n**[External Resources](https://pytorch-geometric.readthedocs.io/en/latest/external/resources.html)** |\n**[OGB Examples](https://github.com/snap-stanford/ogb/tree/master/examples)**\n\n</div>\n\n**PyG** *(PyTorch Geometric)* is a library built upon [PyTorch](https://pytorch.org/) to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data.\n\nIt consists of various methods for deep learning on graphs and other irregular structures, also known as *[geometric deep learning](http://geometricdeeplearning.com/)*, from a variety of published papers.\nIn addition, it consists of easy-to-use mini-batch loaders for operating on many small and single giant graphs, [multi GPU-support](https://github.com/pyg-team/pytorch_geometric/tree/master/examples/multi_gpu), [`torch.compile`](https://pytorch-geometric.readthedocs.io/en/latest/advanced/compile.html) support, [`DataPipe`](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/datapipe.py) support, a large number of common benchmark datasets (based on simple interfaces to create your own), and helpful transforms, both for learning on arbitrary graphs as well as on 3D meshes or point clouds.\n\n**[Click here to join our Slack community!][slack-url]**\n\n<p align="center">\n  <a href="https://medium.com/stanford-cs224w"><img style="max-width: 941px" src="https://data.pyg.org/img/cs224w_tutorials.png" /></a>\n</p>\n\n______________________________________________________________________\n\n- [Library Highlights](#library-highlights)\n- [Quick Tour for New Users](#quick-tour-for-new-users)\n- [Architecture Overview](#architecture-overview)\n- [Implemented GNN Models](#implemented-gnn-models)\n- [Installation](#installation)\n\n## Library Highlights\n\nWhether you are a machine learning researcher or first-time user of machine learning toolkits, here are some reasons to try out PyG for machine learning on graph-structured data.\n\n- **Easy-to-use and unified API**:\n  All it takes is 10-20 lines of code to get started with training a GNN model (see the next section for a [quick tour](#quick-tour-for-new-users)).\n  PyG is *PyTorch-on-the-rocks*: It utilizes a tensor-centric API and keeps design principles close to vanilla PyTorch.\n  If you are already familiar with PyTorch, utilizing PyG is straightforward.\n- **Comprehensive and well-maintained GNN models**:\n  Most of the state-of-the-art Graph Neural Network architectures have been implemented by library developers or authors of research papers and are ready to be applied.\n- **Great flexibility**:\n  Existing PyG models can easily be extended for conducting your own research with GNNs.\n  Making modifications to existing models or creating new architectures is simple, thanks to its easy-to-use message passing API, and a variety of operators and utility functions.\n- **Large-scale real-world GNN models**:\n  We focus on the need of GNN applications in challenging real-world scenarios, and support learning on diverse types of graphs, including but not limited to: scalable GNNs for graphs with millions of nodes; dynamic GNNs for node predictions over time; heterogeneous GNNs with multiple node types and edge types.\n\n## Quick Tour for New Users\n\nIn this quick tour, we highlight the ease of creating and training a GNN model with only a few lines of code.\n\n### Train your own GNN model\n\nIn the first glimpse of PyG, we implement the training of a GNN for classifying papers in a citation graph.\nFor this, we load the [Cora](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html) dataset, and create a simple 2-layer GCN model using the pre-defined [`GCNConv`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html):\n\n```python\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.datasets import Planetoid\n\ndataset = Planetoid(root=''.'', name=''Cora'')\n\nclass GCN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels):\n        super().__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.conv2 = GCNConv(hidden_channels, out_channels)\n\n    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n        # x: Node feature matrix of shape [num_nodes, in_channels]\n        # edge_index: Graph connectivity matrix of shape [2, num_edges]\n        x = self.conv1(x, edge_index).relu()\n        x = self.conv2(x, edge_index)\n        return x\n\nmodel = GCN(dataset.num_features, 16, dataset.num_classes)\n```\n\n<details>\n<summary>We can now optimize the model in a training loop, similar to the <a href="https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html#full-implementation">standard PyTorch training procedure</a>.</summary>\n\n```python\nimport torch.nn.functional as F\n\ndata = dataset[0]\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nfor epoch in range(200):\n    pred = model(data.x, data.edge_index)\n    loss = F.cross_entropy(pred[data.train_mask], data.y[data.train_mask])\n\n    # Backpropagation\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\n</details>\n\nMore information about evaluating final model performance can be found in the corresponding [example](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn.py).\n\n### Create your own GNN layer\n\nIn addition to the easy application of existing GNNs, PyG makes it simple to implement custom Graph Neural Networks (see [here](https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_gnn.html) for the accompanying tutorial).\nFor example, this is all it takes to implement the [edge convolutional layer](https://arxiv.org/abs/1801.07829) from Wang *et al.*:\n\n$$x_i^{\\prime} ~ = ~ \\max\_{j \\in \\mathcal{N}(i)} ~ \\textrm{MLP}\_{\\theta} \\left( [ ~ x_i, ~ x_j - x_i ~ ] \\right)$$\n\n```python\nimport torch\nfrom torch import Tensor\nfrom torch.nn import Sequential, Linear, ReLU\nfrom torch_geometric.nn import MessagePassing\n\nclass EdgeConv(MessagePassing):\n    def __init__(self, in_channels, out_channels):\n        super().__init__(aggr="max")  # "Max" aggregation.\n        self.mlp = Sequential(\n            Linear(2 * in_channels, out_channels),\n            ReLU(),\n            Linear(out_channels, out_channels),\n        )\n\n    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n        # x: Node feature matrix of shape [num_nodes, in_channels]\n        # edge_index: Graph connectivity matrix of shape [2, num_edges]\n        return self.propagate(edge_index, x=x)  # shape [num_nodes, out_channels]\n\n    def message(self, x_j: Tensor, x_i: Tensor) -> Tensor:\n        # x_j: Source node features of shape [num_edges, in_channels]\n        # x_i: Target node features of shape [num_edges, in_channels]\n        edge_features = torch.cat([x_i, x_j - x_i], dim=-1)\n        return self.mlp(edge_features)  # shape [num_edges, out_channels]\n```\n\n## Architecture Overview\n\nPyG provides a multi-layer framework that enables users to build Graph Neural Network solutions on both low and high levels.\nIt comprises of the following components:\n\n- The PyG **engine** utilizes the powerful PyTorch deep learning framework with full [`torch.compile`](https://pytorch-geometric.readthedocs.io/en/latest/advanced/compile.html) and [TorchScript](https://pytorch-geometric.readthedocs.io/en/latest/advanced/jit.html) support, as well as additions of efficient CPU/CUDA libraries for operating on sparse data, *e.g.*, [`pyg-lib`](https://github.com/pyg-team/pyg-lib).\n- The PyG **storage** handles data processing, transformation and loading pipelines. It is capable of handling and processing large-scale graph datasets, and provides effective solutions for heterogeneous graphs. It further provides a variety of sampling solutions, which enable training of GNNs on large-scale graphs.\n- The PyG **operators** bundle essential functionalities for implementing Graph Neural Networks. PyG supports important GNN building blocks that can be combined and applied to various parts of a GNN model, ensuring rich flexibility of GNN design.\n- Finally, PyG provides an abundant set of GNN **models**, and examples that showcase GNN models on standard graph benchmarks. Thanks to its flexibility, users can easily build and modify custom GNN models to fit their specific needs.\n\n<p align="center">\n  <img width="100%" src="https://raw.githubusercontent.com/pyg-team/pytorch_geometric/master/docs/source/_figures/architecture.svg?sanitize=true" />\n</p>\n\n## Implemented GNN Models\n\nWe list currently supported PyG models, layers and operators according to category:\n\n**GNN layers:**\nAll Graph Neural Network layers are implemented via the **[`nn.MessagePassing`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.MessagePassing.html)** interface.\nA GNN layer specifies how to perform message passing, *i.e.* by designing different message, aggregation and update functions as defined [here](https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_gnn.html).\nThese GNN layers can be stacked together to create Graph Neural Network models.\n\n- **[GCNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html)** from Kipf and Welling: [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907) (ICLR 2017) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn.py)\]\n- **[ChebConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.ChebConv.html)** from Defferrard *et al.*: [Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering](https://arxiv.org/abs/1606.09375) (NIPS 2016) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn.py#L36-L37)\]\n- **[GATConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html)** from VeliÄkoviÄ‡ *et al.*: [Graph Attention Networks](https://arxiv.org/abs/1710.10903) (ICLR 2018) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gat.py)\]\n\n<details>\n<summary><b>Expand to see all implemented GNN layers...</b></summary>\n\n- **[GCN2Conv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCN2Conv.html)** from Chen *et al.*: [Simple and Deep Graph Convolutional Networks](https://arxiv.org/abs/2007.02133) (ICML 2020) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn2_cora.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn2_ppi.py)\]\n- **[SplineConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SplineConv.html)** from Fey *et al.*: [SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels](https://arxiv.org/abs/1711.08920) (CVPR 2018) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/cora.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/faust.py)\]\n- **[NNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.NNConv.html)** from Gilmer *et al.*: [Neural Message Passing for Quantum Chemistry](https://arxiv.org/abs/1704.01212) (ICML 2017) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/qm9_nn_conv.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/mnist_nn_conv.py)\]\n- **[CGConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.CGConv.html)** from Xie and Grossman: [Crystal Graph Convolutional Neural Networks for an Accurate and Interpretable Prediction of Material Properties](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.120.145301) (Physical Review Letters 120, 2018)\n- **[ECConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.ECConv.html)** from Simonovsky and Komodakis: [Edge-Conditioned Convolution on Graphs](https://arxiv.org/abs/1704.02901) (CVPR 2017)\n- **[EGConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.EGConv.html)** from Tailor *et al.*: [Adaptive Filters and Aggregator Fusion for Efficient Graph Convolutions](https://arxiv.org/abs/2104.01481) (GNNSys 2021) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/egc.py)\]\n- **[GATv2Conv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATv2Conv.html)** from Brody *et al.*: [How Attentive are Graph Attention Networks?](https://arxiv.org/abs/2105.14491) (ICLR 2022)\n- **[TransformerConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.TransformerConv.html)** from Shi *et al.*: [Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification](https://arxiv.org/abs/2009.03509) (CoRR 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/unimp_arxiv.py)\]\n- **[SAGEConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SAGEConv.html)** from Hamilton *et al.*: [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216) (NIPS 2017) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/reddit.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/ogbn_train.py), [**Example3**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_sage_unsup.py), [**Example4**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_sage_unsup_ppi.py)\]\n- **[GraphConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GraphConv.html)** from, *e.g.*, Morris *et al.*: [Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks](https://arxiv.org/abs/1810.02244) (AAAI 2019)\n- **[GatedGraphConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GatedGraphConv.html)** from Li *et al.*: [Gated Graph Sequence Neural Networks](https://arxiv.org/abs/1511.05493) (ICLR 2016)\n- **[ResGatedGraphConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.ResGatedGraphConv.html)** from Bresson and Laurent: [Residual Gated Graph ConvNets](https://arxiv.org/abs/1711.07553) (CoRR 2017)\n- **[GINConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINConv.html)** from Xu *et al.*: [How Powerful are Graph Neural Networks?](https://arxiv.org/abs/1810.00826) (ICLR 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/mutag_gin.py)\]\n- **[GINEConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINEConv.html)** from Hu *et al.*: [Strategies for Pre-training Graph Neural Networks](https://arxiv.org/abs/1905.12265) (ICLR 2020)\n- **[ARMAConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.ARMAConv.html)** from Bianchi *et al.*: [Graph Neural Networks with Convolutional ARMA Filters](https://arxiv.org/abs/1901.01343) (CoRR 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/arma.py)\]\n- **[SGConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SGConv.html)** from Wu *et al.*: [Simplifying Graph Convolutional Networks](https://arxiv.org/abs/1902.07153) (CoRR 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/sgc.py)\]\n- **[APPNP](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.APPNP.html)** from Klicpera *et al.*: [Predict then Propagate: Graph Neural Networks meet Personalized PageRank](https://arxiv.org/abs/1810.05997) (ICLR 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/citation/appnp.py)\]\n- **[MFConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.MFConv.html)** from Duvenaud *et al.*: [Convolutional Networks on Graphs for Learning Molecular Fingerprints](https://arxiv.org/abs/1509.09292) (NIPS 2015)\n- **[AGNNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.AGNNConv.html)** from Thekumparampil *et al.*: [Attention-based Graph Neural Network for Semi-Supervised Learning](https://arxiv.org/abs/1803.03735) (CoRR 2017) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/agnn.py)\]\n- **[TAGConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.TAGConv.html)** from Du *et al.*: [Topology Adaptive Graph Convolutional Networks](https://arxiv.org/abs/1710.10370) (CoRR 2017) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/tagcn.py)\]\n- **[PNAConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.PNAConv.html)** from Corso *et al.*: [Principal Neighbourhood Aggregation for Graph Nets](https://arxiv.org/abs/2004.05718) (CoRR 2020) \[**[Example](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/pna.py)**\]\n- **[FAConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.FAConv.html)** from Bo *et al.*: [Beyond Low-Frequency Information in Graph Convolutional Networks](https://arxiv.org/abs/2101.00797) (AAAI 2021)\n- **[PDNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.nn.conv.PDNConv.html)** from Rozemberczki *et al.*: [Pathfinder Discovery Networks for Neural Message Passing](https://arxiv.org/abs/2010.12878) (WWW 2021)\n- **[RGCNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.RGCNConv.html)** from Schlichtkrull *et al.*: [Modeling Relational Data with Graph Convolutional Networks](https://arxiv.org/abs/1703.06103) (ESWC 2018) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/rgcn.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/rgcn_link_pred.py)\]\n- **[RGATConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.RGATConv.html)** from Busbridge *et al.*: [Relational Graph Attention Networks](https://arxiv.org/abs/1904.05811) (CoRR 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/rgat.py)\]\n- **[FiLMConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.FiLMConv.html)** from Brockschmidt: [GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation](https://arxiv.org/abs/1906.12192) (ICML 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/film.py)\]\n- **[SignedConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SignedConv.html)** from Derr *et al.*: [Signed Graph Convolutional Network](https://arxiv.org/abs/1808.06354) (ICDM 2018) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/signed_gcn.py)\]\n- **[DNAConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.DNAConv.html)** from Fey: [Just Jump: Dynamic Neighborhood Aggregation in Graph Neural Networks](https://arxiv.org/abs/1904.04849) (ICLR-W 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/dna.py)\]\n- **[PANConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.PANConv.html)** from Ma *et al.*: [Path Integral Based Convolution and Pooling for Graph Neural Networks](https://arxiv.org/abs/2006.16811) (NeurIPS 2020)\n- **[PointNetConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.PointNetConv.html)** (including **[Iterative Farthest Point Sampling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.fps.html)**, dynamic graph generation based on **[nearest neighbor](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.knn_graph.html)** or **[maximum distance](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.radius_graph.html)**, and **[k-NN interpolation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.unpool.knn_interpolate.html)** for upsampling) from Qi *et al.*: [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://arxiv.org/abs/1612.00593) (CVPR 2017) and [PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space](https://arxiv.org/abs/1706.02413) (NIPS 2017) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/pointnet2_classification.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/pointnet2_segmentation.py)\]\n- **[EdgeConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.EdgeConv.html)** from Wang *et al.*: [Dynamic Graph CNN for Learning on Point Clouds](https://arxiv.org/abs/1801.07829) (CoRR, 2018) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/dgcnn_classification.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/dgcnn_segmentation.py)\]\n- **[XConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.XConv.html)** from Li *et al.*: [PointCNN: Convolution On X-Transformed Points](https://arxiv.org/abs/1801.07791) (NeurIPS 2018) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/points/point_cnn.py)\]\n- **[PPFConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.PPFConv.html)** from Deng *et al.*: [PPFNet: Global Context Aware Local Features for Robust 3D Point Matching](https://arxiv.org/abs/1802.02669) (CVPR 2018)\n- **[GMMConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GMMConv.html)** from Monti *et al.*: [Geometric Deep Learning on Graphs and Manifolds using Mixture Model CNNs](https://arxiv.org/abs/1611.08402) (CVPR 2017)\n- **[FeaStConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.FeaStConv.html)** from Verma *et al.*: [FeaStNet: Feature-Steered Graph Convolutions for 3D Shape Analysis](https://arxiv.org/abs/1706.05206) (CVPR 2018)\n- **[PointTransformerConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.PointTransformerConv.html)** from Zhao *et al.*: [Point Transformer](https://arxiv.org/abs/2012.09164) (2020)\n- **[HypergraphConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.HypergraphConv.html)** from Bai *et al.*: [Hypergraph Convolution and Hypergraph Attention](https://arxiv.org/abs/1901.08150) (CoRR 2019)\n- **[GravNetConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GravNetConv.html)** from Qasim *et al.*: [Learning Representations of Irregular Particle-detector Geometry with Distance-weighted Graph Networks](https://arxiv.org/abs/1902.07987) (European Physics Journal C, 2019)\n- **[SuperGAT](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SuperGATConv.html)** from Kim and Oh: [How To Find Your Friendly Neighborhood: Graph Attention Design With Self-Supervision](https://openreview.net/forum?id=Wi5KUNlqWty) (ICLR 2021) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/super_gat.py)\]\n- **[HGTConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.HGTConv.html)** from Hu *et al.*: [Heterogeneous Graph Transformer](https://arxiv.org/abs/2003.01332) (WWW 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/hgt_dblp.py)\]\n- **[HEATConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.HEATonv.html)** from Mo *et al.*: [Heterogeneous Edge-Enhanced Graph Attention Network For Multi-Agent Trajectory Prediction](https://arxiv.org/abs/2106.07161) (CoRR 2021)\n- **[SSGConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SSGConv.html)** from Zhu *et al.*: [Simple Spectral Graph Convolution](https://openreview.net/forum?id=CYO5T-YjWZV) (ICLR 2021)\n- **[FusedGATConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.FusedGATConv.html)** from Zhang *et al.*: [Understanding GNN Computational Graph: A Coordinated Computation, IO, and Memory Perspective](https://proceedings.mlsys.org/paper/2022/file/9a1158154dfa42caddbd0694a4e9bdc8-Paper.pdf) (MLSys 2022)\n- **[GPSConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GPSConv.html)** from RampÃ¡Å¡ek *et al.*: [Recipe for a General, Powerful, Scalable Graph Transformer](https://arxiv.org/abs/2205.12454) (NeurIPS 2022) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_gps.py)\]\n\n</details>\n\n**Pooling layers:**\nGraph pooling layers combine the vectorial representations of a set of nodes in a graph (or a subgraph) into a single vector representation that summarizes its properties of nodes.\nIt is commonly applied to graph-level tasks, which require combining node features into a single graph representation.\n\n- **[Top-K Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.TopKPooling.html)** from Gao and Ji: [Graph U-Nets](https://arxiv.org/abs/1905.05178) (ICML 2019), Cangea *et al.*: [Towards Sparse Hierarchical Graph Classifiers](https://arxiv.org/abs/1811.01287) (NeurIPS-W 2018) and Knyazev *et al.*: [Understanding Attention and Generalization in Graph Neural Networks](https://arxiv.org/abs/1905.02850) (ICLR-W 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/proteins_topk_pool.py)\]\n- **[DiffPool](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.dense.dense_diff_pool.html)** from Ying *et al.*: [Hierarchical Graph Representation Learning with Differentiable Pooling](https://arxiv.org/abs/1806.08804) (NeurIPS 2018) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/proteins_diff_pool.py)\]\n\n<details>\n<summary><b>Expand to see all implemented pooling layers...</b></summary>\n\n- **[Attentional Aggregation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.AttentionalAggregation.html)** from Li *et al.*: [Graph Matching Networks for Learning the Similarity of Graph Structured Objects](https://arxiv.org/abs/1904.12787) (ICML 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/global_attention.py)\]\n- **[Set2Set](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.Set2Set.html)** from Vinyals *et al.*: [Order Matters: Sequence to Sequence for Sets](https://arxiv.org/abs/1511.06391) (ICLR 2016) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/set2set.py)\]\n- **[Sort Aggregation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.SortAggregation.html)** from Zhang *et al.*: [An End-to-End Deep Learning Architecture for Graph Classification](https://www.cse.wustl.edu/~muhan/papers/AAAI_2018_DGCNN.pdf) (AAAI 2018) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/sort_pool.py)\]\n- **[MinCut Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.dense.dense_mincut_pool.html)** from Bianchi *et al.*: [Spectral Clustering with Graph Neural Networks for Graph Pooling](https://arxiv.org/abs/1907.00481) (ICML 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/proteins_mincut_pool.py)\]\n- **[DMoN Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.dense.DMoNPooling.html)** from Tsitsulin *et al.*: [Graph Clustering with Graph Neural Networks](https://arxiv.org/abs/2006.16904) (CoRR 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/proteins_dmon_pool.py)\]\n- **[Graclus Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.graclus.html)** from Dhillon *et al.*: [Weighted Graph Cuts without Eigenvectors: A Multilevel Approach](http://www.cs.utexas.edu/users/inderjit/public_papers/multilevel_pami.pdf) (PAMI 2007) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/mnist_graclus.py)\]\n- **[Voxel Grid Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.voxel_grid.html)** from, *e.g.*, Simonovsky and Komodakis: [Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs](https://arxiv.org/abs/1704.02901) (CVPR 2017) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/mnist_voxel_grid.py)\]\n- **[SAG Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.SAGPooling.html)** from Lee *et al.*: [Self-Attention Graph Pooling](https://arxiv.org/abs/1904.08082) (ICML 2019) and Knyazev *et al.*: [Understanding Attention and Generalization in Graph Neural Networks](https://arxiv.org/abs/1905.02850) (ICLR-W 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/sag_pool.py)\]\n- **[Edge Pooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.EdgePooling.html)** from Diehl *et al.*: [Towards Graph Pooling by Edge Contraction](https://graphreason.github.io/papers/17.pdf) (ICML-W 2019) and Diehl: [Edge Contraction Pooling for Graph Neural Networks](https://arxiv.org/abs/1905.10990) (CoRR 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/edge_pool.py)\]\n- **[ASAPooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.ASAPooling.html)** from Ranjan *et al.*: [ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations](https://arxiv.org/abs/1911.07979) (AAAI 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/asap.py)\]\n- **[PANPooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.PANPooling.html)** from Ma *et al.*: [Path Integral Based Convolution and Pooling for Graph Neural Networks](https://arxiv.org/abs/2006.16811) (NeurIPS 2020)\n- **[MemPooling](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.MemPooling.html)** from Khasahmadi *et al.*: [Memory-Based Graph Networks](https://arxiv.org/abs/2002.09518) (ICLR 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/mem_pool.py)\]\n- **[Graph Multiset Transformer](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.GraphMultisetTransformer.html)** from Baek *et al.*: [Accurate Learning of Graph Representations with Graph Multiset Pooling](https://arxiv.org/abs/2102.11533) (ICLR 2021) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/proteins_gmt.py)\]\n- **[Equilibrium Aggregation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.aggr.EquilibriumAggregation.html)** from Bartunov *et al.*: [](https://arxiv.org/abs/2202.12795) (UAI 2022) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/equilibrium_median.py)\]\n\n</details>\n\n**GNN models:**\nOur supported GNN models incorporate multiple message passing layers, and users can directly use these pre-defined models to make predictions on graphs.\nUnlike simple stacking of GNN layers, these models could involve pre-processing, additional learnable parameters, skip connections, graph coarsening, etc.\n\n- **[SchNet](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.SchNet.html)** from SchÃ¼tt *et al.*: [SchNet: A Continuous-filter Convolutional Neural Network for Modeling Quantum Interactions](https://arxiv.org/abs/1706.08566) (NIPS 2017) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/qm9_pretrained_schnet.py)\]\n- **[DimeNet](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.DimeNet.html)** and **[DimeNetPlusPlus](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.DimeNetPlusPlus.html)** from Klicpera *et al.*: [Directional Message Passing for Molecular Graphs](https://arxiv.org/abs/2003.03123) (ICLR 2020) and [Fast and Uncertainty-Aware Directional Message Passing for Non-Equilibrium Molecules](https://arxiv.org/abs/2011.14115) (NeurIPS-W 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/qm9_pretrained_dimenet.py)\]\n- **[Node2Vec](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.Node2Vec.html)** from Grover and Leskovec: [node2vec: Scalable Feature Learning for Networks](https://arxiv.org/abs/1607.00653) (KDD 2016) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/node2vec.py)\]\n- **[Deep Graph Infomax](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.DeepGraphInfomax.html)** from VeliÄkoviÄ‡ *et al.*: [Deep Graph Infomax](https://arxiv.org/abs/1809.10341) (ICLR 2019) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/infomax_transductive.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/infomax_inductive.py)\]\n- **Deep Multiplex Graph Infomax** from Park *et al.*: [Unsupervised Attributed Multiplex Network Embedding](https://arxiv.org/abs/1911.06750) (AAAI 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/dmgi_unsup.py)\]\n- **[Masked Label Prediction](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MaskLabel.html)** from Shi *et al.*: [Masked Label Prediction: Unified Message Passing Model for Semi-Supervised Classification](https://arxiv.org/abs/2009.03509) (CoRR 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/unimp_arxiv.py)\]\n- **[PMLP](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.PMLP.html)** from Yang *et al.*: [Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs](https://arxiv.org/abs/2212.09034) (ICLR 2023)\n\n<details>\n<summary><b>Expand to see all implemented GNN models...</b></summary>\n\n- **[Jumping Knowledge](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.JumpingKnowledge.html)** from Xu *et al.*: [Representation Learning on Graphs with Jumping Knowledge Networks](https://arxiv.org/abs/1806.03536) (ICML 2018) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/benchmark/kernel/gin.py#L54-L106)\]\n- A **[MetaLayer](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MetaLayer.html)** for building any kind of graph network similar to the [TensorFlow Graph Nets library](https://github.com/deepmind/graph_nets) from Battaglia *et al.*: [Relational Inductive Biases, Deep Learning, and Graph Networks](https://arxiv.org/abs/1806.01261) (CoRR 2018)\n- **[MetaPath2Vec](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.MetaPath2Vec.html)** from Dong *et al.*: [metapath2vec: Scalable Representation Learning for Heterogeneous Networks](https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf) (KDD 2017) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/metapath2vec.py)\]\n- All variants of **[Graph Autoencoders](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GAE.html)** and **[Variational Autoencoders](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.VGAE.html)** from:\n  - [Variational Graph Auto-Encoders](https://arxiv.org/abs/1611.07308) from Kipf and Welling (NIPS-W 2016) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/autoencoder.py)\]\n  - [Adversarially Regularized Graph Autoencoder for Graph Embedding](https://arxiv.org/abs/1802.04407) from Pan *et al.* (IJCAI 2018) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/argva_node_clustering.py)\]\n  - [Simple and Effective Graph Autoencoders with One-Hop Linear Models](https://arxiv.org/abs/2001.07614) from Salha *et al.* (ECML 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/autoencoder.py)\]\n- **[SEAL](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/seal_link_pred.py)** from Zhang and Chen: [Link Prediction Based on Graph Neural Networks](https://arxiv.org/pdf/1802.09691.pdf) (NeurIPS 2018) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/seal_link_pred.py)\]\n- **[RENet](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.RENet.html)** from Jin *et al.*: [Recurrent Event Network for Reasoning over Temporal Knowledge Graphs](https://arxiv.org/abs/1904.05530) (ICLR-W 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/renet.py)\]\n- **[GraphUNet](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GraphUNet.html)** from Gao and Ji: [Graph U-Nets](https://arxiv.org/abs/1905.05178) (ICML 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_unet.py)\]\n- **[AttentiveFP](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.AttentiveFP.html)** from Xiong *et al.*: [Pushing the Boundaries of Molecular Representation for Drug Discovery with the Graph Attention Mechanism](https://pubs.acs.org/doi/10.1021/acs.jmedchem.9b00959) (J. Med. Chem. 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/attentive_fp.py)\]\n- **[DeepGCN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.DeepGCNLayer.html)** and the **[GENConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GENConv.html)** from Li *et al.*: [DeepGCNs: Can GCNs Go as Deep as CNNs?](https://arxiv.org/abs/1904.03751) (ICCV 2019) and [DeeperGCN: All You Need to Train Deeper GCNs](https://arxiv.org/abs/2006.07739) (CoRR 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/ogbn_proteins_deepgcn.py)\]\n- **[RECT](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.RECT_L.html)** from Wang *et al.*: [Network Embedding with Completely-imbalanced Labels](https://ieeexplore.ieee.org/document/8979355) (TKDE 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/rect.py)\]\n- **[GNNExplainer](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.explain.algorithm.GNNExplainer.html)** from Ying *et al.*: [GNNExplainer: Generating Explanations for Graph Neural Networks](https://arxiv.org/abs/1903.03894) (NeurIPS 2019) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/explain/gnn_explainer.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/explain/gnn_explainer_ba_shapes.py), [**Example3**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/explain/gnn_explainer_link_pred.py)\]\n- **Graph-less Neural Networks** from Zhang *et al.*: [Graph-less Neural Networks: Teaching Old MLPs New Tricks via Distillation](https://arxiv.org/abs/2110.08727) (CoRR 2021) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/glnn.py)\]\n- **[LINKX](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.LINKX.html)** from Lim *et al.*: [Large Scale Learning on Non-Homophilous Graphs:\n  New Benchmarks and Strong Simple Methods](https://arxiv.org/abs/2110.14446) (NeurIPS 2021) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/linkx.py)\]\n- **[RevGNN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GroupAddRev.html)** from Li *et al.*: [Training Graph Neural with 1000 Layers](https://arxiv.org/abs/2106.07476) (ICML 2021) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/rev_gnn.py)\]\n- **[TransE](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.kge.TransE.html)** from Bordes *et al.*: [Translating Embeddings for Modeling Multi-Relational Data](https://proceedings.neurips.cc/paper/2013/file/1cecc7a77928ca8133fa24680a88d2f9-Paper.pdf) (NIPS 2013) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/kge_fb15k_237.py)\]\n- **[ComplEx](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.kge.ComplEx.html)** from Trouillon *et al.*: [Complex Embeddings for Simple Link Prediction](https://arxiv.org/abs/1606.06357) (ICML 2016) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/kge_fb15k_237.py)\]\n- **[DistMult](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.kge.DistMult.html)** from Yang *et al.*: [Embedding Entities and Relations for Learning and Inference in Knowledge Bases](https://arxiv.org/abs/1412.6575) (ICLR 2015) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/kge_fb15k_237.py)\]\n- **[RotatE](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.kge.RotatE.html)** from Sun *et al.*: [RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space](https://arxiv.org/abs/1902.10197) (ICLR 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/kge_fb15k_237.py)\]\n\n</details>\n\n**GNN operators and utilities:**\nPyG comes with a rich set of neural network operators that are commonly used in many GNN models.\nThey follow an extensible design: It is easy to apply these operators and graph utilities to existing GNN layers and models to further enhance model performance.\n\n- **[DropEdge](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.dropout_edge)** from Rong *et al.*: [DropEdge: Towards Deep Graph Convolutional Networks on Node Classification](https://openreview.net/forum?id=Hkx1qkrKPr) (ICLR 2020)\n- **[DropNode](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.dropout_node)**, **[MaskFeature](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.mask_feature)** and **[AddRandomEdge](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.add_random_edge)** from You *et al.*: [Graph Contrastive Learning with Augmentations](https://arxiv.org/abs/2010.13902) (NeurIPS 2020)\n- **[DropPath](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.dropout_path)** from Li *et al.*: [MaskGAE: Masked Graph Modeling Meets Graph Autoencoders](https://arxiv.org/abs/2205.10053) (arXiv 2022)\n- **[ShuffleNode](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.shuffle_node)** from VeliÄkoviÄ‡ *et al.*: [Deep Graph Infomax](https://arxiv.org/abs/1809.10341) (ICLR 2019)\n- **[GraphNorm](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.norm.GraphNorm.html)** from Cai *et al.*: [GraphNorm: A Principled Approach to Accelerating Graph Neural Network Training](https://proceedings.mlr.press/v139/cai21e.html) (ICML 2021)\n- **[GDC](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.GDC.html)** from Klicpera *et al.*: [Diffusion Improves Graph Learning](https://arxiv.org/abs/1911.05485) (NeurIPS 2019) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/gcn.py)\]\n\n<details>\n<summary><b>Expand to see all implemented GNN operators and utilities...</b></summary>\n\n- **[GraphSizeNorm](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.norm.GraphSizeNorm.html)** from Dwivedi *et al.*: [Benchmarking Graph Neural Networks](https://arxiv.org/abs/2003.00982) (CoRR 2020)\n- **[PairNorm](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.norm.PairNorm.html)** from Zhao and Akoglu: [PairNorm: Tackling Oversmoothing in GNNs](https://arxiv.org/abs/1909.12223) (ICLR 2020)\n- **[MeanSubtractionNorm](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.norm.MeanSubtractionNorm.html)** from Yang *et al.*: [Revisiting "Over-smoothing" in Deep GCNs](https://arxiv.org/abs/2003.13663) (CoRR 2020)\n- **[DiffGroupNorm](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.norm.DiffGroupNorm.html)** from Zhou *et al.*: [Towards Deeper Graph Neural Networks with Differentiable Group Normalization](https://arxiv.org/abs/2006.06972) (NeurIPS 2020)\n- **[Tree Decomposition](https://pytorch-geometric.readthedocs.io/en/latest/modules/utils.html#torch_geometric.utils.tree_decomposition)** from Jin *et al.*: [Junction Tree Variational Autoencoder for Molecular Graph Generation](https://arxiv.org/abs/1802.04364) (ICML 2018)\n- **[TGN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.TGNMemory.html)** from Rossi *et al.*: [Temporal Graph Networks for Deep Learning on Dynamic Graphs](https://arxiv.org/abs/2006.10637) (GRL+ 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/tgn.py)\]\n- **[Weisfeiler Lehman Operator](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.WLConv.html)** from Weisfeiler and Lehman: [A Reduction of a Graph to a Canonical Form and an Algebra Arising During this Reduction](https://www.iti.zcu.cz/wl2018/pdf/wl_paper_translation.pdf) (Nauchno-Technicheskaya Informatsia 1968) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/wl_kernel.py)\]\n- **[Continuous Weisfeiler Lehman Operator](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.WLConvContinuous.html)** from Togninalli *et al.*: [Wasserstein Weisfeiler-Lehman Graph Kernels](https://arxiv.org/abs/1906.01277) (NeurIPS 2019)\n- **[Label Propagation](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.LabelPropagation.html)** from Zhu and Ghahramani: [Learning from Labeled and Unlabeled Data with Label Propagation](http://mlg.eng.cam.ac.uk/zoubin/papers/CMU-CALD-02-107.pdf) (CMU-CALD 2002) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/label_prop.py)\]\n- **[Local Degree Profile](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.transforms.LocalDegreeProfile)** from Cai and Wang: [A Simple yet Effective Baseline for Non-attribute Graph Classification](https://arxiv.org/abs/1811.03508) (CoRR 2018)\n- **[CorrectAndSmooth](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.CorrectAndSmooth.html)** from Huang *et al.*: [Combining Label Propagation And Simple Models Out-performs Graph Neural Networks](https://arxiv.org/abs/2010.13993) (CoRR 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/correct_and_smooth.py)\]\n- **[Gini](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.functional.gini.html)** and **[BRO](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.functional.bro.html)** regularization from Henderson *et al.*: [Improving Molecular Graph Neural Network Explainability with Orthonormalization and Induced Sparsity](https://arxiv.org/abs/2105.04854) (ICML 2021)\n- **[RootedEgoNets](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.transforms.RootedEgoNets)** and **[RootedRWSubgraph](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.transforms.RootedRWSubgraph)** from Zhao *et al.*: [From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness](https://arxiv.org/abs/2110.03753) (ICLR 2022)\n- **[FeaturePropagation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.transforms.FeaturePropagation)** from Rossi *et al.*: [On the Unreasonable Effectiveness of Feature Propagation in Learning on Graphs with Missing Node Features](https://arxiv.org/abs/2111.12128) (CoRR 2021)\n\n</details>\n\n**Scalable GNNs:**\nPyG supports the implementation of Graph Neural Networks that can scale to large-scale graphs.\nSuch application is challenging since the entire graph, its associated features and the GNN parameters cannot fit into GPU memory.\nMany state-of-the-art scalability approaches tackle this challenge by sampling neighborhoods for mini-batch training, graph clustering and partitioning, or by using simplified GNN models.\nThese approaches have been implemented in PyG, and can benefit from the above GNN layers, operators and models.\n\n- **[NeighborLoader](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.NeighborLoader)** from Hamilton *et al.*: [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216) (NIPS 2017) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/reddit.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/ogbn_train.py), [**Example3**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/to_hetero_mag.py)\]\n- **[ClusterGCN](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.ClusterLoader)** from Chiang *et al.*: [Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks](https://arxiv.org/abs/1905.07953) (KDD 2019) \[[**Example1**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/cluster_gcn_reddit.py), [**Example2**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/cluster_gcn_ppi.py)\]\n- **[GraphSAINT](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.GraphSAINTSampler)** from Zeng *et al.*: [GraphSAINT: Graph Sampling Based Inductive Learning Method](https://arxiv.org/abs/1907.04931) (ICLR 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/graph_saint.py)\]\n\n<details>\n<summary><b>Expand to see all implemented scalable GNNs...</b></summary>\n\n- **[ShaDow](https://pytorch-geometric.readthedocs.io/en/latest/modules/loader.html#torch_geometric.loader.ShaDowKHopSampler)** from Zeng *et al.*: [Decoupling the Depth and Scope of Graph Neural Networks](https://arxiv.org/abs/2201.07858) (NeurIPS 2021) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/shadow.py)\]\n- **[SIGN](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.transforms.SIGN.html)** from Rossi *et al.*: [SIGN: Scalable Inception Graph Neural Networks](https://arxiv.org/abs/2004.11198) (CoRR 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/sign.py)\]\n- **[HGTLoader](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.loader.HGTLoader.html)** from Hu *et al.*: [Heterogeneous Graph Transformer](https://arxiv.org/abs/2003.01332) (WWW 2020) \[[**Example**](https://github.com/pyg-team/pytorch_geometric/blob/master/examples/hetero/to_hetero_mag.py)\]\n\n</details>\n\n## Installation\n\nPyG is available for Python 3.10 to Python 3.13.\n\nFrom **PyG 2.3** onwards, you can install and use PyG **without any external library** required except for PyTorch.\nFor this, simply run\n\n```\npip install torch_geometric\n```\n\n### Additional Libraries\n\nIf you want to utilize the full set of features from PyG, there exists several additional libraries you may want to install:\n\n- **[`pyg-lib`](https://github.com/pyg-team/pyg-lib)**: Heterogeneous GNN operators and graph sampling routines\n- **[`torch-scatter`](https://github.com/rusty1s/pytorch_scatter)**: Accelerated and efficient sparse reductions\n- **[`torch-sparse`](https://github.com/rusty1s/pytorch_sparse)**: [`SparseTensor`](https://pytorch-geometric.readthedocs.io/en/latest/advanced/sparse_tensor.html) support\n- **[`torch-cluster`](https://github.com/rusty1s/pytorch_cluster)**: Graph clustering routines\n- **[`torch-spline-conv`](https://github.com/rusty1s/pytorch_spline_conv)**: [`SplineConv`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.SplineConv.html) support\n\nThese packages come with their own CPU and GPU kernel implementations based on the [PyTorch C++/CUDA/hip(ROCm) extension interface](https://github.com/pytorch/extension-cpp).\nFor a basic usage of PyG, these dependencies are **fully optional**.\nWe recommend to start with a minimal installation, and install additional dependencies once you start to actually need them.\n\nFor ease of installation of these extensions, we provide `pip` wheels for all major OS/PyTorch/CUDA combinations, see [here](https://data.pyg.org/whl).\n\n#### PyTorch 2.8\n\nTo install the binaries for PyTorch 2.8.0, simply run\n\n```\npip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.8.0+${CUDA}.html\n```\n\nwhere `${CUDA}` should be replaced by either `cpu`, `cu126`, `cu128`, or `cu129` depending on your PyTorch installation.\n\n|             | `cpu` | `cu126` | `cu128` | `cu129` |\n| ----------- | ----- | ------- | ------- | ------- |\n| **Linux**   | âœ…    | âœ…      | âœ…      | âœ…      |\n| **Windows** | âœ…    | âœ…      | âœ…      | âœ…      |\n| **macOS**   | âœ…    |         |         |         |\n\n#### PyTorch 2.7\n\nTo install the binaries for PyTorch 2.7.0, simply run\n\n```\npip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.7.0+${CUDA}.html\n```\n\nwhere `${CUDA}` should be replaced by either `cpu`, `cu118`, `cu126`, or `cu128` depending on your PyTorch installation.\n\n|             | `cpu` | `cu118` | `cu126` | `cu128` |\n| ----------- | ----- | ------- | ------- | ------- |\n| **Linux**   | âœ…    | âœ…      | âœ…      | âœ…      |\n| **Windows** | âœ…    | âœ…      | âœ…      | âœ…      |\n| **macOS**   | âœ…    |         |         |         |\n\n#### PyTorch 2.6\n\nTo install the binaries for PyTorch 2.6.0, simply run\n\n```\npip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+${CUDA}.html\n```\n\nwhere `${CUDA}` should be replaced by either `cpu`, `cu118`, `cu124`, or `cu126` depending on your PyTorch installation.\n\n|             | `cpu` | `cu118` | `cu124` | `cu126` |\n| ----------- | ----- | ------- | ------- | ------- |\n| **Linux**   | âœ…    | âœ…      | âœ…      | âœ…      |\n| **Windows** | âœ…    | âœ…      | âœ…      | âœ…      |\n| **macOS**   | âœ…    |         |         |         |\n\n**Note:** Binaries of older versions are also provided for PyTorch 1.4.0, PyTorch 1.5.0, PyTorch 1.6.0, PyTorch 1.7.0/1.7.1, PyTorch 1.8.0/1.8.1, PyTorch 1.9.0, PyTorch 1.10.0/1.10.1/1.10.2, PyTorch 1.11.0, PyTorch 1.12.0/1.12.1, PyTorch 1.13.0/1.13.1, PyTorch 2.0.0/2.0.1, PyTorch 2.1.0/2.1.1/2.1.2, PyTorch 2.2.0/2.2.1/2.2.2, PyTorch 2.3.0/2.3.1, PyTorch 2.4.0/2.4.1, and PyTorch 2.5.0/2.5.1 (following the same procedure).\n**For older versions, you might need to explicitly specify the latest supported version number** or install via `pip install --no-index` in order to prevent a manual installation from source.\nYou can look up the latest supported version number [here](https://data.pyg.org/whl).\n\n### NVIDIA PyG Container\n\nNVIDIA provides a PyG docker container for effortlessly training and deploying GPU accelerated GNNs with PyG, see [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pyg).\n\n### Nightly and Master\n\nIn case you want to experiment with the latest PyG features which are not fully released yet, either install the **nightly version** of PyG via\n\n```\npip install pyg-nightly\n```\n\nor install PyG **from master** via\n\n```\npip install git+https://github.com/pyg-team/pytorch_geometric.git\n```\n\n### ROCm Wheels\n\nThe external [`pyg-rocm-build` repository](https://github.com/Looong01/pyg-rocm-build) provides wheels and detailed instructions on how to install PyG for ROCm.\nIf you have any questions about it, please open an issue [here](https://github.com/Looong01/pyg-rocm-build/issues).\n\n## Cite\n\nPlease cite our [PyG 1.0](https://arxiv.org/abs/1903.02428) and [PyG 2.0](https://www.arxiv.org/abs/2507.16991) papers if you use this code in your own work:\n\n```\n@inproceedings{Fey/Lenssen/2019,\n  title={Fast Graph Representation Learning with {PyTorch Geometric}},\n  author={Fey, Matthias and Lenssen, Jan E.},\n  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},\n  year={2019},\n}\n\n@inproceedings{Fey/etal/2025,\n  title={{PyG} 2.0: Scalable Learning on Real World Graphs},\n  author={Fey, Matthias and Sunil, Jinu and Nitta, Akihiro and Puri, Rishi and Shah, Manan, and Stojanovi{\v{c}, Bla{\v{z} and Bendias, Ramona and Alexandria, Barghi and Kocijan, Vid and Zhang, Zecheng and He, Xinwei and Lenssen, Jan E. and Leskovec, Jure},\n  booktitle={Temporal Graph Learning Workshop @Â KDD},\n  year={2025},\n}\n```\n\nFeel free to [email us](mailto:matthias.fey@tu-dortmund.de) if you wish your work to be listed in the [external resources](https://pytorch-geometric.readthedocs.io/en/latest/external/resources.html).\nIf you notice anything unexpected, please open an [issue](https://github.com/pyg-team/pytorch_geometric/issues) and let us know.\nIf you have any questions or are missing a specific feature, feel free [to discuss them with us](https://github.com/pyg-team/pytorch_geometric/discussions).\nWe are motivated to constantly make PyG even better.\n\n[contributing-image]: https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat&color=4B26A4\n[contributing-url]: https://github.com/pyg-team/pytorch_geometric/blob/master/.github/CONTRIBUTING.md\n[pypi-download-image]: https://img.shields.io/pypi/dm/torch_geometric?color=4B26A4\n[pypi-download-url]: https://pepy.tech/projects/torch_geometric\n[pypi-image]: https://img.shields.io/pypi/pyversions/torch-geometric?color=4B26A4\n[pypi-url]: https://pypi.python.org/pypi/torch-geometric\n[slack-image]: https://img.shields.io/badge/slack-join-white.svg?logo=slack&color=4B26A4\n[slack-url]: https://data.pyg.org/slack.html\n', '{"language":"Python","stars":23243,"forks":3925,"watchers":23243,"open_issues":1242,"topics":["deep-learning","geometric-deep-learning","graph-convolutional-networks","graph-neural-networks","pytorch"],"default_branch":"master","size_kb":24632,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:snap-stanford:ogb","source_url":"https://github.com/snap-stanford/ogb"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pyg-lib","source_url":"https://github.com/pyg-team/pyg-lib"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:deepmind:graph_nets","source_url":"https://github.com/deepmind/graph_nets"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pyg-lib","source_url":"https://github.com/pyg-team/pyg-lib"},{"type":"has_code","target_id":"github:rusty1s:pytorch_scatter","source_url":"https://github.com/rusty1s/pytorch_scatter"},{"type":"has_code","target_id":"github:rusty1s:pytorch_sparse","source_url":"https://github.com/rusty1s/pytorch_sparse"},{"type":"has_code","target_id":"github:rusty1s:pytorch_cluster","source_url":"https://github.com/rusty1s/pytorch_cluster"},{"type":"has_code","target_id":"github:rusty1s:pytorch_spline_conv","source_url":"https://github.com/rusty1s/pytorch_spline_conv"},{"type":"has_code","target_id":"github:pytorch:extension-cpp","source_url":"https://github.com/pytorch/extension-cpp"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric.git","source_url":"https://github.com/pyg-team/pytorch_geometric.git"},{"type":"has_code","target_id":"github:Looong01:pyg-rocm-build","source_url":"https://github.com/Looong01/pyg-rocm-build"},{"type":"has_code","target_id":"github:Looong01:pyg-rocm-build","source_url":"https://github.com/Looong01/pyg-rocm-build"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"},{"type":"has_code","target_id":"github:pyg-team:pytorch_geometric","source_url":"https://github.com/pyg-team/pytorch_geometric"}]', NULL, 'MIT', 'approved', 80, '47e74c61b452909de05596692041ee34', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-pyg-team-pytorch-geometric from https://github.com/pyg-team.png
Image converted to WebP: data/images/github-pyg-team-pytorch-geometric.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Tencent-ncnn', 'github--tencent--ncnn', 'ncnn', 'Tencent', '!ncnn ncnn is a high-performance neural network inference computing framework optimized for mobile platforms. ncnn is deeply considerate about deployment and uses on mobile phones from the beginning of design. ncnn does not have third-party dependencies. It is cross-platform and runs faster than all known open-source frameworks on mobile phone cpu. Developers can easily deploy deep learning algorithm models to the mobile platform by using efficient ncnn implementation, creating intelligent AP...', '["android","arm-neon","artificial-intelligence","caffe","darknet","deep-learning","high-preformance","inference","ios","keras","mlir","mxnet","ncnn","neural-network","onnx","pytorch","riscv","simd","tensorflow","vulkan","c++"]', 'other', 22381, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Tencent/ncnn","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '![ncnn](https://raw.githubusercontent.com/Tencent/ncnn/master/images/256-ncnn.png)\n\n# ncnn\n\n[![License](https://img.shields.io/badge/license-BSD_3_Clause-blue.svg?style=for-the-badge)](LICENSE.txt)\n[![Download Total Count](https://img.shields.io/github/downloads/Tencent/ncnn/total.svg?style=for-the-badge)](https://github.com/Tencent/ncnn/releases)\n[![codecov](https://img.shields.io/codecov/c/github/Tencent/ncnn/master?style=for-the-badge)](https://codecov.io/gh/Tencent/ncnn)\n\nncnn is a high-performance neural network inference computing framework optimized for mobile platforms.\nncnn is deeply considerate about deployment and uses on mobile phones from the beginning of design.\nncnn does not have third-party dependencies.\nIt is cross-platform and runs faster than all known open-source frameworks on mobile phone cpu.\nDevelopers can easily deploy deep learning algorithm models to the mobile platform by using efficient ncnn implementation, creating intelligent APPs, and bringing artificial intelligence to your fingertips.\nncnn is currently being used in many Tencent applications, such as QQ, Qzone, WeChat, Pitu, and so on.\n\nncnn æ˜¯ä¸€ä¸ªä¸ºæ‰‹æœºç«¯æè‡´ä¼˜åŒ–çš„é«˜æ€§èƒ½ç¥ç»ç½‘ç»œå‰å‘è®¡ç®—æ¡†æ¶ã€‚\nncnn ä»è®¾è®¡ä¹‹åˆæ·±åˆ»è€ƒè™‘æ‰‹æœºç«¯çš„éƒ¨ç½²å’Œä½¿ç”¨ã€‚\næ— ç¬¬ä¸‰æ–¹ä¾èµ–ï¼Œè·¨å¹³å°ï¼Œæ‰‹æœºç«¯ cpu çš„é€Ÿåº¦å¿«äºç›®å‰æ‰€æœ‰å·²çŸ¥çš„å¼€æºæ¡†æ¶ã€‚\nåŸºäº ncnnï¼Œå¼€å‘è€…èƒ½å¤Ÿå°†æ·±åº¦å­¦ä¹ ç®—æ³•è½»æ¾ç§»æ¤åˆ°æ‰‹æœºç«¯é«˜æ•ˆæ‰§è¡Œï¼Œ\nå¼€å‘å‡ºäººå·¥æ™ºèƒ½ APPï¼Œå°† AI å¸¦åˆ°ä½ çš„æŒ‡å°–ã€‚\nncnn ç›®å‰å·²åœ¨è…¾è®¯å¤šæ¬¾åº”ç”¨ä¸­ä½¿ç”¨ï¼Œå¦‚ï¼šQQï¼ŒQzoneï¼Œå¾®ä¿¡ï¼Œå¤©å¤© P å›¾ç­‰ã€‚\n\n---\n\n<table>\n<tr>\n<td>\n<b>æŠ€æœ¯äº¤æµ QQ ç¾¤</b><br />\n637093648 (è¶…å¤šå¤§ä½¬)<br />\nç­”æ¡ˆï¼šå·å·å·å·å·ï¼ˆå·²æ»¡ï¼‰\n</td>\n<td rowspan=3>\n<b>Telegram Group</b>\n\n<https://t.me/ncnnyes>\n</td>\n<td rowspan=3>\n<b>Discord Channel</b>\n\n<https://discord.gg/YRsxgmF>\n</td>\n</tr>\n<tr>\n<td>\n<b>Pocky QQ ç¾¤ï¼ˆMLIR YES!ï¼‰</b><br />\n677104663 (è¶…å¤šå¤§ä½¬)<br />\nç­”æ¡ˆï¼šmulti-level intermediate representation\n</td>\n</tr>\n<tr>\n<td>\n<b>ä»–ä»¬éƒ½ä¸çŸ¥é“ pnnx æœ‰å¤šå¥½ç”¨ç¾¤</b><br />\n818998520 (æ–°ç¾¤ï¼)\n</td>\n</tr>\n</table>\n\n---\n\n## Download & Build status\n\nhttps://github.com/Tencent/ncnn/releases/latest\n\n\n<table>\n<tr>\n<td rowspan=2>\n  <img src="https://user-images.githubusercontent.com/25181517/192108372-f71d70ac-7ae6-4c0d-8395-51d8870c2ef0.png" width="120" height="auto">\n</td>\n<td colspan=3>\n\n  **[how to build ncnn library](https://github.com/Tencent/ncnn/wiki/how-to-build) on Linux / Windows / macOS / Raspberry Pi3, Pi4 / POWER / Android / NVIDIA Jetson / iOS / WebAssembly / AllWinner D1 / Loongson 2K1000**\n\n</td>\n</tr>\n<tr>\n<td>Source</td>\n<td colspan=2>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-full-source.zip)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=3>\n  <img src="https://user-images.githubusercontent.com/25181517/117269608-b7dcfb80-ae58-11eb-8e66-6cc8753553f0.png" width="120" height="auto">\n</td>\n<td colspan=3>\n\n- [Build for Android](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-android)\n- [Build for Termux on Android](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-termux-on-android)\n\n</td>\n</tr>\n<tr>\n<td>Android</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-android-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-android.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/android.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Aandroid)\n\n</td>\n</tr>\n<tr>\n<td>Android shared</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-android-vulkan-shared.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-android-shared.zip)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=3>\n  <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/HMOS_Logo_Icon.svg/240px-HMOS_Logo_Icon.svg.png" width="120" height="auto">\n</td>\n<td colspan=3>\n\n- [Build for HarmonyOS with cross-compiling](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-harmonyos-with-cross-compiling)\n\n</td>\n</tr>\n<tr>\n<td>HarmonyOS</td>\n<td>\n\n</td>\n<td rowspan=2>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/harmonyos.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Aharmonyos)\n\n</td>\n</tr>\n<tr>\n<td>HarmonyOS shared</td>\n<td>\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=3>\n  <img src="https://user-images.githubusercontent.com/25181517/121406611-a8246b80-c95e-11eb-9b11-b771486377f6.png" width="120" height="auto">\n</td>\n<td colspan=3>\n\n- [Build for iOS on macOS with xcode](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-ios-on-macos-with-xcode)\n\n</td>\n</tr>\n<tr>\n<td>iOS</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ios-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ios.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/ios.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Aios)\n\n</td>\n</tr>\n<tr>\n<td>iOS-Simulator</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ios-simulator-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ios-simulator.zip)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=10>\n  <img src="https://user-images.githubusercontent.com/25181517/186884152-ae609cca-8cf1-4175-8d60-1ce1fa078ca2.png" width="120" height="auto">\n</td>\n<td colspan=3>\n\n- [Build for macOS](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-macos)\n\n</td>\n</tr>\n<tr>\n<td>macOS</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-macos-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-macos.zip)\n\n</td>\n<td rowspan=1>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/macos.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Amacos)\n\n</td>\n</tr>\n<tr>\n<td>Mac-Catalyst</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-mac-catalyst-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-mac-catalyst.zip)\n\n</td>\n<td rowspan=1>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/mac-catalyst.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Amac-catalyst)\n\n</td>\n</tr>\n<tr>\n<td>watchOS</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-watchos.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/watchos.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Awatchos)\n\n</td>\n</tr>\n<tr>\n<td>watchOS-Simulator</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-watchos-simulator.zip)\n\n</td>\n</tr>\n<tr>\n<td>tvOS</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-tvos-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-tvos.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/tvos.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Atvos)\n\n</td>\n</tr>\n<tr>\n<td>tvOS-Simulator</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-tvos-simulator-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-tvos-simulator.zip)\n\n</td>\n</tr>\n<tr>\n<td>visionOS</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-visionos-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-visionos.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/visionos.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Avisionos)\n\n</td>\n</tr>\n<tr>\n<td>visionOS-Simulator</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-visionos-simulator-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-visionos-simulator.zip)\n\n</td>\n</tr>\n<tr>\n<td>Apple xcframework</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-apple-vulkan.zip)\n  [<img src="https://img.shields.io/badge/+cpuonly-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-apple.zip)\n\n</td>\n<td rowspan=1>\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=3>\n  <img src="https://user-images.githubusercontent.com/25181517/186884153-99edc188-e4aa-4c84-91b0-e2df260ebc33.png" width="120" height="auto">\n</td>\n<td colspan=3>\n\n- [Build for Linux / NVIDIA Jetson / Raspberry Pi3, Pi4 / POWER](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-linux)\n\n</td>\n</tr>\n<tr>\n<td>Ubuntu 22.04</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ubuntu-2204.zip)\n  [<img src="https://img.shields.io/badge/+shared-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ubuntu-2204-shared.zip)\n\n</td>\n<td rowspan=2>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-x64-gpu-gcc.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-x64-gpu-gcc)\n\n</td>\n</tr>\n<tr>\n<td>Ubuntu 24.04</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ubuntu-2404.zip)\n  [<img src="https://img.shields.io/badge/+shared-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-ubuntu-2404-shared.zip)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=5>\n  <img alt="windows" src="https://user-images.githubusercontent.com/25181517/186884150-05e9ff6d-340e-4802-9533-2c3f02363ee3.png" width="120" height="auto">\n</td>\n<td colspan=3>\n\n- [Build for Windows x64 using VS2017](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-windows-x64-using-visual-studio-community-2017)\n- [Build for Windows x64 using MinGW-w64](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-windows-x64-using-mingw-w64)\n\n</td>\n</tr>\n<tr>\n<td>VS2015</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2015.zip)\n  [<img src="https://img.shields.io/badge/+shared-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2015-shared.zip)\n\n</td>\n<td rowspan=4>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/windows.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Awindows)\n\n</td>\n</tr>\n<tr>\n<td>VS2017</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2017.zip)\n  [<img src="https://img.shields.io/badge/+shared-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2017-shared.zip)\n\n</td>\n</tr>\n<tr>\n<td>VS2019</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2019.zip)\n  [<img src="https://img.shields.io/badge/+shared-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2019-shared.zip)\n\n</td>\n</tr>\n<tr>\n<td>VS2022</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2022.zip)\n  [<img src="https://img.shields.io/badge/+shared-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-windows-vs2022-shared.zip)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=2>\n  <img src="https://user-images.githubusercontent.com/25181517/188324036-d704ac9a-6e61-4722-b978-254b25b61bed.png" width="120" height="auto">\n</td>\n<td colspan=3>\n\n- [Build for WebAssembly](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-webassembly)\n\n</td>\n</tr>\n<tr>\n<td>WebAssembly</td>\n<td>\n\n  [<img src="https://img.shields.io/badge/download-blue?style=for-the-badge">](https://github.com/Tencent/ncnn/releases/latest/download/ncnn-20250503-webassembly.zip)\n\n</td>\n<td>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/web-assembly.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Aweb-assembly)\n\n</td>\n</tr>\n\n<tr>\n<td rowspan=8>\n  <img src="https://github.com/marwin1991/profile-technology-icons/assets/76662862/2481dc48-be6b-4ebb-9e8c-3b957efe69fa" width="120" height="auto">\n</td>\n<td colspan=3>\n\n- [Build for ARM Cortex-A family with cross-compiling](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-arm-cortex-a-family-with-cross-compiling)\n- [Build for Hisilicon platform with cross-compiling](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-hisilicon-platform-with-cross-compiling)\n- [Build for AllWinner D1](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-allwinner-d1)\n- [Build for Loongson 2K1000](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-loongson-2k1000)\n- [Build for QNX](https://github.com/Tencent/ncnn/wiki/how-to-build#build-for-qnx)\n\n</td>\n</tr>\n<tr>\n<td>Linux (arm)</td>\n<td></td>\n<td>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-arm.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-arm)\n\n</td>\n</tr>\n<tr>\n<td>Linux (aarch64)</td>\n<td></td>\n<td>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-aarch64.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-aarch64)\n\n</td>\n</tr>\n<tr>\n<td>Linux (mips)</td>\n<td></td>\n<td>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-mips.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-mips)\n\n</td>\n</tr>\n<tr>\n<td>Linux (mips64)</td>\n<td></td>\n<td>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-mips64.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-mips64)\n\n</td>\n</tr>\n<tr>\n<td>Linux (ppc64)</td>\n<td></td>\n<td>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-ppc64.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-ppc64)\n\n</td>\n</tr>\n<tr>\n<td>Linux (riscv64)</td>\n<td></td>\n<td>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-riscv64.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-riscv64)\n\n</td>\n</tr>\n<tr>\n<td>Linux (loongarch64)</td>\n<td></td>\n<td>\n\n  [<img src="https://img.shields.io/github/actions/workflow/status/Tencent/ncnn/linux-loongarch64.yml?branch=master&style=for-the-badge&label=build">](https://github.com/Tencent/ncnn/actions?query=workflow%3Alinux-loongarch64)\n\n</td>\n</tr>\n\n</table>\n\n\n---\n\n## Support most commonly used CNN network\n\n## æ”¯æŒå¤§éƒ¨åˆ†å¸¸ç”¨çš„ CNN ç½‘ç»œ\n\n- Classical CNN:\n  [VGG](https://github.com/BVLC/caffe/wiki/Model-Zoo#models-used-by-the-vgg-team-in-ilsvrc-2014)\n  [AlexNet](https://github.com/BVLC/caffe/tree/9b891540183ddc834a02b2bd81b31afae71b2153/models/bvlc_alexnet)\n  [GoogleNet](https://github.com/BVLC/caffe/tree/9b891540183ddc834a02b2bd81b31afae71b2153/models/bvlc_googlenet)\n  Inception\n  ...\n- Practical CNN:\n  [ResNet](https://github.com/tornadomeet/ResNet)\n  [DenseNet](https://github.com/liuzhuang13/DenseNet)\n  [SENet](https://github.com/hujie-frank/SENet)\n  [FPN](https://github.com/unsky/FPN)\n  ...\n- Light-weight CNN:\n  [SqueezeNet](https://github.com/forresti/SqueezeNet)\n  [MobileNetV1](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md)\n  [MobileNetV2/V3](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md)\n  [ShuffleNetV1](https://github.com/farmingyard/ShuffleNet)\n  [ShuffleNetV2](https://github.com/opconty/keras-shufflenetV2)\n  [MNasNet](https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet)\n  ...\n- Face Detection:\n  [MTCNN](https://github.com/ipazc/mtcnn)\n  [RetinaFace](https://github.com/biubug6/Pytorch_Retinaface)\n  [scrfd](https://github.com/nihui/ncnn-android-scrfd)\n  ...\n- Detection:\n  [VGG-SSD](https://github.com/lzx1413/CAFFE_SSD)\n  [MobileNet-SSD](https://github.com/chuanqi305/MobileNet-SSD)\n  [SqueezeNet-SSD](https://github.com/chuanqi305/SqueezeNet-SSD)\n  [MobileNetV2-SSDLite](https://github.com/chuanqi305/MobileNetv2-SSDLite)\n  [MobileNetV3-SSDLite](https://github.com/XiaoyuHuang96/MobilenetV3SSDLite-tfkeras)\n  ...\n- Detection:\n  [Faster-RCNN](https://github.com/rbgirshick/py-faster-rcnn)\n  [R-FCN](https://github.com/daijifeng001/R-FCN)\n  ...\n- Detection:\n  [YOLOv2](https://github.com/longcw/yolo2-pytorch)\n  [YOLOv3](https://github.com/ultralytics/yolov3)\n  [MobileNet-YOLOv3](https://github.com/eric612/MobileNet-YOLO)\n  [YOLOv4](https://github.com/Tianxiaomo/pytorch-YOLOv4)\n  [YOLOv5](https://github.com/ultralytics/yolov5)\n  [YOLOv7](https://github.com/WongKinYiu/yolov7)\n  [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)\n  [YOLOv8](https://github.com/nihui/ncnn-android-yolov8)\n  ...\n- Detection:\n  [NanoDet](https://github.com/RangiLyu/nanodet)\n- Segmentation:\n  [FCN](https://github.com/unsky/FPN)\n  [PSPNet](https://github.com/hszhao/PSPNet)\n  [UNet](https://github.com/zhixuhao/unet)\n  [YOLACT](https://github.com/dbolya/yolact)\n  ...\n- Pose Estimation:\n  [SimplePose](https://github.com/dog-qiuqiu/Ultralight-SimplePose)\n  ...\n\n---\n\n## HowTo\n\n**[use ncnn with alexnet](https://github.com/Tencent/ncnn/wiki/use-ncnn-with-alexnet) with detailed steps, recommended for beginners :)**\n\n**[ncnn ç»„ä»¶ä½¿ç”¨æŒ‡åŒ— alexnet](https://github.com/Tencent/ncnn/wiki/use-ncnn-with-alexnet.zh) é™„å¸¦è¯¦ç»†æ­¥éª¤ï¼Œæ–°äººå¼ºçƒˆæ¨è :)**\n\n**[use netron for ncnn model visualization](https://netron.app)**\n\n**[use ncnn with pytorch or onnx](https://github.com/Tencent/ncnn/wiki/use-ncnn-with-pytorch-or-onnx)**\n\n[ncnn low-level operation api](https://github.com/Tencent/ncnn/wiki/low-level-operation-api)\n\n[ncnn param and model file spec](https://github.com/Tencent/ncnn/wiki/param-and-model-file-structure)\n\n[ncnn operation param weight table](https://github.com/Tencent/ncnn/wiki/operation-param-weight-table)\n\n[how to implement custom layer step by step](https://github.com/Tencent/ncnn/wiki/how-to-implement-custom-layer-step-by-step)\n\n---\n\n## FAQ\n\n**[ncnn deepwiki](https://deepwiki.com/Tencent/ncnn) LLM Answering Questions ;)** \n\n**[ncnn throw error](https://github.com/Tencent/ncnn/wiki/FAQ-ncnn-throw-error)**\n\n**[ncnn produce wrong result](https://github.com/Tencent/ncnn/wiki/FAQ-ncnn-produce-wrong-result)**\n\n**[ncnn vulkan](https://github.com/Tencent/ncnn/wiki/FAQ-ncnn-vulkan)**\n\n---\n\n## Features\n\n- Supports convolutional neural networks, supports multiple input and multi-branch structure, can calculate part of the branch\n- No third-party library dependencies, does not rely on BLAS / NNPACK or any other computing framework\n- Pure C++ implementation, cross-platform, supports Android, iOS and so on\n- ARM NEON assembly level of careful optimization, calculation speed is extremely high\n- Sophisticated memory management and data structure design, very low memory footprint\n- Supports multi-core parallel computing acceleration, ARM big.LITTLE CPU scheduling optimization\n- Supports GPU acceleration via the next-generation low-overhead Vulkan API\n- Extensible model design, supports 8bit quantization and half-precision floating point storage, can import caffe/pytorch/mxnet/onnx/darknet/keras/tensorflow(mlir) models\n- Support direct memory zero copy reference load network model\n- Can be registered with custom layer implementation and extended\n- Well, it is strong, not afraid of being stuffed with å· QvQ\n\n## åŠŸèƒ½æ¦‚è¿°\n\n- æ”¯æŒå·ç§¯ç¥ç»ç½‘ç»œï¼Œæ”¯æŒå¤šè¾“å…¥å’Œå¤šåˆ†æ”¯ç»“æ„ï¼Œå¯è®¡ç®—éƒ¨åˆ†åˆ†æ”¯\n- æ— ä»»ä½•ç¬¬ä¸‰æ–¹åº“ä¾èµ–ï¼Œä¸ä¾èµ– BLAS/NNPACK ç­‰è®¡ç®—æ¡†æ¶\n- çº¯ C++ å®ç°ï¼Œè·¨å¹³å°ï¼Œæ”¯æŒ Android / iOS ç­‰\n- ARM Neon æ±‡ç¼–çº§è‰¯å¿ƒä¼˜åŒ–ï¼Œè®¡ç®—é€Ÿåº¦æå¿«\n- ç²¾ç»†çš„å†…å­˜ç®¡ç†å’Œæ•°æ®ç»“æ„è®¾è®¡ï¼Œå†…å­˜å ç”¨æä½\n- æ”¯æŒå¤šæ ¸å¹¶è¡Œè®¡ç®—åŠ é€Ÿï¼ŒARM big.LITTLE CPU è°ƒåº¦ä¼˜åŒ–\n- æ”¯æŒåŸºäºå…¨æ–°ä½æ¶ˆè€—çš„ Vulkan API GPU åŠ é€Ÿ\n- å¯æ‰©å±•çš„æ¨¡å‹è®¾è®¡ï¼Œæ”¯æŒ 8bit [é‡åŒ–](tools/quantize) å’ŒåŠç²¾åº¦æµ®ç‚¹å­˜å‚¨ï¼Œå¯å¯¼å…¥ caffe/pytorch/mxnet/onnx/darknet/keras/tensorflow(mlir) æ¨¡å‹\n- æ”¯æŒç›´æ¥å†…å­˜é›¶æ‹·è´å¼•ç”¨åŠ è½½ç½‘ç»œæ¨¡å‹\n- å¯æ³¨å†Œè‡ªå®šä¹‰å±‚å®ç°å¹¶æ‰©å±•\n- æ©ï¼Œå¾ˆå¼ºå°±æ˜¯äº†ï¼Œä¸æ€•è¢«å¡å· QvQ\n\n---\n\n## supported platform matrix\n\n- âœ… = known work and runs fast with good optimization\n- âœ”ï¸ = known work, but speed may not be fast enough\n- â” = shall work, not confirmed\n- / = not applied\n\n|            | Windows | Linux | Android | macOS | iOS |\n| ---------- | ------- | ----- | ------- | ----- | --- |\n| intel-cpu  | âœ”ï¸      | âœ”ï¸    | âœ”ï¸      | âœ”ï¸    | /   |\n| intel-gpu  | âœ”ï¸      | âœ”ï¸    | âœ”ï¸      | âœ”ï¸    | /   |\n| amd-cpu    | âœ”ï¸      | âœ”ï¸    | âœ”ï¸      | âœ”ï¸    | /   |\n| amd-gpu    | âœ”ï¸      | âœ”ï¸    | âœ”ï¸      | âœ”ï¸    | /   |\n| nvidia-gpu | âœ”ï¸      | âœ”ï¸    | âœ”ï¸      | âœ”ï¸    | /   |\n| qcom-cpu   | âœ…      | âœ…    | âœ…      | /     | /   |\n| qcom-gpu   | âœ”ï¸      | âœ”ï¸    | âœ”ï¸      | /     | /   |\n| arm-cpu    | âœ…      | âœ…    | âœ…      | /     | /   |\n| arm-gpu    | â”      | âœ”ï¸    | âœ”ï¸      | /     | /   |\n| apple-cpu  | /       | /     | /       | âœ”ï¸    | âœ…  |\n| apple-gpu  | /       | /     | /       | âœ”ï¸    | âœ”ï¸  |\n| ibm-cpu    | /       | âœ”ï¸     | /       | /    | /  |\n\n---\n\n## Project examples\n\n- <https://github.com/nihui/ncnn-android-squeezenet>\n- <https://github.com/nihui/ncnn-android-styletransfer>\n- <https://github.com/nihui/ncnn-android-mobilenetssd>\n- <https://github.com/moli232777144/mtcnn_ncnn>\n- <https://github.com/nihui/ncnn-android-yolov5>\n- <https://github.com/xiang-wuu/ncnn-android-yolov7>\n- <https://github.com/nihui/ncnn-android-scrfd> ğŸ¤©\n- <https://github.com/shaoshengsong/qt_android_ncnn_lib_encrypt_example>\n\n<img src="https://github.com/nihui/ncnn-assets/raw/master/20181217/ncnn-2.jpg" height ="230"/><img src="https://github.com/nihui/ncnn-assets/raw/master/20181217/4.jpg" height ="230"/><img src="https://github.com/nihui/ncnn-assets/raw/master/20181217/ncnn-33.jpg" height ="230"/><img src="https://github.com/nihui/ncnn-assets/raw/master/20181217/ncnn-m.png" height ="230"/><img src="https://github.com/nihui/ncnn-android-yolov5/raw/master/screenshot.jpg" height ="230"/><img src="https://github.com/nihui/ncnn-android-scrfd/raw/master/screenshot.jpg" height ="230"/><br>\n\n- <https://github.com/magicse/ncnn-colorization-siggraph17><br>\n<img src="https://user-images.githubusercontent.com/13585785/189326958-f5a8d6f8-caef-49bf-88da-ae494371195d.jpg" width ="700"/>\n\n- <https://github.com/mizu-bai/ncnn-fortran> Call ncnn from Fortran\n\n- <https://github.com/k2-fsa/sherpa> Use ncnn for real-time speech\n  recognition (i.e., speech-to-text); also support embedded devices and provide\n  mobile Apps (e.g., Android App)\n\n---\n\n## License\n\n[BSD 3 Clause](LICENSE.txt)\n', '{"language":"C++","stars":22381,"forks":4355,"watchers":22381,"open_issues":1143,"topics":["android","arm-neon","artificial-intelligence","caffe","darknet","deep-learning","high-preformance","inference","ios","keras","mlir","mxnet","ncnn","neural-network","onnx","pytorch","riscv","simd","tensorflow","vulkan"],"default_branch":"master","size_kb":33028,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:marwin1991:profile-technology-icons","source_url":"https://github.com/marwin1991/profile-technology-icons"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"},{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"},{"type":"has_code","target_id":"github:BVLC:caffe","source_url":"https://github.com/BVLC/caffe"},{"type":"has_code","target_id":"github:tornadomeet:ResNet","source_url":"https://github.com/tornadomeet/ResNet"},{"type":"has_code","target_id":"github:liuzhuang13:DenseNet","source_url":"https://github.com/liuzhuang13/DenseNet"},{"type":"has_code","target_id":"github:hujie-frank:SENet","source_url":"https://github.com/hujie-frank/SENet"},{"type":"has_code","target_id":"github:unsky:FPN","source_url":"https://github.com/unsky/FPN"},{"type":"has_code","target_id":"github:forresti:SqueezeNet","source_url":"https://github.com/forresti/SqueezeNet"},{"type":"has_code","target_id":"github:tensorflow:models","source_url":"https://github.com/tensorflow/models"},{"type":"has_code","target_id":"github:tensorflow:models","source_url":"https://github.com/tensorflow/models"},{"type":"has_code","target_id":"github:farmingyard:ShuffleNet","source_url":"https://github.com/farmingyard/ShuffleNet"},{"type":"has_code","target_id":"github:opconty:keras-shufflenetV2","source_url":"https://github.com/opconty/keras-shufflenetV2"},{"type":"has_code","target_id":"github:tensorflow:models","source_url":"https://github.com/tensorflow/models"},{"type":"has_code","target_id":"github:ipazc:mtcnn","source_url":"https://github.com/ipazc/mtcnn"},{"type":"has_code","target_id":"github:biubug6:Pytorch_Retinaface","source_url":"https://github.com/biubug6/Pytorch_Retinaface"},{"type":"has_code","target_id":"github:nihui:ncnn-android-scrfd","source_url":"https://github.com/nihui/ncnn-android-scrfd"},{"type":"has_code","target_id":"github:lzx1413:CAFFE_SSD","source_url":"https://github.com/lzx1413/CAFFE_SSD"},{"type":"has_code","target_id":"github:chuanqi305:MobileNet-SSD","source_url":"https://github.com/chuanqi305/MobileNet-SSD"},{"type":"has_code","target_id":"github:chuanqi305:SqueezeNet-SSD","source_url":"https://github.com/chuanqi305/SqueezeNet-SSD"},{"type":"has_code","target_id":"github:chuanqi305:MobileNetv2-SSDLite","source_url":"https://github.com/chuanqi305/MobileNetv2-SSDLite"},{"type":"has_code","target_id":"github:XiaoyuHuang96:MobilenetV3SSDLite-tfkeras","source_url":"https://github.com/XiaoyuHuang96/MobilenetV3SSDLite-tfkeras"},{"type":"has_code","target_id":"github:rbgirshick:py-faster-rcnn","source_url":"https://github.com/rbgirshick/py-faster-rcnn"},{"type":"has_code","target_id":"github:daijifeng001:R-FCN","source_url":"https://github.com/daijifeng001/R-FCN"},{"type":"has_code","target_id":"github:longcw:yolo2-pytorch","source_url":"https://github.com/longcw/yolo2-pytorch"},{"type":"has_code","target_id":"github:ultralytics:yolov3","source_url":"https://github.com/ultralytics/yolov3"},{"type":"has_code","target_id":"github:eric612:MobileNet-YOLO","source_url":"https://github.com/eric612/MobileNet-YOLO"},{"type":"has_code","target_id":"github:Tianxiaomo:pytorch-YOLOv4","source_url":"https://github.com/Tianxiaomo/pytorch-YOLOv4"},{"type":"has_code","target_id":"github:ultralytics:yolov5","source_url":"https://github.com/ultralytics/yolov5"},{"type":"has_code","target_id":"github:WongKinYiu:yolov7","source_url":"https://github.com/WongKinYiu/yolov7"},{"type":"has_code","target_id":"github:Megvii-BaseDetection:YOLOX","source_url":"https://github.com/Megvii-BaseDetection/YOLOX"},{"type":"has_code","target_id":"github:nihui:ncnn-android-yolov8","source_url":"https://github.com/nihui/ncnn-android-yolov8"},{"type":"has_code","target_id":"github:RangiLyu:nanodet","source_url":"https://github.com/RangiLyu/nanodet"},{"type":"has_code","target_id":"github:unsky:FPN","source_url":"https://github.com/unsky/FPN"},{"type":"has_code","target_id":"github:hszhao:PSPNet","source_url":"https://github.com/hszhao/PSPNet"},{"type":"has_code","target_id":"github:zhixuhao:unet","source_url":"https://github.com/zhixuhao/unet"},{"type":"has_code","target_id":"github:dbolya:yolact","source_url":"https://github.com/dbolya/yolact"},{"type":"has_code","target_id":"github:dog-qiuqiu:Ultralight-SimplePose","source_url":"https://github.com/dog-qiuqiu/Ultralight-SimplePose"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:Tencent:ncnn","source_url":"https://github.com/Tencent/ncnn"},{"type":"has_code","target_id":"github:nihui:ncnn-android-squeezenet>","source_url":"https://github.com/nihui/ncnn-android-squeezenet>"},{"type":"has_code","target_id":"github:nihui:ncnn-android-styletransfer>","source_url":"https://github.com/nihui/ncnn-android-styletransfer>"},{"type":"has_code","target_id":"github:nihui:ncnn-android-mobilenetssd>","source_url":"https://github.com/nihui/ncnn-android-mobilenetssd>"},{"type":"has_code","target_id":"github:moli232777144:mtcnn_ncnn>","source_url":"https://github.com/moli232777144/mtcnn_ncnn>"},{"type":"has_code","target_id":"github:nihui:ncnn-android-yolov5>","source_url":"https://github.com/nihui/ncnn-android-yolov5>"},{"type":"has_code","target_id":"github:xiang-wuu:ncnn-android-yolov7>","source_url":"https://github.com/xiang-wuu/ncnn-android-yolov7>"},{"type":"has_code","target_id":"github:nihui:ncnn-android-scrfd>","source_url":"https://github.com/nihui/ncnn-android-scrfd>"},{"type":"has_code","target_id":"github:shaoshengsong:qt_android_ncnn_lib_encrypt_example>","source_url":"https://github.com/shaoshengsong/qt_android_ncnn_lib_encrypt_example>"},{"type":"has_code","target_id":"github:nihui:ncnn-assets","source_url":"https://github.com/nihui/ncnn-assets"},{"type":"has_code","target_id":"github:nihui:ncnn-assets","source_url":"https://github.com/nihui/ncnn-assets"},{"type":"has_code","target_id":"github:nihui:ncnn-assets","source_url":"https://github.com/nihui/ncnn-assets"},{"type":"has_code","target_id":"github:nihui:ncnn-assets","source_url":"https://github.com/nihui/ncnn-assets"},{"type":"has_code","target_id":"github:nihui:ncnn-android-yolov5","source_url":"https://github.com/nihui/ncnn-android-yolov5"},{"type":"has_code","target_id":"github:nihui:ncnn-android-scrfd","source_url":"https://github.com/nihui/ncnn-android-scrfd"},{"type":"has_code","target_id":"github:magicse:ncnn-colorization-siggraph17><br>","source_url":"https://github.com/magicse/ncnn-colorization-siggraph17><br>"},{"type":"has_code","target_id":"github:mizu-bai:ncnn-fortran>","source_url":"https://github.com/mizu-bai/ncnn-fortran>"},{"type":"has_code","target_id":"github:k2-fsa:sherpa>","source_url":"https://github.com/k2-fsa/sherpa>"}]', NULL, 'NOASSERTION', 'approved', 80, '9e2a1f04c32e99099ce2adfa6a63cb1c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Tencent-ncnn from https://github.com/Tencent.png
Image converted to WebP: data/images/github-Tencent-ncnn.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-SYSTRAN-faster-whisper', 'github--systran--faster-whisper', 'faster-whisper', 'SYSTRAN', '**faster-whisper** is a reimplementation of OpenAI''s Whisper model using CTranslate2, which is a fast inference engine for Transformer models. This implementation is up to 4 times faster than openai/whisper for the same accuracy while using less memory. The efficiency can be further improved with 8-bit quantization on both CPU and GPU. For reference, here''s the time and memory usage that are required to transcribe **13 minutes** of audio using different implementations: * openai/whisper@v2024...', '["deep-learning","inference","openai","quantization","speech-recognition","speech-to-text","transformer","whisper","python"]', 'other', 19366, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/SYSTRAN/faster-whisper","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '[![CI](https://github.com/SYSTRAN/faster-whisper/workflows/CI/badge.svg)](https://github.com/SYSTRAN/faster-whisper/actions?query=workflow%3ACI) [![PyPI version](https://badge.fury.io/py/faster-whisper.svg)](https://badge.fury.io/py/faster-whisper)\n\n# Faster Whisper transcription with CTranslate2\n\n**faster-whisper** is a reimplementation of OpenAI''s Whisper model using [CTranslate2](https://github.com/OpenNMT/CTranslate2/), which is a fast inference engine for Transformer models.\n\nThis implementation is up to 4 times faster than [openai/whisper](https://github.com/openai/whisper) for the same accuracy while using less memory. The efficiency can be further improved with 8-bit quantization on both CPU and GPU.\n\n## Benchmark\n\n### Whisper\n\nFor reference, here''s the time and memory usage that are required to transcribe [**13 minutes**](https://www.youtube.com/watch?v=0u7tTptBo9I) of audio using different implementations:\n\n* [openai/whisper](https://github.com/openai/whisper)@[v20240930](https://github.com/openai/whisper/tree/v20240930)\n* [whisper.cpp](https://github.com/ggerganov/whisper.cpp)@[v1.7.2](https://github.com/ggerganov/whisper.cpp/tree/v1.7.2)\n* [transformers](https://github.com/huggingface/transformers)@[v4.46.3](https://github.com/huggingface/transformers/tree/v4.46.3)\n* [faster-whisper](https://github.com/SYSTRAN/faster-whisper)@[v1.1.0](https://github.com/SYSTRAN/faster-whisper/tree/v1.1.0)\n\n### Large-v2 model on GPU\n\n| Implementation | Precision | Beam size | Time | VRAM Usage |\n| --- | --- | --- | --- | --- |\n| openai/whisper | fp16 | 5 | 2m23s | 4708MB |\n| whisper.cpp (Flash Attention) | fp16 | 5 | 1m05s | 4127MB |\n| transformers (SDPA)[^1] | fp16 | 5 | 1m52s | 4960MB |\n| faster-whisper | fp16 | 5 | 1m03s | 4525MB |\n| faster-whisper (`batch_size=8`) | fp16 | 5 | 17s | 6090MB |\n| faster-whisper | int8 | 5 | 59s | 2926MB |\n| faster-whisper (`batch_size=8`) | int8 | 5 | 16s | 4500MB |\n\n### distil-whisper-large-v3 model on GPU\n\n| Implementation | Precision | Beam size | Time | YT Commons WER |\n| --- | --- | --- | --- | --- |\n| transformers (SDPA) (`batch_size=16`) | fp16 | 5 | 46m12s | 14.801 |\n| faster-whisper (`batch_size=16`) | fp16 | 5 | 25m50s | 13.527 |\n\n*GPU Benchmarks are Executed with CUDA 12.4 on a NVIDIA RTX 3070 Ti 8GB.*\n[^1]: transformers OOM for any batch size > 1\n\n### Small model on CPU\n\n| Implementation | Precision | Beam size | Time | RAM Usage |\n| --- | --- | --- | --- | --- |\n| openai/whisper | fp32 | 5 | 6m58s | 2335MB |\n| whisper.cpp | fp32 | 5 | 2m05s | 1049MB |\n| whisper.cpp (OpenVINO) | fp32 | 5 | 1m45s | 1642MB |\n| faster-whisper | fp32 | 5 | 2m37s | 2257MB |\n| faster-whisper (`batch_size=8`) | fp32 | 5 | 1m06s | 4230MB |\n| faster-whisper | int8 | 5 | 1m42s | 1477MB |\n| faster-whisper (`batch_size=8`) | int8 | 5 | 51s | 3608MB |\n\n*Executed with 8 threads on an Intel Core i7-12700K.*\n\n\n## Requirements\n\n* Python 3.9 or greater\n\nUnlike openai-whisper, FFmpeg does **not** need to be installed on the system. The audio is decoded with the Python library [PyAV](https://github.com/PyAV-Org/PyAV) which bundles the FFmpeg libraries in its package.\n\n### GPU\n\nGPU execution requires the following NVIDIA libraries to be installed:\n\n* [cuBLAS for CUDA 12](https://developer.nvidia.com/cublas)\n* [cuDNN 9 for CUDA 12](https://developer.nvidia.com/cudnn)\n\n**Note**: The latest versions of `ctranslate2` only support CUDA 12 and cuDNN 9. For CUDA 11 and cuDNN 8, the current workaround is downgrading to the `3.24.0` version of `ctranslate2`, for CUDA 12 and cuDNN 8, downgrade to the `4.4.0` version of `ctranslate2`, (This can be done with `pip install --force-reinstall ctranslate2==4.4.0` or specifying the version in a `requirements.txt`).\n\nThere are multiple ways to install the NVIDIA libraries mentioned above. The recommended way is described in the official NVIDIA documentation, but we also suggest other installation methods below. \n\n<details>\n<summary>Other installation methods (click to expand)</summary>\n\n\n**Note:** For all these methods below, keep in mind the above note regarding CUDA versions. Depending on your setup, you may need to install the _CUDA 11_ versions of libraries that correspond to the CUDA 12 libraries listed in the instructions below.\n\n#### Use Docker\n\nThe libraries (cuBLAS, cuDNN) are installed in this official NVIDIA CUDA Docker images: `nvidia/cuda:12.3.2-cudnn9-runtime-ubuntu22.04`.\n\n#### Install with `pip` (Linux only)\n\nOn Linux these libraries can be installed with `pip`. Note that `LD_LIBRARY_PATH` must be set before launching Python.\n\n```bash\npip install nvidia-cublas-cu12 nvidia-cudnn-cu12==9.*\n\nexport LD_LIBRARY_PATH=`python3 -c ''import os; import nvidia.cublas.lib; import nvidia.cudnn.lib; print(os.path.dirname(nvidia.cublas.lib.__file__) + ":" + os.path.dirname(nvidia.cudnn.lib.__file__))''`\n```\n\n#### Download the libraries from Purfview''s repository (Windows & Linux)\n\nPurfview''s [whisper-standalone-win](https://github.com/Purfview/whisper-standalone-win) provides the required NVIDIA libraries for Windows & Linux in a [single archive](https://github.com/Purfview/whisper-standalone-win/releases/tag/libs). Decompress the archive and place the libraries in a directory included in the `PATH`.\n\n</details>\n\n## Installation\n\nThe module can be installed from [PyPI](https://pypi.org/project/faster-whisper/):\n\n```bash\npip install faster-whisper\n```\n\n<details>\n<summary>Other installation methods (click to expand)</summary>\n\n### Install the master branch\n\n```bash\npip install --force-reinstall "faster-whisper @ https://github.com/SYSTRAN/faster-whisper/archive/refs/heads/master.tar.gz"\n```\n\n### Install a specific commit\n\n```bash\npip install --force-reinstall "faster-whisper @ https://github.com/SYSTRAN/faster-whisper/archive/a4f1cc8f11433e454c3934442b5e1a4ed5e865c3.tar.gz"\n```\n\n</details>\n\n## Usage\n\n### Faster-whisper\n\n```python\nfrom faster_whisper import WhisperModel\n\nmodel_size = "large-v3"\n\n# Run on GPU with FP16\nmodel = WhisperModel(model_size, device="cuda", compute_type="float16")\n\n# or run on GPU with INT8\n# model = WhisperModel(model_size, device="cuda", compute_type="int8_float16")\n# or run on CPU with INT8\n# model = WhisperModel(model_size, device="cpu", compute_type="int8")\n\nsegments, info = model.transcribe("audio.mp3", beam_size=5)\n\nprint("Detected language ''%s'' with probability %f" % (info.language, info.language_probability))\n\nfor segment in segments:\n    print("[%.2fs -> %.2fs] %s" % (segment.start, segment.end, segment.text))\n```\n\n**Warning:** `segments` is a *generator* so the transcription only starts when you iterate over it. The transcription can be run to completion by gathering the segments in a list or a `for` loop:\n\n```python\nsegments, _ = model.transcribe("audio.mp3")\nsegments = list(segments)  # The transcription will actually run here.\n```\n\n### Batched Transcription\nThe following code snippet illustrates how to run batched transcription on an example audio file. `BatchedInferencePipeline.transcribe` is a drop-in replacement for `WhisperModel.transcribe`\n\n```python\nfrom faster_whisper import WhisperModel, BatchedInferencePipeline\n\nmodel = WhisperModel("turbo", device="cuda", compute_type="float16")\nbatched_model = BatchedInferencePipeline(model=model)\nsegments, info = batched_model.transcribe("audio.mp3", batch_size=16)\n\nfor segment in segments:\n    print("[%.2fs -> %.2fs] %s" % (segment.start, segment.end, segment.text))\n```\n\n### Faster Distil-Whisper\n\nThe Distil-Whisper checkpoints are compatible with the Faster-Whisper package. In particular, the latest [distil-large-v3](https://huggingface.co/distil-whisper/distil-large-v3)\ncheckpoint is intrinsically designed to work with the Faster-Whisper transcription algorithm. The following code snippet \ndemonstrates how to run inference with distil-large-v3 on a specified audio file:\n\n```python\nfrom faster_whisper import WhisperModel\n\nmodel_size = "distil-large-v3"\n\nmodel = WhisperModel(model_size, device="cuda", compute_type="float16")\nsegments, info = model.transcribe("audio.mp3", beam_size=5, language="en", condition_on_previous_text=False)\n\nfor segment in segments:\n    print("[%.2fs -> %.2fs] %s" % (segment.start, segment.end, segment.text))\n```\n\nFor more information about the distil-large-v3 model, refer to the original [model card](https://huggingface.co/distil-whisper/distil-large-v3).\n\n### Word-level timestamps\n\n```python\nsegments, _ = model.transcribe("audio.mp3", word_timestamps=True)\n\nfor segment in segments:\n    for word in segment.words:\n        print("[%.2fs -> %.2fs] %s" % (word.start, word.end, word.word))\n```\n\n### VAD filter\n\nThe library integrates the [Silero VAD](https://github.com/snakers4/silero-vad) model to filter out parts of the audio without speech:\n\n```python\nsegments, _ = model.transcribe("audio.mp3", vad_filter=True)\n```\n\nThe default behavior is conservative and only removes silence longer than 2 seconds. See the available VAD parameters and default values in the [source code](https://github.com/SYSTRAN/faster-whisper/blob/master/faster_whisper/vad.py). They can be customized with the dictionary argument `vad_parameters`:\n\n```python\nsegments, _ = model.transcribe(\n    "audio.mp3",\n    vad_filter=True,\n    vad_parameters=dict(min_silence_duration_ms=500),\n)\n```\nVad filter is enabled by default for batched transcription.\n\n### Logging\n\nThe library logging level can be configured like this:\n\n```python\nimport logging\n\nlogging.basicConfig()\nlogging.getLogger("faster_whisper").setLevel(logging.DEBUG)\n```\n\n### Going further\n\nSee more model and transcription options in the [`WhisperModel`](https://github.com/SYSTRAN/faster-whisper/blob/master/faster_whisper/transcribe.py) class implementation.\n\n## Community integrations\n\nHere is a non exhaustive list of open-source projects using faster-whisper. Feel free to add your project to the list!\n\n\n* [speaches](https://github.com/speaches-ai/speaches) is an OpenAI compatible server using `faster-whisper`. It''s easily deployable with Docker, works with OpenAI SDKs/CLI, supports streaming, and live transcription.\n* [WhisperX](https://github.com/m-bain/whisperX) is an award-winning Python library that offers speaker diarization and accurate word-level timestamps using wav2vec2 alignment\n* [whisper-ctranslate2](https://github.com/Softcatala/whisper-ctranslate2) is a command line client based on faster-whisper and compatible with the original client from openai/whisper.\n* [whisper-diarize](https://github.com/MahmoudAshraf97/whisper-diarization) is a speaker diarization tool that is based on faster-whisper and NVIDIA NeMo.\n* [whisper-standalone-win](https://github.com/Purfview/whisper-standalone-win) Standalone CLI executables of faster-whisper for Windows, Linux & macOS. \n* [asr-sd-pipeline](https://github.com/hedrergudene/asr-sd-pipeline) provides a scalable, modular, end to end multi-speaker speech to text solution implemented using AzureML pipelines.\n* [Open-Lyrics](https://github.com/zh-plus/Open-Lyrics) is a Python library that transcribes voice files using faster-whisper, and translates/polishes the resulting text into `.lrc` files in the desired language using OpenAI-GPT.\n* [wscribe](https://github.com/geekodour/wscribe) is a flexible transcript generation tool supporting faster-whisper, it can export word level transcript and the exported transcript then can be edited with [wscribe-editor](https://github.com/geekodour/wscribe-editor)\n* [aTrain](https://github.com/BANDAS-Center/aTrain) is a graphical user interface implementation of faster-whisper developed at the BANDAS-Center at the University of Graz for transcription and diarization in Windows ([Windows Store App](https://apps.microsoft.com/detail/atrain/9N15Q44SZNS2)) and Linux.\n* [Whisper-Streaming](https://github.com/ufal/whisper_streaming) implements real-time mode for offline Whisper-like speech-to-text models with faster-whisper as the most recommended back-end. It implements a streaming policy with self-adaptive latency based on the actual source complexity, and demonstrates the state of the art.\n* [WhisperLive](https://github.com/collabora/WhisperLive) is a nearly-live implementation of OpenAI''s Whisper which uses faster-whisper as the backend to transcribe audio in real-time.\n* [Faster-Whisper-Transcriber](https://github.com/BBC-Esq/ctranslate2-faster-whisper-transcriber) is a simple but reliable voice transcriber that provides a user-friendly interface.\n* [Open-dubbing](https://github.com/softcatala/open-dubbing) is open dubbing is an AI dubbing system which uses machine learning models to automatically translate and synchronize audio dialogue into different languages.\n* [Whisper-FastAPI](https://github.com/heimoshuiyu/whisper-fastapi) whisper-fastapi is a very simple script that provides an API backend compatible with OpenAI, HomeAssistant, and Konele (Android voice typing) formats.\n\n## Model conversion\n\nWhen loading a model from its size such as `WhisperModel("large-v3")`, the corresponding CTranslate2 model is automatically downloaded from the [Hugging Face Hub](https://huggingface.co/Systran).\n\nWe also provide a script to convert any Whisper models compatible with the Transformers library. They could be the original OpenAI models or user fine-tuned models.\n\nFor example the command below converts the [original "large-v3" Whisper model](https://huggingface.co/openai/whisper-large-v3) and saves the weights in FP16:\n\n```bash\npip install transformers[torch]>=4.23\n\nct2-transformers-converter --model openai/whisper-large-v3 --output_dir whisper-large-v3-ct2\n--copy_files tokenizer.json preprocessor_config.json --quantization float16\n```\n\n* The option `--model` accepts a model name on the Hub or a path to a model directory.\n* If the option `--copy_files tokenizer.json` is not used, the tokenizer configuration is automatically downloaded when the model is loaded later.\n\nModels can also be converted from the code. See the [conversion API](https://opennmt.net/CTranslate2/python/ctranslate2.converters.TransformersConverter.html).\n\n### Load a converted model\n\n1. Directly load the model from a local directory:\n```python\nmodel = faster_whisper.WhisperModel("whisper-large-v3-ct2")\n```\n\n2. [Upload your model to the Hugging Face Hub](https://huggingface.co/docs/transformers/model_sharing#upload-with-the-web-interface) and load it from its name:\n```python\nmodel = faster_whisper.WhisperModel("username/whisper-large-v3-ct2")\n```\n\n## Comparing performance against other implementations\n\nIf you are comparing the performance against other Whisper implementations, you should make sure to run the comparison with similar settings. In particular:\n\n* Verify that the same transcription options are used, especially the same beam size. For example in openai/whisper, `model.transcribe` uses a default beam size of 1 but here we use a default beam size of 5.\n* Transcription speed is closely affected by the number of words in the transcript, so ensure that other implementations have a similar WER (Word Error Rate) to this one.\n* When running on CPU, make sure to set the same number of threads. Many frameworks will read the environment variable `OMP_NUM_THREADS`, which can be set when running your script:\n\n```bash\nOMP_NUM_THREADS=4 python3 my_script.py\n```\n', '{"language":"Python","stars":19366,"forks":1610,"watchers":19366,"open_issues":294,"topics":["deep-learning","inference","openai","quantization","speech-recognition","speech-to-text","transformer","whisper"],"default_branch":"master","size_kb":40514,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:SYSTRAN:faster-whisper","source_url":"https://github.com/SYSTRAN/faster-whisper"},{"type":"has_code","target_id":"github:SYSTRAN:faster-whisper","source_url":"https://github.com/SYSTRAN/faster-whisper"},{"type":"has_code","target_id":"github:OpenNMT:CTranslate2","source_url":"https://github.com/OpenNMT/CTranslate2"},{"type":"has_code","target_id":"github:openai:whisper","source_url":"https://github.com/openai/whisper"},{"type":"has_code","target_id":"github:openai:whisper","source_url":"https://github.com/openai/whisper"},{"type":"has_code","target_id":"github:openai:whisper","source_url":"https://github.com/openai/whisper"},{"type":"has_code","target_id":"github:ggerganov:whisper.cpp","source_url":"https://github.com/ggerganov/whisper.cpp"},{"type":"has_code","target_id":"github:ggerganov:whisper.cpp","source_url":"https://github.com/ggerganov/whisper.cpp"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:SYSTRAN:faster-whisper","source_url":"https://github.com/SYSTRAN/faster-whisper"},{"type":"has_code","target_id":"github:SYSTRAN:faster-whisper","source_url":"https://github.com/SYSTRAN/faster-whisper"},{"type":"has_code","target_id":"github:PyAV-Org:PyAV","source_url":"https://github.com/PyAV-Org/PyAV"},{"type":"has_code","target_id":"github:Purfview:whisper-standalone-win","source_url":"https://github.com/Purfview/whisper-standalone-win"},{"type":"has_code","target_id":"github:Purfview:whisper-standalone-win","source_url":"https://github.com/Purfview/whisper-standalone-win"},{"type":"has_code","target_id":"github:SYSTRAN:faster-whisper","source_url":"https://github.com/SYSTRAN/faster-whisper"},{"type":"has_code","target_id":"github:SYSTRAN:faster-whisper","source_url":"https://github.com/SYSTRAN/faster-whisper"},{"type":"has_code","target_id":"github:snakers4:silero-vad","source_url":"https://github.com/snakers4/silero-vad"},{"type":"has_code","target_id":"github:SYSTRAN:faster-whisper","source_url":"https://github.com/SYSTRAN/faster-whisper"},{"type":"has_code","target_id":"github:SYSTRAN:faster-whisper","source_url":"https://github.com/SYSTRAN/faster-whisper"},{"type":"has_code","target_id":"github:speaches-ai:speaches","source_url":"https://github.com/speaches-ai/speaches"},{"type":"has_code","target_id":"github:m-bain:whisperX","source_url":"https://github.com/m-bain/whisperX"},{"type":"has_code","target_id":"github:Softcatala:whisper-ctranslate2","source_url":"https://github.com/Softcatala/whisper-ctranslate2"},{"type":"has_code","target_id":"github:MahmoudAshraf97:whisper-diarization","source_url":"https://github.com/MahmoudAshraf97/whisper-diarization"},{"type":"has_code","target_id":"github:Purfview:whisper-standalone-win","source_url":"https://github.com/Purfview/whisper-standalone-win"},{"type":"has_code","target_id":"github:hedrergudene:asr-sd-pipeline","source_url":"https://github.com/hedrergudene/asr-sd-pipeline"},{"type":"has_code","target_id":"github:zh-plus:Open-Lyrics","source_url":"https://github.com/zh-plus/Open-Lyrics"},{"type":"has_code","target_id":"github:geekodour:wscribe","source_url":"https://github.com/geekodour/wscribe"},{"type":"has_code","target_id":"github:geekodour:wscribe-editor","source_url":"https://github.com/geekodour/wscribe-editor"},{"type":"has_code","target_id":"github:BANDAS-Center:aTrain","source_url":"https://github.com/BANDAS-Center/aTrain"},{"type":"has_code","target_id":"github:ufal:whisper_streaming","source_url":"https://github.com/ufal/whisper_streaming"},{"type":"has_code","target_id":"github:collabora:WhisperLive","source_url":"https://github.com/collabora/WhisperLive"},{"type":"has_code","target_id":"github:BBC-Esq:ctranslate2-faster-whisper-transcriber","source_url":"https://github.com/BBC-Esq/ctranslate2-faster-whisper-transcriber"},{"type":"has_code","target_id":"github:softcatala:open-dubbing","source_url":"https://github.com/softcatala/open-dubbing"},{"type":"has_code","target_id":"github:heimoshuiyu:whisper-fastapi","source_url":"https://github.com/heimoshuiyu/whisper-fastapi"}]', NULL, 'MIT', 'approved', 80, 'd320e66d3ef8f69fbf01257efd04ad5d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-SYSTRAN-faster-whisper from https://github.com/SYSTRAN.png
Image converted to WebP: data/images/github-SYSTRAN-faster-whisper.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ShusenTang-Dive-into-DL-PyTorch', 'github--shusentang--dive-into-dl-pytorch', 'Dive-into-DL-PyTorch', 'ShusenTang', '<div align=center> <img width="500" src="img/cover.png" alt="å°é¢"/> </div> æœ¬é¡¹ç›®å°†ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹ åŸä¹¦ä¸­MXNetä»£ç å®ç°æ”¹ä¸ºPyTorchå®ç°ã€‚åŸä¹¦ä½œè€…ï¼šé˜¿æ–¯é¡¿Â·å¼ ã€ææ²ã€æ‰å¡é‡Œ C. ç«‹é¡¿ã€äºšå†å±±å¤§ J. æ–¯è«æ‹‰ä»¥åŠå…¶ä»–ç¤¾åŒºè´¡çŒ®è€…ï¼ŒGitHubåœ°å€ï¼šhttps://github.com/d2l-ai/d2l-zh æ­¤ä¹¦çš„ä¸­è‹±ç‰ˆæœ¬å­˜åœ¨ä¸€äº›ä¸åŒï¼Œé’ˆå¯¹æ­¤ä¹¦è‹±æ–‡ç‰ˆçš„PyTorché‡æ„å¯å‚è€ƒè¿™ä¸ªé¡¹ç›®ã€‚ There are some differences between the Chinese and English versions of this book. For the PyTorch modifying of the English version, you can refer to this repo. æœ¬ä»“åº“ä¸»è¦åŒ…å«codeå’Œdocsä¸¤ä¸ªæ–‡ä»¶å¤¹ï¼ˆå¤–åŠ ä¸€äº›æ•°æ®å­˜æ”¾åœ¨dataä¸­ï¼‰ã€‚å…¶ä¸­codeæ–‡ä»¶å¤¹å°±æ˜¯æ¯ç« ç›¸å…³jupyter notebookä»£ç ï¼ˆåŸºäºPyTorchï¼‰ï¼›docsæ–‡ä»¶å¤¹å°±æ˜¯markdo...', '["computer-vision","d2l","deep-learning","deep-learning-tutorial","natural-language-processing","pytorch","pytorch-tutorial","jupyter notebook"]', 'other', 19200, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ShusenTang/Dive-into-DL-PyTorch","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '\n<div align=center>\n<img width="500" src="img/cover.png" alt="å°é¢"/>\n</div>\n\n[æœ¬é¡¹ç›®](https://tangshusen.me/Dive-into-DL-PyTorch)å°†[ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹](http://zh.d2l.ai/)Â åŸä¹¦ä¸­MXNetä»£ç å®ç°æ”¹ä¸ºPyTorchå®ç°ã€‚åŸä¹¦ä½œè€…ï¼šé˜¿æ–¯é¡¿Â·å¼ ã€ææ²ã€æ‰å¡é‡Œ C. ç«‹é¡¿ã€äºšå†å±±å¤§ J. æ–¯è«æ‹‰ä»¥åŠå…¶ä»–ç¤¾åŒºè´¡çŒ®è€…ï¼ŒGitHubåœ°å€ï¼šhttps://github.com/d2l-ai/d2l-zh\n\næ­¤ä¹¦çš„[ä¸­](https://zh.d2l.ai/)[è‹±](https://d2l.ai/)ç‰ˆæœ¬å­˜åœ¨ä¸€äº›ä¸åŒï¼Œé’ˆå¯¹æ­¤ä¹¦è‹±æ–‡ç‰ˆçš„PyTorché‡æ„å¯å‚è€ƒ[è¿™ä¸ªé¡¹ç›®](https://github.com/dsgiitr/d2l-pytorch)ã€‚\nThere are some differences between the [Chinese](https://zh.d2l.ai/) and [English](https://d2l.ai/) versions of this book. For the PyTorch modifying of the English version, you can refer to [this repo](https://github.com/dsgiitr/d2l-pytorch).\n\n\n## ç®€ä»‹\næœ¬ä»“åº“ä¸»è¦åŒ…å«codeå’Œdocsä¸¤ä¸ªæ–‡ä»¶å¤¹ï¼ˆå¤–åŠ ä¸€äº›æ•°æ®å­˜æ”¾åœ¨dataä¸­ï¼‰ã€‚å…¶ä¸­codeæ–‡ä»¶å¤¹å°±æ˜¯æ¯ç« ç›¸å…³jupyter notebookä»£ç ï¼ˆåŸºäºPyTorchï¼‰ï¼›docsæ–‡ä»¶å¤¹å°±æ˜¯markdownæ ¼å¼çš„ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹ä¹¦ä¸­çš„ç›¸å…³å†…å®¹ï¼Œç„¶ååˆ©ç”¨[docsify](https://docsify.js.org/#/zh-cn/)å°†ç½‘é¡µæ–‡æ¡£éƒ¨ç½²åˆ°GitHub Pagesä¸Šï¼Œç”±äºåŸä¹¦ä½¿ç”¨çš„æ˜¯MXNetæ¡†æ¶ï¼Œæ‰€ä»¥docså†…å®¹å¯èƒ½ä¸åŸä¹¦ç•¥æœ‰ä¸åŒï¼Œä½†æ˜¯æ•´ä½“å†…å®¹æ˜¯ä¸€æ ·çš„ã€‚æ¬¢è¿å¯¹æœ¬é¡¹ç›®åšå‡ºè´¡çŒ®æˆ–æå‡ºissueã€‚\n\n## é¢å‘äººç¾¤\næœ¬é¡¹ç›®é¢å‘å¯¹æ·±åº¦å­¦ä¹ æ„Ÿå…´è¶£ï¼Œå°¤å…¶æ˜¯æƒ³ä½¿ç”¨PyTorchè¿›è¡Œæ·±åº¦å­¦ä¹ çš„ç«¥é‹ã€‚æœ¬é¡¹ç›®å¹¶ä¸è¦æ±‚ä½ æœ‰ä»»ä½•æ·±åº¦å­¦ä¹ æˆ–è€…æœºå™¨å­¦ä¹ çš„èƒŒæ™¯çŸ¥è¯†ï¼Œä½ åªéœ€äº†è§£åŸºç¡€çš„æ•°å­¦å’Œç¼–ç¨‹ï¼Œå¦‚åŸºç¡€çš„çº¿æ€§ä»£æ•°ã€å¾®åˆ†å’Œæ¦‚ç‡ï¼Œä»¥åŠåŸºç¡€çš„Pythonç¼–ç¨‹ã€‚\n\n## é£Ÿç”¨æ–¹æ³• \n### æ–¹æ³•ä¸€\næœ¬ä»“åº“åŒ…å«ä¸€äº›latexå…¬å¼ï¼Œä½†githubçš„markdownåŸç”Ÿæ˜¯ä¸æ”¯æŒå…¬å¼æ˜¾ç¤ºçš„ï¼Œè€Œdocsæ–‡ä»¶å¤¹å·²ç»åˆ©ç”¨[docsify](https://docsify.js.org/#/zh-cn/)è¢«éƒ¨ç½²åˆ°äº†GitHub Pagesä¸Šï¼Œæ‰€ä»¥æŸ¥çœ‹æ–‡æ¡£æœ€ç®€ä¾¿çš„æ–¹æ³•å°±æ˜¯ç›´æ¥è®¿é—®[æœ¬é¡¹ç›®ç½‘é¡µç‰ˆ](https://tangshusen.me/Dive-into-DL-PyTorch)ã€‚å½“ç„¶å¦‚æœä½ è¿˜æƒ³è·‘ä¸€ä¸‹è¿è¡Œç›¸å…³ä»£ç çš„è¯è¿˜æ˜¯å¾—æŠŠæœ¬é¡¹ç›®cloneä¸‹æ¥ï¼Œç„¶åè¿è¡Œcodeæ–‡ä»¶å¤¹ä¸‹ç›¸å…³ä»£ç ã€‚\n\n### æ–¹æ³•äºŒ\nä½ è¿˜å¯ä»¥åœ¨æœ¬åœ°è®¿é—®æ–‡æ¡£ï¼Œå…ˆå®‰è£…`docsify-cli`å·¥å…·:\n``` shell\nnpm i docsify-cli -g\n```\nç„¶åå°†æœ¬é¡¹ç›®cloneåˆ°æœ¬åœ°:\n``` shell\ngit clone https://github.com/ShusenTang/Dive-into-DL-PyTorch.git\ncd Dive-into-DL-PyTorch\n```\nç„¶åè¿è¡Œä¸€ä¸ªæœ¬åœ°æœåŠ¡å™¨ï¼Œè¿™æ ·å°±å¯ä»¥å¾ˆæ–¹ä¾¿çš„åœ¨`http://localhost:3000`å®æ—¶è®¿é—®æ–‡æ¡£ç½‘é¡µæ¸²æŸ“æ•ˆæœã€‚\n``` shell\ndocsify serve docs\n```\n\n### æ–¹æ³•ä¸‰\nå¦‚æœä½ ä¸æƒ³å®‰è£…`docsify-cli`å·¥å…·ï¼Œç”šè‡³ä½ çš„ç”µè„‘ä¸Šéƒ½æ²¡æœ‰å®‰è£…`Node.js`ï¼Œè€Œå‡ºäºæŸäº›åŸå› ä½ åˆæƒ³åœ¨æœ¬åœ°æµè§ˆæ–‡æ¡£ï¼Œé‚£ä¹ˆä½ å¯ä»¥åœ¨`docker`å®¹å™¨ä¸­è¿è¡Œç½‘é¡µæœåŠ¡ã€‚\n\né¦–å…ˆå°†æœ¬é¡¹ç›®cloneåˆ°æœ¬åœ°:\n``` shell\ngit clone https://github.com/ShusenTang/Dive-into-DL-PyTorch.git\ncd Dive-into-DL-PyTorch\n```\nä¹‹åä½¿ç”¨å¦‚ä¸‹å‘½ä»¤åˆ›å»ºä¸€ä¸ªåç§°ä¸ºã€Œd2dlã€çš„`docker`é•œåƒï¼š\n``` shell\ndocker build -t d2dl .\n```\né•œåƒåˆ›å»ºå¥½åï¼Œè¿è¡Œå¦‚ä¸‹å‘½ä»¤åˆ›å»ºä¸€ä¸ªæ–°çš„å®¹å™¨ï¼š\n``` shell\ndocker run -dp 3000:3000 d2dl\n```\næœ€ååœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¿™ä¸ªåœ°å€`http://localhost:3000/#/`ï¼Œå°±èƒ½æ„‰å¿«åœ°è®¿é—®æ–‡æ¡£äº†ã€‚é€‚åˆé‚£äº›ä¸æƒ³åœ¨ç”µè„‘ä¸Šè£…å¤ªå¤šå·¥å…·çš„å°ä¼™ä¼´ã€‚\n\n\n## ç›®å½•\n* [ç®€ä»‹]()\n* [é˜…è¯»æŒ‡å—](read_guide.md)\n* [1. æ·±åº¦å­¦ä¹ ç®€ä»‹](chapter01_DL-intro/deep-learning-intro.md)\n* 2\. é¢„å¤‡çŸ¥è¯†\n   * [2.1 ç¯å¢ƒé…ç½®](chapter02_prerequisite/2.1_install.md)\n   * [2.2 æ•°æ®æ“ä½œ](chapter02_prerequisite/2.2_tensor.md)\n   * [2.3 è‡ªåŠ¨æ±‚æ¢¯åº¦](chapter02_prerequisite/2.3_autograd.md)\n* 3\. æ·±åº¦å­¦ä¹ åŸºç¡€\n   * [3.1 çº¿æ€§å›å½’](chapter03_DL-basics/3.1_linear-regression.md)\n   * [3.2 çº¿æ€§å›å½’çš„ä»é›¶å¼€å§‹å®ç°](chapter03_DL-basics/3.2_linear-regression-scratch.md)\n   * [3.3 çº¿æ€§å›å½’çš„ç®€æ´å®ç°](chapter03_DL-basics/3.3_linear-regression-pytorch.md)\n   * [3.4 softmaxå›å½’](chapter03_DL-basics/3.4_softmax-regression.md)\n   * [3.5 å›¾åƒåˆ†ç±»æ•°æ®é›†ï¼ˆFashion-MNISTï¼‰](chapter03_DL-basics/3.5_fashion-mnist.md)\n   * [3.6 softmaxå›å½’çš„ä»é›¶å¼€å§‹å®ç°](chapter03_DL-basics/3.6_softmax-regression-scratch.md)\n   * [3.7 softmaxå›å½’çš„ç®€æ´å®ç°](chapter03_DL-basics/3.7_softmax-regression-pytorch.md)\n   * [3.8 å¤šå±‚æ„ŸçŸ¥æœº](chapter03_DL-basics/3.8_mlp.md)\n   * [3.9 å¤šå±‚æ„ŸçŸ¥æœºçš„ä»é›¶å¼€å§‹å®ç°](chapter03_DL-basics/3.9_mlp-scratch.md)\n   * [3.10 å¤šå±‚æ„ŸçŸ¥æœºçš„ç®€æ´å®ç°](chapter03_DL-basics/3.10_mlp-pytorch.md)\n   * [3.11 æ¨¡å‹é€‰æ‹©ã€æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆ](chapter03_DL-basics/3.11_underfit-overfit.md)\n   * [3.12 æƒé‡è¡°å‡](chapter03_DL-basics/3.12_weight-decay.md)\n   * [3.13 ä¸¢å¼ƒæ³•](chapter03_DL-basics/3.13_dropout.md)\n   * [3.14 æ­£å‘ä¼ æ’­ã€åå‘ä¼ æ’­å’Œè®¡ç®—å›¾](chapter03_DL-basics/3.14_backprop.md)\n   * [3.15 æ•°å€¼ç¨³å®šæ€§å’Œæ¨¡å‹åˆå§‹åŒ–](chapter03_DL-basics/3.15_numerical-stability-and-init.md)\n   * [3.16 å®æˆ˜Kaggleæ¯”èµ›ï¼šæˆ¿ä»·é¢„æµ‹](chapter03_DL-basics/3.16_kaggle-house-price.md)\n* 4\. æ·±åº¦å­¦ä¹ è®¡ç®—\n   * [4.1 æ¨¡å‹æ„é€ ](chapter04_DL_computation/4.1_model-construction.md)\n   * [4.2 æ¨¡å‹å‚æ•°çš„è®¿é—®ã€åˆå§‹åŒ–å’Œå…±äº«](chapter04_DL_computation/4.2_parameters.md)\n   * [4.3 æ¨¡å‹å‚æ•°çš„å»¶ååˆå§‹åŒ–](chapter04_DL_computation/4.3_deferred-init.md)\n   * [4.4 è‡ªå®šä¹‰å±‚](chapter04_DL_computation/4.4_custom-layer.md)\n   * [4.5 è¯»å–å’Œå­˜å‚¨](chapter04_DL_computation/4.5_read-write.md)\n   * [4.6 GPUè®¡ç®—](chapter04_DL_computation/4.6_use-gpu.md)\n* 5\. å·ç§¯ç¥ç»ç½‘ç»œ\n   * [5.1 äºŒç»´å·ç§¯å±‚](chapter05_CNN/5.1_conv-layer.md)\n   * [5.2 å¡«å……å’Œæ­¥å¹…](chapter05_CNN/5.2_padding-and-strides.md)\n   * [5.3 å¤šè¾“å…¥é€šé“å’Œå¤šè¾“å‡ºé€šé“](chapter05_CNN/5.3_channels.md)\n   * [5.4 æ± åŒ–å±‚](chapter05_CNN/5.4_pooling.md)\n   * [5.5 å·ç§¯ç¥ç»ç½‘ç»œï¼ˆLeNetï¼‰](chapter05_CNN/5.5_lenet.md)\n   * [5.6 æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆAlexNetï¼‰](chapter05_CNN/5.6_alexnet.md)\n   * [5.7 ä½¿ç”¨é‡å¤å…ƒç´ çš„ç½‘ç»œï¼ˆVGGï¼‰](chapter05_CNN/5.7_vgg.md)\n   * [5.8 ç½‘ç»œä¸­çš„ç½‘ç»œï¼ˆNiNï¼‰](chapter05_CNN/5.8_nin.md)\n   * [5.9 å«å¹¶è¡Œè¿ç»“çš„ç½‘ç»œï¼ˆGoogLeNetï¼‰](chapter05_CNN/5.9_googlenet.md)\n   * [5.10 æ‰¹é‡å½’ä¸€åŒ–](chapter05_CNN/5.10_batch-norm.md)\n   * [5.11 æ®‹å·®ç½‘ç»œï¼ˆResNetï¼‰](chapter05_CNN/5.11_resnet.md)\n   * [5.12 ç¨ å¯†è¿æ¥ç½‘ç»œï¼ˆDenseNetï¼‰](chapter05_CNN/5.12_densenet.md)\n* 6\. å¾ªç¯ç¥ç»ç½‘ç»œ\n   * [6.1 è¯­è¨€æ¨¡å‹](chapter06_RNN/6.1_lang-model.md)\n   * [6.2 å¾ªç¯ç¥ç»ç½‘ç»œ](chapter06_RNN/6.2_rnn.md)\n   * [6.3 è¯­è¨€æ¨¡å‹æ•°æ®é›†ï¼ˆå‘¨æ°ä¼¦ä¸“è¾‘æ­Œè¯ï¼‰](chapter06_RNN/6.3_lang-model-dataset.md)\n   * [6.4 å¾ªç¯ç¥ç»ç½‘ç»œçš„ä»é›¶å¼€å§‹å®ç°](chapter06_RNN/6.4_rnn-scratch.md)\n   * [6.5 å¾ªç¯ç¥ç»ç½‘ç»œçš„ç®€æ´å®ç°](chapter06_RNN/6.5_rnn-pytorch.md)\n   * [6.6 é€šè¿‡æ—¶é—´åå‘ä¼ æ’­](chapter06_RNN/6.6_bptt.md)\n   * [6.7 é—¨æ§å¾ªç¯å•å…ƒï¼ˆGRUï¼‰](chapter06_RNN/6.7_gru.md)\n   * [6.8 é•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰](chapter06_RNN/6.8_lstm.md)\n   * [6.9 æ·±åº¦å¾ªç¯ç¥ç»ç½‘ç»œ](chapter06_RNN/6.9_deep-rnn.md)\n   * [6.10 åŒå‘å¾ªç¯ç¥ç»ç½‘ç»œ](chapter06_RNN/6.10_bi-rnn.md)\n* 7\. ä¼˜åŒ–ç®—æ³•\n   * [7.1 ä¼˜åŒ–ä¸æ·±åº¦å­¦ä¹ ](chapter07_optimization/7.1_optimization-intro.md)\n   * [7.2 æ¢¯åº¦ä¸‹é™å’Œéšæœºæ¢¯åº¦ä¸‹é™](chapter07_optimization/7.2_gd-sgd.md)\n   * [7.3 å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™](chapter07_optimization/7.3_minibatch-sgd.md)\n   * [7.4 åŠ¨é‡æ³•](chapter07_optimization/7.4_momentum.md)\n   * [7.5 AdaGradç®—æ³•](chapter07_optimization/7.5_adagrad.md)\n   * [7.6 RMSPropç®—æ³•](chapter07_optimization/7.6_rmsprop.md)\n   * [7.7 AdaDeltaç®—æ³•](chapter07_optimization/7.7_adadelta.md)\n   * [7.8 Adamç®—æ³•](chapter07_optimization/7.8_adam.md)\n* 8\. è®¡ç®—æ€§èƒ½\n   * [8.1 å‘½ä»¤å¼å’Œç¬¦å·å¼æ··åˆç¼–ç¨‹](chapter08_computational-performance/8.1_hybridize.md)\n   * [8.2 å¼‚æ­¥è®¡ç®—](chapter08_computational-performance/8.2_async-computation.md)\n   * [8.3 è‡ªåŠ¨å¹¶è¡Œè®¡ç®—](chapter08_computational-performance/8.3_auto-parallelism.md)\n   * [8.4 å¤šGPUè®¡ç®—](chapter08_computational-performance/8.4_multiple-gpus.md)\n* 9\. è®¡ç®—æœºè§†è§‰\n   * [9.1 å›¾åƒå¢å¹¿](chapter09_computer-vision/9.1_image-augmentation.md)\n   * [9.2 å¾®è°ƒ](chapter09_computer-vision/9.2_fine-tuning.md)\n   * [9.3 ç›®æ ‡æ£€æµ‹å’Œè¾¹ç•Œæ¡†](chapter09_computer-vision/9.3_bounding-box.md)\n   * [9.4 é”šæ¡†](chapter09_computer-vision/9.4_anchor.md)\n   * [9.5 å¤šå°ºåº¦ç›®æ ‡æ£€æµ‹](chapter09_computer-vision/9.5_multiscale-object-detection.md)\n   * [9.6 ç›®æ ‡æ£€æµ‹æ•°æ®é›†ï¼ˆçš®å¡ä¸˜ï¼‰](chapter09_computer-vision/9.6_object-detection-dataset.md)\n   - [ ] 9.7 å•å‘å¤šæ¡†æ£€æµ‹ï¼ˆSSDï¼‰\n   * [9.8 åŒºåŸŸå·ç§¯ç¥ç»ç½‘ç»œï¼ˆR-CNNï¼‰ç³»åˆ—](chapter09_computer-vision/9.8_rcnn.md)\n   * [9.9 è¯­ä¹‰åˆ†å‰²å’Œæ•°æ®é›†](chapter09_computer-vision/9.9_semantic-segmentation-and-dataset.md)\n   - [ ] 9.10 å…¨å·ç§¯ç½‘ç»œï¼ˆFCNï¼‰\n   * [9.11 æ ·å¼è¿ç§»](chapter09_computer-vision/9.11_neural-style.md)\n   - [ ] 9.12 å®æˆ˜Kaggleæ¯”èµ›ï¼šå›¾åƒåˆ†ç±»ï¼ˆCIFAR-10ï¼‰\n   - [ ] 9.13 å®æˆ˜Kaggleæ¯”èµ›ï¼šç‹—çš„å“ç§è¯†åˆ«ï¼ˆImageNet Dogsï¼‰\n* 10\. è‡ªç„¶è¯­è¨€å¤„ç†\n   * [10.1 è¯åµŒå…¥ï¼ˆword2vecï¼‰](chapter10_natural-language-processing/10.1_word2vec.md)\n   * [10.2 è¿‘ä¼¼è®­ç»ƒ](chapter10_natural-language-processing/10.2_approx-training.md)\n   * [10.3 word2vecçš„å®ç°](chapter10_natural-language-processing/10.3_word2vec-pytorch.md)\n   * [10.4 å­è¯åµŒå…¥ï¼ˆfastTextï¼‰](chapter10_natural-language-processing/10.4_fasttext.md)\n   * [10.5 å…¨å±€å‘é‡çš„è¯åµŒå…¥ï¼ˆGloVeï¼‰](chapter10_natural-language-processing/10.5_glove.md)\n   * [10.6 æ±‚è¿‘ä¹‰è¯å’Œç±»æ¯”è¯](chapter10_natural-language-processing/10.6_similarity-analogy.md)\n   * [10.7 æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ï¼šä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œ](chapter10_natural-language-processing/10.7_sentiment-analysis-rnn.md)\n   * [10.8 æ–‡æœ¬æƒ…æ„Ÿåˆ†ç±»ï¼šä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆtextCNNï¼‰](chapter10_natural-language-processing/10.8_sentiment-analysis-cnn.md)\n   * [10.9 ç¼–ç å™¨â€”è§£ç å™¨ï¼ˆseq2seqï¼‰](chapter10_natural-language-processing/10.9_seq2seq.md)\n   * [10.10 æŸæœç´¢](chapter10_natural-language-processing/10.10_beam-search.md)\n   * [10.11 æ³¨æ„åŠ›æœºåˆ¶](chapter10_natural-language-processing/10.11_attention.md)\n   * [10.12 æœºå™¨ç¿»è¯‘](chapter10_natural-language-processing/10.12_machine-translation.md)\n\n\n\næŒç»­æ›´æ–°ä¸­......\n\n\n\n\n## åŸä¹¦åœ°å€\nä¸­æ–‡ç‰ˆï¼š[åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ](https://zh.d2l.ai/) | [Githubä»“åº“](https://github.com/d2l-ai/d2l-zh)       \nEnglish Version: [Dive into Deep Learning](https://d2l.ai/) | [Github Repo](https://github.com/d2l-ai/d2l-en)\n\n\n## å¼•ç”¨\nå¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†è¿™ä¸ªé¡¹ç›®è¯·å¼•ç”¨åŸä¹¦:\n```\n@book{zhang2019dive,\n    title={Dive into Deep Learning},\n    author={Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},\n    note={\url{http://www.d2l.ai}},\n    year={2020}\n}\n```\n', '{"language":"Jupyter Notebook","stars":19200,"forks":5432,"watchers":19200,"open_issues":78,"topics":["computer-vision","d2l","deep-learning","deep-learning-tutorial","natural-language-processing","pytorch","pytorch-tutorial"],"default_branch":"master","size_kb":35157,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:d2l-ai:d2l-zh","source_url":"https://github.com/d2l-ai/d2l-zh"},{"type":"has_code","target_id":"github:dsgiitr:d2l-pytorch","source_url":"https://github.com/dsgiitr/d2l-pytorch"},{"type":"has_code","target_id":"github:dsgiitr:d2l-pytorch","source_url":"https://github.com/dsgiitr/d2l-pytorch"},{"type":"has_code","target_id":"github:ShusenTang:Dive-into-DL-PyTorch.git","source_url":"https://github.com/ShusenTang/Dive-into-DL-PyTorch.git"},{"type":"has_code","target_id":"github:ShusenTang:Dive-into-DL-PyTorch.git","source_url":"https://github.com/ShusenTang/Dive-into-DL-PyTorch.git"},{"type":"has_code","target_id":"github:d2l-ai:d2l-zh","source_url":"https://github.com/d2l-ai/d2l-zh"},{"type":"has_code","target_id":"github:d2l-ai:d2l-en","source_url":"https://github.com/d2l-ai/d2l-en"}]', NULL, 'Apache-2.0', 'approved', 65, 'c8e3194c3e886650a4cb348193680386', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ShusenTang-Dive-into-DL-PyTorch from https://github.com/ShusenTang.png
Image converted to WebP: data/images/github-ShusenTang-Dive-into-DL-PyTorch.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-CNTK', 'github--microsoft--cntk', 'CNTK', 'microsoft', '| **Chat** | **Windows build status** | **Linux build status** | |-------------|-------------|---------------| | | | | The Microsoft Cognitive Toolkit (https://cntk.ai) is a unified deep learning toolkit that describes neural networks as a series of computational steps via a directed graph. In this directed graph, leaf nodes represent input values or network parameters, while other nodes represent matrix operations upon their inputs. CNTK allows users to easily realize and combine popular mod...', '["c-plus-plus","c-sharp","cntk","cognitive-toolkit","deep-learning","deep-neural-networks","distributed","java","machine-learning","neural-network","python","c++"]', 'other', 17601, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/CNTK","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '## CNTK\n\n| **Chat** | **Windows build status** | **Linux build status** |\n|-------------|-------------|---------------|\n| [![Join the chat at https://gitter.im/Microsoft/CNTK](https://badges.gitter.im/Microsoft/CNTK.svg)](https://gitter.im/Microsoft/CNTK?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge) | [![Build Status](https://aiinfra.visualstudio.com/_apis/public/build/definitions/a95b3960-90bb-440b-bd18-d3ec5d1cf8c3/126/badge)](https://cntk.ai/nightly-windows.html) | [![Build Status](https://aiinfra.visualstudio.com/_apis/public/build/definitions/a95b3960-90bb-440b-bd18-d3ec5d1cf8c3/127/badge)](https://cntk.ai/nightly-linux.html) |\n\nThe Microsoft Cognitive Toolkit (https://cntk.ai) is a unified deep learning toolkit that describes neural networks as a series of computational steps via a directed graph. In this directed graph, leaf nodes represent input values or network parameters, while other nodes represent matrix operations upon their inputs. CNTK allows users to easily realize and combine popular model types such as feed-forward DNNs, convolutional nets (CNNs), and recurrent networks (RNNs/LSTMs). It implements stochastic gradient descent (SGD, error backpropagation) learning with automatic differentiation and parallelization across multiple GPUs and servers. CNTK has been available under an open-source license since April 2015. It is our hope that the community will take advantage of CNTK to share ideas more quickly through the exchange of open source working code.\n\n## Installation\n\n* [Setup CNTK](https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-CNTK-on-your-machine)\n    * Windows ([Python-only](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-windows-python) / [Script-driven](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-windows-binary-script) / [Manual](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-windows-binary-manual))\n    * Linux ([Python-only](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-linux-python) / [Script-driven](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-linux-binary-script) / [Manual](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-linux-binary-manual) / [Docker](https://docs.microsoft.com/en-us/cognitive-toolkit/cntk-docker-containers))\n* [CNTK backend for Keras](https://docs.microsoft.com/en-us/cognitive-toolkit/using-cntk-with-keras)\n* [Setup CNTK development environment](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-development-environment)\n    * Windows ([Script-driven](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-cntk-with-script-on-windows) / [Manual](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-cntk-on-windows))\n    * Linux ([Manual](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-cntk-on-linux))\n    \n### Installing nightly packages\n\nIf you prefer to use latest CNTK bits from master, use one of the CNTK nightly packages:\n\n* [Nightly packages for Windows](https://cntk.ai/nightly-windows.html)\n* [Nightly packages for Linux](https://cntk.ai/nightly-linux.html)\n\n## Learning CNTK\n\nYou can learn more about using and contributing to CNTK with the following resources:\n\n* [General documentation](https://docs.microsoft.com/en-us/cognitive-toolkit/)\n* [Python API documentation](https://cntk.ai/pythondocs/)\n* [Evaluation documentation (C++, C#/.NET, Python, Java)](https://docs.microsoft.com/en-us/cognitive-toolkit/CNTK-Evaluation-Overview)\n* [Manual](https://github.com/Microsoft/CNTK/tree/master/Manual)\n* [Tutorials](https://docs.microsoft.com/en-us/cognitive-toolkit/tutorials)\n* [Examples](https://docs.microsoft.com/en-us/cognitive-toolkit/Examples)\n* [Pretrained models](./PretrainedModels)\n* [Blog](https://www.microsoft.com/en-us/cognitive-toolkit/blog/)\n* [Presentations](https://docs.microsoft.com/en-us/cognitive-toolkit/Presentations)\n* [License](./LICENSE.md)\n\n## More information\n\n* [Contribute to CNTK](https://docs.microsoft.com/en-us/cognitive-toolkit/Contributing-to-CNTK)\n* [FAQ](https://docs.microsoft.com/en-us/cognitive-toolkit/CNTK-FAQ)\n* [Feedback](https://docs.microsoft.com/en-us/cognitive-toolkit/Feedback-Channels)\n\n## Disclaimer\n\nDear community, \n\nWith our ongoing contributions to ONNX and the ONNX Runtime, we have made it easier to interoperate within the AI framework ecosystem and to access high performance, cross-platform inferencing capabilities for both traditional ML models and deep neural networks. Over the last few years we have been privileged to develop such key open-source machine learning projects, including the Microsoft Cognitive Toolkit, which has enabled its users to leverage industry-wide advancements in deep learning at scale. \n\nTodayâ€™s 2.7 release will be the last main release of CNTK. We may have some subsequent minor releases for bug fixes, but these will be evaluated on a case-by-case basis. There are no plans for new feature development post this release. \n\nThe CNTK 2.7 release has full support for ONNX 1.4.1, and we encourage those seeking to operationalize their CNTK models to take advantage of ONNX and the ONNX Runtime. Moving forward, users can continue to leverage evolving ONNX innovations via the number of frameworks that support it. For example, users can natively export ONNX models from PyTorch or convert TensorFlow models to ONNX with the TensorFlow-ONNX converter. \n\nWe are incredibly grateful for all the support we have received from contributors and users over the years since the initial open-source release of CNTK. CNTK has enabled both Microsoft teams and external users to execute complex and large-scale workloads in all manner of deep learning applications, such as historical breakthroughs in speech recognition achieved by Microsoft Speech researchers, the originators of the framework. \n\nAs ONNX is increasingly employed in serving models used across Microsoft products such as Bing and Office, we are dedicated to synthesizing innovations from research with the rigorous demands of production to progress the ecosystem forward. \n\nAbove all, our goal is to make innovations in deep learning across the software and hardware stacks as open and accessible as possible. We will be working hard to bring both the existing strengths of CNTK and new state-of-the-art research into other open-source projects to truly broaden the reach of such technologies. \n\nWith gratitude, \n\n-- The CNTK Team \n\n## Microsoft Open Source Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## News\n\n> You can find more news on [the official project feed](https://docs.microsoft.com/en-us/cognitive-toolkit/news)\n\n***2019-03-29.*** CNTK 2.7.0\n## Highlights of this release\n* Moved to CUDA 10 for both Windows and Linux.\n* Support advance RNN loop in ONNX export.\n* Export larger than 2GB models in ONNX format.\n* Support FP16 in Brain Script train action.\n\n## CNTK support for CUDA 10\n\n### CNTK now supports CUDA 10. This requires an update to build environment to Visual Studio 2017 v15.9 for Windows.\n\nTo setup build and runtime environment on Windows:\n* Install [Visual Studio 2017](https://www.visualstudio.com/downloads/). Note: going forward for CUDA 10 and beyond, it is no longer required to install and run with the specific VC Tools version 14.11.\n* Install [Nvidia CUDA 10](https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64)\n* From PowerShell, run:\n    [DevInstall.ps1](../Tools/devInstall/Windows/DevInstall.ps1)\n* Start Visual Studio 2017 and open [CNTK.sln](./CNTK.sln).\n\nTo setup build and runtime environment on Linux using docker, please build Unbuntu 16.04 docker image using Dockerfiles [here](./Tools/docker). For other Linux systems, please refer to the Dockerfiles to setup dependent libraries for CNTK.\n\n## Support advance RNN loop in ONNX export\nCNTK models with recursive loops can be exported to ONNX models with scan ops.\n\n## Export larger than 2GB models in ONNX format\nTo export models larger than 2GB in ONNX format, use cntk.Function API:\nsave(self, filename, format=ModelFormat.CNTKv2, use_external_files_to_store_parameters=False)\nwith ''format'' set to ModelFormat.ONNX and use_external_files_to_store_parameters set to True.\nIn this case, model parameters are saved in external files. Exported models shall be used with external parameter files when doing model evaluation with onnxruntime.\n\n***2018-11-26.***  \n[Netron](https://github.com/lutzroeder/netron) now supports visualizing CNTK v1 and CNTK v2 `.model` files.\n\n<img src=https://cntk.ai/Images/netron/netron-cntk-dark-1.png alt="NetronCNTKDark1" width="300"> <img src=https://cntk.ai/Images/netron/netron-cntk-light-1.png alt="NetronCNTKLight1" width="300">\n\n\n### Project changelog\n\n***2018-09-17.*** CNTK 2.6.0\n## Efficient group convolution\nThe implementation of group convolution in CNTK has been updated. The updated implementation moves away from creating a sub-graph for group convolution (using slicing and splicing), and instead uses cuDNN7 and MKL2017 APIs directly. This improves the experience both in terms of performance and model size. \n\nAs an example, for a single group convolution op with the following attributes:\n\n- Input tensor (C, H, W) = (32, 128, 128)\n- Number of output channels = 32 (channel multiplier is 1)\n- Groups = 32 (depth wise convolution)\n- Kernel size = (5, 5)\n\nThe comparison numbers for this single node are as follows:\n\n| First Header  | GPU exec. time (in millisec., 1000 run avg.) | CPU exec. time (in millisec., 1000 run avg.) | Model Size (in KB, CNTK format)\n| ------------- | ------------- | ------------- | ------------- |\n| Old implementation  | 9.349  | 41.921  | 38  |\n| New implementation  | 6.581  | 9.963  | 5  |\n| Speedup/savings   Approx.  | 30%  Approx.  | 65-75%   Approx.  | 87% |\n\n## Sequential Convolution\nThe implementation of sequential convolution in CNTK has been updated. The updated implementation creates a separate sequential convolution layer. Different from regular convolution layer, this operation convolves also on the dynamic axis(sequence), and filter_shape[0] is applied to that axis. The updated implementation supports broader cases, such as where stride > 1 for the sequence axis.\n\nFor example, a sequential convolution over a batch of one-channel black-and-white images. The images have the same fixed height of 640, but each with width of variable lengths. The width is then represented by sequential axis. Padding is enabled, and strides for both width and height are 2.\n\n     >>> f = SequentialConvolution((3,3), reduction_rank=0, pad=True, strides=(2,2), activation=C.relu)\n     >>> x = C.input_variable(**Sequence[Tensor[640]])\n     >>> x.shape\n         (640,)\n     >>> h = f(x)\n     >>> h.shape\n         (320,)\n     >>> f.W.shape\n         (1, 1, 3, 3)\n\n## Operators\n### depth_to_space and space_to_depth\nThere is a breaking change in the **depth_to_space** and **space_to_depth** operators. These have been updated to match ONNX specification, specifically\nthe permutation for how the depth dimension is placed as blocks in the spatial dimensions, and vice-versa, has been changed. Please refer to the updated doc\nexamples for these two ops to see the change.\n\n### Tan and Atan\nAdded support for trigonometric ops `Tan` and `Atan`.\n\n### ELU\nAdded support for `alpha` attribute in ELU op.\n\n### Convolution\nUpdated auto padding algorithms of `Convolution` to produce symmetric padding at best effort on CPU, without affecting the final convolution output values. This update increases the range of cases that could be covered by MKL API and improves the performance, E.g. ResNet50.\n\n## Default arguments order\nThere is a breaking change in the **arguments** property in CNTK python API. The default behavior has been updated to return arguments in python order instead of in C++ order. This way it will return arguments in the same order as they are fed into ops. If you wish to still get arguments in C++ order, you can simply override the global option. This change should only affect the following ops: Times, TransposeTimes, and Gemm(internal). \n\n## Bug fixes\n- Updated doc for Convolution layer to include group and dilation arguments.\n- Added improved input validation for group convolution.\n- Updated `LogSoftMax` to use more numerically stable implementation.\n- Fixed Gather op''s incorrect gradient value.\n- Added validation for ''None'' node in python clone substitution.\n- Added validation for padding channel axis in convolution.\n- Added CNTK native default lotusIR logger to fix the "Attempt to use DefaultLogger" error when loading some ONNX models.\n- Added proper initialization for ONNX TypeStrToProtoMap.\n- Updated python doctest to handle different print format for newer version numpy(version >= 1.14).\n- Fixed Pooling(CPU) to produce correct output values when kernel center is on padded input cells.\n\n## ONNX\n### Updates\n- Updated CNTK''s ONNX import/export to use ONNX 1.2 spec.\n- Major update to how batch and sequence axes are handled in export and import. As a result, the complex scenarios and edge cases are handled accurately.\n- Updated CNTK''s ONNX `BatchNormalization` op export/import to latest spec.\n- Added model domain to ONNX model export.\n- Improved error reporting during import and export of ONNX models.\n- Updated `DepthToSpace` and `SpaceToDepth` ops to match ONNX spec on the permutation for how the depth dimension is placed as block dimension.\n- Added support for exporting `alpha` attribute in `ELU` ONNX op.\n- Major overhaul to `Convolution` and `Pooling` export. Unlike before, these ops do not export an explicit `Pad` op in any situation.\n- Major overhaul to `ConvolutionTranspose` export and import. Attributes such as `output_shape`, `output_padding`, and `pads` are fully supported.\n- Added support for CNTK''s `StopGradient` as a no-op.\n- Added ONNX support for TopK op.\n- Added ONNX support for sequence ops: sequence.slice, sequence.first, sequence.last, sequence.reduce_sum, sequence.reduce_max, sequence.softmax. For these ops, there is no need to expand ONNX spec. CNTK ONNX exporter just builds computation equivalent graphs for these sequence ops.\n- Added full support for Softmax op.\n- Made CNTK broadcast ops compatible with ONNX specification.\n- Handle to_batch, to_sequence, unpack_batch, sequence.unpack ops in CNTK ONNX exporter.\n- ONNX tests to export ONNX test cases for other toolkits to run and to validate.\n- Fixed `Hardmax`/`Softmax`/`LogSoftmax` import/export.\n- Added support for `Select` op export.\n- Added import/export support for several trigonometric ops.\n- Updated CNTK support for ONNX `MatMul` op.\n- Updated CNTK support for ONNX `Gemm` op.\n- Updated CNTK''s ONNX `MeanVarianceNormalization` op export/import to latest spec.\n- Updated CNTK''s ONNX `LayerNormalization` op export/import to latest spec.\n- Updated CNTK''s ONNX `PRelu` op export/import to latest spec.\n- Updated CNTK''s ONNX `Gather` op export/import to latest spec.\n- Updated CNTK''s ONNX `ImageScaler` op export/import to latest spec.\n- Updated CNTK''s ONNX `Reduce` ops export/import to latest spec.\n- Updated CNTK''s ONNX `Flatten` op export/import to latest spec.\n- Added CNTK support for ONNX `Unsqueeze` op.\n\n### Bug or minor fixes:\n- Updated LRN op to match ONNX 1.2 spec where the `size` attribute has the semantics of diameter, not radius. Added validation if LRN kernel size is larger than channel size.\n- Updated `Min`/`Max` import implementation to handle variadic inputs.\n- Fixed possible file corruption when resaving on top of existing ONNX model file.\n\n## .Net Support\nThe Cntk.Core.Managed library has officially been converted to .Net Standard and supports .Net Core and .Net Framework applications on both Windows and Linux. Starting from this release, .Net developers should be able to restore CNTK Nuget packages using new .Net SDK style project file with package management format set to PackageReference.\n\nThe following C# code now works on both Windows and Linux:\n\n     >>> var weightParameterName = "weight";\n	 >>> var biasParameterName = "bias";\n	 >>> var inputName = "input";\n	 >>> var outputDim = 2;\n	 >>> var inputDim = 3;\n	 >>> Variable inputVariable = Variable.InputVariable(new int[] { inputDim }, DataType.Float, inputName);\n	 >>> var weightParameter = new Parameter(new int[] { outputDim, inputDim }, DataType.Float, 1, device, weightParameterName);\n	 >>> var biasParameter = new Parameter(new int[] { outputDim }, DataType.Float, 0, device, biasParameterName);\n	 >>> \n     >>> Function modelFunc = CNTKLib.Times(weightParameter, inputVariable) + biasParameter;\n\nFor example, simply adding an ItemGroup clause in the .csproj file of a .Net Core application is sufficient:\n     >>> <Project Sdk="Microsoft.NET.Sdk">\n     >>>\n     >>>   <PropertyGroup>\n     >>>     <TargetFramework>netcoreapp2.1</TargetFramework>\n     >>>     <Platforms>x64</Platforms>\n     >>>   </PropertyGroup>\n     >>>\n     >>>   <ItemGroup>\n     >>>     <PackageReference Include="CNTK.GPU" Version="2.6.0" />\n     >>>   </ItemGroup>\n     >>>\n     >>> </Project>\n\n### Bug or minor fixes:\n- Fixed C# string and char to native wstring and wchar UTF conversion issues on Linux.\n- Fixed multibyte and wide character conversions across the codebase.\n- Fixed Nuget package mechanism to pack for .Net Standard.\n- Fixed a memory leak issue in Value class in C# API where Dispose was not called upon object destruction.\n\n## Misc\n\n\n***2018-04-16.*** CNTK 2.5.1\n\nRepack CNTK 2.5 with third party libraries included in the bundles (Python wheel packages)\n\n---\n\n***2018-03-15.*** CNTK 2.5\n\nChange profiler details output format to be `chrome://tracing`\n\nEnable per-node timing. Working example [here](/Examples/Image/Classification/MLP/Python/SimpleMNIST.py)\n* per-node timing creates items in profiler details when profiler is enabled.\n* usage in Python:\n\n```python\nimport cntk as C\nC.debugging.debug.set_node_timing(True)\nC.debugging.start_profiler() # optional\nC.debugging.enable_profiler() # optional\n#<trainer|evaluator|function> executions\n<trainer|evaluator|function>.print_node_timing()\nC.debugging.stop_profiler()\n```\n\nExample profiler details view in `chrome://tracing`\n![ProfilerDetailWithNodeTiming](https://cntk.ai/Images/ProfilerDetailWithNodeTiming.jpg)\n\nCPU inference performance improvements using MKL\n* Accelerates some common tensor ops in Intel CPU inference for float32, especially for fully connected networks\n* Can be turned on/off by `cntk.cntk_py.enable_cpueval_optimization()/cntk.cntk_py.disable_cpueval_optimization()`\n\n1BitSGD incorporated into CNTK\n* `1BitSGD` source code is now available with CNTK license (MIT license) under `Source/1BitSGD/`\n* `1bitsgd` build target was merged into existing gpu target\n\nNew loss function: hierarchical softmax\n* Thanks @yaochengji for the contribution!\n\nDistributed Training with Multiple Learners\n* Trainer now accepts multiple parameter learners for distributed training. With this change, different parameters of a network can be learned by different learners in a single training session. This also facilitates distributed training for GANs. For more information, please refer to the [Basic_GAN_Distributed.py](/Examples/Image/GAN/Basic_GAN_Distributed.py) and the [cntk.learners.distributed_multi_learner_test.py](/bindings/python/cntk/learners/tests/distributed_multi_learner_test.py)\n\nOperators\n* Added `MeanVarianceNormalization` operator. \n\nBug fixes\n* Fixed convergence issue in Tutorial 201B\n* Fixed pooling/unpooling to support free dimension for sequences\n* Fixed crash in `CNTKBinaryFormat` deserializer when crossing sweep boundary\n* Fixed shape inference bug in RNN step function for scalar broadcasting\n* Fixed a build bug when `mpi=no`\n* Improved distributed training aggregation speed by increasing packing threshold, and expose the knob in V2\n* Fixed a memory leak in MKL layout\n* Fixed a bug in `cntk.convert` API in `misc.converter.py`, which prevents converting complex networks.\n\nONNX\n* Updates\n    * CNTK exported ONNX models are now `ONNX.checker` compliant. \n    * Added ONNX support for CNTKâ€™s `OptimizedRNNStack` operator (LSTM only).\n    * Added support for LSTM and GRU operators\n    * Added support for experimental ONNX op `MeanVarianceNormalization`.\n    * Added support for experimental ONNX op `Identity`.\n    * Added support for exporting CNTKâ€™s `LayerNormalization` layer using ONNX `MeanVarianceNormalization` op.\n* Bug or minor fixes:\n    * Axis attribute is optional in CNTKâ€™s ONNX `Concat` operator.\n    * Bug fix in ONNX broadcasting for scalars.\n    * Bug fix in ONNX ConvTranspose operator. \n    * Backward compatibility bug fix in `LeakyReLu` (argument â€˜alphaâ€™ reverted to type double).\n\nMisc\n* Added a new API `find_by_uid()` under `cntk.logging.graph`. \n\n---\n\n***2018-02-28.*** CNTK supports nightly build\n\nIf you prefer to use latest CNTK bits from master, use one of the CNTK nightly package.\n* [Nightly packages for Windows](https://cntk.ai/nightly-windows.html)\n* [Nightly packages for Linux](https://cntk.ai/nightly-linux.html)\n\nAlternatively, you can also click corresponding build badge to land to nightly build page.\n\n---\n\n***2018-01-31.* CNTK 2.4**\n\nHighlights:\n* Moved to CUDA9, cuDNN 7 and Visual Studio 2017.\n* Removed Python 3.4 support.\n* Added Volta GPU and FP16 support.\n* Better ONNX support.\n* CPU perf improvement.\n* More OPs.\n\nOPs\n* `top_k` operation: in the forward pass it computes the top (largest) k values and corresponding indices along the specified axis. In the backward pass the gradient is scattered to the top k elements (an element not in the top k gets a zero gradient).\n* `gather` operation now supports an axis argument\n* `squeeze` and `expand_dims` operations for easily removing and adding singleton axes\n* `zeros_like` and `ones_like` operations. In many situations you can just rely on CNTK correctly broadcasting a simple 0 or 1 but sometimes you need the actual tensor.\n* `depth_to_space`: Rearranges elements in the input tensor from the depth dimension into spatial blocks. Typical use of this operation is for implementing sub-pixel convolution for some image super-resolution models.\n* `space_to_depth`: Rearranges elements in the input tensor from the spatial dimensions to the depth dimension. It is largely the inverse of DepthToSpace.\n* `sum` operation: Create a new Function instance that computes element-wise sum of input tensors.\n* `softsign` operation: Create a new Function instance that computes the element-wise softsign of a input tensor.\n* `asinh` operation: Create a new Function instance that computes the element-wise asinh of a input tensor.\n* `log_softmax` operation: Create a new Function instance that computes the logsoftmax normalized values of a input tensor.\n* `hard_sigmoid` operation: Create a new Function instance that computes the hard_sigmoid normalized values of a input tensor.\n* `element_and`, `element_not`, `element_or`, `element_xor` element-wise logic operations\n* `reduce_l1` operation: Computes the L1 norm of the input tensor''s element along the provided axes.\n* `reduce_l2` operation: Computes the L2 norm of the input tensor''s element along the provided axes.\n* `reduce_sum_square` operation: Computes the sum square of the input tensor''s element along the provided axes.\n* `image_scaler` operation: Alteration of image by scaling its individual values.\n\nONNX\n* There have been several improvements to ONNX support in CNTK.\n* Updates\n  * Updated ONNX `Reshape` op to handle `InferredDimension`.\n  * Adding `producer_name` and `producer_version` fields to ONNX models.\n  * Handling the case when neither `auto_pad` nor `pads` atrribute is specified in ONNX `Conv` op.\n* Bug fixes\n  * Fixed bug in ONNX `Pooling` op serialization\n  * Bug fix to create ONNX `InputVariable` with only one batch axis.\n  * Bug fixes and updates to implementation of ONNX `Transpose` op to match updated spec.\n  * Bug fixes and updates to implementation of ONNX `Conv`, `ConvTranspose`, and `Pooling` ops to match updated spec.\n\nOperators\n* Group convolution\n  * Fixed bug in group convolution. Output of CNTK `Convolution` op will change for groups > 1. More optimized implementation of group convolution is expected in the next release.\n  * Better error reporting for group convolution in `Convolution` layer.\n\nHalide Binary Convolution\n- The CNTK build can now use optional [Halide](http://halide-lang.org/) libraries to build `Cntk.BinaryConvolution.so/dll` library that can be used with the `netopt` module. The library contains optimized binary convolution operators that perform better than the python based binarized convolution operators. To enable Halide in the build, please download [Halide release](https://github.com/halide/Halide/releases) and set `HALIDE_PATH` environment varibale before starting a build. In Linux, you can use `./configure --with-halide[=directory]` to enable it. For more information on how to use this feature, please refer to [How_to_use_network_optimization](https://github.com/Microsoft/CNTK/blob/master/Manual/Manual_How_to_use_network_optimizations.ipynb).\n\nSee more in the [Release Notes](https://docs.microsoft.com/en-us/cognitive-toolkit/ReleaseNotes/CNTK_2_4_Release_Notes).\nGet the Release from the [CNTK Releases page](https://github.com/Microsoft/CNTK/releases).\n', '{"language":"C++","stars":17601,"forks":4262,"watchers":17601,"open_issues":839,"topics":["c-plus-plus","c-sharp","cntk","cognitive-toolkit","deep-learning","deep-neural-networks","distributed","java","machine-learning","neural-network","python"],"default_branch":"master","size_kb":935217,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:Microsoft:CNTK","source_url":"https://github.com/Microsoft/CNTK"},{"type":"has_code","target_id":"github:lutzroeder:netron","source_url":"https://github.com/lutzroeder/netron"},{"type":"has_code","target_id":"github:halide:Halide","source_url":"https://github.com/halide/Halide"},{"type":"has_code","target_id":"github:Microsoft:CNTK","source_url":"https://github.com/Microsoft/CNTK"},{"type":"has_code","target_id":"github:Microsoft:CNTK","source_url":"https://github.com/Microsoft/CNTK"}]', NULL, 'NOASSERTION', 'approved', 80, 'c6ecf367a378350939a2af57379e79b1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-CNTK from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-CNTK.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-NLP-LOVE-ML-NLP', 'github--nlp-love--ml-nlp', 'ML-NLP', 'NLP-LOVE', '- æ­¤é¡¹ç›®æ˜¯**æœºå™¨å­¦ä¹ ã€NLPé¢è¯•**ä¸­å¸¸è€ƒåˆ°çš„**çŸ¥è¯†ç‚¹å’Œä»£ç å®ç°**ï¼Œä¹Ÿæ˜¯ä½œä¸ºä¸€ä¸ªç®—æ³•å·¥ç¨‹å¸ˆå¿…ä¼šçš„ç†è®ºåŸºç¡€çŸ¥è¯†ã€‚ - æ—¢ç„¶æ˜¯ä»¥é¢è¯•ä¸ºä¸»è¦ç›®çš„ï¼Œäº¦ä¸å¯ä»¥ç¯‡æ¦‚å…¨ï¼Œè¯·è°…è§£ï¼Œæœ‰é—®é¢˜å¯æå‡ºã€‚ - æ­¤é¡¹ç›®ä»¥å„ä¸ªæ¨¡å—ä¸ºåˆ‡å…¥ç‚¹ï¼Œè®©å¤§å®¶æœ‰ä¸€ä¸ªæ¸…æ™°çš„çŸ¥è¯†ä½“ç³»ã€‚ - æ­¤é¡¹ç›®äº¦å¯æ‹¿æ¥å¸¸è¯»ã€å¸¸è®°ä»¥åŠé¢è¯•æ—¶å¤ä¹ ä¹‹ç”¨ã€‚ - æ¯ä¸€ç« é‡Œçš„é—®é¢˜éƒ½æ˜¯é¢è¯•æ—¶æœ‰å¯èƒ½é—®åˆ°çš„çŸ¥è¯†ç‚¹ï¼Œå¦‚æœ‰é—æ¼å¯è”ç³»æˆ‘è¿›è¡Œè¡¥å……ï¼Œç»“å°¾å¤„éƒ½æœ‰ç®—æ³•çš„**å®æˆ˜ä»£ç æ¡ˆä¾‹**ã€‚ - æ€ç»´å¯¼å›¾ï¼Œ**è¯·å…³æ³¨ AIArea å…¬ä¼—å·å¹¶å›å¤ï¼šNLPæ€ç»´å¯¼å›¾** ï¼Œå³èƒ½ä¸‹è½½é«˜æ¸…å¤§å›¾ã€‚ - ------ - **é¡¹ç›®æŒç»­æ›´æ–°ä¸­......** | æ¨¡å— | ç« èŠ‚ | è´Ÿè´£äºº(GitHub) | è”ç³»QQ | | -------- | ------------------------------------------------------------ | --------------------------------------- | --------- | | æœºå™¨å­¦ä¹  | 1. çº¿æ€§å›å½’(Liner Regression) | @mantchs | 448...', '["deep-learning","machine-learning","nlp","jupyter notebook"]', 'other', 17361, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/NLP-LOVE/ML-NLP","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '## é¡¹ç›®ä»‹ç»\n\n- æ­¤é¡¹ç›®æ˜¯**æœºå™¨å­¦ä¹ ã€NLPé¢è¯•**ä¸­å¸¸è€ƒåˆ°çš„**çŸ¥è¯†ç‚¹å’Œä»£ç å®ç°**ï¼Œä¹Ÿæ˜¯ä½œä¸ºä¸€ä¸ªç®—æ³•å·¥ç¨‹å¸ˆå¿…ä¼šçš„ç†è®ºåŸºç¡€çŸ¥è¯†ã€‚\n- æ—¢ç„¶æ˜¯ä»¥é¢è¯•ä¸ºä¸»è¦ç›®çš„ï¼Œäº¦ä¸å¯ä»¥ç¯‡æ¦‚å…¨ï¼Œè¯·è°…è§£ï¼Œæœ‰é—®é¢˜å¯æå‡ºã€‚\n- æ­¤é¡¹ç›®ä»¥å„ä¸ªæ¨¡å—ä¸ºåˆ‡å…¥ç‚¹ï¼Œè®©å¤§å®¶æœ‰ä¸€ä¸ªæ¸…æ™°çš„çŸ¥è¯†ä½“ç³»ã€‚\n- æ­¤é¡¹ç›®äº¦å¯æ‹¿æ¥å¸¸è¯»ã€å¸¸è®°ä»¥åŠé¢è¯•æ—¶å¤ä¹ ä¹‹ç”¨ã€‚\n- æ¯ä¸€ç« é‡Œçš„é—®é¢˜éƒ½æ˜¯é¢è¯•æ—¶æœ‰å¯èƒ½é—®åˆ°çš„çŸ¥è¯†ç‚¹ï¼Œå¦‚æœ‰é—æ¼å¯è”ç³»æˆ‘è¿›è¡Œè¡¥å……ï¼Œç»“å°¾å¤„éƒ½æœ‰ç®—æ³•çš„**å®æˆ˜ä»£ç æ¡ˆä¾‹**ã€‚\n- æ€ç»´å¯¼å›¾ï¼Œ**è¯·å…³æ³¨ AIArea å…¬ä¼—å·å¹¶å›å¤ï¼šNLPæ€ç»´å¯¼å›¾** ï¼Œå³èƒ½ä¸‹è½½é«˜æ¸…å¤§å›¾ã€‚\n- ![](https://github.com/NLP-LOVE/Introduction-NLP/raw/master/img/2020-2-3_16-0-25.png?raw=true)\n\n\n------\n\n\n\n## ç›®å½•\n\n- **é¡¹ç›®æŒç»­æ›´æ–°ä¸­......**\n\n| æ¨¡å—     | ç« èŠ‚                                                         | è´Ÿè´£äºº(GitHub)                          | è”ç³»QQ    |\n| -------- | ------------------------------------------------------------ | --------------------------------------- | --------- |\n| æœºå™¨å­¦ä¹  | [1. çº¿æ€§å›å½’(Liner Regression)](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/Liner%20Regression/1.Liner%20Regression.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [2. é€»è¾‘å›å½’(Logistics Regression)](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/2.Logistics%20Regression/2.Logistics%20Regression.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [3. å†³ç­–æ ‘(Desision Tree)](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.Desition%20Tree/Desition%20Tree.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [3.1 éšæœºæ£®æ—(Random Forest)](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.1%20Random%20Forest/3.1%20Random%20Forest.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [3.2 æ¢¯åº¦æå‡å†³ç­–æ ‘(GBDT)](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.2%20GBDT/3.2%20GBDT.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [3.3 XGBoost](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.3%20XGBoost/3.3%20XGBoost.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [3.4 LightGBM](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.4%20LightGBM/3.4%20LightGBM.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [4. æ”¯æŒå‘é‡æœº(SVM)](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/4.%20SVM/4.%20SVM.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | 5. æ¦‚ç‡å›¾æ¨¡å‹(Probabilistic Graphical Model)                 |                                         |           |\n| æœºå™¨å­¦ä¹  | [5.1 è´å¶æ–¯ç½‘ç»œ(Bayesian Network)](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/5.1%20Bayes%20Network/5.1%20Bayes%20Network.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [5.2 é©¬å°”ç§‘å¤«(Markov)](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/5.2%20Markov/5.2%20Markov.md) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [5.3 ä¸»é¢˜æ¨¡å‹(Topic Model)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/5.3%20Topic%20Model) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [6.æœ€å¤§æœŸæœ›ç®—æ³•(EM)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/6.%20EM) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [7.èšç±»(Clustering)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/7.%20Clustering) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [8.MLç‰¹å¾å·¥ç¨‹å’Œä¼˜åŒ–æ–¹æ³•](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/8.%20ML%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E5%92%8C%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æœºå™¨å­¦ä¹  | [9.Kè¿‘é‚»ç®—æ³•(KNN)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/9.%20KNN) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æ·±åº¦å­¦ä¹  | [10.ç¥ç»ç½‘ç»œ(Neural Network)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/10.%20Neural%20Network) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æ·±åº¦å­¦ä¹  | [11. å·ç§¯ç¥ç»ç½‘ç»œ(CNN)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/11.%20CNN) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æ·±åº¦å­¦ä¹  | [12. å¾ªç¯ç¥ç»ç½‘ç»œ(RNN)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/12.%20RNN) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æ·±åº¦å­¦ä¹  | [12.1 é—¨æ§å¾ªç¯å•å…ƒ(GRU)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/12.1%20GRU) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æ·±åº¦å­¦ä¹  | [12.2 é•¿çŸ­æœŸè®°å¿†(LSTM)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/12.2%20LSTM) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æ·±åº¦å­¦ä¹  | [13.è¿ç§»å­¦ä¹ (Transfer)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/13.%20Transfer%20Learning) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æ·±åº¦å­¦ä¹  | [14.å¼ºåŒ–å­¦ä¹ (Reinforcement) & å¤šä»»åŠ¡](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/14.%20Reinforcement%20Learning) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| æ·±åº¦å­¦ä¹  | [15. æ·±åº¦å­¦ä¹ çš„ä¼˜åŒ–æ–¹æ³•](https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/15.%20DL%20Optimizer) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16. è‡ªç„¶è¯­è¨€å¤„ç†(NLP)](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.%20NLP) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.1 è¯åµŒå…¥(Word2Vec)](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.1%20Word%20Embedding) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.2 å­è¯åµŒå…¥(fastText)](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.2%20fastText) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.3 å…¨å±€å‘é‡è¯åµŒå…¥(GloVe)](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.3%20GloVe) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.4 textRNN & textCNN](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.4%20textRNN%20%26%20textCNN) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.5 åºåˆ—åˆ°åºåˆ—æ¨¡å‹(seq2seq)](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.5%20seq2seq) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.6 æ³¨æ„åŠ›æœºåˆ¶(Attention Mechanism)](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.6%20Attention) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.7 Transformeræ¨¡å‹](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.7%20Transformer) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.8 BERTæ¨¡å‹](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.8%20BERT) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| NLP      | [16.9 XLNetæ¨¡å‹](https://github.com/NLP-LOVE/ML-NLP/tree/master/NLP/16.9%20XLNet) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| é¡¹ç›®     | [17. æ¨èç³»ç»Ÿ(Recommendation System)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Project/17.%20Recommendation%20System) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| é¡¹ç›®     | [18. æ™ºèƒ½å®¢æœ(Intelligent Customer Service)](https://github.com/NLP-LOVE/ML-NLP/tree/master/Project/18.%20Intelligent%20Customer%20Service) | [@mantchs](https://github.com/NLP-LOVE) | 448966528 |\n| é¡¹ç›®     | 19. çŸ¥è¯†å›¾è°±(Knowledge Graph)                                |                                         |           |\n| é¡¹ç›®     | 20. è¯„è®ºåˆ†æ                                                 |                                         |           |\n\n\n\n> æ¬¢è¿å¤§å®¶åŠ å…¥ï¼å…±åŒå®Œå–„æ­¤é¡¹ç›®ï¼NLPå­¦ä¹ QQ2ç¾¤ã€207576902ã€‘<a target="_blank" href="http://shang.qq.com/wpa/qunwpa?idkey=1defd70810d9e67ca6ab3a30e1425a8a358139315a186dd2192d82a4c0ca1ce9"><img border="0" src="http://pub.idqqimg.com/wpa/images/group.png" alt="NLPå­¦ä¹ ç¾¤â‘¡" title="NLPå­¦ä¹ ç¾¤â‘¡"></a>\n', '{"language":"Jupyter Notebook","stars":17361,"forks":4647,"watchers":17361,"open_issues":37,"topics":["deep-learning","machine-learning","nlp"],"default_branch":"master","size_kb":11965,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:NLP-LOVE:Introduction-NLP","source_url":"https://github.com/NLP-LOVE/Introduction-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"},{"type":"has_code","target_id":"github:NLP-LOVE:ML-NLP","source_url":"https://github.com/NLP-LOVE/ML-NLP"}]', NULL, NULL, 'pending', 55, '8de718eed45fb1c387dc8800961aaf61', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-NLP-LOVE-ML-NLP from https://github.com/NLP-LOVE.png
Image converted to WebP: data/images/github-NLP-LOVE-ML-NLP.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ujjwalkarn-Machine-Learning-Tutorials', 'github--ujjwalkarn--machine-learning-tutorials', 'Machine-Learning-Tutorials', 'ujjwalkarn', '- This repository contains a topic-wise curated list of Machine Learning and Deep Learning tutorials, articles and other resources. Other awesome lists can be found in this list. - If you want to contribute to this list, please read Contributing Guidelines. - Curated list of R tutorials for Data Science, NLP and Machine Learning. - Curated list of Python tutorials for Data Science, NLP and Machine Learning. - Introduction - Interview Resources - Artificial Intelligence - Genetic Algorithms - ...', '["awesome","awesome-list","deep-learning","deep-learning-tutorial","deep-neural-networks","deeplearning","list","machine-learning","machinelearning","neural-network","neural-networks"]', 'other', 17250, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ujjwalkarn/Machine-Learning-Tutorials","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '\n# Machine Learning & Deep Learning Tutorials [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n\n- This repository contains a topic-wise curated list of Machine Learning and Deep Learning tutorials, articles and other resources. Other awesome lists can be found in this [list](https://github.com/sindresorhus/awesome).\n\n- If you want to contribute to this list, please read [Contributing Guidelines](https://github.com/ujjwalkarn/Machine-Learning-Tutorials/blob/master/contributing.md).\n\n- [Curated list of R tutorials for Data Science, NLP and Machine Learning](https://github.com/ujjwalkarn/DataScienceR).\n\n- [Curated list of Python tutorials for Data Science, NLP and Machine Learning](https://github.com/ujjwalkarn/DataSciencePython).\n\n\n## Contents\n- [Introduction](#general)\n- [Interview Resources](#interview)\n- [Artificial Intelligence](#ai)\n- [Genetic Algorithms](#ga)\n- [Statistics](#stat)\n- [Useful Blogs](#blogs)\n- [Resources on Quora](#quora)\n- [Resources on Kaggle](#kaggle)\n- [Cheat Sheets](#cs)\n- [Classification](#classification)\n- [Linear Regression](#linear)\n- [Logistic Regression](#logistic)\n- [Model Validation using Resampling](#validation)\n    - [Cross Validation](#cross)\n    - [Bootstraping](#boot)\n- [Deep Learning](#deep)\n    - [Frameworks](#frame)\n    - [Feed Forward Networks](#feed)\n    - [Recurrent Neural Nets, LSTM, GRU](#rnn)\n    - [Restricted Boltzmann Machine, DBNs](#rbm)\n    - [Autoencoders](#auto)\n    - [Convolutional Neural Nets](#cnn)\n    - [Graph Representation Learning](#nrl)\n- [Natural Language Processing](#nlp)\n    - [Topic Modeling, LDA](#topic)\n    - [Word2Vec](#word2vec)\n- [Computer Vision](#vision)\n- [Support Vector Machine](#svm)\n- [Reinforcement Learning](#rl)\n- [Decision Trees](#dt)\n- [Random Forest / Bagging](#rf)\n- [Boosting](#gbm)\n- [Ensembles](#ensem)\n- [Stacking Models](#stack)\n- [VC Dimension](#vc)\n- [Bayesian Machine Learning](#bayes)\n- [Semi Supervised Learning](#semi)\n- [Optimizations](#opt)\n- [Other Useful Tutorials](#other)\n\n<a name="general" />\n\n## Introduction\n\n- [Machine Learning Course by Andrew Ng (Stanford University)](https://www.coursera.org/learn/machine-learning)\n\n- [AI/ML YouTube Courses](https://github.com/dair-ai/ML-YouTube-Courses)\n\n- [Curated List of Machine Learning Resources](https://hackr.io/tutorials/learn-machine-learning-ml)\n\n- [In-depth introduction to machine learning in 15 hours of expert videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/)\n\n- [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)\n\n- [List of Machine Learning University Courses](https://github.com/prakhar1989/awesome-courses#machine-learning)\n\n- [Machine Learning for Software Engineers](https://github.com/ZuzooVn/machine-learning-for-software-engineers)\n\n- [Dive into Machine Learning](https://github.com/hangtwenty/dive-into-machine-learning)\n\n- [A curated list of awesome Machine Learning frameworks, libraries and software](https://github.com/josephmisiti/awesome-machine-learning)\n\n- [A curated list of awesome data visualization libraries and resources.](https://github.com/fasouto/awesome-dataviz)\n\n- [An awesome Data Science repository to learn and apply for real world problems](https://github.com/okulbilisim/awesome-datascience)\n\n- [The Open Source Data Science Masters](http://datasciencemasters.org/)\n\n- [Machine Learning FAQs on Cross Validated](http://stats.stackexchange.com/questions/tagged/machine-learning)\n\n- [Machine Learning algorithms that you should always have a strong understanding of](https://www.quora.com/What-are-some-Machine-Learning-algorithms-that-you-should-always-have-a-strong-understanding-of-and-why)\n\n- [Difference between Linearly Independent, Orthogonal, and Uncorrelated Variables](http://terpconnect.umd.edu/~bmomen/BIOM621/LineardepCorrOrthogonal.pdf)\n\n- [List of Machine Learning Concepts](https://en.wikipedia.org/wiki/List_of_machine_learning_concepts)\n\n- [Slides on Several Machine Learning Topics](http://www.slideshare.net/pierluca.lanzi/presentations)\n\n- [MIT Machine Learning Lecture Slides](http://www.ai.mit.edu/courses/6.867-f04/lectures.html)\n\n- [Comparison Supervised Learning Algorithms](http://www.dataschool.io/comparing-supervised-learning-algorithms/)\n\n- [Learning Data Science Fundamentals](http://www.dataschool.io/learning-data-science-fundamentals/)\n\n- [Machine Learning mistakes to avoid](https://medium.com/@nomadic_mind/new-to-machine-learning-avoid-these-three-mistakes-73258b3848a4#.lih061l3l)\n\n- [Statistical Machine Learning Course](http://www.stat.cmu.edu/~larry/=sml/)\n\n- [TheAnalyticsEdge edX Notes and Codes](https://github.com/pedrosan/TheAnalyticsEdge)\n\n- [Have Fun With Machine Learning](https://github.com/humphd/have-fun-with-machine-learning)\n\n- [Twitter''s Most Shared #machineLearning Content From The Past 7 Days](http://theherdlocker.com/tweet/popularity/machinelearning)\n\n- [Grokking Machine Learning](https://www.manning.com/books/grokking-machine-learning)\n\n<a name="interview" />\n\n## Interview Resources\n\n- [41 Essential Machine Learning Interview Questions (with answers)](https://www.springboard.com/blog/machine-learning-interview-questions/)\n\n- [How can a computer science graduate student prepare himself for data scientist interviews?](https://www.quora.com/How-can-a-computer-science-graduate-student-prepare-himself-for-data-scientist-machine-learning-intern-interviews)\n\n- [How do I learn Machine Learning?](https://www.quora.com/How-do-I-learn-machine-learning-1)\n\n- [FAQs about Data Science Interviews](https://www.quora.com/topic/Data-Science-Interviews/faq)\n\n- [What are the key skills of a data scientist?](https://www.quora.com/What-are-the-key-skills-of-a-data-scientist)\n\n- [The Big List of DS/ML Interview Resources](https://towardsdatascience.com/the-big-list-of-ds-ml-interview-resources-2db4f651bd63)\n\n<a name="ai" />\n\n## Artificial Intelligence\n\n- [Awesome Artificial Intelligence (GitHub Repo)](https://github.com/owainlewis/awesome-artificial-intelligence)\n\n- [UC Berkeley CS188 Intro to AI](http://ai.berkeley.edu/home.html), [Lecture Videos](http://ai.berkeley.edu/lecture_videos.html), [2](https://www.youtube.com/watch?v=W1S-HSakPTM)\n\n- [Programming Community Curated Resources for learning Artificial Intelligence](https://hackr.io/tutorials/learn-artificial-intelligence-ai) \n\n- [MIT 6.034 Artificial Intelligence Lecture Videos](https://www.youtube.com/playlist?list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi), [Complete Course](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/)\n\n- [edX course | Klein & Abbeel](https://courses.edx.org/courses/BerkeleyX/CS188x_1/1T2013/info)\n\n- [Udacity Course | Norvig & Thrun](https://www.udacity.com/course/intro-to-artificial-intelligence--cs271)\n\n- [TED talks on AI](http://www.ted.com/playlists/310/talks_on_artificial_intelligen)\n\n<a name="ga" />\n\n## Genetic Algorithms\n\n- [Genetic Algorithms Wikipedia Page](https://en.wikipedia.org/wiki/Genetic_algorithm)\n\n- [Simple Implementation of Genetic Algorithms in Python (Part 1)](http://outlace.com/miniga.html), [Part 2](http://outlace.com/miniga_addendum.html)\n\n- [Genetic Algorithms vs Artificial Neural Networks](http://stackoverflow.com/questions/1402370/when-to-use-genetic-algorithms-vs-when-to-use-neural-networks)\n\n- [Genetic Algorithms Explained in Plain English](http://www.ai-junkie.com/ga/intro/gat1.html)\n\n- [Genetic Programming](https://en.wikipedia.org/wiki/Genetic_programming)\n\n    - [Genetic Programming in Python (GitHub)](https://github.com/trevorstephens/gplearn)\n    \n    - [Genetic Alogorithms vs Genetic Programming (Quora)](https://www.quora.com/Whats-the-difference-between-Genetic-Algorithms-and-Genetic-Programming), [StackOverflow](http://stackoverflow.com/questions/3819977/what-are-the-differences-between-genetic-algorithms-and-genetic-programming)\n\n<a name="stat" />\n\n## Statistics\n\n- [Stat Trek Website](http://stattrek.com/) - A dedicated website to teach yourselves Statistics\n\n- [Learn Statistics Using Python](https://github.com/rouseguy/intro2stats) - Learn Statistics using an application-centric programming approach\n\n- [Statistics for Hackers | Slides | @jakevdp](https://speakerdeck.com/jakevdp/statistics-for-hackers) - Slides by Jake VanderPlas\n\n- [Online Statistics Book](http://onlinestatbook.com/2/index.html) - An Interactive Multimedia Course for Studying Statistics\n\n- [What is a Sampling Distribution?](http://stattrek.com/sampling/sampling-distribution.aspx)\n\n- Tutorials\n\n    - [AP Statistics Tutorial](http://stattrek.com/tutorials/ap-statistics-tutorial.aspx)\n    \n    - [Statistics and Probability Tutorial](http://stattrek.com/tutorials/statistics-tutorial.aspx)\n    \n    - [Matrix Algebra Tutorial](http://stattrek.com/tutorials/matrix-algebra-tutorial.aspx)\n    \n- [What is an Unbiased Estimator?](https://www.physicsforums.com/threads/what-is-an-unbiased-estimator.547728/)\n\n- [Goodness of Fit Explained](https://en.wikipedia.org/wiki/Goodness_of_fit)\n\n- [What are QQ Plots?](http://onlinestatbook.com/2/advanced_graphs/q-q_plots.html)\n\n- [OpenIntro Statistics](https://www.openintro.org/stat/textbook.php?stat_book=os) - Free PDF textbook\n\n<a name="blogs" />\n\n## Useful Blogs\n\n- [Edwin Chen''s Blog](http://blog.echen.me/) - A blog about Math, stats, ML, crowdsourcing, data science\n\n- [The Data School Blog](http://www.dataschool.io/) - Data science for beginners!\n\n- [ML Wave](http://mlwave.com/) - A blog for Learning Machine Learning\n\n- [Andrej Karpathy](http://karpathy.github.io/) - A blog about Deep Learning and Data Science in general\n\n- [Colah''s Blog](http://colah.github.io/) - Awesome Neural Networks Blog\n\n- [Alex Minnaar''s Blog](http://alexminnaar.com/) - A blog about Machine Learning and Software Engineering\n\n- [Statistically Significant](http://andland.github.io/) - Andrew Landgraf''s Data Science Blog\n\n- [Simply Statistics](http://simplystatistics.org/) - A blog by three biostatistics professors\n\n- [Yanir Seroussi''s Blog](https://yanirseroussi.com/) - A blog about Data Science and beyond\n\n- [fastML](http://fastml.com/) - Machine learning made easy\n\n- [Trevor Stephens Blog](http://trevorstephens.com/) - Trevor Stephens Personal Page\n\n- [no free hunch | kaggle](http://blog.kaggle.com/) - The Kaggle Blog about all things Data Science\n\n- [A Quantitative Journey | outlace](http://outlace.com/) -  learning quantitative applications\n\n- [r4stats](http://r4stats.com/) - analyze the world of data science, and to help people learn to use R\n\n- [Variance Explained](http://varianceexplained.org/) - David Robinson''s Blog\n\n- [AI Junkie](http://www.ai-junkie.com/) - a blog about Artificial Intellingence\n\n- [Deep Learning Blog by Tim Dettmers](http://timdettmers.com/) - Making deep learning accessible\n\n- [J Alammar''s Blog](http://jalammar.github.io/)- Blog posts about Machine Learning and Neural Nets\n\n- [Adam Geitgey](https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471#.f7vwrtfne) - Easiest Introduction to machine learning\n\n- [Ethen''s Notebook Collection](https://github.com/ethen8181/machine-learning) - Continuously updated machine learning documentations (mainly in Python3). Contents include educational implementation of machine learning algorithms from scratch and open-source library usage\n\n<a name="quora" />\n\n## Resources on Quora\n\n- [Most Viewed Machine Learning writers](https://www.quora.com/topic/Machine-Learning/writers)\n\n- [Data Science Topic on Quora](https://www.quora.com/Data-Science)\n\n- [William Chen''s Answers](https://www.quora.com/William-Chen-6/answers)\n\n- [Michael Hochster''s Answers](https://www.quora.com/Michael-Hochster/answers)\n\n- [Ricardo Vladimiro''s Answers](https://www.quora.com/Ricardo-Vladimiro-1/answers)\n\n- [Storytelling with Statistics](https://datastories.quora.com/)\n\n- [Data Science FAQs on Quora](https://www.quora.com/topic/Data-Science/faq)\n\n- [Machine Learning FAQs on Quora](https://www.quora.com/topic/Machine-Learning/faq)\n\n<a name="kaggle" />\n\n## Kaggle Competitions WriteUp\n\n- [How to almost win Kaggle Competitions](https://yanirseroussi.com/2014/08/24/how-to-almost-win-kaggle-competitions/)\n\n- [Convolution Neural Networks for EEG detection](http://blog.kaggle.com/2015/10/05/grasp-and-lift-eeg-detection-winners-interview-3rd-place-team-hedj/)\n\n- [Facebook Recruiting III Explained](http://alexminnaar.com/tag/kaggle-competitions.html)\n\n- [Predicting CTR with Online ML](http://mlwave.com/predicting-click-through-rates-with-online-machine-learning/)\n\n- [How to Rank 10% in Your First Kaggle Competition](https://dnc1994.com/2016/05/rank-10-percent-in-first-kaggle-competition-en/)\n\n<a name="cs" />\n\n## Cheat Sheets\n\n- [Probability Cheat Sheet](http://static1.squarespace.com/static/54bf3241e4b0f0d81bf7ff36/t/55e9494fe4b011aed10e48e5/1441352015658/probability_cheatsheet.pdf),\n[Source](http://www.wzchen.com/probability-cheatsheet/)\n\n- [Machine Learning Cheat Sheet](https://github.com/soulmachine/machine-learning-cheat-sheet)\n\n- [ML Compiled](https://ml-compiled.readthedocs.io/en/latest/)\n\n<a name="classification" />\n\n## Classification\n\n- [Does Balancing Classes Improve Classifier Performance?](http://www.win-vector.com/blog/2015/02/does-balancing-classes-improve-classifier-performance/)\n\n- [What is Deviance?](http://stats.stackexchange.com/questions/6581/what-is-deviance-specifically-in-cart-rpart)\n\n- [When to choose which machine learning classifier?](http://stackoverflow.com/questions/2595176/when-to-choose-which-machine-learning-classifier)\n\n- [What are the advantages of different classification algorithms?](https://www.quora.com/What-are-the-advantages-of-different-classification-algorithms)\n\n- [ROC and AUC Explained](http://www.dataschool.io/roc-curves-and-auc-explained/) ([related video](https://youtu.be/OAl6eAyP-yo))\n\n- [An introduction to ROC analysis](https://ccrma.stanford.edu/workshops/mir2009/references/ROCintro.pdf)\n\n- [Simple guide to confusion matrix terminology](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)\n\n\n<a name="linear" />\n\n## Linear Regression\n\n- [General](#general-)\n\n    - [Assumptions of Linear Regression](http://pareonline.net/getvn.asp?n=2&v=8), [Stack Exchange](http://stats.stackexchange.com/questions/16381/what-is-a-complete-list-of-the-usual-assumptions-for-linear-regression)\n    \n    - [Linear Regression Comprehensive Resource](http://people.duke.edu/~rnau/regintro.htm)\n    \n    - [Applying and Interpreting Linear Regression](http://www.dataschool.io/applying-and-interpreting-linear-regression/)\n    \n    - [What does having constant variance in a linear regression model mean?](http://stats.stackexchange.com/questions/52089/what-does-having-constant-variance-in-a-linear-regression-model-mean/52107?stw=2#52107)\n    \n    - [Difference between linear regression on y with x and x with y](http://stats.stackexchange.com/questions/22718/what-is-the-difference-between-linear-regression-on-y-with-x-and-x-with-y?lq=1)\n    \n    - [Is linear regression valid when the dependant variable is not normally distributed?](https://www.researchgate.net/post/Is_linear_regression_valid_when_the_outcome_dependant_variable_not_normally_distributed)\n- Multicollinearity and VIF\n\n    - [Dummy Variable Trap | Multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)\n    \n    - [Dealing with multicollinearity using VIFs](https://jonlefcheck.net/2012/12/28/dealing-with-multicollinearity-using-variance-inflation-factors/)\n\n- [Residual Analysis](#residuals-)\n\n    - [Interpreting plot.lm() in R](http://stats.stackexchange.com/questions/58141/interpreting-plot-lm)\n    \n    - [How to interpret a QQ plot?](http://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot?lq=1)\n    \n    - [Interpreting Residuals vs Fitted Plot](http://stats.stackexchange.com/questions/76226/interpreting-the-residuals-vs-fitted-values-plot-for-verifying-the-assumptions)\n\n- [Outliers](#outliers-)\n\n    - [How should outliers be dealt with?](http://stats.stackexchange.com/questions/175/how-should-outliers-be-dealt-with-in-linear-regression-analysis)\n\n- [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization)\n    - [Regularization and Variable Selection via the\nElastic Net](https://web.stanford.edu/~hastie/Papers/elasticnet.pdf)\n\n<a name="logistic" />\n\n## Logistic Regression\n\n- [Logistic Regression Wiki](https://en.wikipedia.org/wiki/Logistic_regression)\n\n- [Geometric Intuition of Logistic Regression](http://florianhartl.com/logistic-regression-geometric-intuition.html)\n\n- [Obtaining predicted categories (choosing threshold)](http://stats.stackexchange.com/questions/25389/obtaining-predicted-values-y-1-or-0-from-a-logistic-regression-model-fit)\n\n- [Residuals in logistic regression](http://stats.stackexchange.com/questions/1432/what-do-the-residuals-in-a-logistic-regression-mean)\n\n- [Difference between logit and probit models](http://stats.stackexchange.com/questions/20523/difference-between-logit-and-probit-models#30909), [Logistic Regression Wiki](https://en.wikipedia.org/wiki/Logistic_regression), [Probit Model Wiki](https://en.wikipedia.org/wiki/Probit_model)\n\n- [Pseudo R2 for Logistic Regression](http://stats.stackexchange.com/questions/3559/which-pseudo-r2-measure-is-the-one-to-report-for-logistic-regression-cox-s), [How to calculate](http://stats.stackexchange.com/questions/8511/how-to-calculate-pseudo-r2-from-rs-logistic-regression), [Other Details](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/Psuedo_RSquareds.htm)\n\n- [Guide to an in-depth understanding of logistic regression](http://www.dataschool.io/guide-to-logistic-regression/)\n\n<a name="validation" />\n\n## Model Validation using Resampling\n\n- [Resampling Explained](https://en.wikipedia.org/wiki/Resampling_(statistics))\n\n- [Partioning data set in R](http://stackoverflow.com/questions/13536537/partitioning-data-set-in-r-based-on-multiple-classes-of-observations)\n\n- [Implementing hold-out Validaion in R](http://stackoverflow.com/questions/22972854/how-to-implement-a-hold-out-validation-in-r), [2](http://www.gettinggeneticsdone.com/2011/02/split-data-frame-into-testing-and.html)\n\n<a name="cross" />\n\n- [Cross Validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))\n    - [How to use cross-validation in predictive modeling](http://stuartlacy.co.uk/2016/02/04/how-to-correctly-use-cross-validation-in-predictive-modelling/)\n    - [Training with Full dataset after CV?](http://stats.stackexchange.com/questions/11602/training-with-the-full-dataset-after-cross-validation)\n    \n    - [Which CV method is best?](http://stats.stackexchange.com/questions/103459/how-do-i-know-which-method-of-cross-validation-is-best)\n    \n    - [Variance Estimates in k-fold CV](http://stats.stackexchange.com/questions/31190/variance-estimates-in-k-fold-cross-validation)\n    \n    - [Is CV a subsitute for Validation Set?](http://stats.stackexchange.com/questions/18856/is-cross-validation-a-proper-substitute-for-validation-set)\n    \n    - [Choice of k in k-fold CV](http://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation)\n    \n    - [CV for ensemble learning](http://stats.stackexchange.com/questions/102631/k-fold-cross-validation-of-ensemble-learning)\n    \n    - [k-fold CV in R](http://stackoverflow.com/questions/22909197/creating-folds-for-k-fold-cv-in-r-using-caret)\n    \n    - [Good Resources](http://www.chioka.in/tag/cross-validation/)\n    \n    - Overfitting and Cross Validation\n    \n        - [Preventing Overfitting the Cross Validation Data | Andrew Ng](http://ai.stanford.edu/~ang/papers/cv-final.pdf)\n        \n        - [Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation](http://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf)\n\n        - [CV for detecting and preventing Overfitting](http://www.autonlab.org/tutorials/overfit10.pdf)\n        \n        - [How does CV overcome the Overfitting Problem](http://stats.stackexchange.com/questions/9053/how-does-cross-validation-overcome-the-overfitting-problem)\n\n\n<a name="boot" />\n\n- [Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))\n\n    - [Why Bootstrapping Works?](http://stats.stackexchange.com/questions/26088/explaining-to-laypeople-why-bootstrapping-works)\n    \n    - [Good Animation](https://www.stat.auckland.ac.nz/~wild/BootAnim/)\n    \n    - [Example of Bootstapping](http://statistics.about.com/od/Applications/a/Example-Of-Bootstrapping.htm)\n    \n    - [Understanding Bootstapping for Validation and Model Selection](http://stats.stackexchange.com/questions/14516/understanding-bootstrapping-for-validation-and-model-selection?rq=1)\n    \n    - [Cross Validation vs Bootstrap to estimate prediction error](http://stats.stackexchange.com/questions/18348/differences-between-cross-validation-and-bootstrapping-to-estimate-the-predictio), [Cross-validation vs .632 bootstrapping to evaluate classification performance](http://stats.stackexchange.com/questions/71184/cross-validation-or-bootstrapping-to-evaluate-classification-performance)\n\n\n<a name="deep" />\n\n## Deep Learning\n\n- [fast.ai - Practical Deep Learning For Coders](http://course.fast.ai/)\n\n- [fast.ai - Cutting Edge Deep Learning For Coders](http://course.fast.ai/part2.html)\n\n- [A curated list of awesome Deep Learning tutorials, projects and communities](https://github.com/ChristosChristofidis/awesome-deep-learning)\n\n- **[Deep Learning Papers Reading Roadmap](https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap/blob/master/README.md)**\n\n- [Lots of Deep Learning Resources](http://deeplearning4j.org/documentation.html)\n\n- [Interesting Deep Learning and NLP Projects (Stanford)](http://cs224d.stanford.edu/reports.html), [Website](http://cs224d.stanford.edu/)\n\n- [Core Concepts of Deep Learning](https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/)\n\n- [Understanding Natural Language with Deep Neural Networks Using Torch](https://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)\n\n- [Stanford Deep Learning Tutorial](http://ufldl.stanford.edu/tutorial/)\n\n- [Deep Learning FAQs on Quora](https://www.quora.com/topic/Deep-Learning/faq)\n\n- [Google+ Deep Learning Page](https://plus.google.com/communities/112866381580457264725)\n\n- [Recent Reddit AMAs related to Deep Learning](http://deeplearning.net/2014/11/22/recent-reddit-amas-about-deep-learning/), [Another AMA](https://www.reddit.com/r/IAmA/comments/3mdk9v/we_are_google_researchers_working_on_deep/)\n\n- [Where to Learn Deep Learning?](http://www.kdnuggets.com/2014/05/learn-deep-learning-courses-tutorials-overviews.html)\n\n- [Deep Learning nvidia concepts](http://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/)\n\n- [Introduction to Deep Learning Using Python (GitHub)](https://github.com/rouseguy/intro2deeplearning), [Good Introduction Slides](https://speakerdeck.com/bargava/introduction-to-deep-learning)\n\n- [Video Lectures Oxford 2015](https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu), [Video Lectures Summer School Montreal](http://videolectures.net/deeplearning2015_montreal/)\n\n- [Deep Learning Software List](http://deeplearning.net/software_links/)\n\n- [Hacker''s guide to Neural Nets](http://karpathy.github.io/neuralnets/)\n\n- [Top arxiv Deep Learning Papers explained](http://www.kdnuggets.com/2015/10/top-arxiv-deep-learning-papers-explained.html)\n\n- [Geoff Hinton Youtube Vidoes on Deep Learning](https://www.youtube.com/watch?v=IcOMKXAw5VA)\n\n- [Awesome Deep Learning Reading List](http://deeplearning.net/reading-list/)\n\n- [Deep Learning Comprehensive Website](http://deeplearning.net/), [Software](http://deeplearning.net/software_links/)\n\n- [deeplearning Tutorials](http://deeplearning4j.org/)\n\n- [AWESOME! Deep Learning Tutorial](https://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks)\n\n- [Deep Learning Basics](http://alexminnaar.com/deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html)\n\n- [Intuition Behind Backpropagation](https://medium.com/spidernitt/breaking-down-neural-networks-an-intuitive-approach-to-backpropagation-3b2ff958794c)\n\n- [Stanford Tutorials](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/)\n\n- [Train, Validation & Test in Artificial Neural Networks](http://stackoverflow.com/questions/2976452/whats-is-the-difference-between-train-validation-and-test-set-in-neural-networ)\n\n- [Artificial Neural Networks Tutorials](http://stackoverflow.com/questions/478947/what-are-some-good-resources-for-learning-about-artificial-neural-networks)\n\n- [Neural Networks FAQs on Stack Overflow](http://stackoverflow.com/questions/tagged/neural-network?sort=votes&pageSize=50)\n\n- [Deep Learning Tutorials on deeplearning.net](http://deeplearning.net/tutorial/index.html)\n\n- [Neural Networks and Deep Learning Online Book](http://neuralnetworksanddeeplearning.com/)\n\n- Neural Machine Translation\n\n    - **[Machine Translation Reading List](https://github.com/THUNLP-MT/MT-Reading-List#machine-translation-reading-list)**\n\n    - [Introduction to Neural Machine Translation with GPUs (part 1)](https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-with-gpus/), [Part 2](https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/), [Part 3](https://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/)\n    \n    - [Deep Speech: Accurate Speech Recognition with GPU-Accelerated Deep Learning](https://devblogs.nvidia.com/parallelforall/deep-speech-accurate-speech-recognition-gpu-accelerated-deep-learning/)\n\n<a name="frame" />\n\n- Deep Learning Frameworks\n\n    - [Torch vs. Theano](http://fastml.com/torch-vs-theano/)\n    \n    - [dl4j vs. torch7 vs. theano](http://deeplearning4j.org/compare-dl4j-torch7-pylearn.html)\n    \n    - [Deep Learning Libraries by Language](http://www.teglor.com/b/deep-learning-libraries-language-cm569/)\n    \n\n    - [Theano](https://en.wikipedia.org/wiki/Theano_(software))\n    \n        - [Website](http://deeplearning.net/software/theano/)\n        \n        - [Theano Introduction](http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/)\n        \n        - [Theano Tutorial](http://outlace.com/Beginner-Tutorial-Theano/)\n        \n        - [Good Theano Tutorial](http://deeplearning.net/software/theano/tutorial/)\n        \n        - [Logistic Regression using Theano for classifying digits](http://deeplearning.net/tutorial/logreg.html#logreg)\n        \n        - [MLP using Theano](http://deeplearning.net/tutorial/mlp.html#mlp)\n        \n        - [CNN using Theano](http://deeplearning.net/tutorial/lenet.html#lenet)\n        \n        - [RNNs using Theano](http://deeplearning.net/tutorial/rnnslu.html#rnnslu)\n        \n        - [LSTM for Sentiment Analysis in Theano](http://deeplearning.net/tutorial/lstm.html#lstm)\n        \n        - [RBM using Theano](http://deeplearning.net/tutorial/rbm.html#rbm)\n        \n        - [DBNs using Theano](http://deeplearning.net/tutorial/DBN.html#dbn)\n        \n        - [All Codes](https://github.com/lisa-lab/DeepLearningTutorials)\n        \n        - [Deep Learning Implementation Tutorials - Keras and Lasagne](https://github.com/vict0rsch/deep_learning/)\n\n    - [Torch](http://torch.ch/)\n    \n        - [Torch ML Tutorial](http://code.madbits.com/wiki/doku.php), [Code](https://github.com/torch/tutorials)\n        \n        - [Intro to Torch](http://ml.informatik.uni-freiburg.de/_media/teaching/ws1415/presentation_dl_lect3.pdf)\n        \n        - [Learning Torch GitHub Repo](https://github.com/chetannaik/learning_torch)\n        \n        - [Awesome-Torch (Repository on GitHub)](https://github.com/carpedm20/awesome-torch)\n        \n        - [Machine Learning using Torch Oxford Univ](https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/), [Code](https://github.com/oxford-cs-ml-2015)\n        \n        - [Torch Internals Overview](https://apaszke.github.io/torch-internals.html)\n        \n        - [Torch Cheatsheet](https://github.com/torch/torch7/wiki/Cheatsheet)\n        \n        - [Understanding Natural Language with Deep Neural Networks Using Torch](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)\n\n    - Caffe\n        - [Deep Learning for Computer Vision with Caffe and cuDNN](https://devblogs.nvidia.com/parallelforall/deep-learning-computer-vision-caffe-cudnn/)\n\n    - TensorFlow\n        - [Website](http://tensorflow.org/)\n        \n        - [TensorFlow Examples for Beginners](https://github.com/aymericdamien/TensorFlow-Examples)\n        \n        - [Stanford Tensorflow for Deep Learning Research Course](https://web.stanford.edu/class/cs20si/syllabus.html)\n        \n            - [GitHub Repo](https://github.com/chiphuyen/tf-stanford-tutorials)\n            \n        - [Simplified Scikit-learn Style Interface to TensorFlow](https://github.com/tensorflow/skflow)\n        \n        - [Learning TensorFlow GitHub Repo](https://github.com/chetannaik/learning_tensorflow)\n        \n        - [Benchmark TensorFlow GitHub](https://github.com/soumith/convnet-benchmarks/issues/66)\n        \n        - [Awesome TensorFlow List](https://github.com/jtoy/awesome-tensorflow)\n        \n        - [TensorFlow Book](https://github.com/BinRoot/TensorFlow-Book)\n        \n        - [Android TensorFlow Machine Learning Example](https://blog.mindorks.com/android-tensorflow-machine-learning-example-ff0e9b2654cc)\n        \n            - [GitHub Repo](https://github.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample)\n        - [Creating Custom Model For Android Using TensorFlow](https://blog.mindorks.com/creating-custom-model-for-android-using-tensorflow-3f963d270bfb)\n            - [GitHub Repo](https://github.com/MindorksOpenSource/AndroidTensorFlowMNISTExample)            \n\n<a name="feed" />\n\n- Feed Forward Networks\n\n    - [A Quick Introduction to Neural Networks](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/)\n    \n    - [Implementing a Neural Network from scratch](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/), [Code](https://github.com/dennybritz/nn-from-scratch)\n    \n    - [Speeding up your Neural Network with Theano and the gpu](http://www.wildml.com/2015/09/speeding-up-your-neural-network-with-theano-and-the-gpu/), [Code](https://github.com/dennybritz/nn-theano)\n    \n    - [Basic ANN Theory](https://takinginitiative.wordpress.com/2008/04/03/basic-neural-network-tutorial-theory/)\n    \n    - [Role of Bias in Neural Networks](http://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks)\n    \n    - [Choosing number of hidden layers and nodes](http://stackoverflow.com/questions/3345079/estimating-the-number-of-neurons-and-number-of-layers-of-an-artificial-neural-ne),[2](http://stackoverflow.com/questions/10565868/multi-layer-perceptron-mlp-architecture-criteria-for-choosing-number-of-hidde?lq=1),[3](http://stackoverflow.com/questions/9436209/how-to-choose-number-of-hidden-layers-and-nodes-in-neural-network/2#)\n    \n    - [Backpropagation in Matrix Form](http://sudeepraja.github.io/Neural/)\n    \n    - [ANN implemented in C++ | AI Junkie](http://www.ai-junkie.com/ann/evolved/nnt6.html)\n    \n    - [Simple Implementation](http://stackoverflow.com/questions/15395835/simple-multi-layer-neural-network-implementation)\n    \n    - [NN for Beginners](http://www.codeproject.com/Articles/16419/AI-Neural-Network-for-beginners-Part-of)\n    \n    - [Regression and Classification with NNs (Slides)](http://www.autonlab.org/tutorials/neural13.pdf)\n    \n    - [Another Intro](http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html)\n\n<a name="rnn" />\n\n- Recurrent and LSTM Networks\n    - [awesome-rnn: list of resources (GitHub Repo)](https://github.com/kjw0612/awesome-rnn)\n    \n    - [Recurrent Neural Net Tutorial Part 1](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/), [Part 2](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/), [Part 3](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/), [Code](https://github.com/dennybritz/rnn-tutorial-rnnlm/)\n    \n    - [NLP RNN Representations](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)\n    \n    - [The Unreasonable effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), [Torch Code](https://github.com/karpathy/char-rnn), [Python Code](https://gist.github.com/karpathy/d4dee566867f8291f086)\n    \n    - [Intro to RNN](http://deeplearning4j.org/recurrentnetwork.html), [LSTM](http://deeplearning4j.org/lstm.html)\n    \n    - [An application of RNN](http://hackaday.com/2015/10/15/73-computer-scientists-created-a-neural-net-and-you-wont-believe-what-happened-next/)\n    \n    - [Optimizing RNN Performance](http://svail.github.io/)\n    \n    - [Simple RNN](http://outlace.com/Simple-Recurrent-Neural-Network/)\n    \n    - [Auto-Generating Clickbait with RNN](https://larseidnes.com/2015/10/13/auto-generating-clickbait-with-recurrent-neural-networks/)\n    \n    - [Sequence Learning using RNN (Slides)](http://www.slideshare.net/indicods/general-sequence-learning-with-recurrent-neural-networks-for-next-ml)\n    \n    - [Machine Translation using RNN (Paper)](http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf)\n    \n    - [Music generation using RNNs (Keras)](https://github.com/MattVitelli/GRUV)\n    \n    - [Using RNN to create on-the-fly dialogue (Keras)](http://neuralniche.com/post/tutorial/)\n    \n    - Long Short Term Memory (LSTM)\n    \n        - [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n        \n        - [LSTM explained](https://apaszke.github.io/lstm-explained.html)\n        \n        - [Beginnerâ€™s Guide to LSTM](http://deeplearning4j.org/lstm.html)\n        \n        - [Implementing LSTM from scratch](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/), [Python/Theano code](https://github.com/dennybritz/rnn-tutorial-gru-lstm)\n        \n        - [Torch Code for character-level language models using LSTM](https://github.com/karpathy/char-rnn)\n        \n        - [LSTM for Kaggle EEG Detection competition (Torch Code)](https://github.com/apaszke/kaggle-grasp-and-lift)\n        \n        - [LSTM for Sentiment Analysis in Theano](http://deeplearning.net/tutorial/lstm.html#lstm)\n        \n        - [Deep Learning for Visual Q&A | LSTM | CNN](http://avisingh599.github.io/deeplearning/visual-qa/), [Code](https://github.com/avisingh599/visual-qa)\n        \n        - [Computer Responds to email using LSTM | Google](http://googleresearch.blogspot.in/2015/11/computer-respond-to-this-email.html)\n        \n        - [LSTM dramatically improves Google Voice Search](http://googleresearch.blogspot.ch/2015/09/google-voice-search-faster-and-more.html), [Another Article](http://deeplearning.net/2015/09/30/long-short-term-memory-dramatically-improves-google-voice-etc-now-available-to-a-billion-users/)\n        \n        - [Understanding Natural Language with LSTM Using Torch](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)\n        \n        - [Torch code for Visual Question Answering using a CNN+LSTM model](https://github.com/abhshkdz/neural-vqa)\n        \n        - [LSTM for Human Activity Recognition](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/)\n        \n    - Gated Recurrent Units (GRU)\n    \n        - [LSTM vs GRU](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)\n    \n    - [Time series forecasting with Sequence-to-Sequence (seq2seq) rnn models](https://github.com/guillaume-chevalier/seq2seq-signal-prediction)\n\n\n<a name="rnn2" />\n\n- [Recursive Neural Network (not Recurrent)](https://en.wikipedia.org/wiki/Recursive_neural_network)\n\n    - [Recursive Neural Tensor Network (RNTN)](http://deeplearning4j.org/recursiveneuraltensornetwork.html)\n    \n    - [word2vec, DBN, RNTN for Sentiment Analysis ](http://deeplearning4j.org/zh-sentiment_analysis_word2vec.html)\n\n<a name="rbm" />\n\n- Restricted Boltzmann Machine\n\n    - [Beginner''s Guide about RBMs](http://deeplearning4j.org/restrictedboltzmannmachine.html)\n    \n    - [Another Good Tutorial](http://deeplearning.net/tutorial/rbm.html)\n    \n    - [Introduction to RBMs](http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/)\n    \n    - [Hinton''s Guide to Training RBMs](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)\n    \n    - [RBMs in R](https://github.com/zachmayer/rbm)\n    \n    - [Deep Belief Networks Tutorial](http://deeplearning4j.org/deepbeliefnetwork.html)\n    \n    - [word2vec, DBN, RNTN for Sentiment Analysis ](http://deeplearning4j.org/zh-sentiment_analysis_word2vec.html)\n\n<a name="auto" />\n\n- Autoencoders: Unsupervised (applies BackProp after setting target = input)\n\n    - [Andrew Ng Sparse Autoencoders pdf](https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf)\n    \n    - [Deep Autoencoders Tutorial](http://deeplearning4j.org/deepautoencoder.html)\n    \n    - [Denoising Autoencoders](http://deeplearning.net/tutorial/dA.html), [Theano Code](http://deeplearning.net/tutorial/code/dA.py)\n    \n    - [Stacked Denoising Autoencoders](http://deeplearning.net/tutorial/SdA.html#sda)\n\n\n<a name="cnn" />\n\n- Convolutional Neural Networks\n\n    - [An Intuitive Explanation of Convolutional Neural Networks](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n    \n    - [Awesome Deep Vision: List of Resources (GitHub)](https://github.com/kjw0612/awesome-deep-vision)\n    \n    - [Intro to CNNs](http://deeplearning4j.org/convolutionalnets.html)\n    \n    - [Understanding CNN for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n    \n    - [Stanford Notes](http://vision.stanford.edu/teaching/cs231n/), [Codes](http://cs231n.github.io/), [GitHub](https://github.com/cs231n/cs231n.github.io)\n    \n    - [JavaScript Library (Browser Based) for CNNs](http://cs.stanford.edu/people/karpathy/convnetjs/)\n    \n    - [Using CNNs to detect facial keypoints](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)\n    \n    - [Deep learning to classify business photos at Yelp](http://engineeringblog.yelp.com/2015/10/how-we-use-deep-learning-to-classify-business-photos-at-yelp.html)\n    \n    - [Interview with Yann LeCun | Kaggle](http://blog.kaggle.com/2014/12/22/convolutional-nets-and-cifar-10-an-interview-with-yan-lecun/)\n    \n    - [Visualising and Understanding CNNs](https://www.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf)\n\n<a name="nrl" />\n\n- Network Representation Learning\n\n    - [Awesome Graph Embedding](https://github.com/benedekrozemberczki/awesome-graph-embedding)\n    \n    - [Awesome Network Embedding](https://github.com/chihming/awesome-network-embedding)\n    \n    - [Network Representation Learning Papers](https://github.com/thunlp)\n    \n    - [Knowledge Representation Learning Papers](https://github.com/thunlp/KRLPapers)\n    \n    - [Graph Based Deep Learning Literature](https://github.com/naganandy/graph-based-deep-learning-literature)\n\n<a name="nlp" />\n\n## Natural Language Processing\n\n- [A curated list of speech and natural language processing resources](https://github.com/edobashira/speech-language-processing)\n\n- [Understanding Natural Language with Deep Neural Networks Using Torch](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch/)\n\n- [tf-idf explained](http://michaelerasm.us/post/tf-idf-in-10-minutes/)\n\n- [Interesting Deep Learning NLP Projects Stanford](http://cs224d.stanford.edu/reports.html), [Website](http://cs224d.stanford.edu/)\n\n- [The Stanford NLP Group](https://nlp.stanford.edu/)\n\n- [NLP from Scratch | Google Paper](https://static.googleusercontent.com/media/research.google.com/en/us/pubs/archive/35671.pdf)\n\n- [Graph Based Semi Supervised Learning for NLP](http://graph-ssl.wdfiles.com/local--files/blog%3A_start/graph_ssl_acl12_tutorial_slides_final.pdf)\n\n- [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model)\n\n    - [Classification text with Bag of Words](http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/)\n    \n<a name="topic" />\n\n- Topic Modeling\n    - [Topic Modeling Wikipedia](https://en.wikipedia.org/wiki/Topic_model) \n    - [**Probabilistic Topic Models Princeton PDF**](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf)\n\n    - [LDA Wikipedia](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation), [LSA Wikipedia](https://en.wikipedia.org/wiki/Latent_semantic_analysis), [Probabilistic LSA Wikipedia](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis)\n    \n    - [What is a good explanation of Latent Dirichlet Allocation (LDA)?](https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation)\n    \n    - [**Introduction to LDA**](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/), [Another good explanation](http://confusedlanguagetech.blogspot.in/2012/07/jordan-boyd-graber-and-philip-resnik.html)\n    \n    - [The LDA Buffet - Intuitive Explanation](http://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/)\n    \n    - [Your Guide to Latent Dirichlet Allocation (LDA)](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d)\n    \n    - [Difference between LSI and LDA](https://www.quora.com/Whats-the-difference-between-Latent-Semantic-Indexing-LSI-and-Latent-Dirichlet-Allocation-LDA)\n    \n    - [Original LDA Paper](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf)\n    \n    - [alpha and beta in LDA](http://datascience.stackexchange.com/questions/199/what-does-the-alpha-and-beta-hyperparameters-contribute-to-in-latent-dirichlet-a)\n    \n    - [Intuitive explanation of the Dirichlet distribution](https://www.quora.com/What-is-an-intuitive-explanation-of-the-Dirichlet-distribution)\n    - [topicmodels: An R Package for Fitting Topic Models](https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf)\n\n    - [Topic modeling made just simple enough](https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/)\n    \n    - [Online LDA](http://alexminnaar.com/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html), [Online LDA with Spark](http://alexminnaar.com/distributed-online-latent-dirichlet-allocation-with-apache-spark.html)\n    \n    - [LDA in Scala](http://alexminnaar.com/latent-dirichlet-allocation-in-scala-part-i-the-theory.html), [Part 2](http://alexminnaar.com/latent-dirichlet-allocation-in-scala-part-ii-the-code.html)\n    \n    - [Segmentation of Twitter Timelines via Topic Modeling](https://alexisperrier.com/nlp/2015/09/16/segmentation_twitter_timelines_lda_vs_lsa.html)\n    \n    - [Topic Modeling of Twitter Followers](http://alexperrier.github.io/jekyll/update/2015/09/04/topic-modeling-of-twitter-followers.html)\n\n    - [Multilingual Latent Dirichlet Allocation (LDA)](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA). ([Tutorial here](https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA/blob/master/Multilingual-LDA-Pipeline-Tutorial.ipynb))\n\n    - [Deep Belief Nets for Topic Modeling](https://github.com/larsmaaloee/deep-belief-nets-for-topic-modeling)\n    - [Gaussian LDA for Topic Models with Word Embeddings](http://www.cs.cmu.edu/~rajarshd/papers/acl2015.pdf)\n    - Python\n        - [Series of lecture notes for probabilistic topic models written in ipython notebook](https://github.com/arongdari/topic-model-lecture-note)\n        - [Implementation of various topic models in Python](https://github.com/arongdari/python-topic-model)\n           \n<a name="word2vec" />\n\n- word2vec\n\n    - [Google word2vec](https://code.google.com/archive/p/word2vec)\n    \n    - [Bag of Words Model Wiki](https://en.wikipedia.org/wiki/Bag-of-words_model)\n    \n    - [word2vec Tutorial](https://rare-technologies.com/word2vec-tutorial/)\n    \n    - [A closer look at Skip Gram Modeling](http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf)\n    \n    - [Skip Gram Model Tutorial](http://alexminnaar.com/word2vec-tutorial-part-i-the-skip-gram-model.html), [CBoW Model](http://alexminnaar.com/word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html)\n    \n    - [Word Vectors Kaggle Tutorial Python](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors), [Part 2](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors)\n    \n    - [Making sense of word2vec](http://rare-technologies.com/making-sense-of-word2vec/)\n    \n    - [word2vec explained on deeplearning4j](http://deeplearning4j.org/word2vec.html)\n    \n    - [Quora word2vec](https://www.quora.com/How-does-word2vec-work)\n    \n    - [Other Quora Resources](https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures-in-laymans-terms), [2](https://www.quora.com/What-is-the-difference-between-the-Bag-of-Words-model-and-the-Continuous-Bag-of-Words-model), [3](https://www.quora.com/Is-skip-gram-negative-sampling-better-than-CBOW-NS-for-word2vec-If-so-why)\n    \n    - [word2vec, DBN, RNTN for Sentiment Analysis ](http://deeplearning4j.org/zh-sentiment_analysis_word2vec.html)\n\n- Text Clustering\n\n    - [How string clustering works](http://stackoverflow.com/questions/8196371/how-clustering-works-especially-string-clustering)\n    \n    - [Levenshtein distance for measuring the difference between two sequences](https://en.wikipedia.org/wiki/Levenshtein_distance)\n    \n    - [Text clustering with Levenshtein distances](http://stackoverflow.com/questions/21511801/text-clustering-with-levenshtein-distances)\n\n- Text Classification\n\n    - [Classification Text with Bag of Words](http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/)\n\n- Named Entity Recognitation \n    \n     - [Stanford Named Entity Recognizer (NER)](https://nlp.stanford.edu/software/CRF-NER.shtml)\n\n     - [Named Entity Recognition: Applications and Use Cases- Towards Data Science](https://towardsdatascience.com/named-entity-recognition-applications-and-use-cases-acdbf57d595e)\n	\n- [Language learning with NLP and reinforcement learning](http://blog.dennybritz.com/2015/09/11/reimagining-language-learning-with-nlp-and-reinforcement-learning/)\n\n- [Kaggle Tutorial Bag of Words and Word vectors](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words), [Part 2](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors), [Part 3](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors)\n\n- [What would Shakespeare say (NLP Tutorial)](https://gigadom.wordpress.com/2015/10/02/natural-language-processing-what-would-shakespeare-say/)\n\n- [A closer look at Skip Gram Modeling](http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf)\n\n<a name="vision" />\n\n## Computer Vision\n- [Awesome computer vision (github)](https://github.com/jbhuang0604/awesome-computer-vision)\n\n- [Awesome deep vision (github)](https://github.com/kjw0612/awesome-deep-vision)\n\n\n<a name="svm" />\n\n## Support Vector Machine\n\n- [Highest Voted Questions about SVMs on Cross Validated](http://stats.stackexchange.com/questions/tagged/svm)\n\n- [Help me Understand SVMs!](http://stats.stackexchange.com/questions/3947/help-me-understand-support-vector-machines)\n\n- [SVM in Layman''s terms](https://www.quora.com/What-does-support-vector-machine-SVM-mean-in-laymans-terms)\n\n- [How does SVM Work | Comparisons](http://stats.stackexchange.com/questions/23391/how-does-a-support-vector-machine-svm-work)\n\n- [A tutorial on SVMs](http://alex.smola.org/papers/2003/SmoSch03b.pdf)\n\n- [Practical Guide to SVC](http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf), [Slides](http://www.csie.ntu.edu.tw/~cjlin/talks/freiburg.pdf)\n\n- [Introductory Overview of SVMs](http://www.statsoft.com/Textbook/Support-Vector-Machines)\n\n- Comparisons\n\n    - [SVMs > ANNs](http://stackoverflow.com/questions/6699222/support-vector-machines-better-than-artificial-neural-networks-in-which-learn?rq=1), [ANNs > SVMs](http://stackoverflow.com/questions/11632516/what-are-advantages-of-artificial-neural-networks-over-support-vector-machines), [Another Comparison](http://www.svms.org/anns.html)\n    \n    - [Trees > SVMs](http://stats.stackexchange.com/questions/57438/why-is-svm-not-so-good-as-decision-tree-on-the-same-data)\n    \n    - [Kernel Logistic Regression vs SVM](http://stats.stackexchange.com/questions/43996/kernel-logistic-regression-vs-svm)\n    \n    - [Logistic Regression vs SVM](http://stats.stackexchange.com/questions/58684/regularized-logistic-regression-and-support-vector-machine), [2](http://stats.stackexchange.com/questions/95340/svm-v-s-logistic-regression), [3](https://www.quora.com/Support-Vector-Machines/What-is-the-difference-between-Linear-SVMs-and-Logistic-Regression)\n    \n- [Optimization Algorithms in Support Vector Machines](http://pages.cs.wisc.edu/~swright/talks/sjw-complearning.pdf)\n\n- [Variable Importance from SVM](http://stats.stackexchange.com/questions/2179/variable-importance-from-svm)\n\n- Software\n\n    - [LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)\n    \n    - [Intro to SVM in R](http://cbio.ensmp.fr/~jvert/svn/tutorials/practical/svmbasic/svmbasic_notes.pdf)\n    \n- Kernels\n    - [What are Kernels in ML and SVM?](https://www.quora.com/What-are-Kernels-in-Machine-Learning-and-SVM)\n    \n    - [Intuition Behind Gaussian Kernel in SVMs?](https://www.quora.com/Support-Vector-Machines/What-is-the-intuition-behind-Gaussian-kernel-in-SVM)\n    \n- Probabilities post SVM\n\n    - [Platt''s Probabilistic Outputs for SVM](http://www.csie.ntu.edu.tw/~htlin/paper/doc/plattprob.pdf)\n    \n    - [Platt Calibration Wiki](https://en.wikipedia.org/wiki/Platt_scaling)\n    \n    - [Why use Platts Scaling](http://stats.stackexchange.com/questions/5196/why-use-platts-scaling)\n    \n    - [Classifier Classification with Platt''s Scaling](http://fastml.com/classifier-calibration-with-platts-scaling-and-isotonic-regression/)\n\n\n<a name="rl" />\n\n## Reinforcement Learning\n\n- [Awesome Reinforcement Learning (GitHub)](https://github.com/aikorea/awesome-rl)\n\n- [RL Tutorial Part 1](http://outlace.com/Reinforcement-Learning-Part-1/), [Part 2](http://outlace.com/Reinforcement-Learning-Part-2/)\n\n<a name="dt" />\n\n## Decision Trees\n\n- [Wikipedia Page - Lots of Good Info](https://en.wikipedia.org/wiki/Decision_tree_learning)\n\n- [FAQs about Decision Trees](http://stats.stackexchange.com/questions/tagged/cart)\n\n- [Brief Tour of Trees and Forests](https://statistical-research.com/index.php/2013/04/29/a-brief-tour-of-the-trees-and-forests/)\n\n- [Tree Based Models in R](http://www.statmethods.net/advstats/cart.html)\n\n- [How Decision Trees work?](http://www.aihorizon.com/essays/generalai/decision_trees.htm)\n\n- [Weak side of Decision Trees](http://stats.stackexchange.com/questions/1292/what-is-the-weak-side-of-decision-trees)\n\n- [Thorough Explanation and different algorithms](http://www.ise.bgu.ac.il/faculty/liorr/hbchap9.pdf)\n\n- [What is entropy and information gain in the context of building decision trees?](http://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain)\n\n- [Slides Related to Decision Trees](http://www.slideshare.net/pierluca.lanzi/machine-learning-and-data-mining-11-decision-trees)\n\n- [How do decision tree learning algorithms deal with missing values?](http://stats.stackexchange.com/questions/96025/how-do-decision-tree-learning-algorithms-deal-with-missing-values-under-the-hoo)\n\n- [Using Surrogates to Improve Datasets with Missing Values](https://www.salford-systems.com/videos/tutorials/tips-and-tricks/using-surrogates-to-improve-datasets-with-missing-values)\n\n- [Good Article](https://www.mindtools.com/dectree.html)\n\n- [Are decision trees almost always binary trees?](http://stats.stackexchange.com/questions/12187/are-decision-trees-almost-always-binary-trees)\n\n- [Pruning Decision Trees](https://en.wikipedia.org/wiki/Pruning_(decision_trees)), [Grafting of Decision Trees](https://en.wikipedia.org/wiki/Grafting_(decision_trees))\n\n- [What is Deviance in context of Decision Trees?](http://stats.stackexchange.com/questions/6581/what-is-deviance-specifically-in-cart-rpart)\n\n- [Discover structure behind data with decision trees](http://vooban.com/en/tips-articles-geek-stuff/discover-structure-behind-data-with-decision-trees/) - Grow and plot a decision tree to automatically figure out hidden rules in your data\n\n- Comparison of Different Algorithms\n\n    - [CART vs CTREE](http://stats.stackexchange.com/questions/12140/conditional-inference-trees-vs-traditional-decision-trees)\n    \n    - [Comparison of complexity or performance](https://stackoverflow.com/questions/9979461/different-decision-tree-algorithms-with-comparison-of-complexity-or-performance)\n    \n    - [CHAID vs CART](http://stats.stackexchange.com/questions/61230/chaid-vs-crt-or-cart) , [CART vs CHAID](http://www.bzst.com/2006/10/classification-trees-cart-vs-chaid.html)\n    \n    - [Good Article on comparison](http://www.ftpress.com/articles/article.aspx?p=2248639&seqNum=11)\n    \n- CART\n\n    - [Recursive Partitioning Wikipedia](https://en.wikipedia.org/wiki/Recursive_partitioning)\n    \n    - [CART Explained](http://documents.software.dell.com/Statistics/Textbook/Classification-and-Regression-Trees)\n    \n    - [How to measure/rank â€œvariable importanceâ€ when using CART?](http://stats.stackexchange.com/questions/6478/how-to-measure-rank-variable-importance-when-using-cart-specifically-using)\n    \n    - [Pruning a Tree in R](http://stackoverflow.com/questions/15318409/how-to-prune-a-tree-in-r)\n    \n    - [Does rpart use multivariate splits by default?](http://stats.stackexchange.com/questions/4356/does-rpart-use-multivariate-splits-by-default)\n    \n    - [FAQs about Recursive Partitioning](http://stats.stackexchange.com/questions/tagged/rpart)\n    \n- CTREE\n\n    - [party package in R](https://cran.r-project.org/web/packages/party/party.pdf)\n    \n    - [Show volumne in each node using ctree in R](http://stackoverflow.com/questions/13772715/show-volume-in-each-node-using-ctree-plot-in-r)\n    \n    - [How to extract tree structure from ctree function?](http://stackoverflow.com/questions/8675664/how-to-extract-tree-structure-from-ctree-function)\n    \n- CHAID\n\n    - [Wikipedia Artice on CHAID](https://en.wikipedia.org/wiki/CHAID)\n    \n    - [Basic Introduction to CHAID](https://smartdrill.com/Introduction-to-CHAID.html)\n    \n    - [Good Tutorial on CHAID](http://www.statsoft.com/Textbook/CHAID-Analysis)\n    \n- MARS\n\n    - [Wikipedia Article on MARS](https://en.wikipedia.org/wiki/Multivariate_adaptive_regression_splines)\n    \n- Probabilistic Decision Trees\n\n    - [Bayesian Learning in Probabilistic Decision Trees](http://www.stats.org.uk/bayesian/Jordan.pdf)\n    \n    - [Probabilistic Trees Research Paper](http://people.stern.nyu.edu/adamodar/pdfiles/papers/probabilistic.pdf)\n\n<a name="rf" />\n\n## Random Forest / Bagging\n\n- [Awesome Random Forest (GitHub)**](https://github.com/kjw0612/awesome-random-forest)\n\n- [How to tune RF parameters in practice?](https://www.kaggle.com/forums/f/15/kaggle-forum/t/4092/how-to-tune-rf-parameters-in-practice)\n\n- [Measures of variable importance in random forests](http://stats.stackexchange.com/questions/12605/measures-of-variable-importance-in-random-forests)\n\n- [Compare R-squared from two different Random Forest models](http://stats.stackexchange.com/questions/13869/compare-r-squared-from-two-different-random-forest-models)\n\n- [OOB Estimate Explained | RF vs LDA](https://stat.ethz.ch/education/semesters/ss2012/ams/slides/v10.2.pdf)\n\n- [Evaluating Random Forests for Survival Analysis Using Prediction Error Curve](https://www.jstatsoft.org/index.php/jss/article/view/v050i11)\n\n- [Why doesn''t Random Forest handle missing values in predictors?](http://stats.stackexchange.com/questions/98953/why-doesnt-random-forest-handle-missing-values-in-predictors)\n\n- [How to build random forests in R with missing (NA) values?](http://stackoverflow.com/questions/8370455/how-to-build-random-forests-in-r-with-missing-na-values)\n\n- [FAQs about Random Forest](http://stats.stackexchange.com/questions/tagged/random-forest), [More FAQs](http://stackoverflow.com/questions/tagged/random-forest)\n\n- [Obtaining knowledge from a random forest](http://stats.stackexchange.com/questions/21152/obtaining-knowledge-from-a-random-forest)\n\n- [Some Questions for R implementation](http://stackoverflow.com/questions/20537186/getting-predictions-after-rfimpute), [2](http://stats.stackexchange.com/questions/81609/whether-preprocessing-is-needed-before-prediction-using-finalmodel-of-randomfore), [3](http://stackoverflow.com/questions/17059432/random-forest-package-in-r-shows-error-during-prediction-if-there-are-new-fact)\n\n<a name="gbm" />\n\n## Boosting\n\n- [Boosting for Better Predictions](http://www.datasciencecentral.com/profiles/blogs/boosting-algorithms-for-better-predictions)\n\n- [Boosting Wikipedia Page](https://en.wikipedia.org/wiki/Boosting_(machine_learning))\n\n- [Introduction to Boosted Trees | Tianqi Chen](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)\n\n- Gradient Boosting Machine\n\n    - [Gradiet Boosting Wiki](https://en.wikipedia.org/wiki/Gradient_boosting)\n    \n    - [Guidelines for GBM parameters in R](http://stats.stackexchange.com/questions/25748/what-are-some-useful-guidelines-for-gbm-parameters), [Strategy to set parameters](http://stats.stackexchange.com/questions/35984/strategy-to-set-the-gbm-parameters)\n    \n    - [Meaning of Interaction Depth](http://stats.stackexchange.com/questions/16501/what-does-interaction-depth-mean-in-gbm), [2](http://stats.stackexchange.com/questions/16501/what-does-interaction-depth-mean-in-gbm)\n    \n    - [Role of n.minobsinnode parameter of GBM in R](http://stats.stackexchange.com/questions/30645/role-of-n-minobsinnode-parameter-of-gbm-in-r)\n    \n    - [GBM in R](http://www.slideshare.net/mark_landry/gbm-package-in-r)\n    \n    - [FAQs about GBM](http://stats.stackexchange.com/tags/gbm/hot)\n    \n    - [GBM vs xgboost](https://www.kaggle.com/c/higgs-boson/forums/t/9497/r-s-gbm-vs-python-s-xgboost)\n\n- xgboost\n\n    - [xgboost tuning kaggle](https://www.kaggle.com/khozzy/rossmann-store-sales/xgboost-parameter-tuning-template/log)\n    \n    - [xgboost vs gbm](https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/13012/question-to-experienced-kagglers-and-anyone-who-wants-to-take-a-shot/68296#post68296)\n    \n    - [xgboost survey](https://www.kaggle.com/c/higgs-boson/forums/t/10335/xgboost-post-competition-survey)\n    \n    - [Practical XGBoost in Python online course (free)](http://education.parrotprediction.teachable.com/courses/practical-xgboost-in-python)\n    \n- AdaBoost\n\n    - [AdaBoost Wiki](https://en.wikipedia.org/wiki/AdaBoost), [Python Code](https://gist.github.com/tristanwietsma/5486024)\n    \n    - [AdaBoost Sparse Input Support](http://hamzehal.blogspot.com/2014/06/adaboost-sparse-input-support.html)\n    \n    - [adaBag R package](https://cran.r-project.org/web/packages/adabag/adabag.pdf)\n    \n    - [Tutorial](http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf)\n\n- CatBoost\n\n    - [CatBoost Documentation](https://catboost.ai/docs/)\n\n    - [Benchmarks](https://catboost.ai/#benchmark)\n\n    - [Tutorial](https://github.com/catboost/tutorials)\n\n    - [GitHub Project](https://github.com/catboost)\n\n    - [CatBoost vs. Light GBM vs. XGBoost](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)\n\n<a name="ensem" />\n\n## Ensembles\n\n- [Wikipedia Article on Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning)\n\n- [Kaggle Ensembling Guide](http://mlwave.com/kaggle-ensembling-guide/)\n\n- [The Power of Simple Ensembles](http://www.overkillanalytics.net/more-is-always-better-the-power-of-simple-ensembles/)\n\n- [Ensemble Learning Intro](http://machine-learning.martinsewell.com/ensembles/)\n\n- [Ensemble Learning Paper](http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf)\n\n- [Ensembling models with R](http://amunategui.github.io/blending-models/), [Ensembling Regression Models in R](http://stats.stackexchange.com/questions/26790/ensembling-regression-models), [Intro to Ensembles in R](http://www.vikparuchuri.com/blog/intro-to-ensemble-learning-in-r/)\n\n- [Ensembling Models with caret](http://stats.stackexchange.com/questions/27361/stacking-ensembling-models-with-caret)\n\n- [Bagging vs Boosting vs Stacking](http://stats.stackexchange.com/questions/18891/bagging-boosting-and-stacking-in-machine-learning)\n\n- [Good Resources | Kaggle Africa Soil Property Prediction](https://www.kaggle.com/c/afsis-soil-properties/forums/t/10391/best-ensemble-references)\n\n- [Boosting vs Bagging](http://www.chioka.in/which-is-better-boosting-or-bagging/)\n\n- [Resources for learning how to implement ensemble methods](http://stats.stackexchange.com/questions/32703/resources-for-learning-how-to-implement-ensemble-methods)\n\n- [How are classifications merged in an ensemble classifier?](http://stats.stackexchange.com/questions/21502/how-are-classifications-merged-in-an-ensemble-classifier)\n\n<a name="stack" />\n\n## Stacking Models\n\n- [Stacking, Blending and Stacked Generalization](http://www.chioka.in/stacking-blending-and-stacked-generalization/)\n\n- [Stacked Generalization (Stacking)](http://machine-learning.martinsewell.com/ensembles/stacking/)\n\n- [Stacked Generalization: when does it work?](http://www.ijcai.org/Proceedings/97-2/011.pdf)\n\n- [Stacked Generalization Paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.56.1533&rep=rep1&type=pdf)\n\n<a name="vc" />\n\n## Vapnikâ€“Chervonenkis Dimension\n\n- [Wikipedia article on VC Dimension](https://en.wikipedia.org/wiki/VC_dimension)\n\n- [Intuitive Explanantion of VC Dimension](https://www.quora.com/Explain-VC-dimension-and-shattering-in-lucid-Way)\n\n- [Video explaining VC Dimension](https://www.youtube.com/watch?v=puDzy2XmR5c)\n\n- [Introduction to VC Dimension](http://www.svms.org/vc-dimension/)\n\n- [FAQs about VC Dimension](http://stats.stackexchange.com/questions/tagged/vc-dimension)\n\n- [Do ensemble techniques increase VC-dimension?](http://stats.stackexchange.com/questions/78076/do-ensemble-techniques-increase-vc-dimension)\n\n\n<a name="bayes" />\n\n## Bayesian Machine Learning\n\n- [Bayesian Methods for Hackers (using pyMC)](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)\n\n- [Should all Machine Learning be Bayesian?](http://videolectures.net/bark08_ghahramani_samlbb/)\n\n- [Tutorial on Bayesian Optimisation for Machine Learning](http://www.iro.umontreal.ca/~bengioy/cifar/NCAP2014-summerschool/slides/Ryan_adams_140814_bayesopt_ncap.pdf)\n\n- [Bayesian Reasoning and Deep Learning](http://blog.shakirm.com/2015/10/bayesian-reasoning-and-deep-learning/), [Slides](http://blog.shakirm.com/wp-content/uploads/2015/10/Bayes_Deep.pdf)\n\n- [Bayesian Statistics Made Simple](http://greenteapress.com/wp/think-bayes/)\n\n- [Kalman & Bayesian Filters in Python](https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python)\n\n- [Markov Chain Wikipedia Page](https://en.wikipedia.org/wiki/Markov_chain)\n\n\n<a name="semi" />\n\n## Semi Supervised Learning\n\n- [Wikipedia article on Semi Supervised Learning](https://en.wikipedia.org/wiki/Semi-supervised_learning)\n\n- [Tutorial on Semi Supervised Learning](http://pages.cs.wisc.edu/~jerryzhu/pub/sslicml07.pdf)\n\n- [Graph Based Semi Supervised Learning for NLP](http://graph-ssl.wdfiles.com/local--files/blog%3A_start/graph_ssl_acl12_tutorial_slides_final.pdf)\n\n- [Taxonomy](http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/taxo_[0].pdf)\n\n- [Video Tutorial Weka](https://www.youtube.com/watch?v=sWxcIjZFGNM)\n\n- [Unsupervised, Supervised and Semi Supervised learning](http://stats.stackexchange.com/questions/517/unsupervised-supervised-and-semi-supervised-learning)\n\n- [Research Papers 1](http://mlg.eng.cam.ac.uk/zoubin/papers/zglactive.pdf), [2](http://mlg.eng.cam.ac.uk/zoubin/papers/zgl.pdf), [3](http://icml.cc/2012/papers/616.pdf)\n\n\n<a name="opt" />\n\n## Optimization\n\n- [Mean Variance Portfolio Optimization with R and Quadratic Programming](http://www.wdiam.com/2012/06/10/mean-variance-portfolio-optimization-with-r-and-quadratic-programming/?utm_content=buffer04c12&utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer)\n\n- [Algorithms for Sparse Optimization and Machine Learning](http://www.ima.umn.edu/2011-2012/W3.26-30.12/activities/Wright-Steve/sjw-ima12)\n\n- [Optimization Algorithms in Machine Learning](http://pages.cs.wisc.edu/~swright/nips2010/sjw-nips10.pdf), [Video Lecture](http://videolectures.net/nips2010_wright_oaml/)\n\n- [Optimization Algorithms for Data Analysis](http://www.birs.ca/workshops/2011/11w2035/files/Wright.pdf)\n\n- [Video Lectures on Optimization](http://videolectures.net/stephen_j_wright/)\n\n- [Optimization Algorithms in Support Vector Machines](http://pages.cs.wisc.edu/~swright/talks/sjw-complearning.pdf)\n\n- [The Interplay of Optimization and Machine Learning Research](http://jmlr.org/papers/volume7/MLOPT-intro06a/MLOPT-intro06a.pdf)\n\n- [Hyperopt tutorial for Optimizing Neural Networksâ€™ Hyperparameters](http://vooban.com/en/tips-articles-geek-stuff/hyperopt-tutorial-for-optimizing-neural-networks-hyperparameters/)\n\n\n<a name="other" />\n\n## Other Tutorials\n\n- For a collection of Data Science Tutorials using R, please refer to [this list](https://github.com/ujjwalkarn/DataScienceR).\n\n- For a collection of Data Science Tutorials using Python, please refer to [this list](https://github.com/ujjwalkarn/DataSciencePython).\n', '{"language":null,"stars":17250,"forks":3964,"watchers":17250,"open_issues":43,"topics":["awesome","awesome-list","deep-learning","deep-learning-tutorial","deep-neural-networks","deeplearning","list","machine-learning","machinelearning","neural-network","neural-networks"],"default_branch":"master","size_kb":9024,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:sindresorhus:awesome","source_url":"https://github.com/sindresorhus/awesome"},{"type":"has_code","target_id":"github:sindresorhus:awesome","source_url":"https://github.com/sindresorhus/awesome"},{"type":"has_code","target_id":"github:ujjwalkarn:Machine-Learning-Tutorials","source_url":"https://github.com/ujjwalkarn/Machine-Learning-Tutorials"},{"type":"has_code","target_id":"github:ujjwalkarn:DataScienceR","source_url":"https://github.com/ujjwalkarn/DataScienceR"},{"type":"has_code","target_id":"github:ujjwalkarn:DataSciencePython","source_url":"https://github.com/ujjwalkarn/DataSciencePython"},{"type":"has_code","target_id":"github:dair-ai:ML-YouTube-Courses","source_url":"https://github.com/dair-ai/ML-YouTube-Courses"},{"type":"has_code","target_id":"github:prakhar1989:awesome-courses","source_url":"https://github.com/prakhar1989/awesome-courses#machine-learning"},{"type":"has_code","target_id":"github:ZuzooVn:machine-learning-for-software-engineers","source_url":"https://github.com/ZuzooVn/machine-learning-for-software-engineers"},{"type":"has_code","target_id":"github:hangtwenty:dive-into-machine-learning","source_url":"https://github.com/hangtwenty/dive-into-machine-learning"},{"type":"has_code","target_id":"github:josephmisiti:awesome-machine-learning","source_url":"https://github.com/josephmisiti/awesome-machine-learning"},{"type":"has_code","target_id":"github:fasouto:awesome-dataviz","source_url":"https://github.com/fasouto/awesome-dataviz"},{"type":"has_code","target_id":"github:okulbilisim:awesome-datascience","source_url":"https://github.com/okulbilisim/awesome-datascience"},{"type":"has_code","target_id":"github:pedrosan:TheAnalyticsEdge","source_url":"https://github.com/pedrosan/TheAnalyticsEdge"},{"type":"has_code","target_id":"github:humphd:have-fun-with-machine-learning","source_url":"https://github.com/humphd/have-fun-with-machine-learning"},{"type":"has_code","target_id":"github:owainlewis:awesome-artificial-intelligence","source_url":"https://github.com/owainlewis/awesome-artificial-intelligence"},{"type":"has_code","target_id":"github:trevorstephens:gplearn","source_url":"https://github.com/trevorstephens/gplearn"},{"type":"has_code","target_id":"github:rouseguy:intro2stats","source_url":"https://github.com/rouseguy/intro2stats"},{"type":"has_code","target_id":"github:ethen8181:machine-learning","source_url":"https://github.com/ethen8181/machine-learning"},{"type":"has_code","target_id":"github:soulmachine:machine-learning-cheat-sheet","source_url":"https://github.com/soulmachine/machine-learning-cheat-sheet"},{"type":"has_code","target_id":"github:ChristosChristofidis:awesome-deep-learning","source_url":"https://github.com/ChristosChristofidis/awesome-deep-learning"},{"type":"has_code","target_id":"github:floodsung:Deep-Learning-Papers-Reading-Roadmap","source_url":"https://github.com/floodsung/Deep-Learning-Papers-Reading-Roadmap"},{"type":"has_code","target_id":"github:rouseguy:intro2deeplearning","source_url":"https://github.com/rouseguy/intro2deeplearning"},{"type":"has_code","target_id":"github:THUNLP-MT:MT-Reading-List","source_url":"https://github.com/THUNLP-MT/MT-Reading-List#machine-translation-reading-list"},{"type":"has_code","target_id":"github:lisa-lab:DeepLearningTutorials","source_url":"https://github.com/lisa-lab/DeepLearningTutorials"},{"type":"has_code","target_id":"github:vict0rsch:deep_learning","source_url":"https://github.com/vict0rsch/deep_learning"},{"type":"has_code","target_id":"github:torch:tutorials","source_url":"https://github.com/torch/tutorials"},{"type":"has_code","target_id":"github:chetannaik:learning_torch","source_url":"https://github.com/chetannaik/learning_torch"},{"type":"has_code","target_id":"github:carpedm20:awesome-torch","source_url":"https://github.com/carpedm20/awesome-torch"},{"type":"has_code","target_id":"github:torch:torch7","source_url":"https://github.com/torch/torch7"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples","source_url":"https://github.com/aymericdamien/TensorFlow-Examples"},{"type":"has_code","target_id":"github:chiphuyen:tf-stanford-tutorials","source_url":"https://github.com/chiphuyen/tf-stanford-tutorials"},{"type":"has_code","target_id":"github:tensorflow:skflow","source_url":"https://github.com/tensorflow/skflow"},{"type":"has_code","target_id":"github:chetannaik:learning_tensorflow","source_url":"https://github.com/chetannaik/learning_tensorflow"},{"type":"has_code","target_id":"github:soumith:convnet-benchmarks","source_url":"https://github.com/soumith/convnet-benchmarks"},{"type":"has_code","target_id":"github:jtoy:awesome-tensorflow","source_url":"https://github.com/jtoy/awesome-tensorflow"},{"type":"has_code","target_id":"github:BinRoot:TensorFlow-Book","source_url":"https://github.com/BinRoot/TensorFlow-Book"},{"type":"has_code","target_id":"github:MindorksOpenSource:AndroidTensorFlowMachineLearningExample","source_url":"https://github.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample"},{"type":"has_code","target_id":"github:MindorksOpenSource:AndroidTensorFlowMNISTExample","source_url":"https://github.com/MindorksOpenSource/AndroidTensorFlowMNISTExample"},{"type":"has_code","target_id":"github:dennybritz:nn-from-scratch","source_url":"https://github.com/dennybritz/nn-from-scratch"},{"type":"has_code","target_id":"github:dennybritz:nn-theano","source_url":"https://github.com/dennybritz/nn-theano"},{"type":"has_code","target_id":"github:kjw0612:awesome-rnn","source_url":"https://github.com/kjw0612/awesome-rnn"},{"type":"has_code","target_id":"github:dennybritz:rnn-tutorial-rnnlm","source_url":"https://github.com/dennybritz/rnn-tutorial-rnnlm"},{"type":"has_code","target_id":"github:karpathy:char-rnn","source_url":"https://github.com/karpathy/char-rnn"},{"type":"has_code","target_id":"github:MattVitelli:GRUV","source_url":"https://github.com/MattVitelli/GRUV"},{"type":"has_code","target_id":"github:dennybritz:rnn-tutorial-gru-lstm","source_url":"https://github.com/dennybritz/rnn-tutorial-gru-lstm"},{"type":"has_code","target_id":"github:karpathy:char-rnn","source_url":"https://github.com/karpathy/char-rnn"},{"type":"has_code","target_id":"github:apaszke:kaggle-grasp-and-lift","source_url":"https://github.com/apaszke/kaggle-grasp-and-lift"},{"type":"has_code","target_id":"github:avisingh599:visual-qa","source_url":"https://github.com/avisingh599/visual-qa"},{"type":"has_code","target_id":"github:abhshkdz:neural-vqa","source_url":"https://github.com/abhshkdz/neural-vqa"},{"type":"has_code","target_id":"github:guillaume-chevalier:LSTM-Human-Activity-Recognition","source_url":"https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition"},{"type":"has_code","target_id":"github:guillaume-chevalier:seq2seq-signal-prediction","source_url":"https://github.com/guillaume-chevalier/seq2seq-signal-prediction"},{"type":"has_code","target_id":"github:zachmayer:rbm","source_url":"https://github.com/zachmayer/rbm"},{"type":"has_code","target_id":"github:kjw0612:awesome-deep-vision","source_url":"https://github.com/kjw0612/awesome-deep-vision"},{"type":"has_code","target_id":"github:cs231n:cs231n.github.io","source_url":"https://github.com/cs231n/cs231n.github.io"},{"type":"has_code","target_id":"github:benedekrozemberczki:awesome-graph-embedding","source_url":"https://github.com/benedekrozemberczki/awesome-graph-embedding"},{"type":"has_code","target_id":"github:chihming:awesome-network-embedding","source_url":"https://github.com/chihming/awesome-network-embedding"},{"type":"has_code","target_id":"github:thunlp:KRLPapers","source_url":"https://github.com/thunlp/KRLPapers"},{"type":"has_code","target_id":"github:naganandy:graph-based-deep-learning-literature","source_url":"https://github.com/naganandy/graph-based-deep-learning-literature"},{"type":"has_code","target_id":"github:edobashira:speech-language-processing","source_url":"https://github.com/edobashira/speech-language-processing"},{"type":"has_code","target_id":"github:ArtificiAI:Multilingual-Latent-Dirichlet-Allocation-LDA","source_url":"https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA"},{"type":"has_code","target_id":"github:ArtificiAI:Multilingual-Latent-Dirichlet-Allocation-LDA","source_url":"https://github.com/ArtificiAI/Multilingual-Latent-Dirichlet-Allocation-LDA"},{"type":"has_code","target_id":"github:larsmaaloee:deep-belief-nets-for-topic-modeling","source_url":"https://github.com/larsmaaloee/deep-belief-nets-for-topic-modeling"},{"type":"has_code","target_id":"github:arongdari:topic-model-lecture-note","source_url":"https://github.com/arongdari/topic-model-lecture-note"},{"type":"has_code","target_id":"github:arongdari:python-topic-model","source_url":"https://github.com/arongdari/python-topic-model"},{"type":"has_code","target_id":"github:jbhuang0604:awesome-computer-vision","source_url":"https://github.com/jbhuang0604/awesome-computer-vision"},{"type":"has_code","target_id":"github:kjw0612:awesome-deep-vision","source_url":"https://github.com/kjw0612/awesome-deep-vision"},{"type":"has_code","target_id":"github:aikorea:awesome-rl","source_url":"https://github.com/aikorea/awesome-rl"},{"type":"has_code","target_id":"github:kjw0612:awesome-random-forest","source_url":"https://github.com/kjw0612/awesome-random-forest"},{"type":"has_code","target_id":"github:catboost:tutorials","source_url":"https://github.com/catboost/tutorials"},{"type":"has_code","target_id":"github:CamDavidsonPilon:Probabilistic-Programming-and-Bayesian-Methods-for-Hackers","source_url":"https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers"},{"type":"has_code","target_id":"github:rlabbe:Kalman-and-Bayesian-Filters-in-Python","source_url":"https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python"},{"type":"has_code","target_id":"github:ujjwalkarn:DataScienceR","source_url":"https://github.com/ujjwalkarn/DataScienceR"},{"type":"has_code","target_id":"github:ujjwalkarn:DataSciencePython","source_url":"https://github.com/ujjwalkarn/DataSciencePython"}]', NULL, 'CC0-1.0', 'approved', 80, '224a9c5fed89db501db5947318d8e208', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ujjwalkarn-Machine-Learning-Tutorials from https://github.com/ujjwalkarn.png
Image converted to WebP: data/images/github-ujjwalkarn-Machine-Learning-Tutorials.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-dair-ai-ML-YouTube-Courses', 'github--dair-ai--ml-youtube-courses', 'ML-YouTube-Courses', 'dair-ai', 'At DAIR.AI we â¤ï¸ open AI education. In this repo, we index and organize some of the best and most recent machine learning courses available on YouTube. **Machine Learning** - Caltech CS156: Learning from Data - Stanford CS229: Machine Learning - Making Friends with Machine Learning - Applied Machine Learning - Introduction to Machine Learning (TÃ¼bingen) - Machine Learning Lecture (Stefan Harmeling) - Statistical Machine Learning (TÃ¼bingen) - Probabilistic Machine Learning - MIT 6.S897: Machin...', '["ai","data-science","deep-learning","machine-learning","natural-language-processing","nlp"]', 'other', 17048, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/dair-ai/ML-YouTube-Courses","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# ğŸ“º ML YouTube Courses\n\nAt DAIR.AI we â¤ï¸ open AI education. In this repo, we index and organize some of the best and most recent machine learning courses available on YouTube.\n\n**Machine Learning**\n\n- [Caltech CS156: Learning from Data](#caltech-cs156-learning-from-data)\n- [Stanford CS229: Machine Learning](#stanford-cs229-machine-learning)\n- [Making Friends with Machine Learning](#making-friends-with-machine-learning)\n- [Applied Machine Learning](#applied-machine-learning)\n- [Introduction to Machine Learning (TÃ¼bingen)](#introduction-to-machine-learning-TÃ¼bingen)\n- [Machine Learning Lecture (Stefan Harmeling)](#machine-learning-lecture-stefan-harmeling)\n- [Statistical Machine Learning (TÃ¼bingen)](#statistical-machine-learning-TÃ¼bingen)\n- [Probabilistic Machine Learning](#probabilistic-machine-learning)\n- [MIT 6.S897: Machine Learning for Healthcare (2019)](#mit-6s897-machine-learning-for-healthcare-2019)\n\n**Deep Learning**\n\n- [Neural Networks: Zero to Hero](#neural-networks-zero-to-hero-by-andrej-karpathy)\n- [MIT: Deep Learning for Art, Aesthetics, and Creativity](#mit-deep-learning-for-art-aesthetics-and-creativity)\n- [Stanford CS230: Deep Learning (2018)](#stanford-cs230-deep-learning-2018)\n- [Introduction to Deep Learning (MIT)](#introduction-to-deep-learning)\n- [CMU Introduction to Deep Learning (11-785)](#cmu-introduction-to-deep-learning-11-785)\n- [Deep Learning: CS 182](#deep-learning-cs-182)\n- [Deep Unsupervised Learning](#deep-unsupervised-learning)\n- [NYU Deep Learning SP21](#nyu-deep-learning-sp21)\n- [Foundation Models](#foundation-models)\n- [Deep Learning (TÃ¼bingen)](#deep-learning-TÃ¼bingen)\n\n**Scientific Machine Learning**\n\n- [Parallel Computing and Scientific Machine Learning](#parallel-computing-and-scientific-machine-learning)\n\n**Practical Machine Learning**\n\n- [LLMOps: Building Real-World Applications With Large Language Models](#llmops-building-real-world-applications-with-large-language-models)\n- [Evaluating and Debugging Generative AI](#evaluating-and-debugging-generative-ai)\n- [ChatGPT Prompt Engineering for Developers](#chatgpt-prompt-engineering-for-developers)\n- [LangChain for LLM Application Development](#langchain-for-llm-application-development)\n- [LangChain: Chat with Your Data](#langchain-chat-with-your-data)\n- [Building Systems with the ChatGPT API](#building-systems-with-the-chatgpt-api)\n- [LangChain & Vector Databases in Production](#langchain--vector-databases-in-production)\n- [Building LLM-Powered Apps](#building-llm-powered-apps)\n- [Full Stack LLM Bootcamp](#full-stack-llm-bootcamp)\n- [Full Stack Deep Learning](#full-stack-deep-learning)\n- [Practical Deep Learning for Coders](#practical-deep-learning-for-coders)\n- [Stanford MLSys Seminars](#stanford-mlsys-seminars)\n- [Machine Learning Engineering for Production (MLOps)](#machine-learning-engineering-for-production-mlops)\n- [MIT Introduction to Data-Centric AI](#mit-introduction-to-data-centric-ai)\n\n**Natural Language Processing**\n\n- [XCS224U: Natural Language Understanding (2023)](#xcs224u-natural-language-understanding-2023)\n- [Stanford CS25 - Transformers United](#stanford-cs25---transformers-united)\n- [NLP Course (Hugging Face)](#nlp-course-hugging-face)\n- [CS224N: Natural Language Processing with Deep Learning](#cs224n-natural-language-processing-with-deep-learning)\n- [CMU Neural Networks for NLP](#cmu-neural-networks-for-nlp)\n- [CS224U: Natural Language Understanding](#cs224u-natural-language-understanding)\n- [CMU Advanced NLP 2021/2022/2024](#cmu-advanced-nlp)\n- [Multilingual NLP](#multilingual-nlp)\n- [Advanced NLP](#advanced-nlp)\n\n**Computer Vision**\n\n- [CS231N: Convolutional Neural Networks for Visual Recognition](#cs231n-convolutional-neural-networks-for-visual-recognition)\n- [Deep Learning for Computer Vision](#deep-learning-for-computer-vision)\n- [Deep Learning for Computer Vision (DL4CV)](#deep-learning-for-computer-vision-dl4cv)\n- [Deep Learning for Computer Vision (neuralearn.ai)](#deep-learning-for-computer-vision-neuralearnai)\n\n**Reinforcement Learning**\n\n- [Deep Reinforcement Learning](#deep-reinforcement-learning)\n- [Reinforcement Learning Lecture Series (DeepMind)](#reinforcement-learning-lecture-series-deepmind)\n- [Reinforcement Learning (Polytechnique Montreal, Fall 2021)](#reinforcement-learning-polytechnique-montreal-fall-2021)\n- [Foundations of Deep RL](#foundations-of-deep-rl)\n- [Stanford CS234: Reinforcement Learning](#stanford-cs234-reinforcement-learning)\n\n**Graph Machine Learning**\n\n- [Machine Learning with Graphs (Stanford)](#machine-learning-with-graphs-stanford)\n- [AMMI Geometric Deep Learning Course](#ammi-geometric-deep-learning-course)\n\n**Multi-Task Learning**\n\n- [Multi-Task and Meta-Learning (Stanford)](#stanford-cs330-deep-multi-task-and-meta-learning)\n\n**Others**\n\n- [MIT Deep Learning in Life Sciences](#mit-deep-learning-in-life-sciences)\n- [Self-Driving Cars (TÃ¼bingen)](#self-driving-cars-TÃ¼bingen)\n- [Advanced Robotics (Berkeley)](#advanced-robotics-uc-berkeley)\n\n---\n\n## Caltech CS156: Learning from Data\n\nAn introductory course in machine learning that covers the basic theory, algorithms, and applications.\n\n- Lecture 1: The Learning Problem\n- Lecture 2: Is Learning Feasible?\n- Lecture 3: The Linear Model I\n- Lecture 4: Error and Noise\n- Lecture 5: Training versus Testing\n- Lecture 6: Theory of Generalization\n- Lecture 7: The VC Dimension\n- Lecture 8: Bias-Variance Tradeoff\n- Lecture 9: The Linear Model II\n- Lecture 10: Neural Networks\n- Lecture 11: Overfitting\n- Lecture 12: Regularization\n- Lecture 13: Validation\n- Lecture 14: Support Vector Machines\n- Lecture 15: Kernel Methods\n- Lecture 16: Radial Basis Functions\n- Lecture 17: Three Learning Principles\n- Lecture 18: Epilogue\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLD63A284B7615313A)\n\n## Stanford CS229: Machine Learning\n\nTo learn some of the basics of ML:\n\n- Linear Regression and Gradient Descent\n- Logistic Regression\n- Naive Bayes\n- SVMs\n- Kernels\n- Decision Trees\n- Introduction to Neural Networks\n- Debugging ML Models\n  ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU)\n\n## Making Friends with Machine Learning\n\nA series of mini lectures covering various introductory topics in ML:\n\n- Explainability in AI\n- Classification vs. Regression\n- Precession vs. Recall\n- Statistical Significance\n- Clustering and K-means\n- Ensemble models\n  ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLRKtJ4IpxJpDxl0NTvNYQWKCYzHNuy2xG)\n\n## Neural Networks: Zero to Hero (by Andrej Karpathy)\n\nCourse providing an in-depth overview of neural networks.\n\n- Backpropagation\n- Spelled-out intro to Language Modeling\n- Activation and Gradients\n- Becoming a Backprop Ninja\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ)\n\n## MIT: Deep Learning for Art, Aesthetics, and Creativity\n\nCovers the application of deep learning for art, aesthetics, and creativity.\n\n- Nostalgia -> Art -> Creativity -> Evolution as Data + Direction\n- Efficient GANs\n- Explorations in AI for Creativity\n- Neural Abstractions\n- Easy 3D Content Creation with Consistent Neural Fields\n  ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLCpMvp7ftsnIbNwRnQJbDNRqO6qiN3EyH)\n\n## Stanford CS230: Deep Learning (2018)\n\nCovers the foundations of deep learning, how to build different neural networks(CNNs, RNNs, LSTMs, etc...), how to lead machine learning projects, and career advice for deep learning practitioners.\n\n- Deep Learning Intuition\n- Adversarial examples - GANs\n- Full-cycle of a Deep Learning Project\n- AI and Healthcare\n- Deep Learning Strategy\n- Interpretability of Neural Networks\n- Career Advice and Reading Research Papers\n- Deep Reinforcement Learning\n\nğŸ”— [Link to Course](https://youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb) ğŸ”— [Link to Materials](https://cs230.stanford.edu/syllabus/)\n\n## Applied Machine Learning\n\nTo learn some of the most widely used techniques in ML:\n\n- Optimization and Calculus\n- Overfitting and Underfitting\n- Regularization\n- Monte Carlo Estimation\n- Maximum Likelihood Learning\n- Nearest Neighbours\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL2UML_KCiC0UlY7iCQDSiGDMovaupqc83)\n\n## Introduction to Machine Learning (TÃ¼bingen)\n\nThe course serves as a basic introduction to machine learning and covers key concepts in regression, classification, optimization, regularization, clustering, and dimensionality reduction.\n\n- Linear regression\n- Logistic regression\n- Regularization\n- Boosting\n- Neural networks\n- PCA\n- Clustering\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL05umP7R6ij35ShKLDqccJSDntugY4FQT)\n\n## Machine Learning Lecture (Stefan Harmeling)\n\nCovers many fundamental ML concepts:\n\n- Bayes rule\n- From logic to probabilities\n- Distributions\n- Matrix Differential Calculus\n- PCA\n- K-means and EM\n- Causality\n- Gaussian Processes\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLzrCXlf6ypbxS5OYOY3EN_0u2fDuIT6Gt)\n\n## Statistical Machine Learning (TÃ¼bingen)\n\nThe course covers the standard paradigms and algorithms in statistical machine learning.\n\n- KNN\n- Bayesian decision theory\n- Convex optimization\n- Linear and ridge regression\n- Logistic regression\n- SVM\n- Random Forests\n- Boosting\n- PCA\n- Clustering\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL05umP7R6ij2XCvrRzLokX6EoHWaGA2cC)\n\n## Practical Deep Learning for Coders\n\nThis course covers topics such as how to:\n\n- Build and train deep learning models for computer vision, natural language processing, tabular analysis, and collaborative filtering problems\n- Create random forests and regression models\n- Deploy models\n- Use PyTorch, the worldâ€™s fastest growing deep learning software, plus popular libraries like fastai and Hugging Face\n- Foundations and Deep Dive to Diffusion Models\n- ...\n\nğŸ”— [Link to Course - Part 1](https://www.youtube.com/playlist?list=PLfYUBJiXbdtSvpQjSnJJ_PmDQB_VyT5iU)\n\nğŸ”— [Link to Course - Part 2](https://www.youtube.com/watch?v=_7rMfsA24Ls&ab_channel=JeremyHoward)\n\n## Stanford MLSys Seminars\n\nA seminar series on all sorts of topics related to building machine learning systems.\n\nğŸ”— [Link to Lectures](https://www.youtube.com/playlist?list=PLSrTvUm384I9PV10koj_cqit9OfbJXEkq)\n\n## Machine Learning Engineering for Production (MLOps)\n\nSpecialization course on MLOPs by Andrew Ng.\n\nğŸ”— [Link to Lectures](https://www.youtube.com/playlist?list=PLkDaE6sCZn6GMoA0wbpJLi3t34Gd8l0aK)\n\n## MIT Introduction to Data-Centric AI\n\nCovers the emerging science of Data-Centric AI (DCAI) that studies techniques to improve datasets, which is often the best way to improve performance in practical ML applications. Topics include:\n\n- Data-Centric AI vs. Model-Centric AI\n- Label Errors\n- Dataset Creation and Curation\n- Data-centric Evaluation of ML Models\n- Class Imbalance, Outliers, and Distribution Shift\n- ...\n\nğŸ”— [Course Website](https://dcai.csail.mit.edu/)\n\nğŸ”— [Lecture Videos](https://www.youtube.com/watch?v=ayzOzZGHZy4&list=PLnSYPjg2dHQKdig0vVbN-ZnEU0yNJ1mo5)\n\nğŸ”— [Lab Assignments](https://github.com/dcai-course/dcai-lab)\n\n## Machine Learning with Graphs (Stanford)\n\nTo learn some of the latest graph techniques in machine learning:\n\n- PageRank\n- Matrix Factorizing\n- Node Embeddings\n- Graph Neural Networks\n- Knowledge Graphs\n- Deep Generative Models for Graphs\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLoROMvodv4rPLKxIpqhjhPgdQy7imNkDn)\n\n## Probabilistic Machine Learning\n\nTo learn the probabilistic paradigm of ML:\n\n- Reasoning about uncertainty\n- Continuous Variables\n- Sampling\n- Markov Chain Monte Carlo\n- Gaussian Distributions\n- Graphical Models\n- Tuning Inference Algorithms\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL05umP7R6ij2YE8rRJSb-olDNbntAQ_Bx)\n\n## MIT 6.S897: Machine Learning for Healthcare (2019)\n\nThis course introduces students to machine learning in healthcare, including the nature of clinical data and the use of machine learning for risk stratification, disease progression modeling, precision medicine, diagnosis, subtype discovery, and improving clinical workflows.\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLUl4u3cNGP60B0PQXVQyGNdCyCTDU1Q5j)\n\n## Introduction to Deep Learning\n\nTo learn some of the fundamentals of deep learning:\n\n- Introduction to Deep Learning\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI)\n\n## CMU Introduction to Deep Learning (11-785)\n\nThe course starts off gradually from MLPs (Multi Layer Perceptrons) and then progresses into concepts like attention\nand sequence-to-sequence models.\n\nğŸ”— [Link to Course](https://deeplearning.cs.cmu.edu/F22/index.html) \\nğŸ”— [Lectures](https://www.youtube.com/playlist?list=PLp-0K3kfddPxRmjgjm0P1WT6H-gTqE8j9) \\nğŸ”— [Tutorials/Recitations](https://www.youtube.com/playlist?list=PLp-0K3kfddPz8WXg8RqH0sEN6X2L65HUZ)\n\n## Deep Learning: CS 182\n\nTo learn some of the widely used techniques in deep learning:\n\n- Machine Learning Basics\n- Error Analysis\n- Optimization\n- Backpropagation\n- Initialization\n- Batch Normalization\n- Style transfer\n- Imitation Learning\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL_iWQOsE6TfVmKkQHucjPAoRtIJYt8a5A)\n\n## Deep Unsupervised Learning\n\nTo learn the latest and most widely used techniques in deep unsupervised learning:\n\n- Autoregressive Models\n- Flow Models\n- Latent Variable Models\n- Self-supervised learning\n- Implicit Models\n- Compression\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLwRJQ4m4UJjPiJP3691u-qWwPGVKzSlNP)\n\n## NYU Deep Learning SP21\n\nTo learn some of the advanced techniques in deep learning:\n\n- Neural Nets: rotation and squashing\n- Latent Variable Energy Based Models\n- Unsupervised Learning\n- Generative Adversarial Networks\n- Autoencoders\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI)\n\n## Foundation Models\n\nTo learn about foundation models like GPT-3, CLIP, Flamingo, Codex, and DINO.\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL9t0xVFP90GD8hox0KipBkJcLX_C3ja67)\n\n## Deep Learning (TÃ¼bingen)\n\nThis course introduces the practical and theoretical principles of deep neural networks.\n\n- Computation graphs\n- Activation functions and loss functions\n- Training, regularization and data augmentation\n- Basic and state-of-the-art deep neural network architectures including convolutional networks and graph neural networks\n- Deep generative models such as auto-encoders, variational auto-encoders and generative adversarial networks\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL05umP7R6ij3NTWIdtMbfvX7Z-4WEXRqD)\n\n## Parallel Computing and Scientific Machine Learning\n\n- The Basics of Scientific Simulators\n- Introduction to Parallel Computing\n- Continuous Dynamics\n- Inverse Problems and Differentiable Programming\n- Distributed Parallel Computing\n- Physics-Informed Neural Networks and Neural Differential Equations\n- Probabilistic Programming, AKA Bayesian Estimation on Programs\n- Globalizing the Understanding of Models\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLCAl7tjCwWyGjdzOOnlbGnVNZk0kB8VSa)\n\n## XCS224U: Natural Language Understanding (2023)\n\nThis course covers topics such as:\n\n- Contextual Word Representations\n- Information Retrieval\n- In-context learning\n- Behavioral Evaluation of NLU models\n- NLP Methods and Metrics\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLoROMvodv4rOwvldxftJTmoR3kRcWkJBp)\n\n## Stanford CS25 - Transformers United\n\nThis course consists of lectures focused on Transformers, providing a deep dive and their applications\n\n- Introduction to Transformers\n- Transformers in Language: GPT-3, Codex\n- Applications in Vision\n- Transformers in RL & Universal\n  Compute Engines\n- Scaling transformers\n- Interpretability with transformers\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)\n\n## NLP Course (Hugging Face)\n\nLearn about different NLP concepts and how to apply language models and Transformers to NLP:\n\n- What is Transfer Learning?\n- BPE Tokenization\n- Batching inputs\n- Fine-tuning models\n- Text embeddings and semantic search\n- Model evaluation\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o)\n\n## CS224N: Natural Language Processing with Deep Learning\n\nTo learn the latest approaches for deep learning based NLP:\n\n- Dependency parsing\n- Language models and RNNs\n- Question Answering\n- Transformers and pretraining\n- Natural Language Generation\n- T5 and Large Language Models\n- Future of NLP\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)\n\n## CMU Neural Networks for NLP\n\nTo learn the latest neural network based techniques for NLP:\n\n- Language Modeling\n- Efficiency tricks\n- Conditioned Generation\n- Structured Prediction\n- Model Interpretation\n- Advanced Search Algorithms\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL8PYTP1V4I8AkaHEJ7lOOrlex-pcxS-XV)\n\n## CS224U: Natural Language Understanding\n\nTo learn the latest concepts in natural language understanding:\n\n- Grounded Language Understanding\n- Relation Extraction\n- Natural Language Inference (NLI)\n- NLU and Neural Information Extraction\n- Adversarial testing\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLoROMvodv4rPt5D0zs3YhbWSZA8Q_DyiJ)\n\n## CMU Advanced NLP\n\nTo learn:\n\n- Basics of modern NLP techniques\n- Multi-task, Multi-domain, multi-lingual learning\n- Prompting + Sequence-to-sequence pre-training\n- Interpreting and Debugging NLP Models\n- Learning from Knowledge-bases\n- Adversarial learning\n- ...\n\nğŸ”— [Link to 2021 Edition](https://www.youtube.com/playlist?list=PL8PYTP1V4I8AYSXn_GKVgwXVluCT9chJ6)\n\nğŸ”— [Link to 2022 Edition](https://www.youtube.com/playlist?list=PL8PYTP1V4I8D0UkqW2fEhgLrnlDW9QK7z)\n\nğŸ”— [Link to 2024 Edition](https://www.youtube.com/playlist?list=PL8PYTP1V4I8DZprnWryM4nR8IZl1ZXDjg)\n\n## Multilingual NLP\n\nTo learn the latest concepts for doing multilingual NLP:\n\n- Typology\n- Words, Part of Speech, and Morphology\n- Advanced Text Classification\n- Machine Translation\n- Data Augmentation for MT\n- Low Resource ASR\n- Active Learning\n- ...\n\nğŸ”— [Link to 2020 Course](https://www.youtube.com/playlist?list=PL8PYTP1V4I8CHhppU6n1Q9-04m96D9gt5)\n\nğŸ”— [Link to 2022 Course](https://www.youtube.com/playlist?list=PL8PYTP1V4I8BhCpzfdKKdd1OnTfLcyZr7)\n\n## Advanced NLP\n\nTo learn advanced concepts in NLP:\n\n- Attention Mechanisms\n- Transformers\n- BERT\n- Question Answering\n- Model Distillation\n- Vision + Language\n- Ethics in NLP\n- Commonsense Reasoning\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLWnsVgP6CzadmQX6qevbar3_vDBioWHJL)\n\n## CS231N: Convolutional Neural Networks for Visual Recognition\n\nStanford''s Famous CS231n course. The videos are only available for the Spring 2017 semester. The course is currently known as Deep Learning for Computer Vision, but the Spring 2017 version is titled Convolutional Neural Networks for Visual Recognition.\n\n- Image Classification\n- Loss Functions and Optimization\n- Introduction to Neural Networks\n- Convolutional Neural Networks\n- Training Neural Networks\n- Deep Learning Software\n- CNN Architectures\n- Recurrent Neural Networks\n- Detection and Segmentation\n- Visualizing and Understanding\n- Generative Models\n- Deep Reinforcement Learning\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) ğŸ”— [Link to Materials](http://cs231n.stanford.edu/2017/)\n\n## Deep Learning for Computer Vision\n\nTo learn some of the fundamental concepts in CV:\n\n- Introduction to deep learning for CV\n- Image Classification\n- Convolutional Networks\n- Attention Networks\n- Detection and Segmentation\n- Generative Models\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r)\n\n## Deep Learning for Computer Vision (DL4CV)\n\nTo learn modern methods for computer vision:\n\n- CNNs\n- Advanced PyTorch\n- Understanding Neural Networks\n- RNN, Attention and ViTs\n- Generative Models\n- GPU Fundamentals\n- Self-Supervision\n- Neural Rendering\n- Efficient Architectures\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL_Z2_U9MIJdNgFM7-f2fZ9ZxjVRP_jhJv)\n\n## Deep Learning for Computer Vision (neuralearn.ai)\n\nTo learn modern methods for computer vision:\n\n- Self-Supervised Learning\n- Neural Rendering\n- Efficient Architectures\n- Machine Learning Operations (MLOps)\n- Modern Convolutional Neural Networks\n- Transformers in Vision\n- Model Deployment\n\nğŸ”— [Link to Course](https://www.youtube.com/watch?v=IA3WxTTPXqQ)\n\n## AMMI Geometric Deep Learning Course\n\nTo learn about concepts in geometric deep learning:\n\n- Learning in High Dimensions\n- Geometric Priors\n- Grids\n- Manifolds and Meshes\n- Sequences and Time Warping\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLn2-dEmQeTfSLXW8yXP4q_Ii58wFdxb3C)\n\n## Deep Reinforcement Learning\n\nTo learn the latest concepts in deep RL:\n\n- Intro to RL\n- RL algorithms\n- Real-world sequential decision making\n- Supervised learning of behaviors\n- Deep imitation learning\n- Cost functions and reward functions\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc)\n\n## Reinforcement Learning Lecture Series (DeepMind)\n\nThe Deep Learning Lecture Series is a collaboration between DeepMind and the UCL Centre for Artificial Intelligence.\n\n- Introduction to RL\n- Dynamic Programming\n- Model-free algorithms\n- Deep reinforcement learning\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm)\n\n\n## LLMOps: Building Real-World Applications With Large Language Models\n\nLearn to build modern software with LLMs using the newest tools and techniques in the field.\n\nğŸ”— [Link to Course](https://www.comet.com/site/llm-course/)\n\n## Evaluating and Debugging Generative AI\n\nYou''ll learn:\n\n- Instrument A Jupyter Notebook\n- Manage Hyperparameters Config\n- Log Run Metrics\n- Collect artifacts for dataset and model versioning\n- Log experiment results\n- Trace prompts and responses for LLMs\n- ...\n\nğŸ”— [Link to Course](https://www.deeplearning.ai/short-courses/evaluating-debugging-generative-ai/)\n\n## ChatGPT Prompt Engineering for Developers\n\nLearn how to use a large language model (LLM) to quickly build new and powerful applications.\n\nğŸ”— [Link to Course](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)\n\n## LangChain for LLM Application Development\n\nYou''ll learn:\n\n- Models, Prompt, and Parsers\n- Memories for LLMs\n- Chains\n- Question Answering over Documents\n- Agents\n\nğŸ”— [Link to Course](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)\n\n## LangChain: Chat with Your Data\n\nYou''ll learn about:\n\n- Document Loading\n- Document Splitting\n- Vector Stores and Embeddings\n- Retrieval\n- Question Answering\n- Chat\n\nğŸ”— [Link to Course](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/)\n\n## Building Systems with the ChatGPT API\n\nLearn how to automate complex workflows using chain calls to a large language model.\n\nğŸ”— [Link to Course](https://www.deeplearning.ai/short-courses/building-systems-with-chatgpt/)\n\n## LangChain & Vector Databases in Production\n\nLearn how to use LangChain and Vector DBs in Production:\n\n- LLMs and LangChain\n- Learning how to Prompt\n- Keeping Knowledge Organized with Indexes\n- Combining Components Together with Chains\n- ...\n\nğŸ”— [Link to Course](https://learn.activeloop.ai/courses/langchain)\n\n## Building LLM-Powered Apps\n\nLearn how to build LLM-powered applications using LLM APIs\n\n- Unpacking LLM APIs\n- Building a Baseline LLM Application\n- Enhancing and Optimizing LLM Applications\n- ...\n\nğŸ”— [Link to Course](https://www.wandb.courses/courses/building-llm-powered-apps)\n\n## Full Stack LLM Bootcamp\n\nTo learn how to build and deploy LLM-powered applications:\n\n- Learn to Spell: Prompt Engineering\n- LLMOPs\n- UX for Language User Interfaces\n- Augmented Language Models\n- Launch an LLM App in One Hour\n- LLM Foundations\n- Project Walkthrough: askFSDL\n- ...\n\nğŸ”— [Link to Course](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/)\n\n## Full Stack Deep Learning\n\nTo learn full-stack production deep learning:\n\n- ML Projects\n- Infrastructure and Tooling\n- Experiment Managing\n- Troubleshooting DNNs\n- Data Management\n- Data Labeling\n- Monitoring ML Models\n- Web deployment\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL1T8fO7ArWlcWg04OgNiJy91PywMKT2lv)\n\n## Introduction to Deep Learning and Deep Generative Models\n\nCovers the fundamental concepts of deep learning\n\n- Single-layer neural networks and gradient descent\n- Multi-layer neural networks and backpropagation\n- Convolutional neural networks for images\n- Recurrent neural networks for text\n- Autoencoders, variational autoencoders, and generative adversarial networks\n- Encoder-decoder recurrent neural networks and transformers\n- PyTorch code examples\n\nğŸ”— [Link to Course](https://www.youtube.com/watch?v=1nqCZqDYPp0&list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51) ğŸ”— [Link to Materials](https://sebastianraschka.com/blog/2021/dl-course.html)\n\n## Self-Driving Cars (TÃ¼bingen)\n\nCovers the most dominant paradigms of self-driving cars: modular pipeline-based approaches as well as deep-learning based end-to-end driving techniques.\n\n- Camera, lidar and radar-based perception\n- Localization, navigation, path planning\n- Vehicle modeling/control\n- Deep Learning\n- Imitation learning\n- Reinforcement learning\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PL05umP7R6ij321zzKXK6XCQXAaaYjQbzr)\n\n## Reinforcement Learning (Polytechnique Montreal, Fall 2021)\n\nDesigning autonomous decision making systems is one of the longstanding goals of Artificial Intelligence. Such decision making systems, if realized, can have a big impact in machine learning for robotics, game playing, control, health care to name a few. This course introduces Reinforcement Learning as a general framework to design such autonomous decision making systems.\n\n- Introduction to RL\n- Multi-armed bandits\n- Policy Gradient Methods\n- Contextual Bandits\n- Finite Markov Decision Process\n- Dynamic Programming\n- Policy Iteration, Value Iteration\n- Monte Carlo Methods\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLImtCgowF_ES_JdF_UcM60EXTcGZg67Ua) ğŸ”— [Link to Materials](https://chandar-lab.github.io/INF8953DE/)\n\n## Foundations of Deep RL\n\nA mini 6-lecture series by Pieter Abbeel.\n\n- MDPs, Exact Solution Methods, Max-ent RL\n- Deep Q-Learning\n- Policy Gradients and Advantage Estimation\n- TRPO and PPO\n- DDPG and SAC\n- Model-based RL\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0)\n\n## Stanford CS234: Reinforcement Learning\n\nCovers topics from basic concepts of Reinforcement Learning to more advanced ones:\n\n- Markov decision processes & planning\n- Model-free policy evaluation\n- Model-free control\n- Reinforcement learning with function approximation & Deep RL\n- Policy Search\n- Exploration\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u) ğŸ”— [Link to Materials](https://web.stanford.edu/class/cs234/)\n\n## Stanford CS330: Deep Multi-Task and Meta Learning\n\nThis is a graduate-level course covering different aspects of deep multi-task and meta learning.\n\n- Multi-task learning, transfer learning basics\n- Meta-learning algorithms\n- Advanced meta-learning topics\n- Multi-task RL, goal-conditioned RL\n- Meta-reinforcement learning\n- Hierarchical RL\n- Lifelong learning\n- Open problems\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5) ğŸ”— [Link to Materials](https://cs330.stanford.edu/)\n\n## MIT Deep Learning in Life Sciences\n\nA course introducing foundations of ML for applications in genomics and the life sciences more broadly.\n\n- Interpreting ML Models\n- DNA Accessibility, Promoters and Enhancers\n- Chromatin and gene regulation\n- Gene Expression, Splicing\n- RNA-seq, Splicing\n- Single cell RNA-sequencing\n- Dimensionality Reduction, Genetics, and Variation\n- Drug Discovery\n- Protein Structure Prediction\n- Protein Folding\n- Imaging and Cancer\n- Neuroscience\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLypiXJdtIca5ElZMWHl4HMeyle2AzUgVB)\n\nğŸ”— [Link to Materials](https://mit6874.github.io/)\n\n## Advanced Robotics: UC Berkeley\n\nThis is course is from Peter Abbeel and covers a review on reinforcement learning and continues to applications in robotics.\n\n- MDPs: Exact Methods\n- Discretization of Continuous State Space MDPs\n- Function Approximation / Feature-based Representations\n- LQR, iterative LQR / Differential Dynamic Programming\n- ...\n\nğŸ”— [Link to Course](https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNBPJdt8WamRAt4XKc639wF) ğŸ”— [Link to Materials](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa19/)\n\n---\n\nReach out on [Twitter](https://twitter.com/omarsar0) if you have any questions.\n\nIf you are interested to contribute, feel free to open a PR with a link to the course. It will take a bit of time, but I have plans to do many things with these individual lectures. We can summarize the lectures, include notes, provide additional reading material, include difficulty of content, etc.\n\nYou can now find ML Course notes [here](https://github.com/dair-ai/ML-Course-Notes).\n', '{"language":null,"stars":17048,"forks":2082,"watchers":17048,"open_issues":18,"topics":["ai","data-science","deep-learning","machine-learning","natural-language-processing","nlp"],"default_branch":"main","size_kb":114,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:dcai-course:dcai-lab","source_url":"https://github.com/dcai-course/dcai-lab"},{"type":"has_code","target_id":"github:dair-ai:ML-Course-Notes","source_url":"https://github.com/dair-ai/ML-Course-Notes"}]', NULL, 'CC0-1.0', 'approved', 80, 'b5de36445314c51d7f137232fa167576', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-dair-ai-ML-YouTube-Courses from https://github.com/dair-ai.png
Image converted to WebP: data/images/github-dair-ai-ML-YouTube-Courses.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Mikoto10032-DeepLearning', 'github--mikoto10032--deeplearning', 'DeepLearning', 'Mikoto10032', '**å®Œå¤‡çš„ AI å­¦ä¹ è·¯çº¿ï¼Œæœ€è¯¦ç»†çš„ä¸­è‹±æ–‡èµ„æºæ•´ç†** :star: AiLearning: æœºå™¨å­¦ä¹  - MachineLearning - MLã€æ·±åº¦å­¦ä¹  - DeepLearning - DLã€è‡ªç„¶è¯­è¨€å¤„ç† NL Machine-Learning * çŸ©é˜µå¾®ç§¯åˆ† * æœºå™¨å­¦ä¹ çš„æ•°å­¦åŸºç¡€ * CS229çº¿æ€§ä»£æ•°ä¸æ¦‚ç‡è®ºåŸºç¡€ * æœºå™¨å­¦ä¹ ç®—æ³•åœ°å›¾ * æœºå™¨å­¦ä¹  å´æ©è¾¾ Courseraä¸ªäººç¬”è®° && è§†é¢‘ï¼ˆå«å®˜æ–¹ç¬”è®°ï¼‰ * CS229 è¯¾ç¨‹è®²ä¹‰ä¸­æ–‡ç¿»è¯‘ && æœºå™¨å­¦ä¹  å´æ©è¾¾ cs229ä¸ªäººç¬”è®° && å®˜ç½‘ï¼ˆç¬”è®°ï¼‰ && è§†é¢‘ï¼ˆä¸­æ–‡å­—å¹•ï¼‰ * ç™¾é¡µæœºå™¨å­¦ä¹  * ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹æèˆª && ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹å„ç« èŠ‚ç¬”è®° && ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹å„ç« èŠ‚ç¬”è®° && æ¨èç­”æ¡ˆï¼šstatistical-learning-method-solutions-manual ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹å„ç« èŠ‚ç¬”è®° && ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹å„ç« èŠ‚ä»£ç å®ç°ä¸è¯¾åä¹ é¢˜å‚è€ƒè§£ç­” * ã€Šæ¨¡å¼è¯†åˆ«ä¸æœºå™¨å­¦ä¹ ã€‹ Christopher Bishop * ã€Šæœºå™¨å­¦ä¹ ã€‹ å‘¨å¿—å && å—ç“œä¹¦ï¼špumpkin-book * ã€Šæœºå™¨å­¦...', '["cnn","deep-learning","deeplearning","gan","gcn","kaggle","machine-learning","machinelearning","mxnet","pytorch","rnn","tensorflow","jupyter notebook"]', 'other', 16940, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Mikoto10032/DeepLearning","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# 		DeepLearning Tutorial\n## ä¸€. å…¥é—¨èµ„æ–™\n\n[**å®Œå¤‡çš„ AI å­¦ä¹ è·¯çº¿ï¼Œæœ€è¯¦ç»†çš„ä¸­è‹±æ–‡èµ„æºæ•´ç†**](https://zhuanlan.zhihu.com/p/64052743) :star:\n\n[AiLearning: æœºå™¨å­¦ä¹  - MachineLearning - MLã€æ·±åº¦å­¦ä¹  - DeepLearning - DLã€è‡ªç„¶è¯­è¨€å¤„ç† NL](https://github.com/apachecn/AiLearning)\n\n[Machine-Learning](https://github.com/shunliz/Machine-Learning)\n\n### æ•°å­¦åŸºç¡€\n\n![](notes/Images/MathematicalBasis.png)\n\n* [çŸ©é˜µå¾®ç§¯åˆ†](https://zh.wikipedia.org/wiki/%E7%9F%A9%E9%98%B5%E5%BE%AE%E7%A7%AF%E5%88%86)\n* [æœºå™¨å­¦ä¹ çš„æ•°å­¦åŸºç¡€](https://github.com/fengdu78/Data-Science-Notes/tree/master/0.math/0.basic)\n* [CS229çº¿æ€§ä»£æ•°ä¸æ¦‚ç‡è®ºåŸºç¡€](https://github.com/fengdu78/Data-Science-Notes/tree/master/0.math/1.CS229)\n\n### æœºå™¨å­¦ä¹ åŸºç¡€\n\n#### å¿«é€Ÿå…¥é—¨\n* [æœºå™¨å­¦ä¹ ç®—æ³•åœ°å›¾](http://www.tensorinfinity.com/paper_18.html)\n* [æœºå™¨å­¦ä¹  å´æ©è¾¾ Courseraä¸ªäººç¬”è®°](https://github.com/Mikoto10032/DeepLearning/blob/master/books/%5BML-Coursera%5D%5B2014%5D%5BAndrew%20Ng%5D/%5B2014%5D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AA%E4%BA%BA%E7%AC%94%E8%AE%B0%E5%AE%8C%E6%95%B4%E7%89%88v5.1.pdf) Â && [è§†é¢‘ï¼ˆå«å®˜æ–¹ç¬”è®°ï¼‰](https://www.coursera.org/learn/machine-learning) Â \n* [CS229 è¯¾ç¨‹è®²ä¹‰ä¸­æ–‡ç¿»è¯‘](https://kivy-cn.github.io/Stanford-CS-229-CN/#/) && [æœºå™¨å­¦ä¹  å´æ©è¾¾ cs229ä¸ªäººç¬”è®°](https://github.com/Mikoto10032/DeepLearning/blob/master/books/%5BML-CS229%5D%5B2011%5D%5BAndrew%20NG%5D/%5B2011%5D%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E4%B8%AA%E4%BA%BA%E7%AC%94.pdf) && [å®˜ç½‘ï¼ˆç¬”è®°ï¼‰](http://cs229.stanford.edu/) Â && [è§†é¢‘ï¼ˆä¸­æ–‡å­—å¹•ï¼‰](http://open.163.com/newview/movie/free?pid=M6SGF6VB4&mid=M6SGHFBMC) Â  \n* [ç™¾é¡µæœºå™¨å­¦ä¹ ](http://themlbook.com/wiki/doku.php)\n\n#### æ·±å…¥ç†è§£\n* [ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹æèˆª](https://github.com/Mikoto10032/DeepLearning/tree/master/books/%E6%9D%8E%E8%88%AA-%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0) && [ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹å„ç« èŠ‚ç¬”è®°](https://www.cnblogs.com/YongSun/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/) && [ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹å„ç« èŠ‚ç¬”è®°](https://zhuanlan.zhihu.com/c_1213397558586257408) && [æ¨èç­”æ¡ˆï¼šstatistical-learning-method-solutions-manual](https://github.com/datawhalechina/statistical-learning-method-solutions-manual)  [ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹å„ç« èŠ‚ç¬”è®°](https://www.cnblogs.com/liaohuiqiang/category/1039314.html) &&  [ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹å„ç« èŠ‚ä»£ç å®ç°ä¸è¯¾åä¹ é¢˜å‚è€ƒè§£ç­”](https://blog.csdn.net/breeze_blows/article/details/85469944)\n* [ã€Šæ¨¡å¼è¯†åˆ«ä¸æœºå™¨å­¦ä¹ ã€‹ Christopher Bishop](https://github.com/Mikoto10032/DeepLearning/blob/master/books/æ¨¡å¼è¯†åˆ«ä¸æœºå™¨å­¦ä¹ PRML_Chinese_vision.pdf)\n* [ã€Šæœºå™¨å­¦ä¹ ã€‹ å‘¨å¿—å](https://github.com/Mikoto10032/DeepLearning/blob/master/books/æœºå™¨å­¦ä¹ å‘¨å¿—å.pdf)    && [å—ç“œä¹¦ï¼špumpkin-book](https://github.com/datawhalechina/pumpkin-book)\n* [ã€Šæœºå™¨å­¦ä¹ å®æˆ˜ã€‹ PelerHarrington](https://github.com/Mikoto10032/DeepLearning/blob/master/books/æœºå™¨å­¦ä¹ å®æˆ˜%20ä¸­æ–‡åŒé¡µç‰ˆ.pdf)\n* [æœºå™¨å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ ä¹¦å•](https://mp.weixin.qq.com/s?__biz=MzAxMjcyNjE5MQ==&mid=2650488718&idx=1&sn=815a79d27d500f0fb8db1fe1fc6cfe48&chksm=83a2e54eb4d56c58a0989654f920d64ad2784ce52e4b2bc6883974257cf475c9983f05fb88c1&scene=0&xtrack=1&ascene=14&devicetype=android-28&version=27000339&nettype=WIFI&abtest_cookie=AwABAAoACwATAAQAI5ceAFaZHgDQmR4A3JkeAAAA&lang=zh_CN&pass_ticket=oEB1108Pes6HkdxEITmBjTb2Glju5%2BEGqHZKz50fMg0rgK4l9Fodlbe%2FDm96iX57&wx_header=1)\n\n### æ·±åº¦å­¦ä¹ åŸºç¡€\n\n#### å¿«é€Ÿå…¥é—¨\n* [æ·±åº¦å­¦ä¹ æ€ç»´å¯¼å›¾](https://github.com/dformoso/deeplearning-mindmap)	&& [æ·±åº¦å­¦ä¹ ç®—æ³•åœ°å›¾](http://www.tensorinfinity.com/paper_158.html)\n* [ã€Šæ–¯å¦ç¦å¤§å­¦æ·±åº¦å­¦ä¹ åŸºç¡€æ•™ç¨‹ã€‹ Andrew Ngï¼ˆå´æ©è¾¾ï¼‰](https://github.com/Mikoto10032/DeepLearning/blob/master/books/æ–¯å¦ç¦å¤§å­¦-æ·±åº¦å­¦ä¹ åŸºç¡€æ•™ç¨‹.pdf)  \n* [æ·±åº¦å­¦ä¹  å´æ©è¾¾ ä¸ªäººç¬”è®°](http://www.ai-start.com/dl2017/) Â && [è§†é¢‘](http://mooc.study.163.com/smartSpec/detail/1001319001.htm)\n* [MITæ·±åº¦å­¦ä¹ åŸºç¡€-2019è§†é¢‘è¯¾ç¨‹](https://deeplearning.mit.edu/)\n* [å°æ¹¾å¤§å­¦ï¼ˆNTUï¼‰æå®æ¯…æ•™æˆè¯¾ç¨‹](http://speech.ee.ntu.edu.tw/~tlkagk/index.html) && [[leeml-notes](https://github.com/datawhalechina/leeml-notes)\n* [å›¾è§£æ·±åº¦å­¦ä¹ _Grokking-Deep-Learning](https://github.com/iamtrask/Grokking-Deep-Learning)\n* [ã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹ Michael Nielsen](https://github.com/Mikoto10032/DeepLearning/blob/master/books/ç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ neural%20networks%20and%20deep-learning-ä¸­æ–‡_ALL.pdf) Â  Â  \n* [ CS321-Hinton](http://www.cs.toronto.edu/~tijmen/csc321/)\n* [ CS230: Deep Learning](https://web.stanford.edu/class/cs230/)		\n* [ CS294-112](http://rail.eecs.berkeley.edu/deeprlcourse/resources/)\n\n##### è®¡ç®—æœºè§†è§‰\n* [CS231 æé£é£ å·²æˆæƒä¸ªäººç¿»è¯‘ç¬”è®°](https://zhuanlan.zhihu.com/p/21930884) && [è§†é¢‘](http://study.163.com/course/courseMain.htm?courseId=1003223001)\n* [è®¡ç®—æœºè§†è§‰ç ”ç©¶æ–¹å‘](https://mp.weixin.qq.com/s/WNkzfvYtEO5zJoe_-yAPow)\n\n##### è‡ªç„¶è¯­è¨€å¤„ç†\n* [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/index.html)\n* [NLPä¸Šæ‰‹æ•™ç¨‹](https://github.com/FudanNLP/nlp-beginner)\n* [NLPå…¥é—¨æ¨èä¹¦ç›®ï¼ˆ2019ç‰ˆï¼‰](https://zhuanlan.zhihu.com/p/58874484)\n\n##### æ·±åº¦å¼ºåŒ–å­¦ä¹ \n* [CS234: Reinforcement Learning](http://web.stanford.edu/class/cs234/index.html)\n\n#### æ·±å…¥ç†è§£\n* [ã€Šæ·±åº¦å­¦ä¹ ã€‹ Yoshua Bengio.Ian GoodFellow](https://github.com/Mikoto10032/DeepLearning/blob/master/books/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.Yoshua%20Bengio%2BIan%20GoodFellow.pdf):star:      Â \n* [ã€Šè‡ªç„¶è¯­è¨€å¤„ç†ã€‹Jacob Eisenstein](https://github.com/Mikoto10032/DeepLearning/blob/master/books/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86.Jacob%20Eisenstein.pdf)		\n* [ã€Šå¼ºåŒ–å­¦ä¹ ã€‹](https://github.com/Mikoto10032/DeepLearning/blob/master/books/Reinforcement%20Learning.Sutton.pdf) && [ç¬¬äºŒç‰ˆ](http://incompleteideas.net/book/RLbook2018trimmed.pdf)	\n* [hangdongçš„æ·±åº¦å­¦ä¹ åšå®¢,è®ºæ–‡æ¨è](https://handong1587.github.io/categories.html#deep_learning-ref)\n* [Practical Deep Learning for Coders, v3](https://course.fast.ai/)\n* [ã€ŠTensorflowå®æˆ˜Googleæ·±åº¦å­¦ä¹ æ¡†æ¶ã€‹ éƒ‘æ³½å®‡ é¡¾æ€å®‡](https://github.com/Mikoto10032/DeepLearning/blob/master/books/Tensorflow%20å®æˆ˜Googleæ·±åº¦å­¦ä¹ æ¡†æ¶.pdf)\n\n#### ä¸€äº›ä¹¦å•\n\n* [2019å¹´æœ€æ–°-æ·±åº¦å­¦ä¹ ã€ç”Ÿæˆå¯¹æŠ—ã€Pytorchä¼˜ç§€æ•™ææ¨è](https://zhuanlan.zhihu.com/p/63784033)\n\n### å·¥ç¨‹èƒ½åŠ›\n\n![](https://pic4.zhimg.com/v2-009013278688f520c070b27910255cb1_r.jpg)\n\n* [å¦‚ä½•ç³»ç»Ÿåœ°å­¦ä¹ ç®—æ³•ï¼Ÿ](https://www.zhihu.com/question/20588261/answer/798928056) && [LeetCode](https://leetcode.com/) && [leetcodeé¢˜è§£](https://github.com/azl397985856/leetcode) && [ã€Šç®—æ³•å¯¼è®ºã€‹ä¸­ç®—æ³•çš„C++å®ç°](https://github.com/huaxz1986/cplusplus-_Implementation_Of_Introduction_to_Algorithms)\n* [æœºå™¨å­¦ä¹ ç®—æ³•å®æˆ˜](#æœºå™¨å­¦ä¹ å®æˆ˜ç¯‡)\n* [æ·±åº¦å­¦ä¹ æ¡†æ¶](#æ·±åº¦å­¦ä¹ æ¡†æ¶)\n* [å¦‚ä½•æˆä¸ºä¸€åç®—æ³•å·¥ç¨‹å¸ˆ](https://mp.weixin.qq.com/s/YMtnBAVDZepsMTO4h-VRtQ) && [ä»å°ç™½åˆ°å…¥é—¨ç®—æ³•ï¼Œæˆ‘çš„ç»éªŒåˆ†äº«ç»™ä½ ï½](https://mp.weixin.qq.com/s?__biz=MzAxMjcyNjE5MQ==&mid=2650488786&idx=1&sn=68b9536d0b0b3105ab8d79f8efcb0a4b&chksm=83a2e512b4d56c045c6ab0349108842e6a5b26e8f3e507ff5d19ee50e3bd63ef149a36d23eef&scene=0&xtrack=1&ascene=14&devicetype=android-28&version=27000437&nettype=WIFI&abtest_cookie=BAABAAoACwASABMABgAjlx4AVpkeANCZHgDcmR4A8ZkeAAOaHgAAAA%3D%3D&lang=zh_CN&pass_ticket=4yovfEr0v09yZCvvQ1NEy12qGIonnRpGi774X09Mh5EZD2oL%2BRz6FTtX9R5gALB1&wx_header=1) && [æˆ‘çš„ç ”ç©¶ç”Ÿè¿™ä¸‰å¹´](https://zhuanlan.zhihu.com/p/54161673) :star:\n* [ç¼–ç¨‹é¢è¯•çš„é¢˜ç›®åˆ†ç±»](https://zhuanlan.zhihu.com/p/89392459)\n* [ã€ŠAIç®—æ³•å·¥ç¨‹å¸ˆæ‰‹å†Œã€‹](http://www.huaxiaozhuan.com/)\n* [å¦‚ä½•å‡†å¤‡ç®—æ³•å·¥ç¨‹å¸ˆé¢è¯•ï¼Œæ–©è·ä¸€çº¿äº’è”ç½‘å…¬å¸æœºå™¨å­¦ä¹ å²—offerï¼Ÿ](https://zhuanlan.zhihu.com/p/76827460)\n* [ã€å®Œç»“ã€‘æ·±åº¦å­¦ä¹ CVç®—æ³•å·¥ç¨‹å¸ˆä»å…¥é—¨åˆ°åˆçº§é¢è¯•æœ‰å¤šè¿œï¼Œå¤§æ¦‚æ˜¯25ç¯‡æ–‡ç« çš„è·ç¦»](https://mp.weixin.qq.com/s/HZ3Cd2jHuikyFN9ydvcMTw)\n* [ è®¡ç®—æœºç›¸å…³æŠ€æœ¯é¢è¯•å¿…å¤‡](https://github.com/CyC2018/CS-Notes) && [CS-WiKi](https://veal98.gitee.io/cs-wiki/#/) && [è®¡ç®—æœºåŸºç¡€é¢è¯•é—®é¢˜å…¨é¢æ€»ç»“](https://github.com/wolverinn/Waking-Up) && [TeachYourselfCS-CN](https://github.com/keithnull/TeachYourselfCS-CN) && [é¢è¯•ç®—æ³•ç¬”è®°-ä¸­æ–‡](https://github.com/imhuay/Algorithm_for_Interview-Chinese) \n* [ç®—æ³•å·¥ç¨‹å¸ˆé¢è¯•](https://github.com/DarLiner/Algorithm_Interview_Notes-Chinese)\n* [æ·±åº¦å­¦ä¹ é¢è¯•é¢˜ç›®](https://github.com/ShanghaiTechAIClub/DLInterview)\n* [æ·±åº¦å­¦ä¹ 500é—®](https://github.com/scutan90/DeepLearning-500-questions)\n* [AIç®—æ³•å²—æ±‚èŒæ”»ç•¥](https://github.com/amusi/AI-Job-Notes#Strategy)\n* [Kaggleå®æˆ˜]()\n  * å¸¸ç”¨ç®—æ³•ï¼š\n    * Feature Engineeringï¼šcontinue variable && categorical variable\n    * Classic machine learning algorithmï¼šLR, KNN, SVM, Random Forest, GBDT(XGBoost&&LightGBM), Factorization Machine, Field-aware Factorization Machine, Neural Network\n    * Cross validation, model selectionï¼šgrid search, random search, hyper-opt\n    * Ensemble learning\n  * [kaggleç«èµ›å®å…¸ç¬¬ä¸€ç« -ç«èµ›æ¡†æ¶ç¯‡ï¼:star:](https://mp.weixin.qq.com/s/EGiFG6u9BYr1aBdq0a0wIQ)\n  * [Kaggle é¡¹ç›®å®æˆ˜ï¼ˆæ•™ç¨‹ï¼‰ = æ–‡æ¡£ + ä»£ç  + è§†é¢‘](https://github.com/apachecn/kaggle)\n  * [Kaggleå…¥é—¨ç³»åˆ—ï¼šï¼ˆä¸€ï¼‰æœºå™¨å­¦ä¹ ç¯å¢ƒæ­å»º](https://zhuanlan.zhihu.com/p/29086448) && [Kaggleå…¥é—¨ç³»åˆ—ï¼šï¼ˆäºŒï¼‰Kaggleç®€ä»‹](https://zhuanlan.zhihu.com/p/29417603) && [Kaggleå…¥é—¨ç³»åˆ—ï¼ˆä¸‰ï¼‰Titanicåˆè¯•èº«æ‰‹](https://zhuanlan.zhihu.com/p/29086614)\n  * [ä» 0 åˆ° 1 èµ°è¿› Kaggle](https://zhuanlan.zhihu.com/p/61660061) \n  * [Kaggle å…¥é—¨æŒ‡å—](https://zhuanlan.zhihu.com/p/25742261) \n  * [ä¸€ä¸ªæ¡†æ¶è§£å†³å‡ ä¹æ‰€æœ‰æœºå™¨å­¦ä¹ é—®é¢˜](https://zhuanlan.zhihu.com/p/61657532) && [Approaching (Almost) Any Machine Learning Problem | Abhishek Thakur](http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/)\n  * [åˆ†åˆ†é’Ÿå¸¦ä½ æ€å…¥Kaggle Top 1%](https://zhuanlan.zhihu.com/p/27424282)\n  * [å¦‚ä½•è¾¾åˆ°Kaggleç«èµ›top 2%ï¼Ÿè¿™é‡Œæœ‰ä¸€ç¯‡ç‰¹å¾æ¢ç´¢ç»éªŒå¸–](https://zhuanlan.zhihu.com/p/48758045) \n  * [å¦‚ä½•åœ¨ Kaggle é¦–æˆ˜ä¸­è¿›å…¥å‰ 10%ï¼Ÿ](https://zhuanlan.zhihu.com/p/27486736)\n  * [Kaggle é¦–æˆ˜ Top 2%, APTOS 2019 å¤ç›˜æ€»ç»“ + æœºå™¨å­¦ä¹ ç«èµ›é€šç”¨æµç¨‹å½’çº³](http://bbs.cvmart.net/topics/1717)\n  * [kaggleçš„riiidæ¯”èµ›é‡Œå…³äºæ•°æ®å¤„ç†æ—¶é—´ç©ºé—´ä¼˜åŒ–çš„ç¬”è®°](https://zhuanlan.zhihu.com/p/344388290)\n* [å¤§æ•°æ®&æœºå™¨å­¦ä¹ ç›¸å…³ç«èµ›æ¨è](https://blog.csdn.net/weixin_33739541/article/details/87565983)\n\n## äºŒ. ç¥ç»ç½‘ç»œæ¨¡å‹æ¦‚è§ˆ		\n\n* [1. ä¸€æ–‡çœ‹æ‡‚25ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹](https://blog.csdn.net/qq_35082030/article/details/73368962)\n* [2. DNNæ¦‚è¿°è®ºæ–‡ï¼šè¯¦è§£å‰é¦ˆã€å·ç§¯å’Œå¾ªç¯ç¥ç»ç½‘ç»œæŠ€æœ¯](https://zhuanlan.zhihu.com/p/29141828)\n* [3. colah''s blog](http://colah.github.io/)\n* [4. Model Zoom](https://modelzoo.co/)\n* [5. DNNæ¦‚è¿°](https://zhuanlan.zhihu.com/p/29141828)\n* [GitHubä¸Šçš„æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ ç»¼è¿°é¡¹ç›®åˆé›†](https://zhuanlan.zhihu.com/p/60245227)\n* [AlphaTree-graphic-deep-neural-network](https://github.com/weslynn/AlphaTree-graphic-deep-neural-network)\n\n### CNN\n\n#### å‘å±•å²\n\n* [94é¡µè®ºæ–‡ç»¼è¿°å·ç§¯ç¥ç»ç½‘ç»œï¼šä»åŸºç¡€æŠ€æœ¯åˆ°ç ”ç©¶å‰æ™¯](https://zhuanlan.zhihu.com/p/35388569)\n\n##### å›¾åƒåˆ†ç±»\n\n* [ä»LeNet-5åˆ°DenseNet](https://zhuanlan.zhihu.com/p/31006686)      \n* [æ·±åº¦å­¦ä¹ ç¬”è®°ï¼ˆåä¸€ï¼‰ç½‘ç»œ Inception, Xception, MobileNet, ShuffeNet, ResNeXt, SqueezeNet, EfficientNet, MixConv](https://www.cnblogs.com/xuanyuyt/p/11329998.html)\n* [CNNç½‘ç»œç»“æ„çš„å‘å±•](https://zhuanlan.zhihu.com/p/68411179)\n* [Awesome - Image Classificationï¼šè®ºæ–‡&&ä»£ç å¤§å…¨](https://github.com/weiaicunzai/awesome-image-classification)\n* [pytorch-image-models](https://github.com/rwightman/pytorch-image-models)\n\n##### ç›®æ ‡æ£€æµ‹\n\n- [æ·±åº¦å­¦ä¹ ä¹‹ç›®æ ‡æ£€æµ‹çš„å‰ä¸–ä»Šç”Ÿï¼ˆMask R-CNNï¼‰](https://zhuanlan.zhihu.com/p/32830206)      \n- [æ·±åº¦å­¦ä¹ ç›®æ ‡æ£€æµ‹æ¨¡å‹å…¨é¢ç»¼è¿°ï¼šFaster R-CNNã€R-FCNå’ŒSSD](https://zhuanlan.zhihu.com/p/29434605)      \n- [ä»RCNNåˆ°SSDï¼Œè¿™åº”è¯¥æ˜¯æœ€å…¨çš„ä¸€ä»½ç›®æ ‡æ£€æµ‹ç®—æ³•ç›˜ç‚¹](https://zhuanlan.zhihu.com/p/36184131)    \n- [ç›®æ ‡æ£€æµ‹ç®—æ³•ç»¼è¿°ä¸‰éƒ¨æ›²](https://zhuanlan.zhihu.com/p/40047760)\n  - [åŸºäºæ·±åº¦å­¦ä¹ çš„ç›®æ ‡æ£€æµ‹ç®—æ³•ç»¼è¿°ï¼ˆä¸€ï¼‰](https://zhuanlan.zhihu.com/p/40047760)\n  - [åŸºäºæ·±åº¦å­¦ä¹ çš„ç›®æ ‡æ£€æµ‹ç®—æ³•ç»¼è¿°ï¼ˆäºŒï¼‰](https://zhuanlan.zhihu.com/p/40020809)\n  - [åŸºäºæ·±åº¦å­¦ä¹ çš„ç›®æ ‡æ£€æµ‹ç®—æ³•ç»¼è¿°ï¼ˆä¸‰ï¼‰](https://zhuanlan.zhihu.com/p/40102001)\n- [From RCNN to YOLOv3]()ï¼š[ä¸Š](https://zhuanlan.zhihu.com/p/35724768)ï¼Œ[ä¸‹](https://zhuanlan.zhihu.com/p/35731743)\n- [å R-CNNæ—¶ä»£ï¼Œ Faster R-CNNã€SSDã€YOLO å„ç±»å˜ä½“ç»Ÿæ²»ä¸‹çš„ç›®æ ‡æ£€æµ‹ç»¼è¿°ï¼šFaster R-CNNç³»åˆ—èƒœäº†å—ï¼Ÿ](https://zhuanlan.zhihu.com/p/38709522)\n- [ç›®æ ‡æ£€æµ‹è¿›åŒ–å²](https://zhuanlan.zhihu.com/p/60590369)\n- [CVPR2019ç›®æ ‡æ£€æµ‹æ–¹æ³•è¿›å±•ç»¼è¿°](https://zhuanlan.zhihu.com/p/59376548)\n- [ä¸€æ–‡çœ‹å°½21ç¯‡ç›®æ ‡æ£€æµ‹æœ€æ–°è®ºæ–‡ï¼ˆè…¾è®¯/Google/å•†æ±¤/æ—·è§†/æ¸…å/æµ™å¤§/CMU/åç§‘/ä¸­ç§‘é™¢ç­‰](https://zhuanlan.zhihu.com/p/61080508)\n- [æˆ‘è¿™ä¸¤å¹´çš„ç›®æ ‡æ£€æµ‹](https://zhuanlan.zhihu.com/p/82491218)\n- [Anchor-Freeç›®æ ‡æ£€æµ‹ç®—æ³•](): [ç¬¬ä¸€ç¯‡ï¼šarxiv2015_baidu_DenseBox](https://zhuanlan.zhihu.com/p/40221183)ï¼Œ [å¦‚ä½•è¯„ä»·æœ€æ–°çš„anchor-freeç›®æ ‡æ£€æµ‹æ¨¡å‹FoveaBoxï¼Ÿ](https://www.zhihu.com/question/319605567/answer/647844997), [FCOS: æœ€æ–°çš„one-stageé€åƒç´ ç›®æ ‡æ£€æµ‹ç®—æ³•](https://zhuanlan.zhihu.com/p/61644900) && [æœ€æ–°çš„Anchor-Freeç›®æ ‡æ£€æµ‹æ¨¡å‹FCOSï¼Œç°å·²å¼€æºï¼](https://zhuanlan.zhihu.com/p/62198865) && [ä¸­ç§‘é™¢ç‰›æ´¥åä¸ºè¯ºäºšæå‡ºCenterNetï¼Œone-stage detectorå¯è¾¾47APï¼Œå·²å¼€æºï¼](https://zhuanlan.zhihu.com/p/62789701) && [AnchorFreeDetection](https://github.com/VCBE123/AnchorFreeDetection)\n- [Anchor freeæ·±åº¦å­¦ä¹ çš„ç›®æ ‡æ£€æµ‹æ–¹æ³•](https://zhuanlan.zhihu.com/p/64563186)\n- [èŠèŠAnchorçš„"å‰ä¸–ä»Šç”Ÿ"ï¼ˆä¸Šï¼‰](https://zhuanlan.zhihu.com/p/63273342)&&[èŠèŠAnchorçš„"å‰ä¸–ä»Šç”Ÿ"ï¼ˆä¸‹ï¼‰](https://zhuanlan.zhihu.com/p/68291859)\n- [ç›®æ ‡æ£€æµ‹ç®—æ³•ç»¼è¿°ä¹‹FPNä¼˜åŒ–ç¯‡](https://zhuanlan.zhihu.com/p/62975854) && [ä¸€æ–‡çœ‹å°½ç‰©ä½“æ£€æµ‹ä¸­çš„å„ç§FPN](https://zhuanlan.zhihu.com/p/148738276)\n- [awesome-object-detectionï¼šè®ºæ–‡&&ä»£ç ](https://github.com/amusi/awesome-object-detection)\n- [deep_learning_object_detection](https://github.com/hoya012/deep_learning_object_detection)\n- [ObjectDetectionImbalance](https://github.com/kemaloksuz/ObjectDetectionImbalance)\n\n##### å›¾åƒåˆ†å‰²ï¼ˆè¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²ã€å…¨æ™¯åˆ†å‰²ï¼‰\n\n* [å›¾åƒè¯­ä¹‰åˆ†å‰²(Semantic segmentation) Survey](https://zhuanlan.zhihu.com/p/36801104)\n* [å¹²è´§ | ä¸€æ–‡æ¦‚è§ˆä¸»è¦è¯­ä¹‰åˆ†å‰²ç½‘ç»œ](https://blog.csdn.net/qq_20084101/article/details/80432960)\n* [è¯­ä¹‰åˆ†å‰² å‘å±•ç»¼è¿°](https://zhuanlan.zhihu.com/p/37618829)       \n* [9102å¹´äº†ï¼Œè¯­ä¹‰åˆ†å‰²çš„å…¥å‘æŒ‡å—å’Œæœ€æ–°è¿›å±•éƒ½æ˜¯ä»€ä¹ˆæ ·çš„](https://zhuanlan.zhihu.com/p/76603228)\n* [å®ä¾‹åˆ†å‰²æœ€æ–°æœ€å…¨é¢ç»¼è¿°ï¼šä»Mask R-CNNåˆ°BlendMask](https://zhuanlan.zhihu.com/p/110132002)\n* [è¯­ä¹‰åˆ†å‰²ç»¼è¿°ï¼šæ·±åº¦å­¦ä¹ èƒŒæ™¯ä¸‹çš„è¯­ä¹‰åˆ†å‰²çš„å‘å±•çŠ¶å†µã€æ¨èã€‘](https://zhuanlan.zhihu.com/p/133212654)\n* [Awesome Semantic Segmentationï¼šè®ºæ–‡&&ä»£ç ](https://github.com/mrgloom/awesome-semantic-segmentation)\n* [ä¸€ç¯‡çœ‹å®Œå°±æ‡‚çš„æœ€æ–°è¯­ä¹‰åˆ†å‰²ç»¼è¿°](https://zhuanlan.zhihu.com/p/110123136)\n* [åŸºäºæ·±åº¦å­¦ä¹ çš„è¯­ä¹‰åˆ†å‰²ç»¼è¿°](https://zhuanlan.zhihu.com/p/142451150)\n\n##### è½»é‡åŒ–å·ç§¯ç¥ç»ç½‘ç»œ\n\n- [çºµè§ˆè½»é‡åŒ–å·ç§¯ç¥ç»ç½‘ç»œï¼šSqueezeNetã€MobileNetã€ShuffleNetã€Xception](https://zhuanlan.zhihu.com/p/32746221)   \n\n##### äººè„¸ç›¸å…³\n\n* [å¦‚ä½•èµ°è¿‘æ·±åº¦å­¦ä¹ äººè„¸è¯†åˆ«ï¼Ÿä½ éœ€è¦è¿™ç¯‡è¶…é•¿ç»¼è¿° | é™„å¼€æºä»£ç ](https://zhuanlan.zhihu.com/p/35295839)    \n* [äººè„¸æ£€æµ‹å’Œè¯†åˆ«ç®—æ³•ç»¼è¿°]() Â  Â  Â \n    * [äººè„¸æ£€æµ‹ç®—æ³•ç»¼è¿° ](https://zhuanlan.zhihu.com/p/36621308) Â  Â  Â  Â  Â \n    * [äººè„¸æ£€æµ‹èƒŒæ™¯ä»‹ç»å’Œå‘å±•ç°çŠ¶](https://zhuanlan.zhihu.com/p/32702868)\n    * [äººè„¸è¯†åˆ«ç®—æ³•æ¼”åŒ–å²](https://zhuanlan.zhihu.com/p/36416906)\n    * [CascadeCNN](https://blog.csdn.net/shuzfan/article/details/50358809) Â \n    * [MTCNN](https://blog.csdn.net/qq_14845119/article/details/52680940)\n    * [awesome-Face_Recognition](https://github.com/ChanChiChoi/awesome-Face_Recognition)\n    * [å¼‚è´¨äººè„¸è¯†åˆ«ç ”ç©¶ç»¼è¿°](https://zhuanlan.zhihu.com/p/64191484)\n    * [è€æ¿æ¥äº†ï¼šäººè„¸è¯†åˆ«+æ‰‹æœºæ¨é€ï¼Œè€æ¿æ¥äº†ä½ ç«‹åˆ»çŸ¥é“ã€‚](https://zhuanlan.zhihu.com/p/26431250)&& [æ‰‹æŠŠæ‰‹æ•™ä½ ç”¨Pythonå®ç°äººè„¸è¯†åˆ«](https://zhuanlan.zhihu.com/p/33456076) && [äººè„¸è¯†åˆ«é¡¹ç›®ï¼Œç½‘ç»œæ¨¡å‹ï¼ŒæŸå¤±å‡½æ•°ï¼Œæ•°æ®é›†ç›¸å…³æ€»ç»“](https://www.jianshu.com/p/e57205edc364)\n    * [åŸºäºæ·±åº¦å­¦ä¹ çš„äººè„¸è¯†åˆ«æŠ€æœ¯ç»¼è¿°](https://zhuanlan.zhihu.com/p/24816781) && [å¦‚ä½•èµ°è¿‘æ·±åº¦å­¦ä¹ äººè„¸è¯†åˆ«ï¼Ÿä½ éœ€è¦è¿™ç¯‡è¶…é•¿ç»¼è¿°](https://zhuanlan.zhihu.com/p/35295839) && [äººè„¸è¯†åˆ«æŸå¤±å‡½æ•°ç»¼è¿°ï¼ˆé™„å¼€æºå®ç°ï¼‰](https://zhuanlan.zhihu.com/p/51324547) && [Face Recognition Loss on Mnist with Pytorch](https://zhuanlan.zhihu.com/p/64427565) && [äººè„¸è¯†åˆ«çš„LOSSï¼ˆä¸Šï¼‰](https://zhuanlan.zhihu.com/p/34404607) && [äººè„¸è¯†åˆ«çš„LOSSï¼ˆä¸‹ï¼‰](https://zhuanlan.zhihu.com/p/34436551)\n* [äººè„¸å…³é”®ç‚¹æ£€æµ‹]()\n    * [ã€æ¯å‘¨CVè®ºæ–‡æ¨èã€‘ åˆå­¦æ·±åº¦å­¦ä¹ äººè„¸å…³é”®ç‚¹æ£€æµ‹å¿…è¯»æ–‡ç« ](https://zhuanlan.zhihu.com/p/88344339)\n    * [ä»ä¼ ç»Ÿæ–¹æ³•åˆ°æ·±åº¦å­¦ä¹ ï¼Œäººè„¸å…³é”®ç‚¹æ£€æµ‹æ–¹æ³•ç»¼è¿°](https://mp.weixin.qq.com/s/CvdeV5xgUF0kStJQdRst0w)\n    * [äººè„¸å…³é”®ç‚¹æ£€æµ‹ç»¼è¿°](https://zhuanlan.zhihu.com/p/42968117)\n    * [äººè„¸ä¸“é›†4 | é®æŒ¡ã€å…‰ç…§ç­‰å› ç´ çš„äººè„¸å…³é”®ç‚¹æ£€æµ‹](https://zhuanlan.zhihu.com/p/62824113)\n    * [ã€Face key point detectionã€‘äººè„¸å…³é”®ç‚¹æ£€æµ‹å®ç°](https://zhuanlan.zhihu.com/p/52525598)\n    * [OpenCVå®æˆ˜ï¼šäººè„¸å…³é”®ç‚¹æ£€æµ‹ï¼ˆFaceMarkï¼‰](https://zhuanlan.zhihu.com/p/35390012)\n    * [CenterFace+TensorRTéƒ¨ç½²äººè„¸å’Œå…³é”®ç‚¹æ£€æµ‹400fps](https://zhuanlan.zhihu.com/p/106774468)\n\n##### å›¾åƒè¶…åˆ†è¾¨ç‡\n\n* [æ·±åº¦å­¦ä¹ å›¾åƒè¶…åˆ†è¾¨ç‡ç»¼è¿°](https://zhuanlan.zhihu.com/p/57564211)\n* [ä»SRCNNåˆ°EDSRï¼Œæ€»ç»“æ·±åº¦å­¦ä¹ ç«¯åˆ°ç«¯è¶…åˆ†è¾¨ç‡æ–¹æ³•å‘å±•å†ç¨‹](https://zhuanlan.zhihu.com/p/31664818)\n\n##### è¡Œäººé‡è¯†åˆ«\n\n* [ã€CVPR2019æ­£å¼å…¬å¸ƒã€‘è¡Œäººé‡è¯†åˆ«è®ºæ–‡](https://zhuanlan.zhihu.com/p/62843442)\n* [ã€CVPR2019æ­£å¼å…¬å¸ƒã€‘è¡Œäººé‡è¯†åˆ«è®ºæ–‡](https://zhuanlan.zhihu.com/p/62843442)ï¼Œ[2019 è¡Œäººå†è¯†åˆ«å¹´åº¦è¿›å±•å›é¡¾](https://zhuanlan.zhihu.com/p/64004977)\n\n##### å›¾åƒç€è‰²\n\n* [Awesome-Image-Colorization](https://github.com/MarkMoHR/Awesome-Image-Colorization)\n\n##### è¾¹æ£€æµ‹\n\n* [Awesome-Edge-Detection-Papers](https://github.com/MarkMoHR/Awesome-Edge-Detection-Papers)\n\n##### OCR&&æ–‡æœ¬æ£€æµ‹\n\n* [2019CVPRæ–‡æœ¬æ£€æµ‹ç»¼è¿°](https://zhuanlan.zhihu.com/p/67319122)\n* [OCRæ–‡å­—å¤„ç†](https://zhuanlan.zhihu.com/p/65707543)\n* [è‡ªç„¶åœºæ™¯æ–‡æœ¬æ£€æµ‹è¯†åˆ«æŠ€æœ¯ç»¼è¿°](https://mp.weixin.qq.com/s?__biz=MzU4MjQ3MDkwNA==&mid=2247485142&idx=1&sn=c0e01da30eb5e750be453eabe4be2bf4&chksm=fdb69b41cac11257ae22c7dac395e9651dab628fc35dd6d3c02d9566a8c7f5f2b56353d58a64&token=1065243837&lang=zh_CN#rd)\n\n##### ç‚¹äº‘\n\n* [awesome-point-cloud-analysis](https://zhuanlan.zhihu.com/p/65690433)\n\n##### ç»†ç²’åº¦å›¾åƒåˆ†ç±»\n\n* [è¶…å…¨æ·±åº¦å­¦ä¹ ç»†ç²’åº¦å›¾åƒåˆ†æï¼šé¡¹ç›®ã€ç»¼è¿°ã€æ•™ç¨‹ä¸€ç½‘æ‰“å°½](https://zhuanlan.zhihu.com/p/73542103)\n\n##### å›¾åƒæ£€ç´¢\n\n* å›¾åƒæ£€ç´¢çš„åå¹´[ä¸Š](https://mp.weixin.qq.com/s/sM78DCOK3fuG2JrP2QaSZA)ã€[ä¸‹](https://mp.weixin.qq.com/s/yzVMDEpwbXVS0y-CwWSBEA)\n\n##### äººç¾¤è®¡æ•°\n\n* [äººç¾¤è®¡æ•°](http://chuansong.me/n/443237851736), [1](https://www.cnblogs.com/wmr95/p/8134692.html), [2](https://blog.csdn.net/u011285477/article/details/51954989), [3](https://blog.csdn.net/qingqingdeaini/article/details/79922549)\n\n#### æ•™ç¨‹\n\n##### å‰é¦ˆç¥ç»ç½‘ç»œ\n\n* [ä»åŸºæœ¬åŸç†åˆ°æ¢¯åº¦ä¸‹é™ï¼Œå°ç™½éƒ½èƒ½çœ‹æ‡‚çš„ç¥ç»ç½‘ç»œæ•™ç¨‹](https://zhuanlan.zhihu.com/p/59385110)\n\n##### æ¿€æ´»å‡½æ•°\n\n* [æ¿€æ´»å‡½æ•°ä¸€è§ˆ](https://zhuanlan.zhihu.com/p/30567264) && [æ·±åº¦å­¦ä¹ ä¸­å‡ ç§å¸¸è§çš„æ¿€æ´»å‡½æ•°ç†è§£ä¸æ€»ç»“](https://www.cnblogs.com/XDU-Lakers/p/10557496.html)\n* [ä¸€ä¸ªæ¿€æ´»å‡½æ•°éœ€è¦å…·æœ‰å“ªäº›å¿…è¦çš„å±æ€§](https://www.zhihu.com/question/67366051)\n\n##### åå‘ä¼ æ’­ç®—æ³•\n\n* [åå‘ä¼ æ’­ç®—æ³•ï¼ˆè¿‡ç¨‹åŠå…¬å¼æ¨å¯¼ï¼‰](https://blog.csdn.net/u014313009/article/details/51039334)\n* [é€šä¿—ç†è§£ç¥ç»ç½‘ç»œBPä¼ æ’­ç®—æ³•](https://zhuanlan.zhihu.com/p/24801814)\n\n##### ä¼˜åŒ–é—®é¢˜\n\n* [ç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„æ¢¯åº¦æ¶ˆå¤±ä¸æ¢¯åº¦çˆ†ç‚¸](https://zhuanlan.zhihu.com/p/25631496)\n* [æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜è¯¦è§£](https://www.jianshu.com/p/3f35e555d5ba)\n* [è¯¦è§£æ·±åº¦å­¦ä¹ ä¸­çš„æ¢¯åº¦æ¶ˆå¤±ã€çˆ†ç‚¸åŸå› åŠå…¶è§£å†³æ–¹æ³•](https://zhuanlan.zhihu.com/p/33006526) && [ç¥ç»ç½‘ç»œæ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸åŠè§£å†³åŠæ³•](https://blog.csdn.net/program_developer/article/details/80032376)\n\n##### å·ç§¯å±‚\n\n* [A Comprehensive Introduction to Different Types of Convolutions in Deep Learning](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215) && ç¿»è¯‘ï¼š[ä¸Š](https://www.leiphone.com/news/201902/D2Mkv61w9IPq9qGh.html)ã€[ä¸‹](https://www.leiphone.com/news/201902/biIqSBpehsaXFwpN.html?uniqueCode=OTEsp9649VqJfUcO)\n* [å·ç§¯æœ‰å¤šå°‘ç§ï¼Ÿä¸€æ–‡è¯»æ‡‚æ·±åº¦å­¦ä¹ ä¸­çš„å„ç§å·ç§¯](https://zhuanlan.zhihu.com/p/57575810)\n* [å„ç§å·ç§¯](https://www.cnblogs.com/cvtoEyes/p/8848815.html)\n* [Convolution NetworkåŠå…¶å˜ç§ï¼ˆåå·ç§¯ã€æ‰©å±•å·ç§¯ã€å› æœå·ç§¯ã€å›¾å·ç§¯ï¼‰](https://www.cnblogs.com/yangperasd/p/7071657.html)\n* [æ·±åº¦å­¦ä¹ åŸºç¡€--å·ç§¯ç±»å‹](https://zhuanlan.zhihu.com/p/59839551)\n* [å˜å½¢å·ç§¯æ ¸ã€å¯åˆ†ç¦»å·ç§¯](https://zhuanlan.zhihu.com/p/28749411)\n* [å¯¹æ·±åº¦å¯åˆ†ç¦»å·ç§¯ã€åˆ†ç»„å·ç§¯ã€æ‰©å¼ å·ç§¯ã€è½¬ç½®å·ç§¯ï¼ˆåå·ç§¯ï¼‰çš„ç†è§£](https://blog.csdn.net/chaolei3/article/details/79374563)\n* [åå·ç§¯](https://buptldy.github.io/2016/10/29/2016-10-29-deconv/) \n* [Dilated/Atrous conv ç©ºæ´å·ç§¯/å¤šå­”å·ç§¯](https://blog.csdn.net/silence2015/article/details/79748729)\n* [å·ç§¯å±‚è¾“å‡ºå¤§å°å°ºå¯¸è®¡ç®—åŠ â€œSAMEâ€ å’Œ â€œVALIDâ€](https://blog.csdn.net/weixin_37697191/article/details/89527315) && [å·ç§¯çš„ä¸‰ç§æ¨¡å¼full, same, validä»¥åŠpaddingçš„same, valid](https://zhuanlan.zhihu.com/p/62760780)\n* [æ­£å¸¸å·ç§¯ä¸ç©ºæ´å·ç§¯è¾“å‡ºç‰¹å¾å›¾ä¸æ„Ÿå—é‡å¤§å°çš„è®¡ç®—](https://blog.csdn.net/qq_43232545/article/details/103317773)\n* [ã€Tensorflowã€‘tf.nn.depthwise_conv2då¦‚ä½•å®ç°æ·±åº¦å·ç§¯?](https://blog.csdn.net/mao_xiao_feng/article/details/78003476)	\n* [ã€Tensorflowã€‘tf.nn.atrous_conv2då¦‚ä½•å®ç°ç©ºæ´å·ç§¯ï¼Ÿ](https://blog.csdn.net/mao_xiao_feng/article/details/78003730)\n* [ã€Tensorflowã€‘tf.nn.separable_conv2då¦‚ä½•å®ç°æ·±åº¦å¯åˆ†å·ç§¯?](https://blog.csdn.net/mao_xiao_feng/article/details/78002811)	\n* [ã€TensorFlowã€‘tf.nn.conv2d_transposeæ˜¯æ€æ ·å®ç°åå·ç§¯çš„ï¼Ÿ](https://blog.csdn.net/mao_xiao_feng/article/details/71713358)\n\n##### æ± åŒ–å±‚\n\n* [å·ç§¯ç¥ç»ç½‘ç»œä¸­çš„å„ç§æ± åŒ–æ“ä½œ](https://zhuanlan.zhihu.com/p/112216409)\n\n##### å·ç§¯ç¥ç»ç½‘ç»œ\n\n- [å·ç§¯ç¥ç»ç½‘ç»œå·¥ä½œåŸç†](https://www.zhihu.com/question/39022858)\n- [ã€Œä¸ƒå¤•çš„ç¤¼ç‰©ã€: ä¸€æ—¥ææ‡‚å·ç§¯ç¥ç»ç½‘ç»œ](https://zhuanlan.zhihu.com/p/28863709)\n- [ä¸€æ–‡è¯»æ‡‚å·ç§¯ç¥ç»ç½‘ç»œä¸­çš„1x1å·ç§¯æ ¸](https://zhuanlan.zhihu.com/p/40050371)\n- [å¦‚ä½•ç†è§£ç¥ç»ç½‘ç»œä¸­é€šè¿‡addå’Œconcateçš„æ–¹å¼èåˆç‰¹å¾ï¼Ÿ](https://blog.csdn.net/xiaojiajia007/article/details/86008415) && [ç¥ç»ç½‘ç»œä¸­å¯¹éœ€è¦concatçš„ç‰¹å¾è¿›è¡Œçº¿æ€§å˜æ¢ç„¶åç›¸åŠ æ˜¯å¦å¥½äºç›´æ¥concat?](https://www.zhihu.com/question/389912594/answer/1178054600)\n- [CNN æ¨¡å‹æ‰€éœ€çš„è®¡ç®—åŠ›ï¼ˆflopsï¼‰å’Œå‚æ•°ï¼ˆparametersï¼‰æ•°é‡æ˜¯æ€ä¹ˆè®¡ç®—çš„ï¼Ÿ](https://www.zhihu.com/question/65305385) && [æ·±åº¦å­¦ä¹ ä¸­å·ç§¯çš„å‚æ•°é‡å’Œè®¡ç®—é‡](https://www.cnblogs.com/hejunlin1992/p/12978988.html)\n\n##### å›¾åƒåˆ†ç±»ç½‘ç»œè¯¦è§£\n\n* [ç»å…¸CNNæ¨¡å‹LeNetè§£è¯»](https://zhuanlan.zhihu.com/p/41736894)\n* [æœºå™¨å­¦ä¹ è¿›é˜¶ç¬”è®°ä¹‹ä¸‰ | æ·±å…¥ç†è§£Alexnet](https://zhuanlan.zhihu.com/p/22659166)\n* [ä¸€æ–‡è¯»æ‡‚VGGç½‘ç»œ](https://zhuanlan.zhihu.com/p/41423739)\n* [Inception V1,V2,V3,V4 æ¨¡å‹æ€»ç»“](https://zhuanlan.zhihu.com/p/52802896)\n* [ResNetè§£æ](https://blog.csdn.net/lanran2/article/details/79057994)\n* [ä¸€æ–‡ç®€è¿°ResNetåŠå…¶å¤šç§å˜ä½“](https://zhuanlan.zhihu.com/p/35985680)\n* [CapsNetå…¥é—¨ç³»åˆ—](http://mp.weixin.qq.com/s?__biz=MzI3ODkxODU3Mg==&mid=2247484099&idx=1&sn=97e209f1a9860c8d8c51e81d98fc8a0a&chksm=eb4ee600dc396f16624a33cdfc0ead905e62ae9447b49b20146020e6cbd7d71f089101512a40&scene=21#wechat_redirect)\n  - [CapsNetå…¥é—¨ç³»åˆ—ä¹‹ä¸€ï¼šèƒ¶å›Šç½‘ç»œèƒŒåçš„ç›´è§‰](http://mp.weixin.qq.com/s?__biz=MzI3ODkxODU3Mg==&mid=2247484099&idx=1&sn=97e209f1a9860c8d8c51e81d98fc8a0a&chksm=eb4ee600dc396f16624a33cdfc0ead905e62ae9447b49b20146020e6cbd7d71f089101512a40&scene=21#wechat_redirect)\n  - [CapsNetå…¥é—¨ç³»åˆ—ä¹‹äºŒï¼šèƒ¶å›Šå¦‚ä½•å·¥ä½œ](http://mp.weixin.qq.com/s?__biz=MzI3ODkxODU3Mg==&mid=2247484165&idx=1&sn=0ca679e3a5f499f8d8addb405fe3df83&chksm=eb4ee7c6dc396ed0a330fcac12690110bcaf9a8a10794dbc5e1a326c69ecbb140140f55fd6ba&scene=21#wechat_redirect)\n  - [CapsNetå…¥é—¨ç³»åˆ—ä¹‹ä¸‰ï¼šå›Šé—´åŠ¨æ€è·¯ç”±ç®—æ³•](http://mp.weixin.qq.com/s?__biz=MzI3ODkxODU3Mg==&mid=2247484433&idx=1&sn=3afe4605bc2501eebbc41c6dd1af9572&chksm=eb4ee0d2dc3969c4619d6c1097d5c949c76c6c854e60d36eba4388da2c3855747818d062c90a&scene=21#wechat_redirect)\n  - [CapsNetå…¥é—¨ç³»åˆ—ä¹‹å››ï¼šèƒ¶å›Šç½‘ç»œæ¶æ„](https://mp.weixin.qq.com/s/6CRSen8P6zKaMGtX8IRfqw)\n* [æ·±å…¥å‰–æMobileNetå’Œå®ƒçš„å˜ç§ï¼ˆä¾‹å¦‚ï¼šShuffleNetï¼‰ä¸ºä»€ä¹ˆä¼šå˜å¿«ï¼Ÿ](https://zhuanlan.zhihu.com/p/158591662)\n* [CNNæ¨¡å‹ä¹‹ShuffleNet](https://zhuanlan.zhihu.com/p/32304419)\n* [ShuffleNet V2å’Œå››ä¸ªç½‘ç»œæ¶æ„è®¾è®¡å‡†åˆ™](https://zhuanlan.zhihu.com/p/40980942)\n* [ResNeXt æ·±å…¥è§£è¯»ä¸æ¨¡å‹å®ç°](https://zhuanlan.zhihu.com/p/78019001)\n* [å¦‚ä½•è¯„ä»·Momenta ImageNet 2017å¤ºå† æ¶æ„SENet?](https://www.zhihu.com/question/63460684)\n* [CBAMï¼šå·ç§¯å—æ³¨æ„åŠ›æ¨¡å—](https://zhuanlan.zhihu.com/p/79419670) && [CBAM: Convolutional Block Attention Module](https://zhuanlan.zhihu.com/p/65529934)\n* [SKNetâ€”â€”SENetå­ªç”Ÿå…„å¼Ÿç¯‡](https://zhuanlan.zhihu.com/p/59690223)\n* [GCNetï¼šå½“Non-localé‡è§SENet](https://zhuanlan.zhihu.com/p/64988633)\n* [æ·±åº¦å­¦ä¹ ç¬”è®°ï¼ˆåä¸€ï¼‰ç½‘ç»œ Inception, Xception, MobileNet, ShuffeNet, ResNeXt, SqueezeNet, EfficientNet, MixConv](https://www.cnblogs.com/xuanyuyt/p/11329998.html)\n* [å¦‚ä½•è¯„ä»·æœ€æ–°çš„Octave Convolutionï¼Ÿ](https://www.zhihu.com/question/320462422)\n* [ResNeSt ä¹‹è¯­ä¹‰åˆ†å‰²](https://zhuanlan.zhihu.com/p/136105870) && [å…³äºResNeStçš„ç‚¹æ»´ç–‘æƒ‘](https://zhuanlan.zhihu.com/p/133805433) && [ResNeStåœ¨åˆ·æ¦œä¹‹åè¢«ECCV2020 strong reject](https://zhuanlan.zhihu.com/p/143214871)\n\n##### ç›®æ ‡æ£€æµ‹ç½‘ç»œè¯¦è§£\n\n* [ç›®æ ‡æ£€æµ‹çš„æ€§èƒ½è¯„ä»·æŒ‡æ ‡](https://zhuanlan.zhihu.com/p/70306015) && [NMSå’Œè®¡ç®—mAPæ—¶çš„ç½®ä¿¡åº¦é˜ˆå€¼å’ŒIoUé˜ˆå€¼ ](https://zhuanlan.zhihu.com/p/75348108) && [ç™½è¯mAP](https://zhuanlan.zhihu.com/p/60834912) && [ç›®æ ‡æ£€æµ‹æ¨¡å‹çš„è¯„ä¼°æŒ‡æ ‡mAPè¯¦è§£(é™„ä»£ç ï¼‰](https://zhuanlan.zhihu.com/p/37910324)\n* [æ·±åº¦å­¦ä¹ ä¸­IUã€IoU(Intersection over Union)](https://blog.csdn.net/iamoldpan/article/details/78799857)\n* [Selective Search for Object Detection ](https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/)[ï¼ˆè¯‘æ–‡ï¼‰](https://blog.csdn.net/guoyunfei20/article/details/78723646)\n* [Region Proposal Network(RPN)](https://zhuanlan.zhihu.com/p/106192020)\n* [è¾¹æ¡†å›å½’(Bounding Box Regression)è¯¦è§£](https://blog.csdn.net/zijin0802034/article/details/77685438)\n* [NMSâ€”â€”éæå¤§å€¼æŠ‘åˆ¶](https://blog.csdn.net/shuzfan/article/details/52711706) && [éæå¤§å€¼æŠ‘åˆ¶NMSçš„pythonå®ç°](https://zhuanlan.zhihu.com/p/128125301)\n* [ä¸€æ–‡æ‰“å°½ç›®æ ‡æ£€æµ‹NMSâ€”â€”ç²¾åº¦æå‡ç¯‡](https://zhuanlan.zhihu.com/p/151914931) && [ä¸€æ–‡æ‰“å°½ç›®æ ‡æ£€æµ‹NMSâ€”â€”æ•ˆç‡æå‡ç¯‡](https://zhuanlan.zhihu.com/p/157900024)\n* [ç›®æ ‡æ£€æµ‹å›å½’æŸå¤±å‡½æ•°ç®€ä»‹ï¼šSmoothL1/IoU/GIoU/DIoU/CIoU Loss](https://zhuanlan.zhihu.com/p/104236411)\n* [å°†CNNå¼•å…¥ç›®æ ‡æ£€æµ‹çš„å¼€å±±ä¹‹ä½œï¼šR-CNN](https://zhuanlan.zhihu.com/p/23006190)\n* [R-CNNè®ºæ–‡è¯¦è§£](https://blog.csdn.net/u014696921/article/details/52824097)\n* [æ·±åº¦å­¦ä¹ ï¼ˆåå…«ï¼‰åŸºäºR-CNNçš„ç‰©ä½“æ£€æµ‹](https://blog.csdn.net/hjimce/article/details/50187029)\n* [Fast R-CNN](https://zhuanlan.zhihu.com/p/24780395)\n* [æ·±åº¦å­¦ä¹ ï¼ˆå…­åå››ï¼‰Faster R-CNNç‰©ä½“æ£€æµ‹](https://blog.csdn.net/hjimce/article/details/73382553) && [ä½ çœŸçš„å­¦ä¼šRoI Poolingäº†å—?](https://zhuanlan.zhihu.com/p/59692298)\n* [ç›®æ ‡æ£€æµ‹è®ºæ–‡é˜…è¯»ï¼šFeature Pyramid Networks for Object Detection](https://zhuanlan.zhihu.com/p/36461718)\n* [SSD](https://zhuanlan.zhihu.com/p/24954433)\n* [å®ä¾‹åˆ†å‰²--Mask RCNNè¯¦è§£(ROI Align / Loss Function)](https://www.codetd.com/article/2554465) && [ä»¤äººæ‹æ¡ˆç§°å¥‡çš„Mask RCNN](https://zhuanlan.zhihu.com/p/37998710)\n* [ä½•æºæ˜å¤§ç¥çš„ã€ŒFocal Lossã€ï¼Œå¦‚ä½•æ›´å¥½åœ°ç†è§£ï¼Ÿ](https://zhuanlan.zhihu.com/p/32423092) && [FocalLoss å¯¹æ ·æœ¬ä¸å¹³è¡¡çš„æƒé‡è°ƒèŠ‚å’Œå‡ä½æŸå¤±å€¼](https://zhuanlan.zhihu.com/p/82148525) && [focal_loss å¤šç±»åˆ«å’ŒäºŒåˆ†ç±» Pytorchä»£ç å®ç°](https://blog.csdn.net/qq_33278884/article/details/91572173) && [å¤šåˆ†ç±»focal lossåŠå…¶tensorflowå®ç°](https://blog.csdn.net/qq_39012149/article/details/96184383)\n* [å ªæ¯”Focal Lossï¼è§£å†³ç›®æ ‡æ£€æµ‹ä¸­æ ·æœ¬ä¸å¹³è¡¡çš„æ— é‡‡æ ·æ–¹æ³•](https://zhuanlan.zhihu.com/p/93658728)\n* [ç›®æ ‡æ£€æµ‹æ­£è´Ÿæ ·æœ¬åŒºåˆ†ç­–ç•¥å’Œå¹³è¡¡ç­–ç•¥æ€»ç»“(ä¸€)](https://zhuanlan.zhihu.com/p/138824387) && [ç›®æ ‡æ£€æµ‹æ­£è´Ÿæ ·æœ¬åŒºåˆ†ç­–ç•¥å’Œå¹³è¡¡ç­–ç•¥æ€»ç»“(äºŒ)](https://zhuanlan.zhihu.com/p/138828372) && [ç›®æ ‡æ£€æµ‹æ­£è´Ÿæ ·æœ¬åŒºåˆ†ç­–ç•¥å’Œå¹³è¡¡ç­–ç•¥æ€»ç»“(ä¸‰ï¼‰](https://zhuanlan.zhihu.com/p/144659734)\n* [YOLO](http://www.mamicode.com/info-detail-2314392.html) && [ç›®æ ‡æ£€æµ‹|YOLOåŸç†ä¸å®ç°](https://zhuanlan.zhihu.com/p/32525231) && [å›¾è§£YOLO](https://zhuanlan.zhihu.com/p/24916786) && [ã€è®ºæ–‡è§£è¯»ã€‘Yoloä¸‰éƒ¨æ›²è§£è¯»â€”â€”Yolov1](https://zhuanlan.zhihu.com/p/70387154)\n* [ç›®æ ‡æ£€æµ‹|YOLOv2åŸç†ä¸å®ç°(é™„YOLOv3)](https://zhuanlan.zhihu.com/p/35325884?group_id=966229905398362112) && [YOLO2](https://zhuanlan.zhihu.com/p/25167153) && [ã€è®ºæ–‡è§£è¯»ã€‘Yoloä¸‰éƒ¨æ›²è§£è¯»â€”â€”Yolov2](https://zhuanlan.zhihu.com/p/74540100)\n* [<æœºå™¨çˆ±å­¦ä¹ >YOLO v3æ·±å…¥ç†è§£](https://zhuanlan.zhihu.com/p/49556105) && [ã€è®ºæ–‡è§£è¯»ã€‘Yoloä¸‰éƒ¨æ›²è§£è¯»â€”â€”Yolov3](https://zhuanlan.zhihu.com/p/76802514)\n* [YOLOv4](https://zhuanlan.zhihu.com/p/138510087)\n* [ç›®æ ‡æ£€æµ‹ä¹‹CornerNet](https://arxiv.org/abs/1808.01244), [1](https://zhuanlan.zhihu.com/p/41825737), [2](https://blog.csdn.net/Hibercraft/article/details/81637451), [3](https://zhuanlan.zhihu.com/p/41759548)\n* [ç›®æ ‡æ£€æµ‹å°tricks--æ ·æœ¬ä¸å‡è¡¡å¤„ç†](https://zhuanlan.zhihu.com/p/60612064)\n\n##### å›¾åƒåˆ†å‰²ç½‘ç»œè¯¦è§£\n\n* [è¶…åƒç´ ã€è¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²ã€å…¨æ™¯åˆ†å‰² å‚»å‚»åˆ†ä¸æ¸… ](https://zhuanlan.zhihu.com/p/50996404) && [è¯­ä¹‰åˆ†å‰²ã€å®ä¾‹åˆ†å‰²å’Œå…¨æ™¯åˆ†å‰²çš„åŒºåˆ«](https://blog.csdn.net/u013066730/article/details/103613154)\n* [è¯­ä¹‰åˆ†å‰²å·ç§¯ç¥ç»ç½‘ç»œå¿«é€Ÿå…¥é—¨](https://blog.csdn.net/qq_20084101/article/details/80455877)          \n* [å›¾åƒè¯­ä¹‰åˆ†å‰²å…¥é—¨+FCN/U-Netç½‘ç»œè§£æ](https://zhuanlan.zhihu.com/p/31428783) && [æ·±å…¥ç†è§£æ·±åº¦å­¦ä¹ åˆ†å‰²ç½‘ç»œï¼µnet](https://blog.csdn.net/Formlsl/article/details/80373200)\n* [Unetç¥ç»ç½‘ç»œä¸ºä»€ä¹ˆä¼šåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²è¡¨ç°å¥½ï¼Ÿ](https://www.zhihu.com/question/269914775)\n* [å›¾åƒè¯­ä¹‰åˆ†å‰²çš„å·¥ä½œåŸç†å’ŒCNNæ¶æ„å˜è¿](https://zhuanlan.zhihu.com/p/38033032)\n* [è¯­ä¹‰åˆ†å‰²ä¸­çš„Attentionå’Œä½ç§©é‡å»º](https://zhuanlan.zhihu.com/p/77834369)\n* [æ‰“é€šå¤šä¸ªè§†è§‰ä»»åŠ¡çš„å…¨èƒ½Backbone:HRNet](https://zhuanlan.zhihu.com/p/134253318)\n\n##### æ³¨æ„åŠ›æœºåˆ¶\n\n* [æ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æ¨¡å‹ï¼ˆ2017ç‰ˆï¼‰](https://zhuanlan.zhihu.com/p/37601161)\n* [Attention Modelï¼ˆmechanismï¼‰ çš„ å¥—è·¯](https://blog.csdn.net/bvl10101111/article/details/78470716)\n* [è®¡ç®—æœºè§†è§‰ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆæ¨èï¼‰](https://zhuanlan.zhihu.com/p/146130215) \n* [More About Attentionï¼ˆæ¨èï¼‰](https://zhuanlan.zhihu.com/p/106662375)\n* [è®¡ç®—æœºè§†è§‰ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶](https://zhuanlan.zhihu.com/p/32928645)\n* [NLPä¸­çš„Attention Mechanism](https://zhuanlan.zhihu.com/p/31547842)\n* [Transformerä¸­çš„Attention](https://mp.weixin.qq.com/s/k8PdZAld2ANVoekuyQxI3w)\n* [ç»¼è¿°ï¼šå›¾åƒå¤„ç†ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶](https://bbs.cvmart.net/topics/2581)\n\n##### ç‰¹å¾èåˆ\n\n* [ç›˜ç‚¹ç›®æ ‡æ£€æµ‹ä¸­çš„ç‰¹å¾èåˆæŠ€å·§ï¼ˆæ ¹æ®YOLO v4æ€»ç»“ï¼‰](https://zhuanlan.zhihu.com/p/141685352)\n* [å¤šå°ºåº¦èåˆä»‹ç»](https://zhuanlan.zhihu.com/p/147820687)\n\n#### Action\n\n* [PyTorchå®˜æ–¹å®ç°ResNet](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py) && [pytorch_resnet_cifar10](https://github.com/akamaster/pytorch_resnet_cifar10)\n* [PyTorch 63.Coding for FLOPs, Params and Latency](https://zhuanlan.zhihu.com/p/268816646)\n* [å…ˆè¯»æ‡‚CapsNetæ¶æ„ç„¶åç”¨TensorFlowå®ç°](https://zhuanlan.zhihu.com/p/30753326)\n* [ç›®æ ‡æ£€æµ‹-20ç§æ¨¡å‹çš„åŸå‘³ä»£ç æ±‡æ€»](https://zhuanlan.zhihu.com/p/37056927)     \n* [TensorFlow Object Detection API æ•™ç¨‹](https://blog.csdn.net/qq_36148847/article/details/79306762)\n  * [TensorFlow å¯¹è±¡æ£€æµ‹ API æ•™ç¨‹1](https://blog.csdn.net/qq_36148847/article/details/79306762)\n  * [TensorFlow å¯¹è±¡æ£€æµ‹ API æ•™ç¨‹2](https://blog.csdn.net/qq_36148847/article/details/79307598)\n  * [TensorFlow å¯¹è±¡æ£€æµ‹ API æ•™ç¨‹3](https://blog.csdn.net/qq_36148847/article/details/79307751)\n  * [TensorFlow å¯¹è±¡æ£€æµ‹ API æ•™ç¨‹ 4](https://blog.csdn.net/qq_36148847/article/details/79307931)\n  * [TensorFlow å¯¹è±¡æ£€æµ‹ API æ•™ç¨‹5](https://blog.csdn.net/qq_36148847/article/details/79307933)\n* [åœ¨TensorFlow+Kerasç¯å¢ƒä¸‹ä½¿ç”¨RoIæ± åŒ–ä¸€æ­¥æ­¥å®ç°æ³¨æ„åŠ›æœºåˆ¶](https://zhuanlan.zhihu.com/p/65327747)\n* [mxnetå¦‚ä½•æŸ¥çœ‹å‚æ•°æ•°é‡](https://discuss.gluon.ai/t/topic/7216) && [mxnetæŸ¥çœ‹FLOPS](https://github.com/likelyzhao/CalFLOPS-Mxnet)\n* [Pytorch-UNet](https://github.com/milesial/Pytorch-UNet)\n* [segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch)\n\n### GAN\n\n#### å‘å±•å²      \n\n* [åƒå¥‡ç™¾æ€ªçš„GANå˜ä½“](https://zhuanlan.zhihu.com/p/26491601)      \n* [è‹å‰‘æ—åšå®¢ï¼Œè®²è§£å¾—æ·‹æ¼“å°½è‡´](https://kexue.fm/tag/GAN/)\n* [The GAN Landscapeï¼šLosses, Architectures, Regularization, and Normalization](https://arxiv.org/pdf/1807.04720.pdf)\n* [æ·±åº¦å­¦ä¹ æ–°æ˜Ÿï¼šGANçš„åŸºæœ¬åŸç†ã€åº”ç”¨å’Œèµ°å‘](https://www.leiphone.com/news/201701/Kq6FvnjgbKK8Lh8N.html)\n* [GANç”Ÿæˆå›¾åƒç»¼è¿°](https://zhuanlan.zhihu.com/p/62746494)\n* [2017å¹´GAN è®¡ç®—æœºè§†è§‰ç›¸å…³paperæ±‡æ€»](https://zhuanlan.zhihu.com/p/29882709)\n* [å¿…è¯»çš„10ç¯‡å…³äºGANçš„è®ºæ–‡](https://zhuanlan.zhihu.com/p/72745900)\n\n#### æ•™ç¨‹     \n\n* [GANåŸç†å­¦ä¹ ç¬”è®°](https://zhuanlan.zhihu.com/p/27295635)\n* [GANä¸‡å­—é•¿æ–‡ç»¼è¿°](https://zhuanlan.zhihu.com/p/58812258)\n* [æç«¯å›¾åƒå‹ç¼©çš„å¯¹æŠ—ç”Ÿæˆç½‘ç»œ](https://zhuanlan.zhihu.com/p/35783437?group_id=969598777652420608)\n* [å°æ¹¾å¤§å­¦æå®æ¯…GANæ•™ç¨‹](https://www.youtube.com/watch?v=0CKeqXl5IY0&feature=youtu.be)\n    * [Basic](https://github.com/Mikoto10032/DeepLearning/blob/master/books/GAN-Basic%20Idea%20(2017.04.21).pdf)\n    * [Improving](https://github.com/Mikoto10032/DeepLearning/blob/master/books/GAN-Improving%20GAN%20(2017.05.05).pdf)\n* [CycleGANï¼šå›¾ç‰‡é£æ ¼ï¼Œæƒ³æ¢å°±æ¢ | ICCV 2017è®ºæ–‡è§£è¯»](https://zhuanlan.zhihu.com/p/34711316)\n* [Wasserstein GAN](https://zhuanlan.zhihu.com/p/25071913) && [GANï¼šä¸¤è€…åˆ†å¸ƒä¸é‡åˆJSæ•£åº¦ä¸ºlog2çš„æ•°å­¦è¯æ˜](https://blog.csdn.net/Invokar/article/details/88917214)\n* [ç”¨å˜åˆ†æ¨æ–­ç»Ÿä¸€ç†è§£ç”Ÿæˆæ¨¡å‹ï¼ˆVAEã€GANã€AAEã€ALIï¼‰](https://zhuanlan.zhihu.com/p/40105143)\n#### Action     \n\n* [GANå­¦ä¹ æŒ‡å—ï¼šä»åŸç†å…¥é—¨åˆ°åˆ¶ä½œç”ŸæˆDemo](https://zhuanlan.zhihu.com/p/24767059)    \n* [æœºå™¨ä¹‹å¿ƒGitHubé¡¹ç›®ï¼šGANå®Œæ•´ç†è®ºæ¨å¯¼ä¸å®ç°](https://zhuanlan.zhihu.com/p/29837245)   \n* [åœ¨Kerasä¸Šå®ç°GANï¼šæ„å»ºæ¶ˆé™¤å›¾ç‰‡æ¨¡ç³Šçš„åº”ç”¨](https://zhuanlan.zhihu.com/p/35030377)  \n\n### RNN      \n\n#### å‘å±•å²      \n\n* [ä»90å¹´ä»£çš„SRNNå¼€å§‹ï¼Œçºµè§ˆå¾ªç¯ç¥ç»ç½‘ç»œ27å¹´çš„ç ”ç©¶è¿›å±•](https://zhuanlan.zhihu.com/p/32668465)       \n\n#### æ•™ç¨‹     \n\n* [Awesome-Chinese-NLP](https://github.com/crownpku/Awesome-Chinese-NLP)\n* [nlp-pytorch-zh](https://github.com/apachecn/nlp-pytorch-zh)\n* [å®Œå…¨å›¾è§£RNNã€RNNå˜ä½“ã€Seq2Seqã€Attentionæœºåˆ¶](https://zhuanlan.zhihu.com/p/28054589)\n* [å¾ªç¯ç¥ç»ç½‘ç»œ(RNN, Recurrent Neural Networks)ä»‹ç»](https://blog.csdn.net/heyongluoyao8/article/details/48636251)\n* [RNNä»¥åŠLSTMçš„ä»‹ç»å’Œå…¬å¼æ¢³ç†](https://blog.csdn.net/Dark_Scope/article/details/47056361)\n* [ï¼ˆè¯‘ï¼‰ç†è§£é•¿çŸ­æœŸè®°å¿†(LSTM) ç¥ç»ç½‘ç»œ](https://zhuanlan.zhihu.com/p/24018768)\n* [ ä¸€æ–‡è¯»æ‡‚LSTMå’ŒRNN](https://zhuanlan.zhihu.com/p/35878575?group_id=970350175025385472)\n* [æ¢ç´¢LSTMï¼šåŸºæœ¬æ¦‚å¿µåˆ°å†…éƒ¨ç»“æ„](https://zhuanlan.zhihu.com/p/27345523)\n* [ ç¿»è¯‘ï¼šæ·±å…¥ç†è§£LSTMç³»åˆ—](https://blog.csdn.net/matrix_space/article/details/53374040) Â            Â  Â  Â  Â  Â \n* [æ·±å…¥ç†è§£ LSTM ç½‘ç»œ (ä¸€)](https://blog.csdn.net/matrix_space/article/details/53374040)\n* [æ·±å…¥ç†è§£ LSTM ç½‘ç»œ (äºŒ)](https://blog.csdn.net/matrix_space/article/details/53376870)\n* [LSTM](https://zhuanlan.zhihu.com/p/32085405)\n* [æ·±åº¦å­¦ä¹ å…¶äº” å¾ªç¯ç¥ç»ç½‘ç»œ](https://zybuluo.com/hanbingtao/note/541458) Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n* [ç”¨å¾ªç¯ç¥ç»ç½‘ç»œè¿›è¡Œæ–‡ä»¶æ— æŸå‹ç¼©ï¼šæ–¯å¦ç¦å¤§å­¦æå‡ºDeepZip](https://zhuanlan.zhihu.com/p/32582764)         \n* [å´æ©è¾¾åºåˆ—å»ºæ¨¡è¯¾ç¨‹]()\n    * [Courseraå´æ©è¾¾ã€Šåºåˆ—æ¨¡å‹ã€‹è¯¾ç¨‹ç¬”è®°ï¼ˆ1ï¼‰-- å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰](https://zhuanlan.zhihu.com/p/34309635)\n    * [Courseraå´æ©è¾¾ã€Šåºåˆ—æ¨¡å‹ã€‹è¯¾ç¨‹ç¬”è®°ï¼ˆ2ï¼‰-- NLP & Word Embeddings](https://zhuanlan.zhihu.com/p/34975871)\n    * [Courseraå´æ©è¾¾ã€Šåºåˆ—æ¨¡å‹ã€‹è¯¾ç¨‹ç¬”è®°ï¼ˆ3ï¼‰-- Sequence models & Attention mechanism](https://zhuanlan.zhihu.com/p/35532553)\n* word2vec\n\n    - åŸç†\n      - [NLP ç§’æ‡‚è¯å‘é‡Word2vecçš„æœ¬è´¨](https://zhuanlan.zhihu.com/p/26306795)\n      - [ä¸€ç¯‡é€šä¿—æ˜“æ‡‚çš„word2vec](https://zhuanlan.zhihu.com/p/35500923)\n      - [YJangoçš„Word Embedding--ä»‹ç»](https://zhuanlan.zhihu.com/p/27830489)\n      - [nlpä¸­çš„è¯å‘é‡å¯¹æ¯”ï¼šword2vec/glove/fastText/elmo/GPT/bert](https://zhuanlan.zhihu.com/p/56382372)\n      - [è¯åµŒå…¥ï¼ˆword2vecï¼‰](https://zh.diveintodeeplearning.org/chapter_natural-language-processing/word2vec.html)\n      - [è°ˆè°ˆè°·æ­Œword2vecçš„åŸç†](https://blog.csdn.net/wangyangzhizhou/article/details/77073023)\n      - [Word2Vecä¸­ä¸ºä»€ä¹ˆä½¿ç”¨è´Ÿé‡‡æ ·ï¼Ÿ](https://zhuanlan.zhihu.com/p/67117737)\n    - è®­ç»ƒè¯å‘é‡\n      - [ç»ƒä¹ -word2vec](https://zhuanlan.zhihu.com/p/29200034)\n      - [word2vecæ–¹æ³•çš„å®ç°å’Œåº”ç”¨](https://zhuanlan.zhihu.com/p/31886824)\n      - [è‡ªç„¶è¯­è¨€å¤„ç†å…¥é—¨ word2vec ä½¿ç”¨tensorflowè‡ªå·±è®­ç»ƒè¯å‘é‡](https://blog.csdn.net/wzdjsgf/article/details/79541492)\n      - [ä½¿ç”¨tensorflowå®ç°word2vecä¸­æ–‡è¯å‘é‡çš„è®­ç»ƒ](https://zhuanlan.zhihu.com/p/28979653)\n      - [å¦‚ä½•ç”¨TensorFlowè®­ç»ƒè¯å‘é‡](https://blog.csdn.net/wangyangzhizhou/article/details/77530479?locationNum=1&fps=1)\n* [èŠèŠ Transformer](https://zhuanlan.zhihu.com/p/47812375)\n* [åŸºäºTransformçš„æœºå™¨ç¿»è¯‘ç³»ç»Ÿ](https://zhuanlan.zhihu.com/p/144825330)\n* [åŸºäºword2vecè®­ç»ƒè¯å‘é‡(ä¸€)](https://zhuanlan.zhihu.com/p/35648927)\n* [åŸºäºword2vecè®­ç»ƒè¯å‘é‡(äºŒ)](https://zhuanlan.zhihu.com/p/35889385)\n* [è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attention Mechanismï¼‰](https://zhuanlan.zhihu.com/p/35041012) Â  Â \n* [è‡ªç„¶è¯­è¨€å¤„ç†ä¸­æ³¨æ„åŠ›æœºåˆ¶ç»¼è¿°](https://zhuanlan.zhihu.com/p/54491016)\n* [YJangoçš„Word Embedding--ä»‹ç»](https://zhuanlan.zhihu.com/p/27830489)          \n\n#### Action     \n\n* [æ¨èï¼šnlp-tutorial](https://github.com/graykode/nlp-tutorial)\n* [nlp-tutorial](https://github.com/lyeoni/nlp-tutorial)\n* [tensorflowä¸­RNNcellæºç åˆ†æä»¥åŠè‡ªå®šä¹‰RNNCellçš„æ–¹æ³•](https://blog.csdn.net/liuchonge/article/details/78405185?locationNum=8&fps=1)     \n* [TensorFlowä¸­RNNå®ç°çš„æ­£ç¡®æ‰“å¼€æ–¹å¼](https://zhuanlan.zhihu.com/p/28196873)      \n* [TensorFlow RNN ä»£ç ](https://zhuanlan.zhihu.com/p/27906426)\n* [Tensorflowå®ç°çš„æ·±åº¦NLPæ¨¡å‹é›†é”¦](https://zhuanlan.zhihu.com/p/67031035)\n* [ç”¨tensorflow LSTMå¦‚ä½•é¢„æµ‹è‚¡ç¥¨ä»·æ ¼](https://zhuanlan.zhihu.com/p/33186759)\n* [TensorFlowçš„å¤šå±‚LSTMå®è·µ](https://zhuanlan.zhihu.com/p/29797089)\n* [ã€Šå®‰å¨œå¡åˆ—å°¼å¨œã€‹æ–‡æœ¬ç”Ÿæˆâ€”â€”åˆ©ç”¨TensorFlowæ„å»ºLSTMæ¨¡å‹](https://zhuanlan.zhihu.com/p/27087310)\n\n### GNN\n\n#### å‘å±•å²\n\n* [Graph Neural Networkï¼ˆGNNï¼‰ç»¼è¿°](https://zhuanlan.zhihu.com/p/65539782)\n* [æ·±åº¦å­¦ä¹ æ—¶ä»£çš„å›¾æ¨¡å‹ï¼Œæ¸…åå‘æ–‡ç»¼è¿°å›¾ç½‘ç»œ](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650754422&idx=4&sn=0dc881487f362322a875b4ce06e645f7&chksm=871a8908b06d001ef7386ccc752827c20711877a4a23d6a8318978095dd241d118257c607b22&scene=21#wechat_redirect)\n* [æ¸…åå¤§å­¦å›¾ç¥ç»ç½‘ç»œç»¼è¿°ï¼šæ¨¡å‹ä¸åº”ç”¨](https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650754558&idx=2&sn=7d79191b9ed30679d5d40e22d9cabdf8&chksm=871a8980b06d00962e0dbe984e1d3469214db31cb402b4725a0dfe330249a830b45cb26932b5&scene=21#wechat_redirect)\n* [å›¾ç¥ç»ç½‘ç»œæ¦‚è¿°ç¬¬ä¸‰å¼¹ï¼šæ¥è‡ªIEEE Fellowçš„GNNç»¼è¿°](https://zhuanlan.zhihu.com/p/54241746)\n* [GNNæœ€å…¨æ–‡çŒ®èµ„æ–™æ•´ç†](https://github.com/DeepGraphLearning/LiteratureDL4Graph) && [Awesome-Graph-Neural-Networks](https://github.com/nnzhan/Awesome-Graph-Neural-Networks)\n\n#### æ•™ç¨‹\n\n* [å¦‚ä½•ç†è§£ Graph Convolutional Networkï¼ˆGCNï¼‰](https://www.zhihu.com/question/54504471)\n* [å›¾å·ç§¯ç½‘ç»œ(GCN)æ–°æ‰‹æ‘å®Œå…¨æŒ‡å—](https://zhuanlan.zhihu.com/p/54505069)\n* [ä½•æ—¶èƒ½æ‡‚ä½ çš„å¿ƒâ€”â€”å›¾å·ç§¯ç¥ç»ç½‘ç»œï¼ˆGCNï¼‰](https://zhuanlan.zhihu.com/p/71200936)\n* [å›¾å·ç§¯ç½‘ç»œGCNçš„ç†è§£ä¸ä»‹ç»](https://zhuanlan.zhihu.com/p/90470499)\n* [ä¸€æ–‡è¯»æ‡‚å›¾å·ç§¯GCN](https://zhuanlan.zhihu.com/p/89503068)\n* [2020 å¹´ GNN å¼€å·æœ‰ç›Šä¸å†è°ˆå›¾å·ç§¯](https://zhuanlan.zhihu.com/p/101310106)\n* [ã€GCNã€‘ä¸‡å­—é•¿æ–‡å¸¦ä½ å…¥é—¨ GCN](https://zhuanlan.zhihu.com/p/120311352)\n* [å¦‚ä½•è§£å†³å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰è®­ç»ƒä¸­è¿‡åº¦å¹³æ»‘çš„é—®é¢˜ï¼Ÿ](https://www.zhihu.com/question/346942899/answer/848298494)\n* [å…¨è¿æ¥çš„å›¾å·ç§¯ç½‘ç»œ(GCN)å’Œself-attentionè¿™äº›æœºåˆ¶æœ‰ä»€ä¹ˆåŒºåˆ«è”ç³»](https://www.zhihu.com/question/366088445) && [CNNä¸GCNçš„åŒºåˆ«ã€è”ç³»åŠèåˆ](https://zhuanlan.zhihu.com/p/147654689)\n\n#### Action\n\n* [å›¾å·ç§¯ç½‘ç»œåˆ°åº•æ€ä¹ˆåšï¼Œè¿™æ˜¯ä¸€ä»½æç®€çš„Numpyå®ç°](https://zhuanlan.zhihu.com/p/57235377)\n* [DGL](https://docs.dgl.ai/index.html)\n\n## ä¸‰. æ·±åº¦æ¨¡å‹çš„ä¼˜åŒ–ä¸æ­£åˆ™åŒ–\n\n* [1. ä¼˜åŒ–ç®—æ³•çºµè§ˆ](http://fa.bianp.net/teaching/2018/eecs227at/)\n* [2. ä»æ¢¯åº¦ä¸‹é™åˆ°Adam](https://zhuanlan.zhihu.com/p/27449596)\n* [3. ä»æ¢¯åº¦ä¸‹é™åˆ°æ‹Ÿç‰›é¡¿æ³•ï¼šç›˜ç‚¹è®­ç»ƒç¥ç»ç½‘ç»œçš„äº”å¤§å­¦ä¹ ç®—æ³•](https://zhuanlan.zhihu.com/p/25703402)\n* [4. æ­£åˆ™åŒ–æŠ€æœ¯æ€»ç»“](https://zhuanlan.zhihu.com/p/35429054?group_id=966442942538444800)\n  * [å²ä¸Šæœ€å…¨é¢çš„æ­£åˆ™åŒ–æŠ€æœ¯æ€»ç»“ä¸åˆ†æ--part1](https://zhuanlan.zhihu.com/p/35429054?group_id=966442942538444800)\n  * [å²ä¸Šæœ€å…¨é¢çš„æ­£åˆ™åŒ–æŠ€æœ¯æ€»ç»“ä¸åˆ†æ--part2](https://zhuanlan.zhihu.com/p/35432128?group_id=966443101011738624)\n* [æƒé‡è¡°å‡ï¼ˆweight decayï¼‰ä¸å­¦ä¹ ç‡è¡°å‡ï¼ˆlearning rate decayï¼‰](https://zhuanlan.zhihu.com/p/38709373) && [pytorchå¿…é¡»æŒæ¡çš„çš„4ç§å­¦ä¹ ç‡è¡°å‡ç­–ç•¥](https://zhuanlan.zhihu.com/p/93624972)\n* [5. æœ€ä¼˜åŒ–ç®—æ³•ç³»åˆ—ï¼ˆmathï¼‰](https://blog.csdn.net/chunyun0716/article/category/6188191/2)\n* [6. ç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„æ¢¯åº¦æ¶ˆå¤±ä¸æ¢¯åº¦çˆ†ç‚¸](https://zhuanlan.zhihu.com/p/25631496) Â  Â  Â  Â \n* [7. ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–åŠè®­ç»ƒ](https://zhuanlan.zhihu.com/p/36050743)\n* [8. é€šä¿—è®²è§£æŸ¥å…¨ç‡å’ŒæŸ¥å‡†ç‡](https://zhuanlan.zhihu.com/p/35888543) && [å…¨é¢æ¢³ç†ï¼šå‡†ç¡®ç‡,ç²¾ç¡®ç‡,å¬å›ç‡,æŸ¥å‡†ç‡,æŸ¥å…¨ç‡,å‡é˜³æ€§,çœŸé˜³æ€§,PRC,ROC,AUC,F1](https://zhuanlan.zhihu.com/p/34079183) && [æœºå™¨å­¦ä¹ ä¹‹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ (1) â€”â€” å„ç§è¯„ä¼°æŒ‡æ ‡](https://zhuanlan.zhihu.com/p/34473430) && [æœºå™¨å­¦ä¹ ä¹‹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ (2) â€”â€” ROCå’ŒPRæ›²çº¿](https://zhuanlan.zhihu.com/p/34655990) && [AUCè¯¦è§£ä¸pythonå®ç°](https://zhuanlan.zhihu.com/p/84035782) && [å¾®å¹³å‡å’Œå®å¹³å‡](https://zhuanlan.zhihu.com/p/78628437)  && [æœºå™¨å­¦ä¹ ä¸­çš„æ€§èƒ½åº¦é‡](https://zhuanlan.zhihu.com/p/74980268) && [ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1 å€¼ã€ROCã€AUC å„è‡ªçš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆ](https://www.zhihu.com/question/30643044)\n* [æ¿€æ´»å‡½æ•°ä¸€è§ˆ](https://zhuanlan.zhihu.com/p/30567264) && [æ·±åº¦å­¦ä¹ ä¸­å‡ ç§å¸¸è§çš„æ¿€æ´»å‡½æ•°ç†è§£ä¸æ€»ç»“](https://www.cnblogs.com/XDU-Lakers/p/10557496.html)\n* [æ·±åº¦å­¦ä¹ ç¬”è®°(ä¸‰)ï¼šæ¿€æ´»å‡½æ•°å’ŒæŸå¤±å‡½æ•°](https://blog.csdn.net/u014595019/article/details/52562159)\n* [æ¿€æ´»å‡½æ•°/æŸå¤±å‡½æ•°æ±‡æ€»](https://zhuanlan.zhihu.com/p/30385380)\n* [æœºå™¨å­¦ä¹ ä¸­å¸¸è§çš„æŸå¤±å‡½æ•°åŠå…¶åº”ç”¨åœºæ™¯](https://blog.csdn.net/zuolixiangfisher/article/details/88649110) && [PyTorchçš„åå…«ä¸ªæŸå¤±å‡½æ•°](https://zhuanlan.zhihu.com/p/61379965)\n* [æ·±åº¦åº¦é‡å­¦ä¹ ä¸­çš„æŸå¤±å‡½æ•°](https://zhuanlan.zhihu.com/p/82199561)\n* [åå‘ä¼ æ’­ç®—æ³•ï¼ˆè¿‡ç¨‹åŠå…¬å¼æ¨å¯¼ï¼‰](https://blog.csdn.net/u014313009/article/details/51039334)\n* [é€šä¿—ç†è§£ç¥ç»ç½‘ç»œBPä¼ æ’­ç®—æ³•](https://zhuanlan.zhihu.com/p/24801814)\n* [10. Courseraå´æ©è¾¾ã€Šä¼˜åŒ–æ·±åº¦ç¥ç»ç½‘ç»œã€‹è¯¾ç¨‹ç¬”è®°ï¼ˆ3ï¼‰-- è¶…å‚æ•°è°ƒè¯•ã€Batchæ­£åˆ™åŒ–å’Œç¼–ç¨‹æ¡†æ¶](https://zhuanlan.zhihu.com/p/30922689)\n* [11. æœºå™¨å­¦ä¹ å„ç§ç†µ](https://zhuanlan.zhihu.com/p/35423404)\n* [12. è·ç¦»å’Œç›¸ä¼¼æ€§åº¦é‡](https://zhuanlan.zhihu.com/p/27305237)\n* [13. æœºå™¨å­¦ä¹ é‡Œçš„é»‘è‰²è‰ºæœ¯ï¼šnormalization, standardization, regularization](https://zhuanlan.zhihu.com/p/29974820) && [æ•°æ®æ ‡å‡†åŒ–/å½’ä¸€åŒ–normalization](https://blog.csdn.net/pipisorry/article/details/52247379) && [ç‰¹å¾å·¥ç¨‹ä¸­çš„ã€Œå½’ä¸€åŒ–ã€æœ‰ä»€ä¹ˆä½œç”¨ï¼Ÿ](https://www.zhihu.com/question/20455227)\n* [14. LSTMç³»åˆ—çš„æ¢¯åº¦é—®é¢˜](https://zhuanlan.zhihu.com/p/36101196)\n* [15. æŸå¤±å‡½æ•°æ•´ç†](https://zhuanlan.zhihu.com/p/35027284)\n* [16. è¯¦è§£æ®‹å·®å—ä¸ºä½•æœ‰åŠ©äºè§£å†³æ¢¯åº¦å¼¥æ•£é—®é¢˜](https://zhuanlan.zhihu.com/p/28124810)\n* [17. FAIRä½•æºæ˜ç­‰äººæå‡ºç»„å½’ä¸€åŒ–ï¼šæ›¿ä»£æ‰¹å½’ä¸€åŒ–ï¼Œä¸å—æ‰¹é‡å¤§å°é™åˆ¶](https://zhuanlan.zhihu.com/p/34858971)\n* [18. Batch Normalizationï¼ˆBNï¼‰]():[1 ](https://zhuanlan.zhihu.com/p/26702482),[2 ](https://blog.csdn.net/hjimce/article/details/50866313),[3 ](https://bbs.cvmart.net/topics/576),[4 ](https://blog.csdn.net/edogawachia/article/details/80040456), [5](https://zhuanlan.zhihu.com/p/38176412), [6](https://www.zhihu.com/question/38102762), [7](https://zhuanlan.zhihu.com/p/52132614)\n* [19. è¯¦è§£æ·±åº¦å­¦ä¹ ä¸­çš„Normalizationï¼Œä¸åªæ˜¯BN](https://zhuanlan.zhihu.com/p/33173246) && [å¦‚ä½•åŒºåˆ†å¹¶è®°ä½å¸¸è§çš„å‡ ç§ Normalization ç®—æ³•](https://zhuanlan.zhihu.com/p/69659844)\n* [20. BFGS](https://blog.csdn.net/philosophyatmath/article/details/70173128)\n* [21. è¯¦è§£æ·±åº¦å­¦ä¹ ä¸­çš„æ¢¯åº¦æ¶ˆå¤±ã€çˆ†ç‚¸åŸå› åŠå…¶è§£å†³æ–¹æ³•](https://zhuanlan.zhihu.com/p/33006526) && [ç¥ç»ç½‘ç»œæ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸åŠè§£å†³åŠæ³•](https://blog.csdn.net/program_developer/article/details/80032376)\n* [22. Dropout](https://arxiv.org/pdf/1207.0580.pdf), [1](https://blog.csdn.net/stdcoutzyx/article/details/49022443), [2](https://blog.csdn.net/hjimce/article/details/50413257), [3](https://blog.csdn.net/shuzfan/article/details/50580915)ï¼Œ[ç³»åˆ—è§£è¯»Dropout](https://blog.csdn.net/shuzfan/article/details/50580915)\n* [23.è°±å½’ä¸€åŒ–ï¼ˆSpectral Normalizationï¼‰çš„ç†è§£](https://blog.csdn.net/StreamRock/article/details/83590347)ï¼Œ[å¸¸è§å‘é‡èŒƒæ•°å’ŒçŸ©é˜µèŒƒæ•°](https://blog.csdn.net/left_la/article/details/9159949)ï¼Œ[è°±èŒƒæ•°æ­£åˆ™ï¼ˆSpectral Norm Regularizationï¼‰çš„ç†è§£](https://blog.csdn.net/StreamRock/article/details/83539937)\n* [24.L1æ­£åˆ™åŒ–ä¸L2æ­£åˆ™åŒ–](https://zhuanlan.zhihu.com/p/35356992) && [æ·±å…¥ç†è§£L1ã€L2æ­£åˆ™åŒ–](https://zhuanlan.zhihu.com/p/29360425) && [L2æ­£åˆ™=Weight Decayï¼Ÿå¹¶ä¸æ˜¯è¿™æ ·](https://zhuanlan.zhihu.com/p/40814046) && [éƒ½9102å¹´äº†ï¼Œåˆ«å†ç”¨Adam + L2 regularization](https://zhuanlan.zhihu.com/p/63982470)\n* [25.ä¸ºä»€ä¹ˆé€‰ç”¨äº¤å‰ç†µè€Œä¸æ˜¯MSE](https://zhuanlan.zhihu.com/p/61944055) &&[ä¸ºä»€ä¹ˆä½¿ç”¨äº¤å‰ç†µä½œä¸ºæŸå¤±å‡½æ•°](https://zhuanlan.zhihu.com/p/63731947) &&[äºŒå…ƒåˆ†ç±»ä¸ºä»€ä¹ˆä¸èƒ½ç”¨MSEåšä¸ºæŸå¤±å‡½æ•°ï¼Ÿ](http://sofasofa.io/forum_main_post.php?postid=1001792)&& [ä¸ºä»€ä¹ˆå¹³æ–¹æŸå¤±å‡½æ•°ä¸é€‚ç”¨åˆ†ç±»é—®é¢˜ï¼Ÿ](https://www.zhihu.com/question/319865092)\n* [æµ…è°ˆç¥ç»ç½‘ç»œä¸­çš„æ¢¯åº¦çˆ†ç‚¸é—®é¢˜](https://zhuanlan.zhihu.com/p/32154263)\n* [ä¸ºä»€ä¹ˆweight decayèƒ½å¤Ÿé˜²æ­¢è¿‡æ‹Ÿåˆ](https://www.zhihu.com/question/65626362)\n* [äº¤å‰ç†µä»£ä»·å‡½æ•°ï¼ˆä½œç”¨åŠå…¬å¼æ¨å¯¼ï¼‰](https://blog.csdn.net/u014313009/article/details/51043064) && [äº¤å‰ç†µæŸå¤±çš„æ¥æºã€è¯´æ˜ã€æ±‚å¯¼ä¸pytorchå®ç°](https://zhuanlan.zhihu.com/p/67782576) && [Softmaxå‡½æ•°ä¸äº¤å‰ç†µ](https://zhuanlan.zhihu.com/p/27223959) && [æå¤§ä¼¼ç„¶ä¼°è®¡ä¸æœ€å°åŒ–äº¤å‰ç†µæŸå¤±æˆ–è€…KLæ•£åº¦ä¸ºä»€ä¹ˆç­‰ä»·](https://zhuanlan.zhihu.com/p/84764177)\n* [æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•çºµè§ˆ](http://ruder.io/optimizing-gradient-descent/), [1](https://blog.csdn.net/qq_23269761/article/details/80901411), [2](https://www.cnblogs.com/guoyaohua/p/8542554.html), [å‡ ç§ä¼˜åŒ–ç®—æ³•çš„æ¯”è¾ƒï¼ˆBGDã€SGDã€Adamã€RMSPROPï¼‰](https://blog.csdn.net/qq_32172681/article/details/100979476)\n* **Softmax**ï¼š[è¯¦è§£softmaxå‡½æ•°ä»¥åŠç›¸å…³æ±‚å¯¼è¿‡ç¨‹](https://zhuanlan.zhihu.com/p/25723112)  &&  [softmaxçš„logä¼¼ç„¶ä»£ä»·å‡½æ•°ï¼ˆå…¬å¼æ±‚å¯¼ï¼‰](https://blog.csdn.net/u014313009/article/details/51045303) && [ã€æŠ€æœ¯ç»¼è¿°ã€‘ä¸€æ–‡é“å°½softmax lossåŠå…¶å˜ç§](https://zhuanlan.zhihu.com/p/34044634)\n* [ä»æœ€ä¼˜åŒ–çš„è§’åº¦çœ‹å¾…SoftmaxæŸå¤±å‡½æ•°](https://zhuanlan.zhihu.com/p/45014864)  && [Softmaxç†è§£ä¹‹äºŒåˆ†ç±»ä¸å¤šåˆ†ç±»](https://zhuanlan.zhihu.com/p/45368976) && [Softmaxç†è§£ä¹‹Smoothç¨‹åº¦æ§åˆ¶](https://zhuanlan.zhihu.com/p/49939159) && [Softmaxç†è§£ä¹‹margin](https://zhuanlan.zhihu.com/p/52108088)\n* **æƒé‡åˆå§‹åŒ–**\n  * [ç¥ç»ç½‘ç»œä¸­çš„æƒé‡åˆå§‹åŒ–ä¸€è§ˆï¼šä»åŸºç¡€åˆ°Kaiming](https://zhuanlan.zhihu.com/p/62850258)\n  * [æ·±åº¦å­¦ä¹ ä¸­å¸¸è§çš„æƒé‡åˆå§‹åŒ–æ–¹æ³•](https://zhuanlan.zhihu.com/p/138064188)\n  * [æ·±åº¦å­¦ä¹ ä¸­ç¥ç»ç½‘ç»œçš„å‡ ç§æƒé‡åˆå§‹åŒ–æ–¹æ³•](https://blog.csdn.net/u012328159/article/details/80025785)\n  * [è°ˆè°ˆç¥ç»ç½‘ç»œæƒé‡ä¸ºä»€ä¹ˆä¸èƒ½åˆå§‹åŒ–ä¸º0](https://zhuanlan.zhihu.com/p/75879624)\n  * [ç¥ç»ç½‘ç»œä¸­çš„åç½®ï¼ˆbiasï¼‰ç©¶ç«Ÿæœ‰è¿™ä¹ˆç”¨ï¼Ÿ](https://www.zhihu.com/question/305340182)\n  * [æ·±åº¦å­¦ä¹ é‡Œé¢çš„åç½®ä¸ºä»€ä¹ˆä¸åŠ æ­£åˆ™ï¼Ÿ](https://www.zhihu.com/question/66894061)\n* [ä¸ºä»€ä¹ˆè¯´baggingæ˜¯å‡å°‘varianceï¼Œè€Œboostingæ˜¯å‡å°‘bias?](https://www.zhihu.com/question/26760839)\n\n## å››. ç‚¼ä¸¹æœ¯å£«é‚£äº›äº‹\n\n### è°ƒå‚ç»éªŒ\n* [è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¸å·¥ä½œï¼Ÿä¸€æ–‡å¸¦ä½ è·¨è¿‡è¿™37ä¸ªå‘](https://blog.csdn.net/jiandanjinxin/article/details/77190687)\n* [Deep Learning ä¹‹ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°NaNé—®é¢˜](https://blog.csdn.net/BVL10101111/article/details/76086344)\n* [ç¥ç»ç½‘ç»œè®­ç»ƒtrick](https://zhuanlan.zhihu.com/p/59918821)\n* [ä½ æœ‰å“ªäº›deep learningï¼ˆrnnã€cnnï¼‰è°ƒå‚çš„ç»éªŒï¼Ÿ](https://www.zhihu.com/question/41631631)\n* [GANçš„ä¸€äº›å°trick](https://zhuanlan.zhihu.com/p/27725664)\n* [æ·±åº¦å­¦ä¹ ä¸è®¡ç®—æœºè§†è§‰ç³»åˆ—(8)_ç¥ç»ç½‘ç»œè®­ç»ƒä¸æ³¨æ„ç‚¹](https://blog.csdn.net/han_xiaoyang/article/details/50521064)\n* [ç¥ç»ç½‘ç»œè®­ç»ƒlossä¸ä¸‹é™åŸå› é›†åˆ](https://blog.csdn.net/liuweiyuxiang/article/details/80856991) && [ lossä¸ä¸‹é™çš„è§£å†³æ–¹æ³•](https://blog.csdn.net/zongza/article/details/89185852)\n* [æ·±åº¦å­¦ä¹ ï¼šæ¬ æ‹Ÿåˆé—®é¢˜çš„å‡ ç§è§£å†³æ–¹æ¡ˆ](https://blog.csdn.net/u014038273/article/details/84108688) &&[è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆé—®é¢˜](https://blog.csdn.net/mzpmzk/article/details/79741682)\n* [æœºå™¨å­¦ä¹ ï¼šå¦‚ä½•æ‰¾åˆ°æœ€ä¼˜å­¦ä¹ ç‡](https://blog.csdn.net/whut_ldz/article/details/78882871)åŠ[å®ç°](https://github.com/L1aoXingyu/torchlib)\n* [ç¥ç»ç½‘ç»œä¸­ warmup ç­–ç•¥ä¸ºä»€ä¹ˆæœ‰æ•ˆ](https://www.zhihu.com/question/338066667)\n* [ä¸å¹³è¡¡æ•°æ®é›†å¤„ç†æ–¹æ³•](): [å…¶ä¸€](https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/), [å…¶äºŒ](https://www.zhihu.com/question/285824343), [å…¶ä¸‰](https://blog.csdn.net/songhk0209/article/details/71484469) && [Awesome Imbalanced Learning](https://github.com/ZhiningLiu1998/awesome-imbalanced-learning) && [Class-balanced-loss-pytorch](https://github.com/vandit15/Class-balanced-loss-pytorch)\n* [åŒä¸€ä¸ªç¥ç»ç½‘ç»œä½¿ç”¨ä¸åŒæ¿€æ´»å‡½æ•°çš„è¡¨è¾¾èƒ½åŠ›æ˜¯å¦ä¸€è‡´](https://www.zhihu.com/question/41841299)\n* [è®ºæ–‡ç¬”è®°ä¹‹æ•°æ®å¢å¹¿ï¼šmixup](https://blog.csdn.net/ly244855983/article/details/78938667#%E8%AE%A8%E8%AE%BA)\n* [é¿å‘æŒ‡å—ï¼šæ•°æ®ç§‘å­¦å®¶æ–°æ‰‹å¸¸çŠ¯çš„13ä¸ªé”™è¯¯](https://zhuanlan.zhihu.com/p/44331706)	\n* [å‡­ä»€ä¹ˆç›¸ä¿¡CNNçš„ç»“æœ?--å¯è§†åŒ–](https://bindog.github.io/blog/2018/02/10/model-explanation/)				\n  * [å‡­ä»€ä¹ˆç›¸ä¿¡ä½ ï¼Œæˆ‘çš„CNNæ¨¡å‹ï¼Ÿï¼ˆç¯‡ä¸€ï¼šCAMå’ŒGrad-CAM)](https://bindog.github.io/blog/2018/02/10/model-explanation/) && [pytorch-grad-cam](https://github.com/jacobgil/pytorch-grad-cam) && [Grad-CAM-tensorflow](https://github.com/insikk/Grad-CAM-tensorflow) && [grad-cam.tensorflow](https://github.com/Ankush96/grad-cam.tensorflow) && [cnn_visualization](https://github.com/js-fan/mxnet/tree/d2b802e2d2af3dae5b4ac941354602630d2ef1c7/example/cnn_visualization)\n  * [å‡­ä»€ä¹ˆç›¸ä¿¡ä½ ï¼Œæˆ‘çš„CNNæ¨¡å‹ï¼Ÿï¼ˆç¯‡äºŒï¼šä¸‡é‡‘æ²¹LIME)](http://bindog.github.io/blog/2018/02/11/model-explanation-2/)\n  * [è®ºæ–‡ç¬”è®°:Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://www.jianshu.com/p/294ad9ae2e50)\n  * [CVï¼šåŸºäºKerasåˆ©ç”¨è®­ç»ƒå¥½çš„hdf5æ¨¡å‹è¿›è¡Œç›®æ ‡æ£€æµ‹å®ç°è¾“å‡ºæ¨¡å‹ä¸­çš„è¡¨æƒ…æˆ–æ€§åˆ«çš„gradcam(å¯è§†åŒ–)](https://blog.csdn.net/qq_41185868/article/details/80323646)\n* [å¤§å·ç§¯æ ¸è¿˜æ˜¯å°å·ç§¯æ ¸?]() [1](https://www.jianshu.com/p/d75375dd7ebd), [2](https://blog.csdn.net/kuangtun9713/article/details/79475457)	\n* [æ¨¡å‹å¯è§£é‡Šæ€§å·®ï¼Ÿä½ è€ƒè™‘äº†å„ç§ä¸ç¡®å®šæ€§äº†å—ï¼Ÿ](https://baijiahao.baidu.com/s?id=1608193373391996908)\n* [ç‚¼ä¸¹ç¬”è®°ç³»åˆ—]()\n  * [ç‚¼ä¸¹ç¬”è®°ä¸€ï¼šæ ·æœ¬ä¸å¹³è¡¡é—®é¢˜](https://zhuanlan.zhihu.com/p/56882616)\n  * [ç‚¼ä¸¹ç¬”è®°äºŒï¼šæ•°æ®æ¸…æ´—](https://zhuanlan.zhihu.com/p/56022212)\n  * [ç‚¼ä¸¹ç¬”è®°ä¸‰ï¼šæ•°æ®å¢å¼º](https://zhuanlan.zhihu.com/p/56139575)\n  * [ç‚¼ä¸¹ç¬”è®°å››ï¼šå°æ ·æœ¬é—®é¢˜](https://zhuanlan.zhihu.com/p/56365469)\n  * [ç‚¼ä¸¹ç¬”è®°äº”ï¼šæ•°æ®æ ‡æ³¨](https://zhuanlan.zhihu.com/p/56443169)\n  * [ç‚¼ä¸¹ç¬”è®°å…­ : è°ƒå‚æŠ€å·§](https://zhuanlan.zhihu.com/p/56745640)\n  * [ç‚¼ä¸¹ç¬”è®°ä¸ƒï¼šå·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹è®¾è®¡](https://zhuanlan.zhihu.com/p/57738934)\n\n### åˆ·æ’è¡Œæ¦œçš„å°æŠ€å·§\n\n* [Kaggle å…­å¤§æ¯”èµ›æœ€å…¨é¢è§£æï¼ˆä¸Šï¼‰](https://www.leiphone.com/news/201803/XBjvQriKTyTMPLcz.html)\n\n* [Kaggle å…­å¤§æ¯”èµ›æœ€å…¨é¢è§£æï¼ˆä¸‹ï¼‰](https://www.leiphone.com/news/201803/chz1DNHqgVWNEm5t.html)\n\n#### å›¾åƒåˆ†ç±»\n\n* [ç‚¼ä¸¹ç¬”è®°ä¸‰ï¼šæ•°æ®å¢å¼º](https://zhuanlan.zhihu.com/p/56139575) && [æ•°æ®å¢å¼º(Data Augmentation)](https://zhuanlan.zhihu.com/p/41679153)\n* [ã€æŠ€æœ¯ç»¼è¿°ã€‘ æ·±åº¦å­¦ä¹ ä¸­çš„æ•°æ®å¢å¼ºï¼ˆä¸Šï¼‰](https://zhuanlan.zhihu.com/p/38345420) && [ã€æŠ€æœ¯ç»¼è¿°ã€‘æ·±åº¦å­¦ä¹ ä¸­çš„æ•°æ®å¢å¼ºï¼ˆä¸‹ï¼‰](https://zhuanlan.zhihu.com/p/38437739)\n* [æ·±åº¦å­¦ä¹ æ•°æ®å¢å¹¿æŠ€æœ¯ä¸€è§ˆ](https://zhuanlan.zhihu.com/p/144921458)\n* [ã€ŠBag of Tricks for Image Classification with CNNã€‹](https://zhuanlan.zhihu.com/p/53324148)&& [pdf](https://arxiv.org/pdf/1812.01187.pdf)\n* [æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹è®­ç»ƒä¸­çš„æœ€æ–°tricksæ€»ç»“ã€åŸç†ä¸ä»£ç æ±‡æ€»ã€‘](https://zhuanlan.zhihu.com/p/66080948) && [ç¥ç»ç½‘ç»œè®­ç»ƒtrick](https://zhuanlan.zhihu.com/p/59918821)\n* [Kaggleè§£å†³æ–¹æ¡ˆåˆ†äº«]()\n  * [ä»0ä¸Šæ‰‹Kaggleå›¾åƒåˆ†ç±»æŒ‘æˆ˜ï¼šå† å†›è§£å†³æ–¹æ¡ˆè¯¦è§£](https://www.itcodemonkey.com/article/4898.html)\n  * [Kaggle å†°å±±å›¾åƒåˆ†ç±»å¤§èµ›è¿‘æ—¥è½å¹•ï¼Œçœ‹å† å†›å›¢é˜Ÿæ–¹æ¡ˆæœ‰ä½•äº®ç‚¹](https://www.leiphone.com/news/201803/u40cjEZWArBfFaBm.html)\n  * [ã€Kaggleå† å†›åˆ†äº«ã€‘å›¾åƒè¯†åˆ«å’Œåˆ†ç±»ç«èµ›ï¼Œæ•°æ®å¢å¼ºåŠä¼˜åŒ–ç®—æ³•](https://mp.weixin.qq.com/s/_S8EBBJ-u9g_fHp7I3ChMQ?)\n  * [è¯†åˆ«åº§å¤´é²¸ï¼ŒKaggleç«èµ›ç¬¬ä¸€åè§£å†³æ–¹æ¡ˆè§£è¯»](https://zhuanlan.zhihu.com/p/58496385)\n  * [kaggle é¦–æˆ˜æ‹¿é‡‘ç‰Œæ€»ç»“](https://zhuanlan.zhihu.com/p/60953933)\n  * [16å²é«˜ä¸­ç”Ÿå¤ºå† Kaggleåœ°æ ‡æ£€ç´¢æŒ‘æˆ˜èµ›ï¼è€Œä¸”ç«Ÿç„¶æ˜¯Kaggleè€å…µ](https://zhuanlan.zhihu.com/p/37522227)\n  * [6æ¬¡Kaggleè®¡ç®—æœºè§†è§‰ç±»æ¯”èµ›èµ›åæ„Ÿ](https://zhuanlan.zhihu.com/p/37663895)\n  * [Kaggleé¦–æˆ˜æ–©è·ç¬¬ä¸‰-å«æ˜Ÿå›¾åƒè¯†åˆ«](https://zhuanlan.zhihu.com/p/63275166)\n\n#### ç›®æ ‡æ£€æµ‹\n\n* ensemble\n* deformable\n* sync bn\n* ms train/test\n* [ç›®æ ‡æ£€æµ‹ä»»åŠ¡çš„ä¼˜åŒ–ç­–ç•¥tricks](https://zhuanlan.zhihu.com/p/56792817)\n* [ç›®æ ‡æ£€æµ‹å°tricks--æ ·æœ¬ä¸å‡è¡¡å¤„ç†](https://zhuanlan.zhihu.com/p/60612064)\n* [æ±‡æ€»|ç›®æ ‡æ£€æµ‹ä¸­çš„æ•°æ®å¢å¼ºã€backboneã€headã€neckã€æŸå¤±å‡½æ•°](https://zhuanlan.zhihu.com/p/137769687)\n* [ç›®æ ‡æ£€æµ‹ç®—æ³•ä¸­çš„å¸¸è§trick](https://zhuanlan.zhihu.com/p/39262769)\n* [Bag of Freebies â€”â€” æå‡ç›®æ ‡æ£€æµ‹æ¨¡å‹æ€§èƒ½çš„å…è´¹tricks](https://zhuanlan.zhihu.com/p/141878389)\n* [ç›®æ ‡æ£€æµ‹æ¯”èµ›ä¸­çš„tricksï¼ˆå·²æ›´æ–°æ›´å¤šä»£ç è§£æï¼‰](https://zhuanlan.zhihu.com/p/102817180)\n* [Kaggleï¼šè‚ºç™Œè‡ªåŠ¨è¯Šæ–­ç³»ç»Ÿ3D Deep Leaky Noisy-or Network è®ºæ–‡é˜…è¯»](https://www.jianshu.com/p/50158f8daf0d)\n* [å¹²è´§|å¤§ç¥æ•™ä½ å¦‚ä½•å‚åŠ kaggleæ¯”èµ›â€”â€”æ ¹æ®CTæ‰«æå›¾é¢„æµ‹è‚ºç™Œ](https://yq.aliyun.com/articles/89312)\n\n## äº”. å¹´åº¦æ€»ç»“\n* [æ–°å¹´å¤§ç¤¼åŒ…ï¼šæœºå™¨ä¹‹å¿ƒ2018é«˜åˆ†æ•™ç¨‹åˆé›†](https://zhuanlan.zhihu.com/p/53717510)   \n* [æ”¶è—ã€é€€å‡ºä¸€æ°”å‘µæˆï¼Œ2019å¹´æœºå™¨ä¹‹å¿ƒå¹²è´§æ•™ç¨‹éƒ½åœ¨è¿™é‡Œäº†](https://zhuanlan.zhihu.com/p/104022144)\n\n## å…­. ç§‘ç ”ç›¸å…³\n\n### æ·±åº¦å­¦ä¹ æ¡†æ¶\n\n#### Python3.x(å…ˆä¿®)		\n\n* [The Python Tutorial](https://docs.python.org/3/tutorial/)\n* [å»–é›ªå³°Pythonæ•™ç¨‹](https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000)\n* [èœé¸Ÿæ•™ç¨‹](http://www.runoob.com/python3/python3-tutorial.html) Â  Â  \n* [ç»™æ·±åº¦å­¦ä¹ å…¥é—¨è€…çš„Pythonå¿«é€Ÿæ•™ç¨‹ - åŸºç¡€ç¯‡](https://zhuanlan.zhihu.com/p/24162430)\n* [Python - 100å¤©ä»æ–°æ‰‹åˆ°å¤§å¸ˆ](https://github.com/jackfrued/Python-100-Days)\n* [Pythonä¸­è¯»å–,æ˜¾ç¤º,ä¿å­˜å›¾ç‰‡çš„æ–¹æ³•](https://blog.csdn.net/u010472607/article/details/78855816) && [Pythonçš„å›¾åƒæ‰“å¼€ä¿å­˜æ˜¾ç¤ºçš„å‡ ç§æ–¹å¼](https://blog.csdn.net/weixin_37619439/article/details/86559239)\n\n#### Numpy(å…ˆä¿®)\n\n* [Quickstart tutorial](https://www.numpy.org/devdocs/user/quickstart.html)\n\n* [Numpyå¿«é€Ÿå…¥é—¨(Numpy 1.14 å®˜æ–¹æ–‡æ¡£ä¸­æ–‡ç¿»è¯‘)](https://www.jianshu.com/p/3e566f09a0cf)\n* [Numpyä¸­æ–‡æ–‡æ¡£](https://www.numpy.org.cn/index.html)\n* [ç»™æ·±åº¦å­¦ä¹ å…¥é—¨è€…çš„Pythonå¿«é€Ÿæ•™ç¨‹ - numpyå’ŒMatplotlibç¯‡](https://zhuanlan.zhihu.com/p/24309547)\n\n#### Opencv-python\n\n* [OpenCV-Python Tutorials](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html)\n* [OpenCVå®˜æ–¹æ•™ç¨‹ä¸­æ–‡ç‰ˆï¼ˆFor Pythonï¼‰](https://www.cnblogs.com/Undo-self-blog/p/8423851.html)\n* [æ•°å­—å›¾åƒå¤„ç†ç³»åˆ—](https://blog.csdn.net/feilong_csdn/article/category/8037591)\n* [python+OpenCVå›¾åƒå¤„ç†](https://blog.csdn.net/qq_40962368/article/category/7688903)\n* [ç»™æ·±åº¦å­¦ä¹ å…¥é—¨è€…çš„Pythonå¿«é€Ÿæ•™ç¨‹ - ç•ªå¤–ç¯‡ä¹‹Python-OpenCV](https://zhuanlan.zhihu.com/p/24425116)\n\n#### Pandas\n\n* [Python æ•°æ®ç§‘å­¦å…¥é—¨æ•™ç¨‹ï¼šPandas](https://www.jianshu.com/p/d9774cf1fea5?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation)\n\n#### Tensorflow		\n\n* [å¦‚ä½•é«˜æ•ˆåœ°å­¦ä¹  TensorFlow ä»£ç ](https://www.zhihu.com/question/41667903)\n* [ä¸­æ–‡æ•™ç¨‹](http://www.tensorfly.cn/tfdoc/tutorials/overview.html)\n* [TensorFlowå®˜æ–¹æ–‡æ¡£](https://www.w3cschool.cn/tensorflow_python/)\n* [CS20:Tensorflow for DeepLearning Research](http://web.stanford.edu/class/cs20si/syllabus.html)\n* [å´æ©è¾¾TensorFlowä¸“é¡¹è¯¾ç¨‹](https://zhuanlan.zhihu.com/p/62981537)\n* [ã€å¹²è´§ã€‘å²ä¸Šæœ€å…¨çš„Tensorflowå­¦ä¹ èµ„æºæ±‡æ€»](https://zhuanlan.zhihu.com/p/35515805?group_id=967136289941897216)\n* [ã€Š21ä¸ªé¡¹ç›®ç©è½¬æ·±åº¦å­¦ä¹ â€”â€”â€”åŸºäºTensorFlowçš„å®è·µè¯¦è§£ã€‹](https://github.com/hzy46/Deep-Learning-21-Examples) Â \n* [æœ€å…¨Tensorflow2.0 å…¥é—¨æ•™ç¨‹æŒç»­æ›´æ–°](https://zhuanlan.zhihu.com/p/59507137)\n* [Githubä¼˜ç§€å¼€æºæ•™ç¨‹](https://github.com/search?o=desc&q=tensorflow+tutorial&s=&type=Repositories)\n\n#### MXNet		\n* [Gluon](http://zh.gluon.ai/#)\n* [GluonCV](https://gluon-cv.mxnet.io/index.html#)\n* [GluonNLP](http://gluon-nlp.mxnet.io/)\n\n#### PyTorch\n\n* [Pytorchç‰ˆåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ](https://github.com/ShusenTang/Dive-into-DL-PyTorch)\n* [PyTorchä¸­æ–‡æ–‡æ¡£](https://pytorch-cn.readthedocs.io/zh/latest/)\n* [WELCOME TO PYTORCH TUTORIALS](https://pytorch.org/tutorials/index.html)\n* [å²ä¸Šæœ€å…¨çš„PyTorchå­¦ä¹ èµ„æºæ±‡æ€»](https://zhuanlan.zhihu.com/p/64895011)\n* [ã€å¹²è´§ã€‘å²ä¸Šæœ€å…¨çš„PyTorchå­¦ä¹ èµ„æºæ±‡æ€»](https://github.com/INTERMT/Awesome-PyTorch-Chinese)\n* [Hands-on tour to deep learning with PyTorch](https://mlelarge.github.io/dataflowr-web/cea_edf_inria.html)\n* [pytorchå­¦ä¹ (äº”)â€”å›¾åƒçš„åŠ è½½/è¯»å–æ–¹å¼](https://www.jianshu.com/p/cfca9c4338e7) && [PyTorchâ€”ImageFolder/è‡ªå®šä¹‰ç±» è¯»å–å›¾ç‰‡æ•°æ®](https://blog.csdn.net/wsp_1138886114/article/details/83620869)\n\n### æ·±åº¦å­¦ä¹ å¸¸ç”¨å‘½ä»¤\n\n* [command_for_deeplearning](https://github.com/Stephenfang51/command_for_deeplearning/blob/master/command%20for%20deeplearning.md)\n* [Shellç¼–ç¨‹](https://zhuanlan.zhihu.com/p/102176365)\n\n### Pythonå¯è§†åŒ–\n\n* [Top 50 matplotlib Visualizations â€“ The Master Plots (with full python code)](https://www.machinelearningplus.com/plots/top-50-matplotlib-visualizations-the-master-plots-python/)\n* [Pythonä¹‹MatPlotLibä½¿ç”¨æ•™ç¨‹](https://blog.csdn.net/zhw864680355/article/details/102500263)\n* [ååˆ†é’Ÿä¸Šæ‰‹matplotlibï¼Œå¼€å¯ä½ çš„pythonå¯è§†åŒ–](https://mp.weixin.qq.com/s/UfvEdzr-ZGmyT08yKDOchA)\n* [ç»™æ·±åº¦å­¦ä¹ å…¥é—¨è€…çš„Pythonå¿«é€Ÿæ•™ç¨‹ - numpyå’ŒMatplotlibç¯‡](https://zhuanlan.zhihu.com/p/24309547)\n\n### æ ‡æ³¨å·¥å…·\n* ç›®æ ‡æ£€æµ‹æ ‡æ³¨å·¥å…·\n	* [labelImg](https://github.com/tzutalin/labelImg)\n* è¯­ä¹‰åˆ†å‰²æ ‡æ³¨å·¥å…·\n	* [labelme](https://github.com/wkentaro/labelme)\n\n### æ•°æ®é›†  		\n* [1. 25ä¸ªæ·±åº¦å­¦ä¹ ç›¸å…³å…¬å¼€æ•°æ®é›†](https://zhuanlan.zhihu.com/p/35449783)\n* [2. è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ•°æ®é›†](https://zhuanlan.zhihu.com/p/35423943)\n* [3.å…¨å”è¯—(43030é¦–)](https://pan.baidu.com/s/1o7QlUhO)\n* [4. ä¼¯å…‹åˆ©å¤§å­¦å…¬å¼€æ•°æ®é›†](https://people.eecs.berkeley.edu/~taesung_park/)\n* [5. ACL 2018èµ„æºï¼š100+ é¢„è®­ç»ƒçš„ä¸­æ–‡è¯å‘é‡](https://zhuanlan.zhihu.com/p/36835964)\n* [6. é¢„è®­ç»ƒä¸­æ–‡è¯å‘é‡](https://github.com/Embedding/Chinese-Word-Vectors)\n* [7. å…¬å¼€æ•°æ®é›†ç§å­åº“](http://academictorrents.com)\n* [8. è®¡ç®—æœºè§†è§‰ï¼Œæ·±åº¦å­¦ä¹ ï¼Œæ•°æ®æŒ–æ˜æ•°æ®é›†æ•´ç†](https://blog.csdn.net/c20081052/article/details/79814082)\n* [9. è®¡ç®—æœºè§†è§‰è‘—åæ•°æ®é›†CV Datasets](https://blog.csdn.net/accepthjp/article/details/51831026)\n* [10. è®¡ç®—æœºè§†è§‰ç›¸å…³æ•°æ®é›†å’Œæ¯”èµ›](https://blog.csdn.net/NNNNNNNNNNNNY/article/details/68485160)\n* [11. è¿™æ˜¯ä¸€ä»½éå¸¸å…¨é¢çš„å¼€æºæ•°æ®é›†ï¼Œä½ ï¼ŒçœŸçš„ä¸æƒ³è¦å—ï¼Ÿ](https://zhuanlan.zhihu.com/p/43846002)\n* [12. äººç¾¤å¯†åº¦ä¼°è®¡ç°æœ‰ä¸»è¦æ•°æ®é›†ç‰¹ç‚¹åŠå…¶æ¯”è¾ƒ](https://blog.csdn.net/weixin_40516558/article/details/81564464)\n* [13. DANBOORU2017: A LARGE-SCALE CROWDSOURCED AND TAGGED ANIME ILLUSTRATION DATASET](https://www.gwern.net/Danbooru2017)\n* [14. è¡Œäººé‡è¯†åˆ«æ•°æ®é›†](http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html)\n* [15. è‡ªç„¶è¯­è¨€å¤„ç†å¸¸è§æ•°æ®é›†ã€è®ºæ–‡æœ€å…¨æ•´ç†åˆ†äº«](https://zhuanlan.zhihu.com/p/56144877)\n* [16. paper, code, sota](https://paperswithcode.com/)\n* [17. æ—·è§†RPCå¤§å‹å•†å“æ•°æ®é›†å‘å¸ƒï¼](https://zhuanlan.zhihu.com/p/55627416)\n* [18. CVPR 2019ã€Œå‡†æ»¡åˆ†ã€è®ºæ–‡ï¼šè‹±ä¼Ÿè¾¾æ¨å‡ºé¦–ä¸ªè·¨æ‘„åƒå¤´æ±½è½¦è·Ÿè¸ªæ•°æ®é›†(æ±½è½¦Re-ID)](https://zhuanlan.zhihu.com/p/60617001)\n* [19.ã€OCRæŠ€æœ¯ã€‘å¤§æ‰¹é‡ç”Ÿæˆæ–‡å­—è®­ç»ƒé›†](https://zhuanlan.zhihu.com/p/59052013)\n* [20. è¯­ä¹‰åˆ†ææ•°æ®é›†-MSRA](https://github.com/msra-nlc/MSParS)\n* [IEEE DataPort](https://ieee-dataport.org/)\n* [æ•°æ®é›†å¸‚](http://www.shujujishi.com/)\n* [åŒ»ç–—/åŒ»å­¦å›¾åƒæ•°æ®é›†]()ï¼š[Medical Data for Machine Learning](https://github.com/beamandrew/medical-data) && [åŒ»ç–—é¢†åŸŸå›¾åƒæŒ‘æˆ˜èµ›æ•°æ®é›†](https://grand-challenge.org/challenges/) && [ã€åŒ»å­¦å½±åƒç³»åˆ—ï¼šä¸€ã€‘æ•°æ®é›†åˆé›† æœ€æ–°æœ€å…¨](https://blog.csdn.net/qq_31622015/article/details/90573874) && [medical-imaging-datasets](https://github.com/sfikas/medical-imaging-datasets) && [ã€æ•°æ®é›†ã€‘ä¸€æ–‡é“å°½åŒ»å­¦å›¾åƒæ•°æ®é›†ä¸ç«èµ›](https://zhuanlan.zhihu.com/p/50615907) && [åŒ»å­¦å›¾åƒæ•°æ®é›†æ±‡æ€»](https://zhuanlan.zhihu.com/p/102855802)\n\n### è®°ç¬”è®°å·¥å…·\n\n* [Markdownç¼–è¾‘å™¨ï¼šTyporaä»‹ç»](https://zhuanlan.zhihu.com/p/67153848) \n* [Markdownè¯­æ³•ä»‹ç»ï¼ˆå¸¸ç”¨ï¼‰](https://zhuanlan.zhihu.com/p/47897214)\n* [Markdown è¯­æ³•æ‰‹å†Œ ï¼ˆå®Œæ•´æ•´ç†ç‰ˆï¼‰](https://blog.csdn.net/witnessai1/article/details/52551362)\n* [Markdownä¸­Latex æ•°å­¦å…¬å¼åŸºæœ¬è¯­æ³•](https://blog.csdn.net/u014630987/article/details/70156489)\n\n### ä¼šè®®æœŸåˆŠåˆ—è¡¨\n\n* [å›½é™…ä¼šè®®æ—¥æœŸè¡¨](https://github.com/JackieTseng/conference_call_for_paper)\n* [ai-deadlines](https://github.com/abhshkdz/ai-deadlines/)\n* [Keep Up With New Trends](https://handong1587.github.io/deep_learning/2017/12/18/keep-up-with-new-trends.html)\n* [è®¡ç®—æœºä¼šè®®æ’åç­‰çº§](https://blog.csdn.net/cserchen/article/details/40508181)\n* [ä¸­å›½è®¡ç®—æœºå­¦ä¼š(CCF)æ¨èå›½é™…å­¦æœ¯åˆŠç‰©å’Œä¼šè®®](https://www.ccf.org.cn/Academic_Evaluation/By_category/)\n\n### è®ºæ–‡å†™ä½œå·¥å…·\n* [Windows: Texlive+Texstudio](https://jingyan.baidu.com/article/b2c186c83c9b40c46ff6ff4f.html)\n* [Ubuntu: Texlive+Texmaker](https://jingyan.baidu.com/article/7c6fb4280b024180642c90e4.html)\n* [Latexï¼šåŸºæœ¬ç”¨æ³•ã€è¡¨æ ¼ã€å…¬å¼ã€ç®—æ³•](https://blog.csdn.net/quiet_girl/article/details/72847208)\n* [LaTeX å„ç§å‘½ä»¤ï¼Œç¬¦å·](https://blog.csdn.net/garfielder007/article/details/51646604)\n\n### è®ºæ–‡ç”»å›¾å·¥å…·\n* [Visio2016](https://msdn.itellyou.cn/)\n* [Matplotlib](#Pythonå¯è§†åŒ–)\n\n### è®ºæ–‡å†™ä½œæ•™ç¨‹\n* [åˆ˜çŸ¥è¿œ_å¦‚ä½•å†™ä¸€ç¯‡åˆæ ¼çš„NLPè®ºæ–‡](https://zhuanlan.zhihu.com/p/58752815)\n* [åˆ˜æ´‹_å¦‚ä½•å†™è®ºæ–‡_V7](http://nlp.csai.tsinghua.edu.cn/~ly/talks/cwmt14_tut.pdf)\n* [å¦‚ä½•ç«¯åˆ°ç«¯åœ°å†™ç§‘ç ”è®ºæ–‡-é‚±é”¡é¹](https://xpqiu.github.io/slides/20181019-PaperWriting.pdf)\n* [è®ºæ–‡Introductionå†™ä½œå…¶ä¸€](https://zhuanlan.zhihu.com/p/33876355), [è®ºæ–‡Introductionå†™ä½œå…¶äºŒ](https://zhuanlan.zhihu.com/p/52494933), [è®ºæ–‡Introductionå†™ä½œå…¶ä¸‰](https://zhuanlan.zhihu.com/p/52494879)\n* [æ¯•ä¸šè®ºæ–‡æ€ä¹ˆå†™](https://zhuanlan.zhihu.com/c_179195484)\n* [æµ…è°ˆå­¦æœ¯è®ºæ–‡rebuttal](https://zhuanlan.zhihu.com/p/104298923) && [å­¦æœ¯è®ºæ–‡æŠ•ç¨¿ä¸è¿”ä¿®ï¼ˆRebuttalï¼‰åˆ†äº«](https://zhuanlan.zhihu.com/p/344008879)\n* [ç ”ä¹‹æˆç†å†™ä½œå®éªŒå®¤](https://zhuanlan.zhihu.com/rationalscience-writing-lab)\n* [æ™ºæºè®ºå›Â·è®ºæ–‡å†™ä½œä¸“é¢˜æŠ¥å‘Šä¼š]()ï¼š[ã€Šè®ºæ–‡å†™ä½œå°ç™½çš„æˆé•¿ä¹‹è·¯ã€‹](https://zhuanlan.zhihu.com/p/135989892) && [ã€Šè°ˆå¦‚ä½•å†™ä¸€ç¯‡åˆæ ¼çš„å›½é™…å­¦æœ¯è®ºæ–‡ã€‹](https://zhuanlan.zhihu.com/p/136005095) && [ã€Šè®¡ç®—æœºè§†è§‰ä¼šè®®è®ºæ–‡ä»æŠ•ç¨¿åˆ°æ¥æ”¶ã€‹](https://zhuanlan.zhihu.com/p/139571199)\n\n### ResearchGos\n\n* [ResearchGo:ç ”ç©¶ç”Ÿæ´»ç¬¬ä¸€å¸–â€”â€”æ–‡çŒ®æ£€ç´¢ä¸ç®¡ç†](https://zhuanlan.zhihu.com/p/22323250?refer=wjdml)\n* [ResearchGo:ç ”ç©¶ç”Ÿæ´»ç¬¬äºŒè´´â€”â€”æ–‡çŒ®é˜…è¯»](https://zhuanlan.zhihu.com/p/22402393?refer=wjdml)\n* [ResearchGo:ç ”ç©¶ç”Ÿæ´»ç¬¬ä¸‰å¸–â€”â€”é˜…è¯»è¾…åŠ©](https://zhuanlan.zhihu.com/p/22622502?refer=wjdml)\n* [ResearchGo:ç ”ç©¶ç”Ÿæ´»ç¬¬å››å¸–â€”â€”æ–‡çŒ®è°ƒç ”](https://zhuanlan.zhihu.com/p/23178836?refer=wjdml)\n* [ResearchGo:ç ”ç©¶ç”Ÿæ´»ç¬¬äº”å¸–â€”â€”æ–‡çŒ®ç»¼è¿°](https://zhuanlan.zhihu.com/p/23356843?refer=wjdml)\n* [ResearchGo:ç ”ç©¶ç”Ÿæ´»ç¬¬å…­å¸–â€”â€”å¦‚ä½•è®²è®ºæ–‡](https://zhuanlan.zhihu.com/p/23872063?refer=wjdml)\n* [ResearchGo:ç ”ç©¶ç”Ÿæ´»ç¬¬ä¸ƒå¸–â€”â€”ä¸“åˆ©æ£€ç´¢ä¸ç”³è¯·](https://zhuanlan.zhihu.com/p/25191025)\n* [ResearchGo:ç ”ç©¶ç”Ÿæ´»ç¬¬å…«å¸–â€”â€”å†™è®ºæ–‡ã€åšPPTã€å†™æ–‡æ¡£å¿…å¤‡å·¥å…·é›†é”¦](https://zhuanlan.zhihu.com/p/62100815)\n\n### æ¯•ä¸šè®ºæ–‡æ’ç‰ˆ\n\n* [åè¡€æ¨èæ”¶è—çš„å­¦ä½è®ºæ–‡æ’ç‰ˆæ•™ç¨‹ï¼ˆå®Œæ•´ç‰ˆï¼‰](https://zhuanlan.zhihu.com/p/52495345)\n* [è®ºæ–‡æ€ä¹ˆå†™â€”â€”å¦‚ä½•ä¿®æ”¹æ¯•ä¸šè®ºæ–‡æ ¼å¼](https://zhuanlan.zhihu.com/p/35951260)\n\n_____\n\n______\n\n## ä¿¡å·å¤„ç†\n\n### å‚…é‡Œå¶å˜æ¢\n\n* [å‚…é‡Œå¶åˆ†æä¹‹ææ­»æ•™ç¨‹ï¼ˆå®Œæ•´ç‰ˆï¼‰æ›´æ–°äº2014.06.06](https://zhuanlan.zhihu.com/p/19763358)\n* [å¦‚ä½•ç®€æ˜çš„æ€»ç»“å‚…é‡Œå¶å˜æ¢ï¼Ÿ](https://www.zhihu.com/question/34899574/answer/612923473)\n* [ä»è¿ç»­æ—¶é—´å‚…é‡Œå¶çº§æ•°åˆ°å¿«é€Ÿå‚…é‡Œå¶å˜æ¢](https://blog.csdn.net/clover13/article/details/79469851)\n* [ååˆ†ç®€æ˜æ˜“æ‡‚çš„FFTï¼ˆå¿«é€Ÿå‚…é‡Œå¶å˜æ¢ï¼‰](https://blog.csdn.net/enjoy_pascal/article/details/81478582)\n* [å‚…é‡Œå¶çº§æ•°æ¨å¯¼è¿‡ç¨‹](https://blog.csdn.net/hanxiaohu88/article/details/8245687)\n\n### å°æ³¢å˜æ¢\n\n* [å½¢è±¡æ˜“æ‡‚è®²è§£ç®—æ³•Iâ€”â€”å°æ³¢å˜æ¢](https://zhuanlan.zhihu.com/p/22450818)\n\n* [å°æ³¢å˜æ¢å®Œç¾é€šä¿—è®²è§£ç³»åˆ—ä¹‹ ï¼ˆä¸€ï¼‰](https://zhuanlan.zhihu.com/p/44215123) && [å°æ³¢å˜æ¢å®Œç¾é€šä¿—è®²è§£ç³»åˆ—ä¹‹ ï¼ˆäºŒï¼‰](https://zhuanlan.zhihu.com/p/44217268)\n\n#### å®æˆ˜\n* [MWCNNä¸­ä½¿ç”¨çš„haarå°æ³¢å˜æ¢ pytorch](https://www.cnblogs.com/wanghui-garcia/p/12524515.html)\n* [ã€å°æ³¢å˜æ¢ã€‘å°æ³¢å˜æ¢å…¥é—¨----haarå°æ³¢](https://blog.csdn.net/baidu_27643275/article/details/84826773)\n* [ï¼ˆ3ï¼‰å°æ³¢å˜æ¢åŸç†åŠåº”ç”¨](https://blog.csdn.net/hhaowang/article/details/82909332)\n* [å›¾åƒå¤„ç†-å°æ³¢å˜æ¢](https://blog.csdn.net/qq_30815237/article/details/89704855)\n\n## æœºå™¨å­¦ä¹ ç†è®ºä¸å®æˆ˜\n\n* [æœºå™¨å­¦ä¹ åŸç†](https://github.com/shunliz/Machine-Learning):star:\n* [ID3ã€C4.5ã€CARTã€éšæœºæ£®æ—ã€baggingã€boostingã€Adaboostã€GBDTã€xgboostç®—æ³•æ€»ç»“](https://zhuanlan.zhihu.com/p/34534004)\n* [æ•°æ®æŒ–æ˜åå¤§ç®—æ³•ç®€è¦è¯´æ˜](http://www.cnblogs.com/en-heng/p/5013995.html)ï¼Œ[æœºå™¨å­¦ä¹ åå¤§ç»å…¸ç®—æ³•å…¥é—¨](https://blog.csdn.net/qq_42379006/article/details/80741808) && [ã€ç®—æ³•æ¨¡å‹ã€‘è½»æ¾çœ‹æ‡‚æœºå™¨å­¦ä¹ åå¤§å¸¸ç”¨ç®—æ³•](https://www.cnblogs.com/ljt1412451704/p/9678248.html)\n* [AdaBooståˆ°GBDTç³»åˆ—]()\n  * [å½“æˆ‘ä»¬åœ¨è°ˆè®ºGBDTï¼šä» AdaBoost åˆ° Gradient Boosting](https://zhuanlan.zhihu.com/p/25096501?refer=data-miner)\n  * [å½“æˆ‘ä»¬åœ¨è°ˆè®ºGBDTï¼šGradient Boosting ç”¨äºåˆ†ç±»ä¸å›å½’](https://zhuanlan.zhihu.com/p/25257856?refer=data-miner)\n  * [å½“æˆ‘ä»¬åœ¨è°ˆè®ºGBDTï¼šå…¶ä»– Ensemble Learning ç®—æ³•](https://zhuanlan.zhihu.com/p/25443980)\n\n### æœºå™¨å­¦ä¹ ç†è®ºç¯‡ä¹‹ç»å…¸ç®—æ³•\n\n#### ä¿¡æ¯è®º\n* [1. æœºå™¨å­¦ä¹ ä¸­çš„å„ç§ç†µ](https://zhuanlan.zhihu.com/p/35423404) Â  Â \n* [2. ä»é¦™å†œç†µåˆ°æ‰‹æ¨KLæ•£åº¦ï¼šçºµè§ˆæœºå™¨å­¦ä¹ ä¸­çš„ä¿¡æ¯è®º](https://zhuanlan.zhihu.com/p/32985487)\n\n#### å¤šå±‚æ„ŸçŸ¥æœº(MLP)\n* [å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰å­¦ä¹ ä¸æ€»ç»“åšå®¢](https://blog.csdn.net/baidu_33718858/article/details/84972537)\n* [å¤šå±‚æ„ŸçŸ¥æœºï¼šMulti-Layer Perceptron](https://blog.csdn.net/xholes/article/details/78461164)\n* [ç¥ç»ç½‘ç»œåŸºç¡€-å¤šå±‚æ„ŸçŸ¥å™¨(MLP)](https://blog.csdn.net/weixin_38206214/article/details/81137911)\n\n#### kè¿‘é‚»(KNN)\n\n* [æœºå™¨å­¦ä¹ ä¹‹KNNï¼ˆkè¿‘é‚»ï¼‰ç®—æ³•è¯¦è§£](https://blog.csdn.net/sinat_30353259/article/details/80901746)\n\n#### kå‡å€¼(K-means)\n\n* [Kmeansèšç±»ç®—æ³•è¯¦è§£](https://blog.csdn.net/qq_32892383/article/details/80107795)\n\n#### æœ´ç´ è´å¶æ–¯(Naive Bayesian)\n\n* [ä¸€ä¸ªä¾‹å­ææ¸…æ¥šï¼ˆå…ˆéªŒåˆ†å¸ƒ/åéªŒåˆ†å¸ƒ/ä¼¼ç„¶ä¼°è®¡ï¼‰](https://blog.csdn.net/qq_23947237/article/details/78265026)\n* [æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ï¼ˆNaive Bayesian Classifierï¼‰](https://blog.csdn.net/qq_32690999/article/details/78737393)\n* [æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨ è¯¦ç»†è§£æ](https://blog.csdn.net/qq_17073497/article/details/81076250)\n\n#### å†³ç­–æ ‘(Decision Tree)\n* [æœ€å¸¸è§æ ¸å¿ƒçš„å†³ç­–æ ‘ç®—æ³•è¯¦ç»†ä»‹ç»ï¼Œå«ID3ã€C4.5ã€CART:star:](https://mp.weixin.qq.com/s/lXaPZyNrgG9LBv-JHdGm9A) && [æœ€å¸¸ç”¨çš„å†³ç­–æ ‘ç®—æ³•ï¼Random Forestã€Adaboostã€GBDT ç®—æ³•:star:](https://mp.weixin.qq.com/s/Nl_-PdF0nHBq8yGp6AdI-Q) && [ç»ˆäºæœ‰äººæŠŠXGBoost å’Œ LightGBM è®²æ˜ç™½äº†ï¼Œé¡¹ç›®ä¸­æœ€ä¸»æµçš„é›†æˆç®—æ³•ï¼:star:](https://mp.weixin.qq.com/s/LoX987dypDg8jbeTJMpEPQ)\n* [ä¸ºä»€ä¹ˆxgboostè¦ç”¨æ³°å‹’å±•å¼€ï¼Œä¼˜åŠ¿åœ¨å“ªé‡Œ](http://blog.itblood.com/4082.html)\n* [Python3ã€Šæœºå™¨å­¦ä¹ å®æˆ˜ã€‹å­¦ä¹ ç¬”è®°ï¼ˆäºŒï¼‰ï¼šå†³ç­–æ ‘åŸºç¡€ç¯‡ä¹‹è®©æˆ‘ä»¬ä»ç›¸äº²è¯´èµ·](https://blog.csdn.net/c406495762/article/details/75663451)\n* [Python3ã€Šæœºå™¨å­¦ä¹ å®æˆ˜ã€‹å­¦ä¹ ç¬”è®°ï¼ˆä¸‰ï¼‰ï¼šå†³ç­–æ ‘å®æˆ˜ç¯‡ä¹‹ä¸ºè‡ªå·±é…ä¸ªéšå½¢çœ¼é•œ](https://blog.csdn.net/c406495762/article/details/76262487)\n* [æœºå™¨å­¦ä¹ å®æˆ˜æ•™ç¨‹ï¼ˆåä¸‰ï¼‰ï¼šæ ‘å›å½’åŸºç¡€ç¯‡ä¹‹CARTç®—æ³•ä¸æ ‘å‰ªæ](http://cuijiahua.com/blog/2017/12/ml_13_regtree_1.html)\n* [ã€Šæœºå™¨å­¦ä¹ å®æˆ˜ã€‹åŸºäºä¿¡æ¯è®ºçš„ä¸‰ç§å†³ç­–æ ‘ç®—æ³•(ID3,C4.5,CART)](https://blog.csdn.net/gamer_gyt/article/details/51242815)\n* [è¯´è¯´å†³ç­–æ ‘å‰ªæç®—æ³•](https://zhuanlan.zhihu.com/p/31404571)\n* [æœºå™¨å­¦ä¹ å®æˆ˜ ç¬¬ä¹ç«  æ ‘å›å½’](https://blog.csdn.net/namelessml/article/details/52595066)\n* [å†³ç­–æ ‘å€¼ID3ã€C4.5å®ç°](https://blog.csdn.net/u014688145/article/details/53212112)\n* [å†³ç­–æ ‘ä¹‹CARTå®ç°](https://blog.csdn.net/u014688145/article/details/53326910)\n\n#### éšæœºæ£®æ—(Random Forest)\n* [éšæœºæ£®æ—å’ŒGBDTçš„åŒºåˆ«](https://blog.csdn.net/login_sonata/article/details/73929426)\n* [éšæœºæ£®æ—ï¼ˆRandom Forestï¼‰å…¥é—¨ä¸å®æˆ˜](https://blog.csdn.net/sb19931201/article/details/52601058)\n* [éšæœºæ£®æ—ä¹‹ç‰¹å¾é€‰æ‹©](https://www.cnblogs.com/justcxtoworld/p/3447231.html)\n\n#### çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰\n\n* [çº¿æ€§å›å½’æœ€å°äºŒä¹˜æ³•å’Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡](https://blog.csdn.net/lt793843439/article/details/91392646)\n* [ã€ä»å…¥é—¨åˆ°æ”¾å¼ƒã€‘çº¿æ€§å›å½’](https://zhuanlan.zhihu.com/p/147297924)\n* [çº¿æ€§å›å½’(é¢‘ç‡å­¦æ´¾-æœ€å¤§ä¼¼ç„¶ä¼°è®¡)ä¸å²­å›å½’(è´å¶æ–¯è§’åº¦-æœ€å¤§åéªŒä¼°è®¡)çš„æ¦‚ç‡è§£é‡Š](https://blog.csdn.net/z_feng12489/article/details/101388745)\n* [æœºå™¨å­¦ä¹ ç¬”è®°å››ï¼šçº¿æ€§å›å½’å›é¡¾ä¸logisticå›å½’](https://blog.csdn.net/xierhacker/article/details/53316138)\n\n#### é€»è¾‘å›å½’(Logistic Regression)\n\n* [ã€æœºå™¨å­¦ä¹ é¢è¯•æ€»ç»“ã€‘â€”â€” LRï¼ˆé€»è¾‘å›å½’ï¼‰](https://zhuanlan.zhihu.com/p/100763009)\n* [ã€æœºå™¨å­¦ä¹ é¢è¯•é¢˜ã€‘é€»è¾‘å›å½’ç¯‡](https://zhuanlan.zhihu.com/p/62653034)\n* [æå¤§ä¼¼ç„¶æ¦‚ç‡å’Œæœ€å°æŸå¤±å‡½æ•°ï¼Œä»¥åŠæ­£åˆ™åŒ–ç®€ä»‹](https://www.jianshu.com/p/9d2686cd407e)\n* [GLM(å¹¿ä¹‰çº¿æ€§æ¨¡å‹) ä¸ LR(é€»è¾‘å›å½’) è¯¦è§£](https://blog.csdn.net/Cdd2xd/article/details/75635688)\n\n#### æ”¯æŒå‘é‡æœº(SVM) \n* [ã€æœºå™¨å­¦ä¹ é¢è¯•æ€»ç»“ã€‘â€”â€” SVM](https://zhuanlan.zhihu.com/p/93715996)\n* [SVMç³»åˆ—-ä»åŸºç¡€åˆ°æŒæ¡](https://zhuanlan.zhihu.com/p/61123737)\n* [SVMé€šä¿—å¯¼è®º July](https://github.com/Mikoto10032/DeepLearning/blob/master/books/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E9%80%9A%E4%BF%97%E5%AF%BC%E8%AE%BA%EF%BC%88%E7%90%86%E8%A7%A3SVM%E7%9A%84%E4%B8%89%E5%B1%82%E5%A2%83%E7%95%8C%EF%BC%89LaTeX%E6%9C%80%E6%96%B0%E7%89%88_2015.1.9.pdf)Â \n* [æ ¸å‡½æ•° ](): [æœºå™¨å­¦ä¹ æœ‰å¾ˆå¤šå…³äºæ ¸å‡½æ•°çš„è¯´æ³•ï¼Œæ ¸å‡½æ•°çš„å®šä¹‰å’Œä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ](https://www.zhihu.com/question/24627666) && [SVMä¸­ï¼Œé«˜æ–¯æ ¸ä¸ºä»€ä¹ˆä¼šæŠŠåŸå§‹ç»´åº¦æ˜ å°„åˆ°æ— ç©·å¤šç»´ï¼Ÿ](https://www.zhihu.com/question/35602879) && [svmæ ¸å‡½æ•°çš„ç†è§£å’Œé€‰æ‹©](https://blog.csdn.net/leonis_v/article/details/50688766) && [æ ¸å‡½æ•°å’Œå¾„å‘åŸºæ ¸å‡½æ•° (Radial Basis Function)--RBF](https://blog.csdn.net/huang1024rui/article/details/51510611) && [SVMæ ¸å‡½æ•°](https://blog.csdn.net/xiaowei_cqu/article/details/35993729)\n\n#### æå‡æ–¹æ³•(Adaboost)\n\n* [å½“æˆ‘ä»¬åœ¨è°ˆè®ºGBDTï¼šä» AdaBoost åˆ° Gradient Boosting](https://zhuanlan.zhihu.com/p/25096501)\n\n#### æ¢¯åº¦æå‡å†³ç­–æ ‘(GBDT)\n* [LightGBMå¤§æˆ˜XGBoost](https://zhuanlan.zhihu.com/p/35645973)\n* [æ¦‚è¿°XGBoostã€Light GBMå’ŒCatBoostçš„åŒä¸ä¸åŒ](https://zhuanlan.zhihu.com/p/34698733) Â   && [XGBoostã€LightGBMã€Catboostæ€»ç»“](https://www.cnblogs.com/lvdongjie/p/11391245.html) && [XGBoostã€Light GBMå’ŒCatBoostçš„å‚æ•°åŠæ€§èƒ½æ¯”è¾ƒ](https://zhuanlan.zhihu.com/p/34698733)\n* [æ¢¯åº¦æå‡å†³ç­–æ ‘](https://zhuanlan.zhihu.com/p/36339161)\n* [GBDTåŸç†åŠåº”ç”¨](https://zhuanlan.zhihu.com/p/30339807)\n* [XGBOOSTåŸç†ç¯‡](https://zhuanlan.zhihu.com/p/31654000)\n* [xgboostå…¥é—¨ä¸å®æˆ˜ï¼ˆåŸç†ç¯‡ï¼‰](https://blog.csdn.net/sb19931201/article/details/52557382) && [xgboostå…¥é—¨ä¸å®æˆ˜ï¼ˆå®æˆ˜è°ƒå‚ç¯‡ï¼‰](https://blog.csdn.net/sb19931201/article/details/52577592)\n* [ã€å¹²è´§åˆé›†ã€‘é€šä¿—ç†è§£kaggleæ¯”èµ›å¤§æ€å™¨xgboost](https://zhuanlan.zhihu.com/p/41417638)\n* [GBDTåˆ†ç±»çš„åŸç†åŠPythonå®ç°](https://blog.csdn.net/bf02jgtrs00xktcx/article/details/82719765)\n* [GBDTåŸç†åŠåˆ©ç”¨GBDTæ„é€ æ–°çš„ç‰¹å¾-Pythonå®ç°](https://blog.csdn.net/shine19930820/article/details/71713680)\n* [Python+GBDTç®—æ³•å®æˆ˜â€”â€”é¢„æµ‹å®ç°100%å‡†ç¡®ç‡](https://www.jianshu.com/p/47e73a985ba1)\n* [xgboostä¹‹è¿‘ä¼¼åˆ†ä½æ•°ç®—æ³•ï¼ˆç›´æ–¹å›¾ç®—æ³•ï¼‰è¯¦è§£](https://blog.csdn.net/m0_37870649/article/details/104561431)\n\n#### EM(æœŸæœ›æœ€å¤§åŒ–)			\n* [äººäººéƒ½æ‡‚çš„EMç®—æ³• ](https://zhuanlan.zhihu.com/p/36331115)\n* [EMç®—æ³•å…¥é—¨æ–‡ç« ](https://zhuanlan.zhihu.com/p/61768577) Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â \n\n#### é«˜æ–¯æ··åˆæ¨¡å‹(GMM)\n* [é«˜æ–¯æ··åˆæ¨¡å‹ä¸EMç®—æ³•çš„æ•°å­¦åŸç†åŠåº”ç”¨å®ä¾‹](https://zhuanlan.zhihu.com/p/67107370)\n* [é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆGMMï¼‰](https://zhuanlan.zhihu.com/p/30483076)\n\n#### é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹(MDP)		\n* [é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ä¹‹Markov Processesï¼ˆé©¬å°”ç§‘å¤«è¿‡ç¨‹ï¼‰](https://zhuanlan.zhihu.com/p/35124726)\n* [é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ä¹‹Markov Reward Processï¼ˆé©¬å°”ç§‘å¤«å¥–åŠ±è¿‡ç¨‹ï¼‰](https://zhuanlan.zhihu.com/p/35231424)\n* [é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ä¹‹Bellman Equationï¼ˆè´å°”æ›¼æ–¹ç¨‹ï¼‰](https://zhuanlan.zhihu.com/p/35261164)\n* [é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ä¹‹Markov Decision Process(é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹)](https://zhuanlan.zhihu.com/p/35354956)\n* [é©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ä¹‹æœ€ä¼˜ä»·å€¼å‡½æ•°ä¸æœ€ä¼˜ç­–ç•¥](https://zhuanlan.zhihu.com/p/35373905)\n\n#### æ¡ä»¶éšæœºåœº(CRF, åˆ¤åˆ«å¼æ¨¡å‹)\n* [å¦‚ä½•è½»æ¾æ„‰å¿«åœ°ç†è§£æ¡ä»¶éšæœºåœº](https://zhuanlan.zhihu.com/p/104562658)\n* [å¦‚ä½•ç”¨ç®€å•æ˜“æ‡‚çš„ä¾‹å­è§£é‡Šæ¡ä»¶éšæœºåœºï¼ˆCRFï¼‰æ¨¡å‹ï¼Ÿå®ƒå’ŒHMMæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ](https://www.zhihu.com/question/35866596)\n* [HMM ,MHMM,CRF ä¼˜ç¼ºç‚¹ä¸åŒºåˆ«](https://blog.csdn.net/u013378306/article/details/55213029)\n\n#### é™ç»´ç®—æ³•\n* [æ•°æ®é™ç»´ç®—æ³•-ä»PCAåˆ°LargeVis](https://zhuanlan.zhihu.com/p/62470700)\n* [12ç§é™ç»´æ–¹æ³•ç»ˆææŒ‡å—ï¼ˆå«Pythonä»£ç ï¼‰](https://zhuanlan.zhihu.com/p/43225794)\n\n#### ä¸»æˆåˆ†åˆ†æ(PCA)\n* [ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰åŸç†è¯¦è§£](https://blog.csdn.net/program_developer/article/details/80632779)\n* [å›¾æ–‡å¹¶èŒ‚çš„PCAæ•™ç¨‹](https://blog.csdn.net/hustqb/article/details/78394058)\n* [PCAæ•°å­¦åŸç†](http://www.360doc.com/content/13/1124/02/9482_331688889.shtml)\n\n#### å¥‡å¼‚å€¼åˆ†è§£(SVD)\n* [å¼ºå¤§çš„çŸ©é˜µå¥‡å¼‚å€¼åˆ†è§£(SVD)åŠå…¶åº”ç”¨](https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html)\n* [å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰](https://zhuanlan.zhihu.com/p/29846048)\n* [å¥‡å¼‚å€¼åˆ†è§£(SVD)åŸç†è¯¦è§£åŠæ¨å¯¼](https://blog.csdn.net/zhongkejingwang/article/details/43053513) Â  Â \n* [SVDåœ¨æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨è¯¦è§£ä»¥åŠç®—æ³•æ¨å¯¼](https://blog.csdn.net/zhongkejingwang/article/details/43083603)\n\n#### çº¿æ€§åˆ¤åˆ«åˆ†æ(LDA)\n* [æ•™ç§‘ä¹¦ä¸Šçš„LDAä¸ºä»€ä¹ˆé•¿è¿™ä¸ªæ ·å­ï¼Ÿ](https://zhuanlan.zhihu.com/p/42238953)\n\n#### æ ‡ç­¾ä¼ æ’­ç®—æ³•(Label Propagation Algorithm) Â  Â \n* [æ ‡ç­¾ä¼ æ’­ç®—æ³•ï¼ˆLabel Propagationï¼‰åŠPythonå®ç°](https://blog.csdn.net/zouxy09/article/details/49105265)\n    * [å‚è€ƒèµ„æ–™](https://github.com/Mikoto10032/DeepLearning/blob/master/books/Semi-Supervised%20Learning%20with%20Graphs.pdf)\n\n#### è’™å¡”å¡ç½—æ ‘æœç´¢(MCTS)\n* [è’™ç‰¹å¡æ´›æ ‘æœç´¢å…¥é—¨æŒ‡å—](https://zhuanlan.zhihu.com/p/34950988)\n\n#### é›†æˆ(Ensemble)\n\n* [é›†æˆå­¦ä¹ ä¹‹bagging,stacking,boostingæ¦‚å¿µç†è§£](https://zhuanlan.zhihu.com/p/41809927) && [Baggingå’ŒBoostingçš„æ€»ç»“](https://www.zhihu.com/follow)\n\n* [é›†æˆå­¦ä¹ æ³•ä¹‹baggingæ–¹æ³•å’Œboostingæ–¹æ³•](https://blog.csdn.net/qq_30189255/article/details/51532442)\n* [Bagging,Boosting,Stacking](https://blog.csdn.net/Mr_tyting/article/details/72957853) && [å¸¸ç”¨çš„æ¨¡å‹é›†æˆæ–¹æ³•ä»‹ç»ï¼šbaggingã€boosting ã€stacking](https://zhuanlan.zhihu.com/p/65888174)\n\n#### tåˆ†å¸ƒéšæœºé‚»å±…åµŒå…¥(TSNE)\n* [æµå½¢å­¦ä¹ -é«˜ç»´æ•°æ®çš„é™ç»´ä¸å¯è§†åŒ–](https://blog.csdn.net/u012162613/article/details/45920827)\n* [tSNE](https://blog.csdn.net/flyingzhan/article/details/79521765)\n* [ä½¿ç”¨t-SNEå¯è§†åŒ–å›¾åƒembedding](https://zhuanlan.zhihu.com/p/81400277)\n\n#### è°±èšç±»(Spectral Clustering)\n* [è°±èšç±»ï¼ˆSpectral Clusteringï¼‰ç®—æ³•ä»‹ç»](https://blog.csdn.net/qq_24519677/article/details/82291867)\n* [èšç±»5--è°±å’Œè°±èšç±»](https://blog.csdn.net/xueyingxue001/article/details/51966980)\n\n#### å¼‚å¸¸ç‚¹æ£€æµ‹\n* [æ•°æ®æŒ–æ˜ä¸­å¸¸è§çš„ã€Œå¼‚å¸¸æ£€æµ‹ã€ç®—æ³•æœ‰å“ªäº›ï¼Ÿ](https://www.zhihu.com/question/280696035/answer/417091151)\n* [å¼‚å¸¸ç‚¹æ£€æµ‹ç®—æ³•ç»¼è¿°](https://zhuanlan.zhihu.com/p/30169110)\n* [å¼‚å¸¸æ£€æµ‹çš„Nç§æ–¹æ³•ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªä½ ä¸€å®šæƒ³ä¸åˆ°](https://mp.weixin.qq.com/s/RYLlUJiYbWqGIhzflbRGEg)\n* [å¼‚å¸¸æ£€æµ‹èµ„æºæ±‡æ€»ï¼šanomaly-detection-resources](https://zhuanlan.zhihu.com/p/158349346)\n\n### æœºå™¨å­¦ä¹ å®æˆ˜ç¯‡\n* [æœºå™¨å­¦ä¹ ä¸­ï¼Œæœ‰å“ªäº›ç‰¹å¾é€‰æ‹©çš„å·¥ç¨‹æ–¹æ³•ï¼Ÿ](https://www.zhihu.com/question/28641663) && [æœºå™¨å­¦ä¹ ï¼ˆå››ï¼‰ï¼šæ•°æ®é¢„å¤„ç†--ç‰¹å¾å·¥ç¨‹æ¦‚è¿°](https://zhuanlan.zhihu.com/p/103070096) && [ç‰¹å¾å·¥ç¨‹å®Œå…¨æ‰‹å†Œ - ä»é¢„å¤„ç†ã€æ„é€ ã€é€‰æ‹©ã€é™ç»´ã€ä¸å¹³è¡¡å¤„ç†ï¼Œåˆ°æ”¾å¼ƒ](https://zhuanlan.zhihu.com/p/94994902) && [ç‰¹å¾å·¥ç¨‹ä¸­çš„ã€Œå½’ä¸€åŒ–ã€æœ‰ä»€ä¹ˆä½œç”¨](https://www.zhihu.com/question/20455227)\n* [15åˆ†é’Ÿå¸¦ä½ å…¥é—¨sklearnä¸æœºå™¨å­¦ä¹ â€”â€”åˆ†ç±»ç®—æ³•ç¯‡](https://mp.weixin.qq.com/s?__biz=Mzg5NzAxMDgwNg==&mid=2247484110&idx=1&sn=b016e270d7b7707e6ad41a81ca45fc28&chksm=c0791fd7f70e96c103a8a2aebee166ce14f5648b3b889dd85dd9786f48b6b8269f11e5e27e1c&scene=21#wechat_redirect) && [å¦‚ä½•ä¸ºä½ çš„å›å½’é—®é¢˜é€‰æ‹©æœ€åˆé€‚çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Ÿ](https://zhuanlan.zhihu.com/p/62034592)\n* [ååˆ†é’Ÿä¸Šæ‰‹sklearnï¼šå®‰è£…ï¼Œè·å–æ•°æ®ï¼Œæ•°æ®é¢„å¤„ç†](https://zhuanlan.zhihu.com/p/105039597) && [ååˆ†é’Ÿä¸Šæ‰‹sklearnï¼šç‰¹å¾æå–ï¼Œå¸¸ç”¨æ¨¡å‹ï¼Œäº¤å‰éªŒè¯](https://zhuanlan.zhihu.com/p/105041301)\n* [MachineLearning_Python](https://github.com/lawlite19/MachineLearning_Python)\n* [Machine Learning Course with Python](https://github.com/machinelearningmindset/machine-learning-course)\n* [Statistical-Learning-Method_Code](https://github.com/Dod-o/Statistical-Learning-Method_Code)\n* [Python3æœºå™¨å­¦ä¹ ](https://blog.csdn.net/c406495762/column/info/16415)\n* [å«å¤§ç‰›æ€»ç»“çš„åˆ†ç±»æ¨¡å‹ä¸€èˆ¬éœ€è¦è°ƒèŠ‚çš„å‚æ•°](https://www.jianshu.com/p/9d2452fc93c2)\n\n\n## æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ çš„ä¸€äº›ç ”ç©¶æ–¹å‘\n\n### å¤šä»»åŠ¡å­¦ä¹ (Multi-Task Learning)\n* [æ¨¡å‹æ±‡æ€»-14 å¤šä»»åŠ¡å­¦ä¹ -Multitask Learningæ¦‚è¿°](https://zhuanlan.zhihu.com/p/27421983)\n* [(è¯‘)æ·±åº¦ç¥ç»ç½‘ç»œçš„å¤šä»»åŠ¡å­¦ä¹ æ¦‚è§ˆ(An Overview of Multi-task Learning in Deep Neural Networks)](http://www.cnblogs.com/shuzirank/p/7141017.html)\n* [Multi-task Learning and Beyond: è¿‡å»ï¼Œç°åœ¨ä¸æœªæ¥](https://zhuanlan.zhihu.com/p/138597214)ï¼›\n\n### é›¶æ¬¡å­¦ä¹ (Zero Shot Learning)\n* [é›¶æ¬¡å­¦ä¹ ï¼ˆZero-Shot Learningï¼‰å…¥é—¨](https://zhuanlan.zhihu.com/p/34656727)\n\n### å°æ ·æœ¬å­¦ä¹ (Few-Shot Learning)\n\n* [few-shot learningæ˜¯ä»€ä¹ˆ](https://blog.csdn.net/xhw205/article/details/79491649)\n* [é›¶æ¬¡å­¦ä¹ ï¼ˆZero-Shot Learningï¼‰å…¥é—¨](https://zhuanlan.zhihu.com/p/34656727)\n* [å°æ ·æœ¬å­¦ä¹ ï¼ˆFew-shot Learningï¼‰ç»¼è¿°](https://zhuanlan.zhihu.com/p/61215293)\n* [Few-Shot Learning in CVPR 2019](https://towardsdatascience.com/few-shot-learning-in-cvpr19-6c6892fc8c5)\n* [å½“å°æ ·æœ¬é‡ä¸Šæœºå™¨å­¦ä¹  fewshot learning](https://blog.csdn.net/mao_feng/article/details/78939864)\n\n### å¤šè§†è§‰å­¦ä¹ (Multi-View Learning)\n* [Multi-view Learning å¤šè§†è§’å­¦ä¹ å…¥é—¨](https://blog.csdn.net/danliwoo/article/details/79278574)\n* [å¤šè§†è§’å­¦ä¹  (Multi-View Learning)](https://blog.csdn.net/shine19930820/article/details/77426599)\n\n### åµŒå…¥(Embedding)\n\n* [ä¸‡ç‰©çš†Embeddingï¼Œä»ç»å…¸çš„word2vecåˆ°æ·±åº¦å­¦ä¹ åŸºæœ¬æ“ä½œitem2vec](https://zhuanlan.zhihu.com/p/53194407)\n* [YJangoçš„Word Embedding--ä»‹ç»](https://zhuanlan.zhihu.com/p/27830489)\n\n### è¿ç§»å­¦ä¹ (Transfer Learning)		\n* [1. è¿ç§»å­¦ä¹ ï¼šç»å…¸ç®—æ³•è§£æ](https://blog.csdn.net/linolzhang/article/details/73358219)\n* [2. ä»€ä¹ˆæ˜¯è¿ç§»å­¦ä¹  (Transfer Learning)ï¼Ÿè¿™ä¸ªé¢†åŸŸå†å²å‘å±•å‰æ™¯å¦‚ä½•ï¼Ÿ](https://www.zhihu.com/question/41979241)\n* [3. è¿ç§»å­¦ä¹ ä¸ªäººç¬”è®°](https://github.com/Mikoto10032/DeepLearning/blob/master/notes/æ—¥å¸¸é˜…è¯»ç¬”è®°/2018_4_12_è¿ç§»å­¦ä¹ .pdf) Â \n* [è¿ç§»å­¦ä¹ æ€»ç»“(One Shot Learning, Zero Shot Learning)](https://blog.csdn.net/XJTU_NOC_Wei/article/details/77850221)\n\n### åŸŸè‡ªé€‚åº”(Domain Adaptation)\n\n* [Domain Adaptationè§†é¢‘æ•™ç¨‹ï¼ˆé™„PPTï¼‰åŠç»å…¸è®ºæ–‡åˆ†äº«](https://zhuanlan.zhihu.com/p/27519182)\n* [æ¨¡å‹æ±‡æ€»15 é¢†åŸŸé€‚åº”æ€§Domain Adaptationã€One-shot/zero-shot Learningæ¦‚è¿°](https://zhuanlan.zhihu.com/p/27449079)\n* [ã€æ·±åº¦å­¦ä¹ ã€‘è®ºæ–‡å¯¼è¯»ï¼šæ— ç›‘ç£åŸŸé€‚åº”ï¼ˆDeep Transfer Network: Unsupervised Domain Adaptationï¼‰](https://blog.csdn.net/mao_xiao_feng/article/details/54426101)\n* [ã€è®ºæ–‡é˜…è¯»ç¬”è®°ã€‘åŸºäºåå‘ä¼ æ’­çš„æ— ç›‘ç£åŸŸè‡ªé€‚åº”ç ”ç©¶](https://zhuanlan.zhihu.com/p/37298073)\n* [ã€Valseå¤§ä¼šé¦–å‘ã€‘é¢†åŸŸè‡ªé€‚åº”åŠå…¶åœ¨äººè„¸è¯†åˆ«ä¸­çš„åº”ç”¨](https://zhuanlan.zhihu.com/p/21441807)\n* [CVPR 2018ï¼šåŸºäºåŸŸé€‚åº”å¼±ç›‘ç£å­¦ä¹ çš„ç›®æ ‡æ£€æµ‹](https://zhuanlan.zhihu.com/p/41126114)\n\n### å…ƒå­¦ä¹ (Meta Learning)		\n\n* [OpenAIæå‡ºæ–°å‹å…ƒå­¦ä¹ æ–¹æ³•EPGï¼Œè°ƒæ•´æŸå¤±å‡½æ•°å®ç°æ–°ä»»åŠ¡ä¸Šçš„å¿«é€Ÿè®­ç»ƒ](https://zhuanlan.zhihu.com/p/35869158?group_id=970310501209645056) Â  Â  Â \n\n### å¼ºåŒ–å­¦ä¹ (Reinforcement Learning)	\n\n* [å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰çŸ¥è¯†æ•´ç†](https://zhuanlan.zhihu.com/p/25498081)\n* [å¼ºåŒ–å­¦ä¹ ä»å…¥é—¨åˆ°æ”¾å¼ƒçš„èµ„æ–™](https://zhuanlan.zhihu.com/p/34918639)\n* [å¼ºåŒ–å­¦ä¹ å…¥é—¨](https://zhuanlan.zhihu.com/p/25498081)\n    * [å¼ºåŒ–å­¦ä¹ å…¥é—¨ ç¬¬ä¸€è®² MDP](https://zhuanlan.zhihu.com/p/25498081)\n* [å¼ºåŒ–å­¦ä¹ â€”â€”ä»Q-Learningåˆ°DQNåˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ](https://zhuanlan.zhihu.com/p/35882937)\n* [ä»å¼ºåŒ–å­¦ä¹ åˆ°æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆä¸Šï¼‰](https://zhuanlan.zhihu.com/p/35688924) Â  Â  Â  Â  Â  Â  Â  Â  Â \n* [ä»å¼ºåŒ–å­¦ä¹ åˆ°æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆä¸‹ï¼‰](https://zhuanlan.zhihu.com/p/35965070)\n* [ä¸€æ–‡å¸¦ä½ ç†è§£Q-Learningçš„æœç´¢ç­–ç•¥](https://zhuanlan.zhihu.com/p/37048004)\n\n### å¯¹æ¯”å­¦ä¹ (Contrastive Learning)\n\n* [è®ºæ–‡åˆ—è¡¨](https://github.com/asheeshcric/awesome-contrastive-self-supervised-learning)\n* [å¯¹æ¯”å­¦ä¹ ï¼ˆContrastive Learningï¼‰:ç ”ç©¶è¿›å±•ç²¾è¦](https://zhuanlan.zhihu.com/p/367290573)\n* [å¯¹æ¯”å­¦ä¹ ï¼ˆContrastive Learningï¼‰ç»¼è¿°](https://zhuanlan.zhihu.com/p/346686467)\n* [ç†è§£å¯¹æ¯”æŸå¤±çš„æ€§è´¨ä»¥åŠæ¸©åº¦ç³»æ•°çš„ä½œç”¨](https://zhuanlan.zhihu.com/p/357071960)\n\n### æ¨èç³»ç»Ÿ(Recommendation System)\n\n#### è®ºæ–‡åˆ—è¡¨\n\n* [Embeddingä»å…¥é—¨åˆ°ä¸“å®¶å¿…è¯»çš„åç¯‡è®ºæ–‡](https://zhuanlan.zhihu.com/p/58805184)\n* [Reco-papers](https://github.com/wzhe06/Reco-papers)\n* [Ad-papers](https://github.com/wzhe06/Ad-papers)\n* [deep-recommender-system](https://github.com/chocoluffy/deep-recommender-system)\n* [CTRé¢„ä¼°ç³»åˆ—å…¥é—¨æ‰‹å†Œ](https://zhuanlan.zhihu.com/p/243243145)\n\n#### æ•™ç¨‹\n\n* [æ¨èç³»ç»Ÿä»å…¥é—¨åˆ°æ¥ç€å…¥é—¨](https://zhuanlan.zhihu.com/p/27502172)\n* [æ·±åº¦å­¦ä¹ æ¨èç³»ç»Ÿç¬”è®°](https://zhuanlan.zhihu.com/p/133528693)\n* [æ¨èç³»ç»Ÿå¹²è´§æ€»ç»“](https://zhuanlan.zhihu.com/p/34004488)\n* [å…¥é—¨æ¨èç³»ç»Ÿï¼Œä½ ä¸åº”è¯¥é”™è¿‡çš„çŸ¥è¯†æ¸…å•](https://zhuanlan.zhihu.com/p/54819505)\n* [ä»é›¶å¼€å§‹äº†è§£æ¨èç³»ç»Ÿå…¨è²Œ](https://zhuanlan.zhihu.com/p/259985388)\n* [æ¨èç³»ç»Ÿç©å®¶ ä¹‹ æ¨èç³»ç»Ÿå…¥é—¨â€”â€”æ¨èç³»ç»Ÿçš„å‘å±•å†ç¨‹ï¼ˆä¸Šï¼‰](https://zhuanlan.zhihu.com/p/148207613)\n* [æ¨èç³»ç»ŸæŠ€æœ¯æ¼”è¿›è¶‹åŠ¿ï¼šä»å¬å›åˆ°æ’åºå†åˆ°é‡æ’](https://zhuanlan.zhihu.com/p/100019681)\n* [æ·±å…¥ç†è§£æ¨èç³»ç»Ÿï¼šå¬å›](https://zhuanlan.zhihu.com/p/115690499) && [æ·±å…¥ç†è§£æ¨èç³»ç»Ÿï¼šæ’åº](https://zhuanlan.zhihu.com/p/138235048)   \n* [å¬å›ç®—æ³•æœ‰å“ªäº›](https://www.zhihu.com/question/423384620/answer/1687201890)\n* [ã€Šæ·±åº¦å­¦ä¹ æ¨èç³»ç»Ÿã€‹æ€»ç»“ç³»åˆ—ä¸€](https://zhuanlan.zhihu.com/p/138446984) && [ã€Šæ·±åº¦å­¦ä¹ æ¨èç³»ç»Ÿã€‹æ€»ç»“ç³»åˆ—äºŒ](https://zhuanlan.zhihu.com/p/140894123)\n* [æ¨èç³»ç»Ÿ--å®Œæ•´çš„æ¶æ„è®¾è®¡å’Œç®—æ³•(ååŒè¿‡æ»¤ã€éšè¯­ä¹‰)](https://zhuanlan.zhihu.com/p/81752025) && [ä»0åˆ°1æ‰“é€ æ¨èç³»ç»Ÿ-æ¶æ„ç¯‡](https://zhuanlan.zhihu.com/p/123951784)\n* [ååŒè¿‡æ»¤å’ŒåŸºäºå†…å®¹æ¨èæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ](https://www.zhihu.com/question/19971859)\n* [CTRæ·±åº¦äº¤å‰ç‰¹å¾å…¥é—¨æ€»ç»“](https://zhuanlan.zhihu.com/p/257895631)\n* [æ¨èç³»ç»Ÿå­¦ä¹ ç¬”è®°](https://blog.csdn.net/wuzhongqiang/category_10128687.html)\n\n#### å®æˆ˜\n\n* [ AI-RecommenderSystem](https://github.com/zhongqiangwu960812/AI-RecommenderSystem)\n* [team-learning-rs](https://github.com/datawhalechina/team-learning-rs)\n* [RecommendSystemPractice](https://github.com/Magic-Bubble/RecommendSystemPractice)\n* [Surprise](http://surpriselib.com/)', '{"language":"Jupyter Notebook","stars":16940,"forks":3837,"watchers":16940,"open_issues":15,"topics":["cnn","deep-learning","deeplearning","gan","gcn","kaggle","machine-learning","machinelearning","mxnet","pytorch","rnn","tensorflow"],"default_branch":"master","size_kb":958284,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:apachecn:AiLearning","source_url":"https://github.com/apachecn/AiLearning"},{"type":"has_code","target_id":"github:shunliz:Machine-Learning","source_url":"https://github.com/shunliz/Machine-Learning"},{"type":"has_code","target_id":"github:fengdu78:Data-Science-Notes","source_url":"https://github.com/fengdu78/Data-Science-Notes"},{"type":"has_code","target_id":"github:fengdu78:Data-Science-Notes","source_url":"https://github.com/fengdu78/Data-Science-Notes"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:datawhalechina:statistical-learning-method-solutions-manual","source_url":"https://github.com/datawhalechina/statistical-learning-method-solutions-manual"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:datawhalechina:pumpkin-book","source_url":"https://github.com/datawhalechina/pumpkin-book"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:dformoso:deeplearning-mindmap","source_url":"https://github.com/dformoso/deeplearning-mindmap"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:datawhalechina:leeml-notes","source_url":"https://github.com/datawhalechina/leeml-notes"},{"type":"has_code","target_id":"github:iamtrask:Grokking-Deep-Learning","source_url":"https://github.com/iamtrask/Grokking-Deep-Learning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:FudanNLP:nlp-beginner","source_url":"https://github.com/FudanNLP/nlp-beginner"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:azl397985856:leetcode","source_url":"https://github.com/azl397985856/leetcode"},{"type":"has_code","target_id":"github:huaxz1986:cplusplus-_Implementation_Of_Introduction_to_Algorithms","source_url":"https://github.com/huaxz1986/cplusplus-_Implementation_Of_Introduction_to_Algorithms"},{"type":"has_code","target_id":"github:CyC2018:CS-Notes","source_url":"https://github.com/CyC2018/CS-Notes"},{"type":"has_code","target_id":"github:wolverinn:Waking-Up","source_url":"https://github.com/wolverinn/Waking-Up"},{"type":"has_code","target_id":"github:keithnull:TeachYourselfCS-CN","source_url":"https://github.com/keithnull/TeachYourselfCS-CN"},{"type":"has_code","target_id":"github:imhuay:Algorithm_for_Interview-Chinese","source_url":"https://github.com/imhuay/Algorithm_for_Interview-Chinese"},{"type":"has_code","target_id":"github:DarLiner:Algorithm_Interview_Notes-Chinese","source_url":"https://github.com/DarLiner/Algorithm_Interview_Notes-Chinese"},{"type":"has_code","target_id":"github:ShanghaiTechAIClub:DLInterview","source_url":"https://github.com/ShanghaiTechAIClub/DLInterview"},{"type":"has_code","target_id":"github:scutan90:DeepLearning-500-questions","source_url":"https://github.com/scutan90/DeepLearning-500-questions"},{"type":"has_code","target_id":"github:amusi:AI-Job-Notes","source_url":"https://github.com/amusi/AI-Job-Notes#Strategy"},{"type":"has_code","target_id":"github:apachecn:kaggle","source_url":"https://github.com/apachecn/kaggle"},{"type":"has_code","target_id":"github:weslynn:AlphaTree-graphic-deep-neural-network","source_url":"https://github.com/weslynn/AlphaTree-graphic-deep-neural-network"},{"type":"has_code","target_id":"github:weiaicunzai:awesome-image-classification","source_url":"https://github.com/weiaicunzai/awesome-image-classification"},{"type":"has_code","target_id":"github:rwightman:pytorch-image-models","source_url":"https://github.com/rwightman/pytorch-image-models"},{"type":"has_code","target_id":"github:VCBE123:AnchorFreeDetection","source_url":"https://github.com/VCBE123/AnchorFreeDetection"},{"type":"has_code","target_id":"github:amusi:awesome-object-detection","source_url":"https://github.com/amusi/awesome-object-detection"},{"type":"has_code","target_id":"github:hoya012:deep_learning_object_detection","source_url":"https://github.com/hoya012/deep_learning_object_detection"},{"type":"has_code","target_id":"github:kemaloksuz:ObjectDetectionImbalance","source_url":"https://github.com/kemaloksuz/ObjectDetectionImbalance"},{"type":"has_code","target_id":"github:mrgloom:awesome-semantic-segmentation","source_url":"https://github.com/mrgloom/awesome-semantic-segmentation"},{"type":"has_code","target_id":"github:ChanChiChoi:awesome-Face_Recognition","source_url":"https://github.com/ChanChiChoi/awesome-Face_Recognition"},{"type":"has_code","target_id":"github:MarkMoHR:Awesome-Image-Colorization","source_url":"https://github.com/MarkMoHR/Awesome-Image-Colorization"},{"type":"has_code","target_id":"github:MarkMoHR:Awesome-Edge-Detection-Papers","source_url":"https://github.com/MarkMoHR/Awesome-Edge-Detection-Papers"},{"type":"has_code","target_id":"github:pytorch:vision","source_url":"https://github.com/pytorch/vision"},{"type":"has_code","target_id":"github:akamaster:pytorch_resnet_cifar10","source_url":"https://github.com/akamaster/pytorch_resnet_cifar10"},{"type":"has_code","target_id":"github:likelyzhao:CalFLOPS-Mxnet","source_url":"https://github.com/likelyzhao/CalFLOPS-Mxnet"},{"type":"has_code","target_id":"github:milesial:Pytorch-UNet","source_url":"https://github.com/milesial/Pytorch-UNet"},{"type":"has_code","target_id":"github:qubvel:segmentation_models.pytorch","source_url":"https://github.com/qubvel/segmentation_models.pytorch"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:crownpku:Awesome-Chinese-NLP","source_url":"https://github.com/crownpku/Awesome-Chinese-NLP"},{"type":"has_code","target_id":"github:apachecn:nlp-pytorch-zh","source_url":"https://github.com/apachecn/nlp-pytorch-zh"},{"type":"has_code","target_id":"github:graykode:nlp-tutorial","source_url":"https://github.com/graykode/nlp-tutorial"},{"type":"has_code","target_id":"github:lyeoni:nlp-tutorial","source_url":"https://github.com/lyeoni/nlp-tutorial"},{"type":"has_code","target_id":"github:DeepGraphLearning:LiteratureDL4Graph","source_url":"https://github.com/DeepGraphLearning/LiteratureDL4Graph"},{"type":"has_code","target_id":"github:nnzhan:Awesome-Graph-Neural-Networks","source_url":"https://github.com/nnzhan/Awesome-Graph-Neural-Networks"},{"type":"has_code","target_id":"github:L1aoXingyu:torchlib","source_url":"https://github.com/L1aoXingyu/torchlib"},{"type":"has_code","target_id":"github:ZhiningLiu1998:awesome-imbalanced-learning","source_url":"https://github.com/ZhiningLiu1998/awesome-imbalanced-learning"},{"type":"has_code","target_id":"github:vandit15:Class-balanced-loss-pytorch","source_url":"https://github.com/vandit15/Class-balanced-loss-pytorch"},{"type":"has_code","target_id":"github:jacobgil:pytorch-grad-cam","source_url":"https://github.com/jacobgil/pytorch-grad-cam"},{"type":"has_code","target_id":"github:insikk:Grad-CAM-tensorflow","source_url":"https://github.com/insikk/Grad-CAM-tensorflow"},{"type":"has_code","target_id":"github:Ankush96:grad-cam.tensorflow","source_url":"https://github.com/Ankush96/grad-cam.tensorflow"},{"type":"has_code","target_id":"github:js-fan:mxnet","source_url":"https://github.com/js-fan/mxnet"},{"type":"has_code","target_id":"github:jackfrued:Python-100-Days","source_url":"https://github.com/jackfrued/Python-100-Days"},{"type":"has_code","target_id":"github:hzy46:Deep-Learning-21-Examples","source_url":"https://github.com/hzy46/Deep-Learning-21-Examples"},{"type":"has_code","target_id":"github:ShusenTang:Dive-into-DL-PyTorch","source_url":"https://github.com/ShusenTang/Dive-into-DL-PyTorch"},{"type":"has_code","target_id":"github:INTERMT:Awesome-PyTorch-Chinese","source_url":"https://github.com/INTERMT/Awesome-PyTorch-Chinese"},{"type":"has_code","target_id":"github:Stephenfang51:command_for_deeplearning","source_url":"https://github.com/Stephenfang51/command_for_deeplearning"},{"type":"has_code","target_id":"github:tzutalin:labelImg","source_url":"https://github.com/tzutalin/labelImg"},{"type":"has_code","target_id":"github:wkentaro:labelme","source_url":"https://github.com/wkentaro/labelme"},{"type":"has_code","target_id":"github:Embedding:Chinese-Word-Vectors","source_url":"https://github.com/Embedding/Chinese-Word-Vectors"},{"type":"has_code","target_id":"github:msra-nlc:MSParS","source_url":"https://github.com/msra-nlc/MSParS"},{"type":"has_code","target_id":"github:beamandrew:medical-data","source_url":"https://github.com/beamandrew/medical-data"},{"type":"has_code","target_id":"github:sfikas:medical-imaging-datasets","source_url":"https://github.com/sfikas/medical-imaging-datasets"},{"type":"has_code","target_id":"github:JackieTseng:conference_call_for_paper","source_url":"https://github.com/JackieTseng/conference_call_for_paper"},{"type":"has_code","target_id":"github:abhshkdz:ai-deadlines","source_url":"https://github.com/abhshkdz/ai-deadlines"},{"type":"has_code","target_id":"github:shunliz:Machine-Learning","source_url":"https://github.com/shunliz/Machine-Learning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:lawlite19:MachineLearning_Python","source_url":"https://github.com/lawlite19/MachineLearning_Python"},{"type":"has_code","target_id":"github:machinelearningmindset:machine-learning-course","source_url":"https://github.com/machinelearningmindset/machine-learning-course"},{"type":"has_code","target_id":"github:Dod-o:Statistical-Learning-Method_Code","source_url":"https://github.com/Dod-o/Statistical-Learning-Method_Code"},{"type":"has_code","target_id":"github:Mikoto10032:DeepLearning","source_url":"https://github.com/Mikoto10032/DeepLearning"},{"type":"has_code","target_id":"github:asheeshcric:awesome-contrastive-self-supervised-learning","source_url":"https://github.com/asheeshcric/awesome-contrastive-self-supervised-learning"},{"type":"has_code","target_id":"github:wzhe06:Reco-papers","source_url":"https://github.com/wzhe06/Reco-papers"},{"type":"has_code","target_id":"github:wzhe06:Ad-papers","source_url":"https://github.com/wzhe06/Ad-papers"},{"type":"has_code","target_id":"github:chocoluffy:deep-recommender-system","source_url":"https://github.com/chocoluffy/deep-recommender-system"},{"type":"has_code","target_id":"github:zhongqiangwu960812:AI-RecommenderSystem","source_url":"https://github.com/zhongqiangwu960812/AI-RecommenderSystem"},{"type":"has_code","target_id":"github:datawhalechina:team-learning-rs","source_url":"https://github.com/datawhalechina/team-learning-rs"},{"type":"has_code","target_id":"github:Magic-Bubble:RecommendSystemPractice","source_url":"https://github.com/Magic-Bubble/RecommendSystemPractice"}]', NULL, 'Apache-2.0', 'approved', 80, 'd25acbfc62960e72d13cad79dfa1fb4c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Mikoto10032-DeepLearning from https://github.com/Mikoto10032.png
Image converted to WebP: data/images/github-Mikoto10032-DeepLearning.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-mlc-ai-web-llm', 'github--mlc-ai--web-llm', 'web-llm', 'mlc-ai', '<div align="center" id="top"> **High-Performance In-Browser LLM Inference Engine.** Documentation | Blogpost | Paper | Examples </div> WebLLM is a high-performance in-browser LLM inference engine that brings language model inference directly onto web browsers with hardware acceleration. Everything runs inside the browser with no server support and is accelerated with WebGPU. WebLLM is **fully compatible with OpenAI API.** That is, you can use the same OpenAI API on **any open source models** ...', '["chatgpt","deep-learning","language-model","llm","tvm","webgpu","webml","typescript"]', 'other', 16913, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/mlc-ai/web-llm","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<div align="center" id="top">\n\n# WebLLM\n[![NPM Package](https://img.shields.io/badge/NPM_Package-Published-cc3534)](https://www.npmjs.com/package/@mlc-ai/web-llm)\n[!["WebLLM Chat Deployed"](https://img.shields.io/badge/WebLLM_Chat-Deployed-%2332a852)](https://chat.webllm.ai/)\n[![Join Discord](https://img.shields.io/badge/Join-Discord-7289DA?logo=discord&logoColor=white)](https://discord.gg/9Xpy2HGBuD)\n[![Related Repository: WebLLM Chat](https://img.shields.io/badge/Related_Repo-WebLLM_Chat-fafbfc?logo=github)](https://github.com/mlc-ai/web-llm-chat/)\n[![Related Repository: MLC LLM](https://img.shields.io/badge/Related_Repo-MLC_LLM-fafbfc?logo=github)](https://github.com/mlc-ai/mlc-llm/)\n\n**High-Performance In-Browser LLM Inference Engine.**\n\n\n[Documentation](https://webllm.mlc.ai/docs/) | [Blogpost](https://blog.mlc.ai/2024/06/13/webllm-a-high-performance-in-browser-llm-inference-engine) | [Paper](https://arxiv.org/abs/2412.15803) | [Examples](examples)\n\n</div>\n\n## Overview\nWebLLM is a high-performance in-browser LLM inference engine that brings language model inference directly onto web browsers with hardware acceleration.\nEverything runs inside the browser with no server support and is accelerated with WebGPU.\n\nWebLLM is **fully compatible with [OpenAI API](https://platform.openai.com/docs/api-reference/chat).**\nThat is, you can use the same OpenAI API on **any open source models** locally, with functionalities\nincluding streaming, JSON-mode, function-calling (WIP), etc.\n\nWe can bring a lot of fun opportunities to build AI assistants for everyone and enable privacy while enjoying GPU acceleration.\n\nYou can use WebLLM as a base [npm package](https://www.npmjs.com/package/@mlc-ai/web-llm) and build your own web application on top of it by following the examples below. This project is a companion project of [MLC LLM](https://github.com/mlc-ai/mlc-llm), which enables universal deployment of LLM across hardware environments.\n\n<div align="center">\n\n**[Check out WebLLM Chat to try it out!](https://chat.webllm.ai/)**\n\n</div>\n\n## Key Features\n- **In-Browser Inference**: WebLLM is a high-performance, in-browser language model inference engine that leverages WebGPU for hardware acceleration, enabling powerful LLM operations directly within web browsers without server-side processing.\n\n- [**Full OpenAI API Compatibility**](#full-openai-compatibility): Seamlessly integrate your app with WebLLM using OpenAI API with functionalities such as streaming, JSON-mode, logit-level control, seeding, and more.\n\n- **Structured JSON Generation**: WebLLM supports state-of-the-art JSON mode structured generation, implemented in the WebAssembly portion of the model library for optimal performance. Check [WebLLM JSON Playground](https://huggingface.co/spaces/mlc-ai/WebLLM-JSON-Playground) on HuggingFace to try generating JSON output with custom JSON schema.\n\n- [**Extensive Model Support**](#built-in-models): WebLLM natively supports a range of models including Llama 3, Phi 3, Gemma, Mistral, Qwen(é€šä¹‰åƒé—®), and many others, making it versatile for various AI tasks. For the complete supported model list, check [MLC Models](https://mlc.ai/models).\n\n- [**Custom Model Integration**](#custom-models): Easily integrate and deploy custom models in MLC format, allowing you to adapt WebLLM to specific needs and scenarios, enhancing flexibility in model deployment.\n\n- **Plug-and-Play Integration**: Easily integrate WebLLM into your projects using package managers like NPM and Yarn, or directly via CDN, complete with comprehensive [examples](./examples/) and a modular design for connecting with UI components.\n\n- **Streaming & Real-Time Interactions**: Supports streaming chat completions, allowing real-time output generation which enhances interactive applications like chatbots and virtual assistants.\n\n- **Web Worker & Service Worker Support**: Optimize UI performance and manage the lifecycle of models efficiently by offloading computations to separate worker threads or service workers.\n\n- **Chrome Extension Support**: Extend the functionality of web browsers through custom Chrome extensions using WebLLM, with examples available for building both basic and advanced extensions.\n\n## Built-in Models\n\nCheck the complete list of available models on [MLC Models](https://mlc.ai/models). WebLLM supports a subset of these available models and the list can be accessed at [`prebuiltAppConfig.model_list`](https://github.com/mlc-ai/web-llm/blob/main/src/config.ts#L293).\n\nHere are the primary families of models currently supported:\n\n- **Llama**: Llama 3, Llama 2, Hermes-2-Pro-Llama-3\n- **Phi**: Phi 3, Phi 2, Phi 1.5\n- **Gemma**: Gemma-2B\n- **Mistral**: Mistral-7B-v0.3, Hermes-2-Pro-Mistral-7B, NeuralHermes-2.5-Mistral-7B, OpenHermes-2.5-Mistral-7B\n- **Qwen (é€šä¹‰åƒé—®)**: Qwen2 0.5B, 1.5B, 7B\n\nIf you need more models, [request a new model via opening an issue](https://github.com/mlc-ai/web-llm/issues/new/choose) or check [Custom Models](#custom-models) for how to compile and use your own models with WebLLM.\n\n## Jumpstart with Examples\n\nLearn how to use WebLLM to integrate large language models into your application and generate chat completions through this simple Chatbot example: \n\n[![Example Chatbot on JSFiddle](https://img.shields.io/badge/Example-JSFiddle-blue?logo=jsfiddle&logoColor=white)](https://jsfiddle.net/neetnestor/4nmgvsa2/)\n[![Example Chatbot on Codepen](https://img.shields.io/badge/Example-Codepen-gainsboro?logo=codepen)](https://codepen.io/neetnestor/pen/vYwgZaG)\n\nFor an advanced example of a larger, more complicated project, check [WebLLM Chat](https://github.com/mlc-ai/web-llm-chat/blob/main/app/client/webllm.ts).\n\nMore examples for different use cases are available in the [examples](./examples/) folder.\n\n## Get Started\n\nWebLLM offers a minimalist and modular interface to access the chatbot in the browser.\nThe package is designed in a modular way to hook to any of the UI components.\n\n### Installation\n\n#### Package Manager\n\n```sh\n# npm\nnpm install @mlc-ai/web-llm\n# yarn\nyarn add @mlc-ai/web-llm\n# or pnpm\npnpm install @mlc-ai/web-llm\n```\n\nThen import the module in your code.\n\n```typescript\n// Import everything\nimport * as webllm from "@mlc-ai/web-llm";\n// Or only import what you need\nimport { CreateMLCEngine } from "@mlc-ai/web-llm";\n```\n\n#### CDN Delivery\n\nThanks to [jsdelivr.com](https://www.jsdelivr.com/package/npm/@mlc-ai/web-llm), WebLLM can be imported directly through URL and work out-of-the-box on cloud development platforms like [jsfiddle.net](https://jsfiddle.net/), [Codepen.io](https://codepen.io/), and [Scribbler](https://scribbler.live):\n\n```javascript\nimport * as webllm from "https://esm.run/@mlc-ai/web-llm";\n```\nIt can also be dynamically imported as:\n```javascript\nconst webllm = await import ("https://esm.run/@mlc-ai/web-llm");\n```\n\n### Create MLCEngine\n\nMost operations in WebLLM are invoked through the `MLCEngine` interface. You can create an `MLCEngine` instance and loading the model by calling the `CreateMLCEngine()` factory function.\n\n(Note that loading models requires downloading and it can take a significant amount of time for the very first run without caching previously. You should properly handle this asynchronous call.)\n\n```typescript\nimport { CreateMLCEngine } from "@mlc-ai/web-llm";\n\n// Callback function to update model loading progress\nconst initProgressCallback = (initProgress) => {\n  console.log(initProgress);\n}\nconst selectedModel = "Llama-3.1-8B-Instruct-q4f32_1-MLC";\n\nconst engine = await CreateMLCEngine(\n  selectedModel,\n  { initProgressCallback: initProgressCallback }, // engineConfig\n);\n```\n\nUnder the hood, this factory function does the following steps for first creating an engine instance (synchronous) and then loading the model (asynchronous). You can also do them separately in your application.\n\n```typescript\nimport { MLCEngine } from "@mlc-ai/web-llm";\n\n// This is a synchronous call that returns immediately\nconst engine = new MLCEngine({\n  initProgressCallback: initProgressCallback\n});\n\n// This is an asynchronous call and can take a long time to finish\nawait engine.reload(selectedModel);\n```\n\n### Chat Completion\nAfter successfully initializing the engine, you can now invoke chat completions using OpenAI style chat APIs through the `engine.chat.completions` interface. For the full list of parameters and their descriptions, check [section below](#full-openai-compatibility) and [OpenAI API reference](https://platform.openai.com/docs/api-reference/chat/create).\n\n(Note: The `model` parameter is not supported and will be ignored here. Instead, call `CreateMLCEngine(model)` or `engine.reload(model)` instead as shown in the [Create MLCEngine](#create-mlcengine) above.)\n\n\n```typescript\nconst messages = [\n  { role: "system", content: "You are a helpful AI assistant." },\n  { role: "user", content: "Hello!" },\n]\n\nconst reply = await engine.chat.completions.create({\n  messages,\n});\nconsole.log(reply.choices[0].message);\nconsole.log(reply.usage);\n```\n\n### Streaming\n\nWebLLM also supports streaming chat completion generating. To use it, simply pass `stream: true` to the `engine.chat.completions.create` call.\n\n```typescript\nconst messages = [\n  { role: "system", content: "You are a helpful AI assistant." },\n  { role: "user", content: "Hello!" },\n]\n\n// Chunks is an AsyncGenerator object\nconst chunks = await engine.chat.completions.create({\n  messages,\n  temperature: 1,\n  stream: true, // <-- Enable streaming\n  stream_options: { include_usage: true },\n});\n\nlet reply = "";\nfor await (const chunk of chunks) {\n  reply += chunk.choices[0]?.delta.content || "";\n  console.log(reply);\n  if (chunk.usage) {\n    console.log(chunk.usage); // only last chunk has usage\n  }\n}\n\nconst fullReply = await engine.getMessage();\nconsole.log(fullReply);\n```\n\n## Advanced Usage\n\n### Using Workers\n\nYou can put the heavy computation in a worker script to optimize your application performance. To do so, you need to:\n\n1. Create a handler in the worker thread that communicates with the frontend while handling the requests.\n2. Create a Worker Engine in your main application, which under the hood sends messages to the handler in the worker thread.\n\nFor detailed implementations of different kinds of Workers, check the following sections.\n\n#### Dedicated Web Worker\n\nWebLLM comes with API support for WebWorker so you can hook\nthe generation process into a separate worker thread so that\nthe computing in the worker thread won''t disrupt the UI.\n\nWe create a handler in the worker thread that communicates with the frontend while handling the requests.\n\n```typescript\n// worker.ts\nimport { WebWorkerMLCEngineHandler } from "@mlc-ai/web-llm";\n\n// A handler that resides in the worker thread\nconst handler = new WebWorkerMLCEngineHandler();\nself.onmessage = (msg: MessageEvent) => {\n  handler.onmessage(msg);\n};\n```\n\nIn the main logic, we create a `WebWorkerMLCEngine` that\nimplements the same `MLCEngineInterface`. The rest of the logic remains the same.\n\n```typescript\n// main.ts\nimport { CreateWebWorkerMLCEngine } from "@mlc-ai/web-llm";\n\nasync function main() {\n  // Use a WebWorkerMLCEngine instead of MLCEngine here\n  const engine = await CreateWebWorkerMLCEngine(\n    new Worker(\n      new URL("./worker.ts", import.meta.url), \n      {\n        type: "module",\n      }\n    ),\n    selectedModel,\n    { initProgressCallback }, // engineConfig\n  );\n\n  // everything else remains the same\n}\n```\n\n### Use Service Worker\n\nWebLLM comes with API support for ServiceWorker so you can hook the generation process\ninto a service worker to avoid reloading the model in every page visit and optimize\nyour application''s offline experience.\n\n(Note, Service Worker''s life cycle is managed by the browser and can be killed any time without notifying the webapp. `ServiceWorkerMLCEngine` will try to keep the service worker thread alive by periodically sending heartbeat events, but your application should also include proper error handling. Check `keepAliveMs` and `missedHeatbeat` in [`ServiceWorkerMLCEngine`](https://github.com/mlc-ai/web-llm/blob/main/src/service_worker.ts#L234) for more details.)\n\nWe create a handler in the worker thread that communicates with the frontend while handling the requests.\n\n\n```typescript\n// sw.ts\nimport { ServiceWorkerMLCEngineHandler } from "@mlc-ai/web-llm";\n\nlet handler: ServiceWorkerMLCEngineHandler;\n\nself.addEventListener("activate", function (event) {\n  handler = new ServiceWorkerMLCEngineHandler();\n  console.log("Service Worker is ready");\n});\n```\n\nThen in the main logic, we register the service worker and create the engine using\n`CreateServiceWorkerMLCEngine` function. The rest of the logic remains the same.\n\n```typescript\n// main.ts\nimport { MLCEngineInterface, CreateServiceWorkerMLCEngine } from "@mlc-ai/web-llm";\n\nif ("serviceWorker" in navigator) {\n  navigator.serviceWorker.register(\n    new URL("sw.ts", import.meta.url),  // worker script\n    { type: "module" },\n  );\n}\n\nconst engine: MLCEngineInterface =\n  await CreateServiceWorkerMLCEngine(\n    selectedModel,\n    { initProgressCallback }, // engineConfig\n  );\n```\n\nYou can find a complete example on how to run WebLLM in service worker in [examples/service-worker](examples/service-worker/).\n\n### Chrome Extension\nYou can also find examples of building Chrome extension with WebLLM in [examples/chrome-extension](examples/chrome-extension/) and [examples/chrome-extension-webgpu-service-worker](examples/chrome-extension-webgpu-service-worker/). The latter one leverages service worker, so the extension is persistent in the background. Additionally, you can explore another full project of a Chrome extension, WebLLM Assistant, which leverages WebLLM [here](https://github.com/mlc-ai/web-llm-assistant).\n\n## Full OpenAI Compatibility\nWebLLM is designed to be fully compatible with [OpenAI API](https://platform.openai.com/docs/api-reference/chat). Thus, besides building a simple chatbot, you can also have the following functionalities with WebLLM:\n\n- [streaming](examples/streaming): return output as chunks in real-time in the form of an AsyncGenerator\n- [json-mode](examples/json-mode): efficiently ensure output is in JSON format, see [OpenAI Reference](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) for more.\n- [seed-to-reproduce](examples/seed-to-reproduce): use seeding to ensure a reproducible output with fields `seed`.\n- [function-calling](examples/function-calling) (WIP): function calling with fields `tools` and `tool_choice` (with preliminary support); or manual function calling without `tools` or `tool_choice` (keeps the most flexibility).\n\n## Custom Models\n\nWebLLM works as a companion project of [MLC LLM](https://github.com/mlc-ai/mlc-llm) and it supports custom models in MLC format. \nIt reuses the model artifact and builds the flow of MLC LLM. To compile and use your own models with WebLLM, please check out\n[MLC LLM document](https://llm.mlc.ai/docs/deploy/webllm.html)\non how to compile and deploy new model weights and libraries to WebLLM. \n\nHere, we go over the high-level idea. There are two elements of the WebLLM package that enable new models and weight variants.\n\n- `model`: Contains a URL to model artifacts, such as weights and meta-data.\n- `model_lib`: A URL to the web assembly library (i.e. wasm file) that contains the executables to accelerate the model computations.\n\nBoth are customizable in the WebLLM.\n\n```typescript\nimport { CreateMLCEngine } from "@mlc-ai/web-llm";\n\nasync main() {\n  const appConfig = {\n    "model_list": [\n      {\n        "model": "/url/to/my/llama",\n        "model_id": "MyLlama-3b-v1-q4f32_0",\n        "model_lib": "/url/to/myllama3b.wasm",\n      }\n    ],\n  };\n  // override default\n  const chatOpts = {\n    "repetition_penalty": 1.01\n  };\n\n  // load a prebuilt model\n  // with a chat option override and app config\n  // under the hood, it will load the model from myLlamaUrl\n  // and cache it in the browser cache\n  // The chat will also load the model library from "/url/to/myllama3b.wasm",\n  // assuming that it is compatible to the model in myLlamaUrl.\n  const engine = await CreateMLCEngine(\n    "MyLlama-3b-v1-q4f32_0",\n    { appConfig }, // engineConfig\n    chatOpts,\n  );\n}\n```\n\nIn many cases, we only want to supply the model weight variant, but\nnot necessarily a new model (e.g. `NeuralHermes-Mistral` can reuse `Mistral`''s\nmodel library). For examples of how a model library can be shared by different model variants,\nsee `webllm.prebuiltAppConfig`.\n\n## Build WebLLM Package From Source\n\nNOTE: you don''t need to build from source unless you would like to modify the WebLLM package.\nTo use the npm, simply follow [Get Started](#get-started) or any of the [examples](examples) instead.\n\nTo build from source, simply run:\n\n```bash\nnpm install\nnpm run build\n```\n\nThen, to test the effects of your code change in an example, inside `examples/get-started/package.json`, change from `"@mlc-ai/web-llm": "^0.2.80"` to `"@mlc-ai/web-llm": ../..`.\n\nThen run:\n\n```bash\ncd examples/get-started\nnpm install\nnpm start\n```\n\nNote that sometimes you would need to switch between `file:../..` and `../..` to trigger npm to recognize new changes. In the worst case, you can run:\n\n```bash\ncd examples/get-started\nrm -rf node_modules dist package-lock.json .parcel-cache\nnpm install\nnpm start\n```\n\n### In case you need to build TVMjs from source\n\nWebLLM''s runtime largely depends on TVMjs: https://github.com/apache/tvm/tree/main/web\n\nWhile it is also available as an npm package: https://www.npmjs.com/package/@mlc-ai/web-runtime, you can build it from source if needed by following the steps below.\n\n1. Install [emscripten](https://emscripten.org). It is an LLVM-based compiler that compiles C/C++ source code to WebAssembly.\n    - Follow the [installation instruction](https://emscripten.org/docs/getting_started/downloads.html#installation-instructions-using-the-emsdk-recommended) to install the latest emsdk.\n    - Source `emsdk_env.sh` by `source path/to/emsdk_env.sh`, so that `emcc` is reachable from PATH and the command `emcc` works.\n\n    We can verify the successful installation by trying out `emcc` terminal.\n\n    Note: We recently found that using the latest `emcc` version may run into issues during runtime. Use `./emsdk install 3.1.56` instead of `./emsdk install latest` for now as a workaround. The error may look like\n    ```\n    Init error, LinkError: WebAssembly.instantiate(): Import #6 module="wasi_snapshot_preview1"\n    function="proc_exit": function import requires a callable\n    ```\n\n2. In `./package.json`, change from `"@mlc-ai/web-runtime": "0.18.0-dev2",` to `"@mlc-ai/web-runtime": "file:./tvm_home/web",`.\n\n3. Setup necessary environment\n\n   Prepare all the necessary dependencies for web build:\n\n   ```shell\n   ./scripts/prep_deps.sh\n   ```\n\n   In this step, if `$TVM_SOURCE_DIR` is not defined in the environment, we will execute the following line to build `tvmjs` dependency:\n   ```shell\n   git clone https://github.com/mlc-ai/relax 3rdparty/tvm-unity --recursive\n   ```\n\n   This clones the current HEAD of `mlc-ai/relax`. However, it may not always be the correct branch or commit to clone. To build a specific npm version from source, refer to the version bump PR, which states which branch (i.e. `mlc-ai/relax` or `apache/tvm`) and which commit the current WebLLM version depends on. For instance, version 0.2.52, according to its version bump PR https://github.com/mlc-ai/web-llm/pull/521, is built by checking out the following commit https://github.com/apache/tvm/commit/e6476847753c80e054719ac47bc2091c888418b6 in `apache/tvm`, rather than the HEAD of `mlc-ai/relax`.\n\n   Besides, `--recursive` is necessary and important. Otherwise, you may encounter errors like `fatal error: ''dlpack/dlpack.h'' file not found`.\n\n4. Build WebLLM Package\n\n   ```shell\n   npm run build\n   ```\n\n5. Validate some of the sub-packages\n\n   You can then go to the subfolders in [examples](examples) to validate some of the sub-packages.\n   We use Parcelv2 for bundling. Although Parcel is not very good at tracking parent directory\n   changes sometimes. When you make a change in the WebLLM package, try to edit the `package.json`\n   of the subfolder and save it, which will trigger Parcel to rebuild.\n\n## Links\n\n- [Demo App: WebLLM Chat](https://chat.webllm.ai/)\n- If you want to run LLM on native runtime, check out [MLC-LLM](https://github.com/mlc-ai/mlc-llm)\n- You might also be interested in [Web Stable Diffusion](https://github.com/mlc-ai/web-stable-diffusion/).\n\n## Acknowledgement\n\nThis project is initiated by members from CMU Catalyst, UW SAMPL, SJTU, OctoML, and the MLC community. We would love to continue developing and supporting the open-source ML community.\n\nThis project is only possible thanks to the shoulders open-source ecosystems that we stand on. We want to thank the Apache TVM community and developers of the TVM Unity effort. The open-source ML community members made these models publicly available. PyTorch and Hugging Face communities make these models accessible. We would like to thank the teams behind Vicuna, SentencePiece, LLaMA, and Alpaca. We also would like to thank the WebAssembly, Emscripten, and WebGPU communities. Finally, thanks to Dawn and WebGPU developers.\n\n## Citation\nIf you find this project to be useful, please cite:\n\n```\n@misc{ruan2024webllmhighperformanceinbrowserllm,\n      title={WebLLM: A High-Performance In-Browser LLM Inference Engine}, \n      author={Charlie F. Ruan and Yucheng Qin and Xun Zhou and Ruihang Lai and Hongyi Jin and Yixin Dong and Bohan Hou and Meng-Shiun Yu and Yiyan Zhai and Sudeep Agarwal and Hangrui Cao and Siyuan Feng and Tianqi Chen},\n      year={2024},\n      eprint={2412.15803},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2412.15803}, \n}\n```\n\n## Contributors\n\n<a href="https://github.com/mlc-ai/web-llm/graphs/contributors">\n  <img alt="contributors" src="https://contrib.rocks/image?repo=mlc-ai/web-llm"/>\n</a>\n\n<p align="right">\n  <a href="#top">â¬† Back to Top â¬†</a>\n</p>\n', '{"language":"TypeScript","stars":16913,"forks":1151,"watchers":16913,"open_issues":147,"topics":["chatgpt","deep-learning","language-model","llm","tvm","webgpu","webml"],"default_branch":"main","size_kb":67640,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:mlc-ai:web-llm-chat","source_url":"https://github.com/mlc-ai/web-llm-chat"},{"type":"has_code","target_id":"github:mlc-ai:mlc-llm","source_url":"https://github.com/mlc-ai/mlc-llm"},{"type":"has_code","target_id":"github:mlc-ai:mlc-llm","source_url":"https://github.com/mlc-ai/mlc-llm"},{"type":"has_code","target_id":"github:mlc-ai:web-llm","source_url":"https://github.com/mlc-ai/web-llm"},{"type":"has_code","target_id":"github:mlc-ai:web-llm","source_url":"https://github.com/mlc-ai/web-llm"},{"type":"has_code","target_id":"github:mlc-ai:web-llm-chat","source_url":"https://github.com/mlc-ai/web-llm-chat"},{"type":"has_code","target_id":"github:mlc-ai:web-llm","source_url":"https://github.com/mlc-ai/web-llm"},{"type":"has_code","target_id":"github:mlc-ai:web-llm-assistant","source_url":"https://github.com/mlc-ai/web-llm-assistant"},{"type":"has_code","target_id":"github:mlc-ai:mlc-llm","source_url":"https://github.com/mlc-ai/mlc-llm"},{"type":"has_code","target_id":"github:apache:tvm","source_url":"https://github.com/apache/tvm"},{"type":"has_code","target_id":"github:mlc-ai:relax","source_url":"https://github.com/mlc-ai/relax"},{"type":"has_code","target_id":"github:mlc-ai:web-llm","source_url":"https://github.com/mlc-ai/web-llm"},{"type":"has_code","target_id":"github:apache:tvm","source_url":"https://github.com/apache/tvm"},{"type":"has_code","target_id":"github:mlc-ai:mlc-llm","source_url":"https://github.com/mlc-ai/mlc-llm"},{"type":"has_code","target_id":"github:mlc-ai:web-stable-diffusion","source_url":"https://github.com/mlc-ai/web-stable-diffusion"},{"type":"has_code","target_id":"github:mlc-ai:web-llm","source_url":"https://github.com/mlc-ai/web-llm"}]', NULL, 'Apache-2.0', 'approved', 80, 'e54c4f928fa01016bf231f0e89304211', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-mlc-ai-web-llm from https://github.com/mlc-ai.png
Image converted to WebP: data/images/github-mlc-ai-web-llm.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-tensorflow-tensor2tensor', 'github--tensorflow--tensor2tensor', 'tensor2tensor', 'tensorflow', 'Tensor2Tensor, or T2T for short, is a library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research. T2T was developed by researchers and engineers in the Google Brain team and a community of users. It is now deprecated &mdash; we keep it running and welcome bug-fixes, but encourage users to use the successor library Trax. This iPython notebook explains T2T and runs in your browser using a free VM from Google, no installation needed. Al...', '["deep-learning","machine-learning","machine-translation","reinforcement-learning","tpu","python"]', 'other', 16799, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/tensorflow/tensor2tensor","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# Tensor2Tensor\n\n[![PyPI\nversion](https://badge.fury.io/py/tensor2tensor.svg)](https://badge.fury.io/py/tensor2tensor)\n[![GitHub\nIssues](https://img.shields.io/github/issues/tensorflow/tensor2tensor.svg)](https://github.com/tensorflow/tensor2tensor/issues)\n[![Contributions\nwelcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![Gitter](https://img.shields.io/gitter/room/nwjs/nw.js.svg)](https://gitter.im/tensor2tensor/Lobby)\n[![License](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://opensource.org/licenses/Apache-2.0)\n[![Travis](https://img.shields.io/travis/tensorflow/tensor2tensor.svg)](https://travis-ci.org/tensorflow/tensor2tensor)\n[![Run on FH](https://static.floydhub.com/button/button-small.svg)](https://floydhub.com/run)\n\n[Tensor2Tensor](https://github.com/tensorflow/tensor2tensor), or\n[T2T](https://github.com/tensorflow/tensor2tensor) for short, is a library\nof deep learning models and datasets designed to make deep learning more\naccessible and [accelerate ML\nresearch](https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html).\n\n\nT2T was developed by researchers and engineers in the\n[Google Brain team](https://research.google.com/teams/brain/) and a community\nof users. It is now deprecated &mdash; we keep it running and welcome\nbug-fixes, but encourage users to use the successor library [Trax](https://github.com/google/trax).\n\n### Quick Start\n\n[This iPython notebook](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)\nexplains T2T and runs in your browser using a free VM from Google,\nno installation needed. Alternatively, here is a one-command version that\ninstalls T2T, downloads MNIST, trains a model and evaluates it:\n\n```\npip install tensor2tensor && t2t-trainer \\n  --generate_data \\n  --data_dir=~/t2t_data \\n  --output_dir=~/t2t_train/mnist \\n  --problem=image_mnist \\n  --model=shake_shake \\n  --hparams_set=shake_shake_quick \\n  --train_steps=1000 \\n  --eval_steps=100\n```\n\n### Contents\n\n* [Suggested Datasets and Models](#suggested-datasets-and-models)\n  * [Mathematical Language Understanding](#mathematical-language-understanding)\n  * [Story, Question and Answer](#story-question-and-answer)\n  * [Image Classification](#image-classification)\n  * [Image Generation](#image-generation)\n  * [Language Modeling](#language-modeling)\n  * [Sentiment Analysis](#sentiment-analysis)\n  * [Speech Recognition](#speech-recognition)\n  * [Summarization](#summarization)\n  * [Translation](#translation)\n* [Basics](#basics)\n  * [Walkthrough](#walkthrough)\n  * [Installation](#installation)\n  * [Features](#features)\n* [T2T Overview](#t2t-overview)\n  * [Datasets](#datasets)\n  * [Problems and Modalities](#problems-and-modalities)\n  * [Models](#models)\n  * [Hyperparameter Sets](#hyperparameter-sets)\n  * [Trainer](#trainer)\n* [Adding your own components](#adding-your-own-components)\n* [Adding a dataset](#adding-a-dataset)\n* [Papers](#papers)\n* [Run on FloydHub](#run-on-floydhub)\n\n## Suggested Datasets and Models\n\nBelow we list a number of tasks that can be solved with T2T when\nyou train the appropriate model on the appropriate problem.\nWe give the problem and model below and we suggest a setting of\nhyperparameters that we know works well in our setup. We usually\nrun either on Cloud TPUs or on 8-GPU machines; you might need\nto modify the hyperparameters if you run on a different setup.\n\n### Mathematical Language Understanding\n\nFor evaluating mathematical expressions at the character level involving addition, subtraction and multiplication of both positive and negative decimal numbers with variable digits assigned to symbolic variables, use\n\n* the [MLU](https://art.wangperawong.com/mathematical_language_understanding_train.tar.gz) data-set:\n `--problem=algorithmic_math_two_variables`\n\nYou can try solving the problem with different transformer models and hyperparameters as described in the [paper](https://arxiv.org/abs/1812.02825):\n* Standard transformer:\n`--model=transformer`\n`--hparams_set=transformer_tiny`\n* Universal transformer:\n`--model=universal_transformer`\n`--hparams_set=universal_transformer_tiny`\n* Adaptive universal transformer:\n`--model=universal_transformer`\n`--hparams_set=adaptive_universal_transformer_tiny`\n\n### Story, Question and Answer\n\nFor answering questions based on a story, use\n\n* the [bAbi](https://research.fb.com/downloads/babi/) data-set:\n `--problem=babi_qa_concat_task1_1k`\n\nYou can choose the bAbi task from the range [1,20] and the subset from 1k or\n10k. To combine test data from all tasks into a single test set, use\n`--problem=babi_qa_concat_all_tasks_10k`\n\n### Image Classification\n\nFor image classification, we have a number of standard data-sets:\n\n* ImageNet (a large data-set): `--problem=image_imagenet`, or one\n   of the re-scaled versions (`image_imagenet224`, `image_imagenet64`,\n   `image_imagenet32`)\n* CIFAR-10: `--problem=image_cifar10` (or\n    `--problem=image_cifar10_plain` to turn off data augmentation)\n* CIFAR-100: `--problem=image_cifar100`\n* MNIST: `--problem=image_mnist`\n\nFor ImageNet, we suggest to use the ResNet or Xception, i.e.,\nuse `--model=resnet --hparams_set=resnet_50` or\n`--model=xception --hparams_set=xception_base`.\nResnet should get to above 76% top-1 accuracy on ImageNet.\n\nFor CIFAR and MNIST, we suggest to try the shake-shake model:\n`--model=shake_shake --hparams_set=shakeshake_big`.\nThis setting trained for `--train_steps=700000` should yield\nclose to 97% accuracy on CIFAR-10.\n\n### Image Generation\n\nFor (un)conditional image generation, we have a number of standard data-sets:\n\n* CelebA: `--problem=img2img_celeba` for image-to-image translation, namely,\n    superresolution from 8x8 to 32x32.\n* CelebA-HQ: `--problem=image_celeba256_rev` for a downsampled 256x256.\n* CIFAR-10: `--problem=image_cifar10_plain_gen_rev` for class-conditional\n    32x32 generation.\n* LSUN Bedrooms: `--problem=image_lsun_bedrooms_rev`\n* MS-COCO: `--problem=image_text_ms_coco_rev` for text-to-image generation.\n* Small ImageNet (a large data-set): `--problem=image_imagenet32_gen_rev` for\n    32x32 or `--problem=image_imagenet64_gen_rev` for 64x64.\n\nWe suggest to use the Image Transformer, i.e., `--model=imagetransformer`, or\nthe Image Transformer Plus, i.e., `--model=imagetransformerpp` that uses\ndiscretized mixture of logistics, or variational auto-encoder, i.e.,\n`--model=transformer_ae`.\nFor CIFAR-10, using `--hparams_set=imagetransformer_cifar10_base` or\n`--hparams_set=imagetransformer_cifar10_base_dmol` yields 2.90 bits per\ndimension. For Imagenet-32, using\n`--hparams_set=imagetransformer_imagenet32_base` yields 3.77 bits per dimension.\n\n### Language Modeling\n\nFor language modeling, we have these data-sets in T2T:\n\n* PTB (a small data-set): `--problem=languagemodel_ptb10k` for\n    word-level modeling and `--problem=languagemodel_ptb_characters`\n    for character-level modeling.\n* LM1B (a billion-word corpus): `--problem=languagemodel_lm1b32k` for\n    subword-level modeling and `--problem=languagemodel_lm1b_characters`\n    for character-level modeling.\n\nWe suggest to start with `--model=transformer` on this task and use\n`--hparams_set=transformer_small` for PTB and\n`--hparams_set=transformer_base` for LM1B.\n\n### Sentiment Analysis\n\nFor the task of recognizing the sentiment of a sentence, use\n\n* the IMDB data-set: `--problem=sentiment_imdb`\n\nWe suggest to use `--model=transformer_encoder` here and since it is\na small data-set, try `--hparams_set=transformer_tiny` and train for\nfew steps (e.g., `--train_steps=2000`).\n\n### Speech Recognition\n\nFor speech-to-text, we have these data-sets in T2T:\n\n* Librispeech (US English): `--problem=librispeech` for\n    the whole set and `--problem=librispeech_clean` for a smaller\n    but nicely filtered part.\n\n* Mozilla Common Voice (US English): `--problem=common_voice` for the whole set\n    `--problem=common_voice_clean` for a quality-checked subset.\n\n### Summarization\n\nFor summarizing longer text into shorter one we have these data-sets:\n\n* CNN/DailyMail articles summarized into a few sentences:\n  `--problem=summarize_cnn_dailymail32k`\n\nWe suggest to use `--model=transformer` and\n`--hparams_set=transformer_prepend` for this task.\nThis yields good ROUGE scores.\n\n### Translation\n\nThere are a number of translation data-sets in T2T:\n\n* English-German: `--problem=translate_ende_wmt32k`\n* English-French: `--problem=translate_enfr_wmt32k`\n* English-Czech: `--problem=translate_encs_wmt32k`\n* English-Chinese: `--problem=translate_enzh_wmt32k`\n* English-Vietnamese: `--problem=translate_envi_iwslt32k`\n* English-Spanish: `--problem=translate_enes_wmt32k`\n\nYou can get translations in the other direction by appending `_rev` to\nthe problem name, e.g., for German-English use\n`--problem=translate_ende_wmt32k_rev`\n(note that you still need to download the original data with t2t-datagen\n`--problem=translate_ende_wmt32k`).\n\nFor all translation problems, we suggest to try the Transformer model:\n`--model=transformer`. At first it is best to try the base setting,\n`--hparams_set=transformer_base`. When trained on 8 GPUs for 300K steps\nthis should reach a BLEU score of about 28 on the English-German data-set,\nwhich is close to state-of-the art. If training on a single GPU, try the\n`--hparams_set=transformer_base_single_gpu` setting. For very good results\nor larger data-sets (e.g., for English-French), try the big model\nwith `--hparams_set=transformer_big`.\n\nSee this [example](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/Transformer_translate.ipynb) to know how the translation works.\n\n## Basics\n\n### Walkthrough\n\nHere''s a walkthrough training a good English-to-German translation\nmodel using the Transformer model from [*Attention Is All You\nNeed*](https://arxiv.org/abs/1706.03762) on WMT data.\n\n```\npip install tensor2tensor\n\n# See what problems, models, and hyperparameter sets are available.\n# You can easily swap between them (and add new ones).\nt2t-trainer --registry_help\n\nPROBLEM=translate_ende_wmt32k\nMODEL=transformer\nHPARAMS=transformer_base_single_gpu\n\nDATA_DIR=$HOME/t2t_data\nTMP_DIR=/tmp/t2t_datagen\nTRAIN_DIR=$HOME/t2t_train/$PROBLEM/$MODEL-$HPARAMS\n\nmkdir -p $DATA_DIR $TMP_DIR $TRAIN_DIR\n\n# Generate data\nt2t-datagen \\n  --data_dir=$DATA_DIR \\n  --tmp_dir=$TMP_DIR \\n  --problem=$PROBLEM\n\n# Train\n# *  If you run out of memory, add --hparams=''batch_size=1024''.\nt2t-trainer \\n  --data_dir=$DATA_DIR \\n  --problem=$PROBLEM \\n  --model=$MODEL \\n  --hparams_set=$HPARAMS \\n  --output_dir=$TRAIN_DIR\n\n# Decode\n\nDECODE_FILE=$DATA_DIR/decode_this.txt\necho "Hello world" >> $DECODE_FILE\necho "Goodbye world" >> $DECODE_FILE\necho -e ''Hallo Welt\nAuf Wiedersehen Welt'' > ref-translation.de\n\nBEAM_SIZE=4\nALPHA=0.6\n\nt2t-decoder \\n  --data_dir=$DATA_DIR \\n  --problem=$PROBLEM \\n  --model=$MODEL \\n  --hparams_set=$HPARAMS \\n  --output_dir=$TRAIN_DIR \\n  --decode_hparams="beam_size=$BEAM_SIZE,alpha=$ALPHA" \\n  --decode_from_file=$DECODE_FILE \\n  --decode_to_file=translation.en\n\n# See the translations\ncat translation.en\n\n# Evaluate the BLEU score\n# Note: Report this BLEU score in papers, not the internal approx_bleu metric.\nt2t-bleu --translation=translation.en --reference=ref-translation.de\n```\n\n### Installation\n\n\n```\n# Assumes tensorflow or tensorflow-gpu installed\npip install tensor2tensor\n\n# Installs with tensorflow-gpu requirement\npip install tensor2tensor[tensorflow_gpu]\n\n# Installs with tensorflow (cpu) requirement\npip install tensor2tensor[tensorflow]\n```\n\nBinaries:\n\n```\n# Data generator\nt2t-datagen\n\n# Trainer\nt2t-trainer --registry_help\n```\n\nLibrary usage:\n\n```\npython -c "from tensor2tensor.models.transformer import Transformer"\n```\n\n### Features\n\n* Many state of the art and baseline models are built-in and new models can be\n  added easily (open an issue or pull request!).\n* Many datasets across modalities - text, audio, image - available for\n  generation and use, and new ones can be added easily (open an issue or pull\n  request for public datasets!).\n* Models can be used with any dataset and input mode (or even multiple); all\n  modality-specific processing (e.g. embedding lookups for text tokens) is done\n  with `bottom` and `top` transformations, which are specified per-feature in the\n  model.\n* Support for multi-GPU machines and synchronous (1 master, many workers) and\n  asynchronous (independent workers synchronizing through a parameter server)\n  [distributed training](https://tensorflow.github.io/tensor2tensor/distributed_training.html).\n* Easily swap amongst datasets and models by command-line flag with the data\n  generation script `t2t-datagen` and the training script `t2t-trainer`.\n* Train on [Google Cloud ML](https://tensorflow.github.io/tensor2tensor/cloud_mlengine.html) and [Cloud TPUs](https://tensorflow.github.io/tensor2tensor/cloud_tpu.html).\n\n## T2T overview\n\n### Problems\n\n**Problems** consist of features such as inputs and targets, and metadata such\nas each feature''s modality (e.g. symbol, image, audio) and vocabularies. Problem\nfeatures are given by a dataset, which is stored as a `TFRecord` file with\n`tensorflow.Example` protocol buffers. All\nproblems are imported in\n[`all_problems.py`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/all_problems.py)\nor are registered with `@registry.register_problem`. Run\n[`t2t-datagen`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/bin/t2t-datagen)\nto see the list of available problems and download them.\n\n### Models\n\n**`T2TModel`s** define the core tensor-to-tensor computation. They apply a\ndefault transformation to each input and output so that models may deal with\nmodality-independent tensors (e.g. embeddings at the input; and a linear\ntransform at the output to produce logits for a softmax over classes). All\nmodels are imported in the\n[`models` subpackage](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/models/__init__.py),\ninherit from [`T2TModel`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/utils/t2t_model.py),\nand are registered with\n[`@registry.register_model`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/utils/registry.py).\n\n### Hyperparameter Sets\n\n**Hyperparameter sets** are encoded in\n[`HParams`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/utils/hparam.py)\nobjects, and are registered with\n[`@registry.register_hparams`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/utils/registry.py).\nEvery model and problem has a `HParams`. A basic set of hyperparameters are\ndefined in\n[`common_hparams.py`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/layers/common_hparams.py)\nand hyperparameter set functions can compose other hyperparameter set functions.\n\n### Trainer\n\nThe **trainer** binary is the entrypoint for training, evaluation, and\ninference. Users can easily switch between problems, models, and hyperparameter\nsets by using the `--model`, `--problem`, and `--hparams_set` flags. Specific\nhyperparameters can be overridden with the `--hparams` flag. `--schedule` and\nrelated flags control local and distributed training/evaluation\n([distributed training documentation](https://github.com/tensorflow/tensor2tensor/tree/master/docs/distributed_training.md)).\n\n## Adding your own components\n\nT2T''s components are registered using a central registration mechanism that\nenables easily adding new ones and easily swapping amongst them by command-line\nflag. You can add your own components without editing the T2T codebase by\nspecifying the `--t2t_usr_dir` flag in `t2t-trainer`.\n\nYou can do so for models, hyperparameter sets, modalities, and problems. Please\ndo submit a pull request if your component might be useful to others.\n\nSee the [`example_usr_dir`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/test_data/example_usr_dir)\nfor an example user directory.\n\n## Adding a dataset\n\nTo add a new dataset, subclass\n[`Problem`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/problem.py)\nand register it with `@registry.register_problem`. See\n[`TranslateEndeWmt8k`](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/translate_ende.py)\nfor an example. Also see the [data generators\nREADME](https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/data_generators/README.md).\n\n## Run on FloydHub\n\n[![Run on FloydHub](https://static.floydhub.com/button/button.svg)](https://floydhub.com/run)\n\nClick this button to open a [Workspace](https://blog.floydhub.com/workspaces/) on [FloydHub](https://www.floydhub.com/?utm_medium=readme&utm_source=tensor2tensor&utm_campaign=jul_2018). You can use the workspace to develop and test your code on a fully configured cloud GPU machine.\n\nTensor2Tensor comes preinstalled in the environment, you can simply open a [Terminal](https://docs.floydhub.com/guides/workspace/#using-terminal) and run your code.\n\n```bash\n# Test the quick-start on a Workspace''s Terminal with this command\nt2t-trainer \\n  --generate_data \\n  --data_dir=./t2t_data \\n  --output_dir=./t2t_train/mnist \\n  --problem=image_mnist \\n  --model=shake_shake \\n  --hparams_set=shake_shake_quick \\n  --train_steps=1000 \\n  --eval_steps=100\n```\n\nNote: Ensure compliance with the FloydHub [Terms of Service](https://www.floydhub.com/about/terms).\n\n## Papers\n\nWhen referencing Tensor2Tensor, please cite [this\npaper](https://arxiv.org/abs/1803.07416).\n\n```\n@article{tensor2tensor,\n  author    = {Ashish Vaswani and Samy Bengio and Eugene Brevdo and\n    Francois Chollet and Aidan N. Gomez and Stephan Gouws and Llion Jones and\n    \L{}ukasz Kaiser and Nal Kalchbrenner and Niki Parmar and Ryan Sepassi and\n    Noam Shazeer and Jakob Uszkoreit},\n  title     = {Tensor2Tensor for Neural Machine Translation},\n  journal   = {CoRR},\n  volume    = {abs/1803.07416},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1803.07416},\n}\n```\n\nTensor2Tensor was used to develop a number of state-of-the-art models\nand deep learning methods. Here we list some papers that were based on T2T\nfrom the start and benefited from its features and architecture in ways\ndescribed in the [Google Research Blog post introducing\nT2T](https://research.googleblog.com/2017/06/accelerating-deep-learning-research.html).\n\n* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n* [Depthwise Separable Convolutions for Neural Machine\n   Translation](https://arxiv.org/abs/1706.03059)\n* [One Model To Learn Them All](https://arxiv.org/abs/1706.05137)\n* [Discrete Autoencoders for Sequence Models](https://arxiv.org/abs/1801.09797)\n* [Generating Wikipedia by Summarizing Long\n   Sequences](https://arxiv.org/abs/1801.10198)\n* [Image Transformer](https://arxiv.org/abs/1802.05751)\n* [Training Tips for the Transformer Model](https://arxiv.org/abs/1804.00247)\n* [Self-Attention with Relative Position Representations](https://arxiv.org/abs/1803.02155)\n* [Fast Decoding in Sequence Models using Discrete Latent Variables](https://arxiv.org/abs/1803.03382)\n* [Adafactor: Adaptive Learning Rates with Sublinear Memory Cost](https://arxiv.org/abs/1804.04235)\n* [Universal Transformers](https://arxiv.org/abs/1807.03819)\n* [Attending to Mathematical Language with Transformers](https://arxiv.org/abs/1812.02825)\n* [The Evolved Transformer](https://arxiv.org/abs/1901.11117)\n* [Model-Based Reinforcement Learning for Atari](https://arxiv.org/abs/1903.00374)\n* [VideoFlow: A Flow-Based Generative Model for Video](https://arxiv.org/abs/1903.01434)\n\n*NOTE: This is not an official Google product.*\n', '{"language":"Python","stars":16799,"forks":3698,"watchers":16799,"open_issues":590,"topics":["deep-learning","machine-learning","machine-translation","reinforcement-learning","tpu"],"default_branch":"master","size_kb":17508,"archived":true,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:google:trax","source_url":"https://github.com/google/trax"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"},{"type":"has_code","target_id":"github:tensorflow:tensor2tensor","source_url":"https://github.com/tensorflow/tensor2tensor"}]', NULL, 'Apache-2.0', 'approved', 80, '8ed5a7c64c5807665ce518fa1c8e18ed', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-tensorflow-tensor2tensor from https://github.com/tensorflow.png
Image converted to WebP: data/images/github-tensorflow-tensor2tensor.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-emilwallner-Screenshot-to-code', 'github--emilwallner--screenshot-to-code', 'Screenshot-to-code', 'emilwallner', '<img src="/README_images/screenshot-to-code.svg?raw=true" width="800px"> --- **A detailed tutorial covering the code in this repository:** Turning design mockups into code with deep learning. **Plug:** ğŸ‘‰ Check out my 60-page guide, No ML Degree, on how to land a machine learning job without a degree. The neural network is built in three iterations. Starting with a Hello World version, followed by the main neural network layers, and ending by training it to generalize. The models are based on...', '["cnn","cnn-keras","deep-learning","encoder-decoder","floydhub","jupyter","jupyter-notebook","keras","lstm","machine-learning","seq2seq","html"]', 'other', 16523, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/emilwallner/Screenshot-to-code","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<img src="/README_images/screenshot-to-code.svg?raw=true" width="800px">\n\n---\n\n**A detailed tutorial covering the code in this repository:** [Turning design mockups into code with deep learning](https://emilwallner.medium.com/how-you-can-train-an-ai-to-convert-your-design-mockups-into-html-and-css-cc7afd82fed4).\n\n**Plug:** ğŸ‘‰ Check out my 60-page guide, [No ML Degree](https://www.emilwallner.com/p/no-ml-degree), on how to land a machine learning job without a degree.\n\nThe neural network is built in three iterations. Starting with a Hello World version, followed by the main neural network layers, and ending by training it to generalize. \n\nThe models are based on Tony Beltramelli''s [pix2code](https://github.com/tonybeltramelli/pix2code), and inspired by Airbnb''s [sketching interfaces](https://airbnb.design/sketching-interfaces/), and Harvard''s [im2markup](https://github.com/harvardnlp/im2markup).\n\n**Note:** only the Bootstrap version can generalize on new design mock-ups. It uses 16 domain-specific tokens which are translated into HTML/CSS. It has a 97% accuracy. The best model uses a GRU instead of an LSTM. This version can be trained on a few GPUs. The raw HTML version has potential to generalize, but is still unproven and requires a significant amount of GPUs to train. The current model is also trained on a homogeneous and small dataset, thus it''s hard to tell how well it behaves on more complex layouts.\n\nDataset: https://github.com/tonybeltramelli/pix2code/tree/master/datasets\n\nA quick overview of the process: \n\n### 1) Give a design image to the trained neural network\n\n![Insert image](https://i.imgur.com/LDmoLLV.png)\n\n### 2) The neural network converts the image into HTML markup \n\n<img src="/README_images/html_display.gif?raw=true" width="800px">\n\n### 3) Rendered output\n\n![Screenshot](https://i.imgur.com/tEAfyZ8.png)\n\n\n## Installation\n\n### FloydHub\n\n[![Run on FloydHub](https://static.floydhub.com/button/button.svg)](https://floydhub.com/run?template=https://github.com/floydhub/pix2code-template)\n\nClick this button to open a [Workspace](https://blog.floydhub.com/workspaces/) on [FloydHub](https://www.floydhub.com/?utm_medium=readme&utm_source=pix2code&utm_campaign=aug_2018) where you will find the same environment and dataset used for the *Bootstrap version*. You can also find the trained models for testing.\n\n### Local\n``` bash\npip install keras tensorflow pillow h5py jupyter\n```\n```\ngit clone https://github.com/emilwallner/Screenshot-to-code.git\ncd Screenshot-to-code/\njupyter notebook\n```\nGo do the desired notebook, files that end with ''.ipynb''. To run the model, go to the menu then click on Cell > Run all\n\nThe final version, the Bootstrap version, is prepared with a small set to test run the model. If you want to try it with all the data, you need to download the data here: https://www.floydhub.com/emilwallner/datasets/imagetocode, and specify the correct ```dir_name```.\n\n## Folder structure\n\n``` bash\n  |  |-Bootstrap                           #The Bootstrap version\n  |  |  |-compiler                         #A compiler to turn the tokens to HTML/CSS (by pix2code)\n  |  |  |-resources											\n  |  |  |  |-eval_light                    #10 test images and markup\n  |  |-Hello_world                         #The Hello World version\n  |  |-HTML                                #The HTML version\n  |  |  |-Resources_for_index_file         #CSS,images and scripts to test index.html file\n  |  |  |-html                             #HTML files to train it on\n  |  |  |-images                           #Screenshots for training\n  |-readme_images                          #Images for the readme page\n```\n\n\n## Hello World\n<p align="center"><img src="/README_images/Hello_world_model.png?raw=true" width="400px"></p>\n\n\n## HTML\n<p align="center"><img src="/README_images/HTML_model.png?raw=true" width="400px"></p>\n\n\n## Bootstrap\n<p align="center"><img src="/README_images/Bootstrap_model.png?raw=true" width="400px"></p>\n\n## Model weights\n- [Bootstrap](https://www.floydhub.com/emilwallner/datasets/imagetocode) (The pre-trained model uses GRUs instead of LSTMs)\n- [HTML](https://www.floydhub.com/emilwallner/datasets/html_models)\n\n## Acknowledgments\n- Thanks to IBM for donating computing power through their PowerAI platform\n- The code is largely influenced by Tony Beltramelli''s pix2code paper. [Code](https://github.com/tonybeltramelli/pix2code) [Paper](https://arxiv.org/abs/1705.07962)\n- The structure and some of the functions are from Jason Brownlee''s [excellent tutorial](https://machinelearningmastery.com/develop-a-caption-generation-model-in-keras/)\n', '{"language":"HTML","stars":16523,"forks":1556,"watchers":16523,"open_issues":22,"topics":["cnn","cnn-keras","deep-learning","encoder-decoder","floydhub","jupyter","jupyter-notebook","keras","lstm","machine-learning","seq2seq"],"default_branch":"master","size_kb":51643,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:tonybeltramelli:pix2code","source_url":"https://github.com/tonybeltramelli/pix2code"},{"type":"has_code","target_id":"github:harvardnlp:im2markup","source_url":"https://github.com/harvardnlp/im2markup"},{"type":"has_code","target_id":"github:tonybeltramelli:pix2code","source_url":"https://github.com/tonybeltramelli/pix2code"},{"type":"has_code","target_id":"github:floydhub:pix2code-template","source_url":"https://github.com/floydhub/pix2code-template"},{"type":"has_code","target_id":"github:emilwallner:Screenshot-to-code.git","source_url":"https://github.com/emilwallner/Screenshot-to-code.git"},{"type":"has_code","target_id":"github:tonybeltramelli:pix2code","source_url":"https://github.com/tonybeltramelli/pix2code"}]', NULL, 'NOASSERTION', 'approved', 65, '945213fedb35f93605bde15ce17fd92e', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-emilwallner-Screenshot-to-code from https://github.com/emilwallner.png
Image converted to WebP: data/images/github-emilwallner-Screenshot-to-code.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-mrdbourke-pytorch-deep-learning', 'github--mrdbourke--pytorch-deep-learning', 'pytorch-deep-learning', 'mrdbourke', 'Welcome to the Zero to Mastery Learn PyTorch for Deep Learning course, the second best place to learn PyTorch on the internet (the first being the PyTorch documentation). * **Update April 2023:** New tutorial for PyTorch 2.0 is live! And because PyTorch 2.0 is an additive (new features) and backward-compatible release, all previous course materials will *still* work with PyTorch 2.0. <div align="center"> <a href="https://learnpytorch.io"> <img src="https://raw.githubusercontent.com/mrdbourke/...', '["deep-learning","machine-learning","pytorch","jupyter notebook"]', 'other', 16454, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/mrdbourke/pytorch-deep-learning","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Learn PyTorch for Deep Learning\n\nWelcome to the [Zero to Mastery Learn PyTorch for Deep Learning course](https://dbourke.link/ZTMPyTorch), the second best place to learn PyTorch on the internet (the first being the [PyTorch documentation](https://pytorch.org/docs/stable/index.html)).\n\n* **Update April 2023:** New [tutorial for PyTorch 2.0](https://www.learnpytorch.io/pytorch_2_intro/) is live! And because PyTorch 2.0 is an additive (new features) and backward-compatible release, all previous course materials will *still* work with PyTorch 2.0.\n\n<div align="center">\n    <a href="https://learnpytorch.io">\n        <img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/misc-pytorch-course-launch-cover-white-text-black-background.jpg" width=750 alt="pytorch deep learning by zero to mastery cover photo with different sections of the course">\n    </a>\n</div>\n\n## Contents of this page\n\n* [Course materials/outline](https://github.com/mrdbourke/pytorch-deep-learning#course-materialsoutline)\n* [About this course](https://github.com/mrdbourke/pytorch-deep-learning#about-this-course)\n* [Status](https://github.com/mrdbourke/pytorch-deep-learning#status) (the progress of the course creation)\n* [Log](https://github.com/mrdbourke/pytorch-deep-learning#log) (a log of the course material creation process)\n\n## Course materials/outline\n\n* ğŸ“– **Online book version:** All of course materials are available in a readable online book at [learnpytorch.io](https://learnpytorch.io).\n* ğŸ¥ **First five sections on YouTube:** Learn Pytorch in a day by watching the [first 25-hours of material](https://youtu.be/Z_ikDlimN6A).\n* ğŸ”¬ **Course focus:** code, code, code, experiment, experiment, experiment.\n* ğŸƒâ€â™‚ï¸ **Teaching style:** [https://sive.rs/kimo](https://sive.rs/kimo).\n* ğŸ¤” **Ask a question:** See the [GitHub Discussions page](https://github.com/mrdbourke/pytorch-deep-learning/discussions) for existing questions/ask your own.\n\n| **Section** | **What does it cover?** | **Exercises & Extra-curriculum** | **Slides** |\n| ----- | ----- | ----- | ----- |\n| [00 - PyTorch Fundamentals](https://www.learnpytorch.io/00_pytorch_fundamentals/) | Many fundamental PyTorch operations used for deep learning and neural networks. | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/00_pytorch_fundamentals/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/00_pytorch_and_deep_learning_fundamentals.pdf) |\n| [01 - PyTorch Workflow](https://www.learnpytorch.io/01_pytorch_workflow/) | Provides an outline for approaching deep learning problems and building neural networks with PyTorch. | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/01_pytorch_workflow/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/01_pytorch_workflow.pdf) |\n| [02 - PyTorch Neural Network Classification](https://www.learnpytorch.io/02_pytorch_classification/) | Uses the PyTorch workflow from 01 to go through a neural network classification problem. | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/02_pytorch_classification/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/02_pytorch_classification.pdf) |\n| [03 - PyTorch Computer Vision](https://www.learnpytorch.io/03_pytorch_computer_vision/) | Let''s see how PyTorch can be used for computer vision problems using the same workflow from 01 & 02. | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/03_pytorch_computer_vision/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/03_pytorch_computer_vision.pdf) |\n| [04 - PyTorch Custom Datasets](https://www.learnpytorch.io/04_pytorch_custom_datasets/) | How do you load a custom dataset into PyTorch? Also we''ll be laying the foundations in this notebook for our modular code (covered in 05). | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/04_pytorch_custom_datasets/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/04_pytorch_custom_datasets.pdf) |\n| [05 - PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/) | PyTorch is designed to be modular, let''s turn what we''ve created into a series of Python scripts (this is how you''ll often find PyTorch code in the wild). | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/05_pytorch_going_modular/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/05_pytorch_going_modular.pdf) |\n| [06 - PyTorch Transfer Learning](https://www.learnpytorch.io/06_pytorch_transfer_learning/) | Let''s take a well performing pre-trained model and adjust it to one of our own problems. | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/06_pytorch_transfer_learning/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/06_pytorch_transfer_learning.pdf) |\n| [07 - Milestone Project 1: PyTorch Experiment Tracking](https://www.learnpytorch.io/07_pytorch_experiment_tracking/) | We''ve built a bunch of models... wouldn''t it be good to track how they''re all going? | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/07_pytorch_experiment_tracking.pdf) |\n| [08 - Milestone Project 2: PyTorch Paper Replicating](https://www.learnpytorch.io/08_pytorch_paper_replicating/) | PyTorch is the most popular deep learning framework for machine learning research, let''s see why by replicating a machine learning paper. | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/08_pytorch_paper_replicating/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/08_pytorch_paper_replicating.pdf) |\n| [09 - Milestone Project 3: Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/) | So we''ve built a working PyTorch model... how do we get it in the hands of others? Hint: deploy it to the internet. | [Go to exercises & extra-curriculum](https://www.learnpytorch.io/09_pytorch_model_deployment/#exercises) | [Go to slides](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/09_pytorch_model_deployment.pdf) |\n| [PyTorch Extra Resources](https://www.learnpytorch.io/pytorch_extra_resources/) | This course covers a large amount of PyTorch and deep learning but the field of machine learning is vast, inside here you''ll find recommended books and resources for: PyTorch and deep learning, ML engineering, NLP (natural language processing), time series data, where to find datasets and more. | - | - |\n| [PyTorch Cheatsheet](https://www.learnpytorch.io/pytorch_cheatsheet/) | A very quick overview of some of the main features of PyTorch plus links to various resources where more can be found in the course and in the PyTorch documentation. | - | - |\n| [A Quick PyTorch 2.0 Tutorial](https://www.learnpytorch.io/pytorch_2_intro/) | A fasssssst introduction to PyTorch 2.0, what''s new and how to get started along with resources to learn more. | - | - |\n\n## Status\n\nAll materials completed and videos published on Zero to Mastery!\n\nSee the project page for work-in-progress board - https://github.com/users/mrdbourke/projects/1 \n\n* **Total video count:** 321\n* **Done skeleton code for:** 00, 01, 02, 03, 04, 05, 06, 07, 08, 09\n* **Done annotations (text) for:** 00, 01, 02, 03, 04, 05, 06, 07, 08, 09\n* **Done images for:** 00, 01, 02, 03, 04, 05, 06, 07, 08, 09\n* **Done keynotes for:** 00, 01, 02, 03, 04, 05, 06, 07, 08, 09\n* **Done exercises and solutions for:** 00, 01, 02, 03, 04, 05, 06, 07, 08, 09\n\nSee the [log](https://github.com/mrdbourke/pytorch-deep-learning#log) for almost daily updates.\n\n## About this course\n\n### Who is this course for?\n\n**You:** Are a beginner in the field of machine learning or deep learning and would like to learn PyTorch.\n\n**This course:** Teaches you PyTorch and many machine learning concepts in a hands-on, code-first way.\n\nIf you already have 1-year+ experience in machine learning, this course may help but it is specifically designed to be beginner-friendly.\n\n### What are the prerequisites?\n\n1. 3-6 months coding Python.\n2. At least one beginner machine learning course (however this might be able to be skipped, resources are linked for many different topics).\n3. Experience using Jupyter Notebooks or Google Colab (though you can pick this up as we go along).\n4. A willingness to learn (most important).\n\nFor 1 & 2, I''d recommend the [Zero to Mastery Data Science and Machine Learning Bootcamp](https://dbourke.link/ZTMMLcourse), it''ll teach you the fundamentals of machine learning and Python (I''m biased though, I also teach that course).\n\n### How is the course taught?\n\nAll of the course materials are available for free in an online book at [learnpytorch.io](https://learnpytorch.io). If you like to read, I''d recommend going through the resources there.\n\nIf you prefer to learn via video, the course is also taught in apprenticeship-style format, meaning I write PyTorch code, you write PyTorch code.\n\nThere''s a reason the course motto''s include *if in doubt, run the code* and *experiment, experiment, experiment!*.\n\nMy whole goal is to help you to do one thing: learn machine learning by writing PyTorch code.\n\nThe code is all written via [Google Colab Notebooks](https://colab.research.google.com) (you could also use Jupyter Notebooks), an incredible free resource to experiment with machine learning.\n\n### What will I get if I finish the course?\n\nThere''s certificates and all that jazz if you go through the videos.\n\nBut certificates are meh.\n\nYou can consider this course a machine learning momentum builder.\n\nBy the end, you''ll have written hundreds of lines of PyTorch code.\n\nAnd will have been exposed to many of the most important concepts in machine learning.\n\nSo when you go to build your own machine learning projects or inspect a public machine learning project made with PyTorch, it''ll feel familiar and if it doesn''t, at least you''ll know where to look.\n\n### What will I build in the course?\n\nWe start with the barebone fundamentals of PyTorch and machine learning, so even if you''re new to machine learning you''ll be caught up to speed.\n\nThen weâ€™ll explore more advanced areas including PyTorch neural network classification, PyTorch workflows, computer vision, custom datasets, experiment tracking, model deployment, and my personal favourite: transfer learning, a powerful technique for taking what one machine learning model has learned on another problem and applying it to your own!\n\nAlong the way, youâ€™ll build three milestone projects surrounding an overarching project called FoodVision, a neural network computer vision model to classify images of food. \n\nThese milestone projects will help you practice using PyTorch to cover important machine learning concepts and create a portfolio you can show employers and say "here''s what I''ve done".\n\n### How do I get started?\n\nYou can read the materials on any device but this course is best viewed and coded along within a desktop browser.\n\nThe course uses a free tool called Google Colab. If you''ve got no experience with it, I''d go through the free [Introduction to Google Colab tutorial](https://colab.research.google.com/notebooks/basic_features_overview.ipynb) and then come back here.\n\nTo start:\n\n1. Click on one of the notebook or section links above like "[00. PyTorch Fundamentals](https://www.learnpytorch.io/00_pytorch_fundamentals/)". \n2. Click the "Open in Colab" button up the top.\n3. Press SHIFT+Enter a few times and see what happens.\n\n### My question isn''t answered \n\nPlease leave a [discussion](https://github.com/mrdbourke/pytorch-deep-learning/discussions) or send me an email directly: daniel (at) mrdbourke (dot) com.\n\n## Log\n\nAlmost daily updates of what''s happening.\n\n* 15 May 2023 - PyTorch 2.0 tutorial finished + videos added to ZTM/Udemy, see code: https://www.learnpytorch.io/pytorch_2_intro/\n* 13 Apr 2023 - update PyTorch 2.0 notebook\n* 30 Mar 2023 - update PyTorch 2.0 notebook with more info/clean code\n* 23 Mar 2023 - upgrade PyTorch 2.0 tutorial with annotations and images\n* 13 Mar 2023 - add starter code for PyTorch 2.0 tutorial \n* 18 Nov 2022 - add a reference for 3 most common errors in PyTorch + links to course sections for more: https://www.learnpytorch.io/pytorch_most_common_errors/ \n* 9 Nov 2022 - add PyTorch cheatsheet for a very quick overview of the main features of PyTorch + links to course sections: https://www.learnpytorch.io/pytorch_cheatsheet/ \n* 9 Nov 2022 - full course materials (300+ videos) are now live on Udemy! You can sign up here: https://www.udemy.com/course/pytorch-for-deep-learning/?couponCode=ZTMGOODIES7 (launch deal code valid for 3-4 days from this line)\n* 4 Nov 2022 - add a notebook for PyTorch Cheatsheet in `extras/` (a simple overview of many of the most important functionality of PyTorch)\n* 2 Oct 2022 - all videos for section 08 and 09 published (100+ videos for the last two sections)!\n* 30 Aug 2022 - recorded 15 videos for 09, total videos: 321, finished section 09 videos!!!! ... even bigger than 08!!\n* 29 Aug 2022 - recorded 16 videos for 09, total videos: 306\n* 28 Aug 2022 - recorded 11 videos for 09, total videos: 290\n* 27 Aug 2022 - recorded 16 videos for 09, total videos: 279\n* 26 Aug 2022 - add finishing touchs to notebook 09, add slides for 09, create solutions and exercises for 09\n* 25 Aug 2022 - add annotations and cleanup 09, remove TK''s, cleanup images, make slides for 09\n* 24 Aug 2022 - add annotations to 09, main takeaways, exercises and extra-curriculum done\n* 23 Aug 2022 - add annotations to 09, add plenty of images/slides\n* 22 Aug 2022 - add annotations to 09, start working on slides/images\n* 20 Aug 2022 - add annotations to 09 \n* 19 Aug 2022 - add annotations to 09, check out the awesome demos!\n* 18 Aug 2022 - add annotations to 09 \n* 17 Aug 2022 - add annotations to 09\n* 16 Aug 2022 - add annotations to 09\n* 15 Aug 2022 - add annotations to 09\n* 13 Aug 2022 - add annotations to 09\n* 12 Aug 2022 - add demo files for notebook 09 to `demos/`, start annotating notebook 09 with explainer text\n* 11 Aug 2022 - finish skeleton code for notebook 09, course finishes deploying 2x models, one for FoodVision Mini & one for (secret)\n* 10 Aug 2022 - add section for PyTorch Extra Resources (places to learn more about PyTorch/deep learning): https://www.learnpytorch.io/pytorch_extra_resources/ \n* 09 Aug 2022 - add more skeleton code to notebook 09\n* 08 Aug 2022 - create draft notebook for 09, end goal to deploy FoodVision Mini model and make it publically accessible\n* 05 Aug 2022 - recorded 11 videos for 08, total videos: 263, section 08 videos finished!... the biggest section so far\n* 04 Aug 2022 - recorded 13 videos for 08, total videos: 252\n* 03 Aug 2022 - recorded 3 videos for 08, total videos: 239\n* 02 Aug 2022 - recorded 12 videos for 08, total videos: 236\n* 30 July 2022 - recorded 11 videos for 08, total videos: 224\n* 29 July 2022 - add exercises + solutions for 08, see live walkthrough on YouTube: https://youtu.be/tjpW_BY8y3g\n* 28 July 2022 - add slides for 08\n* 27 July 2022 - cleanup much of 08, start on slides for 08, exercises and extra-curriculum next\n* 26 July 2022 - add annotations and images for 08\n* 25 July 2022 - add annotations for 08 \n* 24 July 2022 - launched first half of course (notebooks 00-04) in a single video (25+ hours!!!) on YouTube: https://youtu.be/Z_ikDlimN6A \n* 21 July 2022 - add annotations and images for 08\n* 20 July 2022 - add annotations and images for 08, getting so close! this is an epic section \n* 19 July 2022 - add annotations and images for 08\n* 15 July 2022 - add annotations and images for 08 \n* 14 July 2022 - add annotations for 08\n* 12 July 2022 - add annotations for 08, woo woo this is bigggg section! \n* 11 July 2022 - add annotations for 08 \n* 9 July 2022 - add annotations for 08\n* 8 July 2022 - add a bunch of annotations to 08\n* 6 July 2022 - course launched on ZTM Academy with videos for sections 00-07! ğŸš€ - https://dbourke.link/ZTMPyTorch \n* 1 July 2022 - add annotations and images for 08 \n* 30 June 2022 - add annotations for 08\n* 28 June 2022 - recorded 11 videos for section 07, total video count 213, all videos for section 07 complete!\n* 27 June 2022 - recorded 11 videos for section 07, total video count 202\n* 25 June 2022 - recreated 7 videos for section 06 to include updated APIs, total video count 191\n* 24 June 2022 - recreated 12 videos for section 06 to include updated APIs\n* 23 June 2022 - finish annotations for 07, add exercise template and solutions for 07 + video walkthrough on YouTube: https://youtu.be/cO_r2FYcAjU\n* 21 June 2022 - make 08 runnable end-to-end, add images and annotations for 07\n* 17 June 2022 - fix up 06, 07 v2 for upcoming torchvision version upgrade, add plenty of annotations to 08\n* 13 June 2022 - add notebook 08 first version, starting to replicate the Vision Transformer paper\n* 10 June 2022 - add annotations for 07 v2\n* 09 June 2022 - create 07 v2 for `torchvision` v0.13 (this will replace 07 v1 when `torchvision=0.13` is released)\n* 08 June 2022 - adapt 06 v2 for `torchvision` v0.13 (this will replace 06 v1 when `torchvision=0.13` is released)\n* 07 June 2022 - create notebook 06 v2 for upcoming `torchvision` v0.13 update (new transfer learning methods)\n* 04 June 2022 - add annotations for 07\n* 03 June 2022 - huuuuuuge amount of annotations added to 07 \n* 31 May 2022 - add a bunch of annotations for 07, make code runnable end-to-end\n* 30 May 2022 - record 4 videos for 06, finished section 06, onto section 07, total videos 186\n* 28 May 2022 - record 10 videos for 06, total videos 182\n* 24 May 2022 - add solutions and exercises for 06\n* 23 May 2022 - finished annotations and images for 06, time to do exercises and solutions \n* 22 May 2202 - add plenty of images to 06\n* 18 May 2022 - add plenty of annotations to 06\n* 17 May 2022 - added a bunch of annotations for section 06\n* 16 May 2022 - recorded 10 videos for section 05, finish videos for section 05 âœ…\n* 12 May 2022 - added exercises and solutions for 05\n* 11 May 2022 - clean up part 1 and part 2 notebooks for 05, make slides for 05, start on exercises and solutions for 05\n* 10 May 2022 - huuuuge updates to the 05 section, see the website, it looks pretty: https://www.learnpytorch.io/05_pytorch_going_modular/ \n* 09 May 2022 - add a bunch of materials for 05, cleanup docs\n* 08 May 2022 - add a bunch of materials for 05\n* 06 May 2022 - continue making materials for 05\n* 05 May 2022 - update section 05 with headings/outline\n* 28 Apr 2022 - recorded 13 videos for 04, finished videos for 04, now to make materials for 05\n* 27 Apr 2022 - recorded 3 videos for 04\n* 26 Apr 2022 - recorded 10 videos for 04\n* 25 Apr 2022 - recorded 11 videos for 04\n* 24 Apr 2022 - prepared slides for 04\n* 23 Apr 2022 - recorded 6 videos for 03, finished videos for 03, now to 04 \n* 22 Apr 2022 - recorded 5 videos for 03\n* 21 Apr 2022 - recorded 9 videos for 03\n* 20 Apr 2022 - recorded 3 videos for 03\n* 19 Apr 2022 - recorded 11 videos for 03\n* 18 Apr 2022 - finish exercises/solutions for 04, added live-coding walkthrough of 04 exercises/solutions on YouTube: https://youtu.be/vsFMF9wqWx0\n* 16 Apr 2022 - finish exercises/solutions for 03, added live-coding walkthrough of 03 exercises/solutions on YouTube: https://youtu.be/_PibmqpEyhA\n* 14 Apr 2022 - add final images/annotations for 04, begin on exercises/solutions for 03 & 04\n* 13 Apr 2022 - add more images/annotations for 04\n* 3 Apr 2022 - add more annotations for 04\n* 2 Apr 2022 - add more annotations for 04\n* 1 Apr 2022 - add more annotations for 04\n* 31 Mar 2022 - add more annotations for 04\n* 29 Mar 2022 - add more annotations for 04\n* 27 Mar 2022 - starting to add annotations for 04\n* 26 Mar 2022 - making dataset for 04\n* 25 Mar 2022 - make slides for 03\n* 24 Mar 2022 - fix error for 03 not working in docs (finally)\n* 23 Mar 2022 - add more images for 03\n* 22 Mar 2022 - add images for 03\n* 20 Mar 2022 - add more annotations for 03\n* 18 Mar 2022 - add more annotations for 03\n* 17 Mar 2022 - add more annotations for 03 \n* 16 Mar 2022 - add more annotations for 03\n* 15 Mar 2022 - add more annotations for 03\n* 14 Mar 2022 - start adding annotations for notebook 03, see the work in progress here: https://www.learnpytorch.io/03_pytorch_computer_vision/\n* 12 Mar 2022 - recorded 12 videos for 02, finished section 02, now onto making materials for 03, 04, 05\n* 11 Mar 2022 - recorded 9 videos for 02\n* 10 Mar 2022 - recorded 10 videos for 02\n* 9 Mar 2022 - cleaning up slides/code for 02, getting ready for recording\n* 8 Mar 2022 - recorded 9 videos for section 01, finished section 01, now onto 02\n* 7 Mar 2022 - recorded 4 videos for section 01\n* 6 Mar 2022 - recorded 4 videos for section 01\n* 4 Mar 2022 - recorded 10 videos for section 01\n* 20 Feb 2022 - recorded 8 videos for section 00, finished section, now onto 01\n* 18 Feb 2022 - recorded 13 videos for section 00\n* 17 Feb 2022 - recorded 11 videos for section 00 \n* 16 Feb 2022 - added setup guide \n* 12 Feb 2022 - tidy up README with table of course materials, finish images and slides for 01\n* 10 Feb 2022 - finished slides and images for 00, notebook is ready for publishing: https://www.learnpytorch.io/00_pytorch_fundamentals/\n* 01-07 Feb 2022 - add annotations for 02, finished, still need images, going to work on exercises/solutions today \n* 31 Jan 2022 - start adding annotations for 02\n* 28 Jan 2022 - add exercies and solutions for 01\n* 26 Jan 2022 - lots more annotations to 01, should be finished tomorrow, will do exercises + solutions then too\n* 24 Jan 2022 - add a bunch of annotations to 01\n* 21 Jan 2022 - start adding annotations for 01 \n* 20 Jan 2022 - finish annotations for 00 (still need to add images), add exercises and solutions for 00\n* 19 Jan 2022 - add more annotations for 00\n* 18 Jan 2022 - add more annotations for 00\n* 17 Jan 2022 - back from holidays, adding more annotations to 00 \n* 10 Dec 2021 - start adding annotations for 00\n* 9 Dec 2021 - Created a website for the course ([learnpytorch.io](https://learnpytorch.io)) you''ll see updates posted there as development continues \n* 8 Dec 2021 - Clean up notebook 07, starting to go back through code and add annotations\n* 26 Nov 2021 - Finish skeleton code for 07, added four different experiments, need to clean up and make more straightforward\n* 25 Nov 2021 - clean code for 06, add skeleton code for 07 (experiment tracking)\n* 24 Nov 2021 - Update 04, 05, 06 notebooks for easier digestion and learning, each section should cover a max of 3 big ideas, 05 is now dedicated to turning notebook code into modular code \n* 22 Nov 2021 - Update 04 train and test functions to make more straightforward\n* 19 Nov 2021 - Added 05 (transfer learning) notebook, update custom data loading code in 04\n* 18 Nov 2021 - Updated vision code for 03 and added custom dataset loading code in 04\n* 12 Nov 2021 - Added a bunch of skeleton code to notebook 04 for custom dataset loading, next is modelling with custom data\n* 10 Nov 2021 - researching best practice for custom datasets for 04\n* 9 Nov 2021 - Update 03 skeleton code to finish off building CNN model, onto 04 for loading custom datasets\n* 4 Nov 2021 - Add GPU code to 03 + train/test loops + `helper_functions.py`\n* 3 Nov 2021 - Add basic start for 03, going to finish by end of week\n* 29 Oct 2021 - Tidied up skeleton code for 02, still a few more things to clean/tidy, created 03\n* 28 Oct 2021 - Finished skeleton code for 02, going to clean/tidy tomorrow, 03 next week\n* 27 Oct 2021 - add a bunch of code for 02, going to finish tomorrow/by end of week\n* 26 Oct 2021 - update 00, 01, 02 with outline/code, skeleton code for 00 & 01 done, 02 next\n* 23, 24 Oct 2021 - update 00 and 01 notebooks with more outline/code\n* 20 Oct 2021 - add v0 outlines for 01 and 02, add rough outline of course to README, this course will focus on less but better \n* 19 Oct 2021 - Start repo ğŸ”¥, add fundamentals notebook draft v0\n', '{"language":"Jupyter Notebook","stars":16454,"forks":4529,"watchers":16454,"open_issues":140,"topics":["deep-learning","machine-learning","pytorch"],"default_branch":"main","size_kb":626988,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning#course-materialsoutline"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning#about-this-course"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning#status"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning#log"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:users:mrdbourke","source_url":"https://github.com/users/mrdbourke"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning#log"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"}]', NULL, 'MIT', 'approved', 80, '9e0372c1dfe3172f374e820b44b070f4', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-mrdbourke-pytorch-deep-learning from https://github.com/mrdbourke.png
Image converted to WebP: data/images/github-mrdbourke-pytorch-deep-learning.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-instillai-TensorFlow-Course', 'github--instillai--tensorflow-course', 'TensorFlow-Course', 'instillai', '******************** _ ******************** .. image:: https://travis-ci.org/instillai/TensorFlow-Course.svg?branch=master :target: https://travis-ci.org/instillai/TensorFlow-Course .. image:: https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat :target: https://github.com/open-source-for-science/TensorFlow-Course/pulls .. image:: https://img.shields.io/twitter/follow/machinemindset.svg?label=Follow&style=social :target: https://twitter.com/machinemindset .. image:: h...', '["deep-learning","deep-learning-tutorial","python","tensorflow","jupyter notebook"]', 'other', 16345, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/instillai/TensorFlow-Course","fetched_at":"2025-12-08T10:30:37.949Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '\n\n********************\n`TensorFlow Course`_\n********************\n.. image:: https://travis-ci.org/instillai/TensorFlow-Course.svg?branch=master\n    :target: https://travis-ci.org/instillai/TensorFlow-Course\n.. image:: https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat\n    :target: https://github.com/open-source-for-science/TensorFlow-Course/pulls\n.. image:: https://img.shields.io/twitter/follow/machinemindset.svg?label=Follow&style=social\n    :target: https://twitter.com/machinemindset\n.. image:: https://zenodo.org/badge/151300862.svg\n   :target: https://zenodo.org/badge/latestdoi/151300862\n\n\nThis repository aims to provide simple and ready-to-use tutorials for TensorFlow.\nEach tutorial includes ``source code`` and most of them are associated with a ``documentation``.\n\n.. .. image:: _img/mainpage/TensorFlow_World.gif\n\n.. The links.\n.. _TensorFlow: https://www.tensorflow.org/install/\n.. _Wikipedia: https://en.wikipedia.org/wiki/TensorFlow/\n\n\n##########################################################################\nSponsorship\n##########################################################################\n\nTo support maintaining and upgrading this project, please kindly consider `Sponsoring the project developer <https://github.com/sponsors/astorfi/dashboard>`_.\n\nAny level of support is a great contribution here :heart:\n\n**Status:** *This project has been updated to **TensorFlow 2.3**.*\n\n\n#################\nTable of Contents\n#################\n.. contents::\n  :local:\n  :depth: 3\n\n\n==========================================\nDownload Free TensorFlow Roadmap EBook\n==========================================\n\n.. raw:: html\n\n   <div align="center">\n\n.. raw:: html\n\n <a href="http://www.machinelearningmindset.com/tensorflow-roadmap-ebook/" target="_blank">\n  <img width="710" height="500" align="center" src="https://github.com/machinelearningmindset/TensorFlow-Course/blob/master/_img/mainpage/booksubscribe.png"/>\n </a>\n\n.. raw:: html\n\n   </div>\n\n==========================================\nSlack Group\n==========================================\n\n.. raw:: html\n\n   <div align="center">\n\n.. raw:: html\n\n <a href="https://www.machinelearningmindset.com/slack-group/" target="_blank">\n  <img width="1033" height="350" align="center" src="https://github.com/machinelearningmindset/TensorFlow-Course/blob/master/_img/0-welcome/joinslack.png"/>\n </a>\n\n.. raw:: html\n\n   </div>\n\n\n\n~~~~~~~~~~~~~~~~~~~~~\nWhat is TensorFlow?\n~~~~~~~~~~~~~~~~~~~~~\nTensorFlow is an open-source software library for dataflow programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks. It is used for both research and production at Google often replacing its closed-source predecessor, DistBelief.\n\nTensorFlow was developed by the Google Brain team for internal Google use. It was released under the Apache 2.0 open source license on November 9, 2015.\n\n\n============\nMotivation\n============\n\nThere are different motivations for this open source project. TensorFlow (as we write this document) is one of / the best deep learning frameworks available. The question that should be asked is why has this repository been created when there are so many other tutorials about TensorFlow available on the web?\n\n~~~~~~~~~~~~~~~~~~~~~\nWhy use TensorFlow?\n~~~~~~~~~~~~~~~~~~~~~\n\nDeep Learning is in very high interest these days - there''s a crucial need for rapid and optimized implementations of the algorithms and architectures. TensorFlow is designed to facilitate this goal.\n\nThe strong advantage of TensorFlow is it flexibility in designing highly modular models which can also be a disadvantage for beginners since a lot of the pieces must be considered together when creating the model.\n\nThis issue has been facilitated as well by developing high-level APIs such as `Keras <https://keras.io/>`_ and `Slim <https://github.com/tensorflow/models/blob/031a5a4ab41170d555bc3e8f8545cf9c8e3f1b28/research/inception/inception/slim/README.md>`_ which abstract a lot of the pieces used in designing machine learning algorithms.\n\nThe interesting thing about TensorFlow is that **it can be found anywhere these days**. Lots of the researchers and developers are using it and *its community is growing at the speed of light*! So many issues can be dealt with easily since they''re usually the same issues that a lot of other people run into considering the large number of people involved in the TensorFlow community.\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nWhat''s the point of this repository?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n**Developing open source projects for the sake of just developing something is not the reason behind this effort**.\nConsidering the large number of tutorials that are being added to this large community, this repository has been created to break the jump-in and jump-out process that usually happens to most of the open source projects, **but why and how**?\n\nFirst of all, what''s the point of putting effort into something that most of the people won''t stop by and take a look? What''s the point of creating something that does not help anyone in the developers and researchers community? Why spend time for something that can easily be forgotten? But **how we try to do it?** Even up to this\nvery moment there are countless tutorials on TensorFlow whether on the model design or TensorFlow\nworkflow.\n\nMost of them are too complicated or suffer from a lack of documentation. There are only a few available tutorials which are concise and well-structured and provide enough insight for their specific implemented models.\n\nThe goal of this project is to help the community with structured tutorials and simple and optimized code implementations to provide better insight about how to use TensorFlow *quick and effectively*.\n\nIt is worth noting that, **the main goal of this project is to provide well-documented tutorials and less-complicated code**!\n\n=================================================\nTensorFlow Installation and Setup the Environment\n=================================================\n\n\n.. image:: _img/mainpage/installation-logo.gif\n   :height: 100px\n   :width: 200 px\n   :scale: 50 %\n   :alt: alternate text\n   :align: right\n   :target: docs/tutorials/installation\n\n.. _TensorFlow Installation: https://www.tensorflow.org/install\n\nIn order to install TensorFlow please refer to the following link:\n\n  * `TensorFlow Installation`_\n\n\n.. image:: _img/mainpage/installation.gif\n    :target: https://www.tensorflow.org/install\n\nThe virtual environment installation is recommended in order to prevent package conflict and having the capacity to customize the working environment.\n\n====================\nTensorFlow Tutorials\n====================\n\nThe tutorials in this repository are partitioned into relevant categories.\n\n==========================\n\n~~~~~~~~\nWarm-up\n~~~~~~~~\n\n.. image:: _img/mainpage/welcome.gif\n   :height: 100px\n   :width: 200 px\n   :scale: 50 %\n   :alt: alternate text\n   :align: right\n\n\n.. _colab: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/0-welcome/welcome.ipynb\n.. _Documentationcnnwelcome: docs/tutorials/0-welcome\n.. _ipythonwelcome: codes/ipython/0-welcome/welcome.ipynb\n.. _pythonwelcome: https://github.com/instillai/TensorFlow-Course/blob/master/codes/python/0-welcome/welcome.py\n.. _videowelcome: https://youtu.be/xd0DVygHlNE\n\n\n.. |Welcome| image:: https://colab.research.google.com/assets/colab-badge.svg\n   :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/0-welcome/welcome.ipynb\n\n.. |youtubeim| image:: _img/mainpage/YouTube.png\n  :target: https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/YouTube.png\n\n\n+----+---------------------+--------------------------+------------------------------------------------------------------------+-------------------------------------------+\n| #  |       topic         |          Run             |  Source Code                                                           |  Media                                    |\n+====+=====================+==========================+========================================================================+===========================================+\n| 1  | Start-up            |       |Welcome|          | `Notebook <ipythonwelcome_>`_  / `Python <pythonwelcome_>`_            | `Video Tutorial <videowelcome_>`_         |\n+----+---------------------+--------------------------+------------------------------------------------------------------------+-------------------------------------------+\n\n==========================\n\n~~~~~~\nBasics\n~~~~~~\n\n.. raw:: html\n\n   <div align="left">\n\n.. raw:: html\n\n <a href="https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/basics.gif" target="_blank">\n  <img width="250" height="250" align="center" src="https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/basics.gif"/>\n </a>\n\n.. raw:: html\n\n   </div>\n\n.. raw:: html\n\n   <br>\n\n\n\n.. _ipythontensors: codes/ipython/1-basics/tensors.ipynb\n.. _pythontensors: codes/python/1-basics/tensors.py\n.. _videotensors: https://youtu.be/Od-VvnYUbFw\n.. |Tensors| image:: https://colab.research.google.com/assets/colab-badge.svg\n  :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/1-basics/tensors.ipynb\n\n.. _ipythonad: codes/ipython/1-basics/automatic_differentiation.ipynb\n.. _pythonad: codes/python/1-basics/automatic_differentiation.py\n.. _videoad: https://youtu.be/l-MGydWW-UE\n.. |AD| image:: https://colab.research.google.com/assets/colab-badge.svg\n  :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/1-basics/automatic_differentiation.ipynb\n\n.. _ipythongraphs: codes/ipython/1-basics/graph.ipynb\n.. _pythongraphs: codes/python/1-basics/graph.py\n.. _videographs: https://youtu.be/P9xA1s6AUNk\n.. |graphs| image:: https://colab.research.google.com/assets/colab-badge.svg\n  :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/1-basics/graph.ipynb\n\n\n.. _ipythonmodels: codes/ipython/1-basics/models.ipynb\n.. _pythonmodels: codes/python/1-basics/models.py\n.. _videomodels: https://youtu.be/WnlUE04REOY\n.. |models| image:: https://colab.research.google.com/assets/colab-badge.svg\n  :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/1-basics/models.ipynb\n\n\n\n+----+-----------------------------------+--------------------------+------------------------------------------------------------------------+-----------------------------------------+\n| #  |       topic                       |          Run             |  Source Code                                                           |        Media                            |\n+====+===================================+==========================+========================================================================+=========================================+\n| 1  | Tensors                           |       |Tensors|          | `Notebook <ipythontensors_>`_  / `Python <pythontensors_>`_            | `Video Tutorial <videotensors_>`_       |\n+----+-----------------------------------+--------------------------+------------------------------------------------------------------------+-----------------------------------------+\n| 2  | Automatic Differentiation         |       |AD|               | `Notebook <ipythonad_>`_  / `Python <pythonad_>`_                      | `Video Tutorial <videoad_>`_            |\n+----+-----------------------------------+--------------------------+------------------------------------------------------------------------+-----------------------------------------+\n| 3  | Introduction to Graphs            |       |graphs|           | `Notebook <ipythongraphs_>`_ / `Python <pythongraphs_>`_               | `Video Tutorial <videographs_>`_        |\n+----+-----------------------------------+--------------------------+------------------------------------------------------------------------+-----------------------------------------+\n| 4  | TensorFlow Models                 |       |models|           | `Notebook <ipythonmodels_>`_  / `Python <pythonmodels_>`_              | `Video Tutorial <videomodels_>`_        |\n+----+-----------------------------------+--------------------------+------------------------------------------------------------------------+-----------------------------------------+\n\n==========================\n\n~~~~~~~~~~~~~~~~~~~~~~\nBasic Machine Learning\n~~~~~~~~~~~~~~~~~~~~~~\n\n.. raw:: html\n\n   <div align="left">\n\n.. raw:: html\n\n <a href="https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/basicmodels.gif" target="_blank">\n  <img width="250" height="250" align="center" src="https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/basicmodels.gif"/>\n </a>\n\n.. raw:: html\n\n   </div>\n\n.. raw:: html\n\n   <br>\n\n.. .. image:: _img/mainpage/basicmodels.gif\n..    :height: 100px\n..    :width: 200 px\n..    :scale: 50 %\n..    :alt: alternate text\n..    :align: right\n\n\n.. _ipythonlinearreg: codes/ipython/basics_in_machine_learning/linearregression.ipynb\n.. _pythonlinearreg: codes/python/basics_in_machine_learning/linearregression.py\n.. _tutoriallinearreg: https://www.machinelearningmindset.com/linear-regression-with-tensorflow/\n.. _videoinearreg: https://youtu.be/2RTBBiKKuLI\n\n.. _tutorialdataaugmentation: https://www.machinelearningmindset.com/data-augmentation-with-tensorflow/\n.. _ipythondataaugmentation: https://github.com/instillai/TensorFlow-Course/blob/master/codes/ipython/basics_in_machine_learning/dataaugmentation.ipynb\n.. _pythondataaugmentation: https://github.com/instillai/TensorFlow-Course/blob/master/codes/python/basics_in_machine_learning/dataaugmentation.py\n.. _videodataaugmentation: https://youtu.be/HbzR2snHJF0\n\n.. |lr| image:: https://colab.research.google.com/assets/colab-badge.svg\n  :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/basics_in_machine_learning/linearregression.ipynb\n.. |da| image:: https://colab.research.google.com/assets/colab-badge.svg\n  :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/basics_in_machine_learning/dataaugmentation.ipynb\n\n\n+----+-----------------------------------+--------------------------+------------------------------------------------------------------------------------+----------------------------------------------+----------------------------------------------+\n| #  |       topic                       |          Run             |  Source Code                                                                       |  More                                        |           Media                              |\n+====+===================================+==========================+====================================================================================+==============================================+==============================================+\n| 1  | Linear Regression                 |       |lr|               | `Notebook <ipythonlinearreg_>`_  / `Python <pythonlinearreg_>`_                    | `Tutorial <tutoriallinearreg_>`_             | `Video Tutorial <videoinearreg_>`_           |\n+----+-----------------------------------+--------------------------+------------------------------------------------------------------------------------+----------------------------------------------+----------------------------------------------+\n| 2  | Data Augmentation                 |       |da|               | `Notebook <ipythondataaugmentation_>`_ / `Python <pythondataaugmentation_>`_       | `Tutorial <tutorialdataaugmentation_>`_      | `Video Tutorial <videodataaugmentation_>`_   |\n+----+-----------------------------------+--------------------------+------------------------------------------------------------------------------------+----------------------------------------------+----------------------------------------------+\n\n\n\n.. +----+----------------------------+----------------------------------------------------------------------------------------+----------------------------------------------+\n\n==========================\n\n~~~~~~~~~~~~~~~~\nNeural Networks\n~~~~~~~~~~~~~~~~\n\n.. raw:: html\n\n   <div align="left">\n\n.. raw:: html\n\n <a href="https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/CNNs.png" target="_blank">\n  <img width="600" height="180" align="center" src="https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/CNNs.png"/>\n </a>\n\n.. raw:: html\n\n   </div>\n\n.. raw:: html\n\n    <br>\n\n\n.. _ipythonmlp: https://github.com/instillai/TensorFlow-Course/blob/master/codes/ipython/neural_networks/mlp.ipynb\n.. _pythonmlp: https://github.com/instillai/TensorFlow-Course/blob/master/codes/python/neural_networks/mlp.py\n.. _videomlp: https://youtu.be/w20efZqSK2Y\n\n.. _ipythoncnn: https://github.com/instillai/TensorFlow-Course/blob/master/codes/ipython/neural_networks/CNNs.ipynb\n.. _pythoncnn: https://github.com/instillai/TensorFlow-Course/blob/master/codes/python/neural_networks/cnns.py\n.. _videocnn: https://youtu.be/WVifZBCRz8g\n\n\n.. |mlp| image:: https://colab.research.google.com/assets/colab-badge.svg\n :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/neural_networks/mlp.ipynb\n.. |cnn| image:: https://colab.research.google.com/assets/colab-badge.svg\n :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/neural_networks/CNNs.ipynb\n\n\n+----+------------------------------------------+--------------------------+------------------------------------------------------+------------------------------------+\n| #  |       topic                              |          Run             |  Source Code                                         |            Media                   |\n+====+==========================================+==========================+======================================================+====================================+\n| 1  |  *Multi Layer Perceptron*                |       |mlp|              | `Notebook <ipythonmlp_>`_ / `Python <pythonmlp_>`_   | `Video Tutorial <videomlp_>`_      |\n+----+------------------------------------------+--------------------------+------------------------------------------------------+------------------------------------+\n| 2  |  *Convolutional Neural Networks*         |       |cnn|              | `Notebook <ipythoncnn_>`_ / `Python <pythoncnn_>`_   | `Video Tutorial <videocnn_>`_      |\n+----+------------------------------------------+--------------------------+------------------------------------------------------+------------------------------------+\n\n==========================\n\n~~~~~~~~~~~~~~~~\nAdvanced\n~~~~~~~~~~~~~~~~\n\n\n.. raw:: html\n\n   <div align="left">\n\n.. raw:: html\n\n <a href="https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/Build.png" target="_blank">\n  <img width="180" height="180" align="center" src="https://github.com/instillai/TensorFlow-Course/blob/master/_img/mainpage/Build.png"/>\n </a>\n\n.. raw:: html\n\n   </div>\n\n.. raw:: html\n\n    <br>\n\n\n\n\n.. _ipythoncustomtr: https://github.com/instillai/TensorFlow-Course/blob/master/codes/ipython/advanced/custom_training.ipynb\n.. _pythoncustomtr: https://github.com/instillai/TensorFlow-Course/blob/master/codes/python/advanced/custom_training.py\n.. _videocustomtr: https://youtu.be/z5gcabfyPfA\n\n.. _ipythondgenerator: https://github.com/instillai/TensorFlow-Course/blob/master/codes/ipython/advanced/dataset_generator.ipynb\n.. _pythondgenerator: https://github.com/instillai/TensorFlow-Course/blob/master/codes/python/advanced/dataset_generator.py\n.. _videodgenerator: https://youtu.be/-YsgMdDPu3g\n\n.. _ipythontfrecords: https://github.com/instillai/TensorFlow-Course/blob/master/codes/ipython/advanced/tfrecords.ipynb\n.. _pythontfrecords: https://github.com/instillai/TensorFlow-Course/blob/master/codes/python/advanced/tfrecords.py\n.. _videotfrecords: https://youtu.be/zqavy_5QMk8\n\n\n.. |ctraining| image:: https://colab.research.google.com/assets/colab-badge.svg\n :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/advanced/custom_training.ipynb\n\n.. |dgenerator| image:: https://colab.research.google.com/assets/colab-badge.svg\n  :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/advanced/dataset_generator.ipynb\n\n.. |tfrecords| image:: https://colab.research.google.com/assets/colab-badge.svg\n  :target: https://colab.research.google.com/github/instillai/TensorFlow-Course/blob/master/codes/ipython/advanced/tfrecords.ipynb\n\n\n+----+------------------------------------------+--------------------------+--------------------------------------------------------------------+----------------------------------------+\n| #  |       topic                              |          Run             |  Source Code                                                       |           Media                        |\n+====+==========================================+==========================+====================================================================+========================================+\n| 1  |  *Custom Training*                       |       |ctraining|        | `Notebook <ipythoncustomtr_>`_ / `Python <pythoncustomtr_>`_       | `Video Tutorial <videocustomtr_>`_     |\n+----+------------------------------------------+--------------------------+--------------------------------------------------------------------+----------------------------------------+\n| 2  |  *Dataset Generator*                     |       |dgenerator|       | `Notebook <ipythondgenerator_>`_ / `Python <pythondgenerator_>`_   | `Video Tutorial <videodgenerator_>`_   |\n+----+------------------------------------------+--------------------------+--------------------------------------------------------------------+----------------------------------------+\n| 3  |  *Create TFRecords*                      |       |tfrecords|        | `Notebook <ipythontfrecords_>`_ / `Python <pythontfrecords_>`_     | `Video Tutorial <videotfrecords_>`_    |\n+----+------------------------------------------+--------------------------+--------------------------------------------------------------------+----------------------------------------+\n\n\n\n=====================\nSome Useful Tutorials\n=====================\n\n  * `TensorFlow Examples <https://github.com/aymericdamien/TensorFlow-Examples>`_ - TensorFlow tutorials and code examples for beginners\n  * `Sungjoon''s TensorFlow-101 <https://github.com/sjchoi86/Tensorflow-101>`_ - TensorFlow tutorials written in Python with Jupyter Notebook\n  * `Terry Umâ€™s TensorFlow Exercises <https://github.com/terryum/TensorFlow_Exercises>`_ - Re-create the codes from other TensorFlow examples\n  * `Classification on time series <https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition>`_ - Recurrent Neural Network classification in TensorFlow with LSTM on cellphone sensor data\n\n\n=============\nContributing\n=============\n\nWhen contributing to this repository, please first discuss the change you wish to make via issue,\nemail, or any other method with the owners of this repository before making a change. *For typos, please\ndo not create a pull request. Instead, declare them in issues or email the repository owner*.\n\nPlease note we have a code of conduct, please follow it in all your interactions with the project.\n\n~~~~~~~~~~~~~~~~~~~~\nPull Request Process\n~~~~~~~~~~~~~~~~~~~~\n\nPlease consider the following criterions in order to help us in a better way:\n\n  * The pull request is mainly expected to be a code script suggestion or improvement.\n  * Please do NOT change the ipython files. Instead, change the corresponsing PYTHON files.\n  * A pull request related to non-code-script sections is expected to make a significant difference in the documentation. Otherwise, it is expected to be announced in the issues section.\n  * Ensure any install or build dependencies are removed before the end of the layer when doing a build and creating a pull request.\n  * Add comments with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and container parameters.\n  * You may merge the Pull Request in once you have the sign-off of at least one other developer, or if you do not have permission to do that, you may request the owner to merge it for you if you believe all checks are passed.\n\n\n~~~~~~~~~~~\nFinal Note\n~~~~~~~~~~~\n\nWe are looking forward to your kind feedback. Please help us to improve this open source project and make our work better.\nFor contribution, please create a pull request and we will investigate it promptly. Once again, we appreciate\nyour kind feedback and elaborate code inspections.\n\n========================\nDevelopers\n========================\n\n\n**Company**: Instill AI [`Website\n<https://instillai.com/>`_]\n\n**Creator**: Machine Learning Mindset [`Blog\n<https://machinelearningmindset.com/blog/>`_, `GitHub\n<https://github.com/machinelearningmindset>`_, `Twitter\n<https://twitter.com/machinemindset>`_]\n\n**Developer**: Amirsina Torfi [`GitHub\n<https://github.com/astorfi>`_, `Personal Website\n<https://astorfi.github.io/>`_, `Linkedin\n<https://www.linkedin.com/in/amirsinatorfi/>`_ ]\n', '{"language":"Jupyter Notebook","stars":16345,"forks":3173,"watchers":16345,"open_issues":2,"topics":["deep-learning","deep-learning-tutorial","python","tensorflow"],"default_branch":"master","size_kb":21714,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:open-source-for-science:TensorFlow-Course","source_url":"https://github.com/open-source-for-science/TensorFlow-Course"},{"type":"has_code","target_id":"github:sponsors:astorfi","source_url":"https://github.com/sponsors/astorfi"},{"type":"has_code","target_id":"github:machinelearningmindset:TensorFlow-Course","source_url":"https://github.com/machinelearningmindset/TensorFlow-Course"},{"type":"has_code","target_id":"github:machinelearningmindset:TensorFlow-Course","source_url":"https://github.com/machinelearningmindset/TensorFlow-Course"},{"type":"has_code","target_id":"github:tensorflow:models","source_url":"https://github.com/tensorflow/models"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:instillai:TensorFlow-Course","source_url":"https://github.com/instillai/TensorFlow-Course"},{"type":"has_code","target_id":"github:aymericdamien:TensorFlow-Examples>`_","source_url":"https://github.com/aymericdamien/TensorFlow-Examples>`_"},{"type":"has_code","target_id":"github:sjchoi86:Tensorflow-101>`_","source_url":"https://github.com/sjchoi86/Tensorflow-101>`_"},{"type":"has_code","target_id":"github:terryum:TensorFlow_Exercises>`_","source_url":"https://github.com/terryum/TensorFlow_Exercises>`_"},{"type":"has_code","target_id":"github:guillaume-chevalier:LSTM-Human-Activity-Recognition>`_","source_url":"https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition>`_"}]', NULL, 'MIT', 'approved', 80, '5a06479f67d9e266ed030ab9458c39de', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-instillai-TensorFlow-Course from https://github.com/instillai.png
Image converted to WebP: data/images/github-instillai-TensorFlow-Course.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-bharathgs-Awesome-pytorch-list', 'github--bharathgs--awesome-pytorch-list', 'Awesome-pytorch-list', 'bharathgs', 'Awesome-Pytorch-list ======================== !pytorch-logo-dark <p align="center"> <img src="https://img.shields.io/badge/stars-12400+-brightgreen.svg?style=flat"/> <img src="https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat"> </p> - Pytorch & related libraries - NLP & Speech Processing - Computer Vision - Probabilistic/Generative Libraries - Other libraries - Tutorials, books & examples - Paper implementations - Talks & Conferences - Pytorch elsewhere 1. pytorch:...', '["awesome","awesome-list","computer-vision","cv","data-science","deep-learning","facebook","machine-learning","natural-language-processing","neural-network","nlp","nlp-library","papers","probabilistic-programming","python","pytorch","pytorch-model","pytorch-tutorials","tutorials","utility-library"]', 'other', 16284, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/bharathgs/Awesome-pytorch-list","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', 'Awesome-Pytorch-list\n========================\n\n![pytorch-logo-dark](https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/pytorch-logo-dark.png)\n\n<p align="center">\n	<img src="https://img.shields.io/badge/stars-12400+-brightgreen.svg?style=flat"/>\n	<img src="https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat">\n</p>\n\n## Contents\n- [Pytorch & related libraries](#pytorch--related-libraries)\n  - [NLP & Speech Processing](#nlp--Speech-Processing)\n  - [Computer Vision](#cv)\n  - [Probabilistic/Generative Libraries](#probabilisticgenerative-libraries)\n  - [Other libraries](#other-libraries)\n- [Tutorials, books & examples](#tutorials-books--examples)\n- [Paper implementations](#paper-implementations)\n- [Talks & Conferences](#talks--conferences)\n- [Pytorch elsewhere](#pytorch-elsewhere)\n\n## Pytorch & related libraries\n\n1. [pytorch](http://pytorch.org): Tensors and Dynamic neural networks in Python with strong GPU acceleration.\n2. [Captum](https://github.com/pytorch/captum): Model interpretability and understanding for PyTorch.\n\n### NLP & Speech Processing:\n\n1. [pytorch text](https://github.com/pytorch/text): Torch text related contents.  \n2. [pytorch-seq2seq](https://github.com/IBM/pytorch-seq2seq): A framework for sequence-to-sequence (seq2seq) models implemented in PyTorch.  \n3. [anuvada](https://github.com/Sandeep42/anuvada): Interpretable Models for NLP using PyTorch.\n4. [audio](https://github.com/pytorch/audio): simple audio I/O for pytorch.\n5. [loop](https://github.com/facebookresearch/loop): A method to generate speech across multiple speakers\n6. [fairseq-py](https://github.com/facebookresearch/fairseq-py): Facebook AI Research Sequence-to-Sequence Toolkit written in Python.\n7. [speech](https://github.com/awni/speech): PyTorch ASR Implementation.\n8. [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py): Open-Source Neural Machine Translation in PyTorch http://opennmt.net \n9. [neuralcoref](https://github.com/huggingface/neuralcoref): State-of-the-art coreference resolution based on neural nets and spaCy huggingface.co/coref\n10. [sentiment-discovery](https://github.com/NVIDIA/sentiment-discovery): Unsupervised Language Modeling at scale for robust sentiment classification.\n11. [MUSE](https://github.com/facebookresearch/MUSE): A library for Multilingual Unsupervised or Supervised word Embeddings\n12. [nmtpytorch](https://github.com/lium-lst/nmtpytorch): Neural Machine Translation Framework in PyTorch.\n13. [pytorch-wavenet](https://github.com/vincentherrmann/pytorch-wavenet): An implementation of WaveNet with fast generation\n14. [Tacotron-pytorch](https://github.com/soobinseo/Tacotron-pytorch): Tacotron: Towards End-to-End Speech Synthesis.\n15. [AllenNLP](https://github.com/allenai/allennlp): An open-source NLP research library, built on PyTorch.\n16. [PyTorch-NLP](https://github.com/PetrochukM/PyTorch-NLP): Text utilities and datasets for PyTorch pytorchnlp.readthedocs.io\n17. [quick-nlp](https://github.com/outcastofmusic/quick-nlp): Pytorch NLP library based on FastAI. \n18. [TTS](https://github.com/mozilla/TTS): Deep learning for Text2Speech\n19. [LASER](https://github.com/facebookresearch/LASER): Language-Agnostic SEntence Representations\n20. [pyannote-audio](https://github.com/pyannote/pyannote-audio): Neural building blocks for speaker diarization: speech activity detection, speaker change detection, speaker embedding\n21. [gensen](https://github.com/Maluuba/gensen): Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning.\n22. [translate](https://github.com/pytorch/translate): Translate - a PyTorch Language Library.\n23. [espnet](https://github.com/espnet/espnet): End-to-End Speech Processing Toolkit espnet.github.io/espnet\n24. [pythia](https://github.com/facebookresearch/pythia): A software suite for Visual Question Answering\n25. [UnsupervisedMT](https://github.com/facebookresearch/UnsupervisedMT): Phrase-Based & Neural Unsupervised Machine Translation.\n26. [jiant](https://github.com/jsalt18-sentence-repl/jiant): The jiant sentence representation learning toolkit. \n27. [BERT-PyTorch](https://github.com/codertimo/BERT-pytorch): Pytorch implementation of Google AI''s 2018 BERT, with simple annotation\n28. [InferSent](https://github.com/facebookresearch/InferSent): Sentence embeddings (InferSent) and training code for NLI.\n29. [uis-rnn](https://github.com/google/uis-rnn):This is the library for the Unbounded Interleaved-State Recurrent Neural Network (UIS-RNN) algorithm, corresponding to the paper Fully Supervised Speaker Diarization. arxiv.org/abs/1810.04719 \n30. [flair](https://github.com/zalandoresearch/flair): A very simple framework for state-of-the-art Natural Language Processing (NLP)\n31. [pytext](https://github.com/facebookresearch/pytext): A natural language modeling framework based on PyTorch fb.me/pytextdocs\n32. [voicefilter](https://github.com/mindslab-ai/voicefilter): Unofficial PyTorch implementation of Google AI''s VoiceFilter system http://swpark.me/voicefilter\n33. [BERT-NER](https://github.com/kamalkraj/BERT-NER): Pytorch-Named-Entity-Recognition-with-BERT. \n34. [transfer-nlp](https://github.com/feedly/transfer-nlp): NLP library designed for flexible research and development\n35. [texar-pytorch](https://github.com/asyml/texar-pytorch): Toolkit for Machine Learning and Text Generation, in PyTorch texar.io\n36. [pytorch-kaldi](https://github.com/mravanelli/pytorch-kaldi): pytorch-kaldi is a project for developing state-of-the-art DNN/RNN hybrid speech recognition systems. The DNN part is managed by pytorch, while feature extraction, label computation, and decoding are performed with the kaldi toolkit.\n37. [NeMo](https://github.com/NVIDIA/NeMo): Neural Modules: a toolkit for conversational AI nvidia.github.io/NeMo\n38. [pytorch-struct](https://github.com/harvardnlp/pytorch-struct): A library of vectorized implementations of core structured prediction algorithms (HMM, Dep Trees, CKY, ..,)\n39. [espresso](https://github.com/freewym/espresso): Espresso: A Fast End-to-End Neural Speech Recognition Toolkit\n40. [transformers](https://github.com/huggingface/transformers): huggingface Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch. huggingface.co/transformers\n41. [reformer-pytorch](https://github.com/lucidrains/reformer-pytorch): Reformer, the efficient Transformer, in Pytorch\n42. [torch-metrics](https://github.com/enochkan/torch-metrics): Metrics for model evaluation in pytorch\n43. [speechbrain](https://github.com/speechbrain/speechbrain): SpeechBrain is an open-source and all-in-one speech toolkit based on PyTorch.\n44. [Backprop](https://github.com/backprop-ai/backprop): Backprop makes it simple to use, finetune, and deploy state-of-the-art ML models.\n\n### CV:\n\n1. [pytorch vision](https://github.com/pytorch/vision): Datasets, Transforms and Models specific to Computer Vision.\n2. [pt-styletransfer](https://github.com/tymokvo/pt-styletransfer): Neural style transfer as a class in PyTorch.\n3. [OpenFacePytorch](https://github.com/thnkim/OpenFacePytorch):  PyTorch module to use OpenFace''s nn4.small2.v1.t7 model\n4. [img_classification_pk_pytorch](https://github.com/felixgwu/img_classification_pk_pytorch): Quickly comparing your image classification models with the state-of-the-art models (such as DenseNet, ResNet, ...)\n5. [SparseConvNet](https://github.com/facebookresearch/SparseConvNet): Submanifold sparse convolutional networks.\n6. [Convolution_LSTM_pytorch](https://github.com/automan000/Convolution_LSTM_pytorch): A multi-layer convolution LSTM module\n7. [face-alignment](https://github.com/1adrianb/face-alignment): :fire: 2D and 3D Face alignment library build using pytorch adrianbulat.com\n8. [pytorch-semantic-segmentation](https://github.com/ZijunDeng/pytorch-semantic-segmentation): PyTorch for Semantic Segmentation.\n9. [RoIAlign.pytorch](https://github.com/longcw/RoIAlign.pytorch): This is a PyTorch version of RoIAlign. This implementation is based on crop_and_resize and supports both forward and backward on CPU and GPU.\n10. [pytorch-cnn-finetune](https://github.com/creafz/pytorch-cnn-finetune): Fine-tune pretrained Convolutional Neural Networks with PyTorch.\n11. [detectorch](https://github.com/ignacio-rocco/detectorch): Detectorch - detectron for PyTorch\n12. [Augmentor](https://github.com/mdbloice/Augmentor): Image augmentation library in Python for machine learning. http://augmentor.readthedocs.io\n13. [s2cnn](https://github.com/jonas-koehler/s2cnn): \nThis library contains a PyTorch implementation of the SO(3) equivariant CNNs for spherical signals (e.g. omnidirectional cameras, signals on the globe)\n14. [TorchCV](https://github.com/donnyyou/torchcv): A PyTorch-Based Framework for Deep Learning in Computer Vision. \n15. [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark): Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch.\n16. [image-classification-mobile](https://github.com/osmr/imgclsmob): Collection of classification models pretrained on the ImageNet-1K.\n17. [medicaltorch](https://github.com/perone/medicaltorch): A medical imaging framework for Pytorch http://medicaltorch.readthedocs.io\n18. [albumentations](https://github.com/albu/albumentations): Fast image augmentation library.\n19. [kornia](https://github.com/arraiyopensource/kornia): Differentiable computer vision library.\n20. [pytorch-text-recognition](https://github.com/s3nh/pytorch-text-recognition): Text recognition combo - CRAFT + CRNN.\n21. [facenet-pytorch](https://github.com/timesler/facenet-pytorch): Pretrained Pytorch face detection and recognition models ported from davidsandberg/facenet.\n22. [detectron2](https://github.com/facebookresearch/detectron2): Detectron2 is FAIR''s next-generation research platform for object detection and segmentation.\n23. [vedaseg](https://github.com/Media-Smart/vedaseg): A semantic segmentation framework by pyotrch\n24. [ClassyVision](https://github.com/facebookresearch/ClassyVision): An end-to-end PyTorch framework for image and video classification.\n25. [detecto](https://github.com/alankbi/detecto):Computer vision in Python with less than 10 lines of code\n26. [pytorch3d](https://github.com/facebookresearch/pytorch3d): PyTorch3D is FAIR''s library of reusable components for deep learning with 3D data pytorch3d.org\n27. [MMDetection](https://github.com/open-mmlab/mmdetection): MMDetection is an open source object detection toolbox, a part of the [OpenMMLab project](https://open-mmlab.github.io/).\n28. [neural-dream](https://github.com/ProGamerGov/neural-dream): A PyTorch implementation of the DeepDream algorithm. Creates dream-like hallucinogenic visuals.\n29. [FlashTorch](https://github.com/MisaOgura/flashtorch): Visualization toolkit for neural networks in PyTorch!\n30. [Lucent](https://github.com/greentfrapp/lucent): Tensorflow and OpenAI Clarity''s Lucid adapted for PyTorch.\n31. [MMDetection3D](https://github.com/open-mmlab/mmdetection3d): MMDetection3D is OpenMMLab''s next-generation platform for general 3D object detection, a part of the [OpenMMLab project](https://open-mmlab.github.io/).\n32. [MMSegmentation](https://github.com/open-mmlab/mmsegmentation): MMSegmentation is a semantic segmentation toolbox and benchmark, a part of the [OpenMMLab project](https://open-mmlab.github.io/).\n33. [MMEditing](https://github.com/open-mmlab/mmediting): MMEditing is a image and video editing toolbox, a part of the [OpenMMLab project](https://open-mmlab.github.io/).\n34. [MMAction2](https://github.com/open-mmlab/mmaction2): MMAction2 is OpenMMLab''s next generation action understanding toolbox and benchmark, a part of the [OpenMMLab project](https://open-mmlab.github.io/).\n35. [MMPose](https://github.com/open-mmlab/mmpose): MMPose is a pose estimation toolbox and benchmark, a part of the [OpenMMLab project](https://open-mmlab.github.io/).\n36. [lightly](https://github.com/lightly-ai/lightly) - Lightly is a computer vision framework for self-supervised learning.\n37. [RoMa](https://naver.github.io/roma/): a lightweight and efficient library to deal with 3D rotations.\n\n\n### Probabilistic/Generative Libraries:\n\n1. [ptstat](https://github.com/stepelu/ptstat): Probabilistic Programming and Statistical Inference in PyTorch\n2. [pyro](https://github.com/uber/pyro): Deep universal probabilistic programming with Python and PyTorch http://pyro.ai\n3. [probtorch](https://github.com/probtorch/probtorch): Probabilistic Torch is library for deep generative models that extends PyTorch.\n4. [paysage](https://github.com/drckf/paysage): Unsupervised learning and generative models in python/pytorch.\n5. [pyvarinf](https://github.com/ctallec/pyvarinf): Python package facilitating the use of Bayesian Deep Learning methods with Variational Inference for PyTorch. \n6. [pyprob](https://github.com/probprog/pyprob): A PyTorch-based library for probabilistic programming and inference compilation.\n7. [mia](https://github.com/spring-epfl/mia): A library for running membership inference attacks against ML models. \n8. [pro_gan_pytorch](https://github.com/akanimax/pro_gan_pytorch): ProGAN package implemented as an extension of PyTorch nn.Module.\n9. [botorch](https://github.com/pytorch/botorch): Bayesian optimization in PyTorch\n\n### Other libraries:\n\n1. [pytorch extras](https://github.com/mrdrozdov/pytorch-extras): Some extra features for pytorch.    \n2. [functional zoo](https://github.com/szagoruyko/functional-zoo): PyTorch, unlike lua torch, has autograd in it''s core, so using modular structure of torch.nn modules is not necessary, one can easily allocate needed Variables and write a function that utilizes them, which is sometimes more convenient. This repo contains model definitions in this functional way, with pretrained weights for some models. \n3. [torch-sampling](https://github.com/ncullen93/torchsample): This package provides a set of transforms and data structures for sampling from in-memory or out-of-memory data. \n4. [torchcraft-py](https://github.com/deepcraft/torchcraft-py): Python wrapper for TorchCraft, a bridge between Torch and StarCraft for AI research.\n5. [aorun](https://github.com/ramon-oliveira/aorun): Aorun intend to be a Keras with PyTorch as backend. \n6. [logger](https://github.com/oval-group/logger): A simple logger for experiments.\n7. [PyTorch-docset](https://github.com/iamaziz/PyTorch-docset): PyTorch docset! use with Dash, Zeal, Velocity, or LovelyDocs.  \n8. [convert_torch_to_pytorch](https://github.com/clcarwin/convert_torch_to_pytorch): Convert torch t7 model to pytorch model and source.\n9. [pretrained-models.pytorch](https://github.com/Cadene/pretrained-models.pytorch): The goal of this repo is to help to reproduce research papers results.  \n10. [pytorch_fft](https://github.com/locuslab/pytorch_fft): PyTorch wrapper for FFTs\n11. [caffe_to_torch_to_pytorch](https://github.com/fanq15/caffe_to_torch_to_pytorch)\n12. [pytorch-extension](https://github.com/sniklaus/pytorch-extension): This is a CUDA extension for PyTorch which computes the Hadamard product of two tensors.\n13. [tensorboard-pytorch](https://github.com/lanpa/tensorboard-pytorch): This module saves PyTorch tensors in tensorboard format for inspection. Currently supports scalar, image, audio, histogram features in tensorboard.\n14. [gpytorch](https://github.com/jrg365/gpytorch): GPyTorch is a Gaussian Process library, implemented using PyTorch. It is designed for creating flexible and modular Gaussian Process models with ease, so that you don''t have to be an expert to use GPs.\n15. [spotlight](https://github.com/maciejkula/spotlight): Deep recommender models using PyTorch.\n16. [pytorch-cns](https://github.com/awentzonline/pytorch-cns): Compressed Network Search with PyTorch\n17. [pyinn](https://github.com/szagoruyko/pyinn): CuPy fused PyTorch neural networks ops\n18. [inferno](https://github.com/nasimrahaman/inferno): A utility library around PyTorch\n19. [pytorch-fitmodule](https://github.com/henryre/pytorch-fitmodule): Super simple fit method for PyTorch modules\n20. [inferno-sklearn](https://github.com/dnouri/inferno): A scikit-learn compatible neural network library that wraps pytorch.\n21. [pytorch-caffe-darknet-convert](https://github.com/marvis/pytorch-caffe-darknet-convert): convert between pytorch, caffe prototxt/weights and darknet cfg/weights\n22. [pytorch2caffe](https://github.com/longcw/pytorch2caffe): Convert PyTorch model to Caffemodel\n23. [pytorch-tools](https://github.com/nearai/pytorch-tools): Tools for PyTorch\n24. [sru](https://github.com/taolei87/sru): Training RNNs as Fast as CNNs (arxiv.org/abs/1709.02755)\n25. [torch2coreml](https://github.com/prisma-ai/torch2coreml): Torch7 -> CoreML\n26. [PyTorch-Encoding](https://github.com/zhanghang1989/PyTorch-Encoding): PyTorch Deep Texture Encoding Network http://hangzh.com/PyTorch-Encoding\n27. [pytorch-ctc](https://github.com/ryanleary/pytorch-ctc): PyTorch-CTC is an implementation of CTC (Connectionist Temporal Classification) beam search decoding for PyTorch. C++ code borrowed liberally from TensorFlow with some improvements to increase flexibility.\n28. [candlegp](https://github.com/t-vi/candlegp): Gaussian Processes in Pytorch. \n29. [dpwa](https://github.com/loudinthecloud/dpwa): Distributed Learning by Pair-Wise Averaging. \n30. [dni-pytorch](https://github.com/koz4k/dni-pytorch): Decoupled Neural Interfaces using Synthetic Gradients for PyTorch.\n31. [skorch](https://github.com/dnouri/skorch): A scikit-learn compatible neural network library that wraps pytorch\n32. [ignite](https://github.com/pytorch/ignite): Ignite is a high-level library to help with training neural networks in PyTorch.\n33. [Arnold](https://github.com/glample/Arnold): Arnold - DOOM Agent\n34. [pytorch-mcn](https://github.com/albanie/pytorch-mcn): Convert models from MatConvNet to PyTorch\n35. [simple-faster-rcnn-pytorch](https://github.com/chenyuntc/simple-faster-rcnn-pytorch): A simplified implemention of Faster R-CNN with competitive performance.\n36. [generative_zoo](https://github.com/DL-IT/generative_zoo): generative_zoo is a repository that provides working implementations of some generative models in PyTorch.\n37. [pytorchviz](https://github.com/szagoruyko/pytorchviz): A small package to create visualizations of PyTorch execution graphs. \n38. [cogitare](https://github.com/cogitare-ai/cogitare): Cogitare - A Modern, Fast, and Modular Deep Learning and Machine Learning framework in Python. \n39. [pydlt](https://github.com/dmarnerides/pydlt): PyTorch based Deep Learning Toolbox\n40. [semi-supervised-pytorch](https://github.com/wohlert/semi-supervised-pytorch): Implementations of different VAE-based semi-supervised and generative models in PyTorch. \n41. [pytorch_cluster](https://github.com/rusty1s/pytorch_cluster): PyTorch Extension Library of Optimised Graph Cluster Algorithms.\n42. [neural-assembly-compiler](https://github.com/aditya-khant/neural-assembly-compiler): A neural assembly compiler for pyTorch based on adaptive-neural-compilation. \n43. [caffemodel2pytorch](https://github.com/vadimkantorov/caffemodel2pytorch): Convert Caffe models to PyTorch.\n44. [extension-cpp](https://github.com/pytorch/extension-cpp): C++ extensions in PyTorch\n45. [pytoune](https://github.com/GRAAL-Research/pytoune): A Keras-like framework and utilities for PyTorch\n46. [jetson-reinforcement](https://github.com/dusty-nv/jetson-reinforcement): Deep reinforcement learning libraries for NVIDIA Jetson TX1/TX2 with PyTorch, OpenAI Gym, and Gazebo robotics simulator.\n47. [matchbox](https://github.com/salesforce/matchbox): Write PyTorch code at the level of individual examples, then run it efficiently on minibatches.\n48. [torch-two-sample](https://github.com/josipd/torch-two-sample): A PyTorch library for two-sample tests\n49. [pytorch-summary](https://github.com/sksq96/pytorch-summary): Model summary in PyTorch similar to `model.summary()` in Keras\n50. [mpl.pytorch](https://github.com/BelBES/mpl.pytorch): Pytorch implementation of MaxPoolingLoss.\n51. [scVI-dev](https://github.com/YosefLab/scVI-dev): Development branch of the scVI project in PyTorch\n52. [apex](https://github.com/NVIDIA/apex): An Experimental PyTorch Extension(will be deprecated at a later point)\n53. [ELF](https://github.com/pytorch/ELF): ELF: a platform for game research.\n54. [Torchlite](https://github.com/EKami/Torchlite): A high level library on top of(not only) Pytorch\n55. [joint-vae](https://github.com/Schlumberger/joint-vae): Pytorch implementation of JointVAE, a framework for disentangling continuous and discrete factors of variation star2\n56. [SLM-Lab](https://github.com/kengz/SLM-Lab): Modular Deep Reinforcement Learning framework in PyTorch.\n57. [bindsnet](https://github.com/Hananel-Hazan/bindsnet): A Python package used for simulating spiking neural networks (SNNs) on CPUs or GPUs using PyTorch\n58. [pro_gan_pytorch](https://github.com/akanimax/pro_gan_pytorch): ProGAN package implemented as an extension of PyTorch nn.Module\n59. [pytorch_geometric](https://github.com/rusty1s/pytorch_geometric): Geometric Deep Learning Extension Library for PyTorch\n60. [torchplus](https://github.com/knighton/torchplus): Implements the + operator on PyTorch modules, returning sequences.\n61. [lagom](https://github.com/zuoxingdong/lagom): lagom: A light PyTorch infrastructure to quickly prototype reinforcement learning algorithms.\n62. [torchbearer](https://github.com/ecs-vlc/torchbearer): torchbearer: A model training library for researchers using PyTorch.\n63. [pytorch-maml-rl](https://github.com/tristandeleu/pytorch-maml-rl): Reinforcement Learning with Model-Agnostic Meta-Learning in Pytorch. \n64. [NALU](https://github.com/bharathgs/NALU): Basic pytorch implementation of NAC/NALU from Neural Arithmetic Logic Units paper by trask et.al arxiv.org/pdf/1808.00508.pdf\n66. [QuCumber](https://github.com/PIQuIL/QuCumber): Neural Network Many-Body Wavefunction Reconstruction\n67. [magnet](https://github.com/MagNet-DL/magnet): Deep Learning Projects that Build Themselves http://magnet-dl.readthedocs.io/\n68. [opencv_transforms](https://github.com/jbohnslav/opencv_transforms): OpenCV implementation of Torchvision''s image augmentations\n69. [fastai](https://github.com/fastai/fastai): The fast.ai deep learning library, lessons, and tutorials\n70. [pytorch-dense-correspondence](https://github.com/RobotLocomotion/pytorch-dense-correspondence): Code for "Dense Object Nets: Learning Dense Visual Object Descriptors By and For Robotic Manipulation" arxiv.org/pdf/1806.08756.pdf\n71. [colorization-pytorch](https://github.com/richzhang/colorization-pytorch): PyTorch reimplementation of Interactive Deep Colorization richzhang.github.io/ideepcolor\n72. [beauty-net](https://github.com/cms-flash/beauty-net): A simple, flexible, and extensible template for PyTorch. It''s beautiful.\n73. [OpenChem](https://github.com/Mariewelt/OpenChem): OpenChem: Deep Learning toolkit for Computational Chemistry and Drug Design Research mariewelt.github.io/OpenChem \n74. [torchani](https://github.com/aiqm/torchani): Accurate Neural Network Potential on PyTorch aiqm.github.io/torchani\n75. [PyTorch-LBFGS](https://github.com/hjmshi/PyTorch-LBFGS): A PyTorch implementation of L-BFGS.\n76. [gpytorch](https://github.com/cornellius-gp/gpytorch): A highly efficient and modular implementation of Gaussian Processes in PyTorch.\n77. [hessian](https://github.com/mariogeiger/hessian): hessian in pytorch. \n78. [vel](https://github.com/MillionIntegrals/vel): Velocity in deep-learning research.\n79. [nonechucks](https://github.com/msamogh/nonechucks): Skip bad items in your PyTorch DataLoader, use Transforms as Filters, and more!\n80. [torchstat](https://github.com/Swall0w/torchstat): Model analyzer in PyTorch.\n81. [QNNPACK](https://github.com/pytorch/QNNPACK): Quantized Neural Network PACKage - mobile-optimized implementation of quantized neural network operators.\n82. [torchdiffeq](https://github.com/rtqichen/torchdiffeq): Differentiable ODE solvers with full GPU support and O(1)-memory backpropagation.\n83. [redner](https://github.com/BachiLi/redner): A differentiable Monte Carlo path tracer\n84. [pixyz](https://github.com/masa-su/pixyz): a library for developing deep generative models in a more concise, intuitive and extendable way. \n85. [euclidesdb](https://github.com/perone/euclidesdb): A multi-model machine learning feature embedding database http://euclidesdb.readthedocs.io \n86. [pytorch2keras](https://github.com/nerox8664/pytorch2keras): Convert PyTorch dynamic graph to Keras model.\n87. [salad](https://github.com/domainadaptation/salad): Semi-Supervised Learning and Domain Adaptation.\n88. [netharn](https://github.com/Erotemic/netharn): Parameterized fit and prediction harnesses for pytorch.\n89. [dgl](https://github.com/dmlc/dgl): Python package built to ease deep learning on graph, on top of existing DL frameworks. http://dgl.ai. \n90. [gandissect](https://github.com/CSAILVision/gandissect): Pytorch-based tools for visualizing and understanding the neurons of a GAN. gandissect.csail.mit.edu \n91. [delira](https://github.com/justusschock/delira): Lightweight framework for fast prototyping and training deep neural networks in medical imaging delira.rtfd.io\n92. [mushroom](https://github.com/AIRLab-POLIMI/mushroom): Python library for Reinforcement Learning experiments.\n93. [Xlearn](https://github.com/thuml/Xlearn): Transfer Learning Library\n94. [geoopt](https://github.com/ferrine/geoopt): Riemannian Adaptive Optimization Methods with pytorch optim\n95. [vegans](https://github.com/unit8co/vegans): A library providing various existing GANs in PyTorch.\n96. [torchgeometry](https://github.com/arraiyopensource/torchgeometry): TGM: PyTorch Geometry\n97. [AdverTorch](https://github.com/BorealisAI/advertorch): A Toolbox for Adversarial Robustness (attack/defense/training) Research\n98. [AdaBound](https://github.com/Luolc/AdaBound): An optimizer that trains as fast as Adam and as good as SGD.a\n99. [fenchel-young-losses](https://github.com/mblondel/fenchel-young-losses): Probabilistic classification in PyTorch/TensorFlow/scikit-learn with Fenchel-Young losses\n100. [pytorch-OpCounter](https://github.com/Lyken17/pytorch-OpCounter): Count the FLOPs of your PyTorch model.\n101. [Tor10](https://github.com/kaihsin/Tor10): A Generic Tensor-Network library that is designed for quantum simulation, base on the pytorch.\n102. [Catalyst](https://github.com/catalyst-team/catalyst): High-level utils for PyTorch DL & RL research. It was developed with a focus on reproducibility, fast experimentation and code/ideas reusing. Being able to research/develop something new, rather than write another regular train loop.\n103. [Ax](https://github.com/facebook/Ax): Adaptive Experimentation Platform\n104. [pywick](https://github.com/achaiah/pywick): High-level batteries-included neural network training library for Pytorch\n105. [torchgpipe](https://github.com/kakaobrain/torchgpipe): A GPipe implementation in PyTorch torchgpipe.readthedocs.io\n106. [hub](https://github.com/pytorch/hub): Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.\n107. [pytorch-lightning](https://github.com/williamFalcon/pytorch-lightning): Rapid research framework for Pytorch. The researcher''s version of keras.\n108. [Tor10](https://github.com/kaihsin/Tor10): A Generic Tensor-Network library that is designed for quantum simulation, base on the pytorch.\n109. [tensorwatch](https://github.com/microsoft/tensorwatch): Debugging, monitoring and visualization for Deep Learning and Reinforcement Learning from Microsoft Research.\n110. [wavetorch](https://github.com/fancompute/wavetorch): Numerically solving and backpropagating through the wave equation arxiv.org/abs/1904.12831\n111. [diffdist](https://github.com/ag14774/diffdist): diffdist is a python library for pytorch. It extends the default functionality of torch.autograd and adds support for differentiable communication between processes. \n112. [torchprof](https://github.com/awwong1/torchprof): A minimal dependency library for layer-by-layer profiling of Pytorch models.\n113. [osqpth](https://github.com/oxfordcontrol/osqpth): The differentiable OSQP solver layer for PyTorch. \n114. [mctorch](https://github.com/mctorch/mctorch): A manifold optimization library for deep learning. \n115. [pytorch-hessian-eigenthings](https://github.com/noahgolmant/pytorch-hessian-eigenthings): Efficient PyTorch Hessian eigendecomposition using the Hessian-vector product and stochastic power iteration. \n116. [MinkowskiEngine](https://github.com/StanfordVL/MinkowskiEngine): Minkowski Engine is an auto-diff library for generalized sparse convolutions and high-dimensional sparse tensors.\n117. [pytorch-cpp-rl](https://github.com/Omegastick/pytorch-cpp-rl): PyTorch C++ Reinforcement Learning\n118. [pytorch-toolbelt](https://github.com/BloodAxe/pytorch-toolbelt): PyTorch extensions for fast R&D prototyping and Kaggle farming\n119. [argus-tensor-stream](https://github.com/Fonbet/argus-tensor-stream): A library for real-time video stream decoding to CUDA memory tensorstream.argus-ai.com\n120. [macarico](https://github.com/hal3/macarico): learning to search in pytorch\n121. [rlpyt](https://github.com/astooke/rlpyt): Reinforcement Learning in PyTorch\n122. [pywarm](https://github.com/blue-season/pywarm): A cleaner way to build neural networks for PyTorch. blue-season.github.io/pywarm\n123. [learn2learn](https://github.com/learnables/learn2learn): PyTorch Meta-learning Framework for Researchers http://learn2learn.net\n124. [torchbeast](https://github.com/facebookresearch/torchbeast): A PyTorch Platform for Distributed RL\n125. [higher](https://github.com/facebookresearch/higher): higher is a pytorch library allowing users to obtain higher order gradients over losses spanning training loops rather than individual training steps.\n126. [Torchelie](https://github.com/Vermeille/Torchelie/): TorchÃ©lie is a set of utility functions, layers, losses, models, trainers and other things for PyTorch. torchelie.readthedocs.org \n127. [CrypTen](https://github.com/facebookresearch/CrypTen): CrypTen is a Privacy Preserving Machine Learning framework written using PyTorch that allows researchers and developers to train models using encrypted data. CrypTen currently supports Secure multi-party computation as its encryption mechanism.\n128. [cvxpylayers](https://github.com/cvxgrp/cvxpylayers): cvxpylayers is a Python library for constructing differentiable convex optimization layers in PyTorch\n129. [RepDistiller](https://github.com/HobbitLong/RepDistiller): Contrastive Representation Distillation (CRD), and benchmark of recent knowledge distillation methods\n130. [kaolin](https://github.com/NVIDIAGameWorks/kaolin): PyTorch library aimed at accelerating 3D deep learning research\n131. [PySNN](https://github.com/BasBuller/PySNN): Efficient Spiking Neural Network framework, built on top of PyTorch for GPU acceleration.\n132. [sparktorch](https://github.com/dmmiller612/sparktorch): Train and run Pytorch models on Apache Spark.\n133. [pytorch-metric-learning](https://github.com/KevinMusgrave/pytorch-metric-learning): The easiest way to use metric learning in your application. Modular, flexible, and extensible. Written in PyTorch.\n134. [autonomous-learning-library](https://github.com/cpnota/autonomous-learning-library): A PyTorch library for building deep reinforcement learning agents.\n135. [flambe](https://github.com/asappresearch/flambe): An ML framework to accelerate research and its path to production. flambe.ai\n136. [pytorch-optimizer](https://github.com/jettify/pytorch-optimizer): Collections of modern optimization algorithms for PyTorch, includes: AccSGD, AdaBound, AdaMod, DiffGrad, Lamb, RAdam, RAdam, Yogi.\n137. [PyTorch-VAE](https://github.com/AntixK/PyTorch-VAE): A Collection of Variational Autoencoders (VAE) in PyTorch.\n138. [ray](https://github.com/ray-project/ray): A fast and simple framework for building and running distributed applications. Ray is packaged with RLlib, a scalable reinforcement learning library, and Tune, a scalable hyperparameter tuning library. ray.io\n139. [Pytorch Geometric Temporal](https://github.com/benedekrozemberczki/pytorch_geometric_temporal): A temporal extension library for PyTorch Geometric \n140. [Poutyne](https://github.com/GRAAL-Research/poutyne): A Keras-like framework for PyTorch that handles much of the boilerplating code needed to train neural networks.\n141. [Pytorch-Toolbox](https://github.com/PistonY/torch-toolbox): This is toolbox project for Pytorch. Aiming to make you write Pytorch code more easier, readable and concise.\n142. [Pytorch-contrib](https://github.com/pytorch/contrib): It contains reviewed implementations of ideas from recent machine learning papers.\n143. [EfficientNet PyTorch](https://github.com/lukemelas/EfficientNet-PyTorch): It contains an op-for-op PyTorch reimplementation of EfficientNet, along with pre-trained models and examples.\n144. [PyTorch/XLA](https://github.com/pytorch/xla): PyTorch/XLA is a Python package that uses the XLA deep learning compiler to connect the PyTorch deep learning framework and Cloud TPUs.\n145. [webdataset](https://github.com/tmbdev/webdataset): WebDataset is a PyTorch Dataset (IterableDataset) implementation providing efficient access to datasets stored in POSIX tar archives.\n146. [volksdep](https://github.com/Media-Smart/volksdep): volksdep is an open-source toolbox for deploying and accelerating PyTorch, Onnx and Tensorflow models with TensorRT.\n147. [PyTorch-StudioGAN](https://github.com/POSTECH-CVLab/PyTorch-StudioGAN): StudioGAN is a Pytorch library providing implementations of representative Generative Adversarial Networks (GANs) for conditional/unconditional image generation. StudioGAN aims to offer an identical playground for modern GANs so that machine learning researchers can readily compare and analyze a new idea.\n148. [torchdrift](https://github.com/torchdrift/torchdrift/): drift detection library\n149. [accelerate](https://github.com/huggingface/accelerate) : A simple way to train and use PyTorch models with multi-GPU, TPU, mixed-precision \n150. [lightning-transformers](https://github.com/PyTorchLightning/lightning-transformers):  Flexible interface for high-performance research using SOTA Transformers leveraging Pytorch Lightning, Transformers, and Hydra. \n151. [Flower](https://flower.dev/) A unified approach to federated learning, analytics, and evaluation. It allows to federated any machine learning workload.\n152. [lightning-flash](https://github.com/PyTorchLightning/lightning-flash): Flash is a collection of tasks for fast prototyping, baselining and fine-tuning scalable Deep Learning models, built on PyTorch Lightning.\n153. [Pytorch Geometric Signed Directed](https://github.com/SherylHYX/pytorch_geometric_signed_directed): A signed and directed extension library for PyTorch Geometric. \n154. [Koila](https://github.com/rentruewang/koila): A simple wrapper around pytorch that prevents CUDA out of memory issues.\n155. [Renate](https://github.com/awslabs/renate): A library for real-world continual learning.\n\n## Tutorials, books, & examples\n\n1. **[Practical Pytorch](https://github.com/spro/practical-pytorch)**: Tutorials explaining different RNN models\n2. [DeepLearningForNLPInPytorch](https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html): An IPython Notebook tutorial on deep learning, with an emphasis on Natural Language Processing. \n3. [pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial): tutorial for researchers to learn deep learning with pytorch.\n4.  [pytorch-exercises](https://github.com/keon/pytorch-exercises): pytorch-exercises collection. \n5.  [pytorch tutorials](https://github.com/pytorch/tutorials): Various pytorch tutorials. \n6.  [pytorch examples](https://github.com/pytorch/examples):  A repository showcasing examples of using pytorch \n7. [pytorch practice](https://github.com/napsternxg/pytorch-practice): Some example scripts on pytorch.  \n8.  [pytorch mini tutorials](https://github.com/vinhkhuc/PyTorch-Mini-Tutorials):  Minimal tutorials for PyTorch adapted from Alec Radford''s Theano tutorials. \n9.  [pytorch text classification](https://github.com/xiayandi/Pytorch_text_classification): A simple implementation of CNN based text classification in Pytorch \n10. [cats vs dogs](https://github.com/desimone/pytorch-cat-vs-dogs): Example of network fine-tuning in pytorch for the kaggle competition Dogs vs. Cats Redux: Kernels Edition. Currently #27 (0.05074) on the leaderboard.  \n11. [convnet](https://github.com/eladhoffer/convNet.pytorch): This is a complete training example for Deep Convolutional Networks on various datasets (ImageNet, Cifar10, Cifar100, MNIST).\n12. [pytorch-generative-adversarial-networks](https://github.com/mailmahee/pytorch-generative-adversarial-networks): simple generative adversarial network (GAN) using PyTorch.   \n13. [pytorch containers](https://github.com/amdegroot/pytorch-containers): This repository aims to help former Torchies more seamlessly transition to the "Containerless" world of PyTorch by providing a list of PyTorch implementations of Torch Table Layers.  \n14. [T-SNE in pytorch](https://github.com/cemoody/topicsne): t-SNE experiments in pytorch \n15. [AAE_pytorch](https://github.com/fducau/AAE_pytorch): Adversarial Autoencoders (with Pytorch). \n16. [Kind_PyTorch_Tutorial](https://github.com/GunhoChoi/Kind_PyTorch_Tutorial): Kind PyTorch Tutorial for beginners.  \n17.  [pytorch-poetry-gen](https://github.com/justdark/pytorch-poetry-gen): a char-RNN based on pytorch.  \n18. [pytorch-REINFORCE](https://github.com/JamesChuanggg/pytorch-REINFORCE): PyTorch implementation of REINFORCE, This repo supports both continuous and discrete environments in OpenAI gym.\n19.  **[PyTorch-Tutorial](https://github.com/MorvanZhou/PyTorch-Tutorial)**: Build your neural network easy and fast  https://morvanzhou.github.io/tutorials/ \n20. [pytorch-intro](https://github.com/joansj/pytorch-intro): A couple of scripts to illustrate how to do CNNs and RNNs in PyTorch\n21. [pytorch-classification](https://github.com/bearpaw/pytorch-classification): A unified framework for the image classification task on CIFAR-10/100 and ImageNet.\n22. [pytorch_notebooks - hardmaru](https://github.com/hardmaru/pytorch_notebooks): Random tutorials created in NumPy and PyTorch.\n23. [pytorch_tutoria-quick](https://github.com/soravux/pytorch_tutorial): Quick PyTorch introduction and tutorial. Targets computer vision, graphics and machine learning researchers eager to try a new framework.  \n24. [Pytorch_fine_tuning_Tutorial](https://github.com/Spandan-Madan/Pytorch_fine_tuning_Tutorial): A short tutorial on performing fine tuning or transfer learning in PyTorch.\n25. [pytorch_exercises](https://github.com/Kyubyong/pytorch_exercises): pytorch-exercises \n26. [traffic-sign-detection](https://github.com/soumith/traffic-sign-detection-homework): nyu-cv-fall-2017 example\n27. [mss_pytorch](https://github.com/Js-Mim/mss_pytorch): Singing Voice Separation via Recurrent Inference and Skip-Filtering Connections - PyTorch Implementation. Demo: js-mim.github.io/mss_pytorch\n28. [DeepNLP-models-Pytorch](https://github.com/DSKSD/DeepNLP-models-Pytorch) Pytorch implementations of various Deep NLP models in cs-224n(Stanford Univ: NLP with Deep Learning)\n29. [Mila introductory tutorials](https://github.com/mila-udem/welcome_tutorials): Various tutorials given for welcoming new students at MILA.\n30. [pytorch.rl.learning](https://github.com/moskomule/pytorch.rl.learning): for learning reinforcement learning using PyTorch.\n31. [minimal-seq2seq](https://github.com/keon/seq2seq): Minimal Seq2Seq model with Attention for Neural Machine Translation in PyTorch\n32. [tensorly-notebooks](https://github.com/JeanKossaifi/tensorly-notebooks): Tensor methods in Python with TensorLy tensorly.github.io/dev\n33. [pytorch_bits](https://github.com/jpeg729/pytorch_bits): time-series prediction related examples.\n34. [skip-thoughts](https://github.com/sanyam5/skip-thoughts): An implementation of Skip-Thought Vectors in PyTorch.\n35. [video-caption-pytorch](https://github.com/xiadingZ/video-caption-pytorch): pytorch code for video captioning. \n36. [Capsule-Network-Tutorial](https://github.com/higgsfield/Capsule-Network-Tutorial): Pytorch easy-to-follow Capsule Network tutorial.\n37. [code-of-learn-deep-learning-with-pytorch](https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch): This is code of book "Learn Deep Learning with PyTorch" item.jd.com/17915495606.html\n38. [RL-Adventure](https://github.com/higgsfield/RL-Adventure): Pytorch easy-to-follow step-by-step Deep Q Learning tutorial with clean readable code.\n39. [accelerated_dl_pytorch](https://github.com/hpcgarage/accelerated_dl_pytorch): Accelerated Deep Learning with PyTorch at Jupyter Day Atlanta II. \n40. [RL-Adventure-2](https://github.com/higgsfield/RL-Adventure-2): PyTorch4 tutorial of: actor critic / proximal policy optimization / acer / ddpg / twin dueling ddpg / soft actor critic / generative adversarial imitation learning / hindsight experience replay\n41. [Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch)](https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f)\n42. [adversarial-autoencoders-with-pytorch](https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/)\n43. [transfer learning using pytorch](https://medium.com/@vishnuvig/transfer-learning-using-pytorch-4c3475f4495)\n44. [how-to-implement-a-yolo-object-detector-in-pytorch](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/)\n45. [pytorch-for-recommenders-101](http://blog.fastforwardlabs.com/2018/04/10/pytorch-for-recommenders-101.html)\n46. [pytorch-for-numpy-users](https://github.com/wkentaro/pytorch-for-numpy-users)\n47. [PyTorch Tutorial](http://www.pytorchtutorial.com/): PyTorch Tutorials in Chinese.\n48. [grokking-pytorch](https://github.com/Kaixhin/grokking-pytorch): The Hitchiker''s Guide to PyTorch\n49. [PyTorch-Deep-Learning-Minicourse](https://github.com/Atcold/PyTorch-Deep-Learning-Minicourse): Minicourse in Deep Learning with PyTorch.\n50. [pytorch-custom-dataset-examples](https://github.com/utkuozbulak/pytorch-custom-dataset-examples): Some custom dataset examples for PyTorch\n51. [Multiplicative LSTM for sequence-based Recommenders](https://florianwilhelm.info/2018/08/multiplicative_LSTM_for_sequence_based_recos/)\n52. [deeplearning.ai-pytorch](https://github.com/furkanu/deeplearning.ai-pytorch): PyTorch Implementations of Coursera''s Deep Learning(deeplearning.ai) Specialization. \n53. [MNIST_Pytorch_python_and_capi](https://github.com/tobiascz/MNIST_Pytorch_python_and_capi): This is an example of how to train a MNIST network in Python and run it in c++ with pytorch 1.0\n54. [torch_light](https://github.com/ne7ermore/torch_light): Tutorials and examples include Reinforcement Training, NLP, CV\n55. [portrain-gan](https://github.com/dribnet/portrain-gan): torch code to decode (and almost encode) latents from art-DCGAN''s Portrait GAN.\n56. [mri-analysis-pytorch](https://github.com/omarsar/mri-analysis-pytorch): MRI analysis using PyTorch and MedicalTorch\n57. [cifar10-fast](https://github.com/davidcpage/cifar10-fast): \nDemonstration of training a small ResNet on CIFAR10 to 94% test accuracy in 79 seconds as described in this [blog series](https://www.myrtle.ai/2018/09/24/how_to_train_your_resnet/).\n58. [Intro to Deep Learning with PyTorch](https://in.udacity.com/course/deep-learning-pytorch--ud188): A free course by Udacity and facebook, with a good intro to PyTorch, and an interview with Soumith Chintala, one of the original authors of PyTorch.\n59. [pytorch-sentiment-analysis](https://github.com/bentrevett/pytorch-sentiment-analysis): Tutorials on getting started with PyTorch and TorchText for sentiment analysis.\n60. [pytorch-image-models](https://github.com/rwightman/pytorch-image-models): PyTorch image models, scripts, pretrained weights -- (SE)ResNet/ResNeXT, DPN, EfficientNet, MobileNet-V3/V2/V1, MNASNet, Single-Path NAS, FBNet, and more.\n61. [CIFAR-ZOO](https://github.com/BIGBALLON/CIFAR-ZOO): Pytorch implementation for multiple CNN architectures and improve methods with state-of-the-art results. \n62. [d2l-pytorch](https://github.com/dsgiitr/d2l-pytorch): This is an attempt to modify Dive into Deep Learning, Berkeley STAT 157 (Spring 2019) textbook''s code into PyTorch.\n63. [thinking-in-tensors-writing-in-pytorch](https://github.com/stared/thinking-in-tensors-writing-in-pytorch): Thinking in tensors, writing in PyTorch (a hands-on deep learning intro).\n64. [NER-BERT-pytorch](https://github.com/lemonhu/NER-BERT-pytorch): PyTorch solution of named entity recognition task Using Google AI''s pre-trained BERT model.\n65. [pytorch-sync-batchnorm-example](https://github.com/dougsouza/pytorch-sync-batchnorm-example): How to use Cross Replica / Synchronized Batchnorm in Pytorch. \n66. [SentimentAnalysis](https://github.com/barissayil/SentimentAnalysis): Sentiment analysis neural network trained by fine tuning BERT on the Stanford Sentiment Treebank, thanks to [Hugging Face](https://huggingface.co/transformers/)''s Transformers library.\n67. [pytorch-cpp](https://github.com/prabhuomkar/pytorch-cpp): C++ implementations of PyTorch tutorials for deep learning researchers (based on the Python tutorials from [pytorch-tutorial](https://github.com/yunjey/pytorch-tutorial)). \n68. [Deep Learning with PyTorch: Zero to GANs](https://jovian.ml/aakashns/collections/deep-learning-with-pytorch): Interactive and coding-focused tutorial series on introduction to Deep Learning with PyTorch ([video](https://www.youtube.com/watch?v=GIsg-ZUy0MY)).\n69. [Deep Learning with PyTorch](https://www.manning.com/books/deep-learning-with-pytorch): Deep Learning with PyTorch teaches you how to implement deep learning algorithms with Python and PyTorch, the book includes a case study: building an algorithm capable of detecting malignant lung tumors using CT scans.\n70. [Serverless Machine Learning in Action with PyTorch and AWS](https://www.manning.com/books/serverless-machine-learning-in-action): Serverless Machine Learning in Action is a guide to bringing your experimental PyTorch machine learning code to production using serverless capabilities from major cloud providers like AWS, Azure, or GCP.\n71. [LabML NN](https://github.com/lab-ml/nn): A collection of PyTorch implementations of neural networks architectures and algorithms with side-by-side notes.\n72. [Run your PyTorch Example Fedarated with Flower](https://github.com/adap/flower/tree/main/examples/pytorch_from_centralized_to_federated): This example demonstrates how an already existing centralized PyTorch machine learning project can be federated with Flower. A Cifar-10 dataset is used together with a convolutional neural network (CNN).\n\n## Paper implementations\n\n1. [google_evolution](https://github.com/neuralix/google_evolution): This implements one of result networks from Large-scale evolution of image classifiers by Esteban Real, et. al. \n2. [pyscatwave](https://github.com/edouardoyallon/pyscatwave): Fast Scattering Transform with CuPy/PyTorch,read the paper [here](https://arxiv.org/abs/1703.08961)\n3. [scalingscattering](https://github.com/edouardoyallon/scalingscattering): Scaling The Scattering Transform : Deep Hybrid Networks.  \n4. [deep-auto-punctuation](https://github.com/episodeyang/deep-auto-punctuation): a pytorch implementation of auto-punctuation learned character by character.  \n5. [Realtime_Multi-Person_Pose_Estimation](https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation): This is a pytorch version of Realtime_Multi-Person_Pose_Estimation, origin code is [here](https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation) .\n6. [PyTorch-value-iteration-networks](https://github.com/onlytailei/PyTorch-value-iteration-networks): PyTorch implementation of the Value Iteration Networks (NIPS ''16) paper  \n7. [pytorch_Highway](https://github.com/analvikingur/pytorch_Highway): Highway network implemented in pytorch.\n8. [pytorch_NEG_loss](https://github.com/analvikingur/pytorch_NEG_loss): NEG loss implemented in pytorch.  \n9. [pytorch_RVAE](https://github.com/analvikingur/pytorch_RVAE): Recurrent Variational Autoencoder that generates sequential data implemented in pytorch.   \n10. [pytorch_TDNN](https://github.com/analvikingur/pytorch_TDNN): Time Delayed NN implemented in pytorch.  \n11. [eve.pytorch](https://github.com/moskomule/eve.pytorch): An implementation of Eve Optimizer, proposed in Imploving Stochastic Gradient Descent with Feedback, Koushik and Hayashi, 2016.  \n12. [e2e-model-learning](https://github.com/locuslab/e2e-model-learning): Task-based end-to-end model learning.  \n13. [pix2pix-pytorch](https://github.com/mrzhu-cool/pix2pix-pytorch): PyTorch implementation of "Image-to-Image Translation Using Conditional Adversarial Networks".   \n14. [Single Shot MultiBox Detector](https://github.com/amdegroot/ssd.pytorch): A PyTorch Implementation of Single Shot MultiBox Detector.  \n15. [DiscoGAN](https://github.com/carpedm20/DiscoGAN-pytorch): PyTorch implementation of "Learning to Discover Cross-Domain Relations with Generative Adversarial Networks"  \n16. [official DiscoGAN implementation](https://github.com/SKTBrain/DiscoGAN): Official implementation of "Learning to Discover Cross-Domain Relations with Generative Adversarial Networks".  \n17. [pytorch-es](https://github.com/atgambardella/pytorch-es): This is a PyTorch implementation of [Evolution Strategies](https://arxiv.org/abs/1703.03864) .  \n18. [piwise](https://github.com/bodokaiser/piwise): Pixel-wise segmentation on VOC2012 dataset using pytorch.  \n19. [pytorch-dqn](https://github.com/transedward/pytorch-dqn): Deep Q-Learning Network in pytorch.  \n20. [neuraltalk2-pytorch](https://github.com/ruotianluo/neuraltalk2.pytorch): image captioning model in pytorch(finetunable cnn in branch with_finetune)\n21. [vnet.pytorch](https://github.com/mattmacy/vnet.pytorch): A Pytorch implementation for V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation.    \n22. [pytorch-fcn](https://github.com/wkentaro/pytorch-fcn): PyTorch implementation of Fully Convolutional Networks.  \n23. [WideResNets](https://github.com/xternalz/WideResNet-pytorch): WideResNets for CIFAR10/100 implemented in PyTorch. This implementation requires less GPU memory than what is required by the official Torch implementation: https://github.com/szagoruyko/wide-residual-networks .\n24. [pytorch_highway_networks](https://github.com/c0nn3r/pytorch_highway_networks): Highway networks implemented in PyTorch.  \n25. [pytorch-NeuCom](https://github.com/ypxie/pytorch-NeuCom): Pytorch implementation of DeepMind''s differentiable neural computer paper.  \n26. [captionGen](https://github.com/eladhoffer/captionGen): Generate captions for an image using PyTorch.  \n27. [AnimeGAN](https://github.com/jayleicn/animeGAN): A simple PyTorch Implementation of Generative Adversarial Networks, focusing on anime face drawing. \n28. [Cnn-text classification](https://github.com/Shawn1993/cnn-text-classification-pytorch): This is the implementation of Kim''s Convolutional Neural Networks for Sentence Classification paper in PyTorch.  \n29. [deepspeech2](https://github.com/SeanNaren/deepspeech.pytorch): Implementation of DeepSpeech2 using Baidu Warp-CTC. Creates a network based on the DeepSpeech2 architecture, trained with the CTC activation function.\n30. [seq2seq](https://github.com/MaximumEntropy/Seq2Seq-PyTorch): This repository contains implementations of Sequence to Sequence (Seq2Seq) models in PyTorch  \n31. [Asynchronous Advantage Actor-Critic in PyTorch](https://github.com/rarilurelo/pytorch_a3c): This is PyTorch implementation of A3C as described in Asynchronous Methods for Deep Reinforcement Learning. Since PyTorch has a easy method to control shared memory within multiprocess, we can easily implement asynchronous method like A3C.    \n32. [densenet](https://github.com/bamos/densenet.pytorch): This is a PyTorch implementation of the DenseNet-BC architecture as described in the paper Densely Connected Convolutional Networks by G. Huang, Z. Liu, K. Weinberger, and L. van der Maaten. This implementation gets a CIFAR-10+ error rate of 4.77 with a 100-layer DenseNet-BC with a growth rate of 12. Their official implementation and links to many other third-party implementations are available in the liuzhuang13/DenseNet repo on GitHub.  \n33. [nninit](https://github.com/alykhantejani/nninit): Weight initialization schemes for PyTorch nn.Modules. This is a port of the popular nninit for Torch7 by @kaixhin.  \n34. [faster rcnn](https://github.com/longcw/faster_rcnn_pytorch): This is a PyTorch implementation of Faster RCNN. This project is mainly based on py-faster-rcnn and TFFRCNN.For details about R-CNN please refer to the paper Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks by Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \n35. [doomnet](https://github.com/akolishchak/doom-net-pytorch): PyTorch''s version of Doom-net implementing some RL models in ViZDoom environment.  \n36. [flownet](https://github.com/ClementPinard/FlowNetPytorch): Pytorch implementation of FlowNet by Dosovitskiy et al.  \n37. [sqeezenet](https://github.com/gsp-27/pytorch_Squeezenet): Implementation of Squeezenet in pytorch, #### pretrained models on CIFAR10 data to come Plan to train the model on cifar 10 and add block connections too.  \n38. [WassersteinGAN](https://github.com/martinarjovsky/WassersteinGAN): wassersteinGAN in pytorch. \n39. [optnet](https://github.com/locuslab/optnet): This repository is by Brandon Amos and J. Zico Kolter and contains the PyTorch source code to reproduce the experiments in our paper OptNet: Differentiable Optimization as a Layer in Neural Networks.  \n40. [qp solver](https://github.com/locuslab/qpth): A fast and differentiable QP solver for PyTorch. Crafted by Brandon Amos and J. Zico Kolter.  \n41. [Continuous Deep Q-Learning with Model-based Acceleration ](https://github.com/ikostrikov/pytorch-naf): Reimplementation of Continuous Deep Q-Learning with Model-based Acceleration.  \n42. [Learning to learn by gradient descent by gradient descent](https://github.com/ikostrikov/pytorch-meta-optimizer): PyTorch implementation of Learning to learn by gradient descent by gradient descent.\n43. [fast-neural-style](https://github.com/darkstar112358/fast-neural-style): pytorch implementation of fast-neural-style, The model uses the method described in [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155) along with Instance Normalization.\n44. [PytorchNeuralStyleTransfer](https://github.com/leongatys/PytorchNeuralStyleTransfer): Implementation of Neural Style Transfer in Pytorch. \n45. [Fast Neural Style for Image Style Transform by Pytorch](https://github.com/bengxy/FastNeuralStyle): Fast Neural Style for Image Style Transform by Pytorch .\n46. [neural style transfer](https://github.com/alexis-jacq/Pytorch-Tutorials): An introduction to PyTorch through the Neural-Style algorithm (https://arxiv.org/abs/1508.06576) developed by Leon A. Gatys, Alexander S. Ecker and Matthias Bethge.   \n47. [VIN_PyTorch_Visdom](https://github.com/zuoxingdong/VIN_PyTorch_Visdom): PyTorch implementation of Value Iteration Networks (VIN): Clean, Simple and Modular. Visualization in Visdom.  \n48. [YOLO2](https://github.com/longcw/yolo2-pytorch): YOLOv2 in PyTorch.   \n49. [attention-transfer](https://github.com/szagoruyko/attention-transfer): Attention transfer in pytorch, read the paper [here](https://arxiv.org/abs/1612.03928).  \n50. [SVHNClassifier](https://github.com/potterhsu/SVHNClassifier-PyTorch): A PyTorch implementation of [Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks](https://arxiv.org/pdf/1312.6082.pdf).  \n51. [pytorch-deform-conv](https://github.com/oeway/pytorch-deform-conv): PyTorch implementation of Deformable Convolution.  \n52. [BEGAN-pytorch](https://github.com/carpedm20/BEGAN-pytorch): PyTorch implementation of [BEGAN](https://arxiv.org/abs/1703.10717): Boundary Equilibrium Generative Adversarial Networks.  \n53. [treelstm.pytorch](https://github.com/dasguptar/treelstm.pytorch): Tree LSTM implementation in PyTorch.\n54. [AGE](https://github.com/DmitryUlyanov/AGE): Code for paper "Adversarial Generator-Encoder Networks" by Dmitry Ulyanov, Andrea Vedaldi and Victor Lempitsky which can be found [here](http://sites.skoltech.ru/app/data/uploads/sites/25/2017/04/AGE.pdf) \n55. [ResNeXt.pytorch](https://github.com/prlz77/ResNeXt.pytorch): Reproduces ResNet-V3 (Aggregated Residual Transformations for Deep Neural Networks) with pytorch.\n56. [pytorch-rl](https://github.com/jingweiz/pytorch-rl): Deep Reinforcement Learning with pytorch & visdom  \n57. [Deep-Leafsnap](https://github.com/sujithv28/Deep-Leafsnap): LeafSnap replicated using deep neural networks to test accuracy compared to traditional computer vision methods.  \n58. [pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix): PyTorch implementation for both unpaired and paired image-to-image translation.\n59. [A3C-PyTorch](https://github.com/onlytailei/A3C-PyTorch):PyTorch implementation of Advantage async actor-critic Algorithms (A3C) in PyTorch\n60. [pytorch-value-iteration-networks](https://github.com/kentsommer/pytorch-value-iteration-networks): Pytorch implementation of Value Iteration Networks (NIPS 2016 best paper)  \n61. [PyTorch-Style-Transfer](https://github.com/zhanghang1989/PyTorch-Style-Transfer): PyTorch Implementation of Multi-style Generative Network for Real-time Transfer\n62. [pytorch-deeplab-resnet](https://github.com/isht7/pytorch-deeplab-resnet): pytorch-deeplab-resnet-model.\n63. [pointnet.pytorch](https://github.com/fxia22/pointnet.pytorch): pytorch implementation for "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation" https://arxiv.org/abs/1612.00593  \n64. **[pytorch-playground](https://github.com/aaron-xichen/pytorch-playground): Base pretrained models and datasets in pytorch (MNIST, SVHN, CIFAR10, CIFAR100, STL10, AlexNet, VGG16, VGG19, ResNet, Inception, SqueezeNet)**.\n65. [pytorch-dnc](https://github.com/jingweiz/pytorch-dnc): Neural Turing Machine (NTM) & Differentiable Neural Computer (DNC) with pytorch & visdom. \n66. [pytorch_image_classifier](https://github.com/jinfagang/pytorch_image_classifier): Minimal But Practical Image Classifier Pipline Using Pytorch, Finetune on ResNet18, Got 99% Accuracy on Own Small Datasets.  \n67. [mnist-svhn-transfer](https://github.com/yunjey/mnist-svhn-transfer): PyTorch Implementation of CycleGAN and SGAN for Domain Transfer (Minimal).\n68. [pytorch-yolo2](https://github.com/marvis/pytorch-yolo2): pytorch-yolo2\n69. [dni](https://github.com/andrewliao11/dni.pytorch): Implement Decoupled Neural Interfaces using Synthetic Gradients in Pytorch\n70. [wgan-gp](https://github.com/caogang/wgan-gp): A pytorch implementation of Paper "Improved Training of Wasserstein GANs".\n71. [pytorch-seq2seq-intent-parsing](https://github.com/spro/pytorch-seq2seq-intent-parsing): Intent parsing and slot filling in PyTorch with seq2seq + attention\n72. [pyTorch_NCE](https://github.com/demelin/pyTorch_NCE): An implementation of the Noise Contrastive Estimation algorithm for pyTorch. Working, yet not very efficient.\n73. [molencoder](https://github.com/cxhernandez/molencoder): Molecular AutoEncoder in PyTorch\n74. [GAN-weight-norm](https://github.com/stormraiser/GAN-weight-norm): Code for "On the Effects of Batch and Weight Normalization in Generative Adversarial Networks"\n75. [lgamma](https://github.com/rachtsingh/lgamma): Implementations of polygamma, lgamma, and beta functions for PyTorch\n76. [bigBatch](https://github.com/eladhoffer/bigBatch): Code used to generate the results appearing in "Train longer, generalize better: closing the generalization gap in large batch training of neural networks" \n77. [rl_a3c_pytorch](https://github.com/dgriff777/rl_a3c_pytorch): Reinforcement learning with implementation of A3C LSTM for Atari 2600. \n78. [pytorch-retraining](https://github.com/ahirner/pytorch-retraining): Transfer Learning Shootout for PyTorch''s model zoo (torchvision)\n79. [nmp_qc](https://github.com/priba/nmp_qc): Neural Message Passing for Computer Vision\n80. [grad-cam](https://github.com/jacobgil/pytorch-grad-cam): Pytorch implementation of Grad-CAM\n81. [pytorch-trpo](https://github.com/mjacar/pytorch-trpo): PyTorch Implementation of Trust Region Policy Optimization (TRPO)\n82. [pytorch-explain-black-box](https://github.com/jacobgil/pytorch-explain-black-box): PyTorch implementation of Interpretable Explanations of Black Boxes by Meaningful Perturbation\n83. [vae_vpflows](https://github.com/jmtomczak/vae_vpflows): Code in PyTorch for the convex combination linear IAF and the Householder Flow, J.M. Tomczak & M. Welling https://jmtomczak.github.io/deebmed.html \n84. [relational-networks](https://github.com/kimhc6028/relational-networks): Pytorch implementation of "A simple neural network module for relational reasoning" (Relational Networks) https://arxiv.org/pdf/1706.01427.pdf\n85. [vqa.pytorch](https://github.com/Cadene/vqa.pytorch): Visual Question Answering in Pytorch\n86. [end-to-end-negotiator](https://github.com/facebookresearch/end-to-end-negotiator): Deal or No Deal? End-to-End Learning for Negotiation Dialogues\n87. [odin-pytorch](https://github.com/ShiyuLiang/odin-pytorch): Principled Detection of Out-of-Distribution Examples in Neural Networks. \n88. [FreezeOut](https://github.com/ajbrock/FreezeOut): Accelerate Neural Net Training by Progressively Freezing Layers. \n89. [ARAE](https://github.com/jakezhaojb/ARAE): Code for the paper "Adversarially Regularized Autoencoders for Generating Discrete Structures" by Zhao, Kim, Zhang, Rush and LeCun.\n90. [forward-thinking-pytorch](https://github.com/kimhc6028/forward-thinking-pytorch): Pytorch implementation of "Forward Thinking: Building and Training Neural Networks One Layer at a Time" https://arxiv.org/pdf/1706.02480.pdf  \n91. [context_encoder_pytorch](https://github.com/BoyuanJiang/context_encoder_pytorch): PyTorch Implement of Context Encoders\n92. [attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch): A PyTorch implementation of the Transformer model in "Attention is All You Need".https://github.com/thnkim/OpenFacePytorch\n93. [OpenFacePytorch](https://github.com/thnkim/OpenFacePytorch): PyTorch module to use OpenFace''s nn4.small2.v1.t7 model \n94. [neural-combinatorial-rl-pytorch](https://github.com/pemami4911/neural-combinatorial-rl-pytorch):  PyTorch implementation of Neural Combinatorial Optimization with Reinforcement Learning.\n95. [pytorch-nec](https://github.com/mjacar/pytorch-nec): PyTorch Implementation of Neural Episodic Control (NEC)\n96. [seq2seq.pytorch](https://github.com/eladhoffer/seq2seq.pytorch): Sequence-to-Sequence learning using PyTorch\n97. [Pytorch-Sketch-RNN](https://github.com/alexis-jacq/Pytorch-Sketch-RNN): a pytorch implementation of arxiv.org/abs/1704.03477\n98. [pytorch-pruning](https://github.com/jacobgil/pytorch-pruning): PyTorch Implementation of [1611.06440] Pruning Convolutional Neural Networks for Resource Efficient Inference\n99. [DrQA](https://github.com/hitvoice/DrQA): A pytorch implementation of Reading Wikipedia to Answer Open-Domain Questions.\n100. [YellowFin_Pytorch](https://github.com/JianGoForIt/YellowFin_Pytorch): auto-tuning momentum SGD optimizer\n101. [samplernn-pytorch](https://github.com/deepsound-project/samplernn-pytorch): PyTorch implementation of SampleRNN: An Unconditional End-to-End Neural Audio Generation Model. \n102. [AEGeAN](https://github.com/tymokvo/AEGeAN): Deeper DCGAN with AE stabilization\n103. [/pytorch-SRResNet](https://github.com/twtygqyy/pytorch-SRResNet): pytorch implementation for Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network arXiv:1609.04802v2 \n104. [vsepp](https://github.com/fartashf/vsepp): Code for the paper "VSE++: Improved Visual Semantic Embeddings"\n105. [Pytorch-DPPO](https://github.com/alexis-jacq/Pytorch-DPPO): Pytorch implementation of Distributed Proximal Policy Optimization: arxiv.org/abs/1707.02286\n106. [UNIT](https://github.com/mingyuliutw/UNIT): PyTorch Implementation of our Coupled VAE-GAN algorithm for Unsupervised Image-to-Image Translation\n107. [efficient_densenet_pytorch](https://github.com/gpleiss/efficient_densenet_pytorch): A memory-efficient implementation of DenseNets\n108. [tsn-pytorch](https://github.com/yjxiong/tsn-pytorch): Temporal Segment Networks (TSN) in PyTorch.\n109. [SMASH](https://github.com/ajbrock/SMASH): An experimental technique for efficiently exploring neural architectures.\n110. [pytorch-retinanet](https://github.com/kuangliu/pytorch-retinanet): RetinaNet in PyTorch\n111. [biogans](https://github.com/aosokin/biogans):  Implementation supporting the ICCV 2017 paper "GANs for Biological Image Synthesis". \n112. [Semantic Image Synthesis via Adversarial Learning]( https://github.com/woozzu/dong_iccv_2017): A PyTorch implementation of the paper "Semantic Image Synthesis via Adversarial Learning" in ICCV 2017. \n113. [fmpytorch](https://github.com/jmhessel/fmpytorch): A PyTorch implementation of a Factorization Machine module in cython.\n114. [ORN](https://github.com/ZhouYanzhao/ORN): A PyTorch implementation of the paper "Oriented Response Networks" in CVPR 2017. \n115. [pytorch-maml](https://github.com/katerakelly/pytorch-maml): PyTorch implementation of MAML: arxiv.org/abs/1703.03400\n116. [pytorch-generative-model-collections](https://github.com/znxlwm/pytorch-generative-model-collections):  Collection of generative models in Pytorch version.\n117. [vqa-winner-cvprw-2017](https://github.com/markdtw/vqa-winner-cvprw-2017): Pytorch Implementation of winner from VQA Chllange Workshop in CVPR''17. \n118. [tacotron_pytorch](https://github.com/r9y9/tacotron_pytorch):  PyTorch implementation of Tacotron speech synthesis model. \n119. [pspnet-pytorch](https://github.com/Lextal/pspnet-pytorch): PyTorch implementation of PSPNet segmentation network\n120. [LM-LSTM-CRF](https://github.com/LiyuanLucasLiu/LM-LSTM-CRF): Empower Sequence Labeling with Task-Aware Language Model http://arxiv.org/abs/1709.04109\n121. [face-alignment](https://github.com/1adrianb/face-alignment): Pytorch implementation of the paper "How far are we from solving the 2D & 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)", ICCV 2017\n122. [DepthNet](https://github.com/ClementPinard/DepthNet): PyTorch DepthNet Training on Still Box dataset. \n123. [EDSR-PyTorch](https://github.com/thstkdgus35/EDSR-PyTorch): PyTorch version of the paper ''Enhanced Deep Residual Networks for Single Image Super-Resolution'' (CVPRW 2017)\n124. [e2c-pytorch](https://github.com/ethanluoyc/e2c-pytorch): Embed to Control implementation in PyTorch.\n125. [3D-ResNets-PyTorch](https://github.com/kenshohara/3D-ResNets-PyTorch): 3D ResNets for Action Recognition.\n126. [bandit-nmt](https://github.com/khanhptnk/bandit-nmt): This is code repo for our EMNLP 2017 paper "Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback", which implements the A2C algorithm on top of a neural encoder-decoder model and benchmarks the combination under simulated noisy rewards.\n127. [pytorch-a2c-ppo-acktr](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr): PyTorch implementation of Advantage Actor Critic (A2C), Proximal Policy Optimization (PPO) and Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (ACKTR).\n128. [zalando-pytorch](https://github.com/baldassarreFe/zalando-pytorch): Various experiments on the [Fashion-MNIST](zalandoresearch/fashion-mnist) dataset from Zalando.\n129. [sphereface_pytorch](https://github.com/clcarwin/sphereface_pytorch): A PyTorch Implementation of SphereFace.\n130. [Categorical DQN](https://github.com/floringogianu/categorical-dqn): A PyTorch Implementation of Categorical DQN from [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887).\n131. [pytorch-ntm](https://github.com/loudinthecloud/pytorch-ntm): pytorch ntm implementation. \n132. [mask_rcnn_pytorch](https://github.com/felixgwu/mask_rcnn_pytorch): Mask RCNN in PyTorch.\n133. [graph_convnets_pytorch](https://github.com/xbresson/graph_convnets_pytorch): PyTorch implementation of graph ConvNets, NIPSâ€™16\n134. [pytorch-faster-rcnn](https://github.com/ruotianluo/pytorch-faster-rcnn): A pytorch implementation of faster RCNN detection framework based on Xinlei Chen''s tf-faster-rcnn.\n135. [torchMoji](https://github.com/huggingface/torchMoji): A pyTorch implementation of the DeepMoji model: state-of-the-art deep learning model for analyzing sentiment, emotion, sarcasm etc.\n136. [semantic-segmentation-pytorch](https://github.com/hangzhaomit/semantic-segmentation-pytorch): Pytorch implementation for Semantic Segmentation/Scene Parsing on [MIT ADE20K dataset](http://sceneparsing.csail.mit.edu)\n137. [pytorch-qrnn](https://github.com/salesforce/pytorch-qrnn): PyTorch implementation of the Quasi-Recurrent Neural Network - up to 16 times faster than NVIDIA''s cuDNN LSTM\n138. [pytorch-sgns](https://github.com/theeluwin/pytorch-sgns): Skipgram Negative Sampling in PyTorch.\n139. [SfmLearner-Pytorch ](https://github.com/ClementPinard/SfmLearner-Pytorch): Pytorch version of SfmLearner from Tinghui Zhou et al.\n140. [deformable-convolution-pytorch](https://github.com/1zb/deformable-convolution-pytorch): PyTorch implementation of Deformable Convolution. \n141. [skip-gram-pytorch](https://github.com/fanglanting/skip-gram-pytorch): A complete pytorch implementation of skipgram model (with subsampling and negative sampling). The embedding result is tested with Spearman''s rank correlation.\n142. [stackGAN-v2](https://github.com/hanzhanggit/StackGAN-v2): Pytorch implementation for reproducing StackGAN_v2 results in the paper StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks by Han Zhang*, Tao Xu*, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, Dimitris Metaxas.\n143. [self-critical.pytorch](https://github.com/ruotianluo/self-critical.pytorch): Unofficial pytorch implementation for Self-critical Sequence Training for Image Captioning. \n144. [pygcn](https://github.com/tkipf/pygcn): Graph Convolutional Networks in PyTorch.\n145. [dnc](https://github.com/ixaxaar/pytorch-dnc): Differentiable Neural Computers, for Pytorch\n146. [prog_gans_pytorch_inference](https://github.com/ptrblck/prog_gans_pytorch_inference): PyTorch inference for "Progressive Growing of GANs" with CelebA snapshot.\n147. [pytorch-capsule](https://github.com/timomernick/pytorch-capsule): Pytorch implementation of Hinton''s Dynamic Routing Between Capsules.\n148. [PyramidNet-PyTorch](https://github.com/dyhan0920/PyramidNet-PyTorch): A PyTorch implementation for PyramidNets (Deep Pyramidal Residual Networks, arxiv.org/abs/1610.02915)\n149. [radio-transformer-networks](https://github.com/gram-ai/radio-transformer-networks): A PyTorch implementation of Radio Transformer Networks from the paper "An Introduction to Deep Learning for the Physical Layer". arxiv.org/abs/1702.00832\n150. [honk](https://github.com/castorini/honk): PyTorch reimplementation of Google''s TensorFlow CNNs for keyword spotting.\n151. [DeepCORAL](https://github.com/SSARCandy/DeepCORAL): A PyTorch implementation of ''Deep CORAL: Correlation Alignment for Deep Domain Adaptation.'', ECCV 2016\n152. [pytorch-pose](https://github.com/bearpaw/pytorch-pose): A PyTorch toolkit for 2D Human Pose Estimation.\n153. [lang-emerge-parlai](https://github.com/karandesai-96/lang-emerge-parlai): Implementation of EMNLP 2017 Paper "Natural Language Does Not Emerge ''Naturally'' in Multi-Agent Dialog" using PyTorch and ParlAI\n154. [Rainbow](https://github.com/Kaixhin/Rainbow): Rainbow: Combining Improvements in Deep Reinforcement Learning \n155. [pytorch_compact_bilinear_pooling v1](https://github.com/gdlg/pytorch_compact_bilinear_pooling): This repository has a pure Python implementation of Compact Bilinear Pooling and Count Sketch for PyTorch.\n156. [CompactBilinearPooling-Pytorch v2](https://github.com/DeepInsight-PCALab/CompactBilinearPooling-Pytorch): (Yang Gao, et al.) A Pytorch Implementation for Compact Bilinear Pooling.\n157. [FewShotLearning](https://github.com/gitabcworld/FewShotLearning): Pytorch implementation of the paper "Optimization as a Model for Few-Shot Learning"\n158. [meProp](https://github.com/jklj077/meProp): Codes for "meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting".\n159. [SFD_pytorch](https://github.com/clcarwin/SFD_pytorch): A PyTorch Implementation of Single Shot Scale-invariant Face Detector.\n160. [GradientEpisodicMemory](https://github.com/facebookresearch/GradientEpisodicMemory): Continuum Learning with GEM: Gradient Episodic Memory. https://arxiv.org/abs/1706.08840\n161. [DeblurGAN](https://github.com/KupynOrest/DeblurGAN): Pytorch implementation of the paper DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks.\n162. [StarGAN](https://github.com/yunjey/StarGAN): StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Tranlsation.\n163. [CapsNet-pytorch](https://github.com/adambielski/CapsNet-pytorch): PyTorch implementation of NIPS 2017 paper Dynamic Routing Between Capsules.\n164. [CondenseNet](https://github.com/ShichenLiu/CondenseNet): CondenseNet: An Efficient DenseNet using Learned Group Convolutions.\n165. [deep-image-prior](https://github.com/DmitryUlyanov/deep-image-prior): Image restoration with neural networks but without learning.\n166. [deep-head-pose](https://github.com/natanielruiz/deep-head-pose): Deep Learning Head Pose Estimation using PyTorch.\n167. [Random-Erasing](https://github.com/zhunzhong07/Random-Erasing): This code has the source code for the paper "Random Erasing Data Augmentation".\n168. [FaderNetworks](https://github.com/facebookresearch/FaderNetworks): Fader Networks: Manipulating Images by Sliding Attributes - NIPS 2017\n169. [FlowNet 2.0](https://github.com/NVIDIA/flownet2-pytorch): FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks\n170. [pix2pixHD](https://github.com/NVIDIA/pix2pixHD): Synthesizing and manipulating 2048x1024 images with conditional GANs tcwang0509.github.io/pix2pixHD \n171. [pytorch-smoothgrad](https://github.com/pkdn/pytorch-smoothgrad): SmoothGrad implementation in PyTorch\n172. [RetinaNet](https://github.com/c0nn3r/RetinaNet): An implementation of RetinaNet in PyTorch.\n173. [faster-rcnn.pytorch](https://github.com/jwyang/faster-rcnn.pytorch): This project is a faster faster R-CNN implementation, aimed to accelerating the training of faster R-CNN object detection models. \n174. [mixup_pytorch](https://github.com/leehomyc/mixup_pytorch): A PyTorch implementation of the paper Mixup: Beyond Empirical Risk Minimization in PyTorch.\n175. [inplace_abn](https://github.com/mapillary/inplace_abn): In-Place Activated BatchNorm for Memory-Optimized Training of DNNs\n176. [pytorch-pose-hg-3d](https://github.com/xingyizhou/pytorch-pose-hg-3d): PyTorch implementation for 3D human pose estimation\n177. [nmn-pytorch](https://github.com/HarshTrivedi/nmn-pytorch): Neural Module Network for VQA in Pytorch.\n178. [bytenet](https://github.com/kefirski/bytenet): Pytorch implementation of bytenet from "Neural Machine Translation in Linear Time" paper\n179. [bottom-up-attention-vqa](https://github.com/hengyuan-hu/bottom-up-attention-vqa): vqa, bottom-up-attention, pytorch\n180. [yolo2-pytorch](https://github.com/ruiminshen/yolo2-pytorch): The YOLOv2 is one of the most popular one-stage object detector. This project adopts PyTorch as the developing framework to increase productivity, and utilize ONNX to convert models into Caffe 2 to benifit engineering deployment.\n181. [reseg-pytorch](https://github.com/Wizaron/reseg-pytorch): PyTorch Implementation of ReSeg (arxiv.org/pdf/1511.07053.pdf)\n182. [binary-stochastic-neurons](https://github.com/Wizaron/binary-stochastic-neurons): Binary Stochastic Neurons in PyTorch.\n183. [pytorch-pose-estimation](https://github.com/DavexPro/pytorch-pose-estimation): PyTorch Implementation of Realtime Multi-Person Pose Estimation project.\n184. [interaction_network_pytorch](https://github.com/higgsfield/interaction_network_pytorch): Pytorch Implementation of Interaction Networks for Learning about Objects, Relations and Physics.\n185. [NoisyNaturalGradient](https://github.com/wlwkgus/NoisyNaturalGradient): Pytorch Implementation of paper "Noisy Natural Gradient as Variational Inference". \n186. [ewc.pytorch](https://github.com/moskomule/ewc.pytorch): An implementation of Elastic Weight Consolidation (EWC), proposed in James Kirkpatrick et al. Overcoming catastrophic forgetting in neural networks 2016(10.1073/pnas.1611835114).\n187. [pytorch-zssr](https://github.com/jacobgil/pytorch-zssr): PyTorch implementation of 1712.06087 "Zero-Shot" Super-Resolution using Deep Internal Learning\n188. [deep_image_prior](https://github.com/atiyo/deep_image_prior): An implementation of image reconstruction methods from Deep Image Prior (Ulyanov et al., 2017) in PyTorch.\n189. [pytorch-transformer](https://github.com/leviswind/pytorch-transformer): pytorch implementation of Attention is all you need.\n190. [DeepRL-Grounding](https://github.com/devendrachaplot/DeepRL-Grounding): This is a PyTorch implementation of the AAAI-18 paper Gated-Attention Architectures for Task-Oriented Language Grounding\n191. [deep-forecast-pytorch](https://github.com/Wizaron/deep-forecast-pytorch): Wind Speed Prediction using LSTMs in PyTorch (arxiv.org/pdf/1707.08110.pdf)\n192. [cat-net](https://github.com/utiasSTARS/cat-net):  Canonical Appearance Transformations\n193. [minimal_glo](https://github.com/tneumann/minimal_glo): Minimal PyTorch implementation of Generative Latent Optimization from the paper "Optimizing the Latent Space of Generative Networks"\n194. [LearningToCompare-Pytorch](https://github.com/dragen1860/LearningToCompare-Pytorch): Pytorch Implementation for Paper: Learning to Compare: Relation Network for Few-Shot Learning. \n195. [poincare-embeddings](https://github.com/facebookresearch/poincare-embeddings): PyTorch implementation of the NIPS-17 paper "PoincarÃ© Embeddings for Learning Hierarchical Representations". \n196. [pytorch-trpo(Hessian-vector product version)](https://github.com/ikostrikov/pytorch-trpo): This is a PyTorch implementation of "Trust Region Policy Optimization (TRPO)" with exact Hessian-vector product instead of finite differences approximation.\n197. [ggnn.pytorch](https://github.com/JamesChuanggg/ggnn.pytorch): A PyTorch Implementation of Gated Graph Sequence Neural Networks (GGNN). \n198. [visual-interaction-networks-pytorch](https://github.com/Mrgemy95/visual-interaction-networks-pytorch): This''s an implementation of deepmind Visual Interaction Networks paper using pytorch\n199. [adversarial-patch](https://github.com/jhayes14/adversarial-patch): PyTorch implementation of adversarial patch. \n200. [Prototypical-Networks-for-Few-shot-Learning-PyTorch](https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch): Implementation of Prototypical Networks for Few Shot Learning (arxiv.org/abs/1703.05175) in Pytorch\n201. [Visual-Feature-Attribution-Using-Wasserstein-GANs-Pytorch](https://github.com/orobix/Visual-Feature-Attribution-Using-Wasserstein-GANs-Pytorch): Implementation of Visual Feature Attribution using Wasserstein GANs (arxiv.org/abs/1711.08998) in PyTorch.\n202. [PhotographicImageSynthesiswithCascadedRefinementNetworks-Pytorch](https://github.com/Blade6570/PhotographicImageSynthesiswithCascadedRefinementNetworks-Pytorch): Photographic Image Synthesis with Cascaded Refinement Networks - Pytorch Implementation\n203. [ENAS-pytorch](https://github.com/carpedm20/ENAS-pytorch): PyTorch implementation of "Efficient Neural Architecture Search via Parameters Sharing". \n204. [Neural-IMage-Assessment](https://github.com/kentsyx/Neural-IMage-Assessment): A PyTorch Implementation of Neural IMage Assessment. \n205. [proxprop](https://github.com/tfrerix/proxprop): Proximal Backpropagation - a neural network training algorithm that takes implicit instead of explicit gradient steps.\n206. [FastPhotoStyle](https://github.com/NVIDIA/FastPhotoStyle): A Closed-form Solution to Photorealistic Image Stylization\n207. [Deep-Image-Analogy-PyTorch](https://github.com/Ben-Louis/Deep-Image-Analogy-PyTorch): A python implementation of Deep-Image-Analogy based on pytorch.\n208. [Person-reID_pytorch](https://github.com/layumi/Person_reID_baseline_pytorch): PyTorch for Person re-ID. \n209. [pt-dilate-rnn](https://github.com/zalandoresearch/pt-dilate-rnn): Dilated RNNs in pytorch. \n210. [pytorch-i-revnet](https://github.com/jhjacobsen/pytorch-i-revnet): Pytorch implementation of i-RevNets.\n211. [OrthNet](https://github.com/Orcuslc/OrthNet): TensorFlow and PyTorch layers for generating Orthogonal Polynomials.\n212. [DRRN-pytorch](https://github.com/jt827859032/DRRN-pytorch): An implementation of Deep Recursive Residual Network for Super Resolution (DRRN), CVPR 2017\n213. [shampoo.pytorch](https://github.com/moskomule/shampoo.pytorch): An implementation of shampoo.\n214. [Neural-IMage-Assessment 2](https://github.com/truskovskiyk/nima.pytorch): A PyTorch Implementation of Neural IMage Assessment.\n215. [TCN](https://github.com/locuslab/TCN): Sequence modeling benchmarks and temporal convolutional networks locuslab/TCN\n216. [DCC](https://github.com/shahsohil/DCC): This repository contains the source code and data for reproducing results of Deep Continuous Clustering paper.\n217. [packnet](https://github.com/arunmallya/packnet): Code for PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning arxiv.org/abs/1711.05769\n218. [PyTorch-progressive_growing_of_gans](https://github.com/github-pengge/PyTorch-progressive_growing_of_gans): PyTorch implementation of Progressive Growing of GANs for Improved Quality, Stability, and Variation.\n219. [nonauto-nmt](https://github.com/salesforce/nonauto-nmt): PyTorch Implementation of "Non-Autoregressive Neural Machine Translation"\n220. [PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN): PyTorch implementations of Generative Adversarial Networks.\n221. [PyTorchWavelets](https://github.com/tomrunia/PyTorchWavelets): PyTorch implementation of the wavelet analysis found in Torrence and Compo (1998)\n222. [pytorch-made](https://github.com/karpathy/pytorch-made): MADE (Masked Autoencoder Density Estimation) implementation in PyTorch\n223. [VRNN](https://github.com/emited/VariationalRecurrentNeuralNetwork): Pytorch implementation of the Variational RNN (VRNN), from A Recurrent Latent Variable Model for Sequential Data.\n224. [flow](https://github.com/emited/flow): Pytorch implementation of ICLR 2018 paper Deep Learning for Physical Processes: Integrating Prior Scientific Knowledge.\n225. [deepvoice3_pytorch](https://github.com/r9y9/deepvoice3_pytorch): PyTorch implementation of convolutional networks-based text-to-speech synthesis models\n226. [psmm](https://github.com/elanmart/psmm): imlementation of the the Pointer Sentinel Mixture Model, as described in the paper by Stephen Merity et al.\n227. [tacotron2](https://github.com/NVIDIA/tacotron2): Tacotron 2 - PyTorch implementation with faster-than-realtime inference.\n228. [AccSGD](https://github.com/rahulkidambi/AccSGD): Implements pytorch code for the Accelerated SGD algorithm.\n229. [QANet-pytorch](https://github.com/hengruo/QANet-pytorch): an implementation of QANet with PyTorch (EM/F1 = 70.5/77.2 after 20 epoches for about 20 hours on one 1080Ti card.)\n230. [ConvE](https://github.com/TimDettmers/ConvE): Convolutional 2D Knowledge Graph Embeddings\n231. [Structured-Self-Attention](https://github.com/kaushalshetty/Structured-Self-Attention): Implementation for the paper A Structured Self-Attentive Sentence Embedding, which is published in ICLR 2017: arxiv.org/abs/1703.03130 .\n232. [graphsage-simple](https://github.com/williamleif/graphsage-simple): Simple reference implementation of GraphSAGE.\n233. [Detectron.pytorch](https://github.com/roytseng-tw/Detectron.pytorch): A pytorch implementation of Detectron. Both training from scratch and inferring directly from pretrained Detectron weights are available.\n234. [R2Plus1D-PyTorch](https://github.com/irhumshafkat/R2Plus1D-PyTorch): PyTorch implementation of the R2Plus1D convolution based ResNet architecture described in the paper "A Closer Look at Spatiotemporal Convolutions for Action Recognition"\n235. [StackNN](https://github.com/viking-sudo-rm/StackNN): A PyTorch implementation of differentiable stacks for use in neural networks.\n236. [translagent](https://github.com/facebookresearch/translagent): Code for Emergent Translation in Multi-Agent Communication.\n237. [ban-vqa](https://github.com/jnhwkim/ban-vqa): Bilinear attention networks for visual question answering. \n238. [pytorch-openai-transformer-lm](https://github.com/huggingface/pytorch-openai-transformer-lm): This is a PyTorch implementation of the TensorFlow code provided with OpenAI''s paper "Improving Language Understanding by Generative Pre-Training" by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.\n239. [T2F](https://github.com/akanimax/T2F): Text-to-Face generation using Deep Learning. This project combines two of the recent architectures StackGAN and ProGAN for synthesizing faces from textual descriptions.\n240. [pytorch - fid](https://github.com/mseitzer/pytorch-fid): A Port of FrÃ©chet Inception Distance (FID score) to PyTorch\n241. [vae_vpflows](https://github.com/jmtomczak/vae_vpflows):Code in PyTorch for the convex combination linear IAF and the Householder Flow, J.M. Tomczak & M. Welling jmtomczak.github.io/deebmed.html\n242. [CoordConv-pytorch](https://github.com/mkocabas/CoordConv-pytorch): Pytorch implementation of CoordConv introduced in ''An intriguing failing of convolutional neural networks and the CoordConv solution'' paper. (arxiv.org/pdf/1807.03247.pdf)\n243. [SDPoint](https://github.com/xternalz/SDPoint): Implementation of "Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks", published in CVPR 2018. \n244. [SRDenseNet-pytorch](https://github.com/wxywhu/SRDenseNet-pytorch): SRDenseNet-pytorchï¼ˆICCV_2017ï¼‰\n245. [GAN_stability](https://github.com/LMescheder/GAN_stability): Code for paper "Which Training Methods for GANs do actually Converge? (ICML 2018)"\n246. [Mask-RCNN](https://github.com/wannabeOG/Mask-RCNN): A PyTorch implementation of the architecture of Mask RCNN, serves as an introduction to working with PyTorch\n247. [pytorch-coviar](https://github.com/chaoyuaw/pytorch-coviar): Compressed Video Action Recognition\n248. [PNASNet.pytorch](https://github.com/chenxi116/PNASNet.pytorch): PyTorch implementation of PNASNet-5 on ImageNet. \n249. [NALU-pytorch](https://github.com/kevinzakka/NALU-pytorch): Basic pytorch implementation of NAC/NALU from Neural Arithmetic Logic Units arxiv.org/pdf/1808.00508.pdf\n250. [LOLA_DiCE](https://github.com/alexis-jacq/LOLA_DiCE): Pytorch implementation of LOLA (arxiv.org/abs/1709.04326) using DiCE (arxiv.org/abs/1802.05098)\n251. [generative-query-network-pytorch](https://github.com/wohlert/generative-query-network-pytorch): Generative Query Network (GQN) in PyTorch as described in "Neural Scene Representation and Rendering"\n252. [pytorch_hmax](https://github.com/wmvanvliet/pytorch_hmax): Implementation of the HMAX model of vision in PyTorch.\n253. [FCN-pytorch-easiest](https://github.com/yunlongdong/FCN-pytorch-easiest): trying to be the most easiest and just get-to-use pytorch implementation of FCN (Fully Convolotional Networks)\n254. [transducer](https://github.com/awni/transducer): A Fast Sequence Transducer Implementation with PyTorch Bindings.\n255. [AVO-pytorch](https://github.com/artix41/AVO-pytorch): Implementation of Adversarial Variational Optimization in PyTorch.\n256. [HCN-pytorch](https://github.com/huguyuehuhu/HCN-pytorch): A pytorch reimplementation of { Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation }.\n257. [binary-wide-resnet](https://github.com/szagoruyko/binary-wide-resnet): PyTorch implementation of Wide Residual Networks with 1-bit weights by McDonnel (ICLR 2018)\n258. [piggyback](https://github.com/arunmallya/piggyback): Code for Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights arxiv.org/abs/1801.06519\n259. [vid2vid](https://github.com/NVIDIA/vid2vid): Pytorch implementation of our method for high-resolution (e.g. 2048x1024) photorealistic video-to-video translation.\n260. [poisson-convolution-sum](https://github.com/cranmer/poisson-convolution-sum): Implements an infinite sum of poisson-weighted convolutions\n261. [tbd-nets](https://github.com/davidmascharka/tbd-nets): PyTorch implementation of "Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning" arxiv.org/abs/1803.05268 \n262. [attn2d](https://github.com/elbayadm/attn2d): Pervasive Attention: 2D Convolutional Networks for Sequence-to-Sequence Prediction\n263. [yolov3](https://github.com/ultralytics/yolov3): YOLOv3: Training and inference in PyTorch pjreddie.com/darknet/yolo\n264. [deep-dream-in-pytorch](https://github.com/duc0/deep-dream-in-pytorch): Pytorch implementation of the DeepDream computer vision algorithm. \n265. [pytorch-flows](https://github.com/ikostrikov/pytorch-flows): PyTorch implementations of algorithms for density estimation\n266. [quantile-regression-dqn-pytorch](https://github.com/ars-ashuha/quantile-regression-dqn-pytorch): Quantile Regression DQN a Minimal Working Example\n267. [relational-rnn-pytorch](https://github.com/L0SG/relational-rnn-pytorch): An implementation of DeepMind''s Relational Recurrent Neural Networks in PyTorch.\n268. [DEXTR-PyTorch](https://github.com/scaelles/DEXTR-PyTorch): Deep Extreme Cut http://www.vision.ee.ethz.ch/~cvlsegmentation/dextr\n269. [PyTorch_GBW_LM](https://github.com/rdspring1/PyTorch_GBW_LM): PyTorch Language Model for Google Billion Word Dataset.\n270. [Pytorch-NCE](https://github.com/Stonesjtu/Pytorch-NCE): The Noise Contrastive Estimation for softmax output written in Pytorch\n271. [generative-models](https://github.com/shayneobrien/generative-models): Annotated, understandable, and visually interpretable PyTorch implementations of: VAE, BIRVAE, NSGAN, MMGAN, WGAN, WGANGP, LSGAN, DRAGAN, BEGAN, RaGAN, InfoGAN, fGAN, FisherGAN. \n272. [convnet-aig](https://github.com/andreasveit/convnet-aig): PyTorch implementation for Convolutional Networks with Adaptive Inference Graphs.\n273. [integrated-gradient-pytorch](https://github.com/TianhongDai/integrated-gradient-pytorch): This is the pytorch implementation of the paper - Axiomatic Attribution for Deep Networks.\n274. [MalConv-Pytorch](https://github.com/Alexander-H-Liu/MalConv-Pytorch): Pytorch implementation of MalConv. \n275. [trellisnet](https://github.com/locuslab/trellisnet): Trellis Networks for Sequence Modeling\n276. [Learning to Communicate with Deep Multi-Agent Reinforcement Learning](https://github.com/minqi/learning-to-communicate-pytorch): pytorch implementation of  Learning to Communicate with Deep Multi-Agent Reinforcement Learning paper.\n277. [pnn.pytorch](https://github.com/michaelklachko/pnn.pytorch): PyTorch implementation of CVPR''18 - Perturbative Neural Networks http://xujuefei.com/pnn.html.\n278. [Face_Attention_Network](https://github.com/rainofmine/Face_Attention_Network): Pytorch implementation of face attention network as described in Face Attention Network: An Effective Face Detector for the Occluded Faces.\n279. [waveglow](https://github.com/NVIDIA/waveglow): A Flow-based Generative Network for Speech Synthesis.\n280. [deepfloat](https://github.com/facebookresearch/deepfloat): This repository contains the SystemVerilog RTL, C++, HLS (Intel FPGA OpenCL to wrap RTL code) and Python needed to reproduce the numerical results in "Rethinking floating point for deep learning" \n281. [EPSR](https://github.com/subeeshvasu/2018_subeesh_epsr_eccvw): Pytorch implementation of [Analyzing Perception-Distortion Tradeoff using Enhanced Perceptual Super-resolution Network](https://arxiv.org/pdf/1811.00344.pdf). This work has won the first place in PIRM2018-SR competition (region 1) held as part of the ECCV 2018.\n282. [ClariNet](https://github.com/ksw0306/ClariNet): A Pytorch Implementation of ClariNet arxiv.org/abs/1807.07281\n283. [pytorch-pretrained-BERT](https://github.com/huggingface/pytorch-pretrained-BERT): PyTorch version of Google AI''s BERT model with script to load Google''s pre-trained models\n284. [torch_waveglow](https://github.com/npuichigo/waveglow): A PyTorch implementation of the WaveGlow: A Flow-based Generative Network for Speech Synthesis. \n285. [3DDFA](https://github.com/cleardusk/3DDFA): The pytorch improved re-implementation of TPAMI 2017 paper: Face Alignment in Full Pose Range: A 3D Total Solution.\n286. [loss-landscape](https://github.com/tomgoldstein/loss-landscape): loss-landscape Code for visualizing the loss landscape of neural nets.\n287. [famos](https://github.com/zalandoresearch/famos): \nPytorch implementation of the paper "Copy the Old or Paint Anew? An Adversarial Framework for (non-) Parametric Image Stylization" available at http://arxiv.org/abs/1811.09236.\n288. [back2future.pytorch](https://github.com/anuragranj/back2future.pytorch): This is a Pytorch implementation of\nJanai, J., GÃ¼ney, F., Ranjan, A., Black, M. and Geiger, A., Unsupervised Learning of Multi-Frame Optical Flow with Occlusions. ECCV 2018.\n289. [FFTNet](https://github.com/mozilla/FFTNet): Unofficial Implementation of FFTNet vocode paper.\n290. [FaceBoxes.PyTorch](https://github.com/zisianw/FaceBoxes.PyTorch): A PyTorch Implementation of FaceBoxes.\n291. [Transformer-XL](https://github.com/kimiyoung/transformer-xl): Transformer-XL: Attentive Language Models Beyond a Fixed-Length Contexthttps://github.com/kimiyoung/transformer-xl\n292. [associative_compression_networks](https://github.com/jalexvig/associative_compression_networks): Associative Compression Networks for Representation Learning. \n293. [fluidnet_cxx](https://github.com/jolibrain/fluidnet_cxx): FluidNet re-written with ATen tensor lib. \n294. [Deep-Reinforcement-Learning-Algorithms-with-PyTorch](https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch): This repository contains PyTorch implementations of deep reinforcement learning algorithms.\n295. [Shufflenet-v2-Pytorch](https://github.com/ericsun99/Shufflenet-v2-Pytorch): This is a Pytorch implementation of faceplusplus''s ShuffleNet-v2. \n296. [GraphWaveletNeuralNetwork](https://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork): This is a Pytorch implementation of Graph Wavelet Neural Network. ICLR 2019. \n297. [AttentionWalk](https://github.com/benedekrozemberczki/AttentionWalk): This is a Pytorch implementation of Watch Your Step: Learning Node Embeddings via Graph Attention. NIPS 2018.\n298. [SGCN](https://github.com/benedekrozemberczki/SGCN): This is a Pytorch implementation of Signed Graph Convolutional Network. ICDM 2018.\n299. [SINE](https://github.com/benedekrozemberczki/SINE): This is a Pytorch implementation of SINE: Scalable Incomplete Network Embedding. ICDM 2018.\n300. [GAM](https://github.com/benedekrozemberczki/GAM): This is a Pytorch implementation of Graph Classification using Structural Attention. KDD 2018.\n301. [neural-style-pt](https://github.com/ProGamerGov/neural-style-pt): A PyTorch implementation of Justin Johnson''s Neural-style.\n302. [TuckER](https://github.com/ibalazevic/TuckER): TuckER: Tensor Factorization for Knowledge Graph Completion.\n303. [pytorch-prunes](https://github.com/BayesWatch/pytorch-prunes): Pruning neural networks: is it time to nip it in the bud?\n304. [SimGNN](https://github.com/benedekrozemberczki/SimGNN): SimGNN: A Neural Network Approach to Fast Graph Similarity Computation.\n305. [Character CNN](https://github.com/ahmedbesbes/character-based-cnn): PyTorch implementation of the Character-level Convolutional Networks for Text Classification paper. \n306. [XLM](https://github.com/facebookresearch/XLM): PyTorch original implementation of Cross-lingual Language Model Pretraining.\n307. [DiffAI](https://github.com/eth-sri/diffai): A provable defense against adversarial examples and library for building compatible PyTorch models.\n308. [APPNP](https://github.com/benedekrozemberczki/APPNP): Combining Neural Networks with Personalized PageRank for Classification on Graphs. ICLR 2019.\n309. [NGCN](https://github.com/benedekrozemberczki/MixHop-and-N-GCN): A Higher-Order Graph Convolutional Layer. NeurIPS 2018.\n310. [gpt-2-Pytorch](https://github.com/graykode/gpt-2-Pytorch): Simple Text-Generator with OpenAI gpt-2 Pytorch Implementation\n311. [Splitter](https://github.com/benedekrozemberczki/Splitter): Splitter: Learning Node Representations that Capture Multiple Social Contexts. (WWW 2019).\n312. [CapsGNN](https://github.com/benedekrozemberczki/CapsGNN): Capsule Graph Neural Network. (ICLR 2019).\n313. [BigGAN-PyTorch](https://github.com/ajbrock/BigGAN-PyTorch): The author''s officially unofficial PyTorch BigGAN implementation.\n314. [ppo_pytorch_cpp](https://github.com/mhubii/ppo_pytorch_cpp): This is an implementation of the proximal policy optimization algorithm for the C++ API of Pytorch.\n315. [RandWireNN](https://github.com/seungwonpark/RandWireNN): Implementation of: "Exploring Randomly Wired Neural Networks for Image Recognition".\n316. [Zero-shot Intent CapsNet](https://github.com/joel-huang/zeroshot-capsnet-pytorch): GPU-accelerated PyTorch implementation of "Zero-shot User Intent Detection via Capsule Neural Networks".\n317. [SEAL-CI](https://github.com/benedekrozemberczki/SEAL-CI) Semi-Supervised Graph Classification: A Hierarchical Graph Perspective. (WWW 2019).\n318. [MixHop](https://github.com/benedekrozemberczki/MixHop-and-N-GCN): MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing. ICML 2019.\n319. [densebody_pytorch](https://github.com/Lotayou/densebody_pytorch): PyTorch implementation of CloudWalk''s recent paper DenseBody.\n320. [voicefilter](https://github.com/mindslab-ai/voicefilter): Unofficial PyTorch implementation of Google AI''s VoiceFilter system http://swpark.me/voicefilter. \n321. [NVIDIA/semantic-segmentation](https://github.com/NVIDIA/semantic-segmentation): A PyTorch Implementation of [Improving Semantic Segmentation via Video Propagation and Label Relaxation](https://arxiv.org/abs/1812.01593), In CVPR2019. \n322. [ClusterGCN](https://github.com/benedekrozemberczki/ClusterGCN): A PyTorch implementation of "Cluster-GCN: An Efficient Algorithm for Training Deep and Large Graph Convolutional Networks" (KDD 2019).\n323. [NVlabs/DG-Net](https://github.com/NVlabs/DG-Net): A PyTorch implementation of "Joint Discriminative and Generative Learning for Person Re-identification" (CVPR19 Oral). \n324. [NCRF](https://github.com/baidu-research/NCRF): Cancer metastasis detection with neural conditional random field (NCRF)\n325. [pytorch-sift](https://github.com/ducha-aiki/pytorch-sift): PyTorch implementation of SIFT descriptor. \n326. [brain-segmentation-pytorch](https://github.com/mateuszbuda/brain-segmentation-pytorch): U-Net implementation in PyTorch for FLAIR abnormality segmentation in brain MRI. \n327. [glow-pytorch](https://github.com/rosinality/glow-pytorch): PyTorch implementation of Glow, Generative Flow with Invertible 1x1 Convolutions (arxiv.org/abs/1807.03039) \n328. [EfficientNets-PyTorch](https://github.com/zsef123/EfficientNets-PyTorch): A PyT', '{"language":null,"stars":16284,"forks":2828,"watchers":16284,"open_issues":10,"topics":["awesome","awesome-list","computer-vision","cv","data-science","deep-learning","facebook","machine-learning","natural-language-processing","neural-network","nlp","nlp-library","papers","probabilistic-programming","python","pytorch","pytorch-model","pytorch-tutorials","tutorials","utility-library"],"default_branch":"master","size_kb":888,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:pytorch:captum","source_url":"https://github.com/pytorch/captum"},{"type":"has_code","target_id":"github:pytorch:text","source_url":"https://github.com/pytorch/text"},{"type":"has_code","target_id":"github:IBM:pytorch-seq2seq","source_url":"https://github.com/IBM/pytorch-seq2seq"},{"type":"has_code","target_id":"github:Sandeep42:anuvada","source_url":"https://github.com/Sandeep42/anuvada"},{"type":"has_code","target_id":"github:pytorch:audio","source_url":"https://github.com/pytorch/audio"},{"type":"has_code","target_id":"github:facebookresearch:loop","source_url":"https://github.com/facebookresearch/loop"},{"type":"has_code","target_id":"github:facebookresearch:fairseq-py","source_url":"https://github.com/facebookresearch/fairseq-py"},{"type":"has_code","target_id":"github:awni:speech","source_url":"https://github.com/awni/speech"},{"type":"has_code","target_id":"github:OpenNMT:OpenNMT-py","source_url":"https://github.com/OpenNMT/OpenNMT-py"},{"type":"has_code","target_id":"github:huggingface:neuralcoref","source_url":"https://github.com/huggingface/neuralcoref"},{"type":"has_code","target_id":"github:NVIDIA:sentiment-discovery","source_url":"https://github.com/NVIDIA/sentiment-discovery"},{"type":"has_code","target_id":"github:facebookresearch:MUSE","source_url":"https://github.com/facebookresearch/MUSE"},{"type":"has_code","target_id":"github:lium-lst:nmtpytorch","source_url":"https://github.com/lium-lst/nmtpytorch"},{"type":"has_code","target_id":"github:vincentherrmann:pytorch-wavenet","source_url":"https://github.com/vincentherrmann/pytorch-wavenet"},{"type":"has_code","target_id":"github:soobinseo:Tacotron-pytorch","source_url":"https://github.com/soobinseo/Tacotron-pytorch"},{"type":"has_code","target_id":"github:allenai:allennlp","source_url":"https://github.com/allenai/allennlp"},{"type":"has_code","target_id":"github:PetrochukM:PyTorch-NLP","source_url":"https://github.com/PetrochukM/PyTorch-NLP"},{"type":"has_code","target_id":"github:outcastofmusic:quick-nlp","source_url":"https://github.com/outcastofmusic/quick-nlp"},{"type":"has_code","target_id":"github:mozilla:TTS","source_url":"https://github.com/mozilla/TTS"},{"type":"has_code","target_id":"github:facebookresearch:LASER","source_url":"https://github.com/facebookresearch/LASER"},{"type":"has_code","target_id":"github:pyannote:pyannote-audio","source_url":"https://github.com/pyannote/pyannote-audio"},{"type":"has_code","target_id":"github:Maluuba:gensen","source_url":"https://github.com/Maluuba/gensen"},{"type":"has_code","target_id":"github:pytorch:translate","source_url":"https://github.com/pytorch/translate"},{"type":"has_code","target_id":"github:espnet:espnet","source_url":"https://github.com/espnet/espnet"},{"type":"has_code","target_id":"github:facebookresearch:pythia","source_url":"https://github.com/facebookresearch/pythia"},{"type":"has_code","target_id":"github:facebookresearch:UnsupervisedMT","source_url":"https://github.com/facebookresearch/UnsupervisedMT"},{"type":"has_code","target_id":"github:jsalt18-sentence-repl:jiant","source_url":"https://github.com/jsalt18-sentence-repl/jiant"},{"type":"has_code","target_id":"github:codertimo:BERT-pytorch","source_url":"https://github.com/codertimo/BERT-pytorch"},{"type":"has_code","target_id":"github:facebookresearch:InferSent","source_url":"https://github.com/facebookresearch/InferSent"},{"type":"has_code","target_id":"github:google:uis-rnn","source_url":"https://github.com/google/uis-rnn"},{"type":"has_code","target_id":"github:zalandoresearch:flair","source_url":"https://github.com/zalandoresearch/flair"},{"type":"has_code","target_id":"github:facebookresearch:pytext","source_url":"https://github.com/facebookresearch/pytext"},{"type":"has_code","target_id":"github:mindslab-ai:voicefilter","source_url":"https://github.com/mindslab-ai/voicefilter"},{"type":"has_code","target_id":"github:kamalkraj:BERT-NER","source_url":"https://github.com/kamalkraj/BERT-NER"},{"type":"has_code","target_id":"github:feedly:transfer-nlp","source_url":"https://github.com/feedly/transfer-nlp"},{"type":"has_code","target_id":"github:asyml:texar-pytorch","source_url":"https://github.com/asyml/texar-pytorch"},{"type":"has_code","target_id":"github:mravanelli:pytorch-kaldi","source_url":"https://github.com/mravanelli/pytorch-kaldi"},{"type":"has_code","target_id":"github:NVIDIA:NeMo","source_url":"https://github.com/NVIDIA/NeMo"},{"type":"has_code","target_id":"github:harvardnlp:pytorch-struct","source_url":"https://github.com/harvardnlp/pytorch-struct"},{"type":"has_code","target_id":"github:freewym:espresso","source_url":"https://github.com/freewym/espresso"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:lucidrains:reformer-pytorch","source_url":"https://github.com/lucidrains/reformer-pytorch"},{"type":"has_code","target_id":"github:enochkan:torch-metrics","source_url":"https://github.com/enochkan/torch-metrics"},{"type":"has_code","target_id":"github:speechbrain:speechbrain","source_url":"https://github.com/speechbrain/speechbrain"},{"type":"has_code","target_id":"github:backprop-ai:backprop","source_url":"https://github.com/backprop-ai/backprop"},{"type":"has_code","target_id":"github:pytorch:vision","source_url":"https://github.com/pytorch/vision"},{"type":"has_code","target_id":"github:tymokvo:pt-styletransfer","source_url":"https://github.com/tymokvo/pt-styletransfer"},{"type":"has_code","target_id":"github:thnkim:OpenFacePytorch","source_url":"https://github.com/thnkim/OpenFacePytorch"},{"type":"has_code","target_id":"github:felixgwu:img_classification_pk_pytorch","source_url":"https://github.com/felixgwu/img_classification_pk_pytorch"},{"type":"has_code","target_id":"github:facebookresearch:SparseConvNet","source_url":"https://github.com/facebookresearch/SparseConvNet"},{"type":"has_code","target_id":"github:automan000:Convolution_LSTM_pytorch","source_url":"https://github.com/automan000/Convolution_LSTM_pytorch"},{"type":"has_code","target_id":"github:1adrianb:face-alignment","source_url":"https://github.com/1adrianb/face-alignment"},{"type":"has_code","target_id":"github:ZijunDeng:pytorch-semantic-segmentation","source_url":"https://github.com/ZijunDeng/pytorch-semantic-segmentation"},{"type":"has_code","target_id":"github:longcw:RoIAlign.pytorch","source_url":"https://github.com/longcw/RoIAlign.pytorch"},{"type":"has_code","target_id":"github:creafz:pytorch-cnn-finetune","source_url":"https://github.com/creafz/pytorch-cnn-finetune"},{"type":"has_code","target_id":"github:ignacio-rocco:detectorch","source_url":"https://github.com/ignacio-rocco/detectorch"},{"type":"has_code","target_id":"github:mdbloice:Augmentor","source_url":"https://github.com/mdbloice/Augmentor"},{"type":"has_code","target_id":"github:jonas-koehler:s2cnn","source_url":"https://github.com/jonas-koehler/s2cnn"},{"type":"has_code","target_id":"github:donnyyou:torchcv","source_url":"https://github.com/donnyyou/torchcv"},{"type":"has_code","target_id":"github:facebookresearch:maskrcnn-benchmark","source_url":"https://github.com/facebookresearch/maskrcnn-benchmark"},{"type":"has_code","target_id":"github:osmr:imgclsmob","source_url":"https://github.com/osmr/imgclsmob"},{"type":"has_code","target_id":"github:perone:medicaltorch","source_url":"https://github.com/perone/medicaltorch"},{"type":"has_code","target_id":"github:albu:albumentations","source_url":"https://github.com/albu/albumentations"},{"type":"has_code","target_id":"github:arraiyopensource:kornia","source_url":"https://github.com/arraiyopensource/kornia"},{"type":"has_code","target_id":"github:s3nh:pytorch-text-recognition","source_url":"https://github.com/s3nh/pytorch-text-recognition"},{"type":"has_code","target_id":"github:timesler:facenet-pytorch","source_url":"https://github.com/timesler/facenet-pytorch"},{"type":"has_code","target_id":"github:facebookresearch:detectron2","source_url":"https://github.com/facebookresearch/detectron2"},{"type":"has_code","target_id":"github:Media-Smart:vedaseg","source_url":"https://github.com/Media-Smart/vedaseg"},{"type":"has_code","target_id":"github:facebookresearch:ClassyVision","source_url":"https://github.com/facebookresearch/ClassyVision"},{"type":"has_code","target_id":"github:alankbi:detecto","source_url":"https://github.com/alankbi/detecto"},{"type":"has_code","target_id":"github:facebookresearch:pytorch3d","source_url":"https://github.com/facebookresearch/pytorch3d"},{"type":"has_code","target_id":"github:open-mmlab:mmdetection","source_url":"https://github.com/open-mmlab/mmdetection"},{"type":"has_code","target_id":"github:ProGamerGov:neural-dream","source_url":"https://github.com/ProGamerGov/neural-dream"},{"type":"has_code","target_id":"github:MisaOgura:flashtorch","source_url":"https://github.com/MisaOgura/flashtorch"},{"type":"has_code","target_id":"github:greentfrapp:lucent","source_url":"https://github.com/greentfrapp/lucent"},{"type":"has_code","target_id":"github:open-mmlab:mmdetection3d","source_url":"https://github.com/open-mmlab/mmdetection3d"},{"type":"has_code","target_id":"github:open-mmlab:mmsegmentation","source_url":"https://github.com/open-mmlab/mmsegmentation"},{"type":"has_code","target_id":"github:open-mmlab:mmediting","source_url":"https://github.com/open-mmlab/mmediting"},{"type":"has_code","target_id":"github:open-mmlab:mmaction2","source_url":"https://github.com/open-mmlab/mmaction2"},{"type":"has_code","target_id":"github:open-mmlab:mmpose","source_url":"https://github.com/open-mmlab/mmpose"},{"type":"has_code","target_id":"github:lightly-ai:lightly","source_url":"https://github.com/lightly-ai/lightly"},{"type":"has_code","target_id":"github:stepelu:ptstat","source_url":"https://github.com/stepelu/ptstat"},{"type":"has_code","target_id":"github:uber:pyro","source_url":"https://github.com/uber/pyro"},{"type":"has_code","target_id":"github:probtorch:probtorch","source_url":"https://github.com/probtorch/probtorch"},{"type":"has_code","target_id":"github:drckf:paysage","source_url":"https://github.com/drckf/paysage"},{"type":"has_code","target_id":"github:ctallec:pyvarinf","source_url":"https://github.com/ctallec/pyvarinf"},{"type":"has_code","target_id":"github:probprog:pyprob","source_url":"https://github.com/probprog/pyprob"},{"type":"has_code","target_id":"github:spring-epfl:mia","source_url":"https://github.com/spring-epfl/mia"},{"type":"has_code","target_id":"github:akanimax:pro_gan_pytorch","source_url":"https://github.com/akanimax/pro_gan_pytorch"},{"type":"has_code","target_id":"github:pytorch:botorch","source_url":"https://github.com/pytorch/botorch"},{"type":"has_code","target_id":"github:mrdrozdov:pytorch-extras","source_url":"https://github.com/mrdrozdov/pytorch-extras"},{"type":"has_code","target_id":"github:szagoruyko:functional-zoo","source_url":"https://github.com/szagoruyko/functional-zoo"},{"type":"has_code","target_id":"github:ncullen93:torchsample","source_url":"https://github.com/ncullen93/torchsample"},{"type":"has_code","target_id":"github:deepcraft:torchcraft-py","source_url":"https://github.com/deepcraft/torchcraft-py"},{"type":"has_code","target_id":"github:ramon-oliveira:aorun","source_url":"https://github.com/ramon-oliveira/aorun"},{"type":"has_code","target_id":"github:oval-group:logger","source_url":"https://github.com/oval-group/logger"},{"type":"has_code","target_id":"github:iamaziz:PyTorch-docset","source_url":"https://github.com/iamaziz/PyTorch-docset"},{"type":"has_code","target_id":"github:clcarwin:convert_torch_to_pytorch","source_url":"https://github.com/clcarwin/convert_torch_to_pytorch"},{"type":"has_code","target_id":"github:Cadene:pretrained-models.pytorch","source_url":"https://github.com/Cadene/pretrained-models.pytorch"},{"type":"has_code","target_id":"github:locuslab:pytorch_fft","source_url":"https://github.com/locuslab/pytorch_fft"},{"type":"has_code","target_id":"github:fanq15:caffe_to_torch_to_pytorch","source_url":"https://github.com/fanq15/caffe_to_torch_to_pytorch"},{"type":"has_code","target_id":"github:sniklaus:pytorch-extension","source_url":"https://github.com/sniklaus/pytorch-extension"},{"type":"has_code","target_id":"github:lanpa:tensorboard-pytorch","source_url":"https://github.com/lanpa/tensorboard-pytorch"},{"type":"has_code","target_id":"github:jrg365:gpytorch","source_url":"https://github.com/jrg365/gpytorch"},{"type":"has_code","target_id":"github:maciejkula:spotlight","source_url":"https://github.com/maciejkula/spotlight"},{"type":"has_code","target_id":"github:awentzonline:pytorch-cns","source_url":"https://github.com/awentzonline/pytorch-cns"},{"type":"has_code","target_id":"github:szagoruyko:pyinn","source_url":"https://github.com/szagoruyko/pyinn"},{"type":"has_code","target_id":"github:nasimrahaman:inferno","source_url":"https://github.com/nasimrahaman/inferno"},{"type":"has_code","target_id":"github:henryre:pytorch-fitmodule","source_url":"https://github.com/henryre/pytorch-fitmodule"},{"type":"has_code","target_id":"github:dnouri:inferno","source_url":"https://github.com/dnouri/inferno"},{"type":"has_code","target_id":"github:marvis:pytorch-caffe-darknet-convert","source_url":"https://github.com/marvis/pytorch-caffe-darknet-convert"},{"type":"has_code","target_id":"github:longcw:pytorch2caffe","source_url":"https://github.com/longcw/pytorch2caffe"},{"type":"has_code","target_id":"github:nearai:pytorch-tools","source_url":"https://github.com/nearai/pytorch-tools"},{"type":"has_code","target_id":"github:taolei87:sru","source_url":"https://github.com/taolei87/sru"},{"type":"has_code","target_id":"github:prisma-ai:torch2coreml","source_url":"https://github.com/prisma-ai/torch2coreml"},{"type":"has_code","target_id":"github:zhanghang1989:PyTorch-Encoding","source_url":"https://github.com/zhanghang1989/PyTorch-Encoding"},{"type":"has_code","target_id":"github:ryanleary:pytorch-ctc","source_url":"https://github.com/ryanleary/pytorch-ctc"},{"type":"has_code","target_id":"github:t-vi:candlegp","source_url":"https://github.com/t-vi/candlegp"},{"type":"has_code","target_id":"github:loudinthecloud:dpwa","source_url":"https://github.com/loudinthecloud/dpwa"},{"type":"has_code","target_id":"github:koz4k:dni-pytorch","source_url":"https://github.com/koz4k/dni-pytorch"},{"type":"has_code","target_id":"github:dnouri:skorch","source_url":"https://github.com/dnouri/skorch"},{"type":"has_code","target_id":"github:pytorch:ignite","source_url":"https://github.com/pytorch/ignite"},{"type":"has_code","target_id":"github:glample:Arnold","source_url":"https://github.com/glample/Arnold"},{"type":"has_code","target_id":"github:albanie:pytorch-mcn","source_url":"https://github.com/albanie/pytorch-mcn"},{"type":"has_code","target_id":"github:chenyuntc:simple-faster-rcnn-pytorch","source_url":"https://github.com/chenyuntc/simple-faster-rcnn-pytorch"},{"type":"has_code","target_id":"github:DL-IT:generative_zoo","source_url":"https://github.com/DL-IT/generative_zoo"},{"type":"has_code","target_id":"github:szagoruyko:pytorchviz","source_url":"https://github.com/szagoruyko/pytorchviz"},{"type":"has_code","target_id":"github:cogitare-ai:cogitare","source_url":"https://github.com/cogitare-ai/cogitare"},{"type":"has_code","target_id":"github:dmarnerides:pydlt","source_url":"https://github.com/dmarnerides/pydlt"},{"type":"has_code","target_id":"github:wohlert:semi-supervised-pytorch","source_url":"https://github.com/wohlert/semi-supervised-pytorch"},{"type":"has_code","target_id":"github:rusty1s:pytorch_cluster","source_url":"https://github.com/rusty1s/pytorch_cluster"},{"type":"has_code","target_id":"github:aditya-khant:neural-assembly-compiler","source_url":"https://github.com/aditya-khant/neural-assembly-compiler"},{"type":"has_code","target_id":"github:vadimkantorov:caffemodel2pytorch","source_url":"https://github.com/vadimkantorov/caffemodel2pytorch"},{"type":"has_code","target_id":"github:pytorch:extension-cpp","source_url":"https://github.com/pytorch/extension-cpp"},{"type":"has_code","target_id":"github:GRAAL-Research:pytoune","source_url":"https://github.com/GRAAL-Research/pytoune"},{"type":"has_code","target_id":"github:dusty-nv:jetson-reinforcement","source_url":"https://github.com/dusty-nv/jetson-reinforcement"},{"type":"has_code","target_id":"github:salesforce:matchbox","source_url":"https://github.com/salesforce/matchbox"},{"type":"has_code","target_id":"github:josipd:torch-two-sample","source_url":"https://github.com/josipd/torch-two-sample"},{"type":"has_code","target_id":"github:sksq96:pytorch-summary","source_url":"https://github.com/sksq96/pytorch-summary"},{"type":"has_code","target_id":"github:BelBES:mpl.pytorch","source_url":"https://github.com/BelBES/mpl.pytorch"},{"type":"has_code","target_id":"github:YosefLab:scVI-dev","source_url":"https://github.com/YosefLab/scVI-dev"},{"type":"has_code","target_id":"github:NVIDIA:apex","source_url":"https://github.com/NVIDIA/apex"},{"type":"has_code","target_id":"github:pytorch:ELF","source_url":"https://github.com/pytorch/ELF"},{"type":"has_code","target_id":"github:EKami:Torchlite","source_url":"https://github.com/EKami/Torchlite"},{"type":"has_code","target_id":"github:Schlumberger:joint-vae","source_url":"https://github.com/Schlumberger/joint-vae"},{"type":"has_code","target_id":"github:kengz:SLM-Lab","source_url":"https://github.com/kengz/SLM-Lab"},{"type":"has_code","target_id":"github:Hananel-Hazan:bindsnet","source_url":"https://github.com/Hananel-Hazan/bindsnet"},{"type":"has_code","target_id":"github:akanimax:pro_gan_pytorch","source_url":"https://github.com/akanimax/pro_gan_pytorch"},{"type":"has_code","target_id":"github:rusty1s:pytorch_geometric","source_url":"https://github.com/rusty1s/pytorch_geometric"},{"type":"has_code","target_id":"github:knighton:torchplus","source_url":"https://github.com/knighton/torchplus"},{"type":"has_code","target_id":"github:zuoxingdong:lagom","source_url":"https://github.com/zuoxingdong/lagom"},{"type":"has_code","target_id":"github:ecs-vlc:torchbearer","source_url":"https://github.com/ecs-vlc/torchbearer"},{"type":"has_code","target_id":"github:tristandeleu:pytorch-maml-rl","source_url":"https://github.com/tristandeleu/pytorch-maml-rl"},{"type":"has_code","target_id":"github:bharathgs:NALU","source_url":"https://github.com/bharathgs/NALU"},{"type":"has_code","target_id":"github:PIQuIL:QuCumber","source_url":"https://github.com/PIQuIL/QuCumber"},{"type":"has_code","target_id":"github:MagNet-DL:magnet","source_url":"https://github.com/MagNet-DL/magnet"},{"type":"has_code","target_id":"github:jbohnslav:opencv_transforms","source_url":"https://github.com/jbohnslav/opencv_transforms"},{"type":"has_code","target_id":"github:fastai:fastai","source_url":"https://github.com/fastai/fastai"},{"type":"has_code","target_id":"github:RobotLocomotion:pytorch-dense-correspondence","source_url":"https://github.com/RobotLocomotion/pytorch-dense-correspondence"},{"type":"has_code","target_id":"github:richzhang:colorization-pytorch","source_url":"https://github.com/richzhang/colorization-pytorch"},{"type":"has_code","target_id":"github:cms-flash:beauty-net","source_url":"https://github.com/cms-flash/beauty-net"},{"type":"has_code","target_id":"github:Mariewelt:OpenChem","source_url":"https://github.com/Mariewelt/OpenChem"},{"type":"has_code","target_id":"github:aiqm:torchani","source_url":"https://github.com/aiqm/torchani"},{"type":"has_code","target_id":"github:hjmshi:PyTorch-LBFGS","source_url":"https://github.com/hjmshi/PyTorch-LBFGS"},{"type":"has_code","target_id":"github:cornellius-gp:gpytorch","source_url":"https://github.com/cornellius-gp/gpytorch"},{"type":"has_code","target_id":"github:mariogeiger:hessian","source_url":"https://github.com/mariogeiger/hessian"},{"type":"has_code","target_id":"github:MillionIntegrals:vel","source_url":"https://github.com/MillionIntegrals/vel"},{"type":"has_code","target_id":"github:msamogh:nonechucks","source_url":"https://github.com/msamogh/nonechucks"},{"type":"has_code","target_id":"github:Swall0w:torchstat","source_url":"https://github.com/Swall0w/torchstat"},{"type":"has_code","target_id":"github:pytorch:QNNPACK","source_url":"https://github.com/pytorch/QNNPACK"},{"type":"has_code","target_id":"github:rtqichen:torchdiffeq","source_url":"https://github.com/rtqichen/torchdiffeq"},{"type":"has_code","target_id":"github:BachiLi:redner","source_url":"https://github.com/BachiLi/redner"},{"type":"has_code","target_id":"github:masa-su:pixyz","source_url":"https://github.com/masa-su/pixyz"},{"type":"has_code","target_id":"github:perone:euclidesdb","source_url":"https://github.com/perone/euclidesdb"},{"type":"has_code","target_id":"github:nerox8664:pytorch2keras","source_url":"https://github.com/nerox8664/pytorch2keras"},{"type":"has_code","target_id":"github:domainadaptation:salad","source_url":"https://github.com/domainadaptation/salad"},{"type":"has_code","target_id":"github:Erotemic:netharn","source_url":"https://github.com/Erotemic/netharn"},{"type":"has_code","target_id":"github:dmlc:dgl","source_url":"https://github.com/dmlc/dgl"},{"type":"has_code","target_id":"github:CSAILVision:gandissect","source_url":"https://github.com/CSAILVision/gandissect"},{"type":"has_code","target_id":"github:justusschock:delira","source_url":"https://github.com/justusschock/delira"},{"type":"has_code","target_id":"github:AIRLab-POLIMI:mushroom","source_url":"https://github.com/AIRLab-POLIMI/mushroom"},{"type":"has_code","target_id":"github:thuml:Xlearn","source_url":"https://github.com/thuml/Xlearn"},{"type":"has_code","target_id":"github:ferrine:geoopt","source_url":"https://github.com/ferrine/geoopt"},{"type":"has_code","target_id":"github:unit8co:vegans","source_url":"https://github.com/unit8co/vegans"},{"type":"has_code","target_id":"github:arraiyopensource:torchgeometry","source_url":"https://github.com/arraiyopensource/torchgeometry"},{"type":"has_code","target_id":"github:BorealisAI:advertorch","source_url":"https://github.com/BorealisAI/advertorch"},{"type":"has_code","target_id":"github:Luolc:AdaBound","source_url":"https://github.com/Luolc/AdaBound"},{"type":"has_code","target_id":"github:mblondel:fenchel-young-losses","source_url":"https://github.com/mblondel/fenchel-young-losses"},{"type":"has_code","target_id":"github:Lyken17:pytorch-OpCounter","source_url":"https://github.com/Lyken17/pytorch-OpCounter"},{"type":"has_code","target_id":"github:kaihsin:Tor10","source_url":"https://github.com/kaihsin/Tor10"},{"type":"has_code","target_id":"github:catalyst-team:catalyst","source_url":"https://github.com/catalyst-team/catalyst"},{"type":"has_code","target_id":"github:facebook:Ax","source_url":"https://github.com/facebook/Ax"},{"type":"has_code","target_id":"github:achaiah:pywick","source_url":"https://github.com/achaiah/pywick"},{"type":"has_code","target_id":"github:kakaobrain:torchgpipe","source_url":"https://github.com/kakaobrain/torchgpipe"},{"type":"has_code","target_id":"github:pytorch:hub","source_url":"https://github.com/pytorch/hub"},{"type":"has_code","target_id":"github:williamFalcon:pytorch-lightning","source_url":"https://github.com/williamFalcon/pytorch-lightning"},{"type":"has_code","target_id":"github:kaihsin:Tor10","source_url":"https://github.com/kaihsin/Tor10"},{"type":"has_code","target_id":"github:microsoft:tensorwatch","source_url":"https://github.com/microsoft/tensorwatch"},{"type":"has_code","target_id":"github:fancompute:wavetorch","source_url":"https://github.com/fancompute/wavetorch"},{"type":"has_code","target_id":"github:ag14774:diffdist","source_url":"https://github.com/ag14774/diffdist"},{"type":"has_code","target_id":"github:awwong1:torchprof","source_url":"https://github.com/awwong1/torchprof"},{"type":"has_code","target_id":"github:oxfordcontrol:osqpth","source_url":"https://github.com/oxfordcontrol/osqpth"},{"type":"has_code","target_id":"github:mctorch:mctorch","source_url":"https://github.com/mctorch/mctorch"},{"type":"has_code","target_id":"github:noahgolmant:pytorch-hessian-eigenthings","source_url":"https://github.com/noahgolmant/pytorch-hessian-eigenthings"},{"type":"has_code","target_id":"github:StanfordVL:MinkowskiEngine","source_url":"https://github.com/StanfordVL/MinkowskiEngine"},{"type":"has_code","target_id":"github:Omegastick:pytorch-cpp-rl","source_url":"https://github.com/Omegastick/pytorch-cpp-rl"},{"type":"has_code","target_id":"github:BloodAxe:pytorch-toolbelt","source_url":"https://github.com/BloodAxe/pytorch-toolbelt"},{"type":"has_code","target_id":"github:Fonbet:argus-tensor-stream","source_url":"https://github.com/Fonbet/argus-tensor-stream"},{"type":"has_code","target_id":"github:hal3:macarico","source_url":"https://github.com/hal3/macarico"},{"type":"has_code","target_id":"github:astooke:rlpyt","source_url":"https://github.com/astooke/rlpyt"},{"type":"has_code","target_id":"github:blue-season:pywarm","source_url":"https://github.com/blue-season/pywarm"},{"type":"has_code","target_id":"github:learnables:learn2learn","source_url":"https://github.com/learnables/learn2learn"},{"type":"has_code","target_id":"github:facebookresearch:torchbeast","source_url":"https://github.com/facebookresearch/torchbeast"},{"type":"has_code","target_id":"github:facebookresearch:higher","source_url":"https://github.com/facebookresearch/higher"},{"type":"has_code","target_id":"github:Vermeille:Torchelie","source_url":"https://github.com/Vermeille/Torchelie"},{"type":"has_code","target_id":"github:facebookresearch:CrypTen","source_url":"https://github.com/facebookresearch/CrypTen"},{"type":"has_code","target_id":"github:cvxgrp:cvxpylayers","source_url":"https://github.com/cvxgrp/cvxpylayers"},{"type":"has_code","target_id":"github:HobbitLong:RepDistiller","source_url":"https://github.com/HobbitLong/RepDistiller"},{"type":"has_code","target_id":"github:NVIDIAGameWorks:kaolin","source_url":"https://github.com/NVIDIAGameWorks/kaolin"},{"type":"has_code","target_id":"github:BasBuller:PySNN","source_url":"https://github.com/BasBuller/PySNN"},{"type":"has_code","target_id":"github:dmmiller612:sparktorch","source_url":"https://github.com/dmmiller612/sparktorch"},{"type":"has_code","target_id":"github:KevinMusgrave:pytorch-metric-learning","source_url":"https://github.com/KevinMusgrave/pytorch-metric-learning"},{"type":"has_code","target_id":"github:cpnota:autonomous-learning-library","source_url":"https://github.com/cpnota/autonomous-learning-library"},{"type":"has_code","target_id":"github:asappresearch:flambe","source_url":"https://github.com/asappresearch/flambe"},{"type":"has_code","target_id":"github:jettify:pytorch-optimizer","source_url":"https://github.com/jettify/pytorch-optimizer"},{"type":"has_code","target_id":"github:AntixK:PyTorch-VAE","source_url":"https://github.com/AntixK/PyTorch-VAE"},{"type":"has_code","target_id":"github:ray-project:ray","source_url":"https://github.com/ray-project/ray"},{"type":"has_code","target_id":"github:benedekrozemberczki:pytorch_geometric_temporal","source_url":"https://github.com/benedekrozemberczki/pytorch_geometric_temporal"},{"type":"has_code","target_id":"github:GRAAL-Research:poutyne","source_url":"https://github.com/GRAAL-Research/poutyne"},{"type":"has_code","target_id":"github:PistonY:torch-toolbox","source_url":"https://github.com/PistonY/torch-toolbox"},{"type":"has_code","target_id":"github:pytorch:contrib","source_url":"https://github.com/pytorch/contrib"},{"type":"has_code","target_id":"github:lukemelas:EfficientNet-PyTorch","source_url":"https://github.com/lukemelas/EfficientNet-PyTorch"},{"type":"has_code","target_id":"github:pytorch:xla","source_url":"https://github.com/pytorch/xla"},{"type":"has_code","target_id":"github:tmbdev:webdataset","source_url":"https://github.com/tmbdev/webdataset"},{"type":"has_code","target_id":"github:Media-Smart:volksdep","source_url":"https://github.com/Media-Smart/volksdep"},{"type":"has_code","target_id":"github:POSTECH-CVLab:PyTorch-StudioGAN","source_url":"https://github.com/POSTECH-CVLab/PyTorch-StudioGAN"},{"type":"has_code","target_id":"github:torchdrift:torchdrift","source_url":"https://github.com/torchdrift/torchdrift"},{"type":"has_code","target_id":"github:huggingface:accelerate","source_url":"https://github.com/huggingface/accelerate"},{"type":"has_code","target_id":"github:PyTorchLightning:lightning-transformers","source_url":"https://github.com/PyTorchLightning/lightning-transformers"},{"type":"has_code","target_id":"github:PyTorchLightning:lightning-flash","source_url":"https://github.com/PyTorchLightning/lightning-flash"},{"type":"has_code","target_id":"github:SherylHYX:pytorch_geometric_signed_directed","source_url":"https://github.com/SherylHYX/pytorch_geometric_signed_directed"},{"type":"has_code","target_id":"github:rentruewang:koila","source_url":"https://github.com/rentruewang/koila"},{"type":"has_code","target_id":"github:awslabs:renate","source_url":"https://github.com/awslabs/renate"},{"type":"has_code","target_id":"github:spro:practical-pytorch","source_url":"https://github.com/spro/practical-pytorch"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:keon:pytorch-exercises","source_url":"https://github.com/keon/pytorch-exercises"},{"type":"has_code","target_id":"github:pytorch:tutorials","source_url":"https://github.com/pytorch/tutorials"},{"type":"has_code","target_id":"github:pytorch:examples","source_url":"https://github.com/pytorch/examples"},{"type":"has_code","target_id":"github:napsternxg:pytorch-practice","source_url":"https://github.com/napsternxg/pytorch-practice"},{"type":"has_code","target_id":"github:vinhkhuc:PyTorch-Mini-Tutorials","source_url":"https://github.com/vinhkhuc/PyTorch-Mini-Tutorials"},{"type":"has_code","target_id":"github:xiayandi:Pytorch_text_classification","source_url":"https://github.com/xiayandi/Pytorch_text_classification"},{"type":"has_code","target_id":"github:desimone:pytorch-cat-vs-dogs","source_url":"https://github.com/desimone/pytorch-cat-vs-dogs"},{"type":"has_code","target_id":"github:eladhoffer:convNet.pytorch","source_url":"https://github.com/eladhoffer/convNet.pytorch"},{"type":"has_code","target_id":"github:mailmahee:pytorch-generative-adversarial-networks","source_url":"https://github.com/mailmahee/pytorch-generative-adversarial-networks"},{"type":"has_code","target_id":"github:amdegroot:pytorch-containers","source_url":"https://github.com/amdegroot/pytorch-containers"},{"type":"has_code","target_id":"github:cemoody:topicsne","source_url":"https://github.com/cemoody/topicsne"},{"type":"has_code","target_id":"github:fducau:AAE_pytorch","source_url":"https://github.com/fducau/AAE_pytorch"},{"type":"has_code","target_id":"github:GunhoChoi:Kind_PyTorch_Tutorial","source_url":"https://github.com/GunhoChoi/Kind_PyTorch_Tutorial"},{"type":"has_code","target_id":"github:justdark:pytorch-poetry-gen","source_url":"https://github.com/justdark/pytorch-poetry-gen"},{"type":"has_code","target_id":"github:JamesChuanggg:pytorch-REINFORCE","source_url":"https://github.com/JamesChuanggg/pytorch-REINFORCE"},{"type":"has_code","target_id":"github:MorvanZhou:PyTorch-Tutorial","source_url":"https://github.com/MorvanZhou/PyTorch-Tutorial"},{"type":"has_code","target_id":"github:joansj:pytorch-intro","source_url":"https://github.com/joansj/pytorch-intro"},{"type":"has_code","target_id":"github:bearpaw:pytorch-classification","source_url":"https://github.com/bearpaw/pytorch-classification"},{"type":"has_code","target_id":"github:hardmaru:pytorch_notebooks","source_url":"https://github.com/hardmaru/pytorch_notebooks"},{"type":"has_code","target_id":"github:soravux:pytorch_tutorial","source_url":"https://github.com/soravux/pytorch_tutorial"},{"type":"has_code","target_id":"github:Spandan-Madan:Pytorch_fine_tuning_Tutorial","source_url":"https://github.com/Spandan-Madan/Pytorch_fine_tuning_Tutorial"},{"type":"has_code","target_id":"github:Kyubyong:pytorch_exercises","source_url":"https://github.com/Kyubyong/pytorch_exercises"},{"type":"has_code","target_id":"github:soumith:traffic-sign-detection-homework","source_url":"https://github.com/soumith/traffic-sign-detection-homework"},{"type":"has_code","target_id":"github:Js-Mim:mss_pytorch","source_url":"https://github.com/Js-Mim/mss_pytorch"},{"type":"has_code","target_id":"github:DSKSD:DeepNLP-models-Pytorch","source_url":"https://github.com/DSKSD/DeepNLP-models-Pytorch"},{"type":"has_code","target_id":"github:mila-udem:welcome_tutorials","source_url":"https://github.com/mila-udem/welcome_tutorials"},{"type":"has_code","target_id":"github:moskomule:pytorch.rl.learning","source_url":"https://github.com/moskomule/pytorch.rl.learning"},{"type":"has_code","target_id":"github:keon:seq2seq","source_url":"https://github.com/keon/seq2seq"},{"type":"has_code","target_id":"github:JeanKossaifi:tensorly-notebooks","source_url":"https://github.com/JeanKossaifi/tensorly-notebooks"},{"type":"has_code","target_id":"github:jpeg729:pytorch_bits","source_url":"https://github.com/jpeg729/pytorch_bits"},{"type":"has_code","target_id":"github:sanyam5:skip-thoughts","source_url":"https://github.com/sanyam5/skip-thoughts"},{"type":"has_code","target_id":"github:xiadingZ:video-caption-pytorch","source_url":"https://github.com/xiadingZ/video-caption-pytorch"},{"type":"has_code","target_id":"github:higgsfield:Capsule-Network-Tutorial","source_url":"https://github.com/higgsfield/Capsule-Network-Tutorial"},{"type":"has_code","target_id":"github:SherlockLiao:code-of-learn-deep-learning-with-pytorch","source_url":"https://github.com/SherlockLiao/code-of-learn-deep-learning-with-pytorch"},{"type":"has_code","target_id":"github:higgsfield:RL-Adventure","source_url":"https://github.com/higgsfield/RL-Adventure"},{"type":"has_code","target_id":"github:hpcgarage:accelerated_dl_pytorch","source_url":"https://github.com/hpcgarage/accelerated_dl_pytorch"},{"type":"has_code","target_id":"github:higgsfield:RL-Adventure-2","source_url":"https://github.com/higgsfield/RL-Adventure-2"},{"type":"has_code","target_id":"github:wkentaro:pytorch-for-numpy-users","source_url":"https://github.com/wkentaro/pytorch-for-numpy-users"},{"type":"has_code","target_id":"github:Kaixhin:grokking-pytorch","source_url":"https://github.com/Kaixhin/grokking-pytorch"},{"type":"has_code","target_id":"github:Atcold:PyTorch-Deep-Learning-Minicourse","source_url":"https://github.com/Atcold/PyTorch-Deep-Learning-Minicourse"},{"type":"has_code","target_id":"github:utkuozbulak:pytorch-custom-dataset-examples","source_url":"https://github.com/utkuozbulak/pytorch-custom-dataset-examples"},{"type":"has_code","target_id":"github:furkanu:deeplearning.ai-pytorch","source_url":"https://github.com/furkanu/deeplearning.ai-pytorch"},{"type":"has_code","target_id":"github:tobiascz:MNIST_Pytorch_python_and_capi","source_url":"https://github.com/tobiascz/MNIST_Pytorch_python_and_capi"},{"type":"has_code","target_id":"github:ne7ermore:torch_light","source_url":"https://github.com/ne7ermore/torch_light"},{"type":"has_code","target_id":"github:dribnet:portrain-gan","source_url":"https://github.com/dribnet/portrain-gan"},{"type":"has_code","target_id":"github:omarsar:mri-analysis-pytorch","source_url":"https://github.com/omarsar/mri-analysis-pytorch"},{"type":"has_code","target_id":"github:davidcpage:cifar10-fast","source_url":"https://github.com/davidcpage/cifar10-fast"},{"type":"has_code","target_id":"github:bentrevett:pytorch-sentiment-analysis","source_url":"https://github.com/bentrevett/pytorch-sentiment-analysis"},{"type":"has_code","target_id":"github:rwightman:pytorch-image-models","source_url":"https://github.com/rwightman/pytorch-image-models"},{"type":"has_code","target_id":"github:BIGBALLON:CIFAR-ZOO","source_url":"https://github.com/BIGBALLON/CIFAR-ZOO"},{"type":"has_code","target_id":"github:dsgiitr:d2l-pytorch","source_url":"https://github.com/dsgiitr/d2l-pytorch"},{"type":"has_code","target_id":"github:stared:thinking-in-tensors-writing-in-pytorch","source_url":"https://github.com/stared/thinking-in-tensors-writing-in-pytorch"},{"type":"has_code","target_id":"github:lemonhu:NER-BERT-pytorch","source_url":"https://github.com/lemonhu/NER-BERT-pytorch"},{"type":"has_code","target_id":"github:dougsouza:pytorch-sync-batchnorm-example","source_url":"https://github.com/dougsouza/pytorch-sync-batchnorm-example"},{"type":"has_code","target_id":"github:barissayil:SentimentAnalysis","source_url":"https://github.com/barissayil/SentimentAnalysis"},{"type":"has_code","target_id":"github:prabhuomkar:pytorch-cpp","source_url":"https://github.com/prabhuomkar/pytorch-cpp"},{"type":"has_code","target_id":"github:yunjey:pytorch-tutorial","source_url":"https://github.com/yunjey/pytorch-tutorial"},{"type":"has_code","target_id":"github:lab-ml:nn","source_url":"https://github.com/lab-ml/nn"},{"type":"has_code","target_id":"github:adap:flower","source_url":"https://github.com/adap/flower"},{"type":"has_code","target_id":"github:neuralix:google_evolution","source_url":"https://github.com/neuralix/google_evolution"},{"type":"has_code","target_id":"github:edouardoyallon:pyscatwave","source_url":"https://github.com/edouardoyallon/pyscatwave"},{"type":"has_code","target_id":"github:edouardoyallon:scalingscattering","source_url":"https://github.com/edouardoyallon/scalingscattering"},{"type":"has_code","target_id":"github:episodeyang:deep-auto-punctuation","source_url":"https://github.com/episodeyang/deep-auto-punctuation"},{"type":"has_code","target_id":"github:tensorboy:pytorch_Realtime_Multi-Person_Pose_Estimation","source_url":"https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation"},{"type":"has_code","target_id":"github:ZheC:Realtime_Multi-Person_Pose_Estimation","source_url":"https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation"},{"type":"has_code","target_id":"github:onlytailei:PyTorch-value-iteration-networks","source_url":"https://github.com/onlytailei/PyTorch-value-iteration-networks"},{"type":"has_code","target_id":"github:analvikingur:pytorch_Highway","source_url":"https://github.com/analvikingur/pytorch_Highway"},{"type":"has_code","target_id":"github:analvikingur:pytorch_NEG_loss","source_url":"https://github.com/analvikingur/pytorch_NEG_loss"},{"type":"has_code","target_id":"github:analvikingur:pytorch_RVAE","source_url":"https://github.com/analvikingur/pytorch_RVAE"},{"type":"has_code","target_id":"github:analvikingur:pytorch_TDNN","source_url":"https://github.com/analvikingur/pytorch_TDNN"},{"type":"has_code","target_id":"github:moskomule:eve.pytorch","source_url":"https://github.com/moskomule/eve.pytorch"},{"type":"has_code","target_id":"github:locuslab:e2e-model-learning","source_url":"https://github.com/locuslab/e2e-model-learning"},{"type":"has_code","target_id":"github:mrzhu-cool:pix2pix-pytorch","source_url":"https://github.com/mrzhu-cool/pix2pix-pytorch"},{"type":"has_code","target_id":"github:amdegroot:ssd.pytorch","source_url":"https://github.com/amdegroot/ssd.pytorch"},{"type":"has_code","target_id":"github:carpedm20:DiscoGAN-pytorch","source_url":"https://github.com/carpedm20/DiscoGAN-pytorch"},{"type":"has_code","target_id":"github:SKTBrain:DiscoGAN","source_url":"https://github.com/SKTBrain/DiscoGAN"},{"type":"has_code","target_id":"github:atgambardella:pytorch-es","source_url":"https://github.com/atgambardella/pytorch-es"},{"type":"has_code","target_id":"github:bodokaiser:piwise","source_url":"https://github.com/bodokaiser/piwise"},{"type":"has_code","target_id":"github:transedward:pytorch-dqn","source_url":"https://github.com/transedward/pytorch-dqn"},{"type":"has_code","target_id":"github:ruotianluo:neuraltalk2.pytorch","source_url":"https://github.com/ruotianluo/neuraltalk2.pytorch"},{"type":"has_code","target_id":"github:mattmacy:vnet.pytorch","source_url":"https://github.com/mattmacy/vnet.pytorch"},{"type":"has_code","target_id":"github:wkentaro:pytorch-fcn","source_url":"https://github.com/wkentaro/pytorch-fcn"},{"type":"has_code","target_id":"github:xternalz:WideResNet-pytorch","source_url":"https://github.com/xternalz/WideResNet-pytorch"},{"type":"has_code","target_id":"github:szagoruyko:wide-residual-networks","source_url":"https://github.com/szagoruyko/wide-residual-networks"},{"type":"has_code","target_id":"github:c0nn3r:pytorch_highway_networks","source_url":"https://github.com/c0nn3r/pytorch_highway_networks"},{"type":"has_code","target_id":"github:ypxie:pytorch-NeuCom","source_url":"https://github.com/ypxie/pytorch-NeuCom"},{"type":"has_code","target_id":"github:eladhoffer:captionGen","source_url":"https://github.com/eladhoffer/captionGen"},{"type":"has_code","target_id":"github:jayleicn:animeGAN","source_url":"https://github.com/jayleicn/animeGAN"},{"type":"has_code","target_id":"github:Shawn1993:cnn-text-classification-pytorch","source_url":"https://github.com/Shawn1993/cnn-text-classification-pytorch"},{"type":"has_code","target_id":"github:SeanNaren:deepspeech.pytorch","source_url":"https://github.com/SeanNaren/deepspeech.pytorch"},{"type":"has_code","target_id":"github:MaximumEntropy:Seq2Seq-PyTorch","source_url":"https://github.com/MaximumEntropy/Seq2Seq-PyTorch"},{"type":"has_code","target_id":"github:rarilurelo:pytorch_a3c","source_url":"https://github.com/rarilurelo/pytorch_a3c"},{"type":"has_code","target_id":"github:bamos:densenet.pytorch","source_url":"https://github.com/bamos/densenet.pytorch"},{"type":"has_code","target_id":"github:alykhantejani:nninit","source_url":"https://github.com/alykhantejani/nninit"},{"type":"has_code","target_id":"github:longcw:faster_rcnn_pytorch","source_url":"https://github.com/longcw/faster_rcnn_pytorch"},{"type":"has_code","target_id":"github:akolishchak:doom-net-pytorch","source_url":"https://github.com/akolishchak/doom-net-pytorch"},{"type":"has_code","target_id":"github:ClementPinard:FlowNetPytorch","source_url":"https://github.com/ClementPinard/FlowNetPytorch"},{"type":"has_code","target_id":"github:gsp-27:pytorch_Squeezenet","source_url":"https://github.com/gsp-27/pytorch_Squeezenet"},{"type":"has_code","target_id":"github:martinarjovsky:WassersteinGAN","source_url":"https://github.com/martinarjovsky/WassersteinGAN"},{"type":"has_code","target_id":"github:locuslab:optnet","source_url":"https://github.com/locuslab/optnet"},{"type":"has_code","target_id":"github:locuslab:qpth","source_url":"https://github.com/locuslab/qpth"},{"type":"has_code","target_id":"github:ikostrikov:pytorch-naf","source_url":"https://github.com/ikostrikov/pytorch-naf"},{"type":"has_code","target_id":"github:ikostrikov:pytorch-meta-optimizer","source_url":"https://github.com/ikostrikov/pytorch-meta-optimizer"},{"type":"has_code","target_id":"github:darkstar112358:fast-neural-style","source_url":"https://github.com/darkstar112358/fast-neural-style"},{"type":"has_code","target_id":"github:leongatys:PytorchNeuralStyleTransfer","source_url":"https://github.com/leongatys/PytorchNeuralStyleTransfer"},{"type":"has_code","target_id":"github:bengxy:FastNeuralStyle","source_url":"https://github.com/bengxy/FastNeuralStyle"},{"type":"has_code","target_id":"github:alexis-jacq:Pytorch-Tutorials","source_url":"https://github.com/alexis-jacq/Pytorch-Tutorials"},{"type":"has_code","target_id":"github:zuoxingdong:VIN_PyTorch_Visdom","source_url":"https://github.com/zuoxingdong/VIN_PyTorch_Visdom"},{"type":"has_code","target_id":"github:longcw:yolo2-pytorch","source_url":"https://github.com/longcw/yolo2-pytorch"},{"type":"has_code","target_id":"github:szagoruyko:attention-transfer","source_url":"https://github.com/szagoruyko/attention-transfer"},{"type":"has_code","target_id":"github:potterhsu:SVHNClassifier-PyTorch","source_url":"https://github.com/potterhsu/SVHNClassifier-PyTorch"},{"type":"has_code","target_id":"github:oeway:pytorch-deform-conv","source_url":"https://github.com/oeway/pytorch-deform-conv"},{"type":"has_code","target_id":"github:carpedm20:BEGAN-pytorch","source_url":"https://github.com/carpedm20/BEGAN-pytorch"},{"type":"has_code","target_id":"github:dasguptar:treelstm.pytorch","source_url":"https://github.com/dasguptar/treelstm.pytorch"},{"type":"has_code","target_id":"github:DmitryUlyanov:AGE","source_url":"https://github.com/DmitryUlyanov/AGE"},{"type":"has_code","target_id":"github:prlz77:ResNeXt.pytorch","source_url":"https://github.com/prlz77/ResNeXt.pytorch"},{"type":"has_code","target_id":"github:jingweiz:pytorch-rl","source_url":"https://github.com/jingweiz/pytorch-rl"},{"type":"has_code","target_id":"github:sujithv28:Deep-Leafsnap","source_url":"https://github.com/sujithv28/Deep-Leafsnap"},{"type":"has_code","target_id":"github:junyanz:pytorch-CycleGAN-and-pix2pix","source_url":"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix"},{"type":"has_code","target_id":"github:onlytailei:A3C-PyTorch","source_url":"https://github.com/onlytailei/A3C-PyTorch"},{"type":"has_code","target_id":"github:kentsommer:pytorch-value-iteration-networks","source_url":"https://github.com/kentsommer/pytorch-value-iteration-networks"},{"type":"has_code","target_id":"github:zhanghang1989:PyTorch-Style-Transfer","source_url":"https://github.com/zhanghang1989/PyTorch-Style-Transfer"},{"type":"has_code","target_id":"github:isht7:pytorch-deeplab-resnet","source_url":"https://github.com/isht7/pytorch-deeplab-resnet"},{"type":"has_code","target_id":"github:fxia22:pointnet.pytorch","source_url":"https://github.com/fxia22/pointnet.pytorch"},{"type":"has_code","target_id":"github:aaron-xichen:pytorch-playground","source_url":"https://github.com/aaron-xichen/pytorch-playground"},{"type":"has_code","target_id":"github:jingweiz:pytorch-dnc","source_url":"https://github.com/jingweiz/pytorch-dnc"},{"type":"has_code","target_id":"github:jinfagang:pytorch_image_classifier","source_url":"https://github.com/jinfagang/pytorch_image_classifier"},{"type":"has_code","target_id":"github:yunjey:mnist-svhn-transfer","source_url":"https://github.com/yunjey/mnist-svhn-transfer"},{"type":"has_code","target_id":"github:marvis:pytorch-yolo2","source_url":"https://github.com/marvis/pytorch-yolo2"},{"type":"has_code","target_id":"github:andrewliao11:dni.pytorch","source_url":"https://github.com/andrewliao11/dni.pytorch"},{"type":"has_code","target_id":"github:caogang:wgan-gp","source_url":"https://github.com/caogang/wgan-gp"},{"type":"has_code","target_id":"github:spro:pytorch-seq2seq-intent-parsing","source_url":"https://github.com/spro/pytorch-seq2seq-intent-parsing"},{"type":"has_code","target_id":"github:demelin:pyTorch_NCE","source_url":"https://github.com/demelin/pyTorch_NCE"},{"type":"has_code","target_id":"github:cxhernandez:molencoder","source_url":"https://github.com/cxhernandez/molencoder"},{"type":"has_code","target_id":"github:stormraiser:GAN-weight-norm","source_url":"https://github.com/stormraiser/GAN-weight-norm"},{"type":"has_code","target_id":"github:rachtsingh:lgamma","source_url":"https://github.com/rachtsingh/lgamma"},{"type":"has_code","target_id":"github:eladhoffer:bigBatch","source_url":"https://github.com/eladhoffer/bigBatch"},{"type":"has_code","target_id":"github:dgriff777:rl_a3c_pytorch","source_url":"https://github.com/dgriff777/rl_a3c_pytorch"},{"type":"has_code","target_id":"github:ahirner:pytorch-retraining","source_url":"https://github.com/ahirner/pytorch-retraining"},{"type":"has_code","target_id":"github:priba:nmp_qc","source_url":"https://github.com/priba/nmp_qc"},{"type":"has_code","target_id":"github:jacobgil:pytorch-grad-cam","source_url":"https://github.com/jacobgil/pytorch-grad-cam"},{"type":"has_code","target_id":"github:mjacar:pytorch-trpo","source_url":"https://github.com/mjacar/pytorch-trpo"},{"type":"has_code","target_id":"github:jacobgil:pytorch-explain-black-box","source_url":"https://github.com/jacobgil/pytorch-explain-black-box"},{"type":"has_code","target_id":"github:jmtomczak:vae_vpflows","source_url":"https://github.com/jmtomczak/vae_vpflows"},{"type":"has_code","target_id":"github:kimhc6028:relational-networks","source_url":"https://github.com/kimhc6028/relational-networks"},{"type":"has_code","target_id":"github:Cadene:vqa.pytorch","source_url":"https://github.com/Cadene/vqa.pytorch"},{"type":"has_code","target_id":"github:facebookresearch:end-to-end-negotiator","source_url":"https://github.com/facebookresearch/end-to-end-negotiator"},{"type":"has_code","target_id":"github:ShiyuLiang:odin-pytorch","source_url":"https://github.com/ShiyuLiang/odin-pytorch"},{"type":"has_code","target_id":"github:ajbrock:FreezeOut","source_url":"https://github.com/ajbrock/FreezeOut"},{"type":"has_code","target_id":"github:jakezhaojb:ARAE","source_url":"https://github.com/jakezhaojb/ARAE"},{"type":"has_code","target_id":"github:kimhc6028:forward-thinking-pytorch","source_url":"https://github.com/kimhc6028/forward-thinking-pytorch"},{"type":"has_code","target_id":"github:BoyuanJiang:context_encoder_pytorch","source_url":"https://github.com/BoyuanJiang/context_encoder_pytorch"},{"type":"has_code","target_id":"github:jadore801120:attention-is-all-you-need-pytorch","source_url":"https://github.com/jadore801120/attention-is-all-you-need-pytorch"},{"type":"has_code","target_id":"github:thnkim:OpenFacePytorch","source_url":"https://github.com/thnkim/OpenFacePytorch"},{"type":"has_code","target_id":"github:thnkim:OpenFacePytorch","source_url":"https://github.com/thnkim/OpenFacePytorch"},{"type":"has_code","target_id":"github:pemami4911:neural-combinatorial-rl-pytorch","source_url":"https://github.com/pemami4911/neural-combinatorial-rl-pytorch"},{"type":"has_code","target_id":"github:mjacar:pytorch-nec","source_url":"https://github.com/mjacar/pytorch-nec"},{"type":"has_code","target_id":"github:eladhoffer:seq2seq.pytorch","source_url":"https://github.com/eladhoffer/seq2seq.pytorch"},{"type":"has_code","target_id":"github:alexis-jacq:Pytorch-Sketch-RNN","source_url":"https://github.com/alexis-jacq/Pytorch-Sketch-RNN"},{"type":"has_code","target_id":"github:jacobgil:pytorch-pruning","source_url":"https://github.com/jacobgil/pytorch-pruning"},{"type":"has_code","target_id":"github:hitvoice:DrQA","source_url":"https://github.com/hitvoice/DrQA"},{"type":"has_code","target_id":"github:JianGoForIt:YellowFin_Pytorch","source_url":"https://github.com/JianGoForIt/YellowFin_Pytorch"},{"type":"has_code","target_id":"github:deepsound-project:samplernn-pytorch","source_url":"https://github.com/deepsound-project/samplernn-pytorch"},{"type":"has_code","target_id":"github:tymokvo:AEGeAN","source_url":"https://github.com/tymokvo/AEGeAN"},{"type":"has_code","target_id":"github:twtygqyy:pytorch-SRResNet","source_url":"https://github.com/twtygqyy/pytorch-SRResNet"},{"type":"has_code","target_id":"github:fartashf:vsepp","source_url":"https://github.com/fartashf/vsepp"},{"type":"has_code","target_id":"github:alexis-jacq:Pytorch-DPPO","source_url":"https://github.com/alexis-jacq/Pytorch-DPPO"},{"type":"has_code","target_id":"github:mingyuliutw:UNIT","source_url":"https://github.com/mingyuliutw/UNIT"},{"type":"has_code","target_id":"github:gpleiss:efficient_densenet_pytorch","source_url":"https://github.com/gpleiss/efficient_densenet_pytorch"},{"type":"has_code","target_id":"github:yjxiong:tsn-pytorch","source_url":"https://github.com/yjxiong/tsn-pytorch"},{"type":"has_code","target_id":"github:ajbrock:SMASH","source_url":"https://github.com/ajbrock/SMASH"},{"type":"has_code","target_id":"github:kuangliu:pytorch-retinanet","source_url":"https://github.com/kuangliu/pytorch-retinanet"},{"type":"has_code","target_id":"github:aosokin:biogans","source_url":"https://github.com/aosokin/biogans"},{"type":"has_code","target_id":"github:woozzu:dong_iccv_2017","source_url":"https://github.com/woozzu/dong_iccv_2017"},{"type":"has_code","target_id":"github:jmhessel:fmpytorch","source_url":"https://github.com/jmhessel/fmpytorch"},{"type":"has_code","target_id":"github:ZhouYanzhao:ORN","source_url":"https://github.com/ZhouYanzhao/ORN"},{"type":"has_code","target_id":"github:katerakelly:pytorch-maml","source_url":"https://github.com/katerakelly/pytorch-maml"},{"type":"has_code","target_id":"github:znxlwm:pytorch-generative-model-collections","source_url":"https://github.com/znxlwm/pytorch-generative-model-collections"},{"type":"has_code","target_id":"github:markdtw:vqa-winner-cvprw-2017","source_url":"https://github.com/markdtw/vqa-winner-cvprw-2017"},{"type":"has_code","target_id":"github:r9y9:tacotron_pytorch","source_url":"https://github.com/r9y9/tacotron_pytorch"},{"type":"has_code","target_id":"github:Lextal:pspnet-pytorch","source_url":"https://github.com/Lextal/pspnet-pytorch"},{"type":"has_code","target_id":"github:LiyuanLucasLiu:LM-LSTM-CRF","source_url":"https://github.com/LiyuanLucasLiu/LM-LSTM-CRF"},{"type":"has_code","target_id":"github:1adrianb:face-alignment","source_url":"https://github.com/1adrianb/face-alignment"},{"type":"has_code","target_id":"github:ClementPinard:DepthNet","source_url":"https://github.com/ClementPinard/DepthNet"},{"type":"has_code","target_id":"github:thstkdgus35:EDSR-PyTorch","source_url":"https://github.com/thstkdgus35/EDSR-PyTorch"},{"type":"has_code","target_id":"github:ethanluoyc:e2c-pytorch","source_url":"https://github.com/ethanluoyc/e2c-pytorch"},{"type":"has_code","target_id":"github:kenshohara:3D-ResNets-PyTorch","source_url":"https://github.com/kenshohara/3D-ResNets-PyTorch"},{"type":"has_code","target_id":"github:khanhptnk:bandit-nmt","source_url":"https://github.com/khanhptnk/bandit-nmt"},{"type":"has_code","target_id":"github:ikostrikov:pytorch-a2c-ppo-acktr","source_url":"https://github.com/ikostrikov/pytorch-a2c-ppo-acktr"},{"type":"has_code","target_id":"github:baldassarreFe:zalando-pytorch","source_url":"https://github.com/baldassarreFe/zalando-pytorch"},{"type":"has_code","target_id":"github:clcarwin:sphereface_pytorch","source_url":"https://github.com/clcarwin/sphereface_pytorch"},{"type":"has_code","target_id":"github:floringogianu:categorical-dqn","source_url":"https://github.com/floringogianu/categorical-dqn"},{"type":"has_code","target_id":"github:loudinthecloud:pytorch-ntm","source_url":"https://github.com/loudinthecloud/pytorch-ntm"},{"type":"has_code","target_id":"github:felixgwu:mask_rcnn_pytorch","source_url":"https://github.com/felixgwu/mask_rcnn_pytorch"},{"type":"has_code","target_id":"github:xbresson:graph_convnets_pytorch","source_url":"https://github.com/xbresson/graph_convnets_pytorch"},{"type":"has_code","target_id":"github:ruotianluo:pytorch-faster-rcnn","source_url":"https://github.com/ruotianluo/pytorch-faster-rcnn"},{"type":"has_code","target_id":"github:huggingface:torchMoji","source_url":"https://github.com/huggingface/torchMoji"},{"type":"has_code","target_id":"github:hangzhaomit:semantic-segmentation-pytorch","source_url":"https://github.com/hangzhaomit/semantic-segmentation-pytorch"},{"type":"has_code","target_id":"github:salesforce:pytorch-qrnn","source_url":"https://github.com/salesforce/pytorch-qrnn"},{"type":"has_code","target_id":"github:theeluwin:pytorch-sgns","source_url":"https://github.com/theeluwin/pytorch-sgns"},{"type":"has_code","target_id":"github:ClementPinard:SfmLearner-Pytorch","source_url":"https://github.com/ClementPinard/SfmLearner-Pytorch"},{"type":"has_code","target_id":"github:1zb:deformable-convolution-pytorch","source_url":"https://github.com/1zb/deformable-convolution-pytorch"},{"type":"has_code","target_id":"github:fanglanting:skip-gram-pytorch","source_url":"https://github.com/fanglanting/skip-gram-pytorch"},{"type":"has_code","target_id":"github:hanzhanggit:StackGAN-v2","source_url":"https://github.com/hanzhanggit/StackGAN-v2"},{"type":"has_code","target_id":"github:ruotianluo:self-critical.pytorch","source_url":"https://github.com/ruotianluo/self-critical.pytorch"},{"type":"has_code","target_id":"github:tkipf:pygcn","source_url":"https://github.com/tkipf/pygcn"},{"type":"has_code","target_id":"github:ixaxaar:pytorch-dnc","source_url":"https://github.com/ixaxaar/pytorch-dnc"},{"type":"has_code","target_id":"github:ptrblck:prog_gans_pytorch_inference","source_url":"https://github.com/ptrblck/prog_gans_pytorch_inference"},{"type":"has_code","target_id":"github:timomernick:pytorch-capsule","source_url":"https://github.com/timomernick/pytorch-capsule"},{"type":"has_code","target_id":"github:dyhan0920:PyramidNet-PyTorch","source_url":"https://github.com/dyhan0920/PyramidNet-PyTorch"},{"type":"has_code","target_id":"github:gram-ai:radio-transformer-networks","source_url":"https://github.com/gram-ai/radio-transformer-networks"},{"type":"has_code","target_id":"github:castorini:honk","source_url":"https://github.com/castorini/honk"},{"type":"has_code","target_id":"github:SSARCandy:DeepCORAL","source_url":"https://github.com/SSARCandy/DeepCORAL"},{"type":"has_code","target_id":"github:bearpaw:pytorch-pose","source_url":"https://github.com/bearpaw/pytorch-pose"},{"type":"has_code","target_id":"github:karandesai-96:lang-emerge-parlai","source_url":"https://github.com/karandesai-96/lang-emerge-parlai"},{"type":"has_code","target_id":"github:Kaixhin:Rainbow","source_url":"https://github.com/Kaixhin/Rainbow"},{"type":"has_code","target_id":"github:gdlg:pytorch_compact_bilinear_pooling","source_url":"https://github.com/gdlg/pytorch_compact_bilinear_pooling"},{"type":"has_code","target_id":"github:DeepInsight-PCALab:CompactBilinearPooling-Pytorch","source_url":"https://github.com/DeepInsight-PCALab/CompactBilinearPooling-Pytorch"},{"type":"has_code","target_id":"github:gitabcworld:FewShotLearning","source_url":"https://github.com/gitabcworld/FewShotLearning"},{"type":"has_code","target_id":"github:jklj077:meProp","source_url":"https://github.com/jklj077/meProp"},{"type":"has_code","target_id":"github:clcarwin:SFD_pytorch","source_url":"https://github.com/clcarwin/SFD_pytorch"},{"type":"has_code","target_id":"github:facebookresearch:GradientEpisodicMemory","source_url":"https://github.com/facebookresearch/GradientEpisodicMemory"},{"type":"has_code","target_id":"github:KupynOrest:DeblurGAN","source_url":"https://github.com/KupynOrest/DeblurGAN"},{"type":"has_code","target_id":"github:yunjey:StarGAN","source_url":"https://github.com/yunjey/StarGAN"},{"type":"has_code","target_id":"github:adambielski:CapsNet-pytorch","source_url":"https://github.com/adambielski/CapsNet-pytorch"},{"type":"has_code","target_id":"github:ShichenLiu:CondenseNet","source_url":"https://github.com/ShichenLiu/CondenseNet"},{"type":"has_code","target_id":"github:DmitryUlyanov:deep-image-prior","source_url":"https://github.com/DmitryUlyanov/deep-image-prior"},{"type":"has_code","target_id":"github:natanielruiz:deep-head-pose","source_url":"https://github.com/natanielruiz/deep-head-pose"},{"type":"has_code","target_id":"github:zhunzhong07:Random-Erasing","source_url":"https://github.com/zhunzhong07/Random-Erasing"},{"type":"has_code","target_id":"github:facebookresearch:FaderNetworks","source_url":"https://github.com/facebookresearch/FaderNetworks"},{"type":"has_code","target_id":"github:NVIDIA:flownet2-pytorch","source_url":"https://github.com/NVIDIA/flownet2-pytorch"},{"type":"has_code","target_id":"github:NVIDIA:pix2pixHD","source_url":"https://github.com/NVIDIA/pix2pixHD"},{"type":"has_code","target_id":"github:pkdn:pytorch-smoothgrad","source_url":"https://github.com/pkdn/pytorch-smoothgrad"},{"type":"has_code","target_id":"github:c0nn3r:RetinaNet","source_url":"https://github.com/c0nn3r/RetinaNet"},{"type":"has_code","target_id":"github:jwyang:faster-rcnn.pytorch","source_url":"https://github.com/jwyang/faster-rcnn.pytorch"},{"type":"has_code","target_id":"github:leehomyc:mixup_pytorch","source_url":"https://github.com/leehomyc/mixup_pytorch"},{"type":"has_code","target_id":"github:mapillary:inplace_abn","source_url":"https://github.com/mapillary/inplace_abn"},{"type":"has_code","target_id":"github:xingyizhou:pytorch-pose-hg-3d","source_url":"https://github.com/xingyizhou/pytorch-pose-hg-3d"},{"type":"has_code","target_id":"github:HarshTrivedi:nmn-pytorch","source_url":"https://github.com/HarshTrivedi/nmn-pytorch"},{"type":"has_code","target_id":"github:kefirski:bytenet","source_url":"https://github.com/kefirski/bytenet"},{"type":"has_code","target_id":"github:hengyuan-hu:bottom-up-attention-vqa","source_url":"https://github.com/hengyuan-hu/bottom-up-attention-vqa"},{"type":"has_code","target_id":"github:ruiminshen:yolo2-pytorch","source_url":"https://github.com/ruiminshen/yolo2-pytorch"},{"type":"has_code","target_id":"github:Wizaron:reseg-pytorch","source_url":"https://github.com/Wizaron/reseg-pytorch"},{"type":"has_code","target_id":"github:Wizaron:binary-stochastic-neurons","source_url":"https://github.com/Wizaron/binary-stochastic-neurons"},{"type":"has_code","target_id":"github:DavexPro:pytorch-pose-estimation","source_url":"https://github.com/DavexPro/pytorch-pose-estimation"},{"type":"has_code","target_id":"github:higgsfield:interaction_network_pytorch","source_url":"https://github.com/higgsfield/interaction_network_pytorch"},{"type":"has_code","target_id":"github:wlwkgus:NoisyNaturalGradient","source_url":"https://github.com/wlwkgus/NoisyNaturalGradient"},{"type":"has_code","target_id":"github:moskomule:ewc.pytorch","source_url":"https://github.com/moskomule/ewc.pytorch"},{"type":"has_code","target_id":"github:jacobgil:pytorch-zssr","source_url":"https://github.com/jacobgil/pytorch-zssr"},{"type":"has_code","target_id":"github:atiyo:deep_image_prior","source_url":"https://github.com/atiyo/deep_image_prior"},{"type":"has_code","target_id":"github:leviswind:pytorch-transformer","source_url":"https://github.com/leviswind/pytorch-transformer"},{"type":"has_code","target_id":"github:devendrachaplot:DeepRL-Grounding","source_url":"https://github.com/devendrachaplot/DeepRL-Grounding"},{"type":"has_code","target_id":"github:Wizaron:deep-forecast-pytorch","source_url":"https://github.com/Wizaron/deep-forecast-pytorch"},{"type":"has_code","target_id":"github:utiasSTARS:cat-net","source_url":"https://github.com/utiasSTARS/cat-net"},{"type":"has_code","target_id":"github:tneumann:minimal_glo","source_url":"https://github.com/tneumann/minimal_glo"},{"type":"has_code","target_id":"github:dragen1860:LearningToCompare-Pytorch","source_url":"https://github.com/dragen1860/LearningToCompare-Pytorch"},{"type":"has_code","target_id":"github:facebookresearch:poincare-embeddings","source_url":"https://github.com/facebookresearch/poincare-embeddings"},{"type":"has_code","target_id":"github:ikostrikov:pytorch-trpo","source_url":"https://github.com/ikostrikov/pytorch-trpo"},{"type":"has_code","target_id":"github:JamesChuanggg:ggnn.pytorch","source_url":"https://github.com/JamesChuanggg/ggnn.pytorch"},{"type":"has_code","target_id":"github:Mrgemy95:visual-interaction-networks-pytorch","source_url":"https://github.com/Mrgemy95/visual-interaction-networks-pytorch"},{"type":"has_code","target_id":"github:jhayes14:adversarial-patch","source_url":"https://github.com/jhayes14/adversarial-patch"},{"type":"has_code","target_id":"github:orobix:Prototypical-Networks-for-Few-shot-Learning-PyTorch","source_url":"https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch"},{"type":"has_code","target_id":"github:orobix:Visual-Feature-Attribution-Using-Wasserstein-GANs-Pytorch","source_url":"https://github.com/orobix/Visual-Feature-Attribution-Using-Wasserstein-GANs-Pytorch"},{"type":"has_code","target_id":"github:Blade6570:PhotographicImageSynthesiswithCascadedRefinementNetworks-Pytorch","source_url":"https://github.com/Blade6570/PhotographicImageSynthesiswithCascadedRefinementNetworks-Pytorch"},{"type":"has_code","target_id":"github:carpedm20:ENAS-pytorch","source_url":"https://github.com/carpedm20/ENAS-pytorch"},{"type":"has_code","target_id":"github:kentsyx:Neural-IMage-Assessment","source_url":"https://github.com/kentsyx/Neural-IMage-Assessment"},{"type":"has_code","target_id":"github:tfrerix:proxprop","source_url":"https://github.com/tfrerix/proxprop"},{"type":"has_code","target_id":"github:NVIDIA:FastPhotoStyle","source_url":"https://github.com/NVIDIA/FastPhotoStyle"},{"type":"has_code","target_id":"github:Ben-Louis:Deep-Image-Analogy-PyTorch","source_url":"https://github.com/Ben-Louis/Deep-Image-Analogy-PyTorch"},{"type":"has_code","target_id":"github:layumi:Person_reID_baseline_pytorch","source_url":"https://github.com/layumi/Person_reID_baseline_pytorch"},{"type":"has_code","target_id":"github:zalandoresearch:pt-dilate-rnn","source_url":"https://github.com/zalandoresearch/pt-dilate-rnn"},{"type":"has_code","target_id":"github:jhjacobsen:pytorch-i-revnet","source_url":"https://github.com/jhjacobsen/pytorch-i-revnet"},{"type":"has_code","target_id":"github:Orcuslc:OrthNet","source_url":"https://github.com/Orcuslc/OrthNet"},{"type":"has_code","target_id":"github:jt827859032:DRRN-pytorch","source_url":"https://github.com/jt827859032/DRRN-pytorch"},{"type":"has_code","target_id":"github:moskomule:shampoo.pytorch","source_url":"https://github.com/moskomule/shampoo.pytorch"},{"type":"has_code","target_id":"github:truskovskiyk:nima.pytorch","source_url":"https://github.com/truskovskiyk/nima.pytorch"},{"type":"has_code","target_id":"github:locuslab:TCN","source_url":"https://github.com/locuslab/TCN"},{"type":"has_code","target_id":"github:shahsohil:DCC","source_url":"https://github.com/shahsohil/DCC"},{"type":"has_code","target_id":"github:arunmallya:packnet","source_url":"https://github.com/arunmallya/packnet"},{"type":"has_code","target_id":"github:github-pengge:PyTorch-progressive_growing_of_gans","source_url":"https://github.com/github-pengge/PyTorch-progressive_growing_of_gans"},{"type":"has_code","target_id":"github:salesforce:nonauto-nmt","source_url":"https://github.com/salesforce/nonauto-nmt"},{"type":"has_code","target_id":"github:eriklindernoren:PyTorch-GAN","source_url":"https://github.com/eriklindernoren/PyTorch-GAN"},{"type":"has_code","target_id":"github:tomrunia:PyTorchWavelets","source_url":"https://github.com/tomrunia/PyTorchWavelets"},{"type":"has_code","target_id":"github:karpathy:pytorch-made","source_url":"https://github.com/karpathy/pytorch-made"},{"type":"has_code","target_id":"github:emited:VariationalRecurrentNeuralNetwork","source_url":"https://github.com/emited/VariationalRecurrentNeuralNetwork"},{"type":"has_code","target_id":"github:emited:flow","source_url":"https://github.com/emited/flow"},{"type":"has_code","target_id":"github:r9y9:deepvoice3_pytorch","source_url":"https://github.com/r9y9/deepvoice3_pytorch"},{"type":"has_code","target_id":"github:elanmart:psmm","source_url":"https://github.com/elanmart/psmm"},{"type":"has_code","target_id":"github:NVIDIA:tacotron2","source_url":"https://github.com/NVIDIA/tacotron2"},{"type":"has_code","target_id":"github:rahulkidambi:AccSGD","source_url":"https://github.com/rahulkidambi/AccSGD"},{"type":"has_code","target_id":"github:hengruo:QANet-pytorch","source_url":"https://github.com/hengruo/QANet-pytorch"},{"type":"has_code","target_id":"github:TimDettmers:ConvE","source_url":"https://github.com/TimDettmers/ConvE"},{"type":"has_code","target_id":"github:kaushalshetty:Structured-Self-Attention","source_url":"https://github.com/kaushalshetty/Structured-Self-Attention"},{"type":"has_code","target_id":"github:williamleif:graphsage-simple","source_url":"https://github.com/williamleif/graphsage-simple"},{"type":"has_code","target_id":"github:roytseng-tw:Detectron.pytorch","source_url":"https://github.com/roytseng-tw/Detectron.pytorch"},{"type":"has_code","target_id":"github:irhumshafkat:R2Plus1D-PyTorch","source_url":"https://github.com/irhumshafkat/R2Plus1D-PyTorch"},{"type":"has_code","target_id":"github:viking-sudo-rm:StackNN","source_url":"https://github.com/viking-sudo-rm/StackNN"},{"type":"has_code","target_id":"github:facebookresearch:translagent","source_url":"https://github.com/facebookresearch/translagent"},{"type":"has_code","target_id":"github:jnhwkim:ban-vqa","source_url":"https://github.com/jnhwkim/ban-vqa"},{"type":"has_code","target_id":"github:huggingface:pytorch-openai-transformer-lm","source_url":"https://github.com/huggingface/pytorch-openai-transformer-lm"},{"type":"has_code","target_id":"github:akanimax:T2F","source_url":"https://github.com/akanimax/T2F"},{"type":"has_code","target_id":"github:mseitzer:pytorch-fid","source_url":"https://github.com/mseitzer/pytorch-fid"},{"type":"has_code","target_id":"github:jmtomczak:vae_vpflows","source_url":"https://github.com/jmtomczak/vae_vpflows"},{"type":"has_code","target_id":"github:mkocabas:CoordConv-pytorch","source_url":"https://github.com/mkocabas/CoordConv-pytorch"},{"type":"has_code","target_id":"github:xternalz:SDPoint","source_url":"https://github.com/xternalz/SDPoint"},{"type":"has_code","target_id":"github:wxywhu:SRDenseNet-pytorch","source_url":"https://github.com/wxywhu/SRDenseNet-pytorch"},{"type":"has_code","target_id":"github:LMescheder:GAN_stability","source_url":"https://github.com/LMescheder/GAN_stability"},{"type":"has_code","target_id":"github:wannabeOG:Mask-RCNN","source_url":"https://github.com/wannabeOG/Mask-RCNN"},{"type":"has_code","target_id":"github:chaoyuaw:pytorch-coviar","source_url":"https://github.com/chaoyuaw/pytorch-coviar"},{"type":"has_code","target_id":"github:chenxi116:PNASNet.pytorch","source_url":"https://github.com/chenxi116/PNASNet.pytorch"},{"type":"has_code","target_id":"github:kevinzakka:NALU-pytorch","source_url":"https://github.com/kevinzakka/NALU-pytorch"},{"type":"has_code","target_id":"github:alexis-jacq:LOLA_DiCE","source_url":"https://github.com/alexis-jacq/LOLA_DiCE"},{"type":"has_code","target_id":"github:wohlert:generative-query-network-pytorch","source_url":"https://github.com/wohlert/generative-query-network-pytorch"},{"type":"has_code","target_id":"github:wmvanvliet:pytorch_hmax","source_url":"https://github.com/wmvanvliet/pytorch_hmax"},{"type":"has_code","target_id":"github:yunlongdong:FCN-pytorch-easiest","source_url":"https://github.com/yunlongdong/FCN-pytorch-easiest"},{"type":"has_code","target_id":"github:awni:transducer","source_url":"https://github.com/awni/transducer"},{"type":"has_code","target_id":"github:artix41:AVO-pytorch","source_url":"https://github.com/artix41/AVO-pytorch"},{"type":"has_code","target_id":"github:huguyuehuhu:HCN-pytorch","source_url":"https://github.com/huguyuehuhu/HCN-pytorch"},{"type":"has_code","target_id":"github:szagoruyko:binary-wide-resnet","source_url":"https://github.com/szagoruyko/binary-wide-resnet"},{"type":"has_code","target_id":"github:arunmallya:piggyback","source_url":"https://github.com/arunmallya/piggyback"},{"type":"has_code","target_id":"github:NVIDIA:vid2vid","source_url":"https://github.com/NVIDIA/vid2vid"},{"type":"has_code","target_id":"github:cranmer:poisson-convolution-sum","source_url":"https://github.com/cranmer/poisson-convolution-sum"},{"type":"has_code","target_id":"github:davidmascharka:tbd-nets","source_url":"https://github.com/davidmascharka/tbd-nets"},{"type":"has_code","target_id":"github:elbayadm:attn2d","source_url":"https://github.com/elbayadm/attn2d"},{"type":"has_code","target_id":"github:ultralytics:yolov3","source_url":"https://github.com/ultralytics/yolov3"},{"type":"has_code","target_id":"github:duc0:deep-dream-in-pytorch","source_url":"https://github.com/duc0/deep-dream-in-pytorch"},{"type":"has_code","target_id":"github:ikostrikov:pytorch-flows","source_url":"https://github.com/ikostrikov/pytorch-flows"},{"type":"has_code","target_id":"github:ars-ashuha:quantile-regression-dqn-pytorch","source_url":"https://github.com/ars-ashuha/quantile-regression-dqn-pytorch"},{"type":"has_code","target_id":"github:L0SG:relational-rnn-pytorch","source_url":"https://github.com/L0SG/relational-rnn-pytorch"},{"type":"has_code","target_id":"github:scaelles:DEXTR-PyTorch","source_url":"https://github.com/scaelles/DEXTR-PyTorch"},{"type":"has_code","target_id":"github:rdspring1:PyTorch_GBW_LM","source_url":"https://github.com/rdspring1/PyTorch_GBW_LM"},{"type":"has_code","target_id":"github:Stonesjtu:Pytorch-NCE","source_url":"https://github.com/Stonesjtu/Pytorch-NCE"},{"type":"has_code","target_id":"github:shayneobrien:generative-models","source_url":"https://github.com/shayneobrien/generative-models"},{"type":"has_code","target_id":"github:andreasveit:convnet-aig","source_url":"https://github.com/andreasveit/convnet-aig"},{"type":"has_code","target_id":"github:TianhongDai:integrated-gradient-pytorch","source_url":"https://github.com/TianhongDai/integrated-gradient-pytorch"},{"type":"has_code","target_id":"github:Alexander-H-Liu:MalConv-Pytorch","source_url":"https://github.com/Alexander-H-Liu/MalConv-Pytorch"},{"type":"has_code","target_id":"github:locuslab:trellisnet","source_url":"https://github.com/locuslab/trellisnet"},{"type":"has_code","target_id":"github:minqi:learning-to-communicate-pytorch","source_url":"https://github.com/minqi/learning-to-communicate-pytorch"},{"type":"has_code","target_id":"github:michaelklachko:pnn.pytorch","source_url":"https://github.com/michaelklachko/pnn.pytorch"},{"type":"has_code","target_id":"github:rainofmine:Face_Attention_Network","source_url":"https://github.com/rainofmine/Face_Attention_Network"},{"type":"has_code","target_id":"github:NVIDIA:waveglow","source_url":"https://github.com/NVIDIA/waveglow"},{"type":"has_code","target_id":"github:facebookresearch:deepfloat","source_url":"https://github.com/facebookresearch/deepfloat"},{"type":"has_code","target_id":"github:subeeshvasu:2018_subeesh_epsr_eccvw","source_url":"https://github.com/subeeshvasu/2018_subeesh_epsr_eccvw"},{"type":"has_code","target_id":"github:ksw0306:ClariNet","source_url":"https://github.com/ksw0306/ClariNet"},{"type":"has_code","target_id":"github:huggingface:pytorch-pretrained-BERT","source_url":"https://github.com/huggingface/pytorch-pretrained-BERT"},{"type":"has_code","target_id":"github:npuichigo:waveglow","source_url":"https://github.com/npuichigo/waveglow"},{"type":"has_code","target_id":"github:cleardusk:3DDFA","source_url":"https://github.com/cleardusk/3DDFA"},{"type":"has_code","target_id":"github:tomgoldstein:loss-landscape","source_url":"https://github.com/tomgoldstein/loss-landscape"},{"type":"has_code","target_id":"github:zalandoresearch:famos","source_url":"https://github.com/zalandoresearch/famos"},{"type":"has_code","target_id":"github:anuragranj:back2future.pytorch","source_url":"https://github.com/anuragranj/back2future.pytorch"},{"type":"has_code","target_id":"github:mozilla:FFTNet","source_url":"https://github.com/mozilla/FFTNet"},{"type":"has_code","target_id":"github:zisianw:FaceBoxes.PyTorch","source_url":"https://github.com/zisianw/FaceBoxes.PyTorch"},{"type":"has_code","target_id":"github:kimiyoung:transformer-xl","source_url":"https://github.com/kimiyoung/transformer-xl"},{"type":"has_code","target_id":"github:kimiyoung:transformer-xl","source_url":"https://github.com/kimiyoung/transformer-xl"},{"type":"has_code","target_id":"github:jalexvig:associative_compression_networks","source_url":"https://github.com/jalexvig/associative_compression_networks"},{"type":"has_code","target_id":"github:jolibrain:fluidnet_cxx","source_url":"https://github.com/jolibrain/fluidnet_cxx"},{"type":"has_code","target_id":"github:p-christ:Deep-Reinforcement-Learning-Algorithms-with-PyTorch","source_url":"https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch"},{"type":"has_code","target_id":"github:ericsun99:Shufflenet-v2-Pytorch","source_url":"https://github.com/ericsun99/Shufflenet-v2-Pytorch"},{"type":"has_code","target_id":"github:benedekrozemberczki:GraphWaveletNeuralNetwork","source_url":"https://github.com/benedekrozemberczki/GraphWaveletNeuralNetwork"},{"type":"has_code","target_id":"github:benedekrozemberczki:AttentionWalk","source_url":"https://github.com/benedekrozemberczki/AttentionWalk"},{"type":"has_code","target_id":"github:benedekrozemberczki:SGCN","source_url":"https://github.com/benedekrozemberczki/SGCN"},{"type":"has_code","target_id":"github:benedekrozemberczki:SINE","source_url":"https://github.com/benedekrozemberczki/SINE"},{"type":"has_code","target_id":"github:benedekrozemberczki:GAM","source_url":"https://github.com/benedekrozemberczki/GAM"},{"type":"has_code","target_id":"github:ProGamerGov:neural-style-pt","source_url":"https://github.com/ProGamerGov/neural-style-pt"},{"type":"has_code","target_id":"github:ibalazevic:TuckER","source_url":"https://github.com/ibalazevic/TuckER"},{"type":"has_code","target_id":"github:BayesWatch:pytorch-prunes","source_url":"https://github.com/BayesWatch/pytorch-prunes"},{"type":"has_code","target_id":"github:benedekrozemberczki:SimGNN","source_url":"https://github.com/benedekrozemberczki/SimGNN"},{"type":"has_code","target_id":"github:ahmedbesbes:character-based-cnn","source_url":"https://github.com/ahmedbesbes/character-based-cnn"},{"type":"has_code","target_id":"github:facebookresearch:XLM","source_url":"https://github.com/facebookresearch/XLM"},{"type":"has_code","target_id":"github:eth-sri:diffai","source_url":"https://github.com/eth-sri/diffai"},{"type":"has_code","target_id":"github:benedekrozemberczki:APPNP","source_url":"https://github.com/benedekrozemberczki/APPNP"},{"type":"has_code","target_id":"github:benedekrozemberczki:MixHop-and-N-GCN","source_url":"https://github.com/benedekrozemberczki/MixHop-and-N-GCN"},{"type":"has_code","target_id":"github:graykode:gpt-2-Pytorch","source_url":"https://github.com/graykode/gpt-2-Pytorch"},{"type":"has_code","target_id":"github:benedekrozemberczki:Splitter","source_url":"https://github.com/benedekrozemberczki/Splitter"},{"type":"has_code","target_id":"github:benedekrozemberczki:CapsGNN","source_url":"https://github.com/benedekrozemberczki/CapsGNN"},{"type":"has_code","target_id":"github:ajbrock:BigGAN-PyTorch","source_url":"https://github.com/ajbrock/BigGAN-PyTorch"},{"type":"has_code","target_id":"github:mhubii:ppo_pytorch_cpp","source_url":"https://github.com/mhubii/ppo_pytorch_cpp"},{"type":"has_code","target_id":"github:seungwonpark:RandWireNN","source_url":"https://github.com/seungwonpark/RandWireNN"},{"type":"has_code","target_id":"github:joel-huang:zeroshot-capsnet-pytorch","source_url":"https://github.com/joel-huang/zeroshot-capsnet-pytorch"},{"type":"has_code","target_id":"github:benedekrozemberczki:SEAL-CI","source_url":"https://github.com/benedekrozemberczki/SEAL-CI"},{"type":"has_code","target_id":"github:benedekrozemberczki:MixHop-and-N-GCN","source_url":"https://github.com/benedekrozemberczki/MixHop-and-N-GCN"},{"type":"has_code","target_id":"github:Lotayou:densebody_pytorch","source_url":"https://github.com/Lotayou/densebody_pytorch"},{"type":"has_code","target_id":"github:mindslab-ai:voicefilter","source_url":"https://github.com/mindslab-ai/voicefilter"},{"type":"has_code","target_id":"github:NVIDIA:semantic-segmentation","source_url":"https://github.com/NVIDIA/semantic-segmentation"},{"type":"has_code","target_id":"github:benedekrozemberczki:ClusterGCN","source_url":"https://github.com/benedekrozemberczki/ClusterGCN"},{"type":"has_code","target_id":"github:NVlabs:DG-Net","source_url":"https://github.com/NVlabs/DG-Net"},{"type":"has_code","target_id":"github:baidu-research:NCRF","source_url":"https://github.com/baidu-research/NCRF"},{"type":"has_code","target_id":"github:ducha-aiki:pytorch-sift","source_url":"https://github.com/ducha-aiki/pytorch-sift"},{"type":"has_code","target_id":"github:mateuszbuda:brain-segmentation-pytorch","source_url":"https://github.com/mateuszbuda/brain-segmentation-pytorch"},{"type":"has_code","target_id":"github:rosinality:glow-pytorch","source_url":"https://github.com/rosinality/glow-pytorch"},{"type":"has_code","target_id":"github:zsef123:EfficientNets-PyTorch","source_url":"https://github.com/zsef123/EfficientNets-PyTorch"}]', NULL, NULL, 'pending', 70, '43b5e78113302d8cdd55a4593728cdac', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-bharathgs-Awesome-pytorch-list from https://github.com/bharathgs.png
Image converted to WebP: data/images/github-bharathgs-Awesome-pytorch-list.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-dair-ai-ml-visuals', 'github--dair-ai--ml-visuals', 'ml-visuals', 'dair-ai', 'ğŸ“£ Stay tuned for significant updates to both the slides and repository.!!! ğŸ“£ In the meantime, Join our Discord ML Visuals is a new collaborative effort to help the machine learning community in improving science communication by providing free professional, compelling and adequate visuals and figures. Currently, we have over 100 figures (all open community contributions). You are free to use the visuals in your machine learning presentations or blog posts. You donâ€™t need to ask permission t...', '["artificial-intelligence","deep-learning","design","machine-learning","natural-language-processing"]', 'other', 16185, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/dair-ai/ml-visuals","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# ML Visuals\n\nğŸ“£ Stay tuned for significant updates to both the slides and repository.!!!\n\nğŸ“£ In the meantime, [Join our Discord](https://discord.gg/SKgkVT8BGJ)\n\n[ML Visuals](https://docs.google.com/presentation/d/11mR1nkIR9fbHegFkcFq8z9oDQ5sjv8E3JJp1LfLGKuk/edit?usp=sharing) is a new collaborative effort to help the machine learning community in improving science communication by providing free professional, compelling and adequate visuals and figures. Currently, we have over 100 figures (all open community contributions). You are free to use the visuals in your machine learning presentations or blog posts. You donâ€™t need to ask permission to use any of the visuals but it will be nice if you can provide credit to the designer/author (author information found in the slide notes). Check out the versions of the visuals below. \n\nThis is a project made by the [dair.ai](https://dair.ai/) community. The latest version of the Google slides can be found in this GitHub repository. Our community members will continue to add more common figures and basic elements in upcoming versions. Think of this as free and open artifacts and templates which you can freely and easily download, copy, distribute, reuse and customize to your own needs.\n\nML Visuals is now being used to power 100s of figures used by master/PhD students, papers (like this [one](https://arxiv.org/abs/2010.05113)), among other use cases. \n\n## How to Use?\n\nEssentially, we are using Google Slides to maintain all visuals and figures (check the versions below). To add your own custom figures, simply add a new slide and reuse any of the basic visual components (remember to request edit permissions). You can also create your own copy of the slides and customize whatever you like. We encourage authors/designers to add their visuals here and allow others to reuse them. Make sure to include your author information (in the notes section of the slide) so that others can provide credit if they use the visuals elsewhere (e.g. blog/presentations). Also, provide a short description of your visual to help the user understand what it is about and how they can use it. If you need "Edit" permission, just click on the "request edit access" option under the "view only" toolbar (in Google Slides) or send me an email at ellfae@gmail.com.\n\nDownloading a figure from any of the slides is easy. Just click on Fileâ†’Downloadâ†’(choose your format).\n\nIf you need help with customizing a figure or have an idea of something that could be valuable to others, we can help. Just open an issue [here](https://github.com/dair-ai/ml-visuals/issues/new) and we will do our best to come up with the visual. Thanks.\n\nFeel free to reach out to me on [Twitter](https://twitter.com/omarsar0) for an invite to our Slack group.\n\n## Versions:\n- [Version 1.0](https://docs.google.com/presentation/d/11mR1nkIR9fbHegFkcFq8z9oDQ5sjv8E3JJp1LfLGKuk/edit?usp=sharing)\n\n\n## How to Contribute?\n- You can check out our [Project page](https://github.com/orgs/dair-ai/projects/8) to see all the ongoing tasks or issues related to this research project. Lookout for the main `ml_visuals` tag. Issues with the `good first issue` tag are good tasks to get started with.\n- You can also just check the [issues tab](https://github.com/dair-ai/ml-visuals/issues).\n- You can ask anything related to this project in our Slack group\n- Slack channel: #ml_visuals\n\n**Some ideas for figures to add to the Slides** ([issue](https://github.com/dair-ai/ml-visuals/issues/14))\n\n- [ ] Linear regression, single-layer neural network\n- [ ] Multilayer Perceptron with hidden layer\n- [ ] Backpropagation\n- [ ] Batch Normalization and alternatives\n- [ ] Computational Graphs\n- [ ] Dropout\n- [ ] CNN - padding, stride, pooling,... \n- [ ] LeNet\n- [ ] AlexNet\n- [ ] VGG\n- [ ] GoogleNet\n- [ ] ResNet\n- [ ] DenseNet\n- [ ] Memory Networks\n- [ ] RNN\n- [ ] Deep RNN\n- [ ] Bidirectional RNN\n- [ ] GRU\n- [ ] LSTM\n- [ ] Language RNN models\n- [ ] Backpropagation through time\n- [ ] Encoder-Decoder Architecture\n- [ ] Seq2seq with RNN encoder-decoder\n- [ ] Bearm search and other decoding strategies\n- [ ] Attention\n- [ ] Multi-head attention\n- [ ] Self-attention\n- [ ] Transformer\n- [ ] Word2vec/GloVe/Skip-gram/CBOW/BERT/GPT....\n- [ ] Common/Popular CV/NLP Tasks\n\nList adopted from multiple resources including [nlpoverview](https://nlpoverview.com/index.html) and [d2l.ai](https://d2l.ai/) which both contain a very solid syllabus.  \n\n## Examples of Visuals\n![](1.png)\n![](2.png)\n![](3.png)\n', '{"language":null,"stars":16185,"forks":1506,"watchers":16185,"open_issues":44,"topics":["artificial-intelligence","deep-learning","design","machine-learning","natural-language-processing"],"default_branch":"master","size_kb":141,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:dair-ai:ml-visuals","source_url":"https://github.com/dair-ai/ml-visuals"},{"type":"has_code","target_id":"github:orgs:dair-ai","source_url":"https://github.com/orgs/dair-ai"},{"type":"has_code","target_id":"github:dair-ai:ml-visuals","source_url":"https://github.com/dair-ai/ml-visuals"},{"type":"has_code","target_id":"github:dair-ai:ml-visuals","source_url":"https://github.com/dair-ai/ml-visuals"}]', NULL, 'MIT', 'approved', 65, '2a19415931fe1a20eb283192ff83d26c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-dair-ai-ml-visuals from https://github.com/dair-ai.png
Image converted to WebP: data/images/github-dair-ai-ml-visuals.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Significant-Gravitas-AutoGPT', 'github--significant-gravitas--autogpt', 'AutoGPT', 'Significant-Gravitas', '&ensp; &ensp; <!-- Keep these links. Translations will automatically update with the README. --> Deutsch | EspaÃ±ol | franÃ§ais | æ—¥æœ¬èª | í•œêµ­ì–´ | PortuguÃªs | Ğ ÑƒÑÑĞºĞ¸Ğ¹ | ä¸­æ–‡ **AutoGPT** is a powerful platform that allows you to create, deploy, and manage continuous AI agents that automate complex workflows. - Download to self-host (Free!) - Join the Waitlist for the cloud-hosted beta (Closed Beta - Public release Coming Soon!) > [!NOTE] > Setting up and hosting the AutoGPT Platform yourself is a techn...', '["ai","artificial-intelligence","autonomous-agents","gpt-4","llama-api","openai","python","python"]', 'other', 180172, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Significant-Gravitas/AutoGPT","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# AutoGPT: Build, Deploy, and Run AI Agents\n\n[![Discord Follow](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fautogpt%3Fwith_counts%3Dtrue&query=%24.approximate_member_count&label=total%20members&logo=discord&logoColor=white&color=7289da)](https://discord.gg/autogpt) &ensp;\n[![Twitter Follow](https://img.shields.io/twitter/follow/Auto_GPT?style=social)](https://twitter.com/Auto_GPT) &ensp;\n\n<!-- Keep these links. Translations will automatically update with the README. -->\n[Deutsch](https://zdoc.app/de/Significant-Gravitas/AutoGPT) | \n[EspaÃ±ol](https://zdoc.app/es/Significant-Gravitas/AutoGPT) | \n[franÃ§ais](https://zdoc.app/fr/Significant-Gravitas/AutoGPT) | \n[æ—¥æœ¬èª](https://zdoc.app/ja/Significant-Gravitas/AutoGPT) | \n[í•œêµ­ì–´](https://zdoc.app/ko/Significant-Gravitas/AutoGPT) | \n[PortuguÃªs](https://zdoc.app/pt/Significant-Gravitas/AutoGPT) | \n[Ğ ÑƒÑÑĞºĞ¸Ğ¹](https://zdoc.app/ru/Significant-Gravitas/AutoGPT) | \n[ä¸­æ–‡](https://zdoc.app/zh/Significant-Gravitas/AutoGPT)\n\n**AutoGPT** is a powerful platform that allows you to create, deploy, and manage continuous AI agents that automate complex workflows. \n\n## Hosting Options \n   - Download to self-host (Free!)\n   - [Join the Waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta (Closed Beta - Public release Coming Soon!)\n\n## How to Self-Host the AutoGPT Platform\n> [!NOTE]\n> Setting up and hosting the AutoGPT Platform yourself is a technical process. \n> If you''d rather something that just works, we recommend [joining the waitlist](https://bit.ly/3ZDijAI) for the cloud-hosted beta.\n\n### System Requirements\n\nBefore proceeding with the installation, ensure your system meets the following requirements:\n\n#### Hardware Requirements\n- CPU: 4+ cores recommended\n- RAM: Minimum 8GB, 16GB recommended\n- Storage: At least 10GB of free space\n\n#### Software Requirements\n- Operating Systems:\n  - Linux (Ubuntu 20.04 or newer recommended)\n  - macOS (10.15 or newer)\n  - Windows 10/11 with WSL2\n- Required Software (with minimum versions):\n  - Docker Engine (20.10.0 or newer)\n  - Docker Compose (2.0.0 or newer)\n  - Git (2.30 or newer)\n  - Node.js (16.x or newer)\n  - npm (8.x or newer)\n  - VSCode (1.60 or newer) or any modern code editor\n\n#### Network Requirements\n- Stable internet connection\n- Access to required ports (will be configured in Docker)\n- Ability to make outbound HTTPS connections\n\n### Updated Setup Instructions:\nWe''ve moved to a fully maintained and regularly updated documentation site.\n\nğŸ‘‰ [Follow the official self-hosting guide here](https://docs.agpt.co/platform/getting-started/)\n\n\nThis tutorial assumes you have Docker, VSCode, git and npm installed.\n\n---\n\n#### âš¡ Quick Setup with One-Line Script (Recommended for Local Hosting)\n\nSkip the manual steps and get started in minutes using our automatic setup script.\n\nFor macOS/Linux:\n```\ncurl -fsSL https://setup.agpt.co/install.sh -o install.sh && bash install.sh\n```\n\nFor Windows (PowerShell):\n```\npowershell -c "iwr https://setup.agpt.co/install.bat -o install.bat; ./install.bat"\n```\n\nThis will install dependencies, configure Docker, and launch your local instance â€” all in one go.\n\n### ğŸ§± AutoGPT Frontend\n\nThe AutoGPT frontend is where users interact with our powerful AI automation platform. It offers multiple ways to engage with and leverage our AI agents. This is the interface where you''ll bring your AI automation ideas to life:\n\n   **Agent Builder:** For those who want to customize, our intuitive, low-code interface allows you to design and configure your own AI agents. \n   \n   **Workflow Management:** Build, modify, and optimize your automation workflows with ease. You build your agent by connecting blocks, where each block     performs a single action.\n   \n   **Deployment Controls:** Manage the lifecycle of your agents, from testing to production.\n   \n   **Ready-to-Use Agents:** Don''t want to build? Simply select from our library of pre-configured agents and put them to work immediately.\n   \n   **Agent Interaction:** Whether you''ve built your own or are using pre-configured agents, easily run and interact with them through our user-friendly      interface.\n\n   **Monitoring and Analytics:** Keep track of your agents'' performance and gain insights to continually improve your automation processes.\n\n[Read this guide](https://docs.agpt.co/platform/new_blocks/) to learn how to build your own custom blocks.\n\n### ğŸ’½ AutoGPT Server\n\nThe AutoGPT Server is the powerhouse of our platform This is where your agents run. Once deployed, agents can be triggered by external sources and can operate continuously. It contains all the essential components that make AutoGPT run smoothly.\n\n   **Source Code:** The core logic that drives our agents and automation processes.\n   \n   **Infrastructure:** Robust systems that ensure reliable and scalable performance.\n   \n   **Marketplace:** A comprehensive marketplace where you can find and deploy a wide range of pre-built agents.\n\n### ğŸ™ Example Agents\n\nHere are two examples of what you can do with AutoGPT:\n\n1. **Generate Viral Videos from Trending Topics**\n   - This agent reads topics on Reddit.\n   - It identifies trending topics.\n   - It then automatically creates a short-form video based on the content. \n\n2. **Identify Top Quotes from Videos for Social Media**\n   - This agent subscribes to your YouTube channel.\n   - When you post a new video, it transcribes it.\n   - It uses AI to identify the most impactful quotes to generate a summary.\n   - Then, it writes a post to automatically publish to your social media. \n\nThese examples show just a glimpse of what you can achieve with AutoGPT! You can create customized workflows to build agents for any use case.\n\n---\n\n### **License Overview:**\n\nğŸ›¡ï¸ **Polyform Shield License:**\nAll code and content within the `autogpt_platform` folder is licensed under the Polyform Shield License. This new project is our in-developlemt platform for building, deploying and managing agents.</br>_[Read more about this effort](https://agpt.co/blog/introducing-the-autogpt-platform)_\n\nğŸ¦‰ **MIT License:**\nAll other portions of the AutoGPT repository (i.e., everything outside the `autogpt_platform` folder) are licensed under the MIT License. This includes the original stand-alone AutoGPT Agent, along with projects such as [Forge](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/forge), [agbenchmark](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/benchmark) and the [AutoGPT Classic GUI](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/frontend).</br>We also publish additional work under the MIT Licence in other repositories, such as [GravitasML](https://github.com/Significant-Gravitas/gravitasml) which is developed for and used in the AutoGPT Platform. See also our MIT Licenced [Code Ability](https://github.com/Significant-Gravitas/AutoGPT-Code-Ability) project.\n\n---\n### Mission\nOur mission is to provide the tools, so that you can focus on what matters:\n\n- ğŸ—ï¸ **Building** - Lay the foundation for something amazing.\n- ğŸ§ª **Testing** - Fine-tune your agent to perfection.\n- ğŸ¤ **Delegating** - Let AI work for you, and have your ideas come to life.\n\nBe part of the revolution! **AutoGPT** is here to stay, at the forefront of AI innovation.\n\n**ğŸ“– [Documentation](https://docs.agpt.co)**\n&ensp;|&ensp;\n**ğŸš€ [Contributing](CONTRIBUTING.md)**\n\n---\n## ğŸ¤– AutoGPT Classic\n> Below is information about the classic version of AutoGPT.\n\n**ğŸ› ï¸ [Build your own Agent - Quickstart](classic/FORGE-QUICKSTART.md)**\n\n### ğŸ—ï¸ Forge\n\n**Forge your own agent!** &ndash; Forge is a ready-to-go toolkit to build your own agent application. It handles most of the boilerplate code, letting you channel all your creativity into the things that set *your* agent apart. All tutorials are located [here](https://medium.com/@aiedge/autogpt-forge-e3de53cc58ec). Components from [`forge`](/classic/forge/) can also be used individually to speed up development and reduce boilerplate in your agent project.\n\nğŸš€ [**Getting Started with Forge**](https://github.com/Significant-Gravitas/AutoGPT/blob/master/classic/forge/tutorials/001_getting_started.md) &ndash;\nThis guide will walk you through the process of creating your own agent and using the benchmark and user interface.\n\nğŸ“˜ [Learn More](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/forge) about Forge\n\n### ğŸ¯ Benchmark\n\n**Measure your agent''s performance!** The `agbenchmark` can be used with any agent that supports the agent protocol, and the integration with the project''s [CLI] makes it even easier to use with AutoGPT and forge-based agents. The benchmark offers a stringent testing environment. Our framework allows for autonomous, objective performance evaluations, ensuring your agents are primed for real-world action.\n\n<!-- TODO: insert visual demonstrating the benchmark -->\n\nğŸ“¦ [`agbenchmark`](https://pypi.org/project/agbenchmark/) on Pypi\n&ensp;|&ensp;\nğŸ“˜ [Learn More](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/benchmark) about the Benchmark\n\n### ğŸ’» UI\n\n**Makes agents easy to use!** The `frontend` gives you a user-friendly interface to control and monitor your agents. It connects to agents through the [agent protocol](#-agent-protocol), ensuring compatibility with many agents from both inside and outside of our ecosystem.\n\n<!-- TODO: insert screenshot of front end -->\n\nThe frontend works out-of-the-box with all agents in the repo. Just use the [CLI] to run your agent of choice!\n\nğŸ“˜ [Learn More](https://github.com/Significant-Gravitas/AutoGPT/tree/master/classic/frontend) about the Frontend\n\n### âŒ¨ï¸ CLI\n\n[CLI]: #-cli\n\nTo make it as easy as possible to use all of the tools offered by the repository, a CLI is included at the root of the repo:\n\n```shell\n$ ./run\nUsage: cli.py [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  agent      Commands to create, start and stop agents\n  benchmark  Commands to start the benchmark and list tests and categories\n  setup      Installs dependencies needed for your system.\n```\n\nJust clone the repo, install dependencies with `./run setup`, and you should be good to go!\n\n## ğŸ¤” Questions? Problems? Suggestions?\n\n### Get help - [Discord ğŸ’¬](https://discord.gg/autogpt)\n\n[![Join us on Discord](https://invidget.switchblade.xyz/autogpt)](https://discord.gg/autogpt)\n\nTo report a bug or request a feature, create a [GitHub Issue](https://github.com/Significant-Gravitas/AutoGPT/issues/new/choose). Please ensure someone else hasn''t created an issue for the same topic.\n\n## ğŸ¤ Sister projects\n\n### ğŸ”„ Agent Protocol\n\nTo maintain a uniform standard and ensure seamless compatibility with many current and future applications, AutoGPT employs the [agent protocol](https://agentprotocol.ai/) standard by the AI Engineer Foundation. This standardizes the communication pathways from your agent to the frontend and benchmark.\n\n---\n\n## Stars stats\n\n<p align="center">\n<a href="https://star-history.com/#Significant-Gravitas/AutoGPT">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&type=Date&theme=dark" />\n    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&type=Date" />\n    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Significant-Gravitas/AutoGPT&type=Date" />\n  </picture>\n</a>\n</p>\n\n\n## âš¡ Contributors\n\n<a href="https://github.com/Significant-Gravitas/AutoGPT/graphs/contributors" alt="View Contributors">\n  <img src="https://contrib.rocks/image?repo=Significant-Gravitas/AutoGPT&max=1000&columns=10" alt="Contributors" />\n</a>\n', '{"language":"Python","stars":180172,"forks":46185,"watchers":180172,"open_issues":297,"topics":["ai","artificial-intelligence","autonomous-agents","gpt-4","llama-api","openai","python"],"default_branch":"master","size_kb":309077,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"},{"type":"has_code","target_id":"github:Significant-Gravitas:gravitasml","source_url":"https://github.com/Significant-Gravitas/gravitasml"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT-Code-Ability","source_url":"https://github.com/Significant-Gravitas/AutoGPT-Code-Ability"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"},{"type":"has_code","target_id":"github:Significant-Gravitas:AutoGPT","source_url":"https://github.com/Significant-Gravitas/AutoGPT"}]', NULL, 'NOASSERTION', 'approved', 80, '63a0d38ddc945dafe65d7edb9e691830', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Significant-Gravitas-AutoGPT from https://github.com/Significant-Gravitas.png
Image converted to WebP: data/images/github-Significant-Gravitas-AutoGPT.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-n8n-io-n8n', 'github--n8n-io--n8n', 'n8n', 'n8n-io', '!Banner image n8n is a workflow automation platform that gives technical teams the flexibility of code with the speed of no-code. With 400+ integrations, native AI capabilities, and a fair-code license, n8n lets you build powerful automations while maintaining full control over your data and deployments. !n8n.io - Screenshot - **Code When You Need It**: Write JavaScript/Python, add npm packages, or use the visual interface - **AI-Native Platform**: Build AI agent workflows based on LangChain ...', '["ai","apis","automation","cli","data-flow","development","integration-framework","integrations","ipaas","low-code","low-code-platform","mcp","mcp-client","mcp-server","n8n","no-code","self-hosted","typescript","workflow","workflow-automation","typescript"]', 'other', 161254, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/n8n-io/n8n","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '![Banner image](https://user-images.githubusercontent.com/10284570/173569848-c624317f-42b1-45a6-ab09-f0ea3c247648.png)\n\n# n8n - Secure Workflow Automation for Technical Teams\n\nn8n is a workflow automation platform that gives technical teams the flexibility of code with the speed of no-code. With 400+ integrations, native AI capabilities, and a fair-code license, n8n lets you build powerful automations while maintaining full control over your data and deployments.\n\n![n8n.io - Screenshot](https://raw.githubusercontent.com/n8n-io/n8n/master/assets/n8n-screenshot-readme.png)\n\n## Key Capabilities\n\n- **Code When You Need It**: Write JavaScript/Python, add npm packages, or use the visual interface\n- **AI-Native Platform**: Build AI agent workflows based on LangChain with your own data and models\n- **Full Control**: Self-host with our fair-code license or use our [cloud offering](https://app.n8n.cloud/login)\n- **Enterprise-Ready**: Advanced permissions, SSO, and air-gapped deployments\n- **Active Community**: 400+ integrations and 900+ ready-to-use [templates](https://n8n.io/workflows)\n\n## Quick Start\n\nTry n8n instantly with [npx](https://docs.n8n.io/hosting/installation/npm/) (requires [Node.js](https://nodejs.org/en/)):\n\n```\nnpx n8n\n```\n\nOr deploy with [Docker](https://docs.n8n.io/hosting/installation/docker/):\n\n```\ndocker volume create n8n_data\ndocker run -it --rm --name n8n -p 5678:5678 -v n8n_data:/home/node/.n8n docker.n8n.io/n8nio/n8n\n```\n\nAccess the editor at http://localhost:5678\n\n## Resources\n\n- ğŸ“š [Documentation](https://docs.n8n.io)\n- ğŸ”§ [400+ Integrations](https://n8n.io/integrations)\n- ğŸ’¡ [Example Workflows](https://n8n.io/workflows)\n- ğŸ¤– [AI & LangChain Guide](https://docs.n8n.io/advanced-ai/)\n- ğŸ‘¥ [Community Forum](https://community.n8n.io)\n- ğŸ“– [Community Tutorials](https://community.n8n.io/c/tutorials/28)\n\n## Support\n\nNeed help? Our community forum is the place to get support and connect with other users:\n[community.n8n.io](https://community.n8n.io)\n\n## License\n\nn8n is [fair-code](https://faircode.io) distributed under the [Sustainable Use License](https://github.com/n8n-io/n8n/blob/master/LICENSE.md) and [n8n Enterprise License](https://github.com/n8n-io/n8n/blob/master/LICENSE_EE.md).\n\n- **Source Available**: Always visible source code\n- **Self-Hostable**: Deploy anywhere\n- **Extensible**: Add your own nodes and functionality\n\n[Enterprise licenses](mailto:license@n8n.io) available for additional features and support.\n\nAdditional information about the license model can be found in the [docs](https://docs.n8n.io/sustainable-use-license/).\n\n## Contributing\n\nFound a bug ğŸ› or have a feature idea âœ¨? Check our [Contributing Guide](https://github.com/n8n-io/n8n/blob/master/CONTRIBUTING.md) to get started.\n\n## Join the Team\n\nWant to shape the future of automation? Check out our [job posts](https://n8n.io/careers) and join our team!\n\n## What does n8n mean?\n\n**Short answer:** It means "nodemation" and is pronounced as n-eight-n.\n\n**Long answer:** "I get that question quite often (more often than I expected) so I decided it is probably best to answer it here. While looking for a good name for the project with a free domain I realized very quickly that all the good ones I could think of were already taken. So, in the end, I chose nodemation. ''node-'' in the sense that it uses a Node-View and that it uses Node.js and ''-mation'' for ''automation'' which is what the project is supposed to help with. However, I did not like how long the name was and I could not imagine writing something that long every time in the CLI. That is when I then ended up on ''n8n''." - **Jan Oberhauser, Founder and CEO, n8n.io**\n', '{"language":"TypeScript","stars":161254,"forks":51640,"watchers":161254,"open_issues":1313,"topics":["ai","apis","automation","cli","data-flow","development","integration-framework","integrations","ipaas","low-code","low-code-platform","mcp","mcp-client","mcp-server","n8n","no-code","self-hosted","typescript","workflow","workflow-automation"],"default_branch":"master","size_kb":303710,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:n8n-io:n8n","source_url":"https://github.com/n8n-io/n8n"},{"type":"has_code","target_id":"github:n8n-io:n8n","source_url":"https://github.com/n8n-io/n8n"},{"type":"has_code","target_id":"github:n8n-io:n8n","source_url":"https://github.com/n8n-io/n8n"}]', NULL, 'NOASSERTION', 'approved', 65, '3a35f7002dfb43f18dac3118cc475e64', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-n8n-io-n8n from https://github.com/n8n-io.png
Image converted to WebP: data/images/github-n8n-io-n8n.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-langchain-ai-langchain', 'github--langchain-ai--langchain', 'langchain', 'langchain-ai', '<div align="center"> <a href="https://www.langchain.com/"> <picture> <source media="(prefers-color-scheme: light)" srcset=".github/images/logo-dark.svg"> <source media="(prefers-color-scheme: dark)" srcset=".github/images/logo-light.svg"> <img alt="LangChain Logo" src=".github/images/logo-dark.svg" width="80%"> </picture> </a> </div> <div align="center"> <h3>The platform for reliable agents.</h3> </div> <div align="center"> <a href="https://opensource.org/licenses/MIT" target="_blank"><img sr...', '["agents","ai","ai-agents","ai-agents-framework","aiagentframework","anthropic","chatgpt","enterprise","framework","gemini","generative-ai","langchain","llm","multiagent","open-source","openai","pydantic","python","rag","python"]', 'other', 121388, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/langchain-ai/langchain","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n  <a href="https://www.langchain.com/">\n    <picture>\n      <source media="(prefers-color-scheme: light)" srcset=".github/images/logo-dark.svg">\n      <source media="(prefers-color-scheme: dark)" srcset=".github/images/logo-light.svg">\n      <img alt="LangChain Logo" src=".github/images/logo-dark.svg" width="80%">\n    </picture>\n  </a>\n</div>\n\n<div align="center">\n  <h3>The platform for reliable agents.</h3>\n</div>\n\n<div align="center">\n  <a href="https://opensource.org/licenses/MIT" target="_blank"><img src="https://img.shields.io/pypi/l/langchain" alt="PyPI - License"></a>\n  <a href="https://pypistats.org/packages/langchain" target="_blank"><img src="https://img.shields.io/pepy/dt/langchain" alt="PyPI - Downloads"></a>\n  <a href="https://pypi.org/project/langchain/#history" target="_blank"><img src="https://img.shields.io/pypi/v/langchain?label=%20" alt="Version"></a>\n  <a href="https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/langchain-ai/langchain" target="_blank"><img src="https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode" alt="Open in Dev Containers"></a>\n  <a href="https://codespaces.new/langchain-ai/langchain" target="_blank"><img src="https://github.com/codespaces/badge.svg" alt="Open in Github Codespace" title="Open in Github Codespace" width="150" height="20"></a>\n  <a href="https://codspeed.io/langchain-ai/langchain" target="_blank"><img src="https://img.shields.io/endpoint?url=https://codspeed.io/badge.json" alt="CodSpeed Badge"></a>\n  <a href="https://twitter.com/langchainai" target="_blank"><img src="https://img.shields.io/twitter/url/https/twitter.com/langchainai.svg?style=social&label=Follow%20%40LangChainAI" alt="Twitter / X"></a>\n</div>\n\nLangChain is a framework for building agents and LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development â€“ all while future-proofing decisions as the underlying technology evolves.\n\n```bash\npip install langchain\n```\n\nIf you''re looking for more advanced customization or agent orchestration, check out [LangGraph](https://docs.langchain.com/oss/python/langgraph/overview), our framework for building controllable agent workflows.\n\n---\n\n**Documentation**:\n\n- [docs.langchain.com](https://docs.langchain.com/oss/python/langchain/overview) â€“ Comprehensive documentation, including conceptual overviews and guides\n- [reference.langchain.com/python](https://reference.langchain.com/python) â€“ API reference docs for LangChain packages\n\n**Discussions**: Visit the [LangChain Forum](https://forum.langchain.com) to connect with the community and share all of your technical questions, ideas, and feedback.\n\n> [!NOTE]\n> Looking for the JS/TS library? Check out [LangChain.js](https://github.com/langchain-ai/langchainjs).\n\n## Why use LangChain?\n\nLangChain helps developers build applications powered by LLMs through a standard interface for models, embeddings, vector stores, and more.\n\nUse LangChain for:\n\n- **Real-time data augmentation**. Easily connect LLMs to diverse data sources and external/internal systems, drawing from LangChain''s vast library of integrations with model providers, tools, vector stores, retrievers, and more.\n- **Model interoperability**. Swap models in and out as your engineering team experiments to find the best choice for your application''s needs. As the industry frontier evolves, adapt quickly â€“ LangChain''s abstractions keep you moving without losing momentum.\n- **Rapid prototyping**. Quickly build and iterate on LLM applications with LangChain''s modular, component-based architecture. Test different approaches and workflows without rebuilding from scratch, accelerating your development cycle.\n- **Production-ready features**. Deploy reliable applications with built-in support for monitoring, evaluation, and debugging through integrations like LangSmith. Scale with confidence using battle-tested patterns and best practices.\n- **Vibrant community and ecosystem**. Leverage a rich ecosystem of integrations, templates, and community-contributed components. Benefit from continuous improvements and stay up-to-date with the latest AI developments through an active open-source community.\n- **Flexible abstraction layers**. Work at the level of abstraction that suits your needs - from high-level chains for quick starts to low-level components for fine-grained control. LangChain grows with your application''s complexity.\n\n## LangChain ecosystem\n\nWhile the LangChain framework can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools when building LLM applications.\n\nTo improve your LLM application development, pair LangChain with:\n\n- [LangGraph](https://docs.langchain.com/oss/python/langgraph/overview) â€“ Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows â€“ and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.\n- [Integrations](https://docs.langchain.com/oss/python/integrations/providers/overview) â€“ List of LangChain integrations, including chat & embedding models, tools & toolkits, and more\n- [LangSmith](https://www.langchain.com/langsmith) â€“ Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\n- [LangSmith Deployment](https://docs.langchain.com/langsmith/deployments) â€“ Deploy and scale agents effortlessly with a purpose-built deployment platform for long-running, stateful workflows. Discover, reuse, configure, and share agents across teams â€“ and iterate quickly with visual prototyping in [LangSmith Studio](https://docs.langchain.com/langsmith/studio).\n- [Deep Agents](https://github.com/langchain-ai/deepagents) *(new!)* â€“ Build agents that can plan, use subagents, and leverage file systems for complex tasks\n\n## Additional resources\n\n- [API Reference](https://reference.langchain.com/python) â€“ Detailed reference on navigating base packages and integrations for LangChain.\n- [Contributing Guide](https://docs.langchain.com/oss/python/contributing/overview) â€“ Learn how to contribute to LangChain projects and find good first issues.\n- [Code of Conduct](https://github.com/langchain-ai/langchain/blob/master/.github/CODE_OF_CONDUCT.md) â€“ Our community guidelines and standards for participation.\n', '{"language":"Python","stars":121388,"forks":20013,"watchers":121388,"open_issues":339,"topics":["agents","ai","ai-agents","ai-agents-framework","aiagentframework","anthropic","chatgpt","enterprise","framework","gemini","generative-ai","langchain","llm","multiagent","open-source","openai","pydantic","python","rag"],"default_branch":"master","size_kb":499322,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:langchain-ai:langchain\"","source_url":"https://github.com/langchain-ai/langchain\""},{"type":"has_code","target_id":"github:codespaces:badge.svg\"","source_url":"https://github.com/codespaces/badge.svg\""},{"type":"has_code","target_id":"github:langchain-ai:langchainjs","source_url":"https://github.com/langchain-ai/langchainjs"},{"type":"has_code","target_id":"github:langchain-ai:deepagents","source_url":"https://github.com/langchain-ai/deepagents"},{"type":"has_code","target_id":"github:langchain-ai:langchain","source_url":"https://github.com/langchain-ai/langchain"}]', NULL, 'MIT', 'approved', 65, '22dc3b11f24ba6584642a92694f4d8e3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-langchain-ai-langchain from https://github.com/langchain-ai.png
Image converted to WebP: data/images/github-langchain-ai-langchain.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-langgenius-dify', 'github--langgenius--dify', 'dify', 'langgenius', '!cover-v5-optimized <p align="center"> ğŸ“Œ <a href="https://dify.ai/blog/introducing-dify-workflow-file-upload-a-demo-on-ai-podcast">Introducing Dify Workflow File Upload: Recreate Google NotebookLM Podcast</a> </p> <p align="center"> <a href="https://cloud.dify.ai">Dify Cloud</a> Â· <a href="https://docs.dify.ai/getting-started/install-self-hosted">Self-hosting</a> Â· <a href="https://docs.dify.ai">Documentation</a> Â· <a href="https://dify.ai/pricing">Dify edition overview</a> </p> <p align="ce...', '["agent","agentic-ai","agentic-framework","agentic-workflow","ai","automation","gemini","genai","gpt","gpt-4","llm","low-code","mcp","nextjs","no-code","openai","orchestration","python","rag","workflow","typescript"]', 'other', 120848, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/langgenius/dify","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '![cover-v5-optimized](./images/GitHub_README_if.png)\n\n<p align="center">\n  ğŸ“Œ <a href="https://dify.ai/blog/introducing-dify-workflow-file-upload-a-demo-on-ai-podcast">Introducing Dify Workflow File Upload: Recreate Google NotebookLM Podcast</a>\n</p>\n\n<p align="center">\n  <a href="https://cloud.dify.ai">Dify Cloud</a> Â·\n  <a href="https://docs.dify.ai/getting-started/install-self-hosted">Self-hosting</a> Â·\n  <a href="https://docs.dify.ai">Documentation</a> Â·\n  <a href="https://dify.ai/pricing">Dify edition overview</a>\n</p>\n\n<p align="center">\n    <a href="https://dify.ai" target="_blank">\n        <img alt="Static Badge" src="https://img.shields.io/badge/Product-F04438"></a>\n    <a href="https://dify.ai/pricing" target="_blank">\n        <img alt="Static Badge" src="https://img.shields.io/badge/free-pricing?logo=free&color=%20%23155EEF&label=pricing&labelColor=%20%23528bff"></a>\n    <a href="https://discord.gg/FngNHpbcY7" target="_blank">\n        <img src="https://img.shields.io/discord/1082486657678311454?logo=discord&labelColor=%20%235462eb&logoColor=%20%23f5f5f5&color=%20%235462eb"\n            alt="chat on Discord"></a>\n    <a href="https://reddit.com/r/difyai" target="_blank">  \n        <img src="https://img.shields.io/reddit/subreddit-subscribers/difyai?style=plastic&logo=reddit&label=r%2Fdifyai&labelColor=white"\n            alt="join Reddit"></a>\n    <a href="https://twitter.com/intent/follow?screen_name=dify_ai" target="_blank">\n        <img src="https://img.shields.io/twitter/follow/dify_ai?logo=X&color=%20%23f5f5f5"\n            alt="follow on X(Twitter)"></a>\n    <a href="https://www.linkedin.com/company/langgenius/" target="_blank">\n        <img src="https://custom-icon-badges.demolab.com/badge/LinkedIn-0A66C2?logo=linkedin-white&logoColor=fff"\n            alt="follow on LinkedIn"></a>\n    <a href="https://hub.docker.com/u/langgenius" target="_blank">\n        <img alt="Docker Pulls" src="https://img.shields.io/docker/pulls/langgenius/dify-web?labelColor=%20%23FDB062&color=%20%23f79009"></a>\n    <a href="https://github.com/langgenius/dify/graphs/commit-activity" target="_blank">\n        <img alt="Commits last month" src="https://img.shields.io/github/commit-activity/m/langgenius/dify?labelColor=%20%2332b583&color=%20%2312b76a"></a>\n    <a href="https://github.com/langgenius/dify/" target="_blank">\n        <img alt="Issues closed" src="https://img.shields.io/github/issues-search?query=repo%3Alanggenius%2Fdify%20is%3Aclosed&label=issues%20closed&labelColor=%20%237d89b0&color=%20%235d6b98"></a>\n    <a href="https://github.com/langgenius/dify/discussions/" target="_blank">\n        <img alt="Discussion posts" src="https://img.shields.io/github/discussions/langgenius/dify?labelColor=%20%239b8afb&color=%20%237a5af8"></a>\n    <a href="https://insights.linuxfoundation.org/project/langgenius-dify" target="_blank">\n        <img alt="LFX Health Score" src="https://insights.linuxfoundation.org/api/badge/health-score?project=langgenius-dify"></a>\n    <a href="https://insights.linuxfoundation.org/project/langgenius-dify" target="_blank">\n        <img alt="LFX Contributors" src="https://insights.linuxfoundation.org/api/badge/contributors?project=langgenius-dify"></a>\n    <a href="https://insights.linuxfoundation.org/project/langgenius-dify" target="_blank">\n        <img alt="LFX Active Contributors" src="https://insights.linuxfoundation.org/api/badge/active-contributors?project=langgenius-dify"></a>\n</p>\n\n<p align="center">\n  <a href="./README.md"><img alt="README in English" src="https://img.shields.io/badge/English-d9d9d9"></a>\n  <a href="./docs/zh-TW/README.md"><img alt="ç¹é«”ä¸­æ–‡æ–‡ä»¶" src="https://img.shields.io/badge/ç¹é«”ä¸­æ–‡-d9d9d9"></a>\n  <a href="./docs/zh-CN/README.md"><img alt="ç®€ä½“ä¸­æ–‡æ–‡ä»¶" src="https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-d9d9d9"></a>\n  <a href="./docs/ja-JP/README.md"><img alt="æ—¥æœ¬èªã®README" src="https://img.shields.io/badge/æ—¥æœ¬èª-d9d9d9"></a>\n  <a href="./docs/es-ES/README.md"><img alt="README en EspaÃ±ol" src="https://img.shields.io/badge/EspaÃ±ol-d9d9d9"></a>\n  <a href="./docs/fr-FR/README.md"><img alt="README en FranÃ§ais" src="https://img.shields.io/badge/FranÃ§ais-d9d9d9"></a>\n  <a href="./docs/tlh/README.md"><img alt="README tlhIngan Hol" src="https://img.shields.io/badge/Klingon-d9d9d9"></a>\n  <a href="./docs/ko-KR/README.md"><img alt="README in Korean" src="https://img.shields.io/badge/í•œêµ­ì–´-d9d9d9"></a>\n  <a href="./docs/ar-SA/README.md"><img alt="README Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©" src="https://img.shields.io/badge/Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©-d9d9d9"></a>\n  <a href="./docs/tr-TR/README.md"><img alt="TÃ¼rkÃ§e README" src="https://img.shields.io/badge/TÃ¼rkÃ§e-d9d9d9"></a>\n  <a href="./docs/vi-VN/README.md"><img alt="README Tiáº¿ng Viá»‡t" src="https://img.shields.io/badge/Ti%E1%BA%BFng%20Vi%E1%BB%87t-d9d9d9"></a>\n  <a href="./docs/de-DE/README.md"><img alt="README in Deutsch" src="https://img.shields.io/badge/German-d9d9d9"></a>\n  <a href="./docs/bn-BD/README.md"><img alt="README in à¦¬à¦¾à¦‚à¦²à¦¾" src="https://img.shields.io/badge/à¦¬à¦¾à¦‚à¦²à¦¾-d9d9d9"></a>\n</p>\n\nDify is an open-source platform for developing LLM applications. Its intuitive interface combines agentic AI workflows, RAG pipelines, agent capabilities, model management, observability features, and moreâ€”allowing you to quickly move from prototype to production.\n\n## Quick start\n\n> Before installing Dify, make sure your machine meets the following minimum system requirements:\n>\n> - CPU >= 2 Core\n> - RAM >= 4 GiB\n\n<br/>\n\nThe easiest way to start the Dify server is through [Docker Compose](docker/docker-compose.yaml). Before running Dify with the following commands, make sure that [Docker](https://docs.docker.com/get-docker/) and [Docker Compose](https://docs.docker.com/compose/install/) are installed on your machine:\n\n```bash\ncd dify\ncd docker\ncp .env.example .env\ndocker compose up -d\n```\n\nAfter running, you can access the Dify dashboard in your browser at [http://localhost/install](http://localhost/install) and start the initialization process.\n\n#### Seeking help\n\nPlease refer to our [FAQ](https://docs.dify.ai/getting-started/install-self-hosted/faqs) if you encounter problems setting up Dify. Reach out to [the community and us](#community--contact) if you are still having issues.\n\n> If you''d like to contribute to Dify or do additional development, refer to our [guide to deploying from source code](https://docs.dify.ai/getting-started/install-self-hosted/local-source-code)\n\n## Key features\n\n**1. Workflow**:\nBuild and test powerful AI workflows on a visual canvas, leveraging all the following features and beyond.\n\n**2. Comprehensive model support**:\nSeamless integration with hundreds of proprietary / open-source LLMs from dozens of inference providers and self-hosted solutions, covering GPT, Mistral, Llama3, and any OpenAI API-compatible models. A full list of supported model providers can be found [here](https://docs.dify.ai/getting-started/readme/model-providers).\n\n![providers-v5](https://github.com/langgenius/dify/assets/13230914/5a17bdbe-097a-4100-8363-40255b70f6e3)\n\n**3. Prompt IDE**:\nIntuitive interface for crafting prompts, comparing model performance, and adding additional features such as text-to-speech to a chat-based app.\n\n**4. RAG Pipeline**:\nExtensive RAG capabilities that cover everything from document ingestion to retrieval, with out-of-box support for text extraction from PDFs, PPTs, and other common document formats.\n\n**5. Agent capabilities**:\nYou can define agents based on LLM Function Calling or ReAct, and add pre-built or custom tools for the agent. Dify provides 50+ built-in tools for AI agents, such as Google Search, DALLÂ·E, Stable Diffusion and WolframAlpha.\n\n**6. LLMOps**:\nMonitor and analyze application logs and performance over time. You could continuously improve prompts, datasets, and models based on production data and annotations.\n\n**7. Backend-as-a-Service**:\nAll of Dify''s offerings come with corresponding APIs, so you could effortlessly integrate Dify into your own business logic.\n\n## Using Dify\n\n- **Cloud <br/>**\n  We host a [Dify Cloud](https://dify.ai) service for anyone to try with zero setup. It provides all the capabilities of the self-deployed version, and includes 200 free GPT-4 calls in the sandbox plan.\n\n- **Self-hosting Dify Community Edition<br/>**\n  Quickly get Dify running in your environment with this [starter guide](#quick-start).\n  Use our [documentation](https://docs.dify.ai) for further references and more in-depth instructions.\n\n- **Dify for enterprise / organizations<br/>**\n  We provide additional enterprise-centric features. [Send us an email](mailto:business@dify.ai?subject=%5BGitHub%5DBusiness%20License%20Inquiry) to discuss your enterprise needs. <br/>\n\n  > For startups and small businesses using AWS, check out [Dify Premium on AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-t22mebxzwjhu6) and deploy it to your own AWS VPC with one click. It''s an affordable AMI offering with the option to create apps with custom logo and branding.\n\n## Staying ahead\n\nStar Dify on GitHub and be instantly notified of new releases.\n\n![star-us](https://github.com/langgenius/dify/assets/13230914/b823edc1-6388-4e25-ad45-2f6b187adbb4)\n\n## Advanced Setup\n\n### Custom configurations\n\nIf you need to customize the configuration, please refer to the comments in our [.env.example](docker/.env.example) file and update the corresponding values in your `.env` file. Additionally, you might need to make adjustments to the `docker-compose.yaml` file itself, such as changing image versions, port mappings, or volume mounts, based on your specific deployment environment and requirements. After making any changes, please re-run `docker-compose up -d`. You can find the full list of available environment variables [here](https://docs.dify.ai/getting-started/install-self-hosted/environments).\n\n#### Customizing Suggested Questions\n\nYou can now customize the "Suggested Questions After Answer" feature to better fit your use case. For example, to generate longer, more technical questions:\n\n```bash\n# In your .env file\nSUGGESTED_QUESTIONS_PROMPT=''Please help me predict the five most likely technical follow-up questions a developer would ask. Focus on implementation details, best practices, and architecture considerations. Keep each question between 40-60 characters. Output must be JSON array: ["question1","question2","question3","question4","question5"]''\nSUGGESTED_QUESTIONS_MAX_TOKENS=512\nSUGGESTED_QUESTIONS_TEMPERATURE=0.3\n```\n\nSee the [Suggested Questions Configuration Guide](docs/suggested-questions-configuration.md) for detailed examples and usage instructions.\n\n### Metrics Monitoring with Grafana\n\nImport the dashboard to Grafana, using Dify''s PostgreSQL database as data source, to monitor metrics in granularity of apps, tenants, messages, and more.\n\n- [Grafana Dashboard by @bowenliang123](https://github.com/bowenliang123/dify-grafana-dashboard)\n\n### Deployment with Kubernetes\n\nIf you''d like to configure a highly-available setup, there are community-contributed [Helm Charts](https://helm.sh/) and YAML files which allow Dify to be deployed on Kubernetes.\n\n- [Helm Chart by @LeoQuote](https://github.com/douban/charts/tree/master/charts/dify)\n- [Helm Chart by @BorisPolonsky](https://github.com/BorisPolonsky/dify-helm)\n- [Helm Chart by @magicsong](https://github.com/magicsong/ai-charts)\n- [YAML file by @Winson-030](https://github.com/Winson-030/dify-kubernetes)\n- [YAML file by @wyy-holding](https://github.com/wyy-holding/dify-k8s)\n- [ğŸš€ NEW! YAML files (Supports Dify v1.6.0) by @Zhoneym](https://github.com/Zhoneym/DifyAI-Kubernetes)\n\n#### Using Terraform for Deployment\n\nDeploy Dify to Cloud Platform with a single click using [terraform](https://www.terraform.io/)\n\n##### Azure Global\n\n- [Azure Terraform by @nikawang](https://github.com/nikawang/dify-azure-terraform)\n\n##### Google Cloud\n\n- [Google Cloud Terraform by @sotazum](https://github.com/DeNA/dify-google-cloud-terraform)\n\n#### Using AWS CDK for Deployment\n\nDeploy Dify to AWS with [CDK](https://aws.amazon.com/cdk/)\n\n##### AWS\n\n- [AWS CDK by @KevinZhao (EKS based)](https://github.com/aws-samples/solution-for-deploying-dify-on-aws)\n- [AWS CDK by @tmokmss (ECS based)](https://github.com/aws-samples/dify-self-hosted-on-aws)\n\n#### Using Alibaba Cloud Computing Nest\n\nQuickly deploy Dify to Alibaba cloud with [Alibaba Cloud Computing Nest](https://computenest.console.aliyun.com/service/instance/create/default?type=user&ServiceName=Dify%E7%A4%BE%E5%8C%BA%E7%89%88)\n\n#### Using Alibaba Cloud Data Management\n\nOne-Click deploy Dify to Alibaba Cloud with [Alibaba Cloud Data Management](https://www.alibabacloud.com/help/en/dms/dify-in-invitational-preview/)\n\n#### Deploy to AKS with Azure Devops Pipeline\n\nOne-Click deploy Dify to AKS with [Azure Devops Pipeline Helm Chart by @LeoZhang](https://github.com/Ruiruiz30/Dify-helm-chart-AKS)\n\n## Contributing\n\nFor those who''d like to contribute code, see our [Contribution Guide](https://github.com/langgenius/dify/blob/main/CONTRIBUTING.md).\nAt the same time, please consider supporting Dify by sharing it on social media and at events and conferences.\n\n> We are looking for contributors to help translate Dify into languages other than Mandarin or English. If you are interested in helping, please see the [i18n README](https://github.com/langgenius/dify/blob/main/web/i18n-config/README.md) for more information, and leave us a comment in the `global-users` channel of our [Discord Community Server](https://discord.gg/8Tpq4AcN9c).\n\n## Community & contact\n\n- [GitHub Discussion](https://github.com/langgenius/dify/discussions). Best for: sharing feedback and asking questions.\n- [GitHub Issues](https://github.com/langgenius/dify/issues). Best for: bugs you encounter using Dify.AI, and feature proposals. See our [Contribution Guide](https://github.com/langgenius/dify/blob/main/CONTRIBUTING.md).\n- [Discord](https://discord.gg/FngNHpbcY7). Best for: sharing your applications and hanging out with the community.\n- [X(Twitter)](https://twitter.com/dify_ai). Best for: sharing your applications and hanging out with the community.\n\n**Contributors**\n\n<a href="https://github.com/langgenius/dify/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=langgenius/dify" />\n</a>\n\n## Star history\n\n[![Star History Chart](https://api.star-history.com/svg?repos=langgenius/dify&type=Date)](https://star-history.com/#langgenius/dify&Date)\n\n## Security disclosure\n\nTo protect your privacy, please avoid posting security issues on GitHub. Instead, report issues to security@dify.ai, and our team will respond with detailed answer.\n\n## License\n\nThis repository is licensed under the [Dify Open Source License](LICENSE), based on Apache 2.0 with additional conditions.\n', '{"language":"TypeScript","stars":120848,"forks":18780,"watchers":120848,"open_issues":681,"topics":["agent","agentic-ai","agentic-framework","agentic-workflow","ai","automation","gemini","genai","gpt","gpt-4","llm","low-code","mcp","nextjs","no-code","openai","orchestration","python","rag","workflow"],"default_branch":"main","size_kb":180565,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:bowenliang123:dify-grafana-dashboard","source_url":"https://github.com/bowenliang123/dify-grafana-dashboard"},{"type":"has_code","target_id":"github:douban:charts","source_url":"https://github.com/douban/charts"},{"type":"has_code","target_id":"github:BorisPolonsky:dify-helm","source_url":"https://github.com/BorisPolonsky/dify-helm"},{"type":"has_code","target_id":"github:magicsong:ai-charts","source_url":"https://github.com/magicsong/ai-charts"},{"type":"has_code","target_id":"github:Winson-030:dify-kubernetes","source_url":"https://github.com/Winson-030/dify-kubernetes"},{"type":"has_code","target_id":"github:wyy-holding:dify-k8s","source_url":"https://github.com/wyy-holding/dify-k8s"},{"type":"has_code","target_id":"github:Zhoneym:DifyAI-Kubernetes","source_url":"https://github.com/Zhoneym/DifyAI-Kubernetes"},{"type":"has_code","target_id":"github:nikawang:dify-azure-terraform","source_url":"https://github.com/nikawang/dify-azure-terraform"},{"type":"has_code","target_id":"github:DeNA:dify-google-cloud-terraform","source_url":"https://github.com/DeNA/dify-google-cloud-terraform"},{"type":"has_code","target_id":"github:aws-samples:solution-for-deploying-dify-on-aws","source_url":"https://github.com/aws-samples/solution-for-deploying-dify-on-aws"},{"type":"has_code","target_id":"github:aws-samples:dify-self-hosted-on-aws","source_url":"https://github.com/aws-samples/dify-self-hosted-on-aws"},{"type":"has_code","target_id":"github:Ruiruiz30:Dify-helm-chart-AKS","source_url":"https://github.com/Ruiruiz30/Dify-helm-chart-AKS"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"},{"type":"has_code","target_id":"github:langgenius:dify","source_url":"https://github.com/langgenius/dify"}]', NULL, 'NOASSERTION', 'approved', 80, '7e25c28120be0a2a1279d33460894fb1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-langgenius-dify from https://github.com/langgenius.png
Image converted to WebP: data/images/github-langgenius-dify.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-open-webui-open-webui', 'github--open-webui--open-webui', 'open-webui', 'open-webui', '!GitHub stars !GitHub forks !GitHub watchers !GitHub repo size !GitHub language count !GitHub top language !GitHub last commit **Open WebUI is an extensible, feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline.** It supports various LLM runners like **Ollama** and **OpenAI-compatible APIs**, with **built-in inference engine** for RAG, making it a **powerful AI deployment solution**. Passionate about open-source AI? Join our team â†’ !Open WebUI Demo > [!...', '["ai","llm","llm-ui","llm-webui","llms","mcp","ollama","ollama-webui","open-webui","openai","openapi","rag","self-hosted","ui","webui","svelte"]', 'other', 117240, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/open-webui/open-webui","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Open WebUI ğŸ‘‹\n\n![GitHub stars](https://img.shields.io/github/stars/open-webui/open-webui?style=social)\n![GitHub forks](https://img.shields.io/github/forks/open-webui/open-webui?style=social)\n![GitHub watchers](https://img.shields.io/github/watchers/open-webui/open-webui?style=social)\n![GitHub repo size](https://img.shields.io/github/repo-size/open-webui/open-webui)\n![GitHub language count](https://img.shields.io/github/languages/count/open-webui/open-webui)\n![GitHub top language](https://img.shields.io/github/languages/top/open-webui/open-webui)\n![GitHub last commit](https://img.shields.io/github/last-commit/open-webui/open-webui?color=red)\n[![Discord](https://img.shields.io/badge/Discord-Open_WebUI-blue?logo=discord&logoColor=white)](https://discord.gg/5rJgQTnV4s)\n[![](https://img.shields.io/static/v1?label=Sponsor&message=%E2%9D%A4&logo=GitHub&color=%23fe8e86)](https://github.com/sponsors/tjbck)\n\n**Open WebUI is an [extensible](https://docs.openwebui.com/features/plugin/), feature-rich, and user-friendly self-hosted AI platform designed to operate entirely offline.** It supports various LLM runners like **Ollama** and **OpenAI-compatible APIs**, with **built-in inference engine** for RAG, making it a **powerful AI deployment solution**.\n\nPassionate about open-source AI? [Join our team â†’](https://careers.openwebui.com/)\n\n![Open WebUI Demo](./demo.gif)\n\n> [!TIP]  \n> **Looking for an [Enterprise Plan](https://docs.openwebui.com/enterprise)?** â€“ **[Speak with Our Sales Team Today!](https://docs.openwebui.com/enterprise)**\n>\n> Get **enhanced capabilities**, including **custom theming and branding**, **Service Level Agreement (SLA) support**, **Long-Term Support (LTS) versions**, and **more!**\n\nFor more information, be sure to check out our [Open WebUI Documentation](https://docs.openwebui.com/).\n\n## Key Features of Open WebUI â­\n\n- ğŸš€ **Effortless Setup**: Install seamlessly using Docker or Kubernetes (kubectl, kustomize or helm) for a hassle-free experience with support for both `:ollama` and `:cuda` tagged images.\n\n- ğŸ¤ **Ollama/OpenAI API Integration**: Effortlessly integrate OpenAI-compatible APIs for versatile conversations alongside Ollama models. Customize the OpenAI API URL to link with **LMStudio, GroqCloud, Mistral, OpenRouter, and more**.\n\n- ğŸ›¡ï¸ **Granular Permissions and User Groups**: By allowing administrators to create detailed user roles and permissions, we ensure a secure user environment. This granularity not only enhances security but also allows for customized user experiences, fostering a sense of ownership and responsibility amongst users.\n\n- ğŸ“± **Responsive Design**: Enjoy a seamless experience across Desktop PC, Laptop, and Mobile devices.\n\n- ğŸ“± **Progressive Web App (PWA) for Mobile**: Enjoy a native app-like experience on your mobile device with our PWA, providing offline access on localhost and a seamless user interface.\n\n- âœ’ï¸ğŸ”¢ **Full Markdown and LaTeX Support**: Elevate your LLM experience with comprehensive Markdown and LaTeX capabilities for enriched interaction.\n\n- ğŸ¤ğŸ“¹ **Hands-Free Voice/Video Call**: Experience seamless communication with integrated hands-free voice and video call features using multiple Speech-to-Text providers (Local Whisper, OpenAI, Deepgram, Azure) and Text-to-Speech engines (Azure, ElevenLabs, OpenAI, Transformers, WebAPI), allowing for dynamic and interactive chat environments.\n\n- ğŸ› ï¸ **Model Builder**: Easily create Ollama models via the Web UI. Create and add custom characters/agents, customize chat elements, and import models effortlessly through [Open WebUI Community](https://openwebui.com/) integration.\n\n- ğŸ **Native Python Function Calling Tool**: Enhance your LLMs with built-in code editor support in the tools workspace. Bring Your Own Function (BYOF) by simply adding your pure Python functions, enabling seamless integration with LLMs.\n\n- ğŸ’¾ **Persistent Artifact Storage**: Built-in key-value storage API for artifacts, enabling features like journals, trackers, leaderboards, and collaborative tools with both personal and shared data scopes across sessions.\n\n- ğŸ“š **Local RAG Integration**: Dive into the future of chat interactions with groundbreaking Retrieval Augmented Generation (RAG) support using your choice of 9 vector databases and multiple content extraction engines (Tika, Docling, Document Intelligence, Mistral OCR, External loaders). Load documents directly into chat or add files to your document library, effortlessly accessing them using the `#` command before a query.\n\n- ğŸ” **Web Search for RAG**: Perform web searches using 15+ providers including `SearXNG`, `Google PSE`, `Brave Search`, `Kagi`, `Mojeek`, `Tavily`, `Perplexity`, `serpstack`, `serper`, `Serply`, `DuckDuckGo`, `SearchApi`, `SerpApi`, `Bing`, `Jina`, `Exa`, `Sougou`, `Azure AI Search`, and `Ollama Cloud`, injecting results directly into your chat experience.\n\n- ğŸŒ **Web Browsing Capability**: Seamlessly integrate websites into your chat experience using the `#` command followed by a URL. This feature allows you to incorporate web content directly into your conversations, enhancing the richness and depth of your interactions.\n\n- ğŸ¨ **Image Generation & Editing Integration**: Create and edit images using multiple engines including OpenAI''s DALL-E, Gemini, ComfyUI (local), and AUTOMATIC1111 (local), with support for both generation and prompt-based editing workflows.\n\n- âš™ï¸ **Many Models Conversations**: Effortlessly engage with various models simultaneously, harnessing their unique strengths for optimal responses. Enhance your experience by leveraging a diverse set of models in parallel.\n\n- ğŸ” **Role-Based Access Control (RBAC)**: Ensure secure access with restricted permissions; only authorized individuals can access your Ollama, and exclusive model creation/pulling rights are reserved for administrators.\n\n- ğŸ—„ï¸ **Flexible Database & Storage Options**: Choose from SQLite (with optional encryption), PostgreSQL, or configure cloud storage backends (S3, Google Cloud Storage, Azure Blob Storage) for scalable deployments.\n\n- ğŸ” **Advanced Vector Database Support**: Select from 9 vector database options including ChromaDB, PGVector, Qdrant, Milvus, Elasticsearch, OpenSearch, Pinecone, S3Vector, and Oracle 23ai for optimal RAG performance.\n\n- ğŸ” **Enterprise Authentication**: Full support for LDAP/Active Directory integration, SCIM 2.0 automated provisioning, and SSO via trusted headers alongside OAuth providers. Enterprise-grade user and group provisioning through SCIM 2.0 protocol, enabling seamless integration with identity providers like Okta, Azure AD, and Google Workspace for automated user lifecycle management.\n\n- â˜ï¸ **Cloud-Native Integration**: Native support for Google Drive and OneDrive/SharePoint file picking, enabling seamless document import from enterprise cloud storage.\n\n- ğŸ“Š **Production Observability**: Built-in OpenTelemetry support for traces, metrics, and logs, enabling comprehensive monitoring with your existing observability stack.\n\n- âš–ï¸ **Horizontal Scalability**: Redis-backed session management and WebSocket support for multi-worker and multi-node deployments behind load balancers.\n\n- ğŸŒğŸŒ **Multilingual Support**: Experience Open WebUI in your preferred language with our internationalization (i18n) support. Join us in expanding our supported languages! We''re actively seeking contributors!\n\n- ğŸ§© **Pipelines, Open WebUI Plugin Support**: Seamlessly integrate custom logic and Python libraries into Open WebUI using [Pipelines Plugin Framework](https://github.com/open-webui/pipelines). Launch your Pipelines instance, set the OpenAI URL to the Pipelines URL, and explore endless possibilities. [Examples](https://github.com/open-webui/pipelines/tree/main/examples) include **Function Calling**, User **Rate Limiting** to control access, **Usage Monitoring** with tools like Langfuse, **Live Translation with LibreTranslate** for multilingual support, **Toxic Message Filtering** and much more.\n\n- ğŸŒŸ **Continuous Updates**: We are committed to improving Open WebUI with regular updates, fixes, and new features.\n\nWant to learn more about Open WebUI''s features? Check out our [Open WebUI documentation](https://docs.openwebui.com/features) for a comprehensive overview!\n\n---\n\nWe are incredibly grateful for the generous support of our sponsors. Their contributions help us to maintain and improve our project, ensuring we can continue to deliver quality work to our community. Thank you!\n\n## How to Install ğŸš€\n\n### Installation via Python pip ğŸ\n\nOpen WebUI can be installed using pip, the Python package installer. Before proceeding, ensure you''re using **Python 3.11** to avoid compatibility issues.\n\n1. **Install Open WebUI**:\n   Open your terminal and run the following command to install Open WebUI:\n\n   ```bash\n   pip install open-webui\n   ```\n\n2. **Running Open WebUI**:\n   After installation, you can start Open WebUI by executing:\n\n   ```bash\n   open-webui serve\n   ```\n\nThis will start the Open WebUI server, which you can access at [http://localhost:8080](http://localhost:8080)\n\n### Quick Start with Docker ğŸ³\n\n> [!NOTE]  \n> Please note that for certain Docker environments, additional configurations might be needed. If you encounter any connection issues, our detailed guide on [Open WebUI Documentation](https://docs.openwebui.com/) is ready to assist you.\n\n> [!WARNING]\n> When using Docker to install Open WebUI, make sure to include the `-v open-webui:/app/backend/data` in your Docker command. This step is crucial as it ensures your database is properly mounted and prevents any loss of data.\n\n> [!TIP]  \n> If you wish to utilize Open WebUI with Ollama included or CUDA acceleration, we recommend utilizing our official images tagged with either `:cuda` or `:ollama`. To enable CUDA, you must install the [Nvidia CUDA container toolkit](https://docs.nvidia.com/dgx/nvidia-container-runtime-upgrade/) on your Linux/WSL system.\n\n### Installation with Default Configuration\n\n- **If Ollama is on your computer**, use this command:\n\n  ```bash\n  docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n  ```\n\n- **If Ollama is on a Different Server**, use this command:\n\n  To connect to Ollama on another server, change the `OLLAMA_BASE_URL` to the server''s URL:\n\n  ```bash\n  docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n  ```\n\n- **To run Open WebUI with Nvidia GPU support**, use this command:\n\n  ```bash\n  docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda\n  ```\n\n### Installation for OpenAI API Usage Only\n\n- **If you''re only using OpenAI API**, use this command:\n\n  ```bash\n  docker run -d -p 3000:8080 -e OPENAI_API_KEY=your_secret_key -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n  ```\n\n### Installing Open WebUI with Bundled Ollama Support\n\nThis installation method uses a single container image that bundles Open WebUI with Ollama, allowing for a streamlined setup via a single command. Choose the appropriate command based on your hardware setup:\n\n- **With GPU Support**:\n  Utilize GPU resources by running the following command:\n\n  ```bash\n  docker run -d -p 3000:8080 --gpus=all -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n  ```\n\n- **For CPU Only**:\n  If you''re not using a GPU, use this command instead:\n\n  ```bash\n  docker run -d -p 3000:8080 -v ollama:/root/.ollama -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:ollama\n  ```\n\nBoth commands facilitate a built-in, hassle-free installation of both Open WebUI and Ollama, ensuring that you can get everything up and running swiftly.\n\nAfter installation, you can access Open WebUI at [http://localhost:3000](http://localhost:3000). Enjoy! ğŸ˜„\n\n### Other Installation Methods\n\nWe offer various installation alternatives, including non-Docker native installation methods, Docker Compose, Kustomize, and Helm. Visit our [Open WebUI Documentation](https://docs.openwebui.com/getting-started/) or join our [Discord community](https://discord.gg/5rJgQTnV4s) for comprehensive guidance.\n\nLook at the [Local Development Guide](https://docs.openwebui.com/getting-started/advanced-topics/development) for instructions on setting up a local development environment.\n\n### Troubleshooting\n\nEncountering connection issues? Our [Open WebUI Documentation](https://docs.openwebui.com/troubleshooting/) has got you covered. For further assistance and to join our vibrant community, visit the [Open WebUI Discord](https://discord.gg/5rJgQTnV4s).\n\n#### Open WebUI: Server Connection Error\n\nIf you''re experiencing connection issues, itâ€™s often due to the WebUI docker container not being able to reach the Ollama server at 127.0.0.1:11434 (host.docker.internal:11434) inside the container . Use the `--network=host` flag in your docker command to resolve this. Note that the port changes from 3000 to 8080, resulting in the link: `http://localhost:8080`.\n\n**Example Docker Command**:\n\n```bash\ndocker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n```\n\n### Keeping Your Docker Installation Up-to-Date\n\nIn case you want to update your local Docker installation to the latest version, you can do it with [Watchtower](https://containrrr.dev/watchtower/):\n\n```bash\ndocker run --rm --volume /var/run/docker.sock:/var/run/docker.sock containrrr/watchtower --run-once open-webui\n```\n\nIn the last part of the command, replace `open-webui` with your container name if it is different.\n\nCheck our Updating Guide available in our [Open WebUI Documentation](https://docs.openwebui.com/getting-started/updating).\n\n### Using the Dev Branch ğŸŒ™\n\n> [!WARNING]\n> The `:dev` branch contains the latest unstable features and changes. Use it at your own risk as it may have bugs or incomplete features.\n\nIf you want to try out the latest bleeding-edge features and are okay with occasional instability, you can use the `:dev` tag like this:\n\n```bash\ndocker run -d -p 3000:8080 -v open-webui:/app/backend/data --name open-webui --add-host=host.docker.internal:host-gateway --restart always ghcr.io/open-webui/open-webui:dev\n```\n\n### Offline Mode\n\nIf you are running Open WebUI in an offline environment, you can set the `HF_HUB_OFFLINE` environment variable to `1` to prevent attempts to download models from the internet.\n\n```bash\nexport HF_HUB_OFFLINE=1\n```\n\n## What''s Next? ğŸŒŸ\n\nDiscover upcoming features on our roadmap in the [Open WebUI Documentation](https://docs.openwebui.com/roadmap/).\n\n## License ğŸ“œ\n\nThis project contains code under multiple licenses. The current codebase includes components licensed under the Open WebUI License with an additional requirement to preserve the "Open WebUI" branding, as well as prior contributions under their respective original licenses. For a detailed record of license changes and the applicable terms for each section of the code, please refer to [LICENSE_HISTORY](./LICENSE_HISTORY). For complete and updated licensing details, please see the [LICENSE](./LICENSE) and [LICENSE_HISTORY](./LICENSE_HISTORY) files.\n\n## Support ğŸ’¬\n\nIf you have any questions, suggestions, or need assistance, please open an issue or join our\n[Open WebUI Discord community](https://discord.gg/5rJgQTnV4s) to connect with us! ğŸ¤\n\n## Star History\n\n<a href="https://star-history.com/#open-webui/open-webui&Date">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date&theme=dark" />\n    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date" />\n    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=open-webui/open-webui&type=Date" />\n  </picture>\n</a>\n\n---\n\nCreated by [Timothy Jaeryang Baek](https://github.com/tjbck) - Let''s make Open WebUI even more amazing together! ğŸ’ª\n', '{"language":"Svelte","stars":117240,"forks":16476,"watchers":117240,"open_issues":249,"topics":["ai","llm","llm-ui","llm-webui","llms","mcp","ollama","ollama-webui","open-webui","openai","openapi","rag","self-hosted","ui","webui"],"default_branch":"main","size_kb":304982,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:sponsors:tjbck","source_url":"https://github.com/sponsors/tjbck"},{"type":"has_code","target_id":"github:open-webui:pipelines","source_url":"https://github.com/open-webui/pipelines"},{"type":"has_code","target_id":"github:open-webui:pipelines","source_url":"https://github.com/open-webui/pipelines"}]', NULL, 'NOASSERTION', 'approved', 80, '5e128e481ea323ede0b7e29b44135e48', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-open-webui-open-webui from https://github.com/open-webui.png
Image converted to WebP: data/images/github-open-webui-open-webui.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-generative-ai-for-beginners', 'github--microsoft--generative-ai-for-beginners', 'generative-ai-for-beginners', 'microsoft', '!Generative AI For Beginners <!-- CO-OP TRANSLATOR LANGUAGES TABLE START --> Arabic | Bengali | Bulgarian | Burmese (Myanmar) | Chinese (Simplified) | Chinese (Traditional, Hong Kong) | Chinese (Traditional, Macau) | Chinese (Traditional, Taiwan) | Croatian | Czech | Danish | Dutch | Estonian | Finnish | French | German | Greek | Hebrew | Hindi | Hungarian | Indonesian | Italian | Japanese | Korean | Lithuanian | Malay | Marathi | Nepali | Norwegian | Persian (Farsi) | Polish | Portuguese (Br...', '["ai","azure","chatgpt","dall-e","generative-ai","generativeai","gpt","language-model","llms","microsoft-for-beginners","openai","prompt-engineering","semantic-search","transformers","jupyter notebook"]', 'other', 102792, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/generative-ai-for-beginners","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '![Generative AI For Beginners](./images/repo-thumbnailv4-fixed.png?WT.mc_id=academic-105485-koreyst)\n\n### 21 Lessons teaching everything you need to know to start building Generative AI applications\n\n[![GitHub license](https://img.shields.io/github/license/microsoft/Generative-AI-For-Beginners.svg)](https://github.com/microsoft/Generative-AI-For-Beginners/blob/master/LICENSE?WT.mc_id=academic-105485-koreyst)\n[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/Generative-AI-For-Beginners.svg)](https://GitHub.com/microsoft/Generative-AI-For-Beginners/graphs/contributors/?WT.mc_id=academic-105485-koreyst)\n[![GitHub issues](https://img.shields.io/github/issues/microsoft/Generative-AI-For-Beginners.svg)](https://GitHub.com/microsoft/Generative-AI-For-Beginners/issues/?WT.mc_id=academic-105485-koreyst)\n[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/Generative-AI-For-Beginners.svg)](https://GitHub.com/microsoft/Generative-AI-For-Beginners/pulls/?WT.mc_id=academic-105485-koreyst)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com?WT.mc_id=academic-105485-koreyst)\n\n[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/Generative-AI-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/Generative-AI-For-Beginners/watchers/?WT.mc_id=academic-105485-koreyst)\n[![GitHub forks](https://img.shields.io/github/forks/microsoft/Generative-AI-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/Generative-AI-For-Beginners/network/?WT.mc_id=academic-105485-koreyst)\n[![GitHub stars](https://img.shields.io/github/stars/microsoft/Generative-AI-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/Generative-AI-For-Beginners/stargazers/?WT.mc_id=academic-105485-koreyst)\n\n[![Microsoft Foundry Discord](https://dcbadge.limes.pink/api/server/nTYy5BXMWG)](https://discord.gg/nTYy5BXMWG)\n\n### ğŸŒ Multi-Language Support\n\n#### Supported via GitHub Action (Automated & Always Up-to-Date)\n\n<!-- CO-OP TRANSLATOR LANGUAGES TABLE START -->\n[Arabic](./translations/ar/README.md) | [Bengali](./translations/bn/README.md) | [Bulgarian](./translations/bg/README.md) | [Burmese (Myanmar)](./translations/my/README.md) | [Chinese (Simplified)](./translations/zh/README.md) | [Chinese (Traditional, Hong Kong)](./translations/hk/README.md) | [Chinese (Traditional, Macau)](./translations/mo/README.md) | [Chinese (Traditional, Taiwan)](./translations/tw/README.md) | [Croatian](./translations/hr/README.md) | [Czech](./translations/cs/README.md) | [Danish](./translations/da/README.md) | [Dutch](./translations/nl/README.md) | [Estonian](./translations/et/README.md) | [Finnish](./translations/fi/README.md) | [French](./translations/fr/README.md) | [German](./translations/de/README.md) | [Greek](./translations/el/README.md) | [Hebrew](./translations/he/README.md) | [Hindi](./translations/hi/README.md) | [Hungarian](./translations/hu/README.md) | [Indonesian](./translations/id/README.md) | [Italian](./translations/it/README.md) | [Japanese](./translations/ja/README.md) | [Korean](./translations/ko/README.md) | [Lithuanian](./translations/lt/README.md) | [Malay](./translations/ms/README.md) | [Marathi](./translations/mr/README.md) | [Nepali](./translations/ne/README.md) | [Norwegian](./translations/no/README.md) | [Persian (Farsi)](./translations/fa/README.md) | [Polish](./translations/pl/README.md) | [Portuguese (Brazil)](./translations/br/README.md) | [Portuguese (Portugal)](./translations/pt/README.md) | [Punjabi (Gurmukhi)](./translations/pa/README.md) | [Romanian](./translations/ro/README.md) | [Russian](./translations/ru/README.md) | [Serbian (Cyrillic)](./translations/sr/README.md) | [Slovak](./translations/sk/README.md) | [Slovenian](./translations/sl/README.md) | [Spanish](./translations/es/README.md) | [Swahili](./translations/sw/README.md) | [Swedish](./translations/sv/README.md) | [Tagalog (Filipino)](./translations/tl/README.md) | [Tamil](./translations/ta/README.md) | [Thai](./translations/th/README.md) | [Turkish](./translations/tr/README.md) | [Ukrainian](./translations/uk/README.md) | [Urdu](./translations/ur/README.md) | [Vietnamese](./translations/vi/README.md)\n<!-- CO-OP TRANSLATOR LANGUAGES TABLE END -->\n\n# Generative AI for Beginners (Version 3) - A Course\n\nLearn the fundamentals of building Generative AI applications with our 21-lesson comprehensive course by Microsoft Cloud Advocates.\n\n## ğŸŒ± Getting Started\n\nThis course has 21 lessons. Each lesson covers its own topic so start wherever you like!\n\nLessons are labeled either "Learn" lessons explaining a Generative AI concept or "Build" lessons that explain a concept and code examples in both **Python** and **TypeScript** when possible.\n\nFor .NET Developers checkout [Generative AI for Beginners (.NET Edition)](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)!\n\nEach lesson also includes a "Keep Learning" section with additional learning tools.\n\n## What You Need\n### To run the code of this course, you can use either: \n - [Azure OpenAI Service](https://aka.ms/genai-beginners/azure-open-ai?WT.mc_id=academic-105485-koreyst) - **Lessons:** "aoai-assignment"\n - [GitHub Marketplace Model Catalog](https://aka.ms/genai-beginners/gh-models?WT.mc_id=academic-105485-koreyst) - **Lessons:** "githubmodels"\n - [OpenAI API](https://aka.ms/genai-beginners/open-ai?WT.mc_id=academic-105485-koreyst) - **Lessons:** "oai-assignment" \n   \n- Basic knowledge of Python or TypeScript is helpful - \*For absolute beginners check out these [Python](https://aka.ms/genai-beginners/python?WT.mc_id=academic-105485-koreyst) and [TypeScript](https://aka.ms/genai-beginners/typescript?WT.mc_id=academic-105485-koreyst) courses\n- A GitHub account to [fork this entire repo](https://aka.ms/genai-beginners/github?WT.mc_id=academic-105485-koreyst) to your own GitHub account\n\nWe have created a **[Course Setup](./00-course-setup/README.md?WT.mc_id=academic-105485-koreyst)** lesson to help you with setting up your development environment.\n\nDon''t forget to [star (ğŸŒŸ) this repo](https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars?WT.mc_id=academic-105485-koreyst) to find it easier later.\n\n## ğŸ§  Ready to Deploy?\n\nIf you are looking for more advanced code samples, check out our [collection of Generative AI Code Samples](https://aka.ms/genai-beg-code?WT.mc_id=academic-105485-koreyst) in both **Python** and **TypeScript**.\n\n## ğŸ—£ï¸ Meet Other Learners, Get Support\n\nJoin our [official Azure AI Foundry Discord server](https://aka.ms/genai-discord?WT.mc_id=academic-105485-koreyst) to meet and network with other learners taking this course and get support.\n\nAsk questions or share product feedback in our [Azure AI Foundry Developer Forum](https://aka.ms/azureaifoundry/forum) on Github.\n\n## ğŸš€ Building a Startup?\n\nVisit [Microsoft for Startups](https://www.microsoft.com/startups) to find out how to get started building with Azure credits today.\n\n## ğŸ™ Want to help?\n\nDo you have suggestions or found spelling or code errors? [Raise an issue](https://github.com/microsoft/generative-ai-for-beginners/issues?WT.mc_id=academic-105485-koreyst) or [Create a pull request](https://github.com/microsoft/generative-ai-for-beginners/pulls?WT.mc_id=academic-105485-koreyst)\n\n## ğŸ“‚ Each lesson includes:\n\n- A short video introduction to the topic\n- A written lesson located in the README\n- Python and TypeScript code samples supporting Azure OpenAI and OpenAI API\n- Links to extra resources to continue your learning\n\n## ğŸ—ƒï¸ Lessons\n\n| #   | **Lesson Link**                                                                                                                              | **Description**                                                                                 | **Video**                                                                   | **Extra Learning**                                                             |\n| --- | -------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |\n| 00  | [Course Setup](./00-course-setup/README.md?WT.mc_id=academic-105485-koreyst)                                                                 | **Learn:** How to Setup Your Development Environment                                            | Video Coming Soon                                                                 | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 01  | [Introduction to Generative AI and LLMs](./01-introduction-to-genai/README.md?WT.mc_id=academic-105485-koreyst)                              | **Learn:** Understanding what Generative AI is and how Large Language Models (LLMs) work.       | [Video](https://aka.ms/gen-ai-lesson-1-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 02  | [Exploring and comparing different LLMs](./02-exploring-and-comparing-different-llms/README.md?WT.mc_id=academic-105485-koreyst)             | **Learn:** How to select the right model for your use case                                      | [Video](https://aka.ms/gen-ai-lesson2-gh?WT.mc_id=academic-105485-koreyst)  | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 03  | [Using Generative AI Responsibly](./03-using-generative-ai-responsibly/README.md?WT.mc_id=academic-105485-koreyst)                           | **Learn:** How to build Generative AI Applications responsibly                                  | [Video](https://aka.ms/gen-ai-lesson3-gh?WT.mc_id=academic-105485-koreyst)  | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 04  | [Understanding Prompt Engineering Fundamentals](./04-prompt-engineering-fundamentals/README.md?WT.mc_id=academic-105485-koreyst)             | **Learn:** Hands-on Prompt Engineering Best Practices                                           | [Video](https://aka.ms/gen-ai-lesson4-gh?WT.mc_id=academic-105485-koreyst)  | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 05  | [Creating Advanced Prompts](./05-advanced-prompts/README.md?WT.mc_id=academic-105485-koreyst)                                                | **Learn:** How to apply prompt engineering techniques that improve the outcome of your prompts. | [Video](https://aka.ms/gen-ai-lesson5-gh?WT.mc_id=academic-105485-koreyst)  | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 06  | [Building Text Generation Applications](./06-text-generation-apps/README.md?WT.mc_id=academic-105485-koreyst)                                | **Build:** A text generation app using Azure OpenAI / OpenAI API                                | [Video](https://aka.ms/gen-ai-lesson6-gh?WT.mc_id=academic-105485-koreyst)  | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 07  | [Building Chat Applications](./07-building-chat-applications/README.md?WT.mc_id=academic-105485-koreyst)                                     | **Build:** Techniques for efficiently building and integrating chat applications.               | [Video](https://aka.ms/gen-ai-lessons7-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 08  | [Building Search Apps Vector Databases](./08-building-search-applications/README.md?WT.mc_id=academic-105485-koreyst)                        | **Build:** A search application that uses Embeddings to search for data.                        | [Video](https://aka.ms/gen-ai-lesson8-gh?WT.mc_id=academic-105485-koreyst)  | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 09  | [Building Image Generation Applications](./09-building-image-applications/README.md?WT.mc_id=academic-105485-koreyst)                        | **Build:** An image generation application                                                       | [Video](https://aka.ms/gen-ai-lesson9-gh?WT.mc_id=academic-105485-koreyst)  | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 10  | [Building Low Code AI Applications](./10-building-low-code-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)                       | **Build:** A Generative AI application using Low Code tools                                     | [Video](https://aka.ms/gen-ai-lesson10-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 11  | [Integrating External Applications with Function Calling](./11-integrating-with-function-calling/README.md?WT.mc_id=academic-105485-koreyst) | **Build:** What is function calling and its use cases for applications                          | [Video](https://aka.ms/gen-ai-lesson11-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 12  | [Designing UX for AI Applications](./12-designing-ux-for-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)                         | **Learn:** How to apply UX design principles when developing Generative AI Applications         | [Video](https://aka.ms/gen-ai-lesson12-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 13  | [Securing Your Generative AI Applications](./13-securing-ai-applications/README.md?WT.mc_id=academic-105485-koreyst)                         | **Learn:** The threats and risks to AI systems and methods to secure these systems.             | [Video](https://aka.ms/gen-ai-lesson13-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 14  | [The Generative AI Application Lifecycle](./14-the-generative-ai-application-lifecycle/README.md?WT.mc_id=academic-105485-koreyst)           | **Learn:** The tools and metrics to manage the LLM Lifecycle and LLMOps                         | [Video](https://aka.ms/gen-ai-lesson14-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 15  | [Retrieval Augmented Generation (RAG) and Vector Databases](./15-rag-and-vector-databases/README.md?WT.mc_id=academic-105485-koreyst)        | **Build:** An application using a RAG Framework to retrieve embeddings from a Vector Databases  | [Video](https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 16  | [Open Source Models and Hugging Face](./16-open-source-models/README.md?WT.mc_id=academic-105485-koreyst)                                    | **Build:** An application using open source models available on Hugging Face                    | [Video](https://aka.ms/gen-ai-lesson16-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 17  | [AI Agents](./17-ai-agents/README.md?WT.mc_id=academic-105485-koreyst)                                                                       | **Build:** An application using an AI Agent Framework                                           | [Video](https://aka.ms/gen-ai-lesson17-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 18  | [Fine-Tuning LLMs](./18-fine-tuning/README.md?WT.mc_id=academic-105485-koreyst)                                                              | **Learn:** The what, why and how of fine-tuning LLMs                                            | [Video](https://aka.ms/gen-ai-lesson18-gh?WT.mc_id=academic-105485-koreyst) | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 19  | [Building with SLMs](./19-slm/README.md?WT.mc_id=academic-105485-koreyst)                                                              | **Learn:** The benefits of building with Small Language Models                                            | Video Coming Soon | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 20  | [Building with Mistral Models](./20-mistral/README.md?WT.mc_id=academic-105485-koreyst)                                                              | **Learn:** The features and differences of the Mistral Family Models                                           | Video Coming Soon | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n| 21  | [Building with Meta Models](./21-meta/README.md?WT.mc_id=academic-105485-koreyst)                                                              | **Learn:** The features and differences of the Meta Family Models                                           | Video Coming Soon | [Learn More](https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst) |\n\n### ğŸŒŸ Special thanks\n\nSpecial thanks to [**John Aziz**](https://www.linkedin.com/in/john0isaac/) for creating all of the GitHub Actions and workflows\n\n[**Bernhard Merkle**](https://www.linkedin.com/in/bernhard-merkle-738b73/) for making key contributions to each lesson to improve the learner and code experience. \n\n## ğŸ’ Other Courses\n\nOur team produces other courses! Check out:\n\n<!-- CO-OP TRANSLATOR OTHER COURSES START -->\n### Azure / Edge / MCP / Agents\n[![AZD for Beginners](https://img.shields.io/badge/AZD%20for%20Beginners-0078D4?style=for-the-badge&labelColor=E5E7EB&color=0078D4)](https://github.com/microsoft/AZD-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![Edge AI for Beginners](https://img.shields.io/badge/Edge%20AI%20for%20Beginners-00B8E4?style=for-the-badge&labelColor=E5E7EB&color=00B8E4)](https://github.com/microsoft/edgeai-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![MCP for Beginners](https://img.shields.io/badge/MCP%20for%20Beginners-009688?style=for-the-badge&labelColor=E5E7EB&color=009688)](https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![AI Agents for Beginners](https://img.shields.io/badge/AI%20Agents%20for%20Beginners-00C49A?style=for-the-badge&labelColor=E5E7EB&color=00C49A)](https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Generative AI Series\n[![Generative AI for Beginners](https://img.shields.io/badge/Generative%20AI%20for%20Beginners-8B5CF6?style=for-the-badge&labelColor=E5E7EB&color=8B5CF6)](https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (.NET)](https://img.shields.io/badge/Generative%20AI%20(.NET)-9333EA?style=for-the-badge&labelColor=E5E7EB&color=9333EA)](https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (Java)](https://img.shields.io/badge/Generative%20AI%20(Java)-C084FC?style=for-the-badge&labelColor=E5E7EB&color=C084FC)](https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst)\n[![Generative AI (JavaScript)](https://img.shields.io/badge/Generative%20AI%20(JavaScript)-E879F9?style=for-the-badge&labelColor=E5E7EB&color=E879F9)](https://github.com/microsoft/generative-ai-with-javascript?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Core Learning\n[![ML for Beginners](https://img.shields.io/badge/ML%20for%20Beginners-22C55E?style=for-the-badge&labelColor=E5E7EB&color=22C55E)](https://aka.ms/ml-beginners?WT.mc_id=academic-105485-koreyst)\n[![Data Science for Beginners](https://img.shields.io/badge/Data%20Science%20for%20Beginners-84CC16?style=for-the-badge&labelColor=E5E7EB&color=84CC16)](https://aka.ms/datascience-beginners?WT.mc_id=academic-105485-koreyst)\n[![AI for Beginners](https://img.shields.io/badge/AI%20for%20Beginners-A3E635?style=for-the-badge&labelColor=E5E7EB&color=A3E635)](https://aka.ms/ai-beginners?WT.mc_id=academic-105485-koreyst)\n[![Cybersecurity for Beginners](https://img.shields.io/badge/Cybersecurity%20for%20Beginners-F97316?style=for-the-badge&labelColor=E5E7EB&color=F97316)](https://github.com/microsoft/Security-101?WT.mc_id=academic-96948-sayoung)\n[![Web Dev for Beginners](https://img.shields.io/badge/Web%20Dev%20for%20Beginners-EC4899?style=for-the-badge&labelColor=E5E7EB&color=EC4899)](https://aka.ms/webdev-beginners?WT.mc_id=academic-105485-koreyst)\n[![IoT for Beginners](https://img.shields.io/badge/IoT%20for%20Beginners-14B8A6?style=for-the-badge&labelColor=E5E7EB&color=14B8A6)](https://aka.ms/iot-beginners?WT.mc_id=academic-105485-koreyst)\n[![XR Development for Beginners](https://img.shields.io/badge/XR%20Development%20for%20Beginners-38BDF8?style=for-the-badge&labelColor=E5E7EB&color=38BDF8)](https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst)\n\n---\n \n### Copilot Series\n[![Copilot for AI Paired Programming](https://img.shields.io/badge/Copilot%20for%20AI%20Paired%20Programming-FACC15?style=for-the-badge&labelColor=E5E7EB&color=FACC15)](https://aka.ms/GitHubCopilotAI?WT.mc_id=academic-105485-koreyst)\n[![Copilot for C#/.NET](https://img.shields.io/badge/Copilot%20for%20C%23/.NET-FBBF24?style=for-the-badge&labelColor=E5E7EB&color=FBBF24)](https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst)\n[![Copilot Adventure](https://img.shields.io/badge/Copilot%20Adventure-FDE68A?style=for-the-badge&labelColor=E5E7EB&color=FDE68A)](https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst)\n<!-- CO-OP TRANSLATOR OTHER COURSES END -->\n\n## Getting Help\n\nIf you get stuck or have any questions about building AI apps. Join fellow learners and experienced developers in discussions about MCP. It''s a supportive community where questions are welcome and knowledge is shared freely.\n\n[![Microsoft Foundry Discord](https://dcbadge.limes.pink/api/server/nTYy5BXMWG)](https://discord.gg/nTYy5BXMWG)\n\nIf you have product feedback or errors while building visit:\n\n[![Microsoft Foundry Developer Forum](https://img.shields.io/badge/GitHub-Microsoft_Foundry_Developer_Forum-blue?style=for-the-badge&logo=github&color=000000&logoColor=fff)](https://aka.ms/foundry/forum)\n', '{"language":"Jupyter Notebook","stars":102792,"forks":54721,"watchers":102792,"open_issues":11,"topics":["ai","azure","chatgpt","dall-e","generative-ai","generativeai","gpt","language-model","llms","microsoft-for-beginners","openai","prompt-engineering","semantic-search","transformers"],"default_branch":"main","size_kb":5933211,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:microsoft:Generative-AI-For-Beginners","source_url":"https://github.com/microsoft/Generative-AI-For-Beginners"},{"type":"has_code","target_id":"github:microsoft:Generative-AI-for-beginners-dotnet","source_url":"https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-for-beginners","source_url":"https://github.com/microsoft/generative-ai-for-beginners"},{"type":"has_code","target_id":"github:microsoft:generative-ai-for-beginners","source_url":"https://github.com/microsoft/generative-ai-for-beginners"},{"type":"has_code","target_id":"github:microsoft:AZD-for-beginners","source_url":"https://github.com/microsoft/AZD-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:edgeai-for-beginners","source_url":"https://github.com/microsoft/edgeai-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:mcp-for-beginners","source_url":"https://github.com/microsoft/mcp-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:ai-agents-for-beginners","source_url":"https://github.com/microsoft/ai-agents-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-for-beginners","source_url":"https://github.com/microsoft/generative-ai-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:Generative-AI-for-beginners-dotnet","source_url":"https://github.com/microsoft/Generative-AI-for-beginners-dotnet?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-for-beginners-java","source_url":"https://github.com/microsoft/generative-ai-for-beginners-java?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:generative-ai-with-javascript","source_url":"https://github.com/microsoft/generative-ai-with-javascript?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:Security-101","source_url":"https://github.com/microsoft/Security-101?WT.mc_id=academic-96948-sayoung"},{"type":"has_code","target_id":"github:microsoft:xr-development-for-beginners","source_url":"https://github.com/microsoft/xr-development-for-beginners?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:mastering-github-copilot-for-dotnet-csharp-developers","source_url":"https://github.com/microsoft/mastering-github-copilot-for-dotnet-csharp-developers?WT.mc_id=academic-105485-koreyst"},{"type":"has_code","target_id":"github:microsoft:CopilotAdventures","source_url":"https://github.com/microsoft/CopilotAdventures?WT.mc_id=academic-105485-koreyst"}]', NULL, 'MIT', 'approved', 80, '7489091b5b6b66c96a81fbd4503b0ac3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-generative-ai-for-beginners from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-generative-ai-for-beginners.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-x1xhlol-system-prompts-and-models-of-ai-tools', 'github--x1xhlol--system-prompts-and-models-of-ai-tools', 'system-prompts-and-models-of-ai-tools', 'x1xhlol', '--- <p align="center"> <sub>Special thanks to</sub> </p> <p align="center"> <a href="https://latitude.so/developers?utm_source=github&utm_medium=readme&utm_campaign=prompt_repo_sponsorship" target="_blank"> <img src="assets/Latitude_logo.png" alt="Latitude Logo" width="700"/> </a> </p> <div align="center" markdown="1"> <a href="https://latitude.so/developers?utm_source=github&utm_medium=readme&utm_campaign=prompt_repo_sponsorship" target="_blank">Open Source AI Engineering Platform</a><br> </...', '["ai","bolt","cluely","copilot","cursor","cursorai","devin","github-copilot","lovable","open-source","perplexity","replit","system-prompts","trae","trae-ai","trae-ide","v0","vscode","windsurf","windsurf-ai"]', 'other', 99874, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# **System Prompts and Models of AI Tools**  \n---\n<p align="center">\n  <sub>Special thanks to</sub>  \n</p> \n\n<p align="center">\n  <a href="https://latitude.so/developers?utm_source=github&utm_medium=readme&utm_campaign=prompt_repo_sponsorship" target="_blank">\n    <img src="assets/Latitude_logo.png" alt="Latitude Logo" width="700"/>\n  </a>\n</p>\n\n<div align="center" markdown="1">\n\n### <a href="https://latitude.so/developers?utm_source=github&utm_medium=readme&utm_campaign=prompt_repo_sponsorship" target="_blank">The tools you need for building reliable Agents and Prompts</a>  \n<a href="https://latitude.so/developers?utm_source=github&utm_medium=readme&utm_campaign=prompt_repo_sponsorship" target="_blank">Open Source AI Engineering Platform</a><br>\n\n</div>\n\n---\n\n<a href="https://discord.gg/NwzrWErdMU" target="_blank">\n  <img src="https://img.shields.io/discord/1402660735833604126?label=LeaksLab%20Discord&logo=discord&style=for-the-badge" alt="LeaksLab Discord" />\n</a>\n\n\n<a href="https://trendshift.io/repositories/14084" target="_blank"><img src="https://trendshift.io/api/badge/repositories/14084" alt="x1xhlol%2Fsystem-prompts-and-models-of-ai-tools | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n\nğŸ“œ Over **30,000+ lines** of insights into their structure and functionality.  \n\n\n[![Build Status](https://app.cloudback.it/badge/x1xhlol/system-prompts-and-models-of-ai-tools)](https://cloudback.it)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/x1xhlol/system-prompts-and-models-of-ai-tools)\n\n---\n\n## â¤ï¸ Support the Project\n\nIf you find this collection valuable and appreciate the effort involved in obtaining and sharing these insights, please consider supporting the project.\n\nYou can show your support via:\n\n- **Cryptocurrency:**  \n  - **BTC:** `bc1q7zldmzjwspnaa48udvelwe6k3fef7xrrhg5625`  \n  - **LTC:** `LRWgqwEYDwqau1WeiTs6Mjg85NJ7m3fsdQ`  \n  - **ETH:** `0x3f844B2cc3c4b7242964373fB0A41C4fdffB192A`\n- **Patreon:** https://patreon.com/lucknite\n- **Ko-fi:** https://ko-fi.com/lucknite\n\nğŸ™ Thank you for your support!\n\n---\n\n# Sponsors\n\nSponsor the most comprehensive repository of AI system prompts and reach thousands of developers.\n\n[Get Started](mailto:lucknitelol@proton.me)\n\n---\n\n## ğŸ›  Roadmap & Feedback\n\n> Open an issue.\n\n> **Latest Update:** 02/12/2025\n\n---\n\n## ğŸ”— Connect With Me\n\n- **X:** [NotLucknite](https://x.com/NotLucknite)\n- **Discord**: `x1xhlol`\n- **Email**: `lucknitelol@pm.me`\n\n---\n\n## ğŸ›¡ï¸ Security Notice for AI Startups\n\n> âš ï¸ **Warning:** If you''re an AI startup, make sure your data is secure. Exposed prompts or AI models can easily become a target for hackers.\n\n> ğŸ” **Important:** Interested in securing your AI systems?  \n> Check out **[ZeroLeaks](https://zeroleaks.io/)**, a service designed to help startups **identify and secure** leaks in system instructions, internal tools, and model configurations. **Get a free AI security audit** to ensure your AI is protected from vulnerabilities.\n\n---\n\n## ğŸ“Š Star History\n\n<a href="https://www.star-history.com/#x1xhlol/system-prompts-and-models-of-ai-tools&Date">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=x1xhlol/system-prompts-and-models-of-ai-tools&type=Date&theme=dark" />\n    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=x1xhlol/system-prompts-and-models-of-ai-tools&type=Date" />\n    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=x1xhlol/system-prompts-and-models-of-ai-tools&type=Date" />\n  </picture>\n</a>\n\nâ­ **Drop a star if you find this useful!**\n', '{"language":null,"stars":99874,"forks":26730,"watchers":99874,"open_issues":101,"topics":["ai","bolt","cluely","copilot","cursor","cursorai","devin","github-copilot","lovable","open-source","perplexity","replit","system-prompts","trae","trae-ai","trae-ide","v0","vscode","windsurf","windsurf-ai"],"default_branch":"main","size_kb":1244,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[]', NULL, 'GPL-3.0', 'approved', 65, '2466dab53967703ae52eb6991df07de9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-x1xhlol-system-prompts-and-models-of-ai-tools from https://github.com/x1xhlol.png
Image converted to WebP: data/images/github-x1xhlol-system-prompts-and-models-of-ai-tools.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-comfyanonymous-ComfyUI', 'github--comfyanonymous--comfyui', 'ComfyUI', 'comfyanonymous', '<div align="center"> **The most powerful and modular visual AI engine and application.** [![Website][website-shield]][website-url] [![Dynamic JSON Badge][discord-shield]][discord-url] [![Twitter][twitter-shield]][twitter-url] [![Matrix][matrix-shield]][matrix-url] <br> [![][github-release-shield]][github-release-link] [![][github-release-date-shield]][github-release-link] [![][github-downloads-shield]][github-downloads-link] [![][github-downloads-latest-shield]][github-downloads-link] [matrix...', '["ai","comfy","comfyui","python","pytorch","stable-diffusion","python"]', 'other', 96114, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/comfyanonymous/ComfyUI","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<div align="center">\n\n# ComfyUI\n**The most powerful and modular visual AI engine and application.**\n\n\n[![Website][website-shield]][website-url]\n[![Dynamic JSON Badge][discord-shield]][discord-url]\n[![Twitter][twitter-shield]][twitter-url]\n[![Matrix][matrix-shield]][matrix-url]\n<br>\n[![][github-release-shield]][github-release-link]\n[![][github-release-date-shield]][github-release-link]\n[![][github-downloads-shield]][github-downloads-link]\n[![][github-downloads-latest-shield]][github-downloads-link]\n\n[matrix-shield]: https://img.shields.io/badge/Matrix-000000?style=flat&logo=matrix&logoColor=white\n[matrix-url]: https://app.element.io/#/room/%23comfyui_space%3Amatrix.org\n[website-shield]: https://img.shields.io/badge/ComfyOrg-4285F4?style=flat\n[website-url]: https://www.comfy.org/\n<!-- Workaround to display total user from https://github.com/badges/shields/issues/4500#issuecomment-2060079995 -->\n[discord-shield]: https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Fcomfyorg%3Fwith_counts%3Dtrue&query=%24.approximate_member_count&logo=discord&logoColor=white&label=Discord&color=green&suffix=%20total\n[discord-url]: https://www.comfy.org/discord\n[twitter-shield]: https://img.shields.io/twitter/follow/ComfyUI\n[twitter-url]: https://x.com/ComfyUI\n\n[github-release-shield]: https://img.shields.io/github/v/release/comfyanonymous/ComfyUI?style=flat&sort=semver\n[github-release-link]: https://github.com/comfyanonymous/ComfyUI/releases\n[github-release-date-shield]: https://img.shields.io/github/release-date/comfyanonymous/ComfyUI?style=flat\n[github-downloads-shield]: https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/total?style=flat\n[github-downloads-latest-shield]: https://img.shields.io/github/downloads/comfyanonymous/ComfyUI/latest/total?style=flat&label=downloads%40latest\n[github-downloads-link]: https://github.com/comfyanonymous/ComfyUI/releases\n\n![ComfyUI Screenshot](https://github.com/user-attachments/assets/7ccaf2c1-9b72-41ae-9a89-5688c94b7abe)\n</div>\n\nComfyUI lets you design and execute advanced stable diffusion pipelines using a graph/nodes/flowchart based interface. Available on Windows, Linux, and macOS.\n\n## Get Started\n\n#### [Desktop Application](https://www.comfy.org/download)\n- The easiest way to get started.\n- Available on Windows & macOS.\n\n#### [Windows Portable Package](#installing)\n- Get the latest commits and completely portable.\n- Available on Windows.\n\n#### [Manual Install](#manual-install-windows-linux)\nSupports all operating systems and GPU types (NVIDIA, AMD, Intel, Apple Silicon, Ascend).\n\n## [Examples](https://comfyanonymous.github.io/ComfyUI_examples/)\nSee what ComfyUI can do with the [example workflows](https://comfyanonymous.github.io/ComfyUI_examples/).\n\n## Features\n- Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything.\n- Image Models\n   - SD1.x, SD2.x ([unCLIP](https://comfyanonymous.github.io/ComfyUI_examples/unclip/))\n   - [SDXL](https://comfyanonymous.github.io/ComfyUI_examples/sdxl/), [SDXL Turbo](https://comfyanonymous.github.io/ComfyUI_examples/sdturbo/)\n   - [Stable Cascade](https://comfyanonymous.github.io/ComfyUI_examples/stable_cascade/)\n   - [SD3 and SD3.5](https://comfyanonymous.github.io/ComfyUI_examples/sd3/)\n   - Pixart Alpha and Sigma\n   - [AuraFlow](https://comfyanonymous.github.io/ComfyUI_examples/aura_flow/)\n   - [HunyuanDiT](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_dit/)\n   - [Flux](https://comfyanonymous.github.io/ComfyUI_examples/flux/)\n   - [Lumina Image 2.0](https://comfyanonymous.github.io/ComfyUI_examples/lumina2/)\n   - [HiDream](https://comfyanonymous.github.io/ComfyUI_examples/hidream/)\n   - [Qwen Image](https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/)\n   - [Hunyuan Image 2.1](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_image/)\n   - [Flux 2](https://comfyanonymous.github.io/ComfyUI_examples/flux2/)\n   - [Z Image](https://comfyanonymous.github.io/ComfyUI_examples/z_image/)\n- Image Editing Models\n   - [Omnigen 2](https://comfyanonymous.github.io/ComfyUI_examples/omnigen/)\n   - [Flux Kontext](https://comfyanonymous.github.io/ComfyUI_examples/flux/#flux-kontext-image-editing-model)\n   - [HiDream E1.1](https://comfyanonymous.github.io/ComfyUI_examples/hidream/#hidream-e11)\n   - [Qwen Image Edit](https://comfyanonymous.github.io/ComfyUI_examples/qwen_image/#edit-model)\n- Video Models\n   - [Stable Video Diffusion](https://comfyanonymous.github.io/ComfyUI_examples/video/)\n   - [Mochi](https://comfyanonymous.github.io/ComfyUI_examples/mochi/)\n   - [LTX-Video](https://comfyanonymous.github.io/ComfyUI_examples/ltxv/)\n   - [Hunyuan Video](https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/)\n   - [Wan 2.1](https://comfyanonymous.github.io/ComfyUI_examples/wan/)\n   - [Wan 2.2](https://comfyanonymous.github.io/ComfyUI_examples/wan22/)\n   - [Hunyuan Video 1.5](https://docs.comfy.org/tutorials/video/hunyuan/hunyuan-video-1-5)\n- Audio Models\n   - [Stable Audio](https://comfyanonymous.github.io/ComfyUI_examples/audio/)\n   - [ACE Step](https://comfyanonymous.github.io/ComfyUI_examples/audio/)\n- 3D Models\n   - [Hunyuan3D 2.0](https://docs.comfy.org/tutorials/3d/hunyuan3D-2)\n- Asynchronous Queue system\n- Many optimizations: Only re-executes the parts of the workflow that changes between executions.\n- Smart memory management: can automatically run large models on GPUs with as low as 1GB vram with smart offloading.\n- Works even if you don''t have a GPU with: ```--cpu``` (slow)\n- Can load ckpt and safetensors: All in one checkpoints or standalone diffusion models, VAEs and CLIP models.\n- Safe loading of ckpt, pt, pth, etc.. files.\n- Embeddings/Textual inversion\n- [Loras (regular, locon and loha)](https://comfyanonymous.github.io/ComfyUI_examples/lora/)\n- [Hypernetworks](https://comfyanonymous.github.io/ComfyUI_examples/hypernetworks/)\n- Loading full workflows (with seeds) from generated PNG, WebP and FLAC files.\n- Saving/Loading workflows as Json files.\n- Nodes interface can be used to create complex workflows like one for [Hires fix](https://comfyanonymous.github.io/ComfyUI_examples/2_pass_txt2img/) or much more advanced ones.\n- [Area Composition](https://comfyanonymous.github.io/ComfyUI_examples/area_composition/)\n- [Inpainting](https://comfyanonymous.github.io/ComfyUI_examples/inpaint/) with both regular and inpainting models.\n- [ControlNet and T2I-Adapter](https://comfyanonymous.github.io/ComfyUI_examples/controlnet/)\n- [Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...)](https://comfyanonymous.github.io/ComfyUI_examples/upscale_models/)\n- [GLIGEN](https://comfyanonymous.github.io/ComfyUI_examples/gligen/)\n- [Model Merging](https://comfyanonymous.github.io/ComfyUI_examples/model_merging/)\n- [LCM models and Loras](https://comfyanonymous.github.io/ComfyUI_examples/lcm/)\n- Latent previews with [TAESD](#how-to-show-high-quality-previews)\n- Works fully offline: core will never download anything unless you want to.\n- Optional API nodes to use paid models from external providers through the online [Comfy API](https://docs.comfy.org/tutorials/api-nodes/overview).\n- [Config file](extra_model_paths.yaml.example) to set the search paths for models.\n\nWorkflow examples can be found on the [Examples page](https://comfyanonymous.github.io/ComfyUI_examples/)\n\n## Release Process\n\nComfyUI follows a weekly release cycle targeting Monday but this regularly changes because of model releases or large changes to the codebase. There are three interconnected repositories:\n\n1. **[ComfyUI Core](https://github.com/comfyanonymous/ComfyUI)**\n   - Releases a new stable version (e.g., v0.7.0) roughly every week.\n   - Commits outside of the stable release tags may be very unstable and break many custom nodes.\n   - Serves as the foundation for the desktop release\n\n2. **[ComfyUI Desktop](https://github.com/Comfy-Org/desktop)**\n   - Builds a new release using the latest stable core version\n\n3. **[ComfyUI Frontend](https://github.com/Comfy-Org/ComfyUI_frontend)**\n   - Weekly frontend updates are merged into the core repository\n   - Features are frozen for the upcoming core release\n   - Development continues for the next release cycle\n\n## Shortcuts\n\n| Keybind                            | Explanation                                                                                                        |\n|------------------------------------|--------------------------------------------------------------------------------------------------------------------|\n| `Ctrl` + `Enter`                      | Queue up current graph for generation                                                                              |\n| `Ctrl` + `Shift` + `Enter`              | Queue up current graph as first for generation                                                                     |\n| `Ctrl` + `Alt` + `Enter`                | Cancel current generation                                                                                          |\n| `Ctrl` + `Z`/`Ctrl` + `Y`                 | Undo/Redo                                                                                                          |\n| `Ctrl` + `S`                          | Save workflow                                                                                                      |\n| `Ctrl` + `O`                          | Load workflow                                                                                                      |\n| `Ctrl` + `A`                          | Select all nodes                                                                                                   |\n| `Alt `+ `C`                           | Collapse/uncollapse selected nodes                                                                                 |\n| `Ctrl` + `M`                          | Mute/unmute selected nodes                                                                                         |\n| `Ctrl` + `B`                           | Bypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)            |\n| `Delete`/`Backspace`                   | Delete selected nodes                                                                                              |\n| `Ctrl` + `Backspace`                   | Delete the current graph                                                                                           |\n| `Space`                              | Move the canvas around when held and moving the cursor                                                             |\n| `Ctrl`/`Shift` + `Click`                 | Add clicked node to selection                                                                                      |\n| `Ctrl` + `C`/`Ctrl` + `V`                  | Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)                     |\n| `Ctrl` + `C`/`Ctrl` + `Shift` + `V`          | Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes) |\n| `Shift` + `Drag`                       | Move multiple selected nodes at the same time                                                                      |\n| `Ctrl` + `D`                           | Load default graph                                                                                                 |\n| `Alt` + `+`                          | Canvas Zoom in                                                                                                     |\n| `Alt` + `-`                          | Canvas Zoom out                                                                                                    |\n| `Ctrl` + `Shift` + LMB + Vertical drag | Canvas Zoom in/out                                                                                                 |\n| `P`                                  | Pin/Unpin selected nodes                                                                                           |\n| `Ctrl` + `G`                           | Group selected nodes                                                                                               |\n| `Q`                                 | Toggle visibility of the queue                                                                                     |\n| `H`                                  | Toggle visibility of history                                                                                       |\n| `R`                                  | Refresh graph                                                                                                      |\n| `F`                                  | Show/Hide menu                                                                                                      |\n| `.`                                  | Fit view to selection (Whole graph when nothing is selected)                                                        |\n| Double-Click LMB                   | Open node quick search palette                                                                                     |\n| `Shift` + Drag                       | Move multiple wires at once                                                                                        |\n| `Ctrl` + `Alt` + LMB                   | Disconnect all wires from clicked slot                                                                             |\n\n`Ctrl` can also be replaced with `Cmd` instead for macOS users\n\n# Installing\n\n## Windows Portable\n\nThere is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the [releases page](https://github.com/comfyanonymous/ComfyUI/releases).\n\n### [Direct link to download](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z)\n\nSimply download, extract with [7-Zip](https://7-zip.org) or with the windows explorer on recent windows versions and run. For smaller models you normally only need to put the checkpoints (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints but many of the larger models have multiple files. Make sure to follow the instructions to know which subfolder to put them in ComfyUI\models\\n\nIf you have trouble extracting it, right click the file -> properties -> unblock\n\nUpdate your Nvidia drivers if it doesn''t start.\n\n#### Alternative Downloads:\n\n[Experimental portable for AMD GPUs](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_amd.7z)\n\n[Portable with pytorch cuda 12.8 and python 3.12](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia_cu128.7z).\n\n[Portable with pytorch cuda 12.6 and python 3.12](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia_cu126.7z) (Supports Nvidia 10 series and older GPUs).\n\n#### How do I share models between another UI and ComfyUI?\n\nSee the [Config file](extra_model_paths.yaml.example) to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.\n\n\n## [comfy-cli](https://docs.comfy.org/comfy-cli/getting-started)\n\nYou can install and start ComfyUI using comfy-cli:\n```bash\npip install comfy-cli\ncomfy install\n```\n\n## Manual Install (Windows, Linux)\n\nPython 3.14 works but you may encounter issues with the torch compile node. The free threaded variant is still missing some dependencies.\n\nPython 3.13 is very well supported. If you have trouble with some custom node dependencies on 3.13 you can try 3.12\n\n### Instructions:\n\nGit clone this repo.\n\nPut your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints\n\nPut your VAE in: models/vae\n\n\n### AMD GPUs (Linux)\n\nAMD users can install rocm and pytorch with pip if you don''t have it already installed, this is the command to install the stable version:\n\n```pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.4```\n\nThis is the command to install the nightly with ROCm 7.0 which might have some performance improvements:\n\n```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm7.1```\n\n\n### AMD GPUs (Experimental: Windows and Linux), RDNA 3, 3.5 and 4 only.\n\nThese have less hardware support than the builds above but they work on windows. You also need to install the pytorch version specific to your hardware.\n\nRDNA 3 (RX 7000 series):\n\n```pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx110X-dgpu/```\n\nRDNA 3.5 (Strix halo/Ryzen AI Max+ 365):\n\n```pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx1151/```\n\nRDNA 4 (RX 9000 series):\n\n```pip install --pre torch torchvision torchaudio --index-url https://rocm.nightlies.amd.com/v2/gfx120X-all/```\n\n### Intel GPUs (Windows and Linux)\n\nIntel Arc GPU users can install native PyTorch with torch.xpu support using pip. More information can be found [here](https://pytorch.org/docs/main/notes/get_start_xpu.html)\n\n1. To install PyTorch xpu, use the following command:\n\n```pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu```\n\nThis is the command to install the Pytorch xpu nightly which might have some performance improvements:\n\n```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu```\n\n### NVIDIA\n\nNvidia users should install stable pytorch using this command:\n\n```pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu130```\n\nThis is the command to install pytorch nightly instead which might have performance improvements.\n\n```pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu130```\n\n#### Troubleshooting\n\nIf you get the "Torch not compiled with CUDA enabled" error, uninstall torch with:\n\n```pip uninstall torch```\n\nAnd install it again with the command above.\n\n### Dependencies\n\nInstall the dependencies by opening your terminal inside the ComfyUI folder and:\n\n```pip install -r requirements.txt```\n\nAfter this you should have everything installed and can proceed to running ComfyUI.\n\n### Others:\n\n#### Apple Mac silicon\n\nYou can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version.\n\n1. Install pytorch nightly. For instructions, read the [Accelerated PyTorch training on Mac](https://developer.apple.com/metal/pytorch/) Apple Developer guide (make sure to install the latest pytorch nightly).\n1. Follow the [ComfyUI manual installation](#manual-install-windows-linux) instructions for Windows and Linux.\n1. Install the ComfyUI [dependencies](#dependencies). If you have another Stable Diffusion UI [you might be able to reuse the dependencies](#i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies).\n1. Launch ComfyUI by running `python main.py`\n\n> **Note**: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discussed in [ComfyUI manual installation](#manual-install-windows-linux).\n\n#### Ascend NPUs\n\nFor models compatible with Ascend Extension for PyTorch (torch_npu). To get started, ensure your environment meets the prerequisites outlined on the [installation](https://ascend.github.io/docs/sources/ascend/quick_install.html) page. Here''s a step-by-step guide tailored to your platform and installation method:\n\n1. Begin by installing the recommended or newer kernel version for Linux as specified in the Installation page of torch-npu, if necessary.\n2. Proceed with the installation of Ascend Basekit, which includes the driver, firmware, and CANN, following the instructions provided for your specific platform.\n3. Next, install the necessary packages for torch-npu by adhering to the platform-specific instructions on the [Installation](https://ascend.github.io/docs/sources/pytorch/install.html#pytorch) page.\n4. Finally, adhere to the [ComfyUI manual installation](#manual-install-windows-linux) guide for Linux. Once all components are installed, you can run ComfyUI as described earlier.\n\n#### Cambricon MLUs\n\nFor models compatible with Cambricon Extension for PyTorch (torch_mlu). Here''s a step-by-step guide tailored to your platform and installation method:\n\n1. Install the Cambricon CNToolkit by adhering to the platform-specific instructions on the [Installation](https://www.cambricon.com/docs/sdk_1.15.0/cntoolkit_3.7.2/cntoolkit_install_3.7.2/index.html)\n2. Next, install the PyTorch(torch_mlu) following the instructions on the [Installation](https://www.cambricon.com/docs/sdk_1.15.0/cambricon_pytorch_1.17.0/user_guide_1.9/index.html)\n3. Launch ComfyUI by running `python main.py`\n\n#### Iluvatar Corex\n\nFor models compatible with Iluvatar Extension for PyTorch. Here''s a step-by-step guide tailored to your platform and installation method:\n\n1. Install the Iluvatar Corex Toolkit by adhering to the platform-specific instructions on the [Installation](https://support.iluvatar.com/#/DocumentCentre?id=1&nameCenter=2&productId=520117912052801536)\n2. Launch ComfyUI by running `python main.py`\n\n\n## [ComfyUI-Manager](https://github.com/Comfy-Org/ComfyUI-Manager/tree/manager-v4)\n\n**ComfyUI-Manager** is an extension that allows you to easily install, update, and manage custom nodes for ComfyUI.\n\n### Setup\n\n1. Install the manager dependencies:\n   ```bash\n   pip install -r manager_requirements.txt\n   ```\n\n2. Enable the manager with the `--enable-manager` flag when running ComfyUI:\n   ```bash\n   python main.py --enable-manager\n   ```\n\n### Command Line Options\n\n| Flag | Description |\n|------|-------------|\n| `--enable-manager` | Enable ComfyUI-Manager |\n| `--enable-manager-legacy-ui` | Use the legacy manager UI instead of the new UI (requires `--enable-manager`) |\n| `--disable-manager-ui` | Disable the manager UI and endpoints while keeping background features like security checks and scheduled installation completion (requires `--enable-manager`) |\n\n\n# Running\n\n```python main.py```\n\n### For AMD cards not officially supported by ROCm\n\nTry running it with this command if you have issues:\n\nFor 6700, 6600 and maybe other RDNA2 or older: ```HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py```\n\nFor AMD 7600 and maybe other RDNA3 cards: ```HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py```\n\n### AMD ROCm Tips\n\nYou can enable experimental memory efficient attention on recent pytorch in ComfyUI on some AMD GPUs using this command, it should already be enabled by default on RDNA3. If this improves speed for you on latest pytorch on your GPU please report it so that I can enable it by default.\n\n```TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 python main.py --use-pytorch-cross-attention```\n\nYou can also try setting this env variable `PYTORCH_TUNABLEOP_ENABLED=1` which might speed things up at the cost of a very slow initial run.\n\n# Notes\n\nOnly parts of the graph that have an output with all the correct inputs will be executed.\n\nOnly parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed.\n\nDragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it.\n\nYou can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \\( or \\).\n\nYou can use {day|night}, for wildcard/dynamic prompts. With this syntax "{wild|card|test}" will be randomly replaced by either "wild", "card" or "test" by the frontend every time you queue the prompt. To use {} characters in your actual prompt escape them like: \\{ or \\}.\n\nDynamic prompts also support C-style comments, like `// comment` or `/* comment */`.\n\nTo use a textual inversion concepts/embeddings in a text prompt put them in the models/embeddings directory and use them in the CLIPTextEncode node like this (you can omit the .pt extension):\n\n```embedding:embedding_filename.pt```\n\n\n## How to show high-quality previews?\n\nUse ```--preview-method auto``` to enable previews.\n\nThe default installation includes a fast latent preview method that''s low-resolution. To enable higher-quality previews with [TAESD](https://github.com/madebyollin/taesd), download the [taesd_decoder.pth, taesdxl_decoder.pth, taesd3_decoder.pth and taef1_decoder.pth](https://github.com/madebyollin/taesd/) and place them in the `models/vae_approx` folder. Once they''re installed, restart ComfyUI and launch it with `--preview-method taesd` to enable high-quality previews.\n\n## How to use TLS/SSL?\nGenerate a self-signed certificate (not appropriate for shared/production use) and key by running the command: `openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 3650 -nodes -subj "/C=XX/ST=StateName/L=CityName/O=CompanyName/OU=CompanySectionName/CN=CommonNameOrHostname"`\n\nUse `--tls-keyfile key.pem --tls-certfile cert.pem` to enable TLS/SSL, the app will now be accessible with `https://...` instead of `http://...`.\n\n> Note: Windows users can use [alexisrolland/docker-openssl](https://github.com/alexisrolland/docker-openssl) or one of the [3rd party binary distributions](https://wiki.openssl.org/index.php/Binaries) to run the command example above.\n<br/><br/>If you use a container, note that the volume mount `-v` can be a relative path so `... -v ".\:/openssl-certs" ...` would create the key & cert files in the current directory of your command prompt or powershell terminal.\n\n## Support and dev channel\n\n[Discord](https://comfy.org/discord): Try the #help or #feedback channels.\n\n[Matrix space: #comfyui_space:matrix.org](https://app.element.io/#/room/%23comfyui_space%3Amatrix.org) (it''s like discord but open source).\n\nSee also: [https://www.comfy.org/](https://www.comfy.org/)\n\n## Frontend Development\n\nAs of August 15, 2024, we have transitioned to a new frontend, which is now hosted in a separate repository: [ComfyUI Frontend](https://github.com/Comfy-Org/ComfyUI_frontend). This repository now hosts the compiled JS (from TS/Vue) under the `web/` directory.\n\n### Reporting Issues and Requesting Features\n\nFor any bugs, issues, or feature requests related to the frontend, please use the [ComfyUI Frontend repository](https://github.com/Comfy-Org/ComfyUI_frontend). This will help us manage and address frontend-specific concerns more efficiently.\n\n### Using the Latest Frontend\n\nThe new frontend is now the default for ComfyUI. However, please note:\n\n1. The frontend in the main ComfyUI repository is updated fortnightly.\n2. Daily releases are available in the separate frontend repository.\n\nTo use the most up-to-date frontend version:\n\n1. For the latest daily release, launch ComfyUI with this command line argument:\n\n   ```\n   --front-end-version Comfy-Org/ComfyUI_frontend@latest\n   ```\n\n2. For a specific version, replace `latest` with the desired version number:\n\n   ```\n   --front-end-version Comfy-Org/ComfyUI_frontend@1.2.2\n   ```\n\nThis approach allows you to easily switch between the stable fortnightly release and the cutting-edge daily updates, or even specific versions for testing purposes.\n\n### Accessing the Legacy Frontend\n\nIf you need to use the legacy frontend for any reason, you can access it using the following command line argument:\n\n```\n--front-end-version Comfy-Org/ComfyUI_legacy_frontend@latest\n```\n\nThis will use a snapshot of the legacy frontend preserved in the [ComfyUI Legacy Frontend repository](https://github.com/Comfy-Org/ComfyUI_legacy_frontend).\n\n# QA\n\n### Which GPU should I buy for this?\n\n[See this page for some recommendations](https://github.com/comfyanonymous/ComfyUI/wiki/Which-GPU-should-I-buy-for-ComfyUI)\n', '{"language":"Python","stars":96114,"forks":10872,"watchers":96114,"open_issues":3276,"topics":["ai","comfy","comfyui","python","pytorch","stable-diffusion"],"default_branch":"master","size_kb":76756,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:badges:shields","source_url":"https://github.com/badges/shields"},{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI","source_url":"https://github.com/comfyanonymous/ComfyUI"},{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI","source_url":"https://github.com/comfyanonymous/ComfyUI"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI","source_url":"https://github.com/comfyanonymous/ComfyUI"},{"type":"has_code","target_id":"github:Comfy-Org:desktop","source_url":"https://github.com/Comfy-Org/desktop"},{"type":"has_code","target_id":"github:Comfy-Org:ComfyUI_frontend","source_url":"https://github.com/Comfy-Org/ComfyUI_frontend"},{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI","source_url":"https://github.com/comfyanonymous/ComfyUI"},{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI","source_url":"https://github.com/comfyanonymous/ComfyUI"},{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI","source_url":"https://github.com/comfyanonymous/ComfyUI"},{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI","source_url":"https://github.com/comfyanonymous/ComfyUI"},{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI","source_url":"https://github.com/comfyanonymous/ComfyUI"},{"type":"has_code","target_id":"github:Comfy-Org:ComfyUI-Manager","source_url":"https://github.com/Comfy-Org/ComfyUI-Manager"},{"type":"has_code","target_id":"github:madebyollin:taesd","source_url":"https://github.com/madebyollin/taesd"},{"type":"has_code","target_id":"github:madebyollin:taesd","source_url":"https://github.com/madebyollin/taesd"},{"type":"has_code","target_id":"github:alexisrolland:docker-openssl","source_url":"https://github.com/alexisrolland/docker-openssl"},{"type":"has_code","target_id":"github:Comfy-Org:ComfyUI_frontend","source_url":"https://github.com/Comfy-Org/ComfyUI_frontend"},{"type":"has_code","target_id":"github:Comfy-Org:ComfyUI_frontend","source_url":"https://github.com/Comfy-Org/ComfyUI_frontend"},{"type":"has_code","target_id":"github:Comfy-Org:ComfyUI_legacy_frontend","source_url":"https://github.com/Comfy-Org/ComfyUI_legacy_frontend"},{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI","source_url":"https://github.com/comfyanonymous/ComfyUI"}]', NULL, 'GPL-3.0', 'approved', 80, 'ccecae2cc0f34fc681358c248a32b7a8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-comfyanonymous-ComfyUI from https://github.com/comfyanonymous.png
Image converted to WebP: data/images/github-comfyanonymous-ComfyUI.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-supabase-supabase', 'github--supabase--supabase', 'supabase', 'supabase', '<p align="center"> <img src="https://user-images.githubusercontent.com/8291514/213727234-cda046d6-28c6-491a-b284-b86c5cede25d.png#gh-light-mode-only"> <img src="https://user-images.githubusercontent.com/8291514/213727225-56186826-bee8-43b5-9b15-86e839d89393.png#gh-dark-mode-only"> </p> Supabase is the Postgres development platform. We''re building the features of Firebase using enterprise-grade open source tools. - [x] Hosted Postgres Database. Docs - [x] Authentication and Authorization. Docs...', '["ai","alternative","auth","database","deno","embeddings","example","firebase","nextjs","oauth2","pgvector","postgis","postgres","postgresql","postgrest","realtime","supabase","vectors","websockets","typescript"]', 'other', 94301, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/supabase/supabase","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n<img src="https://user-images.githubusercontent.com/8291514/213727234-cda046d6-28c6-491a-b284-b86c5cede25d.png#gh-light-mode-only">\n<img src="https://user-images.githubusercontent.com/8291514/213727225-56186826-bee8-43b5-9b15-86e839d89393.png#gh-dark-mode-only">\n</p>\n\n# Supabase\n\n[Supabase](https://supabase.com) is the Postgres development platform. We''re building the features of Firebase using enterprise-grade open source tools.\n\n- [x] Hosted Postgres Database. [Docs](https://supabase.com/docs/guides/database)\n- [x] Authentication and Authorization. [Docs](https://supabase.com/docs/guides/auth)\n- [x] Auto-generated APIs.\n  - [x] REST. [Docs](https://supabase.com/docs/guides/api)\n  - [x] GraphQL. [Docs](https://supabase.com/docs/guides/graphql)\n  - [x] Realtime subscriptions. [Docs](https://supabase.com/docs/guides/realtime)\n- [x] Functions.\n  - [x] Database Functions. [Docs](https://supabase.com/docs/guides/database/functions)\n  - [x] Edge Functions [Docs](https://supabase.com/docs/guides/functions)\n- [x] File Storage. [Docs](https://supabase.com/docs/guides/storage)\n- [x] AI + Vector/Embeddings Toolkit. [Docs](https://supabase.com/docs/guides/ai)\n- [x] Dashboard\n\n![Supabase Dashboard](https://raw.githubusercontent.com/supabase/supabase/master/apps/www/public/images/github/supabase-dashboard.png)\n\nWatch "releases" of this repo to get notified of major updates.\n\n<kbd><img src="https://raw.githubusercontent.com/supabase/supabase/d5f7f413ab356dc1a92075cb3cee4e40a957d5b1/web/static/watch-repo.gif" alt="Watch this repo"/></kbd>\n\n## Documentation\n\nFor full documentation, visit [supabase.com/docs](https://supabase.com/docs)\n\nTo see how to Contribute, visit [Getting Started](./DEVELOPERS.md)\n\n## Community & Support\n\n- [Community Forum](https://github.com/supabase/supabase/discussions). Best for: help with building, discussion about database best practices.\n- [GitHub Issues](https://github.com/supabase/supabase/issues). Best for: bugs and errors you encounter using Supabase.\n- [Email Support](https://supabase.com/docs/support#business-support). Best for: problems with your database or infrastructure.\n- [Discord](https://discord.supabase.com). Best for: sharing your applications and hanging out with the community.\n\n## How it works\n\nSupabase is a combination of open source tools. Weâ€™re building the features of Firebase using enterprise-grade, open source products. If the tools and communities exist, with an MIT, Apache 2, or equivalent open license, we will use and support that tool. If the tool doesn''t exist, we build and open source it ourselves. Supabase is not a 1-to-1 mapping of Firebase. Our aim is to give developers a Firebase-like developer experience using open source tools.\n\n**Architecture**\n\nSupabase is a [hosted platform](https://supabase.com/dashboard). You can sign up and start using Supabase without installing anything.\nYou can also [self-host](https://supabase.com/docs/guides/hosting/overview) and [develop locally](https://supabase.com/docs/guides/local-development).\n\n![Architecture](apps/docs/public/img/supabase-architecture.svg)\n\n- [Postgres](https://www.postgresql.org/) is an object-relational database system with over 30 years of active development that has earned it a strong reputation for reliability, feature robustness, and performance.\n- [Realtime](https://github.com/supabase/realtime) is an Elixir server that allows you to listen to PostgreSQL inserts, updates, and deletes using websockets. Realtime polls Postgres'' built-in replication functionality for database changes, converts changes to JSON, then broadcasts the JSON over websockets to authorized clients.\n- [PostgREST](http://postgrest.org/) is a web server that turns your PostgreSQL database directly into a RESTful API.\n- [GoTrue](https://github.com/supabase/gotrue) is a JWT-based authentication API that simplifies user sign-ups, logins, and session management in your applications.\n- [Storage](https://github.com/supabase/storage-api) a RESTful API for managing files in S3, with Postgres handling permissions.\n- [pg_graphql](http://github.com/supabase/pg_graphql/) a PostgreSQL extension that exposes a GraphQL API.\n- [postgres-meta](https://github.com/supabase/postgres-meta) is a RESTful API for managing your Postgres, allowing you to fetch tables, add roles, and run queries, etc.\n- [Kong](https://github.com/Kong/kong) is a cloud-native API gateway.\n\n#### Client libraries\n\nOur approach for client libraries is modular. Each sub-library is a standalone implementation for a single external system. This is one of the ways we support existing tools.\n\n<table style="table-layout:fixed; white-space: nowrap;">\n  <tr>\n    <th>Language</th>\n    <th>Client</th>\n    <th colspan="5">Feature-Clients (bundled in Supabase client)</th>\n  </tr>\n  <!-- notranslate -->\n  <tr>\n    <th></th>\n    <th>Supabase</th>\n    <th><a href="https://github.com/postgrest/postgrest" target="_blank" rel="noopener noreferrer">PostgREST</a></th>\n    <th><a href="https://github.com/supabase/gotrue" target="_blank" rel="noopener noreferrer">GoTrue</a></th>\n    <th><a href="https://github.com/supabase/realtime" target="_blank" rel="noopener noreferrer">Realtime</a></th>\n    <th><a href="https://github.com/supabase/storage-api" target="_blank" rel="noopener noreferrer">Storage</a></th>\n    <th>Functions</th>\n  </tr>\n  <!-- TEMPLATE FOR NEW ROW -->\n  <!-- START ROW\n  <tr>\n    <td>lang</td>\n    <td><a href="https://github.com/supabase-community/supabase-lang" target="_blank" rel="noopener noreferrer">supabase-lang</a></td>\n    <td><a href="https://github.com/supabase-community/postgrest-lang" target="_blank" rel="noopener noreferrer">postgrest-lang</a></td>\n    <td><a href="https://github.com/supabase-community/gotrue-lang" target="_blank" rel="noopener noreferrer">gotrue-lang</a></td>\n    <td><a href="https://github.com/supabase-community/realtime-lang" target="_blank" rel="noopener noreferrer">realtime-lang</a></td>\n    <td><a href="https://github.com/supabase-community/storage-lang" target="_blank" rel="noopener noreferrer">storage-lang</a></td>\n  </tr>\n  END ROW -->\n  <!-- /notranslate -->\n  <th colspan="7">âš¡ï¸ Official âš¡ï¸</th>\n  <!-- notranslate -->\n  <tr>\n    <td>JavaScript (TypeScript)</td>\n    <td><a href="https://github.com/supabase/supabase-js" target="_blank" rel="noopener noreferrer">supabase-js</a></td>\n    <td><a href="https://github.com/supabase/postgrest-js" target="_blank" rel="noopener noreferrer">postgrest-js</a></td>\n    <td><a href="https://github.com/supabase/gotrue-js" target="_blank" rel="noopener noreferrer">gotrue-js</a></td>\n    <td><a href="https://github.com/supabase/realtime-js" target="_blank" rel="noopener noreferrer">realtime-js</a></td>\n    <td><a href="https://github.com/supabase/storage-js" target="_blank" rel="noopener noreferrer">storage-js</a></td>\n    <td><a href="https://github.com/supabase/functions-js" target="_blank" rel="noopener noreferrer">functions-js</a></td>\n  </tr>\n    <tr>\n    <td>Flutter</td>\n    <td><a href="https://github.com/supabase/supabase-flutter" target="_blank" rel="noopener noreferrer">supabase-flutter</a></td>\n    <td><a href="https://github.com/supabase/postgrest-dart" target="_blank" rel="noopener noreferrer">postgrest-dart</a></td>\n    <td><a href="https://github.com/supabase/gotrue-dart" target="_blank" rel="noopener noreferrer">gotrue-dart</a></td>\n    <td><a href="https://github.com/supabase/realtime-dart" target="_blank" rel="noopener noreferrer">realtime-dart</a></td>\n    <td><a href="https://github.com/supabase/storage-dart" target="_blank" rel="noopener noreferrer">storage-dart</a></td>\n    <td><a href="https://github.com/supabase/functions-dart" target="_blank" rel="noopener noreferrer">functions-dart</a></td>\n  </tr>\n  <tr>\n    <td>Swift</td>\n    <td><a href="https://github.com/supabase/supabase-swift" target="_blank" rel="noopener noreferrer">supabase-swift</a></td>\n    <td><a href="https://github.com/supabase/supabase-swift/tree/main/Sources/PostgREST" target="_blank" rel="noopener noreferrer">postgrest-swift</a></td>\n    <td><a href="https://github.com/supabase/supabase-swift/tree/main/Sources/Auth" target="_blank" rel="noopener noreferrer">auth-swift</a></td>\n    <td><a href="https://github.com/supabase/supabase-swift/tree/main/Sources/Realtime" target="_blank" rel="noopener noreferrer">realtime-swift</a></td>\n    <td><a href="https://github.com/supabase/supabase-swift/tree/main/Sources/Storage" target="_blank" rel="noopener noreferrer">storage-swift</a></td>\n    <td><a href="https://github.com/supabase/supabase-swift/tree/main/Sources/Functions" target="_blank" rel="noopener noreferrer">functions-swift</a></td>\n  </tr>\n  <tr>\n    <td>Python</td>\n    <td><a href="https://github.com/supabase/supabase-py" target="_blank" rel="noopener noreferrer">supabase-py</a></td>\n    <td><a href="https://github.com/supabase/postgrest-py" target="_blank" rel="noopener noreferrer">postgrest-py</a></td>\n    <td><a href="https://github.com/supabase/gotrue-py" target="_blank" rel="noopener noreferrer">gotrue-py</a></td>\n    <td><a href="https://github.com/supabase/realtime-py" target="_blank" rel="noopener noreferrer">realtime-py</a></td>\n    <td><a href="https://github.com/supabase/storage-py" target="_blank" rel="noopener noreferrer">storage-py</a></td>\n    <td><a href="https://github.com/supabase/functions-py" target="_blank" rel="noopener noreferrer">functions-py</a></td>\n  </tr>\n  <!-- /notranslate -->\n  <th colspan="7">ğŸ’š Community ğŸ’š</th>\n  <!-- notranslate -->\n  <tr>\n    <td>C#</td>\n    <td><a href="https://github.com/supabase-community/supabase-csharp" target="_blank" rel="noopener noreferrer">supabase-csharp</a></td>\n    <td><a href="https://github.com/supabase-community/postgrest-csharp" target="_blank" rel="noopener noreferrer">postgrest-csharp</a></td>\n    <td><a href="https://github.com/supabase-community/gotrue-csharp" target="_blank" rel="noopener noreferrer">gotrue-csharp</a></td>\n    <td><a href="https://github.com/supabase-community/realtime-csharp" target="_blank" rel="noopener noreferrer">realtime-csharp</a></td>\n    <td><a href="https://github.com/supabase-community/storage-csharp" target="_blank" rel="noopener noreferrer">storage-csharp</a></td>\n    <td><a href="https://github.com/supabase-community/functions-csharp" target="_blank" rel="noopener noreferrer">functions-csharp</a></td>\n  </tr>\n  <tr>\n    <td>Go</td>\n    <td>-</td>\n    <td><a href="https://github.com/supabase-community/postgrest-go" target="_blank" rel="noopener noreferrer">postgrest-go</a></td>\n    <td><a href="https://github.com/supabase-community/gotrue-go" target="_blank" rel="noopener noreferrer">gotrue-go</a></td>\n    <td>-</td>\n    <td><a href="https://github.com/supabase-community/storage-go" target="_blank" rel="noopener noreferrer">storage-go</a></td>\n    <td><a href="https://github.com/supabase-community/functions-go" target="_blank" rel="noopener noreferrer">functions-go</a></td>\n  </tr>\n  <tr>\n    <td>Java</td>\n    <td>-</td>\n    <td>-</td>\n    <td><a href="https://github.com/supabase-community/gotrue-java" target="_blank" rel="noopener noreferrer">gotrue-java</a></td>\n    <td>-</td>\n    <td><a href="https://github.com/supabase-community/storage-java" target="_blank" rel="noopener noreferrer">storage-java</a></td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Kotlin</td>\n    <td><a href="https://github.com/supabase-community/supabase-kt" target="_blank" rel="noopener noreferrer">supabase-kt</a></td>\n    <td><a href="https://github.com/supabase-community/supabase-kt/tree/master/Postgrest" target="_blank" rel="noopener noreferrer">postgrest-kt</a></td>\n    <td><a href="https://github.com/supabase-community/supabase-kt/tree/master/Auth" target="_blank" rel="noopener noreferrer">auth-kt</a></td>\n    <td><a href="https://github.com/supabase-community/supabase-kt/tree/master/Realtime" target="_blank" rel="noopener noreferrer">realtime-kt</a></td>\n    <td><a href="https://github.com/supabase-community/supabase-kt/tree/master/Storage" target="_blank" rel="noopener noreferrer">storage-kt</a></td>\n    <td><a href="https://github.com/supabase-community/supabase-kt/tree/master/Functions" target="_blank" rel="noopener noreferrer">functions-kt</a></td>\n  </tr>\n  <tr>\n    <td>Ruby</td>\n    <td><a href="https://github.com/supabase-community/supabase-rb" target="_blank" rel="noopener noreferrer">supabase-rb</a></td>\n    <td><a href="https://github.com/supabase-community/postgrest-rb" target="_blank" rel="noopener noreferrer">postgrest-rb</a></td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Rust</td>\n    <td>-</td>\n    <td><a href="https://github.com/supabase-community/postgrest-rs" target="_blank" rel="noopener noreferrer">postgrest-rs</a></td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Godot Engine (GDScript)</td>\n    <td><a href="https://github.com/supabase-community/godot-engine.supabase" target="_blank" rel="noopener noreferrer">supabase-gdscript</a></td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n    <td>-</td>\n  </tr>\n  <!-- /notranslate -->\n</table>\n\n<!--- Remove this list if you''re translating to another language, it''s hard to keep updated across multiple files-->\n<!--- Keep only the link to the list of translation files-->\n\n## Badges\n\n![Made with Supabase](./apps/www/public/badge-made-with-supabase.svg)\n\n```md\n[![Made with Supabase](https://supabase.com/badge-made-with-supabase.svg)](https://supabase.com)\n```\n\n```html\n<a href="https://supabase.com">\n  <img\n    width="168"\n    height="30"\n    src="https://supabase.com/badge-made-with-supabase.svg"\n    alt="Made with Supabase"\n  />\n</a>\n```\n\n![Made with Supabase (dark)](./apps/www/public/badge-made-with-supabase-dark.svg)\n\n```md\n[![Made with Supabase](https://supabase.com/badge-made-with-supabase-dark.svg)](https://supabase.com)\n```\n\n```html\n<a href="https://supabase.com">\n  <img\n    width="168"\n    height="30"\n    src="https://supabase.com/badge-made-with-supabase-dark.svg"\n    alt="Made with Supabase"\n  />\n</a>\n```\n\n## Translations\n\n- [Arabic | Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](/i18n/README.ar.md)\n- [Albanian / Shqip](/i18n/README.sq.md)\n- [Bangla / à¦¬à¦¾à¦‚à¦²à¦¾](/i18n/README.bn.md)\n- [Bulgarian / Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€ÑĞºĞ¸](/i18n/README.bg.md)\n- [Catalan / CatalÃ ](/i18n/README.ca.md)\n- [Croatian / Hrvatski](/i18n/README.hr.md)\n- [Czech / ÄeÅ¡tina](/i18n/README.cs.md)\n- [Danish / Dansk](/i18n/README.da.md)\n- [Dutch / Nederlands](/i18n/README.nl.md)\n- [English](https://github.com/supabase/supabase)\n- [Estonian / eesti keel](/i18n/README.et.md)\n- [Finnish / Suomalainen](/i18n/README.fi.md)\n- [French / FranÃ§ais](/i18n/README.fr.md)\n- [German / Deutsch](/i18n/README.de.md)\n- [Greek / Î•Î»Î»Î·Î½Î¹ÎºÎ¬](/i18n/README.el.md)\n- [Gujarati / àª—à«àªœàª°àª¾àª¤à«€](/i18n/README.gu.md)\n- [Hebrew / ×¢×‘×¨×™×ª](/i18n/README.he.md)\n- [Hindi / à¤¹à¤¿à¤‚à¤¦à¥€](/i18n/README.hi.md)\n- [Hungarian / Magyar](/i18n/README.hu.md)\n- [Nepali / à¤¨à¥‡à¤ªà¤¾à¤²à¥€](/i18n/README.ne.md)\n- [Indonesian / Bahasa Indonesia](/i18n/README.id.md)\n- [Italiano / Italian](/i18n/README.it.md)\n- [Japanese / æ—¥æœ¬èª](/i18n/README.jp.md)\n- [Korean / í•œêµ­ì–´](/i18n/README.ko.md)\n- [Lithuanian / lietuviÅ³](/i18n/README.lt.md)\n- [Latvian / latviski](/i18n/README.lv.md)\n- [Malay / Bahasa Malaysia](/i18n/README.ms.md)\n- [Norwegian (BokmÃ¥l) / Norsk (BokmÃ¥l)](/i18n/README.nb.md)\n- [Persian / ÙØ§Ø±Ø³ÛŒ](/i18n/README.fa.md)\n- [Polish / Polski](/i18n/README.pl.md)\n- [Portuguese / PortuguÃªs](/i18n/README.pt.md)\n- [Portuguese (Brazilian) / PortuguÃªs Brasileiro](/i18n/README.pt-br.md)\n- [Romanian / RomÃ¢nÄƒ](/i18n/README.ro.md)\n- [Russian / PÑƒÑÑĞºĞ¸Ğ¹](/i18n/README.ru.md)\n- [Serbian / Srpski](/i18n/README.sr.md)\n- [Sinhala / à·ƒà·’à¶‚à·„à¶½](/i18n/README.si.md)\n- [Slovak / slovenskÃ½](/i18n/README.sk.md)\n- [Slovenian / SlovenÅ¡Äina](/i18n/README.sl.md)\n- [Spanish / EspaÃ±ol](/i18n/README.es.md)\n- [Simplified Chinese / ç®€ä½“ä¸­æ–‡](/i18n/README.zh-cn.md)\n- [Swedish / Svenska](/i18n/README.sv.md)\n- [Thai / à¹„à¸—à¸¢](/i18n/README.th.md)\n- [Traditional Chinese / ç¹é«”ä¸­æ–‡](/i18n/README.zh-tw.md)\n- [Turkish / TÃ¼rkÃ§e](/i18n/README.tr.md)\n- [Ukrainian / Ğ£ĞºÑ€Ğ°Ñ—Ğ½ÑÑŒĞºĞ°](/i18n/README.uk.md)\n- [Vietnamese / Tiáº¿ng Viá»‡t](/i18n/README.vi-vn.md)\n- [List of translations](/i18n/languages.md) <!--- Keep only this -->\n', '{"language":"TypeScript","stars":94301,"forks":10931,"watchers":94301,"open_issues":704,"topics":["ai","alternative","auth","database","deno","embeddings","example","firebase","nextjs","oauth2","pgvector","postgis","postgres","postgresql","postgrest","realtime","supabase","vectors","websockets"],"default_branch":"master","size_kb":2266533,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:supabase:supabase","source_url":"https://github.com/supabase/supabase"},{"type":"has_code","target_id":"github:supabase:supabase","source_url":"https://github.com/supabase/supabase"},{"type":"has_code","target_id":"github:supabase:realtime","source_url":"https://github.com/supabase/realtime"},{"type":"has_code","target_id":"github:supabase:gotrue","source_url":"https://github.com/supabase/gotrue"},{"type":"has_code","target_id":"github:supabase:storage-api","source_url":"https://github.com/supabase/storage-api"},{"type":"has_code","target_id":"github:supabase:pg_graphql","source_url":"http://github.com/supabase/pg_graphql"},{"type":"has_code","target_id":"github:supabase:postgres-meta","source_url":"https://github.com/supabase/postgres-meta"},{"type":"has_code","target_id":"github:Kong:kong","source_url":"https://github.com/Kong/kong"},{"type":"has_code","target_id":"github:postgrest:postgrest\"","source_url":"https://github.com/postgrest/postgrest\""},{"type":"has_code","target_id":"github:supabase:gotrue\"","source_url":"https://github.com/supabase/gotrue\""},{"type":"has_code","target_id":"github:supabase:realtime\"","source_url":"https://github.com/supabase/realtime\""},{"type":"has_code","target_id":"github:supabase:storage-api\"","source_url":"https://github.com/supabase/storage-api\""},{"type":"has_code","target_id":"github:supabase-community:supabase-lang\"","source_url":"https://github.com/supabase-community/supabase-lang\""},{"type":"has_code","target_id":"github:supabase-community:postgrest-lang\"","source_url":"https://github.com/supabase-community/postgrest-lang\""},{"type":"has_code","target_id":"github:supabase-community:gotrue-lang\"","source_url":"https://github.com/supabase-community/gotrue-lang\""},{"type":"has_code","target_id":"github:supabase-community:realtime-lang\"","source_url":"https://github.com/supabase-community/realtime-lang\""},{"type":"has_code","target_id":"github:supabase-community:storage-lang\"","source_url":"https://github.com/supabase-community/storage-lang\""},{"type":"has_code","target_id":"github:supabase:supabase-js\"","source_url":"https://github.com/supabase/supabase-js\""},{"type":"has_code","target_id":"github:supabase:postgrest-js\"","source_url":"https://github.com/supabase/postgrest-js\""},{"type":"has_code","target_id":"github:supabase:gotrue-js\"","source_url":"https://github.com/supabase/gotrue-js\""},{"type":"has_code","target_id":"github:supabase:realtime-js\"","source_url":"https://github.com/supabase/realtime-js\""},{"type":"has_code","target_id":"github:supabase:storage-js\"","source_url":"https://github.com/supabase/storage-js\""},{"type":"has_code","target_id":"github:supabase:functions-js\"","source_url":"https://github.com/supabase/functions-js\""},{"type":"has_code","target_id":"github:supabase:supabase-flutter\"","source_url":"https://github.com/supabase/supabase-flutter\""},{"type":"has_code","target_id":"github:supabase:postgrest-dart\"","source_url":"https://github.com/supabase/postgrest-dart\""},{"type":"has_code","target_id":"github:supabase:gotrue-dart\"","source_url":"https://github.com/supabase/gotrue-dart\""},{"type":"has_code","target_id":"github:supabase:realtime-dart\"","source_url":"https://github.com/supabase/realtime-dart\""},{"type":"has_code","target_id":"github:supabase:storage-dart\"","source_url":"https://github.com/supabase/storage-dart\""},{"type":"has_code","target_id":"github:supabase:functions-dart\"","source_url":"https://github.com/supabase/functions-dart\""},{"type":"has_code","target_id":"github:supabase:supabase-swift\"","source_url":"https://github.com/supabase/supabase-swift\""},{"type":"has_code","target_id":"github:supabase:supabase-swift","source_url":"https://github.com/supabase/supabase-swift"},{"type":"has_code","target_id":"github:supabase:supabase-swift","source_url":"https://github.com/supabase/supabase-swift"},{"type":"has_code","target_id":"github:supabase:supabase-swift","source_url":"https://github.com/supabase/supabase-swift"},{"type":"has_code","target_id":"github:supabase:supabase-swift","source_url":"https://github.com/supabase/supabase-swift"},{"type":"has_code","target_id":"github:supabase:supabase-swift","source_url":"https://github.com/supabase/supabase-swift"},{"type":"has_code","target_id":"github:supabase:supabase-py\"","source_url":"https://github.com/supabase/supabase-py\""},{"type":"has_code","target_id":"github:supabase:postgrest-py\"","source_url":"https://github.com/supabase/postgrest-py\""},{"type":"has_code","target_id":"github:supabase:gotrue-py\"","source_url":"https://github.com/supabase/gotrue-py\""},{"type":"has_code","target_id":"github:supabase:realtime-py\"","source_url":"https://github.com/supabase/realtime-py\""},{"type":"has_code","target_id":"github:supabase:storage-py\"","source_url":"https://github.com/supabase/storage-py\""},{"type":"has_code","target_id":"github:supabase:functions-py\"","source_url":"https://github.com/supabase/functions-py\""},{"type":"has_code","target_id":"github:supabase-community:supabase-csharp\"","source_url":"https://github.com/supabase-community/supabase-csharp\""},{"type":"has_code","target_id":"github:supabase-community:postgrest-csharp\"","source_url":"https://github.com/supabase-community/postgrest-csharp\""},{"type":"has_code","target_id":"github:supabase-community:gotrue-csharp\"","source_url":"https://github.com/supabase-community/gotrue-csharp\""},{"type":"has_code","target_id":"github:supabase-community:realtime-csharp\"","source_url":"https://github.com/supabase-community/realtime-csharp\""},{"type":"has_code","target_id":"github:supabase-community:storage-csharp\"","source_url":"https://github.com/supabase-community/storage-csharp\""},{"type":"has_code","target_id":"github:supabase-community:functions-csharp\"","source_url":"https://github.com/supabase-community/functions-csharp\""},{"type":"has_code","target_id":"github:supabase-community:postgrest-go\"","source_url":"https://github.com/supabase-community/postgrest-go\""},{"type":"has_code","target_id":"github:supabase-community:gotrue-go\"","source_url":"https://github.com/supabase-community/gotrue-go\""},{"type":"has_code","target_id":"github:supabase-community:storage-go\"","source_url":"https://github.com/supabase-community/storage-go\""},{"type":"has_code","target_id":"github:supabase-community:functions-go\"","source_url":"https://github.com/supabase-community/functions-go\""},{"type":"has_code","target_id":"github:supabase-community:gotrue-java\"","source_url":"https://github.com/supabase-community/gotrue-java\""},{"type":"has_code","target_id":"github:supabase-community:storage-java\"","source_url":"https://github.com/supabase-community/storage-java\""},{"type":"has_code","target_id":"github:supabase-community:supabase-kt\"","source_url":"https://github.com/supabase-community/supabase-kt\""},{"type":"has_code","target_id":"github:supabase-community:supabase-kt","source_url":"https://github.com/supabase-community/supabase-kt"},{"type":"has_code","target_id":"github:supabase-community:supabase-kt","source_url":"https://github.com/supabase-community/supabase-kt"},{"type":"has_code","target_id":"github:supabase-community:supabase-kt","source_url":"https://github.com/supabase-community/supabase-kt"},{"type":"has_code","target_id":"github:supabase-community:supabase-kt","source_url":"https://github.com/supabase-community/supabase-kt"},{"type":"has_code","target_id":"github:supabase-community:supabase-kt","source_url":"https://github.com/supabase-community/supabase-kt"},{"type":"has_code","target_id":"github:supabase-community:supabase-rb\"","source_url":"https://github.com/supabase-community/supabase-rb\""},{"type":"has_code","target_id":"github:supabase-community:postgrest-rb\"","source_url":"https://github.com/supabase-community/postgrest-rb\""},{"type":"has_code","target_id":"github:supabase-community:postgrest-rs\"","source_url":"https://github.com/supabase-community/postgrest-rs\""},{"type":"has_code","target_id":"github:supabase-community:godot-engine.supabase\"","source_url":"https://github.com/supabase-community/godot-engine.supabase\""},{"type":"has_code","target_id":"github:supabase:supabase","source_url":"https://github.com/supabase/supabase"}]', NULL, 'Apache-2.0', 'approved', 80, '73eba0077e93cf2f01244f6827578648', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-supabase-supabase from https://github.com/supabase.png
Image converted to WebP: data/images/github-supabase-supabase.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-punkpeye-awesome-mcp-servers', 'github--punkpeye--awesome-mcp-servers', 'awesome-mcp-servers', 'punkpeye', '> [!IMPORTANT] > Read The State of MCP in 2025 report. A curated list of awesome Model Context Protocol (MCP) servers. * What is MCP? * Clients * Tutorials * Community * Legend * Server Implementations * Frameworks * Tips & Tricks MCP is an open protocol that enables AI models to securely interact with local and remote resources through standardized server implementations. This list focuses on production-ready and experimental MCP servers that extend AI capabilities through file access, datab...', '["ai","mcp"]', 'other', 76279, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/punkpeye/awesome-mcp-servers","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Awesome MCP Servers [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)\n\n[![à¹„à¸—à¸¢](https://img.shields.io/badge/Thai-Click-blue)](README-th.md)\n[![English](https://img.shields.io/badge/English-Click-yellow)](README.md)\n[![ç¹é«”ä¸­æ–‡](https://img.shields.io/badge/ç¹é«”ä¸­æ–‡-é»æ“ŠæŸ¥çœ‹-orange)](README-zh_TW.md)\n[![ç®€ä½“ä¸­æ–‡](https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-ç‚¹å‡»æŸ¥çœ‹-orange)](README-zh.md)\n[![æ—¥æœ¬èª](https://img.shields.io/badge/æ—¥æœ¬èª-ã‚¯ãƒªãƒƒã‚¯-é’)](README-ja.md)\n[![í•œêµ­ì–´](https://img.shields.io/badge/í•œêµ­ì–´-í´ë¦­-yellow)](README-ko.md)\n[![PortuguÃªs Brasileiro](https://img.shields.io/badge/PortuguÃªs_Brasileiro-Clique-green)](README-pt_BR.md)\n[![Discord](https://img.shields.io/discord/1312302100125843476?logo=discord&label=discord)](https://glama.ai/mcp/discord)\n[![Subreddit subscribers](https://img.shields.io/reddit/subreddit-subscribers/mcp?style=flat&logo=reddit&label=subreddit)](https://www.reddit.com/r/mcp/)\n\n> [!IMPORTANT]\n> Read [The State of MCP in 2025](https://glama.ai/blog/2025-12-07-the-state-of-mcp-in-2025) report.\n\nA curated list of awesome Model Context Protocol (MCP) servers.\n\n* [What is MCP?](#what-is-mcp)\n* [Clients](#clients)\n* [Tutorials](#tutorials)\n* [Community](#community)\n* [Legend](#legend)\n* [Server Implementations](#server-implementations)\n* [Frameworks](#frameworks)\n* [Tips & Tricks](#tips-and-tricks)\n\n## What is MCP?\n\n[MCP](https://modelcontextprotocol.io/) is an open protocol that enables AI models to securely interact with local and remote resources through standardized server implementations. This list focuses on production-ready and experimental MCP servers that extend AI capabilities through file access, database connections, API integrations, and other contextual services.\n\n## Clients\n\nCheckout [awesome-mcp-clients](https://github.com/punkpeye/awesome-mcp-clients/) and [glama.ai/mcp/clients](https://glama.ai/mcp/clients).\n\n> [!TIP]\n> [Glama Chat](https://glama.ai/chat) is a multi-modal AI client with MCP support & [AI gateway](https://glama.ai/gateway).\n\n## Tutorials\n\n* [Model Context Protocol (MCP) Quickstart](https://glama.ai/blog/2024-11-25-model-context-protocol-quickstart)\n* [Setup Claude Desktop App to Use a SQLite Database](https://youtu.be/wxCCzo9dGj0)\n\n## Community\n\n* [r/mcp Reddit](https://www.reddit.com/r/mcp)\n* [Discord Server](https://glama.ai/mcp/discord)\n\n## Legend\n\n* ğŸ–ï¸ â€“ official implementation\n* programming language\n  * ğŸ â€“ Python codebase\n  * ğŸ“‡ â€“ TypeScript (or JavaScript) codebase\n  * ğŸï¸ â€“ Go codebase\n  * ğŸ¦€ â€“ Rust codebase\n  * #ï¸âƒ£ - C# Codebase\n  * â˜• - Java codebase\n  * ğŸŒŠ â€“ C/C++ codebase\n  * ğŸ’ - Ruby codebase\n\n* scope\n  * â˜ï¸ - Cloud Service\n  * ğŸ  - Local Service\n  * ğŸ“Ÿ - Embedded Systems\n* operating system\n  * ğŸ â€“ For macOS\n  * ğŸªŸ â€“ For Windows\n  * ğŸ§ - For Linux\n\n> [!NOTE]\n> Confused about Local ğŸ  vs Cloud â˜ï¸?\n> * Use local when MCP server is talking to a locally installed software, e.g. taking control over Chrome browser.\n> * Use cloud when MCP server is talking to remote APIs, e.g. weather API.\n\n## Server Implementations\n\n> [!NOTE]\n> We now have a [web-based directory](https://glama.ai/mcp/servers) that is synced with the repository.\n\n* ğŸ”— - [Aggregators](#aggregators)\n* ğŸ¨ - [Art & Culture](#art-and-culture)\n* ğŸ“ - [Architecture & Design](#architecture-and-design)\n* ğŸ“‚ - [Browser Automation](#browser-automation)\n* ğŸ§¬ - [Biology Medicine and Bioinformatics](#bio)\n* â˜ï¸ - [Cloud Platforms](#cloud-platforms)\n* ğŸ‘¨â€ğŸ’» - [Code Execution](#code-execution)\n* ğŸ¤– - [Coding Agents](#coding-agents)\n* ğŸ–¥ï¸ - [Command Line](#command-line)\n* ğŸ’¬ - [Communication](#communication)\n* ğŸ‘¤ - [Customer Data Platforms](#customer-data-platforms)\n* ğŸ—„ï¸ - [Databases](#databases)\n* ğŸ“Š - [Data Platforms](#data-platforms)\n* ğŸšš - [Delivery](#delivery)\n* ğŸ› ï¸ - [Developer Tools](#developer-tools)\n* ğŸ§® - [Data Science Tools](#data-science-tools)\n* ğŸ“Ÿ - [Embedded system](#embedded-system)\n* ğŸ“‚ - [File Systems](#file-systems)\n* ğŸ’° - [Finance & Fintech](#finance--fintech)\n* ğŸ® - [Gaming](#gaming)\n* ğŸ§  - [Knowledge & Memory](#knowledge--memory)\n* âš–ï¸ - [Legal](#legal)\n* ğŸ—ºï¸ - [Location Services](#location-services)\n* ğŸ¯ - [Marketing](#marketing)\n* ğŸ“Š - [Monitoring](#monitoring)\n* ğŸ¥ - [Multimedia Process](#multimedia-process)\n* ğŸ”¬ - [Research](#research)\n* ğŸ” - [Search & Data Extraction](#search)\n* ğŸ”’ - [Security](#security)\n* ğŸŒ - [Social Media](#social-media)\n* ğŸƒ - [Sports](#sports)\n* ğŸ§ - [Support & Service Management](#support-and-service-management)\n* ğŸŒ - [Translation Services](#translation-services)\n* ğŸ§ - [Text-to-Speech](#text-to-speech)\n* ğŸš† - [Travel & Transportation](#travel-and-transportation)\n* ğŸ”„ - [Version Control](#version-control)\n* ğŸ¢ - [Workplace & Productivity](#workplace-and-productivity)\n* ğŸ› ï¸ - [Other Tools and Integrations](#other-tools-and-integrations)\n\n\n### ğŸ”— <a name="aggregators"></a>Aggregators\n\nServers for accessing many apps and tools through a single MCP server.\n\n- [1mcp/agent](https://github.com/1mcp-app/agent) ğŸ“‡ â˜ï¸ ğŸ  ğŸ ğŸªŸ ğŸ§ - A unified Model Context Protocol server implementation that aggregates multiple MCP servers into one.\n- [askbudi/roundtable](https://github.com/askbudi/roundtable) ğŸ“‡ â˜ï¸ ğŸ  ğŸ ğŸªŸ ğŸ§ - Meta-MCP server that unifies multiple AI coding assistants (Codex, Claude Code, Cursor, Gemini) through intelligent auto-discovery and standardized MCP interface, providing zero-configuration access to the entire AI coding ecosystem.\n- [duaraghav8/MCPJungle](https://github.com/duaraghav8/MCPJungle) ğŸï¸ ğŸ  - Self-hosted MCP Server registry for enterprise AI Agents\n- [glenngillen/mcpmcp-server](https://github.com/glenngillen/mcpmcp-server) â˜ï¸ ğŸ“‡ ğŸ ğŸªŸ ğŸ§ - A list of MCP servers so you can ask your client which servers you can use to improve your daily workflow.\n- [hamflx/imagen3-mcp](https://github.com/hamflx/imagen3-mcp) ğŸ“‡ ğŸ  ğŸªŸ ğŸ ğŸ§ - A powerful image generation tool using Google''s Imagen 3.0 API through MCP. Generate high-quality images from text prompts with advanced photography, artistic, and photorealistic controls.\n- [julien040/anyquery](https://github.com/julien040/anyquery) ğŸï¸ ğŸ  â˜ï¸ - Query more than 40 apps with one binary using SQL. It can also connect to your PostgreSQL, MySQL, or SQLite compatible database. Local-first and private by design.\n- [juspay/neurolink](https://github.com/juspay/neurolink) ğŸ“‡ â˜ï¸ ğŸ  ğŸ ğŸªŸ ğŸ§ - Making enterprise AI infrastructure universally accessible. Edge-first platform unifying 12 providers and 100+ models with multi-agent orchestration, HITL workflows, guardrails middleware, and context summarization.\n- [K-Dense-AI/claude-skills-mcp](https://github.com/K-Dense-AI/claude-skills-mcp) ğŸ â˜ï¸ ğŸ  ğŸ ğŸªŸ ğŸ§ - Intelligent search capabilities to let every model and client use [Claude Agent Skills](https://www.anthropic.com/news/skills) like native.\n- [metatool-ai/metatool-app](https://github.com/metatool-ai/metatool-app) ğŸ“‡ â˜ï¸ ğŸ  ğŸ ğŸªŸ ğŸ§ - MetaMCP is the one unified middleware MCP server that manages your MCP connections with GUI.\n- [mindsdb/mindsdb](https://github.com/mindsdb/mindsdb) - Connect and unify data across various platforms and databases with [MindsDB as a single MCP server](https://docs.mindsdb.com/mcp/overview).\n- [portel-dev/ncp](https://github.com/portel-dev/ncp) ğŸ“‡ â˜ï¸ ğŸ  ğŸ ğŸªŸ ğŸ§ - NCP orchestrates your entire MCP ecosystem through intelligent discovery, eliminating token overhead while maintaining 98.2% accuracy.\n- [particlefuture/MCPDiscovery](https://github.com/particlefuture/MCPDiscovery) - MCP of MCPs. A central hub for MCP servers. Helps you discover available MCP servers and learn how to install and use them.\n- [PipedreamHQ/pipedream](https://github.com/PipedreamHQ/pipedream/tree/master/modelcontextprotocol) â˜ï¸ ğŸ  - Connect with 2,500 APIs with 8,000+ prebuilt tools, and manage servers for your users, in your own app.\n- [sitbon/magg](https://github.com/sitbon/magg) ğŸ ğŸªŸ ğŸ§ â˜ï¸ ğŸ  ğŸ - Magg: A meta-MCP server that acts as a universal hub, allowing LLMs to autonomously discover, install, and orchestrate multiple MCP servers - essentially giving AI assistants the power to extend their own capabilities on-demand.\n- [thinkchainai/mcpbundles](https://github.com/thinkchainai/mcpbundles) - MCP Bundles: Create custom bundles of tools and connect providers with OAuth or API keys. Use one MCP server across thousands of integrations, with programmatic tool calling and MCP UI for managing bundles and credentials.\n- [SureScaleAI/openai-gpt-image-mcp](https://github.com/SureScaleAI/openai-gpt-image-mcp) ğŸ“‡ â˜ï¸ - OpenAI GPT image generation/editing MCP server.\n- [sxhxliang/mcp-access-point](https://github.com/sxhxliang/mcp-access-point) ğŸ“‡ â˜ï¸ ğŸ  ğŸ ğŸªŸ ğŸ§ - Turn a web service into an MCP server in one click without making any code changes.\n- [TheLunarCompany/lunar#mcpx](https://github.com/TheLunarCompany/lunar/tree/main/mcpx) ğŸ“‡ ğŸ   â˜ï¸ ğŸ ğŸªŸ ğŸ§ - MCPX is a production-ready, open-source gateway to manage MCP servers at scaleâ€”centralize tool discovery, access controls, call prioritization, and usage tracking to simplify agent workflows.\n- [tigranbs/mcgravity](https://github.com/tigranbs/mcgravity) ğŸ“‡ ğŸ  - A proxy tool for composing multiple MCP servers into one unified endpoint. Scale your AI tools by load balancing requests across multiple MCP servers, similar to how Nginx works for web servers.\n- [VeriTeknik/pluggedin-mcp-proxy](https://github.com/VeriTeknik/pluggedin-mcp-proxy)  ğŸ“‡ ğŸ  - A comprehensive proxy server that combines multiple MCP servers into a single interface with extensive visibility features. It provides discovery and management of tools, prompts, resources, and templates across servers, plus a playground for debugging when building MCP servers.\n- [WayStation-ai/mcp](https://github.com/waystation-ai/mcp) â˜ï¸ ğŸ ğŸªŸ - Seamlessly and securely connect Claude Desktop and other MCP hosts to your favorite apps (Notion, Slack, Monday, Airtable, etc.). Takes less than 90 secs.\n- [wegotdocs/open-mcp](https://github.com/wegotdocs/open-mcp) ğŸ“‡ ğŸ  ğŸ ğŸªŸ ğŸ§ - Turn a web API into an MCP server in 10 seconds and add it to the open source registry: https://open-mcp.org\n- [Data-Everything/mcp-server-templates](https://github.com/Data-Everything/mcp-server-templates) ğŸ“‡ ğŸ  ğŸ ğŸªŸ ğŸ§ - One server. All tools. A unified MCP platform that connects many apps, tools, and services behind one powerful interfaceâ€”ideal for local devs or production agents.\n- [YangLiangwei/PersonalizationMCP](https://github.com/YangLiangwei/PersonalizationMCP) ğŸ â˜ï¸ ğŸ  ğŸ ğŸªŸ ğŸ§ - Comprehensive personal data aggregation MCP server with Steam, YouTube, Bilibili, Spotify, Reddit and other platforms integrations. Features OAuth2 authentication, automatic token management, and 90+ tools for gaming, music, video, and social platform data access.\n\n### ğŸš€ <a name="aerospace-and-astrodynamics"></a>Aerospace & Astrodynamics\n\n- [IO-Aerospace-software-community/mcp-server](https://github.com/IO-Aerospace-software-engineering/mcp-server) #ï¸âƒ£ â˜ï¸/ğŸ  ğŸ§ - IO Aerospace MCP Server: a .NET-based MCP server for aerospace & astrodynamics â€” ephemeris, orbital conversions, DSS tools, time conversions, and unit/math utilities. Supports STDIO and SSE transports; Docker and native .NET deployment documented.\n\n### ğŸ¨ <a name="art-and-culture"></a>Art & Culture\n\nAccess and explore art collections, cultural heritage, and museum databases. Enables AI models to search and analyze artistic and cultural content.\n\n- [drakonkat/wizzy-mcp-tmdb](https://github.com/drakonkat/wizzy-mcp-tmdb) ğŸ“‡ â˜ï¸ - A MCP server for The Movie Database API that enables AI assistants to search and retrieve movie, TV show, and person information.\n- [8enSmith/mcp-open-library](https://github.com/8enSmith/mcp-open-library) ğŸ“‡ â˜ï¸ - A MCP server for the Open Library API that enables AI assistants to search for book information.\n- [abhiemj/manim-mcp-server](https://github.com/abhiemj/manim-mcp-server) ğŸ ğŸ  ğŸªŸ ğŸ§ - A local MCP server that generates animations using Manim.\n- [ahujasid/blender-mcp](https://github.com/ahujasid/blender-mcp) ğŸ - MCP server for working with Blender\n- [burningion/video-editing-mcp](https://github.com/burningion/video-editing-mcp) ğŸ - Add, Analyze, Search, and Generate Video Edits from your Video Jungle Collection\n- [cantian-ai/bazi-mcp](https://github.com/cantian-ai/bazi-mcp) ğŸ“‡ ğŸ  â˜ï¸ ğŸ ğŸªŸ - Provides comprehensive and accurate Bazi (Chinese Astrology) charting and analysis\n- [cswkim/discogs-mcp-server](https://github.com/cswkim/discogs-mcp-server) ğŸ“‡ â˜ï¸ - MCP server to interact with the Discogs API\n- [diivi/aseprite-mcp](https://github.com/diivi/aseprite-mcp) ğŸ ğŸ  - MCP server using the Aseprite API to create pixel art\n- [djalal/quran-mcp-server](https://github.com/djalal/quran-mcp-server) ğŸ“‡ â˜ï¸ MCP server to interact with Quran.com corpus via the official REST API v4.\n- [raveenb/fal-mcp-server](https://github.com/raveenb/fal-mcp-server) ğŸ â˜ï¸ - Generate AI images, videos, and music using Fal.ai models (FLUX, Stable Diffusion, MusicGen) directly in Claude Desktop\n- [GenWaveLLC/svgmaker-mcp](https://github.com/GenWaveLLC/svgmaker-mcp) ğŸ“‡ â˜ï¸ - Provides AI-driven SVG generation and editing via natural language, with real-time updates and secure file handling.\n- [mikechao/metmuseum-mcp](https://github.com/mikechao/metmuseum-mcp) ğŸ“‡ â˜ï¸ - Metropolitan Museum of Art Collection API integration to search and display artworks in the collection.\n- [molanojustin/smithsonian-mcp](https://github.com/molanojustin/smithsonian-mcp) ğŸ â˜ï¸ - MCP server that provides AI assistants with access to the Smithsonian Institution''s Open Access collections.  \n- [OctoEverywhere/mcp](https://github.com/OctoEverywhere/mcp) #ï¸âƒ£ â˜ï¸ - A 3D printer MCP server that allows for getting live printer state, webcam snapshots, and printer control.\n- [omni-mcp/isaac-sim-mcp](https://github.com/omni-mcp/isaac-sim-mcp) ğŸ“‡ â˜ï¸ - A MCP Server and an extension enables natural language control of NVIDIA Isaac Sim, Lab, OpenUSD and etc.\n- [PatrickPalmer/MayaMCP](https://github.com/PatrickPalmer/MayaMCP) ğŸ ğŸ  - MCP server for Autodesk Maya\n- [peek-travel/mcp-intro](https://github.com/peek-travel/mcp-intro) â˜ï¸ ğŸ ğŸªŸ ğŸ§ - Remote MCP Server for discovering and planning experiences, at home and on vacation\n- [r-huijts/oorlogsbronnen-mcp](https://github.com/r-huijts/oorlogsbronnen-mcp) ğŸ“‡ â˜ï¸ - Oorlogsbronnen (War Sources) API integration for accessing historical WWII records, photographs, and documents from the Netherlands (1940-1945)\n- [r-huijts/rijksmuseum-mcp](https://github.com/r-huijts/rijksmuseum-mcp) ğŸ“‡ â˜ï¸ - Rijksmuseum API integration for artwork search, details, and collections\n- [samuelgursky/davinci-resolve-mcp](https://github.com/samuelgursky/davinci-resolve-mcp) ğŸ - MCP server integration for DaVinci Resolve providing powerful tools for video editing, color grading, media management, and project control\n- [yuna0x0/anilist-mcp](https://github.com/yuna0x0/anilist-mcp) ğŸ“‡ â˜ï¸ - A MCP server integrating AniList API for anime and manga information\n\n\n### ğŸ“ <a name="architecture-and-design"></a>Architecture & Design\n\nDesign and visualize software architecture, system diagrams, and technical documentation. Enables AI models to generate professional diagrams and architectural documentation.\n\n- [Narasimhaponnada/mermaid-mcp](https://github.com/Narasimhaponnada/mermaid-mcp) ğŸ“‡ â˜ï¸ ğŸ ğŸªŸ ğŸ§ - AI-powered Mermaid diagram generation with 22+ diagram types including flowcharts, sequence diagrams, class diagrams, ER diagrams, architecture diagrams, state machines, and more. Features 50+ pre-built templates, advanced layout engines, SVG/PNG/PDF exports, and seamless integration with GitHub Copilot, Claude, and any MCP-compatible client. Install via NPM: `npm install -g @narasimhaponnada/mermaid-mcp-server`\n\n### <a name="bio"></a>Biology, Medicine and Bioinformatics\n\n- [dnaerys/onekgp-mcp](https://github.com/dnaerys/onekgp-mcp) â˜• â˜ï¸ - natural language access to 1000 Genomes Project dataset\n- [genomoncology/biomcp](https://github.com/genomoncology/biomcp) ğŸ â˜ï¸ - Biomedical research MCP server providing access to PubMed, ClinicalTrials.gov, and MyVariant.info.\n- [hlydecker/ucsc-genome-mcp](https://github.com/hlydecker/ucsc-genome-mcp) ğŸ â˜ï¸ - MCP server to interact with the UCSC Genome Browser API, letting you find genomes, chromosomes, and more.\n- [longevity-genie/biothings-mcp](https://github.com/longevity-genie/biothings-mcp) ğŸ ğŸ  â˜ï¸ - MCP server to interact with the BioThings API, including genes, genetic variants, drugs, and taxonomic information.\n- [longevity-genie/gget-mcp](https://github.com/longevity-genie/gget-mcp) ğŸ ğŸ  â˜ï¸ - MCP server providing a powerful bioinformatics toolkit for genomics queries and analysis, wrapping the popular `gget` library.\n- [longevity-genie/opengenes-mcp](https://github.com/longevity-genie/opengenes-mcp) ğŸ–ï¸ ğŸ ğŸ  â˜ï¸ - MCP server for a queryable database for aging and longevity research from the OpenGenes project.\n- [longevity-genie/synergy-age-mcp](https://github.com/longevity-genie/synergy-age-mcp) ğŸ–ï¸ ğŸ ğŸ  â˜ï¸ - MCP server for the SynergyAge database of synergistic and antagonistic genetic interactions in longevity.\n- [the-momentum/fhir-mcp-server](https://github.com/the-momentum/fhir-mcp-server) ğŸ ğŸ  â˜ï¸ - MCP Server that connects AI agents to FHIR servers. One example use case is querying patient history in natural language.\n- [wso2/fhir-mcp-server](https://github.com/wso2/fhir-mcp-server) ğŸ ğŸ  â˜ï¸ - Model Context Protocol server for Fast Healthcare Interoperability Resources (FHIR) APIs. Provides seamless integration with FHIR servers, enabling AI assistants to search, retrieve, create, update, and analyze clinical healthcare data with SMART-on-FHIR authentication support.\n- [JamesANZ/medical-mcp](https://github.com/JamesANZ/medical-mcp) ğŸ“‡ ğŸ  - An MCP server that provides access to medical information, drug databases, and healthcare resources. Enables AI assistants to query medical data, drug interactions, and clinical guidelines.\n- [the-momentum/apple-health-mcp-server](https://github.com/the-momentum/apple-health-mcp-server) ğŸ ğŸ  ğŸ ğŸªŸ ğŸ§ - An MCP server that provides access to exported data from Apple Health. Data analytics included.\n- [OHNLP/omop_mcp](https://github.com/OHNLP/omop_mcp) ğŸ ğŸ  â˜ï¸ - Map clinical terminology to OMOP concepts using LLMs for healthcare data standardization and interoperability.\n\n### ğŸ“‚ <a name="browser-automation"></a>Browser Automation\n\nWeb content access and automation capabilities. Enables searching, scraping, and processing web content in AI-friendly formats.\n\n- [BB-fat/browser-use-rs](https://github.com/BB-fat/browser-use-rs) ğŸ¦€ Lightweight browser automation MCP server in Rust with zero dependencies.\n- [34892002/bilibili-mcp-js](https://github.com/34892002/bilibili-mcp-js) ğŸ“‡ ğŸ  - A MCP server that supports searching for Bilibili content. Provides LangChain integration examples and test scripts.\n- [agent-infra/mcp-server-browser](https://github.com/bytedance/UI-TARS-desktop/tree/main/packages/agent-infra/mcp-servers/browser) ğŸ“‡ ğŸ  - Browser automation capabilities using Puppeteer, both support local and remote browser connection.\n- [automatalabs/mcp-server-playwright](https://github.com/Automata-Labs-team/MCP-Server-Playwright) ğŸ - An MCP server for browser automation using Playwright\n- [blackwhite084/playwright-plus-python-mcp](https://github.com/blackwhite084/playwright-plus-python-mcp) ğŸ - An MCP python server using Playwright for browser automation,more suitable for llm\n- [browserbase/mcp-server-browserbase](https://github.com/browserbase/mcp-server-browserbase) ğŸ–ï¸ ğŸ“‡ - Automate browser interactions in the cloud (e.g. web navigation, data extraction, form filling, and more)\n- [browsermcp/mcp](https://github.com/browsermcp/mcp) ğŸ“‡ ğŸ  - Automate your local Chrome browser\n- [co-browser/browser-use-mcp-server](https://github.com/co-browser/browser-use-mcp-server) ğŸ - browser-use packaged as an MCP server with SSE transport. includes a dockerfile to run chromium in docker + a vnc server.\n- [eat-pray-ai/yutu](https://github.com/eat-pray-ai/yutu) ğŸï¸ ğŸ  ğŸ ğŸ§ ğŸªŸ - A fully functional MCP server and CLI for YouTube to automate YouTube operation\n- [executeautomation/playwright-mcp-server](https://github.com/executeautomation/mcp-playwright) ğŸ“‡ - An MCP server using Playwright for browser automation and webscrapping\n- [eyalzh/browser-control-mcp](https://github.com/eyalzh/browser-control-mcp) ğŸ“‡ ğŸ  - An MCP server paired with a browser extension that enables LLM clients to control the user''s browser (Firefox).\n- [freema/firefox-devtools-mcp](https://github.com/freema/firefox-devtools-mcp) ğŸ“‡ ğŸ  - Firefox browser automation via WebDriver BiDi for testing, scraping, and browser control. Supports snapshot/UID-based interactions, network monitoring, console capture, and screenshots.\n- [fradser/mcp-server-apple-reminders](https://github.com/FradSer/mcp-server-apple-reminders) ğŸ“‡ ğŸ  ğŸ - An MCP server for interacting with Apple Reminders on macOS\n- [getrupt/ashra-mcp](https://github.com/getrupt/ashra-mcp) ğŸ“‡ ğŸ  - Extract structured data from any website. Just prompt and get JSON.\n- [kimtaeyoon83/mcp-server-youtube-transcript](https://github.com/kimtaeyoon83/mcp-server-youtube-transcript) ğŸ“‡ â˜ï¸ - Fetch YouTube subtitles and transcripts for AI analysis\n- [kimtth/mcp-aoai-web-browsing](https://github.com/kimtth/mcp-aoai-web-browsing) ğŸ ğŸ  - A `minimal` server/client MCP implementation using Azure OpenAI and Playwright.\n- [lightpanda-io/gomcp](https://github.com/lightpanda-io/gomcp) ğŸ ğŸ /â˜ï¸ ğŸ§/ğŸ - An MCP server in Go for Lightpanda, the ultra fast headless browser designed for web automation\n- [microsoft/playwright-mcp](https://github.com/microsoft/playwright-mcp) - Official Microsoft Playwright MCP server, enabling LLMs to interact with web pages through structured accessibility snapshots\n- [modelcontextprotocol/server-puppeteer](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/puppeteer) ğŸ“‡ ğŸ  - Browser automation for web scraping and interaction\n- [ndthanhdev/mcp-browser-kit](https://github.com/ndthanhdev/mcp-browser-kit) ğŸ“‡ ğŸ  - An MCP Server that enables AI assistants to interact with your local browsers.\n- [operative_sh/web-eval-agent](https://github.com/Operative-Sh/web-eval-agent) ğŸ ğŸ  ğŸ - An MCP Server that autonomously debugs web applications with browser-use browser agents\n- [olostep/olostep-mcp-server](https://github.com/olostep/olostep-mcp-server) ğŸ“‡ â˜ï¸ - Web scraping, crawling, and search API. Extract content in Markdown/JSON, batch process 10k URLs, and get AI-powered answers with citations.\n- [pskill9/web-search](https://github.com/pskill9/web-search) ğŸ“‡ ğŸ  - An MCP server that enables free web searching using Google search results, with no API keys required.\n- [PhungXuanAnh/selenium-mcp-server](https://github.com/PhungXuanAnh/selenium-mcp-server) ğŸ ğŸ  ğŸ ğŸªŸ ğŸ§ - A Model Context Protocol server providing web automation capabilities through Selenium WebDriver\n- [recursechat/mcp-server-apple-shortcuts](https://github.com/recursechat/mcp-server-apple-shortcuts) ğŸ“‡ ğŸ  ğŸ - An MCP Server Integration with Apple Shortcuts\n- [xspadex/bilibili-mcp](https://github.com/xspadex/bilibili-mcp.git) ğŸ“‡ ğŸ  - A FastMCP-based tool that fetches Bilibili''s trending videos and exposes them via a standard MCP interface.\n- [imprvhub/mcp-browser-agent](https://github.com/imprvhub/mcp-browser-agent) ğŸ“‡ ğŸ  - A Model Context Protocol (MCP) integration that provides Claude Desktop with autonomous browser automation capabilities.\n\n### â˜ï¸ <a name="cloud-platforms"></a>Cloud Platforms\n\nCloud platform service integration. Enables management and interaction with cloud infrastructure and services.\n\n- [4everland/4everland-hosting-mcp](https://github.com/4everland/4everland-hosting-mcp) ğŸ–ï¸ ğŸ“‡ ğŸ  ğŸ ğŸ§ - An MCP server implementation for 4EVERLAND Hosting enabling instant deployment of AI-generated code to decentralized storage networks like Greenfield, IPFS, and Arweave.\n- [aashari/mcp-server-aws-sso](https://github.com/aashari/mcp-server-aws-sso) ğŸ“‡ â˜ï¸ ğŸ  - AWS Single Sign-On (SSO) integration enabling AI systems to securely interact with AWS resources by initiating SSO login, listing accounts/roles, and executing AWS CLI commands using temporary credentials.\n- [alexbakers/mcp-ipfs](https://github.com/alexbakers/mcp-ipfs) ğŸ“‡ â˜ï¸ - upload and manipulation of IPFS storage\n- [alexei-led/aws-mcp-server](https://github.com/alexei-led/aws-mcp-server) ğŸ â˜ï¸ - A lightweight but powerful server that enables AI assistants to execute AWS CLI commands, use Unix pipes, and apply prompt templates for common AWS tasks in a safe Docker environment with multi-architecture support\n- [alexei-led/k8s-mcp-server](https://github.com/alexei-led/k8s-mcp-server) ğŸ - A lightweight yet robust server that empowers AI assistants to securely execute Kubernetes CLI commands (`kubectl`, `helm`, `istioctl`, and `argocd`) using Unix pipes in a safe Docker environment with multi-architecture support.\n- [aliyun/alibaba-cloud-ops-mcp-server](https://github.com/aliyun/alibaba-cloud-ops-mcp-server) ğŸ–ï¸ ğŸ â˜ï¸ - A MCP server that enables AI assistants to operation resources on Alibaba Cloud, supporting ECS, Cloud Monitor, OOS and widely used cloud products.\n- [awslabs/mcp](https://github.com/awslabs/mcp) ğŸ–ï¸ â˜ï¸ - AWS MCP servers for seamless integration with AWS services and resources.\n- [localstack/localstack-mcp-server](https://github.com/localstack/localstack-mcp-server) ğŸ–ï¸ ğŸ“‡ ğŸ  - A MCP server for LocalStack to manage local AWS environments, including lifecycle operations, infra deployments, log analysis, fault injection, and state management.\n- [bright8192/esxi-mcp-server](https://github.com/bright8192/esxi-mcp-server) ğŸ â˜ï¸ - A VMware ESXi/vCenter management server based on MCP (Model Control Protocol), providing simple REST API interfaces for virtual machine management.\n- [cloudflare/mcp-server-cloudflare](https://github.com/cloudflare/mcp-server-cloudflare) ğŸ–ï¸ ğŸ“‡ â˜ï¸ - Integration with Cloudflare services including Workers, KV, R2, and D1\n- [cyclops-ui/mcp-cyclops](https://github.com/cyclops-ui/mcp-cyclops) ğŸ–ï¸ ğŸï¸ â˜ï¸ - An MCP server that allows AI agents to manage Kubernetes resources through Cyclops abstraction\n- [elementfm/mcp](https://gitlab.com/elementfm/mcp) ğŸ–ï¸ ğŸ ğŸ“‡ ğŸ  â˜ï¸ - Open source podcast hosting platform\n- [erikhoward/adls-mcp-server](https://github.com/erikhoward/adls-mcp-server) ğŸ â˜ï¸/ğŸ  - MCP Server for Azure Data Lake Storage. It can perform manage containers, read/write/upload/download operations on container files and manage file metadata.\n- [espressif/esp-rainmaker-mcp](https://github.com/espressif/esp-rainmaker-mcp) ğŸ–ï¸ ğŸ ğŸ  â˜ï¸ ğŸ“Ÿ - Official Espressif MCP Server to manage and control ESP RainMaker Devices.\n- [flux159/mcp-server-kubernetes](https://github.com/Flux159/mcp-server-kubernetes) ğŸ“‡ â˜ï¸/ğŸ  - Typescript implementation of Kubernetes cluster operations for pods, deployments, services.\n- [hardik-id/azure-resource-graph-mcp-server](https://github.com/hardik-id/azure-resource-graph-mcp-server) ğŸ“‡ â˜ï¸/ğŸ  - A Model Context Protocol server for querying and analyzing Azure resources at scale using Azure Resource Graph, enabling AI assistants to explore and monitor Azure infrastructure.\n- [jdubois/azure-cli-mcp](https://github.com/jdubois/azure-cli-mcp) - A wrapper around the Azure CLI command line that allows you to talk directly to Azure\n- [johnneerdael/netskope-mcp](https://github.com/johnneerdael/netskope-mcp) ğŸ”’ â˜ï¸ - An MCP to give access to all Netskope Private Access components within a Netskope Private Access environments including detailed setup information and LLM examples on usage.\n- [kestra-io/mcp-server-python](https://github.com/kestra-io/mcp-server-python) ğŸ â˜ï¸ - Implementation of MCP server for [Kestra](https://kestra.io) workflow orchestration platform.\n- [liveblocks/liveblocks-mcp-server](https://github.com/liveblocks/liveblocks-mcp-server) ğŸ–ï¸ ğŸ“‡ â˜ï¸ - Create, modify, and delete different aspects of [Liveblocks](https://liveblocks.io) such as rooms, threads, comments, notifications, and more. Additionally, it has read access to Storage and Yjs.\n- [manusa/Kubernetes MCP Server](https://github.com/manusa/kubernetes-mcp-server) ğŸï¸ ğŸ  A - powerful Kubernetes MCP server with additional support for OpenShift. Besides providing CRUD operations for **any** Kubernetes resource, this server provides specialized tools to interact with your cluster.\n- [Nebula-Block-Data/nebulablock-mcp-server](https://github.com/Nebula-Block-Data/nebulablock-mcp-server) ğŸ“‡ ğŸ  - integrates with the fastmcp library to expose the full range of NebulaBlock API functionalities as accessible tools\n- [nwiizo/tfmcp](https://github.com/nwiizo/tfmcp) - ğŸ¦€ ğŸ  - A Terraform MCP server allowing AI assistants to manage and operate Terraform environments, enabling reading configurations, analyzing plans, applying configurations, and managing Terraform state.\n- [openstack-kr/python-openstackmcp-server](https://github.com/openstack-kr/python-openstackmcp-server) ğŸ â˜ï¸ - OpenStack MCP server for cloud infrastructure management based on openstacksdk.\n- [pibblokto/cert-manager-mcp-server](https://github.com/pibblokto/cert-manager-mcp-server) ğŸ ğŸ/ğŸ§ â˜ï¸ - mcp server for [cert-manager](https://github.com/cert-manager/cert-manager) management and troubleshooting\n- [portainer/portainer-mcp](https://github.com/portainer/portainer-mcp) ğŸï¸ â˜ï¸/ğŸ  - A powerful MCP server that enables AI assistants to seamlessly interact with Portainer instances, providing natural language access to container management, deployment operations, and infrastructure monitoring capabilities.\n- [pulumi/mcp-server](https://github.com/pulumi/mcp-server) ğŸ–ï¸ ğŸ“‡ ğŸ  - MCP server for interacting with Pulumi using the Pulumi Automation API and Pulumi Cloud API. Enables MCP clients to perform Pulumi operations like retrieving package information, previewing changes, deploying updates, and retrieving stack outputs programmatically.\n- [pythonanywhere/pythonanywhere-mcp-server](https://github.com/pythonanywhere/pythonanywhere-mcp-server) ğŸ ğŸ  - MCP server implementation for PythonAnywhere cloud platform.\n- [qiniu/qiniu-mcp-server](https://github.com/qiniu/qiniu-mcp-server) ğŸ â˜ï¸ - A MCP built on Qiniu Cloud products, supporting access to Qiniu Cloud Storage, media processing services, etc.\n- [redis/mcp-redis-cloud](https://github.com/redis/mcp-redis-cloud) ğŸ“‡ â˜ï¸ - Manage your Redis Cloud resources effortlessly using natural language. Create databases, monitor subscriptions, and configure cloud deployments with simple commands.\n- [reza-gholizade/k8s-mcp-server](https://github.com/reza-gholizade/k8s-mcp-server) ğŸï¸ â˜ï¸/ğŸ  - A Kubernetes Model Context Protocol (MCP) server that provides tools for interacting with Kubernetes clusters through a standardized interface, including API resource discovery, resource management, pod logs, metrics, and events.\n- [rohitg00/kubectl-mcp-server](https://github.com/rohitg00/kubectl-mcp-server) ğŸ â˜ï¸/ğŸ  - A Model Context Protocol (MCP) server for Kubernetes that enables AI assistants like Claude, Cursor, and others to interact with Kubernetes clusters through natural language.\n- [rrmistry/tilt-mcp](https://github.com/rrmistry/tilt-mcp) ğŸ ğŸ  ğŸ ğŸªŸ ğŸ§ - A Model Context Protocol server that integrates with Tilt to provide programmatic access to Tilt resources, logs, and management operations for Kubernetes development environments.\n- [silenceper/mcp-k8s](https://github.com/silenceper/mcp-k8s) ğŸï¸ â˜ï¸/ğŸ  - MCP-K8S is an AI-driven Kubernetes resource management tool that allows users to operate any resources in Kubernetes clusters through natural language interaction, including native resources (like Deployment, Service) and custom resources (CRD). No need to memorize complex commands - just describe your needs, and AI will accurately execute the corresponding cluster operations, greatly enhancing the usability of Kubernetes.\n- [StacklokLabs/mkp](https://github.com/StacklokLabs/mkp) ğŸï¸ â˜ï¸ - MKP is a Model Context Protocol (MCP) server for Kubernetes that allows LLM-powered applications to interact with Kubernetes clusters. It provides tools for listing and applying Kubernetes resources through the MCP protocol.\n- [StacklokLabs/ocireg-mcp](https://github.com/StacklokLabs/ocireg-mcp) ğŸï¸ â˜ï¸ - An SSE-based MCP server that allows LLM-powered applications to interact with OCI registries. It provides tools for retrieving information about container images, listing tags, and more.\n- [strowk/mcp-k8s-go](https://github.com/strowk/mcp-k8s-go) ğŸï¸ â˜ï¸/ğŸ  - Kubernetes cluster operations through MCP\n- [thunderboltsid/mcp-nutanix](https://github.com/thunderboltsid/mcp-nutanix) ğŸï¸ ğŸ /â˜ï¸ - Go-based MCP Server for interfacing with Nutanix Prism Central resources.\n- [trilogy-group/aws-pricing-mcp](https://github.com/trilogy-group/aws-pricing-mcp) ğŸï¸ â˜ï¸/ğŸ  - Get up-to-date EC2 pricing information with one call. Fast. Powered by a pre-parsed AWS pricing catalogue.\n- [VmLia/books-mcp-server](https://github.com/VmLia/books-mcp-server) ğŸ“‡ â˜ï¸ - This is an MCP server used for querying books, and it can be applied in common MCP clients, such as Cherry Studio.\n- [weibaohui/k8m](https://github.com/weibaohui/k8m) ğŸï¸ â˜ï¸/ğŸ  - Provides MCP multi-cluster Kubernetes management and operations, featuring a management interface, logging, and nearly 50 built-in tools covering common DevOps and development scenarios. Supports both standard and CRD resources.\n- [weibaohui/kom](https://github.com/weibaohui/kom) ğŸï¸ â˜ï¸/ğŸ  - Provides MCP multi-cluster Kubernetes management and operations. It can be integrated as an SDK into your own project and includes nearly 50 built-in tools covering common DevOps and development scenarios. Supports both standard and CRD resources.\n- [wenhuwang/mcp-k8s-eye](https://github.com/wenhuwang/mcp-k8s-eye) ğŸï¸ â˜ï¸/ğŸ  - MCP Server for kubernetes management, and analyze your cluster, application health\n- [elevy99927/devops-mcp-webui](https://github.com/elevy99927/devops-mcp-webui) ğŸ â˜ï¸/ğŸ  - MCP Server for Kubernetes integrated with Open-WebUI, bridging the gap between DevOps and non-technical teams. Supports `kubectl` and `helm` operations through natural-language commands.\n\n### ğŸ‘¨â€ğŸ’» <a name="code-execution"></a>Code Execution\n\nCode execution servers. Allow LLMs to execute code in a secure environment, e.g. for coding agents.\n\n- [alfonsograziano/node-code-sandbox-mcp](https://github.com/alfonsograziano/node-code-sandbox-mcp) ğŸ“‡ ğŸ  â€“ A Node.js MCP server that spins up isolated Docker-based sandboxes for executing JavaScript snippets with on-the-fly npm dependency installation and clean teardown\n- [ckanthony/openapi-mcp](https://github.com/ckanthony/openapi-mcp) ğŸï¸ â˜ï¸ - OpenAPI-MCP: Dockerized MCP Server to allow your AI agent to access any API with existing api docs.\n- [gwbischof/outsource-mcp](https://github.com/gwbischof/outsource-mcp) ğŸ â˜ï¸ - Give your AI assistant its own AI assistants. For example: "Could you ask openai to generate an image of a dog?"\n- [hileamlakB/PRIMS](https://github.com/hileamlakB/PRIMS) ğŸ ğŸ  â€“ A Python Runtime Interpreter MCP Server that executes user-submitted code in an isolated environment.\n- [ouvreboite/openapi-to-mcp](https://github.com/ouvreboite/openapi-to-mcp) #ï¸âƒ£ â˜ï¸ - Lightweight MCP server to access any API using their OpenAPI specification. Supports OAuth2 and full JSON schema parameters and request body.\n- [pydantic/pydantic-ai/mcp-run-python](https://github.com/pydantic/pydantic-ai/tree/main/mcp-run-python) ğŸ ğŸ  - Run Python code in a secure sandbox via MCP tool calls\n- [r33drichards/mcp-js](https://github.com/r33drichards/mcp-js) ğŸ¦€ ğŸ  ğŸ§ ğŸ - A Javascript code execution sandbox that uses v8 to isolate code to run AI generated javascript locally without fear. Supports heap snapshotting for persistent sessions.\n- [yepcode/mcp-server-js](https://github.com/yepcode/mcp-server-js) ğŸ–ï¸ ğŸ“‡ â˜ï¸ - Execute any LLM-generated code in a secure and scalable sandbox environment and create your own MCP tools using JavaScript or Python, with full support for NPM and PyPI packages\n- [dagger/container-use](https://github.com/dagger/container-use) ğŸï¸ ğŸ  ğŸ§ ğŸ ğŸªŸ - Containerized environments for coding agents. Multiple agents can work independently, isolated in fresh containers and git branches. No conflicts, many experiments. Full execution history, terminal access to agent environments, git workflow. Any agent/model/infra stack.\n\n### ğŸ¤– <a name="coding-agents"></a>Coding Agents\n\nFull coding agents that enable LLMs to read, edit, and execute code and solve general programming tasks completely autonomously.\n\n- [shashankss1205/codegraphcontext](https://github.com/Shashankss1205/CodeGraphContext) ğŸ ğŸ  ğŸ ğŸªŸ ğŸ§ An MCP server that indexes local code into a graph database to provide context to AI assistants with a graphical code visualizations for humans.\n- [doggybee/mcp-server-leetcode](https://github.com/doggybee/mcp-server-leetcode) ğŸ“‡ â˜ï¸ - An MCP server that enables AI models to search, retrieve, and solve LeetCode problems. Supports metadata filtering, user profiles, submissions, and contest data access.\n- [ezyang/codemcp](https://github.com/ezyang/codemcp) ğŸ ğŸ  - Coding agent with basic read, write and command line tools.\n- [gabrielmaialva33/winx-code-agent](https://github.com/gabrielmaialva33/winx-code-agent) ğŸ¦€ ğŸ  - A high-performance Rust reimplementation of WCGW for code agents, providing shell execution and advanced file management capabilities for LLMs via MCP.\n- [jinzcdev/leetcode-mcp-server](https://github.com/jinzcdev/leetcode-mcp-server) ğŸ“‡ â˜ï¸ - MCP server enabling automated access to **LeetCode**''s programming problems, solutions, submissions and public data with optional authentication for user-specific features (e.g., notes), supporting both `leetcode.com` (global) and `leetcode.cn` (China) sites.\n- [juehang/vscode-mcp-server](https://github.com/juehang/vscode-mcp-server) ğŸ“‡ ğŸ  - A MCP Server that allows AI such as Claude to read from the directory structure in a VS Code workspace, see problems picked up by linter(s) and the language server, read code files, and make edits.\n- [micl2e2/code-to-tree](https://github.com/micl2e2/code-to-tree) ğŸŒŠ ğŸ  ğŸ“Ÿ ğŸ§ ğŸªŸ ğŸ - A single-binary MCP server that converts source code into AST, regardless of language.\n- [oraios/serena](https://github.com/oraios/serena) ğŸ ğŸ  - A fully-featured coding agent that relies on symbolic code operations by using language servers.\n- [pdavis68/RepoMapper](https://github.com.mcas.ms/pdavis68/RepoMapper) ğŸ§ ğŸªŸ ğŸ - An MCP server (and command-line tool) to provide a dynamic map of chat-related files from the repository with their function prototypes and related files in order of relevance. Based on the "Repo Map" functionality in Aider.chat\n- [rinadelph/Agent-MCP](https://github.com/rinadelph/Agent-MCP) ğŸ ğŸ  - A framework for creating multi-agent systems using MCP for coordinated AI collaboration, featuring task management, shared context, and RAG capabilities.\n- [sim-xia/blind-auditor](https://github.com/Sim-xia/Blind-Auditor) - ğŸ ğŸ  ğŸ ğŸªŸ ğŸ§ A zero-cost MCP server that forces AI to self-correct generation messages using prompt injection, independent self-audition and context isolation.\n- [stippi/code-assistant](https://github.com/stippi/code-assistant) ğŸ¦€ ğŸ  - Coding agent with basic list, read, replace_in_file, write, execute_command and web search tools. Supports multiple projects concurrently.\n- [tiianhk/MaxMSP-MCP-Server](https://github.com/tiianhk/MaxMSP-MCP-Server) ğŸ ğŸ  ğŸµ ğŸ¥ - A coding agent for Max (Max/MSP/Jitter), which is a visual programming language for music and multimedia.\n- [nesquikm/mcp-rubber-duck](https://github.com/nesquikm/mcp-rubber-duck) ğŸ“‡ ğŸ  â˜ï¸ - An MCP server that bridges to multiple OpenAI-compatible LLMs - your AI rubber duck debugging panel for explaining problems to various AI "ducks" and getting different perspectives\n- [askbudi/roundtable](https://github.com/askbudi/roundtable) ğŸ ğŸ  - Zero-configuration MCP server that unifies multiple AI coding assistants (Claude Code, Cursor, Codex) through intelligent auto-discovery and standardized interface. Essential infrastructure for autonomous agent development and multi-AI collaboration workflows.\n- [VertexStudio/developer](https://github.com/VertexStudio/developer) ğŸ¦€ ğŸ  ğŸ ğŸªŸ ğŸ§ - Comprehensive developer tools for file editing, shell command execution, and screen capture capabilities\n- [x51xxx/codex-mcp-tool](https://github.com/x51xxx/codex-mcp-tool) ğŸ“‡ â˜ï¸ - MCP server that connects your IDE or AI assistant to Codex CLI for code analysis and editing with support for multiple models (gpt-5-codex, o3, codex-1)\n- [x51xxx/copilot-mcp-server](https://github.com/x51xxx/copilot-mcp-server) ğŸ“‡ â˜ï¸ - MCP server that connects your IDE or AI assistant to GitHub Copilot CLI for code analysis, review, and batch processing\n- [wende/cicada](https://github.com/wende/cicada) ğŸ ğŸ  ğŸ ğŸªŸ ğŸ§ - Code Intelligence for Elixir: module search, function tracking, and PR attribution through tree-sitter AST parsing\n\n### ğŸ–¥ï¸ <a name="command-line"></a>Command Line\n\nRun commands, capture output and otherwise interact with shells and command line tools.\n\n- [automateyournetwork/pyATS_MCP](https://github.com/automateyournetwork/pyATS_MCP) - Cisco pyATS server enabling structured, model-driven interaction with network devices.\n- [aymericzip/intlayer](https://github.com/aymericzip/intlayer) ğŸ“‡ â˜ï¸ ğŸ  - A MCP Server that enhance your IDE with AI-powered assistance for Intlayer i18n / CMS tool: smart CLI access, access to the docs.\n- [blakerouse/ssh-mcp](https://github.com/blakerouse/ssh-mcp) ğŸï¸ ğŸ  ğŸ ğŸªŸ ğŸ§ - MCP server exposing SSH control for Linux and Windows servers. Allows long running commands and the ability to perform commands on multiple hosts at the same time.\n- [ferrislucas/iterm-mcp](https://github.com/ferrislucas/iterm-mcp) ğŸ–¥ï¸ ğŸ› ï¸ ğŸ’¬ - A Model Context Protocol server that provides access to iTerm. You can run commands and ask questions about what you see in the iTerm terminal.\n- [g0t4/mcp-server-commands](https://github.com/g0t4/mcp-server-commands) ğŸ“‡ ğŸ  - Run any command with `run_command` and `run_script` tools.\n- [maxim-saplin/mcp_safe_local_python_executor](https://github.com/maxim-saplin/mcp_safe_local_python_executor) - Safe Python interpreter based on HF Smolagents `LocalPythonExecutor`\n- [misiektoja/kill-process-mcp](https://github.com/misiektoja/kill-process-mcp) ğŸ ğŸ  ğŸ ğŸªŸ ğŸ§ - List and terminate OS processes via natural language queries\n- [MladenSU/cli-mcp-server](https://github.com/MladenSU/cli-mcp-server) ğŸ ğŸ  - Command line interface with secure execution and customizable security policies\n- [OthmaneBlial/term_mcp_deepseek](https://github.com/OthmaneBlial/term_mcp_deepseek) ğŸ ğŸ  - A DeepSeek MCP-like Server for Terminal\n- [ooples/mcp-console-automation](https://github.com/ooples/mcp-console-automation) ğŸ“‡ ğŸ  ğŸ ğŸªŸ ğŸ§ - Production-ready MCP server for AI-driven console automation and monitoring. 40 tools for session management, SSH, testing, monitoring, and background jobs. Like Playwright for terminal applications.\n- [sonirico/mcp-shell](https://github.com/sonirico/mcp-shell) - ğŸï¸ ğŸ  ğŸ ğŸªŸ ğŸ§ Give hands to AI. MCP server to run shell commands securely, auditably, and on demand on isolated environments like docker.\n- [louis030195/terminator-mcp-agent](https://github.com/mediar-ai/terminator/tree/main/terminator-mcp-agent) ğŸ¦€ ğŸ“‡ ğŸ  ğŸ ğŸªŸ ğŸ§ - Desktop GUI automation using accessibility APIs. Control Windows, macOS, and Linux applications without vision models or screenshots. Supports workflow recording, structured data extraction, and browser DOM inspection.\n- [tufantunc/ssh-mcp](https://github.com/tufantunc/ssh-mcp) ğŸ“‡ ğŸ  ğŸ§ ğŸªŸ - MCP server exposing SSH control for Linux and Windows servers via Model Context Protocol. Securely execute remote shell commands with password or SSH key authentication.\n- [tumf/mcp-shell-server](https://github.com/tumf/mcp-shell-server) - A secure shell command execution server implementing the Model Context Protocol (MCP)\n- [wonderwhy-er/DesktopCommanderMCP](https://github.com/wonderwhy-er/DesktopCommanderMCP) ğŸ“‡ ğŸ  ğŸ ğŸªŸ ğŸ§ - A swiss-army-knife that can manage/execute programs and read/write/search/edit code and text files.\n- [nihalxkumar/arch-mcp](https://github.com/nihalxkumar/arch-mcp) ğŸ ğŸ  ğŸ§ - Arch Linux MCP Server to the Arch Linux ecosystem of the Arch Wiki, AUR, and official repositories for AI-assisted Arch Linux usage on Arch and non-Arch systems. Features include searching Arch Wiki and AUR, getting package info, checking for updates, installing packages securely, and analyzing PKGBUILDs.\n\n### ğŸ’¬ <a name="communication"></a>Communication\n\nIntegration with communication platforms for message management and channel operations. Enables AI models to interact with team communication tools.\n\n- [AbdelStark/nostr-mcp](https://github.com/AbdelStark/nostr-mcp) â˜ï¸ - A Nostr MCP server that allows to interact with Nostr, enabling posting notes, and more.\n- [adhikasp/mcp-twikit](https://github.com/adhikasp/mcp-twikit) ğŸ â˜ï¸ - Interact with Twitter search and timeline\n- [agentmail-toolkit/mcp](https://github.com/agentmail-to/agentmail-toolkit/tree/main/mcp) ğŸ ğŸ’¬ - An MCP server to create inboxes on the fly to send, receive, and take actions on email. We aren''t AI agents for email, but email for AI Agents.\n- [areweai/tsgram-mcp](https://github.com/areweai/tsgram-mcp) - TSgram: Telegram + Claude with local workspace access on your phone in typescript. Read, write, and vibe code on the go!\n- [arpitbatra123/mcp-googletasks](https://github.com/arpitbatra123/mcp-googletasks) ğŸ“‡ â˜ï¸ - An MCP server to interface with the Google Tasks API\n- [Cactusinhand/mcp_server_notify](https://github.com/Cactusinhand/mcp_server_notify) ğŸ ğŸ  - A MCP server that send desktop notifications with sound effect when agent tasks are completed.\n- [trycourier/courier-mcp](https://github.com/trycourier/courier-mcp) ğŸ–ï¸ ğŸ’¬ â˜ï¸ ğŸ› ï¸ ğŸ“‡ ğŸ¤– - Build multi-channel notifications into your product, send messages, update lists, invoke automations, all without leaving your AI coding space. \n- [PhononX/cv-mcp-server](https://github.com/PhononX/cv-mcp-server) ğŸ–ï¸ ğŸ“‡ ğŸ  â˜ï¸ ğŸ ğŸªŸ ğŸ§ - MCP Server that connects AI Agents to [Carbon Voice](https://getcarbon.app). Create, manage, and interact with voice messages, conversations, direct messages, folders, voice memos, AI actions and more in [Carbon Voice](https://getcarbon.app).\n- [carterlasalle/mac_messages_mcp](https://github.com/carterlasalle/mac_messages_mcp) ğŸ  ğŸ ğŸš€ - An MCP server that securely interfaces with your iMessage database via the Model Context Protocol (MCP), allowing LLMs to query and analyze iMessage conversations. It includes robust phone number validation, attachment processing, contact management, group chat handling, and full support for sending and receiving messages.\n- [chaindead/telegram-mcp](https://github.com/chaindead/telegram-mcp) ğŸï¸ ğŸ  - Telegram API integration for accessing user data, managing dialogs (chats, channels, groups), retrieving messages, and handling read status\n- [chigwell/telegram-mcp](https://github.com/chigwell/telegram-mcp) ğŸ ğŸ  - Telegram API integration for accessing user data, managing dialogs (chats, channels, groups), retrieving messages, sending messages and handling read status.\n- [Danielpeter-99/calcom-mcp](https://github.com/Danielpeter-99/calcom-mcp) ğŸ ğŸ  - MCP server for Calcom. Manage event types, create bookings, and access Cal.com scheduling data through LLMs.\n- [discourse/discourse-mcp](https://github.com/discourse/discourse-mcp) ğŸ–ï¸ ğŸ’ â˜ï¸ ğŸ  ğŸ’¬ ğŸ ğŸªŸ ğŸ§ - Official Discourse MCP server for forum integration. Search topics, read posts, manage categories and tags, discover users, and interact with Discourse communities.\n- [elie222/inbox-zero](https://github.com/elie222/inbox-zero/tree/main/apps/mcp-server) ğŸ â˜ï¸ - An MCP server for Inbox Zero. Adds functionality on top of Gmail like finding out which emails you need to reply to or need to follow up on.\n- [gerkensm/callcenter.js-mcp](https://github.com/gerkensm/callcenter.js-mcp) ğŸ“‡ â˜ï¸ - An MCP server to make phone calls using VoIP/SIP and OpenAI''s Realtime API and observe the transcript.\n- [gitmotion/ntfy-me-mcp](https://github.com/gitmotion/ntfy-me-mcp) ğŸ“‡ â˜ï¸ ğŸ  - An ntfy MCP server for sending/fetching ntfy notifications to your self-hosted ntfy server from AI Agents ğŸ“¤ (supports secure token auth & more - use with npx or docker!)\n- [gotoolkits/wecombot](https://github.com/gotoolkits/mcp-wecombot-server.git) ğŸš€ â˜ï¸ - An MCP server application that sends various types of messages to the WeCom group robot.\n- [hannesrudolph/imessage-query-fastmcp-mcp-server](https://github.com/hannesrudolph/imessage-query-fastmcp-mcp-server) ğŸ ğŸ  ğŸ - An MCP server that provides safe access to your iMessage database through Model Context Protocol (MCP), enabling LLMs to query and analyze iMessage conversations with proper phone number validation and attachment handling\n- [i-am-bee/acp-mcp](https://github.com/i-am-bee/acp-mcp) ğŸ ğŸ’¬ - An MCP server acting as an adapter into the [ACP](https://agentcommunicationprotocol.dev) ecosystem. Seamlessly exposes ACP agents to MCP clients, bridging the communication gap between the two protocols.\n- [InditexTech/mcp-teams-server](https://github.com/InditexTech/mcp-teams-server) ğŸ â˜ï¸ - MCP server that integrates Microsoft Teams messaging (read, post, mention, list members and threads)\n- [Infobip/mcp](https://github.com/infobip/mcp) ğŸ–ï¸ â˜ï¸ - Official Infobip MCP server for integrating Infobip global cloud communication platform. It equips AI agents with communication superpowers, allowing them to send and receive SMS and RCS messages, interact with WhatsApp and Viber, automate communication workflows, and manage customer data, all in a production-ready environment.\n- [jagan-shanmugam/mattermost-mcp-host](https://github.com/jagan-shanmugam/mattermost-mcp-host) ğŸ ğŸ  - A MCP server along with MCP host that provides access to Mattermost teams, channels and messages. MCP host is integrated as a bot in Mattermost with access to MCP servers that can be configured.\n- [jaipandya/producthunt-mcp-server](https://github.com/jaipandya/producthunt-mcp-server) ğŸ ğŸ  - MCP server for Product Hunt. Interact with trending posts, comments, collections, users, and more.\n- [joinly-ai/joinly](https://github.com/joinly-ai/joinly) ğŸâ˜ï¸ - MCP server to interact with browser-based meeting platforms (Zoom, Teams, Google Meet). Enables AI agents to send bots to online meetings, gather live transcripts, speak text, and send messages in the meeting chat.\n- [keturiosakys/bluesky-context-server](https://github.com/keturiosakys/bluesky-context-server) ğŸ“‡ â˜ï¸ - Bluesky instance integration for querying and interaction\n- [khan2a/telephony-mcp-server](https://github.com/khan2a/telephony-mcp-server) ğŸ ğŸ’¬ - MCP Telephony server for automating voice calls with Speech-to-Text and Speech Recognition to summarize call conversations. Send and receive SMS, detect voicemail, and integrate with Vonage APIs for advanced telephony workflows.\n- [korotovsky/slack-mcp-server](https://github.com/korotovsky/slack-mcp-server) ğŸ“‡ â˜ï¸ - The most powerful MCP server for Slack Workspaces.\n- [lharries/whatsapp-mcp](https://github.com/lharries/whatsapp-mcp) ğŸ ğŸï¸ - An MCP server for searching your personal WhatsApp messages, contacts and sending messages to individuals or groups\n- [line/line-bot-mcp-server](https://github.com/line/line-bot-mcp-server) ğŸ– ğŸ“‡ â˜ï¸ - MCP Server for Integrating LINE Official Account\n- [madbonez/caldav-mcp](https://github.com/madbonez/caldav-mcp) ğŸ â˜ï¸ - Universal MCP server for CalDAV protocol integration. Works with any CalDAV-compatible calendar server including Yandex Calendar, Google Calendar (via CalDAV), Nextcloud, ownCloud, Apple iCloud, and others. Supports creating events with recurrence, categories, priority, attendees, reminders, searching events, and retrieving events by UID.\n- [OverQuotaAI/chatterboxio-mcp-server](https://github.com/OverQuotaAI/chatterboxio-mcp-server) ğŸ“‡ â˜ï¸ - MCP server implementation for ChatterBox.io, enabling AI agents to send bots to online meetings (Zoom, Google Meet) and obtain transcripts and recordings.\n- [wyattjoh/imessage-mcp](https://github.com/wyattjoh/imessage-mcp) ğŸ“‡ ğŸ  ğŸ - A Model Context Protocol server for reading iMessage data from macOS.\n- [sawa-zen/vrchat-mcp](https://github.com/sawa-zen/vrchat-mcp) - ğŸ“‡ ğŸ  This is an MCP server for interacting with the VRChat API. You can retrieve information about friends, worlds, avatars, and more in VRChat.\n- [softeria/ms-365-mcp-server](https://github.com/softeria/ms-365-mcp-server) ğŸ“‡ â˜ï¸ - MCP server that connects to Microsoft Office and the whole Microsoft 365 suite using Graph API (including Outlook, mail, files, Excel, calendar)\n- [saseq/discord-mcp](https://github.com/SaseQ/discord-mcp) â˜• ğŸ“‡ ğŸ  ğŸ’¬ - A MCP server for the Discord integration. Enable your AI assistants to seamlessly interact with Discord. Enhance your Discord experience with powerful automation capabilities.\n- [teddyzxcv/ntfy-mcp](https://github.com/teddyzxcv/ntfy-mcp) - The MCP server that keeps you informed by sending the notification on phone using ntfy\n- [userad/didlogic_mcp](https://github.com/UserAd/didlogic_mcp) ğŸ â˜ï¸ - An MCP server for [DIDLogic](https://didlogic.com). Adds functionality to manage SIP endpoints, numbers and destinations.\n- [YCloud-Developers/ycloud-whatsapp-mcp-server](https://github.com/YCloud-Developers/ycloud-whatsapp-mcp-server) ğŸ“‡ ğŸ  - MCP server for WhatsApp Business Platform by YCloud.\n- [zcaceres/gtasks-mcp](https://github.com/zcaceres/gtasks-mcp) ğŸ“‡ â˜ï¸ - An MCP server to Manage Google Tasks\n- [ztxtxwd/open-feishu-mcp-server](https://github.com/ztxtxwd/open-feishu-mcp-server) ğŸ“‡ â˜ï¸ ğŸ  - A Model Context Protocol (MCP) server with built-in Feishu OAuth authentication, supporting remote connections and providing comprehensive Feishu document management tools including block creation, content updates, and advanced features.\n\n\n### ğŸ‘¤ <a name="customer-data-platforms"></a>Customer Data Platforms\n\nProvides access to customer profiles inside of customer data platforms\n\n- [antv/mcp-server-chart](https://github.com/antvis/mcp-server-chart) ğŸ–ï¸ ğŸ“‡ â˜ï¸ - A Model Context Protocol server for generating visual charts using [AntV](https://github.com/antvis).\n- [hustcc/mcp-echarts](https://github.com/hustcc/mcp-echarts) ğŸ“‡ ğŸ  - Generate visual charts using [Apache ECharts](https://echarts.apache.org) with AI MCP dynamically.\n- [hustcc/mcp-mermaid](https://github.com/hustcc/mcp-mermaid) ğŸ“‡ ğŸ  - Generate [mermaid](https://mermaid.js.org/) diagram and chart with AI MCP dynamically.\n- [iaptic/mcp-server-iaptic](https://github.com/iaptic/mcp-server-iaptic) ğŸ–ï¸ ğŸ“‡ â˜ï¸ - Connect with [iaptic](https://www.iaptic.com) to ask about your Customer Purchases, Transaction data and App Revenue statistics.\n- [OpenDataMCP/OpenDataMCP](https://github.com/OpenDataMCP/OpenDataMCP) ğŸ â˜ï¸ - Connect any Open Data to any LLM with Model Context Protocol.\n- [sergehuber/inoyu-mcp-unomi-server](https://github.com/sergehuber/inoyu-mcp-unomi-server) ğŸ“‡ â˜ï¸ - An MCP server to access and updates profiles on an Apache Unomi CDP server.\n- [tinybirdco/mcp-tinybird](https://github.com/tinybirdco/mcp-tinybird) ğŸ â˜ï¸ - An MCP server to interact with a Tinybird Workspace from any MCP client.\n\n### ğŸ—„ï¸ <a name="databases"></a>Databases\n\nSecure database access with schema inspection capabilities. Enables querying and analyzing data with configurable security controls including read-only access.\n\n- [Aiven-Open/mcp-aiven](https://github.com/Aiven-Open/mcp-aiven) - ğŸ â˜ï¸ ğŸ–ï¸ -  Navigate your [Aiven projects](https://go.aiven.io/mcp-server) and interact with the PostgreSQLÂ®, Apache KafkaÂ®, ClickHouseÂ® and OpenSearchÂ® services\n- [alexanderzuev/supabase-mcp-server](https://github.com/alexander-zuev/supabase-mcp-server) - Supabase MCP Server with support for SQL query execution and database exploration tools\n- [aliyun/alibabacloud-tablestore-mcp-server](https://github.com/aliyun/alibabacloud-tablestore-mcp-server) â˜• ğŸ â˜ï¸ - MCP service for Tablestore, features include adding documents, semantic search for documents based on vectors and scalars, RAG-friendly, and serverless.\n- [amineelkouhen/mcp-cockroachdb](https://github.com/amineelkouhen/mcp-cockroachdb) ğŸ â˜ï¸ - A Model Context Protocol server for managing, monitoring, and querying data in [CockroachDB](https://cockroachlabs.com).\n- [benborla29/mcp-server-mysql](https://github.com/benborla/mcp-server-mysql) â˜ï¸ ğŸ  - MySQL database integration in NodeJS with configurable access controls and schema inspection\n- [bram2w/baserow](https://github.com/bram2w/baserow) - Baserow database integration with table search, list, and row create, read, update, and delete capabilities.\n- [c4pt0r/mcp-server-tidb](https://github.com/c4pt0r/mcp-server-tidb) ğŸ â˜ï¸ - TiDB database integration with schema inspection and query capabilities\n- [Canner/wren-engine](https://github.com/Canner/wren-engine) ğŸ ğŸ¦€ ğŸ  - The Semantic Engine for Model Context Protocol(MCP) Clients and AI Agents\n- [centralmind/gateway](https://github.com/centralmind/gateway) ğŸï¸ ğŸ  ğŸ ğŸªŸ - MCP and MCP SSE Server that automatically generate API based on database schema and data. Supports PostgreSQL, Clickhouse, MySQL, Snowflake, BigQuery, Supabase\n- [ChristianHinge/dicom-mcp](https://github.com/ChristianHinge/dicom-mcp) ğŸ â˜ï¸ ğŸ  - DICOM integration to query, read, and move medical images and reports from PACS and other DICOM compliant systems.\n- [chroma-core/chroma-mcp](https://github.com/chroma-core/chroma-mcp) ğŸ–ï¸ ğŸ â˜ï¸ ğŸ  - Chroma MCP server to access local and cloud Chroma instances for retrieval capabilities\n- [ClickHouse/mcp-clickhouse](https://github.com/ClickHouse/mcp-clickhouse) ğŸ â˜ï¸ - ClickHouse database integration with schema inspection and query capabilities\n- [confluentinc/mcp-confluent](https://github.com/confluentinc/mcp-confluent) ğŸ â˜ï¸ - Confluent integration to interact with Confluent Kafka and Confluent Cloud REST APIs.\n- [Couchbase-Ecosystem/mcp-server-couchbase](https://github.com/Couchbase-Ecosystem/mcp-server-couchbase) ğŸ–ï¸ ğŸ â˜ï¸ ğŸ  - Couchbase MCP server provides unfied access to both Capella cloud and self-managed clusters for document operations, SQL++ queries and natural language data analysis.\n- [cr7258/elasticsearch-mcp-server](https://github.com/cr7258/elasticsearch-mcp-server) ğŸ ğŸ  - MCP Server implementation that provides Elasticsearch interaction\n- [crystaldba/postgres-mcp](https://github.com/crystaldba/postgres-mcp) ğŸ ğŸ  - All-in-one MCP server for Postgres development and operations, with tools for performance analysis, tuning, and health checks\n- [Dataring-engineering/mcp-server-trino](https://github.com/Dataring-engineering/mcp-server-trino) ğŸ â˜ï¸ - Trino MCP Server to query and access data from Trino Clusters.\n- [davewind/mysql-mcp-server](https://github.com/dave-wind/mysql-mcp-server) ğŸï¸ ğŸ  A â€“Â user-friendly read-only mysql mcp server for cursor and n8n...\n- [designcomputer/mysql_mcp_server](https://github.com/designcomputer/mysql_mcp_server) ğŸ ğŸ  - MySQL database integration with configurable access controls, schema inspection, and comprehensive security guidelines\n- [domdomegg/airtable-mcp-server](https://github.com/domdomegg/airtable-mcp-server) ğŸ“‡ ğŸ  - Airtable database integration with schema inspection, read and write capabilities\n- [edwinbernadus/nocodb-mcp-server](https://github.com/edwinbernadus/nocodb-mcp-server) ğŸ“‡ â˜ï¸ - Nocodb database integration, read and write capabilities\n- [ergut/mcp-bigquery-server](https://github.com/ergut/mcp-bigquery-server) ğŸ“‡ â˜ï¸ - Server implementation for Google BigQuery integration that enables direct BigQuery database access and querying capabilities\n- [f4ww4z/mcp-mysql-server](https://github.com/f4ww4z/mcp-mysql-server) ğŸ“‡ ğŸ  - Node.js-based MySQL database integration that provides secure MySQL database operations\n- [ferrants/memvid-mcp-server](https://github.com/ferrants/memvid-mcp-server) ğŸ ğŸ  - Python Streamable HTTP Server you can run locally to interact with [memvid](https://github.com/Olow304/memvid) storage and semantic search.\n- [fireproof-storage/mcp-database-server](https://github.com/fireproof-storage/mcp-database-server) ğŸ“‡ â˜ï¸ - Fireproof ledger database with multi-user sync\n- [freema/mcp-gsheets](https://github.com/freema/mcp-gsheets) ğŸ“‡ â˜ï¸ - MCP server for Google Sheets API integration with comprehensive reading, writing, formatting, and sheet management capabilities.\n- [FreePeak/db-mcp-server](https://github.com/FreePeak/db-mcp-server) ğŸï¸ ğŸ  â€“ A high-performance multi-database MCP server built with Golang, supporting MySQL & PostgreSQL (NoSQL coming soon). Includes built-in tools for query execution, transaction management, schema exploration, query building, and performance analysis, with seamless Cursor integration for enhanced database workflows.\n- [furey/mongodb-lens](https://github.com/furey/mongodb-lens) ğŸ“‡ ğŸ  - MongoDB Lens: Full Featured MCP Server for MongoDB Databases\n- [gannonh/firebase-mcp](https://github.com/gannonh/firebase-mcp) ğŸ”¥ â›…ï¸ - Firebase services including Auth, Firestore and Storage.\n- [get-convex/convex-backend](https://stack.convex.dev/convex-mcp-server) ğŸ“‡ â˜ï¸ - Convex database integration to introspect tables, functions, and run oneoff queries ([Source](https://github.com/get-convex/convex-backend/blob/main/npm-packages/convex/src/cli/mcp.ts))\n- [gigamori/mcp-run-sql-connectorx](https://github.com/gigamori/mcp-run-sql-connectorx) ğŸ â˜ï¸ ğŸ  ğŸ ğŸªŸ ğŸ§ - An MCP server that executes SQL via ConnectorX and streams the result to a CSV or Parquet file. Supports PostgreSQL, MariaDB, BigQuery, RedShift, MS SQL Server, etc.\n- [googleapis/genai-toolbox](https://github.com/googleapis/genai-toolbox) ğŸï¸ â˜ï¸ - Open source MCP server specializing in easy, fast, and secure tools for Databases.\n- [GreptimeTeam/greptimedb-mcp-server](https://github.com/GreptimeTeam/greptimedb-mcp-server) ğŸ ğŸ  - MCP Server for querying GreptimeDB.\n- [hannesrudolph/sqlite-explorer-fastmcp-mcp-server](https://github.com/hannesrudolph/sqlite-explorer-fastmcp-mcp-server) ğŸ ğŸ  - An MCP server that provides safe, read-only access to SQLite databases through Model Context Protocol (MCP). This server is built with the FastMCP framework, which enables LLMs to explore and query SQLite databases with built-in safety features and query validation.\n- [henilcalagiya/google-sheets-mcp](https://github.com/henilcalagiya/google-sheets-mcp) ğŸ ğŸ  - Your AI Assistant''s Gateway to Google Sheets! 25 powerful tools for seamless Google Sheets automation via MCP.\n- [hydrolix/mcp-hydrolix](https://github.com/hydrolix/mcp-hydrolix) ğŸ–ï¸ ğŸ â˜ï¸ - Hydrolix time-series datalake integration providing schema exploration and query capabilities to LLM-based workflows.\n- [idoru/influxdb-mcp-server](https://github.com/idoru/influxdb-mcp-server) ğŸ“‡ â˜ï¸ ğŸ  - Run queries against InfluxDB OSS API v2.\n- [InfluxData/influxdb3_mcp_server](https://github.com/influxdata/influxdb3_mcp_server) ğŸ–ï¸ ğŸ“‡ ğŸ  â˜ï¸ - Official MCP server for InfluxDB 3 Core/Enterprise/Cloud Dedicated\n- [isaacwasserman/mcp-snowflake-server](https://github.com/isaacwasserman/mcp-snowflake-server) ğŸ â˜ï¸ - Snowflake integration implementing read and (optional) write operations as well as insight tracking\n- [iunera/druid-mcp-server](https://github.com/iunera/druid-mcp-server) â˜• â˜ï¸ ğŸ  - Comprehensive MCP server for Apache Druid that provides extensive tools, resources, and prompts for managing and analyzing Druid clusters.\n- [yannbrrd/simple_snowflake_mcp](https://github.com/YannBrrd/simple_snowflake_mcp) ğŸ â˜ï¸ - Simple Snowflake MCP server that works behind a corporate proxy. Read and write (optional) operations\n- [joshuarileydev/supabase-mcp-server](https://github.com/joshuarileydev/supabase) - Supabase MCP Server for managing and creating projects and organisations in Supabase\n- [jovezhong/mcp-timeplus](https://github.com/jovezhong/mcp-timeplus) ğŸ â˜ï¸ - MCP server for Apache Kafka and Timeplus. Able to list Kafka topics, poll Kafka messages, save Kafka data locally and query streaming data with SQL via Timeplus\n- [jparkerweb/mcp-sqlite](https://github.com/jparkerweb/mcp-sqlite) ğŸ“‡ ğŸ  - Model Context Protocol (MCP) server that provides comprehensive SQLite database interaction capabilities.\n- [KashiwaByte/vikingdb-mcp-server](https://github.com/KashiwaByte/vikingdb-mcp-server) ğŸ â˜ï¸ - VikingDB integration with collection and index introduction, vector store and search capabilities.\n- [kiliczsh/mcp-mongo-server](https://github.com/kiliczsh/mcp-mongo-server) ğŸ“‡ ğŸ  - A Model Context Protocol Server for MongoDB\n- [ktanaka101/mcp-server-duckdb](https://github.com/ktanaka101/mcp-server-duckdb) ğŸ ğŸ  - DuckDB database integration with schema inspection and query capabilities\n- [LucasHild/mcp-server-bigquery](https://github.com/LucasHild/mcp-server-bigquery) ğŸ â˜ï¸ - BigQuery database integration with schema inspection and query capabilities\n- [memgraph/mcp-memgraph](https://github.com/memgraph/ai-toolkit/tree/main/integrations/mcp-memgraph) ğŸ ğŸ  - Memgraph MCP Server - includes a tool to run a query against Memgraph and a schema resource.\n- [montumodi/mongodb-atlas-mcp-server](https://github.com/montumodi/mongodb-atlas-mcp-server) ğŸ“‡ â˜ï¸ ğŸªŸ ğŸ ğŸ§ - A Model Context Protocol (MCP) that provides access to the MongoDB Atlas API. This server wraps the `mongodb-atlas-api-client` package to expose MongoDB Atlas functionality through MCP tools.\n- [modelcontextprotocol/server-postgres](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/postgres) ğŸ“‡ ğŸ  - PostgreSQL database integration with schema inspection and query capabilities\n- [modelcontextprotocol/server-sqlite](https://github.com/modelcontextprotocol/servers-archived/tree/main/src/sqlite) ğŸ ğŸ  - SQLite database operations with built-in analysis features\n- [neo4j-contrib/mcp-neo4j](https://github.com/neo4j-contrib/mcp-neo4j) ğŸ ğŸ  - Model Context Protocol with Neo4j (Run queries, Knowledge Graph Memory, Manaage Neo4j Aura Instances)\n- [neondatabase/mcp-server-neon](https://github.com/neondatabase/mcp-server-neon) ğŸ“‡ â˜ï¸ â€” An MCP Server for creating and managing Postgres databases using Neon Serverless Postgres\n- [niledatabase/nile-mcp-server](https://github.com/niledatabase/nile-mcp-server) MCP server for Nile''s Postgres platform - Manage and query Postgres databases, tenants, users, auth using LLMs\n- [openlink/mcp-server-jdbc](https://github.com/OpenLinkSoftware/mcp-jdbc-server) ğŸ ğŸ  - An MCP server for generic Database Management System (DBMS) Connectivity via the Java Database Connectivity (JDBC) protocol\n- [openlink/mcp-server-odbc](https://github.com/OpenLinkSoftware/mcp-odbc-server) ğŸ ğŸ  - An MCP server for generic Database Management System (DBMS) Connectivity via the Open Database Connectivity (ODBC) protocol\n- [openlink/mcp-server-sqlalchemy](https://github.com/OpenLinkSoftware/mcp-sqlalchemy-server) ğŸ ğŸ  - An MCP server for generic Database Management System (DBMS) Connectivity via SQLAlchemy using Python ODBC (pyodbc)\n- [pab1it0/adx-mcp-server](https://github.com/pab1it0/adx-mcp-server) ğŸ â˜ï¸ - Query and analyze Azure Data Explorer databases\n- [pab1it0/prometheus-mcp-server](https://github.com/pab1it0/prometheus-mcp-server) ğŸ â˜ï¸ -  Query and analyze Prometheus, open-source monitoring system.\n- [prisma/mcp](https://github.com/prisma/mcp) ğŸ“‡ â˜ï¸ ğŸ  - Gives LLMs the ability to manage Prisma Postgres databases (e.g. spin up new databases and run migrations or queries).\n- [qdrant/mcp-server-qdrant](https://github.com/qdrant/mcp-server-qdrant) ğŸ ğŸ  - A Qdrant MCP server\n- [QuantGeekDev/mongo-mcp](https://github.com/QuantGeekDev/mongo-mcp) ğŸ“‡ ğŸ  - MongoDB integration that enables LLMs to interact directly with databases.\n- [quarkiverse/mcp-server-jdbc](https://github.com/quarkiverse/quarkus-mcp-servers/tree/main/jdbc) â˜• ğŸ  - Connect to any JDBC-compatible database and query, insert, update, delete, and more.\n- [rashidazarang/airtable-mcp](https://github.com/rashidazarang/airtable-mcp) ğŸ â˜ï¸ - Connect AI tools directly to Airtable. Query, create, update, and delete records using natural language. Features include base management, table operations, schema manipulation, record filtering, and data migration through a standardized MCP interface.\n- [redis/mcp-redis](https://github.com/redis/mcp-redis) ğŸ ğŸ  - The Redis official MCP Server offers an interface to manage and search data in Redis.\n\n- [runekaagaard/mcp-alchemy](https://github.com/runekaagaard/mcp-alchemy) ğŸ ğŸ  - Universal SQLAlchemy-based database integration supporting PostgreSQL, MySQL, MariaDB, SQLite, Oracle, MS SQL Server and many more databases. Features schema and relationship inspection, and large dataset analysis capabilities.\n- [wenerme/wener-mssql-mcp](https://github.com/wenerme/wode/tree/develop/packages/wener-mssql-mcp) ğŸ“‡ ğŸ  - MSSQL database integration with schema inspection and query capabilities\n- [s2-streamstore/s2-sdk-typescript](https://github.com/s2-streamstore/s2-sdk-typescript) ğŸ–ï¸ ğŸ“‡ â˜ï¸ - Official MCP server for the S2.dev serverless stream platform.\n- [schemacrawler/SchemaCrawler-MCP-Server-Usage](https://github.com/schemacrawler/SchemaCrawler-MCP-Server-Usage) ğŸ–ï¸ â˜• â€“ Connect to any relational database, and be able to get valid SQL, and ask questions like what does a certain column prefix mean.\n- [sirmews/mcp-pinecone](https://github.com/sirmews/mcp-pinecone) ğŸ â˜ï¸ - Pinecone integration with vector search capabilities\n- [skysqlinc/skysql-mcp](https://github.com/skysqlinc/skysql-mcp) ğŸ–ï¸ â˜ï¸ - Serverless MariaDB Cloud DB MCP server. Tools to launch, delete, execute SQL and work with DB level AI agents for accurate text-2-sql and conversations.\n- [Snowflake-Labs/mcp](https://github.com/Snowflake-Labs/mcp) ğŸ â˜ï¸ - Open-source MCP server for Snowflake from official Snowflake-Labs supports prompting Cortex Agents, querying structured & unstructured data, object management, SQL execution, semantic view querying, and more. RBAC, fine-grained CRUD controls, and all authentication methods supported.\n- [subnetmarco/pgmcp](https://github.com/subnetmarco/pgmcp) ğŸï¸ ğŸ  - Natural language PostgreSQL queries with automatic streaming, read-only safety, and universal database compatibility.\n- [pgtuner_mcp](https://github.com/isdaniel/pgtuner_mcp) ğŸğŸ—„ï¸ - provides AI-powered PostgreSQL performance tuning capabilities.\n- [supabase-community/supabase-mcp](https://github.com/supabase-community/supabase-mcp) ğŸ–ï¸ ğŸ“‡ â˜ï¸ - Official Supabase MCP server to connect AI assistants directly with your Supabase project and allows them to perform tasks like managing tables, fetching config, and querying data.\n- [TheRaLabs/legion-mcp](https://github.com/TheRaLabs/legion-mcp) ğŸ ğŸ  Universal database MCP server supporting multiple database types including PostgreSQL, Redshift, CockroachDB, MySQL, RDS MySQL, Microsoft SQL Server, BigQuery, Oracle DB, and SQLite.\n- [tradercjz/dolphindb-mcp-server](https://github.com/tradercjz/dolphindb-mcp-server) ğŸ â˜ï¸ - TDolphinDB database integration with schema inspection and query capabilities\n- [tuannvm/mcp-trino](https://github.com/tuannvm/mcp-trino) ğŸï¸ â˜ï¸ - A Go implementation of a Model Context Protocol (MCP) server for Trino\n- [VictoriaMetrics-Community/mcp-victorialogs](https://github.com/VictoriaMetrics-Community/mcp-victorialogs) ğŸ–ï¸ ğŸï¸ ğŸ  - Provides comprehensive integration with your [VictoriaLogs instance APIs](https://docs.victoriametrics.com/victorialogs/querying/#http-api) and [documentation](https://docs.victoriametrics.com/victorialogs/) for working with logs, investigating and debugging tasks related to your VictoriaLogs instances.\n- [weaviate/mcp-server-weaviate](https://github.com/weaviate/mcp-server-weaviate) ğŸ ğŸ“‡ â˜ï¸ - An MCP Server to connect to your Weaviate collections as a knowledge base as well as using Weaviate as a chat memory store.\n- [wenb1n-dev/mysql_mcp_server_pro](https://github.com/wenb1n-dev/mysql_mcp_server_pro)  ğŸ ğŸ  - Supports SSE, STDIO; not only limited to MySQL''s CRUD functionality; also includes database exception analysis capabilities; controls database permissions based on roles; and makes it easy for developers to extend tools with customization\n- [wenb1n-dev/SmartDB_MCP](https://github.com/wenb1n-dev/SmartDB_MCP)  ğŸ ğŸ  - A universal database MCP server supporting simultaneous connections to multiple databases. It provides tools for database operations, health analysis, SQL optimization, and more. Compatible with mainstream databases including MySQL, PostgreSQL, SQL Server, MariaDB, Dameng, and Oracle. Supports Streamable HTTP, SSE, and STDIO; integrates OAuth 2.0; and is designed for easy customization and extension by developers.\n- [xexr/mcp-libsql](https://github.com/Xexr/mcp-libsql) ğŸ“‡ ğŸ  â˜ï¸ - Production-ready MCP server for libSQL databases with comprehensive security and management tools.\n- [XGenerationLab/xiyan_mcp_server](https://github.com/XGenerationLab/xiyan_mcp_server) ğŸ“‡ â˜ï¸ â€” An MCP server that supports fetching data from a database using natural language queries, powered by XiyanSQL as the text-to-SQL LLM.\n- [xing5/mcp-google-sheets](https://github.com/xing5/mcp-google-sheets) ğŸ â˜ï¸ - A Model Context Protocol server for interacting with Google Sheets. This server provides tools to create, read, update, and manage spreadsheets through the Google Sheets API.\n- [ydb/ydb-mcp](https://github.com/ydb-platform/ydb-mcp) ğŸ–ï¸ ğŸ â˜ï¸ - MCP server for interacting with [YDB](https://ydb.tech) databases\n- [yincongcyincong/VictoriaMetrics-mcp-server](https://github.com/yincongcyincong/VictoriaMetrics-mcp-server) ğŸ ğŸ  - An MCP server for interacting with VictoriaMetrics database.\n- [Zhwt/go-mcp-mysql](https://github.com/Zhwt/go-mcp-mysql) ğŸï¸ ğŸ  â€“ Easy to use, zero dependency MySQL MCP server built with Golang with configurable readonly mode and schema inspection.\n- [zilliztech/mcp-server-milvus](https://github.com/zilliztech/mcp-server-milvus) ğŸ ğŸ  â˜ï¸ - MCP Server for Milvus / Zilliz, making it possible to interact with your database.\n\n### ğŸ“Š <a name="data-platforms"></a>Data Platforms\n\nData Platforms for data integration, transformation and pipeline orchestration.\n\n- [aywengo/kafka-schema-reg-mcp](https://github.com/aywengo/kafka-schema-reg-mcp) ğŸ â˜ï¸ ğŸ  ğŸ ğŸªŸ ğŸ§ - Comprehensive Kafka Schema Registry MCP server with 48 tools for multi-registry management, schema migration, and enterprise features.\n- [dbt-labs/dbt-mcp](https://github.com/dbt-labs/dbt-mcp) ğŸ–ï¸ ğŸ ğŸ  â˜ï¸ - Official MCP server for [dbt (data build tool)](https://www.getdbt.com/product/what-is-dbt) providing integration with dbt Core/Cloud CLI, project metadata discovery, model information, and semantic layer querying capabilities.\n- [flowcore/mcp-flowcore-platform](https://github.com/flowcore-io/mcp-flowcore-platform) ğŸ–ï¸ ğŸ“‡ â˜ï¸ ğŸ  - Interact with Flowcore to perform actions, ingest data, and analyse, cross reference and utilise any data in your data cores, or in public data cores; all with human language.\n- [JordiNei/mcp-databricks-server](https://github.com/JordiNeil/mcp-databricks-server) ğŸ â˜ï¸ - Connect to Databricks API, allowing LLMs to run SQL queries, list jobs, and get job status.\n- [jwaxman19/qlik-mcp](https://github.com/jwaxman19/qlik-mcp) ğŸ“‡ â˜ï¸ - MCP Server for Qlik Cloud API that enables querying applications, sheets, and extracting data from visualizations with comprehensive authentication and rate limiting support.\n- [keboola/keboola-mcp-server](https://github.com/keboola/keboola-mcp-server) ğŸ - interact with Keboola Connection Data Platform. This server provides tools for listing and accessing data from Keboola Storage API.\n- [mattijsdp/dbt-docs-mcp](https://github.com/mattijsdp/dbt-docs-mcp) ğŸ ğŸ  - MCP server for dbt-core (OSS) users as the official dbt MCP only supports dbt Cloud. Supports project metadata, model and column-level lineage and dbt documentation.\n- [yashshingvi/databricks-genie-MCP](https://github.com/yashshingvi/databricks-genie-MCP) ğŸ â˜ï¸ - A server that connects to the Databricks Genie API, allowing LLMs to ask natural language questions, run SQL queries, and interact with Databricks conversational agents.\n- [alkemiai/alkemi-mcp](https://github.com/alkemi-ai/alkemi-mcp) ğŸ“‡ â˜ï¸ - MCP Server for natural language querying of Snowflake, Google BigQuery, and DataBricks Data Products through Alkemi.ai.\n- [avisangle/method-crm-mcp](https://github.com/avisangle/method-crm-mcp) ğŸ â˜ï¸ ğŸ  ğŸ ğŸªŸ ğŸ§ - Production-ready MCP server for Method CRM API integration with 20 comprehensive tools for tables, files, users, events, and API key management. Features rate limiting, retry logic, and dual transport support (stdio/HTTP).\n- [paracetamol951/caisse-enregistreuse-mcp-server](https://github.com/paracetamol951/caisse-enregistreuse-mcp-server) ğŸ  ğŸ§ ğŸ â˜ï¸ - Allows you to automate or monitor business operations, sales recorder, POS software, CRM.\n\n\n### ğŸ’» <a name="developer-tools"></a>Developer Tools\n\nTools and integrations that enhance the development workflow and environment management.\n\n- [21st-dev/Magic-MCP](https://github.com/21st-dev/magic-mcp) - Create crafted UI components inspired by the best 21st.dev design engineers.\n- [louis030195/gptzero-mcp](https://github.com/louis030195/gptzero-mcp) ğŸ“‡ â˜ï¸ ğŸ ğŸªŸ ğŸ§ - AI detection for text content with GPTZero API. Detect AI-generated text, get confidence scores, multilingual support (French/Spanish), and detailed probability breakdowns.\n- [aashari/mcp-server-atlassian-bitbucket](https://github.com/aashari/mcp-server-atlassian-bitbucket) ğŸ“‡ â˜ï¸ - Atlassian Bitbucket Cloud integration. Enables AI systems to interact with repositories, pull requests, workspaces, and code in real time.\n- [aashari/mcp-server-atlassian-confluence](https://github.com/aashari/mcp-server-atlassian-confluence) ğŸ“‡ â˜ï¸ - Atlassian Confluence Cloud integration. Enables AI systems to interact with Confluence spaces, pages, and content with automatic ADF to Markdown conversion.\n- [aashari/mcp-server-atlassian-jira](https://github.com/aashari/mcp-server-atlassian-jira) ğŸ“‡ â˜ï¸ - Atlassian Jira Cloud integration. Enables AI systems to interact with Jira projects, issues, comments, and related development information in real time.\n- [abrinsmead/mindpilot-mcp](https://github.com/abrinsmead/mindpilot-mcp) ğŸ“‡ ğŸ  - Visualizes code, architecture and other concepts as mermaid diagrams in a locally hosted web app. Just ask your agent to "show me this in a diagram".\n- [admica/FileScopeMCP](https://github.com/admica/FileScopeMCP) ğŸ ğŸ“‡ ğŸ¦€ - Analyzes your codebase identifying important files based on dependency relationships. Generates diagrams and importance scores, helping AI assistants understand the codebase.\n- [agent-hanju/char-index-mcp](https://github.com/agent-hanju/char-index-mcp) ğŸ ğŸ  â˜ï¸ ğŸ ğŸªŸ ğŸ§ - Precise character-level string indexing for LLMs. Provides tools for finding, extracting, and manipulating text by exact character position to solve position-based operations.\n- [akramIOT/MCP_AI_SOC_Sher](https://github.com/akramIOT/MCP_AI_SOC_Sher)  ğŸ â˜ï¸ ğŸ“‡ - MCP Server to do dynamic AI SOC Security Threat analysis for a  Text2SQL  AI Agent.\n- [alimo7amed93/webhook-tester-mcp](https://github.com/alimo7amed93/webhook-tester-mcp)  ğŸ â˜ï¸ â€“ A FastMCP-based server for interacting with webhook-test.com. Enables users to create, retrieve, and delete webhooks locally using Claude.\n- [ambar/simctl-mcp](https://github.com/ambar/simctl-mcp) ğŸ“‡ ğŸ  ğŸ A MCP server implementation for iOS Simulator control.\n- [api7/apisix-mcp](https://github.com/api7/apisix-mcp) ğŸ–ï¸ ğŸ“‡ ğŸ  MCP Server that support for querying and managing all resource in [Apache APISIX](https://github.com/apache/apisix).\n- [ArchAI-Labs/fastmcp-sonarqube-metrics](https://github.com/ArchAI-Labs/fastmcp-sonarqube-metrics) ğŸ ğŸ  ğŸªŸ ğŸ§ ğŸ -  A Model Context Protocol (MCP) server that provides a set of tools for retrieving information about SonarQube projects like metrics (actual and historical), issues, health status.\n- [artmann/package-registry-mcp](https://github.com/artmann/package-registry-mcp) ğŸ  ğŸ“‡ ğŸ ğŸªŸ ğŸ§ - MCP server for searching and getting up-to-date information about NPM, Cargo, PyPi, and NuGet packages.\n- [wyattjoh/jsr-mcp](https://github.com/wyattjoh/jsr-mcp) ğŸ“‡ â˜ï¸ - Model Context Protocol server for the JSR (JavaScript Registry)\n- [augmnt/augments-mcp-server](https://github.com/augmnt/augments-mcp-server) ğŸ“‡ â˜ï¸ ğŸ  - Transform Claude Code with intelligent, real-time access to 90+ framework documentation sources. Get accurate, up-to-date code generation that follows current best practices for React, Next.js, Laravel, FastAPI, Tailwind CSS, and more.\n- [automation-ai-labs/mcp-link](https://github.com/automation-ai-labs/mcp-link) ğŸï¸ ğŸ  - Seamlessly Integrate Any API with AI Agents (with OpenAPI Schema)\n- [avisangle/jenkins-mcp-server](https://github.com/avisangle/jenkins-mcp-server) ğŸ ğŸ  ğŸ ğŸªŸ ğŸ§ - Enterprise-grade Jenkins CI/CD integration with multi-tier caching, pipeline monitoring, artifact management, and batch operations. Features 21 MCP tools for job management, build status tracking, and queue management with CSRF protection and 2FA support.\n- [axliupore/mcp-code-runner](https://github.com/axliupore/mcp-code-runner) ğŸ“‡ ğŸ  - An MCP server for running code locally via Docker and supporting multiple programming languages.\n- [azer/react-analyzer-mcp](https://github.com/azer/react-analyzer-mcp) ğŸ“‡ ğŸ  - Analyze React code locally, generate docs / llm.txt for whole project at once\n- [bitrise-io/bitrise-mcp](https://github.com/bitrise-io/bitrise-mcp) ğŸ–ï¸ ğŸ â˜ï¸ ğŸ ğŸªŸ ğŸ§ - MCP Server for the [Bitrise](https://bitrise.io) API, enabling app management, build operations, artifact management and more.\n- [buildkite/buildkite-mcp-server](https://github.com/buildkite/buildkite-mcp-server) ğŸ–ï¸ ğŸï¸ ğŸ  â˜ï¸ ğŸ ğŸªŸ ğŸ§ - Official MCP server for Buildkite. Create new pipelines, diagnose and fix failures, trigger builds, monitor job queues, and more.\n- [Chunkydotdev/bldbl-mcp](https://github.com/chunkydotdev/bldbl-mcp) ğŸ“‡ â˜ï¸ ğŸ ğŸªŸ ğŸ§ - Official MCP server for Buildable AI-powered development platform [bldbl.dev](https://bldbl.dev). Enables AI assistants to manage tasks, track progress, get project context, and collaborate with humans on software projects.\n- [CircleCI/mcp-server-circleci](https://github.com/CircleCI-Public/mcp-server-circleci) ğŸ“‡ â˜ï¸ Enable AI Agents to fix build failures from CircleCI.\n- [Wolfe-Jam/claude-faf-mcp](https://github.com/Wolfe-Jam/claude-faf-mcp) ğŸ“‡ ğŸ  - First & only persistent project context MCP. Provides .faf (Foundational AI-context Format) Project DNA with 33+ tools, Podium scoring (0-100%), and format-driven architecture. Official Anthropic Registry. 10k+ npm downloads.\n- [cjo4m06/mcp-shrimp-task-manager](https://github.com/cjo4m06/mcp-shrimp-task-manager) ğŸ“‡ â˜ï¸ ğŸ  â€“ A programming-focused task management system that boosts coding agents like Cursor AI with advanced task memory, self-reflection, and dependency management. [ShrimpTaskManager](https://cjo4m06.github.io/mcp-shrimp-task-manager)\n- [ckanthony/gin-mcp](https://github.com/ckanthony/gin-mcp) ğŸï¸ â˜ï¸ ğŸ“Ÿ ğŸªŸ ğŸ§ ğŸ - A zero-configuration Go library to automatically expose existing Gin web framework APIs as MCP tools.\n- [BrunoKrugel/echo-mcp](https://github.com/BrunoKrugel/echo-mcp) ğŸï¸ â˜ï¸ ğŸ“Ÿ ğŸªŸ ğŸ§ ğŸ - A zero-configuration Go library to automatically expose any existing Echo web framework APIs as MCP tools.\n- [ckreiling/mcp-server-docker](https://github.com/ckreiling/mcp-server-docker) ğŸ ğŸ  - Integrate with Docker to manage containers, images, volumes, and networks.\n- [CodeLogicIncEngineering/codelogic-mcp-server](https://github.com/CodeLogicIncEngineering/codelogic-mcp-server) ğŸ–ï¸ ğŸ â˜ï¸ ğŸ ğŸªŸ ğŸ§ - Official MCP server for CodeLogic, providing access to code dependency analytics, architectural risk analysis, and impact assessment tools.\n- [Comet-ML/Opik-MCP](https://github.com/comet-ml/opik-mcp) ğŸ–ï¸ ğŸ“‡ â˜ï¸ ğŸ  - Use natural language to explore LLM observability, traces, and monitoring data captured by Opik.\n- [ConfigCat/mcp-server](https://github.com/configcat/mcp-server) ğŸ–ï¸ ğŸ“‡ â˜ï¸ - MCP server for interacting with ConfigCat feature flag platform. Supports managing feature flags, configs, environments, products and organizations.\n- [cqfn/aibolit-mcp-server](https://github.com/cqfn/aibolit-mcp-server) â˜• - Helping Your AI Agent Identify Hotspots for Refactoring; Help AI Understand How to ''Make Code Better''\n- [currents-dev/currents-mcp](https://github.com/currents-dev/currents-mcp) ğŸ–ï¸ ğŸ“‡ â˜ï¸ Enable AI Agents to fix Playwright test failures reported to [Currents](https://currents.dev).\n- [davidan90/time-node-mcp](https://github.com/davidan90/time-node-mcp) ğŸ“‡ ğŸ  - Timezone-aware date and time operations with support for IANA timezones, timezone conversion, and Daylight Saving Time handling.\n- [davidlin2k/pox-mcp-server](https://github.com/davidlin2k/pox-mcp-server) ğŸ ğŸ  - MCP server for the POX SDN controller to provides network control and management capabilities.\n- [delano/postman-mcp-server](https://github.com/delano/postman-mcp-server) ğŸ“‡ â˜ï¸ - Interact with [Postman API](https://www.postman.com/postman/postman-public-workspace/)\n- [deploy-mcp/deploy-mcp](https://github.com/alexpota/deploy-mcp) ğŸ“‡ â˜ï¸ ğŸ  - Universal deployment tracker for AI assistants with live status badges and deployment monitoring\n- [docker/hub-mcp](https://github.com/docker/hub-mcp) ğŸ–ï¸ ğŸ“‡ â˜ï¸ ğŸ  - Official MCP server to interact with Docker Hub, providing access to repositories, hub search and Docker Hardened Images\n- [V0v1kkk/DotNetMetadataMcpServer](https://github.com/V0v1kkk/DotNetMetadataMcpServer) #ï¸âƒ£ ğŸ  ğŸ ğŸªŸ ğŸ§ - Provides AI agents with detailed type information from .NET projects including assembly exploration, namespace discovery, type reflection, and NuGet integration\n- [endorhq/cli](https://github.com/endorhq/cli) ğŸ“‡ â˜ï¸ ğŸ  ğŸªŸ ğŸ§ ğŸ - Endor lets your AI agents run services like MariaDB, Postgres, Redis, Memcached, Alpine, or Valkey in isolated sandboxes. Get pre-configured applications that boot in less than 5 seconds.\n- [eyaltoledano/claude-task-master](https://github.com/eyaltoledano/claude-task-master) ğŸ“‡ â˜ï¸ ğŸ  - AI-powered task management system for AI-driven development. Features PRD parsing, task expansion, multi-provider support (Claude, OpenAI, Gemini, Perplexity, xAI), and selective tool loading for optimized context usage.\n- [etsd-tech/mcp-pointer](https://github.com/etsd-tech/mcp-pointer) ğŸ“‡ ğŸ  ğŸ ğŸªŸ ğŸ§ - Visual DOM element selector for agentic coding tools. Chrome extension + MCP server bridge for Claude Code, Cursor, Windsurf etc. Option+Click to capture elements.\n- [flipt-io/mcp-server-flipt](https://github.com/flipt-io/mcp-server-flipt) ğŸ“‡ ğŸ  - Enable AI assistants to interact with your feature flags in [Flipt](https://flipt.io).\n- [freema/mcp-design-system-extractor](https://github.com/freema/mcp-design-system-extractor) ğŸ“‡ ğŸ  - Extracts component information from Storybook design systems. Provides HTML, styles, props, dependencies, theme tokens and component metadata for AI-powered design system analysis.\n- [gitkraken/gk-cli](https://github.com/gitkraken/gk-cli) ğŸ–ï¸ ğŸï¸ ğŸ  â˜ï¸ ğŸ ğŸªŸ ğŸ§ - A CLI for interacting with GitKraken APIs. Includes an MCP server via `gk mcp` that not only wraps GitKraken APIs, but also Jira, GitHub, GitLab, and more. Works with local tools and remote services.\n- [GLips/Figma-Context-MCP](https://github.com/GLips/Figma-Context-MCP) ğŸ“‡ ğŸ  - Provide coding agents direct access to Figma data to help them one-shot design implementation.\n- [mhmzdev/Figma-Flutter-MCP](https://github.com/mhmzdev/Figma-Flutter-MCP) ğŸ“‡ ğŸ  - Provide coding agents direct access to Figma data to help them write Flutter code for building apps including assets exports, widgets maintenance and full screens implementations.\n- [gofireflyio/firefly-mcp](https://github.com/gofireflyio/firefly-mcp) ğŸ–ï¸ ğŸ“‡ â˜ï¸ - Integrates, discovers, manages, and codifies cloud resources with [Firefly](https://firefly.ai).\n- [gorosun/unified-diff-mcp](https://github.com/gorosun/unified-diff-mcp) ğŸ“‡ ğŸ  - Generate and visualize unified diff comparisons with beautiful HTML/PNG output, supporting side-by-side and line-by-line views for filesystem dry-run integration\n- [Govcraft/rust-docs-mcp-server](https://github.com/Govcraft/rust-docs-mcp-server) ğŸ¦€ ğŸ  - Provides up-to-date documentation context for a specific Rust crate to LLMs via an MCP tool, using semantic search (embeddings) and LLM summarization.\n- [PromptExecution/cratedocs-mcp](https://github.com/promptexecution/cratedocs-mcp) ğŸ¦€ ğŸ  - Outputs short-form Rust crate derived traits,interfaces, etc. from AST (uses same api as rust-analyzer), output limits (token estimation) & crate docs w/regex stripping.\n- [HainanZhao/mcp-gitlab-jira](https://github.com/HainanZhao/mcp-gitlab-jira) ğŸ“‡ â˜ï¸ ğŸ  - Unified MCP server for GitLab and Jira: manage projects, merge requests, files, releases and tickets with AI agents.\n- [haris-musa/excel-mcp-server](https://github.com/haris-musa/excel-mcp-server) ğŸ ğŸ  - An Excel manipulation server providing workbook creation, data operations, formatting, and advanced features (charts, pivot tables, formulae).\n- [sbroenne/mcp-server-excel](https://github.com/sbroenne/mcp-server-excel) #ï¸âƒ£ ğŸ  ğŸªŸ - Full-featured Excel MCP server. 173 operations: Power Query, DAX, VBA, PivotTables, Tables, Charts, ranges, formatting. 100% Excel compatibility - uses Excel app instead of creating .xlsx files. Windows only.\n- [hechtcarmel/jetbrains-debugger-mcp-plugin](https://github.com/hechtcarmel/jetbrains-debugger-mcp-plugin) â˜• ğŸ  - A JetBrains IDE plugin that exposes an MCP server, giving AI coding assistants full programmatic control over the debugger.\n- [hechtcarmel/jetbrains-index-mcp-plugin](https://github.com/hechtcarmel/jetbrains-index-mcp-plugin) â˜• ğŸ  - A JetBrains IDE plugin that exposes an MCP server, enabling AI coding assistants to leverage the IDE''s indexing and refactoring capabilities (rename, safe delete, find references, call hierarchy, type hierarchy, diagnostics and more).\n- [higress-group/higress-ops-mcp-server](https://github.com/higress-group/higress-ops-mcp-server) ğŸ ğŸ  - MCP server that provides comprehensive tools for managing [Higress](https://github.com/alibaba/higress) gateway configurations and operations.\n- [hijaz/postmancer](https://github.com/hijaz/postmancer) ğŸ“‡ ğŸ  - A MCP server for replacing Rest Clients like Postman/Insomnia, by allowing your LLM to maintain and use api collections.\n- [hloiseaufcms/mcp-gopls](https://github.com/hloiseaufcms/mcp-gopls) ğŸï¸ ğŸ  - A MCP server for interacting with [Go''s Language Server Protocol (gopls)](https://github.com/golang/tools/tree/master/gopls) and benefit from advanced Go code analysis features.\n- [hungthai1401/bruno-mcp](https://github.com/hungthai1401/bruno-mcp) ğŸ“‡ ğŸ  - A MCP server for interacting with [Bruno API Client](https://www.usebruno.com/).\n- [hyperb1iss/droidmind](https://github.com/hyperb1iss/droidmind) ğŸ ğŸ  - Control Android devices with AI through MCP, enabling device control, debugging, system analysis, and UI automation with a comprehensive security framework.\n- [Hypersequent/qasphere-mcp](https://github.com/Hypersequent/qasphere-mcp) ğŸ–ï¸ ğŸ“‡ â˜ï¸ - Integration with [QA Sphere](https://qasphere.com/) test management system, enabling LLMs to discover, summarize, and interact with test cases directly from AI-powered IDEs\n- [idosal/git-mcp](https://github.com/idosal/git-mcp) ğŸ“‡ â˜ï¸ - [gitmcp.io](https://gitmcp.io/) is a generic remote MCP server to connect to ANY [GitHub](https://www.github.com) repository or project for documentation\n- [IlyaGulya/gradle-mcp-server](https://github.com/IlyaGulya/gradle-mcp-server) ğŸ  - Gradle integration using the Gradle Tooling API to inspect projects, execute tasks, and run tests with per-test result reporting\n- [promptexecution/just-mcp](https://github.com/promptexecution/just-mcp) ğŸ¦€ ğŸ  - Justfile integration that enables LLMs to execute any CLI or script commands with parameters safely and easily, with environment variable support and comprehensive testing.\n- [InditexTech/mcp-server-simulator-ios-idb](https://github.com/InditexTech/mcp-server-simulator-ios-idb) ğŸ“‡ ğŸ  ğŸ - A Model Context Protocol (MCP) server that enables LLMs to interact with iOS simulators (iPhone, iPad, etc.) through natural language commands.\n- [InhiblabCore/mcp-image-compression](https://github.com/InhiblabCore/mcp-image-compression) ğŸ ğŸ  - MCP server for local compression of various image formats.\n- [InsForge/insforge-mcp](https://github.com/InsForge/insforge-mcp) ğŸ“‡ â˜ï¸ - AI-native backend-as-a-service platform enabling AI agents to build and manage full-stack applications. Provides Auth, Database (PostgreSQL), Storage, and Functions as production-grade infrastructure, reducing MVP development time from weeks to hours.\n- [Inspizzz/jetbrains-datalore-mcp](https://github.com/inspizzz/jetbrains-datalore-mcp) ğŸ â˜ï¸ - MCP server for interacting with cloud deployments of Jetbrains Datalore platform. Fully incorporated Datalore API ( run, run interactively, get run data, fetch files )\n- [ios-simulator-mcp](https://github.com/joshuayoes/ios-simulator-mcp) ğŸ“‡ ğŸ  ğŸ - A Model Context Protocol (MCP) server for interacting with iOS simulators. This server allows you to interact with iOS simulators by getting information about them, controlling UI interactions, and inspecting UI elements.\n- [isaacphi/mcp-language-server](https://github.com/isaacphi/mcp-language-server) ğŸï¸ ğŸ  - MCP Language Server helps MCP enabled clients navigate codebases more easily by giving them access to semantic tools like get definition, references, rename, and diagnostics.\n- [IvanAmador/vercel-ai-docs-mcp](https://github.com/IvanAmador/vercel-ai-docs-mcp) ğŸ“‡ ğŸ  - A Model Context Protocol (MCP) server that provides AI-powered search and querying capabilities for the Vercel AI SDK documentation.\n- [j4c0bs/mcp-server-sql-analyzer](https://github.com/j4c0bs/mcp-server-sql-analyzer) ğŸ - MCP server that provides SQL analysis, linting, and dialect conversion using [SQLGlot](https://github.com/tobymao/sqlglot)\n- [janreges/ai-distiller-mcp](https://github.com/janreges/ai-distiller) ğŸï¸ ğŸ  - Extracts essential code structure from large codebases into AI-digestible format, helping AI agents write code that correctly uses existing APIs on the first attempt.\n- [jasonjmcghee/claude-debugs-for-you](https://github.com/jasonjmcghee/claude-debugs-for-you) ğŸ“‡ ğŸ  - An MCP Server and VS Code Extension which enables (language agnostic) automatic debugging via breakpoints and expression evaluation.\n- [jetbrains/mcpProxy](https://github.com/JetBrains/mcpProxy) ğŸ–ï¸ ğŸ“‡ ğŸ  - Connect to JetBrains IDE\n- [Jktfe/serveMyAPI](https://github.com/Jktfe/serveMyAPI) ğŸ“‡ ğŸ  ğŸ - A personal MCP (Model Context Protocol) server for securely storing and accessing API keys across projects using the macOS Keychain.\n- [jordandalton/restcsv-mcp-server](https://github.com/JordanDalton/RestCsvMcpServer) ğŸ“‡ â˜ï¸ - An MCP server for CSV files.\n- [joshuarileydev/app-store-connect-mcp-server](https://github.com/JoshuaRileyDev/app-store-connect-mcp-server) ğŸ“‡ ğŸ  - An MCP server to communicate with the App Store Connect API for iOS Developers\n- [joshuarileydev/simulator-mcp-server](https://github.com/JoshuaRileyDev/simulator-mcp-server) ğŸ“‡ ğŸ  - An MCP server to control iOS Simulators\n- [Jpisnice/shadcn-ui-mcp-server](https://github.com/Jpisnice/shadcn-ui-mcp-server) ğŸ“‡ ğŸ  - MCP server that gives AI assistants seamless access to shadcn/ui v4 components, blocks, demos, and metadata.\n- [jsdelivr/globalping-mcp-server](https://github.com/jsdelivr/globalping-mcp-server) ğŸ–ï¸ ğŸ“‡ â˜ï¸ - The Globalping MCP server provides users and LLMs access to run network tools like ping, traceroute, mtr, HTTP and DNS resolve from thousands of locations around the world.\n- [kadykov/mcp-openapi-schema-explorer](https://github.com/kadykov/mcp-openapi-schema-explorer) ğŸ“‡ â˜ï¸ ğŸ  - Token-efficient access to OpenAPI/Swagger specs via MCP Resources.\n- [lamemind/mcp-server-multiverse](https://github.com/lamemind/mcp-server-multiverse) ğŸ“‡ ğŸ  ğŸ› ï¸ - A middleware server that enables multiple isolated instances of the same MCP servers to coexist independently with unique namespaces and configurations.\n- [langfuse/mcp-server-langfuse](https://github.com/langfuse/mcp-server-langfuse) ğŸ ğŸ  - MCP server to access and manage LLM application prompts created with [Langfuse]([https://langfuse.com/](https://langfuse.com/docs/prompts/get-started)) Prompt Management.\n- [JamesANZ/system-prompts-mcp-server](https://github.com/JamesANZ/system-prompts-mcp-server) ğŸ“‡ ğŸ  ğŸ ğŸªŸ ğŸ§ - Exposes a large catalog of coding assistant prompts as MCP tools with model-aware suggestions and persona activation to emulate agents like Cursor or Devin.\n- [linw1995/nvim-mcp](https://github.com/linw1995/nvim-mcp) ğŸ¦€ ğŸ  ğŸ ğŸªŸ ğŸ§  -  A MCP server to interact with Neovim\n- [lpigeon/ros-mcp-server](https://github.com/lpig', '{"language":null,"stars":76279,"forks":6457,"watchers":76279,"open_issues":127,"topics":["ai","mcp"],"default_branch":"main","size_kb":5343,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:punkpeye:awesome-mcp-clients","source_url":"https://github.com/punkpeye/awesome-mcp-clients"},{"type":"has_code","target_id":"github:1mcp-app:agent","source_url":"https://github.com/1mcp-app/agent"},{"type":"has_code","target_id":"github:askbudi:roundtable","source_url":"https://github.com/askbudi/roundtable"},{"type":"has_code","target_id":"github:duaraghav8:MCPJungle","source_url":"https://github.com/duaraghav8/MCPJungle"},{"type":"has_code","target_id":"github:glenngillen:mcpmcp-server","source_url":"https://github.com/glenngillen/mcpmcp-server"},{"type":"has_code","target_id":"github:hamflx:imagen3-mcp","source_url":"https://github.com/hamflx/imagen3-mcp"},{"type":"has_code","target_id":"github:julien040:anyquery","source_url":"https://github.com/julien040/anyquery"},{"type":"has_code","target_id":"github:juspay:neurolink","source_url":"https://github.com/juspay/neurolink"},{"type":"has_code","target_id":"github:K-Dense-AI:claude-skills-mcp","source_url":"https://github.com/K-Dense-AI/claude-skills-mcp"},{"type":"has_code","target_id":"github:metatool-ai:metatool-app","source_url":"https://github.com/metatool-ai/metatool-app"},{"type":"has_code","target_id":"github:mindsdb:mindsdb","source_url":"https://github.com/mindsdb/mindsdb"},{"type":"has_code","target_id":"github:portel-dev:ncp","source_url":"https://github.com/portel-dev/ncp"},{"type":"has_code","target_id":"github:particlefuture:MCPDiscovery","source_url":"https://github.com/particlefuture/MCPDiscovery"},{"type":"has_code","target_id":"github:PipedreamHQ:pipedream","source_url":"https://github.com/PipedreamHQ/pipedream"},{"type":"has_code","target_id":"github:sitbon:magg","source_url":"https://github.com/sitbon/magg"},{"type":"has_code","target_id":"github:thinkchainai:mcpbundles","source_url":"https://github.com/thinkchainai/mcpbundles"},{"type":"has_code","target_id":"github:SureScaleAI:openai-gpt-image-mcp","source_url":"https://github.com/SureScaleAI/openai-gpt-image-mcp"},{"type":"has_code","target_id":"github:sxhxliang:mcp-access-point","source_url":"https://github.com/sxhxliang/mcp-access-point"},{"type":"has_code","target_id":"github:TheLunarCompany:lunar","source_url":"https://github.com/TheLunarCompany/lunar"},{"type":"has_code","target_id":"github:tigranbs:mcgravity","source_url":"https://github.com/tigranbs/mcgravity"},{"type":"has_code","target_id":"github:VeriTeknik:pluggedin-mcp-proxy","source_url":"https://github.com/VeriTeknik/pluggedin-mcp-proxy"},{"type":"has_code","target_id":"github:waystation-ai:mcp","source_url":"https://github.com/waystation-ai/mcp"},{"type":"has_code","target_id":"github:wegotdocs:open-mcp","source_url":"https://github.com/wegotdocs/open-mcp"},{"type":"has_code","target_id":"github:Data-Everything:mcp-server-templates","source_url":"https://github.com/Data-Everything/mcp-server-templates"},{"type":"has_code","target_id":"github:YangLiangwei:PersonalizationMCP","source_url":"https://github.com/YangLiangwei/PersonalizationMCP"},{"type":"has_code","target_id":"github:IO-Aerospace-software-engineering:mcp-server","source_url":"https://github.com/IO-Aerospace-software-engineering/mcp-server"},{"type":"has_code","target_id":"github:drakonkat:wizzy-mcp-tmdb","source_url":"https://github.com/drakonkat/wizzy-mcp-tmdb"},{"type":"has_code","target_id":"github:8enSmith:mcp-open-library","source_url":"https://github.com/8enSmith/mcp-open-library"},{"type":"has_code","target_id":"github:abhiemj:manim-mcp-server","source_url":"https://github.com/abhiemj/manim-mcp-server"},{"type":"has_code","target_id":"github:ahujasid:blender-mcp","source_url":"https://github.com/ahujasid/blender-mcp"},{"type":"has_code","target_id":"github:burningion:video-editing-mcp","source_url":"https://github.com/burningion/video-editing-mcp"},{"type":"has_code","target_id":"github:cantian-ai:bazi-mcp","source_url":"https://github.com/cantian-ai/bazi-mcp"},{"type":"has_code","target_id":"github:cswkim:discogs-mcp-server","source_url":"https://github.com/cswkim/discogs-mcp-server"},{"type":"has_code","target_id":"github:diivi:aseprite-mcp","source_url":"https://github.com/diivi/aseprite-mcp"},{"type":"has_code","target_id":"github:djalal:quran-mcp-server","source_url":"https://github.com/djalal/quran-mcp-server"},{"type":"has_code","target_id":"github:raveenb:fal-mcp-server","source_url":"https://github.com/raveenb/fal-mcp-server"},{"type":"has_code","target_id":"github:GenWaveLLC:svgmaker-mcp","source_url":"https://github.com/GenWaveLLC/svgmaker-mcp"},{"type":"has_code","target_id":"github:mikechao:metmuseum-mcp","source_url":"https://github.com/mikechao/metmuseum-mcp"},{"type":"has_code","target_id":"github:molanojustin:smithsonian-mcp","source_url":"https://github.com/molanojustin/smithsonian-mcp"},{"type":"has_code","target_id":"github:OctoEverywhere:mcp","source_url":"https://github.com/OctoEverywhere/mcp"},{"type":"has_code","target_id":"github:omni-mcp:isaac-sim-mcp","source_url":"https://github.com/omni-mcp/isaac-sim-mcp"},{"type":"has_code","target_id":"github:PatrickPalmer:MayaMCP","source_url":"https://github.com/PatrickPalmer/MayaMCP"},{"type":"has_code","target_id":"github:peek-travel:mcp-intro","source_url":"https://github.com/peek-travel/mcp-intro"},{"type":"has_code","target_id":"github:r-huijts:oorlogsbronnen-mcp","source_url":"https://github.com/r-huijts/oorlogsbronnen-mcp"},{"type":"has_code","target_id":"github:r-huijts:rijksmuseum-mcp","source_url":"https://github.com/r-huijts/rijksmuseum-mcp"},{"type":"has_code","target_id":"github:samuelgursky:davinci-resolve-mcp","source_url":"https://github.com/samuelgursky/davinci-resolve-mcp"},{"type":"has_code","target_id":"github:yuna0x0:anilist-mcp","source_url":"https://github.com/yuna0x0/anilist-mcp"},{"type":"has_code","target_id":"github:Narasimhaponnada:mermaid-mcp","source_url":"https://github.com/Narasimhaponnada/mermaid-mcp"},{"type":"has_code","target_id":"github:dnaerys:onekgp-mcp","source_url":"https://github.com/dnaerys/onekgp-mcp"},{"type":"has_code","target_id":"github:genomoncology:biomcp","source_url":"https://github.com/genomoncology/biomcp"},{"type":"has_code","target_id":"github:hlydecker:ucsc-genome-mcp","source_url":"https://github.com/hlydecker/ucsc-genome-mcp"},{"type":"has_code","target_id":"github:longevity-genie:biothings-mcp","source_url":"https://github.com/longevity-genie/biothings-mcp"},{"type":"has_code","target_id":"github:longevity-genie:gget-mcp","source_url":"https://github.com/longevity-genie/gget-mcp"},{"type":"has_code","target_id":"github:longevity-genie:opengenes-mcp","source_url":"https://github.com/longevity-genie/opengenes-mcp"},{"type":"has_code","target_id":"github:longevity-genie:synergy-age-mcp","source_url":"https://github.com/longevity-genie/synergy-age-mcp"},{"type":"has_code","target_id":"github:the-momentum:fhir-mcp-server","source_url":"https://github.com/the-momentum/fhir-mcp-server"},{"type":"has_code","target_id":"github:wso2:fhir-mcp-server","source_url":"https://github.com/wso2/fhir-mcp-server"},{"type":"has_code","target_id":"github:JamesANZ:medical-mcp","source_url":"https://github.com/JamesANZ/medical-mcp"},{"type":"has_code","target_id":"github:the-momentum:apple-health-mcp-server","source_url":"https://github.com/the-momentum/apple-health-mcp-server"},{"type":"has_code","target_id":"github:OHNLP:omop_mcp","source_url":"https://github.com/OHNLP/omop_mcp"},{"type":"has_code","target_id":"github:BB-fat:browser-use-rs","source_url":"https://github.com/BB-fat/browser-use-rs"},{"type":"has_code","target_id":"github:34892002:bilibili-mcp-js","source_url":"https://github.com/34892002/bilibili-mcp-js"},{"type":"has_code","target_id":"github:bytedance:UI-TARS-desktop","source_url":"https://github.com/bytedance/UI-TARS-desktop"},{"type":"has_code","target_id":"github:Automata-Labs-team:MCP-Server-Playwright","source_url":"https://github.com/Automata-Labs-team/MCP-Server-Playwright"},{"type":"has_code","target_id":"github:blackwhite084:playwright-plus-python-mcp","source_url":"https://github.com/blackwhite084/playwright-plus-python-mcp"},{"type":"has_code","target_id":"github:browserbase:mcp-server-browserbase","source_url":"https://github.com/browserbase/mcp-server-browserbase"},{"type":"has_code","target_id":"github:browsermcp:mcp","source_url":"https://github.com/browsermcp/mcp"},{"type":"has_code","target_id":"github:co-browser:browser-use-mcp-server","source_url":"https://github.com/co-browser/browser-use-mcp-server"},{"type":"has_code","target_id":"github:eat-pray-ai:yutu","source_url":"https://github.com/eat-pray-ai/yutu"},{"type":"has_code","target_id":"github:executeautomation:mcp-playwright","source_url":"https://github.com/executeautomation/mcp-playwright"},{"type":"has_code","target_id":"github:eyalzh:browser-control-mcp","source_url":"https://github.com/eyalzh/browser-control-mcp"},{"type":"has_code","target_id":"github:freema:firefox-devtools-mcp","source_url":"https://github.com/freema/firefox-devtools-mcp"},{"type":"has_code","target_id":"github:FradSer:mcp-server-apple-reminders","source_url":"https://github.com/FradSer/mcp-server-apple-reminders"},{"type":"has_code","target_id":"github:getrupt:ashra-mcp","source_url":"https://github.com/getrupt/ashra-mcp"},{"type":"has_code","target_id":"github:kimtaeyoon83:mcp-server-youtube-transcript","source_url":"https://github.com/kimtaeyoon83/mcp-server-youtube-transcript"},{"type":"has_code","target_id":"github:kimtth:mcp-aoai-web-browsing","source_url":"https://github.com/kimtth/mcp-aoai-web-browsing"},{"type":"has_code","target_id":"github:lightpanda-io:gomcp","source_url":"https://github.com/lightpanda-io/gomcp"},{"type":"has_code","target_id":"github:microsoft:playwright-mcp","source_url":"https://github.com/microsoft/playwright-mcp"},{"type":"has_code","target_id":"github:modelcontextprotocol:servers-archived","source_url":"https://github.com/modelcontextprotocol/servers-archived"},{"type":"has_code","target_id":"github:ndthanhdev:mcp-browser-kit","source_url":"https://github.com/ndthanhdev/mcp-browser-kit"},{"type":"has_code","target_id":"github:Operative-Sh:web-eval-agent","source_url":"https://github.com/Operative-Sh/web-eval-agent"},{"type":"has_code","target_id":"github:olostep:olostep-mcp-server","source_url":"https://github.com/olostep/olostep-mcp-server"},{"type":"has_code","target_id":"github:pskill9:web-search","source_url":"https://github.com/pskill9/web-search"},{"type":"has_code","target_id":"github:PhungXuanAnh:selenium-mcp-server","source_url":"https://github.com/PhungXuanAnh/selenium-mcp-server"},{"type":"has_code","target_id":"github:recursechat:mcp-server-apple-shortcuts","source_url":"https://github.com/recursechat/mcp-server-apple-shortcuts"},{"type":"has_code","target_id":"github:xspadex:bilibili-mcp.git","source_url":"https://github.com/xspadex/bilibili-mcp.git"},{"type":"has_code","target_id":"github:imprvhub:mcp-browser-agent","source_url":"https://github.com/imprvhub/mcp-browser-agent"},{"type":"has_code","target_id":"github:4everland:4everland-hosting-mcp","source_url":"https://github.com/4everland/4everland-hosting-mcp"},{"type":"has_code","target_id":"github:aashari:mcp-server-aws-sso","source_url":"https://github.com/aashari/mcp-server-aws-sso"},{"type":"has_code","target_id":"github:alexbakers:mcp-ipfs","source_url":"https://github.com/alexbakers/mcp-ipfs"},{"type":"has_code","target_id":"github:alexei-led:aws-mcp-server","source_url":"https://github.com/alexei-led/aws-mcp-server"},{"type":"has_code","target_id":"github:alexei-led:k8s-mcp-server","source_url":"https://github.com/alexei-led/k8s-mcp-server"},{"type":"has_code","target_id":"github:aliyun:alibaba-cloud-ops-mcp-server","source_url":"https://github.com/aliyun/alibaba-cloud-ops-mcp-server"},{"type":"has_code","target_id":"github:awslabs:mcp","source_url":"https://github.com/awslabs/mcp"},{"type":"has_code","target_id":"github:localstack:localstack-mcp-server","source_url":"https://github.com/localstack/localstack-mcp-server"},{"type":"has_code","target_id":"github:bright8192:esxi-mcp-server","source_url":"https://github.com/bright8192/esxi-mcp-server"},{"type":"has_code","target_id":"github:cloudflare:mcp-server-cloudflare","source_url":"https://github.com/cloudflare/mcp-server-cloudflare"},{"type":"has_code","target_id":"github:cyclops-ui:mcp-cyclops","source_url":"https://github.com/cyclops-ui/mcp-cyclops"},{"type":"has_code","target_id":"github:erikhoward:adls-mcp-server","source_url":"https://github.com/erikhoward/adls-mcp-server"},{"type":"has_code","target_id":"github:espressif:esp-rainmaker-mcp","source_url":"https://github.com/espressif/esp-rainmaker-mcp"},{"type":"has_code","target_id":"github:Flux159:mcp-server-kubernetes","source_url":"https://github.com/Flux159/mcp-server-kubernetes"},{"type":"has_code","target_id":"github:hardik-id:azure-resource-graph-mcp-server","source_url":"https://github.com/hardik-id/azure-resource-graph-mcp-server"},{"type":"has_code","target_id":"github:jdubois:azure-cli-mcp","source_url":"https://github.com/jdubois/azure-cli-mcp"},{"type":"has_code","target_id":"github:johnneerdael:netskope-mcp","source_url":"https://github.com/johnneerdael/netskope-mcp"},{"type":"has_code","target_id":"github:kestra-io:mcp-server-python","source_url":"https://github.com/kestra-io/mcp-server-python"},{"type":"has_code","target_id":"github:liveblocks:liveblocks-mcp-server","source_url":"https://github.com/liveblocks/liveblocks-mcp-server"},{"type":"has_code","target_id":"github:manusa:kubernetes-mcp-server","source_url":"https://github.com/manusa/kubernetes-mcp-server"},{"type":"has_code","target_id":"github:Nebula-Block-Data:nebulablock-mcp-server","source_url":"https://github.com/Nebula-Block-Data/nebulablock-mcp-server"},{"type":"has_code","target_id":"github:nwiizo:tfmcp","source_url":"https://github.com/nwiizo/tfmcp"},{"type":"has_code","target_id":"github:openstack-kr:python-openstackmcp-server","source_url":"https://github.com/openstack-kr/python-openstackmcp-server"},{"type":"has_code","target_id":"github:pibblokto:cert-manager-mcp-server","source_url":"https://github.com/pibblokto/cert-manager-mcp-server"},{"type":"has_code","target_id":"github:cert-manager:cert-manager","source_url":"https://github.com/cert-manager/cert-manager"},{"type":"has_code","target_id":"github:portainer:portainer-mcp","source_url":"https://github.com/portainer/portainer-mcp"},{"type":"has_code","target_id":"github:pulumi:mcp-server","source_url":"https://github.com/pulumi/mcp-server"},{"type":"has_code","target_id":"github:pythonanywhere:pythonanywhere-mcp-server","source_url":"https://github.com/pythonanywhere/pythonanywhere-mcp-server"},{"type":"has_code","target_id":"github:qiniu:qiniu-mcp-server","source_url":"https://github.com/qiniu/qiniu-mcp-server"},{"type":"has_code","target_id":"github:redis:mcp-redis-cloud","source_url":"https://github.com/redis/mcp-redis-cloud"},{"type":"has_code","target_id":"github:reza-gholizade:k8s-mcp-server","source_url":"https://github.com/reza-gholizade/k8s-mcp-server"},{"type":"has_code","target_id":"github:rohitg00:kubectl-mcp-server","source_url":"https://github.com/rohitg00/kubectl-mcp-server"},{"type":"has_code","target_id":"github:rrmistry:tilt-mcp","source_url":"https://github.com/rrmistry/tilt-mcp"},{"type":"has_code","target_id":"github:silenceper:mcp-k8s","source_url":"https://github.com/silenceper/mcp-k8s"},{"type":"has_code","target_id":"github:StacklokLabs:mkp","source_url":"https://github.com/StacklokLabs/mkp"},{"type":"has_code","target_id":"github:StacklokLabs:ocireg-mcp","source_url":"https://github.com/StacklokLabs/ocireg-mcp"},{"type":"has_code","target_id":"github:strowk:mcp-k8s-go","source_url":"https://github.com/strowk/mcp-k8s-go"},{"type":"has_code","target_id":"github:thunderboltsid:mcp-nutanix","source_url":"https://github.com/thunderboltsid/mcp-nutanix"},{"type":"has_code","target_id":"github:trilogy-group:aws-pricing-mcp","source_url":"https://github.com/trilogy-group/aws-pricing-mcp"},{"type":"has_code","target_id":"github:VmLia:books-mcp-server","source_url":"https://github.com/VmLia/books-mcp-server"},{"type":"has_code","target_id":"github:weibaohui:k8m","source_url":"https://github.com/weibaohui/k8m"},{"type":"has_code","target_id":"github:weibaohui:kom","source_url":"https://github.com/weibaohui/kom"},{"type":"has_code","target_id":"github:wenhuwang:mcp-k8s-eye","source_url":"https://github.com/wenhuwang/mcp-k8s-eye"},{"type":"has_code","target_id":"github:elevy99927:devops-mcp-webui","source_url":"https://github.com/elevy99927/devops-mcp-webui"},{"type":"has_code","target_id":"github:alfonsograziano:node-code-sandbox-mcp","source_url":"https://github.com/alfonsograziano/node-code-sandbox-mcp"},{"type":"has_code","target_id":"github:ckanthony:openapi-mcp","source_url":"https://github.com/ckanthony/openapi-mcp"},{"type":"has_code","target_id":"github:gwbischof:outsource-mcp","source_url":"https://github.com/gwbischof/outsource-mcp"},{"type":"has_code","target_id":"github:hileamlakB:PRIMS","source_url":"https://github.com/hileamlakB/PRIMS"},{"type":"has_code","target_id":"github:ouvreboite:openapi-to-mcp","source_url":"https://github.com/ouvreboite/openapi-to-mcp"},{"type":"has_code","target_id":"github:pydantic:pydantic-ai","source_url":"https://github.com/pydantic/pydantic-ai"},{"type":"has_code","target_id":"github:r33drichards:mcp-js","source_url":"https://github.com/r33drichards/mcp-js"},{"type":"has_code","target_id":"github:yepcode:mcp-server-js","source_url":"https://github.com/yepcode/mcp-server-js"},{"type":"has_code","target_id":"github:dagger:container-use","source_url":"https://github.com/dagger/container-use"},{"type":"has_code","target_id":"github:Shashankss1205:CodeGraphContext","source_url":"https://github.com/Shashankss1205/CodeGraphContext"},{"type":"has_code","target_id":"github:doggybee:mcp-server-leetcode","source_url":"https://github.com/doggybee/mcp-server-leetcode"},{"type":"has_code","target_id":"github:ezyang:codemcp","source_url":"https://github.com/ezyang/codemcp"},{"type":"has_code","target_id":"github:gabrielmaialva33:winx-code-agent","source_url":"https://github.com/gabrielmaialva33/winx-code-agent"},{"type":"has_code","target_id":"github:jinzcdev:leetcode-mcp-server","source_url":"https://github.com/jinzcdev/leetcode-mcp-server"},{"type":"has_code","target_id":"github:juehang:vscode-mcp-server","source_url":"https://github.com/juehang/vscode-mcp-server"},{"type":"has_code","target_id":"github:micl2e2:code-to-tree","source_url":"https://github.com/micl2e2/code-to-tree"},{"type":"has_code","target_id":"github:oraios:serena","source_url":"https://github.com/oraios/serena"},{"type":"has_code","target_id":"github:rinadelph:Agent-MCP","source_url":"https://github.com/rinadelph/Agent-MCP"},{"type":"has_code","target_id":"github:Sim-xia:Blind-Auditor","source_url":"https://github.com/Sim-xia/Blind-Auditor"},{"type":"has_code","target_id":"github:stippi:code-assistant","source_url":"https://github.com/stippi/code-assistant"},{"type":"has_code","target_id":"github:tiianhk:MaxMSP-MCP-Server","source_url":"https://github.com/tiianhk/MaxMSP-MCP-Server"},{"type":"has_code","target_id":"github:nesquikm:mcp-rubber-duck","source_url":"https://github.com/nesquikm/mcp-rubber-duck"},{"type":"has_code","target_id":"github:askbudi:roundtable","source_url":"https://github.com/askbudi/roundtable"},{"type":"has_code","target_id":"github:VertexStudio:developer","source_url":"https://github.com/VertexStudio/developer"},{"type":"has_code","target_id":"github:x51xxx:codex-mcp-tool","source_url":"https://github.com/x51xxx/codex-mcp-tool"},{"type":"has_code","target_id":"github:x51xxx:copilot-mcp-server","source_url":"https://github.com/x51xxx/copilot-mcp-server"},{"type":"has_code","target_id":"github:wende:cicada","source_url":"https://github.com/wende/cicada"},{"type":"has_code","target_id":"github:automateyournetwork:pyATS_MCP","source_url":"https://github.com/automateyournetwork/pyATS_MCP"},{"type":"has_code","target_id":"github:aymericzip:intlayer","source_url":"https://github.com/aymericzip/intlayer"},{"type":"has_code","target_id":"github:blakerouse:ssh-mcp","source_url":"https://github.com/blakerouse/ssh-mcp"},{"type":"has_code","target_id":"github:ferrislucas:iterm-mcp","source_url":"https://github.com/ferrislucas/iterm-mcp"},{"type":"has_code","target_id":"github:g0t4:mcp-server-commands","source_url":"https://github.com/g0t4/mcp-server-commands"},{"type":"has_code","target_id":"github:maxim-saplin:mcp_safe_local_python_executor","source_url":"https://github.com/maxim-saplin/mcp_safe_local_python_executor"},{"type":"has_code","target_id":"github:misiektoja:kill-process-mcp","source_url":"https://github.com/misiektoja/kill-process-mcp"},{"type":"has_code","target_id":"github:MladenSU:cli-mcp-server","source_url":"https://github.com/MladenSU/cli-mcp-server"},{"type":"has_code","target_id":"github:OthmaneBlial:term_mcp_deepseek","source_url":"https://github.com/OthmaneBlial/term_mcp_deepseek"},{"type":"has_code","target_id":"github:ooples:mcp-console-automation","source_url":"https://github.com/ooples/mcp-console-automation"},{"type":"has_code","target_id":"github:sonirico:mcp-shell","source_url":"https://github.com/sonirico/mcp-shell"},{"type":"has_code","target_id":"github:mediar-ai:terminator","source_url":"https://github.com/mediar-ai/terminator"},{"type":"has_code","target_id":"github:tufantunc:ssh-mcp","source_url":"https://github.com/tufantunc/ssh-mcp"},{"type":"has_code","target_id":"github:tumf:mcp-shell-server","source_url":"https://github.com/tumf/mcp-shell-server"},{"type":"has_code","target_id":"github:wonderwhy-er:DesktopCommanderMCP","source_url":"https://github.com/wonderwhy-er/DesktopCommanderMCP"},{"type":"has_code","target_id":"github:nihalxkumar:arch-mcp","source_url":"https://github.com/nihalxkumar/arch-mcp"},{"type":"has_code","target_id":"github:AbdelStark:nostr-mcp","source_url":"https://github.com/AbdelStark/nostr-mcp"},{"type":"has_code","target_id":"github:adhikasp:mcp-twikit","source_url":"https://github.com/adhikasp/mcp-twikit"},{"type":"has_code","target_id":"github:agentmail-to:agentmail-toolkit","source_url":"https://github.com/agentmail-to/agentmail-toolkit"},{"type":"has_code","target_id":"github:areweai:tsgram-mcp","source_url":"https://github.com/areweai/tsgram-mcp"},{"type":"has_code","target_id":"github:arpitbatra123:mcp-googletasks","source_url":"https://github.com/arpitbatra123/mcp-googletasks"},{"type":"has_code","target_id":"github:Cactusinhand:mcp_server_notify","source_url":"https://github.com/Cactusinhand/mcp_server_notify"},{"type":"has_code","target_id":"github:trycourier:courier-mcp","source_url":"https://github.com/trycourier/courier-mcp"},{"type":"has_code","target_id":"github:PhononX:cv-mcp-server","source_url":"https://github.com/PhononX/cv-mcp-server"},{"type":"has_code","target_id":"github:carterlasalle:mac_messages_mcp","source_url":"https://github.com/carterlasalle/mac_messages_mcp"},{"type":"has_code","target_id":"github:chaindead:telegram-mcp","source_url":"https://github.com/chaindead/telegram-mcp"},{"type":"has_code","target_id":"github:chigwell:telegram-mcp","source_url":"https://github.com/chigwell/telegram-mcp"},{"type":"has_code","target_id":"github:Danielpeter-99:calcom-mcp","source_url":"https://github.com/Danielpeter-99/calcom-mcp"},{"type":"has_code","target_id":"github:discourse:discourse-mcp","source_url":"https://github.com/discourse/discourse-mcp"},{"type":"has_code","target_id":"github:elie222:inbox-zero","source_url":"https://github.com/elie222/inbox-zero"},{"type":"has_code","target_id":"github:gerkensm:callcenter.js-mcp","source_url":"https://github.com/gerkensm/callcenter.js-mcp"},{"type":"has_code","target_id":"github:gitmotion:ntfy-me-mcp","source_url":"https://github.com/gitmotion/ntfy-me-mcp"},{"type":"has_code","target_id":"github:gotoolkits:mcp-wecombot-server.git","source_url":"https://github.com/gotoolkits/mcp-wecombot-server.git"},{"type":"has_code","target_id":"github:hannesrudolph:imessage-query-fastmcp-mcp-server","source_url":"https://github.com/hannesrudolph/imessage-query-fastmcp-mcp-server"},{"type":"has_code","target_id":"github:i-am-bee:acp-mcp","source_url":"https://github.com/i-am-bee/acp-mcp"},{"type":"has_code","target_id":"github:InditexTech:mcp-teams-server","source_url":"https://github.com/InditexTech/mcp-teams-server"},{"type":"has_code","target_id":"github:infobip:mcp","source_url":"https://github.com/infobip/mcp"},{"type":"has_code","target_id":"github:jagan-shanmugam:mattermost-mcp-host","source_url":"https://github.com/jagan-shanmugam/mattermost-mcp-host"},{"type":"has_code","target_id":"github:jaipandya:producthunt-mcp-server","source_url":"https://github.com/jaipandya/producthunt-mcp-server"},{"type":"has_code","target_id":"github:joinly-ai:joinly","source_url":"https://github.com/joinly-ai/joinly"},{"type":"has_code","target_id":"github:keturiosakys:bluesky-context-server","source_url":"https://github.com/keturiosakys/bluesky-context-server"},{"type":"has_code","target_id":"github:khan2a:telephony-mcp-server","source_url":"https://github.com/khan2a/telephony-mcp-server"},{"type":"has_code","target_id":"github:korotovsky:slack-mcp-server","source_url":"https://github.com/korotovsky/slack-mcp-server"},{"type":"has_code","target_id":"github:lharries:whatsapp-mcp","source_url":"https://github.com/lharries/whatsapp-mcp"},{"type":"has_code","target_id":"github:line:line-bot-mcp-server","source_url":"https://github.com/line/line-bot-mcp-server"},{"type":"has_code","target_id":"github:madbonez:caldav-mcp","source_url":"https://github.com/madbonez/caldav-mcp"},{"type":"has_code","target_id":"github:OverQuotaAI:chatterboxio-mcp-server","source_url":"https://github.com/OverQuotaAI/chatterboxio-mcp-server"},{"type":"has_code","target_id":"github:wyattjoh:imessage-mcp","source_url":"https://github.com/wyattjoh/imessage-mcp"},{"type":"has_code","target_id":"github:sawa-zen:vrchat-mcp","source_url":"https://github.com/sawa-zen/vrchat-mcp"},{"type":"has_code","target_id":"github:softeria:ms-365-mcp-server","source_url":"https://github.com/softeria/ms-365-mcp-server"},{"type":"has_code","target_id":"github:SaseQ:discord-mcp","source_url":"https://github.com/SaseQ/discord-mcp"},{"type":"has_code","target_id":"github:teddyzxcv:ntfy-mcp","source_url":"https://github.com/teddyzxcv/ntfy-mcp"},{"type":"has_code","target_id":"github:UserAd:didlogic_mcp","source_url":"https://github.com/UserAd/didlogic_mcp"},{"type":"has_code","target_id":"github:YCloud-Developers:ycloud-whatsapp-mcp-server","source_url":"https://github.com/YCloud-Developers/ycloud-whatsapp-mcp-server"},{"type":"has_code","target_id":"github:zcaceres:gtasks-mcp","source_url":"https://github.com/zcaceres/gtasks-mcp"},{"type":"has_code","target_id":"github:ztxtxwd:open-feishu-mcp-server","source_url":"https://github.com/ztxtxwd/open-feishu-mcp-server"},{"type":"has_code","target_id":"github:antvis:mcp-server-chart","source_url":"https://github.com/antvis/mcp-server-chart"},{"type":"has_code","target_id":"github:hustcc:mcp-echarts","source_url":"https://github.com/hustcc/mcp-echarts"},{"type":"has_code","target_id":"github:hustcc:mcp-mermaid","source_url":"https://github.com/hustcc/mcp-mermaid"},{"type":"has_code","target_id":"github:iaptic:mcp-server-iaptic","source_url":"https://github.com/iaptic/mcp-server-iaptic"},{"type":"has_code","target_id":"github:OpenDataMCP:OpenDataMCP","source_url":"https://github.com/OpenDataMCP/OpenDataMCP"},{"type":"has_code","target_id":"github:sergehuber:inoyu-mcp-unomi-server","source_url":"https://github.com/sergehuber/inoyu-mcp-unomi-server"},{"type":"has_code","target_id":"github:tinybirdco:mcp-tinybird","source_url":"https://github.com/tinybirdco/mcp-tinybird"},{"type":"has_code","target_id":"github:Aiven-Open:mcp-aiven","source_url":"https://github.com/Aiven-Open/mcp-aiven"},{"type":"has_code","target_id":"github:alexander-zuev:supabase-mcp-server","source_url":"https://github.com/alexander-zuev/supabase-mcp-server"},{"type":"has_code","target_id":"github:aliyun:alibabacloud-tablestore-mcp-server","source_url":"https://github.com/aliyun/alibabacloud-tablestore-mcp-server"},{"type":"has_code","target_id":"github:amineelkouhen:mcp-cockroachdb","source_url":"https://github.com/amineelkouhen/mcp-cockroachdb"},{"type":"has_code","target_id":"github:benborla:mcp-server-mysql","source_url":"https://github.com/benborla/mcp-server-mysql"},{"type":"has_code","target_id":"github:bram2w:baserow","source_url":"https://github.com/bram2w/baserow"},{"type":"has_code","target_id":"github:c4pt0r:mcp-server-tidb","source_url":"https://github.com/c4pt0r/mcp-server-tidb"},{"type":"has_code","target_id":"github:Canner:wren-engine","source_url":"https://github.com/Canner/wren-engine"},{"type":"has_code","target_id":"github:centralmind:gateway","source_url":"https://github.com/centralmind/gateway"},{"type":"has_code","target_id":"github:ChristianHinge:dicom-mcp","source_url":"https://github.com/ChristianHinge/dicom-mcp"},{"type":"has_code","target_id":"github:chroma-core:chroma-mcp","source_url":"https://github.com/chroma-core/chroma-mcp"},{"type":"has_code","target_id":"github:ClickHouse:mcp-clickhouse","source_url":"https://github.com/ClickHouse/mcp-clickhouse"},{"type":"has_code","target_id":"github:confluentinc:mcp-confluent","source_url":"https://github.com/confluentinc/mcp-confluent"},{"type":"has_code","target_id":"github:Couchbase-Ecosystem:mcp-server-couchbase","source_url":"https://github.com/Couchbase-Ecosystem/mcp-server-couchbase"},{"type":"has_code","target_id":"github:cr7258:elasticsearch-mcp-server","source_url":"https://github.com/cr7258/elasticsearch-mcp-server"},{"type":"has_code","target_id":"github:crystaldba:postgres-mcp","source_url":"https://github.com/crystaldba/postgres-mcp"},{"type":"has_code","target_id":"github:Dataring-engineering:mcp-server-trino","source_url":"https://github.com/Dataring-engineering/mcp-server-trino"},{"type":"has_code","target_id":"github:dave-wind:mysql-mcp-server","source_url":"https://github.com/dave-wind/mysql-mcp-server"},{"type":"has_code","target_id":"github:designcomputer:mysql_mcp_server","source_url":"https://github.com/designcomputer/mysql_mcp_server"},{"type":"has_code","target_id":"github:domdomegg:airtable-mcp-server","source_url":"https://github.com/domdomegg/airtable-mcp-server"},{"type":"has_code","target_id":"github:edwinbernadus:nocodb-mcp-server","source_url":"https://github.com/edwinbernadus/nocodb-mcp-server"},{"type":"has_code","target_id":"github:ergut:mcp-bigquery-server","source_url":"https://github.com/ergut/mcp-bigquery-server"},{"type":"has_code","target_id":"github:f4ww4z:mcp-mysql-server","source_url":"https://github.com/f4ww4z/mcp-mysql-server"},{"type":"has_code","target_id":"github:ferrants:memvid-mcp-server","source_url":"https://github.com/ferrants/memvid-mcp-server"},{"type":"has_code","target_id":"github:Olow304:memvid","source_url":"https://github.com/Olow304/memvid"},{"type":"has_code","target_id":"github:fireproof-storage:mcp-database-server","source_url":"https://github.com/fireproof-storage/mcp-database-server"},{"type":"has_code","target_id":"github:freema:mcp-gsheets","source_url":"https://github.com/freema/mcp-gsheets"},{"type":"has_code","target_id":"github:FreePeak:db-mcp-server","source_url":"https://github.com/FreePeak/db-mcp-server"},{"type":"has_code","target_id":"github:furey:mongodb-lens","source_url":"https://github.com/furey/mongodb-lens"},{"type":"has_code","target_id":"github:gannonh:firebase-mcp","source_url":"https://github.com/gannonh/firebase-mcp"},{"type":"has_code","target_id":"github:get-convex:convex-backend","source_url":"https://github.com/get-convex/convex-backend"},{"type":"has_code","target_id":"github:gigamori:mcp-run-sql-connectorx","source_url":"https://github.com/gigamori/mcp-run-sql-connectorx"},{"type":"has_code","target_id":"github:googleapis:genai-toolbox","source_url":"https://github.com/googleapis/genai-toolbox"},{"type":"has_code","target_id":"github:GreptimeTeam:greptimedb-mcp-server","source_url":"https://github.com/GreptimeTeam/greptimedb-mcp-server"},{"type":"has_code","target_id":"github:hannesrudolph:sqlite-explorer-fastmcp-mcp-server","source_url":"https://github.com/hannesrudolph/sqlite-explorer-fastmcp-mcp-server"},{"type":"has_code","target_id":"github:henilcalagiya:google-sheets-mcp","source_url":"https://github.com/henilcalagiya/google-sheets-mcp"},{"type":"has_code","target_id":"github:hydrolix:mcp-hydrolix","source_url":"https://github.com/hydrolix/mcp-hydrolix"},{"type":"has_code","target_id":"github:idoru:influxdb-mcp-server","source_url":"https://github.com/idoru/influxdb-mcp-server"},{"type":"has_code","target_id":"github:influxdata:influxdb3_mcp_server","source_url":"https://github.com/influxdata/influxdb3_mcp_server"},{"type":"has_code","target_id":"github:isaacwasserman:mcp-snowflake-server","source_url":"https://github.com/isaacwasserman/mcp-snowflake-server"},{"type":"has_code","target_id":"github:iunera:druid-mcp-server","source_url":"https://github.com/iunera/druid-mcp-server"},{"type":"has_code","target_id":"github:YannBrrd:simple_snowflake_mcp","source_url":"https://github.com/YannBrrd/simple_snowflake_mcp"},{"type":"has_code","target_id":"github:joshuarileydev:supabase","source_url":"https://github.com/joshuarileydev/supabase"},{"type":"has_code","target_id":"github:jovezhong:mcp-timeplus","source_url":"https://github.com/jovezhong/mcp-timeplus"},{"type":"has_code","target_id":"github:jparkerweb:mcp-sqlite","source_url":"https://github.com/jparkerweb/mcp-sqlite"},{"type":"has_code","target_id":"github:KashiwaByte:vikingdb-mcp-server","source_url":"https://github.com/KashiwaByte/vikingdb-mcp-server"},{"type":"has_code","target_id":"github:kiliczsh:mcp-mongo-server","source_url":"https://github.com/kiliczsh/mcp-mongo-server"},{"type":"has_code","target_id":"github:ktanaka101:mcp-server-duckdb","source_url":"https://github.com/ktanaka101/mcp-server-duckdb"},{"type":"has_code","target_id":"github:LucasHild:mcp-server-bigquery","source_url":"https://github.com/LucasHild/mcp-server-bigquery"},{"type":"has_code","target_id":"github:memgraph:ai-toolkit","source_url":"https://github.com/memgraph/ai-toolkit"},{"type":"has_code","target_id":"github:montumodi:mongodb-atlas-mcp-server","source_url":"https://github.com/montumodi/mongodb-atlas-mcp-server"},{"type":"has_code","target_id":"github:modelcontextprotocol:servers-archived","source_url":"https://github.com/modelcontextprotocol/servers-archived"},{"type":"has_code","target_id":"github:modelcontextprotocol:servers-archived","source_url":"https://github.com/modelcontextprotocol/servers-archived"},{"type":"has_code","target_id":"github:neo4j-contrib:mcp-neo4j","source_url":"https://github.com/neo4j-contrib/mcp-neo4j"},{"type":"has_code","target_id":"github:neondatabase:mcp-server-neon","source_url":"https://github.com/neondatabase/mcp-server-neon"},{"type":"has_code","target_id":"github:niledatabase:nile-mcp-server","source_url":"https://github.com/niledatabase/nile-mcp-server"},{"type":"has_code","target_id":"github:OpenLinkSoftware:mcp-jdbc-server","source_url":"https://github.com/OpenLinkSoftware/mcp-jdbc-server"},{"type":"has_code","target_id":"github:OpenLinkSoftware:mcp-odbc-server","source_url":"https://github.com/OpenLinkSoftware/mcp-odbc-server"},{"type":"has_code","target_id":"github:OpenLinkSoftware:mcp-sqlalchemy-server","source_url":"https://github.com/OpenLinkSoftware/mcp-sqlalchemy-server"},{"type":"has_code","target_id":"github:pab1it0:adx-mcp-server","source_url":"https://github.com/pab1it0/adx-mcp-server"},{"type":"has_code","target_id":"github:pab1it0:prometheus-mcp-server","source_url":"https://github.com/pab1it0/prometheus-mcp-server"},{"type":"has_code","target_id":"github:prisma:mcp","source_url":"https://github.com/prisma/mcp"},{"type":"has_code","target_id":"github:qdrant:mcp-server-qdrant","source_url":"https://github.com/qdrant/mcp-server-qdrant"},{"type":"has_code","target_id":"github:QuantGeekDev:mongo-mcp","source_url":"https://github.com/QuantGeekDev/mongo-mcp"},{"type":"has_code","target_id":"github:quarkiverse:quarkus-mcp-servers","source_url":"https://github.com/quarkiverse/quarkus-mcp-servers"},{"type":"has_code","target_id":"github:rashidazarang:airtable-mcp","source_url":"https://github.com/rashidazarang/airtable-mcp"},{"type":"has_code","target_id":"github:redis:mcp-redis","source_url":"https://github.com/redis/mcp-redis"},{"type":"has_code","target_id":"github:runekaagaard:mcp-alchemy","source_url":"https://github.com/runekaagaard/mcp-alchemy"},{"type":"has_code","target_id":"github:wenerme:wode","source_url":"https://github.com/wenerme/wode"},{"type":"has_code","target_id":"github:s2-streamstore:s2-sdk-typescript","source_url":"https://github.com/s2-streamstore/s2-sdk-typescript"},{"type":"has_code","target_id":"github:schemacrawler:SchemaCrawler-MCP-Server-Usage","source_url":"https://github.com/schemacrawler/SchemaCrawler-MCP-Server-Usage"},{"type":"has_code","target_id":"github:sirmews:mcp-pinecone","source_url":"https://github.com/sirmews/mcp-pinecone"},{"type":"has_code","target_id":"github:skysqlinc:skysql-mcp","source_url":"https://github.com/skysqlinc/skysql-mcp"},{"type":"has_code","target_id":"github:Snowflake-Labs:mcp","source_url":"https://github.com/Snowflake-Labs/mcp"},{"type":"has_code","target_id":"github:subnetmarco:pgmcp","source_url":"https://github.com/subnetmarco/pgmcp"},{"type":"has_code","target_id":"github:isdaniel:pgtuner_mcp","source_url":"https://github.com/isdaniel/pgtuner_mcp"},{"type":"has_code","target_id":"github:supabase-community:supabase-mcp","source_url":"https://github.com/supabase-community/supabase-mcp"},{"type":"has_code","target_id":"github:TheRaLabs:legion-mcp","source_url":"https://github.com/TheRaLabs/legion-mcp"},{"type":"has_code","target_id":"github:tradercjz:dolphindb-mcp-server","source_url":"https://github.com/tradercjz/dolphindb-mcp-server"},{"type":"has_code","target_id":"github:tuannvm:mcp-trino","source_url":"https://github.com/tuannvm/mcp-trino"},{"type":"has_code","target_id":"github:VictoriaMetrics-Community:mcp-victorialogs","source_url":"https://github.com/VictoriaMetrics-Community/mcp-victorialogs"},{"type":"has_code","target_id":"github:weaviate:mcp-server-weaviate","source_url":"https://github.com/weaviate/mcp-server-weaviate"},{"type":"has_code","target_id":"github:wenb1n-dev:mysql_mcp_server_pro","source_url":"https://github.com/wenb1n-dev/mysql_mcp_server_pro"},{"type":"has_code","target_id":"github:wenb1n-dev:SmartDB_MCP","source_url":"https://github.com/wenb1n-dev/SmartDB_MCP"},{"type":"has_code","target_id":"github:Xexr:mcp-libsql","source_url":"https://github.com/Xexr/mcp-libsql"},{"type":"has_code","target_id":"github:XGenerationLab:xiyan_mcp_server","source_url":"https://github.com/XGenerationLab/xiyan_mcp_server"},{"type":"has_code","target_id":"github:xing5:mcp-google-sheets","source_url":"https://github.com/xing5/mcp-google-sheets"},{"type":"has_code","target_id":"github:ydb-platform:ydb-mcp","source_url":"https://github.com/ydb-platform/ydb-mcp"},{"type":"has_code","target_id":"github:yincongcyincong:VictoriaMetrics-mcp-server","source_url":"https://github.com/yincongcyincong/VictoriaMetrics-mcp-server"},{"type":"has_code","target_id":"github:Zhwt:go-mcp-mysql","source_url":"https://github.com/Zhwt/go-mcp-mysql"},{"type":"has_code","target_id":"github:zilliztech:mcp-server-milvus","source_url":"https://github.com/zilliztech/mcp-server-milvus"},{"type":"has_code","target_id":"github:aywengo:kafka-schema-reg-mcp","source_url":"https://github.com/aywengo/kafka-schema-reg-mcp"},{"type":"has_code","target_id":"github:dbt-labs:dbt-mcp","source_url":"https://github.com/dbt-labs/dbt-mcp"},{"type":"has_code","target_id":"github:flowcore-io:mcp-flowcore-platform","source_url":"https://github.com/flowcore-io/mcp-flowcore-platform"},{"type":"has_code","target_id":"github:JordiNeil:mcp-databricks-server","source_url":"https://github.com/JordiNeil/mcp-databricks-server"},{"type":"has_code","target_id":"github:jwaxman19:qlik-mcp","source_url":"https://github.com/jwaxman19/qlik-mcp"},{"type":"has_code","target_id":"github:keboola:keboola-mcp-server","source_url":"https://github.com/keboola/keboola-mcp-server"},{"type":"has_code","target_id":"github:mattijsdp:dbt-docs-mcp","source_url":"https://github.com/mattijsdp/dbt-docs-mcp"},{"type":"has_code","target_id":"github:yashshingvi:databricks-genie-MCP","source_url":"https://github.com/yashshingvi/databricks-genie-MCP"},{"type":"has_code","target_id":"github:alkemi-ai:alkemi-mcp","source_url":"https://github.com/alkemi-ai/alkemi-mcp"},{"type":"has_code","target_id":"github:avisangle:method-crm-mcp","source_url":"https://github.com/avisangle/method-crm-mcp"},{"type":"has_code","target_id":"github:paracetamol951:caisse-enregistreuse-mcp-server","source_url":"https://github.com/paracetamol951/caisse-enregistreuse-mcp-server"},{"type":"has_code","target_id":"github:21st-dev:magic-mcp","source_url":"https://github.com/21st-dev/magic-mcp"},{"type":"has_code","target_id":"github:louis030195:gptzero-mcp","source_url":"https://github.com/louis030195/gptzero-mcp"},{"type":"has_code","target_id":"github:aashari:mcp-server-atlassian-bitbucket","source_url":"https://github.com/aashari/mcp-server-atlassian-bitbucket"},{"type":"has_code","target_id":"github:aashari:mcp-server-atlassian-confluence","source_url":"https://github.com/aashari/mcp-server-atlassian-confluence"},{"type":"has_code","target_id":"github:aashari:mcp-server-atlassian-jira","source_url":"https://github.com/aashari/mcp-server-atlassian-jira"},{"type":"has_code","target_id":"github:abrinsmead:mindpilot-mcp","source_url":"https://github.com/abrinsmead/mindpilot-mcp"},{"type":"has_code","target_id":"github:admica:FileScopeMCP","source_url":"https://github.com/admica/FileScopeMCP"},{"type":"has_code","target_id":"github:agent-hanju:char-index-mcp","source_url":"https://github.com/agent-hanju/char-index-mcp"},{"type":"has_code","target_id":"github:akramIOT:MCP_AI_SOC_Sher","source_url":"https://github.com/akramIOT/MCP_AI_SOC_Sher"},{"type":"has_code","target_id":"github:alimo7amed93:webhook-tester-mcp","source_url":"https://github.com/alimo7amed93/webhook-tester-mcp"},{"type":"has_code","target_id":"github:ambar:simctl-mcp","source_url":"https://github.com/ambar/simctl-mcp"},{"type":"has_code","target_id":"github:api7:apisix-mcp","source_url":"https://github.com/api7/apisix-mcp"},{"type":"has_code","target_id":"github:apache:apisix","source_url":"https://github.com/apache/apisix"},{"type":"has_code","target_id":"github:ArchAI-Labs:fastmcp-sonarqube-metrics","source_url":"https://github.com/ArchAI-Labs/fastmcp-sonarqube-metrics"},{"type":"has_code","target_id":"github:artmann:package-registry-mcp","source_url":"https://github.com/artmann/package-registry-mcp"},{"type":"has_code","target_id":"github:wyattjoh:jsr-mcp","source_url":"https://github.com/wyattjoh/jsr-mcp"},{"type":"has_code","target_id":"github:augmnt:augments-mcp-server","source_url":"https://github.com/augmnt/augments-mcp-server"},{"type":"has_code","target_id":"github:automation-ai-labs:mcp-link","source_url":"https://github.com/automation-ai-labs/mcp-link"},{"type":"has_code","target_id":"github:avisangle:jenkins-mcp-server","source_url":"https://github.com/avisangle/jenkins-mcp-server"},{"type":"has_code","target_id":"github:axliupore:mcp-code-runner","source_url":"https://github.com/axliupore/mcp-code-runner"},{"type":"has_code","target_id":"github:azer:react-analyzer-mcp","source_url":"https://github.com/azer/react-analyzer-mcp"},{"type":"has_code","target_id":"github:bitrise-io:bitrise-mcp","source_url":"https://github.com/bitrise-io/bitrise-mcp"},{"type":"has_code","target_id":"github:buildkite:buildkite-mcp-server","source_url":"https://github.com/buildkite/buildkite-mcp-server"},{"type":"has_code","target_id":"github:chunkydotdev:bldbl-mcp","source_url":"https://github.com/chunkydotdev/bldbl-mcp"},{"type":"has_code","target_id":"github:CircleCI-Public:mcp-server-circleci","source_url":"https://github.com/CircleCI-Public/mcp-server-circleci"},{"type":"has_code","target_id":"github:Wolfe-Jam:claude-faf-mcp","source_url":"https://github.com/Wolfe-Jam/claude-faf-mcp"},{"type":"has_code","target_id":"github:cjo4m06:mcp-shrimp-task-manager","source_url":"https://github.com/cjo4m06/mcp-shrimp-task-manager"},{"type":"has_code","target_id":"github:ckanthony:gin-mcp","source_url":"https://github.com/ckanthony/gin-mcp"},{"type":"has_code","target_id":"github:BrunoKrugel:echo-mcp","source_url":"https://github.com/BrunoKrugel/echo-mcp"},{"type":"has_code","target_id":"github:ckreiling:mcp-server-docker","source_url":"https://github.com/ckreiling/mcp-server-docker"},{"type":"has_code","target_id":"github:CodeLogicIncEngineering:codelogic-mcp-server","source_url":"https://github.com/CodeLogicIncEngineering/codelogic-mcp-server"},{"type":"has_code","target_id":"github:comet-ml:opik-mcp","source_url":"https://github.com/comet-ml/opik-mcp"},{"type":"has_code","target_id":"github:configcat:mcp-server","source_url":"https://github.com/configcat/mcp-server"},{"type":"has_code","target_id":"github:cqfn:aibolit-mcp-server","source_url":"https://github.com/cqfn/aibolit-mcp-server"},{"type":"has_code","target_id":"github:currents-dev:currents-mcp","source_url":"https://github.com/currents-dev/currents-mcp"},{"type":"has_code","target_id":"github:davidan90:time-node-mcp","source_url":"https://github.com/davidan90/time-node-mcp"},{"type":"has_code","target_id":"github:davidlin2k:pox-mcp-server","source_url":"https://github.com/davidlin2k/pox-mcp-server"},{"type":"has_code","target_id":"github:delano:postman-mcp-server","source_url":"https://github.com/delano/postman-mcp-server"},{"type":"has_code","target_id":"github:alexpota:deploy-mcp","source_url":"https://github.com/alexpota/deploy-mcp"},{"type":"has_code","target_id":"github:docker:hub-mcp","source_url":"https://github.com/docker/hub-mcp"},{"type":"has_code","target_id":"github:V0v1kkk:DotNetMetadataMcpServer","source_url":"https://github.com/V0v1kkk/DotNetMetadataMcpServer"},{"type":"has_code","target_id":"github:endorhq:cli","source_url":"https://github.com/endorhq/cli"},{"type":"has_code","target_id":"github:eyaltoledano:claude-task-master","source_url":"https://github.com/eyaltoledano/claude-task-master"},{"type":"has_code","target_id":"github:etsd-tech:mcp-pointer","source_url":"https://github.com/etsd-tech/mcp-pointer"},{"type":"has_code","target_id":"github:flipt-io:mcp-server-flipt","source_url":"https://github.com/flipt-io/mcp-server-flipt"},{"type":"has_code","target_id":"github:freema:mcp-design-system-extractor","source_url":"https://github.com/freema/mcp-design-system-extractor"},{"type":"has_code","target_id":"github:gitkraken:gk-cli","source_url":"https://github.com/gitkraken/gk-cli"},{"type":"has_code","target_id":"github:GLips:Figma-Context-MCP","source_url":"https://github.com/GLips/Figma-Context-MCP"},{"type":"has_code","target_id":"github:mhmzdev:Figma-Flutter-MCP","source_url":"https://github.com/mhmzdev/Figma-Flutter-MCP"},{"type":"has_code","target_id":"github:gofireflyio:firefly-mcp","source_url":"https://github.com/gofireflyio/firefly-mcp"},{"type":"has_code","target_id":"github:gorosun:unified-diff-mcp","source_url":"https://github.com/gorosun/unified-diff-mcp"},{"type":"has_code","target_id":"github:Govcraft:rust-docs-mcp-server","source_url":"https://github.com/Govcraft/rust-docs-mcp-server"},{"type":"has_code","target_id":"github:promptexecution:cratedocs-mcp","source_url":"https://github.com/promptexecution/cratedocs-mcp"},{"type":"has_code","target_id":"github:HainanZhao:mcp-gitlab-jira","source_url":"https://github.com/HainanZhao/mcp-gitlab-jira"},{"type":"has_code","target_id":"github:haris-musa:excel-mcp-server","source_url":"https://github.com/haris-musa/excel-mcp-server"},{"type":"has_code","target_id":"github:sbroenne:mcp-server-excel","source_url":"https://github.com/sbroenne/mcp-server-excel"},{"type":"has_code","target_id":"github:hechtcarmel:jetbrains-debugger-mcp-plugin","source_url":"https://github.com/hechtcarmel/jetbrains-debugger-mcp-plugin"},{"type":"has_code","target_id":"github:hechtcarmel:jetbrains-index-mcp-plugin","source_url":"https://github.com/hechtcarmel/jetbrains-index-mcp-plugin"},{"type":"has_code","target_id":"github:higress-group:higress-ops-mcp-server","source_url":"https://github.com/higress-group/higress-ops-mcp-server"},{"type":"has_code","target_id":"github:alibaba:higress","source_url":"https://github.com/alibaba/higress"},{"type":"has_code","target_id":"github:hijaz:postmancer","source_url":"https://github.com/hijaz/postmancer"},{"type":"has_code","target_id":"github:hloiseaufcms:mcp-gopls","source_url":"https://github.com/hloiseaufcms/mcp-gopls"},{"type":"has_code","target_id":"github:golang:tools","source_url":"https://github.com/golang/tools"},{"type":"has_code","target_id":"github:hungthai1401:bruno-mcp","source_url":"https://github.com/hungthai1401/bruno-mcp"},{"type":"has_code","target_id":"github:hyperb1iss:droidmind","source_url":"https://github.com/hyperb1iss/droidmind"},{"type":"has_code","target_id":"github:Hypersequent:qasphere-mcp","source_url":"https://github.com/Hypersequent/qasphere-mcp"},{"type":"has_code","target_id":"github:idosal:git-mcp","source_url":"https://github.com/idosal/git-mcp"},{"type":"has_code","target_id":"github:IlyaGulya:gradle-mcp-server","source_url":"https://github.com/IlyaGulya/gradle-mcp-server"},{"type":"has_code","target_id":"github:promptexecution:just-mcp","source_url":"https://github.com/promptexecution/just-mcp"},{"type":"has_code","target_id":"github:InditexTech:mcp-server-simulator-ios-idb","source_url":"https://github.com/InditexTech/mcp-server-simulator-ios-idb"},{"type":"has_code","target_id":"github:InhiblabCore:mcp-image-compression","source_url":"https://github.com/InhiblabCore/mcp-image-compression"},{"type":"has_code","target_id":"github:InsForge:insforge-mcp","source_url":"https://github.com/InsForge/insforge-mcp"},{"type":"has_code","target_id":"github:inspizzz:jetbrains-datalore-mcp","source_url":"https://github.com/inspizzz/jetbrains-datalore-mcp"},{"type":"has_code","target_id":"github:joshuayoes:ios-simulator-mcp","source_url":"https://github.com/joshuayoes/ios-simulator-mcp"},{"type":"has_code","target_id":"github:isaacphi:mcp-language-server","source_url":"https://github.com/isaacphi/mcp-language-server"},{"type":"has_code","target_id":"github:IvanAmador:vercel-ai-docs-mcp","source_url":"https://github.com/IvanAmador/vercel-ai-docs-mcp"},{"type":"has_code","target_id":"github:j4c0bs:mcp-server-sql-analyzer","source_url":"https://github.com/j4c0bs/mcp-server-sql-analyzer"},{"type":"has_code","target_id":"github:tobymao:sqlglot","source_url":"https://github.com/tobymao/sqlglot"},{"type":"has_code","target_id":"github:janreges:ai-distiller","source_url":"https://github.com/janreges/ai-distiller"},{"type":"has_code","target_id":"github:jasonjmcghee:claude-debugs-for-you","source_url":"https://github.com/jasonjmcghee/claude-debugs-for-you"},{"type":"has_code","target_id":"github:JetBrains:mcpProxy","source_url":"https://github.com/JetBrains/mcpProxy"},{"type":"has_code","target_id":"github:Jktfe:serveMyAPI","source_url":"https://github.com/Jktfe/serveMyAPI"},{"type":"has_code","target_id":"github:JordanDalton:RestCsvMcpServer","source_url":"https://github.com/JordanDalton/RestCsvMcpServer"},{"type":"has_code","target_id":"github:JoshuaRileyDev:app-store-connect-mcp-server","source_url":"https://github.com/JoshuaRileyDev/app-store-connect-mcp-server"},{"type":"has_code","target_id":"github:JoshuaRileyDev:simulator-mcp-server","source_url":"https://github.com/JoshuaRileyDev/simulator-mcp-server"},{"type":"has_code","target_id":"github:Jpisnice:shadcn-ui-mcp-server","source_url":"https://github.com/Jpisnice/shadcn-ui-mcp-server"},{"type":"has_code","target_id":"github:jsdelivr:globalping-mcp-server","source_url":"https://github.com/jsdelivr/globalping-mcp-server"},{"type":"has_code","target_id":"github:kadykov:mcp-openapi-schema-explorer","source_url":"https://github.com/kadykov/mcp-openapi-schema-explorer"},{"type":"has_code","target_id":"github:lamemind:mcp-server-multiverse","source_url":"https://github.com/lamemind/mcp-server-multiverse"},{"type":"has_code","target_id":"github:langfuse:mcp-server-langfuse","source_url":"https://github.com/langfuse/mcp-server-langfuse"},{"type":"has_code","target_id":"github:JamesANZ:system-prompts-mcp-server","source_url":"https://github.com/JamesANZ/system-prompts-mcp-server"},{"type":"has_code","target_id":"github:linw1995:nvim-mcp","source_url":"https://github.com/linw1995/nvim-mcp"},{"type":"has_code","target_id":"github:lpigeon:ros-mcp-server","source_url":"https://github.com/lpigeon/ros-mcp-server"},{"type":"has_code","target_id":"github:lpigeon:unitree-go2-mcp-server","source_url":"https://github.com/lpigeon/unitree-go2-mcp-server"},{"type":"has_code","target_id":"github:ukkit:memcord","source_url":"https://github.com/ukkit/memcord"},{"type":"has_code","target_id":"github:mattjegan:swarmia-mcp","source_url":"https://github.com/mattjegan/swarmia-mcp"},{"type":"has_code","target_id":"github:mobile-next:mobile-mcp","source_url":"https://github.com/mobile-next/mobile-mcp"},{"type":"has_code","target_id":"github:mrexodia:user-feedback-mcp","source_url":"https://github.com/mrexodia/user-feedback-mcp"},{"type":"has_code","target_id":"github:mumez:pharo-smalltalk-interop-mcp-server","source_url":"https://github.com/mumez/pharo-smalltalk-interop-mcp-server"},{"type":"has_code","target_id":"github:narumiruna:gitingest-mcp","source_url":"https://github.com/narumiruna/gitingest-mcp"},{"type":"has_code","target_id":"github:cyclotruc:gitingest","source_url":"https://github.com/cyclotruc/gitingest"},{"type":"has_code","target_id":"github:neilberkman:editorconfig_mcp","source_url":"https://github.com/neilberkman/editorconfig_mcp"},{"type":"has_code","target_id":"github:OctoMind-dev:octomind-mcp","source_url":"https://github.com/OctoMind-dev/octomind-mcp"},{"type":"has_code","target_id":"github:OpenZeppelin:contracts-wizard","source_url":"https://github.com/OpenZeppelin/contracts-wizard"},{"type":"has_code","target_id":"github:bgauryy:octocode-mcp","source_url":"https://github.com/bgauryy/octocode-mcp"},{"type":"has_code","target_id":"github:opslevel:opslevel-mcp","source_url":"https://github.com/opslevel/opslevel-mcp"},{"type":"has_code","target_id":"github:ooples:token-optimizer-mcp","source_url":"https://github.com/ooples/token-optimizer-mcp"}]', NULL, 'MIT', 'approved', 80, '03f814126c385248441b1eec8af13603', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-punkpeye-awesome-mcp-servers from https://github.com/punkpeye.png
Image converted to WebP: data/images/github-punkpeye-awesome-mcp-servers.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-hacksider-Deep-Live-Cam', 'github--hacksider--deep-live-cam', 'Deep-Live-Cam', 'hacksider', '<h1 align="center">Deep-Live-Cam</h1> <p align="center"> Real-time face swap and video deepfake with a single click and only a single image. </p> <p align="center"> <a href="https://trendshift.io/repositories/11395" target="_blank"><img src="https://trendshift.io/api/badge/repositories/11395" alt="hacksider%2FDeep-Live-Cam | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a> </p> <p align="center"> <img src="media/demo.gif" alt="Demo GIF" width="800"> </p> This deep...', '["ai","ai-deep-fake","ai-face","ai-webcam","artificial-intelligence","deep-fake","deepfake","deepfake-webcam","faceswap","fake-webcam","gan","real-time-deepfake","realtime","realtime-deepfake","realtime-face-changer","video-deepfake","webcam","webcamera","python"]', 'other', 76185, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/hacksider/Deep-Live-Cam","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<h1 align="center">Deep-Live-Cam</h1>\n\n<p align="center">\n  Real-time face swap and video deepfake with a single click and only a single image.\n</p>\n\n<p align="center">\n<a href="https://trendshift.io/repositories/11395" target="_blank"><img src="https://trendshift.io/api/badge/repositories/11395" alt="hacksider%2FDeep-Live-Cam | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n</p>\n\n<p align="center">\n  <img src="media/demo.gif" alt="Demo GIF" width="800">\n</p>\n\n##  Disclaimer\n\nThis deepfake software is designed to be a productive tool for the AI-generated media industry. It can assist artists in animating custom characters, creating engaging content, and even using models for clothing design.\n\nWe are aware of the potential for unethical applications and are committed to preventative measures. A built-in check prevents the program from processing inappropriate media (nudity, graphic content, sensitive material like war footage, etc.). We will continue to develop this project responsibly, adhering to the law and ethics. We may shut down the project or add watermarks if legally required.\n\n- Ethical Use: Users are expected to use this software responsibly and legally. If using a real person''s face, obtain their consent and clearly label any output as a deepfake when sharing online.\n\n- Content Restrictions: The software includes built-in checks to prevent processing inappropriate media, such as nudity, graphic content, or sensitive material.\n\n- Legal Compliance: We adhere to all relevant laws and ethical guidelines. If legally required, we may shut down the project or add watermarks to the output.\n\n- User Responsibility: We are not responsible for end-user actions. Users must ensure their use of the software aligns with ethical standards and legal requirements.\n\nBy using this software, you agree to these terms and commit to using it in a manner that respects the rights and dignity of others.\n\nUsers are expected to use this software responsibly and legally. If using a real person''s face, obtain their consent and clearly label any output as a deepfake when sharing online. We are not responsible for end-user actions.\n\n## Exclusive v2.3d Quick Start - Pre-built (Windows/Mac Silicon)\n\n  <a href="https://deeplivecam.net/index.php/quickstart"> <img src="media/Download.png" width="285" height="77" />\n\n##### This is the fastest build you can get if you have a discrete NVIDIA or AMD GPU or Mac Silicon, And you''ll receive special priority support.\n \n###### These Pre-builts are perfect for non-technical users or those who don''t have time to, or can''t manually install all the requirements. Just a heads-up: this is an open-source project, so you can also install it manually. \n\n## TLDR; Live Deepfake in just 3 Clicks\n![easysteps](https://github.com/user-attachments/assets/af825228-852c-411b-b787-ffd9aac72fc6)\n1. Select a face\n2. Select which camera to use\n3. Press live!\n\n## Features & Uses - Everything is in real-time\n\n### Mouth Mask\n\n**Retain your original mouth for accurate movement using Mouth Mask**\n\n<p align="center">\n  <img src="media/ludwig.gif" alt="resizable-gif">\n</p>\n\n### Face Mapping\n\n**Use different faces on multiple subjects simultaneously**\n\n<p align="center">\n  <img src="media/streamers.gif" alt="face_mapping_source">\n</p>\n\n### Your Movie, Your Face\n\n**Watch movies with any face in real-time**\n\n<p align="center">\n  <img src="media/movie.gif" alt="movie">\n</p>\n\n### Live Show\n\n**Run Live shows and performances**\n\n<p align="center">\n  <img src="media/live_show.gif" alt="show">\n</p>\n\n### Memes\n\n**Create Your Most Viral Meme Yet**\n\n<p align="center">\n  <img src="media/meme.gif" alt="show" width="450"> \n  <br>\n  <sub>Created using Many Faces feature in Deep-Live-Cam</sub>\n</p>\n\n### Omegle\n\n**Surprise people on Omegle**\n\n<p align="center">\n  <video src="https://github.com/user-attachments/assets/2e9b9b82-fa04-4b70-9f56-b1f68e7672d0" width="450" controls></video>\n</p>\n\n## Installation (Manual)\n\n**Please be aware that the installation requires technical skills and is not for beginners. Consider downloading the quickstart version.**\n\n<details>\n<summary>Click to see the process</summary>\n\n### Installation\n\nThis is more likely to work on your computer but will be slower as it utilizes the CPU.\n\n**1. Set up Your Platform**\n\n-   Python (3.11 recommended)\n-   pip\n-   git\n-   [ffmpeg](https://www.youtube.com/watch?v=OlNWCpFdVMA) - ```iex (irm ffmpeg.tc.ht)```\n-   [Visual Studio 2022 Runtimes (Windows)](https://visualstudio.microsoft.com/visual-cpp-build-tools/)\n\n**2. Clone the Repository**\n\n```bash\ngit clone https://github.com/hacksider/Deep-Live-Cam.git\ncd Deep-Live-Cam\n```\n\n**3. Download the Models**\n\n1. [GFPGANv1.4](https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth)\n2. [inswapper\_128\_fp16.onnx](https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx)\n\nPlace these files in the "**models**" folder.\n\n**4. Install Dependencies**\n\nWe highly recommend using a `venv` to avoid issues.\n\n\nFor Windows:\n```bash\npython -m venv venv\nvenv\Scripts\activate\npip install -r requirements.txt\n```\nFor Linux:\n```bash\n# Ensure you use the installed Python 3.10\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n```\n\n**For macOS:**\n\nApple Silicon (M1/M2/M3) requires specific setup:\n\n```bash\n# Install Python 3.11 (specific version is important)\nbrew install python@3.11\n\n# Install tkinter package (required for the GUI)\nbrew install python-tk@3.10\n\n# Create and activate virtual environment with Python 3.11\npython3.11 -m venv venv\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\n```\n\n** In case something goes wrong and you need to reinstall the virtual environment **\n\n```bash\n# Deactivate the virtual environment\nrm -rf venv\n\n# Reinstall the virtual environment\npython -m venv venv\nsource venv/bin/activate\n\n# install the dependencies again\npip install -r requirements.txt\n\n# gfpgan and basicsrs issue fix\npip install git+https://github.com/xinntao/BasicSR.git@master\npip uninstall gfpgan -y\npip install git+https://github.com/TencentARC/GFPGAN.git@master\n```\n\n**Run:** If you don''t have a GPU, you can run Deep-Live-Cam using `python run.py`. Note that initial execution will download models (~300MB).\n\n### GPU Acceleration\n\n**CUDA Execution Provider (Nvidia)**\n\n1. Install [CUDA Toolkit 12.8.0](https://developer.nvidia.com/cuda-12-8-0-download-archive)\n2. Install [cuDNN v8.9.7 for CUDA 12.x](https://developer.nvidia.com/rdp/cudnn-archive) (required for onnxruntime-gpu):\n   - Download cuDNN v8.9.7 for CUDA 12.x\n   - Make sure the cuDNN bin directory is in your system PATH\n3. Install dependencies:\n\n```bash\npip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\npip uninstall onnxruntime onnxruntime-gpu\npip install onnxruntime-gpu==1.21.0\n```\n\n3. Usage:\n\n```bash\npython run.py --execution-provider cuda\n```\n\n**CoreML Execution Provider (Apple Silicon)**\n\nApple Silicon (M1/M2/M3) specific installation:\n\n1. Make sure you''ve completed the macOS setup above using Python 3.10.\n2. Install dependencies:\n\n```bash\npip uninstall onnxruntime onnxruntime-silicon\npip install onnxruntime-silicon==1.13.1\n```\n\n3. Usage (important: specify Python 3.10):\n\n```bash\npython3.10 run.py --execution-provider coreml\n```\n\n**Important Notes for macOS:**\n- You **must** use Python 3.10, not newer versions like 3.11 or 3.13\n- Always run with `python3.10` command not just `python` if you have multiple Python versions installed\n- If you get error about `_tkinter` missing, reinstall the tkinter package: `brew reinstall python-tk@3.10`\n- If you get model loading errors, check that your models are in the correct folder\n- If you encounter conflicts with other Python versions, consider uninstalling them:\n  ```bash\n  # List all installed Python versions\n  brew list | grep python\n  \n  # Uninstall conflicting versions if needed\n  brew uninstall --ignore-dependencies python@3.11 python@3.13\n  \n  # Keep only Python 3.11\n  brew cleanup\n  ```\n\n**CoreML Execution Provider (Apple Legacy)**\n\n1. Install dependencies:\n\n```bash\npip uninstall onnxruntime onnxruntime-coreml\npip install onnxruntime-coreml==1.21.0\n```\n\n2. Usage:\n\n```bash\npython run.py --execution-provider coreml\n```\n\n**DirectML Execution Provider (Windows)**\n\n1. Install dependencies:\n\n```bash\npip uninstall onnxruntime onnxruntime-directml\npip install onnxruntime-directml==1.21.0\n```\n\n2. Usage:\n\n```bash\npython run.py --execution-provider directml\n```\n\n**OpenVINOâ„¢ Execution Provider (Intel)**\n\n1. Install dependencies:\n\n```bash\npip uninstall onnxruntime onnxruntime-openvino\npip install onnxruntime-openvino==1.21.0\n```\n\n2. Usage:\n\n```bash\npython run.py --execution-provider openvino\n```\n</details>\n\n## Usage\n\n**1. Image/Video Mode**\n\n-   Execute `python run.py`.\n-   Choose a source face image and a target image/video.\n-   Click "Start".\n-   The output will be saved in a directory named after the target video.\n\n**2. Webcam Mode**\n\n-   Execute `python run.py`.\n-   Select a source face image.\n-   Click "Live".\n-   Wait for the preview to appear (10-30 seconds).\n-   Use a screen capture tool like OBS to stream.\n-   To change the face, select a new source image.\n\n## Command Line Arguments (Unmaintained)\n\n```\noptions:\n  -h, --help                                               show this help message and exit\n  -s SOURCE_PATH, --source SOURCE_PATH                     select a source image\n  -t TARGET_PATH, --target TARGET_PATH                     select a target image or video\n  -o OUTPUT_PATH, --output OUTPUT_PATH                     select output file or directory\n  --frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]  frame processors (choices: face_swapper, face_enhancer, ...)\n  --keep-fps                                               keep original fps\n  --keep-audio                                             keep original audio\n  --keep-frames                                            keep temporary frames\n  --many-faces                                             process every face\n  --map-faces                                              map source target faces\n  --mouth-mask                                             mask the mouth region\n  --video-encoder {libx264,libx265,libvpx-vp9}             adjust output video encoder\n  --video-quality [0-51]                                   adjust output video quality\n  --live-mirror                                            the live camera display as you see it in the front-facing camera frame\n  --live-resizable                                         the live camera frame is resizable\n  --max-memory MAX_MEMORY                                  maximum amount of RAM in GB\n  --execution-provider {cpu} [{cpu} ...]                   available execution provider (choices: cpu, ...)\n  --execution-threads EXECUTION_THREADS                    number of execution threads\n  -v, --version                                            show program''s version number and exit\n```\n\nLooking for a CLI mode? Using the -s/--source argument will make the run program in cli mode.\n\n## Press\n\n**We are always open to criticism and are ready to improve, that''s why we didn''t cherry-pick anything.**\n\n - [*"Deep-Live-Cam goes viral, allowing anyone to become a digital doppelganger"*](https://arstechnica.com/information-technology/2024/08/new-ai-tool-enables-real-time-face-swapping-on-webcams-raising-fraud-concerns/) - Ars Technica\n - [*"Thanks Deep Live Cam, shapeshifters are among us now"*](https://dataconomy.com/2024/08/15/what-is-deep-live-cam-github-deepfake/) - Dataconomy\n - [*"This free AI tool lets you become anyone during video-calls"*](https://www.newsbytesapp.com/news/science/deep-live-cam-ai-impersonation-tool-goes-viral/story) - NewsBytes\n - [*"OK, this viral AI live stream software is truly terrifying"*](https://www.creativebloq.com/ai/ok-this-viral-ai-live-stream-software-is-truly-terrifying) - Creative Bloq\n - [*"Deepfake AI Tool Lets You Become Anyone in a Video Call With Single Photo"*](https://petapixel.com/2024/08/14/deep-live-cam-deepfake-ai-tool-lets-you-become-anyone-in-a-video-call-with-single-photo-mark-zuckerberg-jd-vance-elon-musk/) - PetaPixel\n - [*"Deep-Live-Cam Uses AI to Transform Your Face in Real-Time, Celebrities Included"*](https://www.techeblog.com/deep-live-cam-ai-transform-face/) - TechEBlog\n - [*"An AI tool that "makes you look like anyone" during a video call is going viral online"*](https://telegrafi.com/en/a-tool-that-makes-you-look-like-anyone-during-a-video-call-is-going-viral-on-the-Internet/) - Telegrafi\n - [*"This Deepfake Tool Turning Images Into Livestreams is Topping the GitHub Charts"*](https://decrypt.co/244565/this-deepfake-tool-turning-images-into-livestreams-is-topping-the-github-charts) - Emerge\n - [*"New Real-Time Face-Swapping AI Allows Anyone to Mimic Famous Faces"*](https://www.digitalmusicnews.com/2024/08/15/face-swapping-ai-real-time-mimic/) - Digital Music News\n - [*"This real-time webcam deepfake tool raises alarms about the future of identity theft"*](https://www.diyphotography.net/this-real-time-webcam-deepfake-tool-raises-alarms-about-the-future-of-identity-theft/) - DIYPhotography\n - [*"That''s Crazy, Oh God. That''s Fucking Freaky Dude... That''s So Wild Dude"*](https://www.youtube.com/watch?time_continue=1074&v=py4Tc-Y8BcY) - SomeOrdinaryGamers\n - [*"Alright look look look, now look chat, we can do any face we want to look like chat"*](https://www.youtube.com/live/mFsCe7AIxq8?feature=shared&t=2686) - IShowSpeed\n - [*"They do a pretty good job matching poses, expression and even the lighting"*](https://www.youtube.com/watch?v=wnCghLjqv3s&t=551s) - TechLinked (LTT)\n - [*"Als Sean Connery an der Redaktionskonferenz teilnahm"*](https://www.golem.de/news/deepfakes-als-sean-connery-an-der-redaktionskonferenz-teilnahm-2408-188172.html) - Golem.de (German)\n - [*"What the F***! Why do I look like Vinny Jr? I look exactly like Vinny Jr!? No, this shit is crazy! Bro This is F*** Crazy! "*](https://youtu.be/JbUPRmXRUtE?t=3964) - IShowSpeed\n\n\n## Credits\n\n-   [ffmpeg](https://ffmpeg.org/): for making video-related operations easy\n-   [Henry](https://github.com/henryruhs): One of the major contributor in this repo\n-   [deepinsight](https://github.com/deepinsight): for their [insightface](https://github.com/deepinsight/insightface) project which provided a well-made library and models. Please be reminded that the [use of the model is for non-commercial research purposes only](https://github.com/deepinsight/insightface?tab=readme-ov-file#license).\n-   [havok2-htwo](https://github.com/havok2-htwo): for sharing the code for webcam\n-   [GosuDRM](https://github.com/GosuDRM): for the open version of roop\n-   [pereiraroland26](https://github.com/pereiraroland26): Multiple faces support\n-   [vic4key](https://github.com/vic4key): For supporting/contributing to this project\n-   [kier007](https://github.com/kier007): for improving the user experience\n-   [qitianai](https://github.com/qitianai): for multi-lingual support\n-   and [all developers](https://github.com/hacksider/Deep-Live-Cam/graphs/contributors) behind libraries used in this project.\n-   Footnote: Please be informed that the base author of the code is [s0md3v](https://github.com/s0md3v/roop)\n-   All the wonderful users who helped make this project go viral by starring the repo â¤ï¸\n\n[![Stargazers](https://reporoster.com/stars/hacksider/Deep-Live-Cam)](https://github.com/hacksider/Deep-Live-Cam/stargazers)\n\n## Contributions\n\n![Alt](https://repobeats.axiom.co/api/embed/fec8e29c45dfdb9c5916f3a7830e1249308d20e1.svg "Repobeats analytics image")\n\n## Stars to the Moon ğŸš€\n\n<a href="https://star-history.com/#hacksider/deep-live-cam&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&type=Date&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&type=Date" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=hacksider/deep-live-cam&type=Date" />\n </picture>\n</a>\n', '{"language":"Python","stars":76185,"forks":11098,"watchers":76185,"open_issues":92,"topics":["ai","ai-deep-fake","ai-face","ai-webcam","artificial-intelligence","deep-fake","deepfake","deepfake-webcam","faceswap","fake-webcam","gan","real-time-deepfake","realtime","realtime-deepfake","realtime-face-changer","video-deepfake","webcam","webcamera"],"default_branch":"main","size_kb":155912,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:hacksider:Deep-Live-Cam.git","source_url":"https://github.com/hacksider/Deep-Live-Cam.git"},{"type":"has_code","target_id":"github:xinntao:BasicSR.git@master","source_url":"https://github.com/xinntao/BasicSR.git@master"},{"type":"has_code","target_id":"github:TencentARC:GFPGAN.git@master","source_url":"https://github.com/TencentARC/GFPGAN.git@master"},{"type":"has_code","target_id":"github:deepinsight:insightface","source_url":"https://github.com/deepinsight/insightface"},{"type":"has_code","target_id":"github:deepinsight:insightface","source_url":"https://github.com/deepinsight/insightface?tab=readme-ov-file#license"},{"type":"has_code","target_id":"github:hacksider:Deep-Live-Cam","source_url":"https://github.com/hacksider/Deep-Live-Cam"},{"type":"has_code","target_id":"github:s0md3v:roop","source_url":"https://github.com/s0md3v/roop"},{"type":"has_code","target_id":"github:hacksider:Deep-Live-Cam","source_url":"https://github.com/hacksider/Deep-Live-Cam"}]', NULL, 'AGPL-3.0', 'approved', 80, '611f900575dab550638ce8becde79100', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-hacksider-Deep-Live-Cam from https://github.com/hacksider.png
Image converted to WebP: data/images/github-hacksider-Deep-Live-Cam.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-firecrawl-firecrawl', 'github--firecrawl--firecrawl', 'firecrawl', 'firecrawl', '<h3 align="center"> <a name="readme-top"></a> <img src="https://raw.githubusercontent.com/firecrawl/firecrawl/main/img/firecrawl_logo.png" height="200" > </h3> <div align="center"> <a href="https://github.com/firecrawl/firecrawl/blob/main/LICENSE"> <img src="https://img.shields.io/github/license/firecrawl/firecrawl" alt="License"> </a> <a href="https://pepy.tech/project/firecrawl-py"> <img src="https://static.pepy.tech/badge/firecrawl-py" alt="Downloads"> </a> <a href="https://GitHub.com/fire...', '["ai","ai-agents","ai-crawler","ai-scraping","ai-search","crawler","data-extraction","html-to-markdown","llm","markdown","scraper","scraping","web-crawler","web-data","web-data-extraction","web-scraper","web-scraping","web-search","webscraping","typescript"]', 'other', 69324, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/firecrawl/firecrawl","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<h3 align="center">\n  <a name="readme-top"></a>\n  <img\n    src="https://raw.githubusercontent.com/firecrawl/firecrawl/main/img/firecrawl_logo.png"\n    height="200"\n  >\n</h3>\n<div align="center">\n    <a href="https://github.com/firecrawl/firecrawl/blob/main/LICENSE">\n  <img src="https://img.shields.io/github/license/firecrawl/firecrawl" alt="License">\n</a>\n    <a href="https://pepy.tech/project/firecrawl-py">\n  <img src="https://static.pepy.tech/badge/firecrawl-py" alt="Downloads">\n</a>\n<a href="https://GitHub.com/firecrawl/firecrawl/graphs/contributors">\n  <img src="https://img.shields.io/github/contributors/firecrawl/firecrawl.svg" alt="GitHub Contributors">\n</a>\n<a href="https://firecrawl.dev">\n  <img src="https://img.shields.io/badge/Visit-firecrawl.dev-orange" alt="Visit firecrawl.dev">\n</a>\n</div>\n<div>\n  <p align="center">\n    <a href="https://twitter.com/firecrawl_dev">\n      <img src="https://img.shields.io/badge/Follow%20on%20X-000000?style=for-the-badge&logo=x&logoColor=white" alt="Follow on X" />\n    </a>\n    <a href="https://www.linkedin.com/company/104100957">\n      <img src="https://img.shields.io/badge/Follow%20on%20LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white" alt="Follow on LinkedIn" />\n    </a>\n    <a href="https://discord.com/invite/gSmWdAkdwd">\n      <img src="https://img.shields.io/badge/Join%20our%20Discord-5865F2?style=for-the-badge&logo=discord&logoColor=white" alt="Join our Discord" />\n    </a>\n  </p>\n</div>\n\n# ğŸ”¥ Firecrawl\n\nEmpower your AI apps with clean data from any website. Featuring advanced scraping, crawling, and data extraction capabilities.\n\n_This repository is in development, and weâ€™re still integrating custom modules into the mono repo. It''s not fully ready for self-hosted deployment yet, but you can run it locally._\n\n## What is Firecrawl?\n\n[Firecrawl](https://firecrawl.dev?ref=github) is an API service that takes a URL, crawls it, and converts it into clean markdown or structured data. We crawl all accessible subpages and give you clean data for each. No sitemap required. Check out our [documentation](https://docs.firecrawl.dev).\n\nLooking for our MCP? Check out the [repo here](https://github.com/firecrawl/firecrawl-mcp-server).\n\n_Pst. hey, you, join our stargazers :)_\n\n<a href="https://github.com/firecrawl/firecrawl">\n  <img src="https://img.shields.io/github/stars/firecrawl/firecrawl.svg?style=social&label=Star&maxAge=2592000" alt="GitHub stars">\n</a>\n\n## How to use it?\n\nWe provide an easy to use API with our hosted version. You can find the playground and documentation [here](https://firecrawl.dev/playground). You can also self host the backend if you''d like.\n\nCheck out the following resources to get started:\n- [x] **API**: [Documentation](https://docs.firecrawl.dev/api-reference/introduction)\n- [x] **SDKs**: [Python](https://docs.firecrawl.dev/sdks/python), [Node](https://docs.firecrawl.dev/sdks/node)\n- [x] **LLM Frameworks**: [Langchain (python)](https://python.langchain.com/docs/integrations/document_loaders/firecrawl/), [Langchain (js)](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/firecrawl), [Llama Index](https://docs.llamaindex.ai/en/latest/examples/data_connectors/WebPageDemo/#using-firecrawl-reader), [Crew.ai](https://docs.crewai.com/), [Composio](https://composio.dev/tools/firecrawl/all), [PraisonAI](https://docs.praison.ai/firecrawl/), [Superinterface](https://superinterface.ai/docs/assistants/functions/firecrawl), [Vectorize](https://docs.vectorize.io/integrations/source-connectors/firecrawl)\n- [x] **Low-code Frameworks**: [Dify](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl), [Langflow](https://docs.langflow.org/), [Flowise AI](https://docs.flowiseai.com/integrations/langchain/document-loaders/firecrawl), [Cargo](https://docs.getcargo.io/integration/firecrawl), [Pipedream](https://pipedream.com/apps/firecrawl/)\n- [x] **Community SDKs**: [Go](https://docs.firecrawl.dev/sdks/go), [Rust](https://docs.firecrawl.dev/sdks/rust)\n- [x] **Others**: [Zapier](https://zapier.com/apps/firecrawl/integrations), [Pabbly Connect](https://www.pabbly.com/connect/integrations/firecrawl/)\n- [ ] Want an SDK or Integration? Let us know by opening an issue.\n\nTo run locally, refer to guide [here](https://github.com/firecrawl/firecrawl/blob/main/CONTRIBUTING.md).\n\n### API Key\n\nTo use the API, you need to sign up on [Firecrawl](https://firecrawl.dev) and get an API key.\n\n### Features\n\n- [**Scrape**](#scraping): scrapes a URL and get its content in LLM-ready format (markdown, structured data via [LLM Extract](#llm-extraction-beta), screenshot, html)\n- [**Crawl**](#crawling): scrapes all the URLs of a web page and return content in LLM-ready format\n- [**Map**](#map): input a website and get all the website urls - extremely fast\n- [**Search**](#search): search the web and get full content from results\n- [**Extract**](#extract): get structured data from single page, multiple pages or entire websites with AI.\n\n### Powerful Capabilities\n- **LLM-ready formats**: markdown, structured data, screenshot, HTML, links, metadata\n- **The hard stuff**: proxies, anti-bot mechanisms, dynamic content (js-rendered), output parsing, orchestration\n- **Customizability**: exclude tags, crawl behind auth walls with custom headers, max crawl depth, etc...\n- **Media parsing**: pdfs, docx, images\n- **Reliability first**: designed to get the data you need - no matter how hard it is\n- **Actions**: click, scroll, input, wait and more before extracting data\n- **Batching**: scrape thousands of URLs at the same time with a new async endpoint\n- **Change Tracking**: monitor and detect changes in website content over time\n\nYou can find all of Firecrawl''s capabilities and how to use them in our [documentation](https://docs.firecrawl.dev)\n\n### Crawling\n\nUsed to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v2/crawl \\n    -H ''Content-Type: application/json'' \\n    -H ''Authorization: Bearer fc-YOUR_API_KEY'' \\n    -d ''{\n      "url": "https://docs.firecrawl.dev",\n      "limit": 10,\n      "scrapeOptions": {\n        "formats": ["markdown", "html"]\n      }\n    }''\n```\n\nReturns a crawl job id and the url to check the status of the crawl.\n\n```json\n{\n  "success": true,\n  "id": "123-456-789",\n  "url": "https://api.firecrawl.dev/v2/crawl/123-456-789"\n}\n```\n\n### Check Crawl Job\n\nUsed to check the status of a crawl job and get its result.\n\n```bash\ncurl -X GET https://api.firecrawl.dev/v2/crawl/123-456-789 \\n  -H ''Content-Type: application/json'' \\n  -H ''Authorization: Bearer YOUR_API_KEY''\n```\n\n```json\n{\n  "status": "completed",\n  "total": 36,\n  "creditsUsed": 36,\n  "expiresAt": "2024-00-00T00:00:00.000Z",\n  "data": [\n    {\n      "markdown": "[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...",\n      "html": "<!DOCTYPE html><html lang=\"en\" class=\"js-focus-visible lg:[--scroll-mt:9.5rem]\" data-js-focus-visible=\"\">...",\n      "metadata": {\n        "title": "Build a ''Chat with website'' using Groq Llama 3 | Firecrawl",\n        "language": "en",\n        "sourceURL": "https://docs.firecrawl.dev/learn/rag-llama3",\n        "description": "Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a ''Chat with your website'' bot.",\n        "ogLocaleAlternate": [],\n        "statusCode": 200\n      }\n    }\n  ]\n}\n```\n\n### Scraping\n\nUsed to scrape a URL and get its content in the specified formats.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v2/scrape \\n    -H ''Content-Type: application/json'' \\n    -H ''Authorization: Bearer YOUR_API_KEY'' \\n    -d ''{\n      "url": "https://docs.firecrawl.dev",\n      "formats" : ["markdown", "html"]\n    }''\n```\n\nResponse:\n\n```json\n{\n  "success": true,\n  "data": {\n    "markdown": "Launch Week I is here! [See our Day 2 Release ğŸš€](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[ğŸ’¥ Get 2 months free...",\n    "html": "<!DOCTYPE html><html lang=\"en\" class=\"light\" style=\"color-scheme: light;\"><body class=\"__variable_36bd41 __variable_d7dc5d font-inter ...",\n    "metadata": {\n      "title": "Home - Firecrawl",\n      "description": "Firecrawl crawls and converts any website into clean markdown.",\n      "language": "en",\n      "keywords": "Firecrawl,Markdown,Data,Mendable,Langchain",\n      "robots": "follow, index",\n      "ogTitle": "Firecrawl",\n      "ogDescription": "Turn any website into LLM-ready data.",\n      "ogUrl": "https://www.firecrawl.dev/",\n      "ogImage": "https://www.firecrawl.dev/og.png?123",\n      "ogLocaleAlternate": [],\n      "ogSiteName": "Firecrawl",\n      "sourceURL": "https://firecrawl.dev",\n      "statusCode": 200\n    }\n  }\n}\n```\n\n### Map\n\nUsed to map a URL and get urls of the website. This returns most links present on the website.\n\n```bash cURL\ncurl -X POST https://api.firecrawl.dev/v2/map \\n    -H ''Content-Type: application/json'' \\n    -H ''Authorization: Bearer YOUR_API_KEY'' \\n    -d ''{\n      "url": "https://firecrawl.dev"\n    }''\n```\n\nResponse:\n\n```json\n{\n  "success": true,\n  "links": [\n    { "url": "https://firecrawl.dev", "title": "Firecrawl", "description": "Firecrawl is a tool that allows you to crawl a website and get the data you need." },\n    { "url": "https://www.firecrawl.dev/pricing", "title": "Firecrawl Pricing", "description": "Firecrawl Pricing" },\n    { "url": "https://www.firecrawl.dev/blog", "title": "Firecrawl Blog", "description": "Firecrawl Blog" },\n    { "url": "https://www.firecrawl.dev/playground", "title": "Firecrawl Playground", "description": "Firecrawl Playground" },\n    { "url": "https://www.firecrawl.dev/smart-crawl", "title": "Firecrawl Smart Crawl", "description": "Firecrawl Smart Crawl" }\n  ]\n}\n```\n\n#### Map with search\n\nMap with `search` param allows you to search for specific urls inside a website.\n\n```bash cURL\ncurl -X POST https://api.firecrawl.dev/v2/map \\n    -H ''Content-Type: application/json'' \\n    -H ''Authorization: Bearer YOUR_API_KEY'' \\n    -d ''{\n      "url": "https://firecrawl.dev",\n      "search": "docs"\n    }''\n```\n\nResponse will be an ordered list from the most relevant to the least relevant.\n\n```json\n{\n  "success": true,\n  "links": [\n    { "url": "https://docs.firecrawl.dev", "title": "Firecrawl Docs", "description": "Firecrawl Docs" },\n    { "url": "https://docs.firecrawl.dev/sdks/python", "title": "Firecrawl Python SDK", "description": "Firecrawl Python SDK" },\n    { "url": "https://docs.firecrawl.dev/learn/rag-llama3", "title": "Firecrawl RAG Llama 3", "description": "Firecrawl RAG Llama 3" }\n  ]\n}\n```\n\n### Search\n\nSearch the web and get full content from results\n\nFirecrawlâ€™s search API allows you to perform web searches and optionally scrape the search results in one operation.\n\n- Choose specific output formats (markdown, HTML, links, screenshots)\n- Search the web with customizable parameters (language, country, etc.)\n- Optionally retrieve content from search results in various formats\n- Control the number of results and set timeouts\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v2/search \\n  -H "Content-Type: application/json" \\n  -H "Authorization: Bearer fc-YOUR_API_KEY" \\n  -d ''{\n    "query": "what is firecrawl?",\n    "limit": 5\n  }''\n```\n\n#### Response\n\n```json\n{\n  "success": true,\n  "data": [\n    {\n      "url": "https://firecrawl.dev",\n      "title": "Firecrawl | Home Page",\n      "description": "Turn websites into LLM-ready data with Firecrawl"\n    },\n    {\n      "url": "https://docs.firecrawl.dev",\n      "title": "Documentation | Firecrawl",\n      "description": "Learn how to use Firecrawl in your own applications"\n    }\n  ]\n}\n```\n\n#### With content scraping\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v2/search \\n  -H "Content-Type: application/json" \\n  -H "Authorization: Bearer fc-YOUR_API_KEY" \\n  -d ''{\n    "query": "what is firecrawl?",\n    "limit": 5,\n    "scrapeOptions": {\n      "formats": ["markdown", "links"]\n    }\n  }''\n```\n\n### Extract (Beta)\n\nGet structured data from entire websites with a prompt and/or a schema.\n\nYou can extract structured data from one or multiple URLs, including wildcards:\n\nSingle Page:\nExample: https://firecrawl.dev/some-page\n\nMultiple Pages / Full Domain\nExample: https://firecrawl.dev/*\n\nWhen you use /*, Firecrawl will automatically crawl and parse all URLs it can discover in that domain, then extract the requested data.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v2/extract \\n    -H ''Content-Type: application/json'' \\n    -H ''Authorization: Bearer YOUR_API_KEY'' \\n    -d ''{\n      "urls": [\n        "https://firecrawl.dev/*", \n        "https://docs.firecrawl.dev/", \n        "https://www.ycombinator.com/companies"\n      ],\n      "prompt": "Extract the company mission, whether it is open source, and whether it is in Y Combinator from the page.",\n      "schema": {\n        "type": "object",\n        "properties": {\n          "company_mission": {\n            "type": "string"\n          },\n          "is_open_source": {\n            "type": "boolean"\n          },\n          "is_in_yc": {\n            "type": "boolean"\n          }\n        },\n        "required": [\n          "company_mission",\n          "is_open_source",\n          "is_in_yc"\n        ]\n      }\n    }''\n```\n\n```json\n{\n  "success": true,\n  "id": "44aa536d-f1cb-4706-ab87-ed0386685740",\n  "urlTrace": []\n}\n```\n\nIf you are using the sdks, it will auto pull the response for you:\n\n```json\n{\n  "success": true,\n  "data": {\n    "company_mission": "Firecrawl is the easiest way to extract data from the web. Developers use us to reliably convert URLs into LLM-ready markdown or structured data with a single API call.",\n    "supports_sso": false,\n    "is_open_source": true,\n    "is_in_yc": true\n  }\n}\n```\n\n### LLM Extraction (Beta)\n\nUsed to extract structured data from scraped pages.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v2/scrape \\n  -H ''Content-Type: application/json'' \\n  -H ''Authorization: Bearer YOUR_API_KEY'' \\n  -d ''{\n    "url": "https://www.mendable.ai/",\n    "formats": [\n      {\n        "type": "json",\n        "schema": {\n          "type": "object",\n          "properties": {\n            "company_mission": { "type": "string" },\n            "supports_sso": { "type": "boolean" },\n            "is_open_source": { "type": "boolean" },\n            "is_in_yc": { "type": "boolean" }\n          }\n        }\n      }\n    ]\n  }''\n```\n\n```json\n{\n  "success": true,\n  "data": {\n    "content": "Raw Content",\n    "metadata": {\n      "title": "Mendable",\n      "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\n      "robots": "follow, index",\n      "ogTitle": "Mendable",\n      "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\n      "ogUrl": "https://mendable.ai/",\n      "ogImage": "https://mendable.ai/mendable_new_og1.png",\n      "ogLocaleAlternate": [],\n      "ogSiteName": "Mendable",\n      "sourceURL": "https://mendable.ai/"\n    },\n    "json": {\n      "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn''t have to",\n      "supports_sso": true,\n      "is_open_source": false,\n      "is_in_yc": true\n    }\n  }\n}\n```\n\n### Extracting without a schema (New)\n\nYou can now extract without a schema by just passing a `prompt` to the endpoint. The llm chooses the structure of the data.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v2/scrape \\n    -H ''Content-Type: application/json'' \\n    -H ''Authorization: Bearer YOUR_API_KEY'' \\n    -d ''{\n      "url": "https://docs.firecrawl.dev/",\n      "formats": [\n        {\n          "type": "json",\n          "prompt": "Extract the company mission from the page."\n        }\n      ]\n    }''\n```\n\n### Interacting with the page with Actions (Cloud-only)\n\nFirecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.\n\nHere is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, and take a screenshot.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v2/scrape \\n    -H ''Content-Type: application/json'' \\n    -H ''Authorization: Bearer YOUR_API_KEY'' \\n    -d ''{\n        "url": "google.com",\n        "formats": ["markdown"],\n        "actions": [\n            {"type": "wait", "milliseconds": 2000},\n            {"type": "click", "selector": "textarea[title=\"Search\"]"},\n            {"type": "wait", "milliseconds": 2000},\n            {"type": "write", "text": "firecrawl"},\n            {"type": "wait", "milliseconds": 2000},\n            {"type": "press", "key": "ENTER"},\n            {"type": "wait", "milliseconds": 3000},\n            {"type": "click", "selector": "h3"},\n            {"type": "wait", "milliseconds": 3000},\n            {"type": "screenshot"}\n        ]\n    }''\n```\n\n### Batch Scraping Multiple URLs (New)\n\nYou can now batch scrape multiple URLs at the same time. It is very similar to how the /crawl endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape.\n\n```bash\ncurl -X POST https://api.firecrawl.dev/v2/batch/scrape \\n    -H ''Content-Type: application/json'' \\n    -H ''Authorization: Bearer YOUR_API_KEY'' \\n    -d ''{\n      "urls": ["https://docs.firecrawl.dev", "https://docs.firecrawl.dev/sdks/overview"],\n      "formats" : ["markdown", "html"]\n    }''\n```\n\n\n\n## Using Python SDK\n\n### Installing Python SDK\n\n```bash\npip install firecrawl-py\n```\n\n### Crawl a website\n\n```python\nfrom firecrawl import Firecrawl\n\nfirecrawl = Firecrawl(api_key="fc-YOUR_API_KEY")\n\n# Scrape a website (returns a Document)\ndoc = firecrawl.scrape(\n    "https://firecrawl.dev",\n    formats=["markdown", "html"],\n)\nprint(doc.markdown)\n\n# Crawl a website\nresponse = firecrawl.crawl(\n    "https://firecrawl.dev",\n    limit=100,\n    scrape_options={"formats": ["markdown", "html"]},\n    poll_interval=30,\n)\nprint(response)\n```\n\n### Extracting structured data from a URL\n\nWith LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:\n\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nclass Article(BaseModel):\n    title: str\n    points: int\n    by: str\n    commentsURL: str\n\nclass TopArticles(BaseModel):\n    top: List[Article] = Field(..., description="Top 5 stories")\n\n# Use JSON format with a Pydantic schema\ndoc = firecrawl.scrape(\n    "https://news.ycombinator.com",\n    formats=[{"type": "json", "schema": TopArticles}],\n)\nprint(doc.json)\n```\n\n## Using the Node SDK\n\n### Installation\n\nTo install the Firecrawl Node SDK, you can use npm:\n\n```bash\nnpm install @mendable/firecrawl-js\n```\n\n### Usage\n\n1. Get an API key from [firecrawl.dev](https://firecrawl.dev)\n2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `Firecrawl` class.\n\n```js\nimport Firecrawl from ''@mendable/firecrawl-js'';\n\nconst firecrawl = new Firecrawl({ apiKey: ''fc-YOUR_API_KEY'' });\n\n// Scrape a website\nconst doc = await firecrawl.scrape(''https://firecrawl.dev'', {\n  formats: [''markdown'', ''html''],\n});\nconsole.log(doc);\n\n// Crawl a website\nconst response = await firecrawl.crawl(''https://firecrawl.dev'', {\n  limit: 100,\n  scrapeOptions: { formats: [''markdown'', ''html''] },\n});\nconsole.log(response);\n```\n\n\n### Extracting structured data from a URL\n\nWith LLM extraction, you can easily extract structured data from any URL. We support zod schema to make it easier for you too. Here is how to use it:\n\n```js\nimport Firecrawl from ''@mendable/firecrawl-js'';\nimport { z } from ''zod'';\n\nconst firecrawl = new Firecrawl({ apiKey: ''fc-YOUR_API_KEY'' });\n\n// Define schema to extract contents into\nconst schema = z.object({\n  top: z\n    .array(\n      z.object({\n        title: z.string(),\n        points: z.number(),\n        by: z.string(),\n        commentsURL: z.string(),\n      })\n    )\n    .length(5)\n    .describe(''Top 5 stories on Hacker News''),\n});\n\n// Use the v2 extract API with direct Zod schema support\nconst extractRes = await firecrawl.extract({\n  urls: [''https://news.ycombinator.com''],\n  schema,\n  prompt: ''Extract the top 5 stories'',\n});\n\nconsole.log(extractRes);\n```\n\n## Open Source vs Cloud Offering\n\nFirecrawl is open source available under the AGPL-3.0 license. \n\nTo deliver the best possible product, we offer a hosted version of Firecrawl alongside our open-source offering. The cloud solution allows us to continuously innovate and maintain a high-quality, sustainable service for all users.\n\nFirecrawl Cloud is available at [firecrawl.dev](https://firecrawl.dev) and offers a range of features that are not available in the open source version:\n\n![Open Source vs Cloud Offering](https://raw.githubusercontent.com/firecrawl/firecrawl/main/img/open-source-cloud.png)\n\n\n## Contributing\n\nWe love contributions! Please read our [contributing guide](CONTRIBUTING.md) before submitting a pull request. If you''d like to self-host, refer to the [self-hosting guide](SELF_HOST.md).\n\n_It is the sole responsibility of the end users to respect websites'' policies when scraping, searching and crawling with Firecrawl. Users are advised to adhere to the applicable privacy policies and terms of use of the websites prior to initiating any scraping activities. By default, Firecrawl respects the directives specified in the websites'' robots.txt files when crawling. By utilizing Firecrawl, you expressly agree to comply with these conditions._\n\n## Contributors\n\n<a href="https://github.com/firecrawl/firecrawl/graphs/contributors">\n  <img alt="contributors" src="https://contrib.rocks/image?repo=firecrawl/firecrawl"/>\n</a>\n\n## License Disclaimer\n\nThis project is primarily licensed under the GNU Affero General Public License v3.0 (AGPL-3.0), as specified in the LICENSE file in the root directory of this repository. However, certain components of this project are licensed under the MIT License. Refer to the LICENSE files in these specific directories for details.\n\nPlease note:\n\n- The AGPL-3.0 license applies to all parts of the project unless otherwise specified.\n- The SDKs and some UI components are licensed under the MIT License. Refer to the LICENSE files in these specific directories for details.\n- When using or contributing to this project, ensure you comply with the appropriate license terms for the specific component you are working with.\n\nFor more details on the licensing of specific components, please refer to the LICENSE files in the respective directories or contact the project maintainers.\n\n\n<p align="right" style="font-size: 14px; color: #555; margin-top: 20px;">\n    <a href="#readme-top" style="text-decoration: none; color: #007bff; font-weight: bold;">\n        â†‘ Back to Top â†‘\n    </a>\n</p>\n', '{"language":"TypeScript","stars":69324,"forks":5437,"watchers":69324,"open_issues":132,"topics":["ai","ai-agents","ai-crawler","ai-scraping","ai-search","crawler","data-extraction","html-to-markdown","llm","markdown","scraper","scraping","web-crawler","web-data","web-data-extraction","web-scraper","web-scraping","web-search","webscraping"],"default_branch":"main","size_kb":80176,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:firecrawl:firecrawl","source_url":"https://github.com/firecrawl/firecrawl"},{"type":"has_code","target_id":"github:firecrawl:firecrawl-mcp-server","source_url":"https://github.com/firecrawl/firecrawl-mcp-server"},{"type":"has_code","target_id":"github:firecrawl:firecrawl\">","source_url":"https://github.com/firecrawl/firecrawl\">"},{"type":"has_code","target_id":"github:firecrawl:firecrawl","source_url":"https://github.com/firecrawl/firecrawl"},{"type":"has_code","target_id":"github:firecrawl:firecrawl","source_url":"https://github.com/firecrawl/firecrawl"}]', NULL, 'AGPL-3.0', 'approved', 80, '7e7fb4a3b13aba51d270dcb47e2a72f9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-firecrawl-firecrawl from https://github.com/firecrawl.png
Image converted to WebP: data/images/github-firecrawl-firecrawl.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-lobehub-lobe-chat', 'github--lobehub--lobe-chat', 'lobe-chat', 'lobehub', '> \[!NOTE] > > **Version Information** > > - **v1.x** (Stable): Available on the [](https://github.com/lobehub/lobe-chat/tree/main) branch > - **v2.x** (In Development): Currently being actively developed on the [](https://github.com/lobehub/lobe-chat/tree/next) branch ğŸ”¥ <div align="center"><a name="readme-top"></a> [![][image-banner]][vercel-link] An open-source, modern design ChatGPT/LLMs UI/framework.<br/> Supports speech synthesis, multi-modal, and extensible ([function call][docs-functi...', '["agent","ai","artifacts","chat","chatgpt","claude","deepseek","deepseek-r1","function-calling","gemini","gpt","knowledge-base","mcp","nextjs","ollama","openai","rag","typescript"]', 'other', 68749, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/lobehub/lobe-chat","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '> \[!NOTE]\n>\n> **Version Information**\n>\n> - **v1.x** (Stable): Available on the [`main`](https://github.com/lobehub/lobe-chat/tree/main) branch\n> - **v2.x** (In Development): Currently being actively developed on the [`next`](https://github.com/lobehub/lobe-chat/tree/next) branch ğŸ”¥\n\n<div align="center"><a name="readme-top"></a>\n\n[![][image-banner]][vercel-link]\n\n# Lobe Chat\n\nAn open-source, modern design ChatGPT/LLMs UI/framework.<br/>\nSupports speech synthesis, multi-modal, and extensible ([function call][docs-function-call]) plugin system.<br/>\nOne-click **FREE** deployment of your private OpenAI ChatGPT/Claude/Gemini/Groq/Ollama chat application.\n\n**English** Â· [ç®€ä½“ä¸­æ–‡](./README.zh-CN.md) Â· [Official Site][official-site] Â· [Changelog][changelog] Â· [Documents][docs] Â· [Blog][blog] Â· [Feedback][github-issues-link]\n\n<!-- SHIELD GROUP -->\n\n[![][github-release-shield]][github-release-link]\n[![][docker-release-shield]][docker-release-link]\n[![][vercel-shield]][vercel-link]\n[![][discord-shield]][discord-link]<br/>\n[![][codecov-shield]][codecov-link]\n[![][github-action-test-shield]][github-action-test-link]\n[![][github-action-release-shield]][github-action-release-link]\n[![][github-releasedate-shield]][github-releasedate-link]<br/>\n[![][github-contributors-shield]][github-contributors-link]\n[![][github-forks-shield]][github-forks-link]\n[![][github-stars-shield]][github-stars-link]\n[![][github-issues-shield]][github-issues-link]\n[![][github-license-shield]][github-license-link]<br>\n[![][sponsor-shield]][sponsor-link]\n\n**Share LobeChat Repository**\n\n[![][share-x-shield]][share-x-link]\n[![][share-telegram-shield]][share-telegram-link]\n[![][share-whatsapp-shield]][share-whatsapp-link]\n[![][share-reddit-shield]][share-reddit-link]\n[![][share-weibo-shield]][share-weibo-link]\n[![][share-mastodon-shield]][share-mastodon-link]\n[![][share-linkedin-shield]][share-linkedin-link]\n\n<sup>Pioneering the new age of thinking and creating. Built for you, the Super Individual.</sup>\n\n[![][github-trending-shield]][github-trending-url] <br /> <br /> <a href="https://vercel.com/oss"> <img alt="Vercel OSS Program" src="https://vercel.com/oss/program-badge.svg" /> </a>\n\n![][image-overview]\n\n</div>\n\n<details>\n<summary><kbd>Table of contents</kbd></summary>\n\n#### TOC\n\n- [ğŸ‘‹ğŸ» Getting Started & Join Our Community](#-getting-started--join-our-community)\n- [âœ¨ Features](#-features)\n  - [âœ¨ MCP Plugin One-Click Installation](#-mcp-plugin-one-click-installation)\n  - [ğŸª MCP Marketplace](#-mcp-marketplace)\n  - [ğŸ–¥ï¸ Desktop App](#ï¸-desktop-app)\n  - [ğŸŒ Smart Internet Search](#-smart-internet-search)\n  - [Chain of Thought](#chain-of-thought)\n  - [Branching Conversations](#branching-conversations)\n  - [Artifacts Support](#artifacts-support)\n  - [File Upload /Knowledge Base](#file-upload-knowledge-base)\n  - [Multi-Model Service Provider Support](#multi-model-service-provider-support)\n  - [Local Large Language Model (LLM) Support](#local-large-language-model-llm-support)\n  - [Model Visual Recognition](#model-visual-recognition)\n  - [TTS & STT Voice Conversation](#tts--stt-voice-conversation)\n  - [Text to Image Generation](#text-to-image-generation)\n  - [Plugin System (Function Calling)](#plugin-system-function-calling)\n  - [Agent Market (GPTs)](#agent-market-gpts)\n  - [Support Local / Remote Database](#support-local--remote-database)\n  - [Support Multi-User Management](#support-multi-user-management)\n  - [Progressive Web App (PWA)](#progressive-web-app-pwa)\n  - [Mobile Device Adaptation](#mobile-device-adaptation)\n  - [Custom Themes](#custom-themes)\n  - [`*` What''s more](#-whats-more)\n- [âš¡ï¸ Performance](#ï¸-performance)\n- [ğŸ›³ Self Hosting](#-self-hosting)\n  - [`A` Deploying with Vercel, Zeabur , Sealos or Alibaba Cloud](#a-deploying-with-vercel-zeabur--sealos-or-alibaba-cloud)\n  - [`B` Deploying with Docker](#b-deploying-with-docker)\n  - [Environment Variable](#environment-variable)\n- [ğŸ“¦ Ecosystem](#-ecosystem)\n- [ğŸ§© Plugins](#-plugins)\n- [âŒ¨ï¸ Local Development](#ï¸-local-development)\n- [ğŸ¤ Contributing](#-contributing)\n- [â¤ï¸ Sponsor](#ï¸-sponsor)\n- [ğŸ”— More Products](#-more-products)\n\n####\n\n<br/>\n\n</details>\n\n## ğŸ‘‹ğŸ» Getting Started & Join Our Community\n\nWe are a group of e/acc design-engineers, hoping to provide modern design components and tools for AIGC.\nBy adopting the Bootstrapping approach, we aim to provide developers and users with a more open, transparent, and user-friendly product ecosystem.\n\nWhether for users or professional developers, LobeHub will be your AI Agent playground. Please be aware that LobeChat is currently under active development, and feedback is welcome for any [issues][issues-link] encountered.\n\n| [![][vercel-shield-badge]][vercel-link]   | No installation or registration necessary! Visit our website to experience it firsthand.                           |\n| :---------------------------------------- | :----------------------------------------------------------------------------------------------------------------- |\n| [![][discord-shield-badge]][discord-link] | Join our Discord community! This is where you can connect with developers and other enthusiastic users of LobeHub. |\n\n> \[!IMPORTANT]\n>\n> **Star Us**, You will receive all release notifications from GitHub without any delay \~ â­ï¸\n\n[![][image-star]][github-stars-link]\n\n<details>\n  <summary><kbd>Star History</kbd></summary>\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=lobehub%2Flobe-chat&theme=dark&type=Date">\n    <img width="100%" src="https://api.star-history.com/svg?repos=lobehub%2Flobe-chat&type=Date">\n  </picture>\n</details>\n\n## âœ¨ Features\n\nTransform your AI experience with LobeChat''s powerful features designed for seamless connectivity, enhanced productivity, and unlimited creativity.\n\n![][image-feat-mcp]\n\n### âœ¨ MCP Plugin One-Click Installation\n\n**Seamlessly Connect Your AI to the World**\n\nUnlock the full potential of your AI by enabling smooth, secure, and dynamic interactions with external tools, data sources, and services. LobeChat''s MCP (Model Context Protocol) plugin system breaks down the barriers between your AI and the digital ecosystem, allowing for unprecedented connectivity and functionality.\n\nTransform your conversations into powerful workflows by connecting to databases, APIs, file systems, and more. Experience the freedom of AI that truly understands and interacts with your world.\n\n[![][back-to-top]](#readme-top)\n\n![][image-feat-mcp-market]\n\n### ğŸª MCP Marketplace\n\n**Discover, Connect, Extend**\n\nBrowse a growing library of MCP plugins to expand your AI''s capabilities and streamline your workflows effortlessly. Visit [lobehub.com/mcp](https://lobehub.com/mcp) to explore the MCP Marketplace, which offers a curated collection of integrations that enhance your AI''s ability to work with various tools and services.\n\nFrom productivity tools to development environments, discover new ways to extend your AI''s reach and effectiveness. Connect with the community and find the perfect plugins for your specific needs.\n\n[![][back-to-top]](#readme-top)\n\n![][image-feat-desktop]\n\n### ğŸ–¥ï¸ Desktop App\n\n**Peak Performance, Zero Distractions**\n\nGet the full LobeChat experience without browser limitationsâ€”comprehensive, focused, and always ready to go. Our desktop application provides a dedicated environment for your AI interactions, ensuring optimal performance and minimal distractions.\n\nExperience faster response times, better resource management, and a more stable connection to your AI assistant. The desktop app is designed for users who demand the best performance from their AI tools.\n\n[![][back-to-top]](#readme-top)\n\n![][image-feat-web-search]\n\n### ğŸŒ Smart Internet Search\n\n**Online Knowledge On Demand**\n\nWith real-time internet access, your AI keeps up with the worldâ€”news, data, trends, and more. Stay informed and get the most current information available, enabling your AI to provide accurate and up-to-date responses.\n\nAccess live information, verify facts, and explore current events without leaving your conversation. Your AI becomes a gateway to the world''s knowledge, always current and comprehensive.\n\n[![][back-to-top]](#readme-top)\n\n[![][image-feat-cot]][docs-feat-cot]\n\n### [Chain of Thought][docs-feat-cot]\n\nExperience AI reasoning like never before. Watch as complex problems unfold step by step through our innovative Chain of Thought (CoT) visualization. This breakthrough feature provides unprecedented transparency into AI''s decision-making process, allowing you to observe how conclusions are reached in real-time.\n\nBy breaking down complex reasoning into clear, logical steps, you can better understand and validate the AI''s problem-solving approach. Whether you''re debugging, learning, or simply curious about AI reasoning, CoT visualization transforms abstract thinking into an engaging, interactive experience.\n\n[![][back-to-top]](#readme-top)\n\n[![][image-feat-branch]][docs-feat-branch]\n\n### [Branching Conversations][docs-feat-branch]\n\nIntroducing a more natural and flexible way to chat with AI. With Branch Conversations, your discussions can flow in multiple directions, just like human conversations do. Create new conversation branches from any message, giving you the freedom to explore different paths while preserving the original context.\n\nChoose between two powerful modes:\n\n- **Continuation Mode:** Seamlessly extend your current discussion while maintaining valuable context\n- **Standalone Mode:** Start fresh with a new topic based on any previous message\n\nThis groundbreaking feature transforms linear conversations into dynamic, tree-like structures, enabling deeper exploration of ideas and more productive interactions.\n\n[![][back-to-top]](#readme-top)\n\n[![][image-feat-artifacts]][docs-feat-artifacts]\n\n### [Artifacts Support][docs-feat-artifacts]\n\nExperience the power of Claude Artifacts, now integrated into LobeChat. This revolutionary feature expands the boundaries of AI-human interaction, enabling real-time creation and visualization of diverse content formats.\n\nCreate and visualize with unprecedented flexibility:\n\n- Generate and display dynamic SVG graphics\n- Build and render interactive HTML pages in real-time\n- Produce professional documents in multiple formats\n\n[![][back-to-top]](#readme-top)\n\n[![][image-feat-knowledgebase]][docs-feat-knowledgebase]\n\n### [File Upload /Knowledge Base][docs-feat-knowledgebase]\n\nLobeChat supports file upload and knowledge base functionality. You can upload various types of files including documents, images, audio, and video, as well as create knowledge bases, making it convenient for users to manage and search for files. Additionally, you can utilize files and knowledge base features during conversations, enabling a richer dialogue experience.\n\n<https://github.com/user-attachments/assets/faa8cf67-e743-4590-8bf6-ebf6ccc34175>\n\n> \[!TIP]\n>\n> Learn more on [ğŸ“˜ LobeChat Knowledge Base Launch â€” From Now On, Every Step Counts](https://lobehub.com/blog/knowledge-base)\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-privoder]][docs-feat-provider]\n\n### [Multi-Model Service Provider Support][docs-feat-provider]\n\nIn the continuous development of LobeChat, we deeply understand the importance of diversity in model service providers for meeting the needs of the community when providing AI conversation services. Therefore, we have expanded our support to multiple model service providers, rather than being limited to a single one, in order to offer users a more diverse and rich selection of conversations.\n\nIn this way, LobeChat can more flexibly adapt to the needs of different users, while also providing developers with a wider range of choices.\n\n#### Supported Model Service Providers\n\nWe have implemented support for the following model service providers:\n\n<!-- PROVIDER LIST -->\n\n<details><summary><kbd>See more providers (+-10)</kbd></summary>\n\n</details>\n\n> ğŸ“Š Total providers: [<kbd>**0**</kbd>](https://lobechat.com/discover/providers)\n\n <!-- PROVIDER LIST -->\n\nAt the same time, we are also planning to support more model service providers. If you would like LobeChat to support your favorite service provider, feel free to join our [ğŸ’¬ community discussion](https://github.com/lobehub/lobe-chat/discussions/1284).\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-local]][docs-feat-local]\n\n### [Local Large Language Model (LLM) Support][docs-feat-local]\n\nTo meet the specific needs of users, LobeChat also supports the use of local models based on [Ollama](https://ollama.ai), allowing users to flexibly use their own or third-party models.\n\n> \[!TIP]\n>\n> Learn more about [ğŸ“˜ Using Ollama in LobeChat][docs-usage-ollama] by checking it out.\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-vision]][docs-feat-vision]\n\n### [Model Visual Recognition][docs-feat-vision]\n\nLobeChat now supports OpenAI''s latest [`gpt-4-vision`](https://platform.openai.com/docs/guides/vision) model with visual recognition capabilities,\na multimodal intelligence that can perceive visuals. Users can easily upload or drag and drop images into the dialogue box,\nand the agent will be able to recognize the content of the images and engage in intelligent conversation based on this,\ncreating smarter and more diversified chat scenarios.\n\nThis feature opens up new interactive methods, allowing communication to transcend text and include a wealth of visual elements.\nWhether it''s sharing images in daily use or interpreting images within specific industries, the agent provides an outstanding conversational experience.\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-tts]][docs-feat-tts]\n\n### [TTS & STT Voice Conversation][docs-feat-tts]\n\nLobeChat supports Text-to-Speech (TTS) and Speech-to-Text (STT) technologies, enabling our application to convert text messages into clear voice outputs,\nallowing users to interact with our conversational agent as if they were talking to a real person. Users can choose from a variety of voices to pair with the agent.\n\nMoreover, TTS offers an excellent solution for those who prefer auditory learning or desire to receive information while busy.\nIn LobeChat, we have meticulously selected a range of high-quality voice options (OpenAI Audio, Microsoft Edge Speech) to meet the needs of users from different regions and cultural backgrounds.\nUsers can choose the voice that suits their personal preferences or specific scenarios, resulting in a personalized communication experience.\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-t2i]][docs-feat-t2i]\n\n### [Text to Image Generation][docs-feat-t2i]\n\nWith support for the latest text-to-image generation technology, LobeChat now allows users to invoke image creation tools directly within conversations with the agent. By leveraging the capabilities of AI tools such as [`DALL-E 3`](https://openai.com/dall-e-3), [`MidJourney`](https://www.midjourney.com/), and [`Pollinations`](https://pollinations.ai/), the agents are now equipped to transform your ideas into images.\n\nThis enables a more private and immersive creative process, allowing for the seamless integration of visual storytelling into your personal dialogue with the agent.\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-plugin]][docs-feat-plugin]\n\n### [Plugin System (Function Calling)][docs-feat-plugin]\n\nThe plugin ecosystem of LobeChat is an important extension of its core functionality, greatly enhancing the practicality and flexibility of the LobeChat assistant.\n\n<video controls src="https://github.com/lobehub/lobe-chat/assets/28616219/f29475a3-f346-4196-a435-41a6373ab9e2" muted="false"></video>\n\nBy utilizing plugins, LobeChat assistants can obtain and process real-time information, such as searching for web information and providing users with instant and relevant news.\n\nIn addition, these plugins are not limited to news aggregation, but can also extend to other practical functions, such as quickly searching documents, generating images, obtaining data from various platforms like Bilibili, Steam, and interacting with various third-party services.\n\n> \[!TIP]\n>\n> Learn more about [ğŸ“˜ Plugin Usage][docs-usage-plugin] by checking it out.\n\n<!-- PLUGIN LIST -->\n\n| Recent Submits                                                                                                             | Description                                                                                                                               |\n| -------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |\n| [PortfolioMeta](https://lobechat.com/discover/plugin/StockData)<br/><sup>By **portfoliometa** on **2025-11-28**</sup>      | Analyze stocks and get comprehensive real-time investment data and analytics.<br/>`stock`                                                 |\n| [SEO](https://lobechat.com/discover/plugin/SEO)<br/><sup>By **orrenprunckun** on **2025-11-14**</sup>                      | Enter any URL and keyword and get an On-Page SEO analysis & insights!<br/>`seo`                                                           |\n| [Shopping tools](https://lobechat.com/discover/plugin/ShoppingTools)<br/><sup>By **shoppingtools** on **2025-10-27**</sup> | Search for products on eBay & AliExpress, find eBay events & coupons. Get prompt examples.<br/>`shopping` `e-bay` `ali-express` `coupons` |\n| [Web](https://lobechat.com/discover/plugin/web)<br/><sup>By **Proghit** on **2025-01-24**</sup>                            | Smart web search that reads and analyzes pages to deliver comprehensive answers from Google results.<br/>`web` `search`                   |\n\n> ğŸ“Š Total plugins: [<kbd>**41**</kbd>](https://lobechat.com/discover/plugins)\n\n <!-- PLUGIN LIST -->\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-agent]][docs-feat-agent]\n\n### [Agent Market (GPTs)][docs-feat-agent]\n\nIn LobeChat Agent Marketplace, creators can discover a vibrant and innovative community that brings together a multitude of well-designed agents,\nwhich not only play an important role in work scenarios but also offer great convenience in learning processes.\nOur marketplace is not just a showcase platform but also a collaborative space. Here, everyone can contribute their wisdom and share the agents they have developed.\n\n> \[!TIP]\n>\n> By [ğŸ¤–/ğŸª Submit Agents][submit-agents-link], you can easily submit your agent creations to our platform.\n> Importantly, LobeChat has established a sophisticated automated internationalization (i18n) workflow,\n> capable of seamlessly translating your agent into multiple language versions.\n> This means that no matter what language your users speak, they can experience your agent without barriers.\n\n> \[!IMPORTANT]\n>\n> We welcome all users to join this growing ecosystem and participate in the iteration and optimization of agents.\n> Together, we can create more interesting, practical, and innovative agents, further enriching the diversity and practicality of the agent offerings.\n\n<!-- AGENT LIST -->\n\n| Recent Submits                                                                                                                                                                 | Description                                                                                                                                                                                                              |\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| [Turtle Soup Host](https://lobechat.com/discover/assistant/lateral-thinking-puzzle)<br/><sup>By **[CSY2022](https://github.com/CSY2022)** on **2025-06-19**</sup>              | A turtle soup host needs to provide the scenario, the complete story (truth of the event), and the key point (the condition for guessing correctly).<br/>`turtle-soup` `reasoning` `interaction` `puzzle` `role-playing` |\n| [Gourmet ReviewerğŸŸ](https://lobechat.com/discover/assistant/food-reviewer)<br/><sup>By **[renhai-lab](https://github.com/renhai-lab)** on **2025-06-17**</sup>                | Food critique expert<br/>`gourmet` `review` `writing`                                                                                                                                                                    |\n| [Academic Writing Assistant](https://lobechat.com/discover/assistant/academic-writing-assistant)<br/><sup>By **[swarfte](https://github.com/swarfte)** on **2025-06-17**</sup> | Expert in academic research paper writing and formal documentation<br/>`academic-writing` `research` `formal-style`                                                                                                      |\n| [Minecraft Senior Developer](https://lobechat.com/discover/assistant/java-development)<br/><sup>By **[iamyuuk](https://github.com/iamyuuk)** on **2025-06-17**</sup>           | Expert in advanced Java development and Minecraft mod and server plugin development<br/>`development` `programming` `minecraft` `java`                                                                                   |\n\n> ğŸ“Š Total agents: [<kbd>**505**</kbd> ](https://lobechat.com/discover/assistants)\n\n <!-- AGENT LIST -->\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-database]][docs-feat-database]\n\n### [Support Local / Remote Database][docs-feat-database]\n\nLobeChat supports the use of both server-side and local databases. Depending on your needs, you can choose the appropriate deployment solution:\n\n- **Local database**: suitable for users who want more control over their data and privacy protection. LobeChat uses CRDT (Conflict-Free Replicated Data Type) technology to achieve multi-device synchronization. This is an experimental feature aimed at providing a seamless data synchronization experience.\n- **Server-side database**: suitable for users who want a more convenient user experience. LobeChat supports PostgreSQL as a server-side database. For detailed documentation on how to configure the server-side database, please visit [Configure Server-side Database](https://lobehub.com/docs/self-hosting/advanced/server-database).\n\nRegardless of which database you choose, LobeChat can provide you with an excellent user experience.\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-auth]][docs-feat-auth]\n\n### [Support Multi-User Management][docs-feat-auth]\n\nLobeChat supports multi-user management and provides two main user authentication and management solutions to meet different needs:\n\n- **next-auth**: LobeChat integrates `next-auth`, a flexible and powerful identity verification library that supports multiple authentication methods, including OAuth, email login, credential login, etc. With `next-auth`, you can easily implement user registration, login, session management, social login, and other functions to ensure the security and privacy of user data.\n\n- [**Clerk**](https://go.clerk.com/exgqLG0): For users who need more advanced user management features, LobeChat also supports `Clerk`, a modern user management platform. `Clerk` provides richer functions, such as multi-factor authentication (MFA), user profile management, login activity monitoring, etc. With `Clerk`, you can get higher security and flexibility, and easily cope with complex user management needs.\n\nRegardless of which user management solution you choose, LobeChat can provide you with an excellent user experience and powerful functional support.\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-pwa]][docs-feat-pwa]\n\n### [Progressive Web App (PWA)][docs-feat-pwa]\n\nWe deeply understand the importance of providing a seamless experience for users in today''s multi-device environment.\nTherefore, we have adopted Progressive Web Application ([PWA](https://support.google.com/chrome/answer/9658361)) technology,\na modern web technology that elevates web applications to an experience close to that of native apps.\n\nThrough PWA, LobeChat can offer a highly optimized user experience on both desktop and mobile devices while maintaining high-performance characteristics.\nVisually and in terms of feel, we have also meticulously designed the interface to ensure it is indistinguishable from native apps,\nproviding smooth animations, responsive layouts, and adapting to different device screen resolutions.\n\n> \[!NOTE]\n>\n> If you are unfamiliar with the installation process of PWA, you can add LobeChat as your desktop application (also applicable to mobile devices) by following these steps:\n>\n> - Launch the Chrome or Edge browser on your computer.\n> - Visit the LobeChat webpage.\n> - In the upper right corner of the address bar, click on the <kbd>Install</kbd> icon.\n> - Follow the instructions on the screen to complete the PWA Installation.\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-mobile]][docs-feat-mobile]\n\n### [Mobile Device Adaptation][docs-feat-mobile]\n\nWe have carried out a series of optimization designs for mobile devices to enhance the user''s mobile experience. Currently, we are iterating on the mobile user experience to achieve smoother and more intuitive interactions. If you have any suggestions or ideas, we welcome you to provide feedback through GitHub Issues or Pull Requests.\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n[![][image-feat-theme]][docs-feat-theme]\n\n### [Custom Themes][docs-feat-theme]\n\nAs a design-engineering-oriented application, LobeChat places great emphasis on users'' personalized experiences,\nhence introducing flexible and diverse theme modes, including a light mode for daytime and a dark mode for nighttime.\nBeyond switching theme modes, a range of color customization options allow users to adjust the application''s theme colors according to their preferences.\nWhether it''s a desire for a sober dark blue, a lively peach pink, or a professional gray-white, users can find their style of color choices in LobeChat.\n\n> \[!TIP]\n>\n> The default configuration can intelligently recognize the user''s system color mode and automatically switch themes to ensure a consistent visual experience with the operating system.\n> For users who like to manually control details, LobeChat also offers intuitive setting options and a choice between chat bubble mode and document mode for conversation scenarios.\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n### `*` What''s more\n\nBeside these features, LobeChat also have much better basic technique underground:\n\n- [x] ğŸ’¨ **Quick Deployment**: Using the Vercel platform or docker image, you can deploy with just one click and complete the process within 1 minute without any complex configuration.\n- [x] ğŸŒ **Custom Domain**: If users have their own domain, they can bind it to the platform for quick access to the dialogue agent from anywhere.\n- [x] ğŸ”’ **Privacy Protection**: All data is stored locally in the user''s browser, ensuring user privacy.\n- [x] ğŸ’ **Exquisite UI Design**: With a carefully designed interface, it offers an elegant appearance and smooth interaction. It supports light and dark themes and is mobile-friendly. PWA support provides a more native-like experience.\n- [x] ğŸ—£ï¸ **Smooth Conversation Experience**: Fluid responses ensure a smooth conversation experience. It fully supports Markdown rendering, including code highlighting, LaTex formulas, Mermaid flowcharts, and more.\n\n> âœ¨ more features will be added when LobeChat evolve.\n\n---\n\n> \[!NOTE]\n>\n> You can find our upcoming [Roadmap][github-project-link] plans in the Projects section.\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n## âš¡ï¸ Performance\n\n> \[!NOTE]\n>\n> The complete list of reports can be found in the [ğŸ“˜ Lighthouse Reports][docs-lighthouse]\n\n|                   Desktop                   |                   Mobile                   |\n| :-----------------------------------------: | :----------------------------------------: |\n|              ![][chat-desktop]              |              ![][chat-mobile]              |\n| [ğŸ“‘ Lighthouse Report][chat-desktop-report] | [ğŸ“‘ Lighthouse Report][chat-mobile-report] |\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n## ğŸ›³ Self Hosting\n\nLobeChat provides Self-Hosted Version with Vercel, Alibaba Cloud, and [Docker Image][docker-release-link]. This allows you to deploy your own chatbot within a few minutes without any prior knowledge.\n\n> \[!TIP]\n>\n> Learn more about [ğŸ“˜ Build your own LobeChat][docs-self-hosting] by checking it out.\n\n### `A` Deploying with Vercel, Zeabur , Sealos or Alibaba Cloud\n\n"If you want to deploy this service yourself on Vercel, Zeabur or Alibaba Cloud, you can follow these steps:\n\n- Prepare your [OpenAI API Key](https://platform.openai.com/account/api-keys).\n- Click the button below to start deployment: Log in directly with your GitHub account, and remember to fill in the `OPENAI_API_KEY`(required) and `ACCESS_CODE` (recommended) on the environment variable section.\n- After deployment, you can start using it.\n- Bind a custom domain (optional): The DNS of the domain assigned by Vercel is polluted in some areas; binding a custom domain can connect directly.\n\n<div align="center">\n\n|           Deploy with Vercel            |                     Deploy with Zeabur                      |                     Deploy with Sealos                      |                       Deploy with RepoCloud                       |                         Deploy with Alibaba Cloud                         |\n| :-------------------------------------: | :---------------------------------------------------------: | :---------------------------------------------------------: | :---------------------------------------------------------------: | :-----------------------------------------------------------------------: |\n| [![][deploy-button-image]][deploy-link] | [![][deploy-on-zeabur-button-image]][deploy-on-zeabur-link] | [![][deploy-on-sealos-button-image]][deploy-on-sealos-link] | [![][deploy-on-repocloud-button-image]][deploy-on-repocloud-link] | [![][deploy-on-alibaba-cloud-button-image]][deploy-on-alibaba-cloud-link] |\n\n</div>\n\n#### After Fork\n\nAfter fork, only retain the upstream sync action and disable other actions in your repository on GitHub.\n\n#### Keep Updated\n\nIf you have deployed your own project following the one-click deployment steps in the README, you might encounter constant prompts indicating "updates available." This is because Vercel defaults to creating a new project instead of forking this one, resulting in an inability to detect updates accurately.\n\n> \[!TIP]\n>\n> We suggest you redeploy using the following steps, [ğŸ“˜ Auto Sync With Latest][docs-upstream-sync]\n\n<br/>\n\n### `B` Deploying with Docker\n\n[![][docker-release-shield]][docker-release-link]\n[![][docker-size-shield]][docker-size-link]\n[![][docker-pulls-shield]][docker-pulls-link]\n\nWe provide a Docker image for deploying the LobeChat service on your own private device. Use the following command to start the LobeChat service:\n\n1. create a folder to for storage files\n\n```fish\n$ mkdir lobe-chat-db && cd lobe-chat-db\n```\n\n2. init the LobeChat infrastructure\n\n```fish\nbash <(curl -fsSL https://lobe.li/setup.sh)\n```\n\n3. Start the LobeChat service\n\n```fish\ndocker compose up -d\n```\n\n> \[!NOTE]\n>\n> For detailed instructions on deploying with Docker, please refer to the [ğŸ“˜ Docker Deployment Guide][docs-docker]\n\n<br/>\n\n### Environment Variable\n\nThis project provides some additional configuration items set with environment variables:\n\n| Environment Variable | Required | Description                                                                                                                                                               | Example                                                                                                              |\n| -------------------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- |\n| `OPENAI_API_KEY`     | Yes      | This is the API key you apply on the OpenAI account page                                                                                                                  | `sk-xxxxxx...xxxxxx`                                                                                                 |\n| `OPENAI_PROXY_URL`   | No       | If you manually configure the OpenAI interface proxy, you can use this configuration item to override the default OpenAI API request base URL                             | `https://api.chatanywhere.cn` or `https://aihubmix.com/v1` <br/>The default value is<br/>`https://api.openai.com/v1` |\n| `ACCESS_CODE`        | No       | Add a password to access this service; you can set a long password to avoid leaking. If this value contains a comma, it is a password array.                              | `awCTe)re_r74` or `rtrt_ewee3@09!` or `code1,code2,code3`                                                            |\n| `OPENAI_MODEL_LIST`  | No       | Used to control the model list. Use `+` to add a model, `-` to hide a model, and `model_name=display_name` to customize the display name of a model, separated by commas. | `qwen-7b-chat,+glm-6b,-gpt-3.5-turbo`                                                                                |\n\n> \[!NOTE]\n>\n> The complete list of environment variables can be found in the [ğŸ“˜ Environment Variables][docs-env-var]\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n## ğŸ“¦ Ecosystem\n\n| NPM                               | Repository                              | Description                                                                                           | Version                                   |\n| --------------------------------- | --------------------------------------- | ----------------------------------------------------------------------------------------------------- | ----------------------------------------- |\n| [@lobehub/ui][lobe-ui-link]       | [lobehub/lobe-ui][lobe-ui-github]       | Open-source UI component library dedicated to building AIGC web applications.                         | [![][lobe-ui-shield]][lobe-ui-link]       |\n| [@lobehub/icons][lobe-icons-link] | [lobehub/lobe-icons][lobe-icons-github] | Popular AI / LLM Model Brand SVG Logo and Icon Collection.                                            | [![][lobe-icons-shield]][lobe-icons-link] |\n| [@lobehub/tts][lobe-tts-link]     | [lobehub/lobe-tts][lobe-tts-github]     | High-quality & reliable TTS/STT React Hooks library                                                   | [![][lobe-tts-shield]][lobe-tts-link]     |\n| [@lobehub/lint][lobe-lint-link]   | [lobehub/lobe-lint][lobe-lint-github]   | Configurations for ESlint, Stylelint, Commitlint, Prettier, Remark, and Semantic Release for LobeHub. | [![][lobe-lint-shield]][lobe-lint-link]   |\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n## ğŸ§© Plugins\n\nPlugins provide a means to extend the [Function Calling][docs-function-call] capabilities of LobeChat. They can be used to introduce new function calls and even new ways to render message results. If you are interested in plugin development, please refer to our [ğŸ“˜ Plugin Development Guide][docs-plugin-dev] in the Wiki.\n\n- [lobe-chat-plugins][lobe-chat-plugins]: This is the plugin index for LobeChat. It accesses index.json from this repository to display a list of available plugins for LobeChat to the user.\n- [chat-plugin-template][chat-plugin-template]: This is the plugin template for LobeChat plugin development.\n- [@lobehub/chat-plugin-sdk][chat-plugin-sdk]: The LobeChat Plugin SDK assists you in creating exceptional chat plugins for Lobe Chat.\n- [@lobehub/chat-plugins-gateway][chat-plugins-gateway]: The LobeChat Plugins Gateway is a backend service that provides a gateway for LobeChat plugins. We deploy this service using Vercel. The primary API POST /api/v1/runner is deployed as an Edge Function.\n\n> \[!NOTE]\n>\n> The plugin system is currently undergoing major development. You can learn more in the following issues:\n>\n> - [x] [**Plugin Phase 1**](https://github.com/lobehub/lobe-chat/issues/73): Implement separation of the plugin from the main body, split the plugin into an independent repository for maintenance, and realize dynamic loading of the plugin.\n> - [x] [**Plugin Phase 2**](https://github.com/lobehub/lobe-chat/issues/97): The security and stability of the plugin''s use, more accurately presenting abnormal states, the maintainability of the plugin architecture, and developer-friendly.\n> - [x] [**Plugin Phase 3**](https://github.com/lobehub/lobe-chat/issues/149): Higher-level and more comprehensive customization capabilities, support for plugin authentication, and examples.\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n## âŒ¨ï¸ Local Development\n\nYou can use GitHub Codespaces for online development:\n\n[![][codespaces-shield]][codespaces-link]\n\nOr clone it for local development:\n\n```fish\n$ git clone https://github.com/lobehub/lobe-chat.git\n$ cd lobe-chat\n$ pnpm install\n$ pnpm dev\n```\n\nIf you would like to learn more details, please feel free to look at our [ğŸ“˜ Development Guide][docs-dev-guide].\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n## ğŸ¤ Contributing\n\nContributions of all types are more than welcome; if you are interested in contributing code, feel free to check out our GitHub [Issues][github-issues-link] and [Projects][github-project-link] to get stuck in to show us what you''re made of.\n\n> \[!TIP]\n>\n> We are creating a technology-driven forum, fostering knowledge interaction and the exchange of ideas that may culminate in mutual inspiration and collaborative innovation.\n>\n> Help us make LobeChat better. Welcome to provide product design feedback, user experience discussions directly to us.\n>\n> **Principal Maintainers:** [@arvinxx](https://github.com/arvinxx) [@canisminor1990](https://github.com/canisminor1990)\n\n[![][pr-welcome-shield]][pr-welcome-link]\n[![][submit-agents-shield]][submit-agents-link]\n[![][submit-plugin-shield]][submit-plugin-link]\n\n<a href="https://github.com/lobehub/lobe-chat/graphs/contributors" target="_blank">\n  <table>\n    <tr>\n      <th colspan="2">\n        <br><img src="https://contrib.rocks/image?repo=lobehub/lobe-chat"><br><br>\n      </th>\n    </tr>\n    <tr>\n      <td>\n        <picture>\n          <source media="(prefers-color-scheme: dark)" srcset="https://next.ossinsight.io/widgets/official/compose-org-active-contributors/thumbnail.png?activity=active&period=past_28_days&owner_id=131470832&repo_ids=643445235&image_size=2x3&color_scheme=dark">\n          <img src="https://next.ossinsight.io/widgets/official/compose-org-active-contributors/thumbnail.png?activity=active&period=past_28_days&owner_id=131470832&repo_ids=643445235&image_size=2x3&color_scheme=light">\n        </picture>\n      </td>\n      <td rowspan="2">\n        <picture>\n          <source media="(prefers-color-scheme: dark)" srcset="https://next.ossinsight.io/widgets/official/compose-org-participants-growth/thumbnail.png?activity=active&period=past_28_days&owner_id=131470832&repo_ids=643445235&image_size=4x7&color_scheme=dark">\n          <img src="https://next.ossinsight.io/widgets/official/compose-org-participants-growth/thumbnail.png?activity=active&period=past_28_days&owner_id=131470832&repo_ids=643445235&image_size=4x7&color_scheme=light">\n        </picture>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        <picture>\n          <source media="(prefers-color-scheme: dark)" srcset="https://next.ossinsight.io/widgets/official/compose-org-active-contributors/thumbnail.png?activity=new&period=past_28_days&owner_id=131470832&repo_ids=643445235&image_size=2x3&color_scheme=dark">\n          <img src="https://next.ossinsight.io/widgets/official/compose-org-active-contributors/thumbnail.png?activity=new&period=past_28_days&owner_id=131470832&repo_ids=643445235&image_size=2x3&color_scheme=light">\n        </picture>\n      </td>\n    </tr>\n  </table>\n</a>\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n## â¤ï¸ Sponsor\n\nEvery bit counts and your one-time donation sparkles in our galaxy of support! You''re a shooting star, making a swift and bright impact on our journey. Thank you for believing in us â€“ your generosity guides us toward our mission, one brilliant flash at a time.\n\n<a href="https://opencollective.com/lobehub" target="_blank">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://github.com/lobehub/.github/blob/main/static/sponsor-dark.png?raw=true">\n    <img  src="https://github.com/lobehub/.github/blob/main/static/sponsor-light.png?raw=true">\n  </picture>\n</a>\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n## ğŸ”— More Products\n\n- **[ğŸ…°ï¸ Lobe SD Theme][lobe-theme]:** Modern theme for Stable Diffusion WebUI, exquisite interface design, highly customizable UI, and efficiency-boosting features.\n- **[â›µï¸ Lobe Midjourney WebUI][lobe-midjourney-webui]:** WebUI for Midjourney, leverages AI to quickly generate a wide array of rich and diverse images from text prompts, sparking creativity and enhancing conversations.\n- **[ğŸŒ Lobe i18n][lobe-i18n] :** Lobe i18n is an automation tool for the i18n (internationalization) translation process, powered by ChatGPT. It supports features such as automatic splitting of large files, incremental updates, and customization options for the OpenAI model, API proxy, and temperature.\n- **[ğŸ’Œ Lobe Commit][lobe-commit]:** Lobe Commit is a CLI tool that leverages Langchain/ChatGPT to generate Gitmoji-based commit messages.\n\n<div align="right">\n\n[![][back-to-top]](#readme-top)\n\n</div>\n\n---\n\n<details><summary><h4>ğŸ“ License</h4></summary>\n\n[![][fossa-license-shield]][fossa-license-link]\n\n</details>\n\nCopyright Â© 2025 [LobeHub][profile-link]. <br />\nThis project is [LobeHub Community License](./LICENSE) licensed.\n\n<!-- LINK GROUP -->\n\n[back-to-top]: https://img.shields.io/badge/-BACK_TO_TOP-151515?style=flat-square\n[blog]: https://lobehub.com/blog\n[changelog]: https://lobehub.com/changelog\n[chat-desktop]: https://raw.githubusercontent.com/lobehub/lobe-chat/lighthouse/lighthouse/chat/desktop/pagespeed.svg\n[chat-desktop-report]: https://lobehub.github.io/lobe-chat/lighthouse/chat/desktop/chat_preview_lobehub_com_chat.html\n[chat-mobile]: https://raw.githubusercontent.com/lobehub/lobe-chat/lighthouse/lighthouse/chat/mobile/pagespeed.svg\n[chat-mobile-report]: https://lobehub.github.io/lobe-chat/lighthouse/chat/mobile/chat_preview_lobehub_com_chat.html\n[chat-plugin-sdk]: https://github.com/lobehub/chat-plugin-sdk\n[chat-plugin-template]: https://github.com/lobehub/chat-plugin-template\n[chat-plugins-gateway]: https://github.com/lobehub/chat-plugins-gateway\n[codecov-link]: https://codecov.io/gh/lobehub/lobe-chat\n[codecov-shield]: https://img.shields.io/codecov/c/github/lobehub/lobe-chat?labelColor=black&style=flat-square&logo=codecov&logoColor=white\n[codespaces-link]: https://codespaces.new/lobehub/lobe-chat\n[codespaces-shield]: https://github.com/codespaces/badge.svg\n[deploy-button-image]: https://vercel.com/button\n[deploy-link]: https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Flobehub%2Flobe-chat&env=OPENAI_API_KEY,ACCESS_CODE&envDescription=Find%20your%20OpenAI%20API%20Key%20by%20click%20the%20right%20Learn%20More%20button.%20%7C%20Access%20Code%20can%20protect%20your%20website&envLink=https%3A%2F%2Fplatform.openai.com%2Faccount%2Fapi-keys&project-name=lobe-chat&repository-name=lobe-chat\n[deploy-on-alibaba-cloud-button-image]: https://service-info-public.oss-cn-hangzhou.aliyuncs.com/computenest-en.svg\n[deploy-on-alibaba-cloud-link]: https://computenest.console.aliyun.com/service/instance/create/default?type=user&ServiceName=LobeChat%E7%A4%BE%E5%8C%BA%E7%89%88\n[deploy-on-repocloud-button-image]: https://d16t0pc4846x52.cloudfront.net/deploylobe.svg\n[deploy-on-repocloud-link]: https://repocloud.io/details/?app_id=248\n[deploy-on-sealos-button-image]: https://raw.githubusercontent.com/labring-actions/templates/main/Deploy-on-Sealos.svg\n[deploy-on-sealos-link]: https://template.usw.sealos.io/deploy?templateName=lobe-chat-db\n[deploy-on-zeabur-button-image]: https://zeabur.com/button.svg\n[deploy-on-zeabur-link]: https://zeabur.com/templates/VZGGTI\n[discord-link]: https://discord.gg/AYFPHvv2jT\n[discord-shield]: https://img.shields.io/discord/1127171173982154893?color=5865F2&label=discord&labelColor=black&logo=discord&logoColor=white&style=flat-square\n[discord-shield-badge]: https://img.shields.io/discord/1127171173982154893?color=5865F2&label=discord&labelColor=black&logo=discord&logoColor=white&style=for-the-badge\n[docker-pulls-link]: https://hub.docker.com/r/lobehub/lobe-chat-database\n[docker-pulls-shield]: https://img.shields.io/docker/pulls/lobehub/lobe-chat?color=45cc11&labelColor=black&style=flat-square&sort=semver\n[docker-release-link]: https://hub.docker.com/r/lobehub/lobe-chat-database\n[docker-release-shield]: https://img.shields.io/docker/v/lobehub/lobe-chat-database?color=369eff&label=docker&labelColor=black&logo=docker&logoColor=white&style=flat-square&sort=semver\n[docker-size-link]: https://hub.docker.com/r/lobehub/lobe-chat-database\n[docker-size-shield]: https://img.shields.io/docker/image-size/lobehub/lobe-chat-database?color=369eff&labelColor=black&style=flat-square&sort=semver\n[docs]: https://lobehub.com/docs/usage/start\n[docs-dev-guide]: https://github.com/lobehub/lobe-chat/wiki/index\n[docs-docker]: https://lobehub.com/docs/self-hosting/server-database/docker-compose\n[docs-env-var]: https://lobehub.com/docs/self-hosting/environment-variables\n[docs-feat-agent]: https://lobehub.com/docs/usage/features/agent-market\n[docs-feat-artifacts]: https://lobehub.com/docs/usage/features/artifacts\n[docs-feat-auth]: https://lobehub.com/docs/usage/features/auth\n[docs-feat-branch]: https://lobehub.com/docs/usage/features/branching-conversations\n[docs-feat-cot]: https://lobehub.com/docs/usage/features/cot\n[docs-feat-database]: https://lobehub.com/docs/usage/features/database\n[docs-feat-knowledgebase]: https://lobehub.com/blog/knowledge-base\n[docs-feat-local]: https://lobehub.com/docs/usage/features/local-llm\n[docs-feat-mobile]: https://lobehub.com/docs/usage/features/mobile\n[docs-feat-plugin]: https://lobehub.com/docs/usage/features/plugin-system\n[docs-feat-provider]: https://lobehub.com/docs/usage/features/multi-ai-providers\n[docs-feat-pwa]: https://lobehub.com/docs/usage/features/pwa\n[docs-feat-t2i]: https://lobehub.com/docs/usage/features/text-to-image\n[docs-feat-theme]: https://lobehub.com/docs/usage/features/theme\n[docs-feat-tts]: https://lobehub.com/docs/usage/features/tts\n[docs-feat-vision]: https://lobehub.com/docs/usage/features/vision\n[docs-function-call]: https://lobehub.com/blog/openai-function-call\n[docs-lighthouse]: https://github.com/lobehub/lobe-chat/wiki/Lighthouse\n[docs-plugin-dev]: https://lobehub.com/docs/usage/plugins/development\n[docs-self-hosting]: https://lobehub.com/docs/self-hosting/start\n[docs-upstream-sync]: https://lobehub.com/docs/self-hosting/advanced/upstream-sync\n[docs-usage-ollama]: https://lobehub.com/docs/usage/providers/ollama\n[docs-usage-plugin]: https://lobehub.com/docs/usage/plugins/basic\n[fossa-license-link]: https://app.fossa.com/projects/git%2Bgithub.com%2Flobehub%2Flobe-chat\n[fossa-license-shield]: https://app.fossa.com/api/projects/git%2Bgithub.com%2Flobehub%2Flobe-chat.svg?type=large\n[github-action-release-link]: https://github.com/actions/workflows/lobehub/lobe-chat/release.yml\n[github-action-release-shield]: https://img.shields.io/github/actions/workflow/status/lobehub/lobe-chat/release.yml?label=release&labelColor=black&logo=githubactions&logoColor=white&style=flat-square\n[github-action-test-link]: https://github.com/actions/workflows/lobehub/lobe-chat/test.yml\n[github-action-test-shield]: https://img.shields.io/github/actions/workflow/status/lobehub/lobe-chat/test.yml?label=test&labelColor=black&logo=githubactions&logoColor=white&style=flat-square\n[github-contributors-link]: https://github.com/lobehub/lobe-chat/graphs/contributors\n[github-contributors-shield]: https://img.shields.io/github/contributors/lobehub/lobe-chat?color=c4f042&labelColor=black&style=flat-square\n[github-forks-link]: https://github.com/lobehub/lobe-chat/network/members\n[github-forks-shield]: https://img.shields.io/github/forks/lobehub/lobe-chat?color=8ae8ff&labelColor=black&style=flat-square\n[github-issues-link]: https://github.com/lobehub/lobe-chat/issues\n[github-issues-shield]: https://img.shields.io/github/issues/lobehub/lobe-chat?color=ff80eb&labelColor=black&style=flat-square\n[github-license-link]: https://github.com/lobehub/lobe-chat/blob/main/LICENSE\n[github-license-shield]: https://img.shields.io/badge/license-apache%202.0-white?labelColor=black&style=flat-square\n[github-project-link]: https://github.com/lobehub/lobe-chat/projects\n[github-release-link]: https://github.com/lobehub/lobe-chat/releases\n[github-release-shield]: https://img.shields.io/github/v/release/lobehub/lobe-chat?color=369eff&labelColor=black&logo=github&style=flat-square\n[github-releasedate-link]: https://github.com/lobehub/lobe-chat/releases\n[github-releasedate-shield]: https://img.shields.io/github/release-date/lobehub/lobe-chat?labelColor=black&style=flat-square\n[github-stars-link]: https://github.com/lobehub/lobe-chat/stargazers\n[github-stars-shield]: https://img.shields.io/github/stars/lobehub/lobe-chat?color=ffcb47&labelColor=black&style=flat-square\n[github-trending-shield]: https://trendshift.io/api/badge/repositories/2256\n[github-trending-url]: https://trendshift.io/repositories/2256\n[image-banner]: https://github.com/user-attachments/assets/6f293c7f-47b4-47eb-9202-fe68a942d35b\n[image-feat-agent]: https://github.com/user-attachments/assets/b3ab6e35-4fbc-468d-af10-e3e0c687350f\n[image-feat-artifacts]: https://github.com/user-attachments/assets/7f95fad6-b210-4e6e-84a0-7f39e96f3a00\n[image-feat-auth]: https://github.com/user-attachments/assets/80bb232e-19d1-4f97-98d6-e291f3585e6d\n[image-feat-branch]: https://github.com/user-attachments/assets/92f72082-02bd-4835-9c54-b089aad7fd41\n[image-feat-cot]: https://github.com/user-attachments/assets/f74f1139-d115-4e9c-8c43-040a53797a5e\n[image-feat-database]: https://github.com/user-attachments/assets/f1697c8b-d1fb-4dac-ba05-153c6295d91d\n[image-feat-desktop]: https://github.com/user-attachments/assets/a7bac8d3-ea96-4000-bb39-fadc9b610f96\n[image-feat-knowledgebase]: https://github.com/user-attachments/assets/7da7a3b2-92fd-4630-9f4e-8560c74955ae\n[image-feat-local]: https://github.com/user-attachments/assets/1239da50-d832-4632-a7ef-bd754c0f3850\n[image-feat-mcp]: https://github.com/user-attachments/assets/1be85d36-3975-4413-931f-27e05e440995\n[image-feat-mcp-market]: https://github.com/user-attachments/assets/bb114f9f-24c5-4000-a984-c10d187da5a0\n[image-feat-mobile]: https://github.com/user-attachments/assets/32cf43c4-96bd-4a4c-bfb6-59acde6fe380\n[image-feat-plugin]: https://github.com/user-attachments/assets/66a891ac-01b6-4e3f-b978-2eb07b489b1b\n[image-feat-privoder]: https://github.com/user-attachments/assets/e553e407-42de-4919-977d-7dbfcf44a821\n[image-feat-pwa]: https://github.com/user-attachments/assets/9647f70f-b71b-43b6-9564-7cdd12d1c24d\n[image-feat-t2i]: https://github.com/user-attachments/assets/708274a7-2458-494b-a6ec-b73dfa1fa7c2\n[image-feat-theme]: https://github.com/user-attachments/assets/b47c39f1-806f-492b-8fcb-b0fa973937c1\n[image-feat-tts]: https://github.com/user-attachments/assets/50189597-2cc3-4002-b4c8-756a52ad5c0a\n[image-feat-vision]: https://github.com/user-attachments/assets/18574a1f-46c2-4cbc-af2c-35a86e128a07\n[image-feat-web-search]: https://github.com/user-attachments/assets/cfdc48ac-b5f8-4a00-acee-db8f2eba09ad\n[image-overview]: https://github.com/user-attachments/assets/dbfaa84a-2c82-4dd9-815c-5be616f264a4\n[image-star]: https://github.com/user-attachments/assets/c3b482e7-cef5-4e94-bef9-226900ecfaab\n[issues-link]: https://img.shields.io/github/issues/lobehub/lobe-chat.svg?style=flat\n[lobe-chat-plugins]: https://github.com/lobehub/lobe-chat-plugins\n[lobe-commit]: https://github.com/lobehub/lobe-commit/tree/master/packages/lobe-commit\n[lobe-i18n]: https://github.com/lobehub/lobe-commit/tree/master/packages/lobe-i18n\n[lobe-icons-github]: https://github.com/lobehub/lobe-icons\n[lobe-icons-link]: https://www.npmjs.com/package/@lobehub/icons\n[lobe-icons-shield]: https://img.shields.io/npm/v/@lobehub/icons?color=369eff&labelColor=black&logo=npm&logoColor=white&style=flat-square\n[lobe-lint-github]: https://github.com/lobehub/lobe-lint\n[lobe-lint-link]: https://www.npmjs.com/package/@lobehub/lint\n[lobe-lint-shield]: https://img.shields.io/npm/v/@lobehub/lint?color=369eff&labelColor=black&logo=npm&logoColor=white&style=flat-square\n[lobe-midjourney-webui]: https://github.com/lobehub/lobe-midjourney-webui\n[lobe-theme]: https://github.com/lobehub/sd-webui-lobe-theme\n[lobe-tts-github]: https://github.com/lobehub/lobe-tts\n[lobe-tts-link]: https://www.npmjs.com/package/@lobehub/tts\n[lobe-tts-shield]: https://img.shields.io/npm/v/@lobehub/tts?color=369eff&labelColor=black&logo=npm&logoColor=white&style=flat-square\n[lobe-ui-github]: https://github.com/lobehub/lobe-ui\n[lobe-ui-link]: https://www.npmjs.com/package/@lobehub/ui\n[lobe-ui-shield]: https://img.shields.io/npm/v/@lobehub/ui?color=369eff&labelColor=black&logo=npm&logoColor=white&style=flat-square\n[official-site]: https://lobehub.com\n[pr-welcome-link]: https://github.com/lobehub/lobe-chat/pulls\n[pr-welcome-shield]: https://img.shields.io/badge/ğŸ¤¯_pr_welcome-%E2%86%92-ffcb47?labelColor=black&style=for-the-badge\n[profile-link]: https://github.com/lobehub\n[share-linkedin-link]: https://linkedin.com/feed\n[share-linkedin-shield]: https://img.shields.io/badge/-share%20on%20linkedin-black?labelColor=black&logo=linkedin&logoColor=white&style=flat-square\n[share-mastodon-link]: https://mastodon.social/share?text=Check%20this%20GitHub%20repository%20out%20%F0%9F%A4%AF%20LobeChat%20-%20An%20open-source,%20extensible%20%28Function%20Calling%29,%20high-performance%20chatbot%20framework.%20It%20supports%20one-click%20free%20deployment%20of%20your%20private%20ChatGPT%2FLLM%20web%20application.%20https://github.com/lobehub/lobe-chat%20#chatbot%20#chatGPT%20#openAI\n[share-mastodon-shield]: https://img.shields.io/badge/-share%20on%20mastodon-black?labelColor=black&logo=mastodon&logoColor=white&style=flat-square\n[share-reddit-link]: https://www.reddit.com/submit?title=Check%20this%20GitHub%20repository%20out%20%F0%9F%A4%AF%20LobeChat%20-%20An%20open-source%2C%20extensible%20%28Function%20Calling%29%2C%20high-performance%20chatbot%20framework.%20It%20supports%20one-click%20free%20deployment%20of%20your%20private%20ChatGPT%2FLLM%20web%20application.%20%23chatbot%20%23chatGPT%20%23openAI&url=https%3A%2F%2Fgithub.com%2Flobehub%2Flobe-chat\n[share-reddit-shield]: https://img.shields.io/badge/-share%20on%20reddit-black?labelColor=black&logo=reddit&logoColor=white&style=flat-square\n[share-telegram-link]: https://t.me/share/url"?text=Check%20this%20GitHub%20repository%20out%20%F0%9F%A4%AF%20LobeChat%20-%20An%20open-source%2C%20extensible%20%28Function%20Calling%29%2C%20high-performance%20chatbot%20framework.%20It%20supports%20one-click%20free%20deployment%20of%20your%20private%20ChatGPT%2FLLM%20web%20application.%20%23chatbot%20%23chatGPT%20%23openAI&url=https%3A%2F%2Fgithub.com%2Flobehub%2Flobe-chat\n[share-telegram-shield]: https://img.shields.io/badge/-share%20on%20telegram-black?labelColor=black&logo=telegram&logoColor=white&style=flat-square\n[share-weibo-link]: http://service.weibo.com/share/share.php?sharesource=weibo&title=Check%20this%20GitHub%20repository%20out%20%F0%9F%A4%AF%20LobeChat%20-%20An%20open-source%2C%20extensible%20%28Function%20Calling%29%2C%20high-performance%20chatbot%20framework.%20It%20supports%20one-click%20free%20deployment%20of%20your%20private%20ChatGPT%2FLLM%20web%20application.%20%23chatbot%20%23chatGPT%20%23openAI&url=https%3A%2F%2Fgithub.com%2Flobehub%2Flobe-chat\n[share-weibo-shield]: https://img.shields.io/badge/-share%20on%20weibo-black?labelColor=black&logo=sinaweibo&logoColor=white&style=flat-square\n[share-whatsapp-link]: https://api.whatsapp.com/send?text=Check%20this%20GitHub%20repository%20out%20%F0%9F%A4%AF%20LobeChat%20-%20An%20open-source%2C%20extensible%20%28Function%20Calling%29%2C%20high-performance%20chatbot%20framework.%20It%20supports%20one-click%20free%20deployment%20of%20your%20private%20ChatGPT%2FLLM%20web%20application.%20https%3A%2F%2Fgithub.com%2Flobehub%2Flobe-chat%20%23chatbot%20%23chatGPT%20%23openAI\n[share-whatsapp-shield]: https://img.shields.io/badge/-share%20on%20whatsapp-black?labelColor=black&logo=whatsapp&logoColor=white&style=flat-square\n[share-x-link]: https://x.com/intent/tweet?hashtags=chatbot%2CchatGPT%2CopenAI&text=Check%20this%20GitHub%20repository%20out%20%F0%9F%A4%AF%20LobeChat%20-%20An%20open-source%2C%20extensible%20%28Function%20Calling%29%2C%20high-performance%20chatbot%20framework.%20It%20supports%20one-click%20free%20deployment%20of%20your%20private%20ChatGPT%2FLLM%20web%20application.&url=https%3A%2F%2Fgithub.com%2Flobehub%2Flobe-chat\n[share-x-shield]: https://img.shields.io/badge/-share%20on%20x-black?labelColor=black&logo=x&logoColor=white&style=flat-square\n[sponsor-link]: https://opencollective.com/lobehub ''Become â¤ï¸ LobeHub Sponsor''\n[sponsor-shield]: https://img.shields.io/badge/-Sponsor%20LobeHub-f04f88?logo=opencollective&logoColor=white&style=flat-square\n[submit-agents-link]: https://github.com/lobehub/lobe-chat-agents\n[submit-agents-shield]: https://img.shields.io/badge/ğŸ¤–/ğŸª_submit_agent-%E2%86%92-c4f042?labelColor=black&style=for-the-badge\n[submit-plugin-link]: https://github.com/lobehub/lobe-chat-plugins\n[submit-plugin-shield]: https://img.shields.io/badge/ğŸ§©/ğŸª_submit_plugin-%E2%86%92-95f3d9?labelColor=black&style=for-the-badge\n[vercel-link]: https://chat-preview.lobehub.com\n[vercel-shield]: https://img.shields.io/badge/vercel-online-55b467?labelColor=black&logo=vercel&style=flat-square\n[vercel-shield-badge]: https://img.shields.io/badge/TRY%20LOBECHAT-ONLINE-55b467?labelColor=black&logo=vercel&style=for-the-badge\n', '{"language":"TypeScript","stars":68749,"forks":14181,"watchers":68749,"open_issues":1058,"topics":["agent","ai","artifacts","chat","chatgpt","claude","deepseek","deepseek-r1","function-calling","gemini","gpt","knowledge-base","mcp","nextjs","ollama","openai","rag"],"default_branch":"next","size_kb":585695,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:lobehub:lobe-chat.git","source_url":"https://github.com/lobehub/lobe-chat.git"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:lobehub:.github","source_url":"https://github.com/lobehub/.github"},{"type":"has_code","target_id":"github:lobehub:.github","source_url":"https://github.com/lobehub/.github"},{"type":"has_code","target_id":"github:lobehub:chat-plugin-sdk","source_url":"https://github.com/lobehub/chat-plugin-sdk"},{"type":"has_code","target_id":"github:lobehub:chat-plugin-template","source_url":"https://github.com/lobehub/chat-plugin-template"},{"type":"has_code","target_id":"github:lobehub:chat-plugins-gateway","source_url":"https://github.com/lobehub/chat-plugins-gateway"},{"type":"has_code","target_id":"github:codespaces:badge.svg","source_url":"https://github.com/codespaces/badge.svg"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:actions:workflows","source_url":"https://github.com/actions/workflows"},{"type":"has_code","target_id":"github:actions:workflows","source_url":"https://github.com/actions/workflows"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:lobehub:lobe-chat-plugins","source_url":"https://github.com/lobehub/lobe-chat-plugins"},{"type":"has_code","target_id":"github:lobehub:lobe-commit","source_url":"https://github.com/lobehub/lobe-commit"},{"type":"has_code","target_id":"github:lobehub:lobe-commit","source_url":"https://github.com/lobehub/lobe-commit"},{"type":"has_code","target_id":"github:lobehub:lobe-icons","source_url":"https://github.com/lobehub/lobe-icons"},{"type":"has_code","target_id":"github:lobehub:lobe-lint","source_url":"https://github.com/lobehub/lobe-lint"},{"type":"has_code","target_id":"github:lobehub:lobe-midjourney-webui","source_url":"https://github.com/lobehub/lobe-midjourney-webui"},{"type":"has_code","target_id":"github:lobehub:sd-webui-lobe-theme","source_url":"https://github.com/lobehub/sd-webui-lobe-theme"},{"type":"has_code","target_id":"github:lobehub:lobe-tts","source_url":"https://github.com/lobehub/lobe-tts"},{"type":"has_code","target_id":"github:lobehub:lobe-ui","source_url":"https://github.com/lobehub/lobe-ui"},{"type":"has_code","target_id":"github:lobehub:lobe-chat","source_url":"https://github.com/lobehub/lobe-chat"},{"type":"has_code","target_id":"github:lobehub:lobe-chat%20","source_url":"https://github.com/lobehub/lobe-chat%20#chatbot%20#chatGPT%20#openAI"},{"type":"has_code","target_id":"github:lobehub:lobe-chat-agents","source_url":"https://github.com/lobehub/lobe-chat-agents"},{"type":"has_code","target_id":"github:lobehub:lobe-chat-plugins","source_url":"https://github.com/lobehub/lobe-chat-plugins"}]', NULL, 'NOASSERTION', 'approved', 80, '7f944629051a27d1bedb99ea72287649', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-lobehub-lobe-chat from https://github.com/lobehub.png
Image converted to WebP: data/images/github-lobehub-lobe-chat.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-hiyouga-LLaMA-Factory', 'github--hiyouga--llama-factory', 'LLaMA-Factory', 'hiyouga', '!# LLaMA Factory <div align="center" markdown="1"> | <div style="text-align: center;"><a href="https://warp.dev/llama-factory"><img alt="Warp sponsorship" width="400" src="assets/sponsors/warp.jpg"></a><br><a href="https://warp.dev/llama-factory" style="font-size:larger;">Warp, the agentic terminal for developers</a><br><a href="https://warp.dev/llama-factory">Available for MacOS, Linux, & Windows</a> | <a href="https://serpapi.com"><img alt="SerpAPI sponsorship" width="250" src="assets/spons...', '["agent","ai","deepseek","fine-tuning","gemma","gpt","instruction-tuning","large-language-models","llama","llama3","llm","lora","moe","nlp","peft","qlora","quantization","qwen","rlhf","transformers","python"]', 'other', 63676, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/hiyouga/LLaMA-Factory","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '![# LLaMA Factory](assets/logo.png)\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social)](https://github.com/hiyouga/LLaMA-Factory/stargazers)\n[![GitHub last commit](https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory)](https://github.com/hiyouga/LLaMA-Factory/commits/main)\n[![GitHub contributors](https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange)](https://github.com/hiyouga/LLaMA-Factory/graphs/contributors)\n[![GitHub workflow](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg)](https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml)\n[![PyPI](https://img.shields.io/pypi/v/llamafactory)](https://pypi.org/project/llamafactory/)\n[![Citation](https://img.shields.io/badge/citation-1000+-green)](https://scholar.google.com/scholar?cites=12620864006390196564)\n[![Docker Pulls](https://img.shields.io/docker/pulls/hiyouga/llamafactory)](https://hub.docker.com/r/hiyouga/llamafactory/tags)\n\n[![Twitter](https://img.shields.io/twitter/follow/llamafactory_ai)](https://twitter.com/llamafactory_ai)\n[![Discord](assets/thirdparty/discord.svg)](https://discord.gg/rKfvV9r9FK)\n[![WeChat](https://img.shields.io/badge/WeChat-User%20Group-blue?logo=wechat)](https://github.com/hiyouga/llamafactory-community)\n[![Blog](https://img.shields.io/badge/Hugo-Official%20Blog-blue?logo=hugo)](https://blog.llamafactory.net/en/)\n\n[![Open in Colab](assets/thirdparty/colab.svg)](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)\n[![Open in DSW](assets/thirdparty/dsw.svg)](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory)\n[![Open in Lab4ai](assets/thirdparty/lab4ai.svg)](https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&utm_source=LLaMA-Factory)\n[![Open in Online](assets/thirdparty/online.svg)](https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory)\n[![Open in Spaces](https://img.shields.io/badge/ğŸ¤—-Open%20in%20Spaces-blue)](https://huggingface.co/spaces/hiyouga/LLaMA-Board)\n[![Open in Studios](https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue)](https://modelscope.cn/studios/hiyouga/LLaMA-Board)\n[![Open in Novita](https://img.shields.io/badge/Novita-Deploy%20Template-blue)](https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47)\n\n### Used by [Amazon](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/), [NVIDIA](https://developer.nvidia.com/rtx/ai-toolkit), [Aliyun](https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory), etc.\n\n<div align="center" markdown="1">\n\n### Supporters â¤ï¸\n\n| <div style="text-align: center;"><a href="https://warp.dev/llama-factory"><img alt="Warp sponsorship" width="400" src="assets/sponsors/warp.jpg"></a><br><a href="https://warp.dev/llama-factory" style="font-size:larger;">Warp, the agentic terminal for developers</a><br><a href="https://warp.dev/llama-factory">Available for MacOS, Linux, & Windows</a> | <a href="https://serpapi.com"><img alt="SerpAPI sponsorship" width="250" src="assets/sponsors/serpapi.svg"> </a> |\n| ---- | ---- |\n\n----\n\n### Easily fine-tune 100+ large language models with zero-code [CLI](#quickstart) and [Web UI](#fine-tuning-with-llama-board-gui-powered-by-gradio)\n\n![GitHub Trend](https://trendshift.io/api/badge/repositories/4535)\n\n</div>\n\nğŸ‘‹ Join our [WeChat](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/main.jpg), [NPU](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/npu.jpg), [Lab4AI](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/lab4ai.jpg), [LLaMA Factory Online](https://github.com/hiyouga/llamafactory-community/blob/main/wechat/online.jpg) user group.\n\n\[ English | [ä¸­æ–‡](README_zh.md) \]\n\n**Fine-tuning a large language model can be easy as...**\n\nhttps://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e\n\nStart local training:\n- Please refer to [usage](#getting-started)\n\nStart cloud training:\n- **Colab (free)**: https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing\n- **PAI-DSW (free trial)**: https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory\n- **LLaMA Factory Online**: https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory\n- **Alaya NeW (cloud GPU deal)**: https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory\n\nRead technical notes:\n- **Documentation (WIP)**: https://llamafactory.readthedocs.io/en/latest/\n- **Documentation (AMD GPU)**: https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html\n- **Official Blog**: https://blog.llamafactory.net/en/\n- **Official Course**: https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&utm_source=LLaMA-Factory\n\n> [!NOTE]\n> Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.\n\n## Table of Contents\n\n- [Features](#features)\n- [Blogs](#blogs)\n- [Changelog](#changelog)\n- [Supported Models](#supported-models)\n- [Supported Training Approaches](#supported-training-approaches)\n- [Provided Datasets](#provided-datasets)\n- [Requirement](#requirement)\n- [Getting Started](#getting-started)\n  - [Installation](#installation)\n  - [Data Preparation](#data-preparation)\n  - [Quickstart](#quickstart)\n  - [Fine-Tuning with LLaMA Board GUI](#fine-tuning-with-llama-board-gui-powered-by-gradio)\n  - [LLaMA Factory Online](#llama-factory-online)\n  - [Build Docker](#build-docker)\n  - [Deploy with OpenAI-style API and vLLM](#deploy-with-openai-style-api-and-vllm)\n  - [Download from ModelScope Hub](#download-from-modelscope-hub)\n  - [Download from Modelers Hub](#download-from-modelers-hub)\n  - [Use W&B Logger](#use-wb-logger)\n  - [Use SwanLab Logger](#use-swanlab-logger)\n- [Projects using LLaMA Factory](#projects-using-llama-factory)\n- [License](#license)\n- [Citation](#citation)\n- [Acknowledgement](#acknowledgement)\n\n## Features\n\n- **Various models**: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.\n- **Integrated methods**: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.\n- **Scalable resources**: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.\n- **Advanced algorithms**: [GaLore](https://github.com/jiaweizzhao/GaLore), [BAdam](https://github.com/Ledzy/BAdam), [APOLLO](https://github.com/zhuhanqing/APOLLO), [Adam-mini](https://github.com/zyushun/Adam-mini), [Muon](https://github.com/KellerJordan/Muon), [OFT](https://github.com/huggingface/peft/tree/main/src/peft/tuners/oft), DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.\n- **Practical tricks**: [FlashAttention-2](https://github.com/Dao-AILab/flash-attention), [Unsloth](https://github.com/unslothai/unsloth), [Liger Kernel](https://github.com/linkedin/Liger-Kernel), [KTransformers](https://github.com/kvcache-ai/ktransformers/), RoPE scaling, NEFTune and rsLoRA.\n- **Wide tasks**: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.\n- **Experiment monitors**: LlamaBoard, TensorBoard, Wandb, MLflow, [SwanLab](https://github.com/SwanHubX/SwanLab), etc.\n- **Faster inference**: OpenAI-style API, Gradio UI and CLI with [vLLM worker](https://github.com/vllm-project/vllm) or [SGLang worker](https://github.com/sgl-project/sglang).\n\n### Day-N Support for Fine-Tuning Cutting-Edge Models\n\n| Support Date | Model Name                                                           |\n| ------------ | -------------------------------------------------------------------- |\n| Day 0        | Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6 |\n| Day 1        | Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4               |\n\n## Blogs\n\n> [!TIP]\n> Now we have a dedicated blog for LLaMA Factory!\n>\n> Website: https://blog.llamafactory.net/en/\n\n- ğŸ’¡ [KTransformers Fine-Tuning Ã— LLaMA Factory: Fine-tuning 1000 Billion models with 2 4090-GPU + CPU](https://blog.llamafactory.net/en/posts/ktransformers/) (English)\n- ğŸ’¡ [Easy Dataset Ã— LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge](https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g) (English)\n- [Fine-tune a mental health LLM using LLaMA-Factory](https://www.lab4ai.cn/project/detail?id=25cce32ec131497b9e06a93336a0817f&type=project&utm_source=LLaMA-Factory) (Chinese)\n- [Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory](https://docs.llamafactory.com.cn/docs/documents/best-practice/gptroleplay/?utm_source=LLaMA-Factory) (Chinese)\n- [A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1](https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/) (Chinese)\n- [How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod](https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/) (English)\n\n<details><summary>All Blogs</summary>\n\n- [Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory](https://docs.alayanew.com/docs/documents/bestPractice/bigModel/llama70B/?utm_source=LLaMA-Factory) (Chinese)\n- [Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory](https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory) (Chinese)\n- [LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b) (Chinese)\n- [A One-Stop Code-Free Model Fine-Tuning \& Deployment Platform based on SageMaker and LLaMA-Factory](https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/) (Chinese)\n- [LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl) (Chinese)\n- [LLaMA Factory: Fine-tuning Llama3 for Role-Playing](https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory) (Chinese)\n\n</details>\n\n## Changelog\n\n[25/10/26] We support Megatron-core training backend with [**mcore_adapter**](https://github.com/alibaba/ROLL/tree/main/mcore_adapter). See [PR #9237](https://github.com/hiyouga/LLaMA-Factory/pull/9237) to get started.\n\n[25/08/22] We supported **[OFT](https://arxiv.org/abs/2306.07280)** and **[OFTv2](https://arxiv.org/abs/2506.19847)**. See [examples](examples/README.md) for usage.\n\n[25/08/20] We supported fine-tuning the **[Intern-S1-mini](https://huggingface.co/internlm/Intern-S1-mini)** models. See [PR #8976](https://github.com/hiyouga/LLaMA-Factory/pull/8976) to get started.\n\n[25/08/06] We supported fine-tuning the **[GPT-OSS](https://github.com/openai/gpt-oss)** models. See [PR #8826](https://github.com/hiyouga/LLaMA-Factory/pull/8826) to get started.\n\n<details><summary>Full Changelog</summary>\n\n[25/07/02] We supported fine-tuning the **[GLM-4.1V-9B-Thinking](https://github.com/THUDM/GLM-4.1V-Thinking)** model.\n\n[25/04/28] We supported fine-tuning the **[Qwen3](https://qwenlm.github.io/blog/qwen3/)** model family.\n\n[25/04/21] We supported the **[Muon](https://github.com/KellerJordan/Muon)** optimizer. See [examples](examples/README.md) for usage. Thank [@tianshijing](https://github.com/tianshijing)''s PR.\n\n[25/04/16] We supported fine-tuning the **[InternVL3](https://huggingface.co/OpenGVLab/InternVL3-8B)** model. See [PR #7258](https://github.com/hiyouga/LLaMA-Factory/pull/7258) to get started.\n\n[25/04/14] We supported fine-tuning the **[GLM-Z1](https://huggingface.co/THUDM/GLM-Z1-9B-0414)** and **[Kimi-VL](https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct)** models.\n\n[25/04/06] We supported fine-tuning the **[Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)** model. See [PR #7611](https://github.com/hiyouga/LLaMA-Factory/pull/7611) to get started.\n\n[25/03/31] We supported fine-tuning the **[Qwen2.5 Omni](https://qwenlm.github.io/blog/qwen2.5-omni/)** model. See [PR #7537](https://github.com/hiyouga/LLaMA-Factory/pull/7537) to get started.\n\n[25/03/15] We supported **[SGLang](https://github.com/sgl-project/sglang)** as inference backend. Try `infer_backend: sglang` to accelerate inference.\n\n[25/03/12] We supported fine-tuning the **[Gemma 3](https://huggingface.co/blog/gemma3)** model.\n\n[25/02/24] Announcing **[EasyR1](https://github.com/hiyouga/EasyR1)**, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.\n\n[25/02/11] We supported saving the **[Ollama](https://github.com/ollama/ollama)** modelfile when exporting the model checkpoints. See [examples](examples/README.md) for usage.\n\n[25/02/05] We supported fine-tuning the **[Qwen2-Audio](Qwen/Qwen2-Audio-7B-Instruct)** and **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** on audio understanding tasks.\n\n[25/01/31] We supported fine-tuning the **[DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1)** and **[Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)** models.\n\n[25/01/15] We supported **[APOLLO](https://arxiv.org/abs/2412.05270)** optimizer. See [examples](examples/README.md) for usage.\n\n[25/01/14] We supported fine-tuning the **[MiniCPM-o-2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)** and **[MiniCPM-V-2.6](https://huggingface.co/openbmb/MiniCPM-V-2_6)** models. Thank [@BUAADreamer](https://github.com/BUAADreamer)''s PR.\n\n[25/01/14] We supported fine-tuning the **[InternLM 3](https://huggingface.co/collections/internlm/)** models. Thank [@hhaAndroid](https://github.com/hhaAndroid)''s PR.\n\n[25/01/10] We supported fine-tuning the **[Phi-4](https://huggingface.co/microsoft/phi-4)** model.\n\n[24/12/21] We supported using **[SwanLab](https://github.com/SwanHubX/SwanLab)** for experiment tracking and visualization. See [this section](#use-swanlab-logger) for details.\n\n[24/11/27] We supported fine-tuning the **[Skywork-o1](https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B)** model and the **[OpenO1](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)** dataset.\n\n[24/10/09] We supported downloading pre-trained models and datasets from the **[Modelers Hub](https://modelers.cn/models)**. See [this tutorial](#download-from-modelers-hub) for usage.\n\n[24/09/19] We supported fine-tuning the **[Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/)** models.\n\n[24/08/30] We supported fine-tuning the **[Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/)** models. Thank [@simonJJJ](https://github.com/simonJJJ)''s PR.\n\n[24/08/27] We supported **[Liger Kernel](https://github.com/linkedin/Liger-Kernel)**. Try `enable_liger_kernel: true` for efficient training.\n\n[24/08/09] We supported **[Adam-mini](https://github.com/zyushun/Adam-mini)** optimizer. See [examples](examples/README.md) for usage. Thank [@relic-yuexi](https://github.com/relic-yuexi)''s PR.\n\n[24/07/04] We supported [contamination-free packed training](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing). Use `neat_packing: true` to activate it. Thank [@chuan298](https://github.com/chuan298)''s PR.\n\n[24/06/16] We supported **[PiSSA](https://arxiv.org/abs/2404.02948)** algorithm. See [examples](examples/README.md) for usage.\n\n[24/06/07] We supported fine-tuning the **[Qwen2](https://qwenlm.github.io/blog/qwen2/)** and **[GLM-4](https://github.com/THUDM/GLM-4)** models.\n\n[24/05/26] We supported **[SimPO](https://arxiv.org/abs/2405.14734)** algorithm for preference learning. See [examples](examples/README.md) for usage.\n\n[24/05/20] We supported fine-tuning the **PaliGemma** series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with `paligemma` template for chat completion.\n\n[24/05/18] We supported **[KTO](https://arxiv.org/abs/2402.01306)** algorithm for preference learning. See [examples](examples/README.md) for usage.\n\n[24/05/14] We supported training and inference on the Ascend NPU devices. Check [installation](#installation) section for details.\n\n[24/04/26] We supported fine-tuning the **LLaVA-1.5** multimodal LLMs. See [examples](examples/README.md) for usage.\n\n[24/04/22] We provided a **[Colab notebook](https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing)** for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check [Llama3-8B-Chinese-Chat](https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat) and [Llama3-Chinese](https://huggingface.co/zhichen/Llama3-Chinese) for details.\n\n[24/04/21] We supported **[Mixture-of-Depths](https://arxiv.org/abs/2404.02258)** according to [AstraMindAI''s implementation](https://github.com/astramind-ai/Mixture-of-depths). See [examples](examples/README.md) for usage.\n\n[24/04/16] We supported **[BAdam](https://arxiv.org/abs/2404.02827)** optimizer. See [examples](examples/README.md) for usage.\n\n[24/04/16] We supported **[unsloth](https://github.com/unslothai/unsloth)**''s long-sequence training (Llama-2-7B-56k within 24GB). It achieves **117%** speed and **50%** memory compared with FlashAttention-2, more benchmarks can be found in [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison).\n\n[24/03/31] We supported **[ORPO](https://arxiv.org/abs/2403.07691)**. See [examples](examples/README.md) for usage.\n\n[24/03/21] Our paper "[LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models](https://arxiv.org/abs/2403.13372)" is available at arXiv!\n\n[24/03/20] We supported **FSDP+QLoRA** that fine-tunes a 70B model on 2x24GB GPUs. See [examples](examples/README.md) for usage.\n\n[24/03/13] We supported **[LoRA+](https://arxiv.org/abs/2402.12354)**. See [examples](examples/README.md) for usage.\n\n[24/03/07] We supported **[GaLore](https://arxiv.org/abs/2403.03507)** optimizer. See [examples](examples/README.md) for usage.\n\n[24/03/07] We integrated **[vLLM](https://github.com/vllm-project/vllm)** for faster and concurrent inference. Try `infer_backend: vllm` to enjoy **270%** inference speed.\n\n[24/02/28] We supported weight-decomposed LoRA (**[DoRA](https://arxiv.org/abs/2402.09353)**). Try `use_dora: true` to activate DoRA training.\n\n[24/02/15] We supported **block expansion** proposed by [LLaMA Pro](https://github.com/TencentARC/LLaMA-Pro). See [examples](examples/README.md) for usage.\n\n[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this [blog post](https://qwenlm.github.io/blog/qwen1.5/) for details.\n\n[24/01/18] We supported **agent tuning** for most models, equipping model with tool using abilities by fine-tuning with `dataset: glaive_toolcall_en`.\n\n[23/12/23] We supported **[unsloth](https://github.com/unslothai/unsloth)**''s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try `use_unsloth: true` argument to activate unsloth patch. It achieves **170%** speed in our benchmark, check [this page](https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison) for details.\n\n[23/12/12] We supported fine-tuning the latest MoE model **[Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)** in our framework. See hardware requirement [here](#hardware-requirement).\n\n[23/12/01] We supported downloading pre-trained models and datasets from the **[ModelScope Hub](https://modelscope.cn/models)**. See [this tutorial](#download-from-modelscope-hub) for usage.\n\n[23/10/21] We supported **[NEFTune](https://arxiv.org/abs/2310.05914)** trick for fine-tuning. Try `neftune_noise_alpha: 5` argument to activate NEFTune.\n\n[23/09/27] We supported **$S^2$-Attn** proposed by [LongLoRA](https://github.com/dvlab-research/LongLoRA) for the LLaMA models. Try `shift_attn: true` argument to enable shift short attention.\n\n[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See [examples](examples/README.md) for usage.\n\n[23/09/10] We supported **[FlashAttention-2](https://github.com/Dao-AILab/flash-attention)**. Try `flash_attn: fa2` argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.\n\n[23/08/12] We supported **RoPE scaling** to extend the context length of the LLaMA models. Try `rope_scaling: linear` argument in training and `rope_scaling: dynamic` argument at inference to extrapolate the position embeddings.\n\n[23/08/11] We supported **[DPO training](https://arxiv.org/abs/2305.18290)** for instruction-tuned models. See [examples](examples/README.md) for usage.\n\n[23/07/31] We supported **dataset streaming**. Try `streaming: true` and `max_steps: 10000` arguments to load your dataset in streaming mode.\n\n[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos ([LLaMA-2](https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat) / [Baichuan](https://huggingface.co/hiyouga/Baichuan-13B-sft)) for details.\n\n[23/07/18] We developed an **all-in-one Web UI** for training, evaluation and inference. Try `train_web.py` to fine-tune models in your Web browser. Thank [@KanadeSiina](https://github.com/KanadeSiina) and [@codemayq](https://github.com/codemayq) for their efforts in the development.\n\n[23/07/09] We released **[FastEdit](https://github.com/hiyouga/FastEdit)** âš¡ğŸ©¹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow [FastEdit](https://github.com/hiyouga/FastEdit) if you are interested.\n\n[23/06/29] We provided a **reproducible example** of training a chat model using instruction-following datasets, see [Baichuan-7B-sft](https://huggingface.co/hiyouga/Baichuan-7B-sft) for details.\n\n[23/06/22] We aligned the [demo API](src/api_demo.py) with the [OpenAI''s](https://platform.openai.com/docs/api-reference/chat) format where you can insert the fine-tuned model in **arbitrary ChatGPT-based applications**.\n\n[23/06/03] We supported quantized training and inference (aka **[QLoRA](https://github.com/artidoro/qlora)**). See [examples](examples/README.md) for usage.\n\n</details>\n\n> [!TIP]\n> If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.\n\n## Supported Models\n\n| Model                                                             | Model size                       | Template             |\n| ----------------------------------------------------------------- | -------------------------------- | -------------------- |\n| [Baichuan 2](https://huggingface.co/baichuan-inc)                 | 7B/13B                           | baichuan2            |\n| [BLOOM/BLOOMZ](https://huggingface.co/bigscience)                 | 560M/1.1B/1.7B/3B/7.1B/176B      | -                    |\n| [ChatGLM3](https://huggingface.co/THUDM)                          | 6B                               | chatglm3             |\n| [Command R](https://huggingface.co/CohereForAI)                   | 35B/104B                         | cohere               |\n| [DeepSeek (Code/MoE)](https://huggingface.co/deepseek-ai)         | 7B/16B/67B/236B                  | deepseek             |\n| [DeepSeek 2.5/3](https://huggingface.co/deepseek-ai)              | 236B/671B                        | deepseek3            |\n| [DeepSeek R1 (Distill)](https://huggingface.co/deepseek-ai)       | 1.5B/7B/8B/14B/32B/70B/671B      | deepseekr1           |\n| [ERNIE-4.5](https://huggingface.co/baidu)                         | 0.3B/21B/300B                    | ernie/ernie_nothink  |\n| [Falcon](https://huggingface.co/tiiuae)                           | 7B/11B/40B/180B                  | falcon               |\n| [Falcon-H1](https://huggingface.co/tiiuae)                        | 0.5B/1.5B/3B/7B/34B              | falcon_h1            |\n| [Gemma/Gemma 2/CodeGemma](https://huggingface.co/google)          | 2B/7B/9B/27B                     | gemma/gemma2         |\n| [Gemma 3/Gemma 3n](https://huggingface.co/google)                 | 270M/1B/4B/6B/8B/12B/27B         | gemma3/gemma3n       |\n| [GLM-4/GLM-4-0414/GLM-Z1](https://huggingface.co/zai-org)         | 9B/32B                           | glm4/glmz1           |\n| [GLM-4.1V](https://huggingface.co/zai-org)                        | 9B                               | glm4v                |\n| [GLM-4.5/GLM-4.5V](https://huggingface.co/zai-org)                | 106B/355B                        | glm4_moe/glm4v_moe   |\n| [GPT-2](https://huggingface.co/openai-community)                  | 0.1B/0.4B/0.8B/1.5B              | -                    |\n| [GPT-OSS](https://huggingface.co/openai)                          | 20B/120B                         | gpt                  |\n| [Granite 3.0-3.3](https://huggingface.co/ibm-granite)             | 1B/2B/3B/8B                      | granite3             |\n| [Granite 4](https://huggingface.co/ibm-granite)                   | 7B                               | granite4             |\n| [Hunyuan (MT)](https://huggingface.co/tencent/)                   | 7B                               | hunyuan              |\n| [Index](https://huggingface.co/IndexTeam)                         | 1.9B                             | index                |\n| [InternLM 2-3](https://huggingface.co/internlm)                   | 7B/8B/20B                        | intern2              |\n| [InternVL 2.5-3.5](https://huggingface.co/OpenGVLab)              | 1B/2B/4B/8B/14B/30B/38B/78B/241B | intern_vl            |\n| [InternLM/Intern-S1-mini](https://huggingface.co/internlm/)       | 8B                               | intern_s1            |\n| [Kimi-VL](https://huggingface.co/moonshotai)                      | 16B                              | kimi_vl              |\n| [Ling 2.0 (mini/flash)](https://huggingface.co/inclusionAI)       | 16B/100B                         | bailing_v2           |\n| [Llama](https://github.com/facebookresearch/llama)                | 7B/13B/33B/65B                   | -                    |\n| [Llama 2](https://huggingface.co/meta-llama)                      | 7B/13B/70B                       | llama2               |\n| [Llama 3-3.3](https://huggingface.co/meta-llama)                  | 1B/3B/8B/70B                     | llama3               |\n| [Llama 4](https://huggingface.co/meta-llama)                      | 109B/402B                        | llama4               |\n| [Llama 3.2 Vision](https://huggingface.co/meta-llama)             | 11B/90B                          | mllama               |\n| [LLaVA-1.5](https://huggingface.co/llava-hf)                      | 7B/13B                           | llava                |\n| [LLaVA-NeXT](https://huggingface.co/llava-hf)                     | 7B/8B/13B/34B/72B/110B           | llava_next           |\n| [LLaVA-NeXT-Video](https://huggingface.co/llava-hf)               | 7B/34B                           | llava_next_video     |\n| [MiMo](https://huggingface.co/XiaomiMiMo)                         | 7B                               | mimo                 |\n| [MiniCPM 1-4.1](https://huggingface.co/openbmb)                   | 0.5B/1B/2B/4B/8B                 | cpm/cpm3/cpm4        |\n| [MiniCPM-o-2.6/MiniCPM-V-2.6](https://huggingface.co/openbmb)     | 8B                               | minicpm_o/minicpm_v  |\n| [Ministral/Mistral-Nemo](https://huggingface.co/mistralai)        | 8B/12B                           | ministral            |\n| [Mistral/Mixtral](https://huggingface.co/mistralai)               | 7B/8x7B/8x22B                    | mistral              |\n| [Mistral Small](https://huggingface.co/mistralai)                 | 24B                              | mistral_small        |\n| [OLMo](https://huggingface.co/allenai)                            | 1B/7B                            | -                    |\n| [PaliGemma/PaliGemma2](https://huggingface.co/google)             | 3B/10B/28B                       | paligemma            |\n| [Phi-1.5/Phi-2](https://huggingface.co/microsoft)                 | 1.3B/2.7B                        | -                    |\n| [Phi-3/Phi-3.5](https://huggingface.co/microsoft)                 | 4B/14B                           | phi                  |\n| [Phi-3-small](https://huggingface.co/microsoft)                   | 7B                               | phi_small            |\n| [Phi-4](https://huggingface.co/microsoft)                         | 14B                              | phi4                 |\n| [Pixtral](https://huggingface.co/mistralai)                       | 12B                              | pixtral              |\n| [Qwen (1-2.5) (Code/Math/MoE/QwQ)](https://huggingface.co/Qwen)   | 0.5B/1.5B/3B/7B/14B/32B/72B/110B | qwen                 |\n| [Qwen3 (MoE/Instruct/Thinking/Next)](https://huggingface.co/Qwen) | 0.6B/1.7B/4B/8B/14B/32B/80B/235B | qwen3/qwen3_nothink  |\n| [Qwen2-Audio](https://huggingface.co/Qwen)                        | 7B                               | qwen2_audio          |\n| [Qwen2.5-Omni](https://huggingface.co/Qwen)                       | 3B/7B                            | qwen2_omni           |\n| [Qwen3-Omni](https://huggingface.co/Qwen)                         | 30B                              | qwen3_omni           |\n| [Qwen2-VL/Qwen2.5-VL/QVQ](https://huggingface.co/Qwen)            | 2B/3B/7B/32B/72B                 | qwen2_vl             |\n| [Qwen3-VL](https://huggingface.co/Qwen)                           | 2B/4B/8B/30B/32B/235B            | qwen3_vl             |\n| [Seed (OSS/Coder)](https://huggingface.co/ByteDance-Seed)         | 8B/36B                           | seed_oss/seed_coder  |\n| [Skywork o1](https://huggingface.co/Skywork)                      | 8B                               | skywork_o1           |\n| [StarCoder 2](https://huggingface.co/bigcode)                     | 3B/7B/15B                        | -                    |\n| [TeleChat2](https://huggingface.co/Tele-AI)                       | 3B/7B/35B/115B                   | telechat2            |\n| [XVERSE](https://huggingface.co/xverse)                           | 7B/13B/65B                       | xverse               |\n| [Yi/Yi-1.5 (Code)](https://huggingface.co/01-ai)                  | 1.5B/6B/9B/34B                   | yi                   |\n| [Yi-VL](https://huggingface.co/01-ai)                             | 6B/34B                           | yi_vl                |\n| [Yuan 2](https://huggingface.co/IEITYuan)                         | 2B/51B/102B                      | yuan                 |\n\n> [!NOTE]\n> For the "base" models, the `template` argument can be chosen from `default`, `alpaca`, `vicuna` etc. But make sure to use the **corresponding template** for the "instruct/chat" models.\n>\n> If the model has both reasoning and non-reasoning versions, please use the `_nothink` suffix to distinguish between them. For example, `qwen3` and `qwen3_nothink`.\n>\n> Remember to use the **SAME** template in training and inference.\n>\n> \*: You should install the `transformers` from main branch and use `DISABLE_VERSION_CHECK=1` to skip version check.\n>\n> \*\*: You need to install a specific version of `transformers` to use the corresponding model.\n\nPlease refer to [constants.py](src/llamafactory/extras/constants.py) for a full list of models we supported.\n\nYou also can add a custom chat template to [template.py](src/llamafactory/data/template.py).\n\n## Supported Training Approaches\n\n| Approach               |     Full-tuning    |    Freeze-tuning   |       LoRA         |       QLoRA        |        OFT         |        QOFT        |\n| ---------------------- | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ | ------------------ |\n| Pre-Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| Supervised Fine-Tuning | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| Reward Modeling        | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| PPO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| DPO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| KTO Training           | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| ORPO Training          | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n| SimPO Training         | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |\n\n> [!TIP]\n> The implementation details of PPO can be found in [this blog](https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html).\n\n## Provided Datasets\n\n<details><summary>Pre-training datasets</summary>\n\n- [Wiki Demo (en)](data/wiki_demo.txt)\n- [RefinedWeb (en)](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)\n- [RedPajama V2 (en)](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2)\n- [Wikipedia (en)](https://huggingface.co/datasets/olm/olm-wikipedia-20221220)\n- [Wikipedia (zh)](https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered)\n- [Pile (en)](https://huggingface.co/datasets/EleutherAI/pile)\n- [SkyPile (zh)](https://huggingface.co/datasets/Skywork/SkyPile-150B)\n- [FineWeb (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb)\n- [FineWeb-Edu (en)](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu)\n- [CCI3-HQ (zh)](https://huggingface.co/datasets/BAAI/CCI3-HQ)\n- [CCI3-Data (zh)](https://huggingface.co/datasets/BAAI/CCI3-Data)\n- [CCI4.0-M2-Base-v1 (en&zh)](https://huggingface.co/datasets/BAAI/CCI4.0-M2-Base-v1)\n- [CCI4.0-M2-CoT-v1 (en&zh)](https://huggingface.co/datasets/BAAI/CCI4.0-M2-CoT-v1)\n- [CCI4.0-M2-Extra-v1 (en&zh)](https://huggingface.co/datasets/BAAI/CCI4.0-M2-Extra-v1)\n- [The Stack (en)](https://huggingface.co/datasets/bigcode/the-stack)\n- [StarCoder (en)](https://huggingface.co/datasets/bigcode/starcoderdata)\n\n</details>\n\n<details><summary>Supervised fine-tuning datasets</summary>\n\n- [Identity (en&zh)](data/identity.json)\n- [Stanford Alpaca (en)](https://github.com/tatsu-lab/stanford_alpaca)\n- [Stanford Alpaca (zh)](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3)\n- [Alpaca GPT4 (en&zh)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)\n- [Glaive Function Calling V2 (en&zh)](https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2)\n- [LIMA (en)](https://huggingface.co/datasets/GAIR/lima)\n- [Guanaco Dataset (multilingual)](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)\n- [BELLE 2M (zh)](https://huggingface.co/datasets/BelleGroup/train_2M_CN)\n- [BELLE 1M (zh)](https://huggingface.co/datasets/BelleGroup/train_1M_CN)\n- [BELLE 0.5M (zh)](https://huggingface.co/datasets/BelleGroup/train_0.5M_CN)\n- [BELLE Dialogue 0.4M (zh)](https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M)\n- [BELLE School Math 0.25M (zh)](https://huggingface.co/datasets/BelleGroup/school_math_0.25M)\n- [BELLE Multiturn Chat 0.8M (zh)](https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M)\n- [UltraChat (en)](https://github.com/thunlp/UltraChat)\n- [OpenPlatypus (en)](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)\n- [CodeAlpaca 20k (en)](https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k)\n- [Alpaca CoT (multilingual)](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)\n- [OpenOrca (en)](https://huggingface.co/datasets/Open-Orca/OpenOrca)\n- [SlimOrca (en)](https://huggingface.co/datasets/Open-Orca/SlimOrca)\n- [MathInstruct (en)](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)\n- [Firefly 1.1M (zh)](https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M)\n- [Wiki QA (en)](https://huggingface.co/datasets/wiki_qa)\n- [Web QA (zh)](https://huggingface.co/datasets/suolyer/webqa)\n- [WebNovel (zh)](https://huggingface.co/datasets/zxbsmk/webnovel_cn)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [deepctrl (en&zh)](https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data)\n- [Advertise Generating (zh)](https://huggingface.co/datasets/HasturOfficial/adgen)\n- [ShareGPT Hyperfiltered (en)](https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k)\n- [ShareGPT4 (en&zh)](https://huggingface.co/datasets/shibing624/sharegpt_gpt4)\n- [UltraChat 200k (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k)\n- [Infinity Instruct (zh)](https://huggingface.co/datasets/BAAI/Infinity-Instruct)\n- [AgentInstruct (en)](https://huggingface.co/datasets/THUDM/AgentInstruct)\n- [LMSYS Chat 1M (en)](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)\n- [Evol Instruct V2 (en)](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k)\n- [Cosmopedia (en)](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia)\n- [STEM (zh)](https://huggingface.co/datasets/hfl/stem_zh_instruction)\n- [Ruozhiba (zh)](https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo)\n- [Neo-sft (zh)](https://huggingface.co/datasets/m-a-p/neo_sft_phase2)\n- [Magpie-Pro-300K-Filtered (en)](https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered)\n- [Magpie-ultra-v0.1 (en)](https://huggingface.co/datasets/argilla/magpie-ultra-v0.1)\n- [WebInstructSub (en)](https://huggingface.co/datasets/TIGER-Lab/WebInstructSub)\n- [OpenO1-SFT (en&zh)](https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT)\n- [Open-Thoughts (en)](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k)\n- [Open-R1-Math (en)](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k)\n- [Chinese-DeepSeek-R1-Distill (zh)](https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT)\n- [LLaVA mixed (en&zh)](https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k)\n- [Pokemon-gpt4o-captions (en&zh)](https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions)\n- [Open Assistant (de)](https://huggingface.co/datasets/mayflowergmbh/oasst_de)\n- [Dolly 15k (de)](https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de)\n- [Alpaca GPT4 (de)](https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de)\n- [OpenSchnabeltier (de)](https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de)\n- [Evol Instruct (de)](https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de)\n- [Dolphin (de)](https://huggingface.co/datasets/mayflowergmbh/dolphin_de)\n- [Booksum (de)](https://huggingface.co/datasets/mayflowergmbh/booksum_de)\n- [Airoboros (de)](https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de)\n- [Ultrachat (de)](https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de)\n\n</details>\n\n<details><summary>Preference datasets</summary>\n\n- [DPO mixed (en&zh)](https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k)\n- [UltraFeedback (en)](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized)\n- [COIG-P (zh)](https://huggingface.co/datasets/m-a-p/COIG-P)\n- [RLHF-V (en)](https://huggingface.co/datasets/openbmb/RLHF-V-Dataset)\n- [VLFeedback (en)](https://huggingface.co/datasets/Zhihui/VLFeedback)\n- [RLAIF-V (en)](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset)\n- [Orca DPO Pairs (en)](https://huggingface.co/datasets/Intel/orca_dpo_pairs)\n- [HH-RLHF (en)](https://huggingface.co/datasets/Anthropic/hh-rlhf)\n- [Nectar (en)](https://huggingface.co/datasets/berkeley-nest/Nectar)\n- [Orca DPO (de)](https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de)\n- [KTO mixed (en)](https://huggingface.co/datasets/argilla/kto-mix-15k)\n\n</details>\n\nSome datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.\n\n```bash\npip install "huggingface_hub<1.0.0"\nhuggingface-cli login\n```\n\n## Requirement\n\n| Mandatory    | Minimum | Recommend |\n| ------------ | ------- | --------- |\n| python       | 3.9     | 3.10      |\n| torch        | 2.0.0   | 2.6.0     |\n| torchvision  | 0.15.0  | 0.21.0    |\n| transformers | 4.49.0  | 4.50.0    |\n| datasets     | 2.16.0  | 3.2.0     |\n| accelerate   | 0.34.0  | 1.2.1     |\n| peft         | 0.14.0  | 0.15.1    |\n| trl          | 0.8.6   | 0.9.6     |\n\n| Optional     | Minimum | Recommend |\n| ------------ | ------- | --------- |\n| CUDA         | 11.6    | 12.2      |\n| deepspeed    | 0.10.0  | 0.16.4    |\n| bitsandbytes | 0.39.0  | 0.43.1    |\n| vllm         | 0.4.3   | 0.8.2     |\n| flash-attn   | 2.5.6   | 2.7.2     |\n\n### Hardware Requirement\n\n\* *estimated*\n\n| Method                              | Bits |   7B  |  14B  |  30B  |   70B  |   `x`B  |\n| ----------------------------------- | ---- | ----- | ----- | ----- | ------ | ------- |\n| Full (`bf16` or `fp16`)             |  32  | 120GB | 240GB | 600GB | 1200GB | `18x`GB |\n| Full (`pure_bf16`)                  |  16  |  60GB | 120GB | 300GB |  600GB |  `8x`GB |\n| Freeze/LoRA/GaLore/APOLLO/BAdam/OFT |  16  |  16GB |  32GB |  64GB |  160GB |  `2x`GB |\n| QLoRA / QOFT                        |   8  |  10GB |  20GB |  40GB |   80GB |   `x`GB |\n| QLoRA / QOFT                        |   4  |   6GB |  12GB |  24GB |   48GB | `x/2`GB |\n| QLoRA / QOFT                        |   2  |   4GB |   8GB |  16GB |   24GB | `x/4`GB |\n\n## Getting Started\n\n### Installation\n\n> [!IMPORTANT]\n> Installation is mandatory.\n\n#### Install from Source\n\n```bash\ngit clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\npip install -e ".[torch,metrics]" --no-build-isolation\n```\n\nExtra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, openmind, swanlab, dev\n\n#### Install from Docker Image\n\n```bash\ndocker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest\n```\n\nThis image is built on Ubuntu 22.04 (x86\_64), CUDA 12.4, Python 3.11, PyTorch 2.6.0, and Flash-attn 2.7.4.\n\nFind the pre-built images: https://hub.docker.com/r/hiyouga/llamafactory/tags\n\nPlease refer to [build docker](#build-docker) to build the image yourself.\n\n<details><summary>Setting up a virtual environment with <b>uv</b></summary>\n\nCreate an isolated Python environment with [uv](https://github.com/astral-sh/uv):\n\n```bash\nuv sync --extra torch --extra metrics --prerelease=allow\n```\n\nRun LLaMA-Factory in the isolated environment:\n\n```bash\nuv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml\n```\n\n</details>\n\n<details><summary>For Windows users</summary>\n\n#### Install PyTorch\n\nYou need to manually install the GPU version of PyTorch on the Windows platform. Please refer to the [official website](https://pytorch.org/get-started/locally/) and the following command to install PyTorch with CUDA support:\n\n```bash\npip uninstall torch torchvision torchaudio\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\npython -c "import torch; print(torch.cuda.is_available())"\n```\n\nIf you see `True` then you have successfully installed PyTorch with CUDA support.\n\nTry `dataloader_num_workers: 0` if you encounter `Can''t pickle local object` error.\n\n#### Install BitsAndBytes\n\nIf you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of `bitsandbytes` library, which supports CUDA 11.1 to 12.2, please select the appropriate [release version](https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels) based on your CUDA version.\n\n```bash\npip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl\n```\n\n#### Install Flash Attention-2\n\nTo enable FlashAttention-2 on the Windows platform, please use the script from [flash-attention-windows-wheel](https://huggingface.co/lldacing/flash-attention-windows-wheel) to compile and install it by yourself.\n\n</details>\n\n<details><summary>For Ascend NPU users</summary>\n\nTo install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: `pip install -e ".[torch-npu,metrics]"`. Additionally, you need to install the **[Ascend CANN Toolkit and Kernels](https://www.hiascend.com/developer/download/community/result?module=cann)**. Please follow the [installation tutorial](https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html) or use the following commands:\n\n```bash\n# replace the url according to your CANN version and devices\n# install CANN Toolkit\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-"$(uname -i)".run\nbash Ascend-cann-toolkit_8.0.0.alpha002_linux-"$(uname -i)".run --install\n\n# install CANN Kernels\nwget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-"$(uname -i)".run\nbash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-"$(uname -i)".run --install\n\n# set env variables\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh\n```\n\n| Requirement  | Minimum | Recommend      |\n| ------------ | ------- | -------------- |\n| CANN         | 8.0.RC1 | 8.0.0.alpha002 |\n| torch        | 2.1.0   | 2.4.0          |\n| torch-npu    | 2.1.0   | 2.4.0.post2    |\n| deepspeed    | 0.13.2  | 0.13.2         |\n| vllm-ascend  | -       | 0.7.3          |\n\nRemember to use `ASCEND_RT_VISIBLE_DEVICES` instead of `CUDA_VISIBLE_DEVICES` to specify the device to use.\n\nIf you cannot infer model on NPU devices, try setting `do_sample: false` in the configurations.\n\nDownload the pre-built Docker images: [32GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html) | [64GB](http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html)\n\n#### Install BitsAndBytes\n\nTo use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:\n\n1. Manually compile bitsandbytes: Refer to [the installation documentation](https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&platform=Ascend+NPU) for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.\n\n```bash\n# Install bitsandbytes from source\n# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch\ngit clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git\ncd bitsandbytes/\n\n# Install dependencies\npip install -r requirements-dev.txt\n\n# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference\napt-get install -y build-essential cmake\n\n# Compile & install  \ncmake -DCOMPUTE_BACKEND=npu -S .\nmake\npip install .\n```\n\n2. Install transformers from the main branch.\n\n```bash\ngit clone -b main https://github.com/huggingface/transformers.git\ncd transformers\npip install .\n```\n\n3. Set `double_quantization: false` in the configuration. You can refer to the [example](examples/train_qlora/llama3_lora_sft_bnb_npu.yaml).\n\n</details>\n\n### Data Preparation\n\nPlease refer to [data/README.md](data/README.md) for checking the details about the format of dataset files. You can use datasets on HuggingFace / ModelScope / Modelers hub, load the dataset in local disk, or specify a path to s3/gcs cloud storage.\n\n> [!NOTE]\n> Please update `data/dataset_info.json` to use your custom dataset.\n\nYou can also use **[Easy Dataset](https://github.com/ConardLi/easy-dataset)**, **[DataFlow](https://github.com/OpenDCAI/DataFlow)** and **[GraphGen](https://github.com/open-sciencelab/GraphGen)** to create synthetic data for fine-tuning.\n\n### Quickstart\n\nUse the following 3 commands to run LoRA **fine-tuning**, **inference** and **merging** of the Llama3-8B-Instruct model, respectively.\n\n```bash\nllamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\nllamafactory-cli chat examples/inference/llama3_lora_sft.yaml\nllamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml\n```\n\nSee [examples/README.md](examples/README.md) for advanced usage (including distributed training).\n\n> [!TIP]\n> Use `llamafactory-cli help` to show help information.\n>\n> Read [FAQs](https://github.com/hiyouga/LLaMA-Factory/issues/4614) first if you encounter any problems.\n\n### Fine-Tuning with LLaMA Board GUI (powered by [Gradio](https://github.com/gradio-app/gradio))\n\n```bash\nllamafactory-cli webui\n```\n\n### LLaMA Factory Online\n\nRead our [documentation](https://docs.llamafactory.com.cn/docs/documents/quickstart/getstarted/?utm_source=LLaMA-Factory).\n\n### Build Docker\n\nFor CUDA users:\n\n```bash\ncd docker/docker-cuda/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nFor Ascend NPU users:\n\n```bash\ncd docker/docker-npu/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\nFor AMD ROCm users:\n\n```bash\ncd docker/docker-rocm/\ndocker compose up -d\ndocker compose exec llamafactory bash\n```\n\n<details><summary>Build without Docker Compose</summary>\n\nFor CUDA users:\n\n```bash\ndocker build -f ./docker/docker-cuda/Dockerfile \\n    --build-arg PIP_INDEX=https://pypi.org/simple \\n    --build-arg EXTRAS=metrics \\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host --gpus=all \\n    -p 7860:7860 \\n    -p 8000:8000 \\n    --name llamafactory \\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nFor Ascend NPU users:\n\n```bash\ndocker build -f ./docker/docker-npu/Dockerfile \\n    --build-arg PIP_INDEX=https://pypi.org/simple \\n    --build-arg EXTRAS=torch-npu,metrics \\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\n    -v /usr/local/dcmi:/usr/local/dcmi \\n    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\n    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\n    -v /etc/ascend_install.info:/etc/ascend_install.info \\n    -p 7860:7860 \\n    -p 8000:8000 \\n    --device /dev/davinci0 \\n    --device /dev/davinci_manager \\n    --device /dev/devmm_svm \\n    --device /dev/hisi_hdc \\n    --name llamafactory \\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\nFor AMD ROCm users:\n\n```bash\ndocker build -f ./docker/docker-rocm/Dockerfile \\n    --build-arg PIP_INDEX=https://pypi.org/simple \\n    --build-arg EXTRAS=metrics \\n    -t llamafactory:latest .\n\ndocker run -dit --ipc=host \\n    -p 7860:7860 \\n    -p 8000:8000 \\n    --device /dev/kfd \\n    --device /dev/dri \\n    --name llamafactory \\n    llamafactory:latest\n\ndocker exec -it llamafactory bash\n```\n\n</details>\n\n<details><summary>Use Docker volumes</summary>\n\nYou can uncomment `VOLUME [ "/root/.cache/huggingface", "/app/shared_data", "/app/output" ]` in the Dockerfile to use data volumes.\n\nWhen building the Docker image, use `-v ./hf_cache:/root/.cache/huggingface` argument to mount the local directory to the container. The following data volumes are available.\n\n- `hf_cache`: Utilize Hugging Face cache on the host machine.\n- `shared_data`: The directionary to store datasets on the host machine.\n- `output`: Set export dir to this location so that the merged result can be accessed directly on the host machine.\n\n</details>\n\n### Deploy with OpenAI-style API and vLLM\n\n```bash\nAPI_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true\n```\n\n> [!TIP]\n> Visit [this page](https://platform.openai.com/docs/api-reference/chat/create) for API document.\n>\n> Examples: [Image understanding](scripts/api_example/test_image.py) | [Function calling](scripts/api_example/test_toolcall.py)\n\n### Download from ModelScope Hub\n\nIf you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.\n\n```bash\nexport USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows\n```\n\nTrain the model by specifying a model ID of the ModelScope Hub as the `model_name_or_path`. You can find a full list of model IDs at [ModelScope Hub](https://modelscope.cn/models), e.g., `LLM-Research/Meta-Llama-3-8B-Instruct`.\n\n### Download from Modelers Hub\n\nYou can also use Modelers Hub to download models and datasets.\n\n```bash\nexport USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows\n```\n\nTrain the model by specifying a model ID of the Modelers Hub as the `model_name_or_path`. You can find a full list of model IDs at [Modelers Hub](https://modelers.cn/models), e.g., `TeleAI/TeleChat-7B-pt`.\n\n### Use W&B Logger\n\nTo use [Weights & Biases](https://wandb.ai) for logging experimental results, you need to add the following arguments to yaml files.\n\n```yaml\nreport_to: wandb\nrun_name: test_run # optional\n```\n\nSet `WANDB_API_KEY` to [your key](https://wandb.ai/authorize) when launching training tasks to log in with your W&B account.\n\n### Use SwanLab Logger\n\nTo use [SwanLab](https://github.com/SwanHubX/SwanLab) for logging experimental results, you need to add the following arguments to yaml files.\n\n```yaml\nuse_swanlab: true\nswanlab_run_name: test_run # optional\n```\n\nWhen launching training tasks, you can log in to SwanLab in three ways:\n\n1. Add `swanlab_api_key=<your_api_key>` to the yaml file, and set it to your [API key](https://swanlab.cn/settings).\n2. Set the environment variable `SWANLAB_API_KEY` to your [API key](https://swanlab.cn/settings).\n3. Use the `swanlab login` command to complete the login.\n\n## Projects using LLaMA Factory\n\nIf you have a project that should be incorporated, please contact via email or create a pull request.\n\n<details><summary>Click to show</summary>\n\n1. Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. [[arxiv]](https://arxiv.org/abs/2308.02223)\n1. Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. [[arxiv]](https://arxiv.org/abs/2308.10092)\n1. Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. [[arxiv]](https://arxiv.org/abs/2308.10526)\n1. Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. [[arxiv]](https://arxiv.org/abs/2311.07816)\n1. Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. [[arxiv]](https://arxiv.org/abs/2312.15710)\n1. Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. [[arxiv]](https://arxiv.org/abs/2401.04319)\n1. Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2401.07286)\n1. Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2402.05904)\n1. Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. [[arxiv]](https://arxiv.org/abs/2402.07625)\n1. Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11176)\n1. Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. [[arxiv]](https://arxiv.org/abs/2402.11187)\n1. Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. [[arxiv]](https://arxiv.org/abs/2402.11746)\n1. Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11801)\n1. Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2402.11809)\n1. Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.11819)\n1. Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. [[arxiv]](https://arxiv.org/abs/2402.12204)\n1. Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2402.14714)\n1. Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. [[arxiv]](https://arxiv.org/abs/2402.15043)\n1. Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. [[arxiv]](https://arxiv.org/abs/2403.02333)\n1. Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. [[arxiv]](https://arxiv.org/abs/2403.03419)\n1. Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. [[arxiv]](https://arxiv.org/abs/2403.08228)\n1. Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2403.09073)\n1. Zhang et al. EDT: Improving Large Language Models'' Generation by Entropy-based Dynamic Temperature Sampling. 2024. [[arxiv]](https://arxiv.org/abs/2403.14541)\n1. Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2403.15246)\n1. Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. [[arxiv]](https://arxiv.org/abs/2403.16008)\n1. Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. [[arxiv]](https://arxiv.org/abs/2403.16443)\n1. Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2404.00604)\n1. Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.02827)\n1. Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2404.04167)\n1. Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. [[arxiv]](https://arxiv.org/abs/2404.04316)\n1. Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.07084)\n1. Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.09836)\n1. Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2404.11581)\n1. Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. [[arxiv]](https://arxiv.org/abs/2404.14215)\n1. Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2404.16621)\n1. Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. [[arxiv]](https://arxiv.org/abs/2404.17140)\n1. Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. [[arxiv]](https://arxiv.org/abs/2404.18585)\n1. Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. [[arxiv]](https://arxiv.org/abs/2405.04760)\n1. Dammu et al. "They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. [[arxiv]](https://arxiv.org/abs/2405.05378)\n1. Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. [[arxiv]](https://arxiv.org/abs/2405.09055)\n1. Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. [[arxiv]](https://arxiv.org/abs/2405.12739)\n1. Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. [[arxiv]](https://arxiv.org/abs/2405.13816)\n1. Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. [[arxiv]](https://arxiv.org/abs/2405.20215)\n1. Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. [[paper]](https://aclanthology.org/2024.lt4hala-1.30)\n1. Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2406.00380)\n1. Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. [[arxiv]](https://arxiv.org/abs/2406.02106)\n1. Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. [[arxiv]](https://arxiv.org/abs/2406.03136)\n1. Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. [[arxiv]](https://arxiv.org/abs/2406.04496)\n1. Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. [[arxiv]](https://arxiv.org/abs/2406.05688)\n1. Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. [[arxiv]](https://arxiv.org/abs/2406.05955)\n1. Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. [[arxiv]](https://arxiv.org/abs/2406.06973)\n1. Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. [[arxiv]](https://arxiv.org/abs/2406.07115)\n1. Zhu et al. Are Large Language Models Good Statisticians?. 2024. [[arxiv]](https://arxiv.org/abs/2406.07815)\n1. Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2406.10099)\n1. Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. [[arxiv]](https://arxiv.org/abs/2406.10173)\n1. He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. [[arxiv]](https://arxiv.org/abs/2406.12074)\n1. Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. [[arxiv]](https://arxiv.org/abs/2406.14408)\n1. Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. [[arxiv]](https://arxiv.org/abs/2406.14546)\n1. Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. [[arxiv]](https://arxiv.org/abs/2406.15695)\n1. Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. [[arxiv]](https://arxiv.org/abs/2406.17233)\n1. Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. [[arxiv]](https://arxiv.org/abs/2406.18069)\n1. Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh''s Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. [[paper]](https://aclanthology.org/2024.americasnlp-1.25)\n1. Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. [[arxiv]](https://arxiv.org/abs/2406.19949)\n1. Yang et al. Financial Knowledge Large Language Model. 2024. [[arxiv]](https://arxiv.org/abs/2407.00365)\n1. Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. [[arxiv]](https://arxiv.org/abs/2407.01470)\n1. Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. [[arxiv]](https://arxiv.org/abs/2407.06129)\n1. Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. [[arxiv]](https://arxiv.org/abs/2407.08044)\n1. Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. [[arxiv]](https://arxiv.org/abs/2407.09756)\n1. Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. [[paper]](https://scholarcommons.scu.edu/cseng_senior/272/)\n1. Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. [[arxiv]](https://arxiv.org/abs/2407.13561)\n1. Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. [[arxiv]](https://arxiv.org/abs/2407.16637)\n1. Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. [[arxiv]](https://arxiv.org/abs/2407.17535)\n1. Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. [[arxiv]](https://arxiv.org/abs/2407.19705)\n1. Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. [[arxiv]](https://arxiv.org/abs/2408.00137)\n1. Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. [[paper]](https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf)\n1. Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11)\n1. Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. [[paper]](https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23)\n1. Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. [[arxiv]](https://arxiv.org/abs/2408.04693)\n1. Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. [[arxiv]](https://arxiv.org/abs/2408.04168)\n1. Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. [[paper]](https://aclanthology.org/2024.finnlp-2.1/)\n1. Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. [[arxiv]](https://arxiv.org/abs/2408.08072)\n1. Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. [[paper]](https://dl.acm.org/doi/10.1145/3627673.3679611)\n1. Zhang et al. CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling. ACL 2024. [[paper]](https://aclanthology.org/2024.findings-acl.830.pdf)\n1. **[StarWhisper](https://github.com/Yu-Yang-Li/StarWhisper)**: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.\n1. **[DISC-LawLLM](https://github.com/FudanDISC/DISC-LawLLM)**: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.\n1. **[Sunsimiao](https://github.com/X-D-Lab/Sunsimiao)**: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.\n1. **[CareGPT](https://github.com/WangRongsheng/CareGPT)**: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.\n1. **[MachineMindset](https://github.com/PKU-YuanGroup/Machine-Mindset/)**: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.\n1. **[Luminia-13B-v3](https://huggingface.co/Nekochu/Luminia-13B-v3)**: A large language model specialized in generate metadata for stable diffusion. [[demo]](https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt)\n1. **[Chinese-LLaVA-Med](https://github.com/BUAADreamer/Chinese-LLaVA-Med)**: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.\n1. **[AutoRE](https://github.com/THUDM/AutoRE)**: A document-level relation extraction system based on large language models.\n1. **[NVIDIA RTX AI Toolkit](https://github.com/NVIDIA/RTX-AI-Toolkit)**: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.\n1. **[LazyLLM](https://github.com/LazyAGI/LazyLLM)**: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.\n1. **[RAG-Retrieval](https://github.com/NLPJCL/RAG-Retrieval)**: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. [[blog]](https://zhuanlan.zhihu.com/p/987727357)\n1. **[360-LLaMA-Factory](https://github.com/Qihoo360/360-LLaMA-Factory)**: A modified library that supports long sequence SFT & DPO using ring attention.\n1. **[Sky-T1](https://novasky-ai.github.io/posts/sky-t1/)**: An o1-like model fine-tuned by NovaSky AI with very small cost.\n1. **[WeClone](https://github.com/xming521/WeClone)**: One-stop solution for creating your digital avatar from chat logs.\n1. **[EmoLLM](https://github.com/SmartFlowAI/EmoLLM)**: A project about large language models (LLMs) and mental health.\n</details>\n\n## License\n\nThis repository is licensed under the [Apache-2.0 License](LICENSE).\n\nPlease follow the model licenses to use the corresponding model weights: [Baichuan 2](https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf) / [BLOOM](https://huggingface.co/spaces/bigscience/license) / [ChatGLM3](https://github.com/THUDM/ChatGLM3/blob/main/MODEL_LICENSE) / [Command R](https://cohere.com/c4ai-cc-by-nc-license) / [DeepSeek](https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL) / [Falcon](https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt) / [Gemma](https://ai.google.dev/gemma/terms) / [GLM-4](https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE) / [GPT-2](https://github.com/openai/gpt-2/blob/master/LICENSE) / [Granite](LICENSE) / [Index](https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE) / [InternLM](https://github.com/InternLM/InternLM#license) / [Llama](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) / [Llama 2](https://ai.meta.com/llama/license/) / [Llama 3](https://llama.meta.com/llama3/license/) / [Llama 4](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE) / [MiniCPM](https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md) / [Mistral/Mixtral/Pixtral](LICENSE) / [OLMo](LICENSE) / [Phi-1.5/Phi-2](https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx) / [Phi-3/Phi-4](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE) / [Qwen](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT) / [Skywork](https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf) / [StarCoder 2](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) / [TeleChat2](https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf) / [XVERSE](https://github.com/xverse-ai/XVERSE-13B/blob/main/MODEL_LICENSE.pdf) / [Yi](https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE) / [Yi-1.5](LICENSE) / [Yuan 2](https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/LICENSE-Yuan)\n\n## Citation\n\nIf this work is helpful, please kindly cite as:\n\n```bibtex\n@inproceedings{zheng2024llamafactory,\n  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},\n  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},\n  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},\n  address={Bangkok, Thailand},\n  publisher={Association for Computational Linguistics},\n  year={2024},\n  url={http://arxiv.org/abs/2403.13372}\n}\n```\n\n## Acknowledgement\n\nThis repo benefits from [PEFT](https://github.com/huggingface/peft), [TRL](https://github.com/huggingface/trl), [QLoRA](https://github.com/artidoro/qlora) and [FastChat](https://github.com/lm-sys/FastChat). Thanks for their wonderful works.\n\n## Star History\n\n![Star History Chart](https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&type=Date)\n', '{"language":"Python","stars":63676,"forks":7697,"watchers":63676,"open_issues":811,"topics":["agent","ai","deepseek","fine-tuning","gemma","gpt","instruction-tuning","large-language-models","llama","llama3","llm","lora","moe","nlp","peft","qlora","quantization","qwen","rlhf","transformers"],"default_branch":"main","size_kb":12421,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:hiyouga:llamafactory-community","source_url":"https://github.com/hiyouga/llamafactory-community"},{"type":"has_code","target_id":"github:hiyouga:llamafactory-community","source_url":"https://github.com/hiyouga/llamafactory-community"},{"type":"has_code","target_id":"github:hiyouga:llamafactory-community","source_url":"https://github.com/hiyouga/llamafactory-community"},{"type":"has_code","target_id":"github:hiyouga:llamafactory-community","source_url":"https://github.com/hiyouga/llamafactory-community"},{"type":"has_code","target_id":"github:hiyouga:llamafactory-community","source_url":"https://github.com/hiyouga/llamafactory-community"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:jiaweizzhao:GaLore","source_url":"https://github.com/jiaweizzhao/GaLore"},{"type":"has_code","target_id":"github:Ledzy:BAdam","source_url":"https://github.com/Ledzy/BAdam"},{"type":"has_code","target_id":"github:zhuhanqing:APOLLO","source_url":"https://github.com/zhuhanqing/APOLLO"},{"type":"has_code","target_id":"github:zyushun:Adam-mini","source_url":"https://github.com/zyushun/Adam-mini"},{"type":"has_code","target_id":"github:KellerJordan:Muon","source_url":"https://github.com/KellerJordan/Muon"},{"type":"has_code","target_id":"github:huggingface:peft","source_url":"https://github.com/huggingface/peft"},{"type":"has_code","target_id":"github:Dao-AILab:flash-attention","source_url":"https://github.com/Dao-AILab/flash-attention"},{"type":"has_code","target_id":"github:unslothai:unsloth","source_url":"https://github.com/unslothai/unsloth"},{"type":"has_code","target_id":"github:linkedin:Liger-Kernel","source_url":"https://github.com/linkedin/Liger-Kernel"},{"type":"has_code","target_id":"github:kvcache-ai:ktransformers","source_url":"https://github.com/kvcache-ai/ktransformers"},{"type":"has_code","target_id":"github:SwanHubX:SwanLab","source_url":"https://github.com/SwanHubX/SwanLab"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:alibaba:ROLL","source_url":"https://github.com/alibaba/ROLL"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:openai:gpt-oss","source_url":"https://github.com/openai/gpt-oss"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:THUDM:GLM-4.1V-Thinking","source_url":"https://github.com/THUDM/GLM-4.1V-Thinking"},{"type":"has_code","target_id":"github:KellerJordan:Muon","source_url":"https://github.com/KellerJordan/Muon"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:hiyouga:EasyR1","source_url":"https://github.com/hiyouga/EasyR1"},{"type":"has_code","target_id":"github:ollama:ollama","source_url":"https://github.com/ollama/ollama"},{"type":"has_code","target_id":"github:SwanHubX:SwanLab","source_url":"https://github.com/SwanHubX/SwanLab"},{"type":"has_code","target_id":"github:linkedin:Liger-Kernel","source_url":"https://github.com/linkedin/Liger-Kernel"},{"type":"has_code","target_id":"github:zyushun:Adam-mini","source_url":"https://github.com/zyushun/Adam-mini"},{"type":"has_code","target_id":"github:MeetKai:functionary","source_url":"https://github.com/MeetKai/functionary"},{"type":"has_code","target_id":"github:THUDM:GLM-4","source_url":"https://github.com/THUDM/GLM-4"},{"type":"has_code","target_id":"github:astramind-ai:Mixture-of-depths","source_url":"https://github.com/astramind-ai/Mixture-of-depths"},{"type":"has_code","target_id":"github:unslothai:unsloth","source_url":"https://github.com/unslothai/unsloth"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:TencentARC:LLaMA-Pro","source_url":"https://github.com/TencentARC/LLaMA-Pro"},{"type":"has_code","target_id":"github:unslothai:unsloth","source_url":"https://github.com/unslothai/unsloth"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:dvlab-research:LongLoRA","source_url":"https://github.com/dvlab-research/LongLoRA"},{"type":"has_code","target_id":"github:Dao-AILab:flash-attention","source_url":"https://github.com/Dao-AILab/flash-attention"},{"type":"has_code","target_id":"github:hiyouga:FastEdit","source_url":"https://github.com/hiyouga/FastEdit"},{"type":"has_code","target_id":"github:hiyouga:FastEdit","source_url":"https://github.com/hiyouga/FastEdit"},{"type":"has_code","target_id":"github:artidoro:qlora","source_url":"https://github.com/artidoro/qlora"},{"type":"has_code","target_id":"github:facebookresearch:llama","source_url":"https://github.com/facebookresearch/llama"},{"type":"has_code","target_id":"github:tatsu-lab:stanford_alpaca","source_url":"https://github.com/tatsu-lab/stanford_alpaca"},{"type":"has_code","target_id":"github:ymcui:Chinese-LLaMA-Alpaca-3","source_url":"https://github.com/ymcui/Chinese-LLaMA-Alpaca-3"},{"type":"has_code","target_id":"github:Instruction-Tuning-with-GPT-4:GPT-4-LLM","source_url":"https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM"},{"type":"has_code","target_id":"github:thunlp:UltraChat","source_url":"https://github.com/thunlp/UltraChat"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory.git","source_url":"https://github.com/hiyouga/LLaMA-Factory.git"},{"type":"has_code","target_id":"github:astral-sh:uv","source_url":"https://github.com/astral-sh/uv"},{"type":"has_code","target_id":"github:jllllll:bitsandbytes-windows-webui","source_url":"https://github.com/jllllll/bitsandbytes-windows-webui"},{"type":"has_code","target_id":"github:jllllll:bitsandbytes-windows-webui","source_url":"https://github.com/jllllll/bitsandbytes-windows-webui"},{"type":"has_code","target_id":"github:bitsandbytes-foundation:bitsandbytes.git","source_url":"https://github.com/bitsandbytes-foundation/bitsandbytes.git"},{"type":"has_code","target_id":"github:huggingface:transformers.git","source_url":"https://github.com/huggingface/transformers.git"},{"type":"has_code","target_id":"github:ConardLi:easy-dataset","source_url":"https://github.com/ConardLi/easy-dataset"},{"type":"has_code","target_id":"github:OpenDCAI:DataFlow","source_url":"https://github.com/OpenDCAI/DataFlow"},{"type":"has_code","target_id":"github:open-sciencelab:GraphGen","source_url":"https://github.com/open-sciencelab/GraphGen"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"},{"type":"has_code","target_id":"github:SwanHubX:SwanLab","source_url":"https://github.com/SwanHubX/SwanLab"},{"type":"has_code","target_id":"github:Yu-Yang-Li:StarWhisper","source_url":"https://github.com/Yu-Yang-Li/StarWhisper"},{"type":"has_code","target_id":"github:FudanDISC:DISC-LawLLM","source_url":"https://github.com/FudanDISC/DISC-LawLLM"},{"type":"has_code","target_id":"github:X-D-Lab:Sunsimiao","source_url":"https://github.com/X-D-Lab/Sunsimiao"},{"type":"has_code","target_id":"github:WangRongsheng:CareGPT","source_url":"https://github.com/WangRongsheng/CareGPT"},{"type":"has_code","target_id":"github:PKU-YuanGroup:Machine-Mindset","source_url":"https://github.com/PKU-YuanGroup/Machine-Mindset"},{"type":"has_code","target_id":"github:BUAADreamer:Chinese-LLaVA-Med","source_url":"https://github.com/BUAADreamer/Chinese-LLaVA-Med"},{"type":"has_code","target_id":"github:THUDM:AutoRE","source_url":"https://github.com/THUDM/AutoRE"},{"type":"has_code","target_id":"github:NVIDIA:RTX-AI-Toolkit","source_url":"https://github.com/NVIDIA/RTX-AI-Toolkit"},{"type":"has_code","target_id":"github:LazyAGI:LazyLLM","source_url":"https://github.com/LazyAGI/LazyLLM"},{"type":"has_code","target_id":"github:NLPJCL:RAG-Retrieval","source_url":"https://github.com/NLPJCL/RAG-Retrieval"},{"type":"has_code","target_id":"github:Qihoo360:360-LLaMA-Factory","source_url":"https://github.com/Qihoo360/360-LLaMA-Factory"},{"type":"has_code","target_id":"github:xming521:WeClone","source_url":"https://github.com/xming521/WeClone"},{"type":"has_code","target_id":"github:SmartFlowAI:EmoLLM","source_url":"https://github.com/SmartFlowAI/EmoLLM"},{"type":"has_code","target_id":"github:THUDM:ChatGLM3","source_url":"https://github.com/THUDM/ChatGLM3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-LLM","source_url":"https://github.com/deepseek-ai/DeepSeek-LLM"},{"type":"has_code","target_id":"github:openai:gpt-2","source_url":"https://github.com/openai/gpt-2"},{"type":"has_code","target_id":"github:InternLM:InternLM","source_url":"https://github.com/InternLM/InternLM#license"},{"type":"has_code","target_id":"github:facebookresearch:llama","source_url":"https://github.com/facebookresearch/llama"},{"type":"has_code","target_id":"github:meta-llama:llama-models","source_url":"https://github.com/meta-llama/llama-models"},{"type":"has_code","target_id":"github:OpenBMB:MiniCPM","source_url":"https://github.com/OpenBMB/MiniCPM"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:xverse-ai:XVERSE-13B","source_url":"https://github.com/xverse-ai/XVERSE-13B"},{"type":"has_code","target_id":"github:IEIT-Yuan:Yuan-2.0","source_url":"https://github.com/IEIT-Yuan/Yuan-2.0"},{"type":"has_code","target_id":"github:huggingface:peft","source_url":"https://github.com/huggingface/peft"},{"type":"has_code","target_id":"github:huggingface:trl","source_url":"https://github.com/huggingface/trl"},{"type":"has_code","target_id":"github:artidoro:qlora","source_url":"https://github.com/artidoro/qlora"},{"type":"has_code","target_id":"github:lm-sys:FastChat","source_url":"https://github.com/lm-sys/FastChat"}]', NULL, 'Apache-2.0', 'approved', 80, 'c873bca5e0012d5e7c9bf26e25134377', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-hiyouga-LLaMA-Factory from https://github.com/hiyouga.png
Image converted to WebP: data/images/github-hiyouga-LLaMA-Factory.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-AntonOsika-gpt-engineer', 'github--antonosika--gpt-engineer', 'gpt-engineer', 'AntonOsika', '!GitHub Release The OG code genereation experimentation platform! If you are looking for the evolution that is an opinionated, managed service â€“ check out gptengineer.app. If you are looking for a well maintained hackable CLI for â€“ check out aider. gpt-engineer lets you: - Specify software in natural language - Sit back and watch as an AI writes and executes the code - Ask the AI to implement improvements For **stable** release: - For **development**: - - - - to activate the virtual environme...', '["ai","autonomous-agent","code-generation","codebase-generation","codegen","coding-assistant","gpt-4","gpt-engineer","openai","python","python"]', 'other', 55096, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/AntonOsika/gpt-engineer","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# gpt-engineer\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/gpt-engineer-org/gpt-engineer?style=social)](https://github.com/gpt-engineer-org/gpt-engineer)\n[![Discord Follow](https://dcbadge.vercel.app/api/server/8tcDQ89Ej2?style=flat)](https://discord.gg/8tcDQ89Ej2)\n[![License](https://img.shields.io/github/license/gpt-engineer-org/gpt-engineer)](https://github.com/gpt-engineer-org/gpt-engineer/blob/main/LICENSE)\n[![GitHub Issues or Pull Requests](https://img.shields.io/github/issues/gpt-engineer-org/gpt-engineer)](https://github.com/gpt-engineer-org/gpt-engineer/issues)\n![GitHub Release](https://img.shields.io/github/v/release/gpt-engineer-org/gpt-engineer)\n[![Twitter Follow](https://img.shields.io/twitter/follow/antonosika?style=social)](https://twitter.com/antonosika)\n\nThe OG code genereation experimentation platform!\n\nIf you are looking for the evolution that is an opinionated, managed service â€“ check out gptengineer.app.\n\nIf you are looking for a well maintained hackable CLI for â€“ check out aider.\n\n\ngpt-engineer lets you:\n- Specify software in natural language\n- Sit back and watch as an AI writes and executes the code\n- Ask the AI to implement improvements\n\n## Getting Started\n\n### Install gpt-engineer\n\nFor **stable** release:\n\n- `python -m pip install gpt-engineer`\n\nFor **development**:\n- `git clone https://github.com/gpt-engineer-org/gpt-engineer.git`\n- `cd gpt-engineer`\n- `poetry install`\n- `poetry shell` to activate the virtual environment\n\nWe actively support Python 3.10 - 3.12. The last version to support Python 3.8 - 3.9 was [0.2.6](https://pypi.org/project/gpt-engineer/0.2.6/).\n\n### Setup API key\n\nChoose **one** of:\n- Export env variable (you can add this to .bashrc so that you don''t have to do it each time you start the terminal)\n    - `export OPENAI_API_KEY=[your api key]`\n- .env file:\n    - Create a copy of `.env.template` named `.env`\n    - Add your OPENAI_API_KEY in .env\n- Custom model:\n    - See [docs](https://gpt-engineer.readthedocs.io/en/latest/open_models.html), supports local model, azure, etc.\n\nCheck the [Windows README](./WINDOWS_README.md) for Windows usage.\n\n**Other ways to run:**\n- Use Docker ([instructions](docker/README.md))\n- Do everything in your browser:\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/gpt-engineer-org/gpt-engineer/codespaces)\n\n### Create new code (default usage)\n- Create an empty folder for your project anywhere on your computer\n- Create a file called `prompt` (no extension) inside your new folder and fill it with instructions\n- Run `gpte <project_dir>` with a relative path to your folder\n  - For example: `gpte projects/my-new-project` from the gpt-engineer directory root with your new folder in `projects/`\n\n### Improve existing code\n- Locate a folder with code which you want to improve anywhere on your computer\n- Create a file called `prompt` (no extension) inside your new folder and fill it with instructions for how you want to improve the code\n- Run `gpte <project_dir> -i` with a relative path to your folder\n  - For example: `gpte projects/my-old-project -i` from the gpt-engineer directory root with your folder in `projects/`\n\n### Benchmark custom agents\n- gpt-engineer installs the binary ''bench'', which gives you a simple interface for benchmarking your own agent implementations against popular public datasets.\n- The easiest way to get started with benchmarking is by checking out the [template](https://github.com/gpt-engineer-org/gpte-bench-template) repo, which contains detailed instructions and an agent template.\n- Currently supported benchmark:\n  - [APPS](https://github.com/hendrycks/apps)\n  - [MBPP](https://github.com/google-research/google-research/tree/master/mbpp)\n\nThe community has started work with different benchmarking initiatives, as described in [this Loom](https://www.loom.com/share/206805143fbb4302b5455a5329eaab17?sid=f689608f-8e49-44f7-b55f-4c81e9dc93e6) video.\n\n### Research\nSome of our community members have worked on different research briefs that could be taken further. See [this document](https://docs.google.com/document/d/1qmOj2DvdPc6syIAm8iISZFpfik26BYw7ZziD5c-9G0E/edit?usp=sharing) if you are interested.\n\n## Terms\nBy running gpt-engineer, you agree to our [terms](https://github.com/gpt-engineer-org/gpt-engineer/blob/main/TERMS_OF_USE.md).\n\n\n## Relation to gptengineer.app (GPT Engineer)\n[gptengineer.app](https://gptengineer.app/) is a commercial project for the automatic generation of web apps.\nIt features a UI for non-technical users connected to a git-controlled codebase.\nThe gptengineer.app team is actively supporting the open source community.\n\n\n## Features\n\n### Pre Prompts\nYou can specify the "identity" of the AI agent by overriding the `preprompts` folder with your own version of the `preprompts`. You can do so via the `--use-custom-preprompts` argument.\n\nEditing the `preprompts` is how you make the agent remember things between projects.\n\n### Vision\n\nBy default, gpt-engineer expects text input via a `prompt` file. It can also accept image inputs for vision-capable models. This can be useful for adding UX or architecture diagrams as additional context for GPT Engineer. You can do this by specifying an image directory with the `â€”-image_directory` flag and setting a vision-capable model in the second CLI argument.\n\nE.g. `gpte projects/example-vision gpt-4-vision-preview --prompt_file prompt/text --image_directory prompt/images -i`\n\n### Open source, local and alternative models\n\nBy default, gpt-engineer supports OpenAI Models via the OpenAI API or Azure OpenAI API, as well as Anthropic models.\n\nWith a little extra setup, you can also run with open source models like WizardCoder. See the [documentation](https://gpt-engineer.readthedocs.io/en/latest/open_models.html) for example instructions.\n\n## Mission\n\nThe gpt-engineer community mission is to **maintain tools that coding agent builders can use and facilitate collaboration in the open source community**.\n\nIf you are interested in contributing to this, we are interested in having you.\n\nIf you want to see our broader ambitions, check out the [roadmap](https://github.com/gpt-engineer-org/gpt-engineer/blob/main/ROADMAP.md), and join\n[discord](https://discord.gg/8tcDQ89Ej2)\nto learn how you can [contribute](.github/CONTRIBUTING.md) to it.\n\ngpt-engineer is [governed](https://github.com/gpt-engineer-org/gpt-engineer/blob/main/GOVERNANCE.md) by a board of long-term contributors. If you contribute routinely and have an interest in shaping the future of gpt-engineer, you will be considered for the board.\n\n## Significant contributors\n<ul style="list-style-type: none; padding: 0; display: flex; flex-wrap: wrap;"> <li style="margin-right: 10px; margin-bottom: 10px;"> <a href="https://github.com/ATheorell"> <img src="https://avatars.githubusercontent.com/u/143704446?s=64&v=4" alt="@ATheorell" width="32" height="32" style="border-radius: 50%;"> @ATheorell </a> </li> <li style="margin-right: 10px; margin-bottom: 10px;"> <a href="https://github.com/similato87"> <img src="https://avatars.githubusercontent.com/u/71301573?s=64&v=4" alt="@similato87" width="32" height="32" style="border-radius: 50%;"> @similato87 </a> </li> <li style="margin-right: 10px; margin-bottom: 10px;"> <a href="https://github.com/TheoMcCabe"> <img src="https://avatars.githubusercontent.com/u/9841960?s=64&v=4" alt="@TheoMcCabe" width="32" height="32" style="border-radius: 50%;"> @TheoMcCabe </a> </li> <li style="margin-right: 10px; margin-bottom: 10px;"> <a href="https://github.com/captivus"> <img src="https://avatars.githubusercontent.com/u/366332?s=64&v=4" alt="@captivus" width="32" height="32" style="border-radius: 50%;"> @captivus </a> </li> </ul>\n\n\n## Example\n\n\n\nhttps://github.com/gpt-engineer-org/gpt-engineer/assets/4467025/40d0a9a8-82d0-4432-9376-136df0d57c99\n', '{"language":"Python","stars":55096,"forks":7347,"watchers":55096,"open_issues":66,"topics":["ai","autonomous-agent","code-generation","codebase-generation","codegen","coding-assistant","gpt-4","gpt-engineer","openai","python"],"default_branch":"main","size_kb":20474,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:gpt-engineer-org:gpt-engineer","source_url":"https://github.com/gpt-engineer-org/gpt-engineer"},{"type":"has_code","target_id":"github:gpt-engineer-org:gpt-engineer","source_url":"https://github.com/gpt-engineer-org/gpt-engineer"},{"type":"has_code","target_id":"github:gpt-engineer-org:gpt-engineer","source_url":"https://github.com/gpt-engineer-org/gpt-engineer"},{"type":"has_code","target_id":"github:gpt-engineer-org:gpt-engineer.git`","source_url":"https://github.com/gpt-engineer-org/gpt-engineer.git`"},{"type":"has_code","target_id":"github:codespaces:badge.svg","source_url":"https://github.com/codespaces/badge.svg"},{"type":"has_code","target_id":"github:gpt-engineer-org:gpt-engineer","source_url":"https://github.com/gpt-engineer-org/gpt-engineer"},{"type":"has_code","target_id":"github:gpt-engineer-org:gpte-bench-template","source_url":"https://github.com/gpt-engineer-org/gpte-bench-template"},{"type":"has_code","target_id":"github:hendrycks:apps","source_url":"https://github.com/hendrycks/apps"},{"type":"has_code","target_id":"github:google-research:google-research","source_url":"https://github.com/google-research/google-research"},{"type":"has_code","target_id":"github:gpt-engineer-org:gpt-engineer","source_url":"https://github.com/gpt-engineer-org/gpt-engineer"},{"type":"has_code","target_id":"github:gpt-engineer-org:gpt-engineer","source_url":"https://github.com/gpt-engineer-org/gpt-engineer"},{"type":"has_code","target_id":"github:gpt-engineer-org:gpt-engineer","source_url":"https://github.com/gpt-engineer-org/gpt-engineer"},{"type":"has_code","target_id":"github:gpt-engineer-org:gpt-engineer","source_url":"https://github.com/gpt-engineer-org/gpt-engineer"}]', NULL, 'MIT', 'approved', 65, '0b7d284a74123b67f4b8c0b4f75df4b6', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-AntonOsika-gpt-engineer from https://github.com/AntonOsika.png
Image converted to WebP: data/images/github-AntonOsika-gpt-engineer.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-meilisearch-meilisearch', 'github--meilisearch--meilisearch', 'meilisearch', 'meilisearch', '<p align="center"> <a href="https://www.meilisearch.com/?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=logo#gh-light-mode-only" target="_blank"> <img src="assets/meilisearch-logo-light.svg?sanitize=true#gh-light-mode-only"> </a> <a href="https://www.meilisearch.com/?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=logo#gh-dark-mode-only" target="_blank"> <img src="assets/meilisearch-logo-dark.svg?sanitize=true#gh-dark-mode-only"> </a> </p> <h4 alig...', '["ai","api","app-search","database","enterprise-search","faceting","full-text-search","fuzzy-search","geosearch","hybrid-search","instantsearch","search","search-as-you-type","search-engine","semantic-search","site-search","typo-tolerance","vector-database","vector-search","vectors","rust"]', 'other', 54842, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/meilisearch/meilisearch","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <a href="https://www.meilisearch.com/?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=logo#gh-light-mode-only" target="_blank">\n    <img src="assets/meilisearch-logo-light.svg?sanitize=true#gh-light-mode-only">\n  </a>\n  <a href="https://www.meilisearch.com/?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=logo#gh-dark-mode-only" target="_blank">\n    <img src="assets/meilisearch-logo-dark.svg?sanitize=true#gh-dark-mode-only">\n  </a>\n</p>\n\n<h4 align="center">\n  <a href="https://www.meilisearch.com/?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=nav">Website</a> |\n  <a href="https://roadmap.meilisearch.com/tabs/1-under-consideration">Roadmap</a> |\n  <a href="https://www.meilisearch.com/pricing?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=nav">Meilisearch Cloud</a> |\n  <a href="https://blog.meilisearch.com/?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=nav">Blog</a> |\n  <a href="https://www.meilisearch.com/docs?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=nav">Documentation</a> |\n  <a href="https://www.meilisearch.com/docs/faq?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=nav">FAQ</a> |\n  <a href="https://discord.meilisearch.com/?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=nav">Discord</a>\n</h4>\n\n<p align="center">\n  <a href="https://deps.rs/repo/github/meilisearch/meilisearch"><img src="https://deps.rs/repo/github/meilisearch/meilisearch/status.svg" alt="Dependency status"></a>\n  <a href="https://github.com/meilisearch/meilisearch/blob/main/LICENSE"><img src="https://img.shields.io/badge/license-MIT-informational" alt="License"></a>\n  <a href="https://github.com/meilisearch/meilisearch/queue"><img alt="Merge Queues enabled" src="https://img.shields.io/badge/Merge_Queues-enabled-%2357cf60?logo=github"></a>\n</p>\n\n<p align="center">âš¡ A lightning-fast search engine that fits effortlessly into your apps, websites, and workflow ğŸ”</p>\n\n[Meilisearch](https://www.meilisearch.com?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=intro) helps you shape a delightful search experience in a snap, offering features that work out of the box to speed up your workflow.\n\n<p align="center" name="demo">\n  <a href="https://where2watch.meilisearch.com/?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=demo-gif#gh-light-mode-only" target="_blank">\n    <img src="assets/demo-light.gif#gh-light-mode-only" alt="A bright colored application for finding movies screening near the user">\n  </a>\n  <a href="https://where2watch.meilisearch.com/?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=demo-gif#gh-dark-mode-only" target="_blank">\n    <img src="assets/demo-dark.gif#gh-dark-mode-only" alt="A dark colored application for finding movies screening near the user">\n  </a>\n</p>\n\n## ğŸ–¥ Examples\n\n- [**Movies**](https://where2watch.meilisearch.com/?utm_campaign=oss&utm_source=github&utm_medium=organization) â€” An application to help you find streaming platforms to watch movies using [hybrid search](https://www.meilisearch.com/solutions/hybrid-search?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=demos).\n- [**Flickr**](https://flickr.meilisearch.com/?utm_campaign=oss&utm_source=github&utm_medium=organization) â€” Search and explore one hundred million Flickr images with semantic search.\n- [**Ecommerce**](https://ecommerce.meilisearch.com/?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=demos) â€” Ecommerce website using disjunctive [facets](https://www.meilisearch.com/docs/learn/fine_tuning_results/faceted_search?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=demos), range and rating filtering, and pagination.\n- [**Songs**](https://music.meilisearch.com/?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=demos) â€” Search through 47 million of songs.\n- [**SaaS**](https://saas.meilisearch.com/?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=demos) â€” Search for contacts, deals, and companies in this [multi-tenant](https://www.meilisearch.com/docs/learn/security/multitenancy_tenant_tokens?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=demos) CRM application.\n\nSee the list of all our example apps in our [demos repository](https://github.com/meilisearch/demos).\n\n## âœ¨ Features\n- **Hybrid search:** Combine the best of both [semantic](https://www.meilisearch.com/docs/learn/experimental/vector_search?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=features) & full-text search to get the most relevant results\n- **Search-as-you-type:** Find & display results in less than 50 milliseconds to provide an intuitive experience\n- **[Typo tolerance](https://www.meilisearch.com/docs/learn/relevancy/typo_tolerance_settings?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=features):** get relevant matches even when queries contain typos and misspellings\n- **[Filtering](https://www.meilisearch.com/docs/learn/fine_tuning_results/filtering?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=features) and [faceted search](https://www.meilisearch.com/docs/learn/fine_tuning_results/faceted_search?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=features):** enhance your users'' search experience with custom filters and build a faceted search interface in a few lines of code\n- **[Sorting](https://www.meilisearch.com/docs/learn/fine_tuning_results/sorting?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=features):** sort results based on price, date, or pretty much anything else your users need\n- **[Synonym support](https://www.meilisearch.com/docs/learn/relevancy/synonyms?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=features):** configure synonyms to include more relevant content in your search results\n- **[Geosearch](https://www.meilisearch.com/docs/learn/fine_tuning_results/geosearch?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=features):** filter and sort documents based on geographic data\n- **[Extensive language support](https://www.meilisearch.com/docs/learn/what_is_meilisearch/language?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=features):** search datasets in any language, with optimized support for Chinese, Japanese, Hebrew, and languages using the Latin alphabet\n- **[Security management](https://www.meilisearch.com/docs/learn/security/master_api_keys?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=features):** control which users can access what data with API keys that allow fine-grained permissions handling\n- **[Multi-Tenancy](https://www.meilisearch.com/docs/learn/security/multitenancy_tenant_tokens?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=features):** personalize search results for any number of application tenants\n- **Highly Customizable:** customize Meilisearch to your specific needs or use our out-of-the-box and hassle-free presets\n- **[RESTful API](https://www.meilisearch.com/docs/reference/api/overview?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=features):** integrate Meilisearch in your technical stack with our plugins and SDKs\n- **AI-ready:** works out of the box with [langchain](https://www.meilisearch.com/with/langchain) and the [model context protocol](https://github.com/meilisearch/meilisearch-mcp)\n- **Easy to install, deploy, and maintain**\n\n## ğŸ“– Documentation\n\nYou can consult Meilisearch''s documentation at [meilisearch.com/docs](https://www.meilisearch.com/docs/?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=docs).\n\n## ğŸš€ Getting started\n\nFor basic instructions on how to set up Meilisearch, add documents to an index, and search for documents, take a look at our [documentation](https://www.meilisearch.com/docs?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=get-started) guide.\n\n## ğŸŒ Supercharge your Meilisearch experience\n\nSay goodbye to server deployment and manual updates with [Meilisearch Cloud](https://www.meilisearch.com/cloud?utm_campaign=oss&utm_source=github&utm_medium=meilisearch). Additional features include analytics & monitoring in many regions around the world. No credit card is required.\n\n## ğŸ§° SDKs & integration tools\n\nInstall one of our SDKs in your project for seamless integration between Meilisearch and your favorite language or framework!\n\nTake a look at the complete [Meilisearch integration list](https://www.meilisearch.com/docs/learn/what_is_meilisearch/sdks?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=sdks-link).\n\n[![Logos belonging to different languages and frameworks supported by Meilisearch, including React, Ruby on Rails, Go, Rust, and PHP](assets/integrations.png)](https://www.meilisearch.com/docs/learn/what_is_meilisearch/sdks?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=sdks-logos)\n\n## âš™ï¸ Advanced usage\n\nExperienced users will want to keep our [API Reference](https://www.meilisearch.com/docs/reference/api/overview?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=advanced) close at hand.\n\nWe also offer a wide range of dedicated guides to all Meilisearch features, such as [filtering](https://www.meilisearch.com/docs/learn/fine_tuning_results/filtering?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=advanced), [sorting](https://www.meilisearch.com/docs/learn/fine_tuning_results/sorting?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=advanced), [geosearch](https://www.meilisearch.com/docs/learn/fine_tuning_results/geosearch?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=advanced), [API keys](https://www.meilisearch.com/docs/learn/security/master_api_keys?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=advanced), and [tenant tokens](https://www.meilisearch.com/docs/learn/security/tenant_tokens?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=advanced).\n\nFinally, for more in-depth information, refer to our articles explaining fundamental Meilisearch concepts such as [documents](https://www.meilisearch.com/docs/learn/core_concepts/documents?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=advanced) and [indexes](https://www.meilisearch.com/docs/learn/core_concepts/indexes?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=advanced).\n\n## ğŸ§¾ Editions & Licensing\n\nMeilisearch is available in two editions:\n\n### ğŸ§ª Community Edition (CE)\n\n- Fully open source under the [MIT license](./LICENSE)\n- Core search engine with fast and relevant full-text, semantic or hybrid search\n- Free to use for anyone, including commercial usage\n\n### ğŸ¢ Enterprise Edition (EE)\n\n- Includes advanced features such as:\n  - Sharding\n- Governed by a [commercial license](./LICENSE-EE) or the [Business Source License 1.1](https://mariadb.com/bsl11)\n- Not allowed in production without a commercial agreement with Meilisearch.\n  - You may use, modify, and distribute the Licensed Work for non-production purposes only, such as testing, development, or evaluation.\n\nWant access to Enterprise features? â†’ Contact us at [sales@meilisearch.com](maito:sales@meilisearch.com).\n\n## ğŸ“Š Telemetry\n\nMeilisearch collects **anonymized** user data to help us improve our product. You can [deactivate this](https://www.meilisearch.com/docs/learn/what_is_meilisearch/telemetry?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=telemetry#how-to-disable-data-collection) whenever you want.\n\nTo request deletion of collected data, please write to us at [privacy@meilisearch.com](mailto:privacy@meilisearch.com). Remember to include your `Instance UID` in the message, as this helps us quickly find and delete your data.\n\nIf you want to know more about the kind of data we collect and what we use it for, check the [telemetry section](https://www.meilisearch.com/docs/learn/what_is_meilisearch/telemetry?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=telemetry#how-to-disable-data-collection) of our documentation.\n\n## ğŸ“« Get in touch!\n\nMeilisearch is a search engine created by [Meili](https://www.meilisearch.com/careers), a software development company headquartered in France and with team members all over the world. Want to know more about us? [Check out our blog!](https://blog.meilisearch.com/?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=contact)\n\nğŸ— [Subscribe to our newsletter](https://share-eu1.hsforms.com/1LN5N0x_GQgq7ss7tXmSykwfg3aq) if you don''t want to miss any updates! We promise we won''t clutter your mailbox: we only send one edition every two months.\n\nğŸ’Œ Want to make a suggestion or give feedback? Here are some of the channels where you can reach us:\n\n- For feature requests, please visit our [product repository](https://github.com/meilisearch/product/discussions)\n- Found a bug? Open an [issue](https://github.com/meilisearch/meilisearch/issues)!\n- Want to be part of our Discord community? [Join us!](https://discord.meilisearch.com/?utm_campaign=oss&utm_source=github&utm_medium=meilisearch&utm_content=contact)\n\nThank you for your support!\n\n## ğŸ‘©â€ğŸ’» Contributing\n\nMeilisearch is, and will always be, open-source! If you want to contribute to the project, please look at [our contribution guidelines](CONTRIBUTING.md).\n\n## ğŸ“¦ Versioning\n\nMeilisearch releases and their associated binaries are available on the project''s [releases page](https://github.com/meilisearch/meilisearch/releases).\n\nThe binaries are versioned following [SemVer conventions](https://semver.org/). To know more, read our [versioning policy](./documentation/versioning-policy.md).\n\nDifferently from the binaries, crates in this repository are not currently available on [crates.io](https://crates.io/) and do not follow [SemVer conventions](https://semver.org).\n', '{"language":"Rust","stars":54842,"forks":2290,"watchers":54842,"open_issues":276,"topics":["ai","api","app-search","database","enterprise-search","faceting","full-text-search","fuzzy-search","geosearch","hybrid-search","instantsearch","search","search-as-you-type","search-engine","semantic-search","site-search","typo-tolerance","vector-database","vector-search","vectors"],"default_branch":"main","size_kb":85307,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:meilisearch:meilisearch","source_url":"https://github.com/meilisearch/meilisearch"},{"type":"has_code","target_id":"github:meilisearch:meilisearch","source_url":"https://github.com/meilisearch/meilisearch"},{"type":"has_code","target_id":"github:meilisearch:demos","source_url":"https://github.com/meilisearch/demos"},{"type":"has_code","target_id":"github:meilisearch:meilisearch-mcp","source_url":"https://github.com/meilisearch/meilisearch-mcp"},{"type":"has_code","target_id":"github:meilisearch:product","source_url":"https://github.com/meilisearch/product"},{"type":"has_code","target_id":"github:meilisearch:meilisearch","source_url":"https://github.com/meilisearch/meilisearch"},{"type":"has_code","target_id":"github:meilisearch:meilisearch","source_url":"https://github.com/meilisearch/meilisearch"}]', NULL, 'NOASSERTION', 'approved', 80, 'b33202220af15ad8f0fe031afa0102d1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-meilisearch-meilisearch from https://github.com/meilisearch.png
Image converted to WebP: data/images/github-meilisearch-meilisearch.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-lencx-ChatGPT', 'github--lencx--chatgpt', 'ChatGPT', 'lencx', '<p align="center"> <img width="180" src="./public/ChatGPT.png" alt="ChatGPT"> <p align="center">ChatGPT Desktop Application (Available on Mac, Windows, and Linux)</p> </p> <a href="https://www.buymeacoffee.com/lencx" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-blue.png" alt="Buy Me A Coffee" style="height: 40px !important;width: 145px !important;" ></a> --- > [!NOTE] > **If you want to experience a more powerful AI wrapper application, you can try the Noi (https:...', '["ai","app","application","chatgpt","desktop-app","gpt","gpt-3","linux","macos","notes-app","openai","rust","tauri","webview","windows","rust"]', 'other', 54349, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/lencx/ChatGPT","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <img width="180" src="./public/ChatGPT.png" alt="ChatGPT">\n  <p align="center">ChatGPT Desktop Application (Available on Mac, Windows, and Linux)</p>\n</p>\n\n[![ChatGPT downloads](https://img.shields.io/github/downloads/lencx/ChatGPT/total.svg?style=flat-square)](https://github.com/lencx/ChatGPT/releases)\n[![chat](https://img.shields.io/badge/chat-discord-blue?style=flat&logo=discord)](https://discord.gg/aPhCRf4zZr)\n[![twitter](https://img.shields.io/badge/follow-lencx__-blue?style=flat&logo=Twitter)](https://twitter.com/lencx_)\n[![youtube](https://img.shields.io/youtube/channel/subscribers/UC__gTZL-OZKDPic7s_6Ntgg?style=social)](https://www.youtube.com/@lencx)\n\n<a href="https://www.buymeacoffee.com/lencx" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-blue.png" alt="Buy Me A Coffee" style="height: 40px !important;width: 145px !important;" ></a>\n\n---\n\n> [!NOTE]\n> **If you want to experience a more powerful AI wrapper application, you can try the Noi (https://github.com/lencx/Noi), which is a successor to the ChatGPT desktop application concept.**\n\nThank you very much for your interest in this project. OpenAI has now released the macOS version of the application, and a Windows version will be available later ([Introducing GPT-4o and more tools to ChatGPT free users](https://openai.com/index/gpt-4o-and-more-tools-to-chatgpt-free/)). If you prefer the official application, you can stay updated with the latest information from OpenAI.\n\nIf you want to learn about or download the previous version (v1.1.0), please click [here](https://github.com/lencx/ChatGPT/tree/release-v1.1.0).\n\nI am currently looking for some differentiating features to develop version 2.0. If you are interested in this, please stay tuned.\n\n![](./docs/static/chatgpt-v2.gif)', '{"language":"Rust","stars":54349,"forks":6198,"watchers":54349,"open_issues":893,"topics":["ai","app","application","chatgpt","desktop-app","gpt","gpt-3","linux","macos","notes-app","openai","rust","tauri","webview","windows"],"default_branch":"v2-dev","size_kb":32499,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:lencx:ChatGPT","source_url":"https://github.com/lencx/ChatGPT"},{"type":"has_code","target_id":"github:lencx:Noi","source_url":"https://github.com/lencx/Noi"},{"type":"has_code","target_id":"github:lencx:ChatGPT","source_url":"https://github.com/lencx/ChatGPT"}]', NULL, NULL, 'pending', 40, '26e808617996cc76ddbc1113b7722457', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-lencx-ChatGPT from https://github.com/lencx.png
Image converted to WebP: data/images/github-lencx-ChatGPT.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-github-spec-kit', 'github--github--spec-kit', 'spec-kit', 'github', '<div align="center"> <img src="./media/logo_large.webp" alt="Spec Kit Logo" width="200" height="200"/> <h1>ğŸŒ± Spec Kit</h1> <h3><em>Build high-quality software faster.</em></h3> </div> <p align="center"> <strong>An open source toolkit that allows you to focus on product scenarios and predictable outcomes instead of vibe coding every piece from scratch.</strong> </p> <p align="center"> <a href="https://github.com/github/spec-kit/actions/workflows/release.yml"><img src="https://github.com/githu...', '["ai","copilot","development","engineering","prd","spec","spec-driven","python"]', 'other', 54023, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/github/spec-kit","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n    <img src="./media/logo_large.webp" alt="Spec Kit Logo" width="200" height="200"/>\n    <h1>ğŸŒ± Spec Kit</h1>\n    <h3><em>Build high-quality software faster.</em></h3>\n</div>\n\n<p align="center">\n    <strong>An open source toolkit that allows you to focus on product scenarios and predictable outcomes instead of vibe coding every piece from scratch.</strong>\n</p>\n\n<p align="center">\n    <a href="https://github.com/github/spec-kit/actions/workflows/release.yml"><img src="https://github.com/github/spec-kit/actions/workflows/release.yml/badge.svg" alt="Release"/></a>\n    <a href="https://github.com/github/spec-kit/stargazers"><img src="https://img.shields.io/github/stars/github/spec-kit?style=social" alt="GitHub stars"/></a>\n    <a href="https://github.com/github/spec-kit/blob/main/LICENSE"><img src="https://img.shields.io/github/license/github/spec-kit" alt="License"/></a>\n    <a href="https://github.github.io/spec-kit/"><img src="https://img.shields.io/badge/docs-GitHub_Pages-blue" alt="Documentation"/></a>\n</p>\n\n---\n\n## Table of Contents\n\n- [ğŸ¤” What is Spec-Driven Development?](#-what-is-spec-driven-development)\n- [âš¡ Get Started](#-get-started)\n- [ğŸ“½ï¸ Video Overview](#ï¸-video-overview)\n- [ğŸ¤– Supported AI Agents](#-supported-ai-agents)\n- [ğŸ”§ Specify CLI Reference](#-specify-cli-reference)\n- [ğŸ“š Core Philosophy](#-core-philosophy)\n- [ğŸŒŸ Development Phases](#-development-phases)\n- [ğŸ¯ Experimental Goals](#-experimental-goals)\n- [ğŸ”§ Prerequisites](#-prerequisites)\n- [ğŸ“– Learn More](#-learn-more)\n- [ğŸ“‹ Detailed Process](#-detailed-process)\n- [ğŸ” Troubleshooting](#-troubleshooting)\n- [ğŸ‘¥ Maintainers](#-maintainers)\n- [ğŸ’¬ Support](#-support)\n- [ğŸ™ Acknowledgements](#-acknowledgements)\n- [ğŸ“„ License](#-license)\n\n## ğŸ¤” What is Spec-Driven Development?\n\nSpec-Driven Development **flips the script** on traditional software development. For decades, code has been king â€” specifications were just scaffolding we built and discarded once the "real work" of coding began. Spec-Driven Development changes this: **specifications become executable**, directly generating working implementations rather than just guiding them.\n\n## âš¡ Get Started\n\n### 1. Install Specify CLI\n\nChoose your preferred installation method:\n\n#### Option 1: Persistent Installation (Recommended)\n\nInstall once and use everywhere:\n\n```bash\nuv tool install specify-cli --from git+https://github.com/github/spec-kit.git\n```\n\nThen use the tool directly:\n\n```bash\n# Create new project\nspecify init <PROJECT_NAME>\n\n# Or initialize in existing project\nspecify init . --ai claude\n# or\nspecify init --here --ai claude\n\n# Check installed tools\nspecify check\n```\n\nTo upgrade Specify, see the [Upgrade Guide](./docs/upgrade.md) for detailed instructions. Quick upgrade:\n\n```bash\nuv tool install specify-cli --force --from git+https://github.com/github/spec-kit.git\n```\n\n#### Option 2: One-time Usage\n\nRun directly without installing:\n\n```bash\nuvx --from git+https://github.com/github/spec-kit.git specify init <PROJECT_NAME>\n```\n\n**Benefits of persistent installation:**\n\n- Tool stays installed and available in PATH\n- No need to create shell aliases\n- Better tool management with `uv tool list`, `uv tool upgrade`, `uv tool uninstall`\n- Cleaner shell configuration\n\n### 2. Establish project principles\n\nLaunch your AI assistant in the project directory. The `/speckit.*` commands are available in the assistant.\n\nUse the **`/speckit.constitution`** command to create your project''s governing principles and development guidelines that will guide all subsequent development.\n\n```bash\n/speckit.constitution Create principles focused on code quality, testing standards, user experience consistency, and performance requirements\n```\n\n### 3. Create the spec\n\nUse the **`/speckit.specify`** command to describe what you want to build. Focus on the **what** and **why**, not the tech stack.\n\n```bash\n/speckit.specify Build an application that can help me organize my photos in separate photo albums. Albums are grouped by date and can be re-organized by dragging and dropping on the main page. Albums are never in other nested albums. Within each album, photos are previewed in a tile-like interface.\n```\n\n### 4. Create a technical implementation plan\n\nUse the **`/speckit.plan`** command to provide your tech stack and architecture choices.\n\n```bash\n/speckit.plan The application uses Vite with minimal number of libraries. Use vanilla HTML, CSS, and JavaScript as much as possible. Images are not uploaded anywhere and metadata is stored in a local SQLite database.\n```\n\n### 5. Break down into tasks\n\nUse **`/speckit.tasks`** to create an actionable task list from your implementation plan.\n\n```bash\n/speckit.tasks\n```\n\n### 6. Execute implementation\n\nUse **`/speckit.implement`** to execute all tasks and build your feature according to the plan.\n\n```bash\n/speckit.implement\n```\n\nFor detailed step-by-step instructions, see our [comprehensive guide](./spec-driven.md).\n\n## ğŸ“½ï¸ Video Overview\n\nWant to see Spec Kit in action? Watch our [video overview](https://www.youtube.com/watch?v=a9eR1xsfvHg&pp=0gcJCckJAYcqIYzv)!\n\n[![Spec Kit video header](/media/spec-kit-video-header.jpg)](https://www.youtube.com/watch?v=a9eR1xsfvHg&pp=0gcJCckJAYcqIYzv)\n\n## ğŸ¤– Supported AI Agents\n\n| Agent                                                                                | Support | Notes                                                                                                                                     |\n| ------------------------------------------------------------------------------------ | ------- | ----------------------------------------------------------------------------------------------------------------------------------------- |\n| [Qoder CLI](https://qoder.com/cli)                                                   | âœ…      |                                                                                                                                           |\n| [Amazon Q Developer CLI](https://aws.amazon.com/developer/learning/q-developer-cli/) | âš ï¸      | Amazon Q Developer CLI [does not support](https://github.com/aws/amazon-q-developer-cli/issues/3064) custom arguments for slash commands. |\n| [Amp](https://ampcode.com/)                                                          | âœ…      |                                                                                                                                           |\n| [Auggie CLI](https://docs.augmentcode.com/cli/overview)                              | âœ…      |                                                                                                                                           |\n| [Claude Code](https://www.anthropic.com/claude-code)                                 | âœ…      |                                                                                                                                           |\n| [CodeBuddy CLI](https://www.codebuddy.ai/cli)                                        | âœ…      |                                                                                                                                           |\n| [Codex CLI](https://github.com/openai/codex)                                         | âœ…      |                                                                                                                                           |\n| [Cursor](https://cursor.sh/)                                                         | âœ…      |                                                                                                                                           |\n| [Gemini CLI](https://github.com/google-gemini/gemini-cli)                            | âœ…      |                                                                                                                                           |\n| [GitHub Copilot](https://code.visualstudio.com/)                                     | âœ…      |                                                                                                                                           |\n| [IBM Bob](https://www.ibm.com/products/bob)                                          | âœ…      | IDE-based agent with slash command support                                                                                                |\n| [Jules](https://jules.google.com/)                                                   | âœ…      |                                                                                                                                           |\n| [Kilo Code](https://github.com/Kilo-Org/kilocode)                                    | âœ…      |                                                                                                                                           |\n| [opencode](https://opencode.ai/)                                                     | âœ…      |                                                                                                                                           |\n| [Qwen Code](https://github.com/QwenLM/qwen-code)                                     | âœ…      |                                                                                                                                           |\n| [Roo Code](https://roocode.com/)                                                     | âœ…      |                                                                                                                                           |\n| [SHAI (OVHcloud)](https://github.com/ovh/shai)                                       | âœ…      |                                                                                                                                           |\n| [Windsurf](https://windsurf.com/)                                                    | âœ…      |                                                                                                                                           |\n\n## ğŸ”§ Specify CLI Reference\n\nThe `specify` command supports the following options:\n\n### Commands\n\n| Command | Description                                                                                                                                             |\n| ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `init`  | Initialize a new Specify project from the latest template                                                                                               |\n| `check` | Check for installed tools (`git`, `claude`, `gemini`, `code`/`code-insiders`, `cursor-agent`, `windsurf`, `qwen`, `opencode`, `codex`, `shai`, `qoder`) |\n\n### `specify init` Arguments & Options\n\n| Argument/Option        | Type     | Description                                                                                                                                                                                  |\n| ---------------------- | -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `<project-name>`       | Argument | Name for your new project directory (optional if using `--here`, or use `.` for current directory)                                                                                           |\n| `--ai`                 | Option   | AI assistant to use: `claude`, `gemini`, `copilot`, `cursor-agent`, `qwen`, `opencode`, `codex`, `windsurf`, `kilocode`, `auggie`, `roo`, `codebuddy`, `amp`, `shai`, `q`, `bob`, or `qoder` |\n| `--script`             | Option   | Script variant to use: `sh` (bash/zsh) or `ps` (PowerShell)                                                                                                                                  |\n| `--ignore-agent-tools` | Flag     | Skip checks for AI agent tools like Claude Code                                                                                                                                              |\n| `--no-git`             | Flag     | Skip git repository initialization                                                                                                                                                           |\n| `--here`               | Flag     | Initialize project in the current directory instead of creating a new one                                                                                                                    |\n| `--force`              | Flag     | Force merge/overwrite when initializing in current directory (skip confirmation)                                                                                                             |\n| `--skip-tls`           | Flag     | Skip SSL/TLS verification (not recommended)                                                                                                                                                  |\n| `--debug`              | Flag     | Enable detailed debug output for troubleshooting                                                                                                                                             |\n| `--github-token`       | Option   | GitHub token for API requests (or set GH_TOKEN/GITHUB_TOKEN env variable)                                                                                                                    |\n\n### Examples\n\n```bash\n# Basic project initialization\nspecify init my-project\n\n# Initialize with specific AI assistant\nspecify init my-project --ai claude\n\n# Initialize with Cursor support\nspecify init my-project --ai cursor-agent\n\n# Initialize with Qoder support\nspecify init my-project --ai qoder\n\n# Initialize with Windsurf support\nspecify init my-project --ai windsurf\n\n# Initialize with Amp support\nspecify init my-project --ai amp\n\n# Initialize with SHAI support\nspecify init my-project --ai shai\n\n# Initialize with IBM Bob support\nspecify init my-project --ai bob\n\n# Initialize with PowerShell scripts (Windows/cross-platform)\nspecify init my-project --ai copilot --script ps\n\n# Initialize in current directory\nspecify init . --ai copilot\n# or use the --here flag\nspecify init --here --ai copilot\n\n# Force merge into current (non-empty) directory without confirmation\nspecify init . --force --ai copilot\n# or\nspecify init --here --force --ai copilot\n\n# Skip git initialization\nspecify init my-project --ai gemini --no-git\n\n# Enable debug output for troubleshooting\nspecify init my-project --ai claude --debug\n\n# Use GitHub token for API requests (helpful for corporate environments)\nspecify init my-project --ai claude --github-token ghp_your_token_here\n\n# Check system requirements\nspecify check\n```\n\n### Available Slash Commands\n\nAfter running `specify init`, your AI coding agent will have access to these slash commands for structured development:\n\n#### Core Commands\n\nEssential commands for the Spec-Driven Development workflow:\n\n| Command                 | Description                                                              |\n| ----------------------- | ------------------------------------------------------------------------ |\n| `/speckit.constitution` | Create or update project governing principles and development guidelines |\n| `/speckit.specify`      | Define what you want to build (requirements and user stories)            |\n| `/speckit.plan`         | Create technical implementation plans with your chosen tech stack        |\n| `/speckit.tasks`        | Generate actionable task lists for implementation                        |\n| `/speckit.implement`    | Execute all tasks to build the feature according to the plan             |\n\n#### Optional Commands\n\nAdditional commands for enhanced quality and validation:\n\n| Command              | Description                                                                                                                          |\n| -------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |\n| `/speckit.clarify`   | Clarify underspecified areas (recommended before `/speckit.plan`; formerly `/quizme`)                                                |\n| `/speckit.analyze`   | Cross-artifact consistency & coverage analysis (run after `/speckit.tasks`, before `/speckit.implement`)                             |\n| `/speckit.checklist` | Generate custom quality checklists that validate requirements completeness, clarity, and consistency (like "unit tests for English") |\n\n### Environment Variables\n\n| Variable          | Description                                                                                                                                                                                                                                                                                            |\n| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| `SPECIFY_FEATURE` | Override feature detection for non-Git repositories. Set to the feature directory name (e.g., `001-photo-albums`) to work on a specific feature when not using Git branches.<br/>\*\*Must be set in the context of the agent you''re working with prior to using `/speckit.plan` or follow-up commands. |\n\n## ğŸ“š Core Philosophy\n\nSpec-Driven Development is a structured process that emphasizes:\n\n- **Intent-driven development** where specifications define the "*what*" before the "*how*"\n- **Rich specification creation** using guardrails and organizational principles\n- **Multi-step refinement** rather than one-shot code generation from prompts\n- **Heavy reliance** on advanced AI model capabilities for specification interpretation\n\n## ğŸŒŸ Development Phases\n\n| Phase                                    | Focus                    | Key Activities                                                                                                                                                     |\n| ---------------------------------------- | ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| **0-to-1 Development** ("Greenfield")    | Generate from scratch    | <ul><li>Start with high-level requirements</li><li>Generate specifications</li><li>Plan implementation steps</li><li>Build production-ready applications</li></ul> |\n| **Creative Exploration**                 | Parallel implementations | <ul><li>Explore diverse solutions</li><li>Support multiple technology stacks & architectures</li><li>Experiment with UX patterns</li></ul>                         |\n| **Iterative Enhancement** ("Brownfield") | Brownfield modernization | <ul><li>Add features iteratively</li><li>Modernize legacy systems</li><li>Adapt processes</li></ul>                                                                |\n\n## ğŸ¯ Experimental Goals\n\nOur research and experimentation focus on:\n\n### Technology independence\n\n- Create applications using diverse technology stacks\n- Validate the hypothesis that Spec-Driven Development is a process not tied to specific technologies, programming languages, or frameworks\n\n### Enterprise constraints\n\n- Demonstrate mission-critical application development\n- Incorporate organizational constraints (cloud providers, tech stacks, engineering practices)\n- Support enterprise design systems and compliance requirements\n\n### User-centric development\n\n- Build applications for different user cohorts and preferences\n- Support various development approaches (from vibe-coding to AI-native development)\n\n### Creative & iterative processes\n\n- Validate the concept of parallel implementation exploration\n- Provide robust iterative feature development workflows\n- Extend processes to handle upgrades and modernization tasks\n\n## ğŸ”§ Prerequisites\n\n- **Linux/macOS/Windows**\n- [Supported](#-supported-ai-agents) AI coding agent.\n- [uv](https://docs.astral.sh/uv/) for package management\n- [Python 3.11+](https://www.python.org/downloads/)\n- [Git](https://git-scm.com/downloads)\n\nIf you encounter issues with an agent, please open an issue so we can refine the integration.\n\n## ğŸ“– Learn More\n\n- **[Complete Spec-Driven Development Methodology](./spec-driven.md)** - Deep dive into the full process\n- **[Detailed Walkthrough](#-detailed-process)** - Step-by-step implementation guide\n\n---\n\n## ğŸ“‹ Detailed Process\n\n<details>\n<summary>Click to expand the detailed step-by-step walkthrough</summary>\n\nYou can use the Specify CLI to bootstrap your project, which will bring in the required artifacts in your environment. Run:\n\n```bash\nspecify init <project_name>\n```\n\nOr initialize in the current directory:\n\n```bash\nspecify init .\n# or use the --here flag\nspecify init --here\n# Skip confirmation when the directory already has files\nspecify init . --force\n# or\nspecify init --here --force\n```\n\n![Specify CLI bootstrapping a new project in the terminal](./media/specify_cli.gif)\n\nYou will be prompted to select the AI agent you are using. You can also proactively specify it directly in the terminal:\n\n```bash\nspecify init <project_name> --ai claude\nspecify init <project_name> --ai gemini\nspecify init <project_name> --ai copilot\n\n# Or in current directory:\nspecify init . --ai claude\nspecify init . --ai codex\n\n# or use --here flag\nspecify init --here --ai claude\nspecify init --here --ai codex\n\n# Force merge into a non-empty current directory\nspecify init . --force --ai claude\n\n# or\nspecify init --here --force --ai claude\n```\n\nThe CLI will check if you have Claude Code, Gemini CLI, Cursor CLI, Qwen CLI, opencode, Codex CLI, Qoder CLI, or Amazon Q Developer CLI installed. If you do not, or you prefer to get the templates without checking for the right tools, use `--ignore-agent-tools` with your command:\n\n```bash\nspecify init <project_name> --ai claude --ignore-agent-tools\n```\n\n### **STEP 1:** Establish project principles\n\nGo to the project folder and run your AI agent. In our example, we''re using `claude`.\n\n![Bootstrapping Claude Code environment](./media/bootstrap-claude-code.gif)\n\nYou will know that things are configured correctly if you see the `/speckit.constitution`, `/speckit.specify`, `/speckit.plan`, `/speckit.tasks`, and `/speckit.implement` commands available.\n\nThe first step should be establishing your project''s governing principles using the `/speckit.constitution` command. This helps ensure consistent decision-making throughout all subsequent development phases:\n\n```text\n/speckit.constitution Create principles focused on code quality, testing standards, user experience consistency, and performance requirements. Include governance for how these principles should guide technical decisions and implementation choices.\n```\n\nThis step creates or updates the `.specify/memory/constitution.md` file with your project''s foundational guidelines that the AI agent will reference during specification, planning, and implementation phases.\n\n### **STEP 2:** Create project specifications\n\nWith your project principles established, you can now create the functional specifications. Use the `/speckit.specify` command and then provide the concrete requirements for the project you want to develop.\n\n> [!IMPORTANT]\n> Be as explicit as possible about *what* you are trying to build and *why*. **Do not focus on the tech stack at this point**.\n\nAn example prompt:\n\n```text\nDevelop Taskify, a team productivity platform. It should allow users to create projects, add team members,\nassign tasks, comment and move tasks between boards in Kanban style. In this initial phase for this feature,\nlet''s call it "Create Taskify," let''s have multiple users but the users will be declared ahead of time, predefined.\nI want five users in two different categories, one product manager and four engineers. Let''s create three\ndifferent sample projects. Let''s have the standard Kanban columns for the status of each task, such as "To Do,"\n"In Progress," "In Review," and "Done." There will be no login for this application as this is just the very\nfirst testing thing to ensure that our basic features are set up. For each task in the UI for a task card,\nyou should be able to change the current status of the task between the different columns in the Kanban work board.\nYou should be able to leave an unlimited number of comments for a particular card. You should be able to, from that task\ncard, assign one of the valid users. When you first launch Taskify, it''s going to give you a list of the five users to pick\nfrom. There will be no password required. When you click on a user, you go into the main view, which displays the list of\nprojects. When you click on a project, you open the Kanban board for that project. You''re going to see the columns.\nYou''ll be able to drag and drop cards back and forth between different columns. You will see any cards that are\nassigned to you, the currently logged in user, in a different color from all the other ones, so you can quickly\nsee yours. You can edit any comments that you make, but you can''t edit comments that other people made. You can\ndelete any comments that you made, but you can''t delete comments anybody else made.\n```\n\nAfter this prompt is entered, you should see Claude Code kick off the planning and spec drafting process. Claude Code will also trigger some of the built-in scripts to set up the repository.\n\nOnce this step is completed, you should have a new branch created (e.g., `001-create-taskify`), as well as a new specification in the `specs/001-create-taskify` directory.\n\nThe produced specification should contain a set of user stories and functional requirements, as defined in the template.\n\nAt this stage, your project folder contents should resemble the following:\n\n```text\nâ””â”€â”€ .specify\n    â”œâ”€â”€ memory\n    â”‚  â””â”€â”€ constitution.md\n    â”œâ”€â”€ scripts\n    â”‚  â”œâ”€â”€ check-prerequisites.sh\n    â”‚  â”œâ”€â”€ common.sh\n    â”‚  â”œâ”€â”€ create-new-feature.sh\n    â”‚  â”œâ”€â”€ setup-plan.sh\n    â”‚  â””â”€â”€ update-claude-md.sh\n    â”œâ”€â”€ specs\n    â”‚  â””â”€â”€ 001-create-taskify\n    â”‚      â””â”€â”€ spec.md\n    â””â”€â”€ templates\n        â”œâ”€â”€ plan-template.md\n        â”œâ”€â”€ spec-template.md\n        â””â”€â”€ tasks-template.md\n```\n\n### **STEP 3:** Functional specification clarification (required before planning)\n\nWith the baseline specification created, you can go ahead and clarify any of the requirements that were not captured properly within the first shot attempt.\n\nYou should run the structured clarification workflow **before** creating a technical plan to reduce rework downstream.\n\nPreferred order:\n\n1. Use `/speckit.clarify` (structured) â€“ sequential, coverage-based questioning that records answers in a Clarifications section.\n2. Optionally follow up with ad-hoc free-form refinement if something still feels vague.\n\nIf you intentionally want to skip clarification (e.g., spike or exploratory prototype), explicitly state that so the agent doesn''t block on missing clarifications.\n\nExample free-form refinement prompt (after `/speckit.clarify` if still needed):\n\n```text\nFor each sample project or project that you create there should be a variable number of tasks between 5 and 15\ntasks for each one randomly distributed into different states of completion. Make sure that there''s at least\none task in each stage of completion.\n```\n\nYou should also ask Claude Code to validate the **Review & Acceptance Checklist**, checking off the things that are validated/pass the requirements, and leave the ones that are not unchecked. The following prompt can be used:\n\n```text\nRead the review and acceptance checklist, and check off each item in the checklist if the feature spec meets the criteria. Leave it empty if it does not.\n```\n\nIt''s important to use the interaction with Claude Code as an opportunity to clarify and ask questions around the specification - **do not treat its first attempt as final**.\n\n### **STEP 4:** Generate a plan\n\nYou can now be specific about the tech stack and other technical requirements. You can use the `/speckit.plan` command that is built into the project template with a prompt like this:\n\n```text\nWe are going to generate this using .NET Aspire, using Postgres as the database. The frontend should use\nBlazor server with drag-and-drop task boards, real-time updates. There should be a REST API created with a projects API,\ntasks API, and a notifications API.\n```\n\nThe output of this step will include a number of implementation detail documents, with your directory tree resembling this:\n\n```text\n.\nâ”œâ”€â”€ CLAUDE.md\nâ”œâ”€â”€ memory\nâ”‚  â””â”€â”€ constitution.md\nâ”œâ”€â”€ scripts\nâ”‚  â”œâ”€â”€ check-prerequisites.sh\nâ”‚  â”œâ”€â”€ common.sh\nâ”‚  â”œâ”€â”€ create-new-feature.sh\nâ”‚  â”œâ”€â”€ setup-plan.sh\nâ”‚  â””â”€â”€ update-claude-md.sh\nâ”œâ”€â”€ specs\nâ”‚  â””â”€â”€ 001-create-taskify\nâ”‚      â”œâ”€â”€ contracts\nâ”‚      â”‚  â”œâ”€â”€ api-spec.json\nâ”‚      â”‚  â””â”€â”€ signalr-spec.md\nâ”‚      â”œâ”€â”€ data-model.md\nâ”‚      â”œâ”€â”€ plan.md\nâ”‚      â”œâ”€â”€ quickstart.md\nâ”‚      â”œâ”€â”€ research.md\nâ”‚      â””â”€â”€ spec.md\nâ””â”€â”€ templates\n    â”œâ”€â”€ CLAUDE-template.md\n    â”œâ”€â”€ plan-template.md\n    â”œâ”€â”€ spec-template.md\n    â””â”€â”€ tasks-template.md\n```\n\nCheck the `research.md` document to ensure that the right tech stack is used, based on your instructions. You can ask Claude Code to refine it if any of the components stand out, or even have it check the locally-installed version of the platform/framework you want to use (e.g., .NET).\n\nAdditionally, you might want to ask Claude Code to research details about the chosen tech stack if it''s something that is rapidly changing (e.g., .NET Aspire, JS frameworks), with a prompt like this:\n\n```text\nI want you to go through the implementation plan and implementation details, looking for areas that could\nbenefit from additional research as .NET Aspire is a rapidly changing library. For those areas that you identify that\nrequire further research, I want you to update the research document with additional details about the specific\nversions that we are going to be using in this Taskify application and spawn parallel research tasks to clarify\nany details using research from the web.\n```\n\nDuring this process, you might find that Claude Code gets stuck researching the wrong thing - you can help nudge it in the right direction with a prompt like this:\n\n```text\nI think we need to break this down into a series of steps. First, identify a list of tasks\nthat you would need to do during implementation that you''re not sure of or would benefit\nfrom further research. Write down a list of those tasks. And then for each one of these tasks,\nI want you to spin up a separate research task so that the net results is we are researching\nall of those very specific tasks in parallel. What I saw you doing was it looks like you were\nresearching .NET Aspire in general and I don''t think that''s gonna do much for us in this case.\nThat''s way too untargeted research. The research needs to help you solve a specific targeted question.\n```\n\n> [!NOTE]\n> Claude Code might be over-eager and add components that you did not ask for. Ask it to clarify the rationale and the source of the change.\n\n### **STEP 5:** Have Claude Code validate the plan\n\nWith the plan in place, you should have Claude Code run through it to make sure that there are no missing pieces. You can use a prompt like this:\n\n```text\nNow I want you to go and audit the implementation plan and the implementation detail files.\nRead through it with an eye on determining whether or not there is a sequence of tasks that you need\nto be doing that are obvious from reading this. Because I don''t know if there''s enough here. For example,\nwhen I look at the core implementation, it would be useful to reference the appropriate places in the implementation\ndetails where it can find the information as it walks through each step in the core implementation or in the refinement.\n```\n\nThis helps refine the implementation plan and helps you avoid potential blind spots that Claude Code missed in its planning cycle. Once the initial refinement pass is complete, ask Claude Code to go through the checklist once more before you can get to the implementation.\n\nYou can also ask Claude Code (if you have the [GitHub CLI](https://docs.github.com/en/github-cli/github-cli) installed) to go ahead and create a pull request from your current branch to `main` with a detailed description, to make sure that the effort is properly tracked.\n\n> [!NOTE]\n> Before you have the agent implement it, it''s also worth prompting Claude Code to cross-check the details to see if there are any over-engineered pieces (remember - it can be over-eager). If over-engineered components or decisions exist, you can ask Claude Code to resolve them. Ensure that Claude Code follows the [constitution](base/memory/constitution.md) as the foundational piece that it must adhere to when establishing the plan.\n\n### **STEP 6:** Generate task breakdown with /speckit.tasks\n\nWith the implementation plan validated, you can now break down the plan into specific, actionable tasks that can be executed in the correct order. Use the `/speckit.tasks` command to automatically generate a detailed task breakdown from your implementation plan:\n\n```text\n/speckit.tasks\n```\n\nThis step creates a `tasks.md` file in your feature specification directory that contains:\n\n- **Task breakdown organized by user story** - Each user story becomes a separate implementation phase with its own set of tasks\n- **Dependency management** - Tasks are ordered to respect dependencies between components (e.g., models before services, services before endpoints)\n- **Parallel execution markers** - Tasks that can run in parallel are marked with `[P]` to optimize development workflow\n- **File path specifications** - Each task includes the exact file paths where implementation should occur\n- **Test-driven development structure** - If tests are requested, test tasks are included and ordered to be written before implementation\n- **Checkpoint validation** - Each user story phase includes checkpoints to validate independent functionality\n\nThe generated tasks.md provides a clear roadmap for the `/speckit.implement` command, ensuring systematic implementation that maintains code quality and allows for incremental delivery of user stories.\n\n### **STEP 7:** Implementation\n\nOnce ready, use the `/speckit.implement` command to execute your implementation plan:\n\n```text\n/speckit.implement\n```\n\nThe `/speckit.implement` command will:\n\n- Validate that all prerequisites are in place (constitution, spec, plan, and tasks)\n- Parse the task breakdown from `tasks.md`\n- Execute tasks in the correct order, respecting dependencies and parallel execution markers\n- Follow the TDD approach defined in your task plan\n- Provide progress updates and handle errors appropriately\n\n> [!IMPORTANT]\n> The AI agent will execute local CLI commands (such as `dotnet`, `npm`, etc.) - make sure you have the required tools installed on your machine.\n\nOnce the implementation is complete, test the application and resolve any runtime errors that may not be visible in CLI logs (e.g., browser console errors). You can copy and paste such errors back to your AI agent for resolution.\n\n</details>\n\n---\n\n## ğŸ” Troubleshooting\n\n### Git Credential Manager on Linux\n\nIf you''re having issues with Git authentication on Linux, you can install Git Credential Manager:\n\n```bash\n#!/usr/bin/env bash\nset -e\necho "Downloading Git Credential Manager v2.6.1..."\nwget https://github.com/git-ecosystem/git-credential-manager/releases/download/v2.6.1/gcm-linux_amd64.2.6.1.deb\necho "Installing Git Credential Manager..."\nsudo dpkg -i gcm-linux_amd64.2.6.1.deb\necho "Configuring Git to use GCM..."\ngit config --global credential.helper manager\necho "Cleaning up..."\nrm gcm-linux_amd64.2.6.1.deb\n```\n\n## ğŸ‘¥ Maintainers\n\n- Den Delimarsky ([@localden](https://github.com/localden))\n- John Lam ([@jflam](https://github.com/jflam))\n\n## ğŸ’¬ Support\n\nFor support, please open a [GitHub issue](https://github.com/github/spec-kit/issues/new). We welcome bug reports, feature requests, and questions about using Spec-Driven Development.\n\n## ğŸ™ Acknowledgements\n\nThis project is heavily influenced by and based on the work and research of [John Lam](https://github.com/jflam).\n\n## ğŸ“„ License\n\nThis project is licensed under the terms of the MIT open source license. Please refer to the [LICENSE](./LICENSE) file for the full terms.\n', '{"language":"Python","stars":54023,"forks":4680,"watchers":54023,"open_issues":485,"topics":["ai","copilot","development","engineering","prd","spec","spec-driven"],"default_branch":"main","size_kb":5143,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:github:spec-kit","source_url":"https://github.com/github/spec-kit"},{"type":"has_code","target_id":"github:github:spec-kit","source_url":"https://github.com/github/spec-kit"},{"type":"has_code","target_id":"github:github:spec-kit","source_url":"https://github.com/github/spec-kit"},{"type":"has_code","target_id":"github:github:spec-kit","source_url":"https://github.com/github/spec-kit"},{"type":"has_code","target_id":"github:github:spec-kit.git","source_url":"https://github.com/github/spec-kit.git"},{"type":"has_code","target_id":"github:github:spec-kit.git","source_url":"https://github.com/github/spec-kit.git"},{"type":"has_code","target_id":"github:github:spec-kit.git","source_url":"https://github.com/github/spec-kit.git"},{"type":"has_code","target_id":"github:aws:amazon-q-developer-cli","source_url":"https://github.com/aws/amazon-q-developer-cli"},{"type":"has_code","target_id":"github:openai:codex","source_url":"https://github.com/openai/codex"},{"type":"has_code","target_id":"github:google-gemini:gemini-cli","source_url":"https://github.com/google-gemini/gemini-cli"},{"type":"has_code","target_id":"github:Kilo-Org:kilocode","source_url":"https://github.com/Kilo-Org/kilocode"},{"type":"has_code","target_id":"github:QwenLM:qwen-code","source_url":"https://github.com/QwenLM/qwen-code"},{"type":"has_code","target_id":"github:ovh:shai","source_url":"https://github.com/ovh/shai"},{"type":"has_code","target_id":"github:git-ecosystem:git-credential-manager","source_url":"https://github.com/git-ecosystem/git-credential-manager"},{"type":"has_code","target_id":"github:github:spec-kit","source_url":"https://github.com/github/spec-kit"}]', NULL, 'MIT', 'approved', 80, '35483b4bafef6da143e697bf529399b2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-github-spec-kit from https://github.com/github.png
Image converted to WebP: data/images/github-github-spec-kit.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-autogen', 'github--microsoft--autogen', 'autogen', 'microsoft', '<a name="readme-top"></a> <div align="center"> <img src="https://microsoft.github.io/autogen/0.2/img/ag.svg" alt="AutoGen Logo" width="100"> </div> **AutoGen** is a framework for creating multi-agent AI applications that can act autonomously or work alongside humans. > **Important:** if you are new to AutoGen, please checkout Microsoft Agent Framework. > AutoGen will still be maintained and continue to receive bug fixes and critical security patches. > Read our announcement. AutoGen requires ...', '["agentic","agentic-agi","agents","ai","autogen","autogen-ecosystem","chatgpt","framework","llm-agent","llm-framework","python"]', 'other', 52320, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/autogen","fetched_at":"2025-12-08T10:30:37.950Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<a name="readme-top"></a>\n\n<div align="center">\n<img src="https://microsoft.github.io/autogen/0.2/img/ag.svg" alt="AutoGen Logo" width="100">\n\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Company?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/company/105812540)\n[![Discord](https://img.shields.io/badge/discord-chat-green?logo=discord)](https://aka.ms/autogen-discord)\n[![Documentation](https://img.shields.io/badge/Documentation-AutoGen-blue?logo=read-the-docs)](https://microsoft.github.io/autogen/)\n[![Blog](https://img.shields.io/badge/Blog-AutoGen-blue?logo=blogger)](https://devblogs.microsoft.com/autogen/)\n\n</div>\n\n# AutoGen\n\n**AutoGen** is a framework for creating multi-agent AI applications that can act autonomously or work alongside humans.\n\n> **Important:** if you are new to AutoGen, please checkout [Microsoft Agent Framework](https://github.com/microsoft/agent-framework).\n> AutoGen will still be maintained and continue to receive bug fixes and critical security patches.\n> Read our [announcement](https://github.com/microsoft/autogen/discussions/7066).\n\n## Installation\n\nAutoGen requires **Python 3.10 or later**.\n\n```bash\n# Install AgentChat and OpenAI client from Extensions\npip install -U "autogen-agentchat" "autogen-ext[openai]"\n```\n\nThe current stable version can be found in the [releases](https://github.com/microsoft/autogen/releases). If you are upgrading from AutoGen v0.2, please refer to the [Migration Guide](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html) for detailed instructions on how to update your code and configurations.\n\n```bash\n# Install AutoGen Studio for no-code GUI\npip install -U "autogenstudio"\n```\n\n## Quickstart\n\nThe following samples call OpenAI API, so you first need to create an account and export your key as `export OPENAI_API_KEY="sk-..."`.\n\n### Hello World\n\nCreate an assistant agent using OpenAI''s GPT-4o model. See [other supported models](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html).\n\n```python\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model="gpt-4.1")\n    agent = AssistantAgent("assistant", model_client=model_client)\n    print(await agent.run(task="Say ''Hello World!''"))\n    await model_client.close()\n\nasyncio.run(main())\n```\n\n### MCP Server\n\nCreate a web browsing assistant agent that uses the Playwright MCP server.\n\n```python\n# First run `npm install -g @playwright/mcp@latest` to install the MCP server.\nimport asyncio\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\nfrom autogen_ext.tools.mcp import McpWorkbench, StdioServerParams\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model="gpt-4.1")\n    server_params = StdioServerParams(\n        command="npx",\n        args=[\n            "@playwright/mcp@latest",\n            "--headless",\n        ],\n    )\n    async with McpWorkbench(server_params) as mcp:\n        agent = AssistantAgent(\n            "web_browsing_assistant",\n            model_client=model_client,\n            workbench=mcp, # For multiple MCP servers, put them in a list.\n            model_client_stream=True,\n            max_tool_iterations=10,\n        )\n        await Console(agent.run_stream(task="Find out how many contributors for the microsoft/autogen repository"))\n\n\nasyncio.run(main())\n```\n\n> **Warning**: Only connect to trusted MCP servers as they may execute commands\n> in your local environment or expose sensitive information.\n\n### Multi-Agent Orchestration\n\nYou can use `AgentTool` to create a basic multi-agent orchestration setup.\n\n```python\nimport asyncio\n\nfrom autogen_agentchat.agents import AssistantAgent\nfrom autogen_agentchat.tools import AgentTool\nfrom autogen_agentchat.ui import Console\nfrom autogen_ext.models.openai import OpenAIChatCompletionClient\n\n\nasync def main() -> None:\n    model_client = OpenAIChatCompletionClient(model="gpt-4.1")\n\n    math_agent = AssistantAgent(\n        "math_expert",\n        model_client=model_client,\n        system_message="You are a math expert.",\n        description="A math expert assistant.",\n        model_client_stream=True,\n    )\n    math_agent_tool = AgentTool(math_agent, return_value_as_last_message=True)\n\n    chemistry_agent = AssistantAgent(\n        "chemistry_expert",\n        model_client=model_client,\n        system_message="You are a chemistry expert.",\n        description="A chemistry expert assistant.",\n        model_client_stream=True,\n    )\n    chemistry_agent_tool = AgentTool(chemistry_agent, return_value_as_last_message=True)\n\n    agent = AssistantAgent(\n        "assistant",\n        system_message="You are a general assistant. Use expert tools when needed.",\n        model_client=model_client,\n        model_client_stream=True,\n        tools=[math_agent_tool, chemistry_agent_tool],\n        max_tool_iterations=10,\n    )\n    await Console(agent.run_stream(task="What is the integral of x^2?"))\n    await Console(agent.run_stream(task="What is the molecular weight of water?"))\n\n\nasyncio.run(main())\n```\n\nFor more advanced multi-agent orchestrations and workflows, read\n[AgentChat documentation](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html).\n\n### AutoGen Studio\n\nUse AutoGen Studio to prototype and run multi-agent workflows without writing code.\n\n```bash\n# Run AutoGen Studio on http://localhost:8080\nautogenstudio ui --port 8080 --appdir ./my-app\n```\n\n## Why Use AutoGen?\n\n<div align="center">\n  <img src="autogen-landing.jpg" alt="AutoGen Landing" width="500">\n</div>\n\nThe AutoGen ecosystem provides everything you need to create AI agents, especially multi-agent workflows -- framework, developer tools, and applications.\n\nThe _framework_ uses a layered and extensible design. Layers have clearly divided responsibilities and build on top of layers below. This design enables you to use the framework at different levels of abstraction, from high-level APIs to low-level components.\n\n- [Core API](./python/packages/autogen-core/) implements message passing, event-driven agents, and local and distributed runtime for flexibility and power. It also support cross-language support for .NET and Python.\n- [AgentChat API](./python/packages/autogen-agentchat/) implements a simpler but opinionatedÂ API for rapid prototyping. This API is built on top of the Core API and is closest to what users of v0.2 are familiar with and supports common multi-agent patterns such as two-agent chat or group chats.\n- [Extensions API](./python/packages/autogen-ext/) enables first- and third-party extensions continuously expanding framework capabilities. It support specific implementation of LLM clients (e.g., OpenAI, AzureOpenAI), and capabilities such as code execution.\n\nThe ecosystem also supports two essential _developer tools_:\n\n<div align="center">\n  <img src="https://media.githubusercontent.com/media/microsoft/autogen/refs/heads/main/python/packages/autogen-studio/docs/ags_screen.png" alt="AutoGen Studio Screenshot" width="500">\n</div>\n\n- [AutoGen Studio](./python/packages/autogen-studio/) provides a no-code GUI for building multi-agent applications.\n- [AutoGen Bench](./python/packages/agbench/) provides a benchmarking suite for evaluating agent performance.\n\nYou can use the AutoGen framework and developer tools to create applications for your domain. For example, [Magentic-One](./python/packages/magentic-one-cli/) is a state-of-the-art multi-agent team built using AgentChat API and Extensions API that can handle a variety of tasks that require web browsing, code execution, and file handling.\n\nWith AutoGen you get to join and contribute to a thriving ecosystem. We host weekly office hours and talks with maintainers and community. We also have a [Discord server](https://aka.ms/autogen-discord) for real-time chat, GitHub Discussions for Q&A, and a blog for tutorials and updates.\n\n## Where to go next?\n\n<div align="center">\n\n|               | [![Python](https://img.shields.io/badge/AutoGen-Python-blue?logo=python&logoColor=white)](./python)                                                                                                                                                                                                                                                                                                                | [![.NET](https://img.shields.io/badge/AutoGen-.NET-green?logo=.net&logoColor=white)](./dotnet)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | [![Studio](https://img.shields.io/badge/AutoGen-Studio-purple?logo=visual-studio&logoColor=white)](./python/packages/autogen-studio)                        |\n| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Installation  | [![Installation](https://img.shields.io/badge/Install-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/installation.html)                                                                                                                                                                                                                                                         | [![Install](https://img.shields.io/badge/Install-green)](https://microsoft.github.io/autogen/dotnet/dev/core/installation.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | [![Install](https://img.shields.io/badge/Install-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/installation.html) |\n| Quickstart    | [![Quickstart](https://img.shields.io/badge/Quickstart-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/quickstart.html#)                                                                                                                                                                                                                                                         | [![Quickstart](https://img.shields.io/badge/Quickstart-green)](https://microsoft.github.io/autogen/dotnet/dev/core/index.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | [![Usage](https://img.shields.io/badge/Quickstart-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html#)      |\n| Tutorial      | [![Tutorial](https://img.shields.io/badge/Tutorial-blue)](https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/index.html)                                                                                                                                                                                                                                                          | [![Tutorial](https://img.shields.io/badge/Tutorial-green)](https://microsoft.github.io/autogen/dotnet/dev/core/tutorial.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | [![Usage](https://img.shields.io/badge/Tutorial-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html#)        |\n| API Reference | [![API](https://img.shields.io/badge/Docs-blue)](https://microsoft.github.io/autogen/stable/reference/index.html#)                                                                                                                                                                                                                                                                                                 | [![API](https://img.shields.io/badge/Docs-green)](https://microsoft.github.io/autogen/dotnet/dev/api/Microsoft.AutoGen.Contracts.html)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | [![API](https://img.shields.io/badge/Docs-purple)](https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/usage.html)               |\n| Packages      | [![PyPi autogen-core](https://img.shields.io/badge/PyPi-autogen--core-blue?logo=pypi)](https://pypi.org/project/autogen-core/) <br> [![PyPi autogen-agentchat](https://img.shields.io/badge/PyPi-autogen--agentchat-blue?logo=pypi)](https://pypi.org/project/autogen-agentchat/) <br> [![PyPi autogen-ext](https://img.shields.io/badge/PyPi-autogen--ext-blue?logo=pypi)](https://pypi.org/project/autogen-ext/) | [![NuGet Contracts](https://img.shields.io/badge/NuGet-Contracts-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Contracts/) <br> [![NuGet Core](https://img.shields.io/badge/NuGet-Core-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Core/) <br> [![NuGet Core.Grpc](https://img.shields.io/badge/NuGet-Core.Grpc-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.Core.Grpc/) <br> [![NuGet RuntimeGateway.Grpc](https://img.shields.io/badge/NuGet-RuntimeGateway.Grpc-green?logo=nuget)](https://www.nuget.org/packages/Microsoft.AutoGen.RuntimeGateway.Grpc/) | [![PyPi autogenstudio](https://img.shields.io/badge/PyPi-autogenstudio-purple?logo=pypi)](https://pypi.org/project/autogenstudio/)                          |\n\n</div>\n\nInterested in contributing? See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines on how to get started. We welcome contributions of all kinds, including bug fixes, new features, and documentation improvements. Join our community and help us make AutoGen better!\n\nHave questions? Check out our [Frequently Asked Questions (FAQ)](./FAQ.md) for answers to common queries. If you don''t find what you''re looking for, feel free to ask in our [GitHub Discussions](https://github.com/microsoft/autogen/discussions) or join our [Discord server](https://aka.ms/autogen-discord) for real-time support. You can also read our [blog](https://devblogs.microsoft.com/autogen/) for updates.\n\n## Legal Notices\n\nMicrosoft and any contributors grant you a license to the Microsoft documentation and other content\nin this repository under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode),\nsee the [LICENSE](LICENSE) file, and grant you a license to any code in the repository under the [MIT License](https://opensource.org/licenses/MIT), see the\n[LICENSE-CODE](LICENSE-CODE) file.\n\nMicrosoft, Windows, Microsoft Azure, and/or other Microsoft products and services referenced in the documentation\nmay be either trademarks or registered trademarks of Microsoft in the United States and/or other countries.\nThe licenses for this project do not grant you rights to use any Microsoft names, logos, or trademarks.\nMicrosoft''s general trademark guidelines can be found at <http://go.microsoft.com/fwlink/?LinkID=254653>.\n\nPrivacy information can be found at <https://go.microsoft.com/fwlink/?LinkId=521839>\n\nMicrosoft and any contributors reserve all other rights, whether under their respective copyrights, patents,\nor trademarks, whether by implication, estoppel, or otherwise.\n\n<p align="right" style="font-size: 14px; color: #555; margin-top: 20px;">\n  <a href="#readme-top" style="text-decoration: none; color: blue; font-weight: bold;">\n    â†‘ Back to Top â†‘\n  </a>\n</p>\n', '{"language":"Python","stars":52320,"forks":7952,"watchers":52320,"open_issues":522,"topics":["agentic","agentic-agi","agents","ai","autogen","autogen-ecosystem","chatgpt","framework","llm-agent","llm-framework"],"default_branch":"main","size_kb":147840,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:microsoft:agent-framework","source_url":"https://github.com/microsoft/agent-framework"},{"type":"has_code","target_id":"github:microsoft:autogen","source_url":"https://github.com/microsoft/autogen"},{"type":"has_code","target_id":"github:microsoft:autogen","source_url":"https://github.com/microsoft/autogen"},{"type":"has_code","target_id":"github:microsoft:autogen","source_url":"https://github.com/microsoft/autogen"}]', NULL, 'CC-BY-4.0', 'approved', 80, '9c8f6b14ea5c8402286d6fafa6bc9c89', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-autogen from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-autogen.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-harry0703-MoneyPrinterTurbo', 'github--harry0703--moneyprinterturbo', 'MoneyPrinterTurbo', 'harry0703', '<div align="center"> <h1 align="center">MoneyPrinterTurbo ğŸ’¸</h1> <p align="center"> <a href="https://github.com/harry0703/MoneyPrinterTurbo/stargazers"><img src="https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Stargazers"></a> <a href="https://github.com/harry0703/MoneyPrinterTurbo/issues"><img src="https://img.shields.io/github/issues/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Issues"></a> <a href="https://github.com/harry0703/Mon...', '["ai","automation","chatgpt","moviepy","python","shortvideo","tiktok","python"]', 'other', 48177, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/harry0703/MoneyPrinterTurbo","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n<h1 align="center">MoneyPrinterTurbo ğŸ’¸</h1>\n\n<p align="center">\n  <a href="https://github.com/harry0703/MoneyPrinterTurbo/stargazers"><img src="https://img.shields.io/github/stars/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Stargazers"></a>\n  <a href="https://github.com/harry0703/MoneyPrinterTurbo/issues"><img src="https://img.shields.io/github/issues/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Issues"></a>\n  <a href="https://github.com/harry0703/MoneyPrinterTurbo/network/members"><img src="https://img.shields.io/github/forks/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="Forks"></a>\n  <a href="https://github.com/harry0703/MoneyPrinterTurbo/blob/main/LICENSE"><img src="https://img.shields.io/github/license/harry0703/MoneyPrinterTurbo.svg?style=for-the-badge" alt="License"></a>\n</p>\n<br>\n<h3>ç®€ä½“ä¸­æ–‡ | <a href="README-en.md">English</a></h3>\n<div align="center">\n  <a href="https://trendshift.io/repositories/8731" target="_blank"><img src="https://trendshift.io/api/badge/repositories/8731" alt="harry0703%2FMoneyPrinterTurbo | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n</div>\n<br>\nåªéœ€æä¾›ä¸€ä¸ªè§†é¢‘ <b>ä¸»é¢˜</b> æˆ– <b>å…³é”®è¯</b> ï¼Œå°±å¯ä»¥å…¨è‡ªåŠ¨ç”Ÿæˆè§†é¢‘æ–‡æ¡ˆã€è§†é¢‘ç´ æã€è§†é¢‘å­—å¹•ã€è§†é¢‘èƒŒæ™¯éŸ³ä¹ï¼Œç„¶ååˆæˆä¸€ä¸ªé«˜æ¸…çš„çŸ­è§†é¢‘ã€‚\n<br>\n\n<h4>Webç•Œé¢</h4>\n\n![](docs/webui.jpg)\n\n<h4>APIç•Œé¢</h4>\n\n![](docs/api.jpg)\n\n</div>\n\n## ç‰¹åˆ«æ„Ÿè°¢ ğŸ™\n\nç”±äºè¯¥é¡¹ç›®çš„ **éƒ¨ç½²** å’Œ **ä½¿ç”¨**ï¼Œå¯¹äºä¸€äº›å°ç™½ç”¨æˆ·æ¥è¯´ï¼Œè¿˜æ˜¯ **æœ‰ä¸€å®šçš„é—¨æ§›**ï¼Œåœ¨æ­¤ç‰¹åˆ«æ„Ÿè°¢\n**å½•å’–ï¼ˆAIæ™ºèƒ½ å¤šåª’ä½“æœåŠ¡å¹³å°ï¼‰** ç½‘ç«™åŸºäºè¯¥é¡¹ç›®ï¼Œæä¾›çš„å…è´¹`AIè§†é¢‘ç”Ÿæˆå™¨`æœåŠ¡ï¼Œå¯ä»¥ä¸ç”¨éƒ¨ç½²ï¼Œç›´æ¥åœ¨çº¿ä½¿ç”¨ï¼Œéå¸¸æ–¹ä¾¿ã€‚\n\n- ä¸­æ–‡ç‰ˆï¼šhttps://reccloud.cn\n- è‹±æ–‡ç‰ˆï¼šhttps://reccloud.com\n\n![](docs/reccloud.cn.jpg)\n\n## æ„Ÿè°¢èµåŠ© ğŸ™\n\næ„Ÿè°¢ä½ç³– https://picwish.cn å¯¹è¯¥é¡¹ç›®çš„æ”¯æŒå’ŒèµåŠ©ï¼Œä½¿å¾—è¯¥é¡¹ç›®èƒ½å¤ŸæŒç»­çš„æ›´æ–°å’Œç»´æŠ¤ã€‚\n\nä½ç³–ä¸“æ³¨äº**å›¾åƒå¤„ç†é¢†åŸŸ**ï¼Œæä¾›ä¸°å¯Œçš„**å›¾åƒå¤„ç†å·¥å…·**ï¼Œå°†å¤æ‚æ“ä½œæè‡´ç®€åŒ–ï¼ŒçœŸæ­£å®ç°è®©å›¾åƒå¤„ç†æ›´ç®€å•ã€‚\n\n![picwish.jpg](docs/picwish.jpg)\n\n## åŠŸèƒ½ç‰¹æ€§ ğŸ¯\n\n- [x] å®Œæ•´çš„ **MVCæ¶æ„**ï¼Œä»£ç  **ç»“æ„æ¸…æ™°**ï¼Œæ˜“äºç»´æŠ¤ï¼Œæ”¯æŒ `API` å’Œ `Webç•Œé¢`\n- [x] æ”¯æŒè§†é¢‘æ–‡æ¡ˆ **AIè‡ªåŠ¨ç”Ÿæˆ**ï¼Œä¹Ÿå¯ä»¥**è‡ªå®šä¹‰æ–‡æ¡ˆ**\n- [x] æ”¯æŒå¤šç§ **é«˜æ¸…è§†é¢‘** å°ºå¯¸\n    - [x] ç«–å± 9:16ï¼Œ`1080x1920`\n    - [x] æ¨ªå± 16:9ï¼Œ`1920x1080`\n- [x] æ”¯æŒ **æ‰¹é‡è§†é¢‘ç”Ÿæˆ**ï¼Œå¯ä»¥ä¸€æ¬¡ç”Ÿæˆå¤šä¸ªè§†é¢‘ï¼Œç„¶åé€‰æ‹©ä¸€ä¸ªæœ€æ»¡æ„çš„\n- [x] æ”¯æŒ **è§†é¢‘ç‰‡æ®µæ—¶é•¿** è®¾ç½®ï¼Œæ–¹ä¾¿è°ƒèŠ‚ç´ æåˆ‡æ¢é¢‘ç‡\n- [x] æ”¯æŒ **ä¸­æ–‡** å’Œ **è‹±æ–‡** è§†é¢‘æ–‡æ¡ˆ\n- [x] æ”¯æŒ **å¤šç§è¯­éŸ³** åˆæˆï¼Œå¯ **å®æ—¶è¯•å¬** æ•ˆæœ\n- [x] æ”¯æŒ **å­—å¹•ç”Ÿæˆ**ï¼Œå¯ä»¥è°ƒæ•´ `å­—ä½“`ã€`ä½ç½®`ã€`é¢œè‰²`ã€`å¤§å°`ï¼ŒåŒæ—¶æ”¯æŒ`å­—å¹•æè¾¹`è®¾ç½®\n- [x] æ”¯æŒ **èƒŒæ™¯éŸ³ä¹**ï¼Œéšæœºæˆ–è€…æŒ‡å®šéŸ³ä¹æ–‡ä»¶ï¼Œå¯è®¾ç½®`èƒŒæ™¯éŸ³ä¹éŸ³é‡`\n- [x] è§†é¢‘ç´ ææ¥æº **é«˜æ¸…**ï¼Œè€Œä¸” **æ— ç‰ˆæƒ**ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªå·±çš„ **æœ¬åœ°ç´ æ**\n- [x] æ”¯æŒ **OpenAI**ã€**Moonshot**ã€**Azure**ã€**gpt4free**ã€**one-api**ã€**é€šä¹‰åƒé—®**ã€**Google Gemini**ã€**Ollama**ã€**DeepSeek**ã€ **æ–‡å¿ƒä¸€è¨€**, **Pollinations** ç­‰å¤šç§æ¨¡å‹æ¥å…¥\n    - ä¸­å›½ç”¨æˆ·å»ºè®®ä½¿ç”¨ **DeepSeek** æˆ– **Moonshot** ä½œä¸ºå¤§æ¨¡å‹æä¾›å•†ï¼ˆå›½å†…å¯ç›´æ¥è®¿é—®ï¼Œä¸éœ€è¦VPNã€‚æ³¨å†Œå°±é€é¢åº¦ï¼ŒåŸºæœ¬å¤Ÿç”¨ï¼‰\n\n\n### åæœŸè®¡åˆ’ ğŸ“…\n\n- [ ] GPT-SoVITS é…éŸ³æ”¯æŒ\n- [ ] ä¼˜åŒ–è¯­éŸ³åˆæˆï¼Œåˆ©ç”¨å¤§æ¨¡å‹ï¼Œä½¿å…¶åˆæˆçš„å£°éŸ³ï¼Œæ›´åŠ è‡ªç„¶ï¼Œæƒ…ç»ªæ›´åŠ ä¸°å¯Œ\n- [ ] å¢åŠ è§†é¢‘è½¬åœºæ•ˆæœï¼Œä½¿å…¶çœ‹èµ·æ¥æ›´åŠ çš„æµç•…\n- [ ] å¢åŠ æ›´å¤šè§†é¢‘ç´ ææ¥æºï¼Œä¼˜åŒ–è§†é¢‘ç´ æå’Œæ–‡æ¡ˆçš„åŒ¹é…åº¦\n- [ ] å¢åŠ è§†é¢‘é•¿åº¦é€‰é¡¹ï¼šçŸ­ã€ä¸­ã€é•¿\n- [ ] æ”¯æŒæ›´å¤šçš„è¯­éŸ³åˆæˆæœåŠ¡å•†ï¼Œæ¯”å¦‚ OpenAI TTS\n- [ ] è‡ªåŠ¨ä¸Šä¼ åˆ°YouTubeå¹³å°\n\n## è§†é¢‘æ¼”ç¤º ğŸ“º\n\n### ç«–å± 9:16\n\n<table>\n<thead>\n<tr>\n<th align="center"><g-emoji class="g-emoji" alias="arrow_forward">â–¶ï¸</g-emoji> ã€Šå¦‚ä½•å¢åŠ ç”Ÿæ´»çš„ä¹è¶£ã€‹</th>\n<th align="center"><g-emoji class="g-emoji" alias="arrow_forward">â–¶ï¸</g-emoji> ã€Šé‡‘é’±çš„ä½œç”¨ã€‹<br>æ›´çœŸå®çš„åˆæˆå£°éŸ³</th>\n<th align="center"><g-emoji class="g-emoji" alias="arrow_forward">â–¶ï¸</g-emoji> ã€Šç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆã€‹</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align="center"><video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/a84d33d5-27a2-4aba-8fd0-9fb2bd91c6a6"></video></td>\n<td align="center"><video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/af2f3b0b-002e-49fe-b161-18ba91c055e8"></video></td>\n<td align="center"><video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/112c9564-d52b-4472-99ad-970b75f66476"></video></td>\n</tr>\n</tbody>\n</table>\n\n### æ¨ªå± 16:9\n\n<table>\n<thead>\n<tr>\n<th align="center"><g-emoji class="g-emoji" alias="arrow_forward">â–¶ï¸</g-emoji>ã€Šç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆã€‹</th>\n<th align="center"><g-emoji class="g-emoji" alias="arrow_forward">â–¶ï¸</g-emoji>ã€Šä¸ºä»€ä¹ˆè¦è¿åŠ¨ã€‹</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align="center"><video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/346ebb15-c55f-47a9-a653-114f08bb8073"></video></td>\n<td align="center"><video src="https://github.com/harry0703/MoneyPrinterTurbo/assets/4928832/271f2fae-8283-44a0-8aa0-0ed8f9a6fa87"></video></td>\n</tr>\n</tbody>\n</table>\n\n## é…ç½®è¦æ±‚ ğŸ“¦\n\n- å»ºè®®æœ€ä½ CPU **4æ ¸** æˆ–ä»¥ä¸Šï¼Œå†…å­˜ **4G** æˆ–ä»¥ä¸Šï¼Œæ˜¾å¡éå¿…é¡»\n- Windows 10 æˆ– MacOS 11.0 ä»¥ä¸Šç³»ç»Ÿ\n\n\n## å¿«é€Ÿå¼€å§‹ ğŸš€\n\n### åœ¨ Google Colab ä¸­è¿è¡Œ\nå…å»æœ¬åœ°ç¯å¢ƒé…ç½®ï¼Œç‚¹å‡»ç›´æ¥åœ¨ Google Colab ä¸­å¿«é€Ÿä½“éªŒ MoneyPrinterTurbo\n\n[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harry0703/MoneyPrinterTurbo/blob/main/docs/MoneyPrinterTurbo.ipynb)\n\n\n### Windowsä¸€é”®å¯åŠ¨åŒ…\n\nä¸‹è½½ä¸€é”®å¯åŠ¨åŒ…ï¼Œè§£å‹ç›´æ¥ä½¿ç”¨ï¼ˆè·¯å¾„ä¸è¦æœ‰ **ä¸­æ–‡**ã€**ç‰¹æ®Šå­—ç¬¦**ã€**ç©ºæ ¼**ï¼‰\n\n- ç™¾åº¦ç½‘ç›˜ï¼ˆv1.2.6ï¼‰: https://pan.baidu.com/s/1wg0UaIyXpO3SqIpaq790SQ?pwd=sbqx æå–ç : sbqx\n- Google Drive (v1.2.6): https://drive.google.com/file/d/1HsbzfT7XunkrCrHw5ncUjFX8XX4zAuUh/view?usp=sharing\n\nä¸‹è½½åï¼Œå»ºè®®å…ˆ**åŒå‡»æ‰§è¡Œ** `update.bat` æ›´æ–°åˆ°**æœ€æ–°ä»£ç **ï¼Œç„¶ååŒå‡» `start.bat` å¯åŠ¨\n\nå¯åŠ¨åï¼Œä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆå¦‚æœæ‰“å¼€æ˜¯ç©ºç™½ï¼Œå»ºè®®æ¢æˆ **Chrome** æˆ–è€… **Edge** æ‰“å¼€ï¼‰\n\n## å®‰è£…éƒ¨ç½² ğŸ“¥\n\n### å‰ææ¡ä»¶\n\n- å°½é‡ä¸è¦ä½¿ç”¨ **ä¸­æ–‡è·¯å¾„**ï¼Œé¿å…å‡ºç°ä¸€äº›æ— æ³•é¢„æ–™çš„é—®é¢˜\n- è¯·ç¡®ä¿ä½ çš„ **ç½‘ç»œ** æ˜¯æ­£å¸¸çš„ï¼ŒVPNéœ€è¦æ‰“å¼€`å…¨å±€æµé‡`æ¨¡å¼\n\n#### â‘  å…‹éš†ä»£ç \n\n```shell\ngit clone https://github.com/harry0703/MoneyPrinterTurbo.git\n```\n\n#### â‘¡ ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œå»ºè®®å¯åŠ¨åä¹Ÿå¯ä»¥åœ¨ WebUI é‡Œé¢é…ç½®ï¼‰\n\n- å°† `config.example.toml` æ–‡ä»¶å¤åˆ¶ä¸€ä»½ï¼Œå‘½åä¸º `config.toml`\n- æŒ‰ç…§ `config.toml` æ–‡ä»¶ä¸­çš„è¯´æ˜ï¼Œé…ç½®å¥½ `pexels_api_keys` å’Œ `llm_provider`ï¼Œå¹¶æ ¹æ® llm_provider å¯¹åº”çš„æœåŠ¡å•†ï¼Œé…ç½®ç›¸å…³çš„\n  API Key\n\n### Dockeréƒ¨ç½² ğŸ³\n\n#### â‘  å¯åŠ¨Docker\n\nå¦‚æœæœªå®‰è£… Dockerï¼Œè¯·å…ˆå®‰è£… https://www.docker.com/products/docker-desktop/\n\nå¦‚æœæ˜¯Windowsç³»ç»Ÿï¼Œè¯·å‚è€ƒå¾®è½¯çš„æ–‡æ¡£ï¼š\n\n1. https://learn.microsoft.com/zh-cn/windows/wsl/install\n2. https://learn.microsoft.com/zh-cn/windows/wsl/tutorials/wsl-containers\n\n```shell\ncd MoneyPrinterTurbo\ndocker-compose up\n```\n\n> æ³¨æ„ï¼šæœ€æ–°ç‰ˆçš„dockerå®‰è£…æ—¶ä¼šè‡ªåŠ¨ä»¥æ’ä»¶çš„å½¢å¼å®‰è£…docker composeï¼Œå¯åŠ¨å‘½ä»¤è°ƒæ•´ä¸ºdocker compose up\n\n#### â‘¡ è®¿é—®Webç•Œé¢\n\næ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® http://0.0.0.0:8501\n\n#### â‘¢ è®¿é—®APIæ–‡æ¡£\n\næ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® http://0.0.0.0:8080/docs æˆ–è€… http://0.0.0.0:8080/redoc\n\n### æ‰‹åŠ¨éƒ¨ç½² ğŸ“¦\n\n> è§†é¢‘æ•™ç¨‹\n\n- å®Œæ•´çš„ä½¿ç”¨æ¼”ç¤ºï¼šhttps://v.douyin.com/iFhnwsKY/\n- å¦‚ä½•åœ¨Windowsä¸Šéƒ¨ç½²ï¼šhttps://v.douyin.com/iFyjoW3M\n\n#### â‘  åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ\n\nå»ºè®®ä½¿ç”¨ [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html) åˆ›å»º python è™šæ‹Ÿç¯å¢ƒ\n\n```shell\ngit clone https://github.com/harry0703/MoneyPrinterTurbo.git\ncd MoneyPrinterTurbo\nconda create -n MoneyPrinterTurbo python=3.11\nconda activate MoneyPrinterTurbo\npip install -r requirements.txt\n```\n\n#### â‘¡ å®‰è£…å¥½ ImageMagick\n\n- Windows:\n    - ä¸‹è½½ https://imagemagick.org/script/download.php é€‰æ‹©Windowsç‰ˆæœ¬ï¼Œåˆ‡è®°ä¸€å®šè¦é€‰æ‹© **é™æ€åº“** ç‰ˆæœ¬ï¼Œæ¯”å¦‚\n      ImageMagick-7.1.1-32-Q16-x64-**static**.exe\n    - å®‰è£…ä¸‹è½½å¥½çš„ ImageMagickï¼Œ**æ³¨æ„ä¸è¦ä¿®æ”¹å®‰è£…è·¯å¾„**\n    - ä¿®æ”¹ `é…ç½®æ–‡ä»¶ config.toml` ä¸­çš„ `imagemagick_path` ä¸ºä½ çš„ **å®é™…å®‰è£…è·¯å¾„**\n\n- MacOS:\n  ```shell\n  brew install imagemagick\n  ````\n- Ubuntu\n  ```shell\n  sudo apt-get install imagemagick\n  ```\n- CentOS\n  ```shell\n  sudo yum install ImageMagick\n  ```\n\n#### â‘¢ å¯åŠ¨Webç•Œé¢ ğŸŒ\n\næ³¨æ„éœ€è¦åˆ° MoneyPrinterTurbo é¡¹ç›® `æ ¹ç›®å½•` ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤\n\n###### Windows\n\n```bat\nwebui.bat\n```\n\n###### MacOS or Linux\n\n```shell\nsh webui.sh\n```\n\nå¯åŠ¨åï¼Œä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨ï¼ˆå¦‚æœæ‰“å¼€æ˜¯ç©ºç™½ï¼Œå»ºè®®æ¢æˆ **Chrome** æˆ–è€… **Edge** æ‰“å¼€ï¼‰\n\n#### â‘£ å¯åŠ¨APIæœåŠ¡ ğŸš€\n\n```shell\npython main.py\n```\n\nå¯åŠ¨åï¼Œå¯ä»¥æŸ¥çœ‹ `APIæ–‡æ¡£` http://127.0.0.1:8080/docs æˆ–è€… http://127.0.0.1:8080/redoc ç›´æ¥åœ¨çº¿è°ƒè¯•æ¥å£ï¼Œå¿«é€Ÿä½“éªŒã€‚\n\n## è¯­éŸ³åˆæˆ ğŸ—£\n\næ‰€æœ‰æ”¯æŒçš„å£°éŸ³åˆ—è¡¨ï¼Œå¯ä»¥æŸ¥çœ‹ï¼š[å£°éŸ³åˆ—è¡¨](./docs/voice-list.txt)\n\n2024-04-16 v1.1.2 æ–°å¢äº†9ç§Azureçš„è¯­éŸ³åˆæˆå£°éŸ³ï¼Œéœ€è¦é…ç½®API KEYï¼Œè¯¥å£°éŸ³åˆæˆçš„æ›´åŠ çœŸå®ã€‚\n\n## å­—å¹•ç”Ÿæˆ ğŸ“œ\n\nå½“å‰æ”¯æŒ2ç§å­—å¹•ç”Ÿæˆæ–¹å¼ï¼š\n\n- **edge**: ç”Ÿæˆ`é€Ÿåº¦å¿«`ï¼Œæ€§èƒ½æ›´å¥½ï¼Œå¯¹ç”µè„‘é…ç½®æ²¡æœ‰è¦æ±‚ï¼Œä½†æ˜¯è´¨é‡å¯èƒ½ä¸ç¨³å®š\n- **whisper**: ç”Ÿæˆ`é€Ÿåº¦æ…¢`ï¼Œæ€§èƒ½è¾ƒå·®ï¼Œå¯¹ç”µè„‘é…ç½®æœ‰ä¸€å®šè¦æ±‚ï¼Œä½†æ˜¯`è´¨é‡æ›´å¯é `ã€‚\n\nå¯ä»¥ä¿®æ”¹ `config.toml` é…ç½®æ–‡ä»¶ä¸­çš„ `subtitle_provider` è¿›è¡Œåˆ‡æ¢\n\nå»ºè®®ä½¿ç”¨ `edge` æ¨¡å¼ï¼Œå¦‚æœç”Ÿæˆçš„å­—å¹•è´¨é‡ä¸å¥½ï¼Œå†åˆ‡æ¢åˆ° `whisper` æ¨¡å¼\n\n> æ³¨æ„ï¼š\n\n1. whisper æ¨¡å¼ä¸‹éœ€è¦åˆ° HuggingFace ä¸‹è½½ä¸€ä¸ªæ¨¡å‹æ–‡ä»¶ï¼Œå¤§çº¦ 3GB å·¦å³ï¼Œè¯·ç¡®ä¿ç½‘ç»œé€šç•…\n2. å¦‚æœç•™ç©ºï¼Œè¡¨ç¤ºä¸ç”Ÿæˆå­—å¹•ã€‚\n\n> ç”±äºå›½å†…æ— æ³•è®¿é—® HuggingFaceï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•ä¸‹è½½ `whisper-large-v3` çš„æ¨¡å‹æ–‡ä»¶\n\nä¸‹è½½åœ°å€ï¼š\n\n- ç™¾åº¦ç½‘ç›˜: https://pan.baidu.com/s/11h3Q6tsDtjQKTjUu3sc5cA?pwd=xjs9\n- å¤¸å…‹ç½‘ç›˜ï¼šhttps://pan.quark.cn/s/3ee3d991d64b\n\næ¨¡å‹ä¸‹è½½åè§£å‹ï¼Œæ•´ä¸ªç›®å½•æ”¾åˆ° `.\MoneyPrinterTurbo\models` é‡Œé¢ï¼Œ\næœ€ç»ˆçš„æ–‡ä»¶è·¯å¾„åº”è¯¥æ˜¯è¿™æ ·: `.\MoneyPrinterTurbo\models\whisper-large-v3`\n\n```\nMoneyPrinterTurbo  \n  â”œâ”€models\n  â”‚   â””â”€whisper-large-v3\n  â”‚          config.json\n  â”‚          model.bin\n  â”‚          preprocessor_config.json\n  â”‚          tokenizer.json\n  â”‚          vocabulary.json\n```\n\n## èƒŒæ™¯éŸ³ä¹ ğŸµ\n\nç”¨äºè§†é¢‘çš„èƒŒæ™¯éŸ³ä¹ï¼Œä½äºé¡¹ç›®çš„ `resource/songs` ç›®å½•ä¸‹ã€‚\n> å½“å‰é¡¹ç›®é‡Œé¢æ”¾äº†ä¸€äº›é»˜è®¤çš„éŸ³ä¹ï¼Œæ¥è‡ªäº YouTube è§†é¢‘ï¼Œå¦‚æœ‰ä¾µæƒï¼Œè¯·åˆ é™¤ã€‚\n\n## å­—å¹•å­—ä½“ ğŸ…°\n\nç”¨äºè§†é¢‘å­—å¹•çš„æ¸²æŸ“ï¼Œä½äºé¡¹ç›®çš„ `resource/fonts` ç›®å½•ä¸‹ï¼Œä½ ä¹Ÿå¯ä»¥æ”¾è¿›å»è‡ªå·±çš„å­—ä½“ã€‚\n\n## å¸¸è§é—®é¢˜ ğŸ¤”\n\n### â“RuntimeError: No ffmpeg exe could be found\n\né€šå¸¸æƒ…å†µä¸‹ï¼Œffmpeg ä¼šè¢«è‡ªåŠ¨ä¸‹è½½ï¼Œå¹¶ä¸”ä¼šè¢«è‡ªåŠ¨æ£€æµ‹åˆ°ã€‚\nä½†æ˜¯å¦‚æœä½ çš„ç¯å¢ƒæœ‰é—®é¢˜ï¼Œæ— æ³•è‡ªåŠ¨ä¸‹è½½ï¼Œå¯èƒ½ä¼šé‡åˆ°å¦‚ä¸‹é”™è¯¯ï¼š\n\n```\nRuntimeError: No ffmpeg exe could be found.\nInstall ffmpeg on your system, or set the IMAGEIO_FFMPEG_EXE environment variable.\n```\n\næ­¤æ—¶ä½ å¯ä»¥ä» https://www.gyan.dev/ffmpeg/builds/ ä¸‹è½½ffmpegï¼Œè§£å‹åï¼Œè®¾ç½® `ffmpeg_path` ä¸ºä½ çš„å®é™…å®‰è£…è·¯å¾„å³å¯ã€‚\n\n```toml\n[app]\n# è¯·æ ¹æ®ä½ çš„å®é™…è·¯å¾„è®¾ç½®ï¼Œæ³¨æ„ Windows è·¯å¾„åˆ†éš”ç¬¦ä¸º \\\nffmpeg_path = "C:\\Users\\harry\\Downloads\\ffmpeg.exe"\n```\n\n### â“ImageMagickçš„å®‰å…¨ç­–ç•¥é˜»æ­¢äº†ä¸ä¸´æ—¶æ–‡ä»¶@/tmp/tmpur5hyyto.txtç›¸å…³çš„æ“ä½œ\n\nå¯ä»¥åœ¨ImageMagickçš„é…ç½®æ–‡ä»¶policy.xmlä¸­æ‰¾åˆ°è¿™äº›ç­–ç•¥ã€‚\nè¿™ä¸ªæ–‡ä»¶é€šå¸¸ä½äº /etc/ImageMagick-`X`/ æˆ– ImageMagick å®‰è£…ç›®å½•çš„ç±»ä¼¼ä½ç½®ã€‚\nä¿®æ”¹åŒ…å«`pattern="@"`çš„æ¡ç›®ï¼Œå°†`rights="none"`æ›´æ”¹ä¸º`rights="read|write"`ä»¥å…è®¸å¯¹æ–‡ä»¶çš„è¯»å†™æ“ä½œã€‚\n\n### â“OSError: [Errno 24] Too many open files\n\nè¿™ä¸ªé—®é¢˜æ˜¯ç”±äºç³»ç»Ÿæ‰“å¼€æ–‡ä»¶æ•°é™åˆ¶å¯¼è‡´çš„ï¼Œå¯ä»¥é€šè¿‡ä¿®æ”¹ç³»ç»Ÿçš„æ–‡ä»¶æ‰“å¼€æ•°é™åˆ¶æ¥è§£å†³ã€‚\n\næŸ¥çœ‹å½“å‰é™åˆ¶\n\n```shell\nulimit -n\n```\n\nå¦‚æœè¿‡ä½ï¼Œå¯ä»¥è°ƒé«˜ä¸€äº›ï¼Œæ¯”å¦‚\n\n```shell\nulimit -n 10240\n```\n\n### â“Whisper æ¨¡å‹ä¸‹è½½å¤±è´¥ï¼Œå‡ºç°å¦‚ä¸‹é”™è¯¯\n\nLocalEntryNotfoundEror: Cannot find an appropriate cached snapshotfolderfor the specified revision on the local disk and\noutgoing trafic has been disabled.\nTo enablerepo look-ups and downloads online, pass ''local files only=False'' as input.\n\næˆ–è€…\n\nAn error occured while synchronizing the model Systran/faster-whisper-large-v3 from the Hugging Face Hub:\nAn error happened while trying to locate the files on the Hub and we cannot find the appropriate snapshot folder for the\nspecified revision on the local disk. Please check your internet connection and try again.\nTrying to load the model directly from the local cache, if it exists.\n\nè§£å†³æ–¹æ³•ï¼š[ç‚¹å‡»æŸ¥çœ‹å¦‚ä½•ä»ç½‘ç›˜æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹](#%E5%AD%97%E5%B9%95%E7%94%9F%E6%88%90-)\n\n## åé¦ˆå»ºè®® ğŸ“¢\n\n- å¯ä»¥æäº¤ [issue](https://github.com/harry0703/MoneyPrinterTurbo/issues)\n  æˆ–è€… [pull request](https://github.com/harry0703/MoneyPrinterTurbo/pulls)ã€‚\n\n## è®¸å¯è¯ ğŸ“\n\nç‚¹å‡»æŸ¥çœ‹ [`LICENSE`](LICENSE) æ–‡ä»¶\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=harry0703/MoneyPrinterTurbo&type=Date)](https://star-history.com/#harry0703/MoneyPrinterTurbo&Date)', '{"language":"Python","stars":48177,"forks":6766,"watchers":48177,"open_issues":217,"topics":["ai","automation","chatgpt","moviepy","python","shortvideo","tiktok"],"default_branch":"main","size_kb":531961,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:harry0703:MoneyPrinterTurbo","source_url":"https://github.com/harry0703/MoneyPrinterTurbo"},{"type":"has_code","target_id":"github:harry0703:MoneyPrinterTurbo","source_url":"https://github.com/harry0703/MoneyPrinterTurbo"},{"type":"has_code","target_id":"github:harry0703:MoneyPrinterTurbo","source_url":"https://github.com/harry0703/MoneyPrinterTurbo"},{"type":"has_code","target_id":"github:harry0703:MoneyPrinterTurbo","source_url":"https://github.com/harry0703/MoneyPrinterTurbo"},{"type":"has_code","target_id":"github:harry0703:MoneyPrinterTurbo","source_url":"https://github.com/harry0703/MoneyPrinterTurbo"},{"type":"has_code","target_id":"github:harry0703:MoneyPrinterTurbo","source_url":"https://github.com/harry0703/MoneyPrinterTurbo"},{"type":"has_code","target_id":"github:harry0703:MoneyPrinterTurbo","source_url":"https://github.com/harry0703/MoneyPrinterTurbo"},{"type":"has_code","target_id":"github:harry0703:MoneyPrinterTurbo","source_url":"https://github.com/harry0703/MoneyPrinterTurbo"},{"type":"has_code","target_id":"github:harry0703:MoneyPrinterTurbo","source_url":"https://github.com/harry0703/MoneyPrinterTurbo"},{"type":"has_code","target_id":"github:harry0703:MoneyPrinterTurbo.git","source_url":"https://github.com/harry0703/MoneyPrinterTurbo.git"},{"type":"has_code","target_id":"github:harry0703:MoneyPrinterTurbo.git","source_url":"https://github.com/harry0703/MoneyPrinterTurbo.git"},{"type":"has_code","target_id":"github:harry0703:MoneyPrinterTurbo","source_url":"https://github.com/harry0703/MoneyPrinterTurbo"},{"type":"has_code","target_id":"github:harry0703:MoneyPrinterTurbo","source_url":"https://github.com/harry0703/MoneyPrinterTurbo"}]', NULL, 'MIT', 'approved', 65, 'ad0c2dae5e78c9933966eb7822c9d076', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-harry0703-MoneyPrinterTurbo from https://github.com/harry0703.png
Image converted to WebP: data/images/github-harry0703-MoneyPrinterTurbo.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-dbeaver-dbeaver', 'github--dbeaver--dbeaver', 'dbeaver', 'dbeaver', '<img src="https://github.com/dbeaver/dbeaver/wiki/images/dbeaver-icon-64x64.png" align="right"/> Free multi-platform database tool for developers, SQL programmers, database administrators and analysts. * Has a lot of <a href="https://github.com/dbeaver/dbeaver/wiki">features</a> including schema editor, SQL editor, data editor, AI integration, ER diagrams, data export/import/migration, SQL execution plans, database administration tools, database dashboards, Spatial data viewer, proxy and SSH ...', '["ai","copilot","database","db2","dbeaver","erd","gui","java","jdbc","mysql","nosql","openai","oracle","postgresql","redshift","sql","sqlite","sqlserver","java"]', 'other', 47674, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/dbeaver/dbeaver","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '[![Twitter URL](https://img.shields.io/twitter/url/https/twitter.com/dbeaver_news.svg?style=social&label=Follow%20%40dbeaver_news)](https://twitter.com/dbeaver_news)\n[![Codacy Badge](https://app.codacy.com/project/badge/Grade/fa0bb9cf5a904c7d87424f8f6351ba92)](https://app.codacy.com/gh/dbeaver/dbeaver/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade)\n[![Apache 2.0](https://img.shields.io/github/license/cronn-de/jira-sync.svg)](http://www.apache.org/licenses/LICENSE-2.0)\n[![Tickets in review](https://img.shields.io/github/issues/dbeaver/dbeaver/wait%20for%20review)](https://github.com/dbeaver/dbeaver/issues?q=is%3Aissue+is%3Aopen+label%3A"wait%20for%20review")\n<img src="https://github.com/dbeaver/dbeaver/wiki/images/dbeaver-icon-64x64.png" align="right"/>\n\n# DBeaver\n\nFree multi-platform database tool for developers, SQL programmers, database administrators and analysts.  \n\n* Has a lot of <a href="https://github.com/dbeaver/dbeaver/wiki">features</a> including schema editor, SQL editor, data editor, AI integration, ER diagrams, data export/import/migration, SQL execution plans, database administration tools, database dashboards, Spatial data viewer, proxy and SSH tunnelling, custom database drivers editor, etc.\n* Out of the box supports more than <a href="#supported-databases">100 database drivers</a>.\n* Supports any database which has JDBC or ODBC driver (basically - almost all existing databases).\n* Supports smart AI completion and code generation with OpenAI or Copilot\n\n<a href="https://dbeaver.io/product/dbeaver-sql-editor.png"><img src="https://dbeaver.io/product/dbeaver-sql-editor.png" width="400"/></a>\n<a href="https://dbeaver.io/product/dbeaver-gis-viewer.png"><img src="https://dbeaver.io/product/dbeaver-gis-viewer.png" width="400"/></a>\n<a href="https://dbeaver.io/product/dbeaver-data-editor.png"><img src="https://dbeaver.io/product/dbeaver-data-editor.png" width="400"/></a>\n<a href="https://dbeaver.io/product/dbeaver-erd.png"><img src="https://dbeaver.io/product/dbeaver-erd.png" width="400"/></a>\n\n## Download\n\nYou can download prebuilt binaries from <a href="https://dbeaver.io/download" target="_blank">official website</a> or directly from <a href="https://github.com/dbeaver/dbeaver/releases">GitHub releases</a>.  \nYou can also download <a href="https://dbeaver.io/files/ea" target="_blank">Early Access</a> version. We publish daily.  \n\n## Running\n\nJust run an installer (or unzip an archive) and run `dbeaver`.  \n\nNote: DBeaver needs Java to run. <a href="https://adoptium.net/temurin/releases/?package=jre" target="_blank">OpenJDK 21</a> is included in all DBeaver distributions.\nYou can change default JDK version by replacing directory `jre` in dbeaver installation folder.\n\n## Documentation\n\n* [Full product documentation](https://dbeaver.com/docs/dbeaver/)\n* [WIKI](https://github.com/dbeaver/dbeaver/wiki)\n* [Issue tracker](https://github.com/dbeaver/dbeaver/issues)\n* [Building from sources](https://github.com/dbeaver/dbeaver/wiki/Build-from-sources)\n\n## Architecture\n\n- DBeaver is written mostly on Java. However, it also uses a set of native OS-specific components for desktop UI, high performance database drivers and networking.\n- Basic frameworks:\n  - [OSGI](https://en.wikipedia.org/wiki/OSGi) platform for plugins and dependency management. Community version consists of 130+ plugins.\n  - [Eclipse RCP](https://github.com/eclipse-platform/eclipse.platform.ui/blob/master/docs/Rich_Client_Platform.md) platform for rich user interface build.\n  - [JDBC](https://en.wikipedia.org/wiki/Java_Database_Connectivity) for basic database connectivity API.\n  - [JSQLParser](https://github.com/JSQLParser/JSqlParser) and [Antlr4](https://github.com/antlr/antlr4) for SQL grammar and semantic parser.\n- For networking and additional functionality we use wide range of open source libraries such as [SSHJ](https://github.com/hierynomus/sshj), [Apache POI](https://github.com/apache/poi), [JFreeChart](https://github.com/jfree/jfreechart), [JTS](https://github.com/locationtech/jts), [Apache JEXL](https://github.com/apache/commons-jexl) etc.\n- We separate model plugins from desktop UI plugins. This allows us to use the same set of "back-end" plugins in both DBeaver and [CloudBeaver](https://github.com/dbeaver/cloudbeaver).\n- Dependencies: being an OSGI application we use P2 repositories for third party dependencies. For additional Maven dependencies we use our own [DBeaver P2 repo](https://github.com/dbeaver/dbeaver-deps-ce).\n\n## Supported databases\n\n### Community version\n\nOut of the box DBeaver supports following database drivers: \nMySQL, MariaDB, Oracle, DB2, PostgreSQL, SQL Server, Sybase, Apache Hive, Drill, Presto, Trino, Phoenix, Exasol, Informix, Teradata, Vertica, Netezza, Firebird, Derby, H2, H2GIS, WMI, Snowflake, Greenplum, Redshift, Athena, SAP HANA, MaxDB, NuoDB, MS Access, SQLite, CSV, DBF, Firebird, TimescaleDB, Yellowbrick, CockroachDB, OrientDB, MonetDB, Google BigQuery, Google Spanner, Apache Hive/Impala/Spark, Apache Ignite, MapD, Azure SQL, CrateDB, Elasticsearch, Ocient, Ingres, OmniSci, Yugabyte, IRIS, Data Virtuality, Denodo, Virtuoso, Machbase, DuckDB, Babelfish, OceanBase, Salesforce, EnterpriseDB, Apache Druid, Apache Kylin, Databricks, OpenSearch, TiDB, TDEngine, Materialize, JDBCX, Dameng, Altibase, StarRocks, CUBRID, GaussDB, DolphinDB, LibSQL, GBase 8s, Databend, Cloudberry, Teiid, Kingbase.\n\n### PRO versions\n\n<a href="https://dbeaver.com/download/">Commercial versions</a> extends functionality of many popular drivers and also support non-JDBC datasources such as:\nODBC, MongoDB, Cassandra, Couchbase, CouchDB, Redis, InfluxDB, Firestore, BigTable, DynamoDB, Kafka KSQL, Neo4j, AWS Neptune, AWS Timestream, Azure CosmosDB, Yugabyte, Salesforce, etc.  \nAlso, we support flat files as databases: CSV, XLSX, Json, XML, Parquet.  \nYou can find the list of all databases supported in commercial versions <a href="https://dbeaver.com/databases/">here</a>.\n\n## Feedback\n\n- For bug reports and feature requests - please <a href="https://github.com/dbeaver/dbeaver/issues">create a ticket</a>.\n- To promote <a href="https://github.com/dbeaver/dbeaver/issues?q=is%3Aissue+is%3Aopen+sort%3Areactions-%2B1-desc+label%3A%22wait+for+votes%22">a ticket</a> to a higher priority - please vote for it with ğŸ‘ under the ticket description.\n- If you have any questions, ideas, etc - please <a href="https://github.com/dbeaver/dbeaver/discussions">start a discussion</a>.\n- Pull requests are welcome. See our <a href="https://github.com/dbeaver/dbeaver/wiki/Contribute-your-code">guide for contributors</a>.\n- Visit https://dbeaver.com for more information.\n- Follow us on [X](https://x.com/dbeaver_news/) and watch educational video on [YouTube](https://www.youtube.com/@DBeaver_video)\n- Thanks for using DBeaver! Star if you like it.\n\n## Contribution: help the Beaver!\n\nHooray, we have reached 40k+ stars on GitHub and continue to grow!  \nThat''s really cool, and we are glad that you like DBeaver.\n\n- We are actively looking for new source code contributors. We have added labels â€œGood first issueâ€ and â€œHelp wantedâ€ to some tickets. If you want to be a part of our development team, just be brave and take a ticket. <a href="https://dbeaver.com/help-dbeaver/">We are happy to reward</a> our most active contributors every major sprint.\n- You can buy <a href="https://dbeaver.com/buy/">one of our commercial versions</a>. They include NoSQL databases support, additional extensions, and official online support. Also, licensed users have priorities in bug fixes and the development of new features.\n\nThank you!  \n\n- <a href="https://github.com/dbeaver/dbeaver/graphs/contributors">DBeaver Team</a> (contributors)\n\n---------\n\n<a href="https://github.com/dbeaver/cloudbeaver/"><img src="https://github.com/dbeaver/cloudbeaver/wiki/images/cloudbeaver-logo.png" width="250"/></a>\n\n<a href="https://github.com/dbeaver/cloudbeaver">CloudBeaver</a> is a web-based database management tool built on the DBeaver platform. It brings the capabilities of DBeaver to the browser, enabling database management from any device with an internet connection and eliminating the need for local installation. Supporting any database, CloudBeaver incorporates most of DBeaver''s features and includes advanced access management for secure collaboration.\nDesigned with a user-friendly interface, CloudBeaver simplifies complex database operations and is suitable for both individual developers and organizations. Its scalable architecture accommodates various needs, making it a convenient solution for managing databases anytime and anywhere through web-based accessibility.\n', '{"language":"Java","stars":47674,"forks":3962,"watchers":47674,"open_issues":3238,"topics":["ai","copilot","database","db2","dbeaver","erd","gui","java","jdbc","mysql","nosql","openai","oracle","postgresql","redshift","sql","sqlite","sqlserver"],"default_branch":"devel","size_kb":203511,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:dbeaver:dbeaver","source_url":"https://github.com/dbeaver/dbeaver"},{"type":"has_code","target_id":"github:dbeaver:dbeaver","source_url":"https://github.com/dbeaver/dbeaver"},{"type":"has_code","target_id":"github:dbeaver:dbeaver","source_url":"https://github.com/dbeaver/dbeaver"},{"type":"has_code","target_id":"github:dbeaver:dbeaver","source_url":"https://github.com/dbeaver/dbeaver"},{"type":"has_code","target_id":"github:dbeaver:dbeaver","source_url":"https://github.com/dbeaver/dbeaver"},{"type":"has_code","target_id":"github:dbeaver:dbeaver","source_url":"https://github.com/dbeaver/dbeaver"},{"type":"has_code","target_id":"github:dbeaver:dbeaver","source_url":"https://github.com/dbeaver/dbeaver"},{"type":"has_code","target_id":"github:eclipse-platform:eclipse.platform.ui","source_url":"https://github.com/eclipse-platform/eclipse.platform.ui"},{"type":"has_code","target_id":"github:JSQLParser:JSqlParser","source_url":"https://github.com/JSQLParser/JSqlParser"},{"type":"has_code","target_id":"github:antlr:antlr4","source_url":"https://github.com/antlr/antlr4"},{"type":"has_code","target_id":"github:hierynomus:sshj","source_url":"https://github.com/hierynomus/sshj"},{"type":"has_code","target_id":"github:apache:poi","source_url":"https://github.com/apache/poi"},{"type":"has_code","target_id":"github:jfree:jfreechart","source_url":"https://github.com/jfree/jfreechart"},{"type":"has_code","target_id":"github:locationtech:jts","source_url":"https://github.com/locationtech/jts"},{"type":"has_code","target_id":"github:apache:commons-jexl","source_url":"https://github.com/apache/commons-jexl"},{"type":"has_code","target_id":"github:dbeaver:cloudbeaver","source_url":"https://github.com/dbeaver/cloudbeaver"},{"type":"has_code","target_id":"github:dbeaver:dbeaver-deps-ce","source_url":"https://github.com/dbeaver/dbeaver-deps-ce"},{"type":"has_code","target_id":"github:dbeaver:dbeaver","source_url":"https://github.com/dbeaver/dbeaver"},{"type":"has_code","target_id":"github:dbeaver:dbeaver","source_url":"https://github.com/dbeaver/dbeaver"},{"type":"has_code","target_id":"github:dbeaver:dbeaver","source_url":"https://github.com/dbeaver/dbeaver"},{"type":"has_code","target_id":"github:dbeaver:dbeaver","source_url":"https://github.com/dbeaver/dbeaver"},{"type":"has_code","target_id":"github:dbeaver:dbeaver","source_url":"https://github.com/dbeaver/dbeaver"},{"type":"has_code","target_id":"github:dbeaver:cloudbeaver","source_url":"https://github.com/dbeaver/cloudbeaver"},{"type":"has_code","target_id":"github:dbeaver:cloudbeaver","source_url":"https://github.com/dbeaver/cloudbeaver"},{"type":"has_code","target_id":"github:dbeaver:cloudbeaver\">CloudBeaver<","source_url":"https://github.com/dbeaver/cloudbeaver\">CloudBeaver<"}]', NULL, 'Apache-2.0', 'approved', 65, '2b7639ce557548f96b8476596cbdd207', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-dbeaver-dbeaver from https://github.com/dbeaver.png
Image converted to WebP: data/images/github-dbeaver-dbeaver.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-docling-project-docling', 'github--docling-project--docling', 'docling', 'docling-project', '<p align="center"> <a href="https://github.com/docling-project/docling"> <img loading="lazy" alt="Docling" src="https://github.com/docling-project/docling/raw/main/docs/assets/docling_processing.png" width="100%"/> </a> </p> <p align="center"> <a href="https://trendshift.io/repositories/12132" target="_blank"><img src="https://trendshift.io/api/badge/repositories/12132" alt="DS4SD%2Fdocling | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a> </p> Docling simplifies...', '["ai","convert","document-parser","document-parsing","documents","docx","html","markdown","pdf","pdf-converter","pdf-to-json","pdf-to-text","pptx","tables","xlsx","python"]', 'other', 46114, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/docling-project/docling","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <a href="https://github.com/docling-project/docling">\n    <img loading="lazy" alt="Docling" src="https://github.com/docling-project/docling/raw/main/docs/assets/docling_processing.png" width="100%"/>\n  </a>\n</p>\n\n# Docling\n\n<p align="center">\n  <a href="https://trendshift.io/repositories/12132" target="_blank"><img src="https://trendshift.io/api/badge/repositories/12132" alt="DS4SD%2Fdocling | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n</p>\n\n[![arXiv](https://img.shields.io/badge/arXiv-2408.09869-b31b1b.svg)](https://arxiv.org/abs/2408.09869)\n[![Docs](https://img.shields.io/badge/docs-live-brightgreen)](https://docling-project.github.io/docling/)\n[![PyPI version](https://img.shields.io/pypi/v/docling)](https://pypi.org/project/docling/)\n[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/docling)](https://pypi.org/project/docling/)\n[![uv](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/uv/main/assets/badge/v0.json)](https://github.com/astral-sh/uv)\n[![Ruff](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/astral-sh/ruff/main/assets/badge/v2.json)](https://github.com/astral-sh/ruff)\n[![Pydantic v2](https://img.shields.io/endpoint?url=https://raw.githubusercontent.com/pydantic/pydantic/main/docs/badge/v2.json)](https://pydantic.dev)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\n[![License MIT](https://img.shields.io/github/license/docling-project/docling)](https://opensource.org/licenses/MIT)\n[![PyPI Downloads](https://static.pepy.tech/badge/docling/month)](https://pepy.tech/projects/docling)\n[![Docling Actor](https://apify.com/actor-badge?actor=vancura/docling?fpr=docling)](https://apify.com/vancura/docling)\n[![Chat with Dosu](https://dosu.dev/dosu-chat-badge.svg)](https://app.dosu.dev/097760a8-135e-4789-8234-90c8837d7f1c/ask?utm_source=github)\n[![Discord](https://img.shields.io/discord/1399788921306746971?color=6A7EC2&logo=discord&logoColor=ffffff)](https://docling.ai/discord)\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/10101/badge)](https://www.bestpractices.dev/projects/10101)\n[![LF AI & Data](https://img.shields.io/badge/LF%20AI%20%26%20Data-003778?logo=linuxfoundation&logoColor=fff&color=0094ff&labelColor=003778)](https://lfaidata.foundation/projects/)\n\nDocling simplifies document processing, parsing diverse formats â€” including advanced PDF understanding â€” and providing seamless integrations with the gen AI ecosystem.\n\n## Features\n\n* ğŸ—‚ï¸ Parsing of [multiple document formats][supported_formats] incl. PDF, DOCX, PPTX, XLSX, HTML, WAV, MP3, VTT, images (PNG, TIFF, JPEG, ...), and more\n* ğŸ“‘ Advanced PDF understanding incl. page layout, reading order, table structure, code, formulas, image classification, and more\n* ğŸ§¬ Unified, expressive [DoclingDocument][docling_document] representation format\n* â†ªï¸ Various [export formats][supported_formats] and options, including Markdown, HTML, [DocTags](https://arxiv.org/abs/2503.11576) and lossless JSON\n* ğŸ”’ Local execution capabilities for sensitive data and air-gapped environments\n* ğŸ¤– Plug-and-play [integrations][integrations] incl. LangChain, LlamaIndex, Crew AI & Haystack for agentic AI\n* ğŸ” Extensive OCR support for scanned PDFs and images\n* ğŸ‘“ Support of several Visual Language Models ([GraniteDocling](https://huggingface.co/ibm-granite/granite-docling-258M))\n* ğŸ™ï¸ Audio support with Automatic Speech Recognition (ASR) models\n* ğŸ”Œ Connect to any agent using the [MCP server](https://docling-project.github.io/docling/usage/mcp/)\n* ğŸ’» Simple and convenient CLI\n\n### What''s new\n* ğŸ“¤ Structured [information extraction][extraction] \[ğŸ§ª beta\]\n* ğŸ“‘ New layout model (**Heron**) by default, for faster PDF parsing\n* ğŸ”Œ [MCP server](https://docling-project.github.io/docling/usage/mcp/) for agentic applications\n* ğŸ’¬ Parsing of Web Video Text Tracks (WebVTT) files\n\n### Coming soon\n\n* ğŸ“ Metadata extraction, including title, authors, references & language\n* ğŸ“ Chart understanding (Barchart, Piechart, LinePlot, etc)\n* ğŸ“ Complex chemistry understanding (Molecular structures)\n\n## Installation\n\nTo use Docling, simply install `docling` from your package manager, e.g. pip:\n```bash\npip install docling\n```\n\nWorks on macOS, Linux and Windows environments. Both x86_64 and arm64 architectures.\n\nMore [detailed installation instructions](https://docling-project.github.io/docling/installation/) are available in the docs.\n\n## Getting started\n\nTo convert individual documents with python, use `convert()`, for example:\n\n```python\nfrom docling.document_converter import DocumentConverter\n\nsource = "https://arxiv.org/pdf/2408.09869"  # document per local path or URL\nconverter = DocumentConverter()\nresult = converter.convert(source)\nprint(result.document.export_to_markdown())  # output: "## Docling Technical Report[...]"\n```\n\nMore [advanced usage options](https://docling-project.github.io/docling/usage/advanced_options/) are available in\nthe docs.\n\n## CLI\n\nDocling has a built-in CLI to run conversions.\n\n```bash\ndocling https://arxiv.org/pdf/2206.01062\n```\n\nYou can also use ğŸ¥š[GraniteDocling](https://huggingface.co/ibm-granite/granite-docling-258M) and other VLMs via Docling CLI:\n```bash\ndocling --pipeline vlm --vlm-model granite_docling https://arxiv.org/pdf/2206.01062\n```\nThis will use MLX acceleration on supported Apple Silicon hardware.\n\nRead more [here](https://docling-project.github.io/docling/usage/)\n\n## Documentation\n\nCheck out Docling''s [documentation](https://docling-project.github.io/docling/), for details on\ninstallation, usage, concepts, recipes, extensions, and more.\n\n## Examples\n\nGo hands-on with our [examples](https://docling-project.github.io/docling/examples/),\ndemonstrating how to address different application use cases with Docling.\n\n## Integrations\n\nTo further accelerate your AI application development, check out Docling''s native\n[integrations](https://docling-project.github.io/docling/integrations/) with popular frameworks\nand tools.\n\n## Get help and support\n\nPlease feel free to connect with us using the [discussion section](https://github.com/docling-project/docling/discussions).\n\n## Technical report\n\nFor more details on Docling''s inner workings, check out the [Docling Technical Report](https://arxiv.org/abs/2408.09869).\n\n## Contributing\n\nPlease read [Contributing to Docling](https://github.com/docling-project/docling/blob/main/CONTRIBUTING.md) for details.\n\n## References\n\nIf you use Docling in your projects, please consider citing the following:\n\n```bib\n@techreport{Docling,\n  author = {Deep Search Team},\n  month = {8},\n  title = {Docling Technical Report},\n  url = {https://arxiv.org/abs/2408.09869},\n  eprint = {2408.09869},\n  doi = {10.48550/arXiv.2408.09869},\n  version = {1.0.0},\n  year = {2024}\n}\n```\n\n## License\n\nThe Docling codebase is under MIT license.\nFor individual model usage, please refer to the model licenses found in the original packages.\n\n## LF AI & Data\n\nDocling is hosted as a project in the [LF AI & Data Foundation](https://lfaidata.foundation/projects/).\n\n### IBM â¤ï¸ Open Source AI\n\nThe project was started by the AI for knowledge team at IBM Research Zurich.\n\n[supported_formats]: https://docling-project.github.io/docling/usage/supported_formats/\n[docling_document]: https://docling-project.github.io/docling/concepts/docling_document/\n[integrations]: https://docling-project.github.io/docling/integrations/\n[extraction]: https://docling-project.github.io/docling/examples/extraction/\n', '{"language":"Python","stars":46114,"forks":3260,"watchers":46114,"open_issues":752,"topics":["ai","convert","document-parser","document-parsing","documents","docx","html","markdown","pdf","pdf-converter","pdf-to-json","pdf-to-text","pptx","tables","xlsx"],"default_branch":"main","size_kb":168077,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:docling-project:docling\">","source_url":"https://github.com/docling-project/docling\">"},{"type":"has_code","target_id":"github:docling-project:docling","source_url":"https://github.com/docling-project/docling"},{"type":"has_code","target_id":"github:astral-sh:uv","source_url":"https://github.com/astral-sh/uv"},{"type":"has_code","target_id":"github:astral-sh:ruff","source_url":"https://github.com/astral-sh/ruff"},{"type":"has_code","target_id":"github:pre-commit:pre-commit","source_url":"https://github.com/pre-commit/pre-commit"},{"type":"has_code","target_id":"github:docling-project:docling","source_url":"https://github.com/docling-project/docling"},{"type":"has_code","target_id":"github:docling-project:docling","source_url":"https://github.com/docling-project/docling"}]', NULL, 'MIT', 'approved', 65, '89eb560dc22e27fd93cf00cc99a76092', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-docling-project-docling from https://github.com/docling-project.png
Image converted to WebP: data/images/github-docling-project-docling.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-jeecgboot-JeecgBoot', 'github--jeecgboot--jeecgboot', 'JeecgBoot', 'jeecgboot', 'JeecgBoot AIä½ä»£ç å¹³å° =============== å½“å‰æœ€æ–°ç‰ˆæœ¬ï¼š 3.9.0ï¼ˆå‘å¸ƒæ—¥æœŸï¼š2025-12-01ï¼‰ é¡¹ç›®ä»‹ç» ----------------------------------- <h3 align="center">ä¼ä¸šçº§AIä½ä»£ç å¹³å°</h3> JeecgBoot æ˜¯ä¸€æ¬¾èåˆä»£ç ç”Ÿæˆä¸AIåº”ç”¨çš„ä½ä»£ç å¼€å‘å¹³å°ï¼ŒåŠ©åŠ›ä¼ä¸šå¿«é€Ÿå®ç°ä½ä»£ç å¼€å‘å’Œæ„å»ºAIåº”ç”¨ã€‚å¹³å°æ”¯æŒMCPå’Œæ’ä»¶æ‰©å±•ï¼Œæä¾›èŠå¤©å¼ä¸šåŠ¡æ“ä½œ(å¦‚â€œä¸€å¥è¯åˆ›å»ºç”¨æˆ·â€)ï¼Œå¤§å¹…æå‡å¼€å‘æ•ˆç‡ä¸ç”¨æˆ·ä¾¿æ·æ€§ã€‚ é‡‡ç”¨å‰åç«¯åˆ†ç¦»æ¶æ„ï¼ˆAnt Design&Vue3ï¼ŒSpringBoot3ï¼ŒSpringCloud Alibabaï¼ŒMybatis-plusï¼‰ï¼Œå¼ºå¤§ä»£ç ç”Ÿæˆå™¨å®ç°å‰åç«¯ä¸€é”®ç”Ÿæˆï¼Œæ— éœ€æ‰‹å†™ä»£ç ã€‚ å¹³å°å¼•é¢†AIä½ä»£ç å¼€å‘æ¨¡å¼ï¼šAIç”Ÿæˆâ†’åœ¨çº¿ç¼–ç â†’ä»£ç ç”Ÿæˆâ†’æ‰‹å·¥åˆå¹¶ï¼Œè§£å†³Javaé¡¹ç›®80%é‡å¤å·¥ä½œï¼Œæå‡æ•ˆç‡ï¼ŒèŠ‚çœæˆæœ¬ï¼Œå…¼é¡¾çµæ´»æ€§ã€‚ å…·å¤‡å¼ºå¤§ä¸”é¢—ç²’åŒ–çš„æƒé™æ§åˆ¶ï¼Œæ”¯æŒæŒ‰é’®æƒé™å’Œæ•°æ®æƒé™è®¾ç½®ï¼Œæ»¡è¶³å¤§å‹ä¸šåŠ¡ç³»ç»Ÿéœ€æ±‚ã€‚åŠŸèƒ½æ¶µç›–åœ¨çº¿è¡¨å•ã€è¡¨å•è®¾è®¡ã€æµç¨‹è®¾è®¡ã€é—¨æˆ·è®¾è®¡ã€æŠ¥è¡¨ä¸å¤§å±è®¾è®¡ã€OAåŠå…¬ã€AIåº”ç”¨ã€AIçŸ¥è¯†åº“ã€...', '["activiti","agent","ai","aiflow","ant-design-vue","antd","codegenerator","deepseek","flowable","langchain4j","llm","low-code","mcp","mybatis-plus","rag","spring-ai","springboot","springboot3","springcloud","vue3","java"]', 'other', 44607, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/jeecgboot/JeecgBoot","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '\nJeecgBoot AIä½ä»£ç å¹³å°\n===============\n\nå½“å‰æœ€æ–°ç‰ˆæœ¬ï¼š 3.9.0ï¼ˆå‘å¸ƒæ—¥æœŸï¼š2025-12-01ï¼‰ \n\n\n[![AUR](https://img.shields.io/badge/license-Apache%20License%202.0-blue.svg)](https://github.com/jeecgboot/JeecgBoot/blob/master/LICENSE)\n[![](https://img.shields.io/badge/Author-åŒ—äº¬å›½ç‚¬è½¯ä»¶-orange.svg)](https://jeecg.com)\n[![](https://img.shields.io/badge/blog-æŠ€æœ¯åšå®¢-orange.svg)](https://jeecg.blog.csdn.net)\n[![](https://img.shields.io/badge/version-3.9.0-brightgreen.svg)](https://github.com/jeecgboot/JeecgBoot)\n[![GitHub stars](https://img.shields.io/github/stars/zhangdaiscott/jeecg-boot.svg?style=social&label=Stars)](https://github.com/jeecgboot/JeecgBoot)\n[![GitHub forks](https://img.shields.io/github/forks/zhangdaiscott/jeecg-boot.svg?style=social&label=Fork)](https://github.com/jeecgboot/JeecgBoot)\n\n\n\né¡¹ç›®ä»‹ç»\n-----------------------------------\n\n<h3 align="center">ä¼ä¸šçº§AIä½ä»£ç å¹³å°</h3>\n\nJeecgBoot æ˜¯ä¸€æ¬¾èåˆä»£ç ç”Ÿæˆä¸AIåº”ç”¨çš„ä½ä»£ç å¼€å‘å¹³å°ï¼ŒåŠ©åŠ›ä¼ä¸šå¿«é€Ÿå®ç°ä½ä»£ç å¼€å‘å’Œæ„å»ºAIåº”ç”¨ã€‚å¹³å°æ”¯æŒMCPå’Œæ’ä»¶æ‰©å±•ï¼Œæä¾›èŠå¤©å¼ä¸šåŠ¡æ“ä½œ(å¦‚â€œä¸€å¥è¯åˆ›å»ºç”¨æˆ·â€)ï¼Œå¤§å¹…æå‡å¼€å‘æ•ˆç‡ä¸ç”¨æˆ·ä¾¿æ·æ€§ã€‚\n\né‡‡ç”¨å‰åç«¯åˆ†ç¦»æ¶æ„ï¼ˆAnt Design&Vue3ï¼ŒSpringBoot3ï¼ŒSpringCloud Alibabaï¼ŒMybatis-plusï¼‰ï¼Œå¼ºå¤§ä»£ç ç”Ÿæˆå™¨å®ç°å‰åç«¯ä¸€é”®ç”Ÿæˆï¼Œæ— éœ€æ‰‹å†™ä»£ç ã€‚ \nå¹³å°å¼•é¢†AIä½ä»£ç å¼€å‘æ¨¡å¼ï¼šAIç”Ÿæˆâ†’åœ¨çº¿ç¼–ç â†’ä»£ç ç”Ÿæˆâ†’æ‰‹å·¥åˆå¹¶ï¼Œè§£å†³Javaé¡¹ç›®80%é‡å¤å·¥ä½œï¼Œæå‡æ•ˆç‡ï¼ŒèŠ‚çœæˆæœ¬ï¼Œå…¼é¡¾çµæ´»æ€§ã€‚ \nå…·å¤‡å¼ºå¤§ä¸”é¢—ç²’åŒ–çš„æƒé™æ§åˆ¶ï¼Œæ”¯æŒæŒ‰é’®æƒé™å’Œæ•°æ®æƒé™è®¾ç½®ï¼Œæ»¡è¶³å¤§å‹ä¸šåŠ¡ç³»ç»Ÿéœ€æ±‚ã€‚åŠŸèƒ½æ¶µç›–åœ¨çº¿è¡¨å•ã€è¡¨å•è®¾è®¡ã€æµç¨‹è®¾è®¡ã€é—¨æˆ·è®¾è®¡ã€æŠ¥è¡¨ä¸å¤§å±è®¾è®¡ã€OAåŠå…¬ã€AIåº”ç”¨ã€AIçŸ¥è¯†åº“ã€å¤§æ¨¡å‹ç®¡ç†ã€AIæµç¨‹ç¼–æ’ã€AIèŠå¤©ï¼Œæ”¯æŒChatGPTã€DeepSeekã€Ollamaç­‰å¤šç§AIå¤§æ¨¡å‹ã€‚\n\n`å‚»ç“œå¼æŠ¥è¡¨:` JimuReportæ˜¯ä¸€æ¬¾è‡ªä¸»ç ”å‘çš„å¼ºå¤§å¼€æºä¼ä¸šçº§WebæŠ¥è¡¨å·¥å…·ã€‚å®ƒé€šè¿‡é›¶ç¼–ç çš„æ‹–æ‹½å¼æ“ä½œï¼Œèµ‹èƒ½ç”¨æˆ·å¦‚åŒæ­ç§¯æœ¨èˆ¬è½»æ¾æ„å»ºå„ç±»å¤æ‚æŠ¥è¡¨ï¼Œå…¨é¢æ»¡è¶³ä¼ä¸šæ•°æ®å¯è§†åŒ–ä¸åˆ†æéœ€æ±‚ï¼ŒåŠ©åŠ›ä¼ä¸šçº§æ•°æ®äº§å“çš„é«˜æ•ˆæ‰“é€ ä¸åº”ç”¨ã€‚\n\n`å‚»ç“œå¼å¤§å±:` JimuBIä¸€æ¬¾è‡ªä¸»ç ”å‘çš„å¼ºå¤§çš„å¤§å±å’Œä»ªè¡¨ç›˜è®¾è®¡å·¥å…·ã€‚ä¸“æ³¨æ•°å­—å­ªç”Ÿä¸æ•°æ®å¯è§†åŒ–ï¼Œæ”¯æŒäº¤äº’å¼å¤§å±ã€ä»ªè¡¨ç›˜ã€é—¨æˆ·å’Œç§»åŠ¨ç«¯ï¼Œå®ç°â€œä¸€æ¬¡å¼€å‘ï¼Œå¤šç«¯é€‚é…â€ã€‚ å¤§å±è®¾è®¡ç±»Wordé£æ ¼ï¼Œæ”¯æŒå¤šå±åˆ‡æ¢ï¼Œè‡ªç”±æ‹–æ‹½ï¼Œè½»æ¾æ‰“é€ ç‚«é…·åŠ¨æ€ç•Œé¢ã€‚\n\n`æˆç†ŸAIåº”ç”¨åŠŸèƒ½:` æä¾›ä¸€å¥—å®Œå–„AIåº”ç”¨å¹³å°: æ¶µç›–AIåº”ç”¨ç®¡ç†ã€AIæ¨¡å‹ç®¡ç†ã€æ™ºèƒ½å¯¹è¯åŠ©æ‰‹ã€çŸ¥è¯†åº“é—®ç­”ã€æµç¨‹ç¼–æ’ä¸è®¾è®¡å™¨ã€AIå»ºè¡¨ã€MCPæ’ä»¶é…ç½®ç­‰åŠŸèƒ½ã€‚å¹³å°å…¼å®¹ä¸»æµå¤§æ¨¡å‹ï¼ŒåŒ…æ‹¬ChatGPTã€DeepSeekã€Ollamaã€æ™ºæ™®ã€åƒé—®ç­‰ï¼ŒåŠ©åŠ›ä¼ä¸šé«˜æ•ˆæ„å»ºæ™ºèƒ½åŒ–åº”ç”¨ï¼Œæ¨åŠ¨ä½ä»£ç å¼€å‘ä¸AIæ·±åº¦èåˆã€‚\n\n`JEECGå®—æ—¨æ˜¯:` JEECGæ—¨åœ¨é€šè¿‡OnlineCodingå¹³å°å®ç°ç®€å•åŠŸèƒ½çš„é›¶ä»£ç å¿«é€Ÿæ­å»ºï¼ŒåŒæ—¶é’ˆå¯¹å¤æ‚åŠŸèƒ½é‡‡ç”¨ä»£ç ç”Ÿæˆå™¨ç”Ÿæˆä»£ç å¹¶æ‰‹å·¥åˆå¹¶ï¼Œæ‰“é€ æ™ºèƒ½ä¸”çµæ´»çš„ä½ä»£ç å¼€å‘æ¨¡å¼ï¼Œæœ‰æ•ˆè§£å†³äº†å½“å‰ä½ä»£ç äº§å“æ™®éç¼ºä¹çµæ´»æ€§çš„é—®é¢˜ï¼Œæå‡å¼€å‘æ•ˆç‡çš„åŒæ—¶å…¼é¡¾ç³»ç»Ÿçš„æ‰©å±•æ€§å’Œå®šåˆ¶åŒ–èƒ½åŠ›ã€‚\n\n`JEECGä¸šåŠ¡æµç¨‹:` JEECGä¸šåŠ¡æµç¨‹é‡‡ç”¨BPMå·¥ä½œæµå¼•æ“å®ç°ä¸šåŠ¡å®¡æ‰¹ï¼Œæ‰©å±•ä»»åŠ¡æ¥å£ä¾›å¼€å‘äººå‘˜ç¼–å†™ä¸šåŠ¡é€»è¾‘ï¼Œè¡¨å•æä¾›è¡¨å•è®¾è®¡å™¨ã€åœ¨çº¿é…ç½®è¡¨å•å’Œç¼–ç è¡¨å•ç­‰å¤šç§è§£å†³æ–¹æ¡ˆã€‚é€šè¿‡æµç¨‹ä¸è¡¨å•çš„åˆ†ç¦»è®¾è®¡ï¼ˆæ¾è€¦åˆï¼‰åŠä»»åŠ¡èŠ‚ç‚¹çš„çµæ´»é…ç½®ï¼Œæ—¢ä¿éšœäº†ä¼ä¸šæµç¨‹çš„å®‰å…¨æ€§ä¸ä¿å¯†æ€§ï¼Œåˆå¤§å¹…é™ä½äº†å¼€å‘äººå‘˜çš„å·¥ä½œé‡ã€‚\n\n\n\n\n\né€‚ç”¨é¡¹ç›®\n-----------------------------------\nJeecgBootä½ä»£ç å¹³å°å…¼å®¹æ‰€æœ‰J2EEé¡¹ç›®å¼€å‘ï¼Œæ”¯æŒä¿¡åˆ›å›½äº§åŒ–ï¼Œç‰¹åˆ«é€‚ç”¨äºSAASã€ä¼ä¸šä¿¡æ¯ç®¡ç†ç³»ç»Ÿï¼ˆMISï¼‰ã€å†…éƒ¨åŠå…¬ç³»ç»Ÿï¼ˆOAï¼‰ã€ä¼ä¸šèµ„æºè®¡åˆ’ç³»ç»Ÿï¼ˆERPï¼‰ã€å®¢æˆ·å…³ç³»ç®¡ç†ç³»ç»Ÿï¼ˆCRMï¼‰åŠAIçŸ¥è¯†åº“ç­‰åœºæ™¯ã€‚å…¶åŠæ™ºèƒ½æ‰‹å·¥Mergeå¼€å‘æ¨¡å¼ï¼Œå¯æ˜¾è‘—æå‡70%ä»¥ä¸Šçš„å¼€å‘æ•ˆç‡ï¼Œæå¤§é™ä½å¼€å‘æˆæœ¬ã€‚åŒæ—¶ï¼ŒJeecgBootè¿˜æ˜¯ä¸€æ¬¾å…¨æ ˆå¼AIå¼€å‘å¹³å°ï¼ŒåŠ©åŠ›ä¼ä¸šå¿«é€Ÿæ„å»ºå’Œéƒ¨ç½²ä¸ªæ€§åŒ–AIåº”ç”¨ã€‚ã€‚\n\n\n**ä¿¡åˆ›å…¼å®¹è¯´æ˜**\n- æ“ä½œç³»ç»Ÿï¼šå›½äº§éº’éºŸã€é“¶æ²³éº’éºŸç­‰å›½äº§ç³»ç»Ÿå‡ ä¹éƒ½æ˜¯åŸºäº Linux å†…æ ¸ï¼Œå› æ­¤å®ƒä»¬å…·æœ‰è‰¯å¥½çš„å…¼å®¹æ€§ã€‚\n- æ•°æ®åº“ï¼šè¾¾æ¢¦ã€äººå¤§é‡‘ä»“ã€TiDB\n- ä¸­é—´ä»¶ï¼šä¸œæ–¹é€š TongWebã€TongRDSï¼Œå®å…°å¾· AppServerã€CacheDB, [ä¿¡åˆ›é…ç½®æ–‡æ¡£](https://help.jeecg.com/java/tongweb-deploy/)\n\n\nç‰ˆæœ¬è¯´æ˜\n-----------------------------------\n\n|ä¸‹è½½ | SpringBoot3.5 + Shiro                                   |SpringBoot3.5+ SpringAuthorizationServer | SpringBoot3.5 + Sa-Token | SpringBoot2.7(JDK17/JDK8) |\n|------|---------------------------------------------------------|----------------------------|-------------------|--------------------------------------------|\n| Github | [`main`](https://github.com/jeecgboot/JeecgBoot)        | [`springboot3_sas`](https://github.com/jeecgboot/JeecgBoot/tree/springboot3_sas) åˆ†æ”¯  |  [`springboot3-satoken`](https://github.com/jeecgboot/JeecgBoot/tree/springboot3-satoken) åˆ†æ”¯|[`springboot2`](https://github.com/jeecgboot/JeecgBoot/tree/springboot2) åˆ†æ”¯|\n| Gitee | [`main`](https://github.com/jeecgboot/JeecgBoot) | [`springboot3_sas`](https://gitee.com/jeecg/JeecgBoot/tree/springboot3_sas) åˆ†æ”¯|  [`springboot3-satoken`](https://gitee.com/jeecg/JeecgBoot/tree/springboot3-satoken) åˆ†æ”¯|[`springboot2`](https://github.com/jeecgboot/JeecgBoot/tree/springboot2)     åˆ†æ”¯ |\n\n\n- `jeecg-boot` æ˜¯åç«¯JAVAæºç é¡¹ç›®Springboot3+Shiro+Mybatis+SpringCloudAlibabaï¼ˆæ”¯æŒå•ä½“å’Œå¾®æœåŠ¡åˆ‡æ¢ï¼‰.\n- `jeecgboot-vue3` æ˜¯å‰ç«¯VUE3æºç é¡¹ç›®ï¼ˆvue3+vite6+tsæœ€æ–°æŠ€æœ¯æ ˆï¼‰.\n- `JeecgUniapp`  æ˜¯[é…å¥—APPæ¡†æ¶](https://github.com/jeecgboot/JeecgUniapp) é€‚é…å¤šä¸ªç»ˆç«¯ï¼Œæ”¯æŒAPPã€å°ç¨‹åºã€H5ã€é¸¿è’™ã€é¸¿è’™Next.\n- `jeecg-boot-starter`  æ˜¯[jeecg-bootå¯¹åº”çš„åº•å±‚å°è£…starter](https://github.com/jeecgboot/jeecg-boot-starter) ï¼šå¾®æœåŠ¡å¯åŠ¨ã€xxljobã€åˆ†å¸ƒå¼é”starterã€rabbitmqã€åˆ†å¸ƒå¼äº‹åŠ¡ã€åˆ†åº“åˆ†è¡¨shardingsphereç­‰.\n- å‚è€ƒ [æ–‡æ¡£](https://help.jeecg.com/ui/2dev/mini) å¯ä»¥åˆ é™¤ä¸éœ€è¦çš„demoï¼Œåˆ¶ä½œä¸€ä¸ªç²¾ç®€ç‰ˆæœ¬\n\n\n\n\n\nå¯åŠ¨é¡¹ç›®\n-----------------------------------\n\n> é»˜è®¤è´¦å·å¯†ç ï¼š admin/123456\n\n- [å¼€å‘ç¯å¢ƒæ­å»º](https://help.jeecg.com/java/setup/tools)\n- [IDEAå¯åŠ¨å‰åç«¯(å•ä½“æ¨¡å¼)](https://help.jeecg.com/java/setup/idea/startup)\n- [Dockerä¸€é”®å¯åŠ¨(å•ä½“æ¨¡å¼)](https://help.jeecg.com/java/docker/quick)\n- [IDEAå¯åŠ¨å‰åç«¯(å¾®æœåŠ¡æ–¹å¼)](https://help.jeecg.com/java/springcloud/switchcloud/monomer)\n- [Dockerä¸€é”®å¯åŠ¨(å¾®æœåŠ¡æ–¹å¼)](https://help.jeecg.com/java/docker/quickcloud)\n\n\næŠ€æœ¯æ–‡æ¡£\n-----------------------------------\n\n- å®˜æ–¹ç½‘ç«™ï¼š  [http://www.jeecg.com](http://www.jeecg.com)\n- åœ¨çº¿æ¼”ç¤ºï¼š  [å¹³å°æ¼”ç¤º](https://boot3.jeecg.com) | [APPæ¼”ç¤º](https://jeecg.com/appIndex)\n- å…¥é—¨æŒ‡å—ï¼š  [å¿«é€Ÿå…¥é—¨](http://www.jeecg.com/doc/quickstart)  | [ä»£ç ç”Ÿæˆä½¿ç”¨](https://help.jeecg.com/java/codegen/online) | [å¼€å‘æ–‡æ¡£](https://help.jeecg.com)  | [AIåº”ç”¨æ‰‹å†Œ](https://help.jeecg.com/aigc) | [è§†é¢‘æ•™ç¨‹](http://jeecg.com/doc/video)\n- AIç¼–ç¨‹å®æˆ˜è§†é¢‘ï¼š  [JEECGä½ä»£ç ä¸Cursor+GitHub Copilotå®ç°AIé«˜æ•ˆç¼–ç¨‹å®æˆ˜](https://www.bilibili.com/video/BV11XyaBVEoH)\n- æŠ€æœ¯æ”¯æŒï¼š  [åé¦ˆé—®é¢˜](https://github.com/jeecgboot/JeecgBoot/issues/new?template=bug_report.md)    | [ä½ä»£ç ä½“éªŒä¸€åˆ†é’Ÿ](https://jeecg.blog.csdn.net/article/details/106079007) \n- QQäº¤æµç¾¤ ï¼š 964611995ã€â‘©716488839(æ»¡)ã€â‘¨808791225(æ»¡)ã€å…¶ä»–(æ»¡)\n\n\nAI åº”ç”¨å¹³å°ä»‹ç»\n-----------------------------------\n\nä¸€ä¸ªå…¨æ ˆå¼ AI å¼€å‘å¹³å°ï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘è€…å¿«é€Ÿæ„å»ºå’Œéƒ¨ç½²ä¸ªæ€§åŒ–çš„ AI åº”ç”¨ã€‚\n\nJeecgBootå¹³å°æä¾›äº†ä¸€å¥—å®Œå–„çš„AIåº”ç”¨ç®¡ç†ç³»ç»Ÿæ¨¡å—ï¼Œæ˜¯ä¸€å¥—ç±»ä¼¼`Dify`çš„`AIGCåº”ç”¨å¼€å‘å¹³å°`+`çŸ¥è¯†åº“é—®ç­”`ï¼Œæ˜¯ä¸€æ¬¾åŸºäºLLMå¤§è¯­è¨€æ¨¡å‹AIåº”ç”¨å¹³å°å’Œ RAG çš„çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿã€‚\nå…¶ç›´è§‚çš„ç•Œé¢ç»“åˆäº† AI æµç¨‹ç¼–æ’ã€RAG ç®¡é“ã€çŸ¥è¯†åº“ç®¡ç†ã€æ¨¡å‹ç®¡ç†ã€å¯¹æ¥å‘é‡åº“ã€å®æ—¶è¿è¡Œå¯è§‚å¯Ÿç­‰ï¼Œè®©æ‚¨å¯ä»¥å¿«é€Ÿä»åŸå‹åˆ°ç”Ÿäº§ï¼Œæ‹¥æœ‰AIæœåŠ¡èƒ½åŠ›ã€‚ \n\n- [è¯¦ç»†ä¸“é¢˜ä»‹ç»ï¼Œè¯·ç‚¹å‡»æŸ¥çœ‹](README-AI.md)\n\n- AIè§†é¢‘ä»‹ç»\n\n[![](https://jeecgos.oss-cn-beijing.aliyuncs.com/files/jeecg_aivideo.png)](https://www.bilibili.com/video/BV1zmd7YFE4w)\n\n\nä¸ºä»€ä¹ˆé€‰æ‹©JeecgBoot?\n-----------------------------------\n- 1.é‡‡ç”¨æœ€æ–°ä¸»æµå‰ååˆ†ç¦»æ¡†æ¶ï¼ˆSpring Boot3 + MyBatis + Shiro/SpringAuthorizationServer + Ant Design4 + Vue3ï¼‰ï¼Œå®¹æ˜“ä¸Šæ‰‹ï¼›ä»£ç ç”Ÿæˆå™¨ä¾èµ–æ€§ä½ï¼Œçµæ´»çš„æ‰©å±•èƒ½åŠ›ï¼Œå¯å¿«é€Ÿå®ç°äºŒæ¬¡å¼€å‘ã€‚\n- 2.å‰ç«¯å¤§ç‰ˆæœ¬æ¢ä»£ï¼Œæœ€æ–°ç‰ˆé‡‡ç”¨ Vue3.0 + TypeScript + Vite6 + Ant Design Vue4 ç­‰æ–°æŠ€æœ¯æ–¹æ¡ˆã€‚\n- 3.æ”¯æŒå¾®æœåŠ¡Spring Cloud Alibabaï¼ˆNacosã€Gatewayã€Sentinelã€Skywalkingï¼‰ï¼Œæä¾›ç®€æ˜“æœºåˆ¶ï¼Œæ”¯æŒå•ä½“å’Œå¾®æœåŠ¡è‡ªç”±åˆ‡æ¢ï¼ˆè¿™æ ·å¯ä»¥æ»¡è¶³å„ç±»é¡¹ç›®éœ€æ±‚ï¼‰ã€‚\n- 4.å¼€å‘æ•ˆç‡é«˜ï¼Œæ”¯æŒåœ¨çº¿å»ºè¡¨å’ŒAIå»ºè¡¨ï¼Œæä¾›å¼ºå¤§ä»£ç ç”Ÿæˆå™¨ï¼Œå•è¡¨ã€æ ‘åˆ—è¡¨ã€ä¸€å¯¹å¤šã€ä¸€å¯¹ä¸€ç­‰æ•°æ®æ¨¡å‹ï¼Œå¢åˆ æ”¹æŸ¥åŠŸèƒ½ä¸€é”®ç”Ÿæˆï¼Œèœå•é…ç½®ç›´æ¥ä½¿ç”¨ã€‚\n- 5.ä»£ç ç”Ÿæˆå™¨æä¾›å¼ºå¤§æ¨¡æ¿æœºåˆ¶ï¼Œæ”¯æŒè‡ªå®šä¹‰æ¨¡æ¿ï¼Œç›®å‰æä¾›å››å¥—é£æ ¼æ¨¡æ¿ï¼ˆå•è¡¨ä¸¤å¥—ã€æ ‘æ¨¡å‹ä¸€å¥—ã€ä¸€å¯¹å¤šä¸‰å¥—ï¼‰ã€‚\n- 6.æä¾›å¼ºå¤§çš„æŠ¥è¡¨å’Œå¤§å±å¯è§†åŒ–å·¥å…·ï¼Œæ”¯æŒä¸°å¯Œçš„æ•°æ®æºè¿æ¥ï¼Œèƒ½å¤Ÿé€šè¿‡æ‹–æ‹‰æ‹½æ–¹å¼å¿«é€Ÿåˆ¶ä½œæŠ¥è¡¨ã€å¤§å±å’Œé—¨æˆ·è®¾è®¡ï¼›æ”¯æŒå¤šç§å›¾è¡¨ç±»å‹ï¼šæŸ±å½¢å›¾ã€æŠ˜çº¿å›¾ã€æ•£ç‚¹å›¾ã€é¥¼å›¾ã€ç¯å½¢å›¾ã€é¢ç§¯å›¾ã€æ¼æ–—å›¾ã€è¿›åº¦å›¾ã€ä»ªè¡¨ç›˜ã€é›·è¾¾å›¾ã€åœ°å›¾ç­‰ã€‚\n- 7.ä½ä»£ç èƒ½åŠ›ï¼šåœ¨çº¿è¡¨å•ï¼ˆæ— éœ€ç¼–ç ï¼Œé€šè¿‡åœ¨çº¿é…ç½®è¡¨å•ï¼Œå®ç°è¡¨å•çš„å¢åˆ æ”¹æŸ¥ï¼Œæ”¯æŒå•è¡¨ã€æ ‘ã€ä¸€å¯¹å¤šã€ä¸€å¯¹ä¸€ç­‰æ¨¡å‹ï¼Œå®ç°äººäººçš†å¯ç¼–ç ï¼‰ï¼Œåœ¨çº¿é…ç½®é›¶ä»£ç å¼€å‘ã€æ‰€è§å³æ‰€å¾—æ”¯æŒ23ç§ç±»æ§ä»¶ã€‚\n- 8.ä½ä»£ç èƒ½åŠ›ï¼šåœ¨çº¿æŠ¥è¡¨ã€åœ¨çº¿å›¾è¡¨ï¼ˆæ— éœ€ç¼–ç ï¼Œé€šè¿‡åœ¨çº¿é…ç½®æ–¹å¼ï¼Œå®ç°æ•°æ®æŠ¥è¡¨å’Œå›¾å½¢æŠ¥è¡¨ï¼Œå¯ä»¥å¿«é€ŸæŠ½å–æ•°æ®ï¼Œå‡è½»å¼€å‘å‹åŠ›ï¼Œå®ç°äººäººçš†å¯ç¼–ç ï¼‰ã€‚\n- 9.Onlineæ”¯æŒåœ¨çº¿å¢å¼ºå¼€å‘ï¼Œæä¾›åœ¨çº¿ä»£ç ç¼–è¾‘å™¨ï¼Œæ”¯æŒä»£ç é«˜äº®ã€ä»£ç æç¤ºç­‰åŠŸèƒ½ï¼Œæ”¯æŒå¤šç§è¯­è¨€ï¼ˆJavaã€SQLã€JavaScriptç­‰ï¼‰ã€‚\n- 10.å°è£…å®Œå–„çš„ç”¨æˆ·ã€è§’è‰²ã€èœå•ã€ç»„ç»‡æœºæ„ã€æ•°æ®å­—å…¸ã€åœ¨çº¿å®šæ—¶ä»»åŠ¡ç­‰åŸºç¡€åŠŸèƒ½ï¼Œæ”¯æŒè®¿é—®æˆæƒã€æŒ‰é’®æƒé™ã€æ•°æ®æƒé™ç­‰åŠŸèƒ½ã€‚\n- 11.å‰ç«¯UIæä¾›ä¸°å¯Œçš„ç»„ä»¶åº“ï¼Œæ”¯æŒå„ç§å¸¸ç”¨ç»„ä»¶ï¼Œå¦‚è¡¨æ ¼ã€æ ‘å½¢æ§ä»¶ã€ä¸‹æ‹‰æ¡†ã€æ—¥æœŸé€‰æ‹©å™¨ç­‰ï¼Œæ»¡è¶³å„ç§å¤æ‚çš„ä¸šåŠ¡éœ€æ±‚ [UIç»„ä»¶åº“æ–‡æ¡£](https://help.jeecg.com/category/ui%E7%BB%84%E4%BB%B6%E5%BA%93)ã€‚\n- 12.æä¾›APPé…å¥—æ¡†æ¶ï¼Œä¸€ä»½å¤šä»£ç å¤šç»ˆç«¯é€‚é…ï¼Œä¸€ä»½ä»£ç å¤šç»ˆç«¯é€‚é…ï¼Œå°ç¨‹åºã€H5ã€å®‰å“ã€iOSã€é¸¿è’™Nextã€‚\n- 13.æ–°ç‰ˆAPPæ¡†æ¶é‡‡ç”¨Uniappã€Vue3.0ã€Viteã€Wot-design-uniã€TypeScriptç­‰æœ€æ–°æŠ€æœ¯æ ˆï¼ŒåŒ…æ‹¬äºŒæ¬¡å°è£…ç»„ä»¶ã€è·¯ç”±æ‹¦æˆªã€è¯·æ±‚æ‹¦æˆªç­‰åŠŸèƒ½ã€‚å®ç°äº†ä¸JeecgBootå®Œç¾å¯¹æ¥ï¼šç›®å‰å·²ç»å®ç°ç™»å½•ã€ç”¨æˆ·ä¿¡æ¯ã€é€šè®¯å½•ã€å…¬å‘Šã€ç§»åŠ¨é¦–é¡µã€ä¹å®«æ ¼ã€èŠå¤©ã€Onlineè¡¨å•ã€ä»ªè¡¨ç›˜ç­‰åŠŸèƒ½ï¼Œæä¾›äº†ä¸°å¯Œçš„ç»„ä»¶ã€‚\n- 14.æä¾›äº†ä¸€å¥—æˆç†Ÿçš„AIåº”ç”¨å¹³å°åŠŸèƒ½ï¼Œä»AIæ¨¡å‹ã€çŸ¥è¯†åº“åˆ°AIåº”ç”¨æ­å»ºï¼ŒåŠ©åŠ›ä¼ä¸šå¿«é€Ÿè½åœ°AIæœåŠ¡ï¼ŒåŠ é€Ÿæ™ºèƒ½åŒ–å‡çº§ã€‚\n- 15.AIèƒ½åŠ›ï¼šç›®å‰JeecgBootæ”¯æŒAIå¤§æ¨¡å‹chatgptå’Œdeepseekï¼Œç°åœ¨æœ€æ–°ç‰ˆé»˜è®¤ä½¿ç”¨deepseekï¼Œé€Ÿåº¦æ›´å¿«è´¨é‡æ›´é«˜ã€‚ç›®å‰æä¾›äº†AIå¯¹è¯åŠ©æ‰‹ã€AIçŸ¥è¯†åº“ã€AIåº”ç”¨ã€AIå»ºè¡¨ã€AIæŠ¥è¡¨ç­‰åŠŸèƒ½ã€‚\n- 16.æä¾›æ–°è¡Œç¼–è¾‘è¡¨æ ¼JVXETableï¼Œè½»æ¾æ»¡è¶³å„ç§å¤æ‚ERPå¸ƒå±€ï¼Œæ‹¥æœ‰æ›´é«˜çš„æ€§èƒ½ã€æ›´çµæ´»çš„æ‰©å±•ã€æ›´å¼ºå¤§çš„åŠŸèƒ½ã€‚\n- 17.å¹³å°é¦–é¡µé£æ ¼ï¼Œæä¾›å¤šç§ç»„åˆæ¨¡å¼ï¼Œæ”¯æŒè‡ªå®šä¹‰é£æ ¼ï¼›æ”¯æŒé—¨æˆ·è®¾è®¡ï¼Œæ”¯æŒè‡ªå®šä¹‰é¦–é¡µã€‚\n- 18.å¸¸ç”¨å…±é€šå°è£…ï¼Œå„ç§å·¥å…·ç±»ï¼ˆå®šæ—¶ä»»åŠ¡ã€çŸ­ä¿¡æ¥å£ã€é‚®ä»¶å‘é€ã€Excelå¯¼å…¥å¯¼å‡ºç­‰ï¼‰ï¼ŒåŸºæœ¬æ»¡è¶³80%é¡¹ç›®éœ€æ±‚ã€‚\n- 19.ç®€æ˜“Excelå¯¼å…¥å¯¼å‡ºï¼Œæ”¯æŒå•è¡¨å¯¼å‡ºå’Œä¸€å¯¹å¤šè¡¨æ¨¡å¼å¯¼å‡ºï¼Œç”Ÿæˆçš„ä»£ç è‡ªå¸¦å¯¼å…¥å¯¼å‡ºåŠŸèƒ½ã€‚\n- 20.é›†æˆæ™ºèƒ½æŠ¥è¡¨å·¥å…·ï¼ŒæŠ¥è¡¨æ‰“å°ã€å›¾åƒæŠ¥è¡¨å’Œæ•°æ®å¯¼å‡ºéå¸¸æ–¹ä¾¿ï¼Œå¯æå…¶æ–¹ä¾¿åœ°ç”ŸæˆPDFã€Excelã€Wordç­‰æŠ¥è¡¨ã€‚\n- 21.é‡‡ç”¨å‰ååˆ†ç¦»æŠ€æœ¯ï¼Œé¡µé¢UIé£æ ¼ç²¾ç¾ï¼Œé’ˆå¯¹å¸¸ç”¨ç»„ä»¶åšäº†å°è£…ï¼šæ—¶é—´ã€è¡Œè¡¨æ ¼æ§ä»¶ã€æˆªå–æ˜¾ç¤ºæ§ä»¶ã€æŠ¥è¡¨ç»„ä»¶ã€ç¼–è¾‘å™¨ç­‰ã€‚\n- 22.æŸ¥è¯¢è¿‡æ»¤å™¨ï¼šæŸ¥è¯¢åŠŸèƒ½è‡ªåŠ¨ç”Ÿæˆï¼Œåå°åŠ¨æ€æ‹¼SQLè¿½åŠ æŸ¥è¯¢æ¡ä»¶ï¼›æ”¯æŒå¤šç§åŒ¹é…æ–¹å¼ï¼ˆå…¨åŒ¹é…/æ¨¡ç³ŠæŸ¥è¯¢/åŒ…å«æŸ¥è¯¢/ä¸åŒ¹é…æŸ¥è¯¢ï¼‰ã€‚\n- 23.æ•°æ®æƒé™ï¼ˆç²¾ç»†åŒ–æ•°æ®æƒé™æ§åˆ¶ï¼Œæ§åˆ¶åˆ°è¡Œçº§ã€åˆ—è¡¨çº§ã€è¡¨å•å­—æ®µçº§ï¼Œå®ç°ä¸åŒäººçœ‹ä¸åŒæ•°æ®ï¼Œä¸åŒäººå¯¹åŒä¸€ä¸ªé¡µé¢æ“ä½œä¸åŒå­—æ®µï¼‰ã€‚\n- 24.æ¥å£å®‰å…¨æœºåˆ¶ï¼Œå¯ç»†åŒ–æ§åˆ¶æ¥å£æˆæƒï¼Œéå¸¸ç®€ä¾¿å®ç°ä¸åŒå®¢æˆ·ç«¯åªçœ‹è‡ªå·±æ•°æ®ç­‰æ§åˆ¶ï¼›ä¹Ÿæä¾›äº†åŸºäºAKå’ŒSKè®¤è¯é‰´æƒçš„OpenAPIåŠŸèƒ½ã€‚\n- 25.æ´»è·ƒçš„ç¤¾åŒºæ”¯æŒï¼›è¿‘å¹´æ¥ï¼Œéšç€ç½‘ç»œå¨èƒçš„æ—¥ç›Šå¢åŠ ï¼Œå›¢é˜Ÿåœ¨å®‰å…¨å’Œæ¼æ´ç®¡ç†æ–¹é¢ç§¯ç´¯äº†ä¸°å¯Œçš„ç»éªŒï¼Œèƒ½å¤Ÿä¸ºä¼ä¸šæä¾›å…¨é¢çš„å®‰å…¨è§£å†³æ–¹æ¡ˆã€‚\n- 26.æƒé™æ§åˆ¶é‡‡ç”¨RBACï¼ˆRole-Based Access Controlï¼ŒåŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶ï¼‰ã€‚\n- 27.é¡µé¢æ ¡éªŒè‡ªåŠ¨ç”Ÿæˆï¼ˆå¿…é¡»è¾“å…¥ã€æ•°å­—æ ¡éªŒã€é‡‘é¢æ ¡éªŒã€æ—¶é—´ç©ºé—´ç­‰ï¼‰ã€‚\n- 28.æ”¯æŒSaaSæœåŠ¡æ¨¡å¼ï¼Œæä¾›SaaSå¤šç§Ÿæˆ·æ¶æ„æ–¹æ¡ˆã€‚\n- 29.åˆ†å¸ƒå¼æ–‡ä»¶æœåŠ¡ï¼Œé›†æˆMinIOã€é˜¿é‡ŒOSSç­‰ä¼˜ç§€çš„ç¬¬ä¸‰æ–¹ï¼Œæä¾›ä¾¿æ·çš„æ–‡ä»¶ä¸Šä¼ ä¸ç®¡ç†ï¼ŒåŒæ—¶ä¹Ÿæ”¯æŒæœ¬åœ°å­˜å‚¨ã€‚\n- 30.ä¸»æµæ•°æ®åº“å…¼å®¹ï¼Œä¸€å¥—ä»£ç å®Œå…¨å…¼å®¹MySQLã€PostgreSQLã€Oracleã€SQL Serverã€MariaDBã€è¾¾æ¢¦ã€äººå¤§é‡‘ä»“ç­‰ä¸»æµæ•°æ®åº“ã€‚\n- 31.é›†æˆå·¥ä½œæµFlowableï¼Œå¹¶å®ç°äº†åªéœ€åœ¨é¡µé¢é…ç½®æµç¨‹è½¬å‘ï¼Œå¯æå¤§ç®€åŒ–BPMå·¥ä½œæµçš„å¼€å‘ï¼›ç”¨BPMçš„æµç¨‹è®¾è®¡å™¨ç”»å‡ºäº†æµç¨‹èµ°å‘ï¼Œä¸€ä¸ªå·¥ä½œæµåŸºæœ¬å°±å®Œæˆäº†ï¼Œåªéœ€å†™å¾ˆå°‘é‡çš„Javaä»£ç ã€‚\n- 32.ä½ä»£ç èƒ½åŠ›ï¼šåœ¨çº¿æµç¨‹è®¾è®¡ï¼Œé‡‡ç”¨å¼€æºFlowableæµç¨‹å¼•æ“ï¼Œå®ç°åœ¨çº¿ç”»æµç¨‹ã€è‡ªå®šä¹‰è¡¨å•ã€è¡¨å•æŒ‚é ã€ä¸šåŠ¡æµè½¬ã€‚\n- 33.å¤šæ•°æ®æºï¼šæå…¶ç®€æ˜“çš„ä½¿ç”¨æ–¹å¼ï¼Œåœ¨çº¿é…ç½®æ•°æ®æºé…ç½®ï¼Œä¾¿æ·åœ°ä»å…¶ä»–æ•°æ®æŠ“å–æ•°æ®ã€‚\n- 34.æä¾›å•ç‚¹ç™»å½•CASé›†æˆæ–¹æ¡ˆï¼Œé¡¹ç›®ä¸­å·²ç»æä¾›å®Œå–„çš„å¯¹æ¥ä»£ç ã€‚\n- 35.ä½ä»£ç èƒ½åŠ›ï¼šè¡¨å•è®¾è®¡å™¨ï¼Œæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰è¡¨å•å¸ƒå±€ï¼Œæ”¯æŒå•è¡¨ã€ä¸€å¯¹å¤šè¡¨å•ï¼Œæ”¯æŒselectã€radioã€checkboxã€textareaã€dateã€popupã€åˆ—è¡¨ã€å®ç­‰æ§ä»¶ã€‚\n- 36.ä¸“ä¸šæ¥å£å¯¹æ¥æœºåˆ¶ï¼Œç»Ÿä¸€é‡‡ç”¨RESTfulæ¥å£æ–¹å¼ï¼Œé›†æˆSwagger-UIåœ¨çº¿æ¥å£æ–‡æ¡£ï¼ŒJWT tokenå®‰å…¨éªŒè¯ï¼Œæ–¹ä¾¿å®¢æˆ·ç«¯å¯¹æ¥ã€‚\n- 37.é«˜çº§ç»„åˆæŸ¥è¯¢åŠŸèƒ½ï¼Œåœ¨çº¿é…ç½®æ”¯æŒä¸»å­è¡¨å…³è”æŸ¥è¯¢ï¼Œå¯ä¿å­˜æŸ¥è¯¢å†å²ã€‚\n- 38.æä¾›å„ç§ç³»ç»Ÿç›‘æ§ï¼Œå®æ—¶è·Ÿè¸ªç³»ç»Ÿè¿è¡Œæƒ…å†µï¼ˆç›‘æ§Redisã€Tomcatã€JVMã€æœåŠ¡å™¨ä¿¡æ¯ã€è¯·æ±‚è¿½è¸ªã€SQLç›‘æ§ï¼‰ã€‚\n- 39.æ¶ˆæ¯ä¸­å¿ƒï¼ˆæ”¯æŒçŸ­ä¿¡ã€é‚®ä»¶ã€å¾®ä¿¡æ¨é€ç­‰ï¼‰ï¼›é›†æˆWebSocketæ¶ˆæ¯é€šçŸ¥æœºåˆ¶ã€‚\n- 40.æ”¯æŒå¤šè¯­è¨€ï¼Œæä¾›å›½é™…åŒ–æ–¹æ¡ˆã€‚\n- 41.æ•°æ®å˜æ›´è®°å½•æ—¥å¿—ï¼Œå¯è®°å½•æ•°æ®æ¯æ¬¡å˜æ›´å†…å®¹ï¼Œé€šè¿‡ç‰ˆæœ¬å¯¹æ¯”åŠŸèƒ½æŸ¥çœ‹å†å²å˜åŒ–ã€‚\n- 42.æä¾›ç®€å•æ˜“ç”¨çš„æ‰“å°æ’ä»¶ï¼Œæ”¯æŒè°·æ­Œã€ç«ç‹ã€IE11+ç­‰å„ç§æµè§ˆå™¨ã€‚\n- 43.åç«¯é‡‡ç”¨Mavenåˆ†æ¨¡å—å¼€å‘æ–¹å¼ï¼›å‰ç«¯æ”¯æŒèœå•åŠ¨æ€è·¯ç”±ã€‚\n- 44.æä¾›ä¸°å¯Œçš„ç¤ºä¾‹ä»£ç ï¼Œæ¶µç›–äº†å¸¸ç”¨çš„ä¸šåŠ¡åœºæ™¯ï¼Œä¾¿äºå­¦ä¹ å’Œå‚è€ƒã€‚\n\n\n\næŠ€æœ¯æ¶æ„ï¼š\n-----------------------------------\n\n#### å‰ç«¯\n\n- å‰ç«¯ç¯å¢ƒè¦æ±‚ï¼šNode.jsè¦æ±‚`Node 20+` ç‰ˆæœ¬ä»¥ä¸Šã€pnpm è¦æ±‚`9+` ç‰ˆæœ¬ä»¥ä¸Š\n\n ` ( Vite ä¸å†æ”¯æŒå·²ç»“æŸç”Ÿå‘½å‘¨æœŸï¼ˆEOLï¼‰çš„ Node.js 18ã€‚ç°åœ¨éœ€è¦ä½¿ç”¨ Node.js 20.19+ æˆ– 22.12+)`\n\n- ä¾èµ–ç®¡ç†ï¼šnodeã€npmã€pnpm\n- å‰ç«¯IDEå»ºè®®ï¼šIDEAã€WebStormã€Vscode\n- é‡‡ç”¨ Vue3.0+TypeScript+Vite6+Ant-Design-Vue4ç­‰æ–°æŠ€æœ¯æ–¹æ¡ˆï¼ŒåŒ…æ‹¬äºŒæ¬¡å°è£…ç»„ä»¶ã€utilsã€hooksã€åŠ¨æ€èœå•ã€æƒé™æ ¡éªŒã€æŒ‰é’®çº§åˆ«æƒé™æ§åˆ¶ç­‰åŠŸèƒ½\n- æœ€æ–°æŠ€æœ¯æ ˆï¼šVue3.0 + TypeScript + Vite6 + ant-design-vue4 + pinia + echarts + unocss + vxe-table + qiankun + es6\n\n\n#### åç«¯\n\n- IDEå»ºè®®ï¼š IDEA (å¿…é¡»å®‰è£…lombokæ’ä»¶ )\n- è¯­è¨€ï¼šJava é»˜è®¤jdk17(jdk21ã€jdk24)\n- ä¾èµ–ç®¡ç†ï¼šMaven\n- åŸºç¡€æ¡†æ¶ï¼šSpring Boot 3.5.5\n- å¾®æœåŠ¡æ¡†æ¶ï¼š Spring Cloud Alibaba 2023.0.3.3\n- æŒä¹…å±‚æ¡†æ¶ï¼šMybatisPlus 3.5.12\n- æŠ¥è¡¨å·¥å…·ï¼š JimuReport 2.1.3\n- å®‰å…¨æ¡†æ¶ï¼šApache Shiro 2.0.4ï¼ŒJwt 4.5.0\n- å¾®æœåŠ¡æŠ€æœ¯æ ˆï¼šSpring Cloud Alibabaã€Nacosã€Gatewayã€Sentinelã€Skywalking\n- æ•°æ®åº“è¿æ¥æ± ï¼šé˜¿é‡Œå·´å·´Druid 1.2.24\n- AIå¤§æ¨¡å‹ï¼šæ”¯æŒ `ChatGPT` `DeepSeek` `åƒé—®`ç­‰å„ç§å¸¸è§„æ¨¡å¼\n- æ—¥å¿—æ‰“å°ï¼šlogback\n- ç¼“å­˜ï¼šRedis\n- å…¶ä»–ï¼šautopoi, fastjsonï¼Œpoiï¼ŒSwagger-uiï¼Œquartz, lombokï¼ˆç®€åŒ–ä»£ç ï¼‰ç­‰ã€‚\n- é»˜è®¤æä¾›MySQL5.7+æ•°æ®åº“è„šæœ¬\n\n#### æ•°æ®åº“æ”¯æŒ\n\n> jeecgbootå¹³å°æ”¯æŒä»¥ä¸‹æ•°æ®åº“ï¼Œé»˜è®¤æˆ‘ä»¬åªæä¾›mysqlè„šæœ¬ï¼Œå…¶ä»–æ•°æ®åº“å¯ä»¥å‚è€ƒ[è½¬åº“æ–‡æ¡£](https://my.oschina.net/jeecg/blog/4905722)è‡ªå·±è½¬ã€‚\n\n|  æ•°æ®åº“   |  æ”¯æŒ   |\n| --- | --- |\n|   MySQL   |  âˆš   |\n|  Oracle11g   |  âˆš   |\n|  Sqlserver2017   |  âˆš   |\n|   PostgreSQL   |  âˆš   |\n|   MariaDB   |  âˆš   |\n|   è¾¾æ¢¦   |  âˆš   |\n|   äººå¤§é‡‘ä»“   |  âˆš   |\n|   TiDB     |  âˆš   |\n|   kingbase8   |  âˆš   |\n\n\n\n \n## å¾®æœåŠ¡è§£å†³æ–¹æ¡ˆ\n\n- 1ã€æœåŠ¡æ³¨å†Œå’Œå‘ç° Nacos âˆš\n- 2ã€ç»Ÿä¸€é…ç½®ä¸­å¿ƒ Nacos  âˆš\n- 3ã€è·¯ç”±ç½‘å…³ gateway(ä¸‰ç§åŠ è½½æ–¹å¼) âˆš\n- 4ã€åˆ†å¸ƒå¼ http feign âˆš\n- 5ã€ç†”æ–­é™çº§é™æµ Sentinel âˆš\n- 6ã€åˆ†å¸ƒå¼æ–‡ä»¶ Minioã€é˜¿é‡ŒOSS âˆš \n- 7ã€ç»Ÿä¸€æƒé™æ§åˆ¶ JWT + Shiro âˆš\n- 8ã€æœåŠ¡ç›‘æ§ SpringBootAdminâˆš\n- 9ã€é“¾è·¯è·Ÿè¸ª Skywalking   [å‚è€ƒæ–‡æ¡£](https://help.jeecg.com/java/springcloud/super/skywarking)\n- 10ã€æ¶ˆæ¯ä¸­é—´ä»¶ RabbitMQ  âˆš\n- 11ã€åˆ†å¸ƒå¼ä»»åŠ¡ xxl-job  âˆš \n- 12ã€åˆ†å¸ƒå¼äº‹åŠ¡ Seata\n- 13ã€è½»é‡åˆ†å¸ƒå¼æ—¥å¿— Loki+grafanaå¥—ä»¶\n- 14ã€æ”¯æŒ docker-composeã€k8sã€jenkins\n- 15ã€CAS å•ç‚¹ç™»å½•   âˆš\n- 16ã€è·¯ç”±é™æµ   âˆš\n\n#### å¾®æœåŠ¡æ¶æ„å›¾\n![å¾®æœåŠ¡æ¶æ„å›¾](https://jeecgos.oss-cn-beijing.aliyuncs.com/files/jeecgboot_springcloud2022.png "åœ¨è¿™é‡Œè¾“å…¥å›¾ç‰‡æ ‡é¢˜")\n\n\n\n### Jeecg Boot äº§å“åŠŸèƒ½è“å›¾\n![åŠŸèƒ½è“å›¾](https://jeecgos.oss-cn-beijing.aliyuncs.com/upload/test/Jeecg-Boot-lantu202005_1590912449914.jpg "åœ¨è¿™é‡Œè¾“å…¥å›¾ç‰‡æ ‡é¢˜")\n\n\n\n####  ç³»ç»ŸåŠŸèƒ½æ¶æ„å›¾\n\n![](https://oscimg.oschina.net/oscnet/up-1569487b95a07dbc3599fb1349a2e3aaae1.png)\n\n\n\n### å¼€æºç‰ˆåŠŸèƒ½æ¸…å•\n```\nâ”œâ”€ç³»ç»Ÿç®¡ç†\nâ”‚  â”œâ”€ç”¨æˆ·ç®¡ç†\nâ”‚  â”œâ”€è§’è‰²ç®¡ç†\nâ”‚  â”œâ”€èœå•ç®¡ç†\nâ”‚  â”œâ”€é¦–é¡µé…ç½®\nâ”‚  â”œâ”€æƒé™è®¾ç½®ï¼ˆæ”¯æŒæŒ‰é’®æƒé™ã€æ•°æ®æƒé™ï¼‰\nâ”‚  â”œâ”€è¡¨å•æƒé™ï¼ˆæ§åˆ¶å­—æ®µç¦ç”¨ã€éšè—ï¼‰\nâ”‚  â”œâ”€éƒ¨é—¨ç®¡ç†\nâ”‚  â”œâ”€æˆ‘çš„éƒ¨é—¨ï¼ˆäºŒçº§ç®¡ç†å‘˜ï¼‰\nâ”‚  â””â”€å­—å…¸ç®¡ç†\nâ”‚  â””â”€åˆ†ç±»å­—å…¸\nâ”‚  â””â”€ç³»ç»Ÿå…¬å‘Š\nâ”‚  â””â”€èŒåŠ¡ç®¡ç†\nâ”‚  â””â”€é€šè®¯å½•\nâ”‚  â”œâ”€å¤šæ•°æ®æºç®¡ç†\nâ”‚  â”œâ”€ç™½åå•ç®¡ç†\nâ”‚  â”œâ”€ç¬¬ä¸‰æ–¹é…ç½®ï¼ˆå¯¹æ¥é’‰é’‰å’Œä¼ä¸šå¾®ä¿¡ï¼‰\nâ”‚  â””â”€å¤šç§Ÿæˆ·ç®¡ç†ï¼ˆç§Ÿæˆ·ç®¡ç†ã€ç§Ÿæˆ·è§’è‰²ã€æˆ‘çš„ç§Ÿæˆ·ã€ç§Ÿæˆ·é»˜è®¤å¥—é¤ç®¡ç†ï¼‰\nâ”œâ”€Onlineåœ¨çº¿å¼€å‘(ä½ä»£ç )\nâ”‚  â”œâ”€Onlineåœ¨çº¿è¡¨å•\nâ”‚  â”œâ”€Onlineä»£ç ç”Ÿæˆå™¨\nâ”‚  â”œâ”€Onlineåœ¨çº¿æŠ¥è¡¨\nâ”‚  â”œâ”€ä»ªè¡¨ç›˜è®¾è®¡å™¨\nâ”‚  â”œâ”€ç³»ç»Ÿç¼–ç è§„åˆ™\nâ”‚  â”œâ”€ç³»ç»Ÿæ ¡éªŒè§„åˆ™\nâ”‚  â”œâ”€APPç‰ˆæœ¬ç®¡ç†\nâ”œâ”€AIåº”ç”¨å¹³å°\nâ”‚  â”œâ”€AIçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ\nâ”‚  â”œâ”€AIå¤§æ¨¡å‹ç®¡ç†\nâ”‚  â”œâ”€AIæµç¨‹ç¼–æ’\nâ”‚  â”œâ”€AIæµç¨‹è®¾è®¡å™¨\nâ”‚  â”œâ”€AIå¯¹è¯æ”¯æŒå›¾ç‰‡\nâ”‚  â”œâ”€AIå¯¹è¯åŠ©æ‰‹(æ™ºèƒ½é—®ç­”)\nâ”‚  â”œâ”€AIå»ºè¡¨ï¼ˆOnlineè¡¨å•ï¼‰\nâ”‚  â”œâ”€AIèŠå¤©çª—å£æ”¯æŒåµŒå…¥ç¬¬ä¸‰æ–¹\nâ”‚  â”œâ”€AIèŠå¤©çª—å£æ”¯æŒç§»åŠ¨ç«¯\nâ”‚  â”œâ”€æ”¯æŒå¸¸è§å¤§æ¨¡å‹ChatGPTå’ŒDeepSeekã€ollamaç­‰ç­‰\nâ”‚  â”œâ”€AI OCRç¤ºä¾‹\nâ”œâ”€æ•°æ®å¯è§†åŒ–\nâ”‚  â”œâ”€æŠ¥è¡¨è®¾è®¡å™¨(æ”¯æŒæ‰“å°è®¾è®¡ï¼‰\nâ”‚  â”œâ”€å¤§å±è®¾å’Œä»ªè¡¨ç›˜è®¾è®¡\nâ”œâ”€OpenAPIï¼ˆåŸºäºAKå’ŒSKè®¤è¯é‰´æƒï¼‰\nâ”‚  â”œâ”€æ¥å£ç®¡ç†\nâ”‚  â”œâ”€æ¥å£æˆæƒ\nâ”‚  â”œâ”€æ¥å£æ–‡æ¡£\nâ”œâ”€æ¶ˆæ¯ä¸­å¿ƒ\nâ”‚  â”œâ”€æ¶ˆæ¯ç®¡ç†\nâ”‚  â”œâ”€æ¨¡æ¿ç®¡ç†\nâ”œâ”€ä»£ç ç”Ÿæˆå™¨(ä½ä»£ç )\nâ”‚  â”œâ”€ä»£ç ç”Ÿæˆå™¨åŠŸèƒ½ï¼ˆä¸€é”®ç”Ÿæˆå‰åç«¯ä»£ç ï¼Œç”Ÿæˆåæ— éœ€ä¿®æ”¹ç›´æ¥ç”¨ï¼Œç»å¯¹æ˜¯åç«¯å¼€å‘ç¦éŸ³ï¼‰\nâ”‚  â”œâ”€ä»£ç ç”Ÿæˆå™¨æ¨¡æ¿ï¼ˆæä¾›4å¥—æ¨¡æ¿ï¼Œåˆ†åˆ«æ”¯æŒå•è¡¨å’Œä¸€å¯¹å¤šæ¨¡å‹ï¼Œä¸åŒé£æ ¼é€‰æ‹©ï¼‰\nâ”‚  â”œâ”€ä»£ç ç”Ÿæˆå™¨æ¨¡æ¿ï¼ˆç”Ÿæˆä»£ç ï¼Œè‡ªå¸¦excelå¯¼å…¥å¯¼å‡ºï¼‰\nâ”‚  â”œâ”€æŸ¥è¯¢è¿‡æ»¤å™¨ï¼ˆæŸ¥è¯¢é€»è¾‘æ— éœ€ç¼–ç ï¼Œç³»ç»Ÿæ ¹æ®é¡µé¢é…ç½®è‡ªåŠ¨ç”Ÿæˆï¼‰\nâ”‚  â”œâ”€é«˜çº§æŸ¥è¯¢å™¨ï¼ˆå¼¹çª—è‡ªåŠ¨ç»„åˆæŸ¥è¯¢æ¡ä»¶ï¼‰\nâ”‚  â”œâ”€Excelå¯¼å…¥å¯¼å‡ºå·¥å…·é›†æˆï¼ˆæ”¯æŒå•è¡¨ï¼Œä¸€å¯¹å¤š å¯¼å…¥å¯¼å‡ºï¼‰\nâ”‚  â”œâ”€å¹³å°ç§»åŠ¨è‡ªé€‚åº”æ”¯æŒ\nâ”‚  â”œâ”€æä¾›æ–°ç‰ˆuniapp3çš„ä»£ç ç”Ÿæˆå™¨æ¨¡æ¿\nâ”œâ”€ç³»ç»Ÿç›‘æ§\nâ”‚  â”œâ”€Gatewayè·¯ç”±ç½‘å…³\nâ”‚  â”œâ”€åŸºäºAKå’ŒSKè®¤è¯é‰´æƒOpenAPIåŠŸèƒ½\nâ”‚  â”œâ”€å®šæ—¶ä»»åŠ¡\nâ”‚  â”œâ”€æ•°æ®æºç®¡ç†\nâ”‚  â”œâ”€æ€§èƒ½æ‰«æç›‘æ§\nâ”‚  â”‚  â”œâ”€ç›‘æ§ Redis\nâ”‚  â”‚  â”œâ”€Tomcat\nâ”‚  â”‚  â”œâ”€jvm\nâ”‚  â”‚  â”œâ”€æœåŠ¡å™¨ä¿¡æ¯\nâ”‚  â”‚  â”œâ”€è¯·æ±‚è¿½è¸ª\nâ”‚  â”‚  â”œâ”€ç£ç›˜ç›‘æ§\nâ”‚  â”œâ”€ç³»ç»Ÿæ—¥å¿—\nâ”‚  â”œâ”€æ¶ˆæ¯ä¸­å¿ƒï¼ˆæ”¯æŒçŸ­ä¿¡ã€é‚®ä»¶ã€å¾®ä¿¡æ¨é€ç­‰ç­‰ï¼‰\nâ”‚  â”œâ”€æ•°æ®æ—¥å¿—ï¼ˆè®°å½•æ•°æ®å¿«ç…§ï¼Œå¯å¯¹æ¯”å¿«ç…§ï¼ŒæŸ¥çœ‹æ•°æ®å˜æ›´æƒ…å†µï¼‰\nâ”‚  â”œâ”€SQLç›‘æ§\nâ”‚  â”œâ”€åœ¨çº¿ç”¨æˆ·\nâ”‚â”€æŠ¥è¡¨ç¤ºä¾‹\nâ”‚  â”œâ”€æ›²çº¿å›¾\nâ”‚  â””â”€é¥¼çŠ¶å›¾\nâ”‚  â””â”€æŸ±çŠ¶å›¾\nâ”‚  â””â”€æŠ˜çº¿å›¾\nâ”‚  â””â”€é¢ç§¯å›¾\nâ”‚  â””â”€é›·è¾¾å›¾\nâ”‚  â””â”€ä»ªè¡¨å›¾\nâ”‚  â””â”€è¿›åº¦æ¡\nâ”‚  â””â”€æ’ååˆ—è¡¨\nâ”‚  â””â”€ç­‰ç­‰\nâ”‚â”€å¤§å±æ¨¡æ¿\nâ”‚  â”œâ”€ä½œæˆ˜æŒ‡æŒ¥ä¸­å¿ƒå¤§å±\nâ”‚  â””â”€ç‰©æµæœåŠ¡ä¸­å¿ƒå¤§å±\nâ”‚â”€å¸¸ç”¨ç¤ºä¾‹\nâ”‚  â”œâ”€è‡ªå®šä¹‰ç»„ä»¶\nâ”‚  â”œâ”€å¯¹è±¡å­˜å‚¨(å¯¹æ¥é˜¿é‡Œäº‘)\nâ”‚  â”œâ”€JVXETableç¤ºä¾‹ï¼ˆå„ç§å¤æ‚ERPå¸ƒå±€ç¤ºä¾‹ï¼‰\nâ”‚  â”œâ”€å•è¡¨æ¨¡å‹ä¾‹å­\nâ”‚  â””â”€ä¸€å¯¹å¤šæ¨¡å‹ä¾‹å­\nâ”‚  â””â”€æ‰“å°ä¾‹å­\nâ”‚  â””â”€ä¸€å¯¹å¤šTABä¾‹å­\nâ”‚  â””â”€å†…åµŒtableä¾‹å­\nâ”‚  â””â”€å¸¸ç”¨é€‰æ‹©ç»„ä»¶\nâ”‚  â””â”€å¼‚æ­¥æ ‘table\nâ”‚  â””â”€æ¥å£æ¨¡æ‹Ÿæµ‹è¯•\nâ”‚  â””â”€è¡¨æ ¼åˆè®¡ç¤ºä¾‹\nâ”‚  â””â”€å¼‚æ­¥æ ‘åˆ—è¡¨ç¤ºä¾‹\nâ”‚  â””â”€ä¸€å¯¹å¤šJEditable\nâ”‚  â””â”€JEditableç»„ä»¶ç¤ºä¾‹\nâ”‚  â””â”€å›¾ç‰‡æ‹–æ‹½æ’åº\nâ”‚  â””â”€å›¾ç‰‡ç¿»é¡µ\nâ”‚  â””â”€å›¾ç‰‡é¢„è§ˆ\nâ”‚  â””â”€PDFé¢„è§ˆ\nâ”‚  â””â”€åˆ†å±åŠŸèƒ½\nâ”‚â”€å°è£…é€šç”¨ç»„ä»¶	\nâ”‚  â”œâ”€è¡Œç¼–è¾‘è¡¨æ ¼JEditableTable\nâ”‚  â””â”€çœç•¥æ˜¾ç¤ºç»„ä»¶\nâ”‚  â””â”€æ—¶é—´æ§ä»¶\nâ”‚  â””â”€é«˜çº§æŸ¥è¯¢\nâ”‚  â””â”€ç”¨æˆ·é€‰æ‹©ç»„ä»¶\nâ”‚  â””â”€æŠ¥è¡¨ç»„ä»¶å°è£…\nâ”‚  â””â”€å­—å…¸ç»„ä»¶\nâ”‚  â””â”€ä¸‹æ‹‰å¤šé€‰ç»„ä»¶\nâ”‚  â””â”€é€‰äººç»„ä»¶\nâ”‚  â””â”€é€‰éƒ¨é—¨ç»„ä»¶\nâ”‚  â””â”€é€šè¿‡éƒ¨é—¨é€‰äººç»„ä»¶\nâ”‚  â””â”€å°è£…æ›²çº¿ã€æŸ±çŠ¶å›¾ã€é¥¼çŠ¶å›¾ã€æŠ˜çº¿å›¾ç­‰ç­‰æŠ¥è¡¨çš„ç»„ä»¶ï¼ˆç»è¿‡å°è£…ï¼Œä½¿ç”¨ç®€å•ï¼‰\nâ”‚  â””â”€åœ¨çº¿codeç¼–è¾‘å™¨\nâ”‚  â””â”€ä¸Šä¼ æ–‡ä»¶ç»„ä»¶\nâ”‚  â””â”€éªŒè¯ç ç»„ä»¶\nâ”‚  â””â”€æ ‘åˆ—è¡¨ç»„ä»¶\nâ”‚  â””â”€è¡¨å•ç¦ç”¨ç»„ä»¶\nâ”‚  â””â”€ç­‰ç­‰\nâ”‚â”€æ›´å¤šé¡µé¢æ¨¡æ¿\nâ”‚  â”œâ”€å„ç§é«˜çº§è¡¨å•\nâ”‚  â”œâ”€å„ç§åˆ—è¡¨æ•ˆæœ\nâ”‚  â””â”€ç»“æœé¡µé¢\nâ”‚  â””â”€å¼‚å¸¸é¡µé¢\nâ”‚  â””â”€ä¸ªäººé¡µé¢\nâ”œâ”€é«˜çº§åŠŸèƒ½\nâ”‚  â”œâ”€æä¾›å•ç‚¹ç™»å½•CASé›†æˆæ–¹æ¡ˆ\nâ”‚  â”œâ”€æä¾›APPå‘å¸ƒæ–¹æ¡ˆ\nâ”‚  â”œâ”€é›†æˆWebsocketæ¶ˆæ¯é€šçŸ¥æœºåˆ¶\nâ”‚  â”œâ”€æ”¯æŒelectronæ¡Œé¢åº”ç”¨æ‰“åŒ…(æ”¯æŒwindowsã€linuxã€macOSä¸‰å¤§å¹³å°)\nâ”‚  â”œâ”€dockerå®¹å™¨æ”¯æŒ\nâ”‚  â”œâ”€æä¾›ç§»åŠ¨APPæ¡†æ¶åŠæºç ï¼ˆUniapp3ç‰ˆæœ¬ï¼‰æ”¯æŒH5ã€å°ç¨‹åºã€APPã€é¸¿è’™Next\nâ”‚  â”œâ”€æä¾›ç§»åŠ¨APPä½ä»£ç è®¾è®¡(Onlineè¡¨å•ã€ä»ªè¡¨ç›˜)\n```\n\n\n\n### ç³»ç»Ÿæ•ˆæœ\n\n##### PCç«¯\n![](https://oscimg.oschina.net/oscnet/up-000530d95df337b43089ac77e562494f454.png)\n\n![è¾“å…¥å›¾ç‰‡è¯´æ˜](https://static.oschina.net/uploads/img/201904/14155402_AmlV.png "åœ¨è¿™é‡Œè¾“å…¥å›¾ç‰‡æ ‡é¢˜")\n\n![](https://oscimg.oschina.net/oscnet/up-9d6f36f251e71a0b515a01323474b03004c.png)\n\n![è¾“å…¥å›¾ç‰‡è¯´æ˜](https://static.oschina.net/uploads/img/201904/14160813_KmXS.png "åœ¨è¿™é‡Œè¾“å…¥å›¾ç‰‡æ ‡é¢˜")\n\n![è¾“å…¥å›¾ç‰‡è¯´æ˜](https://static.oschina.net/uploads/img/201904/14160935_Nibs.png "åœ¨è¿™é‡Œè¾“å…¥å›¾ç‰‡æ ‡é¢˜")\n\n![è¾“å…¥å›¾ç‰‡è¯´æ˜](https://static.oschina.net/uploads/img/201904/14161004_bxQ4.png "åœ¨è¿™é‡Œè¾“å…¥å›¾ç‰‡æ ‡é¢˜")\n\n#####  ç³»ç»Ÿäº¤äº’\n![](https://oscimg.oschina.net/oscnet/up-78b151fc888d4319377bf1cc311fe826871.png)\n\n![](https://oscimg.oschina.net/oscnet/up-16c07e000278329b69b228ae3189814b8e9.png)\n\n\n##### AIåŠŸèƒ½\n\nAIèŠå¤©åŠ©æ‰‹\n\n![](https://oscimg.oschina.net/oscnet//65298d5710b4e6039a5f802b5f8505c5.png)\n\nAIå»ºè¡¨\n\n![](https://oscimg.oschina.net/oscnet/up-381423599f219a67def45dfd9a99df8ef3f.png)\n\n![](https://oscimg.oschina.net/oscnet/up-1508c2b0708c365605f68893044ee11f20d.png)\n\nAIå†™æ–‡ç« \n\n![](https://oscimg.oschina.net/oscnet/up-e3ee5b1fe497308805aa5e324b72994af79.png)\n\n\n#####  ä»ªè¡¨ç›˜è®¾è®¡å™¨\n\n![](https://jeecgos.oss-cn-beijing.aliyuncs.com/files/darg20240726105556.png)\n\n![](https://jeecgos.oss-cn-beijing.aliyuncs.com/files/drag20240724135626.png)\n\n![](https://jeecgos.oss-cn-beijing.aliyuncs.com/files/drag20240724135619.png)\n\n![](https://jeecgos.oss-cn-beijing.aliyuncs.com/files/drag20240724135630.png)\n\n![](https://jeecgos.oss-cn-beijing.aliyuncs.com/files/drag20240726105547.png)\n\n![](https://oscimg.oschina.net/oscnet/up-fad98d42b2cf92f92a903c9cff7579f18ec.png)\n\n\n##### æŠ¥è¡¨è®¾è®¡å™¨\n![](https://oscimg.oschina.net/oscnet/up-64648de000851f15f6c7b9573d107ebb5f8.png)\n\n![](https://oscimg.oschina.net/oscnet/up-fa52b44445db281c51d3f267dce7450d21b.gif)\n\n![](https://oscimg.oschina.net/oscnet/up-68a19149d640f1646c8ed89ed4375e3326c.png)\n\n![](https://oscimg.oschina.net/oscnet/up-f7e9cb2e3740f2d19ff63b40ec2dd554f96.png)\n\n\n##### æ‰‹æœºç«¯\n![](https://oscimg.oschina.net/oscnet/da543c5d0d57baab0cecaa4670c8b68c521.jpg)\n![](https://oscimg.oschina.net/oscnet/fda4bd82cab9d682de1c1fbf2060bf14fa6.jpg)\n\n##### PADç«¯\n![](https://oscimg.oschina.net/oscnet/e90fef970a8c33790ab03ffd6c4c7cec225.jpg)\n![](https://oscimg.oschina.net/oscnet/d78218803a9e856a0aa82b45efc49849a0c.jpg)\n![](https://oscimg.oschina.net/oscnet/59c23b230f52384e588ee16309b44fa20de.jpg)\n\n\n##### å›¾è¡¨ç¤ºä¾‹\n![](https://oscimg.oschina.net/oscnet/up-218bc6a1669496b241ebb23506440c0083e.png)\n\n![è¾“å…¥å›¾ç‰‡è¯´æ˜](https://static.oschina.net/uploads/img/201904/14160834_Lo23.png "åœ¨è¿™é‡Œè¾“å…¥å›¾ç‰‡æ ‡é¢˜")\n![è¾“å…¥å›¾ç‰‡è¯´æ˜](https://static.oschina.net/uploads/img/201904/14160842_QK7B.png "åœ¨è¿™é‡Œè¾“å…¥å›¾ç‰‡æ ‡é¢˜")\n![è¾“å…¥å›¾ç‰‡è¯´æ˜](https://static.oschina.net/uploads/img/201904/14160849_GBm5.png "åœ¨è¿™é‡Œè¾“å…¥å›¾ç‰‡æ ‡é¢˜")\n![è¾“å…¥å›¾ç‰‡è¯´æ˜](https://static.oschina.net/uploads/img/201904/14160858_6RAM.png "åœ¨è¿™é‡Œè¾“å…¥å›¾ç‰‡æ ‡é¢˜")\n\n##### åœ¨çº¿æ¥å£æ–‡æ¡£\n![è¾“å…¥å›¾ç‰‡è¯´æ˜](https://static.oschina.net/uploads/img/201908/27095258_M2Xq.png "åœ¨è¿™é‡Œè¾“å…¥å›¾ç‰‡æ ‡é¢˜")\n![è¾“å…¥å›¾ç‰‡è¯´æ˜](https://static.oschina.net/uploads/img/201904/14160957_hN3X.png "åœ¨è¿™é‡Œè¾“å…¥å›¾ç‰‡æ ‡é¢˜")\n\n\n##### UNIAPPæ•ˆæœ\n\n![](https://oscimg.oschina.net/oscnet/up-aac943fbd26561879c57a41f7a406edf274.png)\n\n![](https://oscimg.oschina.net/oscnet/up-9a44ba2e82b09c750629d12fafd7f60f553.png)\n\n\n##### å¤§å±è®¾è®¡å™¨\n![](https://oscimg.oschina.net/oscnet/up-402a6034124474bfef8dfc5b4b2bac1ce5c.png)\n\n![](https://oscimg.oschina.net/oscnet/up-6f7ba2e2ebbeea0d203db8d69fd87644c9f.png)\n\n![](https://oscimg.oschina.net/oscnet/up-ee8d34f318da466b8a6070a6e3111d12ce7.png)\n\n![](https://oscimg.oschina.net/oscnet/up-6b81781b43086819049c4421206810667c5.png)\n\n\n\n\n\n\n\n\n## æèµ  \n\nå¦‚æœè§‰å¾—è¿˜ä¸é”™ï¼Œè¯·ä½œè€…å–æ¯å’–å•¡å§ â˜º\n\n![](https://static.oschina.net/uploads/img/201903/08155608_0EFX.png)', '{"language":"Java","stars":44607,"forks":15698,"watchers":44607,"open_issues":50,"topics":["activiti","agent","ai","aiflow","ant-design-vue","antd","codegenerator","deepseek","flowable","langchain4j","llm","low-code","mcp","mybatis-plus","rag","spring-ai","springboot","springboot3","springcloud","vue3"],"default_branch":"main","size_kb":89636,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:jeecgboot:JeecgBoot","source_url":"https://github.com/jeecgboot/JeecgBoot"},{"type":"has_code","target_id":"github:jeecgboot:JeecgBoot","source_url":"https://github.com/jeecgboot/JeecgBoot"},{"type":"has_code","target_id":"github:jeecgboot:JeecgBoot","source_url":"https://github.com/jeecgboot/JeecgBoot"},{"type":"has_code","target_id":"github:jeecgboot:JeecgBoot","source_url":"https://github.com/jeecgboot/JeecgBoot"},{"type":"has_code","target_id":"github:jeecgboot:JeecgBoot","source_url":"https://github.com/jeecgboot/JeecgBoot"},{"type":"has_code","target_id":"github:jeecgboot:JeecgBoot","source_url":"https://github.com/jeecgboot/JeecgBoot"},{"type":"has_code","target_id":"github:jeecgboot:JeecgBoot","source_url":"https://github.com/jeecgboot/JeecgBoot"},{"type":"has_code","target_id":"github:jeecgboot:JeecgBoot","source_url":"https://github.com/jeecgboot/JeecgBoot"},{"type":"has_code","target_id":"github:jeecgboot:JeecgBoot","source_url":"https://github.com/jeecgboot/JeecgBoot"},{"type":"has_code","target_id":"github:jeecgboot:JeecgBoot","source_url":"https://github.com/jeecgboot/JeecgBoot"},{"type":"has_code","target_id":"github:jeecgboot:JeecgUniapp","source_url":"https://github.com/jeecgboot/JeecgUniapp"},{"type":"has_code","target_id":"github:jeecgboot:jeecg-boot-starter","source_url":"https://github.com/jeecgboot/jeecg-boot-starter"},{"type":"has_code","target_id":"github:jeecgboot:JeecgBoot","source_url":"https://github.com/jeecgboot/JeecgBoot"}]', NULL, 'Apache-2.0', 'approved', 80, 'e3ccdaa1e5cfa2171908578c7841d7a3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-jeecgboot-JeecgBoot from https://github.com/jeecgboot.png
Image converted to WebP: data/images/github-jeecgboot-JeecgBoot.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ClickHouse-ClickHouse', 'github--clickhouse--clickhouse', 'ClickHouse', 'ClickHouse', '<div align=center> <picture align=center> <source media="(prefers-color-scheme: dark)" srcset="https://github.com/ClickHouse/clickhouse-docs/assets/9611008/4ef9c104-2d3f-4646-b186-507358d2fe28"> <source media="(prefers-color-scheme: light)" srcset="https://github.com/ClickHouse/clickhouse-docs/assets/9611008/b001dc7b-5a45-4dcd-9275-e03beb7f9177"> <img alt="The ClickHouse company logo." src="https://github.com/ClickHouse/clickhouse-docs/assets/9611008/b001dc7b-5a45-4dcd-9275-e03beb7f9177"> </p...', '["ai","analytics","big-data","clickhouse","cloud-native","cpp","database","dbms","distributed","embedded","hacktoberfest","lakehouse","mpp","olap","rust","self-hosted","sql","c++"]', 'other', 44518, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ClickHouse/ClickHouse","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align=center>\n\n[![Website](https://img.shields.io/website?up_message=AVAILABLE&down_message=DOWN&url=https%3A%2F%2Fclickhouse.com&style=for-the-badge)](https://clickhouse.com)\n[![Apache 2.0 License](https://img.shields.io/badge/license-Apache%202.0-blueviolet?style=for-the-badge)](https://www.apache.org/licenses/LICENSE-2.0)\n\n<picture align=center>\n    <source media="(prefers-color-scheme: dark)" srcset="https://github.com/ClickHouse/clickhouse-docs/assets/9611008/4ef9c104-2d3f-4646-b186-507358d2fe28">\n    <source media="(prefers-color-scheme: light)" srcset="https://github.com/ClickHouse/clickhouse-docs/assets/9611008/b001dc7b-5a45-4dcd-9275-e03beb7f9177">\n    <img alt="The ClickHouse company logo." src="https://github.com/ClickHouse/clickhouse-docs/assets/9611008/b001dc7b-5a45-4dcd-9275-e03beb7f9177">\n</picture>\n\n<h4>ClickHouseÂ® is an open-source column-oriented database management system that allows generating analytical data reports in real-time.</h4>\n\n</div>\n\n## How To Install (Linux, macOS, FreeBSD)\n\n```\ncurl https://clickhouse.com/ | sh\n```\n\n## Useful Links\n\n* [Official website](https://clickhouse.com/) has a quick high-level overview of ClickHouse on the main page.\n* [ClickHouse Cloud](https://clickhouse.cloud) ClickHouse as a service, built by the creators and maintainers.\n* [Tutorial](https://clickhouse.com/docs/getting_started/tutorial/) shows how to set up and query a small ClickHouse cluster.\n* [Documentation](https://clickhouse.com/docs/) provides more in-depth information.\n* [YouTube channel](https://www.youtube.com/c/ClickHouseDB) has a lot of content about ClickHouse in video format.\n* [Slack](https://clickhouse.com/slack) and [Telegram](https://telegram.me/clickhouse_en) allow chatting with ClickHouse users in real-time.\n* [Blog](https://clickhouse.com/blog/) contains various ClickHouse-related articles, as well as announcements and reports about events.\n* [Bluesky](https://bsky.app/profile/clickhouse.com) and [X](https://x.com/ClickHouseDB) for short news.\n* [Code Browser (github.dev)](https://github.dev/ClickHouse/ClickHouse) with syntax highlighting, powered by github.dev.\n* [Contacts](https://clickhouse.com/company/contact) can help to get your questions answered if there are any.\n\n## Monthly Release & Community Call\n\nJoin us for the [ClickHouse **25.11** Community Call](https://clickhouse.com/company/events/v25-11-community-release-call) happening on November 25.\n\nClickHouse **Release 25.10** is published! This release included the new QBit data type, as well as negative LIMIT and OFFSET. [Presentation](https://presentations.clickhouse.com/2025-release-25.10/), [Video](https://www.youtube.com/watch?v=cV2hiOCzDG4), [Changelog](https://github.com/ClickHouse/ClickHouse/blob/master/CHANGELOG.md#2510).\n\n## Upcoming Events\n\nKeep an eye out for upcoming meetups and events around the world.\nSomewhere else you want us to be?\nPlease feel free to reach out to tyler `<at>` clickhouse `<dot>` com.\nYou can also peruse [ClickHouse Events](https://clickhouse.com/company/news-events) for a list of all upcoming trainings, meetups, speaking engagements, etc.\n\nUpcoming meetups\n* [Mumbai Meetup](https://www.meetup.com/clickhouse-mumbai-user-group/events/311852373/) - November 22, 2025\n* [Bangkok Meetup](https://www.meetup.com/clickhouse-thailand-meetup-group/events/311852739/) - November 25, 2025\n* [Warsaw Meetup](https://www.meetup.com/clickhouse-poland-user-group/events/311309076) - November 26, 2025\n* [San Francisco Meetup](https://www.meetup.com/clickhouse-silicon-valley-meetup-group/events/312075592) - December 8, 2025\n* [New York Meetup](https://www.meetup.com/clickhouse-new-york-user-group/events/312080179/) - December 8, 2025\n* [Jakarta Meetup](https://www.meetup.com/clickhouse-indonesia-user-group/events/311988089/) - December 9, 2025\n* [Tokyo Meetup](https://www.meetup.com/clickhouse-tokyo-user-group/events/311974739/) - December 15, 2025\n* [Tel Aviv Meetup](https://www.meetup.com/clickhouse-meetup-israel/events/311868191) - December 29, 2025\n\nRecent meetups\n* [Seoul Meetup](https://www.meetup.com/clickhouse-seoul-user-group/events/311633023/) - November 18, 2025\n* [Bogota Meetup](https://www.meetup.com/clickhouse-latinoamerica/events/311069048) - November 13, 2025\n* [Stockholm Meetup](https://www.meetup.com/clickhouse-stockholm-user-group/events/311630931/) - Nov 3, 2025\n* [Pune Meetup](https://www.meetup.com/clickhouse-pune-user-group/events/310644227/) - September 20, 2025\n* [Denver Meetup](https://www.meetup.com/clickhouse-denver-user-group/events/310965415/) - September 20, 2025\n* [Singapore Meetup](https://www.meetup.com/clickhouse-singapore-meetup-group/events/310761753/) - September 25, 2025\n* [Melbourne Meetup](https://www.meetup.com/clickhouse-melbourne-user-group/events/310761395/) - September 30, 2025\n* [Tokyo Meetup](https://www.meetup.com/clickhouse-tokyo-user-group/events/310875919/) - September 30, 2025\n* [Madrid Meetup](https://www.meetup.com/clickhouse-spain-user-group/events/310124151) - September 30, 2025\n\n\n\n## Recent Recordings\n\n* **Recent Meetup Videos**: [Meetup Playlist](https://www.youtube.com/playlist?list=PL0Z2YDlm0b3iNDUzpY1S3L_iV4nARda_U) Whenever possible recordings of the ClickHouse Community Meetups are edited and presented as individual talks. \n* **Recording available**: [**v25.3 LTS Release Call**](https://www.youtube.com/watch?v=iCKEzp0_Z2Q) All the features of 25.3 LTS, one convenient video! Watch it now!\n\n ## Interested in joining ClickHouse and making it your full-time job?\n\nWe are a globally diverse and distributed team, united behind a common goal of creating industry-leading, real-time analytics.\nHere, you will have an opportunity to solve some of the technical challenges and have direct ownership of your work and vision.\nIf you are a contributor by nature, a thinker and a doer - weâ€™ll definitely click!\n\nCheck out our **current openings** here: https://clickhouse.com/company/careers\n\nCan''t find what you are looking for, but want to let us know you are interested in joining ClickHouse?\nEmail careers@clickhouse.com!\n', '{"language":"C++","stars":44518,"forks":7883,"watchers":44518,"open_issues":5517,"topics":["ai","analytics","big-data","clickhouse","cloud-native","cpp","database","dbms","distributed","embedded","hacktoberfest","lakehouse","mpp","olap","rust","self-hosted","sql"],"default_branch":"master","size_kb":2932881,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:ClickHouse:clickhouse-docs","source_url":"https://github.com/ClickHouse/clickhouse-docs"},{"type":"has_code","target_id":"github:ClickHouse:clickhouse-docs","source_url":"https://github.com/ClickHouse/clickhouse-docs"},{"type":"has_code","target_id":"github:ClickHouse:clickhouse-docs","source_url":"https://github.com/ClickHouse/clickhouse-docs"},{"type":"has_code","target_id":"github:ClickHouse:ClickHouse","source_url":"https://github.com/ClickHouse/ClickHouse"}]', NULL, 'Apache-2.0', 'approved', 65, '28bcc6bf8d419923ccf2be070834e125', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ClickHouse-ClickHouse from https://github.com/ClickHouse.png
Image converted to WebP: data/images/github-ClickHouse-ClickHouse.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-mem0ai-mem0', 'github--mem0ai--mem0', 'mem0', 'mem0ai', '<p align="center"> <a href="https://github.com/mem0ai/mem0"> <img src="docs/images/banner-sm.png" width="800px" alt="Mem0 - The Memory Layer for Personalized AI"> </a> </p> <p align="center" style="display: flex; justify-content: center; gap: 20px; align-items: center;"> <a href="https://trendshift.io/repositories/11194" target="blank"> <img src="https://trendshift.io/api/badge/repositories/11194" alt="mem0ai%2Fmem0 | Trendshift" width="250" height="55"/> </a> </p> <p align="center"> <a href=...', '["agents","ai","ai-agents","application","chatbots","chatgpt","genai","hacktoberfest","llm","long-term-memory","memory","memory-management","python","rag","state-management","python"]', 'other', 44005, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/mem0ai/mem0","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <a href="https://github.com/mem0ai/mem0">\n    <img src="docs/images/banner-sm.png" width="800px" alt="Mem0 - The Memory Layer for Personalized AI">\n  </a>\n</p>\n<p align="center" style="display: flex; justify-content: center; gap: 20px; align-items: center;">\n  <a href="https://trendshift.io/repositories/11194" target="blank">\n    <img src="https://trendshift.io/api/badge/repositories/11194" alt="mem0ai%2Fmem0 | Trendshift" width="250" height="55"/>\n  </a>\n</p>\n\n<p align="center">\n  <a href="https://mem0.ai">Learn more</a>\n  Â·\n  <a href="https://mem0.dev/DiG">Join Discord</a>\n  Â·\n  <a href="https://mem0.dev/demo">Demo</a>\n  Â·\n  <a href="https://mem0.dev/openmemory">OpenMemory</a>\n</p>\n\n<p align="center">\n  <a href="https://mem0.dev/DiG">\n    <img src="https://img.shields.io/badge/Discord-%235865F2.svg?&logo=discord&logoColor=white" alt="Mem0 Discord">\n  </a>\n  <a href="https://pepy.tech/project/mem0ai">\n    <img src="https://img.shields.io/pypi/dm/mem0ai" alt="Mem0 PyPI - Downloads">\n  </a>\n  <a href="https://github.com/mem0ai/mem0">\n    <img src="https://img.shields.io/github/commit-activity/m/mem0ai/mem0?style=flat-square" alt="GitHub commit activity">\n  </a>\n  <a href="https://pypi.org/project/mem0ai" target="blank">\n    <img src="https://img.shields.io/pypi/v/mem0ai?color=%2334D058&label=pypi%20package" alt="Package version">\n  </a>\n  <a href="https://www.npmjs.com/package/mem0ai" target="blank">\n    <img src="https://img.shields.io/npm/v/mem0ai" alt="Npm package">\n  </a>\n  <a href="https://www.ycombinator.com/companies/mem0">\n    <img src="https://img.shields.io/badge/Y%20Combinator-S24-orange?style=flat-square" alt="Y Combinator S24">\n  </a>\n</p>\n\n<p align="center">\n  <a href="https://mem0.ai/research"><strong>ğŸ“„ Building Production-Ready AI Agents with Scalable Long-Term Memory â†’</strong></a>\n</p>\n<p align="center">\n  <strong>âš¡ +26% Accuracy vs. OpenAI Memory â€¢ ğŸš€ 91% Faster â€¢ ğŸ’° 90% Fewer Tokens</strong>\n</p>\n\n> **ğŸ‰ mem0ai v1.0.0 is now available!** This major release includes API modernization, improved vector store support, and enhanced GCP integration. [See migration guide â†’](MIGRATION_GUIDE_v1.0.md)\n\n##  ğŸ”¥ Research Highlights\n- **+26% Accuracy** over OpenAI Memory on the LOCOMO benchmark\n- **91% Faster Responses** than full-context, ensuring low-latency at scale\n- **90% Lower Token Usage** than full-context, cutting costs without compromise\n- [Read the full paper](https://mem0.ai/research)\n\n# Introduction\n\n[Mem0](https://mem0.ai) ("mem-zero") enhances AI assistants and agents with an intelligent memory layer, enabling personalized AI interactions. It remembers user preferences, adapts to individual needs, and continuously learns over timeâ€”ideal for customer support chatbots, AI assistants, and autonomous systems.\n\n### Key Features & Use Cases\n\n**Core Capabilities:**\n- **Multi-Level Memory**: Seamlessly retains User, Session, and Agent state with adaptive personalization\n- **Developer-Friendly**: Intuitive API, cross-platform SDKs, and a fully managed service option\n\n**Applications:**\n- **AI Assistants**: Consistent, context-rich conversations\n- **Customer Support**: Recall past tickets and user history for tailored help\n- **Healthcare**: Track patient preferences and history for personalized care\n- **Productivity & Gaming**: Adaptive workflows and environments based on user behavior\n\n## ğŸš€ Quickstart Guide <a name="quickstart"></a>\n\nChoose between our hosted platform or self-hosted package:\n\n### Hosted Platform\n\nGet up and running in minutes with automatic updates, analytics, and enterprise security.\n\n1. Sign up on [Mem0 Platform](https://app.mem0.ai)\n2. Embed the memory layer via SDK or API keys\n\n### Self-Hosted (Open Source)\n\nInstall the sdk via pip:\n\n```bash\npip install mem0ai\n```\n\nInstall sdk via npm:\n```bash\nnpm install mem0ai\n```\n\n### Basic Usage\n\nMem0 requires an LLM to function, with `gpt-4.1-nano-2025-04-14 from OpenAI as the default. However, it supports a variety of LLMs; for details, refer to our [Supported LLMs documentation](https://docs.mem0.ai/components/llms/overview).\n\nFirst step is to instantiate the memory:\n\n```python\nfrom openai import OpenAI\nfrom mem0 import Memory\n\nopenai_client = OpenAI()\nmemory = Memory()\n\ndef chat_with_memories(message: str, user_id: str = "default_user") -> str:\n    # Retrieve relevant memories\n    relevant_memories = memory.search(query=message, user_id=user_id, limit=3)\n    memories_str = "\n".join(f"- {entry[''memory'']}" for entry in relevant_memories["results"])\n\n    # Generate Assistant response\n    system_prompt = f"You are a helpful AI. Answer the question based on query and memories.\nUser Memories:\n{memories_str}"\n    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": message}]\n    response = openai_client.chat.completions.create(model="gpt-4.1-nano-2025-04-14", messages=messages)\n    assistant_response = response.choices[0].message.content\n\n    # Create new memories from the conversation\n    messages.append({"role": "assistant", "content": assistant_response})\n    memory.add(messages, user_id=user_id)\n\n    return assistant_response\n\ndef main():\n    print("Chat with AI (type ''exit'' to quit)")\n    while True:\n        user_input = input("You: ").strip()\n        if user_input.lower() == ''exit'':\n            print("Goodbye!")\n            break\n        print(f"AI: {chat_with_memories(user_input)}")\n\nif __name__ == "__main__":\n    main()\n```\n\nFor detailed integration steps, see the [Quickstart](https://docs.mem0.ai/quickstart) and [API Reference](https://docs.mem0.ai/api-reference).\n\n## ğŸ”— Integrations & Demos\n\n- **ChatGPT with Memory**: Personalized chat powered by Mem0 ([Live Demo](https://mem0.dev/demo))\n- **Browser Extension**: Store memories across ChatGPT, Perplexity, and Claude ([Chrome Extension](https://chromewebstore.google.com/detail/onihkkbipkfeijkadecaafbgagkhglop?utm_source=item-share-cb))\n- **Langgraph Support**: Build a customer bot with Langgraph + Mem0 ([Guide](https://docs.mem0.ai/integrations/langgraph))\n- **CrewAI Integration**: Tailor CrewAI outputs with Mem0 ([Example](https://docs.mem0.ai/integrations/crewai))\n\n## ğŸ“š Documentation & Support\n\n- Full docs: https://docs.mem0.ai\n- Community: [Discord](https://mem0.dev/DiG) Â· [Twitter](https://x.com/mem0ai)\n- Contact: founders@mem0.ai\n\n## Citation\n\nWe now have a paper you can cite:\n\n```bibtex\n@article{mem0,\n  title={Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory},\n  author={Chhikara, Prateek and Khant, Dev and Aryan, Saket and Singh, Taranjeet and Yadav, Deshraj},\n  journal={arXiv preprint arXiv:2504.19413},\n  year={2025}\n}\n```\n\n## âš–ï¸ License\n\nApache 2.0 â€” see the [LICENSE](https://github.com/mem0ai/mem0/blob/main/LICENSE) file for details.', '{"language":"Python","stars":44005,"forks":4772,"watchers":44005,"open_issues":540,"topics":["agents","ai","ai-agents","application","chatbots","chatgpt","genai","hacktoberfest","llm","long-term-memory","memory","memory-management","python","rag","state-management"],"default_branch":"main","size_kb":44076,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:mem0ai:mem0\">","source_url":"https://github.com/mem0ai/mem0\">"},{"type":"has_code","target_id":"github:mem0ai:mem0\">","source_url":"https://github.com/mem0ai/mem0\">"},{"type":"has_code","target_id":"github:mem0ai:mem0","source_url":"https://github.com/mem0ai/mem0"}]', NULL, 'Apache-2.0', 'approved', 65, 'b5461b29178de3d84a3a46caa311c639', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-mem0ai-mem0 from https://github.com/mem0ai.png
Image converted to WebP: data/images/github-mem0ai-mem0.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-GitHubDaily-GitHubDaily', 'github--githubdaily--githubdaily', 'GitHubDaily', 'GitHubDaily', '<p align="center"> <img src="https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/githubdaily_brand.png"> </p> <p align="center"> <a href="https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/weixin.png"><img src="https://img.shields.io/badge/ GitHubDaily-å…¬ä¼—å·-brightgreen.svg" alt="WeChat"></a> <a href="https://weibo.com/GitHubDaily"><img src="https://img.shields.io/badge/%E6%96%B0%E6%B5%AA-%E5%BE%AE%E5%8D%9A-red" alt="Weibo"></a> <a href="https://www.zhih...', '["ai","algorithms-and-data-structures","backend","developer-tools","development","frontend","github","java","javascript","kubernetes","linux","markdown","open-source","python","tutorials","web"]', 'other', 43235, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/GitHubDaily/GitHubDaily","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <img src="https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/githubdaily_brand.png">\n</p>\n\n<p align="center">\n  <a href="https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/weixin.png"><img src="https://img.shields.io/badge/ GitHubDaily-å…¬ä¼—å·-brightgreen.svg" alt="WeChat"></a>\n  <a href="https://weibo.com/GitHubDaily"><img src="https://img.shields.io/badge/%E6%96%B0%E6%B5%AA-%E5%BE%AE%E5%8D%9A-red" alt="Weibo"></a>\n  <a href="https://www.zhihu.com/people/githubdaily"><img src="https://img.shields.io/badge/%E7%9F%A5%E4%B9%8E-blue" alt="Zhihu"></a>\n  <a href="https://x.com/intent/follow?screen_name=GitHub_Daily"><img alt="X (formerly Twitter) Follow" src="https://img.shields.io/twitter/follow/GitHub_Daily"></a>\n  <a href="https://github.com/GitHubDaily/GitHubDaily/stargazers"><img src="https://img.shields.io/github/stars/GitHubDaily/GitHubDaily" alt="GitHub stars"></a>\n</p>\n\n## å®—æ—¨\n\nå¤šå¹´ä»¥å‰ï¼Œæˆ‘æ›¾çœ‹åˆ° GitHub å¼€æºé¡¹ç›®ä½œè€…ã€å…¨æ ˆå·¥ç¨‹å¸ˆ TJ Holowaychunk è¯´è¿‡è¿™ä¹ˆä¸€å¥è¯ï¼š\n\n"I don''t read books, never went to school, I just read other people''s code and always wonder how things work"ã€‚\n\nä»é‚£æ—¶èµ·ï¼Œæˆ‘ä¾¿è®¤ä¸ºï¼Œé€šè¿‡é˜…è¯»æºç ï¼Œç«™åœ¨å‰è¾ˆçš„è§’åº¦ä¸Šï¼Œå»æ€è€ƒä»£ç æ¶æ„ä¸ç¨‹åºé€»è¾‘ï¼Œä¹ƒæ˜¯æå‡ç¼–ç¨‹æŠ€å·§æœ€å¥½çš„æ–¹å¼ã€‚\n\nå› æ­¤ï¼ŒGitHub ä¹Ÿè‡ªç„¶è€Œç„¶çš„ï¼Œæˆä¸ºæˆ‘æœ€å–œçˆ±çš„å¼€å‘è€…å¹³å°ã€‚\n\nç§‰ç€æŒ–æ˜å¼€æºä»·å€¼çš„åˆè¡·ï¼ŒGitHubDaily è‡ª 2015 å¹´ 10 æœˆ 10 æ—¥æ­£å¼æˆç«‹ã€‚\n\næˆ‘ä»¬å¸Œæœ›èƒ½é€šè¿‡è¿™ä¸€ä¸¾æªï¼Œå¸®åŠ©å¼€å‘è€…ä»¬å‘ç°å½“ä¸‹æœ€ç«çš„å¼€æºé¡¹ç›®ï¼ŒæŒæ§æœ€æ–°æŠ€æœ¯åŠ¨æ€, æ‰©å¤§æŠ€æœ¯è§†é‡, å¹¶ä»å¼€æºé¡¹ç›®çš„å­¦ä¹ ä¸­è·å¾—ç¼–ç¨‹èƒ½åŠ›çš„æå‡ã€‚\n\nç›®å‰ï¼ŒGitHubDaily å·²ç´¯ç§¯åˆ†äº«è¶…è¿‡ 8000 ä¸ªå¼€æºé¡¹ç›®ï¼Œå†…å®¹åŒ…æ‹¬ä½†ä¸é™äº GitHub ä¸Šçš„å¼€æºæŠ€æœ¯èµ„æ–™ã€å¼€å‘è€…å·¥å…·ã€ç¼–ç¨‹ç½‘ç«™ä»¥åŠæˆç†Ÿåº”ç”¨ã€‚\n\né™¤äº† GitHub ä¹‹å¤–ï¼Œæˆ‘ä»¬ä¹Ÿå¼€å§‹åœ¨ä¸‹é¢å¤šä¸ªç¤¾äº¤åª’ä½“å¹³å°ï¼Œå¸®åŠ©å¼€å‘è€…ä¼ æ’­ä¸åˆ†äº«ä¼˜è´¨å¼€æºé¡¹ç›®ï¼ŒæŒ–æ˜å…¶æœªæ¥çš„æŠ€æœ¯åº”ç”¨å‰æ™¯ã€‚\n\nå¦‚æœä½ æƒ³æ¥æ”¶æœ€æ–°çš„ GitHub å¼€æºé¡¹ç›®èµ„è®¯ï¼Œå¯ä»¥å…³æ³¨ä¸€ä¸‹ğŸ‘‡\n\n- å…¬ä¼—å·: [GitHubDaily](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/weixin.png)\n- å¾®åš: [@GitHubDaily](https://weibo.com/GitHubDaily)\n- çŸ¥ä¹: [@GitHubDaily](https://www.zhihu.com/people/githubdaily)\n- X (Twitter)ï¼š[@GitHubDaily](https://x.com/intent/follow?screen_name=GitHub_Daily)\n\n> æœ‰ä¸é”™çš„å¼€æºé¡¹ç›®ï¼Œä¹Ÿæ¬¢è¿åˆ°æœ¬ä»“åº“çš„ [issues](https://github.com/GitHubDaily/GitHubDaily/issues/new) æ¨èæˆ–è‡ªèé¡¹ç›®ï¼Œæˆ‘ä»¬æœŸå¾…ä½ çš„åˆ†äº«ã€‚\n\n---\n\n## 2024 å¹´å¤ç›˜\n\nä¸‹é¢æ˜¯å¯¹ GitHubDaily åœ¨ 2024 å¹´æ‰€æ¨èçš„é¡¹ç›®è¿›è¡Œåˆ†ç±»æ•´ç†ï¼Œæ–¹ä¾¿å¤§å®¶æŸ¥æ‰¾ä»¥å¾€åˆ†äº«è¿‡çš„å†…å®¹ã€‚\n\n> è¿‡å¾€å¤ç›˜ [2023](https://github.com/GitHubDaily/GitHubDaily/blob/master/2023.md)ï¼Œ [2022](https://github.com/GitHubDaily/GitHubDaily/blob/master/2022.md)ï¼Œ [2021](https://github.com/GitHubDaily/GitHubDaily/blob/master/2021.md)ï¼Œ[2020](https://github.com/GitHubDaily/GitHubDaily/blob/master/2020.md)ï¼Œ[2019](https://github.com/GitHubDaily/GitHubDaily/blob/master/2019.md)ï¼Œ[2018](https://github.com/GitHubDaily/GitHubDaily/blob/master/2018.md)\n\n### ç›®å½•\n\n- [å®—æ—¨](#å®—æ—¨)\n- [2024 å¹´å¤ç›˜](#2024-å¹´å¤ç›˜)\n  - [ç›®å½•](#ç›®å½•)\n  - [AI æŠ€æœ¯](#ai-æŠ€æœ¯)\n  - [AI å·¥å…·](#ai-å·¥å…·)\n  - [å…è´¹ä¹¦ç±](#å…è´¹ä¹¦ç±)\n  - [å­¦ä¹ æ•™ç¨‹](#å­¦ä¹ æ•™ç¨‹)\n  - [å®ç”¨å·¥å…·](#å®ç”¨å·¥å…·)\n  - [å®ç”¨æ’ä»¶](#å®ç”¨æ’ä»¶)\n  - [èµ„æ–™é›†åˆ](#èµ„æ–™é›†åˆ)\n  - [å…¶ä»–](#å…¶ä»–)\n- [å£°æ˜](#å£°æ˜)\n\n\n### AI æŠ€æœ¯\n\né¡¹ç›® | ç®€è¿° | æº\n---- | ----- | -----\n[OmniParser](http://t.cn/A6n4HUTW) | ä¸€æ¬¾åŸºäºçº¯è§†è§‰çš„ GUI æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«ç•Œé¢ä¸Šå¯äº¤äº’å›¾æ ‡ä»¥åŠç†è§£æˆªå›¾ä¸­å„å…ƒç´ è¯­ä¹‰ï¼Œå®ç°è‡ªåŠ¨åŒ–ç•Œé¢äº¤äº’åœºæ™¯ï¼Œå¦‚è‡ªåŠ¨åŒ–æµ‹è¯•ã€è‡ªåŠ¨åŒ–æ“ä½œç­‰ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/ODDGE9iKe)\n[ChatTTS](https://github.com/2noise/ChatTTS) | ä¸€æ¬¾ä¸“é—¨ä¸ºå¯¹è¯åœºæ™¯è®¾è®¡çš„è¯­éŸ³ç”Ÿæˆæ¨¡å‹ï¼Œä¸»è¦ç”¨äº LLM åŠ©æ‰‹å¯¹è¯ä»»åŠ¡ã€å¯¹è¯è¯­éŸ³ä»¥åŠè§†é¢‘ä»‹ç»ç­‰ï¼Œæ”¯æŒä¸­è‹±æ–‡æ··åˆæ–‡æœ¬åˆæˆè¯­éŸ³ï¼ŒéŸ³è‰²è¡¨ç°å¼ºï¼Œèƒ½è¾¾åˆ°çœŸå‡éš¾è¾¨ç¨‹åº¦ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OgtYGiyMo)\n[DLRover](https://github.com/intelligent-machine-learning/dlrover) | ä¸€å¥— AI å¤§æ¨¡å‹è®­ç»ƒæ–¹æ¡ˆï¼Œå¯å®ç°è®­ç»ƒ"è‡ªåŠ¨é©¾é©¶"ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ï¼Œé™ä½ç ”å‘æˆæœ¬ï¼Œå…·æœ‰å®¹é”™æ€§ã€å¿«é€Ÿæ¢å¤å’Œè‡ªåŠ¨æ‰©å±•ç­‰ç‰¹ç‚¹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NF2cklwac)\n[SwiftInfer](https://github.com/hpcaitech/SwiftInfer) | ä¸€æ¬¾åŸºäº TensorRT å®ç°çš„ StreamingLLM æ¨ç†åŠ é€Ÿæ¡†æ¶ï¼Œå¯æå‡å¤§æ¨¡å‹æ¨ç†æ€§èƒ½ 46 %ï¼Œä¸ºå¤šè½®å¯¹è¯æ¨ç†æä¾›é«˜æ•ˆå¯é çš„è½åœ°æ–¹æ¡ˆã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NAS7K9B1u)\n[Audio2PhotoReal](https://github.com/facebookresearch/audio2photoreal) | ä¸€é¡¹ç”± Meta AI å‘å¸ƒçš„æ–°æŠ€æœ¯ï¼Œèƒ½å¤Ÿé€šè¿‡éŸ³é¢‘é©±åŠ¨ç”Ÿæˆå…¨èº«é€¼çœŸçš„è™šæ‹Ÿäººç‰©å¯¹è¯ï¼ŒåŒ…æ‹¬åŠ¨ä½œã€è¡¨æƒ…å’Œå¤šäººå¯¹è¯ç­‰ï¼Œå¯å¹¿æ³›åº”ç”¨äºè™šæ‹Ÿç°å®ã€å½±è§†åˆ¶ä½œç­‰é¢†åŸŸã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NAqwLvBIV)\n[OpenVoice](https://github.com/myshell-ai/OpenVoice) | ä¸€é¡¹ç”± MyShell å›¢é˜Ÿå¼€å‘çš„å¤šåŠŸèƒ½å³æ—¶å£°éŸ³å…‹éš†æŠ€æœ¯ï¼Œåªéœ€æä¾›ç®€çŸ­éŸ³é¢‘æ ·æœ¬ï¼Œå³å¯å…‹éš†åŸå‘è¨€è€…çš„å£°éŸ³ï¼Œç”Ÿæˆå¤šç§è¯­è¨€è¯­éŸ³ï¼Œæ”¯æŒé«˜ç²¾åº¦éŸ³è‰²å…‹éš†ã€çµæ´»çš„å£°éŸ³é£æ ¼è°ƒæ§ä»¥åŠæ— éœ€æ ·æœ¬çš„è·¨è¯­è¨€å£°éŸ³å…‹éš†ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NAiw7rRNh)\n\n<div align="right">\n    <b><a href="#2024-å¹´å¤ç›˜">â†¥ è¿”å›ç›®å½•</a></b>\n</div>\n\n### AI å·¥å…·\n\né¡¹ç›® | ç®€è¿° | æº\n---- | ----- | -----\n[Shortest](https://github.com/anti-work/shortest) | ä¸€æ¬¾åˆ©ç”¨ Claude æ¨¡å‹èƒ½åŠ›çš„ç«¯åˆ°ç«¯ AI æµ‹è¯•æ¡†æ¶ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€ç¼–å†™å¯æ‰§è¡Œçš„å•å…ƒæµ‹è¯•ï¼Œæ”¯æŒ GitHub ä¸ 2FA é›†æˆï¼Œæä¾›å®‰å…¨è®¤è¯æ–¹å¼ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P78T3tP8F)\n[STranslate](https://github.com/ZGGSONG/STranslate) | ä¸€æ¬¾å¼€ç®±å³ç”¨çš„ç¿»è¯‘å’Œ OCR å·¥å…·ï¼Œæ”¯æŒå¤šç§ç¿»è¯‘æ–¹å¼ã€å¤šå®¶ç¿»è¯‘æœåŠ¡ã€ç¦»çº¿ OCRã€å›è¯‘ã€TTS ç­‰åŠŸèƒ½ï¼Œæå‡ç¿»è¯‘æ•ˆç‡ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P6VVmEi17)\n[x-kit](https://github.com/xiaoxiunique/x-kit) | ä¸€æ¬¾ç”¨äºæŠ“å–å’Œåˆ†æ Twitter æ¨å‹æ•°æ®çš„å·¥å…·ï¼Œå¯è‡ªåŠ¨æŠ“å–æŒ‡å®šæ¨å‹çš„åŸºæœ¬ä¿¡æ¯å’Œæ¨æ–‡ï¼Œæ”¯æŒå®šæ—¶æ›´æ–°æ—¶é—´çº¿æ•°æ®å’Œæœ¬åœ°å­˜å‚¨ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P6Q246RBv)\n[MMAudio](https://huggingface.co/spaces/hkchengrex/MMAudio) | ä¸€æ¬¾è§†é¢‘è‡ªåŠ¨é…éŸ³å·¥å…·ï¼Œå¯æ ¹æ®è§†é¢‘å†…å®¹æˆ–æ–‡æœ¬æç¤ºç”Ÿæˆç¬¦åˆåœºæ™¯çš„éŸ³æ•ˆï¼Œå¹¶ä¸è§†é¢‘ç”»é¢åŠ¨ä½œä¿æŒåŒæ­¥å¯¹é½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P6OR0gwq7)\n[Hoarder](https://github.com/hoarder-app/hoarder) | ä¸€æ¬¾è‡ªæ‰˜ç®¡çš„ä¹¦ç­¾ç®¡ç† AI å·¥å…·ï¼Œæ”¯æŒä¿å­˜é“¾æ¥ã€ç¬”è®°ã€å›¾ç‰‡å’Œ PDF æ–‡ä»¶ï¼Œåˆ©ç”¨ AI è‡ªåŠ¨æ‰“æ ‡ç­¾åˆ†ç±»å­˜å‚¨ï¼Œæä¾›å…¨æ–‡æœç´¢èƒ½åŠ›ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P6wew9VS5)\n[Midscene.js](https://github.com/web-infra-dev/midscene) | ä¸€æ¬¾ç”± AI é©±åŠ¨çš„ UI è‡ªåŠ¨åŒ– SDKï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€æè¿°å¯¹ä»»æ„ç½‘é¡µè¿›è¡Œè‡ªåŠ¨åŒ–æ“ä½œã€éªŒè¯å’Œæ•°æ®æå–ï¼Œç”Ÿæˆå¯è§†åŒ–æµ‹è¯•æŠ¥å‘Šï¼Œæ”¯æŒå¤šç§ä¸»æµ AI æ¨¡å‹å’Œè‡ªåŠ¨åŒ–åº“é›†æˆã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P6v17rEJR)\n[Browser Use](https://github.com/browser-use/browser-use) | ä¸€æ¬¾å¼€æºå·¥å…·ï¼Œèƒ½è®© AI æ™ºèƒ½ä½“è‡ªåŠ¨åŒ–æ“ä½œæµè§ˆå™¨ï¼ŒåƒçœŸäººä¸€æ ·ç†è§£ç½‘é¡µå†…å®¹å¹¶å®Œæˆå¤æ‚ä»»åŠ¡ï¼Œå¦‚æŠ•é€’ç®€å†ã€æŸ¥è¯¢èˆªç­ç­‰ï¼Œå…·å¤‡è§†è§‰è¯†åˆ«ã€HTML è§£æã€å¤šæ ‡ç­¾é¡µç®¡ç†ç­‰åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P6lCEB88t)\n[Video Ocean](https://github.com/hpcaitech/Open-Sora) | ä¸€æ¬¾åŸºäº Colossal - AI ä¼˜åŒ–çš„ Sora è§†é¢‘æ¨¡å‹çš„å…è´¹è§†é¢‘ç”Ÿæˆå¹³å°ï¼Œæä¾›æä½æˆæœ¬ã€å¿«é€Ÿçš„è§†é¢‘ç”ŸæˆæœåŠ¡ï¼Œå¹¶å°†ä¼˜åŒ–æ–¹æ¡ˆå¼€æºã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P6kdo241Y)\n[LogoCreator](https://github.com/Nutlope/logocreator) | ä¸€æ¬¾å¼€æºçš„ AI Logo ç”Ÿæˆå™¨ï¼Œä½¿ç”¨ Flux Pro 1 . 1 æ¨¡å‹å¿«é€Ÿç”Ÿæˆ Logo å›¾æ ‡ï¼Œæ”¯æŒç¼–è¾‘æ ·å¼ï¼Œä»£ç å®Œå…¨å¼€æºå¹¶æä¾›å…è´¹ä½“éªŒã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P64YnygK8)\n[MarkltDown](https://github.com/microsoft/markitdown) | ä¸€æ¬¾ç”±å¾®è½¯å¼€æºçš„å·¥å…·ï¼Œå¯å°†å¸¸è§æ–‡ä»¶å¦‚ Wordã€Excelã€PPT ç­‰è½¬æ¢ä¸º Markdown æ ¼å¼ï¼Œæ”¯æŒ OCR å’Œ AI æ¨¡å‹å¤„ç†å¤šåª’ä½“æ–‡ä»¶ï¼Œæä¾›å‘½ä»¤è¡Œã€Python API å’Œ Docker ä¸‰ç§ä½¿ç”¨æ–¹å¼ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P5H89CIC8)\n[PDFMathTranslate](https://github.com/Byaidu/PDFMathTranslate) | ä¸€æ¬¾å¼€æºçš„ PDF æ–‡æ¡£ç¿»è¯‘åŠåŒè¯­å¯¹ç…§å·¥å…·ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°ä¿ç•™å…¬å¼ã€å›¾è¡¨ã€ç›®å½•å’Œæ³¨é‡Šç­‰æ’ç‰ˆï¼Œæ”¯æŒå¤šç§è¯­è¨€å’Œç¿»è¯‘æœåŠ¡ï¼Œå¯é€šè¿‡å‘½ä»¤è¡Œã€GUI ç•Œé¢ã€Docker ç­‰æ–¹å¼ä½¿ç”¨ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P5CICinD6)\n[Gemini-Teacher](https://github.com/nishuzumi/gemini-teacher) | ä¸€æ¬¾åŸºäº Gemini 2 . 0 å¼€å‘çš„è‹±è¯­å£è¯­ç»ƒä¹  AI åŠ©æ‰‹ï¼Œèƒ½å®æ—¶è¯†åˆ«å‘éŸ³å¹¶æä¾›åé¦ˆå’Œçº æ­£å»ºè®®ï¼Œé€‚åˆè‹±è¯­å£è¯­å­¦ä¹ å’Œæ•™å­¦ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P5xHF6P6c)\n[XHS Note Generator](https://github.com/whotto/Video_note_generator) | ä¸€æ¬¾å°çº¢ä¹¦ç¬”è®° AI ç”Ÿæˆå™¨ï¼Œæ”¯æŒå¿«é€Ÿæ‰¹é‡å°†è§†é¢‘æˆ–ç›´æ’­å†…å®¹ä¸€é”®è½¬æ¢ä¸ºç¬¦åˆå°çº¢ä¹¦é£æ ¼çš„çˆ†æ¬¾ç¬”è®°ï¼Œè‡ªåŠ¨ä¼˜åŒ–å†…å®¹å’Œé…å›¾ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P5tbArUkt)\n[ScreenPipe](https://github.com/mediar-ai/screenpipe) | ä¸€æ¬¾å¼ºå¤§çš„å¼€æº AI æ¡Œé¢åº”ç”¨ï¼Œå¯ 24 å°æ—¶ç›‘æ§ç”µè„‘ï¼Œé€šè¿‡å±å¹•å½•åˆ¶ã€OCRã€éŸ³é¢‘è¾“å…¥å’Œè½¬å½•æ”¶é›†ä¿¡æ¯ï¼Œä¿å­˜åˆ°æœ¬åœ°æ•°æ®åº“ï¼Œåˆ©ç”¨ LLM å¯¹è¯ã€æ€»ç»“å’Œå›é¡¾ä½ æ‰€åšçš„äº‹æƒ…ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P1c7O5pXF)\n[RMBG-2.0](https://huggingface.co/briaai/RMBG-2.0) | ä¸€æ¬¾é«˜æ•ˆçš„èƒŒæ™¯ç§»é™¤å·¥å…·ï¼Œæ”¯æŒå¤„ç†å„ç§å›¾åƒï¼Œä¸€é”®ç§»é™¤èƒŒæ™¯ï¼Œæ•ˆæœå‡ºè‰²ä¸”å¤„ç†é€Ÿåº¦å¿«ï¼Œé€‚ç”¨äºç”µå•†ã€å¹¿å‘Šç­‰åœºæ™¯ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P12Dlmfo7)\n[Comfyui_Object_Migration](https://github.com/TTPlanetPig/Comfyui_Object_Migration) | ä¸€å¥—åŸºäº ComfyUI çš„æœè£…è¿ç§»å·¥ä½œæµï¼Œå¯å°†æœè£…ç…§ç‰‡è‡ªç„¶çœŸå®åœ°è¿ç§»åˆ°æ¨¡ç‰¹èº«ä¸Šï¼Œæ”¯æŒè™šæ‹Ÿè¯•ç©¿å’Œé£æ ¼è¿ç§»ï¼ˆå¦‚åŠ¨æ¼«è½¬ç°å®é£æ ¼ï¼‰ï¼Œè¿˜åŸåº¦è¾ƒé«˜ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P0YSLnIPZ)\n[NSFW Detector](https://github.com/tmplink/nsfw_detector) | ä¸€æ¬¾å¼€æºå…è´¹çš„ NSFW å†…å®¹æ£€æµ‹å™¨ï¼Œä½¿ç”¨ Google æ¨¡å‹ï¼Œå‡†ç¡®åº¦é«˜ï¼Œæ”¯æŒæ£€æµ‹å›¾ç‰‡ã€PDF æ–‡ä»¶ã€è§†é¢‘ã€å‹ç¼©åŒ…ç­‰å¤šç§æ–‡ä»¶ç±»å‹ï¼Œæ”¯æŒçº¯ CPU æ¨ç†ï¼Œæä¾› API æœåŠ¡ä¾¿äºé›†æˆã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P0UmHjmi7)\n[VideoCaptioner](https://github.com/WEIFENG2333/VideoCaptioner) | ä¸€æ¬¾åŠŸèƒ½å¼ºå¤§çš„è§†é¢‘å­—å¹•ç¿»è¯‘åŠ©æ‰‹ï¼Œæä¾›å¯è§†åŒ–æ“ä½œç•Œé¢ï¼Œæ”¯æŒå­—å¹•æ™ºèƒ½æ–­å¥ã€æ ¡æ­£ã€ä¼˜åŒ–ã€ç¿»è¯‘ï¼Œä¸€é”®ç”ŸæˆåŒå­—å¹•è§†é¢‘ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P0BvHjG5I)\n[MagicQuill](https://github.com/magic-quill/magicquill) | ä¸€æ¬¾å¼€æºçš„ AI äº’åŠ¨å¼å›¾åƒç¼–è¾‘å·¥å…·ï¼Œç”¨æˆ·åªéœ€é€šè¿‡ç”»ç¬”æ¶‚æŠ¹å’Œç®€å•æç¤ºè¯ï¼Œå³å¯è½»æ¾å®ç°æ’å…¥å…ƒç´ ã€æ“¦é™¤ç‰©ä½“ã€è°ƒæ•´é¢œè‰²ç­‰å„ç§å›¾åƒç¼–è¾‘æ“ä½œã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P0sfgqLQl)\n[Voice-Pro](https://github.com/abus-aikorea/voice-pro) | ä¸€æ¬¾é›†è½¬å½•ã€ç¿»è¯‘å’Œæ–‡å­—è½¬è¯­éŸ³ä¸ºä¸€ä½“çš„å¼€æºå·¥å…·ï¼Œæä¾›ç®€æ´ç›´è§‚çš„å¯è§†åŒ–æ“ä½œç•Œé¢ï¼Œæ”¯æŒå®æ—¶è½¬å½•å’Œç¿»è¯‘ï¼Œä»¥åŠæ‰¹é‡å¤„ç†æ¨¡å¼ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OFXg2nrNa)\n[AdvancedLivePortrait-WebUI](https://github.com/jhj0517/AdvancedLivePortrait-WebUI) | ä¸€æ¬¾åŸºäº Gradio å¼€å‘çš„å¼€æºå·¥å…·ï¼Œèƒ½å¤Ÿå¯¹å›¾åƒä¸­çš„äººç‰©è¿›è¡Œå®æ—¶é¢éƒ¨è¡¨æƒ…ç²¾ç¡®æ§åˆ¶ï¼Œå¦‚å¾®ç¬‘ã€çœ¨çœ¼ã€æ‘‡å¤´ç­‰ï¼Œæ”¯æŒå¤šç§éƒ¨ç½²æ–¹å¼ä½¿ç”¨ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OF8Ana05p)\n[pdf-extract-api](https://github.com/CatchTheTornado/pdf-extract-api) | ä¸€æ¬¾åŸºäº FastAPI çš„ PDF æ–‡æ¡£æå–å’Œè§£æå·¥å…·ï¼Œåˆ©ç”¨ OCR æŠ€æœ¯å’Œ Ollama æ¨¡å‹å°† PDF æˆ–å›¾åƒè½¬æ¢ä¸º Markdown æ–‡æœ¬æˆ– JSON æ–‡æ¡£ï¼Œæ”¯æŒè¡¨æ ¼ã€å…¬å¼ç­‰æ ¼å¼è§£æï¼Œä½¿ç”¨ Redis ç¼“å­˜æé«˜æ•ˆç‡ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OES7rAt81)\n[Cerebellum](https://github.com/theredsix/cerebellum) | ä¸€æ¬¾åŸºäºæ™ºèƒ½ä½“çš„æµè§ˆå™¨è‡ªåŠ¨åŒ–å·¥å…·ï¼Œé€šè¿‡ä½¿ç”¨ LLM æ„å»ºçš„æ™ºèƒ½ä½“å®ç°è‡ªåŠ¨åŒ–æ“ä½œé”®ç›˜å’Œé¼ æ ‡ï¼Œåœ¨ç½‘é¡µä¸Šå®Œæˆæ•°æ®æŠ“å–ã€è‡ªåŠ¨åŒ–æµ‹è¯•ç­‰ä»»åŠ¡ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OEPJnt0p7)\n[PromptFix](https://github.com/yeates/PromptFix) | ä¸€æ¬¾åŸºäºæç¤ºçš„ AI å›¾åƒä¿®å¤å·¥å…·ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥çš„æç¤ºå¯¹å›¾åƒè¿›è¡Œä¸Šè‰²ã€ç§»é™¤ç‰©ä½“ã€å»é™¤æ°´å°ã€é«˜æ¸…åŒ–ã€è°ƒæ•´å…‰çº¿ç­‰æ“ä½œã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OEzct54QI)\n[Maxun](https://github.com/getmaxun/maxun) | ä¸€æ¬¾å¼€æºçš„è‡ªåŠ¨åŒ–ç½‘é¡µæ•°æ®çˆ¬å–å·¥å…·ï¼Œæ— éœ€ç¼–å†™ä»£ç ï¼Œé€šè¿‡å¯è§†åŒ–ç•Œé¢æ„å»ºè‡ªå®šä¹‰æœºå™¨äººå®ç°ç½‘é¡µæ•°æ®æ•è·ã€æå–å’Œå¤„ç†ç­‰åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OEpO15B1v)\n[AI Podcast Generator](http://t.cn/A6nVnoTM) | ä¸€æ¬¾å¼€æºçš„è‡ªåŠ¨åŒ– AI æ’­å®¢ç”Ÿæˆå·¥å…·ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æŠ“å–ç½‘ç»œæ–°é—»å†…å®¹ï¼Œå¹¶ç”Ÿæˆè‡ªç„¶æµç•…çš„å™è¿°æ€§éŸ³é¢‘æ’­å®¢ï¼Œæä¾›ç®€å•æ˜“ç”¨çš„ç•Œé¢ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OE6X11vEB)\n[MegaParse](http://t.cn/A6nfRDtg) | ä¸€æ¬¾åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹æ„å»ºçš„åŠŸèƒ½å¼ºå¤§çš„æ–‡æ¡£è§£æå¼€æºå·¥å…·ï¼Œå¯ä»¥è½»æ¾å¤„ç† PDFã€PPTã€Wordã€Excel ç­‰å¸¸è§æ ¼å¼ï¼Œä¿è¯è§£æè¿‡ç¨‹ä¸­ä¸ä¸¢å¤±ä»»ä½•ä¿¡æ¯ï¼Œè¿˜èƒ½å‡†ç¡®è¯†åˆ«æ–‡æ¡£ä¸­çš„è¡¨æ ¼ã€ç›®å½•ã€é¡µçœ‰é¡µè„šå’Œå›¾ç‰‡ç­‰å†…å®¹ï¼Œè§£æé€Ÿåº¦å¿«ï¼Œæ•ˆç‡é«˜ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/ODXwwqWPu)\n[SoniTranslate](http://t.cn/A6nVFPdz) | ä¸€æ¬¾åŸºäº Gradio æ„å»ºçš„ç®€å•æ˜“ç”¨çš„è§†é¢‘ç¿»è¯‘å·¥å…·ï¼Œæ”¯æŒå°†è§†é¢‘ä¸€é”®ç¿»è¯‘æˆå¤šç§è¯­è¨€ï¼Œå¹¶æä¾›è§†é¢‘åŒå£°ç¿»è¯‘åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/ODVaoqul9)\n[EveryoneNobel](https://github.com/16131zzzzzzzz/EveryoneNobel) | ä¸€æ¬¾å¼€æºçš„ AI å·¥å…·ï¼Œå¯ç”¨äºç”Ÿæˆä¸ªæ€§åŒ–è¯ºè´å°”å¥–é¡¹å›¾ç‰‡ï¼Œåˆ©ç”¨ ComfyUI è¿›è¡Œå›¾åƒç”Ÿæˆï¼Œå¹¶ç»“åˆ HTML æ¨¡æ¿å±•ç¤ºå›¾ç‰‡ä¸Šçš„æ–‡æœ¬ï¼Œåªéœ€æä¾›ä¸ªäººè‚–åƒå›¾å³å¯ç”Ÿæˆè¯ºè´å°”å¥–é¡¹é£æ ¼å›¾åƒã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/ODjspfWSC)\n[PodCastLM](https://github.com/YOYZHANG/PodCastLM) | ä¸€æ¬¾å¼€æºå…è´¹çš„å·¥å…·ï¼Œå¯ä»¥å°† PDF å†…å®¹è½¬åŒ–ä¸ºé€‚åˆéŸ³é¢‘æ’­å®¢çš„è‡ªç„¶å¯¹è¯ï¼Œå¹¶è¾“å‡ºä¸º MP3 æ–‡ä»¶ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/ODco2x491)\n[Agent.exe](https://github.com/corbt/agent.exe) | ä¸€æ¬¾å¼€æºçš„ AI è‡ªä¸»æ“ä½œè®¡è„‘å·¥å…·ï¼Œåˆ©ç”¨ Claude 3 . 5 Sonnet ç›´æ¥æ§åˆ¶æœ¬åœ°ç”µè„‘ï¼Œå±•ç¤ºäº† Claude çš„ Computer Use èƒ½åŠ›ï¼Œå¯ç”¨äºè‡ªåŠ¨åŒ–æ™ºèƒ½ä½“å¼€å‘ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OD8EGbgkh)\n[Zerox](https://github.com/getomni-ai/zerox) | ä¸€æ¬¾ç®€å•æ˜“ç”¨çš„ OCR æ–‡æ¡£å·¥å…·ï¼Œæ”¯æŒ PDFã€Docxã€å›¾åƒç­‰æ–‡ä»¶æ ¼å¼ï¼Œå¯å°†æ–‡ä»¶è½¬æ¢ä¸ºå›¾åƒååˆ©ç”¨ gpt - 4o - mini æ¨¡å‹è¯†åˆ«å›¾åƒä¿¡æ¯å¹¶è¾“å‡º Markdown æ ¼å¼æ–‡ä»¶ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OD48BgZ27)\n[UVR5-UI](https://github.com/Eddycrack864/UVR5-UI) | ä¸€æ¬¾åŠŸèƒ½å¼ºå¤§çš„äººå£°ä¼´å¥åˆ†ç¦» AI å·¥å…·ï¼Œæä¾›åœ¨çº¿ç®€æ´æ˜“ç”¨çš„æ“ä½œç•Œé¢ï¼Œæ”¯æŒé€‰æ‹©ä¸åŒçš„ AI æ¨¡å‹ä»¥åŠè¾“å‡ºæ ¼å¼å’Œè´¨é‡ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OCGnbC7UP)\n[Virtual Try-On Application](https://github.com/adarshb3/Virtual-Try-On-Application-using-Flask-Twilio-and-Gradio) | ä¸€æ¬¾åŸºäº WhatsApp API å’Œ IDM - VTON è™šæ‹Ÿè¯•ç©¿æ¨¡å‹çš„ AI åº”ç”¨ï¼Œç”¨æˆ·åªéœ€åœ¨ WhatsApp ä¸Šä¼ ä¸ªäººç…§ç‰‡å’Œæœè£…ç…§ç‰‡ï¼Œå³å¯ç”Ÿæˆè™šæ‹Ÿè¯•ç©¿æ•ˆæœå›¾ï¼Œæä¾›ä¾¿æ·çš„è™šæ‹Ÿè¯•è¡£ä½“éªŒã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OCAG362B4)\n[Claude Financial Data Analyst](https://github.com/anthropics/anthropic-quickstarts/tree/main/financial-data-analyst) | ä¸€æ¬¾ç”± Anthropic å¼€æºçš„åŸºäº Next . js å’Œ Claude AI æ„å»ºçš„é‡‘èæ•°æ®åˆ†æåŠ©æ‰‹ï¼Œæ”¯æŒå¤šç§æ ¼å¼æ–‡ä»¶ä¸Šä¼ ï¼Œé€šè¿‡èŠå¤©ç•Œé¢è¿›è¡Œè´¢åŠ¡æ•°æ®åˆ†æå¹¶ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OCj07iHiB)\n[Tabled](https://github.com/VikParuchuri/tabled) | ä¸€æ¬¾åŸºäº Surya å¼€å‘çš„ OCR å°å·¥å…·ï¼Œèƒ½å¤Ÿè¯†åˆ« PDFã€å›¾åƒã€Word å’Œ PPT ç­‰æ–‡ä»¶ä¸­çš„è¡¨æ ¼ï¼Œå¹¶å°†è¡¨æ ¼å†…å®¹æå–è½¬æ¢ä¸º Markdownã€CSV æˆ– HTML æ ¼å¼ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OChP4iiej)\n[Surya](https://github.com/VikParuchuri/surya) | ä¸€æ¬¾å¼€æºä¸”å¼ºå¤§çš„æ–‡æ¡£ OCR å·¥å…·ï¼Œä¸“æ³¨äºæ–‡æ¡£å›¾åƒçš„å¤„ç†å’Œåˆ†æï¼Œèƒ½å¤Ÿå‡†ç¡®è¿›è¡Œé€è¡Œæ–‡æœ¬æ£€æµ‹å’Œè¯†åˆ«ï¼Œæ”¯æŒ 90 å¤šç§è¯­è¨€ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OC62rozdB)\n[Animate-X](https://lucaria-academy.github.io/Animate-X/) | ä¸€æ¬¾åŸºäº AI çš„åŠ¨ç”»ç”Ÿæˆå·¥å…·ï¼Œå¯ä»¥é€šè¿‡è¾“å…¥è§’è‰²å›¾ç‰‡å’Œå‚è€ƒåŠ¨ä½œè§†é¢‘ï¼Œè‡ªåŠ¨ç”Ÿæˆè§’è‰²æŒ‰ç…§æŒ‡å®šåŠ¨ä½œç§»åŠ¨çš„åŠ¨ç”»æ•ˆæœï¼Œæ”¯æŒçœŸäººã€æ¸¸æˆã€å¡é€šç­‰å¤šç§ç±»å‹çš„è§’è‰²ï¼Œä¿æŒè§’è‰²ä¸€è‡´æ€§å¹¶å…è®¸å¤§å¹…åº¦åŠ¨ä½œã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OC0f8wojR)\n[AsrTools](https://github.com/WEIFENG2333/AsrTools) | ä¸€æ¬¾å¼€æºçš„æ™ºèƒ½è¯­éŸ³è½¬å­—å¹•æ–‡æœ¬å·¥å…·ï¼Œé›†æˆäº†å¤šå®¶å…¬å¸çš„è¯­éŸ³è¯†åˆ«æ¥å£ï¼Œæ”¯æŒå¤šç§éŸ³é¢‘æ ¼å¼ï¼Œæä¾›æ‰¹é‡å¤„ç†å’Œç”Ÿæˆå­—å¹•æ–‡ä»¶åŠŸèƒ½ï¼Œç•Œé¢ç®€å•æ˜“ç”¨ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OBLOcu6QU)\n[Podcastfy](https://huggingface.co/spaces/thatupiso/Podcastfy.ai_demo) | ä¸€æ¬¾å¼€æºå·¥å…·ï¼Œå¯å°†è§†é¢‘ã€PDFã€è®ºæ–‡ã€ç½‘ç«™å’Œæ–‡ç« ç­‰å†…å®¹è½¬æ¢ä¸ºå¯¹è¯å¼çš„æ’­å®¢éŸ³é¢‘ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OBdPAmvWp)\n[gptme](https://github.com/ErikBjare/gptme) | ä¸€æ¬¾åŠŸèƒ½å¼ºå¤§çš„å¼€æºç»ˆç«¯ AI åŠ©æ‰‹ï¼Œæ”¯æŒåœ¨ç»ˆç«¯æ‰§è¡Œæœ¬åœ°ä»£ç ã€è¯»å†™æ–‡ä»¶ã€æœç´¢æµè§ˆç½‘é¡µå’Œè¿›è¡Œè§†è§‰è¯†åˆ«ç­‰æ“ä½œï¼Œå¯è¿æ¥ OpenAIã€Anthropicã€OpenRouter ç­‰ä¸»æµ LLM æä¾›å•†ï¼Œä¹Ÿå¯ä½¿ç”¨ llama . cpp åœ¨æœ¬åœ°æä¾›æœåŠ¡ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OALy5o5df)\n[FinRL](https://github.com/AI4Finance-Foundation/FinRL) | ä¸€ä¸ªå¼€æºçš„é‡åŒ–é‡‘èè‡ªåŠ¨äº¤æ˜“æ¡†æ¶ï¼ŒåŒ…å«å¸‚åœºç¯å¢ƒã€æ™ºèƒ½ä½“å’Œåº”ç”¨ä¸‰å±‚æ¶æ„ï¼Œæä¾›å…ˆè¿›ç®—æ³•æ ¸å¿ƒæ”¯æŒè¿ç»­äº¤æ˜“å†³ç­–ã€‚æ”¯æŒæŠ•èµ„ç»„åˆåˆ†é…ã€åŠ å¯†è´§å¸äº¤æ˜“ã€é«˜é¢‘äº¤æ˜“ç­‰é‡‘èä»»åŠ¡å’Œå®æ—¶äº¤æ˜“åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OACdjiyiF)\n[Youtube-Whisper](https://yt-whisper.danilotpnta.com/) | ä¸€æ¬¾åˆ©ç”¨ OpenAI Whisper æ¨¡å‹å®ç°éŸ³é¢‘è½¬æ–‡æœ¬çš„è½»é‡çº§å¼€æºå…è´¹å·¥å…·ï¼Œå¯è¾“å…¥ YouTube è§†é¢‘é“¾æ¥æå–éŸ³é¢‘å¹¶è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œæ”¯æŒå¤šç§è¯­è¨€ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OAsGnlCN8)\n[ebook2audiobookXTTS](https://github.com/DrewThomasson/ebook2audiobookXTTS) | ä¸€æ¬¾å¼€æºçš„ AI å·¥å…·ï¼Œåˆ©ç”¨ Calibre å’Œ Coqui TTS å°†ç”µå­ä¹¦è½¬æ¢ä¸ºé«˜è´¨é‡çš„æœ‰å£°è¯»ç‰©ï¼Œæ”¯æŒå¤šç§è¯­è¨€å’Œè¯­éŸ³å…‹éš†åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OA0kD8uPB)\n[Text Behind Image](https://github.com/RexanWONG/text-behind-image) | ä¸€æ¬¾ç®€å•æœ‰è¶£çš„å¼€æºå…è´¹å·¥å…·ï¼Œå¯ä»¥è½»æ¾å°†æ–‡æœ¬æ·»åŠ åˆ°å›¾åƒä¸­æŒ‡å®šå¯¹è±¡çš„èƒŒåï¼Œå¦‚åŠ¨ç‰©ã€äººç‰©æˆ–ç‰©å“ç­‰ï¼Œå¹¶æ”¯æŒä¿å­˜åˆ°æœ¬åœ°ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OzSaaDpcL)\n[Compiler Explorer](https://godbolt.org) | ä¸€æ¬¾åœ¨çº¿äº¤äº’å¼ç¼–ç¨‹è¯­è¨€ç¼–è¯‘å™¨æ¢ç´¢å·¥å…·ï¼Œæ”¯æŒè¶…è¿‡ 30 ç§è¯­è¨€å¦‚ Cã€C ++ã€Rustã€Go ç­‰ï¼Œå¯åœ¨æµè§ˆå™¨ä¸­å®æ—¶ç¼–è¾‘ä»£ç å¹¶æŸ¥çœ‹ç¼–è¯‘åçš„æ±‡ç¼–ä»£ç è¾“å‡ºï¼Œæä¾›å¤šç§ç¼–è¾‘å™¨ã€å·¥å…·å’Œå¯è§†åŒ–é€‰é¡¹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OyvjHA5lr)\n[Local File Organizer](https://github.com/QiuYannnn/Local-File-Organizer) | ä¸€æ¬¾åŸºäº AI çš„æœ¬åœ°æ–‡ä»¶æ•´ç†å™¨ï¼Œå¯ä»¥è‡ªåŠ¨æ‰«ææŒ‡å®šç›®å½•ä¸­çš„æ–‡ä»¶ï¼Œç†è§£æ–‡ä»¶å†…å®¹å¹¶ç”Ÿæˆç›¸å…³æè¿°ã€æ–‡ä»¶å¤¹åç§°å’Œæ–‡ä»¶åï¼Œå°†æ–‡ä»¶æ•´ç†åˆ°æ–°çš„ç›®å½•ç»“æ„ä¸­ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OykI8FvUd)\n[OpenScanner](https://github.com/pencilresearch/OpenScanner) | ä¸€æ¬¾å¿«é€Ÿã€æ˜“ç”¨ä¸”å¼€æºå…è´¹çš„æ–‡æ¡£æ‰«æå·¥å…·ï¼Œå…·æœ‰è‡ªåŠ¨æ–‡æœ¬è¯†åˆ«ã€æ–‡æ¡£å‘½åã€åœ°ç†ä½ç½®æ ‡è®°ã€ç­¾åæ·»åŠ ã€Vision Pro æ”¯æŒç­‰åŠŸèƒ½ï¼Œå¯ä¿å­˜ã€ç¼–è¾‘ã€æ³¨é‡Šå’Œåˆ†äº«æ‰«ææ–‡æ¡£ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OxQaLinEV)\n[Diffusers Image Outpaint](https://huggingface.co/spaces/huggingface/diffusers-image-outpaint) | ä¸€æ¬¾åœ¨çº¿ AI æ‰©å›¾å·¥å…·ï¼Œå¯ä»¥å…è´¹ä½¿ç”¨ï¼Œåªéœ€ä¸Šä¼ å›¾ç‰‡å¹¶é€‰æ‹©æ‰©å±•æ¯”ä¾‹ï¼Œå³å¯ä¸€é”®å®Œæˆå›¾åƒæ‰©å±•ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OxKbdDZTX)\n[GOT-OCR2.0](https://huggingface.co/ucaslcl/GOT-OCR2_0) | ä¸€æ¬¾ç«¯åˆ°ç«¯çš„å¼€æº OCR æ¨¡å‹ï¼Œè¢«ç§°ä¸º OCR 2 . 0ï¼Œæ”¯æŒè¯†åˆ«åœºæ™¯æ–‡æœ¬ã€æ–‡æ¡£ã€ä¹è°±ã€å›¾è¡¨ã€æ•°å­¦å…¬å¼ç­‰å¤šç§å†…å®¹ï¼Œåœ¨ BLEU è¯„æµ‹ä¸­å–å¾— 0 . 972 çš„é«˜åˆ†ï¼Œæ¨¡å‹ä½“ç§¯ä»… 1 . 43GBã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OxgIFBROC)\n[Python Tutor](https://ç½‘é¡µé“¾æ¥) | é™¤äº† Python å¤–ï¼Œè¿˜æ”¯æŒ Javaã€Cã€C ++å’Œ JavaScript ç­‰ç¼–ç¨‹è¯­è¨€ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OwFav6ZaV)\n[Fish Speech](https://huggingface.co/spaces/fishaudio/fish-speech-1) | ä¸€æ¬¾å¼€æºçš„å¤šè¯­è¨€ TTS æ¨¡å‹ï¼Œä½¿ç”¨ 70 ä¸‡å°æ—¶æ•°æ®è®­ç»ƒï¼Œæ”¯æŒè‹±è¯­ã€ä¸­æ–‡ã€éŸ©è¯­ã€æ—¥è¯­ã€æ³•è¯­ã€å¾·è¯­ã€é˜¿æ‹‰ä¼¯è¯­å’Œè¥¿ç­ç‰™è¯­ 8 ç§è¯­è¨€ã€‚å…·æœ‰å¿«é€Ÿè¯­éŸ³åˆæˆã€ä½å»¶è¿Ÿã€å£°éŸ³ä¸°å¯Œã€æ··åˆè¯­è¨€å’Œè¯­éŸ³å…‹éš†ç­‰ç‰¹ç‚¹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OwwLfEVIn)\n[wcf.js](https://github.com/wechatferry/wechatferry) | ä¸€æ¬¾å¼ºå¤§ã€å¿«é€Ÿã€å¼€æºçš„å¾®ä¿¡æœºå™¨äººåº•å±‚æ¡†æ¶ï¼Œæä¾›æ¶ˆæ¯ç›‘å¬ã€å‘é€å’Œç¾¤èŠæ“ä½œçš„ä¸€ç«™å¼è§£å†³æ–¹æ¡ˆï¼Œå†…ç½®å¼ºå¤§æ˜“ç”¨çš„ Agent åº“å’Œå¼€å‘è€…å·¥å…·åŒ…ï¼Œå…·æœ‰é«˜æ•ˆç¨³å®šçš„å¤„ç†èƒ½åŠ›å’Œè‡ªç”±æ§åˆ¶å‘æ¶ˆæ¯é¢‘ç‡çš„åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OwaojcwHV)\n[markmap](https://github.com/markmap/markmap) | ä¸€æ¬¾å¼€æºå…è´¹çš„å·¥å…·ï¼Œå¯ä»¥å°† Markdown æ–‡æ¡£å†…å®¹è½¬æ¢ä¸ºç›´è§‚å¯è§†åŒ–çš„æ€ç»´å¯¼å›¾ï¼Œæ”¯æŒå›¾åƒã€é“¾æ¥ã€ä»£ç å—ã€å†…è”æ ·å¼å’Œæ•°å­¦å…¬å¼ç­‰æ ¼å¼å†…å®¹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OvU3a3PwQ)\n[Video2x](https://github.com/k4yt3x/video2x/) | ä¸€æ¬¾å¼€æºå…è´¹çš„æ— æŸæ”¾å¤§è§†é¢‘å’Œå›¾åƒå·¥å…·ï¼Œä½¿ç”¨å¤šç§å…ˆè¿›çš„è¶…åˆ†è¾¨ç‡ç®—æ³•å¦‚ waifu2xã€Anime4Kã€SRMD å’Œ RealSR ç­‰å®ç°è§†é¢‘/ GIF /å›¾åƒçš„æ— æŸæ”¾å¤§å’Œæé«˜å¸§é€Ÿç‡ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OvGIlvBh5)\n[PptxGenJS](https://github.com/gitbrent/PptxGenJS) | ä¸€æ¬¾åŸºäº JavaScript çš„å¼€æºåº“ï¼Œå¯ç”¨äºç”Ÿæˆ PPT æ¼”ç¤ºæ–‡ç¨¿ï¼Œæ”¯æŒæ·»åŠ å›¾è¡¨ã€è¡¨æ ¼ã€å›¾åƒã€è§†é¢‘ç­‰å¤šç§å…ƒç´ ï¼Œç”Ÿæˆçš„æ–‡ä»¶ä¸ PowerPoint ç­‰åº”ç”¨å…¼å®¹ï¼Œå¹¶æä¾› HTML è½¬ PPT åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OvCcgy1il)\n[DataEase](https://github.com/dataease/dataease) | ä¸€æ¬¾åŠŸèƒ½å¼ºå¤§çš„å¼€æºæ•°æ®å¯è§†åŒ–åˆ†æå·¥å…·ï¼Œå¯ä½œä¸º Tableau çš„æ›¿ä»£å“ï¼Œæä¾›ä¸°å¯Œç¾è§‚çš„å›¾è¡¨å±•ç¤ºã€åˆ¶ä½œå’Œæ•°æ®å¼•æ“ç­‰åŠŸèƒ½ï¼Œæ”¯æŒå¤šç§æ•°æ®é“¾æ¥ã€æ‹–æ‹½å¼å›¾è¡¨åˆ¶ä½œå’Œä¸ä»–äººåˆ†äº«ï¼Œå¹¶å…·å¤‡ AI è¾…åŠ©åˆ†æå’Œæ¨¡æ¿å¸‚åœºç­‰åˆ›æ–°åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Ovx7Hct4m)\n[MoneyPrinterPlus](https://github.com/ddean2009/MoneyPrinterPlus) | ä¸€æ¬¾å¼€æºå…è´¹çš„å…¨è‡ªåŠ¨åŒ– AI è§†é¢‘å‰ªè¾‘å·¥å…·ï¼Œå€ŸåŠ© AI æŠ€æœ¯ä¸€é”®ç”Ÿæˆå’Œæ‰¹é‡æ··å‰ªå„ç±»çŸ­è§†é¢‘ï¼Œæ”¯æŒè‡ªåŠ¨å‘å¸ƒåˆ°è§†é¢‘å¹³å°ï¼ŒåŠ©åŠ›å˜ç°ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OvrAIaFqb)\n[Melty](https://github.com/meltylabs/melty) | ä¸€æ¬¾ä¸“ä¸º 10x å·¥ç¨‹å¸ˆæ‰“é€ çš„å¼€æº AI ä»£ç ç¼–è¾‘å™¨ï¼Œèƒ½å¤Ÿä¸å¼€å‘è€…çš„æ•´ä¸ªå¼€å‘æµç¨‹é›†æˆï¼Œç†è§£ä»ç»ˆç«¯åˆ° GitHub çš„æ“ä½œï¼ŒååŠ©é«˜æ•ˆç¼–å†™å’Œé‡æ„ä»£ç ï¼Œæ”¯æŒå¤šæ–‡ä»¶å¤§è§„æ¨¡æ›´æ”¹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OvpglxhG4)\n[Easy Voice Toolkit](https://github.com/Spr-Aachen/Easy-Voice-Toolkit) | ä¸€å¥—åŠŸèƒ½ä¸°å¯Œçš„å¼€æº AI è¯­éŸ³å·¥å…·ç®±ï¼ŒåŒ…æ‹¬éŸ³é¢‘å¤„ç†ã€è¯­éŸ³è¯†åˆ«ã€è¯­éŸ³è½¬å½•ã€æ•°æ®é›†åˆ¶ä½œå’Œè¯­éŸ³åˆæˆç­‰ï¼Œå½¢æˆå®Œæ•´è¯­éŸ³æ¨¡å‹è®­ç»ƒå·¥ä½œæµã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Ov8JJd1Z3)\n[ChartDB](https://github.com/chartdb/chartdb) | ä¸€æ¬¾åŸºäº Web çš„åŠŸèƒ½å¼ºå¤§çš„æ•°æ®åº“å›¾è¡¨ç¼–è¾‘å™¨ï¼Œæ”¯æŒå¤šç§æ•°æ®åº“å¦‚ PostgreSQLã€MySQLã€SQL Server ç­‰ï¼Œæä¾›åŸºäº AI çš„å¯¼å‡ºåŠŸèƒ½ç”Ÿæˆ DDL è„šæœ¬æ–¹ä¾¿æ•°æ®åº“è¿ç§»ï¼Œå¯åœ¨çº¿ä½¿ç”¨æˆ–æœ¬åœ°éƒ¨ç½²ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Ov0uhu4Py)\n[FireCrawl](https://github.com/mendableai/firecrawl) | ä¸€æ¬¾å¼€æºä¸”å¼ºå¤§çš„ Web çˆ¬è™«å·¥å…·ï¼Œå¯çˆ¬å–ä»»ä½•ç½‘ç«™å†…å®¹å¹¶è½¬æ¢ä¸º Markdown æˆ–ç»“æ„åŒ–æ•°æ®ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹è®­ç»ƒã€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æä¾›æ•°æ®ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OuXaFn2ws)\n[MaxKB](https://github.com/1Panel-dev/MaxKB) | æ”¯æŒå¤šç§å¤§è¯­è¨€æ¨¡å‹å¯¹æ¥ï¼Œå†…ç½®å·¥ä½œæµå¼•æ“ç¼–æ’ AI è¿‡ç¨‹ï¼Œå¯æ— ç¼åµŒå…¥ç¬¬ä¸‰æ–¹ç³»ç»Ÿï¼Œåœ¨çŸ­æ—¶é—´å†…è·å¾— 9000 +æ˜Ÿã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Oux1JkeSB)\n[HivisionIDPhoto](https://github.com/Zeyi-Lin/HivisionIDPhotos) | ä¸€æ¬¾è½»é‡çº§çš„ AI è¯ä»¶ç…§åˆ¶ä½œå·¥å…·ï¼Œå¯è¯†åˆ«å¤šç§ç”¨æˆ·æ‹ç…§åœºæ™¯ï¼Œå®ç°æŠ å›¾å’Œç”Ÿæˆæ ‡å‡†å°ºå¯¸çš„è¯ä»¶ç…§ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Outin1JV2)\n[VideoLingo](https://github.com/Huanshere/VideoLingo) | ä¸€æ¬¾å…¨è‡ªåŠ¨è§†é¢‘ç¿»è¯‘çš„ AI å·¥å…·ï¼Œèƒ½å¤Ÿä¸€é”®å¯¹è§†é¢‘è¿›è¡Œå­—å¹•åˆ‡å‰²ã€ç¿»è¯‘ã€ç²¾å‡†å¯¹é½å’Œä¸ªæ€§åŒ–é…éŸ³ï¼Œç”Ÿæˆé«˜è´¨é‡çš„å­—å¹•å’Œé…éŸ³ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Ouarniyk4)\n[Cursor](https://cursor.directory/) | ä¸€æ¬¾æ™ºèƒ½ä»£ç ç¼–è¾‘ç¥å™¨ï¼Œæä¾›ä»£ç è¡¥å…¨ã€é”™è¯¯ä¿®å¤ç­‰åŠŸèƒ½ï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ã€‚è¯¥é¡¹ç›®æ”¶é›†äº†å„è¯­è¨€çš„æœ€ä½³é…ç½®æç¤ºè¯å’Œä½¿ç”¨æ•™ç¨‹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Ou3LmbyQh)\n[NarratoAI](https://github.com/linyqh/NarratoAI) | ä¸€æ¬¾ä¸€ç«™å¼è‡ªåŠ¨åŒ–å½±è§†è§£è¯´çš„ AI è§†é¢‘å‰ªè¾‘å·¥å…·ï¼ŒåŸºäº LLM å®ç°æ–‡æ¡ˆæ’°å†™ã€è‡ªåŠ¨åŒ–è§†é¢‘å‰ªè¾‘ã€é…éŸ³å’Œå­—å¹•ç”Ÿæˆï¼Œæ˜¯å½±è§†è§£è¯´å‰ªè¾‘ç¥å™¨ï¼Œå¯é«˜æ•ˆåˆ›ä½œå†…å®¹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OtVxY92hM)\n[voicechat2](https://github.com/lhl/voicechat2) | ä¸€æ¬¾å“åº”å¿«é€Ÿä¸”å®Œå…¨æœ¬åœ°åŒ–çš„ AI è¯­éŸ³èŠå¤©å·¥å…·ï¼Œä½¿ç”¨ WebSockets å®ç°ä½å»¶è¿Ÿè¯­éŸ³äº¤äº’å¹¶å…è®¸è¿œç¨‹è®¿é—®ï¼Œå¯æœ¬åœ°è¿è¡Œè¯­éŸ³è¯†åˆ«ã€æ–‡æœ¬è½¬è¯­éŸ³å’Œå¤§è¯­è¨€æ¨¡å‹ï¼Œåœ¨ 4090 æ˜¾å¡ä¸Šå»¶è¿Ÿä½è‡³ 300 æ¯«ç§’ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OtfSor20F)\n[CyberScraper 2077](https://github.com/itsOwen/CyberScraper-2077) | ä¸€æ¬¾åŸºäº OpenAI å¤§è¯­è¨€æ¨¡å‹çš„å¼ºå¤§ç½‘ç»œçˆ¬è™«å·¥å…·ï¼Œèƒ½å¤Ÿæ™ºèƒ½ç†è§£å’Œè§£æç½‘é¡µå†…å®¹ï¼Œæä¾›ç®€å•æ˜“ç”¨çš„å¯è§†åŒ–ç•Œé¢ï¼Œæ— éœ€ç¼–ç¨‹çŸ¥è¯†å³å¯æ“ä½œä½¿ç”¨ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Ot7P94c8X)\n[awesome-digital-human-live2d](https://github.com/wan-h/awesome-digital-human-live2d) | ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæ—¨åœ¨æ‰“é€ ä¸€ä¸ªæœ‰æ¸©åº¦çš„æ•°å­—äººï¼Œæ”¯æŒ Docker å¿«é€Ÿéƒ¨ç½²ã€Dify æœåŠ¡æ¥å…¥ã€ASRã€LLMã€TTSã€Agent æ¨¡å—åŒ–æ‰©å±•ã€Live2d äººç‰©æ¨¡å‹æ‰©å±•å’Œæ§åˆ¶ã€PC ç«¯å’Œç§»åŠ¨ç«¯ Web è®¿é—®ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OsX1p7nxr)\n[LLM-Aided OCR](https://github.com/Dicklesworthstone/llm_aided_ocr) | ä¸€æ¬¾åŸºäºå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å¼€æº OCR æ‰«æ PDF å·¥å…·ï¼Œå¯å°†åŸå§‹ OCR æ‰«æ PDF æ–‡æœ¬è½¬æ¢æˆé«˜å‡†ç¡®åº¦ã€æ ¼å¼æ­£ç¡®ä¸”æ˜“äºé˜…è¯»çš„ Markdown æ–‡æ¡£ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OsRkgr9JW)\n[Product Hunt Daily Hot](https://github.com/ViggoZ/producthunt-daily-hot) | ä¸€æ¬¾åŸºäº GitHub Action çš„è‡ªåŠ¨åŒ–å·¥å…·ï¼Œæ¯å¤©å®šæ—¶çˆ¬å–å¹¶ç”Ÿæˆ Product Hunt çƒ­é—¨äº§å“æ¦œå•çš„ Markdown æ–‡ä»¶ï¼Œä½¿ç”¨ GPT - 4 æ¨¡å‹ç¿»è¯‘äº§å“æè¿°ï¼Œå¸®åŠ©å¿«é€ŸæŸ¥çœ‹æ¯æ—¥çƒ­é—¨æ¦œå•ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OsQsKDY0a)\n[moffee](https://github.com/BMPixel/moffee) | ä¸€æ¬¾å¼€æºå…è´¹çš„ PPT åˆ¶ä½œå·¥å…·ï¼Œèƒ½å¤Ÿå°† Markdown æ–‡æ¡£ä¸€é”®è½¬æ¢ä¸ºå¹²å‡€ã€ä¸“ä¸šçš„å¹»ç¯ç‰‡ï¼Œæä¾› web ç•Œé¢å®æ—¶é¢„è§ˆæ•ˆæœã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OsFxEhRIP)\n[Linly-Dubbing](https://github.com/Kedreamix/Linly-Dubbing) | ä¸€æ¬¾å¼€æºä¸”å¼ºå¤§çš„è§†é¢‘å¤šè¯­è¨€ AI é…éŸ³/ç¿»è¯‘å·¥å…·ï¼Œèƒ½è‡ªåŠ¨ä¸‹è½½è§†é¢‘ã€ç¿»è¯‘å­—å¹•ã€äººå£°åˆ†ç¦»ã€å…‹éš†éŸ³è‰²é…éŸ³å¹¶åˆæˆè§†é¢‘ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Osw79mpN5)\n[Transformer Explainer](https://poloclub.github.io/transformer-explainer/) | ä¸€ä¸ªå¯è§†åŒ–äº¤äº’å¼å­¦ä¹ å·¥å…·ï¼Œä»¥ GPT - 2 ä¸ºä¾‹ï¼Œé€šè¿‡è¾“å…¥æ–‡æœ¬å®æ—¶è§‚å¯Ÿ Transformer å„ç»„ä»¶ï¼ˆåµŒå…¥å±‚ã€è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€MLP ç­‰ï¼‰çš„å·¥ä½œè¿‡ç¨‹å’Œé¢„æµ‹ä¸‹ä¸€ä¸ª Tokenï¼Œé€‚åˆæ•™å­¦å’Œç†è§£ Transformer å†…éƒ¨åŸç†ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OsmMgmpvX)\n[AI Scientist](https://github.com/SakanaAI/AI-Scientist) | ä¸€æ¬¾å…¨çƒé¦–ä¸ªè‡ªåŠ¨åŒ–ç§‘å­¦ç ”ç©¶ AI ç³»ç»Ÿï¼Œèƒ½å¤Ÿä»æ„æ€ã€ç¼–å†™ä»£ç ã€è¿›è¡Œå®éªŒã€æ€»ç»“ç»“æœåˆ°æ’°å†™å®Œæ•´è®ºæ–‡å’Œè¿›è¡ŒåŒè¡Œè¯„å®¡çš„å…¨è¿‡ç¨‹ï¼Œæ”¯æŒå¤šç§ä¸»æµå¤§è¯­è¨€æ¨¡å‹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Os6bNfuRq)\n[Clapper](https://github.com/jbilcke-hf/clapper) | ä¸€æ¬¾é›†æˆå¤šç§ AI åŠŸèƒ½çš„å¼€æºè§†é¢‘å‰ªè¾‘å·¥å…·ï¼Œå¯ä¸€é”®ç”Ÿæˆå›¾åƒã€è§†é¢‘ã€è¯­éŸ³ã€éŸ³ä¹ç­‰ç´ æï¼Œå¹¶è¿›è¡ŒäºŒæ¬¡å‰ªè¾‘ï¼Œè®©ç”¨æˆ·æ— éœ€ä¸“ä¸šæŠ€èƒ½å³å¯é€šè¿‡äº’åŠ¨è¿­ä»£çš„æ–¹å¼åˆ¶ä½œè§†é¢‘ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OrWLifEcS)\n[ai-renamer](https://github.com/ozgrozer/ai-renamer) | ä¸€æ¬¾åŸºäº Node . js çš„å‘½ä»¤è¡Œå·¥å…·ï¼Œå¯æ ¹æ®æ–‡ä»¶å†…å®¹è‡ªåŠ¨æ‰¹é‡é‡å‘½åæœ¬åœ°æ–‡ä»¶ã€å›¾åƒæˆ–è§†é¢‘ï¼Œé»˜è®¤ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹å¦‚ Gemmaã€Llama ç­‰è¿›è¡Œæ™ºèƒ½è¯†åˆ«ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Oq3TYgib9)\n[metahuman-stream](https://github.com/lipku/metahuman-stream) | ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œå®ç°å®æ—¶äº¤äº’æµå¼æ•°å­—äººï¼Œæ”¯æŒéŸ³è§†é¢‘åŒæ­¥å¯¹è¯ï¼Œå…·æœ‰å¤šç§åŠŸèƒ½å¦‚æ•°å­—äººæ¨¡å‹é€‰æ‹©ã€å£°éŸ³å…‹éš†ã€å¯¹è¯è¢«æ‰“æ–­å¤„ç†ã€å…¨èº«è§†é¢‘æ‹¼æ¥ã€æ¨æµã€è§†é¢‘ç¼–æ’å’Œå¤§è¯­è¨€æ¨¡å‹å¯¹è¯ç­‰ï¼ŒåŸºæœ¬å¯è¾¾åˆ°å•†ç”¨æ•ˆæœã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OpVQIrJFX)\n[PPTX2MD](https://github.com/ssine/pptx2md) | ä¸€ä¸ªå¼€æºå…è´¹çš„å·¥å…·ï¼Œèƒ½å¤Ÿå°† PPT å¹»ç¯ç‰‡è½¬æ¢ä¸º Markdown æ–‡ä»¶ï¼Œæ”¯æŒä¿ç•™æ ‡é¢˜ã€åˆ—è¡¨ã€ç²—ä½“ã€æ–œä½“ã€é¢œè‰²ã€è¶…é“¾æ¥ã€å›¾ç‰‡ã€è¡¨æ ¼ä»¥åŠåˆå¹¶å•å…ƒæ ¼ç­‰ PPT å†…å®¹æ ¼å¼ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OpPXpBoi2)\n[Fake Screenshot Generator](https://disksing.com/fake-screenshot) | ä¸€æ¬¾å¯ç›´æ¥åœ¨çº¿å…è´¹ä½¿ç”¨çš„å­—å¹•æˆªå›¾ç”Ÿæˆå™¨å·¥å…·ï¼Œå¯ç”Ÿæˆé€¼çœŸçš„å¸¦å­—å¹•çš„æˆªå›¾ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OpCZIs2Dd)\n[MinerU](https://github.com/opendatalab/MinerU) | ä¸€æ¬¾ä¸€ç«™å¼ã€å¼€æºã€é«˜è´¨é‡çš„æ•°æ®æå–å·¥å…·ï¼Œèƒ½å¤Ÿå°† PDFã€ç½‘é¡µä»¥åŠå¤šæ ¼å¼ç”µå­ä¹¦è½¬æ¢ä¸º Markdown æ ¼å¼ï¼Œæå–å›¾åƒã€è¡¨æ ¼å¹¶è½¬æ¢å…¬å¼ä¸º LaTexï¼Œæ”¯æŒå¤šç§è¯­è¨€è¯†åˆ«ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OpvVlCyPv)\n[Llama Tutor](https://github.com/Nutlope/llamatutor) | ä¸€æ¬¾å¼€æºå…è´¹çš„ AI ä¸ªäººå¯¼å¸ˆå·¥å…·ï¼ŒåŸºäº Next . js å’Œ Tailwind CSS æ„å»ºï¼Œé›†æˆ Llama 3 . 1 å’Œ Serper æä¾›å¼ºå¤§çš„æ¨ç†å’Œæœç´¢èƒ½åŠ›ï¼Œå¯æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„æ•™è‚²æ°´å¹³ç”Ÿæˆä¸ªæ€§åŒ–å¯¼å¸ˆè§£ç­”å„ç§é—®é¢˜ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OpmuQEURu)\n[bilingual_book_maker](https://github.com/yihong0618/bilingual_book_maker) | ä¸€æ¬¾åˆ©ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ ChatGPTã€Claude ç­‰ï¼‰å¯¹æ–‡ä»¶å’Œå›¾ä¹¦è¿›è¡Œå¤šè¯­è¨€ç¿»è¯‘çš„å¼€æºå·¥å…·ï¼Œæ”¯æŒ epubã€txt å’Œ srt ç­‰æ ¼å¼ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OoQtZtumW)\n[Claude Engineer](https://github.com/Doriandarko/claude-engineer) | ä¸€æ¬¾åŸºäº Claude 3 . 5 æ¨¡å‹çš„äº¤äº’å¼å‘½ä»¤è¡Œå·¥å…·ï¼Œæ•´åˆäº†æ–‡ä»¶ç³»ç»Ÿæ“ä½œã€ç½‘ç»œæœç´¢ç­‰åŠŸèƒ½ï¼Œå¯ååŠ©å®Œæˆå„ç§è½¯ä»¶å¼€å‘ä»»åŠ¡ï¼Œå¦‚æ™ºèƒ½ä»£ç åˆ†æã€ä¿®æ”¹å»ºè®®ã€é¡¹ç›®ç®¡ç†ç­‰ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OoKMRyN4F)\n[AudioNotes](https://github.com/harry0703/AudioNotes) | ä¸€æ¬¾å¼€æºå…è´¹çš„éŸ³è§†é¢‘è½¬ç»“æ„åŒ–ç¬”è®°å·¥å…·ï¼ŒåŸºäº FunASR å’Œ Qwen2 æ„å»ºï¼Œå¯å¿«é€Ÿæå–éŸ³è§†é¢‘å†…å®¹ï¼Œå¹¶åˆ©ç”¨å¤§æ¨¡å‹èƒ½åŠ›æ•´ç†æˆç»“æ„åŒ– Markdown ç¬”è®°ï¼Œæ–¹ä¾¿å¿«é€Ÿé˜…è¯»ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OoIvdqNoc)\n[Swapy](https://github.com/TahaSh/swapy) | ä¸€ä¸ªç®€å•çš„ JavaScript å¼€æºå·¥å…·ï¼Œåªéœ€å‡ è¡Œä»£ç å°±å¯ä»¥å°†ç½‘é¡µä¸Šä»»ä½•å¸ƒå±€è½¬æ¢ä¸ºå¯æ‹–åŠ¨äº¤æ¢å¸ƒå±€ï¼Œé€‚ç”¨äº Reactã€Vue ç­‰ä¸»æµå‰ç«¯æ¡†æ¶ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OoBmn2TwN)\n[Kspider](https://github.com/kkangert/kspider) | ä¸€æ¬¾å¼€æºå…è´¹çš„å¯è§†åŒ–çˆ¬è™«å¹³å°ï¼Œé€šè¿‡æµç¨‹å›¾é…ç½®å³å¯å®Œæˆæ•°æ®çˆ¬å–å·¥ä½œï¼Œæ— éœ€ç¼–å†™ä»£ç ï¼Œå¯¹å°ç™½æå…¶å‹å¥½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OoeM0ylVt)\n[AI Shell](https://github.com/BuilderIO/ai-shell) | ä¸€æ¬¾å¼€æºçš„ç»ˆç«¯ AI åŠ©æ‰‹ï¼Œå¯åœ¨å‘½ä»¤è¡Œç•Œé¢é€šè¿‡è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºå¯¹åº”çš„ Shell å‘½ä»¤æ‰§è¡Œï¼Œæ”¯æŒå¤šç§è¯­è¨€è¾“å…¥ï¼Œæé«˜ç»ˆç«¯æ“ä½œæ•ˆç‡ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OnGNolhEO)\n[Crawlee](https://github.com/apify/crawlee-python) | ä¸€æ¬¾å¼€æºå…è´¹ä¸”éå¸¸å¼ºå¤§çš„ Python ç½‘é¡µæŠ“å–å’Œæµè§ˆå™¨è‡ªåŠ¨åŒ–åº“ï¼Œæ—¨åœ¨æ„å»ºå¯é çš„çˆ¬è™«ä¸º AIã€å¤§è¯­è¨€æ¨¡å‹ã€æ£€ç´¢å¢å¼ºç”Ÿæˆæˆ– GPTs æå–æ•°æ®ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OntDwxRMQ)\n[Enchanted](https://github.com/AugustDev/enchanted) | ä¸€æ¬¾å¼€æºçš„æœ¬åœ°å¤§æ¨¡å‹ Mac å®¢æˆ·ç«¯ï¼Œç±»ä¼¼äº ChatGPT å®¢æˆ·ç«¯ï¼Œéœ€æ­é… Ollama ä½¿ç”¨ï¼Œå¯è½»æ¾è¿æ¥æœ¬åœ°éƒ¨ç½²çš„ç§æœ‰æ¨¡å‹ï¼Œå¦‚ Llama2ã€Mistral ä»¥åŠ Vicuna ç­‰ï¼Œæä¾›åœ¨ iOS ç”Ÿæ€ç³»ç»Ÿä¸­æ— è¿‡æ»¤ã€å®‰å…¨ã€ç§å¯†å’Œå¤šæ¨¡æ€çš„ä½¿ç”¨ä½“éªŒã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OnnWp76yF)\n[Whisper Timestamped](https://github.com/xenova/transformers.js/tree/v3/examples/whisper-word-timestamps) | ä¸€æ¬¾åŸºäº Transformers . js çš„æµè§ˆå™¨å†…è¯­éŸ³è¯†åˆ«å·¥å…·ï¼Œå¯æœ¬åœ°è¿è¡Œ whisper - base æ¨¡å‹è¿›è¡Œè¯­éŸ³è½¬æ–‡å­—ï¼Œæ”¯æŒ 100 ç§è¯­è¨€å¹¶ç”Ÿæˆå•è¯çº§æ—¶é—´æˆ³ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OnlAhtJnJ)\n[Vanna](https://github.com/vanna-ai/vanna) | ä¸€æ¬¾ SQL ç”Ÿæˆ AI æ¡†æ¶ï¼Œå…è®¸åœ¨è‡ªæœ‰æ•°æ®ä¸Šè®­ç»ƒ RAG æ¨¡å‹ï¼Œé€šè¿‡è‡ªç„¶è¯­è¨€å‡†ç¡®ç”Ÿæˆ SQL æŸ¥è¯¢è¯­å¥ï¼Œæ”¯æŒå¤šç§ä¸»æµå¤§æ¨¡å‹ï¼Œä½¿ç”¨ç®€å•ï¼Œå¼€æºå¯è‡ªè¡Œéƒ¨ç½²ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OmTiME6d3)\n[Great Tables](https://github.com/posit-dev/great-tables) | ä¸€æ¬¾ç”¨äºåˆ¶ä½œé«˜è´¨é‡è¡¨æ ¼çš„ Python åº“ï¼Œå†…ç½®ä¸°å¯Œçš„è¡¨æ ¼ç»„ä»¶å’Œæ ¼å¼åŒ–é€‰é¡¹ï¼Œå¯ä»¥ç»„åˆåˆ›å»ºå¤šç§ç±»å‹çš„ç²¾ç¾è¡¨æ ¼ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OmLfwjEBF)\n[Comic Translate](https://github.com/ogkalu2/comic-translate) | ä¸€æ¬¾åˆ©ç”¨ GPT - 4 è§†è§‰èƒ½åŠ›çš„å¼€æºæ¼«ç”»è‡ªåŠ¨ç¿»è¯‘ç¥å™¨ï¼Œæ”¯æŒå¤šç§æ ¼å¼å¦‚å›¾åƒã€PDFã€Epub ç­‰ï¼Œå¯å®ç°å¤šç§è¯­è¨€ä¹‹é—´äº’è¯‘ï¼ŒåŒ…æ‹¬æ—¥è¯­ã€è‹±è¯­ã€ä¸­æ–‡ã€éŸ©è¯­ã€å¾·è¯­ã€è·å…°è¯­ç­‰ï¼Œå¹¶æä¾›å¯è§†åŒ–æ“ä½œç•Œé¢ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OmpE3eQpQ)\n[CodeGeeX](https://codegeex.cn/zh-CN?article=24075) | ä¸€æ¬¾åŠŸèƒ½å…¨é¢çš„å›½äº§å¼€æºä»£ç æ¨¡å‹ï¼Œé›†æˆä»£ç è¡¥å…¨ã€ç”Ÿæˆã€é—®ç­”ã€è§£é‡Šã€å·¥å…·è°ƒç”¨ã€è”ç½‘æœç´¢ç­‰å¤šç§èƒ½åŠ›ï¼Œè¦†ç›–ç¼–ç¨‹å¼€å‘å„ç§åœºæ™¯ï¼Œåœ¨ç™¾äº¿å‚æ•°ä»¥ä¸‹æ€§èƒ½æœ€å¼ºã€‚åŒæ—¶ä¹Ÿæä¾› CodeGeeX æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹æ’ä»¶ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OmawqkiTI)\n[é‡è§æç™½](https://github.com/BinNong/meet-libai) | ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæ—¨åœ¨é€šè¿‡æ„å»ºæç™½çŸ¥è¯†å›¾è°±çš„ AI æ™ºèƒ½ä½“ï¼Œä»¥ç”Ÿæˆå¼å¯¹è¯åº”ç”¨çš„å½¢å¼æ¨å¹¿å’Œæ™®åŠæç™½å¤è¯—è¯æ–‡åŒ–ï¼Œæœ€ç»ˆç›®æ ‡æ˜¯å¼€å‘ä¸€æ¬¾ç”Ÿæˆå¼å¯¹è¯åº”ç”¨ï¼Œå®ç°å®æ—¶äº’åŠ¨å¹¶æä¾›ä¸ªæ€§åŒ–çš„æç™½è¯—æ­Œé‰´èµä½“éªŒã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Om2gZgi95)\n[WebDesignAgent](https://github.com/DAMO-NLP-SG/WebDesignAgent) | ä¸€æ¬¾åŸºäºäººå·¥æ™ºèƒ½çš„ç½‘ç«™æ„å»ºå·¥å…·ï¼Œæ”¯æŒå¤šé¡µé¢ç®¡ç†ã€ç”¨æˆ·è‡ªå®šä¹‰æ·»åŠ /åˆ é™¤ã€è¿­ä»£ä¼˜åŒ–å’Œè§†è§‰ä¼˜åŒ–ç­‰åŠŸèƒ½ï¼Œå¯é€šè¿‡æ–‡æœ¬ã€å›¾ç‰‡ã€è§†è§‰çº¿ç´¢æˆ–æ··åˆæ–¹å¼ç”Ÿæˆè®¾è®¡ç²¾ç¾çš„ç½‘ç«™ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OlIeVCnbi)\n[AI-YinMei](https://github.com/worm128/AI-YinMei) | ä¸€æ¬¾åŠŸèƒ½é½å…¨çš„ AI è™šæ‹Ÿä¸»æ’­ï¼ˆVtuberï¼‰å·¥å…·ï¼Œé›†æˆäº† FastGPT çŸ¥è¯†åº“èŠå¤©ã€è¯­éŸ³åˆæˆã€Stable Diffusion ç»˜ç”»ã€AI å”±æ­Œç­‰æŠ€æœ¯ï¼Œå¯å®ç°èŠå¤©ã€å”±æ­Œã€ç»˜ç”»ã€è·³èˆã€è¡¨æƒ…åˆ‡æ¢ã€æ¢è£…ã€æœå›¾ã€åœºæ™¯åˆ‡æ¢ç­‰å¤šç§åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OlxOia4uv)\n[OmniParse](https://github.com/adithya-s-k/omniparse) | ä¸€æ¬¾å¼€æºå·¥å…·ï¼Œèƒ½å¤Ÿå°†å„ç§éç»“æ„åŒ–æ•°æ®å¦‚æ–‡æ¡£ã€è¡¨æ ¼ã€å›¾åƒã€è§†é¢‘ã€éŸ³é¢‘å’Œç½‘é¡µè½¬åŒ–ä¸ºç»“æ„åŒ–çš„å¯æ“ä½œæ•°æ®ï¼Œæ–¹ä¾¿ç”¨äºæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å’Œå¾®è°ƒã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Olh8vj7To)\n[gptpdf](https://github.com/CosmosShadow/gptpdf) | ä¸€æ¬¾åŸºäº GPT - 4o è§†è§‰å¤§è¯­è¨€æ¨¡å‹çš„å¼€æºå·¥å…·ï¼Œä»…ç”¨ 293 è¡Œä»£ç å°±èƒ½å°† PDF æ–‡ä»¶è§£æä¸º Markdown æ ¼å¼ï¼Œå‡ ä¹å®Œç¾åœ°è§£ææ’ç‰ˆã€æ•°å­¦å…¬å¼ã€è¡¨æ ¼ã€å›¾ç‰‡å’Œå›¾è¡¨ç­‰å†…å®¹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OlgrhbhsQ)\n[Streamer-Sales](https://github.com/PeterH0323/Streamer-Sales) | ä¸€æ¬¾èƒ½å¤Ÿæ ¹æ®å•†å“ç‰¹ç‚¹è¿›è¡Œè§£è¯´çš„å–è´§ä¸»æ’­å¤§æ¨¡å‹ï¼Œé€šè¿‡ç”Ÿæˆç»†è…»ã€ç‹¬åˆ°çš„è§£è¯´è¯ï¼Œæ¿€å‘ç”¨æˆ·è´­ä¹°æ¬²æœ›ï¼Œæä¾›å¤šç§åŠŸèƒ½å¦‚æ–‡æ¡ˆç”Ÿæˆã€è¯­éŸ³è½¬æ¢ã€è§†é¢‘ç”Ÿæˆç­‰ï¼Œæ—¨åœ¨æˆä¸ºé”€å”®åŠ©æ‰‹ï¼Œæå‡é”€é‡å’Œç”¨æˆ·ä½“éªŒã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OkEft1zqt)\n[Wiseflow](https://github.com/TeamWiseFlow/wiseflow) | ä¸€æ¬¾æ•æ·çš„ä¿¡æ¯æŒ–æ˜å¼€æºå…è´¹å·¥å…·ï¼Œå¯ä»ç½‘ç«™ã€å…¬ä¼—å·ã€ç¤¾äº¤å¹³å°ç­‰æ¸ é“çˆ¬å–ç›¸å…³ä¿¡æ¯ï¼Œæ“…é•¿ä»å…¬ä¼—å·æ–‡ç« ä¸­æå–ä¿¡æ¯ã€‚æ”¯æŒå¯¹çˆ¬å–ä¿¡æ¯è¿›è¡Œç­›é€‰ã€æç‚¼ã€è´´æ ‡ç­¾ç­‰å¤„ç†ï¼Œå¹¶å¯æ•´åˆåˆ°ä»»æ„ Agent é¡¹ç›®ä¸­ä½œä¸ºåŠ¨æ€çŸ¥è¯†åº“ã€‚å¯å®Œå…¨æœ¬åœ°éƒ¨ç½²ï¼Œæ— éœ€ GPUï¼Œé€‚åˆä»»ä½•ç¡¬ä»¶ç¯å¢ƒã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OkBTkBzRA)\n[GPT Computer Assistant](https://github.com/onuratakan/gpt-computer-assistant) | ä¸€æ¬¾å¼€æºçš„ GPT å®¢æˆ·ç«¯åº”ç”¨ç¨‹åºï¼Œæ—¨åœ¨ä¸º Windows å’Œ Linux ç³»ç»Ÿæä¾›ç±»ä¼¼ macOS ä¸Š GPT åº”ç”¨çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬å±å¹•è¯»å–ã€éº¦å…‹é£é•¿éŸ³é¢‘è¾“å…¥ã€ç¼–å†™å’Œè¿è¡Œç¨‹åºä»¥åŠçŸ¥è¯†åº“ç®¡ç†ç­‰ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OkuOY0Env)\n[Diffutoon](https://github.com/modelscope/DiffSynth-Studio/blob/main/examples/Diffutoon/README.md) | ä¸€æ¬¾å¼€æºçš„è§†é¢‘åŠ¨æ¼«åŒ–é¡¹ç›®ï¼Œæä¾› Colab è¿è¡Œç¬”è®°ï¼Œæ— éœ€éƒ¨ç½²å³å¯ä¸€é”®åŠ¨æ¼«åŒ–è§†é¢‘ï¼Œè½¬æ¢åçš„è§†é¢‘ç”»é¢ç¨³å®šæµç•…ï¼Œè¿˜å¯ç¼–è¾‘å’Œæ·»åŠ è§†é¢‘æ•ˆæœã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OkssQgFpm)\n[RTranslator](https://github.com/niedev/RTranslator) | ä¸€æ¬¾å¼€æºå…è´¹ã€å¯ç¦»çº¿çš„å®æ—¶ç¿»è¯‘åº”ç”¨ï¼Œèƒ½å¤Ÿå®æ—¶å°†å¯¹æ–¹è¯´çš„è¯­è¨€ç¿»è¯‘æˆä½ èƒ½å¬æ‡‚çš„è¯­è¨€ï¼Œæ”¯æŒå¯¹è¯æ¨¡å¼ã€å¯¹è®²æ¨¡å¼å’Œæ–‡æœ¬ç¿»è¯‘æ¨¡å¼ï¼Œä½¿ç”¨ Meta çš„ NLLB å’Œ OpenAI çš„ Whisper è¿›è¡Œç¿»è¯‘å’Œè¯­éŸ³è¯†åˆ«ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Okr5B6urE)\n[AI Math Notes](https://github.com/ayushpai/AI-Math-Notes) | ä¸€æ¬¾åŸºäº Python å¼€å‘çš„äº¤äº’å¼ç»˜å›¾åº”ç”¨ç¨‹åºï¼Œå…è®¸ç”¨æˆ·åœ¨ç”»å¸ƒä¸Šæ‰‹å†™æ•°å­¦æ–¹ç¨‹ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®¡ç®—å¹¶æ˜¾ç¤ºç»“æœï¼Œå®ç°äº†ç±»ä¼¼è‹¹æœ"å¤‡å¿˜å½•æ•°å­¦è®¡ç®—å™¨"çš„åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OjqPulncY)\n[Whisper Web](https://huggingface.co/spaces/Xenova/whisper-web) | ä¸€æ¬¾åŸºäº OpenAI çš„ Whisper è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„å¼€æº Web åº”ç”¨ç¨‹åºï¼Œå…è®¸ç›´æ¥åœ¨æµè§ˆå™¨ä¸­è¿è¡Œä½¿ç”¨ Whisper è¿›è¡Œè¯­éŸ³è½¬æ–‡æœ¬ï¼Œæ”¯æŒ WebGPU åŠ é€Ÿï¼Œæ— éœ€åç«¯æœåŠ¡å™¨ï¼Œå¯å¯¼å‡º TXT å’Œ JSON æ ¼å¼æ–‡ä»¶ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Oj5Cn7Qqk)\n[GPT Academic](https://github.com/binary-husky/gpt_academic) | ä¸€æ¬¾åŠŸèƒ½å¼ºå¤§çš„ GPT å­¦æœ¯ä¼˜åŒ–å¼€æºé¡¹ç›®ï¼Œæä¾›è®ºæ–‡ç¿»è¯‘ã€ä»£ç è§£æã€çŸ¥è¯†è·å–ã€è¯­æ³•æ ¡å¯¹ã€è®ºæ–‡æ¶¦è‰²ã€æ‘˜è¦ç”Ÿæˆç­‰å¤šç§åŠŸèƒ½ï¼Œæ”¯æŒæ¥å…¥å¤šç§ LLM æ¨¡å‹ï¼Œæå‡å­¦æœ¯ç ”ç©¶æ•ˆç‡ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OiYyb1dgs)\n[Translation Agent](https://github.com/andrewyng/translation-agent) | ä¸€å¥—ç”±å´æ©è¾¾è€å¸ˆå¼€æºçš„ AI ç¿»è¯‘å·¥ä½œæµç¨‹ï¼ŒåŸºäºå¤§è¯­è¨€æ¨¡å‹è¿›è¡Œç¿»è¯‘ã€åæ€å’Œä¼˜åŒ–ï¼Œå¯æ§åˆ¶è¯­æ°”ã€åœ°åŒºå’Œæœ¯è¯­ç¿»è¯‘ä¸€è‡´æ€§ï¼Œç¿»è¯‘è´¨é‡åª²ç¾å•†ä¸šå·¥å…·ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OiDkSzO3J)\n[ScrapeGraphAI](https://github.com/VinciGit00/Scrapegraph-ai) | ä¸€æ¬¾åŸºäºå¤§è¯­è¨€æ¨¡å‹å’Œç›´æ¥å›¾é€»è¾‘çš„ AI ç½‘é¡µçˆ¬è™«å·¥å…·ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·æç¤ºè‡ªåŠ¨ä¸ºç½‘ç«™å’Œæœ¬åœ°æ–‡æ¡£åˆ›å»ºçˆ¬å–ç®¡é“ï¼Œæ”¯æŒå•é¡µã€å¤šé¡µå’Œè¯­éŸ³çˆ¬å–ï¼Œæé«˜æ•°æ®é‡‡é›†æ•ˆç‡ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OhVJA5eQR)\n[MiGPT](https://github.com/idootop/mi-gpt) | ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæ—¨åœ¨å°†å°ç±³å°çˆ±éŸ³ç®±æ‰“é€ æˆä¸“å±çš„ AI è¯­éŸ³åŠ©æ‰‹ï¼Œå°†å°çˆ±éŸ³ç®±å’Œç±³å®¶æ™ºèƒ½è®¾å¤‡ä¸ ChatGPT ç­‰å¤§æ¨¡å‹çš„ç†è§£èƒ½åŠ›å®Œç¾èåˆï¼Œæä¾› LLM å›ç­”ã€è§’è‰²æ‰®æ¼”ã€æµå¼å“åº”ã€é•¿çŸ­æœŸè®°å¿†ã€è‡ªå®šä¹‰ TTS å’Œæ™ºèƒ½å®¶å±… Agent ç­‰åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OhHoECwbO)\n[pyVideoTrans](https://github.com/jianchang512/pyvideotrans) | ä¸€æ¬¾å¼€æºçš„è§†é¢‘ç¿»è¯‘é…éŸ³å·¥å…·ï¼Œå¯å°†ä¸€ç§è¯­è¨€çš„è§†é¢‘è‡ªåŠ¨ç¿»è¯‘ä¸ºæŒ‡å®šè¯­è¨€çš„è§†é¢‘ï¼Œç”Ÿæˆå­—å¹•å’Œé…éŸ³ï¼Œæ”¯æŒå¤šç§ç¿»è¯‘æœåŠ¡å’Œé…éŸ³å¼•æ“ï¼Œå¯æœ¬åœ°ç¦»çº¿ä½¿ç”¨ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OhCSAkXdw)\n[cloudflare-ai-web](https://github.com/Jazee6/cloudflare-ai-web) | ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œåˆ©ç”¨ Cloudflare Workers AI å…è´¹æä¾›çš„å¤§æ¨¡å‹ï¼Œå¯å¿«é€Ÿæ­å»ºå¤šæ¨¡æ€ AI å¹³å°ï¼Œæ”¯æŒä¸€é”®éƒ¨ç½²ã€æ— éœ€æœåŠ¡å™¨ã€ä¸ªæ€§åŒ–å®šåˆ¶ç­‰ï¼Œé›†æˆäº† ChatGPTã€Stable Diffusion ç­‰å¤šç§ AI æ¨¡å‹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Ohsh2rkhe)\n[ChatTTS-ui](https://github.com/jianchang512/ChatTTS-ui) | ä¸€æ¬¾å¼€ç®±å³ç”¨çš„ ChatTTS å®‰è£…åŒ…ï¼Œæä¾› Web ç•Œé¢å’Œ API æ¥å£ï¼Œæ”¯æŒ Windowsã€Linuxã€macOS éƒ¨ç½²ï¼ŒWindows ç”¨æˆ·å¯ç›´æ¥ä¸‹è½½å®‰è£…åŒ…ä¸€é”®å®‰è£…ä½¿ç”¨ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OgO23Cn8P)\n[Perplexica](https://github.com/ItzCrazyKns/Perplexica) | ä¸€æ¬¾ç”± AI é©±åŠ¨çš„å¼€æºæœç´¢å·¥å…·ï¼Œèƒ½å¤Ÿæ·±å…¥äº’è”ç½‘æä¾›ç²¾å‡†ç­”æ¡ˆï¼Œç†è§£é—®é¢˜å¹¶ä¼˜åŒ–æœç´¢ç»“æœï¼Œæä¾›å¸¦å¼•ç”¨æ¥æºçš„æ˜ç¡®ç­”æ¡ˆã€‚å…·æœ‰éšç§ä¿æŠ¤ã€æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹æ”¯æŒã€åŒæ¨¡å¼æœç´¢å’Œä¸“æ³¨æ¨¡å¼ç­‰ç‰¹å¾ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OgkybrvOe)\n[RAGapp](https://github.com/ragapp/ragapp) | ä¸€æ¬¾æ— éœ€ç¼–ç çš„å¯è§†åŒ–ç•Œé¢å·¥å…·ï¼Œç”¨äºé…ç½®åŸºäº LlamaIndex æ„å»ºçš„ RAG èŠå¤©æœºå™¨äººï¼Œç±»ä¼¼äº OpenAI çš„ GPT æ¨¡å‹ï¼Œå¯è½»æ¾éƒ¨ç½²åœ¨è‡ªæœ‰äº‘åŸºç¡€è®¾æ–½ä¸­ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OgfDLz9xc)\n[MusicGPT](https://github.com/gabotechs/MusicGPT) | ä¸€æ¬¾å¼€æºå…è´¹çš„ AI éŸ³ä¹ç”Ÿæˆå™¨ï¼Œæ”¯æŒåœ¨ Windowsã€macOS å’Œ Linux ç³»ç»Ÿä¸Šæœ¬åœ°é«˜æ•ˆè¿è¡Œæœ€æ–°éŸ³ä¹ç”Ÿæˆ AI æ¨¡å‹ï¼Œå¦‚ Meta çš„ MusicGen æ¨¡å‹ï¼Œæ— éœ€å®‰è£…ä¾èµ–ï¼Œæ˜“äºä½¿ç”¨ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OgcuWezqY)\n[Khoj](https://github.com/khoj-ai/khoj) | ä¸€æ¬¾èåˆæœ¬åœ°æ–‡æ¡£å’Œåœ¨çº¿æœç´¢çš„ AI ç¬¬äºŒå¤§è„‘å·¥å…·ï¼Œå¯è¿æ¥ä¸ªäººçŸ¥è¯†åº“ã€æä¾›å¼ºå¤§æœç´¢å¼•æ“ã€æ”¯æŒåœ¨çº¿ç¦»çº¿ä½¿ç”¨ï¼Œå¹¶æä¾›å®šåˆ¶ AI æ™ºèƒ½ä»£ç†ååŠ©å®Œæˆä»»åŠ¡ï¼Œå®Œå…¨å¼€æºå…è´¹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OfW04hxRS)\n[Talk With Gemini](https://github.com/Amery2010/TalkWithGemini) | ä¸€æ¬¾å¯ä¸€é”®å…è´¹éƒ¨ç½²çš„ç§äºº Gemini åº”ç”¨ç¨‹åºï¼Œæ”¯æŒæœ€æ–°çš„ Gemini æ¨¡å‹ï¼Œå¦‚ Gemini 1 . 5 Proã€Gemini 1 . 5 Flash ç­‰ï¼Œå…·æœ‰å¤šæ¨¡æ€æ”¯æŒã€è¯­éŸ³æ¨¡å¼ã€è§†è§‰è¯†åˆ«ã€åŠ©ç†å¸‚åœºã€Markdown æ”¯æŒã€ä¸Šä¸‹æ–‡å‹ç¼©ã€éšç§å®‰å…¨ã€ç²¾å¿ƒè®¾è®¡çš„ UI ç­‰ç‰¹æ€§ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OfKdrkyjt)\n[ugly-avatar](https://github.com/txstc55/ugly-avatar) | ä¸€ä¸ªå¼€æºå…è´¹çš„æ½¦è‰å¤´åƒç”Ÿæˆå™¨ï¼Œå¯ä»¥ç”Ÿæˆæå…·ç‰¹è‰²çš„å¤´åƒï¼Œå—åˆ°è®¸å¤šç½‘å‹çš„å–œçˆ±ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Ofki4s5aO)\n[DashPlayer](https://github.com/solidSpoon/DashPlayer) | ä¸€æ¬¾ä¸“ä¸ºè‹±è¯­å­¦ä¹ æ‰“é€ çš„è§†é¢‘æ’­æ”¾å™¨ï¼Œæä¾›åŒè¯­å­—å¹•ã€æŒ‰å­—å¹•è·³è½¬ã€æŸ¥è¯æŸ¥è¯¢ã€å¯è°ƒæ•´ç•Œé¢å°ºå¯¸ã€è®°å½•æ’­æ”¾ä½ç½®ã€è“ç‰™é¥æ§æ“ä½œã€å¤œé—´æ¨¡å¼ã€AI å­—å¹•ç”Ÿæˆã€é•¿è§†é¢‘åˆ‡åˆ†å’Œè§†é¢‘ä¸‹è½½ç­‰åŠŸèƒ½ï¼ŒåŠ©åŠ›é€šè¿‡è§‚çœ‹è§†é¢‘æå‡è‹±è¯­æ°´å¹³ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Ofc2DBgRT)\n[OpenGlass](https://github.com/BasedHardware/openglass) | ä¸€æ¬¾åŸºäºå¼€æºç¡¬ä»¶çš„ AI æ™ºèƒ½çœ¼é•œç³»ç»Ÿï¼Œåªéœ€ 20 ç¾å…ƒçš„æˆæœ¬å³å¯å°†æ™®é€šçœ¼é•œå‡çº§ä¸ºæ™ºèƒ½çœ¼é•œï¼Œå®ç°è®°å½•ç”Ÿæ´»ã€è¯†åˆ«ç‰©ä½“ã€è®¡ç®—å¡è·¯é‡Œã€å®æ—¶ç¿»è¯‘ç­‰å¤šé¡¹ AI åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Oe6S6wjjP)\n[PicProse](https://github.com/gezhaoyou/picprose) | ä¸€æ¬¾å¼€æºå…è´¹çš„å°é¢å›¾ç‰‡åˆ¶ä½œå·¥å…·ï¼Œèƒ½å¸®åŠ©ç”¨æˆ·è½»æ¾ä¸º Mediumã€Wordpressã€å¾®ä¿¡ç­‰å¹³å°çš„æ–‡ç« åˆ¶ä½œç²¾ç¾å°é¢ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OaVb6EzZm)\n[Reader](https://github.com/jina-ai/reader) | ä¸€æ¬¾å¼€æºå…è´¹çš„å·¥å…·ï¼Œä¸“é—¨ç”¨äºå°†ç½‘é¡µå†…å®¹è½¬æ¢ä¸º Markdown æ ¼å¼ï¼Œæ–¹ä¾¿å°†ç½‘é¡µå†…å®¹æ•´åˆåˆ°çŸ¥è¯†åº“ä¸­ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Oa7PjB0mS)\n[SuperMemory](https://github.com/Dhravya/supermemory) | ä¸€æ¬¾å¼€æºå…è´¹çš„ä¸ªäººçŸ¥è¯†ç®¡ç†å·¥å…·ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·è½»æ¾æ”¶é›†å’Œç®¡ç†ç½‘ä¸Šçš„æœ‰ä»·å€¼å†…å®¹ï¼Œé€šè¿‡ AI å¿«é€ŸæŸ¥æ‰¾å’Œå›é¡¾å·²ä¿å­˜çš„å†…å®¹ï¼Œæ‰“é€ å±äºè‡ªå·±çš„"ç¬¬äºŒå¤§è„‘"ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Oa0kHfLaq)\n[STORM](https://storm.genie.stanford.edu/) | ä¸€æ¬¾ç”±æ–¯å¦ç¦å¤§å­¦å¼€å‘çš„åˆ›æ–°å‹ AI å†™ä½œå·¥å…·ï¼Œå¯æ ¹æ®ä¸»é¢˜è‡ªåŠ¨æ”¶é›†ä¿¡æ¯ã€åˆ›å»ºå¤§çº²ï¼Œæ¨¡æ‹Ÿä¸“å®¶å¯¹è¯å¹¶æ’°å†™å®Œæ•´æ–‡ç« ï¼Œå¸®åŠ©ç¼–å†™å‡ºå…·æœ‰æ·±åº¦å’Œå¹¿åº¦çš„é«˜è´¨é‡å†…å®¹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O9vH5ciur)\n[LLocalSearch](https://github.com/nilsherzig/LLocalSearch) | ä¸€æ¬¾å®Œå…¨æœ¬åœ°åŒ–çš„ AI æœç´¢é›†æˆå·¥å…·ï¼Œæ— éœ€ä¾èµ–ä»»ä½•å¤–éƒ¨ APIï¼Œé€šè¿‡ LLM Agents å¯»æ‰¾ç­”æ¡ˆï¼Œæ”¯æŒä½é…ç½®ç¡¬ä»¶è¿è¡Œï¼Œæä¾›è¯¦ç»†è¿›åº¦æ—¥å¿—å’Œåç»­é—®é¢˜äº¤äº’ï¼Œç•Œé¢ç¾è§‚æ”¯æŒæµ…è‰²æ·±è‰²ä¸»é¢˜ï¼Œæ”¯æŒ Docker Compose éƒ¨ç½²ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O8XwhBOt1)\n[FreeAskInternet](https://github.com/nashsu/FreeAskInternet) | ä¸€æ¬¾å¼€æºå…è´¹çš„åŸºäºæœç´¢çš„é—®ç­” AI å·¥å…·ï¼Œå¯æœ¬åœ°è¿è¡Œæ— éœ€ GPU ç¡¬ä»¶æ”¯æŒï¼Œé€šè¿‡å¤šå¼•æ“æœç´¢å¹¶åˆ©ç”¨ GPT - 3 . 5 å¤„ç†ç»“æœç”Ÿæˆç­”æ¡ˆï¼Œç¡®ä¿éšç§å®‰å…¨ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O8EFinFty)\n[poster-design](https://github.com/palxiao/poster-design) | ä¸€æ¬¾å¼€æºçš„åœ¨çº¿åˆ›æ„å›¾ç‰‡ç¼–è¾‘å™¨ï¼Œé€‚ç”¨äºæµ·æŠ¥å›¾ç‰‡ç”Ÿæˆã€ç”µå•†åˆ†äº«å›¾ã€æ–‡ç« é•¿å›¾ç­‰åœºæ™¯ï¼Œæä¾›ä¸°å¯Œçš„ç¼–è¾‘åŠŸèƒ½ï¼Œå¦‚å…ƒç´ æ‹–æ‹½ã€å›¾ç‰‡ç¼–è¾‘ã€SVG ç¼–è¾‘ã€ç”»å¸ƒè‡ªå®šä¹‰ç­‰ï¼Œæ— éœ€å®¢æˆ·ç«¯å³å¯è½»æ¾å®Œæˆå›¾æ–‡æ’ç‰ˆã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O8DueqJmG)\n[Open WebUI](https://github.com/open-webui/open-webui) | ä¸€æ¬¾ä¸“ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®¾è®¡çš„å¯æ‰©å±•ä¸”åŠŸèƒ½ä¸°å¯Œçš„ Web ç•Œé¢ï¼Œæ”¯æŒè¿è¡Œå„ç§ LLM æ¨¡å‹ï¼ŒåŒ…æ‹¬ Ollamaã€OpenAI å…¼å®¹ APIã€Geminiã€Groqã€Claude ç­‰ï¼Œæä¾›ç›´è§‚ç•Œé¢ã€å“åº”å¼è®¾è®¡ã€ä»£ç è¯­æ³•é«˜äº®ã€Markdown å’Œ LaTeX æ”¯æŒã€æœ¬åœ°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰é›†æˆã€å¤šæ¨¡å‹æ”¯æŒä¸å¤šæ¨¡æ€äº’åŠ¨ã€å®‰å…¨å’Œå¤šç”¨æˆ·ç®¡ç†ç­‰ä¸°å¯ŒåŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O7TwOm5sS)\n[Douyin_TikTok_Download_API](https://github.com/Evil0ctal/Douyin_TikTok_Download_API) | ä¸€æ¬¾å¼€æºå…è´¹çš„é«˜æ€§èƒ½å¼‚æ­¥æ•°æ®çˆ¬å–å·¥å…·ï¼Œæ”¯æŒ API è°ƒç”¨å’Œåœ¨çº¿æ‰¹é‡è§£æåŠä¸‹è½½æ— æ°´å°è§†é¢‘æˆ–å›¾é›†ï¼Œè¦†ç›–æŠ–éŸ³ã€å¿«æ‰‹ã€TikTokã€B ç«™ç­‰å¤šä¸ªè§†é¢‘å¹³å°ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O7K6jeyVS)\n[AI Comic Factory](https://aicomicfactory.app/) | ä¸€æ¬¾åŸºäº LLM + SDXL æŠ€æœ¯çš„å¼€æºå…è´¹åœ¨çº¿ AI æ¼«ç”»ç”Ÿæˆå·¥å…·ï¼Œå¯è‡ªåŠ¨ç”Ÿæˆç²¾ç¾æ¼«ç”»ä½œå“ï¼Œæ”¯æŒè‡ªå®šä¹‰ LLM å’Œå›¾åƒç”Ÿæˆæ¨¡å‹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O7IVfn7ua)\n[Suno-API](https://github.com/SunoAI-API/Suno-API) | ä¸€ä¸ªéå®˜æ–¹çš„ Suno AI API é¡¹ç›®ï¼Œæ”¯æŒç”Ÿæˆæ­Œæ›²ã€æ­Œè¯ç­‰åŠŸèƒ½ï¼Œå…·æœ‰è‡ªåŠ¨ç»´æŠ¤ tokenã€å…¨å¼‚æ­¥å“åº”ã€ä»£ç ç®€å•æ˜“ç»´æŠ¤ç­‰ç‰¹ç‚¹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O7gDL4J5k)\n[Claude-Journalist](https://github.com/mshumer/ai-journalist) | ä¸€æ¬¾åŸºäº Claude 3 çš„ AI å†™ä½œåŠ©æ‰‹ï¼Œå¯æ ¹æ®è¾“å…¥ä¸»é¢˜è‡ªåŠ¨æœç´¢ç›¸å…³ä¿¡æ¯ã€æ’°å†™å’Œç¼–è¾‘é«˜è´¨é‡æ–‡ç« ï¼Œæé«˜å†™ä½œæ•ˆç‡ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O6TRedZob)\n[MoneyPrinterTurbo](https://github.com/harry0703/MoneyPrinterTurbo) | ä¸€æ¬¾å¼€æºå…è´¹çš„ AI å·¥å…·ï¼Œå¯ä»¥æ ¹æ®æä¾›çš„ä¸»é¢˜æˆ–å…³é”®è¯è‡ªåŠ¨ç”Ÿæˆè§†é¢‘æ–‡æ¡ˆã€ç´ æã€å­—å¹•å’ŒèƒŒæ™¯éŸ³ä¹ï¼Œå¹¶åˆæˆé«˜æ¸…çŸ­è§†é¢‘ï¼Œæ”¯æŒå¤šç§åŠŸèƒ½å¦‚è‡ªå®šä¹‰æ–‡æ¡ˆã€è§†é¢‘å°ºå¯¸ã€æ‰¹é‡ç”Ÿæˆã€è¯­éŸ³åˆæˆã€å­—å¹•è®¾ç½®ç­‰ï¼Œæé«˜çŸ­è§†é¢‘åˆ¶ä½œæ•ˆç‡ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O6Omh9qzQ)\n[LiveStory](https://github.com/zeke/livestory) | ä¸€æ¬¾åˆ©ç”¨ AI è¿›è¡Œå®æ—¶è¯­éŸ³ç»˜å›¾çš„å·¥å…·ï¼Œå°†ä¼ ç»Ÿçš„æ–‡æœ¬è¾“å…¥æ¢æˆè¯­éŸ³è¾“å…¥ï¼Œå®æ—¶ç”Ÿæˆå›¾åƒï¼Œæ”¯æŒè¯­éŸ³æ§åˆ¶ç»˜å›¾ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O6cO4n2jo)\n[Buzz](https://github.com/chidiwilliams/buzz) | ä¸€æ¬¾å¼€æºå…è´¹ã€ç®€æ´ã€å¯ç¦»çº¿çš„éŸ³é¢‘è½¬å½•å’Œç¿»è¯‘å·¥å…·ï¼ŒåŸºäº OpenAI Whisper å¼€å‘ï¼Œæ”¯æŒæ‹–æ”¾å¯¼å…¥éŸ³è§†é¢‘æ–‡ä»¶è¿›è¡Œè½¬å½•å’Œç¿»è¯‘ï¼Œè½¬å½•æ–‡æœ¬å¯å¯¼å‡ºå¤šç§æ ¼å¼ï¼Œæ”¯æŒå¤šç§è¯­éŸ³è¯†åˆ«æ¡†æ¶å’Œå‘½ä»¤è¡Œæ“ä½œã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O5Rwp4vbC)\n[MediaCrawler](https://github.com/NanmiCoder/MediaCrawler) | ä¸€æ¬¾å¼€æºçš„è‡ªåª’ä½“çˆ¬è™«å·¥å…·ï¼Œæ”¯æŒå°çº¢ä¹¦ã€æŠ–éŸ³ã€å¿«æ‰‹ã€B ç«™ã€å¾®åšç­‰å¹³å°çš„è§†é¢‘ã€å›¾ç‰‡ã€è¯„è®ºã€ç‚¹èµã€è½¬å‘æ•°æ®æŠ“å–ï¼Œå…·å¤‡å¤šç§ç™»å½•æ–¹å¼ã€æŒ‡å®šæ•°æ®çˆ¬å–ã€IP ä»£ç†æ± å’Œå¤šç§æ•°æ®æ ¼å¼ä¿å­˜ç­‰åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O5AWioP9X)\n[å°æµ£ç†Šæ™ºèƒ½åŠ©æ‰‹](http://t.cn/A6YeOe86) | ä¸€æ¬¾ç”±å•†æ±¤ç§‘æŠ€ç ”å‘çš„åŠŸèƒ½å…¨é¢çš„å›½äº§ AI å·¥å…·ï¼Œå†…ç½®ä»£ç æ¨¡å¼å’ŒåŠå…¬æ¨¡å¼ï¼Œå¯å¤§å¹…æå‡ç¼–ç¨‹å¼€å‘å’Œåä½œåŠå…¬æ•ˆç‡ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O4QYSysWr)\n[Comflowy](https://github.com/6174/comflowyspace) | ä¸€æ¬¾é’ˆå¯¹ Mac ç”¨æˆ·ä¼˜åŒ–çš„ ComfyUI æ•´åˆåŒ…ï¼Œæä¾›ä¸€é”®éƒ¨ç½²ã€æ¨¡å‹ç®¡ç†ã€å·¥ä½œæµç®¡ç†å’Œæ‰©å±•ç®¡ç†ç­‰åŠŸèƒ½ï¼Œä½¿ç”¨ä½“éªŒæ¯”å®˜æ–¹ ComfyUI æ›´ä½³ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O3D4eDi2N)\n[Inpaint-web](https://github.com/lxfater/inpaint-web/tree/main) | ä¸€æ¬¾åŸºäº WebGPU æŠ€æœ¯å¼€å‘çš„å¼€æºå…è´¹å›¾åƒä¿®å¤å’Œæ”¾å¤§å·¥å…·ï¼Œå¯ç›´æ¥åœ¨æµè§ˆå™¨ä¸Šè¿è¡Œï¼Œæ— éœ€å®¢æˆ·ç«¯ï¼Œæä¾›å›¾åƒå±€éƒ¨æ“¦é™¤ä¿®å¤å’Œè¶…åˆ†è¾¨ç‡æ”¾å¤§åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O3t0QckJj)\n[AutoPrompt](https://github.com/Eladlev/AutoPrompt) | ä¸€ä¸ªè‡ªåŠ¨ä¼˜åŒ–æç¤ºè¯çš„å¼€æºæ¡†æ¶ï¼Œå¯æ ¹æ®ç”¨æˆ·æ„å›¾ç”Ÿæˆé«˜è´¨é‡è¯¦ç»†çš„æç¤ºè¯ï¼Œå¹¶é€šè¿‡è¿­ä»£æ•°æ®é›†ä¸æ–­ä¼˜åŒ–æç¤ºè¯ï¼Œå‡å°‘å·¥ä½œé‡ï¼Œè§£å†³æ•æ„Ÿæ€§å’Œæ­§ä¹‰æ€§é—®é¢˜ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O2QI6s9hf)\n[é’æ¢§å­—å¹•](https://github.com/melon/qingwu-zimu) | ä¸€æ¬¾åŸºäº Whisper çš„å¼€æºå­—å¹•æå–å·¥å…·ï¼Œæ”¯æŒæ™ºèƒ½æå–ã€ç¼–è¾‘å’Œé«˜è´¨é‡ç¿»è¯‘å­—å¹•ï¼Œå¯ç”Ÿæˆå•/åŒè¯­å­—å¹•å¹¶é€‰æ‹©å¤šç§æ ¼å¼ä¸‹è½½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O2IGr0tzU)\n[Chat with MLX](https://github.com/qnguyen3/chat-with-mlx) | ä¸€æ¬¾å¼€æºçš„æœ¬åœ° AI å¯¹è¯åŠ©æ‰‹å·¥å…·ï¼Œå¯ä¸æœ¬åœ°æ•°æ®è¿›è¡Œäº¤äº’ï¼Œæ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼å’Œè¯­è¨€ï¼Œå¹¶å¯é›†æˆå¤šä¸ªå¼€æºå¤§è¯­è¨€æ¨¡å‹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O2Ijy2cy3)\n[StickerBaker](https://github.com/fofr/cog-stickers) | ä¸€æ¬¾å¼€æºçš„ AI è´´çº¸åˆ¶ä½œå·¥å…·ï¼Œç»“åˆäº† Stickers SDXL Lora å’Œ BRIA èƒŒæ™¯ç§»é™¤å·¥å…·ï¼Œåªéœ€è¾“å…¥ç®€å•çš„æç¤ºè¯ï¼Œå³å¯å¿«é€Ÿç”Ÿæˆé«˜æ¸…ç²¾ç¾çš„è´´çº¸ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O2yomp6bK)\n[OOTDiffusion](https://github.com/levihsu/OOTDiffusion) | ä¸€æ¬¾å¼€æºçš„ AI è™šæ‹Ÿè¯•è¡£å·¥å…·ï¼Œæ”¯æŒä¸€é”®è¯•ç©¿ä¸ŠåŠèº«/ä¸‹åŠèº«/è¿è¡£è£™ï¼Œæœè£…ä¸æ¨¡ç‰¹è´´åˆè‡ªç„¶ï¼Œè¯•ç©¿æ•ˆæœé€¼çœŸã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O1CSjxOJV)\n[MoneyPrinter](http://t.cn/A6YSFjDa) | ä¸€æ¬¾è‡ªåŠ¨åŒ–åˆ›å»ºçŸ­è§†é¢‘çš„å·¥å…·ï¼Œå¯æ ¹æ®ä¸»é¢˜è‡ªåŠ¨ç”Ÿæˆå¸¦é…ä¹åŠå­—å¹•çš„è§†é¢‘ï¼Œå¹¶ä¸Šä¼ åˆ° YouTube è·å–æ”¶ç›Šã€‚æ”¯æŒä½¿ç”¨ GPTã€DALL - E ç­‰å¤§æ¨¡å‹ç”Ÿæˆè„šæœ¬å’Œå›¾åƒã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O1m9sxX52)\n[NotesGPT](https://github.com/Nutlope/notesGPT) | ä¸€æ¬¾å¼€æºå…è´¹çš„è¯­éŸ³ç¬”è®°å·¥å…·ï¼Œç•Œé¢ç®€æ´ï¼Œæ”¯æŒè¯­éŸ³è¾“å…¥è½¬æ–‡å­—å’Œè‡ªåŠ¨æ€»ç»“ï¼Œå†…ç½® Mixtral LLM å’Œ Whisper è½¬å½•å¼•æ“ï¼Œæ”¯æŒä¸­è‹±æ··åˆè¾“å…¥ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O1kYomKnv)\n[Reor](https://github.com/reorproject/reor) | ä¸€æ¬¾ç”± AI é©±åŠ¨çš„å¼€æºå…è´¹æ¡Œé¢ç¬”è®°å·¥å…·ï¼Œèƒ½å¤Ÿæ™ºèƒ½æ•´ç†å’Œå½’çº³ç¬”è®°å†…å®¹ï¼Œè‡ªåŠ¨å…³è”ç›¸å…³æƒ³æ³•ï¼Œå†…ç½® LLM æä¾›é—®ç­”å’Œè¯­ä¹‰æœç´¢åŠŸèƒ½ï¼Œå¯ä½œä¸ºä¸ªäººçŸ¥è¯†ç®¡ç†å·¥å…·ä½¿ç”¨ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O1jXYpMNd)\n[Noi](https://github.com/lencx/Noi) | ä¸€æ¬¾å¼€æºçš„é«˜é¢œå€¼ AI å®¢æˆ·ç«¯ï¼Œé›†æˆäº† ChatGPTã€Claudeã€Bardã€Poe ç­‰ä¸»æµ AI æ¨¡å‹ï¼Œå¹¶å†…ç½® GitHubã€HuggingFace å’Œ VS Code ç­‰å·¥å…·ï¼Œæ”¯æŒè‡ªå®šä¹‰ Prompt ç®¡ç†ã€å¤šè¯­è¨€ã€å¤šä¸»é¢˜ç­‰åŠŸèƒ½ï¼Œå¯åœ¨ MacOSã€Windowsã€Linux ç³»ç»Ÿä¸Šå®‰è£…ä½¿ç”¨ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NF1I7z18y)\n[ChatGPT Web + Midjourney Proxy](https://github.com/Dooy/chatgpt-web-midjourney-proxy) | ä¸€æ¬¾åŠŸèƒ½å…¨é¢çš„é•œåƒç«™ï¼Œæ”¯æŒ AI å¯¹è¯ã€AI æ¢è„¸ã€Midjourneyã€GPTsã€TTS Whisper ç­‰å¤šç§ AI åŠŸèƒ½ï¼Œæä¾› Vercel ä¸€é”®éƒ¨ç½²ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NEIagC5Iw)\n[ChatGemini](https://github.com/bclswl0827/ChatGemini) | ä¸€æ¬¾å¼€æºçš„ç®€æ´ Google Gemini ç½‘é¡µå®¢æˆ·ç«¯ï¼Œç•Œé¢å’Œäº¤äº’ç±»ä¼¼ ChatGPTï¼Œæ”¯æŒåœ¨å¯¹è¯æ¡†ä¸­ä¸Šä¼ å›¾ç‰‡å¹¶è‡ªåŠ¨è°ƒç”¨ Gemini - Pro - Vision æ¨¡å‹è¿›è¡Œå›¾åƒè¯†åˆ«ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NExFt37tW)\n[WhisperFusion](https://github.com/collabora/WhisperFusion) | ä¸€æ¬¾åŸºäº WhisperLive å’Œ WhisperSpeech æ„å»ºçš„ AI å¯¹è¯ç³»ç»Ÿï¼Œé›†æˆäº† Mistral å¤§è¯­è¨€æ¨¡å‹ï¼Œå®ç°äº†æä½å»¶è¿Ÿçš„è¯­éŸ³ä¸ AI å¯¹è¯ä½“éªŒï¼Œå¹¶é€šè¿‡ TensorRT å¼•æ“ä¼˜åŒ–å’Œ torch . compile æŠ€æœ¯æå‡äº†è¿è¡Œæ•ˆç‡ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NEqulvSJE)\n[search_with_lepton](https://github.com/leptonai/search_with_lepton) | ä¸€æ¬¾ä½¿ç”¨ä¸åˆ° 500 è¡Œä»£ç æ„å»ºçš„å¯¹è¯å¼æœç´¢å¼•æ“ï¼Œå®ç°äº†ç±»ä¼¼ Perplexity çš„æ•ˆæœï¼Œé›†æˆäº†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLlama2ã€Mixtralï¼‰å’Œæœç´¢å¼•æ“ï¼ˆæ”¯æŒ Bingã€Google æœç´¢ï¼‰ï¼Œå…·æœ‰å¯è‡ªå®šä¹‰çš„ä¸ªæ€§åŒ– UI ç•Œé¢ï¼Œæ”¯æŒæœç´¢ç»“æœçš„å…±äº«å’Œç¼“å­˜ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NEmX9xo03)\n[AIçº¢åŒ…å°é¢ç”Ÿæˆå™¨](https://github.com/all-in-aigc/aicover) | ä¸€æ¬¾åŸºäº DALL - E 3 çš„ AI çº¢åŒ…å°é¢ç”Ÿæˆå·¥å…·ï¼Œæ”¯æŒè‡ªå®šä¹‰ç”Ÿæˆçº¢åŒ…å°é¢å›¾åƒï¼Œé‡‡ç”¨ Next . js å…¨æ ˆå¼€å‘ï¼Œé›†æˆäº†å¤šç§åŠŸèƒ½å¦‚è°·æ­Œç™»å½•ã€å›¾ç‰‡ä¸Šä¼ ã€æ”¯ä»˜ç­‰ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NEf2A0zm1)\n[Zed](https://github.com/zed-industries/zed) | ä¸€æ¬¾å¼€æºçš„é«˜æ€§èƒ½å¯å¤šäººåä½œä»£ç ç¼–è¾‘å™¨ï¼Œé›†æˆäº† AI ä»£ç ç”Ÿæˆå’Œé‡æ„åŠŸèƒ½ï¼Œæä¾›è¯­è¨€æ„ŸçŸ¥ã€é›†æˆç»ˆç«¯ã€å¤šç§ç¼–è¾‘æ¨¡å¼ã€å›¢é˜Ÿåä½œå’Œè¿œç¨‹ä»£ç æ“ä½œç­‰å¼ºå¤§åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NDWgHEVOi)\n[Web LLM](https://github.com/mlc-ai/web-llm) | ä¸€æ¬¾å¯åœ¨æµè§ˆå™¨ä¸­ç›´æ¥è¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹çš„å·¥å…·ï¼Œæ”¯æŒ Llama 2 7B / 13Bã€Mistral 7B å’Œ WizadMath ç­‰æ¨¡å‹ï¼Œå¹¶é€šè¿‡ WebGPU å®ç°åŠ é€Ÿï¼Œæ— éœ€æœåŠ¡å™¨æ”¯æŒã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NDMulfXnj)\n[WhisperSpeech](https://github.com/collabora/WhisperSpeech) | ä¸€æ¬¾åŸºäº Whisper è¯­éŸ³è¯†åˆ«æ¨¡å‹çš„å¼€æºæ–‡æœ¬è½¬è¯­éŸ³å·¥å…·ï¼Œç›®å‰ä»…æ”¯æŒè‹±è¯­ï¼Œä½œè€…è®¡åˆ’åœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬å¢åŠ å¤šè¯­è¨€æ”¯æŒï¼Œå¯ç”¨äºå•†ä¸šç”¨é€”ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NCHfHoIKx)\n[GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS) | ä¸€æ¬¾å¼€æºçš„ä¸­æ–‡è¯­éŸ³å…‹éš†å·¥å…·ï¼Œä»…éœ€ 5 ç§’è¯­éŸ³æ ·æœ¬å³å¯å®ç° 80 %~ 95 %ç›¸ä¼¼åº¦çš„å£°éŸ³å…‹éš†ï¼Œæä¾› 1 åˆ†é’Ÿè¯­éŸ³å¯é€¼è¿‘çœŸäººæ•ˆæœå¹¶ç”Ÿæˆé«˜è´¨é‡ TTS æ¨¡å‹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NCo6Xt3ta)\n[QAnything](https://github.com/netease-youdao/QAnything) | ä¸€æ¬¾å¼€æºçš„çŸ¥è¯†åº“é—®ç­”å¼•æ“ï¼Œæ”¯æŒæœ¬åœ°éƒ¨ç½²å’Œè°ƒç”¨äº‘ç«¯å¤§æ¨¡å‹æœåŠ¡ï¼Œå¯å¯¼å…¥å¤šç§æ ¼å¼æ–‡æ¡£ï¼Œæä¾›å‡†ç¡®å¿«é€Ÿå¯é çš„é—®ç­”ä½“éªŒã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NCivo7c6k)\n[AI Gateway](https://github.com/Portkey-AI/gateway) | ä¸€æ¬¾ç»Ÿä¸€çš„ API ç½‘å…³å·¥å…·ï¼Œå¯è½»æ¾å¿«é€Ÿæ¥å…¥ 100 å¤šç§å¤§è¯­è¨€æ¨¡å‹ï¼Œå¦‚ OpenAIã€Anthropicã€Mistralã€LLama2ã€Google Gemini ç­‰ï¼Œå…·æœ‰å ç”¨ç©ºé—´å°ã€å¤„ç†é€Ÿåº¦å¿«ã€æ”¯æŒè´Ÿè½½å‡è¡¡ã€æ•…éšœè½¬ç§»ã€è‡ªåŠ¨é‡è¯•ç­‰ä¼˜åŠ¿ï¼Œå·²åœ¨è¶…è¿‡ 100B Tokens ä¸Šè¿›è¡Œå®æˆ˜æµ‹è¯•ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NC6ILzu2Q)\n[cutword](https://github.com/liwenju0/cutword) | ä¸€ä¸ªç®€å•å¿«é€Ÿçš„ä¸­æ–‡åˆ†è¯å’Œå‘½åå®ä½“è¯†åˆ«å·¥å…·ï¼Œåˆ†è¯é€Ÿåº¦æ˜¯çŸ¥å"ç»“å·´"ä¸­æ–‡åˆ†è¯çš„ä¸¤å€ï¼Œå­—å…¸æ–‡ä»¶æ ¹æ®æœ€æ–°æ•°æ®ç»Ÿè®¡å¾—åˆ°ï¼Œè¯é¢‘æ›´åŠ åˆç†ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NC3YicKMg)\n[DDColor](https://github.com/piddnad/DDColor) | ä¸€æ¬¾åŸºäº AI çš„å›¾åƒç€è‰²å·¥å…·ï¼Œå¯ä¸ºé»‘ç™½è€æ—§ç…§ç‰‡å’ŒåŠ¨æ¼«æ¸¸æˆåœºæ™¯æä¾›é€¼çœŸè‡ªç„¶çš„ç€è‰²æ•ˆæœï¼Œå®ç°ç…§ç‰‡çº§çœŸå®æ„Ÿã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NBYtl76nL)\n[ReplaceAnything](https://github.com/AIGCDesignGroup/ReplaceAnything) | ä¸€æ¬¾ç”±é˜¿é‡Œå·´å·´å‘å¸ƒçš„ AI å†…å®¹æ›¿æ¢å·¥å…·ï¼Œèƒ½å¤Ÿå‡†ç¡®ä¿ç•™æŒ‡å®šçš„ç‰©ä½“ï¼ˆå¦‚äººè„¸ã€äººç‰©ã€æœè£…ã€ç‰©å“ç­‰ï¼‰ï¼Œå¹¶é€šè¿‡è¾“å…¥æç¤ºè¯å®ç°è¶…é«˜è´¨é‡çš„å†…å®¹æ›¿æ¢ï¼Œå¯ç”¨äºäººç‰©æ›¿æ¢ã€æœè£…æ›¿æ¢ã€èƒŒæ™¯æ›¿æ¢ç­‰å¤šç§åœºæ™¯ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NBXz9pRhx)\n[Jan](https://github.com/janhq/jan) | ä¸€æ¬¾å¼€æºçš„æœ¬åœ° AI å·¥å…·ï¼Œå¯æ— éœ€ç¼–ç è¿è¡Œä¸»æµå¤§è¯­è¨€æ¨¡å‹å¦‚ Mistralã€Llamaã€Mixtral ç­‰ï¼Œæ”¯æŒ Windowsã€Mac å’Œ Linux ç³»ç»Ÿï¼Œæ‹¥æœ‰ç®€æ´ç¾è§‚çš„ UI ç•Œé¢ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NB2WZDMgy)\n[Mobile ALOHA](nan) | ä¸€æ¬¾å¤šåŠŸèƒ½å®¶åŠ¡æœºå™¨äººï¼Œå¯ä»¥å¸®åŠ©æµ‡èŠ±ã€æ‹–åœ°ã€åšé¥­ã€é€—çŒ«ã€æ‰”åƒåœ¾ã€æ´—è¡£æœã€é“ºåºŠå•ã€æ•´ç†è¡£ç‰©ç­‰å®¶åŠ¡æ´»åŠ¨ï¼Œå…·æœ‰æ¨¡ä»¿å­¦ä¹ å’Œè¿œç¨‹æ“ä½œåŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NAqQMF5ST)\n[Auto Job Find Assistant](https://github.com/Frrrrrrrrank/auto_job__find__chatgpt__rpa) | ä¸€æ¬¾ç»“åˆ GPT å’Œ RPA æŠ€æœ¯çš„è‡ªåŠ¨æŠ•ç®€å†åŠ©æ‰‹ï¼Œå¯æ ¹æ®ä¸ªäººç®€å†å’ŒèŒä½è¦æ±‚è‡ªåŠ¨åŒ¹é…åˆé€‚çš„å·¥ä½œæœºä¼šï¼Œç”Ÿæˆè‡ªæˆ‘ä»‹ç»å’Œæ±‚èŒä¿¡ï¼Œå¹¶è‡ªåŠ¨å‘é€ç»™ HRï¼Œå®ç°ä¸€é”®å¯»æ‰¾å·¥ä½œçš„å…¨æµç¨‹è‡ªåŠ¨åŒ–ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NAimK7naB)\n\n<div align="right">\n    <b><a href="#2024-å¹´å¤ç›˜">â†¥ è¿”å›ç›®å½•</a></b>\n</div>\n\n### å…è´¹ä¹¦ç±\n\né¡¹ç›® | ç®€è¿° | æº\n---- | ----- | -----\n[Large Language Model in Action](https://github.com/wangwei1237/LLM_in_Action) | ä¸€æœ¬ä¸“æ³¨äºå¤§è¯­è¨€æ¨¡å‹å®è·µåº”ç”¨çš„å¼€æºä¹¦ç±ï¼Œé€šè¿‡ä»‹ç»å·¥å…·å’Œæ¡ˆä¾‹å®è·µï¼Œå¸®åŠ©è¯»è€…å¿«é€Ÿä¸Šæ‰‹å¤§æ¨¡å‹åº”ç”¨å¼€å‘ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P63IAy7du)\n[Select Star SQL](https://æ— ) | ä¸€æœ¬å…è´¹çš„äº’åŠ¨å¼å­¦ä¹  SQL ä¹¦ç±ï¼Œå…±äº”ç« å†…å®¹ï¼Œæ¯ç« çº¦ 30 åˆ†é’Ÿï¼Œæ¶µç›–è¡Œæ“ä½œã€åˆ†ç»„èšåˆã€è¡¨è¿æ¥ç­‰ä¸»é¢˜ï¼Œç»“åˆå®é™…æ¡ˆä¾‹å­¦ä¹ ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OCnwbDxHU)\n[The Little Book of ML Metrics](https://github.com/NannyML/The-Little-Book-of-ML-Metrics) | ä¸€æœ¬é¢å‘æ•°æ®ç§‘å­¦å®¶çš„å¼€æºå…è´¹ä¹¦ç±ï¼Œæ¶µç›–å¹¿æ³›çš„æœºå™¨å­¦ä¹ æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å›å½’ã€åˆ†ç±»ã€èšç±»ã€æ’è¡Œã€è®¡ç®—æœºè§†è§‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸï¼Œæ—¨åœ¨æˆä¸ºæ•°æ®ç§‘å­¦å®¶çš„å¿«é€Ÿå‚è€ƒæ‰‹å†Œã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OBxRDpUiz)\n[Linux From Scratch ä¸­æ–‡ç¿»è¯‘ç‰ˆ](https://lfs.xry111.site/zh_CN/12.2/index.html) | ä¸€æœ¬å…è´¹çš„åœ¨çº¿ä¹¦ç±ï¼Œæ•™æˆä»æºä»£ç å¼€å§‹æ„å»ºæ•´ä¸ª Linux ç³»ç»Ÿçš„è¿‡ç¨‹ï¼Œæ·±å…¥äº†è§£ Linux å·¥ä½œåŸç†ï¼Œæ”¯æŒä¸ªæ€§åŒ–å®šåˆ¶å’Œä¼˜åŒ–ï¼Œæé«˜ç³»ç»Ÿå®‰å…¨æ€§ï¼Œé€šè¿‡å®è·µè·å¾— Linux ç³»ç»Ÿçš„æ·±åˆ»ç†è§£ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OAJjE4NxJ)\n[Hands-On Large Language Models](https://github.com/handsOnLLM/Hands-On-Large-Language-Models) | ä¸€æœ¬ç”±å´æ©è¾¾è€å¸ˆæ¨èçš„å…³äºå¤§è¯­è¨€æ¨¡å‹çš„å®è·µæŒ‡å—ä¹¦ç±ï¼Œæ¶µç›–è¯­è¨€æ¨¡å‹åŸºç¡€çŸ¥è¯†ã€æ–‡æœ¬åˆ†ç±»ã€æç¤ºå·¥ç¨‹ã€è¯­ä¹‰æœç´¢ã€å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ç­‰å†…å®¹ï¼Œé…æœ‰æ¡ˆä¾‹ä»£ç ã€æ’å›¾å’Œå¼•ç”¨è®ºæ–‡ï¼Œå¸®åŠ©è¯»è€…æ·±å…¥ç†è§£å’Œä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OxSqEpjmb)\n[è®¡ç®—æœºä½“ç³»ç»“æ„åŸºç¡€ï¼ˆç¬¬ä¸‰ç‰ˆï¼‰](https://foxsen.github.io/archbase/index.html) | ä¸€æœ¬ç”±"é¾™èŠ¯ä¹‹çˆ¶"èƒ¡ä¼Ÿæ­¦è€å¸ˆç­‰ç¼–å†™çš„å¼€æºå…è´¹åœ¨çº¿ä¹¦ç±ï¼Œæ¶µç›–æŒ‡ä»¤ç³»ç»Ÿç»“æ„ã€è®¡ç®—æœºç¡¬ä»¶ç»“æ„ã€CPU å¾®ç»“æ„ã€å¹¶è¡Œå¤„ç†ç»“æ„ã€è®¡ç®—æœºæ€§èƒ½åˆ†æç­‰å†…å®¹ï¼Œé€‚åˆæœ¬ç§‘ç”Ÿã€ç ”ç©¶ç”Ÿå’ŒæŠ€æœ¯äººå‘˜å­¦ä¹ å‚è€ƒã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OvI5Adb3R)\n[Decision Making Under Uncertainty](http://t.cn/R7cZfp3) | ä¸€æœ¬ç”±éº»çœç†å·¥å­¦é™¢å‡ºå“çš„å…è´¹ç”µå­ä¹¦ï¼Œæ¶µç›–æ¦‚ç‡æ¨ç†ã€åºåˆ—å†³ç­–é—®é¢˜ã€æ¨¡å‹/çŠ¶æ€ä¸ç¡®å®šæ€§ä»¥åŠå¤š Agent ç³»ç»Ÿç­‰å†…å®¹ï¼Œä½¿ç”¨å¤§é‡ç¤ºä¾‹å’Œç»ƒä¹ å¸®åŠ©è¯»è€…ç†è§£ä¸åŒç®—æ³•çš„ç›´è§‰å’Œåº”ç”¨åœºæ™¯ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OsEapfo3p)\n[Grammar Club](https://github.com/llwslc/grammar-club) | ä¸€æœ¬å¼€æºä¸”å…è´¹çš„åœ¨çº¿è‹±è¯­è¯­æ³•å­¦ä¹ ä¹¦ç±ï¼Œé‡‡ç”¨å¾ªåºæ¸è¿›çš„æ–¹å¼ï¼Œä»åˆçº§åˆ°é«˜çº§åˆ†ä¸ºä¸‰å¤§ç¯‡å¹…ï¼Œæ¶µç›–ç®€å•å¥ã€å¤å¥ã€åˆå¥å’Œç®€åŒ–ä»å¥ç­‰å†…å®¹ï¼Œæ—¨åœ¨åŸ¹å…»è‹±è¯­èƒ½åŠ›ã€å»ºç«‹ä¿¡å¿ƒå¹¶å¢å¼ºé˜…è¯»å…´è¶£ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Oq5hdAmUT)\n[ã€Šç†è§£æ·±åº¦å­¦ä¹ ã€‹ä¸­æ–‡ç‰ˆ](https://github.com/careywyr/UnderstandingDeepLearning-ZH-CN) | ä¸€æœ¬ç”±éº»çœç†å·¥å¼€æºçš„æ·±åº¦å­¦ä¹ æ•™æä¸­æ–‡ç‰ˆï¼Œå…¨é¢æ¶µç›–æ·±åº¦å­¦ä¹ åŸºç¡€æ¦‚å¿µã€ç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ç­‰å†…å®¹ï¼Œå¹¶æä¾› PPTã€ç¬”è®°å’Œ Python ç»ƒä¹  Demoã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OoxD0vPwf)\n[Understanding Deep Learning](https://github.com/udlbook/udlbook) | ä¸€æœ¬æ¥è‡ªéº»çœç†å·¥çš„å¼€æºä¹¦ç±ï¼Œå…¨é¢æ¶µç›–äº†æ·±åº¦å­¦ä¹ çš„åŸºç¡€æ¦‚å¿µã€ç›‘ç£å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ã€çº¿æ€§å›å½’ã€ç¥ç»ç½‘ç»œã€æ‰©æ•£æ¨¡å‹ç­‰å†…å®¹ã€‚è¯¥ä¹¦æä¾›äº† PPTã€ç¬”è®°å’Œ 68 ä¸ª Python ç»ƒä¹  Demo ä¾›å­¦ä¹ å’Œå®è·µã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Oml7ZfYGb)\n[Python Guide for JavaScript Engineers](https://github.com/luckrnx09/python-guide-for-javascript-engineers) | ä¸€æœ¬é¢å‘ JavaScript å·¥ç¨‹å¸ˆçš„ Python å­¦ä¹ æŒ‡å—ï¼Œç”±ä¸€åå‰ç«¯å·¥ç¨‹å¸ˆç¼–å†™ï¼Œç³»ç»Ÿä»‹ç»äº† Python ç¯å¢ƒæ­å»ºã€é¡¹ç›®å¼€å‘ç­‰å†…å®¹ï¼Œå¹¶å¯¹æ¯”äº† JavaScript å’Œ Python çš„å¼‚åŒï¼Œå¸®åŠ© JavaScript å·¥ç¨‹å¸ˆå¿«é€ŸæŒæ¡ Python è¯­è¨€ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Om6N3uysr)\n[æ·±å…¥è¶…é«˜å¯ç”¨æ¶æ„åŸç†ä¸å®è·µ](https://github.com/isno/theByteBook) | ä¸€æœ¬å¼€æºçš„æ¶æ„è®¾è®¡ä¹¦ç±ï¼Œæ¶µç›–äº‘è®¡ç®—ã€ç½‘ç»œã€åˆ†å¸ƒå¼ç³»ç»Ÿã€å®¹å™¨æŠ€æœ¯ã€å¯è§‚æµ‹æ€§ã€æœåŠ¡ç½‘æ ¼ã€DevOps ç­‰ä¸»é¢˜ï¼Œå¸®åŠ©ç¨‹åºå‘˜æ·±å…¥ç†è§£ç›¸å…³æŠ€æœ¯çš„åŸç†ä¸å®è·µã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OlzWze7Y2)\n\n<div align="right">\n    <b><a href="#2024-å¹´å¤ç›˜">â†¥ è¿”å›ç›®å½•</a></b>\n</div>\n\n### å­¦ä¹ æ•™ç¨‹\n\né¡¹ç›® | ç®€è¿° | æº\n---- | ----- | -----\n[Reasoning with o1](http://t.cn/A6mkWHyD) | ä¸€é—¨ç”± OpenAI AI è§£å†³æ–¹æ¡ˆä¸»ç®¡ Colin Jarvis ä¸»è®²çš„å…è´¹ AI çŸ­è¯¾ç¨‹ï¼Œæ•™æˆå¦‚ä½•æœ‰æ•ˆä½¿ç”¨ OpenAI æœ€æ–°æ¨ç†æ¨¡å‹ o1ï¼ŒåŒ…æ‹¬å¤šæ­¥éª¤ä»»åŠ¡è§„åˆ’ã€ä»£ç å¼€å‘ä¸ç¼–è¾‘ã€å›¾åƒç†è§£ç­‰å®è·µé¡¹ç›®ï¼Œå¸®åŠ©å­¦ä¹ è€…æŒæ¡ o1 æ¨¡å‹çš„é«˜çº§æ¨ç†èƒ½åŠ›ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P5KRx9BEa)\n[A Programmer''s Guide to English](http://t.cn/EJKb2nf) | ä¸€ä»½ä¸“ä¸ºç¨‹åºå‘˜è®¾è®¡çš„è‹±è¯­å­¦ä¹ æŒ‡å—ï¼Œé€šè¿‡ç¨‹åºå‘˜æ˜“äºç†è§£çš„æ–¹å¼å‰–æè¯­è¨€å­¦ä¹ æœ¬è´¨ï¼Œæä¾›é€‚åˆä¸åŒæ°´å¹³çš„è®­ç»ƒæ–¹æ³•å’Œå­¦ä¹ èµ„æ–™æ¨èï¼Œå¹¶ä»æ„å»ºè‹±è¯­è¯†åˆ«ç¨‹åºçš„è§’åº¦åŠ©åŠ›å­¦ä¹ ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/ODalbpiMt)\n[ML Retreat](https://github.com/hesamsheikh/ml-retreat) | ä¸€ä»½ä»ä¸­çº§åˆ°é«˜çº§çš„ AI å­¦ä¹ è·¯å¾„ï¼ŒåŒ…å«ä½œè€…åœ¨å­¦ä¹ é«˜çº§æœºå™¨å­¦ä¹ æ—¶çš„ä¸ªäººç¬”è®°å’Œèµ„æºï¼Œæ¶µç›–ä»åŸºç¡€åˆ°æ›´é«˜çº§ä¸»é¢˜çš„æ·±å…¥ç†è§£ï¼Œå¦‚ä»å¤´æ„å»ºå¤§è¯­è¨€æ¨¡å‹ã€LLM å¹»è§‰æ·±å…¥ç ”ç©¶ä»¥åŠ LLM è¶…è¶Šæ³¨æ„åŠ›æœºåˆ¶ç­‰ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OBum9wgGe)\n[Machine Learning Course Notes](https://www.cs.cmu.edu/~hn1/documents/machine-learning/notes.pdf) | ä¸€ä»½æ¥è‡ªå¡å†…åŸºæ¢…éš†å¤§å­¦çš„æœºå™¨å­¦ä¹ å…¥é—¨è¯¾ç¨‹ç¬”è®°ï¼Œæ¶µç›–äº†æœºå™¨å­¦ä¹ å„ä¸»é¢˜çš„è¦ç‚¹ï¼ŒåŒ…æ‹¬éå‚æ•°æ¨¡å‹ã€çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ã€ç¥ç»ç½‘ç»œã€æ”¯æŒå‘é‡æœºç­‰ï¼Œé’ˆå¯¹åˆå­¦è€…æä¾›å…¨é¢çš„æ¦‚å¿µè§£é‡Šã€æ•°å­¦è¯æ˜å’Œç®—æ³•æ­¥éª¤ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OzRhXEtCy)\n[AI Python for Beginners](http://t.cn/A68R6CQ0) | ä¸€é—¨ç”±å´æ©è¾¾è€å¸ˆä¸»è®²çš„å…è´¹ AI çŸ­è¯¾ç¨‹ï¼Œé¢å‘åˆå­¦è€…ï¼Œæ•™æˆ Python ç¼–ç¨‹åŸºç¡€çŸ¥è¯†ä»¥åŠé›†æˆ AI å·¥å…·è¿›è¡Œæ•°æ®æ“ä½œã€åˆ†æå’Œå¯è§†åŒ–ï¼Œé€šè¿‡çœŸå®é¡¹ç›®å®è·µå¼ºåŒ– Python æŠ€èƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OwBhjxcpT)\n[è½¯ä»¶å·¥ç¨‹å¸ˆå­¦ä¹ æŒ‡å—](http://t.cn/A6RCcp0z) | ä¸€ä»½ä»é›¶å¼€å§‹æˆä¸ºè½¯ä»¶å·¥ç¨‹å¸ˆçš„è¯¦ç»†å­¦ä¹ æŒ‡å—ï¼Œæ¶µç›–è®¡ç®—æœºåŸºç¡€çŸ¥è¯†ã€è½¯ä»¶å¼€å‘å·¥å…·å’Œæµç¨‹ã€æ•°æ®åº“ã€é¢å‘å¯¹è±¡ç¼–ç¨‹ã€DevOps å·¥å…·ç­‰å†…å®¹ï¼Œä» Python è¯­è¨€å…¥é—¨ï¼Œæœ€åæä¾›ç¼–ç é¢è¯•å‡†å¤‡ã€å·¥ä½œé€‰æ‹©å’ŒæŒç»­å­¦ä¹ å»ºè®®ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Ov7LUjLhG)\n[Machine-Learning](https://github.com/DorsaRoh/Machine-Learning) | ä¸€ä»½ä»é›¶å¼€å§‹å­¦ä¹ æœºå™¨å­¦ä¹ ç®—æ³•çš„æ•™ç¨‹ï¼ŒåŒ…å«ç¥ç»ç½‘ç»œå’Œ Transformer æ¨¡å‹çš„è¯¦ç»†è§£é‡Šå’Œ Python å®ç°ä»£ç ï¼Œå¦‚ Neuronã€Layer å’Œ NeuralNetwork ç±»ï¼Œè¿˜æä¾›æœºå™¨å­¦ä¹ è§†é¢‘å’Œ Jupyter Notebook æ–‡ä»¶ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OuNwC1Hgi)\n[Spring-Reading](https://github.com/xuchengsheng/spring-reading) | ä¸€ä»½æ·±å…¥äº†è§£ Spring æºç çš„ç³»åˆ—å­¦ä¹ èµ„æ–™ï¼Œæ¶µç›–äº† Spring æ¡†æ¶çš„æ ¸å¿ƒæ¦‚å¿µå’Œå…³é”®åŠŸèƒ½ï¼Œå¦‚èµ„æºåŠ è½½ã€è¡¨è¾¾å¼è¯­è¨€ã€Bean å®šä¹‰ã€Aware æ¥å£ã€æ ¸å¿ƒæ³¨è§£å’Œ AOP ç­‰ï¼Œå¸®åŠ©æ›´é€å½»åœ°ç†è§£ Spring å†…éƒ¨å·¥ä½œæœºåˆ¶ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OpZnTy6ae)\n[Machine Learning Specialization](http://t.cn/A6aP9tQt) | ä¸€å¥—ç”±å´æ©è¾¾è€å¸ˆä¸»è®²çš„æœºå™¨å­¦ä¹ ä¸“ä¸šè¯¾ç¨‹ï¼ŒåŒ…æ‹¬æœºå™¨å­¦ä¹ å›å½’ä¸åˆ†ç±»ã€é«˜çº§å­¦ä¹ ç®—æ³•ã€æ— ç›‘ç£å­¦ä¹ ç­‰å¤šä¸ªé¢†åŸŸï¼Œæ¶µç›–ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€ç¥ç»ç½‘ç»œç­‰å†…å®¹ï¼Œé€‚åˆåˆå­¦è€…å’Œå¸Œæœ›åœ¨äººå·¥æ™ºèƒ½é¢†åŸŸå‘å±•çš„ä¸“ä¸šäººå£«å­¦ä¹ ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Opx6pj0Zz)\n[LLM Twin Course](https://github.com/decodingml/llm-twin-course) | ä¸€é—¨å…¨é¢çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…è´¹å­¦ä¹ è¯¾ç¨‹ï¼Œæ•™æˆå¦‚ä½•æ„å»ºç”Ÿäº§çº§åˆ«çš„ LLM å’ŒåŸºäº LLM çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ç³»ç»Ÿï¼Œæ¶µç›–ç³»ç»Ÿè®¾è®¡ã€æ•°æ®å·¥ç¨‹ã€ç‰¹å¾ç®¡é“ã€è®­ç»ƒç®¡é“å’Œæ¨ç†ç®¡é“ç­‰æ–¹é¢ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OorVRAcQU)\n[Stable Diffusion From Scratch](https://github.com/juraam/stable-diffusion-from-scratch) | ä¸€å¥—ç³»ç»Ÿæ€§çš„æ•™ç¨‹ï¼Œè®²è§£ Stable Diffusion å’Œæ‰©æ•£æ¨¡å‹çš„å·¥ä½œåŸç†åŠå…¶èƒŒåçš„æ•°å­¦çŸ¥è¯†ï¼Œå¹¶æä¾›ä¸€ç³»åˆ—æ­¥éª¤æŒ‡å¯¼è®­ç»ƒå®Œæˆä¸€ä¸ªæ‰©æ•£æ¨¡å‹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OnZEnsNN7)\n[From Tensor to Stable Diffusion](https://github.com/jla524/fromthetensor) | ä¸€ä»½å¼€æºçš„æ·±åº¦å­¦ä¹ æ•™ç¨‹ï¼Œæä¾›ä¸ºæœŸ 9 å‘¨çš„è¯¾ç¨‹å¤§çº²ï¼Œä»å¼ é‡åŸºç¡€åˆ°ç¨³å®šæ‰©æ•£æ¨¡å‹å®ç°ï¼ŒåŒ…æ‹¬è§†é¢‘è®²è§£ã€ä»£ç å®ç°å’Œè®ºæ–‡å­¦ä¹ ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OmNpttYXs)\n[å¤§æ¨¡å‹ç†è®ºåŸºç¡€](https://github.com/datawhalechina/so-large-lm) | ä¸€ä»½å…³äºå¤§è§„æ¨¡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„æ•™ç¨‹èµ„æ–™é›†åˆï¼Œæ¶µç›–æ•°æ®å‡†å¤‡ã€æ¨¡å‹æ„å»ºã€è®­ç»ƒç­–ç•¥ã€æ¨¡å‹è¯„ä¼°ä¸æ”¹è¿›ç­‰å†…å®¹ï¼Œæ—¨åœ¨ä¸ºè¯»è€…æä¾›æ·±å…¥çš„ç†è®ºçŸ¥è¯†å’Œå®è·µæ–¹æ³•ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OlNW47BIP)\n[AI Text-to-Video Model from Scratch](https://github.com/FareedKhan-dev/AI-text-to-video-model-from-scratch) | ä¸€ä»½æ‰‹æŠŠæ‰‹æ•™ç¨‹ï¼Œæ•™ä½ å¦‚ä½•ä½¿ç”¨ Python ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªæ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¨¡å‹ï¼Œæ¶µç›–ç†è®ºæ¦‚å¿µã€æ¶æ„è®¾è®¡å’Œç¼–ç å®ç°ç­‰å…¨è¿‡ç¨‹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OlFSO6VRt)\n[nginx-tutorial](https://github.com/jaywcjlove/nginx-tutorial) | ä¸€ä»½å¼€æºçš„ Nginx å…¥é—¨å­¦ä¹ ç¬”è®°ï¼Œæ¶µç›–åŸºç¡€å®‰è£…ã€å‚æ•°è¯´æ˜ã€é…ç½®è®²è§£ã€ç¬¬ä¸‰æ–¹æ¨¡å—å®‰è£…ã€æ€§èƒ½ä¼˜åŒ–åŠå¸¸è§ä½¿ç”¨åœºæ™¯ç­‰å†…å®¹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OlEvzdgci)\n[PyTorch Deep Learning Tutorial](https://github.com/mrdbourke/pytorch-deep-learning) | ä¸€ä»½å¼€æºçš„ã€Šä»é›¶åˆ°ç²¾é€šæ·±åº¦å­¦ä¹  PyTorchã€‹æ•™ç¨‹ï¼Œæ¶µç›– PyTorch åŸºç¡€ã€æ·±åº¦å­¦ä¹ å·¥ä½œæµç¨‹ã€è®¡ç®—æœºè§†è§‰ã€è‡ªå®šä¹‰æ•°æ®é›†å¤„ç†ã€æ¨¡å—åŒ–ä»£ç ç¼–å†™åŠæ¨¡å‹éƒ¨ç½²ç­‰å†…å®¹ï¼Œæä¾›è§†é¢‘å’Œä»£ç ç¤ºä¾‹ï¼Œé€‚åˆæœ‰ Python ç¼–ç¨‹å’ŒåŸºç¡€æœºå™¨å­¦ä¹ çŸ¥è¯†çš„åˆå­¦è€…ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OlpPMu4WI)\n[Generative AI Handbook](https://genai-handbook.github.io/) | ä¸€ä»½ç³»ç»ŸåŒ–çš„ç”Ÿæˆå¼äººå·¥æ™ºèƒ½å­¦ä¹ è·¯çº¿æ‰‹å†Œï¼ŒåŒ…å« 9 å¤§ç« èŠ‚ï¼Œæ¶µç›–ç”Ÿæˆå¼ AI çš„åŸºç¡€çŸ¥è¯†ã€å®è·µåº”ç”¨å’Œæœ€æ–°ç ”ç©¶è¿›å±•ï¼Œå¹¶æä¾›ç›¸å…³å­¦ä¹ èµ„æºã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Ok3Ix6QQY)\n[LingoHut](https://www.lingohut.com/zh) | ä¸€ä¸ªå…è´¹çš„åœ¨çº¿è¯­è¨€å­¦ä¹ å¹³å°ï¼Œæä¾› 45 ç§è¯­è¨€çš„å­¦ä¹ èµ„æºï¼ŒåŒ…æ‹¬ 125 èŠ‚å…è´¹è¯¾ç¨‹ï¼Œæ¶µç›–æ—¥å¸¸ç”¨è¯­å’Œè¯æ±‡ï¼Œè®©ä½ ç”¨æ¯è¯­è½»æ¾å­¦ä¹ æ–°è¯­è¨€ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OjHklDrnR)\n[WTF-zk](https://github.com/WTFAcademy/WTF-zk) | ä¸€ä»½é›¶çŸ¥è¯†è¯æ˜å…¥é—¨æ•™ç¨‹ï¼Œæ—¨åœ¨è®©ä»…æœ‰é«˜ä¸­æ•°å­¦åŸºç¡€çš„äººä¹Ÿèƒ½å…¥é—¨é›¶çŸ¥è¯†è¯æ˜ï¼ˆzkï¼‰æŠ€æœ¯ï¼Œæ•™ç¨‹ä½¿ç”¨ Python å¤ç°ç›¸å…³ç®—æ³•ï¼Œéœ€è¦ä¸€äº› Python è¯­è¨€åŸºç¡€ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OgvlVhJdx)\n[100-exercises-to-learn-rust](https://github.com/mainmatter/100-exercises-to-learn-rust) | ä¸€å¥—åŒ…å« 100 ä¸ªç»ƒä¹ çš„ Rust ç¼–ç¨‹è¯­è¨€å­¦ä¹ èµ„æºï¼Œæ—¨åœ¨å¸®åŠ©åˆå­¦è€…ä»é›¶åŸºç¡€é€æ­¥æŒæ¡ Rust ç¼–ç¨‹ï¼Œæœ€ç»ˆèƒ½å¤Ÿç‹¬ç«‹ç¼–å†™ Rust ç¨‹åºã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OfDgO9yQA)\n[ã€Šå¤§è¯­è¨€æ¨¡å‹ã€‹ç”µå­ä¹¦ç±](https://github.com/LLMBook-zh/LLMBook-zh.github.io) | ä¸€ä»½å…¨é¢ç³»ç»Ÿåœ°ä»‹ç»å¤§è¯­è¨€æ¨¡å‹çš„ç”µå­ä¹¦ç±ï¼Œæ¶µç›–åŸºç¡€ç†è®ºã€é¢„è®­ç»ƒã€å¾®è°ƒä¸å¯¹é½ã€ä½¿ç”¨éƒ¨ç½²ä»¥åŠè¯„æµ‹ä¸åº”ç”¨ç­‰å†…å®¹ï¼Œä¸ºå¤§æ¨¡å‹å…¥é—¨è¯»è€…æä¾›è¯¦ç»†æŒ‡å—ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/Oflvak2z1)\n[ã€ŠåŠ¨æ‰‹å­¦å¤§æ¨¡å‹ã€‹](https://github.com/Lordog/dive-into-llms) | ä¸€ä»½ç”±ä¸Šæµ·äº¤é€šå¤§å­¦ã€Šäººå·¥æ™ºèƒ½å®‰å…¨æŠ€æœ¯ã€‹è¯¾ç¨‹è®²ä¹‰æ‹“å±•è€Œæ¥çš„ç¼–ç¨‹å®æˆ˜æ•™ç¨‹ç³»åˆ—ï¼Œé€šè¿‡å®è·µå¸®åŠ©å¿«é€Ÿå…¥é—¨å¤§æ¨¡å‹ç›¸å…³æŠ€æœ¯ï¼Œæ¶µç›–æ¨¡å‹å¾®è°ƒä¸éƒ¨ç½²ã€æç¤ºå­¦ä¹ ä¸æ€ç»´é“¾ã€çŸ¥è¯†ç¼–è¾‘ã€æ¨¡å‹æ°´å°ä»¥åŠå¤§æ¨¡å‹æ™ºèƒ½ä½“ä¸å®‰å…¨ç­‰å†…å®¹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OdKhKlOrt)\n[machine-learning-list](https://github.com/elicit/machine-learning-list) | ä¸€ä»½è¾ƒä¸ºå…¨é¢çš„æœºå™¨å­¦ä¹ æ•™ç¨‹ï¼Œæ¶µç›–ä»åŸºç¡€åˆ°é«˜çº§å†åˆ°å®è·µä¸åº”ç”¨çš„å†…å®¹ï¼ŒåŒ…æ‹¬åŸºæœ¬ç®€ä»‹ã€è®­ç»ƒå’Œå¾®è°ƒã€æ¨ç†å’Œæ‰§è¡Œç­–ç•¥ã€æ•°æ®é›†å¤„ç†ä»¥åŠå®é™…åº”ç”¨ç­‰ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OaaWfFiNX)\n[50projects50days](https://github.com/bradtraversy/50projects50days) | ä¸€ä»½åŒ…å« 50 ä¸ªä½¿ç”¨ HTMLã€CSS å’Œ JavaScript å®ç°çš„å° Demo çš„é¡¹ç›®é›†åˆï¼Œé€‚åˆå‰ç«¯åˆå­¦è€…ç»ƒæ‰‹ï¼Œæ¶µç›–åŠ¨ç”»å¯¼èˆªã€èƒŒæ™¯æ»‘å—ã€å¯†ç ç”Ÿæˆå™¨ã€å›¾åƒè½®æ’­ç­‰å¸¸è§ç½‘ç«™åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O9y3cCdKM)\n[self-llm](https://github.com/datawhalechina/self-llm) | ä¸€ä»½è¶…å…¨é¢çš„å¼€æºå¤§æ¨¡å‹ä½¿ç”¨æŒ‡å—ï¼Œæä¾›é’ˆå¯¹å›½å†…åˆå­¦è€…çš„å¼€æºå¤§æ¨¡å‹æ•™ç¨‹ï¼Œé€šè¿‡ AutoDL å¹³å°ç®€åŒ–æ¨¡å‹éƒ¨ç½²ã€ä½¿ç”¨å’Œåº”ç”¨æµç¨‹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O8MUIyrVD)\n[SQLä¹‹æ¯](https://github.com/liyupi/sql-mother) | ä¸€ä¸ªå…è´¹çš„é—¯å…³å¼ SQL è‡ªå­¦ç½‘ç«™ï¼Œé€šè¿‡æœ‰è¶£çš„äº¤äº’å¼å…³å¡ï¼Œè®©åˆå­¦è€…ä» 0 åˆ° 1 æŒæ¡å¸¸ç”¨ SQL è¯­æ³•ï¼ŒåŒ…æ‹¬ 30 å¤šä¸ªå…³å¡ã€åœ¨çº¿æäº¤ä»£ç ã€è‡ªç”±é€‰æ‹©å…³å¡ã€è‡ªå®šä¹‰å…³å¡ç­‰åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O5TNi3BCb)\n[Hugging Face AI Courses](https://github.com/huggingface/course) | ä¸€å¥—ç”± Hugging Face å‡ºå“çš„ 4 é—¨ AI å­¦ä¹ è¯¾ç¨‹ï¼Œæ¶µç›–è‡ªç„¶è¯­è¨€å¤„ç†ã€æ·±åº¦å¼ºåŒ–å­¦ä¹ ã€éŸ³é¢‘å¤„ç†å’Œå¼€æº AI å¼€å‘ç­‰ä¸»é¢˜ï¼Œé€‚åˆåˆå­¦è€…å…è´¹å­¦ä¹ ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O4GmqkbnY)\n[earthworm](https://github.com/cuixueshe/earthworm) | ä¸€ä¸ªé€šè¿‡è¿è¯æ„å¥çš„æ–¹å¼å¸®åŠ©ç”¨æˆ·æ›´å¥½åœ°å­¦ä¹ è‹±è¯­çš„å¼€æºé¡¹ç›®ï¼Œä»ç®€å•åˆ°å¤æ‚çš„å¥å­é€æ­¥è®­ç»ƒï¼Œè®©ç”¨æˆ·è½»æ¾æŒæ¡å†™å‡ºé•¿å¥å­ï¼Œé¡ºä¾¿è¿˜å¯ä»¥ç»ƒä¹ é”®ç›˜è‹±æ–‡æ‰“å­—ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/O4es38OKr)\n[90DaysOfCyberSecurity](https://github.com/farhanashrafdev/90DaysOfCyberSecurity) | ä¸€ä»½ 90 å¤©ç½‘ç»œå®‰å…¨å­¦ä¹ è®¡åˆ’ï¼Œæ¶µç›–ç½‘ç»œåŸºç¡€ã€å®‰å…¨åŸºç¡€ã€Linuxã€Pythonã€æµé‡åˆ†æã€Gitã€ELKã€AWSã€Azure å’Œé»‘å®¢æ”»å‡»ç­‰ä¸»é¢˜ï¼Œæä¾›å®Œæ•´çš„å­¦ä¹ èµ„æºå’Œææ–™ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NEo8e4Qju)\n[Build a Large Language Model (From Scratch)](https://github.com/rasbt/LLMs-from-scratch) | ä¸€å¥—ä»é›¶å¼€å§‹æ„å»ºç±»ä¼¼ ChatGPT çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ•™ç¨‹ï¼Œæ¶µç›–äº†æ–‡æœ¬æ•°æ®å¤„ç†ã€æ³¨æ„åŠ›æœºåˆ¶å®ç°ã€æ¨¡å‹æ„å»ºã€é¢„è®­ç»ƒã€å¾®è°ƒç­‰å†…å®¹ï¼Œå¸®åŠ©è¯»è€…æ·±å…¥äº†è§£ LLM çš„å·¥ä½œåŸç†å’Œå®ç°æ–¹å¼ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NEbXf8R0H)\n[LLM Course](https://github.com/mlabonne/llm-course) | ä¸€ä»½å¼€æºå…è´¹çš„å¤§è¯­è¨€æ¨¡å‹è¯¾ç¨‹ï¼ŒåŒ…å«è¯¦ç»†çš„å­¦ä¹ è·¯çº¿å’Œå®è·µç¬”è®°ï¼Œåˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼šLLM åŸºç¡€æ•™ç¨‹ã€LLM æ·±å…¥æ¢ç´¢å’Œ LLM åº”ç”¨ä¸éƒ¨ç½²ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NDEaNgSEd)\n[CS50](https://cs50.harvard.edu/x/2024/) | ä¸€é—¨ç”±å“ˆä½›å¤§å­¦å…è´¹å¼€æ”¾çš„è®¡ç®—æœºç§‘å­¦å’Œç¼–ç¨‹å…¥é—¨è¯¾ç¨‹ï¼Œæ•™æˆç¼–ç¨‹åŸºæœ¬åŸç†å’Œè®¡ç®—æ€ç»´æ–¹æ³•ï¼Œæ¶µç›– Cã€Pythonã€SQLã€HTMLã€CSS å’Œ JavaScript ç­‰å¤šç§è¯­è¨€ï¼Œæœ€åä»¥ç»ˆæé¡¹ç›®ä½œä¸ºæ”¶å®˜ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NDDbUuY3i)\n[English-level-up-tips](https://github.com/byoungd/English-level-up-tips) | ä¸€ä»½ç³»ç»ŸåŒ–çš„è‹±è¯­å­¦ä¹ æŒ‡å—ï¼ŒåŒ…å«è®¤çŸ¥ã€å•è¯ã€å¬åŠ›ã€é˜…è¯»ã€å£è¯­ã€å†™ä½œå’Œæ‰¯æ·¡ç­‰ä¸ƒå¤§ç« èŠ‚ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·é«˜æ•ˆæå‡è‹±è¯­æ°´å¹³ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NDhArlM4d)\n[Docker ä»å…¥é—¨åˆ°å®è·µ](https://github.com/yeasy/docker_practice) | ä¸€ä»½å…¨é¢çš„ Docker å­¦ä¹ æŒ‡å—ï¼Œæ¶µç›–äº† Docker çš„åŸºç¡€çŸ¥è¯†ã€å®‰è£…ã€é•œåƒä½¿ç”¨ã€å®¹å™¨æ“ä½œã€æ•°æ®ç®¡ç†ã€ç½‘ç»œé…ç½®ç­‰å†…å®¹ï¼Œæ—¨åœ¨æ•™ä¼šæ–°æ‰‹æœ‰æ•ˆä½¿ç”¨ Dockerã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/NCpzLCrZj)\n\n<div align="right">\n    <b><a href="#2024-å¹´å¤ç›˜">â†¥ è¿”å›ç›®å½•</a></b>\n</div>\n\n### å®ç”¨å·¥å…·\n\né¡¹ç›® | ç®€è¿° | æº\n---- | ----- | -----\n[VutronMusic](https://github.com/stark81/VutronMusic) | ä¸€æ¬¾é«˜é¢œå€¼çš„ç¬¬ä¸‰æ–¹ç½‘æ˜“äº‘éŸ³ä¹æ’­æ”¾å™¨ï¼Œæ”¯æŒæœ¬åœ°æ­Œæ›²ã€ç¦»çº¿æ­Œå•æ’­æ”¾ï¼Œè¯»å–æ­Œæ›²å°é¢ã€å†…åµŒæ­Œè¯ç­‰åŠŸèƒ½ï¼ŒåŒæ—¶æ”¯æŒçº¿ä¸Šä¿¡æ¯åŒ¹é…ã€äº‘ç›˜ã€å¯¹æ­Œæ›²è¯„è®ºï¼Œç•Œé¢çº¯å‡€æ— å¹¿å‘Šã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P7iwOnglL)\n[Squish](https://github.com/addyosmani/squish) | ä¸€æ¬¾å¼€æºå…è´¹çš„å›¾åƒå‹ç¼©å·¥å…·ï¼Œæ”¯æŒå¤šç§å›¾åƒæ ¼å¼ï¼Œå…·æœ‰é«˜æ€§èƒ½å‹ç¼©ä¼˜åŒ–åŠŸèƒ½ï¼Œèƒ½ä¿æŒå›¾åƒè´¨é‡ä¸å˜ï¼Œå¹¶æä¾›æ‰¹é‡å¤„ç†ã€æ ¼å¼è½¬æ¢ã€å®æ—¶é¢„è§ˆç­‰åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P6Uy8njRw)\n[PairDrop](https://github.com/schlagmichdoch/PairDrop) | ä¸€æ¬¾å¼€ç®±å³ç”¨çš„è·¨å¹³å°æ–‡ä»¶ä¼ è¾“å·¥å…·ï¼Œæ— éœ€æ³¨å†Œå’Œå®‰è£…ï¼Œåªéœ€æµè§ˆå™¨å³å¯åœ¨åŒä¸€å±€åŸŸç½‘å†…ä»»æ„è®¾å¤‡ä¹‹é—´è‡ªç”±ä¼ è¾“æ–‡ä»¶ï¼Œè¿˜æ”¯æŒåˆ›å»ºä¸´æ—¶å…¬å…±æˆ¿é—´å®ç°å…¬ç½‘ä¼ è¾“ï¼Œé‡‡ç”¨ç‚¹å¯¹ç‚¹ä¼ è¾“ä¿è¯æ•°æ®å®‰å…¨éšç§ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P6PDIrGgz)\n[Checkmate](https://github.com/bluewave-labs/checkmate) | ä¸€æ¬¾é«˜é¢œå€¼çš„å¯è§†åŒ–å®æ—¶ç›‘æ§å·¥å…·ï¼Œæ”¯æŒç½‘ç«™ã€é¡µé¢åŠ è½½é€Ÿåº¦ã€Docker å®¹å™¨ã€Ping å“åº”ç­‰å¤šç§ç›‘æ§ç±»å‹ï¼Œå¹¶æä¾›æœåŠ¡å™¨ CPUã€å†…å­˜ã€ç£ç›˜å’Œæ¸©åº¦ç­‰çŠ¶æ€ä¿¡æ¯ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P6Oatg3Kv)\n[MyIP](https://github.com/jason5ng32/MyIP) | ä¸€æ¬¾å¼€æºçš„ IP å·¥å…·ç®±ï¼Œå¯ä»¥æ£€æŸ¥ IP åœ°å€ã€åœ°ç†ä½ç½®ã€DNS æ³„éœ²ã€ç½‘é€Ÿæµ‹è¯•ã€Ping æµ‹è¯•å’Œç½‘ç«™å¯ç”¨æ€§ç­‰ï¼Œæä¾› 258 é¡¹å®‰å…¨æ£€æŸ¥æ¸…å•åŠè¯¦ç»†è§£é‡Šã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P6GD6rY0f)\n[Presenterm](https://hgithub.com/mfontanini/presenterm) | ä¸€æ¬¾å¼€æºå·¥å…·ï¼Œèƒ½å¤Ÿåœ¨ç»ˆç«¯ä¸Šæ¼”ç¤ºç”± Markdown åˆ›å»ºçš„ç²¾ç¾ PPTï¼Œæ”¯æŒä»£ç é«˜äº®ã€å›¾ç‰‡å±•ç¤ºã€LaTeX å…¬å¼æ¸²æŸ“ç­‰åŠŸèƒ½ï¼Œå†…ç½®ä¸°å¯Œä¸»é¢˜é£æ ¼å¹¶å¯è‡ªå®šä¹‰ï¼Œè¿˜å¯å¯¼å‡º PDF æ–‡ä»¶ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P6sgEaqDS)\n[File Converter](https://github.com/Tichau/FileConverter) | ä¸€æ¬¾å®ç”¨çš„æ–‡ä»¶æ ¼å¼è½¬æ¢å·¥å…·ï¼Œæ”¯æŒè§†é¢‘ã€éŸ³é¢‘ã€å›¾ç‰‡å’Œæ–‡æ¡£ç­‰å¤šç§æ–‡ä»¶æ ¼å¼è½¬æ¢ï¼Œå¯ä»¥å¿«é€Ÿå®Œæˆæ–‡ä»¶æ ¼å¼è½¬æ¢å’Œå‹ç¼©ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P6ek51Q56)\n[CodeSnap](https://github.com/mistricky/CodeSnap) | ä¸€æ¬¾åŸºäº Rust å¼€å‘çš„ä»£ç æˆªå›¾å·¥å…·ï¼Œå¯ç¦»çº¿ä½¿ç”¨ï¼Œæ”¯æŒ Sublime Text ä»£ç è¯­æ³•é«˜äº®ä¸»é¢˜ï¼Œå¯è‡ªå®šä¹‰å­—ä½“ã€è¡Œå·ã€æ°´å°ç­‰ï¼Œæ”¯æŒ PNGã€SVG å’Œ HTML è¾“å‡ºæ ¼å¼ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P6d90tfJ1)\n[IP Helper](https://github.com/soulteary/ip-helper) | ä¸€æ¬¾ç®€æ´çš„ IP ä¿¡æ¯æŸ¥è¯¢å·¥å…·ï¼Œæ”¯æŒ Webã€å‘½ä»¤è¡Œã€Telnet å’Œ FTP ç­‰å¤šç§æŸ¥è¯¢æ–¹å¼ï¼Œæä¾› IP åœ°ç†ä½ç½®æŸ¥è¯¢ã€Token è®¤è¯å’Œè‡ªå®šä¹‰åŸŸåç­‰åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P69pE9DTa)\n[FastSend](https://github.com/ShouChenICU/FastSend) | ä¸€æ¬¾åŸºäº WebRTC æŠ€æœ¯çš„å¼€æºå…è´¹ç‚¹å¯¹ç‚¹æ–‡ä»¶ä¼ è¾“å·¥å…·ï¼Œæ”¯æŒå¿«é€Ÿå®‰å…¨çš„æ–‡ä»¶å’Œç›®å½•åŒæ­¥ä¼ è¾“ï¼Œå…·æœ‰åŠ å¯†ä¼ è¾“ã€å±€åŸŸç½‘ä¼˜åŒ–ã€ç®€æ´ç•Œé¢ç­‰ç‰¹æ€§ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P5QyEsF1z)\n[academic-project-page-template-vue](https://github.com/JunyaoHu/academic-project-page-template-vue) | ä¸€ä¸ªåŸºäº Vue å¼€å‘çš„å­¦æœ¯è®ºæ–‡é¡¹ç›®é¡µé¢æ¨¡æ¿ï¼Œæ”¯æŒè½®æ’­å›¾ã€åµŒå…¥è§†é¢‘ã€æ•°æ®å¯è§†åŒ–ç­‰ä¸°å¯ŒåŠŸèƒ½ï¼Œè¿˜èƒ½ä¸€é”®å¤åˆ¶å¼•ç”¨æ ¼å¼ï¼Œå†…ç½®è¯„è®ºç³»ç»Ÿï¼Œéå¸¸é€‚åˆç ”ç©¶äººå‘˜å¿«é€Ÿç›´è§‚åœ°å‘ˆç°è®ºæ–‡åŸºæœ¬ä¿¡æ¯ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P5BAesHje)\n[SiteOne Crawler](https://github.com/janreges/siteone-crawler) | ä¸€æ¬¾ç®€å•æ˜“ç”¨ä¸”åŠŸèƒ½å¼ºå¤§çš„ç½‘ç«™åˆ†æå·¥å…·ï¼Œå¯ä¸€é”®å®Œæˆç½‘ç«™åˆ†æã€æ€§èƒ½æ£€æµ‹ã€SEO ä¼˜åŒ–å»ºè®®ï¼Œå¹¶å¯¼å‡ºå®Œæ•´çš„ç¦»çº¿ HTML åˆ†æç»“æœï¼Œç”¨äºç½‘ç«™åˆ†æä¼˜åŒ–ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P5s0xc6FE)\n[wechatDataBackup](https://github.com/git-jiadong/wechatDataBackup) | ä¸€æ¬¾å¾®ä¿¡èŠå¤©è®°å½•å¯¼å‡ºå·¥å…·ï¼Œæä¾›ç±»ä¼¼å¾®ä¿¡çš„ç®€æ˜“æ“ä½œç•Œé¢ï¼Œæ”¯æŒä¸€é”®å¯¼å‡ºèŠå¤©è®°å½•ã€å›¾ç‰‡ã€è§†é¢‘ã€é“¾æ¥ã€è¯­éŸ³ã€æ–‡ä»¶å’Œè¡¨æƒ…ç­‰ä¿¡æ¯ã€‚è¿˜æ”¯æŒæŒ‰ç±»å‹ã€æ—¥æœŸã€ç¾¤æˆå‘˜è¿›è¡Œæ£€ç´¢ï¼Œä»¥åŠå¤šå¼€è´¦å·é€‰æ‹©å¯¼å‡ºå’Œæ•°æ®åˆ‡æ¢ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P3k7T5BOE)\n[Neko](https://github.com/m1k1o/neko) | ä¸€æ¬¾å¼€æºçš„è™šæ‹Ÿæµè§ˆå™¨å·¥å…·ï¼Œæ”¯æŒå¤šäººåŒæ—¶è®¿é—®å’Œæ§åˆ¶åŒä¸€æµè§ˆå™¨ç•Œé¢ï¼Œå®ç°ç”»é¢å’Œå£°éŸ³åŒæ­¥ï¼Œå†…ç½®èŠå¤©ç³»ç»Ÿã€æ–‡ä»¶ä¼ è¾“ç­‰åŠŸèƒ½ï¼Œå¯ç”¨äºè¿œç¨‹åä½œã€åœ¨çº¿è§‚å½±ç­‰åœºæ™¯ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P3aB5gbmE)\n[Koodo Reader](https://github.com/koodo-reader/koodo-reader) | ä¸€æ¬¾å¼€æºå…è´¹çš„è·¨å¹³å°ç”µå­ä¹¦é˜…è¯»å™¨ï¼Œæ”¯æŒå¤šç§æ ¼å¼å¯¼å…¥ã€å¤šç§é˜…è¯»æ¨¡å¼ã€å¬ä¹¦ã€ç¿»è¯‘ã€è¯å…¸ç­‰åŠŸèƒ½ï¼Œå¹¶æä¾›ä¸ªæ€§åŒ–æ’ç‰ˆã€ç¬”è®°æ ‡æ³¨ç­‰åŠŸèƒ½ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P1mEcsNta)\n[WindTerm](https://github.com/kingToolbox/WindTerm) | ä¸€æ¬¾é«˜é¢œå€¼ä¸”åŠŸèƒ½å¼ºå¤§çš„è·¨å¹³å°ç»ˆç«¯å·¥å…·ï¼Œæ”¯æŒå¤šç§åè®®è¿æ¥ã€æ–‡ä»¶ä¼ è¾“ã€å¤šè¯­è¨€ç•Œé¢ã€å‘½ä»¤è¡¥å…¨ã€æ–‡ä»¶ç®¡ç†ã€åˆ†å±æ˜¾ç¤ºç­‰åŠŸèƒ½ï¼Œå…·æœ‰é«˜æ€§èƒ½ã€ä½å»¶è¿Ÿå’Œä½å†…å­˜å ç”¨çš„ç‰¹ç‚¹ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P0qU8soSZ)\n[MarkText](https://github.com/marktext/marktext) | ä¸€æ¬¾é«˜é¢œå€¼çš„å¼€æº Markdown ç¼–è¾‘å™¨ï¼Œç•Œé¢ç®€æ´å¹²å‡€ï¼Œæä¾›å®æ—¶é¢„è§ˆã€å¤šç§ä¸»é¢˜å’Œç¼–è¾‘æ¨¡å¼ç­‰åŠŸèƒ½ï¼Œæ”¯æŒæ•°å­¦å…¬å¼ã€è¡¨æƒ…ç¬¦å·ã€å¿«æ·é”®ç­‰ï¼Œå¯å¯¼å‡º HTML å’Œ PDF æ–‡ä»¶ï¼Œé€‚ç”¨äº Windowsã€macOS å’Œ Linux ç³»ç»Ÿã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/P0htEbTHs)\n[RightMenu Master](https://github.com/jaywcjlove/rightmenu-master) | ä¸€æ¬¾ Finder å³é”®èœå•å¢å¼ºå·¥å…·ï¼Œå†…ç½®å¤šä¸ªå®ç”¨é¢„è®¾æ“ä½œï¼Œæ”¯æŒç§»åŠ¨å¤åˆ¶æ–‡ä»¶ã€åˆ›å»ºæ–°æ–‡æ¡£ã€ä»£ç é«˜äº®é¢„è§ˆã€å¿«æ·æ‰“å¼€å¸¸ç”¨æ–‡ä»¶å¤¹å’Œç»ˆç«¯ç­‰ï¼Œæ—¨åœ¨æå‡ Mac ç”¨æˆ·çš„å·¥ä½œæ•ˆç‡ã€‚ | [![](https://raw.githubusercontent.com/GitHubDaily/GitHubDaily/master/assets/sina_logo.png)](https://weibo.com/5722964389/OFSlBwmq4)\n[File Centipede](https://github.com/filecxx/FileCentipede', '{"language":null,"stars":43235,"forks":4373,"watchers":43235,"open_issues":475,"topics":["ai","algorithms-and-data-structures","backend","developer-tools","development","frontend","github","java","javascript","kubernetes","linux","markdown","open-source","python","tutorials","web"],"default_branch":"master","size_kb":2839,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:GitHubDaily:GitHubDaily","source_url":"https://github.com/GitHubDaily/GitHubDaily"},{"type":"has_code","target_id":"github:GitHubDaily:GitHubDaily","source_url":"https://github.com/GitHubDaily/GitHubDaily"},{"type":"has_code","target_id":"github:GitHubDaily:GitHubDaily","source_url":"https://github.com/GitHubDaily/GitHubDaily"},{"type":"has_code","target_id":"github:GitHubDaily:GitHubDaily","source_url":"https://github.com/GitHubDaily/GitHubDaily"},{"type":"has_code","target_id":"github:GitHubDaily:GitHubDaily","source_url":"https://github.com/GitHubDaily/GitHubDaily"},{"type":"has_code","target_id":"github:GitHubDaily:GitHubDaily","source_url":"https://github.com/GitHubDaily/GitHubDaily"},{"type":"has_code","target_id":"github:GitHubDaily:GitHubDaily","source_url":"https://github.com/GitHubDaily/GitHubDaily"},{"type":"has_code","target_id":"github:GitHubDaily:GitHubDaily","source_url":"https://github.com/GitHubDaily/GitHubDaily"},{"type":"has_code","target_id":"github:2noise:ChatTTS","source_url":"https://github.com/2noise/ChatTTS"},{"type":"has_code","target_id":"github:intelligent-machine-learning:dlrover","source_url":"https://github.com/intelligent-machine-learning/dlrover"},{"type":"has_code","target_id":"github:hpcaitech:SwiftInfer","source_url":"https://github.com/hpcaitech/SwiftInfer"},{"type":"has_code","target_id":"github:facebookresearch:audio2photoreal","source_url":"https://github.com/facebookresearch/audio2photoreal"},{"type":"has_code","target_id":"github:myshell-ai:OpenVoice","source_url":"https://github.com/myshell-ai/OpenVoice"},{"type":"has_code","target_id":"github:anti-work:shortest","source_url":"https://github.com/anti-work/shortest"},{"type":"has_code","target_id":"github:ZGGSONG:STranslate","source_url":"https://github.com/ZGGSONG/STranslate"},{"type":"has_code","target_id":"github:xiaoxiunique:x-kit","source_url":"https://github.com/xiaoxiunique/x-kit"},{"type":"has_code","target_id":"github:hoarder-app:hoarder","source_url":"https://github.com/hoarder-app/hoarder"},{"type":"has_code","target_id":"github:web-infra-dev:midscene","source_url":"https://github.com/web-infra-dev/midscene"},{"type":"has_code","target_id":"github:browser-use:browser-use","source_url":"https://github.com/browser-use/browser-use"},{"type":"has_code","target_id":"github:hpcaitech:Open-Sora","source_url":"https://github.com/hpcaitech/Open-Sora"},{"type":"has_code","target_id":"github:Nutlope:logocreator","source_url":"https://github.com/Nutlope/logocreator"},{"type":"has_code","target_id":"github:microsoft:markitdown","source_url":"https://github.com/microsoft/markitdown"},{"type":"has_code","target_id":"github:Byaidu:PDFMathTranslate","source_url":"https://github.com/Byaidu/PDFMathTranslate"},{"type":"has_code","target_id":"github:nishuzumi:gemini-teacher","source_url":"https://github.com/nishuzumi/gemini-teacher"},{"type":"has_code","target_id":"github:whotto:Video_note_generator","source_url":"https://github.com/whotto/Video_note_generator"},{"type":"has_code","target_id":"github:mediar-ai:screenpipe","source_url":"https://github.com/mediar-ai/screenpipe"},{"type":"has_code","target_id":"github:TTPlanetPig:Comfyui_Object_Migration","source_url":"https://github.com/TTPlanetPig/Comfyui_Object_Migration"},{"type":"has_code","target_id":"github:tmplink:nsfw_detector","source_url":"https://github.com/tmplink/nsfw_detector"},{"type":"has_code","target_id":"github:WEIFENG2333:VideoCaptioner","source_url":"https://github.com/WEIFENG2333/VideoCaptioner"},{"type":"has_code","target_id":"github:magic-quill:magicquill","source_url":"https://github.com/magic-quill/magicquill"},{"type":"has_code","target_id":"github:abus-aikorea:voice-pro","source_url":"https://github.com/abus-aikorea/voice-pro"},{"type":"has_code","target_id":"github:jhj0517:AdvancedLivePortrait-WebUI","source_url":"https://github.com/jhj0517/AdvancedLivePortrait-WebUI"},{"type":"has_code","target_id":"github:CatchTheTornado:pdf-extract-api","source_url":"https://github.com/CatchTheTornado/pdf-extract-api"},{"type":"has_code","target_id":"github:theredsix:cerebellum","source_url":"https://github.com/theredsix/cerebellum"},{"type":"has_code","target_id":"github:yeates:PromptFix","source_url":"https://github.com/yeates/PromptFix"},{"type":"has_code","target_id":"github:getmaxun:maxun","source_url":"https://github.com/getmaxun/maxun"},{"type":"has_code","target_id":"github:16131zzzzzzzz:EveryoneNobel","source_url":"https://github.com/16131zzzzzzzz/EveryoneNobel"},{"type":"has_code","target_id":"github:YOYZHANG:PodCastLM","source_url":"https://github.com/YOYZHANG/PodCastLM"},{"type":"has_code","target_id":"github:corbt:agent.exe","source_url":"https://github.com/corbt/agent.exe"},{"type":"has_code","target_id":"github:getomni-ai:zerox","source_url":"https://github.com/getomni-ai/zerox"},{"type":"has_code","target_id":"github:Eddycrack864:UVR5-UI","source_url":"https://github.com/Eddycrack864/UVR5-UI"},{"type":"has_code","target_id":"github:adarshb3:Virtual-Try-On-Application-using-Flask-Twilio-and-Gradio","source_url":"https://github.com/adarshb3/Virtual-Try-On-Application-using-Flask-Twilio-and-Gradio"},{"type":"has_code","target_id":"github:anthropics:anthropic-quickstarts","source_url":"https://github.com/anthropics/anthropic-quickstarts"},{"type":"has_code","target_id":"github:VikParuchuri:tabled","source_url":"https://github.com/VikParuchuri/tabled"},{"type":"has_code","target_id":"github:VikParuchuri:surya","source_url":"https://github.com/VikParuchuri/surya"},{"type":"has_code","target_id":"github:WEIFENG2333:AsrTools","source_url":"https://github.com/WEIFENG2333/AsrTools"},{"type":"has_code","target_id":"github:ErikBjare:gptme","source_url":"https://github.com/ErikBjare/gptme"},{"type":"has_code","target_id":"github:AI4Finance-Foundation:FinRL","source_url":"https://github.com/AI4Finance-Foundation/FinRL"},{"type":"has_code","target_id":"github:DrewThomasson:ebook2audiobookXTTS","source_url":"https://github.com/DrewThomasson/ebook2audiobookXTTS"},{"type":"has_code","target_id":"github:RexanWONG:text-behind-image","source_url":"https://github.com/RexanWONG/text-behind-image"},{"type":"has_code","target_id":"github:QiuYannnn:Local-File-Organizer","source_url":"https://github.com/QiuYannnn/Local-File-Organizer"},{"type":"has_code","target_id":"github:pencilresearch:OpenScanner","source_url":"https://github.com/pencilresearch/OpenScanner"},{"type":"has_code","target_id":"github:wechatferry:wechatferry","source_url":"https://github.com/wechatferry/wechatferry"},{"type":"has_code","target_id":"github:markmap:markmap","source_url":"https://github.com/markmap/markmap"},{"type":"has_code","target_id":"github:k4yt3x:video2x","source_url":"https://github.com/k4yt3x/video2x"},{"type":"has_code","target_id":"github:gitbrent:PptxGenJS","source_url":"https://github.com/gitbrent/PptxGenJS"},{"type":"has_code","target_id":"github:dataease:dataease","source_url":"https://github.com/dataease/dataease"},{"type":"has_code","target_id":"github:ddean2009:MoneyPrinterPlus","source_url":"https://github.com/ddean2009/MoneyPrinterPlus"},{"type":"has_code","target_id":"github:meltylabs:melty","source_url":"https://github.com/meltylabs/melty"},{"type":"has_code","target_id":"github:Spr-Aachen:Easy-Voice-Toolkit","source_url":"https://github.com/Spr-Aachen/Easy-Voice-Toolkit"},{"type":"has_code","target_id":"github:chartdb:chartdb","source_url":"https://github.com/chartdb/chartdb"},{"type":"has_code","target_id":"github:mendableai:firecrawl","source_url":"https://github.com/mendableai/firecrawl"},{"type":"has_code","target_id":"github:1Panel-dev:MaxKB","source_url":"https://github.com/1Panel-dev/MaxKB"},{"type":"has_code","target_id":"github:Zeyi-Lin:HivisionIDPhotos","source_url":"https://github.com/Zeyi-Lin/HivisionIDPhotos"},{"type":"has_code","target_id":"github:Huanshere:VideoLingo","source_url":"https://github.com/Huanshere/VideoLingo"},{"type":"has_code","target_id":"github:linyqh:NarratoAI","source_url":"https://github.com/linyqh/NarratoAI"},{"type":"has_code","target_id":"github:lhl:voicechat2","source_url":"https://github.com/lhl/voicechat2"},{"type":"has_code","target_id":"github:itsOwen:CyberScraper-2077","source_url":"https://github.com/itsOwen/CyberScraper-2077"},{"type":"has_code","target_id":"github:wan-h:awesome-digital-human-live2d","source_url":"https://github.com/wan-h/awesome-digital-human-live2d"},{"type":"has_code","target_id":"github:Dicklesworthstone:llm_aided_ocr","source_url":"https://github.com/Dicklesworthstone/llm_aided_ocr"},{"type":"has_code","target_id":"github:ViggoZ:producthunt-daily-hot","source_url":"https://github.com/ViggoZ/producthunt-daily-hot"},{"type":"has_code","target_id":"github:BMPixel:moffee","source_url":"https://github.com/BMPixel/moffee"},{"type":"has_code","target_id":"github:Kedreamix:Linly-Dubbing","source_url":"https://github.com/Kedreamix/Linly-Dubbing"},{"type":"has_code","target_id":"github:SakanaAI:AI-Scientist","source_url":"https://github.com/SakanaAI/AI-Scientist"},{"type":"has_code","target_id":"github:jbilcke-hf:clapper","source_url":"https://github.com/jbilcke-hf/clapper"},{"type":"has_code","target_id":"github:ozgrozer:ai-renamer","source_url":"https://github.com/ozgrozer/ai-renamer"},{"type":"has_code","target_id":"github:lipku:metahuman-stream","source_url":"https://github.com/lipku/metahuman-stream"},{"type":"has_code","target_id":"github:ssine:pptx2md","source_url":"https://github.com/ssine/pptx2md"},{"type":"has_code","target_id":"github:opendatalab:MinerU","source_url":"https://github.com/opendatalab/MinerU"},{"type":"has_code","target_id":"github:Nutlope:llamatutor","source_url":"https://github.com/Nutlope/llamatutor"},{"type":"has_code","target_id":"github:yihong0618:bilingual_book_maker","source_url":"https://github.com/yihong0618/bilingual_book_maker"},{"type":"has_code","target_id":"github:Doriandarko:claude-engineer","source_url":"https://github.com/Doriandarko/claude-engineer"},{"type":"has_code","target_id":"github:harry0703:AudioNotes","source_url":"https://github.com/harry0703/AudioNotes"},{"type":"has_code","target_id":"github:TahaSh:swapy","source_url":"https://github.com/TahaSh/swapy"},{"type":"has_code","target_id":"github:kkangert:kspider","source_url":"https://github.com/kkangert/kspider"},{"type":"has_code","target_id":"github:BuilderIO:ai-shell","source_url":"https://github.com/BuilderIO/ai-shell"},{"type":"has_code","target_id":"github:apify:crawlee-python","source_url":"https://github.com/apify/crawlee-python"},{"type":"has_code","target_id":"github:AugustDev:enchanted","source_url":"https://github.com/AugustDev/enchanted"},{"type":"has_code","target_id":"github:xenova:transformers.js","source_url":"https://github.com/xenova/transformers.js"},{"type":"has_code","target_id":"github:vanna-ai:vanna","source_url":"https://github.com/vanna-ai/vanna"},{"type":"has_code","target_id":"github:posit-dev:great-tables","source_url":"https://github.com/posit-dev/great-tables"},{"type":"has_code","target_id":"github:ogkalu2:comic-translate","source_url":"https://github.com/ogkalu2/comic-translate"},{"type":"has_code","target_id":"github:BinNong:meet-libai","source_url":"https://github.com/BinNong/meet-libai"},{"type":"has_code","target_id":"github:DAMO-NLP-SG:WebDesignAgent","source_url":"https://github.com/DAMO-NLP-SG/WebDesignAgent"},{"type":"has_code","target_id":"github:worm128:AI-YinMei","source_url":"https://github.com/worm128/AI-YinMei"},{"type":"has_code","target_id":"github:adithya-s-k:omniparse","source_url":"https://github.com/adithya-s-k/omniparse"},{"type":"has_code","target_id":"github:CosmosShadow:gptpdf","source_url":"https://github.com/CosmosShadow/gptpdf"},{"type":"has_code","target_id":"github:PeterH0323:Streamer-Sales","source_url":"https://github.com/PeterH0323/Streamer-Sales"},{"type":"has_code","target_id":"github:TeamWiseFlow:wiseflow","source_url":"https://github.com/TeamWiseFlow/wiseflow"},{"type":"has_code","target_id":"github:onuratakan:gpt-computer-assistant","source_url":"https://github.com/onuratakan/gpt-computer-assistant"},{"type":"has_code","target_id":"github:modelscope:DiffSynth-Studio","source_url":"https://github.com/modelscope/DiffSynth-Studio"},{"type":"has_code","target_id":"github:niedev:RTranslator","source_url":"https://github.com/niedev/RTranslator"},{"type":"has_code","target_id":"github:ayushpai:AI-Math-Notes","source_url":"https://github.com/ayushpai/AI-Math-Notes"},{"type":"has_code","target_id":"github:binary-husky:gpt_academic","source_url":"https://github.com/binary-husky/gpt_academic"},{"type":"has_code","target_id":"github:andrewyng:translation-agent","source_url":"https://github.com/andrewyng/translation-agent"},{"type":"has_code","target_id":"github:VinciGit00:Scrapegraph-ai","source_url":"https://github.com/VinciGit00/Scrapegraph-ai"},{"type":"has_code","target_id":"github:idootop:mi-gpt","source_url":"https://github.com/idootop/mi-gpt"},{"type":"has_code","target_id":"github:jianchang512:pyvideotrans","source_url":"https://github.com/jianchang512/pyvideotrans"},{"type":"has_code","target_id":"github:Jazee6:cloudflare-ai-web","source_url":"https://github.com/Jazee6/cloudflare-ai-web"},{"type":"has_code","target_id":"github:jianchang512:ChatTTS-ui","source_url":"https://github.com/jianchang512/ChatTTS-ui"},{"type":"has_code","target_id":"github:ItzCrazyKns:Perplexica","source_url":"https://github.com/ItzCrazyKns/Perplexica"},{"type":"has_code","target_id":"github:ragapp:ragapp","source_url":"https://github.com/ragapp/ragapp"},{"type":"has_code","target_id":"github:gabotechs:MusicGPT","source_url":"https://github.com/gabotechs/MusicGPT"},{"type":"has_code","target_id":"github:khoj-ai:khoj","source_url":"https://github.com/khoj-ai/khoj"},{"type":"has_code","target_id":"github:Amery2010:TalkWithGemini","source_url":"https://github.com/Amery2010/TalkWithGemini"},{"type":"has_code","target_id":"github:txstc55:ugly-avatar","source_url":"https://github.com/txstc55/ugly-avatar"},{"type":"has_code","target_id":"github:solidSpoon:DashPlayer","source_url":"https://github.com/solidSpoon/DashPlayer"},{"type":"has_code","target_id":"github:BasedHardware:openglass","source_url":"https://github.com/BasedHardware/openglass"},{"type":"has_code","target_id":"github:gezhaoyou:picprose","source_url":"https://github.com/gezhaoyou/picprose"},{"type":"has_code","target_id":"github:jina-ai:reader","source_url":"https://github.com/jina-ai/reader"},{"type":"has_code","target_id":"github:Dhravya:supermemory","source_url":"https://github.com/Dhravya/supermemory"},{"type":"has_code","target_id":"github:nilsherzig:LLocalSearch","source_url":"https://github.com/nilsherzig/LLocalSearch"},{"type":"has_code","target_id":"github:nashsu:FreeAskInternet","source_url":"https://github.com/nashsu/FreeAskInternet"},{"type":"has_code","target_id":"github:palxiao:poster-design","source_url":"https://github.com/palxiao/poster-design"},{"type":"has_code","target_id":"github:open-webui:open-webui","source_url":"https://github.com/open-webui/open-webui"},{"type":"has_code","target_id":"github:Evil0ctal:Douyin_TikTok_Download_API","source_url":"https://github.com/Evil0ctal/Douyin_TikTok_Download_API"},{"type":"has_code","target_id":"github:SunoAI-API:Suno-API","source_url":"https://github.com/SunoAI-API/Suno-API"},{"type":"has_code","target_id":"github:mshumer:ai-journalist","source_url":"https://github.com/mshumer/ai-journalist"},{"type":"has_code","target_id":"github:harry0703:MoneyPrinterTurbo","source_url":"https://github.com/harry0703/MoneyPrinterTurbo"},{"type":"has_code","target_id":"github:zeke:livestory","source_url":"https://github.com/zeke/livestory"},{"type":"has_code","target_id":"github:chidiwilliams:buzz","source_url":"https://github.com/chidiwilliams/buzz"},{"type":"has_code","target_id":"github:NanmiCoder:MediaCrawler","source_url":"https://github.com/NanmiCoder/MediaCrawler"},{"type":"has_code","target_id":"github:6174:comflowyspace","source_url":"https://github.com/6174/comflowyspace"},{"type":"has_code","target_id":"github:lxfater:inpaint-web","source_url":"https://github.com/lxfater/inpaint-web"},{"type":"has_code","target_id":"github:Eladlev:AutoPrompt","source_url":"https://github.com/Eladlev/AutoPrompt"},{"type":"has_code","target_id":"github:melon:qingwu-zimu","source_url":"https://github.com/melon/qingwu-zimu"},{"type":"has_code","target_id":"github:qnguyen3:chat-with-mlx","source_url":"https://github.com/qnguyen3/chat-with-mlx"},{"type":"has_code","target_id":"github:fofr:cog-stickers","source_url":"https://github.com/fofr/cog-stickers"},{"type":"has_code","target_id":"github:levihsu:OOTDiffusion","source_url":"https://github.com/levihsu/OOTDiffusion"},{"type":"has_code","target_id":"github:Nutlope:notesGPT","source_url":"https://github.com/Nutlope/notesGPT"},{"type":"has_code","target_id":"github:reorproject:reor","source_url":"https://github.com/reorproject/reor"},{"type":"has_code","target_id":"github:lencx:Noi","source_url":"https://github.com/lencx/Noi"},{"type":"has_code","target_id":"github:Dooy:chatgpt-web-midjourney-proxy","source_url":"https://github.com/Dooy/chatgpt-web-midjourney-proxy"},{"type":"has_code","target_id":"github:bclswl0827:ChatGemini","source_url":"https://github.com/bclswl0827/ChatGemini"},{"type":"has_code","target_id":"github:collabora:WhisperFusion","source_url":"https://github.com/collabora/WhisperFusion"},{"type":"has_code","target_id":"github:leptonai:search_with_lepton","source_url":"https://github.com/leptonai/search_with_lepton"},{"type":"has_code","target_id":"github:all-in-aigc:aicover","source_url":"https://github.com/all-in-aigc/aicover"},{"type":"has_code","target_id":"github:zed-industries:zed","source_url":"https://github.com/zed-industries/zed"},{"type":"has_code","target_id":"github:mlc-ai:web-llm","source_url":"https://github.com/mlc-ai/web-llm"},{"type":"has_code","target_id":"github:collabora:WhisperSpeech","source_url":"https://github.com/collabora/WhisperSpeech"},{"type":"has_code","target_id":"github:RVC-Boss:GPT-SoVITS","source_url":"https://github.com/RVC-Boss/GPT-SoVITS"},{"type":"has_code","target_id":"github:netease-youdao:QAnything","source_url":"https://github.com/netease-youdao/QAnything"},{"type":"has_code","target_id":"github:Portkey-AI:gateway","source_url":"https://github.com/Portkey-AI/gateway"},{"type":"has_code","target_id":"github:liwenju0:cutword","source_url":"https://github.com/liwenju0/cutword"},{"type":"has_code","target_id":"github:piddnad:DDColor","source_url":"https://github.com/piddnad/DDColor"},{"type":"has_code","target_id":"github:AIGCDesignGroup:ReplaceAnything","source_url":"https://github.com/AIGCDesignGroup/ReplaceAnything"},{"type":"has_code","target_id":"github:janhq:jan","source_url":"https://github.com/janhq/jan"},{"type":"has_code","target_id":"github:Frrrrrrrrank:auto_job__find__chatgpt__rpa","source_url":"https://github.com/Frrrrrrrrank/auto_job__find__chatgpt__rpa"},{"type":"has_code","target_id":"github:wangwei1237:LLM_in_Action","source_url":"https://github.com/wangwei1237/LLM_in_Action"},{"type":"has_code","target_id":"github:NannyML:The-Little-Book-of-ML-Metrics","source_url":"https://github.com/NannyML/The-Little-Book-of-ML-Metrics"},{"type":"has_code","target_id":"github:handsOnLLM:Hands-On-Large-Language-Models","source_url":"https://github.com/handsOnLLM/Hands-On-Large-Language-Models"},{"type":"has_code","target_id":"github:llwslc:grammar-club","source_url":"https://github.com/llwslc/grammar-club"},{"type":"has_code","target_id":"github:careywyr:UnderstandingDeepLearning-ZH-CN","source_url":"https://github.com/careywyr/UnderstandingDeepLearning-ZH-CN"},{"type":"has_code","target_id":"github:udlbook:udlbook","source_url":"https://github.com/udlbook/udlbook"},{"type":"has_code","target_id":"github:luckrnx09:python-guide-for-javascript-engineers","source_url":"https://github.com/luckrnx09/python-guide-for-javascript-engineers"},{"type":"has_code","target_id":"github:isno:theByteBook","source_url":"https://github.com/isno/theByteBook"},{"type":"has_code","target_id":"github:hesamsheikh:ml-retreat","source_url":"https://github.com/hesamsheikh/ml-retreat"},{"type":"has_code","target_id":"github:DorsaRoh:Machine-Learning","source_url":"https://github.com/DorsaRoh/Machine-Learning"},{"type":"has_code","target_id":"github:xuchengsheng:spring-reading","source_url":"https://github.com/xuchengsheng/spring-reading"},{"type":"has_code","target_id":"github:decodingml:llm-twin-course","source_url":"https://github.com/decodingml/llm-twin-course"},{"type":"has_code","target_id":"github:juraam:stable-diffusion-from-scratch","source_url":"https://github.com/juraam/stable-diffusion-from-scratch"},{"type":"has_code","target_id":"github:jla524:fromthetensor","source_url":"https://github.com/jla524/fromthetensor"},{"type":"has_code","target_id":"github:datawhalechina:so-large-lm","source_url":"https://github.com/datawhalechina/so-large-lm"},{"type":"has_code","target_id":"github:FareedKhan-dev:AI-text-to-video-model-from-scratch","source_url":"https://github.com/FareedKhan-dev/AI-text-to-video-model-from-scratch"},{"type":"has_code","target_id":"github:jaywcjlove:nginx-tutorial","source_url":"https://github.com/jaywcjlove/nginx-tutorial"},{"type":"has_code","target_id":"github:mrdbourke:pytorch-deep-learning","source_url":"https://github.com/mrdbourke/pytorch-deep-learning"},{"type":"has_code","target_id":"github:WTFAcademy:WTF-zk","source_url":"https://github.com/WTFAcademy/WTF-zk"},{"type":"has_code","target_id":"github:mainmatter:100-exercises-to-learn-rust","source_url":"https://github.com/mainmatter/100-exercises-to-learn-rust"},{"type":"has_code","target_id":"github:LLMBook-zh:LLMBook-zh.github.io","source_url":"https://github.com/LLMBook-zh/LLMBook-zh.github.io"},{"type":"has_code","target_id":"github:Lordog:dive-into-llms","source_url":"https://github.com/Lordog/dive-into-llms"},{"type":"has_code","target_id":"github:elicit:machine-learning-list","source_url":"https://github.com/elicit/machine-learning-list"},{"type":"has_code","target_id":"github:bradtraversy:50projects50days","source_url":"https://github.com/bradtraversy/50projects50days"},{"type":"has_code","target_id":"github:datawhalechina:self-llm","source_url":"https://github.com/datawhalechina/self-llm"},{"type":"has_code","target_id":"github:liyupi:sql-mother","source_url":"https://github.com/liyupi/sql-mother"},{"type":"has_code","target_id":"github:huggingface:course","source_url":"https://github.com/huggingface/course"},{"type":"has_code","target_id":"github:cuixueshe:earthworm","source_url":"https://github.com/cuixueshe/earthworm"},{"type":"has_code","target_id":"github:farhanashrafdev:90DaysOfCyberSecurity","source_url":"https://github.com/farhanashrafdev/90DaysOfCyberSecurity"},{"type":"has_code","target_id":"github:rasbt:LLMs-from-scratch","source_url":"https://github.com/rasbt/LLMs-from-scratch"},{"type":"has_code","target_id":"github:mlabonne:llm-course","source_url":"https://github.com/mlabonne/llm-course"},{"type":"has_code","target_id":"github:byoungd:English-level-up-tips","source_url":"https://github.com/byoungd/English-level-up-tips"},{"type":"has_code","target_id":"github:yeasy:docker_practice","source_url":"https://github.com/yeasy/docker_practice"},{"type":"has_code","target_id":"github:stark81:VutronMusic","source_url":"https://github.com/stark81/VutronMusic"},{"type":"has_code","target_id":"github:addyosmani:squish","source_url":"https://github.com/addyosmani/squish"},{"type":"has_code","target_id":"github:schlagmichdoch:PairDrop","source_url":"https://github.com/schlagmichdoch/PairDrop"},{"type":"has_code","target_id":"github:bluewave-labs:checkmate","source_url":"https://github.com/bluewave-labs/checkmate"},{"type":"has_code","target_id":"github:jason5ng32:MyIP","source_url":"https://github.com/jason5ng32/MyIP"},{"type":"has_code","target_id":"github:Tichau:FileConverter","source_url":"https://github.com/Tichau/FileConverter"},{"type":"has_code","target_id":"github:mistricky:CodeSnap","source_url":"https://github.com/mistricky/CodeSnap"},{"type":"has_code","target_id":"github:soulteary:ip-helper","source_url":"https://github.com/soulteary/ip-helper"},{"type":"has_code","target_id":"github:ShouChenICU:FastSend","source_url":"https://github.com/ShouChenICU/FastSend"},{"type":"has_code","target_id":"github:JunyaoHu:academic-project-page-template-vue","source_url":"https://github.com/JunyaoHu/academic-project-page-template-vue"},{"type":"has_code","target_id":"github:janreges:siteone-crawler","source_url":"https://github.com/janreges/siteone-crawler"},{"type":"has_code","target_id":"github:git-jiadong:wechatDataBackup","source_url":"https://github.com/git-jiadong/wechatDataBackup"},{"type":"has_code","target_id":"github:m1k1o:neko","source_url":"https://github.com/m1k1o/neko"},{"type":"has_code","target_id":"github:koodo-reader:koodo-reader","source_url":"https://github.com/koodo-reader/koodo-reader"},{"type":"has_code","target_id":"github:kingToolbox:WindTerm","source_url":"https://github.com/kingToolbox/WindTerm"},{"type":"has_code","target_id":"github:marktext:marktext","source_url":"https://github.com/marktext/marktext"},{"type":"has_code","target_id":"github:jaywcjlove:rightmenu-master","source_url":"https://github.com/jaywcjlove/rightmenu-master"},{"type":"has_code","target_id":"github:filecxx:FileCentipede","source_url":"https://github.com/filecxx/FileCentipede"},{"type":"has_code","target_id":"github:paperless-ngx:paperless-ngx","source_url":"https://github.com/paperless-ngx/paperless-ngx"},{"type":"has_code","target_id":"github:yournextstore:yournextstore","source_url":"https://github.com/yournextstore/yournextstore"},{"type":"has_code","target_id":"github:Uahh:ToastFish","source_url":"https://github.com/Uahh/ToastFish"},{"type":"has_code","target_id":"github:HeyPuter:puter","source_url":"https://github.com/HeyPuter/puter"},{"type":"has_code","target_id":"github:junegunn:fzf","source_url":"https://github.com/junegunn/fzf"},{"type":"has_code","target_id":"github:newsapp:newsnow","source_url":"https://github.com/newsapp/newsnow"},{"type":"has_code","target_id":"github:laurent22:joplin","source_url":"https://github.com/laurent22/joplin"},{"type":"has_code","target_id":"github:Guovin:TV","source_url":"https://github.com/Guovin/TV"},{"type":"has_code","target_id":"github:barry-ran:QtScrcpy","source_url":"https://github.com/barry-ran/QtScrcpy"},{"type":"has_code","target_id":"github:mifi:lossless-cut","source_url":"https://github.com/mifi/lossless-cut"},{"type":"has_code","target_id":"github:1Remote:1Remote","source_url":"https://github.com/1Remote/1Remote"},{"type":"has_code","target_id":"github:jooooock:wechat-article-exporter","source_url":"https://github.com/jooooock/wechat-article-exporter"},{"type":"has_code","target_id":"github:localsend:localsend","source_url":"https://github.com/localsend/localsend"},{"type":"has_code","target_id":"github:siyuan-note:siyuan","source_url":"https://github.com/siyuan-note/siyuan"},{"type":"has_code","target_id":"github:1Panel-dev:1Panel","source_url":"https://github.com/1Panel-dev/1Panel"},{"type":"has_code","target_id":"github:sickcodes:Docker-OSX","source_url":"https://github.com/sickcodes/Docker-OSX"},{"type":"has_code","target_id":"github:xushengfeng:eSearch","source_url":"https://github.com/xushengfeng/eSearch"},{"type":"has_code","target_id":"github:ZCShou:GoGoGo","source_url":"https://github.com/ZCShou/GoGoGo"},{"type":"has_code","target_id":"github:jinweijie:notify-me","source_url":"https://github.com/jinweijie/notify-me"},{"type":"has_code","target_id":"github:immich-app:immich","source_url":"https://github.com/immich-app/immich"},{"type":"has_code","target_id":"github:dreamhunter2333:cloudflare_temp_email","source_url":"https://github.com/dreamhunter2333/cloudflare_temp_email"},{"type":"has_code","target_id":"github:honmashironeko:ProxyCat","source_url":"https://github.com/honmashironeko/ProxyCat"},{"type":"has_code","target_id":"github:0xJacky:nginx-ui","source_url":"https://github.com/0xJacky/nginx-ui"},{"type":"has_code","target_id":"github:imfile-io:imfile-desktop","source_url":"https://github.com/imfile-io/imfile-desktop"},{"type":"has_code","target_id":"github:amir1376:ab-download-manager","source_url":"https://github.com/amir1376/ab-download-manager"},{"type":"has_code","target_id":"github:files-community:Files","source_url":"https://github.com/files-community/Files"},{"type":"has_code","target_id":"github:Bao-qing:123pan","source_url":"https://github.com/Bao-qing/123pan"},{"type":"has_code","target_id":"github:Lakr233:BBackupp","source_url":"https://github.com/Lakr233/BBackupp"},{"type":"has_code","target_id":"github:doocs:md","source_url":"https://github.com/doocs/md"},{"type":"has_code","target_id":"github:glushchenko:fsnotes","source_url":"https://github.com/glushchenko/fsnotes"},{"type":"has_code","target_id":"github:Adonis142857:Real-Address-Generator","source_url":"https://github.com/Adonis142857/Real-Address-Generator"},{"type":"has_code","target_id":"github:BookerLiu:GeekDesk","source_url":"https://github.com/BookerLiu/GeekDesk"},{"type":"has_code","target_id":"github:spacedriveapp:spacedrive","source_url":"https://github.com/spacedriveapp/spacedrive"},{"type":"has_code","target_id":"github:tw93:Pake","source_url":"https://github.com/tw93/Pake"},{"type":"has_code","target_id":"github:usual2970:certimate","source_url":"https://github.com/usual2970/certimate"},{"type":"has_code","target_id":"github:wanglin2:mind-map","source_url":"https://github.com/wanglin2/mind-map"},{"type":"has_code","target_id":"github:udecode:plate","source_url":"https://github.com/udecode/plate"},{"type":"has_code","target_id":"github:shadow1ng:fscan","source_url":"https://github.com/shadow1ng/fscan"},{"type":"has_code","target_id":"github:xiaoyaocz:dart_simple_live","source_url":"https://github.com/xiaoyaocz/dart_simple_live"},{"type":"has_code","target_id":"github:hanxi:xiaomusic","source_url":"https://github.com/hanxi/xiaomusic"},{"type":"has_code","target_id":"github:infrost:DeeplxFile","source_url":"https://github.com/infrost/DeeplxFile"},{"type":"has_code","target_id":"github:4gray:iptvnator","source_url":"https://github.com/4gray/iptvnator"},{"type":"has_code","target_id":"github:ayangweb:EcoPaste","source_url":"https://github.com/ayangweb/EcoPaste"},{"type":"has_code","target_id":"github:sinaatalay:rendercv","source_url":"https://github.com/sinaatalay/rendercv"},{"type":"has_code","target_id":"github:Dhravya:cloudflare-saas-stack","source_url":"https://github.com/Dhravya/cloudflare-saas-stack"},{"type":"has_code","target_id":"github:alienator88:Pearcleaner","source_url":"https://github.com/alienator88/Pearcleaner"},{"type":"has_code","target_id":"github:chenfan0:fideo-live-record","source_url":"https://github.com/chenfan0/fideo-live-record"},{"type":"has_code","target_id":"github:codewithsadee:vcard-personal-portfolio","source_url":"https://github.com/codewithsadee/vcard-personal-portfolio"},{"type":"has_code","target_id":"github:Pintree-io:pintree","source_url":"https://github.com/Pintree-io/pintree"},{"type":"has_code","target_id":"github:raycast:ray-so","source_url":"https://github.com/raycast/ray-so"},{"type":"has_code","target_id":"github:kevin2li:PDF-Guru","source_url":"https://github.com/kevin2li/PDF-Guru"},{"type":"has_code","target_id":"github:revezone:revezone","source_url":"https://github.com/revezone/revezone"},{"type":"has_code","target_id":"github:caorushizi:mediago","source_url":"https://github.com/caorushizi/mediago"},{"type":"has_code","target_id":"github:pacexy:flow","source_url":"https://github.com/pacexy/flow"},{"type":"has_code","target_id":"github:danbao:auto-ssl","source_url":"https://github.com/danbao/auto-ssl"},{"type":"has_code","target_id":"github:XPoet:picx","source_url":"https://github.com/XPoet/picx"},{"type":"has_code","target_id":"github:luost26:academic-homepage","source_url":"https://github.com/luost26/academic-homepage"},{"type":"has_code","target_id":"github:aome510:spotify-player","source_url":"https://github.com/aome510/spotify-player"},{"type":"has_code","target_id":"github:CH563:shot-easy-website","source_url":"https://github.com/CH563/shot-easy-website"},{"type":"has_code","target_id":"github:T8RIN:ImageToolbox","source_url":"https://github.com/T8RIN/ImageToolbox"},{"type":"has_code","target_id":"github:docmost:docmost","source_url":"https://github.com/docmost/docmost"},{"type":"has_code","target_id":"github:unilei:image-watermark-tool","source_url":"https://github.com/unilei/image-watermark-tool"},{"type":"has_code","target_id":"github:wanghongenpin:network_proxy_flutter","source_url":"https://github.com/wanghongenpin/network_proxy_flutter"},{"type":"has_code","target_id":"github:raqibnur:quick-waitlist","source_url":"https://github.com/raqibnur/quick-waitlist"},{"type":"has_code","target_id":"github:Neet-Nestor:Telegram-Media-Downloader","source_url":"https://github.com/Neet-Nestor/Telegram-Media-Downloader"},{"type":"has_code","target_id":"github:dreammis:social-auto-upload","source_url":"https://github.com/dreammis/social-auto-upload"},{"type":"has_code","target_id":"github:imputnet:cobalt","source_url":"https://github.com/imputnet/cobalt"},{"type":"has_code","target_id":"github:dwarvesf:hidden","source_url":"https://github.com/dwarvesf/hidden"},{"type":"has_code","target_id":"github:Lymphatus:caesium-image-compressor","source_url":"https://github.com/Lymphatus/caesium-image-compressor"},{"type":"has_code","target_id":"github:CorentinTh:it-tools","source_url":"https://github.com/CorentinTh/it-tools"},{"type":"has_code","target_id":"github:putyy:res-downloader","source_url":"https://github.com/putyy/res-downloader"},{"type":"has_code","target_id":"github:hua0512:stream-rec","source_url":"https://github.com/hua0512/stream-rec"},{"type":"has_code","target_id":"github:zyronon:douyin","source_url":"https://github.com/zyronon/douyin"},{"type":"has_code","target_id":"github:lihaoyun6:QuickRecorder","source_url":"https://github.com/lihaoyun6/QuickRecorder"},{"type":"has_code","target_id":"github:chanind:hanzi-writer","source_url":"https://github.com/chanind/hanzi-writer"},{"type":"has_code","target_id":"github:stonith404:pingvin-share","source_url":"https://github.com/stonith404/pingvin-share"},{"type":"has_code","target_id":"github:usememos:memos","source_url":"https://github.com/usememos/memos"},{"type":"has_code","target_id":"github:drawdb-io:drawdb","source_url":"https://github.com/drawdb-io/drawdb"},{"type":"has_code","target_id":"github:lizongying:my-tv","source_url":"https://github.com/lizongying/my-tv"},{"type":"has_code","target_id":"github:JunkFood02:Seal","source_url":"https://github.com/JunkFood02/Seal"},{"type":"has_code","target_id":"github:houbb:sensitive-word","source_url":"https://github.com/houbb/sensitive-word"},{"type":"has_code","target_id":"github:agalwood:Motrix","source_url":"https://github.com/agalwood/Motrix"},{"type":"has_code","target_id":"github:soybeanjs:soybean-admin","source_url":"https://github.com/soybeanjs/soybean-admin"},{"type":"has_code","target_id":"github:DamascenoRafael:reminders-menubar","source_url":"https://github.com/DamascenoRafael/reminders-menubar"},{"type":"has_code","target_id":"github:tryzealot:zealot","source_url":"https://github.com/tryzealot/zealot"},{"type":"has_code","target_id":"github:Meekdai:Gmeek","source_url":"https://github.com/Meekdai/Gmeek"},{"type":"has_code","target_id":"github:jaywcjlove:DevHub","source_url":"https://github.com/jaywcjlove/DevHub"},{"type":"has_code","target_id":"github:zh-lx:pinyin-pro","source_url":"https://github.com/zh-lx/pinyin-pro"},{"type":"has_code","target_id":"github:yesmore:vmail","source_url":"https://github.com/yesmore/vmail"},{"type":"has_code","target_id":"github:iawia002:lux","source_url":"https://github.com/iawia002/lux"},{"type":"has_code","target_id":"github:milanvarady:Applite","source_url":"https://github.com/milanvarady/Applite"},{"type":"has_code","target_id":"github:027xiguapi:pear-rec","source_url":"https://github.com/027xiguapi/pear-rec"},{"type":"has_code","target_id":"github:lcomplete:huntly","source_url":"https://github.com/lcomplete/huntly"},{"type":"has_code","target_id":"github:BingyanStudio:LapisCV","source_url":"https://github.com/BingyanStudio/LapisCV"},{"type":"has_code","target_id":"github:KurtBestor:Hitomi-Downloader","source_url":"https://github.com/KurtBestor/Hitomi-Downloader"},{"type":"has_code","target_id":"github:cooderl:wewe-rss","source_url":"https://github.com/cooderl/wewe-rss"},{"type":"has_code","target_id":"github:gitbutlerapp:gitbutler","source_url":"https://github.com/gitbutlerapp/gitbutler"},{"type":"has_code","target_id":"github:Lissy93:web-check","source_url":"https://github.com/Lissy93/web-check"},{"type":"has_code","target_id":"github:exelban:stats","source_url":"https://github.com/exelban/stats"},{"type":"has_code","target_id":"github:TeamNewPipe:NewPipe","source_url":"https://github.com/TeamNewPipe/NewPipe"},{"type":"has_code","target_id":"github:qarmin:czkawka","source_url":"https://github.com/qarmin/czkawka"},{"type":"has_code","target_id":"github:oddfar:campus-imaotai","source_url":"https://github.com/oddfar/campus-imaotai"},{"type":"has_code","target_id":"github:MrKai77:Loop","source_url":"https://github.com/MrKai77/Loop"},{"type":"has_code","target_id":"github:0xdevalias:chatgpt-source-watch","source_url":"https://github.com/0xdevalias/chatgpt-source-watch"},{"type":"has_code","target_id":"github:goenning:google-indexing-script","source_url":"https://github.com/goenning/google-indexing-script"},{"type":"has_code","target_id":"github:AmruthPillai:Reactive-Resume","source_url":"https://github.com/AmruthPillai/Reactive-Resume"},{"type":"has_code","target_id":"github:YiNNx:cmd-wrapped","source_url":"https://github.com/YiNNx/cmd-wrapped"},{"type":"has_code","target_id":"github:rwv:lookscanned.io","source_url":"https://github.com/rwv/lookscanned.io"},{"type":"has_code","target_id":"github:lanceliao:china-holiday-calender","source_url":"https://github.com/lanceliao/china-holiday-calender"},{"type":"has_code","target_id":"github:movie-web:movie-web","source_url":"https://github.com/movie-web/movie-web"},{"type":"has_code","target_id":"github:KRTirtho:spotube","source_url":"https://github.com/KRTirtho/spotube"},{"type":"has_code","target_id":"github:fatwang2:fasturl","source_url":"https://github.com/fatwang2/fasturl"},{"type":"has_code","target_id":"github:molvqingtai:WebChat","source_url":"https://github.com/molvqingtai/WebChat"}]', NULL, NULL, 'pending', 70, '38924a4362eb33d84977e6121068dc4e', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-GitHubDaily-GitHubDaily from https://github.com/GitHubDaily.png
Image converted to WebP: data/images/github-GitHubDaily-GitHubDaily.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Kong-kong', 'github--kong--kong', 'kong', 'Kong', '[![][kong-logo]][kong-url] !Stars !GitHub commit activity [![Build Status][badge-action-image]][badge-action-url] !Version !License Kong or Kong Gateway is a cloud-native, platform-agnostic, scalable **API ğ–§¹ LLM ğ–§¹ MCP** Gateway distinguished for its high performance and extensibility via plugins. It also provides advanced AI traffic capabilities with multi-LLM support, semantic security, MCP traffic security and analytics, and more. By providing functionality for proxying, routing, load bal...', '["ai","ai-gateway","api-gateway","api-management","apis","artificial-intelligence","cloud-native","devops","kubernetes","kubernetes-ingress","kubernetes-ingress-controller","llm-gateway","llm-ops","mcp","mcp-gateway","microservice","microservices","openai-proxy","reverse-proxy","serverless","lua"]', 'other', 42325, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Kong/kong","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '[![][kong-logo]][kong-url]\n\n![Stars](https://img.shields.io/github/stars/Kong/kong?style=flat-square) ![GitHub commit activity](https://img.shields.io/docker/pulls/_/kong?style=flat-square) [![Build Status][badge-action-image]][badge-action-url] ![Version](https://img.shields.io/github/v/release/Kong/kong?color=green&label=Version&style=flat-square)  ![License](https://img.shields.io/badge/License-Apache%202.0-blue?style=flat-square) [![Twitter Follow](https://img.shields.io/twitter/follow/thekonginc?style=social)](https://x.com/thekonginc)\n\n\nKong or Kong Gateway is a cloud-native, platform-agnostic, scalable **API ğ–§¹ LLM ğ–§¹ MCP** Gateway distinguished for its high performance and extensibility via plugins. It also provides advanced AI traffic capabilities with multi-LLM support, semantic security, MCP traffic security and analytics, and more.\n\nBy providing functionality for proxying, routing, load balancing, health checking, authentication (and [more](#features)), Kong serves as the central layer for orchestrating microservices or conventional API traffic - and agentic LLM and MCP as well - with ease.\n\nKong runs natively on Kubernetes thanks to its official [Kubernetes Ingress Controller](https://github.com/Kong/kubernetes-ingress-controller).\n\n<br />\n\n[![][kong-diagram]][kong-url]\n\n---\n\n[Installation](https://konghq.com/install/#kong-community) | [Documentation](https://docs.konghq.com) | [Discussions](https://github.com/Kong/kong/discussions) | [Forum](https://discuss.konghq.com) | [Blog](https://konghq.com/blog) | [Builds][kong-master-builds] | [AI Gateway](https://konghq.com/products/kong-ai-gateway) | [Cloud Hosted Kong](https://konghq.com/kong-konnect/)\n\n---\n\n## Getting Started\n\nIf you prefer to use a cloud-hosted Kong, you can [sign up for a free trial of Kong Konnect](https://konghq.com/products/kong-konnect/register?utm_medium=Referral&utm_source=Github&utm_campaign=kong-gateway&utm_content=konnect-promo-in-gateway&utm_term=get-started) and get started in minutes. If not, you can follow the instructions below to get started with Kong on your own infrastructure.\n\nLetâ€™s test drive Kong by adding authentication to an API in under 5 minutes.\n\nWe suggest using the docker-compose distribution via the instructions below, but there is also a [docker installation](https://docs.konghq.com/gateway/latest/install/docker/#install-kong-gateway-in-db-less-mode) procedure if youâ€™d prefer to run the Kong Gateway in DB-less mode.\n\nWhether youâ€™re running in the cloud, on bare metal, or using containers, you can find every supported distribution on our [official installation](https://konghq.com/install/#kong-community) page.\n\n1) To start, clone the Docker repository and navigate to the compose folder.\n```cmd\n  $ git clone https://github.com/Kong/docker-kong\n  $ cd docker-kong/compose/\n```\n\n2) Start the Gateway stack using:\n```cmd\n  $ KONG_DATABASE=postgres docker-compose --profile database up\n```\n\nThe Gateway is now available on the following ports on localhost:\n\n- `:8000` - send traffic to your service via Kong\n- `:8001` - configure Kong using Admin API or via [decK](https://github.com/kong/deck)\n- `:8002` - access Kong''s management Web UI ([Kong Manager](https://github.com/Kong/kong-manager)) on [localhost:8002](http://localhost:8002)\n\nNext, follow the [quick start guide](https://docs.konghq.com/gateway-oss/latest/getting-started/configuring-a-service/\n) to tour the Gateway features.\n\n### Getting started with AI Gateway for LLM and MCP\n\nIf you would like to get started with Kong AI Gateway capabilities including LLM and MCP features, please refer to the [official AI documentation](https://developer.konghq.com/ai-gateway/).\n\n## Features\n\nBy centralizing common API, AI and MCP functionality across all your organization''s services, Kong Gateway creates more freedom for engineering teams to focus on the challenges that matter most.\n\nThe top Kong features include:\n\n- Advanced routing, load balancing, health checking - all configurable via a RESTful admin API or declarative configuration.\n- Authentication and authorization for APIs using methods like JWT, basic auth, OAuth, ACLs and more.\n- Universal LLM API to route across multiple providers like OpenAI, Anthropic, GCP Gemini, AWS Bedrock, Azure AI, Databricks, Mistral, Huggingface and more.\n- MCP traffic governance, MCP security and MCP observability in addition to MCP autogeneration from any RESTful API.\n- 60+ AI features like AI observability, semantic security and caching, semantic routing and more.\n- Proxy, SSL/TLS termination, and connectivity support for L4 or L7 traffic.\n- Plugins for enforcing traffic controls, rate limiting, req/res transformations, logging, monitoring and including a plugin developer hub.\n- Sophisticated deployment models like Declarative Databaseless Deployment and Hybrid Deployment (control plane/data plane separation) without any vendor lock-in.\n- Native [ingress controller](https://github.com/Kong/kubernetes-ingress-controller) support for serving Kubernetes.\n\n[![][kong-benefits]][kong-url]\n\n### Plugin Hub\n\nPlugins provide advanced functionality that extends the use of the Gateway. Many of the Kong Inc. and community-developed plugins like AWS Lambda, Correlation ID, and Response Transformer are showcased at the [Plugin Hub](https://docs.konghq.com/hub/).\n\nContribute to the Plugin Hub and ensure your next innovative idea is published and available to the broader community!\n\n## Contributing\n\nWe â¤ï¸ pull requests, and weâ€™re continually working hard to make it as easy as possible for developers to contribute. Before beginning development with the Kong Gateway, please familiarize yourself with the following developer resources:\n\n- Community Pledge ([COMMUNITY_PLEDGE.md](COMMUNITY_PLEDGE.md)) for our pledge to interact with you, the open source community.\n- Contributor Guide ([CONTRIBUTING.md](CONTRIBUTING.md)) to learn about how to contribute to Kong.\n- Development Guide ([DEVELOPER.md](DEVELOPER.md)): Setting up your development environment.\n- [CODE_OF_CONDUCT](CODE_OF_CONDUCT.md) and [COPYRIGHT](COPYRIGHT)\n\nUse the [Plugin Development Guide](https://docs.konghq.com/latest/plugin-development/) for building new and creative plugins, or browse the online version of Kong''s source code documentation in the [Plugin Development Kit (PDK) Reference](https://docs.konghq.com/latest/pdk/). Developers can build plugins in [Lua](https://docs.konghq.com/gateway/latest/plugin-development/), [Go](https://docs.konghq.com/gateway-oss/latest/external-plugins/#developing-go-plugins) or [JavaScript](https://docs.konghq.com/gateway-oss/latest/external-plugins/#developing-javascript-plugins).\n\n## Releases\n\nPlease see the [Changelog](CHANGELOG.md) for more details about a given release. The [SemVer Specification](https://semver.org) is followed when versioning Gateway releases.\n\n## Join the Community\n\n- Check out the [docs](https://docs.konghq.com/)\n- Join the [Kong discussions forum](https://github.com/Kong/kong/discussions)\n- Join the Kong discussions at the Kong Nation forum: [https://discuss.konghq.com/](https://discuss.konghq.com/)\n- Join our [Community Slack](http://kongcommunity.slack.com/)\n- Read up on the latest happenings at our [blog](https://konghq.com/blog/)\n- Follow us on [X](https://x.com/thekonginc)\n- Subscribe to our [YouTube channel](https://www.youtube.com/c/KongInc/videos)\n- Visit our [homepage](https://konghq.com/) to learn more\n\n## Konnect Cloud\n\nKong Inc. offers commercial subscriptions that enhance the Kong Gateway in a variety of ways. Customers of Kong''s [Konnect Cloud](https://konghq.com/kong-konnect/) subscription take advantage of additional gateway functionality, commercial support, and access to Kong''s managed (SaaS) control plane platform. The Konnect Cloud platform features include real-time analytics, a service catalog, developer portals, and so much more! [Get started](https://konghq.com/products/kong-konnect/register?utm_medium=Referral&utm_source=Github&utm_campaign=kong-gateway&utm_content=konnect-promo-in-gateway&utm_term=get-started) with Konnect Cloud.\n\n## License\n\n```\nCopyright 2016-2025 Kong Inc.\n\nLicensed under the Apache License, Version 2.0 (the "License");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an "AS IS" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n[kong-url]: https://konghq.com/\n[kong-logo]: https://konghq.com/wp-content/uploads/2018/05/kong-logo-github-readme.png\n[kong-diagram]: https://assets.prd.mktg.konghq.com/images/2025/09/68cc1bfd-kong-diagram.png?v=2\n[kong-benefits]: https://konghq.com/wp-content/uploads/2018/05/kong-benefits-github-readme.png\n[kong-master-builds]: https://hub.docker.com/r/kong/kong/tags\n[badge-action-url]: https://github.com/Kong/kong/actions\n[badge-action-image]: https://github.com/Kong/kong/actions/workflows/build_and_test.yml/badge.svg?branch=master&event=push\n\n[busted]: https://github.com/Olivine-Labs/busted\n[luacheck]: https://github.com/mpeterv/luacheck\n', '{"language":"Lua","stars":42325,"forks":5037,"watchers":42325,"open_issues":131,"topics":["ai","ai-gateway","api-gateway","api-management","apis","artificial-intelligence","cloud-native","devops","kubernetes","kubernetes-ingress","kubernetes-ingress-controller","llm-gateway","llm-ops","mcp","mcp-gateway","microservice","microservices","openai-proxy","reverse-proxy","serverless"],"default_branch":"master","size_kb":97393,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:Kong:kubernetes-ingress-controller","source_url":"https://github.com/Kong/kubernetes-ingress-controller"},{"type":"has_code","target_id":"github:Kong:kong","source_url":"https://github.com/Kong/kong"},{"type":"has_code","target_id":"github:Kong:docker-kong","source_url":"https://github.com/Kong/docker-kong"},{"type":"has_code","target_id":"github:kong:deck","source_url":"https://github.com/kong/deck"},{"type":"has_code","target_id":"github:Kong:kong-manager","source_url":"https://github.com/Kong/kong-manager"},{"type":"has_code","target_id":"github:Kong:kubernetes-ingress-controller","source_url":"https://github.com/Kong/kubernetes-ingress-controller"},{"type":"has_code","target_id":"github:Kong:kong","source_url":"https://github.com/Kong/kong"},{"type":"has_code","target_id":"github:Kong:kong","source_url":"https://github.com/Kong/kong"},{"type":"has_code","target_id":"github:Kong:kong","source_url":"https://github.com/Kong/kong"},{"type":"has_code","target_id":"github:Olivine-Labs:busted","source_url":"https://github.com/Olivine-Labs/busted"},{"type":"has_code","target_id":"github:mpeterv:luacheck","source_url":"https://github.com/mpeterv/luacheck"}]', NULL, 'Apache-2.0', 'approved', 65, 'a3f2a3f0db9239ae543a88cf70fe2f78', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Kong-kong from https://github.com/Kong.png
Image converted to WebP: data/images/github-Kong-kong.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-upscayl-upscayl', 'github--upscayl--upscayl', 'upscayl', 'upscayl', '<div align="center"> # v2.15 is out! ğŸ¥³ Download Now â¬‡ï¸ <h3>Special thanks to our sponsors:</h3> <a href="https://www.warp.dev/upscayl"> <div> <img alt="Warp sponsorship" width="500" src="https://github.com/user-attachments/assets/7626b7c6-e673-45ee-ac9a-3392e643f54f"> </div> <b>Warp, the intelligent terminal for developers</b> <div> <sup>Use images as AI context in your terminal!</sup> </div> </a> <a href="https://requestly.com/upscayl"> <div> <img alt="Requestly sponsorship" width="500" src...', '["ai","electron","esrgan","gigapixel","gigapixel-images","image","image-upscaling","topaz","upscale","upscalerimage","upscayl","typescript"]', 'other', 41593, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/upscayl/upscayl","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n\n  # v2.15 is out! ğŸ¥³ [Download Now â¬‡ï¸](https://github.com/upscayl/upscayl/releases/latest)\n\n<h3>Special thanks to our sponsors:</h3>\n<a href="https://www.warp.dev/upscayl">\n   <div>\n   <img alt="Warp sponsorship" width="500" src="https://github.com/user-attachments/assets/7626b7c6-e673-45ee-ac9a-3392e643f54f">\n   </div>\n   <b>Warp, the intelligent terminal for developers</b>\n   <div>\n      <sup>Use images as AI context in your terminal!</sup>\n   </div>\n</a>\n\n<a href="https://requestly.com/upscayl">\n   <div>\n   <img alt="Requestly sponsorship" width="500" src="https://github.com/user-attachments/assets/24670320-997d-4d62-9bca-955c59fe883d">\n   </div>\n   <b>Requestly - Free & Open-Source alternative to Postman</b>\n   <div>\n      <sup>All-in-one platform to Test, Mock and Intercept APIs.</sup>\n   </div>\n</a>\n\n#\n\n<a href="https://github.com/upscayl/upscayl/releases/latest">\n  \n  ![Frame 111](https://github.com/upscayl/upscayl/assets/25067102/d1b4af3c-aade-4bc9-97d0-cf88db679931)\n</a>\n\n<a href="https://upscayl.org/#download">\n  <img src="https://img.shields.io/github/downloads/upscayl/upscayl/total.svg?style=for-the-badge&logo=data:image/png;base64,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&labelColor=ede9fe&color=8e6bf6" width="200px" style="border-radius:50%"/>\n</a>\n\n  </br>\n  </br>\n\n<!--<a href="https://github.com/upscayl/upscayl/releases/latest">\n  <img src="https://github.com/upscayl/upscayl/assets/25067102/6287fd40-2c91-4028-b1d6-3986e77d8211" width="200px" />\n</a>-->\n\n<a href="https://t.me/iamnayam">\n  <img src="https://user-images.githubusercontent.com/25067102/209297095-a3db856f-b760-40bb-a68e-f3a3086e18c7.png" width="200px" />   \n</a>      \n\n<a href="https://x.com/upscayl">\n  <img src="https://github.com/upscayl/upscayl/assets/25067102/917dcf6f-452b-43e6-95cd-2c6b0a47913d" width="200px" />\n</a>\n\n# ğŸ†™ Upscayl\n\n#### Free and Open Source AI Image Upscaler\nUpscayl lets you enlarge and enhance low-resolution images using advanced AI algorithms.\nEnlarge images without losing quality. It''s almost like magic! ğŸ©ğŸª„\n\n**https://upscayl.org**\n\n<video src="https://github.com/upscayl/upscayl/assets/25067102/ad2453b1-3c5a-4eb2-b992-4cf10e6a49f5" autoplay muted loop />\n\n</div>\n\n# Contents\n\n- [ğŸ‘¨â€ğŸ’» Installation](#-installation)\n  - [ğŸ§ Linux](#-linux)\n  - [ğŸ macOS](#-macos)\n  - [ğŸŒ Windows](#-windows)\n- [ğŸ‘¨â€ğŸ« Documentation - Tutorials and Guides](#-documentation---tutorials-and-guides)\n- [âš–ï¸ Demo Results (Before and After)](#%EF%B8%8F-results)\n- [ğŸ¤« Roadmap](#-roadmap)\n- [ğŸ›  Developing Upscayl](#-development)\n- [ğŸ¤“ FAQ](#-faq)\n- [ğŸ Donate and support the project](#-donate)\n- [â¤ Credits](#-credits)\n\n# ğŸ‘¨â€ğŸ’» Installation\n\n> [!IMPORTANT]\n> You''ll need a Vulkan compatible GPU (Graphics Card) to upscale images. Many iGPUs (integrated graphics) do not work but, no harm in trying :)\n\n## ğŸ§ Linux\n\n  <a href="https://flathub.org/apps/org.upscayl.Upscayl">\n    <img src="https://dl.flathub.org/assets/badges/flathub-badge-en.svg" height="50px"/>\n  </a>\n\n  <a href="https://appimage.github.io/Upscayl/">\n    <img src="https://user-images.githubusercontent.com/25067102/191270389-9de37c0f-39a8-41f1-a659-8dd4e7b8ac28.png" height="50px"/>\n  </a>\n\n  <a href="https://aur.archlinux.org/packages/upscayl-bin">\n    <img src="https://user-images.githubusercontent.com/25067102/191269445-87050a77-c304-4284-9ea0-699721309c59.png" height="50px"/>\n  </a>\n\n  <a href="https://snapcraft.io/upscayl/">\n    <img src="https://snapcraft.io/static/images/badges/en/snap-store-black.svg" height="50px"/>\n  </a>\n\n  <a href="https://github.com/MrPenguin07/ebuilds">\n    <img src="https://github.com/upscayl/upscayl/assets/25067102/322aebc5-91aa-4fc6-ac3a-d5a054449554" height="50px"/>\n  </a>\n\nUpscayl should be available on the software listings of most Linux operating systems. Your distro''s Store app might also support the [Flatpak](https://flatpak.org/setup) or Snap version.\n\n### ğŸ’¼ Portable Method\n\n1. Go to [releases section](https://github.com/upscayl/upscayl/releases/latest) or [our official website](https://upscayl.org/).\n2. Download the `upscayl-x.x.x-linux.AppImage` file.\n3. Right Click AppImage -> Go to Permissions tab -> Check ''allow file to execute'' and then double click the file to run Upscayl.\n\n*You can also choose to install using other formats like RPM (Fedora), DEB (Debian/Ubuntu based), and ZIP (Any x86 Linux OS).*\n\n## ğŸ macOS\n(MacOS 12 and later)\n\n<a href="https://apps.apple.com/us/app/upscayl/id6468265473?mt=12">\n  <img src="https://www.upscayl.org/appstore.svg" height="60px"/>\n</a>\n\n1. Go to [releases section](https://github.com/upscayl/upscayl/releases/latest) or [our official website](https://upscayl.org/).\n2. Download the `upscayl-x.x.x-mac.dmg` file.\n3. Double click dmg, drag Upscayl icon into Applications folder.\n4. Open Finder, click ''Applications'' tab in the left sidebar. Find Upscayl and right click on it. Select ''Open''.\n5. In the window that appears, press ''Open'' yet again.\n\n### ğŸº Homebrew\n\n`brew install --cask upscayl`\n\n## ğŸŒ Windows\n(Windows 10 and later)\n\n1. Go to [releases section](https://github.com/upscayl/upscayl/releases/latest) or [our official website](https://upscayl.org/).\n2. Download the `upscayl-x.x.x-win.exe` file.\n3. Double click exe file to launch.\n4. If you get a SmartScreen warning - click ''More Info'' and then ''Run Anyway'' OR press ''YES'' on the unverified publisher dialog.\n5. Follow the installation steps.\n6. Profit!\n\n# ğŸ‘¨â€ğŸ« Documentation - Tutorials and Guides\n\nCheck out our Documentation [here](https://docs.upscayl.org/).\n\n- [Try out even more new models!](https://github.com/upscayl/custom-models)\n- [Convert your own models](https://github.com/upscayl/upscayl/wiki/%F0%9F%96%A5%EF%B8%8F-Model-Conversion---Create-more-AI-models!)\n- [Compatibility List](https://github.com/upscayl/upscayl/wiki/Compatibility-List)\n- [Troubleshooting](https://github.com/upscayl/upscayl/wiki/Troubleshooting)\n\n# âš–ï¸ Results\n\nCheck out Upscayl before/after comparisons [here](COMPARISONS.MD).\n\n# ğŸ¤« Roadmap\n\nYou can track all the progress here: https://github.com/orgs/upscayl/projects/1\n\n- Fix bugs\n- Make the whole world use FOSS (WIP ğŸš§)\n\n# ğŸ›  Development\n\nI recommend using Volta: https://volta.sh for installing Node.js.\nDownload and install volta, then do: `volta install node`.\n\n## ğŸƒ Running\n> [!NOTE]\n> If you are not willing to install [git](https://git-scm.com/downloads), you can skip the first line, download [the source zip](https://github.com/upscayl/upscayl/archive/refs/heads/main.zip) and extract it to `upscayl` instead and carry on with the rest of the instructions.\n\n```sh\ngit clone https://github.com/upscayl/upscayl\ncd upscayl\n\n# INSTALL DEPENDENCIES\nnpm install\n\n# RUN THE DEVELOPMENT SERVER LOCALLY\n## YOUR LOGS WILL NOW APPEAR IN THE TERMINAL\nnpm run start\n```\n\n## ğŸ—ï¸ Building\n\n```sh\n# INSTALL DEPENDENCIES\nnpm install\n\n# PACKAGE THE APP\nnpm run dist\n\n# PUBLISH THE APP, MAKE SURE TO ADD GH_TOKEN= IN SHELL\n# ONLY DO THIS IF YOU''RE A MAINTAINER\nnpm run publish-app\n```\n\n# ğŸ¤“ FAQ\n\n- **How does Upscayl work?**\n  - Upscayl uses AI models to enhance your images by guessing what the details could be. It uses Real-ESRGAN and Vulkan architecture to achieve this. [Our backend](https://github.com/upscayl/upscayl-ncnn) is fully open-source under the AGPLv3 license.\n- **I don''t see a drastic change in my upscaled image. Why is that?**\n  - Upscayl can enhance low resolution images and images that are pixelated but it cannot de-blur or do focus adjustment on your image. If your image is out-of-focus or totally blurred, Upscayl is not the right tool for it. Please use images that are similar to the [examples we''ve given here.](COMPARISONS.MD)\n- **Is there a CLI available?**\n  - The CLI tool is called [upscayl-ncnn](https://github.com/upscayl/upscayl-ncnn).\n- **Do I need a GPU for this to work?**\n  - Yes, unfortunately. NCNN Vulkan requires a Vulkan-compatible GPU. Upscayl won''t work with **most** iGPUs or CPUs. But hey, no harm in trying ;)\n    - @Wyrdgirn has contributed a workaround for Windows and Linux in [#390](https://github.com/upscayl/upscayl/issues/390)! Nobody knows how to manipulate the macOS and Haiku frameworks...\n- **I stopped the magic Batch Upscayl and my images haven''t been processed, compressed, or are in the wrong scale!**\n  - When a model doesn''t support an action, Upscayl will finish upscayling all the images first before post-processing them. What this means is that you should simply **wait** for the process to finish.\n- **How can I contribute?**\n  - You can report issues, fix code and add features by submitting PRs, or donate! ğŸ˜Š\n- **What''s the GPU ID for?**\n  - It is for selecting which GPU to use. The specific procedure is detailed in the [Wiki](https://github.com/upscayl/upscayl/wiki/Guide).\n    - Note that for Windows systems, if Upscayl is not set to performance mode, the system may override this setting.\n- **Where do I find more models?**\n  -  More models can be taken from here: https://github.com/upscayl/custom-models\n\n# ğŸ Donate\n\n<a href="https://www.buymeacoffee.com/fossisthefuture">\n  <img src="https://user-images.githubusercontent.com/25067102/154570688-9e143f2b-fee3-4b05-a9d2-a7a3013b2b51.png" />\n<a/>\n\n# â¤ Credits\n\n- Real-ESRGAN for their wonderful research work.\n[Real-ESRGAN: Copyright (c) 2021, Xintao Wang](https://github.com/xinntao/Real-ESRGAN/)\n- @JanDeDinoMan, @xanderfrangos, @Fdawgs, @keturn for their code contributions\n- @aaronliu0130 for providing community support :)\n- Helaman for their [HFA2k model](https://openmodeldb.info/models/4x-HFA2k) (included as "High Fidelity")\n- Foolhardy for their [Remacri model](https://openmodeldb.info/models/4x-Remacri).\n- [Kim2091](https://upscale.wiki/wiki/User:Kim2091)	for their [Ultrasharp and Ultramix Balanced model](https://openmodeldb.info/models/4x-UltraSharp).\n- @NicKoehler for their amazing logo :)\n#\n\n<div align="center">\n\nCopyright Â© 2023 - **Upscayl**\\nBy Nayam Amarshe and TGS963\\nMade with ğŸ–± & âŒ¨\n\n</div>\n', '{"language":"TypeScript","stars":41593,"forks":1966,"watchers":41593,"open_issues":65,"topics":["ai","electron","esrgan","gigapixel","gigapixel-images","image","image-upscaling","topaz","upscale","upscalerimage","upscayl"],"default_branch":"main","size_kb":567613,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:MrPenguin07:ebuilds\">","source_url":"https://github.com/MrPenguin07/ebuilds\">"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:upscayl:custom-models","source_url":"https://github.com/upscayl/custom-models"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:orgs:upscayl","source_url":"https://github.com/orgs/upscayl"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:upscayl:upscayl-ncnn","source_url":"https://github.com/upscayl/upscayl-ncnn"},{"type":"has_code","target_id":"github:upscayl:upscayl-ncnn","source_url":"https://github.com/upscayl/upscayl-ncnn"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:upscayl:upscayl","source_url":"https://github.com/upscayl/upscayl"},{"type":"has_code","target_id":"github:upscayl:custom-models","source_url":"https://github.com/upscayl/custom-models"},{"type":"has_code","target_id":"github:xinntao:Real-ESRGAN","source_url":"https://github.com/xinntao/Real-ESRGAN"}]', NULL, 'AGPL-3.0', 'approved', 80, '5e63c36efe2f00b606c27ca44665fda8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-upscayl-upscayl from https://github.com/upscayl.png
Image converted to WebP: data/images/github-upscayl-upscayl.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-crewAIInc-crewAI', 'github--crewaiinc--crewai', 'crewAI', 'crewAIInc', '<p align="center"> <a href="https://github.com/crewAIInc/crewAI"> <img src="docs/images/crewai_logo.png" width="600px" alt="Open source Multi-AI Agent orchestration framework"> </a> </p> <p align="center" style="display: flex; justify-content: center; gap: 20px; align-items: center;"> <a href="https://trendshift.io/repositories/11239" target="_blank"> <img src="https://trendshift.io/api/badge/repositories/11239" alt="crewAIInc%2FcrewAI | Trendshift" style="width: 250px; height: 55px;" width="...', '["agents","ai","ai-agents","aiagentframework","llms","python"]', 'other', 41146, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/crewAIInc/crewAI","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <a href="https://github.com/crewAIInc/crewAI">\n    <img src="docs/images/crewai_logo.png" width="600px" alt="Open source Multi-AI Agent orchestration framework">\n  </a>\n</p>\n<p align="center" style="display: flex; justify-content: center; gap: 20px; align-items: center;">\n  <a href="https://trendshift.io/repositories/11239" target="_blank">\n    <img src="https://trendshift.io/api/badge/repositories/11239" alt="crewAIInc%2FcrewAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/>\n  </a>\n</p>\n\n<p align="center">\n  <a href="https://crewai.com">Homepage</a>\n  Â·\n  <a href="https://docs.crewai.com">Docs</a>\n  Â·\n  <a href="https://app.crewai.com">Start Cloud Trial</a>\n  Â·\n  <a href="https://blog.crewai.com">Blog</a>\n  Â·\n  <a href="https://community.crewai.com">Forum</a>\n</p>\n\n<p align="center">\n  <a href="https://github.com/crewAIInc/crewAI">\n    <img src="https://img.shields.io/github/stars/crewAIInc/crewAI" alt="GitHub Repo stars">\n  </a>\n  <a href="https://github.com/crewAIInc/crewAI/network/members">\n    <img src="https://img.shields.io/github/forks/crewAIInc/crewAI" alt="GitHub forks">\n  </a>\n  <a href="https://github.com/crewAIInc/crewAI/issues">\n    <img src="https://img.shields.io/github/issues/crewAIInc/crewAI" alt="GitHub issues">\n  </a>\n  <a href="https://github.com/crewAIInc/crewAI/pulls">\n    <img src="https://img.shields.io/github/issues-pr/crewAIInc/crewAI" alt="GitHub pull requests">\n  </a>\n  <a href="https://opensource.org/licenses/MIT">\n    <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License: MIT">\n  </a>\n</p>\n\n<p align="center">\n  <a href="https://pypi.org/project/crewai/">\n    <img src="https://img.shields.io/pypi/v/crewai" alt="PyPI version">\n  </a>\n  <a href="https://pypi.org/project/crewai/">\n    <img src="https://img.shields.io/pypi/dm/crewai" alt="PyPI downloads">\n  </a>\n  <a href="https://twitter.com/crewAIInc">\n    <img src="https://img.shields.io/twitter/follow/crewAIInc?style=social" alt="Twitter Follow">\n  </a>\n</p>\n\n### Fast and Flexible Multi-Agent Automation Framework\n\n> CrewAI is a lean, lightning-fast Python framework built entirely from scratchâ€”completely **independent of LangChain or other agent frameworks**.\n> It empowers developers with both high-level simplicity and precise low-level control, ideal for creating autonomous AI agents tailored to any scenario.\n\n- **CrewAI Crews**: Optimize for autonomy and collaborative intelligence.\n- **CrewAI Flows**: Enable granular, event-driven control, single LLM calls for precise task orchestration and supports Crews natively\n\nWith over 100,000 developers certified through our community courses at [learn.crewai.com](https://learn.crewai.com), CrewAI is rapidly becoming the\nstandard for enterprise-ready AI automation.\n\n# CrewAI AOP Suite\n\nCrewAI AOP Suite is a comprehensive bundle tailored for organizations that require secure, scalable, and easy-to-manage agent-driven automation.\n\nYou can try one part of the suite the [Crew Control Plane for free](https://app.crewai.com)\n\n## Crew Control Plane Key Features:\n\n- **Tracing & Observability**: Monitor and track your AI agents and workflows in real-time, including metrics, logs, and traces.\n- **Unified Control Plane**: A centralized platform for managing, monitoring, and scaling your AI agents and workflows.\n- **Seamless Integrations**: Easily connect with existing enterprise systems, data sources, and cloud infrastructure.\n- **Advanced Security**: Built-in robust security and compliance measures ensuring safe deployment and management.\n- **Actionable Insights**: Real-time analytics and reporting to optimize performance and decision-making.\n- **24/7 Support**: Dedicated enterprise support to ensure uninterrupted operation and quick resolution of issues.\n- **On-premise and Cloud Deployment Options**: Deploy CrewAI AOP on-premise or in the cloud, depending on your security and compliance requirements.\n\nCrewAI AOP is designed for enterprises seeking a powerful, reliable solution to transform complex business processes into efficient,\nintelligent automations.\n\n## Table of contents\n\n- [Why CrewAI?](#why-crewai)\n- [Getting Started](#getting-started)\n- [Key Features](#key-features)\n- [Understanding Flows and Crews](#understanding-flows-and-crews)\n- [CrewAI vs LangGraph](#how-crewai-compares)\n- [Examples](#examples)\n  - [Quick Tutorial](#quick-tutorial)\n  - [Write Job Descriptions](#write-job-descriptions)\n  - [Trip Planner](#trip-planner)\n  - [Stock Analysis](#stock-analysis)\n  - [Using Crews and Flows Together](#using-crews-and-flows-together)\n- [Connecting Your Crew to a Model](#connecting-your-crew-to-a-model)\n- [How CrewAI Compares](#how-crewai-compares)\n- [Frequently Asked Questions (FAQ)](#frequently-asked-questions-faq)\n- [Contribution](#contribution)\n- [Telemetry](#telemetry)\n- [License](#license)\n\n## Why CrewAI?\n\n<div align="center" style="margin-bottom: 30px;">\n  <img src="docs/images/asset.png" alt="CrewAI Logo" width="100%">\n</div>\n\nCrewAI unlocks the true potential of multi-agent automation, delivering the best-in-class combination of speed, flexibility, and control with either Crews of AI Agents or Flows of Events:\n\n- **Standalone Framework**: Built from scratch, independent of LangChain or any other agent framework.\n- **High Performance**: Optimized for speed and minimal resource usage, enabling faster execution.\n- **Flexible Low Level Customization**: Complete freedom to customize at both high and low levels - from overall workflows and system architecture to granular agent behaviors, internal prompts, and execution logic.\n- **Ideal for Every Use Case**: Proven effective for both simple tasks and highly complex, real-world, enterprise-grade scenarios.\n- **Robust Community**: Backed by a rapidly growing community of over **100,000 certified** developers offering comprehensive support and resources.\n\nCrewAI empowers developers and enterprises to confidently build intelligent automations, bridging the gap between simplicity, flexibility, and performance.\n\n## Getting Started\n\nSetup and run your first CrewAI agents by following this tutorial.\n\n[![CrewAI Getting Started Tutorial](https://img.youtube.com/vi/-kSOTtYzgEw/hqdefault.jpg)](https://www.youtube.com/watch?v=-kSOTtYzgEw "CrewAI Getting Started Tutorial")\n\n###\n Learning Resources\n\nLearn CrewAI through our comprehensive courses:\n\n- [Multi AI Agent Systems with CrewAI](https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/) - Master the fundamentals of multi-agent systems\n- [Practical Multi AI Agents and Advanced Use Cases](https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai/) - Deep dive into advanced implementations\n\n### Understanding Flows and Crews\n\nCrewAI offers two powerful, complementary approaches that work seamlessly together to build sophisticated AI applications:\n\n1. **Crews**: Teams of AI agents with true autonomy and agency, working together to accomplish complex tasks through role-based collaboration. Crews enable:\n\n   - Natural, autonomous decision-making between agents\n   - Dynamic task delegation and collaboration\n   - Specialized roles with defined goals and expertise\n   - Flexible problem-solving approaches\n2. **Flows**: Production-ready, event-driven workflows that deliver precise control over complex automations. Flows provide:\n\n   - Fine-grained control over execution paths for real-world scenarios\n   - Secure, consistent state management between tasks\n   - Clean integration of AI agents with production Python code\n   - Conditional branching for complex business logic\n\nThe true power of CrewAI emerges when combining Crews and Flows. This synergy allows you to:\n\n- Build complex, production-grade applications\n- Balance autonomy with precise control\n- Handle sophisticated real-world scenarios\n- Maintain clean, maintainable code structure\n\n### Getting Started with Installation\n\nTo get started with CrewAI, follow these simple steps:\n\n### 1. Installation\n\nEnsure you have Python >=3.10 <3.14 installed on your system. CrewAI uses [UV](https://docs.astral.sh/uv/) for dependency management and package handling, offering a seamless setup and execution experience.\n\nFirst, install CrewAI:\n\n```shell\npip install crewai\n```\n\nIf you want to install the ''crewai'' package along with its optional features that include additional tools for agents, you can do so by using the following command:\n\n```shell\npip install ''crewai[tools]''\n```\n\nThe command above installs the basic package and also adds extra components which require more dependencies to function.\n\n### Troubleshooting Dependencies\n\nIf you encounter issues during installation or usage, here are some common solutions:\n\n#### Common Issues\n\n1. **ModuleNotFoundError: No module named ''tiktoken''**\n\n   - Install tiktoken explicitly: `pip install ''crewai[embeddings]''`\n   - If using embedchain or other tools: `pip install ''crewai[tools]''`\n2. **Failed building wheel for tiktoken**\n\n   - Ensure Rust compiler is installed (see installation steps above)\n   - For Windows: Verify Visual C++ Build Tools are installed\n   - Try upgrading pip: `pip install --upgrade pip`\n   - If issues persist, use a pre-built wheel: `pip install tiktoken --prefer-binary`\n\n### 2. Setting Up Your Crew with the YAML Configuration\n\nTo create a new CrewAI project, run the following CLI (Command Line Interface) command:\n\n```shell\ncrewai create crew <project_name>\n```\n\nThis command creates a new project folder with the following structure:\n\n```\nmy_project/\nâ”œâ”€â”€ .gitignore\nâ”œâ”€â”€ pyproject.toml\nâ”œâ”€â”€ README.md\nâ”œâ”€â”€ .env\nâ””â”€â”€ src/\n    â””â”€â”€ my_project/\n        â”œâ”€â”€ __init__.py\n        â”œâ”€â”€ main.py\n        â”œâ”€â”€ crew.py\n        â”œâ”€â”€ tools/\n        â”‚   â”œâ”€â”€ custom_tool.py\n        â”‚   â””â”€â”€ __init__.py\n        â””â”€â”€ config/\n            â”œâ”€â”€ agents.yaml\n            â””â”€â”€ tasks.yaml\n```\n\nYou can now start developing your crew by editing the files in the `src/my_project` folder. The `main.py` file is the entry point of the project, the `crew.py` file is where you define your crew, the `agents.yaml` file is where you define your agents, and the `tasks.yaml` file is where you define your tasks.\n\n#### To customize your project, you can:\n\n- Modify `src/my_project/config/agents.yaml` to define your agents.\n- Modify `src/my_project/config/tasks.yaml` to define your tasks.\n- Modify `src/my_project/crew.py` to add your own logic, tools, and specific arguments.\n- Modify `src/my_project/main.py` to add custom inputs for your agents and tasks.\n- Add your environment variables into the `.env` file.\n\n#### Example of a simple crew with a sequential process:\n\nInstantiate your crew:\n\n```shell\ncrewai create crew latest-ai-development\n```\n\nModify the files as needed to fit your use case:\n\n**agents.yaml**\n\n```yaml\n# src/my_project/config/agents.yaml\nresearcher:\n  role: >\n    {topic} Senior Data Researcher\n  goal: >\n    Uncover cutting-edge developments in {topic}\n  backstory: >\n    You''re a seasoned researcher with a knack for uncovering the latest\n    developments in {topic}. Known for your ability to find the most relevant\n    information and present it in a clear and concise manner.\n\nreporting_analyst:\n  role: >\n    {topic} Reporting Analyst\n  goal: >\n    Create detailed reports based on {topic} data analysis and research findings\n  backstory: >\n    You''re a meticulous analyst with a keen eye for detail. You''re known for\n    your ability to turn complex data into clear and concise reports, making\n    it easy for others to understand and act on the information you provide.\n```\n\n**tasks.yaml**\n\n```yaml\n# src/my_project/config/tasks.yaml\nresearch_task:\n  description: >\n    Conduct a thorough research about {topic}\n    Make sure you find any interesting and relevant information given\n    the current year is 2025.\n  expected_output: >\n    A list with 10 bullet points of the most relevant information about {topic}\n  agent: researcher\n\nreporting_task:\n  description: >\n    Review the context you got and expand each topic into a full section for a report.\n    Make sure the report is detailed and contains any and all relevant information.\n  expected_output: >\n    A fully fledge reports with the mains topics, each with a full section of information.\n    Formatted as markdown without ''```''\n  agent: reporting_analyst\n  output_file: report.md\n```\n\n**crew.py**\n\n```python\n# src/my_project/crew.py\nfrom crewai import Agent, Crew, Process, Task\nfrom crewai.project import CrewBase, agent, crew, task\nfrom crewai_tools import SerperDevTool\nfrom crewai.agents.agent_builder.base_agent import BaseAgent\nfrom typing import List\n\n@CrewBase\nclass LatestAiDevelopmentCrew():\n	"""LatestAiDevelopment crew"""\n	agents: List[BaseAgent]\n	tasks: List[Task]\n\n	@agent\n	def researcher(self) -> Agent:\n		return Agent(\n			config=self.agents_config[''researcher''],\n			verbose=True,\n			tools=[SerperDevTool()]\n		)\n\n	@agent\n	def reporting_analyst(self) -> Agent:\n		return Agent(\n			config=self.agents_config[''reporting_analyst''],\n			verbose=True\n		)\n\n	@task\n	def research_task(self) -> Task:\n		return Task(\n			config=self.tasks_config[''research_task''],\n		)\n\n	@task\n	def reporting_task(self) -> Task:\n		return Task(\n			config=self.tasks_config[''reporting_task''],\n			output_file=''report.md''\n		)\n\n	@crew\n	def crew(self) -> Crew:\n		"""Creates the LatestAiDevelopment crew"""\n		return Crew(\n			agents=self.agents, # Automatically created by the @agent decorator\n			tasks=self.tasks, # Automatically created by the @task decorator\n			process=Process.sequential,\n			verbose=True,\n		)\n```\n\n**main.py**\n\n```python\n#!/usr/bin/env python\n# src/my_project/main.py\nimport sys\nfrom latest_ai_development.crew import LatestAiDevelopmentCrew\n\ndef run():\n    """\n    Run the crew.\n    """\n    inputs = {\n        ''topic'': ''AI Agents''\n    }\n    LatestAiDevelopmentCrew().crew().kickoff(inputs=inputs)\n```\n\n### 3. Running Your Crew\n\nBefore running your crew, make sure you have the following keys set as environment variables in your `.env` file:\n\n- An [OpenAI API key](https://platform.openai.com/account/api-keys) (or other LLM API key): `OPENAI_API_KEY=sk-...`\n- A [Serper.dev](https://serper.dev/) API key: `SERPER_API_KEY=YOUR_KEY_HERE`\n\nLock the dependencies and install them by using the CLI command but first, navigate to your project directory:\n\n```shell\ncd my_project\ncrewai install (Optional)\n```\n\nTo run your crew, execute the following command in the root of your project:\n\n```bash\ncrewai run\n```\n\nor\n\n```bash\npython src/my_project/main.py\n```\n\nIf an error happens due to the usage of poetry, please run the following command to update your crewai package:\n\n```bash\ncrewai update\n```\n\nYou should see the output in the console and the `report.md` file should be created in the root of your project with the full final report.\n\nIn addition to the sequential process, you can use the hierarchical process, which automatically assigns a manager to the defined crew to properly coordinate the planning and execution of tasks through delegation and validation of results. [See more about the processes here](https://docs.crewai.com/core-concepts/Processes/).\n\n## Key Features\n\nCrewAI stands apart as a lean, standalone, high-performance multi-AI Agent framework delivering simplicity, flexibility, and precise controlâ€”free from the complexity and limitations found in other agent frameworks.\n\n- **Standalone & Lean**: Completely independent from other frameworks like LangChain, offering faster execution and lighter resource demands.\n- **Flexible & Precise**: Easily orchestrate autonomous agents through intuitive [Crews](https://docs.crewai.com/concepts/crews) or precise [Flows](https://docs.crewai.com/concepts/flows), achieving perfect balance for your needs.\n- **Seamless Integration**: Effortlessly combine Crews (autonomy) and Flows (precision) to create complex, real-world automations.\n- **Deep Customization**: Tailor every aspectâ€”from high-level workflows down to low-level internal prompts and agent behaviors.\n- **Reliable Performance**: Consistent results across simple tasks and complex, enterprise-level automations.\n- **Thriving Community**: Backed by robust documentation and over 100,000 certified developers, providing exceptional support and guidance.\n\nChoose CrewAI to easily build powerful, adaptable, and production-ready AI automations.\n\n## Examples\n\nYou can test different real life examples of AI crews in the [CrewAI-examples repo](https://github.com/crewAIInc/crewAI-examples?tab=readme-ov-file):\n\n- [Landing Page Generator](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/landing_page_generator)\n- [Having Human input on the execution](https://docs.crewai.com/how-to/Human-Input-on-Execution)\n- [Trip Planner](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner)\n- [Stock Analysis](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis)\n\n### Quick Tutorial\n\n[![CrewAI Tutorial](https://img.youtube.com/vi/tnejrr-0a94/maxresdefault.jpg)](https://www.youtube.com/watch?v=tnejrr-0a94 "CrewAI Tutorial")\n\n### Write Job Descriptions\n\n[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/job-posting) or watch a video below:\n\n[![Jobs postings](https://img.youtube.com/vi/u98wEMz-9to/maxresdefault.jpg)](https://www.youtube.com/watch?v=u98wEMz-9to "Jobs postings")\n\n### Trip Planner\n\n[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/trip_planner) or watch a video below:\n\n[![Trip Planner](https://img.youtube.com/vi/xis7rWp-hjs/maxresdefault.jpg)](https://www.youtube.com/watch?v=xis7rWp-hjs "Trip Planner")\n\n### Stock Analysis\n\n[Check out code for this example](https://github.com/crewAIInc/crewAI-examples/tree/main/crews/stock_analysis) or watch a video below:\n\n[![Stock Analysis](https://img.youtube.com/vi/e0Uj4yWdaAg/maxresdefault.jpg)](https://www.youtube.com/watch?v=e0Uj4yWdaAg "Stock Analysis")\n\n### Using Crews and Flows Together\n\nCrewAI''s power truly shines when combining Crews with Flows to create sophisticated automation pipelines.\nCrewAI flows support logical operators like `or_` and `and_` to combine multiple conditions. This can be used with `@start`, `@listen`, or `@router` decorators to create complex triggering conditions.\n\n- `or_`: Triggers when any of the specified conditions are met.\n- `and_`Triggers when all of the specified conditions are met.\n\nHere''s how you can orchestrate multiple Crews within a Flow:\n\n```python\nfrom crewai.flow.flow import Flow, listen, start, router, or_\nfrom crewai import Crew, Agent, Task, Process\nfrom pydantic import BaseModel\n\n# Define structured state for precise control\nclass MarketState(BaseModel):\n    sentiment: str = "neutral"\n    confidence: float = 0.0\n    recommendations: list = []\n\nclass AdvancedAnalysisFlow(Flow[MarketState]):\n    @start()\n    def fetch_market_data(self):\n        # Demonstrate low-level control with structured state\n        self.state.sentiment = "analyzing"\n        return {"sector": "tech", "timeframe": "1W"}  # These parameters match the task description template\n\n    @listen(fetch_market_data)\n    def analyze_with_crew(self, market_data):\n        # Show crew agency through specialized roles\n        analyst = Agent(\n            role="Senior Market Analyst",\n            goal="Conduct deep market analysis with expert insight",\n            backstory="You''re a veteran analyst known for identifying subtle market patterns"\n        )\n        researcher = Agent(\n            role="Data Researcher",\n            goal="Gather and validate supporting market data",\n            backstory="You excel at finding and correlating multiple data sources"\n        )\n\n        analysis_task = Task(\n            description="Analyze {sector} sector data for the past {timeframe}",\n            expected_output="Detailed market analysis with confidence score",\n            agent=analyst\n        )\n        research_task = Task(\n            description="Find supporting data to validate the analysis",\n            expected_output="Corroborating evidence and potential contradictions",\n            agent=researcher\n        )\n\n        # Demonstrate crew autonomy\n        analysis_crew = Crew(\n            agents=[analyst, researcher],\n            tasks=[analysis_task, research_task],\n            process=Process.sequential,\n            verbose=True\n        )\n        return analysis_crew.kickoff(inputs=market_data)  # Pass market_data as named inputs\n\n    @router(analyze_with_crew)\n    def determine_next_steps(self):\n        # Show flow control with conditional routing\n        if self.state.confidence > 0.8:\n            return "high_confidence"\n        elif self.state.confidence > 0.5:\n            return "medium_confidence"\n        return "low_confidence"\n\n    @listen("high_confidence")\n    def execute_strategy(self):\n        # Demonstrate complex decision making\n        strategy_crew = Crew(\n            agents=[\n                Agent(role="Strategy Expert",\n                      goal="Develop optimal market strategy")\n            ],\n            tasks=[\n                Task(description="Create detailed strategy based on analysis",\n                     expected_output="Step-by-step action plan")\n            ]\n        )\n        return strategy_crew.kickoff()\n\n    @listen(or_("medium_confidence", "low_confidence"))\n    def request_additional_analysis(self):\n        self.state.recommendations.append("Gather more data")\n        return "Additional analysis required"\n```\n\nThis example demonstrates how to:\n\n1. Use Python code for basic data operations\n2. Create and execute Crews as steps in your workflow\n3. Use Flow decorators to manage the sequence of operations\n4. Implement conditional branching based on Crew results\n\n## Connecting Your Crew to a Model\n\nCrewAI supports using various LLMs through a variety of connection options. By default your agents will use the OpenAI API when querying the model. However, there are several other ways to allow your agents to connect to models. For example, you can configure your agents to use a local model via the Ollama tool.\n\nPlease refer to the [Connect CrewAI to LLMs](https://docs.crewai.com/how-to/LLM-Connections/) page for details on configuring your agents'' connections to models.\n\n## How CrewAI Compares\n\n**CrewAI''s Advantage**: CrewAI combines autonomous agent intelligence with precise workflow control through its unique Crews and Flows architecture. The framework excels at both high-level orchestration and low-level customization, enabling complex, production-grade systems with granular control.\n\n- **LangGraph**: While LangGraph provides a foundation for building agent workflows, its approach requires significant boilerplate code and complex state management patterns. The framework''s tight coupling with LangChain can limit flexibility when implementing custom agent behaviors or integrating with external systems.\n\n*P.S. CrewAI demonstrates significant performance advantages over LangGraph, executing 5.76x faster in certain cases like this QA task example ([see comparison](https://github.com/crewAIInc/crewAI-examples/tree/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/QA%20Agent)) while achieving higher evaluation scores with faster completion times in certain coding tasks, like in this example ([detailed analysis](https://github.com/crewAIInc/crewAI-examples/blob/main/Notebooks/CrewAI%20Flows%20%26%20Langgraph/Coding%20Assistant/coding_assistant_eval.ipynb)).*\n\n- **Autogen**: While Autogen excels at creating conversational agents capable of working together, it lacks an inherent concept of process. In Autogen, orchestrating agents'' interactions requires additional programming, which can become complex and cumbersome as the scale of tasks grows.\n- **ChatDev**: ChatDev introduced the idea of processes into the realm of AI agents, but its implementation is quite rigid. Customizations in ChatDev are limited and not geared towards production environments, which can hinder scalability and flexibility in real-world applications.\n\n## Contribution\n\nCrewAI is open-source and we welcome contributions. If you''re looking to contribute, please:\n\n- Fork the repository.\n- Create a new branch for your feature.\n- Add your feature or improvement.\n- Send a pull request.\n- We appreciate your input!\n\n### Installing Dependencies\n\n```bash\nuv lock\nuv sync\n```\n\n### Virtual Env\n\n```bash\nuv venv\n```\n\n### Pre-commit hooks\n\n```bash\npre-commit install\n```\n\n### Running Tests\n\n```bash\nuv run pytest .\n```\n\n### Running static type checks\n\n```bash\nuvx mypy src\n```\n\n### Packaging\n\n```bash\nuv build\n```\n\n### Installing Locally\n\n```bash\npip install dist/*.tar.gz\n```\n\n## Telemetry\n\nCrewAI uses anonymous telemetry to collect usage data with the main purpose of helping us improve the library by focusing our efforts on the most used features, integrations and tools.\n\nIt''s pivotal to understand that **NO data is collected** concerning prompts, task descriptions, agents'' backstories or goals, usage of tools, API calls, responses, any data processed by the agents, or secrets and environment variables, with the exception of the conditions mentioned. When the `share_crew` feature is enabled, detailed data including task descriptions, agents'' backstories or goals, and other specific attributes are collected to provide deeper insights while respecting user privacy. Users can disable telemetry by setting the environment variable OTEL_SDK_DISABLED to true.\n\nData collected includes:\n\n- Version of CrewAI\n  - So we can understand how many users are using the latest version\n- Version of Python\n  - So we can decide on what versions to better support\n- General OS (e.g. number of CPUs, macOS/Windows/Linux)\n  - So we know what OS we should focus on and if we could build specific OS related features\n- Number of agents and tasks in a crew\n  - So we make sure we are testing internally with similar use cases and educate people on the best practices\n- Crew Process being used\n  - Understand where we should focus our efforts\n- If Agents are using memory or allowing delegation\n  - Understand if we improved the features or maybe even drop them\n- If Tasks are being executed in parallel or sequentially\n  - Understand if we should focus more on parallel execution\n- Language model being used\n  - Improved support on most used languages\n- Roles of agents in a crew\n  - Understand high level use cases so we can build better tools, integrations and examples about it\n- Tools names available\n  - Understand out of the publicly available tools, which ones are being used the most so we can improve them\n\nUsers can opt-in to Further Telemetry, sharing the complete telemetry data by setting the `share_crew` attribute to `True` on their Crews. Enabling `share_crew` results in the collection of detailed crew and task execution data, including `goal`, `backstory`, `context`, and `output` of tasks. This enables a deeper insight into usage patterns while respecting the user''s choice to share.\n\n## License\n\nCrewAI is released under the [MIT License](https://github.com/crewAIInc/crewAI/blob/main/LICENSE).\n\n## Frequently Asked Questions (FAQ)\n\n### General\n\n- [What exactly is CrewAI?](#q-what-exactly-is-crewai)\n- [How do I install CrewAI?](#q-how-do-i-install-crewai)\n- [Does CrewAI depend on LangChain?](#q-does-crewai-depend-on-langchain)\n- [Is CrewAI open-source?](#q-is-crewai-open-source)\n- [Does CrewAI collect data from users?](#q-does-crewai-collect-data-from-users)\n\n### Features and Capabilities\n\n- [Can CrewAI handle complex use cases?](#q-can-crewai-handle-complex-use-cases)\n- [Can I use CrewAI with local AI models?](#q-can-i-use-crewai-with-local-ai-models)\n- [What makes Crews different from Flows?](#q-what-makes-crews-different-from-flows)\n- [How is CrewAI better than LangChain?](#q-how-is-crewai-better-than-langchain)\n- [Does CrewAI support fine-tuning or training custom models?](#q-does-crewai-support-fine-tuning-or-training-custom-models)\n\n### Resources and Community\n\n- [Where can I find real-world CrewAI examples?](#q-where-can-i-find-real-world-crewai-examples)\n- [How can I contribute to CrewAI?](#q-how-can-i-contribute-to-crewai)\n\n### Enterprise Features\n\n- [What additional features does CrewAI AOP offer?](#q-what-additional-features-does-crewai-amp-offer)\n- [Is CrewAI AOP available for cloud and on-premise deployments?](#q-is-crewai-amp-available-for-cloud-and-on-premise-deployments)\n- [Can I try CrewAI AOP for free?](#q-can-i-try-crewai-amp-for-free)\n\n### Q: What exactly is CrewAI?\n\nA: CrewAI is a standalone, lean, and fast Python framework built specifically for orchestrating autonomous AI agents. Unlike frameworks like LangChain, CrewAI does not rely on external dependencies, making it leaner, faster, and simpler.\n\n### Q: How do I install CrewAI?\n\nA: Install CrewAI using pip:\n\n```shell\npip install crewai\n```\n\nFor additional tools, use:\n\n```shell\npip install ''crewai[tools]''\n```\n\n### Q: Does CrewAI depend on LangChain?\n\nA: No. CrewAI is built entirely from the ground up, with no dependencies on LangChain or other agent frameworks. This ensures a lean, fast, and flexible experience.\n\n### Q: Can CrewAI handle complex use cases?\n\nA: Yes. CrewAI excels at both simple and highly complex real-world scenarios, offering deep customization options at both high and low levels, from internal prompts to sophisticated workflow orchestration.\n\n### Q: Can I use CrewAI with local AI models?\n\nA: Absolutely! CrewAI supports various language models, including local ones. Tools like Ollama and LM Studio allow seamless integration. Check the [LLM Connections documentation](https://docs.crewai.com/how-to/LLM-Connections/) for more details.\n\n### Q: What makes Crews different from Flows?\n\nA: Crews provide autonomous agent collaboration, ideal for tasks requiring flexible decision-making and dynamic interaction. Flows offer precise, event-driven control, ideal for managing detailed execution paths and secure state management. You can seamlessly combine both for maximum effectiveness.\n\n### Q: How is CrewAI better than LangChain?\n\nA: CrewAI provides simpler, more intuitive APIs, faster execution speeds, more reliable and consistent results, robust documentation, and an active communityâ€”addressing common criticisms and limitations associated with LangChain.\n\n### Q: Is CrewAI open-source?\n\nA: Yes, CrewAI is open-source and actively encourages community contributions and collaboration.\n\n### Q: Does CrewAI collect data from users?\n\nA: CrewAI collects anonymous telemetry data strictly for improvement purposes. Sensitive data such as prompts, tasks, or API responses are never collected unless explicitly enabled by the user.\n\n### Q: Where can I find real-world CrewAI examples?\n\nA: Check out practical examples in the [CrewAI-examples repository](https://github.com/crewAIInc/crewAI-examples), covering use cases like trip planners, stock analysis, and job postings.\n\n### Q: How can I contribute to CrewAI?\n\nA: Contributions are warmly welcomed! Fork the repository, create your branch, implement your changes, and submit a pull request. See the Contribution section of the README for detailed guidelines.\n\n### Q: What additional features does CrewAI AOP offer?\n\nA: CrewAI AOP provides advanced features such as a unified control plane, real-time observability, secure integrations, advanced security, actionable insights, and dedicated 24/7 enterprise support.\n\n### Q: Is CrewAI AOP available for cloud and on-premise deployments?\n\nA: Yes, CrewAI AOP supports both cloud-based and on-premise deployment options, allowing enterprises to meet their specific security and compliance requirements.\n\n### Q: Can I try CrewAI AOP for free?\n\nA: Yes, you can explore part of the CrewAI AOP Suite by accessing the [Crew Control Plane](https://app.crewai.com) for free.\n\n### Q: Does CrewAI support fine-tuning or training custom models?\n\nA: Yes, CrewAI can integrate with custom-trained or fine-tuned models, allowing you to enhance your agents with domain-specific knowledge and accuracy.\n\n### Q: Can CrewAI agents interact with external tools and APIs?\n\nA: Absolutely! CrewAI agents can easily integrate with external tools, APIs, and databases, empowering them to leverage real-world data and resources.\n\n### Q: Is CrewAI suitable for production environments?\n\nA: Yes, CrewAI is explicitly designed with production-grade standards, ensuring reliability, stability, and scalability for enterprise deployments.\n\n### Q: How scalable is CrewAI?\n\nA: CrewAI is highly scalable, supporting simple automations and large-scale enterprise workflows involving numerous agents and complex tasks simultaneously.\n\n### Q: Does CrewAI offer debugging and monitoring tools?\n\nA: Yes, CrewAI AOP includes advanced debugging, tracing, and real-time observability features, simplifying the management and troubleshooting of your automations.\n\n### Q: What programming languages does CrewAI support?\n\nA: CrewAI is primarily Python-based but easily integrates with services and APIs written in any programming language through its flexible API integration capabilities.\n\n### Q: Does CrewAI offer educational resources for beginners?\n\nA: Yes, CrewAI provides extensive beginner-friendly tutorials, courses, and documentation through learn.crewai.com, supporting developers at all skill levels.\n\n### Q: Can CrewAI automate human-in-the-loop workflows?\n\nA: Yes, CrewAI fully supports human-in-the-loop workflows, allowing seamless collaboration between human experts and AI agents for enhanced decision-making.\n', '{"language":"Python","stars":41146,"forks":5508,"watchers":41146,"open_issues":194,"topics":["agents","ai","ai-agents","aiagentframework","llms"],"default_branch":"main","size_kb":194581,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:crewAIInc:crewAI\">","source_url":"https://github.com/crewAIInc/crewAI\">"},{"type":"has_code","target_id":"github:crewAIInc:crewAI\">","source_url":"https://github.com/crewAIInc/crewAI\">"},{"type":"has_code","target_id":"github:crewAIInc:crewAI","source_url":"https://github.com/crewAIInc/crewAI"},{"type":"has_code","target_id":"github:crewAIInc:crewAI","source_url":"https://github.com/crewAIInc/crewAI"},{"type":"has_code","target_id":"github:crewAIInc:crewAI","source_url":"https://github.com/crewAIInc/crewAI"},{"type":"has_code","target_id":"github:crewAIInc:crewAI-examples","source_url":"https://github.com/crewAIInc/crewAI-examples?tab=readme-ov-file"},{"type":"has_code","target_id":"github:crewAIInc:crewAI-examples","source_url":"https://github.com/crewAIInc/crewAI-examples"},{"type":"has_code","target_id":"github:crewAIInc:crewAI-examples","source_url":"https://github.com/crewAIInc/crewAI-examples"},{"type":"has_code","target_id":"github:crewAIInc:crewAI-examples","source_url":"https://github.com/crewAIInc/crewAI-examples"},{"type":"has_code","target_id":"github:crewAIInc:crewAI-examples","source_url":"https://github.com/crewAIInc/crewAI-examples"},{"type":"has_code","target_id":"github:crewAIInc:crewAI-examples","source_url":"https://github.com/crewAIInc/crewAI-examples"},{"type":"has_code","target_id":"github:crewAIInc:crewAI-examples","source_url":"https://github.com/crewAIInc/crewAI-examples"},{"type":"has_code","target_id":"github:crewAIInc:crewAI-examples","source_url":"https://github.com/crewAIInc/crewAI-examples"},{"type":"has_code","target_id":"github:crewAIInc:crewAI-examples","source_url":"https://github.com/crewAIInc/crewAI-examples"},{"type":"has_code","target_id":"github:crewAIInc:crewAI","source_url":"https://github.com/crewAIInc/crewAI"},{"type":"has_code","target_id":"github:crewAIInc:crewAI-examples","source_url":"https://github.com/crewAIInc/crewAI-examples"}]', NULL, 'MIT', 'approved', 80, 'eeb578bfe80d71c381bfd9216e3a3cbb', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-crewAIInc-crewAI from https://github.com/crewAIInc.png
Image converted to WebP: data/images/github-crewAIInc-crewAI.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-zhayujie-chatgpt-on-wechat', 'github--zhayujie--chatgpt-on-wechat', 'chatgpt-on-wechat', 'zhayujie', '<p align="center"><img src= "https://github.com/user-attachments/assets/31fb4eab-3be4-477d-aa76-82cf62bfd12c" alt="Chatgpt-on-Wechat" width="600" /></p> <p align="center"> <a href="https://github.com/zhayujie/chatgpt-on-wechat/releases/latest"><img src="https://img.shields.io/github/v/release/zhayujie/chatgpt-on-wechat" alt="Latest release"></a> <a href="https://github.com/zhayujie/chatgpt-on-wechat/blob/master/LICENSE"><img src="https://img.shields.io/github/license/zhayujie/chatgpt-on-wecha...', '["ai","ai-agent","chatgpt","claude-4","deepseek","dingtalk","feishu-bot","gemini","gpt-4","kimi","linkai","llm","mcp","multi-agent","openai","python3","qwen","rag","wechat","wechat-bot","python"]', 'other', 40000, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/zhayujie/chatgpt-on-wechat","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center"><img src= "https://github.com/user-attachments/assets/31fb4eab-3be4-477d-aa76-82cf62bfd12c" alt="Chatgpt-on-Wechat" width="600" /></p>\n\n<p align="center">\n   <a href="https://github.com/zhayujie/chatgpt-on-wechat/releases/latest"><img src="https://img.shields.io/github/v/release/zhayujie/chatgpt-on-wechat" alt="Latest release"></a>\n  <a href="https://github.com/zhayujie/chatgpt-on-wechat/blob/master/LICENSE"><img src="https://img.shields.io/github/license/zhayujie/chatgpt-on-wechat" alt="License: MIT"></a>\n  <a href="https://github.com/zhayujie/chatgpt-on-wechat"><img src="https://img.shields.io/github/stars/zhayujie/chatgpt-on-wechat?style=flat-square" alt="Stars"></a> <br/>\n</p>\n\n**chatgpt-on-wechat**ï¼ˆç®€ç§°CoWï¼‰é¡¹ç›®æ˜¯åŸºäºå¤§æ¨¡å‹çš„æ™ºèƒ½å¯¹è¯æœºå™¨äººï¼Œæ”¯æŒè‡ªç”±åˆ‡æ¢å¤šç§æ¨¡å‹ï¼Œå¯æ¥å…¥ç½‘é¡µã€å¾®ä¿¡å…¬ä¼—å·ã€ä¼ä¸šå¾®ä¿¡åº”ç”¨ã€é£ä¹¦ã€é’‰é’‰ä¸­ä½¿ç”¨ï¼Œèƒ½å¤„ç†æ–‡æœ¬ã€è¯­éŸ³ã€å›¾ç‰‡ã€æ–‡ä»¶ç­‰å¤šæ¨¡æ€æ¶ˆæ¯ï¼Œæ”¯æŒé€šè¿‡æ’ä»¶è®¿é—®æ“ä½œç³»ç»Ÿå’Œäº’è”ç½‘ç­‰å¤–éƒ¨èµ„æºï¼Œä»¥åŠåŸºäºè‡ªæœ‰çŸ¥è¯†åº“å®šåˆ¶ä¼ä¸šAIåº”ç”¨ã€‚\n\n# ç®€ä»‹\n\n> è¯¥é¡¹ç›®æ—¢æ˜¯ä¸€ä¸ªå¯ä»¥å¼€ç®±å³ç”¨çš„å¯¹è¯æœºå™¨äººï¼Œä¹Ÿæ˜¯ä¸€ä¸ªæ”¯æŒé«˜åº¦æ‰©å±•çš„AIåº”ç”¨æ¡†æ¶ï¼Œå¯ä»¥é€šè¿‡ä¸ºé¡¹ç›®æ·»åŠ å¤§æ¨¡å‹æ¥å£ã€æ¥å…¥æ¸ é“ã€è‡ªå®šä¹‰æ’ä»¶æ¥çµæ´»å®ç°å„ç§å®šåˆ¶éœ€æ±‚ã€‚æ”¯æŒçš„åŠŸèƒ½å¦‚ä¸‹ï¼š\n\n-  âœ…   **å¤šç«¯éƒ¨ç½²ï¼š** æœ‰å¤šç§éƒ¨ç½²æ–¹å¼å¯é€‰æ‹©ä¸”åŠŸèƒ½å®Œå¤‡ï¼Œç›®å‰å·²æ”¯æŒç½‘é¡µã€å¾®ä¿¡å…¬ä¼—å·ã€ä¼ä¸šå¾®ä¿¡åº”ç”¨ã€é£ä¹¦ã€é’‰é’‰ç­‰éƒ¨ç½²æ–¹å¼\n-  âœ…   **åŸºç¡€å¯¹è¯ï¼š** ç§èŠåŠç¾¤èŠçš„AIæ™ºèƒ½å›å¤ï¼Œæ”¯æŒå¤šè½®ä¼šè¯ä¸Šä¸‹æ–‡è®°å¿†ï¼ŒåŸºç¡€æ¨¡å‹æ”¯æŒOpenAI, Claude, Gemini, DeepSeek, é€šä¹‰åƒé—®, Kimi, æ–‡å¿ƒä¸€è¨€, è®¯é£æ˜Ÿç«, ChatGLM, MiniMax, GiteeAI, ModelScope, LinkAI\n-  âœ…   **è¯­éŸ³èƒ½åŠ›ï¼š** å¯è¯†åˆ«è¯­éŸ³æ¶ˆæ¯ï¼Œé€šè¿‡æ–‡å­—æˆ–è¯­éŸ³å›å¤ï¼Œæ”¯æŒ openai(whisper/tts), azure, baidu, google ç­‰å¤šç§è¯­éŸ³æ¨¡å‹\n-  âœ…   **å›¾åƒèƒ½åŠ›ï¼š** æ”¯æŒå›¾ç‰‡ç”Ÿæˆã€å›¾ç‰‡è¯†åˆ«ã€å›¾ç”Ÿå›¾ï¼Œå¯é€‰æ‹© Dall-E-3, stable diffusion, replicate, midjourney, CogView-3, visionæ¨¡å‹\n-  âœ…   **ä¸°å¯Œæ’ä»¶ï¼š** æ”¯æŒè‡ªå®šä¹‰æ’ä»¶æ‰©å±•ï¼Œå·²å®ç°å¤šè§’è‰²åˆ‡æ¢ã€æ•æ„Ÿè¯è¿‡æ»¤ã€èŠå¤©è®°å½•æ€»ç»“ã€æ–‡æ¡£æ€»ç»“å’Œå¯¹è¯ã€è”ç½‘æœç´¢ã€æ™ºèƒ½ä½“ç­‰å†…ç½®æ’ä»¶\n-  âœ…   **Agentèƒ½åŠ›ï¼š** æ”¯æŒè®¿é—®æµè§ˆå™¨ã€ç»ˆç«¯ã€æ–‡ä»¶ç³»ç»Ÿã€æœç´¢å¼•æ“ç­‰å„ç±»å·¥å…·ï¼Œå¹¶å¯é€šè¿‡å¤šæ™ºèƒ½ä½“åä½œå®Œæˆå¤æ‚ä»»åŠ¡ï¼ŒåŸºäº [AgentMesh](https://github.com/MinimalFuture/AgentMesh) æ¡†æ¶å®ç°\n-  âœ…   **çŸ¥è¯†åº“ï¼š** é€šè¿‡ä¸Šä¼ çŸ¥è¯†åº“è‡ªå®šä¹‰ä¸“å±æœºå™¨äººï¼Œå¯ä½œä¸ºæ•°å­—åˆ†èº«ã€æ™ºèƒ½å®¢æœã€ä¼ä¸šæ™ºèƒ½ä½“ä½¿ç”¨ï¼ŒåŸºäº [LinkAI](https://link-ai.tech) å®ç°\n\n## å£°æ˜\n\n1. æœ¬é¡¹ç›®éµå¾ª [MITå¼€æºåè®®](/LICENSE)ï¼Œä»…ç”¨äºæŠ€æœ¯ç ”ç©¶å’Œå­¦ä¹ ï¼Œä½¿ç”¨æœ¬é¡¹ç›®æ—¶éœ€éµå®ˆæ‰€åœ¨åœ°æ³•å¾‹æ³•è§„ã€ç›¸å…³æ”¿ç­–ä»¥åŠä¼ä¸šç« ç¨‹ï¼Œç¦æ­¢ç”¨äºä»»ä½•è¿æ³•æˆ–ä¾µçŠ¯ä»–äººæƒç›Šçš„è¡Œä¸ºã€‚ä»»ä½•ä¸ªäººã€å›¢é˜Ÿå’Œä¼ä¸šï¼Œæ— è®ºä»¥ä½•ç§æ–¹å¼ä½¿ç”¨è¯¥é¡¹ç›®ã€å¯¹ä½•å¯¹è±¡æä¾›æœåŠ¡ï¼Œæ‰€äº§ç”Ÿçš„ä¸€åˆ‡åæœï¼Œæœ¬é¡¹ç›®å‡ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»\n2. å¢ƒå†…ä½¿ç”¨è¯¥é¡¹ç›®æ—¶ï¼Œå»ºè®®ä½¿ç”¨å›½å†…å‚å•†çš„å¤§æ¨¡å‹æœåŠ¡ï¼Œå¹¶è¿›è¡Œå¿…è¦çš„å†…å®¹å®‰å…¨å®¡æ ¸åŠè¿‡æ»¤\n3. æœ¬é¡¹ç›®å½“å‰ä¸»è¦æ¥å…¥ååŒåŠå…¬å¹³å°ï¼Œæ¨èä½¿ç”¨ç½‘é¡µã€å…¬ä¼—å·ã€ä¼å¾®è‡ªå»ºåº”ç”¨ã€é’‰é’‰ã€é£ä¹¦ç­‰æ¥å…¥é€šé“ï¼Œå…¶ä»–é€šé“ä¸ºå†å²äº§ç‰©æš‚ä¸ç»´æŠ¤\n\n## æ¼”ç¤º\n\nDEMOè§†é¢‘ï¼šhttps://cdn.link-ai.tech/doc/cow_demo.mp4\n\n## ç¤¾åŒº\n\næ·»åŠ å°åŠ©æ‰‹å¾®ä¿¡åŠ å…¥å¼€æºé¡¹ç›®äº¤æµç¾¤ï¼š\n\n<img width="140" src="https://img-1317903499.cos.ap-guangzhou.myqcloud.com/docs/open-community.png">\n\n<br/>\n\n# ä¼ä¸šæœåŠ¡\n\n<a href="https://link-ai.tech" target="_blank"><img width="720" src="https://cdn.link-ai.tech/image/link-ai-intro.jpg"></a>\n\n> [LinkAI](https://link-ai.tech/) æ˜¯é¢å‘ä¼ä¸šå’Œå¼€å‘è€…çš„ä¸€ç«™å¼AIæ™ºèƒ½ä½“å¹³å°ï¼Œèšåˆå¤šæ¨¡æ€å¤§æ¨¡å‹ã€çŸ¥è¯†åº“ã€Agent æ’ä»¶ã€å·¥ä½œæµç­‰èƒ½åŠ›ï¼Œæ”¯æŒä¸€é”®æ¥å…¥ä¸»æµå¹³å°å¹¶è¿›è¡Œç®¡ç†ï¼Œæ”¯æŒSaaSã€ç§æœ‰åŒ–éƒ¨ç½²ç­‰å¤šç§æ¨¡å¼ã€‚\n>\n> LinkAI ç›®å‰å·²åœ¨æ™ºèƒ½å®¢æœã€ç§åŸŸè¿è¥ã€ä¼ä¸šæ•ˆç‡åŠ©æ‰‹ç­‰åœºæ™¯ç§¯ç´¯äº†ä¸°å¯Œçš„AIè§£å†³æ–¹æ¡ˆï¼Œåœ¨æ¶ˆè´¹ã€å¥åº·ã€æ–‡æ•™ã€ç§‘æŠ€åˆ¶é€ ç­‰å„è¡Œä¸šæ²‰æ·€äº†å¤§æ¨¡å‹è½åœ°åº”ç”¨çš„æœ€ä½³å®è·µï¼Œè‡´åŠ›äºå¸®åŠ©æ›´å¤šä¼ä¸šå’Œå¼€å‘è€…æ‹¥æŠ± AI ç”Ÿäº§åŠ›ã€‚\n\n**äº§å“å’¨è¯¢å’Œä¼ä¸šæœåŠ¡** å¯è”ç³»äº§å“å®¢æœï¼š\n\n<img width="150" src="https://cdn.link-ai.tech/portal/linkai-customer-service.png">\n\n<br/>\n\n# ğŸ· æ›´æ–°æ—¥å¿—\n\n>**2025.05.23ï¼š** [1.7.6ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.6) ä¼˜åŒ–webç½‘é¡µchannelã€æ–°å¢ [AgentMeshå¤šæ™ºèƒ½ä½“æ’ä»¶](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/agent/README.md)ã€ç™¾åº¦è¯­éŸ³åˆæˆä¼˜åŒ–ã€ä¼å¾®åº”ç”¨`access_token`è·å–ä¼˜åŒ–ã€æ”¯æŒ`claude-4-sonnet`å’Œ`claude-4-opus`æ¨¡å‹\n\n>**2025.04.11ï¼š** [1.7.5ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.5) æ–°å¢æ”¯æŒ [wechatferry](https://github.com/zhayujie/chatgpt-on-wechat/pull/2562) åè®®ã€æ–°å¢ deepseek æ¨¡å‹ã€æ–°å¢æ”¯æŒè…¾è®¯äº‘è¯­éŸ³èƒ½åŠ›ã€æ–°å¢æ”¯æŒ ModelScope å’Œ Gitee-AI APIæ¥å£\n\n>**2024.12.13ï¼š** [1.7.4ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.4) æ–°å¢ Gemini 2.0 æ¨¡å‹ã€æ–°å¢web channelã€è§£å†³å†…å­˜æ³„æ¼é—®é¢˜ã€è§£å†³ `#reloadp` å‘½ä»¤é‡è½½ä¸ç”Ÿæ•ˆé—®é¢˜\n\n>**2024.10.31ï¼š** [1.7.3ç‰ˆæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/releases/tag/1.7.3) ç¨‹åºç¨³å®šæ€§æå‡ã€æ•°æ®åº“åŠŸèƒ½ã€Claudeæ¨¡å‹ä¼˜åŒ–ã€linkaiæ’ä»¶ä¼˜åŒ–ã€ç¦»çº¿é€šçŸ¥\n\næ›´å¤šæ›´æ–°å†å²è¯·æŸ¥çœ‹: [æ›´æ–°æ—¥å¿—](/docs/version/release-notes.md)\n\n<br/>\n\n# ğŸš€ å¿«é€Ÿå¼€å§‹\n\né¡¹ç›®æä¾›äº†ä¸€é”®å®‰è£…ã€å¯åŠ¨ã€ç®¡ç†ç¨‹åºçš„è„šæœ¬ï¼Œå¯ä»¥é€‰æ‹©ä½¿ç”¨è„šæœ¬å¿«é€Ÿè¿è¡Œï¼Œä¹Ÿå¯ä»¥æ ¹æ®è¯¦ç»†æŒ‡å¼•ä¸€æ­¥æ­¥å®‰è£…è¿è¡Œã€‚\n\n- è¯¦ç»†æ–‡æ¡£ï¼š[å¿«é€Ÿå¼€å§‹](https://docs.link-ai.tech/cow/quick-start)\n\n- ä¸€é”®å®‰è£…è„šæœ¬è¯´æ˜ï¼š[ä¸€é”®å®‰è£…è„šæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/wiki/%E4%B8%80%E9%94%AE%E5%AE%89%E8%A3%85%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC)\n\n```bash\nbash <(curl -sS https://cdn.link-ai.tech/code/cow/install.sh)\n```\n\n- é¡¹ç›®ç®¡ç†è„šæœ¬è¯´æ˜ï¼š[é¡¹ç›®ç®¡ç†è„šæœ¬](https://github.com/zhayujie/chatgpt-on-wechat/wiki/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86%E8%84%9A%E6%9C%AC)\n\n## ä¸€ã€å‡†å¤‡\n\n### 1. æ¨¡å‹è´¦å·\n\né¡¹ç›®é»˜è®¤ä½¿ç”¨ChatGPTæ¨¡å‹ï¼Œéœ€å‰å¾€ [OpenAIå¹³å°](https://platform.openai.com/api-keys) åˆ›å»ºAPI Keyå¹¶å¡«å…¥é¡¹ç›®é…ç½®æ–‡ä»¶ä¸­ã€‚åŒæ—¶æ”¯æŒå…¶ä»–å›½å†…å¤–äº§å•†ä»¥åŠç¬¬ä¸‰æ–¹è‡ªå®šä¹‰æ¨¡å‹æ¥å£ï¼Œè¯¦æƒ…å‚è€ƒï¼š[æ¨¡å‹è¯´æ˜](#æ¨¡å‹è¯´æ˜)ã€‚\n\nåŒæ—¶æ”¯æŒä½¿ç”¨ **LinkAIå¹³å°** æ¥å£ï¼Œå¯èšåˆä½¿ç”¨ OpenAIã€Claudeã€DeepSeekã€Kimiã€Qwen ç­‰å¤šç§å¸¸ç”¨æ¨¡å‹ï¼Œå¹¶æ”¯æŒçŸ¥è¯†åº“ã€å·¥ä½œæµã€è”ç½‘æœç´¢ã€MJç»˜å›¾ã€æ–‡æ¡£æ€»ç»“ç­‰èƒ½åŠ›ã€‚ä¿®æ”¹é…ç½®å³å¯ä¸€é”®å¯ç”¨ï¼Œå‚è€ƒ [æ¥å…¥æ–‡æ¡£](https://link-ai.tech/platform/link-app/wechat)ã€‚\n\n### 2.ç¯å¢ƒå®‰è£…\n\næ”¯æŒ Linuxã€MacOSã€Windows ç³»ç»Ÿï¼ŒåŒæ—¶éœ€å®‰è£… `Python`ï¼ŒPythonç‰ˆæœ¬éœ€è¦åœ¨3.7ä»¥ä¸Šï¼Œæ¨èä½¿ç”¨3.9ç‰ˆæœ¬ã€‚\n\n> æ³¨æ„ï¼šé€‰æ‹©Dockeréƒ¨ç½²åˆ™æ— éœ€å®‰è£…pythonç¯å¢ƒå’Œä¸‹è½½æºç ï¼Œå¯ç›´æ¥å¿«è¿›åˆ°ä¸‹ä¸€èŠ‚ã€‚\n\n**(1) å…‹éš†é¡¹ç›®ä»£ç ï¼š**\n\n```bash\ngit clone https://github.com/zhayujie/chatgpt-on-wechat\ncd chatgpt-on-wechat/\n```\n\nè‹¥é‡åˆ°ç½‘ç»œé—®é¢˜å¯ä½¿ç”¨å›½å†…ä»“åº“åœ°å€ï¼šhttps://gitee.com/zhayujie/chatgpt-on-wechat\n\n**(2) å®‰è£…æ ¸å¿ƒä¾èµ– (å¿…é€‰)ï¼š**\n\n```bash\npip3 install -r requirements.txt\n```\n\n**(3) æ‹“å±•ä¾èµ– (å¯é€‰ï¼Œå»ºè®®å®‰è£…)ï¼š**\n\n```bash\npip3 install -r requirements-optional.txt\n```\nå¦‚æœæŸé¡¹ä¾èµ–å®‰è£…å¤±è´¥å¯æ³¨é‡Šæ‰å¯¹åº”çš„è¡Œåé‡è¯•ã€‚\n\n## äºŒã€é…ç½®\n\né…ç½®æ–‡ä»¶çš„æ¨¡æ¿åœ¨æ ¹ç›®å½•çš„`config-template.json`ä¸­ï¼Œéœ€å¤åˆ¶è¯¥æ¨¡æ¿åˆ›å»ºæœ€ç»ˆç”Ÿæ•ˆçš„ `config.json` æ–‡ä»¶ï¼š\n\n```bash\n  cp config-template.json config.json\n```\n\nç„¶ååœ¨`config.json`ä¸­å¡«å…¥é…ç½®ï¼Œä»¥ä¸‹æ˜¯å¯¹é»˜è®¤é…ç½®çš„è¯´æ˜ï¼Œå¯æ ¹æ®éœ€è¦è¿›è¡Œè‡ªå®šä¹‰ä¿®æ”¹ï¼ˆæ³¨æ„å®é™…ä½¿ç”¨æ—¶è¯·å»æ‰æ³¨é‡Šï¼Œä¿è¯JSONæ ¼å¼çš„è§„èŒƒï¼‰ï¼š\n\n```bash\n# config.json æ–‡ä»¶å†…å®¹ç¤ºä¾‹\n{\n  "channel_type": "web",                                      # æ¥å…¥æ¸ é“ç±»å‹ï¼Œé»˜è®¤ä¸ºwebï¼Œæ”¯æŒä¿®æ”¹ä¸º:terminal, wechatmp, wechatmp_service, wechatcom_app, dingtalk, feishu\n  "model": "gpt-4o-mini",                                     # æ¨¡å‹åç§°, æ”¯æŒ gpt-4o-mini, gpt-4.1, gpt-4o, deepseek-reasoner, wenxin, xunfei, glm-4, claude-3-7-sonnet-latest, moonshotç­‰\n  "open_ai_api_key": "YOUR API KEY",                          # å¦‚æœä½¿ç”¨openAIæ¨¡å‹åˆ™å¡«å…¥ä¸Šé¢åˆ›å»ºçš„ OpenAI API KEY\n  "open_ai_api_base": "https://api.openai.com/v1",            # OpenAIæ¥å£ä»£ç†åœ°å€ï¼Œä¿®æ”¹æ­¤é¡¹å¯æ¥å…¥ç¬¬ä¸‰æ–¹æ¨¡å‹æ¥å£\n  "proxy": "",                                                # ä»£ç†å®¢æˆ·ç«¯çš„ipå’Œç«¯å£ï¼Œå›½å†…ç¯å¢ƒå¼€å¯ä»£ç†çš„éœ€è¦å¡«å†™è¯¥é¡¹ï¼Œå¦‚ "127.0.0.1:7890"\n  "single_chat_prefix": ["bot", "@bot"],                      # ç§èŠæ—¶æ–‡æœ¬éœ€è¦åŒ…å«è¯¥å‰ç¼€æ‰èƒ½è§¦å‘æœºå™¨äººå›å¤\n  "single_chat_reply_prefix": "[bot] ",                       # ç§èŠæ—¶è‡ªåŠ¨å›å¤çš„å‰ç¼€ï¼Œç”¨äºåŒºåˆ†çœŸäºº\n  "group_chat_prefix": ["@bot"],                              # ç¾¤èŠæ—¶åŒ…å«è¯¥å‰ç¼€åˆ™ä¼šè§¦å‘æœºå™¨äººå›å¤\n  "group_name_white_list": ["ChatGPTæµ‹è¯•ç¾¤", "ChatGPTæµ‹è¯•ç¾¤2"], # å¼€å¯è‡ªåŠ¨å›å¤çš„ç¾¤åç§°åˆ—è¡¨\n  "group_chat_in_one_session": ["ChatGPTæµ‹è¯•ç¾¤"],              # æ”¯æŒä¼šè¯ä¸Šä¸‹æ–‡å…±äº«çš„ç¾¤åç§°  \n  "image_create_prefix": ["ç”»", "çœ‹", "æ‰¾"],                   # å¼€å¯å›¾ç‰‡å›å¤çš„å‰ç¼€\n  "conversation_max_tokens": 1000,                            # æ”¯æŒä¸Šä¸‹æ–‡è®°å¿†çš„æœ€å¤šå­—ç¬¦æ•°\n  "speech_recognition": false,                                # æ˜¯å¦å¼€å¯è¯­éŸ³è¯†åˆ«\n  "group_speech_recognition": false,                          # æ˜¯å¦å¼€å¯ç¾¤ç»„è¯­éŸ³è¯†åˆ«\n  "voice_reply_voice": false,                                 # æ˜¯å¦ä½¿ç”¨è¯­éŸ³å›å¤è¯­éŸ³\n  "character_desc": "ä½ æ˜¯åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„AIæ™ºèƒ½åŠ©æ‰‹ï¼Œæ—¨åœ¨å›ç­”å¹¶è§£å†³äººä»¬çš„ä»»ä½•é—®é¢˜ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨å¤šç§è¯­è¨€ä¸äººäº¤æµã€‚",  # ç³»ç»Ÿæç¤ºè¯\n  # è®¢é˜…æ¬¢è¿è¯­ï¼Œå…¬ä¼—å·å’Œä¼ä¸šå¾®ä¿¡channelä¸­ä½¿ç”¨ï¼Œå½“è¢«è®¢é˜…æ—¶ä¼šè‡ªåŠ¨å›å¤ä»¥ä¸‹å†…å®¹\n  "subscribe_msg": "æ„Ÿè°¢æ‚¨çš„å…³æ³¨ï¼\nè¿™é‡Œæ˜¯AIæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥è‡ªç”±å¯¹è¯ã€‚\næ”¯æŒè¯­éŸ³å¯¹è¯ã€‚\næ”¯æŒå›¾ç‰‡è¾“å…¥ã€‚\næ”¯æŒå›¾ç‰‡è¾“å‡ºï¼Œç”»å­—å¼€å¤´çš„æ¶ˆæ¯å°†æŒ‰è¦æ±‚åˆ›ä½œå›¾ç‰‡ã€‚\næ”¯æŒtoolã€è§’è‰²æ‰®æ¼”å’Œæ–‡å­—å†’é™©ç­‰ä¸°å¯Œçš„æ’ä»¶ã€‚\nè¾“å…¥{trigger_prefix}#help æŸ¥çœ‹è¯¦ç»†æŒ‡ä»¤ã€‚",\n  "use_linkai": false,                                        # æ˜¯å¦ä½¿ç”¨LinkAIæ¥å£ï¼Œé»˜è®¤å…³é—­ï¼Œè®¾ç½®ä¸ºtrueåå¯å¯¹æ¥LinkAIå¹³å°çš„æ™ºèƒ½ä½“\n  "linkai_api_key": "",                                       # LinkAI Api Key\n  "linkai_app_code": ""                                       # LinkAI åº”ç”¨æˆ–å·¥ä½œæµçš„code\n}\n```\n\n**è¯¦ç»†é…ç½®è¯´æ˜:** \n\n<details>\n<summary>1. å•èŠé…ç½®</summary>\n\n+ ä¸ªäººèŠå¤©ä¸­ï¼Œéœ€è¦ä»¥ "bot"æˆ–"@bot" ä¸ºå¼€å¤´çš„å†…å®¹è§¦å‘æœºå™¨äººï¼Œå¯¹åº”é…ç½®é¡¹ `single_chat_prefix` (å¦‚æœä¸éœ€è¦ä»¥å‰ç¼€è§¦å‘å¯ä»¥å¡«å†™  `"single_chat_prefix": [""]`)\n+ æœºå™¨äººå›å¤çš„å†…å®¹ä¼šä»¥ "[bot] " ä½œä¸ºå‰ç¼€ï¼Œ ä»¥åŒºåˆ†çœŸäººï¼Œå¯¹åº”çš„é…ç½®é¡¹ä¸º `single_chat_reply_prefix` (å¦‚æœä¸éœ€è¦å‰ç¼€å¯ä»¥å¡«å†™ `"single_chat_reply_prefix": ""`)\n</details>\n\n\n<details>\n<summary>2. ç¾¤èŠé…ç½®</summary>\n\n+ ç¾¤ç»„èŠå¤©ä¸­ï¼Œç¾¤åç§°éœ€é…ç½®åœ¨ `group_name_white_list ` ä¸­æ‰èƒ½å¼€å¯ç¾¤èŠè‡ªåŠ¨å›å¤ã€‚å¦‚æœæƒ³å¯¹æ‰€æœ‰ç¾¤èŠç”Ÿæ•ˆï¼Œå¯ä»¥ç›´æ¥å¡«å†™ `"group_name_white_list": ["ALL_GROUP"]`\n+ é»˜è®¤åªè¦è¢«äºº @ å°±ä¼šè§¦å‘æœºå™¨äººè‡ªåŠ¨å›å¤ï¼›å¦å¤–ç¾¤èŠå¤©ä¸­åªè¦æ£€æµ‹åˆ°ä»¥ "@bot" å¼€å¤´çš„å†…å®¹ï¼ŒåŒæ ·ä¼šè‡ªåŠ¨å›å¤ï¼ˆæ–¹ä¾¿è‡ªå·±è§¦å‘ï¼‰ï¼Œè¿™å¯¹åº”é…ç½®é¡¹ `group_chat_prefix`\n+ å¯é€‰é…ç½®: `group_name_keyword_white_list`é…ç½®é¡¹æ”¯æŒæ¨¡ç³ŠåŒ¹é…ç¾¤åç§°ï¼Œ`group_chat_keyword`é…ç½®é¡¹åˆ™æ”¯æŒæ¨¡ç³ŠåŒ¹é…ç¾¤æ¶ˆæ¯å†…å®¹ï¼Œç”¨æ³•ä¸ä¸Šè¿°ä¸¤ä¸ªé…ç½®é¡¹ç›¸åŒã€‚ï¼ˆContributed by [evolay](https://github.com/evolay))\n+ `group_chat_in_one_session`ï¼šä½¿ç¾¤èŠå…±äº«ä¸€ä¸ªä¼šè¯ä¸Šä¸‹æ–‡ï¼Œé…ç½® `["ALL_GROUP"]` åˆ™ä½œç”¨äºæ‰€æœ‰ç¾¤èŠ\n</details>\n\n<details>\n<summary>3. è¯­éŸ³é…ç½®</summary>\n\n+ æ·»åŠ  `"speech_recognition": true` å°†å¼€å¯è¯­éŸ³è¯†åˆ«ï¼Œé»˜è®¤ä½¿ç”¨openaiçš„whisperæ¨¡å‹è¯†åˆ«ä¸ºæ–‡å­—ï¼ŒåŒæ—¶ä»¥æ–‡å­—å›å¤ï¼Œè¯¥å‚æ•°ä»…æ”¯æŒç§èŠ (æ³¨æ„ç”±äºè¯­éŸ³æ¶ˆæ¯æ— æ³•åŒ¹é…å‰ç¼€ï¼Œä¸€æ—¦å¼€å¯å°†å¯¹æ‰€æœ‰è¯­éŸ³è‡ªåŠ¨å›å¤ï¼Œæ”¯æŒè¯­éŸ³è§¦å‘ç”»å›¾)ï¼›\n+ æ·»åŠ  `"group_speech_recognition": true` å°†å¼€å¯ç¾¤ç»„è¯­éŸ³è¯†åˆ«ï¼Œé»˜è®¤ä½¿ç”¨openaiçš„whisperæ¨¡å‹è¯†åˆ«ä¸ºæ–‡å­—ï¼ŒåŒæ—¶ä»¥æ–‡å­—å›å¤ï¼Œå‚æ•°ä»…æ”¯æŒç¾¤èŠ (ä¼šåŒ¹é…group_chat_prefixå’Œgroup_chat_keyword, æ”¯æŒè¯­éŸ³è§¦å‘ç”»å›¾)ï¼›\n+ æ·»åŠ  `"voice_reply_voice": true` å°†å¼€å¯è¯­éŸ³å›å¤è¯­éŸ³ï¼ˆåŒæ—¶ä½œç”¨äºç§èŠå’Œç¾¤èŠï¼‰\n</details>\n\n<details>\n<summary>4. å…¶ä»–é…ç½®</summary>\n\n+ `model`: æ¨¡å‹åç§°ï¼Œç›®å‰æ”¯æŒ `gpt-4o-mini`, `gpt-4.1`, `gpt-4o`, `gpt-3.5-turbo`, `wenxin` , `claude` , `gemini`, `glm-4`,  `xunfei`, `moonshot`ç­‰ï¼Œå…¨éƒ¨æ¨¡å‹åç§°å‚è€ƒ[common/const.py](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/common/const.py)æ–‡ä»¶\n+ `temperature`,`frequency_penalty`,`presence_penalty`: Chat APIæ¥å£å‚æ•°ï¼Œè¯¦æƒ…å‚è€ƒ[OpenAIå®˜æ–¹æ–‡æ¡£ã€‚](https://platform.openai.com/docs/api-reference/chat)\n+ `proxy`ï¼šç”±äºç›®å‰ `openai` æ¥å£å›½å†…æ— æ³•è®¿é—®ï¼Œéœ€é…ç½®ä»£ç†å®¢æˆ·ç«¯çš„åœ°å€ï¼Œè¯¦æƒ…å‚è€ƒ  [#351](https://github.com/zhayujie/chatgpt-on-wechat/issues/351)\n+ å¯¹äºå›¾åƒç”Ÿæˆï¼Œåœ¨æ»¡è¶³ä¸ªäººæˆ–ç¾¤ç»„è§¦å‘æ¡ä»¶å¤–ï¼Œè¿˜éœ€è¦é¢å¤–çš„å…³é”®è¯å‰ç¼€æ¥è§¦å‘ï¼Œå¯¹åº”é…ç½® `image_create_prefix `\n+ å…³äºOpenAIå¯¹è¯åŠå›¾ç‰‡æ¥å£çš„å‚æ•°é…ç½®ï¼ˆå†…å®¹è‡ªç”±åº¦ã€å›å¤å­—æ•°é™åˆ¶ã€å›¾ç‰‡å¤§å°ç­‰ï¼‰ï¼Œå¯ä»¥å‚è€ƒ [å¯¹è¯æ¥å£](https://beta.openai.com/docs/api-reference/completions) å’Œ [å›¾åƒæ¥å£](https://beta.openai.com/docs/api-reference/completions)  æ–‡æ¡£ï¼Œåœ¨[`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py)ä¸­æ£€æŸ¥å“ªäº›å‚æ•°åœ¨æœ¬é¡¹ç›®ä¸­æ˜¯å¯é…ç½®çš„ã€‚\n+ `conversation_max_tokens`ï¼šè¡¨ç¤ºèƒ½å¤Ÿè®°å¿†çš„ä¸Šä¸‹æ–‡æœ€å¤§å­—æ•°ï¼ˆä¸€é—®ä¸€ç­”ä¸ºä¸€ç»„å¯¹è¯ï¼Œå¦‚æœç´¯ç§¯çš„å¯¹è¯å­—æ•°è¶…å‡ºé™åˆ¶ï¼Œå°±ä¼šä¼˜å…ˆç§»é™¤æœ€æ—©çš„ä¸€ç»„å¯¹è¯ï¼‰\n+ `rate_limit_chatgpt`ï¼Œ`rate_limit_dalle`ï¼šæ¯åˆ†é’Ÿæœ€é«˜é—®ç­”é€Ÿç‡ã€ç”»å›¾é€Ÿç‡ï¼Œè¶…é€Ÿåæ’é˜ŸæŒ‰åºå¤„ç†ã€‚\n+ `clear_memory_commands`: å¯¹è¯å†…æŒ‡ä»¤ï¼Œä¸»åŠ¨æ¸…ç©ºå‰æ–‡è®°å¿†ï¼Œå­—ç¬¦ä¸²æ•°ç»„å¯è‡ªå®šä¹‰æŒ‡ä»¤åˆ«åã€‚\n+ `hot_reload`: ç¨‹åºé€€å‡ºåï¼Œæš‚å­˜ç­‰äºçŠ¶æ€ï¼Œé»˜è®¤å…³é—­ã€‚\n+ `character_desc` é…ç½®ä¸­ä¿å­˜ç€ä½ å¯¹æœºå™¨äººè¯´çš„ä¸€æ®µè¯ï¼Œä»–ä¼šè®°ä½è¿™æ®µè¯å¹¶ä½œä¸ºä»–çš„è®¾å®šï¼Œä½ å¯ä»¥ä¸ºä»–å®šåˆ¶ä»»ä½•äººæ ¼      (å…³äºä¼šè¯ä¸Šä¸‹æ–‡çš„æ›´å¤šå†…å®¹å‚è€ƒè¯¥ [issue](https://github.com/zhayujie/chatgpt-on-wechat/issues/43))\n+ `subscribe_msg`ï¼šè®¢é˜…æ¶ˆæ¯ï¼Œå…¬ä¼—å·å’Œä¼ä¸šå¾®ä¿¡channelä¸­è¯·å¡«å†™ï¼Œå½“è¢«è®¢é˜…æ—¶ä¼šè‡ªåŠ¨å›å¤ï¼Œ å¯ä½¿ç”¨ç‰¹æ®Šå ä½ç¬¦ã€‚ç›®å‰æ”¯æŒçš„å ä½ç¬¦æœ‰{trigger_prefix}ï¼Œåœ¨ç¨‹åºä¸­å®ƒä¼šè‡ªåŠ¨æ›¿æ¢æˆbotçš„è§¦å‘è¯ã€‚\n</details>\n\n<details>\n<summary>5. LinkAIé…ç½®</summary>\n\n+ `use_linkai`: æ˜¯å¦ä½¿ç”¨LinkAIæ¥å£ï¼Œé»˜è®¤å…³é—­ï¼Œè®¾ç½®ä¸ºtrueåå¯å¯¹æ¥LinkAIå¹³å°çš„Agentï¼Œä½¿ç”¨çŸ¥è¯†åº“ã€å·¥ä½œæµã€è”ç½‘æœç´¢ã€`Midjourney` ç»˜ç”»ç­‰èƒ½åŠ›, å‚è€ƒ [æ–‡æ¡£](https://link-ai.tech/platform/link-app/wechat)\n+ `linkai_api_key`: LinkAI Api Keyï¼Œå¯åœ¨ [æ§åˆ¶å°](https://link-ai.tech/console/interface) åˆ›å»º\n+ `linkai_app_code`: LinkAI åº”ç”¨æˆ–å·¥ä½œæµçš„codeï¼Œé€‰å¡«\n</details>\n\næ³¨ï¼šå®Œæ•´é…ç½®é¡¹è¯´æ˜å¯åœ¨ [`config.py`](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/config.py) æ–‡ä»¶ä¸­æŸ¥çœ‹ã€‚\n\n## ä¸‰ã€è¿è¡Œ\n\n### 1.æœ¬åœ°è¿è¡Œ\n\nå¦‚æœæ˜¯ä¸ªäººè®¡ç®—æœº **æœ¬åœ°è¿è¡Œ**ï¼Œç›´æ¥åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰§è¡Œï¼š\n\n```bash\npython3 app.py         # windowsç¯å¢ƒä¸‹è¯¥å‘½ä»¤é€šå¸¸ä¸º python app.py\n```\n\nè¿è¡Œåé»˜è®¤ä¼šå¯åŠ¨ä¸€ä¸ªwebæœåŠ¡ï¼Œå¯ä»¥é€šè¿‡è®¿é—® `http://localhost:9899/chat` åœ¨ç½‘é¡µç«¯å¯¹è¯ã€‚å¦‚æœéœ€è¦æ¥å…¥å…¶ä»–åº”ç”¨é€šé“åªéœ€ä¿®æ”¹ `config.json` é…ç½®æ–‡ä»¶ä¸­çš„ `channel_type` å‚æ•°ï¼Œè¯¦æƒ…å‚è€ƒï¼š[é€šé“è¯´æ˜](#é€šé“è¯´æ˜)ã€‚\n\nå‘æœºå™¨äººå‘é€ `#help` æ¶ˆæ¯å¯ä»¥æŸ¥çœ‹å¯ç”¨æŒ‡ä»¤åŠæ’ä»¶çš„è¯´æ˜ã€‚\n\n### 2.æœåŠ¡å™¨éƒ¨ç½²\n\nåœ¨æœåŠ¡å™¨ä¸­å¯ä½¿ç”¨ `nohup` å‘½ä»¤åœ¨åå°è¿è¡Œç¨‹åºï¼š\n\n```bash\nnohup python3 app.py & tail -f nohup.out\n```\n\næ‰§è¡Œåç¨‹åºè¿è¡ŒäºæœåŠ¡å™¨åå°ï¼Œå¯é€šè¿‡ `ctrl+c` å…³é—­æ—¥å¿—ï¼Œä¸ä¼šå½±å“åå°ç¨‹åºçš„è¿è¡Œã€‚ä½¿ç”¨ `ps -ef | grep app.py | grep -v grep` å‘½ä»¤å¯æŸ¥çœ‹è¿è¡Œäºåå°çš„è¿›ç¨‹ï¼Œå¦‚æœæƒ³è¦é‡æ–°å¯åŠ¨ç¨‹åºå¯ä»¥å…ˆ `kill` æ‰å¯¹åº”çš„è¿›ç¨‹ã€‚ æ—¥å¿—å…³é—­åå¦‚æœæƒ³è¦å†æ¬¡æ‰“å¼€åªéœ€è¾“å…¥ `tail -f nohup.out`ã€‚ \n\næ­¤å¤–ï¼Œé¡¹ç›®çš„ `scripts` ç›®å½•ä¸‹æœ‰ä¸€é”®è¿è¡Œã€å…³é—­ç¨‹åºçš„è„šæœ¬ä¾›ä½¿ç”¨ã€‚ è¿è¡Œåé»˜è®¤channelä¸ºwebï¼Œé€šè¿‡å¯ä»¥é€šè¿‡ä¿®æ”¹é…ç½®æ–‡ä»¶è¿›è¡Œåˆ‡æ¢ã€‚\n\n\n### 3.Dockeréƒ¨ç½²\n\nä½¿ç”¨dockeréƒ¨ç½²æ— éœ€ä¸‹è½½æºç å’Œå®‰è£…ä¾èµ–ï¼Œåªéœ€è¦è·å– `docker-compose.yml` é…ç½®æ–‡ä»¶å¹¶å¯åŠ¨å®¹å™¨å³å¯ã€‚\n\n> å‰ææ˜¯éœ€è¦å®‰è£…å¥½ `docker` åŠ `docker-compose`ï¼Œå®‰è£…æˆåŠŸåæ‰§è¡Œ `docker -v` å’Œ `docker-compose version` (æˆ– `docker compose version`) å¯æŸ¥çœ‹åˆ°ç‰ˆæœ¬å·ã€‚å®‰è£…åœ°å€ä¸º [dockerå®˜ç½‘](https://docs.docker.com/engine/install/) ã€‚\n\n**(1) ä¸‹è½½ docker-compose.yml æ–‡ä»¶**\n\n```bash\nwget https://cdn.link-ai.tech/code/cow/docker-compose.yml\n```\n\nä¸‹è½½å®Œæˆåæ‰“å¼€ `docker-compose.yml` å¡«å†™æ‰€éœ€é…ç½®ï¼Œä¾‹å¦‚ `CHANNEL_TYPE`ã€`OPEN_AI_API_KEY` å’Œç­‰é…ç½®ã€‚\n\n**(2) å¯åŠ¨å®¹å™¨**\n\nåœ¨ `docker-compose.yml` æ‰€åœ¨ç›®å½•ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨å®¹å™¨ï¼š\n\n```bash\nsudo docker compose up -d         # è‹¥docker-composeä¸º 1.X ç‰ˆæœ¬ï¼Œåˆ™æ‰§è¡Œ `sudo  docker-compose up -d`\n```\n\nè¿è¡Œå‘½ä»¤åï¼Œä¼šè‡ªåŠ¨å– [docker hub](https://hub.docker.com/r/zhayujie/chatgpt-on-wechat) æ‹‰å–æœ€æ–°releaseç‰ˆæœ¬çš„é•œåƒã€‚å½“æ‰§è¡Œ `sudo docker ps` èƒ½æŸ¥çœ‹åˆ° NAMES ä¸º chatgpt-on-wechat çš„å®¹å™¨å³è¡¨ç¤ºè¿è¡ŒæˆåŠŸã€‚æœ€åæ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¯æŸ¥çœ‹å®¹å™¨çš„è¿è¡Œæ—¥å¿—ï¼š\n\n```bash\nsudo docker logs -f chatgpt-on-wechat\n```\n\n**(3) æ’ä»¶ä½¿ç”¨**\n\nå¦‚æœéœ€è¦åœ¨dockerå®¹å™¨ä¸­ä¿®æ”¹æ’ä»¶é…ç½®ï¼Œå¯é€šè¿‡æŒ‚è½½çš„æ–¹å¼å®Œæˆï¼Œå°† [æ’ä»¶é…ç½®æ–‡ä»¶](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/config.json.template)\né‡å‘½åä¸º `config.json`ï¼Œæ”¾ç½®äº `docker-compose.yml` ç›¸åŒç›®å½•ä¸‹ï¼Œå¹¶åœ¨ `docker-compose.yml` ä¸­çš„ `chatgpt-on-wechat` éƒ¨åˆ†ä¸‹æ·»åŠ  `volumes` æ˜ å°„:\n\n```\nvolumes:\n  - ./config.json:/app/plugins/config.json\n```\n**æ³¨**ï¼šä½¿ç”¨dockeræ–¹å¼éƒ¨ç½²çš„è¯¦ç»†æ•™ç¨‹å¯ä»¥å‚è€ƒï¼š[dockeréƒ¨ç½²CoWé¡¹ç›®](https://www.wangpc.cc/ai/docker-deploy-cow/)\n\n\n## æ¨¡å‹è¯´æ˜\n\nä»¥ä¸‹å¯¹æ‰€æœ‰å¯æ”¯æŒçš„æ¨¡å‹çš„é…ç½®å’Œä½¿ç”¨æ–¹æ³•è¿›è¡Œè¯´æ˜ï¼Œæ¨¡å‹æ¥å£å®ç°åœ¨é¡¹ç›®çš„ `bot/` ç›®å½•ä¸‹ã€‚\n>éƒ¨åˆ†æ¨¡å‹å‚å•†æ¥å…¥æœ‰å®˜æ–¹sdkå’ŒOpenAIå…¼å®¹ä¸¤ç§æ–¹å¼ï¼Œå»ºè®®ä½¿ç”¨OpenAIå…¼å®¹çš„æ–¹å¼ã€‚\n\n<details>\n<summary>OpenAI</summary>\n\n1. API Keyåˆ›å»ºï¼šåœ¨ [OpenAIå¹³å°](https://platform.openai.com/api-keys) åˆ›å»ºAPI Key\n\n2. å¡«å†™é…ç½®\n\n```json\n{\n    "model": "gpt-4.1-mini",\n    "open_ai_api_key": "YOUR_API_KEY",\n    "open_ai_api_base": "https://api.openai.com/v1",\n    "bot_type": "chatGPT"\n}\n```\n\n - `model`: ä¸OpenAIæ¥å£çš„ [modelå‚æ•°](https://platform.openai.com/docs/models) ä¸€è‡´ï¼Œæ”¯æŒåŒ…æ‹¬ oç³»åˆ—ã€gpt-4ç³»åˆ—ã€gpt-3.5ç³»åˆ—ç­‰æ¨¡å‹\n - `open_ai_api_base`: å¦‚æœéœ€è¦æ¥å…¥ç¬¬ä¸‰æ–¹ä»£ç†æ¥å£ï¼Œå¯é€šè¿‡ä¿®æ”¹è¯¥å‚æ•°è¿›è¡Œæ¥å…¥\n - `bot_type`: ä½¿ç”¨OpenAIç›¸å…³æ¨¡å‹æ—¶æ— éœ€å¡«å†™ã€‚å½“ä½¿ç”¨ç¬¬ä¸‰æ–¹ä»£ç†æ¥å£æ¥å…¥Claudeç­‰éOpenAIå®˜æ–¹æ¨¡å‹æ—¶ï¼Œè¯¥å‚æ•°è®¾ä¸º `chatGPT`\n</details>\n\n<details>\n<summary>LinkAI</summary>\n\n1. API Keyåˆ›å»ºï¼šåœ¨ [LinkAIå¹³å°](https://link-ai.tech/console/interface) åˆ›å»ºAPI Key \n\n2. å¡«å†™é…ç½®\n\n```json\n{\n "use_linkai": true,\n "linkai_api_key": "YOUR API KEY",\n "linkai_app_code": "YOUR APP CODE"\n}\n```\n\n+ `use_linkai`: æ˜¯å¦ä½¿ç”¨LinkAIæ¥å£ï¼Œé»˜è®¤å…³é—­ï¼Œè®¾ç½®ä¸ºtrueåå¯å¯¹æ¥LinkAIå¹³å°çš„æ™ºèƒ½ä½“ï¼Œä½¿ç”¨çŸ¥è¯†åº“ã€å·¥ä½œæµã€æ•°æ®åº“ã€è”ç½‘æœç´¢ã€MCPå·¥å…·ç­‰ä¸°å¯Œçš„Agentèƒ½åŠ›, å‚è€ƒ [æ–‡æ¡£](https://link-ai.tech/platform/link-app/wechat)\n+ `linkai_api_key`: LinkAIå¹³å°çš„API Keyï¼Œå¯åœ¨ [æ§åˆ¶å°](https://link-ai.tech/console/interface) ä¸­åˆ›å»º\n+ `linkai_app_code`: LinkAIæ™ºèƒ½ä½“ (åº”ç”¨æˆ–å·¥ä½œæµ) çš„codeï¼Œé€‰å¡«ã€‚æ™ºèƒ½ä½“åˆ›å»ºå¯å‚è€ƒ [è¯´æ˜æ–‡æ¡£](https://docs.link-ai.tech/platform/quick-start)\n+ `model`: modelå­—æ®µå¡«å†™ç©ºåˆ™ç›´æ¥ä½¿ç”¨æ™ºèƒ½ä½“çš„æ¨¡å‹ï¼Œå¯åœ¨å¹³å°ä¸­çµæ´»åˆ‡æ¢ï¼Œ[æ¨¡å‹åˆ—è¡¨](https://link-ai.tech/console/models)ä¸­çš„å…¨éƒ¨æ¨¡å‹å‡å¯ä½¿ç”¨\n</details>\n\n<details>\n<summary>DeepSeek</summary>\n\n1. API Keyåˆ›å»ºï¼šåœ¨ [DeepSeekå¹³å°](https://platform.deepseek.com/api_keys) åˆ›å»ºAPI Key \n\n2. å¡«å†™é…ç½®\n\n```json\n{\n  "bot_type": "chatGPT",\n  "model": "deepseek-chat",\n  "open_ai_api_key": "sk-xxxxxxxxxxx",\n  "open_ai_api_base": "https://api.deepseek.com/v1"\n}\n```\n\n - `bot_type`: OpenAIå…¼å®¹æ–¹å¼\n - `model`: å¯å¡« `deepseek-chatã€deepseek-reasoner`ï¼Œåˆ†åˆ«å¯¹åº”çš„æ˜¯ V3 å’Œ R1 æ¨¡å‹\n - `open_ai_api_key`: DeepSeekå¹³å°çš„ API Key\n - `open_ai_api_base`: DeepSeekå¹³å° BASE URL\n</details>\n\n<details>\n<summary>Azure</summary>\n\n1. API Keyåˆ›å»ºï¼šåœ¨ [DeepSeekå¹³å°](https://platform.deepseek.com/api_keys) åˆ›å»ºAPI Key \n\n2. å¡«å†™é…ç½®\n\n```json\n{\n  "model": "",\n  "use_azure_chatgpt": true,\n  "open_ai_api_key": "e7ffc5dd84f14521a53f14a40231ea78",\n  "open_ai_api_base": "https://linkai-240917.openai.azure.com/",\n  "azure_deployment_id": "gpt-4.1",\n  "azure_api_version": "2025-01-01-preview"\n}\n```\n\n - `model`: ç•™ç©ºå³å¯\n - `use_azure_chatgpt`: è®¾ä¸º true \n - `open_ai_api_key`: Azureå¹³å°çš„å¯†é’¥\n - `open_ai_api_base`: Azureå¹³å°çš„ BASE URL\n - `azure_deployment_id`: Azureå¹³å°éƒ¨ç½²çš„æ¨¡å‹åç§°\n - `azure_api_version`: apiç‰ˆæœ¬ä»¥åŠä»¥ä¸Šå‚æ•°å¯ä»¥åœ¨éƒ¨ç½²çš„ [æ¨¡å‹é…ç½®](https://oai.azure.com/resource/deployments) ç•Œé¢æŸ¥çœ‹\n</details>\n\n<details>\n<summary>Claude</summary>\n\n1. API Keyåˆ›å»ºï¼šåœ¨ [Claudeæ§åˆ¶å°](https://console.anthropic.com/settings/keys) åˆ›å»ºAPI Key\n\n2. å¡«å†™é…ç½®\n\n```json\n{\n    "model": "claude-sonnet-4-0",\n    "claude_api_key": "YOUR_API_KEY"\n}\n```\n - `model`: å‚è€ƒ [å®˜æ–¹æ¨¡å‹ID](https://docs.anthropic.com/en/docs/about-claude/models/overview#model-aliases) ï¼Œä¾‹å¦‚`claude-opus-4-0`ã€`claude-3-7-sonnet-latest`ç­‰\n</details>\n\n<details>\n<summary>é€šä¹‰åƒé—®</summary>\n\næ–¹å¼ä¸€ï¼šå®˜æ–¹SDKæ¥å…¥ï¼Œé…ç½®å¦‚ä¸‹ï¼š\n\n```json\n{\n    "model": "qwen-turbo",\n    "dashscope_api_key": "sk-qVxxxxG"\n}\n```\n - `model`: å¯å¡«å†™`qwen-turboã€qwen-plusã€qwen-max`\n - `dashscope_api_key`: é€šä¹‰åƒé—®çš„ API-KEYï¼Œå‚è€ƒ [å®˜æ–¹æ–‡æ¡£](https://bailian.console.aliyun.com/?tab=api#/api) ï¼Œåœ¨ [æ§åˆ¶å°](https://bailian.console.aliyun.com/?tab=model#/api-key) åˆ›å»º\n \næ–¹å¼äºŒï¼šOpenAIå…¼å®¹æ–¹å¼æ¥å…¥ï¼Œé…ç½®å¦‚ä¸‹ï¼š\n```json\n{\n  "bot_type": "chatGPT",\n  "model": "qwen-turbo",\n  "open_ai_api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",\n  "open_ai_api_key": "sk-qVxxxxG"\n}\n```\n- `bot_type`: OpenAIå…¼å®¹æ–¹å¼\n- `model`: æ”¯æŒå®˜æ–¹æ‰€æœ‰æ¨¡å‹ï¼Œå‚è€ƒ[æ¨¡å‹åˆ—è¡¨](https://help.aliyun.com/zh/model-studio/models?spm=a2c4g.11186623.0.0.78d84823Kth5on#9f8890ce29g5u)\n- `open_ai_api_base`: é€šä¹‰åƒé—®APIçš„ BASE URL\n- `open_ai_api_key`: é€šä¹‰åƒé—®çš„ API-KEYï¼Œå‚è€ƒ [å®˜æ–¹æ–‡æ¡£](https://bailian.console.aliyun.com/?tab=api#/api) ï¼Œåœ¨ [æ§åˆ¶å°](https://bailian.console.aliyun.com/?tab=model#/api-key) åˆ›å»º\n</details>\n\n<details>\n<summary>Gemini</summary>\n\nAPI Keyåˆ›å»ºï¼šåœ¨ [æ§åˆ¶å°](https://aistudio.google.com/app/apikey?hl=zh-cn) åˆ›å»ºAPI Key ï¼Œé…ç½®å¦‚ä¸‹\n```json\n{\n    "model": "gemini-2.5-pro",\n    "gemini_api_key": ""\n}\n```\n - `model`: å‚è€ƒ[å®˜æ–¹æ–‡æ¡£-æ¨¡å‹åˆ—è¡¨](https://ai.google.dev/gemini-api/docs/models?hl=zh-cn)\n</details>\n\n<details>\n<summary>Moonshot</summary>\n\næ–¹å¼ä¸€ï¼šå®˜æ–¹æ¥å…¥ï¼Œé…ç½®å¦‚ä¸‹ï¼š\n\n```json\n{\n    "model": "moonshot-v1-8k",\n    "moonshot_api_key": "moonshot-v1-8k"\n}\n```\n - `model`: å¯å¡«å†™`moonshot-v1-8kã€ moonshot-v1-32kã€ moonshot-v1-128k`\n - `moonshot_api_key`: Moonshotçš„API-KEYï¼Œåœ¨ [æ§åˆ¶å°](https://platform.moonshot.cn/console/api-keys) åˆ›å»º\n \næ–¹å¼äºŒï¼šOpenAIå…¼å®¹æ–¹å¼æ¥å…¥ï¼Œé…ç½®å¦‚ä¸‹ï¼š\n```json\n{\n  "bot_type": "chatGPT",\n  "model": "moonshot-v1-8k",\n  "open_ai_api_base": "https://api.moonshot.cn/v1",\n  "open_ai_api_key": ""\n}\n```\n- `bot_type`: OpenAIå…¼å®¹æ–¹å¼\n- `model`: å¯å¡«å†™`moonshot-v1-8kã€ moonshot-v1-32kã€ moonshot-v1-128k`\n- `open_ai_api_base`: Moonshotçš„ BASE URL\n- `open_ai_api_key`: Moonshotçš„ API-KEYï¼Œåœ¨ [æ§åˆ¶å°](https://platform.moonshot.cn/console/api-keys) åˆ›å»º\n</details>\n\n<details>\n<summary>ç™¾åº¦æ–‡å¿ƒ</summary>\næ–¹å¼ä¸€ï¼šå®˜æ–¹SDKæ¥å…¥ï¼Œé…ç½®å¦‚ä¸‹ï¼š\n\n```json\n{\n    "model": "wenxin", \n    "baidu_wenxin_api_key": "IajztZ0bDxgnP9bEykU7lBer",\n    "baidu_wenxin_secret_key": "EDPZn6L24uAS9d8RWFfotK47dPvkjD6G"\n}\n```\n - `model`: å¯å¡« `wenxin`å’Œ`wenxin-4`ï¼Œå¯¹åº”æ¨¡å‹ä¸º æ–‡å¿ƒ-3.5 å’Œ æ–‡å¿ƒ-4.0\n - `baidu_wenxin_api_key`ï¼šå‚è€ƒ [åƒå¸†å¹³å°-access_tokené‰´æƒ](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/dlv4pct3s) æ–‡æ¡£è·å– API Key\n - `baidu_wenxin_secret_key`ï¼šå‚è€ƒ [åƒå¸†å¹³å°-access_tokené‰´æƒ](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/dlv4pct3s) æ–‡æ¡£è·å– Secret Key\n\næ–¹å¼äºŒï¼šOpenAIå…¼å®¹æ–¹å¼æ¥å…¥ï¼Œé…ç½®å¦‚ä¸‹ï¼š\n```json\n{\n  "bot_type": "chatGPT",\n  "model": "qwen-turbo",\n  "open_ai_api_base": "https://qianfan.baidubce.com/v2",\n  "open_ai_api_key": "bce-v3/ALTxxxxxxd2b"\n}\n```\n- `bot_type`: OpenAIå…¼å®¹æ–¹å¼\n- `model`: æ”¯æŒå®˜æ–¹æ‰€æœ‰æ¨¡å‹ï¼Œå‚è€ƒ[æ¨¡å‹åˆ—è¡¨](https://cloud.baidu.com/doc/WENXINWORKSHOP/s/Wm9cvy6rl)\n- `open_ai_api_base`: ç™¾åº¦æ–‡å¿ƒAPIçš„ BASE URL\n- `open_ai_api_key`: ç™¾åº¦æ–‡å¿ƒçš„ API-KEYï¼Œå‚è€ƒ [å®˜æ–¹æ–‡æ¡£](https://cloud.baidu.com/doc/qianfan-api/s/ym9chdsy5) ï¼Œåœ¨ [æ§åˆ¶å°](https://console.bce.baidu.com/iam/#/iam/apikey/list) åˆ›å»ºAPI Key\n\n</details>\n\n<details>\n<summary>è®¯é£æ˜Ÿç«</summary>\n\næ–¹å¼ä¸€ï¼šå®˜æ–¹æ¥å…¥ï¼Œé…ç½®å¦‚ä¸‹ï¼š\nå‚è€ƒ [å®˜æ–¹æ–‡æ¡£-å¿«é€ŸæŒ‡å¼•](https://www.xfyun.cn/doc/platform/quickguide.html#%E7%AC%AC%E4%BA%8C%E6%AD%A5-%E5%88%9B%E5%BB%BA%E6%82%A8%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8-%E5%BC%80%E5%A7%8B%E4%BD%BF%E7%94%A8%E6%9C%8D%E5%8A%A1) è·å– `APPIDã€ APISecretã€ APIKey` ä¸‰ä¸ªå‚æ•°\n\n```json\n{\n  "model": "xunfei",\n  "xunfei_app_id": "",\n  "xunfei_api_key": "",\n  "xunfei_api_secret": "",\n  "xunfei_domain": "4.0Ultra",\n  "xunfei_spark_url": "wss://spark-api.xf-yun.com/v4.0/chat"\n}\n```\n - `model`: å¡« `xunfei`\n - `xunfei_domain`: å¯å¡«å†™ `4.0Ultraã€ generalv3.5ã€ max-32kã€ generalv3ã€ pro-128kã€ lite`\n - `xunfei_spark_url`: å¡«å†™å‚è€ƒ [å®˜æ–¹æ–‡æ¡£-è¯·æ±‚åœ°å€](https://www.xfyun.cn/doc/spark/Web.html#_1-1-%E8%AF%B7%E6%B1%82%E5%9C%B0%E5%9D%80) çš„è¯´æ˜\n \næ–¹å¼äºŒï¼šOpenAIå…¼å®¹æ–¹å¼æ¥å…¥ï¼Œé…ç½®å¦‚ä¸‹ï¼š\n```json\n{\n  "bot_type": "chatGPT",\n  "model": "4.0Ultra",\n  "open_ai_api_base": "https://spark-api-open.xf-yun.com/v1",\n  "open_ai_api_key": ""\n}\n```\n- `bot_type`: OpenAIå…¼å®¹æ–¹å¼\n- `model`: å¯å¡«å†™ `4.0Ultraã€ generalv3.5ã€ max-32kã€ generalv3ã€ pro-128kã€ lite`\n- `open_ai_api_base`: è®¯é£æ˜Ÿç«å¹³å°çš„ BASE URL\n- `open_ai_api_key`: è®¯é£æ˜Ÿç«å¹³å°çš„[APIPassword](https://console.xfyun.cn/services/bm3) ï¼Œå› æ¨¡å‹è€Œå·²\n</details>\n\n<details>\n<summary>æ™ºè°±AI</summary>\n\næ–¹å¼ä¸€ï¼šå®˜æ–¹æ¥å…¥ï¼Œé…ç½®å¦‚ä¸‹ï¼š\n\n```json\n{\n  "model": "glm-4-plus",\n  "zhipu_ai_api_key": ""\n}\n```\n - `model`: å¯å¡« `glm-4-plusã€glm-4-air-250414ã€glm-4-airxã€glm-4-long ã€glm-4-flashx ã€glm-4-flash-250414`, å‚è€ƒ [glm-4ç³»åˆ—æ¨¡å‹ç¼–ç ](https://bigmodel.cn/dev/api/normal-model/glm-4)\n - `zhipu_ai_api_key`: æ™ºè°±AIå¹³å°çš„ API KEYï¼Œåœ¨ [æ§åˆ¶å°](https://www.bigmodel.cn/usercenter/proj-mgmt/apikeys) åˆ›å»º\n \næ–¹å¼äºŒï¼šOpenAIå…¼å®¹æ–¹å¼æ¥å…¥ï¼Œé…ç½®å¦‚ä¸‹ï¼š\n```json\n{\n  "bot_type": "chatGPT",\n  "model": "glm-4-plus",\n  "open_ai_api_base": "https://open.bigmodel.cn/api/paas/v4",\n  "open_ai_api_key": ""\n}\n```\n- `bot_type`: OpenAIå…¼å®¹æ–¹å¼\n- `model`: å¯å¡« `glm-4-plusã€glm-4-air-250414ã€glm-4-airxã€glm-4-long ã€glm-4-flashx ã€glm-4-flash-250414`, å‚è€ƒ [glm-4ç³»åˆ—æ¨¡å‹ç¼–ç ](https://bigmodel.cn/dev/api/normal-model/glm-4) \n- `open_ai_api_base`: æ™ºè°±AIå¹³å°çš„ BASE URL\n- `open_ai_api_key`: æ™ºè°±AIå¹³å°çš„ API KEYï¼Œåœ¨ [æ§åˆ¶å°](https://www.bigmodel.cn/usercenter/proj-mgmt/apikeys) åˆ›å»º\n</details>\n\n<details>\n<summary>MiniMax</summary>\n\næ–¹å¼ä¸€ï¼šå®˜æ–¹æ¥å…¥ï¼Œé…ç½®å¦‚ä¸‹ï¼š\n\n```json\n{\n    "model": "abab6.5-chat",\n    "Minimax_api_key": "",\n    "Minimax_group_id": ""\n}\n```\n - `model`: å¯å¡«å†™`abab6.5-chat`\n - `Minimax_api_key`ï¼šMiniMaxå¹³å°çš„API-KEYï¼Œåœ¨ [æ§åˆ¶å°](https://platform.minimaxi.com/user-center/basic-information/interface-key) åˆ›å»º\n - `Minimax_group_id`: åœ¨ [è´¦æˆ·ä¿¡æ¯](https://platform.minimaxi.com/user-center/basic-information) å³ä¸Šè§’è·å–\n \næ–¹å¼äºŒï¼šOpenAIå…¼å®¹æ–¹å¼æ¥å…¥ï¼Œé…ç½®å¦‚ä¸‹ï¼š\n```json\n{\n  "bot_type": "chatGPT",\n  "model": "MiniMax-M1",\n  "open_ai_api_base": "https://api.minimaxi.com/v1",\n  "open_ai_api_key": ""\n}\n```\n- `bot_type`: OpenAIå…¼å®¹æ–¹å¼\n- `model`: å¯å¡«`MiniMax-M1ã€MiniMax-Text-01`ï¼Œå‚è€ƒ[APIæ–‡æ¡£](https://platform.minimaxi.com/document/%E5%AF%B9%E8%AF%9D?key=66701d281d57f38758d581d0#QklxsNSbaf6kM4j6wjO5eEek)\n- `open_ai_api_base`: MiniMaxå¹³å°APIçš„ BASE URL\n- `open_ai_api_key`: MiniMaxå¹³å°çš„API-KEYï¼Œåœ¨ [æ§åˆ¶å°](https://platform.minimaxi.com/user-center/basic-information/interface-key) åˆ›å»º\n</details>\n\n<details>\n<summary>ModelScope</summary>\n\n```json\n{\n  "bot_type": "modelscope",\n  "model": "Qwen/QwQ-32B",\n  "modelscope_api_key": "your_api_key",\n  "modelscope_base_url": "https://api-inference.modelscope.cn/v1/chat/completions",\n  "text_to_image": "MusePublic/489_ckpt_FLUX_1"\n}\n```\n\n- `bot_type`: modelscopeæ¥å£æ ¼å¼\n- `model`: å‚è€ƒ[æ¨¡å‹åˆ—è¡¨](https://www.modelscope.cn/models?filter=inference_type&page=1)\n- `modelscope_api_key`: å‚è€ƒ [å®˜æ–¹æ–‡æ¡£-è®¿é—®ä»¤ç‰Œ](https://modelscope.cn/docs/accounts/token) ï¼Œåœ¨ [æ§åˆ¶å°](https://modelscope.cn/my/myaccesstoken) \n- `modelscope_base_url`: modelscopeå¹³å°çš„ BASE URL\n- `text_to_image`: å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå‚è€ƒ[æ¨¡å‹åˆ—è¡¨](https://www.modelscope.cn/models?filter=inference_type&page=1)\n</details>\n\n\n## é€šé“è¯´æ˜\n\nä»¥ä¸‹å¯¹å¯æ¥å…¥é€šé“çš„é…ç½®æ–¹å¼è¿›è¡Œè¯´æ˜ï¼Œåº”ç”¨é€šé“ä»£ç åœ¨é¡¹ç›®çš„ `channel/` ç›®å½•ä¸‹ã€‚\n\n<details>\n<summary>Web</summary>\n\né¡¹ç›®å¯åŠ¨åé»˜è®¤è¿è¡Œwebé€šé“ï¼Œé…ç½®å¦‚ä¸‹ï¼š\n\n```json\n{\n    "channel_type": "web",\n    "web_port": 9899\n}\n```\n- `web_port`: é»˜è®¤ä¸º 9899ï¼Œå¯æŒ‰éœ€æ›´æ”¹ï¼Œéœ€è¦æœåŠ¡å™¨é˜²ç«å¢™å’Œå®‰å…¨ç»„æ”¾è¡Œè¯¥ç«¯å£\n- å¦‚æœ¬åœ°è¿è¡Œï¼Œå¯åŠ¨åè¯·è®¿é—® `http://localhost:port/chat` ï¼›å¦‚æœåŠ¡å™¨è¿è¡Œï¼Œè¯·è®¿é—® `http://ip:port/chat` \n> æ³¨ï¼šè¯·å°†ä¸Šè¿° url ä¸­çš„ ip æˆ–è€… port æ›¿æ¢ä¸ºå®é™…çš„å€¼\n</details>\n\n<details>\n<summary>Terminal</summary>\n\nä¿®æ”¹ `config.json` ä¸­çš„ `channel_type` å­—æ®µï¼š\n\n```json\n{\n    "channel_type": "terminal"\n}\n```\n\nè¿è¡Œåå¯åœ¨ç»ˆç«¯ä¸æœºå™¨äººè¿›è¡Œå¯¹è¯ã€‚\n\n</details>\n\n<details>\n<summary>å¾®ä¿¡å…¬ä¼—å·</summary>\n\næœ¬é¡¹ç›®æ”¯æŒè®¢é˜…å·å’ŒæœåŠ¡å·ä¸¤ç§å…¬ä¼—å·ï¼Œé€šè¿‡æœåŠ¡å·(`wechatmp_service`)ä½“éªŒæ›´ä½³ã€‚å°†ä¸‹åˆ—é…ç½®åŠ å…¥ `config.json`ï¼š\n\n```json\n{\n    "channel_type": "wechatmp",\n    "wechatmp_token": "TOKEN",\n    "wechatmp_port": 80,\n    "wechatmp_app_id": "APPID",\n    "wechatmp_app_secret": "APPSECRET",\n    "wechatmp_aes_key": ""\n}\n```\n- `channel_type`: ä¸ªäººè®¢é˜…å·ä¸º`wechatmp`ï¼Œä¼ä¸šæœåŠ¡å·ä¸º`wechatmp_service`\n\nè¯¦ç»†æ­¥éª¤å’Œå‚æ•°è¯´æ˜å‚è€ƒ [å¾®ä¿¡å…¬ä¼—å·æ¥å…¥](https://docs.link-ai.tech/cow/multi-platform/wechat-mp)\n\n</details>\n\n<details>\n<summary>ä¼ä¸šå¾®ä¿¡åº”ç”¨</summary>\n\nä¼ä¸šå¾®ä¿¡è‡ªå»ºåº”ç”¨æ¥å…¥éœ€åœ¨åå°åˆ›å»ºåº”ç”¨å¹¶å¯ç”¨æ¶ˆæ¯å›è°ƒï¼Œé…ç½®ç¤ºä¾‹ï¼š\n\n```json\n{\n    "channel_type": "wechatcom_app",\n    "wechatcom_corp_id": "CORPID",\n    "wechatcomapp_token": "TOKEN",\n    "wechatcomapp_port": 9898,\n    "wechatcomapp_secret": "SECRET",\n    "wechatcomapp_agent_id": "AGENTID",\n    "wechatcomapp_aes_key": "AESKEY"\n}\n```\nè¯¦ç»†æ­¥éª¤å’Œå‚æ•°è¯´æ˜å‚è€ƒ [ä¼å¾®è‡ªå»ºåº”ç”¨æ¥å…¥](https://docs.link-ai.tech/cow/multi-platform/wechat-com)\n\n</details>\n\n<details>\n<summary>é’‰é’‰</summary>\n\né’‰é’‰éœ€è¦åœ¨å¼€æ”¾å¹³å°åˆ›å»ºæ™ºèƒ½æœºå™¨äººåº”ç”¨ï¼Œå°†ä»¥ä¸‹é…ç½®å¡«å…¥ `config.json`ï¼š\n\n```json\n{\n    "channel_type": "dingtalk",\n    "dingtalk_client_id": "CLIENT_ID",\n    "dingtalk_client_secret": "CLIENT_SECRET"\n}\n```\nè¯¦ç»†æ­¥éª¤å’Œå‚æ•°è¯´æ˜å‚è€ƒ [é’‰é’‰æ¥å…¥](https://docs.link-ai.tech/cow/multi-platform/dingtalk)\n</details>\n\n<details>\n<summary>é£ä¹¦</summary>\n\né€šè¿‡è‡ªå»ºåº”ç”¨æ¥å…¥AIç›¸å…³èƒ½åŠ›åˆ°é£ä¹¦åº”ç”¨ä¸­ï¼Œé»˜è®¤å·²æ˜¯é£ä¹¦çš„ä¼ä¸šç”¨æˆ·ï¼Œä¸”å…·æœ‰ä¼ä¸šç®¡ç†æƒé™ï¼Œå°†ä»¥ä¸‹é…ç½®å¡«å…¥ `config.json`ï¼šï¼š\n\n```json\n{\n    "channel_type": "feishu",\n    "feishu_app_id": "APP_ID",\n    "feishu_app_secret": "APP_SECRET",\n    "feishu_token": "VERIFICATION_TOKEN",\n    "feishu_port": 80\n}\n```\nè¯¦ç»†æ­¥éª¤å’Œå‚æ•°è¯´æ˜å‚è€ƒ [é£ä¹¦æ¥å…¥](https://docs.link-ai.tech/cow/multi-platform/feishu)\n</details>\n\n<br/>\n\n# ğŸ”— ç›¸å…³é¡¹ç›®\n\n- [bot-on-anything](https://github.com/zhayujie/bot-on-anything)ï¼šè½»é‡å’Œé«˜å¯æ‰©å±•çš„å¤§æ¨¡å‹åº”ç”¨æ¡†æ¶ï¼Œæ”¯æŒæ¥å…¥Slack, Telegram, Discord, Gmailç­‰æµ·å¤–å¹³å°ï¼Œå¯ä½œä¸ºæœ¬é¡¹ç›®çš„è¡¥å……ä½¿ç”¨ã€‚\n- [AgentMesh](https://github.com/MinimalFuture/AgentMesh)ï¼šå¼€æºçš„å¤šæ™ºèƒ½ä½“(Multi-Agent)æ¡†æ¶ï¼Œå¯ä»¥é€šè¿‡å¤šæ™ºèƒ½ä½“å›¢é˜Ÿçš„ååŒæ¥è§£å†³å¤æ‚é—®é¢˜ã€‚æœ¬é¡¹ç›®åŸºäºè¯¥æ¡†æ¶å®ç°äº†[Agentæ’ä»¶](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/plugins/agent/README.md)ï¼Œå¯è®¿é—®ç»ˆç«¯ã€æµè§ˆå™¨ã€æ–‡ä»¶ç³»ç»Ÿã€æœç´¢å¼•æ“ ç­‰å„ç±»å·¥å…·ï¼Œå¹¶å®ç°äº†å¤šæ™ºèƒ½ä½“ååŒã€‚\n\n\n\n# ğŸ” å¸¸è§é—®é¢˜\n\nFAQsï¼š <https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs>\n\næˆ–ç›´æ¥åœ¨çº¿å’¨è¯¢ [é¡¹ç›®å°åŠ©æ‰‹](https://link-ai.tech/app/Kv2fXJcH)  (çŸ¥è¯†åº“æŒç»­å®Œå–„ä¸­ï¼Œå›å¤ä¾›å‚è€ƒ)\n\n# ğŸ› ï¸ å¼€å‘\n\næ¬¢è¿æ¥å…¥æ›´å¤šåº”ç”¨é€šé“ï¼Œå‚è€ƒ [Terminalä»£ç ](https://github.com/zhayujie/chatgpt-on-wechat/blob/master/channel/terminal/terminal_channel.py) æ–°å¢è‡ªå®šä¹‰é€šé“ï¼Œå®ç°æ¥æ”¶å’Œå‘é€æ¶ˆæ¯é€»è¾‘å³å¯å®Œæˆæ¥å…¥ã€‚ åŒæ—¶æ¬¢è¿è´¡çŒ®æ–°çš„æ’ä»¶ï¼Œå‚è€ƒ [æ’ä»¶å¼€å‘æ–‡æ¡£](https://github.com/zhayujie/chatgpt-on-wechat/tree/master/plugins)ã€‚\n\n# âœ‰ è”ç³»\n\næ¬¢è¿æäº¤PRã€Issuesè¿›è¡Œåé¦ˆï¼Œä»¥åŠé€šè¿‡ ğŸŒŸStar æ”¯æŒå¹¶å…³æ³¨é¡¹ç›®æ›´æ–°ã€‚é¡¹ç›®è¿è¡Œé‡åˆ°é—®é¢˜å¯ä»¥æŸ¥çœ‹ [å¸¸è§é—®é¢˜åˆ—è¡¨](https://github.com/zhayujie/chatgpt-on-wechat/wiki/FAQs) ï¼Œä»¥åŠå‰å¾€ [Issues](https://github.com/zhayujie/chatgpt-on-wechat/issues) ä¸­æœç´¢ã€‚ä¸ªäººå¼€å‘è€…å¯åŠ å…¥å¼€æºäº¤æµç¾¤å‚ä¸æ›´å¤šè®¨è®ºï¼Œä¼ä¸šç”¨æˆ·å¯è”ç³»[äº§å“å®¢æœ](https://cdn.link-ai.tech/portal/linkai-customer-service.png)å’¨è¯¢ã€‚\n\n# ğŸŒŸ è´¡çŒ®è€…\n\n![cow contributors](https://contrib.rocks/image?repo=zhayujie/chatgpt-on-wechat&max=1000)\n', '{"language":"Python","stars":40000,"forks":9537,"watchers":40000,"open_issues":355,"topics":["ai","ai-agent","chatgpt","claude-4","deepseek","dingtalk","feishu-bot","gemini","gpt-4","kimi","linkai","llm","mcp","multi-agent","openai","python3","qwen","rag","wechat","wechat-bot"],"default_branch":"master","size_kb":4475,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat\"><img","source_url":"https://github.com/zhayujie/chatgpt-on-wechat\"><img"},{"type":"has_code","target_id":"github:MinimalFuture:AgentMesh","source_url":"https://github.com/MinimalFuture/AgentMesh"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:bot-on-anything","source_url":"https://github.com/zhayujie/bot-on-anything"},{"type":"has_code","target_id":"github:MinimalFuture:AgentMesh","source_url":"https://github.com/MinimalFuture/AgentMesh"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"},{"type":"has_code","target_id":"github:zhayujie:chatgpt-on-wechat","source_url":"https://github.com/zhayujie/chatgpt-on-wechat"}]', NULL, 'MIT', 'approved', 80, 'ff336a2175a29f29f9a3144e711f4bbf', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-zhayujie-chatgpt-on-wechat from https://github.com/zhayujie.png
Image converted to WebP: data/images/github-zhayujie-chatgpt-on-wechat.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-mudler-LocalAI', 'github--mudler--localai', 'LocalAI', 'mudler', '<h1 align="center"> <br> <img width="300" src="./core/http/static/logo.png"> <br> <br> </h1> <p align="center"> <a href="https://github.com/go-skynet/LocalAI/fork" target="blank"> <img src="https://img.shields.io/github/forks/go-skynet/LocalAI?style=for-the-badge" alt="LocalAI forks"/> </a> <a href="https://github.com/go-skynet/LocalAI/stargazers" target="blank"> <img src="https://img.shields.io/github/stars/go-skynet/LocalAI?style=for-the-badge" alt="LocalAI stars"/> </a> <a href="https://gi...', '["ai","api","audio-generation","decentralized","distributed","gemma","image-generation","libp2p","llama","llm","mamba","mcp","mistral","musicgen","object-detection","rerank","rwkv","stable-diffusion","text-generation","tts","go"]', 'other', 39949, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/mudler/LocalAI","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<h1 align="center">\n  <br>\n  <img width="300" src="./core/http/static/logo.png"> <br>\n<br>\n</h1>\n\n<p align="center">\n<a href="https://github.com/go-skynet/LocalAI/fork" target="blank">\n<img src="https://img.shields.io/github/forks/go-skynet/LocalAI?style=for-the-badge" alt="LocalAI forks"/>\n</a>\n<a href="https://github.com/go-skynet/LocalAI/stargazers" target="blank">\n<img src="https://img.shields.io/github/stars/go-skynet/LocalAI?style=for-the-badge" alt="LocalAI stars"/>\n</a>\n<a href="https://github.com/go-skynet/LocalAI/pulls" target="blank">\n<img src="https://img.shields.io/github/issues-pr/go-skynet/LocalAI?style=for-the-badge" alt="LocalAI pull-requests"/>\n</a>\n<a href=''https://github.com/go-skynet/LocalAI/releases''>\n<img src=''https://img.shields.io/github/release/go-skynet/LocalAI?&label=Latest&style=for-the-badge''>\n</a>\n</p>\n\n<p align="center">\n<a href="https://hub.docker.com/r/localai/localai" target="blank">\n<img src="https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker" alt="LocalAI Docker hub"/>\n</a>\n<a href="https://quay.io/repository/go-skynet/local-ai?tab=tags&tag=latest" target="blank">\n<img src="https://img.shields.io/badge/quay.io-images-important.svg?" alt="LocalAI Quay.io"/>\n</a>\n</p>\n\n<p align="center">\n<a href="https://twitter.com/LocalAI_API" target="blank">\n<img src="https://img.shields.io/badge/X-%23000000.svg?style=for-the-badge&logo=X&logoColor=white&label=LocalAI_API" alt="Follow LocalAI_API"/>\n</a>\n<a href="https://discord.gg/uJAeKSAGDy" target="blank">\n<img src="https://dcbadge.vercel.app/api/server/uJAeKSAGDy?style=flat-square&theme=default-inverted" alt="Join LocalAI Discord Community"/>\n</a>\n</p>\n\n<p align="center">\n<a href="https://trendshift.io/repositories/5539" target="_blank"><img src="https://trendshift.io/api/badge/repositories/5539" alt="mudler%2FLocalAI | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n</p>\n\n> :bulb: Get help - [â“FAQ](https://localai.io/faq/) [ğŸ’­Discussions](https://github.com/go-skynet/LocalAI/discussions) [:speech_balloon: Discord](https://discord.gg/uJAeKSAGDy) [:book: Documentation website](https://localai.io/)\n>\n> [ğŸ’» Quickstart](https://localai.io/basics/getting_started/) [ğŸ–¼ï¸ Models](https://models.localai.io/) [ğŸš€ Roadmap](https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap) [ğŸ›« Examples](https://github.com/mudler/LocalAI-examples) Try on \n[![Telegram](https://img.shields.io/badge/Telegram-2CA5E0?style=for-the-badge&logo=telegram&logoColor=white)](https://t.me/localaiofficial_bot)\n\n[![tests](https://github.com/go-skynet/LocalAI/actions/workflows/test.yml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/test.yml)[![Build and Release](https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/release.yaml)[![build container images](https://github.com/go-skynet/LocalAI/actions/workflows/image.yml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/image.yml)[![Bump dependencies](https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml/badge.svg)](https://github.com/go-skynet/LocalAI/actions/workflows/bump_deps.yaml)[![Artifact Hub](https://img.shields.io/endpoint?url=https://artifacthub.io/badge/repository/localai)](https://artifacthub.io/packages/search?repo=localai)\n\n**LocalAI** is the free, Open Source OpenAI alternative. LocalAI act as a drop-in replacement REST API that''s compatible with OpenAI (Elevenlabs, Anthropic... ) API specifications for local AI inferencing. It allows you to run LLMs, generate images, audio (and not only) locally or on-prem with consumer grade hardware, supporting multiple model families. Does not require GPU. It is created and maintained by [Ettore Di Giacinto](https://github.com/mudler).\n\n\n## ğŸ“šğŸ†• Local Stack Family\n\nğŸ†• LocalAI is now part of a comprehensive suite of AI tools designed to work together:\n\n<table>\n  <tr>\n    <td width="50%" valign="top">\n      <a href="https://github.com/mudler/LocalAGI">\n        <img src="https://raw.githubusercontent.com/mudler/LocalAGI/refs/heads/main/webui/react-ui/public/logo_2.png" width="300" alt="LocalAGI Logo">\n      </a>\n    </td>\n    <td width="50%" valign="top">\n      <h3><a href="https://github.com/mudler/LocalAGI">LocalAGI</a></h3>\n      <p>A powerful Local AI agent management platform that serves as a drop-in replacement for OpenAI''s Responses API, enhanced with advanced agentic capabilities.</p>\n    </td>\n  </tr>\n  <tr>\n    <td width="50%" valign="top">\n      <a href="https://github.com/mudler/LocalRecall">\n        <img src="https://raw.githubusercontent.com/mudler/LocalRecall/refs/heads/main/static/localrecall_horizontal.png" width="300" alt="LocalRecall Logo">\n      </a>\n    </td>\n    <td width="50%" valign="top">\n      <h3><a href="https://github.com/mudler/LocalRecall">LocalRecall</a></h3>\n      <p>A REST-ful API and knowledge base management system that provides persistent memory and storage capabilities for AI agents.</p>\n    </td>\n  </tr>\n</table>\n\n## Screenshots\n\n\n| Talk Interface | Generate Audio |\n| --- | --- |\n| ![Screenshot 2025-03-31 at 12-01-36 LocalAI - Talk](./docs/assets/images/screenshots/screenshot_tts.png) | ![Screenshot 2025-03-31 at 12-01-29 LocalAI - Generate audio with voice-en-us-ryan-low](./docs/assets/images/screenshots/screenshot_tts.png) |\n\n| Models Overview | Generate Images |\n| --- | --- |\n| ![Screenshot 2025-03-31 at 12-01-20 LocalAI - Models](./docs/assets/images/screenshots/screenshot_gallery.png) | ![Screenshot 2025-03-31 at 12-31-41 LocalAI - Generate images with flux 1-dev](./docs/assets/images/screenshots/screenshot_image.png) |\n\n| Chat Interface | Home |\n| --- | --- |\n| ![Screenshot 2025-03-31 at 11-57-44 LocalAI - Chat with localai-functioncall-qwen2 5-7b-v0 5](./docs/assets/images/screenshots/screenshot_chat.png) | ![Screenshot 2025-03-31 at 11-57-23 LocalAI API - c2a39e3 (c2a39e3639227cfd94ffffe9f5691239acc275a8)](./docs/assets/images/screenshots/screenshot_home.png) |\n\n| Login | Swarm |\n| --- | --- |\n|![Screenshot 2025-03-31 at 12-09-59 ](./docs/assets/images/screenshots/screenshot_login.png) | ![Screenshot 2025-03-31 at 12-10-39 LocalAI - P2P dashboard](./docs/assets/images/screenshots/screenshot_p2p.png) |\n\n## ğŸ’» Quickstart\n\nRun the installer script:\n\n```bash\n# Basic installation\ncurl https://localai.io/install.sh | sh\n```\n\nFor more installation options, see [Installer Options](https://localai.io/installation/).\n\n### macOS Download:\n\n<a href="https://github.com/mudler/LocalAI/releases/latest/download/LocalAI.dmg">\n  <img src="https://img.shields.io/badge/Download-macOS-blue?style=for-the-badge&logo=apple&logoColor=white" alt="Download LocalAI for macOS"/>\n</a>\n\n> Note: the DMGs are not signed by Apple as quarantined. See https://github.com/mudler/LocalAI/issues/6268 for a workaround, fix is tracked here: https://github.com/mudler/LocalAI/issues/6244\n\nOr run with docker:\n\n> **ğŸ’¡ Docker Run vs Docker Start**\n> \n> - `docker run` creates and starts a new container. If a container with the same name already exists, this command will fail.\n> - `docker start` starts an existing container that was previously created with `docker run`.\n> \n> If you''ve already run LocalAI before and want to start it again, use: `docker start -i local-ai`\n\n### CPU only image:\n\n```bash\ndocker run -ti --name local-ai -p 8080:8080 localai/localai:latest\n```\n\n### NVIDIA GPU Images:\n\n```bash\n# CUDA 12.0\ndocker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-12\n\n# CUDA 11.7\ndocker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-11\n\n# NVIDIA Jetson (L4T) ARM64\ndocker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-nvidia-l4t-arm64\n```\n\n### AMD GPU Images (ROCm):\n\n```bash\ndocker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-gpu-hipblas\n```\n\n### Intel GPU Images (oneAPI):\n\n```bash\ndocker run -ti --name local-ai -p 8080:8080 --device=/dev/dri/card1 --device=/dev/dri/renderD128 localai/localai:latest-gpu-intel\n```\n\n### Vulkan GPU Images:\n\n```bash\ndocker run -ti --name local-ai -p 8080:8080 localai/localai:latest-gpu-vulkan\n```\n\n### AIO Images (pre-downloaded models):\n\n```bash\n# CPU version\ndocker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-cpu\n\n# NVIDIA CUDA 12 version\ndocker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-12\n\n# NVIDIA CUDA 11 version\ndocker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-11\n\n# Intel GPU version\ndocker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-gpu-intel\n\n# AMD GPU version\ndocker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-aio-gpu-hipblas\n```\n\nFor more information about the AIO images and pre-downloaded models, see [Container Documentation](https://localai.io/basics/container/).\n\nTo load models:\n\n```bash\n# From the model gallery (see available models with `local-ai models list`, in the WebUI from the model tab, or visiting https://models.localai.io)\nlocal-ai run llama-3.2-1b-instruct:q4_k_m\n# Start LocalAI with the phi-2 model directly from huggingface\nlocal-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf\n# Install and run a model from the Ollama OCI registry\nlocal-ai run ollama://gemma:2b\n# Run a model from a configuration file\nlocal-ai run https://gist.githubusercontent.com/.../phi-2.yaml\n# Install and run a model from a standard OCI registry (e.g., Docker Hub)\nlocal-ai run oci://localai/phi-2:latest\n```\n\n> âš¡ **Automatic Backend Detection**: When you install models from the gallery or YAML files, LocalAI automatically detects your system''s GPU capabilities (NVIDIA, AMD, Intel) and downloads the appropriate backend. For advanced configuration options, see [GPU Acceleration](https://localai.io/features/gpu-acceleration/#automatic-backend-detection).\n\nFor more information, see [ğŸ’» Getting started](https://localai.io/basics/getting_started/index.html), if you are interested in our roadmap items and future enhancements, you can see the [Issues labeled as Roadmap here](https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap)\n\n## ğŸ“° Latest project news\n\n- November 2025: Major improvements to the UX. Among these: [Import models via URL](https://github.com/mudler/LocalAI/pull/7245) and [Multiple chats and history](https://github.com/mudler/LocalAI/pull/7325)\n- October 2025: ğŸ”Œ [Model Context Protocol (MCP)](https://localai.io/docs/features/mcp/) support added for agentic capabilities with external tools\n- September 2025: New Launcher application for MacOS and Linux, extended support to many backends for Mac and Nvidia L4T devices. Models: Added MLX-Audio, WAN 2.2. WebUI improvements and Python-based backends now ships portable python environments.\n- August 2025: MLX, MLX-VLM, Diffusers and llama.cpp are now supported on Mac M1/M2/M3+ chips ( with `development` suffix in the gallery ): https://github.com/mudler/LocalAI/pull/6049 https://github.com/mudler/LocalAI/pull/6119 https://github.com/mudler/LocalAI/pull/6121 https://github.com/mudler/LocalAI/pull/6060\n- July/August 2025: ğŸ” [Object Detection](https://localai.io/features/object-detection/) added to the API featuring [rf-detr](https://github.com/roboflow/rf-detr)\n- July 2025: All backends migrated outside of the main binary. LocalAI is now more lightweight, small, and automatically downloads the required backend to run the model. [Read the release notes](https://github.com/mudler/LocalAI/releases/tag/v3.2.0)\n- June 2025: [Backend management](https://github.com/mudler/LocalAI/pull/5607) has been added. Attention: extras images are going to be deprecated from the next release! Read [the backend management PR](https://github.com/mudler/LocalAI/pull/5607).\n- May 2025: [Audio input](https://github.com/mudler/LocalAI/pull/5466) and [Reranking](https://github.com/mudler/LocalAI/pull/5396) in llama.cpp backend, [Realtime API](https://github.com/mudler/LocalAI/pull/5392),  Support to Gemma, SmollVLM, and more multimodal models (available in the gallery).\n- May 2025: Important: image name changes [See release](https://github.com/mudler/LocalAI/releases/tag/v2.29.0)\n- Apr 2025: Rebrand, WebUI enhancements\n- Apr 2025: [LocalAGI](https://github.com/mudler/LocalAGI) and [LocalRecall](https://github.com/mudler/LocalRecall) join the LocalAI family stack.\n- Apr 2025: WebUI overhaul, AIO images updates\n- Feb 2025: Backend cleanup, Breaking changes, new backends (kokoro, OutelTTS, faster-whisper), Nvidia L4T images\n- Jan 2025: LocalAI model release: https://huggingface.co/mudler/LocalAI-functioncall-phi-4-v0.3, SANA support in diffusers: https://github.com/mudler/LocalAI/pull/4603\n- Dec 2024: stablediffusion.cpp backend (ggml) added ( https://github.com/mudler/LocalAI/pull/4289 )\n- Nov 2024: Bark.cpp backend added ( https://github.com/mudler/LocalAI/pull/4287 )\n- Nov 2024: Voice activity detection models (**VAD**) added to the API: https://github.com/mudler/LocalAI/pull/4204\n- Oct 2024: examples moved to [LocalAI-examples](https://github.com/mudler/LocalAI-examples)\n- Aug 2024:  ğŸ†• FLUX-1, [P2P Explorer](https://explorer.localai.io)\n- July 2024: ğŸ”¥ğŸ”¥ ğŸ†• P2P Dashboard, LocalAI Federated mode and AI Swarms: https://github.com/mudler/LocalAI/pull/2723. P2P Global community pools: https://github.com/mudler/LocalAI/issues/3113\n- May 2024: ğŸ”¥ğŸ”¥ Decentralized P2P llama.cpp:  https://github.com/mudler/LocalAI/pull/2343 (peer2peer llama.cpp!) ğŸ‘‰ Docs  https://localai.io/features/distribute/\n- May 2024: ğŸ”¥ğŸ”¥ Distributed inferencing: https://github.com/mudler/LocalAI/pull/2324\n- April 2024: Reranker API: https://github.com/mudler/LocalAI/pull/2121\n\nRoadmap items: [List of issues](https://github.com/mudler/LocalAI/issues?q=is%3Aissue+is%3Aopen+label%3Aroadmap)\n\n## ğŸš€ [Features](https://localai.io/features/)\n\n- ğŸ§© [Backend Gallery](https://localai.io/backends/): Install/remove backends on the fly, powered by OCI images â€” fully customizable and API-driven.\n- ğŸ“– [Text generation with GPTs](https://localai.io/features/text-generation/) (`llama.cpp`, `transformers`, `vllm` ... [:book: and more](https://localai.io/model-compatibility/index.html#model-compatibility-table))\n- ğŸ—£ [Text to Audio](https://localai.io/features/text-to-audio/)\n- ğŸ”ˆ [Audio to Text](https://localai.io/features/audio-to-text/) (Audio transcription with `whisper.cpp`)\n- ğŸ¨ [Image generation](https://localai.io/features/image-generation)\n- ğŸ”¥ [OpenAI-alike tools API](https://localai.io/features/openai-functions/) \n- ğŸ§  [Embeddings generation for vector databases](https://localai.io/features/embeddings/)\n- âœï¸ [Constrained grammars](https://localai.io/features/constrained_grammars/)\n- ğŸ–¼ï¸ [Download Models directly from Huggingface ](https://localai.io/models/)\n- ğŸ¥½ [Vision API](https://localai.io/features/gpt-vision/)\n- ğŸ” [Object Detection](https://localai.io/features/object-detection/)\n- ğŸ“ˆ [Reranker API](https://localai.io/features/reranker/)\n- ğŸ†•ğŸ–§ [P2P Inferencing](https://localai.io/features/distribute/)\n- ğŸ†•ğŸ”Œ [Model Context Protocol (MCP)](https://localai.io/docs/features/mcp/) - Agentic capabilities with external tools and [LocalAGI''s Agentic capabilities](https://github.com/mudler/LocalAGI)\n- ğŸ”Š Voice activity detection (Silero-VAD support)\n- ğŸŒ Integrated WebUI!\n\n## ğŸ§© Supported Backends & Acceleration\n\nLocalAI supports a comprehensive range of AI backends with multiple acceleration options:\n\n### Text Generation & Language Models\n| Backend | Description | Acceleration Support |\n|---------|-------------|---------------------|\n| **llama.cpp** | LLM inference in C/C++ | CUDA 11/12, ROCm, Intel SYCL, Vulkan, Metal, CPU |\n| **vLLM** | Fast LLM inference with PagedAttention | CUDA 12, ROCm, Intel |\n| **transformers** | HuggingFace transformers framework | CUDA 11/12, ROCm, Intel, CPU |\n| **exllama2** | GPTQ inference library | CUDA 12 |\n| **MLX** | Apple Silicon LLM inference | Metal (M1/M2/M3+) |\n| **MLX-VLM** | Apple Silicon Vision-Language Models | Metal (M1/M2/M3+) |\n\n### Audio & Speech Processing\n| Backend | Description | Acceleration Support |\n|---------|-------------|---------------------|\n| **whisper.cpp** | OpenAI Whisper in C/C++ | CUDA 12, ROCm, Intel SYCL, Vulkan, CPU |\n| **faster-whisper** | Fast Whisper with CTranslate2 | CUDA 12, ROCm, Intel, CPU |\n| **bark** | Text-to-audio generation | CUDA 12, ROCm, Intel |\n| **bark-cpp** | C++ implementation of Bark | CUDA, Metal, CPU |\n| **coqui** | Advanced TTS with 1100+ languages | CUDA 12, ROCm, Intel, CPU |\n| **kokoro** | Lightweight TTS model | CUDA 12, ROCm, Intel, CPU |\n| **chatterbox** | Production-grade TTS | CUDA 11/12, CPU |\n| **piper** | Fast neural TTS system | CPU |\n| **kitten-tts** | Kitten TTS models | CPU |\n| **silero-vad** | Voice Activity Detection | CPU |\n| **neutts** | Text-to-speech with voice cloning | CUDA 12, ROCm, CPU |\n\n### Image & Video Generation\n| Backend | Description | Acceleration Support |\n|---------|-------------|---------------------|\n| **stablediffusion.cpp** | Stable Diffusion in C/C++ | CUDA 12, Intel SYCL, Vulkan, CPU |\n| **diffusers** | HuggingFace diffusion models | CUDA 11/12, ROCm, Intel, Metal, CPU |\n\n### Specialized AI Tasks\n| Backend | Description | Acceleration Support |\n|---------|-------------|---------------------|\n| **rfdetr** | Real-time object detection | CUDA 12, Intel, CPU |\n| **rerankers** | Document reranking API | CUDA 11/12, ROCm, Intel, CPU |\n| **local-store** | Vector database | CPU |\n| **huggingface** | HuggingFace API integration | API-based |\n\n### Hardware Acceleration Matrix\n\n| Acceleration Type | Supported Backends | Hardware Support |\n|-------------------|-------------------|------------------|\n| **NVIDIA CUDA 11** | llama.cpp, whisper, stablediffusion, diffusers, rerankers, bark, chatterbox | Nvidia hardware |\n| **NVIDIA CUDA 12** | All CUDA-compatible backends | Nvidia hardware |\n| **AMD ROCm** | llama.cpp, whisper, vllm, transformers, diffusers, rerankers, coqui, kokoro, bark, neutts | AMD Graphics |\n| **Intel oneAPI** | llama.cpp, whisper, stablediffusion, vllm, transformers, diffusers, rfdetr, rerankers, exllama2, coqui, kokoro, bark | Intel Arc, Intel iGPUs |\n| **Apple Metal** | llama.cpp, whisper, diffusers, MLX, MLX-VLM, bark-cpp | Apple M1/M2/M3+ |\n| **Vulkan** | llama.cpp, whisper, stablediffusion | Cross-platform GPUs |\n| **NVIDIA Jetson** | llama.cpp, whisper, stablediffusion, diffusers, rfdetr | ARM64 embedded AI |\n| **CPU Optimized** | All backends | AVX/AVX2/AVX512, quantization support |\n\n### ğŸ”— Community and integrations\n\nBuild and deploy custom containers:\n- https://github.com/sozercan/aikit\n\nWebUIs:\n- https://github.com/Jirubizu/localai-admin\n- https://github.com/go-skynet/LocalAI-frontend\n- QA-Pilot(An interactive chat project that leverages LocalAI LLMs for rapid understanding and navigation of GitHub code repository) https://github.com/reid41/QA-Pilot\n\nAgentic Libraries:\n- https://github.com/mudler/cogito\n\nMCPs:\n- https://github.com/mudler/MCPs\n\nModel galleries\n- https://github.com/go-skynet/model-gallery\n\nVoice:\n- https://github.com/richiejp/VoxInput\n\nOther:\n- Helm chart https://github.com/go-skynet/helm-charts\n- VSCode extension https://github.com/badgooooor/localai-vscode-plugin\n- Langchain: https://python.langchain.com/docs/integrations/providers/localai/\n- Terminal utility https://github.com/djcopley/ShellOracle\n- Local Smart assistant https://github.com/mudler/LocalAGI\n- Home Assistant https://github.com/sammcj/homeassistant-localai / https://github.com/drndos/hass-openai-custom-conversation / https://github.com/valentinfrlch/ha-gpt4vision\n- Discord bot https://github.com/mudler/LocalAGI/tree/main/examples/discord\n- Slack bot https://github.com/mudler/LocalAGI/tree/main/examples/slack\n- Shell-Pilot(Interact with LLM using LocalAI models via pure shell scripts on your Linux or MacOS system) https://github.com/reid41/shell-pilot\n- Telegram bot https://github.com/mudler/LocalAI/tree/master/examples/telegram-bot\n- Another Telegram Bot https://github.com/JackBekket/Hellper\n- Auto-documentation https://github.com/JackBekket/Reflexia\n- Github bot which answer on issues, with code and documentation as context https://github.com/JackBekket/GitHelper\n- Github Actions: https://github.com/marketplace/actions/start-localai\n- Examples: https://github.com/mudler/LocalAI/tree/master/examples/\n  \n\n### ğŸ”— Resources\n\n- [LLM finetuning guide](https://localai.io/docs/advanced/fine-tuning/)\n- [How to build locally](https://localai.io/basics/build/index.html)\n- [How to install in Kubernetes](https://localai.io/basics/getting_started/index.html#run-localai-in-kubernetes)\n- [Projects integrating LocalAI](https://localai.io/docs/integrations/)\n- [How tos section](https://io.midori-ai.xyz/howtos/) (curated by our community)\n\n## :book: ğŸ¥ [Media, Blogs, Social](https://localai.io/basics/news/#media-blogs-social)\n\n- [Run Visual studio code with LocalAI (SUSE)](https://www.suse.com/c/running-ai-locally/)\n- ğŸ†• [Run LocalAI on Jetson Nano Devkit](https://mudler.pm/posts/local-ai-jetson-nano-devkit/)\n- [Run LocalAI on AWS EKS with Pulumi](https://www.pulumi.com/blog/low-code-llm-apps-with-local-ai-flowise-and-pulumi/)\n- [Run LocalAI on AWS](https://staleks.hashnode.dev/installing-localai-on-aws-ec2-instance)\n- [Create a slackbot for teams and OSS projects that answer to documentation](https://mudler.pm/posts/smart-slackbot-for-teams/)\n- [LocalAI meets k8sgpt](https://www.youtube.com/watch?v=PKrDNuJ_dfE)\n- [Question Answering on Documents locally with LangChain, LocalAI, Chroma, and GPT4All](https://mudler.pm/posts/localai-question-answering/)\n- [Tutorial to use k8sgpt with LocalAI](https://medium.com/@tyler_97636/k8sgpt-localai-unlock-kubernetes-superpowers-for-free-584790de9b65)\n\n## Citation\n\nIf you utilize this repository, data in a downstream project, please consider citing it with:\n\n```\n@misc{localai,\n  author = {Ettore Di Giacinto},\n  title = {LocalAI: The free, Open source OpenAI alternative},\n  year = {2023},\n  publisher = {GitHub},\n  journal = {GitHub repository},\n  howpublished = {\url{https://github.com/go-skynet/LocalAI}},\n```\n\n## â¤ï¸ Sponsors\n\n> Do you find LocalAI useful?\n\nSupport the project by becoming [a backer or sponsor](https://github.com/sponsors/mudler). Your logo will show up here with a link to your website.\n\nA huge thank you to our generous sponsors who support this project covering CI expenses, and our [Sponsor list](https://github.com/sponsors/mudler):\n\n<p align="center">\n  <a href="https://www.spectrocloud.com/" target="blank">\n    <img height="200" src="https://github.com/user-attachments/assets/72eab1dd-8b93-4fc0-9ade-84db49f24962">\n  </a>\n  <a href="https://www.premai.io/" target="blank">\n    <img height="200" src="https://github.com/mudler/LocalAI/assets/2420543/42e4ca83-661e-4f79-8e46-ae43689683d6"> <br>\n  </a>\n</p>\n\n## ğŸŒŸ Star history\n\n[![LocalAI Star history Chart](https://api.star-history.com/svg?repos=go-skynet/LocalAI&type=Date)](https://star-history.com/#go-skynet/LocalAI&Date)\n\n## ğŸ“– License\n\nLocalAI is a community-driven project created by [Ettore Di Giacinto](https://github.com/mudler/).\n\nMIT - Author Ettore Di Giacinto <mudler@localai.io>\n\n## ğŸ™‡ Acknowledgements\n\nLocalAI couldn''t have been built without the help of great software already available from the community. Thank you!\n\n- [llama.cpp](https://github.com/ggerganov/llama.cpp)\n- https://github.com/tatsu-lab/stanford_alpaca\n- https://github.com/cornelk/llama-go for the initial ideas\n- https://github.com/antimatter15/alpaca.cpp\n- https://github.com/EdVince/Stable-Diffusion-NCNN\n- https://github.com/ggerganov/whisper.cpp\n- https://github.com/rhasspy/piper\n\n## ğŸ¤— Contributors\n\nThis is a community project, a special thanks to our contributors! ğŸ¤—\n<a href="https://github.com/go-skynet/LocalAI/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=go-skynet/LocalAI" />\n</a>\n', '{"language":"Go","stars":39949,"forks":3190,"watchers":39949,"open_issues":226,"topics":["ai","api","audio-generation","decentralized","distributed","gemma","image-generation","libp2p","llama","llm","mamba","mcp","mistral","musicgen","object-detection","rerank","rwkv","stable-diffusion","text-generation","tts"],"default_branch":"master","size_kb":40383,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:go-skynet:LocalAI","source_url":"https://github.com/go-skynet/LocalAI"},{"type":"has_code","target_id":"github:go-skynet:LocalAI","source_url":"https://github.com/go-skynet/LocalAI"},{"type":"has_code","target_id":"github:go-skynet:LocalAI","source_url":"https://github.com/go-skynet/LocalAI"},{"type":"has_code","target_id":"github:go-skynet:LocalAI","source_url":"https://github.com/go-skynet/LocalAI"},{"type":"has_code","target_id":"github:go-skynet:LocalAI","source_url":"https://github.com/go-skynet/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI-examples","source_url":"https://github.com/mudler/LocalAI-examples"},{"type":"has_code","target_id":"github:go-skynet:LocalAI","source_url":"https://github.com/go-skynet/LocalAI"},{"type":"has_code","target_id":"github:go-skynet:LocalAI","source_url":"https://github.com/go-skynet/LocalAI"},{"type":"has_code","target_id":"github:go-skynet:LocalAI","source_url":"https://github.com/go-skynet/LocalAI"},{"type":"has_code","target_id":"github:go-skynet:LocalAI","source_url":"https://github.com/go-skynet/LocalAI"},{"type":"has_code","target_id":"github:go-skynet:LocalAI","source_url":"https://github.com/go-skynet/LocalAI"},{"type":"has_code","target_id":"github:go-skynet:LocalAI","source_url":"https://github.com/go-skynet/LocalAI"},{"type":"has_code","target_id":"github:go-skynet:LocalAI","source_url":"https://github.com/go-skynet/LocalAI"},{"type":"has_code","target_id":"github:go-skynet:LocalAI","source_url":"https://github.com/go-skynet/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAGI\">","source_url":"https://github.com/mudler/LocalAGI\">"},{"type":"has_code","target_id":"github:mudler:LocalAGI\">LocalAGI<","source_url":"https://github.com/mudler/LocalAGI\">LocalAGI<"},{"type":"has_code","target_id":"github:mudler:LocalRecall\">","source_url":"https://github.com/mudler/LocalRecall\">"},{"type":"has_code","target_id":"github:mudler:LocalRecall\">LocalRecall<","source_url":"https://github.com/mudler/LocalRecall\">LocalRecall<"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:roboflow:rf-detr","source_url":"https://github.com/roboflow/rf-detr"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAGI","source_url":"https://github.com/mudler/LocalAGI"},{"type":"has_code","target_id":"github:mudler:LocalRecall","source_url":"https://github.com/mudler/LocalRecall"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI-examples","source_url":"https://github.com/mudler/LocalAI-examples"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:mudler:LocalAGI","source_url":"https://github.com/mudler/LocalAGI"},{"type":"has_code","target_id":"github:sozercan:aikit","source_url":"https://github.com/sozercan/aikit"},{"type":"has_code","target_id":"github:Jirubizu:localai-admin","source_url":"https://github.com/Jirubizu/localai-admin"},{"type":"has_code","target_id":"github:go-skynet:LocalAI-frontend","source_url":"https://github.com/go-skynet/LocalAI-frontend"},{"type":"has_code","target_id":"github:reid41:QA-Pilot","source_url":"https://github.com/reid41/QA-Pilot"},{"type":"has_code","target_id":"github:mudler:cogito","source_url":"https://github.com/mudler/cogito"},{"type":"has_code","target_id":"github:mudler:MCPs","source_url":"https://github.com/mudler/MCPs"},{"type":"has_code","target_id":"github:go-skynet:model-gallery","source_url":"https://github.com/go-skynet/model-gallery"},{"type":"has_code","target_id":"github:richiejp:VoxInput","source_url":"https://github.com/richiejp/VoxInput"},{"type":"has_code","target_id":"github:go-skynet:helm-charts","source_url":"https://github.com/go-skynet/helm-charts"},{"type":"has_code","target_id":"github:badgooooor:localai-vscode-plugin","source_url":"https://github.com/badgooooor/localai-vscode-plugin"},{"type":"has_code","target_id":"github:djcopley:ShellOracle","source_url":"https://github.com/djcopley/ShellOracle"},{"type":"has_code","target_id":"github:mudler:LocalAGI","source_url":"https://github.com/mudler/LocalAGI"},{"type":"has_code","target_id":"github:sammcj:homeassistant-localai","source_url":"https://github.com/sammcj/homeassistant-localai"},{"type":"has_code","target_id":"github:drndos:hass-openai-custom-conversation","source_url":"https://github.com/drndos/hass-openai-custom-conversation"},{"type":"has_code","target_id":"github:valentinfrlch:ha-gpt4vision","source_url":"https://github.com/valentinfrlch/ha-gpt4vision"},{"type":"has_code","target_id":"github:mudler:LocalAGI","source_url":"https://github.com/mudler/LocalAGI"},{"type":"has_code","target_id":"github:mudler:LocalAGI","source_url":"https://github.com/mudler/LocalAGI"},{"type":"has_code","target_id":"github:reid41:shell-pilot","source_url":"https://github.com/reid41/shell-pilot"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:JackBekket:Hellper","source_url":"https://github.com/JackBekket/Hellper"},{"type":"has_code","target_id":"github:JackBekket:Reflexia","source_url":"https://github.com/JackBekket/Reflexia"},{"type":"has_code","target_id":"github:JackBekket:GitHelper","source_url":"https://github.com/JackBekket/GitHelper"},{"type":"has_code","target_id":"github:marketplace:actions","source_url":"https://github.com/marketplace/actions"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:go-skynet:LocalAI}},","source_url":"https://github.com/go-skynet/LocalAI}},"},{"type":"has_code","target_id":"github:sponsors:mudler","source_url":"https://github.com/sponsors/mudler"},{"type":"has_code","target_id":"github:sponsors:mudler","source_url":"https://github.com/sponsors/mudler"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:mudler:LocalAI","source_url":"https://github.com/mudler/LocalAI"},{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:tatsu-lab:stanford_alpaca","source_url":"https://github.com/tatsu-lab/stanford_alpaca"},{"type":"has_code","target_id":"github:cornelk:llama-go","source_url":"https://github.com/cornelk/llama-go"},{"type":"has_code","target_id":"github:antimatter15:alpaca.cpp","source_url":"https://github.com/antimatter15/alpaca.cpp"},{"type":"has_code","target_id":"github:EdVince:Stable-Diffusion-NCNN","source_url":"https://github.com/EdVince/Stable-Diffusion-NCNN"},{"type":"has_code","target_id":"github:ggerganov:whisper.cpp","source_url":"https://github.com/ggerganov/whisper.cpp"},{"type":"has_code","target_id":"github:rhasspy:piper","source_url":"https://github.com/rhasspy/piper"},{"type":"has_code","target_id":"github:go-skynet:LocalAI","source_url":"https://github.com/go-skynet/LocalAI"}]', NULL, 'MIT', 'approved', 80, 'daebe7d93d19b9d414ae2c8b5121fe9f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-mudler-LocalAI from https://github.com/mudler.png
Image converted to WebP: data/images/github-mudler-LocalAI.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-QuivrHQ-quivr', 'github--quivrhq--quivr', 'quivr', 'QuivrHQ', '<div align="center"> <img src="./logo.png" alt="Quivr-logo" width="31%" style="border-radius: 50%; padding-bottom: 20px"/> </div> Quivr, helps you build your second brain, utilizes the power of GenerativeAI to be your personal assistant ! - **Opiniated RAG**: We created a RAG that is opinionated, fast and efficient so you can focus on your product - **LLMs**: Quivr works with any LLM, you can use it with OpenAI, Anthropic, Mistral, Gemma, etc. - **Any File**: Quivr works with any file, you ca...', '["ai","api","chatbot","chatgpt","database","docker","framework","frontend","groq","html","javascript","llm","openai","postgresql","privacy","rag","react","security","typescript","vector","python"]', 'other', 38675, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/QuivrHQ/quivr","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Quivr - Your Second Brain, Empowered by Generative AI\n\n<div align="center">\n    <img src="./logo.png" alt="Quivr-logo" width="31%"  style="border-radius: 50%; padding-bottom: 20px"/>\n</div>\n\n[![Discord Follow](https://dcbadge.vercel.app/api/server/HUpRgp2HG8?style=flat)](https://discord.gg/HUpRgp2HG8)\n[![GitHub Repo stars](https://img.shields.io/github/stars/quivrhq/quivr?style=social)](https://github.com/quivrhq/quivr)\n[![Twitter Follow](https://img.shields.io/twitter/follow/StanGirard?style=social)](https://twitter.com/_StanGirard)\n\nQuivr, helps you build your second brain, utilizes the power of GenerativeAI to be your personal assistant !\n\n## Key Features ğŸ¯\n\n- **Opiniated RAG**: We created a RAG that is opinionated, fast and efficient so you can focus on your product\n- **LLMs**: Quivr works with any LLM, you can use it with OpenAI, Anthropic, Mistral, Gemma, etc.\n- **Any File**: Quivr works with any file, you can use it with PDF, TXT, Markdown, etc and even add your own parsers.\n- **Customize your RAG**: Quivr allows you to customize your RAG, add internet search, add tools, etc.\n- **Integrations with Megaparse**: Quivr works with [Megaparse](https://github.com/quivrhq/megaparse), so you can ingest your files with Megaparse and use the RAG with Quivr.\n\n>We take care of the RAG so you can focus on your product. Simply install quivr-core and add it to your project. You can now ingest your files and ask questions.*\n\n**We will be improving the RAG and adding more features, stay tuned!**\n\n\nThis is the core of Quivr, the brain of Quivr.com.\n\n<!-- ## Demo Highlight ğŸ¥\n\nhttps://github.com/quivrhq/quivr/assets/19614572/a6463b73-76c7-4bc0-978d-70562dca71f5 -->\n\n## Getting Started ğŸš€\n\nYou can find everything on the [documentation](https://core.quivr.com/).\n\n### Prerequisites ğŸ“‹\n\nEnsure you have the following installed:\n\n- Python 3.10 or newer\n\n### 30 seconds Installation ğŸ’½\n\n\n- **Step 1**: Install the package\n\n  \n\n  ```bash\n  pip install quivr-core # Check that the installation worked\n  ```\n\n\n- **Step 2**: Create a RAG with 5 lines of code\n\n  ```python\n  import tempfile\n\n  from quivr_core import Brain\n\n  if __name__ == "__main__":\n      with tempfile.NamedTemporaryFile(mode="w", suffix=".txt") as temp_file:\n          temp_file.write("Gold is a liquid of blue-like colour.")\n          temp_file.flush()\n\n          brain = Brain.from_files(\n              name="test_brain",\n              file_paths=[temp_file.name],\n          )\n\n          answer = brain.ask(\n              "what is gold? asnwer in french"\n          )\n          print("answer:", answer)\n  ```\n## Configuration\n\n### Workflows\n\n#### Basic RAG\n\n![](docs/docs/workflows/examples/basic_rag.excalidraw.png)\n\n\nCreating a basic RAG workflow like the one above is simple, here are the steps:\n\n\n1. Add your API Keys to your environment variables\n```python\nimport os\nos.environ["OPENAI_API_KEY"] = "myopenai_apikey"\n\n```\nQuivr supports APIs from Anthropic, OpenAI, and Mistral. It also supports local models using Ollama.\n\n1. Create the YAML file ``basic_rag_workflow.yaml`` and copy the following content in it\n```yaml\nworkflow_config:\n  name: "standard RAG"\n  nodes:\n    - name: "START"\n      edges: ["filter_history"]\n\n    - name: "filter_history"\n      edges: ["rewrite"]\n\n    - name: "rewrite"\n      edges: ["retrieve"]\n\n    - name: "retrieve"\n      edges: ["generate_rag"]\n\n    - name: "generate_rag" # the name of the last node, from which we want to stream the answer to the user\n      edges: ["END"]\n\n# Maximum number of previous conversation iterations\n# to include in the context of the answer\nmax_history: 10\n\n# Reranker configuration\nreranker_config:\n  # The reranker supplier to use\n  supplier: "cohere"\n\n  # The model to use for the reranker for the given supplier\n  model: "rerank-multilingual-v3.0"\n\n  # Number of chunks returned by the reranker\n  top_n: 5\n\n# Configuration for the LLM\nllm_config:\n\n  # maximum number of tokens passed to the LLM to generate the answer\n  max_input_tokens: 4000\n\n  # temperature for the LLM\n  temperature: 0.7\n```\n\n3. Create a Brain with the default configuration\n```python\nfrom quivr_core import Brain\n\nbrain = Brain.from_files(name = "my smart brain",\n                        file_paths = ["./my_first_doc.pdf", "./my_second_doc.txt"],\n                        )\n\n```\n\n4. Launch a Chat\n```python\nbrain.print_info()\n\nfrom rich.console import Console\nfrom rich.panel import Panel\nfrom rich.prompt import Prompt\nfrom quivr_core.config import RetrievalConfig\n\nconfig_file_name = "./basic_rag_workflow.yaml"\n\nretrieval_config = RetrievalConfig.from_yaml(config_file_name)\n\nconsole = Console()\nconsole.print(Panel.fit("Ask your brain !", style="bold magenta"))\n\nwhile True:\n    # Get user input\n    question = Prompt.ask("[bold cyan]Question[/bold cyan]")\n\n    # Check if user wants to exit\n    if question.lower() == "exit":\n        console.print(Panel("Goodbye!", style="bold yellow"))\n        break\n\n    answer = brain.ask(question, retrieval_config=retrieval_config)\n    # Print the answer with typing effect\n    console.print(f"[bold green]Quivr Assistant[/bold green]: {answer.answer}")\n\n    console.print("-" * console.width)\n\nbrain.print_info()\n```\n\n5. You are now all set up to talk with your brain and test different retrieval strategies by simply changing the configuration file!\n\n## Go further\n\nYou can go further with Quivr by adding internet search, adding tools, etc. Check the [documentation](https://core.quivr.com/) for more information.\n\n\n## Contributors âœ¨\n\nThanks go to these wonderful people:\n<a href="https://github.com/quivrhq/quivr/graphs/contributors">\n<img src="https://contrib.rocks/image?repo=quivrhq/quivr" />\n</a>\n\n## Contribute ğŸ¤\n\nDid you get a pull request? Open it, and we''ll review it as soon as possible. Check out our project board [here](https://github.com/users/StanGirard/projects/5) to see what we''re currently focused on, and feel free to bring your fresh ideas to the table!\n\n- [Open Issues](https://github.com/quivrhq/quivr/issues)\n- [Open Pull Requests](https://github.com/quivrhq/quivr/pulls)\n- [Good First Issues](https://github.com/quivrhq/quivr/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n\n## Partners â¤ï¸\n\nThis project would not be possible without the support of our partners. Thank you for your support!\n\n\n<a href="https://ycombinator.com/">\n    <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b2/Y_Combinator_logo.svg/1200px-Y_Combinator_logo.svg.png" alt="YCombinator" style="padding: 10px" width="70px">\n</a>\n<a href="https://www.theodo.fr/">\n  <img src="https://avatars.githubusercontent.com/u/332041?s=200&v=4" alt="Theodo" style="padding: 10px" width="70px">\n</a>\n\n## License ğŸ“„\n\nThis project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details\n', '{"language":"Python","stars":38675,"forks":3691,"watchers":38675,"open_issues":16,"topics":["ai","api","chatbot","chatgpt","database","docker","framework","frontend","groq","html","javascript","llm","openai","postgresql","privacy","rag","react","security","typescript","vector"],"default_branch":"main","size_kb":130685,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:quivrhq:quivr","source_url":"https://github.com/quivrhq/quivr"},{"type":"has_code","target_id":"github:quivrhq:megaparse","source_url":"https://github.com/quivrhq/megaparse"},{"type":"has_code","target_id":"github:quivrhq:quivr","source_url":"https://github.com/quivrhq/quivr"},{"type":"has_code","target_id":"github:quivrhq:quivr","source_url":"https://github.com/quivrhq/quivr"},{"type":"has_code","target_id":"github:users:StanGirard","source_url":"https://github.com/users/StanGirard"},{"type":"has_code","target_id":"github:quivrhq:quivr","source_url":"https://github.com/quivrhq/quivr"},{"type":"has_code","target_id":"github:quivrhq:quivr","source_url":"https://github.com/quivrhq/quivr"},{"type":"has_code","target_id":"github:quivrhq:quivr","source_url":"https://github.com/quivrhq/quivr"}]', NULL, 'NOASSERTION', 'approved', 65, '25f98e4869bbd485c33a17f32b3e36ea', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-QuivrHQ-quivr from https://github.com/QuivrHQ.png
Image converted to WebP: data/images/github-QuivrHQ-quivr.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-mindsdb-mindsdb', 'github--mindsdb--mindsdb', 'mindsdb', 'mindsdb', '<a name="readme-top"></a> <div align="center"> <a href="https://pypi.org/project/MindsDB/" target="_blank"><img src="https://badge.fury.io/py/MindsDB.svg" alt="MindsDB Release"></a> <a href="https://www.python.org/downloads/" target="_blank"><img src="https://img.shields.io/badge/python-3.10.x%7C%203.11.x%7C%203.12.x%7C%203.13.x-brightgreen.svg" alt="Python supported"></a> <a href="https://hub.docker.com/u/mindsdb" target="_blank"><img src="https://img.shields.io/docker/pulls/mindsdb/mindsdb"...', '["agents","ai","analytics","artificial-inteligence","bigquery","business-intelligence","databases","hacktoberfest","llms","mcp","mssql","mysql","postgresql","rag","python"]', 'other', 37470, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/mindsdb/mindsdb","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '\n\n<a name="readme-top"></a>\n\n<div align="center">\n	<a href="https://pypi.org/project/MindsDB/" target="_blank"><img src="https://badge.fury.io/py/MindsDB.svg" alt="MindsDB Release"></a>\n	<a href="https://www.python.org/downloads/" target="_blank"><img src="https://img.shields.io/badge/python-3.10.x%7C%203.11.x%7C%203.12.x%7C%203.13.x-brightgreen.svg" alt="Python supported"></a>\n	<a href="https://hub.docker.com/u/mindsdb" target="_blank"><img src="https://img.shields.io/docker/pulls/mindsdb/mindsdb" alt="Docker pulls"></a>\n\n  <br />\n  <br />\n\n  <a href="https://trendshift.io/repositories/3068" target="_blank"><img src="https://trendshift.io/api/badge/repositories/3068" alt="mindsdb%2Fmindsdb | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n\n  <a href="https://github.com/mindsdb/mindsdb">\n    <img src="/docs/assets/mindsdb_logo.png" alt="MindsDB" width="300">\n  </a>\n\n  <p align="center">\n    <br />\n    <a href="https://www.mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo">Website</a>\n    Â·\n    <a href="https://docs.mindsdb.com?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo">Docs</a>\n    Â·\n    <a href="https://mindsdb.com/contact">Contact us for a Demo</a>\n    Â·\n    <a href="https://mindsdb.com/joincommunity?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo">Community Slack</a>\n  </p>\n</div>\n\n----------------------------------------\n\n\nMindsDB enables humans, AI, agents, and applications to get highly accurate answers across large scale data sources.\n\n<a href="https://www.youtube.com/watch?v=MX3OKpnsoLM" target="_blank">\n  <img src="https://github.com/user-attachments/assets/119e7b82-f901-4214-a26f-ff7c5ad86064" alt="MindsDB Demo">\n	\n</a>\n\n\n## Install MindsDB Server \n\nMindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart''s content.\n\n  * [Using Docker Desktop](https://docs.mindsdb.com/setup/self-hosted/docker-desktop). This is the fastest and recommended way to get started and have it all running.\n  * [Using Docker](https://docs.mindsdb.com/setup/self-hosted/docker). This is also simple, but gives you more flexibility on how to further customize your server.\n\n[MindsDB has an MCP server built in](https://docs.mindsdb.com/mcp/overview) that enables your MCP applications to connect, unify and respond to questions over large-scale federated dataâ€”spanning databases, data warehouses, and SaaS applications.\n \n----------------------------------------\n\n# Core Philosophy: Connect, Unify, Respond\n\nMindsDB''s architecture is built around three fundamental capabilities:\n\n## [Connect](https://docs.mindsdb.com/integrations/data-overview) Your Data\n\nYou can connect to hundreds of enterprise [data sources (learn more)](https://docs.mindsdb.com/integrations/data-overview). These integrations allow MindsDB to access data wherever it resides, forming the foundation for all other capabilities.\n\n## [Unify](https://docs.mindsdb.com/mindsdb_sql/overview) Your Data\n\n\nIn many situations, itâ€™s important to be able to prepare and unify data before generating responses from it. MindsDB SQL offers knowledge bases and views that allow indexing and organizing structured and unstructured data as if it were unified in a single system.\n\n* [**KNOWLEDGE BASES**](https://docs.mindsdb.com/mindsdb_sql/knowledge-bases) â€“ Index and organize unstructured data for efficient Q&A.\n* [**VIEWS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/view) â€“ Simplify data access by creating unified views across different sources (no-ETL).\n\n\nUnification of data can be automated using JOBs\n\n* [**JOBS**](https://docs.mindsdb.com/mindsdb_sql/sql/create/jobs) â€“ Schedule synchronization and transformation tasks for real-time processing.\n\n\n## [Respond](https://docs.mindsdb.com/mindsdb_sql/agents/agent) From Your Data\n\nChat with Your Data\n\n* [**AGENTS**](https://docs.mindsdb.com/mindsdb_sql/agents/agent) â€“ Configure built-in agents specialized in answering questions over your connected and unified data.\n* [**MCP**](https://docs.mindsdb.com/mcp/overview) â€“ Connect to MindsDB through the MCP (Model Context Protocol) for seamless interaction.\n\n----------------------------------------\n\n## ğŸ¤ Contribute\n\nInterested in contributing to MindsDB? Follow our [installation guide for development](https://docs.mindsdb.com/contribute/install?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n\nYou can find our [contribution guide here](https://docs.mindsdb.com/contribute/contribute?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n\nWe welcome suggestions! Feel free to open new issues with your ideas, and weâ€™ll guide you.\n\nThis project adheres to a [Contributor Code of Conduct](https://github.com/mindsdb/mindsdb/blob/main/CODE_OF_CONDUCT.md). By participating, you agree to follow its terms.\n\nAlso, check out our [community rewards and programs](https://mindsdb.com/community?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n\n## ğŸ¤ Support\n\nIf you find a bug, please submit an [issue on GitHub](https://github.com/mindsdb/mindsdb/issues/new/choose).\n\nHereâ€™s how you can get community support:\n\n* Ask a question in our [Slack Community](https://mindsdb.com/joincommunity?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n* Join our [GitHub Discussions](https://github.com/mindsdb/mindsdb/discussions).\n* Post on [Stack Overflow](https://stackoverflow.com/questions/tagged/mindsdb) with the MindsDB tag.\n\nFor commercial support, please [contact the MindsDB team](https://mindsdb.com/contact?utm_medium=community&utm_source=github&utm_campaign=mindsdb%20repo).\n\n## ğŸ’š Current Contributors\n\n<a href="https://github.com/mindsdb/mindsdb/graphs/contributors">\n  <img src="https://contributors-img.web.app/image?repo=mindsdb/mindsdb" />\n</a>\n\nGenerated with [contributors-img](https://contributors-img.web.app).\n\n## ğŸ”” Subscribe for Updates\n\nJoin our [Slack community](https://mindsdb.com/joincommunity)\n', '{"language":"Python","stars":37470,"forks":6020,"watchers":37470,"open_issues":167,"topics":["agents","ai","analytics","artificial-inteligence","bigquery","business-intelligence","databases","hacktoberfest","llms","mcp","mssql","mysql","postgresql","rag"],"default_branch":"main","size_kb":267910,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:mindsdb:mindsdb\">","source_url":"https://github.com/mindsdb/mindsdb\">"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:mindsdb:mindsdb","source_url":"https://github.com/mindsdb/mindsdb"},{"type":"has_code","target_id":"github:mindsdb:mindsdb","source_url":"https://github.com/mindsdb/mindsdb"},{"type":"has_code","target_id":"github:mindsdb:mindsdb","source_url":"https://github.com/mindsdb/mindsdb"},{"type":"has_code","target_id":"github:mindsdb:mindsdb","source_url":"https://github.com/mindsdb/mindsdb"}]', NULL, 'NOASSERTION', 'approved', 65, 'e76643910e1cc3db3ec7b2751e294c0d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-mindsdb-mindsdb from https://github.com/mindsdb.png
Image converted to WebP: data/images/github-mindsdb-mindsdb.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-RSSNext-Folo', 'github--rssnext--folo', 'Folo', 'RSSNext', '<div align="center"> <a href="https://github.com/RSSNext/Folo"> <img src="https://github.com/RSSNext/Folo/raw/refs/heads/dev/apps/desktop/layer/renderer/public/icon.svg" alt="Logo" width="80" height="80"> </a> <h3>Folo</h3> <p> <img src="https://github.com/user-attachments/assets/cbe924f2-d8b0-48b0-814e-7c06ccb1911c" height="60" /> &nbsp;&nbsp;&nbsp; <img src="https://github.com/user-attachments/assets/6997a236-3df3-49d5-98a4-514f6d1a02c4" height="60" /> <br /> <br /> <a href="https://github....', '["ai","reader","rss","rss-reader","rsshub","typescript"]', 'other', 36203, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/RSSNext/Folo","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n  <a href="https://github.com/RSSNext/Folo">\n    <img src="https://github.com/RSSNext/Folo/raw/refs/heads/dev/apps/desktop/layer/renderer/public/icon.svg" alt="Logo" width="80" height="80">\n  </a>\n\n  <h3>Folo</h3>\n  <p>\n    <img src="https://github.com/user-attachments/assets/cbe924f2-d8b0-48b0-814e-7c06ccb1911c" height="60" />\n    &nbsp;&nbsp;&nbsp;\n    <img src="https://github.com/user-attachments/assets/6997a236-3df3-49d5-98a4-514f6d1a02c4" height="60" />\n    <br />\n    <br />\n    <a href="https://github.com/RSSNext/Folo/stargazers"><img src="https://img.shields.io/github/stars/RSSNext/Follow?color=ffcb47&labelColor=black&style=flat-square&logo=github&label=Stars" /></a>\n    <a href="https://github.com/RSSNext/Folo/graphs/contributors"><img src="https://img.shields.io/github/contributors/RSSNext/Folo?style=flat-square&logo=github&label=Contributors&labelColor=black" /></a>\n    <a href="https://status.follow.is/" target="_blank"><img src="https://status.follow.is/api/badge/18/uptime?color=%2344CC10&labelColor=black&style=flat-square"/></a>\n    <a href="https://github.com/RSSNext/Folo/releases"><img src="https://img.shields.io/github/downloads/RSSNext/Folo/total?color=369eff&labelColor=black&logo=github&style=flat-square&label=Downloads" /></a>\n    <a href="https://x.com/intent/follow?screen_name=folo_is"><img src="https://img.shields.io/badge/Follow-blue?color=1d9bf0&logo=x&labelColor=black&style=flat-square" /></a>\n    <a href="https://discord.gg/AwWcAQ7euc" target="_blank"><img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Ffollowapp%3Fwith_counts%3Dtrue&query=approximate_member_count&color=5865F2&label=Discord&labelColor=black&logo=discord&logoColor=white&style=flat-square"/></a>\n    <br />\n    <a href="https://apps.apple.com/us/app/folo-follow-everything/id6739802604"><img src="https://img.shields.io/itunes/v/6739802604?style=flat-square&logo=apple&label=App%20Store&color=FF5C00&labelColor=black" /></a>\n    <a href="https://play.google.com/store/apps/details?id=is.follow" target="_blank"><img src="https://img.shields.io/endpoint?url=https%3A%2F%2Fplay.cuzi.workers.dev%2Fplay%3Fi%3Dis.follow%26gl%3DUS%26hl%3Den%26l%3DAndroid%26m%3D%24version&style=flat-square&logo=google-play&label=Google%20Play&labelColor=black&color=FF5C00"/></a>\n    <a href="https://apps.apple.com/us/app/folo-follow-everything/id6739802604"><img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.folo.is%2Fupdates%2Fdistribution%2Fmas&query=data.storeVersion&prefix=v&style=flat-square&logo=apple&label=Mac%20App%20Store&labelColor=black&color=FF5C00&cacheSeconds=3600" /></a>\n    <a href="https://apps.microsoft.com/detail/9nvfzpv0v0ht?mode=direct"><img src="https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fapi.folo.is%2Fupdates%2Fdistribution%2Fmss&query=data.storeVersion&style=flat-square&logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIj48cGF0aCBmaWxsPSIjZmZmIiBkPSJNMyAzaDguNTN2OC41M0gzek0xMi40NjkgM2g4LjUzdjguNTNoLTguNTN6TTMgMTIuNDdoOC41M1YyMUgzek0xMi40NjkgMTIuNDdoOC41M1YyMWgtOC41M3oiLz48L3N2Zz4%3D&logoColor=white&label=Microsoft%20Store&labelColor=black&color=FF5C00&cacheSeconds=3600&prefix=v" /></a>\n    <br />\n    <br />\n    <!-- <a href="https://github.com/RSSNext/Folo" target="_blank"><img src="https://github.com/user-attachments/assets/59b957fb-59ed-4ef0-994e-f6a402a6fe2b" alt="GitHub Trending" height="55"/></a>\n    <br />\n    <br /> -->\n    <a href="https://apps.apple.com/us/app/folo-follow-everything/id6739802604" target="_blank"><img src="https://github.com/user-attachments/assets/35747716-28bf-413a-822b-aa49d49f1aa0" alt="Folo Mobile" width="52%"/></a>\n    <a href="https://apps.apple.com/us/app/folo-follow-everything/id6739802604" target="_blank"><img src="https://github.com/user-attachments/assets/198a0165-b8c9-45c1-9116-b473a13a8d0c" alt="Folo Desktop" width="46%"/></a>\n    <br />\n    <br />\n\n  </p>\n</div>\n\nAs they say, your thoughts are what you readâ€”and weâ€™ve been consuming noisy feeds for too long! Folo organizes content into one timeline, keeping you updated on what matters, noise-free. Share lists, explore collections, and enjoy distraction-free browsing.\n\n## ğŸ‘‹ğŸ» Getting Started & Join Our Community\n\nWhether for users or professional developers, Folo will be your open information playground. Please be aware that Folo is currently under active development, and feedback is welcome for any [issue](https://github.com/RSSNext/Folo/issues) encountered.\n\nFeel free to try it using the following methods:\n\n| Operating System | Source                                                                                                                                                                                                                                                                                                                                                                                                                                |\n| :--------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n| Any              | <a href="https://app.folo.is" target="_blank"><img src="https://github.com/user-attachments/assets/51ef7800-b683-4493-83e8-eb4752366997" alt="Browser" height="55"/></a>                                                                                                                                                                                                                                                              |\n| iOS              | <a href="https://apps.apple.com/us/app/folo-follow-everything/id6739802604" target="_blank"><img src="https://github.com/user-attachments/assets/a94d8698-2a11-4f43-9b0a-b756b17b61f7" alt="App Store" height="55"/></a>                                                                                                                                                                                                              |\n| Android          | <a href="https://play.google.com/store/apps/details?id=is.follow" target="_blank"><img src="https://github.com/user-attachments/assets/0d178e0b-3ace-4f75-bbde-ab3c0a416ce8" alt="Google Play" height="55"/></a> <a href="https://github.com/RSSNext/Folo/releases/latest" target="_blank"><img src="https://github.com/user-attachments/assets/cf61e197-d756-4606-a8ad-fb591f79fdfc" alt="App Store" height="55"/></a>               |\n| macOS            | <a href="https://apps.apple.com/us/app/folo-follow-everything/id6739802604" target="_blank"><img src="https://github.com/user-attachments/assets/0d47f902-7fa3-494e-ad28-9ab11af5e6d4" alt="Microsoft Store" height="55"/></a> <a href="https://github.com/RSSNext/Folo/releases/latest" target="_blank"><img src="https://github.com/user-attachments/assets/cf61e197-d756-4606-a8ad-fb591f79fdfc" alt="App Store" height="55"/></a> |\n| Windows          | <a href="https://apps.microsoft.com/detail/9nvfzpv0v0ht?mode=direct" target="_blank"><img src="https://github.com/user-attachments/assets/b3112bab-9dd0-4893-9488-890dcb368f70" alt="Microsoft Store" height="55"/></a> <a href="https://github.com/RSSNext/Folo/releases/latest" target="_blank"><img src="https://github.com/user-attachments/assets/cf61e197-d756-4606-a8ad-fb591f79fdfc" alt="App Store" height="55"/></a>        |\n| Linux            | <a href="https://github.com/RSSNext/Folo/releases/latest" target="_blank"><img src="https://github.com/user-attachments/assets/cf61e197-d756-4606-a8ad-fb591f79fdfc" alt="App Store" height="55"/></a>                                                                                                                                                                                                                                |\n\nYou can also install using the following methods maintained by our community:\n\n- If you are using Arch Linux, you can install the package [folo-appimage](https://aur.archlinux.org/packages/folo-appimage) that is maintained by [timochan](https://github.com/ttimochan) and [grtsinry43](https://github.com/grtsinry43).\n- If you are using Nix, you can install the package [follow](https://github.com/NixOS/nixpkgs/blob/master/pkgs/by-name/fo/follow/package.nix) that is maintained by [iosmanthus](https://github.com/iosmanthus).\n- If you are using macOS with [Homebrew](https://brew.sh), you can install the cask [folo](https://formulae.brew.sh/cask/folo) that is maintained by [realSunyz](https://github.com/realSunyz).\n- If you are using Windows with [Scoop](https://scoop.sh), you can install the manifest [folo](https://github.com/cscnk52/cetacea/blob/master/bucket/folo.json) that is maintained by [cscnk52](https://github.com/cscnk52).\n\n| [![Discord](https://img.shields.io/badge/dynamic/json?url=https%3A%2F%2Fdiscord.com%2Fapi%2Finvites%2Ffollowapp%3Fwith_counts%3Dtrue&query=approximate_member_count&color=5865F2&label=Discord&labelColor=black&logo=discord&logoColor=white&style=flat-square)](https://discord.gg/AwWcAQ7euc) | Join our Discord server to connect with developers, request features, and receive support. |\n| :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------- |\n| [![](https://img.shields.io/badge/any_text-Follow-blue?color=2CA5E0&label=_&logo=x&labelColor=black&style=flat-square)](https://x.com/intent/follow?screen_name=folo_is)                                                                                                                        | Follow us on X/Twitter for product updates and to join in on reward activities.            |\n\n> \[!IMPORTANT]\n>\n> **Star Us**, You will receive all release notifications from GitHub without any delay \~\n\n![Image](https://github.com/user-attachments/assets/a08f9437-b24c-4388-8f01-2826e09eeaf2)\n\n<a href="https://next.ossinsight.io/widgets/official/compose-last-28-days-stats?repo_id=783512367" target="_blank" style="display: block" align="center">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://next.ossinsight.io/widgets/official/compose-last-28-days-stats/thumbnail.png?repo_id=783512367&image_size=auto&color_scheme=dark" width="655" height="auto">\n    <img alt="Performance Stats of RSSNext/Folo - Last 28 days" src="https://next.ossinsight.io/widgets/official/compose-last-28-days-stats/thumbnail.png?repo_id=783512367&image_size=auto&color_scheme=light" width="655" height="auto">\n  </picture>\n</a>\n\n## âœ¨ Features\n\n### Customized Information Hub\n\nSubscribe to a vast range of feeds and curated lists. Curate your favorites and keep track of what matters most to you.\n\n![](https://github.com/user-attachments/assets/11dc7d21-f5d8-4e41-9269-24fc352aa02b)\n\n### AI At Your Fingertips\n\nA smarter and more efficient browsing with AI-powered features like translation, summary, and more.\n\n![](https://github.com/user-attachments/assets/37cf4f2f-4c5e-4775-86e8-2fa1a1b2ecf5)\n\n### Dynamic Content Support\n\nBecause we know content is more than just text. From articles to videos, images to audio â€” Folo gets it all covered.\n\n![](https://github.com/user-attachments/assets/d1379fd6-8767-476e-b0dc-d61753715e26)\n\n### More Than Just An App\n\nThis isnâ€™t just another app. Folo is a community â€” introducing a new era of openness and community-driven experience.\n\n![](https://github.com/user-attachments/assets/62004a04-eaea-4f5d-bfbf-4e68b6b90286)\n\n## ğŸ¤ Contributing\n\nYou are welcome to join the open source community to build together, please check our [Contributing Guide](./CONTRIBUTING.md) for more details.\n\n## ğŸ” Code signing policy\n\nFolo for Windows uses free code signing provided by [SignPath.io](https://about.signpath.io/), a certificate by [SignPath Foundation](https://signpath.org/).\n\nFolo for macOS and iOS is signed and notarized by [Apple Developer Program](https://developer.apple.com/programs/).\n\nAll released files are verified with [GitHub artifact attestations](https://github.com/RSSNext/Folo/attestations) to ensure their provenance and integrity.\n\n## ğŸ“ License\n\nFolo is licensed under the GNU General Public License version 3 with the addition of the following special exception:\n\nAll content in the `icons/mgc` directory is copyrighted by https://mgc.mingcute.com/ and cannot be redistributed.\n', '{"language":"TypeScript","stars":36203,"forks":1850,"watchers":36203,"open_issues":351,"topics":["ai","reader","rss","rss-reader","rsshub"],"default_branch":"dev","size_kb":67956,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:RSSNext:Folo\">","source_url":"https://github.com/RSSNext/Folo\">"},{"type":"has_code","target_id":"github:RSSNext:Folo","source_url":"https://github.com/RSSNext/Folo"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:RSSNext:Folo","source_url":"https://github.com/RSSNext/Folo"},{"type":"has_code","target_id":"github:RSSNext:Folo","source_url":"https://github.com/RSSNext/Folo"},{"type":"has_code","target_id":"github:RSSNext:Folo","source_url":"https://github.com/RSSNext/Folo"},{"type":"has_code","target_id":"github:RSSNext:Folo\"","source_url":"https://github.com/RSSNext/Folo\""},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:RSSNext:Folo","source_url":"https://github.com/RSSNext/Folo"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:RSSNext:Folo","source_url":"https://github.com/RSSNext/Folo"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:RSSNext:Folo","source_url":"https://github.com/RSSNext/Folo"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:RSSNext:Folo","source_url":"https://github.com/RSSNext/Folo"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:RSSNext:Folo","source_url":"https://github.com/RSSNext/Folo"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:NixOS:nixpkgs","source_url":"https://github.com/NixOS/nixpkgs"},{"type":"has_code","target_id":"github:cscnk52:cetacea","source_url":"https://github.com/cscnk52/cetacea"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:RSSNext:Folo","source_url":"https://github.com/RSSNext/Folo"}]', NULL, 'GPL-3.0', 'approved', 80, '1cb3273ba3884325d9389230cd8f11b9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-RSSNext-Folo from https://github.com/RSSNext.png
Image converted to WebP: data/images/github-RSSNext-Folo.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-agno-agi-agno', 'github--agno-agi--agno', 'agno', 'agno-agi', '<div align="center" id="top"> <a href="https://agno.com"> <picture> <source media="(prefers-color-scheme: dark)" srcset="https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-dark.svg"> <source media="(prefers-color-scheme: light)" srcset="https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg"> <img src="https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg" alt="Agno"> </picture> </a> </div> <div align="center"> <a href="https://docs.agno.com">Documentation...', '["agents","ai","ai-agents","developer-tools","python","python"]', 'other', 35794, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/agno-agi/agno","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center" id="top">\n  <a href="https://agno.com">\n    <picture>\n      <source media="(prefers-color-scheme: dark)" srcset="https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-dark.svg">\n      <source media="(prefers-color-scheme: light)" srcset="https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg">\n      <img src="https://agno-public.s3.us-east-1.amazonaws.com/assets/logo-light.svg" alt="Agno">\n    </picture>\n  </a>\n</div>\n\n<div align="center">\n  <a href="https://docs.agno.com">Documentation</a>\n  <span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>\n  <a href="https://docs.agno.com/examples/use-cases/agents/overview">Examples</a>\n  <span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>\n  <a href="https://www.agno.com/?utm_source=github&utm_medium=readme&utm_campaign=agno-github">Website</a>\n  <br />\n</div>\n\n## What is Agno?\n\nAgno is an incredibly fast multi-agent framework, runtime and control plane.\n\nIt provides the complete stack for building, running and managing multi-agent systems:\n\n- **Framework**: Build agents, multi-agent teams and workflows with memory, knowledge, state, guardrails, HITL, context compression, MCP, A2A and 100+ toolkits.\n- **AgentOS Runtime**: Run your multi-agent system in production with a secure, stateless runtime and ready to use integration endpoints.\n- **AgentOS Control Plane**: Test, monitor and manage AgentOS deployments across environments with full operational visibility.\n\nCheckout the full list of features [here](#features).\n\n## Getting started\n\nIf you''re new to Agno, follow our [quickstart](https://docs.agno.com/get-started/quickstart) to build your first Agent and chat with it using the AgentOS UI.\n\nAfter that, checkout the [examples gallery](https://docs.agno.com/examples/use-cases/agents/overview) and build real-world applications with Agno.\n\n## Documentation, Community & More Examples\n\n- Docs: <a href="https://docs.agno.com" target="_blank" rel="noopener noreferrer">docs.agno.com</a>\n- Cookbook: <a href="https://github.com/agno-agi/agno/tree/main/cookbook" target="_blank" rel="noopener noreferrer">Cookbook</a>\n- Community forum: <a href="https://community.agno.com/" target="_blank" rel="noopener noreferrer">community.agno.com</a>\n- Discord: <a href="https://discord.gg/4MtYHHrgA8" target="_blank" rel="noopener noreferrer">discord</a>\n\n## Example\n\nHere''s an example of an Agent that connects to an MCP server, manages conversation state in a database, is served using a FastAPI application that you can chat with using the [AgentOS UI](https://os.agno.com).\n\n```python agno_agent.py\nfrom agno.agent import Agent\nfrom agno.db.sqlite import SqliteDb\nfrom agno.models.anthropic import Claude\nfrom agno.os import AgentOS\nfrom agno.tools.mcp import MCPTools\n\n# ************* Create Agent *************\nagno_agent = Agent(\n    name="Agno Agent",\n    model=Claude(id="claude-sonnet-4-5"),\n    # Add a database to the Agent\n    db=SqliteDb(db_file="agno.db"),\n    # Add the Agno MCP server to the Agent\n    tools=[MCPTools(transport="streamable-http", url="https://docs.agno.com/mcp")],\n    # Add the previous session history to the context\n    add_history_to_context=True,\n    markdown=True,\n)\n\n\n# ************* Create AgentOS *************\nagent_os = AgentOS(agents=[agno_agent])\n# Get the FastAPI app for the AgentOS\napp = agent_os.get_app()\n\n# ************* Run AgentOS *************\nif __name__ == "__main__":\n    agent_os.serve(app="agno_agent:app", reload=True)\n```\n\n## AgentOS - Production Runtime for Multi-Agent Systems\n\nBuilding Agents is easy, running them as a secure, scalable service is hard. AgentOS solves this by providing a high performance runtime for serving multi-agent systems in production. Key features include:\n\n1. **Pre-built FastAPI app**: AgentOS includes a ready-to-use FastAPI app for running your agents, teams and workflows. This gives you a significant head start when building an AI product.\n\n2. **Integrated Control Plane**: The [AgentOS UI](https://os.agno.com) connects directly to your runtime, so you can test, monitor and manage your system in real time with full operational visibility.\n\n3. **Private by Design**: AgentOS runs entirely in your cloud, ensuring complete data privacy. No data leaves your environment, making it ideal for security conscious enterprises..\n\nWhen you run the example script shared above, you get a FastAPI app that you can connect to the [AgentOS UI](https://os.agno.com). Here''s what it looks like in action:\n\nhttps://github.com/user-attachments/assets/feb23db8-15cc-4e88-be7c-01a21a03ebf6\n\n## The Complete Agentic Solution\n\nAgno provides the complete solution for companies building agentic systems:\n\n- The fastest framework for building agents, multi-agent teams and agentic workflows.\n- A ready-to-use FastAPI app that gets you building AI products on day one.\n- A control plane for testing, monitoring and managing your system.\n\nAgno brings a novel architecture that no other framework provides, your AgentOS runs securely in your cloud, and the control plane connects directly to it from your browser. You don''t need to send data to any external services or pay retention costs, you get complete privacy and control.\n\n## Features\n\nAgno is an incredibly feature-rich framework purpose-built for Agent Engineering. Here are some key features:\n\n| **Category**                           | **Feature**                     | **Description**                                                                                                           |\n| -------------------------------------- | ------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |\n| **Foundational Principles**            | **Model Agnostic**              | Supports all model providers so you can choose the best model for your use case |\n|                                        | **Type Safe**                   | Enforces structured I/O through input_schema and output_schema for predictable and composable agent behavior. |\n|                                        | **Dynamic Context**             | Inject variables, state, and retrieved data at runtime into context. Compress, summarize and filter context to keep your Agents focused and efficient. |\n|                                        | **Designed for Scale**          | Designed around async execution and long-running tasks for high throughput agent workloads. |\n| **Memory, Knowledge, and Persistence** | **Persistent Storage**          | Give your Agents, Teams, and Workflows a database to persist session history, state, and messages. |\n|                                        | **User Memory**                 | Built in memory layer that helps agents recall user specific context across sessions. |\n|                                        | **Agentic RAG**                 | Connect to 20+ vector stores (called **Knowledge**) with hybrid search, reranking, and chunking out of the box. |\n|                                        | **Culture** | Shared long term collective memory that compounds across agents and time. |\n|                                        | **Ephemeral Context** | In memory scratchpad for short lived reasoning without polluting long term state. |\n| **Execution & Control**                | **Human-in-the-Loop**           | Native support for confirmations, approvals, manual overrides, and external actions. |\n|                                        | **Guardrails**                  | Built-in safeguards for validation, security, and prompt protection.                                                      |\n|                                        | **Agent Lifecycle Hooks**       | Pre and post hooks to validate, enrich, or transform inputs and outputs. |\n|                                        | **MCP Integration**             | First-class support for the Model Context Protocol (MCP) to connect Agents with external systems. |\n|                                        | **A2A Integration**             | First-class support for the Agent to Agent communication protocol (A2A). |\n|                                        | **Toolkits**                    | 100+ built in toolkits with thousands of tools covering data, code, web, and enterprise APIs. |\n| **Runtime & Evaluation**               | **Runtime**                     | Prebuilt FastAPI runtime with SSE compatible endpoints. Production ready from day one. |\n|                                        | **Control Plane (UI)**          | Integrated interface to test, observe, and debug your agents, teams, and workflows in real time. |\n|                                        | **Natively Multimodal**         | Agents can process and generate text, images, audio, video, and files. |\n|                                        | **Evals**                       | Measure Accuracy, Performance, Latency, and Reliability across agents and workflows. |\n|                                        | **Durable Execution**           | Built in support for long running, resumable workflows. |\n| **Security & Privacy**                 | **Private by Design**           | Runs entirely in your cloud. The UI connects directly to your AgentOS from your browser, no data is ever sent externally. |\n|                                        | **Data Governance**             | Your data lives securely in your Agent database, no external data sharing or vendor lock-in.                              |\n|                                        | **Access Control**              | Role-based access (RBAC) and per-agent permissions to protect sensitive contexts and tools.                               |\n\nEvery part of Agno is built for real-world deployment â€” where developer experience meets production performance.\n\n## Setup Your Coding Agent to Use Agno\n\nFor LLMs and AI assistants to understand and navigate Agno''s documentation, we provide an [llms.txt](https://docs.agno.com/llms.txt) or [llms-full.txt](https://docs.agno.com/llms-full.txt) file. This file is built for AI systems to efficiently parse and reference our documentation.\n\n### IDE Integration\n\nWhen building Agno agents, using Agno documentation as a source in your IDE is a great way to speed up your development. Here''s how to integrate with Cursor:\n\n1. In Cursor, go to the "Cursor Settings" menu.\n2. Find the "Indexing & Docs" section.\n3. Add `https://docs.agno.com/llms-full.txt` to the list of documentation URLs.\n4. Save the changes.\n\nNow, Cursor will have access to the Agno documentation. You can do the same with other IDEs like VSCode, Windsurf etc.\n\n## Performance\n\nIf you''re building with Agno, you''re guaranteed best-in-class performance by default. Our obsession with performance is necessary because even simple AI workflows can spawn hundreds of Agents and because many tasks are long-running -- stateless, horizontal scalability is key for success.\n\nAt Agno, we optimize performance across 3 dimensions:\n\n1. **Agent performance:** We optimize static operations (instantiation, memory footprint) and runtime operations (tool calls, memory updates, history management).\n2. **System performance:** The AgentOS API is async by default and has a minimal memory footprint. The system is stateless and horizontally scalable, with a focus on preventing memory leaks. It handles parallel and batch embedding generation during knowledge ingestion, metrics collection in background tasks, and other system-level optimizations.\n3. **Agent reliability and accuracy:** Monitored through evals, which we''ll explore later.\n\n### Agent Performance\n\nLet''s measure the time it takes to instantiate an Agent and the memory footprint of an Agent. Here are the numbers (last measured in Oct 2025, on an Apple M4 MacBook Pro):\n\n- **Agent instantiation:** ~3Î¼s on average\n- **Memory footprint:** ~6.6Kib on average\n\nWe''ll show below that Agno Agents instantiate **529Ã— faster than Langgraph**, **57Ã— faster than PydanticAI**, and **70Ã— faster than CrewAI**. Agno Agents also use **24Ã— lower memory than Langgraph**, **4Ã— lower than PydanticAI**, and **10Ã— lower than CrewAI**.\n\n> [!NOTE]\n> Run time performance is bottlenecked by inference and hard to benchmark accurately, so we focus on minimizing overhead, reducing memory usage, and parallelizing tool calls.\n\n### Instantiation Time\n\nLet''s measure instantiation time for an Agent with 1 tool. We''ll run the evaluation 1000 times to get a baseline measurement. We''ll compare Agno to LangGraph, CrewAI and Pydantic AI.\n\n> [!NOTE]\n> The code for this benchmark is available [here](https://github.com/agno-agi/agno/tree/main/cookbook/evals/performance). You should run the evaluation yourself on your own machine, please, do not take these results at face value.\n\n```shell\n# Setup virtual environment\n./scripts/perf_setup.sh\nsource .venvs/perfenv/bin/activate\n\n# Agno\npython cookbook/evals/performance/instantiate_agent_with_tool.py\n\n# LangGraph\npython cookbook/evals/performance/comparison/langgraph_instantiation.py\n# CrewAI\npython cookbook/evals/performance/comparison/crewai_instantiation.py\n# Pydantic AI\npython cookbook/evals/performance/comparison/pydantic_ai_instantiation.py\n```\n\nLangGraph is on the right, **let''s start it first and give it a head start**. Then CrewAI and Pydantic AI follow, and finally Agno. Agno obviously finishes first, but let''s see by how much.\n\nhttps://github.com/user-attachments/assets/54b98576-1859-4880-9f2d-15e1a426719d\n\n### Memory Usage\n\nTo measure memory usage, we use the `tracemalloc` library. We first calculate a baseline memory usage by running an empty function, then run the Agent 1000x times and calculate the difference. This gives a (reasonably) isolated measurement of the memory usage of the Agent.\n\nWe recommend running the evaluation yourself on your own machine, and digging into the code to see how it works. If we''ve made a mistake, please let us know.\n\n### Results\n\nTaking Agno as the baseline, we can see that:\n\n| Metric             | Agno | Langgraph   | PydanticAI | CrewAI     |\n| ------------------ | ---- | ----------- | ---------- | ---------- |\n| **Time (seconds)** | 1Ã—   | 529Ã— slower | 57Ã— slower | 70Ã— slower |\n| **Memory (MiB)**   | 1Ã—   | 24Ã— higher  | 4Ã— higher  | 10Ã— higher |\n\nExact numbers from the benchmark:\n\n| Metric             | Agno     | Langgraph | PydanticAI | CrewAI   |\n| ------------------ | -------- | --------- | ---------- | -------- |\n| **Time (seconds)** | 0.000003 | 0.001587  | 0.000170   | 0.000210 |\n| **Memory (MiB)**   | 0.006642 | 0.161435  | 0.028712   | 0.065652 |\n\n> [!NOTE]\n> Agno agents are designed for performance and while we share benchmarks against other frameworks, we should be mindful that accuracy and reliability are more important than speed.\n\n## Contributions\n\nWe welcome contributions, read our [contributing guide](https://github.com/agno-agi/agno/blob/v2.0/CONTRIBUTING.md) to get started.\n\n## Telemetry\n\nAgno logs which model an agent used so we can prioritize updates to the most popular providers. You can disable this by setting `AGNO_TELEMETRY=false` in your environment.\n\n<p align="left">\n  <a href="#top">â¬†ï¸ Back to Top</a>\n</p>\n', '{"language":"Python","stars":35794,"forks":4726,"watchers":35794,"open_issues":323,"topics":["agents","ai","ai-agents","developer-tools","python"],"default_branch":"main","size_kb":255278,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:agno-agi:agno","source_url":"https://github.com/agno-agi/agno"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:agno-agi:agno","source_url":"https://github.com/agno-agi/agno"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:agno-agi:agno","source_url":"https://github.com/agno-agi/agno"}]', NULL, 'Apache-2.0', 'approved', 80, '4d67b50e86e2de20821784e5c0028997', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-agno-agi-agno from https://github.com/agno-agi.png
Image converted to WebP: data/images/github-agno-agi-agno.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-daytonaio-daytona', 'github--daytonaio--daytona', 'daytona', 'daytonaio', '<div align="center"> !License !GitHub Release </div> &nbsp; <div align="center"> <picture> <source media="(prefers-color-scheme: dark)" srcset="https://github.com/daytonaio/daytona/raw/main/assets/images/Daytona-logotype-white.png"> <source media="(prefers-color-scheme: light)" srcset="https://github.com/daytonaio/daytona/raw/main/assets/images/Daytona-logotype-black.png"> <img alt="Daytona logo" src="https://github.com/daytonaio/daytona/raw/main/assets/images/Daytona-logotype-black.png" widt...', '["agentic-workflow","ai","ai-agents","ai-runtime","ai-sandboxes","code-execution","code-interpreter","developer-tools","typescript"]', 'other', 35407, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/daytonaio/daytona","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n\n[![Documentation](https://img.shields.io/github/v/release/daytonaio/docs?label=Docs&color=23cc71)](https://www.daytona.io/docs)\n![License](https://img.shields.io/badge/License-AGPL--3-blue)\n[![Go Report Card](https://goreportcard.com/badge/github.com/daytonaio/daytona)](https://goreportcard.com/report/github.com/daytonaio/daytona)\n[![Issues - daytona](https://img.shields.io/github/issues/daytonaio/daytona)](https://github.com/daytonaio/daytona/issues)\n![GitHub Release](https://img.shields.io/github/v/release/daytonaio/daytona)\n\n</div>\n\n&nbsp;\n\n<div align="center">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://github.com/daytonaio/daytona/raw/main/assets/images/Daytona-logotype-white.png">\n    <source media="(prefers-color-scheme: light)" srcset="https://github.com/daytonaio/daytona/raw/main/assets/images/Daytona-logotype-black.png">\n    <img alt="Daytona logo" src="https://github.com/daytonaio/daytona/raw/main/assets/images/Daytona-logotype-black.png" width="50%">\n  </picture>\n</div>\n\n<h3 align="center">\n  Run AI Code.\n  <br/>\n  Secure and Elastic Infrastructure for\n  Running Your AI-Generated Code.\n</h3>\n\n<p align="center">\n    <a href="https://www.daytona.io/docs"> Documentation </a>Â·\n    <a href="https://github.com/daytonaio/daytona/issues/new?assignees=&labels=bug&projects=&template=bug_report.md&title=%F0%9F%90%9B+Bug+Report%3A+"> Report Bug </a>Â·\n    <a href="https://github.com/daytonaio/daytona/issues/new?assignees=&labels=enhancement&projects=&template=feature_request.md&title=%F0%9F%9A%80+Feature%3A+"> Request Feature </a>Â·\n    <a href="https://go.daytona.io/slack"> Join our Slack </a>Â·\n    <a href="https://x.com/daytonaio"> Connect on X </a>\n</p>\n\n<p align="center">\n    <a href="https://www.producthunt.com/posts/daytona-2?embed=true&utm_source=badge-top-post-badge&utm_medium=badge&utm_souce=badge-daytona&#0045;2" target="_blank"><img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=957617&theme=neutral&period=daily&t=1746176740150" alt="Daytona&#0032; - Secure&#0032;and&#0032;elastic&#0032;infra&#0032;for&#0032;running&#0032;your&#0032;AI&#0045;generated&#0032;code&#0046; | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /></a>\n    <a href="https://www.producthunt.com/posts/daytona-2?embed=true&utm_source=badge-top-post-topic-badge&utm_medium=badge&utm_souce=badge-daytona&#0045;2" target="_blank"><img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-topic-badge.svg?post_id=957617&theme=neutral&period=monthly&topic_id=237&t=1746176740150" alt="Daytona&#0032; - Secure&#0032;and&#0032;elastic&#0032;infra&#0032;for&#0032;running&#0032;your&#0032;AI&#0045;generated&#0032;code&#0046; | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /></a>\n</p>\n\n---\n\n## Installation\n\n### Python SDK\n\n```bash\npip install daytona\n```\n\n### TypeScript SDK\n\n```bash\nnpm install @daytonaio/sdk\n```\n\n---\n\n## Features\n\n- **Lightning-Fast Infrastructure**: Sub-90ms Sandbox creation from code to execution.\n- **Separated & Isolated Runtime**: Execute AI-generated code with zero risk to your infrastructure.\n- **Massive Parallelization for Concurrent AI Workflows**: Fork Sandbox filesystem and memory state (Coming soon!)\n- **Programmatic Control**: File, Git, LSP, and Execute API\n- **Unlimited Persistence**: Your Sandboxes can live forever\n- **OCI/Docker Compatibility**: Use any OCI/Docker image to create a Sandbox\n\n---\n\n## Quick Start\n\n1. Create an account at https://app.daytona.io\n1. Generate a [new API key](https://app.daytona.io/dashboard/keys)\n1. Follow the [Getting Started docs](https://www.daytona.io/docs/getting-started/) to start using the Daytona SDK\n\n## Creating your first Sandbox\n\n### Python SDK\n\n```py\nfrom daytona import Daytona, DaytonaConfig, CreateSandboxBaseParams\n\n# Initialize the Daytona client\ndaytona = Daytona(DaytonaConfig(api_key="YOUR_API_KEY"))\n\n# Create the Sandbox instance\nsandbox = daytona.create(CreateSandboxBaseParams(language="python"))\n\n# Run code securely inside the Sandbox\nresponse = sandbox.process.code_run(''print("Sum of 3 and 4 is " + str(3 + 4))'')\nif response.exit_code != 0:\n    print(f"Error running code: {response.exit_code} {response.result}")\nelse:\n    print(response.result)\n\n# Clean up the Sandbox\ndaytona.delete(sandbox)\n```\n\n### Typescript SDK\n\n```jsx\nimport { Daytona } from ''@daytonaio/sdk''\n\nasync function main() {\n  // Initialize the Daytona client\n  const daytona = new Daytona({\n    apiKey: ''YOUR_API_KEY'',\n  })\n\n  let sandbox\n  try {\n    // Create the Sandbox instance\n    sandbox = await daytona.create({\n      language: ''typescript'',\n    })\n    // Run code securely inside the Sandbox\n    const response = await sandbox.process.codeRun(''console.log("Sum of 3 and 4 is " + (3 + 4))'')\n    if (response.exitCode !== 0) {\n      console.error(''Error running code:'', response.exitCode, response.result)\n    } else {\n      console.log(response.result)\n    }\n  } catch (error) {\n    console.error(''Sandbox flow error:'', error)\n  } finally {\n    if (sandbox) await daytona.delete(sandbox)\n  }\n}\n\nmain().catch(console.error)\n```\n\n---\n\n## Contributing\n\nDaytona is Open Source under the [GNU AFFERO GENERAL PUBLIC LICENSE](LICENSE), and is the [copyright of its contributors](NOTICE). If you would like to contribute to the software, read the Developer Certificate of Origin Version 1.1 (https://developercertificate.org/). Afterwards, navigate to the [contributing guide](CONTRIBUTING.md) to get started.\n', '{"language":"TypeScript","stars":35407,"forks":2737,"watchers":35407,"open_issues":240,"topics":["agentic-workflow","ai","ai-agents","ai-runtime","ai-sandboxes","code-execution","code-interpreter","developer-tools"],"default_branch":"main","size_kb":55185,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:daytonaio:daytona","source_url":"https://github.com/daytonaio/daytona"},{"type":"has_code","target_id":"github:daytonaio:daytona","source_url":"https://github.com/daytonaio/daytona"},{"type":"has_code","target_id":"github:daytonaio:daytona","source_url":"https://github.com/daytonaio/daytona"},{"type":"has_code","target_id":"github:daytonaio:daytona","source_url":"https://github.com/daytonaio/daytona"},{"type":"has_code","target_id":"github:daytonaio:daytona","source_url":"https://github.com/daytonaio/daytona"},{"type":"has_code","target_id":"github:daytonaio:daytona","source_url":"https://github.com/daytonaio/daytona"}]', NULL, 'AGPL-3.0', 'approved', 65, 'ef9c18998298e7680ec1ae236da39d4a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-daytonaio-daytona from https://github.com/daytonaio.png
Image converted to WebP: data/images/github-daytonaio-daytona.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-danielmiessler-Fabric', 'github--danielmiessler--fabric', 'Fabric', 'danielmiessler', '<div align="center"> <a href="https://go.warp.dev/fabric" target="_blank"> <sup>Special thanks to:</sup> <br> <img alt="Warp sponsorship" width="400" src="https://raw.githubusercontent.com/warpdotdev/brand-assets/refs/heads/main/Github/Sponsor/Warp-Github-LG-02.png"> <br> <h>Warp, built for coding with multiple AI agents</b> <br> <sup>Available for macOS, Linux and Windows</sup> </a> </div> <br> <div align="center"> <img src="./docs/images/fabric-logo-gif.gif" alt="fabriclogo" width="400" hei...', '["ai","augmentation","flourishing","life","work","javascript"]', 'other', 35039, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/danielmiessler/Fabric","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n    <a href="https://go.warp.dev/fabric" target="_blank">\n        <sup>Special thanks to:</sup>\n        <br>\n        <img alt="Warp sponsorship" width="400" src="https://raw.githubusercontent.com/warpdotdev/brand-assets/refs/heads/main/Github/Sponsor/Warp-Github-LG-02.png">\n        <br>\n        <h>Warp, built for coding with multiple AI agents</b>\n        <br>\n        <sup>Available for macOS, Linux and Windows</sup>\n    </a>\n</div>\n\n<br>\n\n<div align="center">\n\n<img src="./docs/images/fabric-logo-gif.gif" alt="fabriclogo" width="400" height="400"/>\n\n# `fabric`\n\n![Static Badge](https://img.shields.io/badge/mission-human_flourishing_via_AI_augmentation-purple)\n<br />\n![GitHub top language](https://img.shields.io/github/languages/top/danielmiessler/fabric)\n![GitHub last commit](https://img.shields.io/github/last-commit/danielmiessler/fabric)\n[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)\n[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/danielmiessler/fabric)\n\n<div align="center">\n<h4><code>fabric</code> is an open-source framework for augmenting humans using AI.</h4>\n</div>\n\n![Screenshot of fabric](./docs/images/fabric-summarize.png)\n\n</div>\n\n[Updates](#updates) â€¢\n[What and Why](#what-and-why) â€¢\n[Philosophy](#philosophy) â€¢\n[Installation](#installation) â€¢\n[Usage](#usage) â€¢\n[Examples](#examples) â€¢\n[Just Use the Patterns](#just-use-the-patterns) â€¢\n[Custom Patterns](#custom-patterns) â€¢\n[Helper Apps](#helper-apps) â€¢\n[Meta](#meta)\n\n</div>\n\n## What and why\n\nSince the start of modern AI in late 2022 we''ve seen an **_extraordinary_** number of AI applications for accomplishing tasks. There are thousands of websites, chat-bots, mobile apps, and other interfaces for using all the different AI out there.\n\nIt''s all really exciting and powerful, but _it''s not easy to integrate this functionality into our lives._\n\n<div class="align center">\n<h4>In other words, AI doesn''t have a capabilities problemâ€”it has an <em>integration</em> problem.</h4>\n</div>\n\n**Fabric was created to address this by creating and organizing the fundamental units of AIâ€”the prompts themselves!**\n\nFabric organizes prompts by real-world task, allowing people to create, collect, and organize their most important AI solutions in a single place for use in their favorite tools. And if you''re command-line focused, you can use Fabric itself as the interface!\n\n## Updates\n\n<details>\n<summary>Click to view recent updates</summary>\n\nDear Users,\n\nWe''ve been doing so many exciting things here at Fabric, I wanted to give a quick summary here to give you a sense of our development velocity!\n\nBelow are the **new features and capabilities** we''ve added (newest first):\n\n### Recent Major Features\n\n- [v1.4.338](https://github.com/danielmiessler/fabric/releases/tag/v1.4.338) (Dec 4, 2025) â€” Add Abacus vendor support for Chat-LLM\n  models (see [RouteLLM APIs](https://abacus.ai/app/route-llm-apis)).\n- [v1.4.337](https://github.com/danielmiessler/fabric/releases/tag/v1.4.337) (Dec 4, 2025) â€” Add "Z AI" vendor support. See the [Z AI overview](https://docs.z.ai/guides/overview/overview) page for more details.\n- [v1.4.334](https://github.com/danielmiessler/fabric/releases/tag/v1.4.334) (Nov 26, 2025) â€” **Claude Opus 4.5**: Updates the Anthropic SDK to the latest and adds the new [Claude Opus 4.5](https://www.anthropic.com/news/claude-opus-4-5) to the available models.\n- [v1.4.331](https://github.com/danielmiessler/fabric/releases/tag/v1.4.331) (Nov 23, 2025) â€” **Support for GitHub Models**: Adds support for using GitHub Models.\n- [v1.4.322](https://github.com/danielmiessler/fabric/releases/tag/v1.4.322) (Nov 5, 2025) â€” **Interactive HTML Concept Maps and Claude Sonnet 4.5**: Adds `create_conceptmap` pattern for visual knowledge representation using Vis.js, introduces WELLNESS category with psychological analysis patterns, and upgrades to Claude Sonnet 4.5\n- [v1.4.317](https://github.com/danielmiessler/fabric/releases/tag/v1.4.317) (Sep 21, 2025) â€” **Portuguese Language Variants**: Adds BCP 47 locale normalization with support for Brazilian Portuguese (pt-BR) and European Portuguese (pt-PT) with intelligent fallback chains\n- [v1.4.314](https://github.com/danielmiessler/fabric/releases/tag/v1.4.314) (Sep 17, 2025) â€” **Azure OpenAI Migration**: Migrates to official `openai-go/azure` SDK with improved authentication and default API version support\n- [v1.4.311](https://github.com/danielmiessler/fabric/releases/tag/v1.4.311) (Sep 13, 2025) â€” **More internationalization support**: Adds de (German), fa (Persian / Farsi), fr (French), it (Italian),\n  ja (Japanese), pt (Portuguese), zh (Chinese)\n- [v1.4.309](https://github.com/danielmiessler/fabric/releases/tag/v1.4.309) (Sep 9, 2025) â€” **Comprehensive internationalization support**: Includes English and Spanish locale files.\n- [v1.4.303](https://github.com/danielmiessler/fabric/releases/tag/v1.4.303) (Aug 29, 2025) â€” **New Binary Releases**: Linux ARM and Windows ARM targets. You can run Fabric on the Raspberry PI and on your Windows Surface!\n- [v1.4.294](https://github.com/danielmiessler/fabric/releases/tag/v1.4.294) (Aug 20, 2025) â€” **Venice AI Support**: Added the Venice AI provider. Venice is a Privacy-First, Open-Source AI provider. See their ["About Venice"](https://docs.venice.ai/overview/about-venice) page for details.\n- [v1.4.291](https://github.com/danielmiessler/fabric/releases/tag/v1.4.291) (Aug 18, 2025) â€” **Speech To Text**: Add OpenAI speech-to-text support with `--transcribe-file`, `--transcribe-model`, and `--split-media-file` flags.\n- [v1.4.287](https://github.com/danielmiessler/fabric/releases/tag/v1.4.287) (Aug 16, 2025) â€” **AI Reasoning**: Add Thinking to Gemini models and introduce `readme_updates` python script\n- [v1.4.286](https://github.com/danielmiessler/fabric/releases/tag/v1.4.286) (Aug 14, 2025) â€” **AI Reasoning**: Introduce Thinking Config Across Anthropic and OpenAI Providers\n- [v1.4.285](https://github.com/danielmiessler/fabric/releases/tag/v1.4.285) (Aug 13, 2025) â€” **Extended Context**: Enable One Million Token Context Beta Feature for Sonnet-4\n- [v1.4.284](https://github.com/danielmiessler/fabric/releases/tag/v1.4.284) (Aug 12, 2025) â€” **Easy Shell Completions Setup**: Introduce One-Liner Curl Install for Completions\n- [v1.4.283](https://github.com/danielmiessler/fabric/releases/tag/v1.4.283) (Aug 12, 2025) â€” **Model Management**: Add Vendor Selection Support for Models\n- [v1.4.282](https://github.com/danielmiessler/fabric/releases/tag/v1.4.282) (Aug 11, 2025) â€” **Enhanced Shell Completions**: Enhanced Shell Completions for Fabric CLI Binaries\n- [v1.4.281](https://github.com/danielmiessler/fabric/releases/tag/v1.4.281) (Aug 11, 2025) â€” **Gemini Search Tool**: Add Web Search Tool Support for Gemini Models\n- [v1.4.278](https://github.com/danielmiessler/fabric/releases/tag/v1.4.278) (Aug 9, 2025) â€” **Enhance YouTube Transcripts**: Enhance YouTube Support with Custom yt-dlp Arguments\n- [v1.4.277](https://github.com/danielmiessler/fabric/releases/tag/v1.4.277) (Aug 8, 2025) â€” **Desktop Notifications**: Add cross-platform desktop notifications to Fabric CLI\n- [v1.4.274](https://github.com/danielmiessler/fabric/releases/tag/v1.4.274) (Aug 7, 2025) â€” **Claude 4.1 Added**: Add Support for Claude Opus 4.1 Model\n- [v1.4.271](https://github.com/danielmiessler/fabric/releases/tag/v1.4.271) (Jul 28, 2025) â€” **AI Summarized Release Notes**: Enable AI summary updates for GitHub releases\n- [v1.4.268](https://github.com/danielmiessler/fabric/releases/tag/v1.4.268) (Jul 26, 2025) â€” **Gemini TTS Voice Selection**: add Gemini TTS voice selection and listing functionality\n- [v1.4.267](https://github.com/danielmiessler/fabric/releases/tag/v1.4.267) (Jul 26, 2025) â€” **Text-to-Speech**: Update Gemini Plugin to New SDK with TTS Support\n- [v1.4.258](https://github.com/danielmiessler/fabric/releases/tag/v1.4.258) (Jul 17, 2025) â€” **Onboarding Improved**: Add startup check to initialize config and .env file automatically\n- [v1.4.257](https://github.com/danielmiessler/fabric/releases/tag/v1.4.257) (Jul 17, 2025) â€” **OpenAI Routing Control**: Introduce CLI Flag to Disable OpenAI Responses API\n- [v1.4.252](https://github.com/danielmiessler/fabric/releases/tag/v1.4.252) (Jul 16, 2025) â€” **Hide Thinking Block**: Optional Hiding of Model Thinking Process with Configurable Tags\n- [v1.4.246](https://github.com/danielmiessler/fabric/releases/tag/v1.4.246) (Jul 14, 2025) â€” **Automatic ChangeLog Updates**: Add AI-powered changelog generation with high-performance Go tool and comprehensive caching\n- [v1.4.245](https://github.com/danielmiessler/fabric/releases/tag/v1.4.245) (Jul 11, 2025) â€” **Together AI**: Together AI Support with OpenAI Fallback Mechanism Added\n- [v1.4.232](https://github.com/danielmiessler/fabric/releases/tag/v1.4.232) (Jul 6, 2025) â€” **Add Custom**: Add Custom Patterns Directory Support\n- [v1.4.231](https://github.com/danielmiessler/fabric/releases/tag/v1.4.231) (Jul 5, 2025) â€” **OAuth Auto-Auth**: OAuth Authentication Support for Anthropic (Use your Max Subscription)\n- [v1.4.230](https://github.com/danielmiessler/fabric/releases/tag/v1.4.230) (Jul 5, 2025) â€” **Model Management**: Add advanced image generation parameters for OpenAI models with four new CLI flags\n- [v1.4.227](https://github.com/danielmiessler/fabric/releases/tag/v1.4.227) (Jul 4, 2025) â€” **Add Image**: Add Image Generation Support to Fabric\n- [v1.4.226](https://github.com/danielmiessler/fabric/releases/tag/v1.4.226) (Jul 4, 2025) â€” **Web Search**: OpenAI Plugin Now Supports Web Search Functionality\n- [v1.4.225](https://github.com/danielmiessler/fabric/releases/tag/v1.4.225) (Jul 4, 2025) â€” **Web Search**: Runtime Web Search Control via Command-Line `--search` Flag\n- [v1.4.224](https://github.com/danielmiessler/fabric/releases/tag/v1.4.224) (Jul 1, 2025) â€” **Add code_review**: Add code_review pattern and updates in Pattern_Descriptions\n- [v1.4.222](https://github.com/danielmiessler/fabric/releases/tag/v1.4.222) (Jul 1, 2025) â€” **OpenAI Plugin**: OpenAI Plugin Migrates to New Responses API\n- [v1.4.218](https://github.com/danielmiessler/fabric/releases/tag/v1.4.218) (Jun 27, 2025) â€” **Model Management**: Add Support for OpenAI Search and Research Model Variants\n- [v1.4.217](https://github.com/danielmiessler/fabric/releases/tag/v1.4.217) (Jun 26, 2025) â€” **New YouTube**: New YouTube Transcript Endpoint Added to REST API\n- [v1.4.212](https://github.com/danielmiessler/fabric/releases/tag/v1.4.212) (Jun 23, 2025) â€” **Add Langdock**: Add Langdock AI and enhance generic OpenAI compatible support\n- [v1.4.211](https://github.com/danielmiessler/fabric/releases/tag/v1.4.211) (Jun 19, 2025) â€” **REST API**: REST API and Web UI Now Support Dynamic Pattern Variables\n- [v1.4.210](https://github.com/danielmiessler/fabric/releases/tag/v1.4.210) (Jun 18, 2025) â€” **Add Citations**: Add Citation Support to Perplexity Response\n- [v1.4.208](https://github.com/danielmiessler/fabric/releases/tag/v1.4.208) (Jun 17, 2025) â€” **Add Perplexity**: Add Perplexity AI Provider with Token Limits Support\n- [v1.4.203](https://github.com/danielmiessler/fabric/releases/tag/v1.4.203) (Jun 14, 2025) â€” **Add Amazon Bedrock**: Add support for Amazon Bedrock\n\nThese features represent our commitment to making Fabric the most powerful and flexible AI augmentation framework available!\n\n</details>\n\n## Intro videos\n\nKeep in mind that many of these were recorded when Fabric was Python-based, so remember to use the current [install instructions](#installation) below.\n\n- [Network Chuck](https://www.youtube.com/watch?v=UbDyjIIGaxQ)\n- [David Bombal](https://www.youtube.com/watch?v=vF-MQmVxnCs)\n- [My Own Intro to the Tool](https://www.youtube.com/watch?v=wPEyyigh10g)\n- [More Fabric YouTube Videos](https://www.youtube.com/results?search_query=fabric+ai)\n\n## Navigation\n\n- [`fabric`](#fabric)\n  - [What and why](#what-and-why)\n  - [Updates](#updates)\n    - [Recent Major Features](#recent-major-features)\n  - [Intro videos](#intro-videos)\n  - [Navigation](#navigation)\n  - [Changelog](#changelog)\n  - [Philosophy](#philosophy)\n    - [Breaking problems into components](#breaking-problems-into-components)\n    - [Too many prompts](#too-many-prompts)\n  - [Installation](#installation)\n    - [One-Line Install (Recommended)](#one-line-install-recommended)\n    - [Manual Binary Downloads](#manual-binary-downloads)\n    - [Using package managers](#using-package-managers)\n      - [macOS (Homebrew)](#macos-homebrew)\n      - [Arch Linux (AUR)](#arch-linux-aur)\n      - [Windows](#windows)\n    - [From Source](#from-source)\n    - [Docker](#docker)\n    - [Environment Variables](#environment-variables)\n    - [Setup](#setup)\n    - [Per-Pattern Model Mapping](#per-pattern-model-mapping)\n    - [Add aliases for all patterns](#add-aliases-for-all-patterns)\n      - [Save your files in markdown using aliases](#save-your-files-in-markdown-using-aliases)\n    - [Migration](#migration)\n    - [Upgrading](#upgrading)\n    - [Shell Completions](#shell-completions)\n      - [Quick install (no clone required)](#quick-install-no-clone-required)\n      - [Zsh Completion](#zsh-completion)\n      - [Bash Completion](#bash-completion)\n      - [Fish Completion](#fish-completion)\n  - [Usage](#usage)\n    - [Debug Levels](#debug-levels)\n    - [Extensions](#extensions)\n  - [Our approach to prompting](#our-approach-to-prompting)\n  - [Examples](#examples)\n  - [Just use the Patterns](#just-use-the-patterns)\n    - [Prompt Strategies](#prompt-strategies)\n  - [Custom Patterns](#custom-patterns)\n    - [Setting Up Custom Patterns](#setting-up-custom-patterns)\n    - [Using Custom Patterns](#using-custom-patterns)\n    - [How It Works](#how-it-works)\n  - [Helper Apps](#helper-apps)\n    - [`to_pdf`](#to_pdf)\n    - [`to_pdf` Installation](#to_pdf-installation)\n    - [`code_helper`](#code_helper)\n  - [pbpaste](#pbpaste)\n  - [Web Interface (Fabric Web App)](#web-interface-fabric-web-app)\n  - [Meta](#meta)\n    - [Primary contributors](#primary-contributors)\n    - [Contributors](#contributors)\n\n<br />\n\n## Changelog\n\nFabric is evolving rapidly.\n\nStay current with the latest features by reviewing the [CHANGELOG](./CHANGELOG.md) for all recent changes.\n\n## Philosophy\n\n> AI isn''t a thing; it''s a _magnifier_ of a thing. And that thing is **human creativity**.\n\nWe believe the purpose of technology is to help humans flourish, so when we talk about AI we start with the **human** problems we want to solve.\n\n### Breaking problems into components\n\nOur approach is to break problems into individual pieces (see below) and then apply AI to them one at a time. See below for some examples.\n\n<img width="2078" alt="augmented_challenges" src="https://github.com/danielmiessler/fabric/assets/50654/31997394-85a9-40c2-879b-b347e4701f06">\n\n### Too many prompts\n\nPrompts are good for this, but the biggest challenge I faced in 2023â€”â€”which still exists todayâ€”is **the sheer number of AI prompts out there**. We all have prompts that are useful, but it''s hard to discover new ones, know if they are good or not, _and manage different versions of the ones we like_.\n\nOne of `fabric`''s primary features is helping people collect and integrate prompts, which we call _Patterns_, into various parts of their lives.\n\nFabric has Patterns for all sorts of life and work activities, including:\n\n- Extracting the most interesting parts of YouTube videos and podcasts\n- Writing an essay in your own voice with just an idea as an input\n- Summarizing opaque academic papers\n- Creating perfectly matched AI art prompts for a piece of writing\n- Rating the quality of content to see if you want to read/watch the whole thing\n- Getting summaries of long, boring content\n- Explaining code to you\n- Turning bad documentation into usable documentation\n- Creating social media posts from any content input\n- And a million moreâ€¦\n\n## Installation\n\n### One-Line Install (Recommended)\n\n**Unix/Linux/macOS:**\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/danielmiessler/fabric/main/scripts/installer/install.sh | bash\n```\n\n**Windows PowerShell:**\n\n```powershell\niwr -useb https://raw.githubusercontent.com/danielmiessler/fabric/main/scripts/installer/install.ps1 | iex\n```\n\n> See [scripts/installer/README.md](./scripts/installer/README.md) for custom installation options and troubleshooting.\n\n### Manual Binary Downloads\n\nThe latest release binary archives and their expected SHA256 hashes can be found at <https://github.com/danielmiessler/fabric/releases/latest>\n\n### Using package managers\n\n**NOTE:** using Homebrew or the Arch Linux package managers makes `fabric` available as `fabric-ai`, so add\nthe following alias to your shell startup files to account for this:\n\n```bash\nalias fabric=''fabric-ai''\n```\n\n#### macOS (Homebrew)\n\n`brew install fabric-ai`\n\n#### Arch Linux (AUR)\n\n`yay -S fabric-ai`\n\n#### Windows\n\nUse the official Microsoft supported `Winget` tool:\n\n`winget install danielmiessler.Fabric`\n\n### From Source\n\nTo install Fabric, [make sure Go is installed](https://go.dev/doc/install), and then run the following command.\n\n```bash\n# Install Fabric directly from the repo\ngo install github.com/danielmiessler/fabric/cmd/fabric@latest\n```\n\n### Docker\n\nRun Fabric using pre-built Docker images:\n\n```bash\n# Use latest image from Docker Hub\ndocker run --rm -it kayvan/fabric:latest --version\n\n# Use specific version from GHCR\ndocker run --rm -it ghcr.io/ksylvan/fabric:v1.4.305 --version\n\n# Run setup (first time)\nmkdir -p $HOME/.fabric-config\ndocker run --rm -it -v $HOME/.fabric-config:/root/.config/fabric kayvan/fabric:latest --setup\n\n# Use Fabric with your patterns\ndocker run --rm -it -v $HOME/.fabric-config:/root/.config/fabric kayvan/fabric:latest -p summarize\n\n# Run the REST API server\ndocker run --rm -it -p 8080:8080 -v $HOME/.fabric-config:/root/.config/fabric kayvan/fabric:latest --serve\n```\n\n**Images available at:**\n\n- Docker Hub: [kayvan/fabric](https://hub.docker.com/repository/docker/kayvan/fabric/general)\n- GHCR: [ksylvan/fabric](https://github.com/ksylvan/fabric/pkgs/container/fabric)\n\nSee [scripts/docker/README.md](./scripts/docker/README.md) for building custom images and advanced configuration.\n\n### Environment Variables\n\nYou may need to set some environment variables in your `~/.bashrc` on linux or `~/.zshrc` file on mac to be able to run the `fabric` command. Here is an example of what you can add:\n\nFor Intel based macs or linux\n\n```bash\n# Golang environment variables\nexport GOROOT=/usr/local/go\nexport GOPATH=$HOME/go\n\n# Update PATH to include GOPATH and GOROOT binaries\nexport PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\n```\n\nfor Apple Silicon based macs\n\n```bash\n# Golang environment variables\nexport GOROOT=$(brew --prefix go)/libexec\nexport GOPATH=$HOME/go\nexport PATH=$GOPATH/bin:$GOROOT/bin:$HOME/.local/bin:$PATH\n```\n\n### Setup\n\nNow run the following command\n\n```bash\n# Run the setup to set up your directories and keys\nfabric --setup\n```\n\nIf everything works you are good to go.\n\n### Per-Pattern Model Mapping\n\n You can configure specific models for individual patterns using environment variables\n like `FABRIC_MODEL_PATTERN_NAME=vendor|model`\n\n This makes it easy to maintain these per-pattern model mappings in your shell startup files.\n\n### Add aliases for all patterns\n\nIn order to add aliases for all your patterns and use them directly as commands, for example, `summarize` instead of `fabric --pattern summarize`\nYou can add the following to your `.zshrc` or `.bashrc` file. You\ncan also optionally set the `FABRIC_ALIAS_PREFIX` environment variable\nbefore, if you''d prefer all the fabric aliases to start with the same prefix.\n\n```bash\n# Loop through all files in the ~/.config/fabric/patterns directory\nfor pattern_file in $HOME/.config/fabric/patterns/*; do\n    # Get the base name of the file (i.e., remove the directory path)\n    pattern_name="$(basename "$pattern_file")"\n    alias_name="${FABRIC_ALIAS_PREFIX:-}${pattern_name}"\n\n    # Create an alias in the form: alias pattern_name="fabric --pattern pattern_name"\n    alias_command="alias $alias_name=''fabric --pattern $pattern_name''"\n\n    # Evaluate the alias command to add it to the current shell\n    eval "$alias_command"\ndone\n\nyt() {\n    if [ "$#" -eq 0 ] || [ "$#" -gt 2 ]; then\n        echo "Usage: yt [-t | --timestamps] youtube-link"\n        echo "Use the ''-t'' flag to get the transcript with timestamps."\n        return 1\n    fi\n\n    transcript_flag="--transcript"\n    if [ "$1" = "-t" ] || [ "$1" = "--timestamps" ]; then\n        transcript_flag="--transcript-with-timestamps"\n        shift\n    fi\n    local video_link="$1"\n    fabric -y "$video_link" $transcript_flag\n}\n```\n\nYou can add the below code for the equivalent aliases inside PowerShell by running `notepad $PROFILE` inside a PowerShell window:\n\n```powershell\n# Path to the patterns directory\n$patternsPath = Join-Path $HOME ".config/fabric/patterns"\nforeach ($patternDir in Get-ChildItem -Path $patternsPath -Directory) {\n    # Prepend FABRIC_ALIAS_PREFIX if set; otherwise use empty string\n    $prefix = $env:FABRIC_ALIAS_PREFIX ?? ''''\n    $patternName = "$($patternDir.Name)"\n    $aliasName = "$prefix$patternName"\n    # Dynamically define a function for each pattern\n    $functionDefinition = @"\nfunction $aliasName {\n    [CmdletBinding()]\n    param(\n        [Parameter(ValueFromPipeline = `$true)]\n        [string] `$InputObject,\n\n        [Parameter(ValueFromRemainingArguments = `$true)]\n        [String[]] `$patternArgs\n    )\n\n    begin {\n        # Initialize an array to collect pipeline input\n        `$collector = @()\n    }\n\n    process {\n        # Collect pipeline input objects\n        if (`$InputObject) {\n            `$collector += `$InputObject\n        }\n    }\n\n    end {\n        # Join all pipeline input into a single string, separated by newlines\n        `$pipelineContent = `$collector -join "`n"\n\n        # If there''s pipeline input, include it in the call to fabric\n        if (`$pipelineContent) {\n            `$pipelineContent | fabric --pattern $patternName `$patternArgs\n        } else {\n            # No pipeline input; just call fabric with the additional args\n            fabric --pattern $patternName `$patternArgs\n        }\n    }\n}\n"@\n    # Add the function to the current session\n    Invoke-Expression $functionDefinition\n}\n\n# Define the ''yt'' function as well\nfunction yt {\n    [CmdletBinding()]\n    param(\n        [Parameter()]\n        [Alias("timestamps")]\n        [switch]$t,\n\n        [Parameter(Position = 0, ValueFromPipeline = $true)]\n        [string]$videoLink\n    )\n\n    begin {\n        $transcriptFlag = "--transcript"\n        if ($t) {\n            $transcriptFlag = "--transcript-with-timestamps"\n        }\n    }\n\n    process {\n        if (-not $videoLink) {\n            Write-Error "Usage: yt [-t | --timestamps] youtube-link"\n            return\n        }\n    }\n\n    end {\n        if ($videoLink) {\n            # Execute and allow output to flow through the pipeline\n            fabric -y $videoLink $transcriptFlag\n        }\n    }\n}\n```\n\nThis also creates a `yt` alias that allows you to use `yt https://www.youtube.com/watch?v=4b0iet22VIk` to get transcripts, comments, and metadata.\n\n#### Save your files in markdown using aliases\n\nIf in addition to the above aliases you would like to have the option to save the output to your favorite markdown note vault like Obsidian then instead of the above add the following to your `.zshrc` or `.bashrc` file:\n\n```bash\n# Define the base directory for Obsidian notes\nobsidian_base="/path/to/obsidian"\n\n# Loop through all files in the ~/.config/fabric/patterns directory\nfor pattern_file in ~/.config/fabric/patterns/*; do\n    # Get the base name of the file (i.e., remove the directory path)\n    pattern_name=$(basename "$pattern_file")\n\n    # Remove any existing alias with the same name\n    unalias "$pattern_name" 2>/dev/null\n\n    # Define a function dynamically for each pattern\n    eval "\n    $pattern_name() {\n        local title=\$1\n        local date_stamp=\$(date +''%Y-%m-%d'')\n        local output_path=\"\$obsidian_base/\${date_stamp}-\${title}.md\"\n\n        # Check if a title was provided\n        if [ -n \"\$title\" ]; then\n            # If a title is provided, use the output path\n            fabric --pattern \"$pattern_name\" -o \"\$output_path\"\n        else\n            # If no title is provided, use --stream\n            fabric --pattern \"$pattern_name\" --stream\n        fi\n    }\n    "\ndone\n```\n\nThis will allow you to use the patterns as aliases like in the above for example `summarize` instead of `fabric --pattern summarize --stream`, however if you pass in an extra argument like this `summarize "my_article_title"` your output will be saved in the destination that you set in `obsidian_base="/path/to/obsidian"` in the following format `YYYY-MM-DD-my_article_title.md` where the date gets autogenerated for you.\nYou can tweak the date format by tweaking the `date_stamp` format.\n\n### Migration\n\nIf you have the Legacy (Python) version installed and want to migrate to the Go version, here''s how you do it. It''s basically two steps: 1) uninstall the Python version, and 2) install the Go version.\n\n```bash\n# Uninstall Legacy Fabric\npipx uninstall fabric\n\n# Clear any old Fabric aliases\n(check your .bashrc, .zshrc, etc.)\n# Install the Go version\ngo install github.com/danielmiessler/fabric/cmd/fabric@latest\n# Run setup for the new version. Important because things have changed\nfabric --setup\n```\n\nThen [set your environmental variables](#environment-variables) as shown above.\n\n### Upgrading\n\nThe great thing about Go is that it''s super easy to upgrade. Just run the same command you used to install it in the first place and you''ll always get the latest version.\n\n```bash\ngo install github.com/danielmiessler/fabric/cmd/fabric@latest\n```\n\n### Shell Completions\n\nFabric provides shell completion scripts for Zsh, Bash, and Fish\nshells, making it easier to use the CLI by providing tab completion\nfor commands and options.\n\n#### Quick install (no clone required)\n\nYou can install completions directly via a one-liner:\n\n```bash\ncurl -fsSL https://raw.githubusercontent.com/danielmiessler/Fabric/refs/heads/main/completions/setup-completions.sh | sh\n```\n\nOptional variants:\n\n```bash\n# Dry-run (see actions without changing your system)\ncurl -fsSL https://raw.githubusercontent.com/danielmiessler/Fabric/refs/heads/main/completions/setup-completions.sh | sh -s -- --dry-run\n\n# Override the download source (advanced)\nFABRIC_COMPLETIONS_BASE_URL="https://raw.githubusercontent.com/danielmiessler/Fabric/refs/heads/main/completions" \\n    sh -c "$(curl -fsSL https://raw.githubusercontent.com/danielmiessler/Fabric/refs/heads/main/completions/setup-completions.sh)"\n```\n\n#### Zsh Completion\n\nTo enable Zsh completion:\n\n```bash\n# Copy the completion file to a directory in your $fpath\nmkdir -p ~/.zsh/completions\ncp completions/_fabric ~/.zsh/completions/\n\n# Add the directory to fpath in your .zshrc before compinit\necho ''fpath=(~/.zsh/completions $fpath)'' >> ~/.zshrc\necho ''autoload -Uz compinit && compinit'' >> ~/.zshrc\n```\n\n#### Bash Completion\n\nTo enable Bash completion:\n\n```bash\n# Source the completion script in your .bashrc\necho ''source /path/to/fabric/completions/fabric.bash'' >> ~/.bashrc\n\n# Or copy to the system-wide bash completion directory\nsudo cp completions/fabric.bash /etc/bash_completion.d/\n```\n\n#### Fish Completion\n\nTo enable Fish completion:\n\n```bash\n# Copy the completion file to the fish completions directory\nmkdir -p ~/.config/fish/completions\ncp completions/fabric.fish ~/.config/fish/completions/\n```\n\n## Usage\n\nOnce you have it all set up, here''s how to use it.\n\n```bash\nfabric -h\n```\n\n```plaintext\nUsage:\n  fabric [OPTIONS]\n\nApplication Options:\n  -p, --pattern=                    Choose a pattern from the available patterns\n  -v, --variable=                   Values for pattern variables, e.g. -v=#role:expert -v=#points:30\n  -C, --context=                    Choose a context from the available contexts\n      --session=                    Choose a session from the available sessions\n  -a, --attachment=                 Attachment path or URL (e.g. for OpenAI image recognition messages)\n  -S, --setup                       Run setup for all reconfigurable parts of fabric\n  -t, --temperature=                Set temperature (default: 0.7)\n  -T, --topp=                       Set top P (default: 0.9)\n  -s, --stream                      Stream\n  -P, --presencepenalty=            Set presence penalty (default: 0.0)\n  -r, --raw                         Use the defaults of the model without sending chat options\n                                    (temperature, top_p, etc.). Only affects OpenAI-compatible providers.\n                                    Anthropic models always use smart parameter selection to comply with\n                                    model-specific requirements.\n  -F, --frequencypenalty=           Set frequency penalty (default: 0.0)\n  -l, --listpatterns                List all patterns\n  -L, --listmodels                  List all available models\n  -x, --listcontexts                List all contexts\n  -X, --listsessions                List all sessions\n  -U, --updatepatterns              Update patterns\n  -c, --copy                        Copy to clipboard\n  -m, --model=                      Choose model\n  -V, --vendor=                     Specify vendor for chosen model (e.g., -V "LM Studio" -m openai/gpt-oss-20b)\n      --modelContextLength=         Model context length (only affects ollama)\n  -o, --output=                     Output to file\n      --output-session              Output the entire session (also a temporary one) to the output file\n  -n, --latest=                     Number of latest patterns to list (default: 0)\n  -d, --changeDefaultModel          Change default model\n  -y, --youtube=                    YouTube video or play list "URL" to grab transcript, comments from it\n                                    and send to chat or print it put to the console and store it in the\n                                    output file\n      --playlist                    Prefer playlist over video if both ids are present in the URL\n      --transcript                  Grab transcript from YouTube video and send to chat (it is used per\n                                    default).\n      --transcript-with-timestamps  Grab transcript from YouTube video with timestamps and send to chat\n      --comments                    Grab comments from YouTube video and send to chat\n      --metadata                    Output video metadata\n  -g, --language=                   Specify the Language Code for the chat, e.g. -g=en -g=zh\n  -u, --scrape_url=                 Scrape website URL to markdown using Jina AI\n  -q, --scrape_question=            Search question using Jina AI\n  -e, --seed=                       Seed to be used for LMM generation\n  -w, --wipecontext=                Wipe context\n  -W, --wipesession=                Wipe session\n      --printcontext=               Print context\n      --printsession=               Print session\n      --readability                 Convert HTML input into a clean, readable view\n      --input-has-vars              Apply variables to user input\n      --no-variable-replacement     Disable pattern variable replacement\n      --dry-run                     Show what would be sent to the model without actually sending it\n      --serve                       Serve the Fabric Rest API\n      --serveOllama                 Serve the Fabric Rest API with ollama endpoints\n      --address=                    The address to bind the REST API (default: :8080)\n      --api-key=                    API key used to secure server routes\n      --config=                     Path to YAML config file\n      --version                     Print current version\n      --listextensions              List all registered extensions\n      --addextension=               Register a new extension from config file path\n      --rmextension=                Remove a registered extension by name\n      --strategy=                   Choose a strategy from the available strategies\n      --liststrategies              List all strategies\n      --listvendors                 List all vendors\n      --shell-complete-list         Output raw list without headers/formatting (for shell completion)\n      --search                      Enable web search tool for supported models (Anthropic, OpenAI, Gemini)\n      --search-location=            Set location for web search results (e.g., ''America/Los_Angeles'')\n      --image-file=                 Save generated image to specified file path (e.g., ''output.png'')\n      --image-size=                 Image dimensions: 1024x1024, 1536x1024, 1024x1536, auto (default: auto)\n      --image-quality=              Image quality: low, medium, high, auto (default: auto)\n      --image-compression=          Compression level 0-100 for JPEG/WebP formats (default: not set)\n      --image-background=           Background type: opaque, transparent (default: opaque, only for\n                                    PNG/WebP)\n      --suppress-think              Suppress text enclosed in thinking tags\n      --think-start-tag=            Start tag for thinking sections (default: <think>)\n      --think-end-tag=              End tag for thinking sections (default: </think>)\n      --disable-responses-api       Disable OpenAI Responses API (default: false)\n      --voice=                      TTS voice name for supported models (e.g., Kore, Charon, Puck)\n                                    (default: Kore)\n      --list-gemini-voices          List all available Gemini TTS voices\n      --notification                Send desktop notification when command completes\n      --notification-command=       Custom command to run for notifications (overrides built-in\n                                    notifications)\n      --yt-dlp-args=                Additional arguments to pass to yt-dlp (e.g. ''--cookies-from-browser brave'')\n      --thinking=                   Set reasoning/thinking level (e.g., off, low, medium, high, or\n                                    numeric tokens for Anthropic or Google Gemini)\n      --debug=                     Set debug level (0: off, 1: basic, 2: detailed, 3: trace)\nHelp Options:\n  -h, --help                        Show this help message\n```\n\n### Debug Levels\n\nUse the `--debug` flag to control runtime logging:\n\n- `0`: off (default)\n- `1`: basic debug info\n- `2`: detailed debugging\n- `3`: trace level\n\n### Extensions\n\nFabric supports extensions that can be called within patterns. See the [Extension Guide](internal/plugins/template/Examples/README.md) for complete documentation.\n\n**Important:** Extensions only work within pattern files, not via direct stdin. See the guide for details and examples.\n\n## Our approach to prompting\n\nFabric _Patterns_ are different than most prompts you''ll see.\n\n- **First, we use `Markdown` to help ensure maximum readability and editability**. This not only helps the creator make a good one, but also anyone who wants to deeply understand what it does. _Importantly, this also includes the AI you''re sending it to!_\n\nHere''s an example of a Fabric Pattern.\n\n```bash\nhttps://github.com/danielmiessler/Fabric/blob/main/data/patterns/extract_wisdom/system.md\n```\n\n<img width="1461" alt="pattern-example" src="https://github.com/danielmiessler/fabric/assets/50654/b910c551-9263-405f-9735-71ca69bbab6d">\n\n- **Next, we are extremely clear in our instructions**, and we use the Markdown structure to emphasize what we want the AI to do, and in what order.\n\n- **And finally, we tend to use the System section of the prompt almost exclusively**. In over a year of being heads-down with this stuff, we''ve just seen more efficacy from doing that. If that changes, or we''re shown data that says otherwise, we will adjust.\n\n## Examples\n\n> The following examples use the macOS `pbpaste` to paste from the clipboard. See the [pbpaste](#pbpaste) section below for Windows and Linux alternatives.\n\nNow let''s look at some things you can do with Fabric.\n\n1. Run the `summarize` Pattern based on input from `stdin`. In this case, the body of an article.\n\n    ```bash\n    pbpaste | fabric --pattern summarize\n    ```\n\n2. Run the `analyze_claims` Pattern with the `--stream` option to get immediate and streaming results.\n\n    ```bash\n    pbpaste | fabric --stream --pattern analyze_claims\n    ```\n\n3. Run the `extract_wisdom` Pattern with the `--stream` option to get immediate and streaming results from any      Youtube video (much like in the original introduction video).\n\n    ```bash\n    fabric -y "https://youtube.com/watch?v=uXs-zPc63kM" --stream --pattern extract_wisdom\n    ```\n\n4. Create patterns- you must create a .md file with the pattern and save it to `~/.config/fabric/patterns/[yourpatternname]`.\n\n5. Run a `analyze_claims` pattern on a website. Fabric uses Jina AI to scrape the URL into markdown format before sending it to the model.\n\n    ```bash\n    fabric -u https://github.com/danielmiessler/fabric/ -p analyze_claims\n    ```\n\n## Just use the Patterns\n\n<img width="1173" alt="fabric-patterns-screenshot" src="https://github.com/danielmiessler/fabric/assets/50654/9186a044-652b-4673-89f7-71cf066f32d8">\n\n<br />\n<br />\n\nIf you''re not looking to do anything fancy, and you just want a lot of great prompts, you can navigate to the [`/patterns`](https://github.com/danielmiessler/fabric/tree/main/data/patterns) directory and start exploring!\n\nWe hope that if you used nothing else from Fabric, the Patterns by themselves will make the project useful.\n\nYou can use any of the Patterns you see there in any AI application that you have, whether that''s ChatGPT or some other app or website. Our plan and prediction is that people will soon be sharing many more than those we''ve published, and they will be way better than ours.\n\nThe wisdom of crowds for the win.\n\n### Prompt Strategies\n\nFabric also implements prompt strategies like "Chain of Thought" or "Chain of Draft" which can\nbe used in addition to the basic patterns.\n\nSee the [Thinking Faster by Writing Less](https://arxiv.org/pdf/2502.18600) paper and\nthe [Thought Generation section of Learn Prompting](https://learnprompting.org/docs/advanced/thought_generation/introduction) for examples of prompt strategies.\n\nEach strategy is available as a small `json` file in the [`/strategies`](https://github.com/danielmiessler/fabric/tree/main/data/strategies) directory.\n\nThe prompt modification of the strategy is applied to the system prompt and passed on to the\nLLM in the chat session.\n\nUse `fabric -S` and select the option to install the strategies in your `~/.config/fabric` directory.\n\n## Custom Patterns\n\nYou may want to use Fabric to create your own custom Patternsâ€”but not share them with others. No problem!\n\nFabric now supports a dedicated custom patterns directory that keeps your personal patterns separate from the built-in ones. This means your custom patterns won''t be overwritten when you update Fabric''s built-in patterns.\n\n### Setting Up Custom Patterns\n\n1. Run the Fabric setup:\n\n   ```bash\n   fabric --setup\n   ```\n\n2. Select the "Custom Patterns" option from the Tools menu and enter your desired directory path (e.g., `~/my-custom-patterns`)\n\n3. Fabric will automatically create the directory if it does not exist.\n\n### Using Custom Patterns\n\n1. Create your custom pattern directory structure:\n\n   ```bash\n   mkdir -p ~/my-custom-patterns/my-analyzer\n   ```\n\n2. Create your pattern file\n\n   ```bash\n   echo "You are an expert analyzer of ..." > ~/my-custom-patterns/my-analyzer/system.md\n   ```\n\n3. **Use your custom pattern:**\n\n   ```bash\n   fabric --pattern my-analyzer "analyze this text"\n   ```\n\n### How It Works\n\n- **Priority System**: Custom patterns take precedence over built-in patterns with the same name\n- **Seamless Integration**: Custom patterns appear in `fabric --listpatterns` alongside built-in ones\n- **Update Safe**: Your custom patterns are never affected by `fabric --updatepatterns`\n- **Private by Default**: Custom patterns remain private unless you explicitly share them\n\nYour custom patterns are completely private and won''t be affected by Fabric updates!\n\n## Helper Apps\n\nFabric also makes use of some core helper apps (tools) to make it easier to integrate with your various workflows. Here are some examples:\n\n### `to_pdf`\n\n`to_pdf` is a helper command that converts LaTeX files to PDF format. You can use it like this:\n\n```bash\nto_pdf input.tex\n```\n\nThis will create a PDF file from the input LaTeX file in the same directory.\n\nYou can also use it with stdin which works perfectly with the `write_latex` pattern:\n\n```bash\necho "ai security primer" | fabric --pattern write_latex | to_pdf\n```\n\nThis will create a PDF file named `output.pdf` in the current directory.\n\n### `to_pdf` Installation\n\nTo install `to_pdf`, install it the same way as you install Fabric, just with a different repo name.\n\n```bash\ngo install github.com/danielmiessler/fabric/cmd/to_pdf@latest\n```\n\nMake sure you have a LaTeX distribution (like TeX Live or MiKTeX) installed on your system, as `to_pdf` requires `pdflatex` to be available in your system''s PATH.\n\n### `code_helper`\n\n`code_helper` is used in conjunction with the `create_coding_feature` pattern.\nIt generates a `json` representation of a directory of code that can be fed into an AI model\nwith instructions to create a new feature or edit the code in a specified way.\n\nSee [the Create Coding Feature Pattern README](./data/patterns/create_coding_feature/README.md) for details.\n\nInstall it first using:\n\n```bash\ngo install github.com/danielmiessler/fabric/cmd/code_helper@latest\n```\n\n## pbpaste\n\nThe [examples](#examples) use the macOS program `pbpaste` to paste content from the clipboard to pipe into `fabric` as the input. `pbpaste` is not available on Windows or Linux, but there are alternatives.\n\nOn Windows, you can use the PowerShell command `Get-Clipboard` from a PowerShell command prompt. If you like, you can also alias it to `pbpaste`. If you are using classic PowerShell, edit the file `~\Documents\WindowsPowerShell\.profile.ps1`, or if you are using PowerShell Core, edit `~\Documents\PowerShell\.profile.ps1` and add the alias,\n\n```powershell\nSet-Alias pbpaste Get-Clipboard\n```\n\nOn Linux, you can use `xclip -selection clipboard -o` to paste from the clipboard. You will likely need to install `xclip` with your package manager. For Debian based systems including Ubuntu,\n\n```sh\nsudo apt update\nsudo apt install xclip -y\n```\n\nYou can also create an alias by editing `~/.bashrc` or `~/.zshrc` and adding the alias,\n\n```sh\nalias pbpaste=''xclip -selection clipboard -o''\n```\n\n## Web Interface (Fabric Web App)\n\nFabric now includes a built-in web interface that provides a GUI alternative to the command-line interface. Refer to [Web App README](/web/README.md) for installation instructions and an overview of features.\n\n## Meta\n\n> [!NOTE]\n> Special thanks to the following people for their inspiration and contributions!\n\n- _Jonathan Dunn_ for being the absolute MVP dev on the project, including spearheading the new Go version, as well as the GUI! All this while also being a full-time medical doctor!\n- _Caleb Sima_ for pushing me over the edge of whether to make this a public project or not.\n- _Eugen Eisler_ and _Frederick Ros_ for their invaluable contributions to the Go version\n- _David Peters_ for his work on the web interface.\n- _Joel Parish_ for super useful input on the project''s Github directory structure..\n- _Joseph Thacker_ for the idea of a `-c` context flag that adds pre-created context in the `./config/fabric/` directory to all Pattern queries.\n- _Jason Haddix_ for the idea of a stitch (chained Pattern) to filter content using a local model before sending on to a cloud model, i.e., cleaning customer data using `llama2` before sending on to `gpt-4` for analysis.\n- _Andre Guerra_ for assisting with numerous components to make things simpler and more maintainable.\n\n### Primary contributors\n\n<a href="https://github.com/danielmiessler"><img src="https://avatars.githubusercontent.com/u/50654?v=4" title="Daniel Miessler" width="50" height="50" alt="Daniel Miessler"></a>\n<a href="https://github.com/xssdoctor"><img src="https://avatars.githubusercontent.com/u/9218431?v=4" title="Jonathan Dunn" width="50" height="50" alt="Jonathan Dunn"></a>\n<a href="https://github.com/sbehrens"><img src="https://avatars.githubusercontent.com/u/688589?v=4" title="Scott Behrens" width="50" height="50" alt="Scott Behrens"></a>\n<a href="https://github.com/agu3rra"><img src="https://avatars.githubusercontent.com/u/10410523?v=4" title="Andre Guerra" width="50" height="50" alt="Andre Guerra"></a>\n\n### Contributors\n\n<a href="https://github.com/danielmiessler/fabric/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=danielmiessler/fabric" alt="contrib.rocks" />\n</a>\n\nMade with [contrib.rocks](https://contrib.rocks).\n\n`fabric` was created by <a href="https://danielmiessler.com/subscribe" target="_blank">Daniel Miessler</a> in January of 2024.\n<br /><br />\n<a href="https://twitter.com/intent/user?screen_name=danielmiessler">![X (formerly Twitter) Follow](https://img.shields.io/twitter/follow/danielmiessler)</a>\n', '{"language":"JavaScript","stars":35039,"forks":3571,"watchers":35039,"open_issues":33,"topics":["ai","augmentation","flourishing","life","work"],"default_branch":"main","size_kb":226210,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:ksylvan:fabric","source_url":"https://github.com/ksylvan/fabric"},{"type":"has_code","target_id":"github:danielmiessler:Fabric","source_url":"https://github.com/danielmiessler/Fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"},{"type":"has_code","target_id":"github:danielmiessler:fabric","source_url":"https://github.com/danielmiessler/fabric"}]', NULL, 'MIT', 'approved', 80, '57e9f954beeef0a436d5f55cf23e120b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-danielmiessler-Fabric from https://github.com/danielmiessler.png
Image converted to WebP: data/images/github-danielmiessler-Fabric.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-xitu-gold-miner', 'github--xitu--gold-miner', 'gold-miner', 'xitu', 'æ˜é‡‘ç¿»è¯‘è®¡åˆ’ æ˜¯ä¸€ä¸ªç¿»è¯‘ä¼˜è´¨äº’è”ç½‘æŠ€æœ¯æ–‡ç« çš„ç¤¾åŒºï¼Œæ–‡ç« æ¥æºä¸º æ˜é‡‘ ä¸Šçš„è‹±æ–‡åˆ†äº«æ–‡ç« ã€‚å†…å®¹è¦†ç›–åŒºå—é“¾ã€äººå·¥æ™ºèƒ½ã€Androidã€iOSã€å‰ç«¯ã€åç«¯ã€è®¾è®¡ã€äº§å“ã€ç®—æ³•å’Œå…¶ä»–ç­‰é¢†åŸŸï¼Œä»¥åŠå„å¤§å‹ä¼˜è´¨ å®˜æ–¹æ–‡æ¡£åŠæ‰‹å†Œï¼Œè¯»è€…ä¸ºçƒ­çˆ±æ–°æŠ€æœ¯çš„æ–°é”å¼€å‘è€…ã€‚ æ˜é‡‘ç¿»è¯‘è®¡åˆ’ç›®å‰ç¿»è¯‘å®Œæˆ 4000 ä½™ç¯‡æ–‡ç« ï¼Œå®˜æ–¹æ–‡æ¡£åŠæ‰‹å†Œ 13 ä¸ªï¼Œå…±æœ‰ 1500 ä½™åè¯‘è€…è´¡çŒ®ç¿»è¯‘å’Œæ ¡å¯¹ã€‚ > ## ğŸ¥‡æ˜é‡‘ç¿»è¯‘è®¡åˆ’ â€” åŒºå—é“¾åˆ†èˆµ **æ¨èä¼˜è´¨è‹±æ–‡æ–‡ç« åˆ°æ˜é‡‘ç¿»è¯‘è®¡åˆ’** <!-- https://github.com/xitu/gold-miner/issues/new?title=æ¨èä¼˜ç§€è‹±æ–‡æ–‡ç« &body=-%20åŸæ–‡é“¾æ¥ï¼šæ¨èæ–‡ç« å‰%20Google%20ä¸€ä¸‹ï¼Œå°½é‡ä¿è¯æœ¬æ–‡æœªè¢«ç¿»è¯‘%0A-%20ç®€è¦ä»‹ç»ï¼šä»‹ç»ä¸€ä¸‹å¥½ä¸å¥½å•¦ï¼Œæ¯•ç«Ÿå°ç¼–ä¹Ÿçœ‹ä¸å¤ªæ‡‚å“_(:Ğ·ã€âˆ )_) --> 1. å¦‚ä½•å‚ä¸ç¿»è¯‘ 2. å…³äºå¦‚ä½•æäº¤ç¿»è¯‘ä»¥åŠåç»­æ›´æ–°çš„æ•™ç¨‹ 3. å¦‚ä½•å‚ä¸æ ¡å¯¹åŠæ ¡å¯¹çš„æ­£ç¡®å§¿åŠ¿ 4. æ–‡ç« åˆ†äº«åˆ°æ˜é‡‘æŒ‡å— 5. è¯‘æ–‡æ’ç‰ˆè§„åˆ™æŒ‡åŒ— * å¹´åº¦æ€»ç»“ç³»åˆ— * TensorFlow ä¸­æ–‡æ–‡æ¡£ * The JavaScript Tuto...', '["ai","android","frontend","ios","javascript","react","swift","tensorflow","translation","tutorials"]', 'other', 34303, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/xitu/gold-miner","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# æ˜é‡‘ç¿»è¯‘è®¡åˆ’\n\n[![xitu](https://camo.githubusercontent.com/c9c9db0a39b56738a62332f0791d58b1522fdf82/68747470733a2f2f7261776769742e636f6d2f616c65656e34322f6261646765732f6d61737465722f7372632f786974752e737667)](https://github.com/xitu/gold-miner)\n[![æ˜é‡‘ç¿»è¯‘è®¡åˆ’](https://rawgit.com/aleen42/badges/master/src/juejin_translation.svg)](https://github.com/xitu/gold-miner/)\n[![](https://img.shields.io/badge/weibo-%E6%8E%98%E9%87%91%E7%BF%BB%E8%AF%91%E8%AE%A1%E5%88%92-brightgreen.svg)](http://weibo.com/juejinfanyi)\n[![](https://img.shields.io/badge/%E7%9F%A5%E4%B9%8E%E4%B8%93%E6%A0%8F-%E6%8E%98%E9%87%91%E7%BF%BB%E8%AF%91%E8%AE%A1%E5%88%92-blue.svg)](https://zhuanlan.zhihu.com/juejinfanyi)\n\n[æ˜é‡‘ç¿»è¯‘è®¡åˆ’](https://juejin.im/tag/%E6%8E%98%E9%87%91%E7%BF%BB%E8%AF%91%E8%AE%A1%E5%88%92) æ˜¯ä¸€ä¸ªç¿»è¯‘ä¼˜è´¨äº’è”ç½‘æŠ€æœ¯æ–‡ç« çš„ç¤¾åŒºï¼Œæ–‡ç« æ¥æºä¸º [æ˜é‡‘](https://juejin.im) ä¸Šçš„è‹±æ–‡åˆ†äº«æ–‡ç« ã€‚å†…å®¹è¦†ç›–[åŒºå—é“¾](#åŒºå—é“¾)ã€[äººå·¥æ™ºèƒ½](#ai--deep-learning--machine-learning)ã€[Android](#android)ã€[iOS](#ios)ã€[å‰ç«¯](#å‰ç«¯)ã€[åç«¯](#åç«¯)ã€[è®¾è®¡](#è®¾è®¡)ã€[äº§å“](#äº§å“)ã€[ç®—æ³•](https://github.com/xitu/gold-miner/blob/master/algorithm.md)å’Œ[å…¶ä»–](#å…¶ä»–)ç­‰é¢†åŸŸï¼Œä»¥åŠå„å¤§å‹ä¼˜è´¨ [å®˜æ–¹æ–‡æ¡£åŠæ‰‹å†Œ](#å®˜æ–¹æ–‡æ¡£åŠæ‰‹å†Œ)ï¼Œè¯»è€…ä¸ºçƒ­çˆ±æ–°æŠ€æœ¯çš„æ–°é”å¼€å‘è€…ã€‚\n\næ˜é‡‘ç¿»è¯‘è®¡åˆ’ç›®å‰ç¿»è¯‘å®Œæˆ [4000](#è¿‘æœŸæ–‡ç« åˆ—è¡¨) ä½™ç¯‡æ–‡ç« ï¼Œå®˜æ–¹æ–‡æ¡£åŠæ‰‹å†Œ [13](#å®˜æ–¹æ–‡æ¡£åŠæ‰‹å†Œ) ä¸ªï¼Œå…±æœ‰ [1500](https://github.com/xitu/gold-miner/wiki/%E8%AF%91%E8%80%85%E7%A7%AF%E5%88%86%E8%A1%A8) ä½™åè¯‘è€…è´¡çŒ®ç¿»è¯‘å’Œæ ¡å¯¹ã€‚\n\n> ## [ğŸ¥‡æ˜é‡‘ç¿»è¯‘è®¡åˆ’ â€” åŒºå—é“¾åˆ†èˆµ](https://github.com/xitu/blockchain-miner)\n\n# å®˜æ–¹æŒ‡å—\n\n[**æ¨èä¼˜è´¨è‹±æ–‡æ–‡ç« åˆ°æ˜é‡‘ç¿»è¯‘è®¡åˆ’**](https://github.com/xitu/gold-miner/issues/new/choose)\n\n<!--\nhttps://github.com/xitu/gold-miner/issues/new?title=æ¨èä¼˜ç§€è‹±æ–‡æ–‡ç« &body=-%20åŸæ–‡é“¾æ¥ï¼šæ¨èæ–‡ç« å‰%20Google%20ä¸€ä¸‹ï¼Œå°½é‡ä¿è¯æœ¬æ–‡æœªè¢«ç¿»è¯‘%0A-%20ç®€è¦ä»‹ç»ï¼šä»‹ç»ä¸€ä¸‹å¥½ä¸å¥½å•¦ï¼Œæ¯•ç«Ÿå°ç¼–ä¹Ÿçœ‹ä¸å¤ªæ‡‚å“_(:Ğ·ã€âˆ )_)\n-->\n\n### ç¿»è¯‘è®¡åˆ’è¯‘è€…æ•™ç¨‹\n\n1. [å¦‚ä½•å‚ä¸ç¿»è¯‘](https://github.com/xitu/gold-miner/wiki/%E5%A6%82%E4%BD%95%E5%8F%82%E4%B8%8E%E7%BF%BB%E8%AF%91)\n2. [å…³äºå¦‚ä½•æäº¤ç¿»è¯‘ä»¥åŠåç»­æ›´æ–°çš„æ•™ç¨‹](https://github.com/xitu/gold-miner/wiki/%E5%85%B3%E4%BA%8E%E5%A6%82%E4%BD%95%E6%8F%90%E4%BA%A4%E7%BF%BB%E8%AF%91%E4%BB%A5%E5%8F%8A%E5%90%8E%E7%BB%AD%E6%9B%B4%E6%96%B0%E7%9A%84%E6%95%99%E7%A8%8B)\n3. [å¦‚ä½•å‚ä¸æ ¡å¯¹åŠæ ¡å¯¹çš„æ­£ç¡®å§¿åŠ¿](https://github.com/xitu/gold-miner/wiki/%E5%8F%82%E4%B8%8E%E6%A0%A1%E5%AF%B9%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%A7%BF%E5%8A%BF)\n4. [æ–‡ç« åˆ†äº«åˆ°æ˜é‡‘æŒ‡å—](https://github.com/xitu/gold-miner/wiki/%E5%88%86%E4%BA%AB%E5%88%B0%E6%8E%98%E9%87%91%E6%8C%87%E5%8D%97)\n5. [è¯‘æ–‡æ’ç‰ˆè§„åˆ™æŒ‡åŒ—](https://github.com/xitu/gold-miner/wiki/%E8%AF%91%E6%96%87%E6%8E%92%E7%89%88%E8%A7%84%E5%88%99%E6%8C%87%E5%8C%97)\n\n\n# è¿‘æœŸæ–‡ç« åˆ—è¡¨\n\n## å®˜æ–¹æ–‡æ¡£åŠæ‰‹å†Œ\n\n* [å¹´åº¦æ€»ç»“ç³»åˆ—](https://github.com/xitu/Annual-Survey)\n* [TensorFlow ä¸­æ–‡æ–‡æ¡£](https://github.com/xitu/tensorflow-docs)\n* [The JavaScript Tutorial](https://github.com/xitu/javascript-tutorial-zh)\n* [ML Kit ä¸­æ–‡æ–‡æ¡£](https://github.com/Quorafind/MLkit-CN)\n* [GraphQL ä¸­æ–‡æ–‡æ¡£](https://github.com/xitu/graphql.github.io)\n* [Under-the-hood-ReactJS ç³»åˆ—æ•™ç¨‹](https://github.com/xitu/Under-the-hood-ReactJS)\n* [ç³»ç»Ÿè®¾è®¡å…¥é—¨æ•™ç¨‹](https://github.com/xitu/system-design-primer)\n* [Google Interview University é¢è¯•æŒ‡åŒ—](https://github.com/xitu/google-interview-university)\n* [å‰ç«¯å¼€å‘è€…æŒ‡å—ï¼ˆ2017ï¼‰](https://github.com/xitu/front-end-handbook-2017)\n* [å‰ç«¯å¼€å‘è€…æŒ‡å—ï¼ˆ2018ï¼‰](https://github.com/xitu/front-end-handbook-2018)\n* [Awesome Flutter](https://github.com/xitu/awesome-flutter)\n* [macOS Security and Privacy Guide](https://github.com/xitu/macOS-Security-and-Privacy-Guide)\n* [State of Vue.js report 2017 ä¸­æ–‡ç‰ˆ](https://github.com/xitu/gold-miner/blob/master/TODO/state-of-vue-report-2017.md)\n* [Next.js è½»é‡çº§ React æœåŠ¡ç«¯æ¸²æŸ“åº”ç”¨æ¡†æ¶ä¸­æ–‡æ–‡æ¡£](http://nextjs.frontendx.cn/)\n\n## åŒºå—é“¾\n\n* [å±äº JavaScript å¼€å‘è€…çš„ Crypto ç®€ä»‹](https://juejin.im/post/5ce0c39a51882525f07ef0fa) ([Xuyuey](https://github.com/Xuyuey) ç¿»è¯‘)\n* [æˆ‘ä»¬ä¸ºä»€ä¹ˆçœ‹å¥½åŠ å¯†æ”¶è—å“ï¼ˆNFTï¼‰çš„å‰æ™¯](https://juejin.im/post/5cb87819518825329e7ea61e) ([portandbridge](https://github.com/portandbridge) ç¿»è¯‘)\n* [2019 åŒºå—é“¾å¹³å°ä¸æŠ€æœ¯å±•æœ›](https://juejin.im/post/5c613e6e6fb9a049e4132ba5) ([gs666](https://github.com/gs666) ç¿»è¯‘)\n* [ä»¥å¤ªåŠå…¥é—¨æŒ‡å—](https://juejin.im/post/5c1080fbe51d452b307969a3) ([gs666](https://github.com/gs666) ç¿»è¯‘)\n* [ä»¥å¤ªåŠå…¥é—¨ï¼šäº’è”ç½‘æ”¿åºœ](https://juejin.im/post/5c03c68851882551236eaa82) ([newraina](https://github.com/newraina) ç¿»è¯‘)\n* [æ‰€æœ‰åŒºå—é“¾è¯‘æ–‡>>](https://github.com/xitu/gold-miner/blob/master/blockchain.md)\n\n## äººå·¥æ™ºèƒ½\n\n* [æœºå™¨å­¦ä¹ ç³»ç»Ÿè®¾è®¡ç›¸å…³é¢è¯•é—®é¢˜çš„å‰–æ](https://juejin.cn/post/7109306303285051406)ï¼ˆ[caiyundong](https://github.com/caiyundong) ç¿»è¯‘ï¼‰\n* [å¦‚ä½•ä½¿ç”¨ Python ç®¡é“ Pipe é«˜æ•ˆç¼–ç ](https://juejin.cn/post/7051051681357758494)ï¼ˆ[zenblofe](https://github.com/zenblofe) ç¿»è¯‘ï¼‰\n* [ä½¿ç”¨äººå·¥æ™ºèƒ½/æœºå™¨å­¦ä¹ æ„å»ºæ–‡ç« æ¨èå¼•æ“](https://juejin.cn/post/7001479252163952670)ï¼ˆ[jaredliw](https://github.com/jaredliw) ç¿»è¯‘ï¼‰\n* [AI æ˜¯å¦å·²ç»æˆä¸ºå†…å®¹è¥é”€çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Ÿ](https://juejin.cn/post/6964280632801394724)ï¼ˆ[5Reasons](https://github.com/5Reasons) ç¿»è¯‘ï¼‰\n* [Google çš„ Apollo èŠ¯ç‰‡è®¾è®¡äººå·¥æ™ºèƒ½æ¡†æ¶å°†æ·±åº¦å­¦ä¹ èŠ¯ç‰‡çš„æ€§èƒ½æé«˜äº† 25ï¼…](https://juejin.cn/post/6952819856429285407)ï¼ˆ[PingHGao](https://github.com/PingHGao) ç¿»è¯‘ï¼‰\n* [æ‰€æœ‰ AI è¯‘æ–‡>>](https://github.com/xitu/gold-miner/blob/master/AI.md)\n\n## Android\n\n* [6 æ¡ Jetpack Compose æŒ‡å—å¸®ä½ ä¼˜åŒ– App æ€§èƒ½](https://juejin.cn/post/7153803045418041358)ï¼ˆ[Quincy-Ye](https://github.com/Quincy-Ye) ç¿»è¯‘ï¼‰\n* [React Native å¼€å‘è€…çš„æµè¡Œå­˜å‚¨æ–¹æ¡ˆ](https://juejin.cn/post/7008020729832669191)ï¼ˆ[KimYangOfCat](https://github.com/KimYangOfCat) ç¿»è¯‘ï¼‰\n* [Jetpack Composeï¼šæ ·å¼å’Œä¸»é¢˜ï¼ˆç¬¬äºŒéƒ¨åˆ†ï¼‰](https://juejin.cn/post/6995419287435345934)ï¼ˆ[Kimhooo](https://github.com/Kimhooo) ç¿»è¯‘ï¼‰\n* [æ¢ç´¢ ANDROID 12ï¼šå¯åŠ¨ç”»é¢](https://juejin.cn/post/6983942336824737822)ï¼ˆ[Kimhooo](https://github.com/Kimhooo) ç¿»è¯‘ï¼‰\n* [Jetpack Composeï¼šæ›´ç®€ä¾¿çš„ RecyclerViewï¼ˆç¬¬ä¸€éƒ¨åˆ†ï¼‰](https://juejin.cn/post/6970858140824764424)ï¼ˆ[Kimhooo](https://github.com/Kimhooo) ç¿»è¯‘ï¼‰\n* [æ‰€æœ‰ Android è¯‘æ–‡>>](https://github.com/xitu/gold-miner/blob/master/android.md)\n\n## iOS\n\n* [2021 çš„ SwiftUIï¼šå¥½å¤„ã€åå¤„ä»¥åŠä¸‘å¤„](https://juejin.cn/post/7140825514108780580)ï¼ˆ[earthaYan](https://github.com/earthaYan) ç¿»è¯‘ï¼‰\n* [4 ä¸ªé²œä¸ºäººçŸ¥çš„ Swift ç‰¹æ€§](https://juejin.cn/post/7069326429397205005)ï¼ˆ[jaredliw](https://github.com/jaredliw) ç¿»è¯‘ï¼‰\n* [React Native å¼€å‘è€…çš„æµè¡Œå­˜å‚¨æ–¹æ¡ˆ](https://juejin.cn/post/7008020729832669191)ï¼ˆ[KimYangOfCat](https://github.com/KimYangOfCat) ç¿»è¯‘ï¼‰\n* [é€†å‘ `.car` æ–‡ä»¶ï¼ˆå·²ç¼–è¯‘çš„ Asset Catalogsï¼‰](https://juejin.cn/post/7002491722550919198)ï¼ˆ[LoneyIsError](https://github.com/LoneyIsError) ç¿»è¯‘ï¼‰\n* [Swift ä¸­çš„å†…å­˜å¸ƒå±€](https://juejin.cn/post/6986520506002472973)ï¼ˆ[LoneyIsError](https://github.com/LoneyIsError) ç¿»è¯‘ï¼‰\n* [æ‰€æœ‰ iOS è¯‘æ–‡>>](https://github.com/xitu/gold-miner/blob/master/ios.md)\n\n## å‰ç«¯\n\n* [å…¨é¢åˆ¨æ CSS-in-JS](https://juejin.cn/post/7172360607201493029)ï¼ˆ[Tong-H](https://github.com/Tong-H) ç¿»è¯‘ï¼‰\n* [WebRTC ä¸ WebSockets æ•™ç¨‹ â€” Web ç«¯çš„å®æ—¶é€šä¿¡](https://juejin.cn/post/7138015673850003493)ï¼ˆ[DylanXie123](https://github.com/DylanXie123) ç¿»è¯‘ï¼‰\n* [ES2022 æœ‰ä»€ä¹ˆæ–°ç‰¹æ€§ï¼Ÿ](https://juejin.cn/post/7114676836851777566)ï¼ˆ[CarlosChenN](https://github.com/CarlosChenN) ç¿»è¯‘ï¼‰\n* [ä½œä¸ºä¸€åå‰ç«¯å·¥ç¨‹å¸ˆæˆ‘æµªè´¹æ—¶é—´å­¦ä¹ äº†è¿™äº›æŠ€æœ¯](https://juejin.cn/post/7086019601372282888)ï¼ˆ[airfri](https://github.com/airfri) ç¿»è¯‘ï¼‰\n* [è¿‡åº¦ä½¿ç”¨æ‡’åŠ è½½å¯¹ Web æ€§èƒ½çš„å½±å“](https://juejin.cn/post/7074759905197948935)ï¼ˆ[Tong-H](https://github.com/Tong-H) ç¿»è¯‘ï¼‰\n* [å¦‚ä½•åœ¨ç½‘é¡µä¸­ä½¿ç”¨å“åº”å¼å›¾åƒ](https://juejin.cn/post/7074199947477778439)ï¼ˆ[zenblofe](https://github.com/zenblofe) ç¿»è¯‘ï¼‰\n* [å¦‚ä½•ç¼–å†™æ›´ç®€æ´ä¼˜é›…çš„ React ä»£ç ](https://juejin.cn/post/7070479272380465166)ï¼ˆ[zenblofe](https://github.com/zenblofe) ç¿»è¯‘ï¼‰\n* [ç”¨ PNPM Workspaces æ›¿æ¢ Lerna + Yarn](https://juejin.cn/post/7071992448511279141)ï¼ˆ[CarlosChenN](https://github.com/CarlosChenN) ç¿»è¯‘ï¼‰\n* [æ‰€æœ‰å‰ç«¯è¯‘æ–‡>>](https://github.com/xitu/gold-miner/blob/master/front-end.md)\n\n## åç«¯\n\n* [å®ç° Bitcask ï¼Œä¸€ç§æ—¥å¿—ç»“æ„çš„å“ˆå¸Œè¡¨](https://juejin.cn/post/7174345557861728292)ï¼ˆ[wangxuanni](https://github.com/wangxuanni) ç¿»è¯‘ï¼‰\n* [ç”¨ Isabelle/HOL éªŒè¯åˆ†å¸ƒå¼ç³»ç»Ÿ](https://juejin.cn/post/7166450887626326030)ï¼ˆ[wangxuanni](https://github.com/wangxuanni) ç¿»è¯‘ï¼‰\n* [åå¤§ Java è¯­è¨€ç‰¹æ€§](https://juejin.cn/post/7140097107000000520)ï¼ˆ[jaredliw](https://github.com/jaredliw) ç¿»è¯‘ï¼‰\n* [ä½¿ç”¨ä»¤ç‰Œæ¡¶å’Œç†”æ–­å™¨è¿›è¡Œé‡è¯•](https://juejin.cn/post/7153093426446237727)ï¼ˆ[wangxuanni](https://github.com/wangxuanni) ç¿»è¯‘ï¼‰\n* [WebRTC ä¸ WebSockets æ•™ç¨‹ â€” Web ç«¯çš„å®æ—¶é€šä¿¡](https://juejin.cn/post/7138015673850003493)ï¼ˆ[DylanXie123](https://github.com/DylanXie123) ç¿»è¯‘ï¼‰\n* [å¾®æœåŠ¡æ¶æ„ä½•æ—¶ä¼šæ˜¯ä¸€ç§åé€‰æ‹©](https://juejin.cn/post/7135364257918484488)ï¼ˆ[DylanXie123](https://github.com/DylanXie123) ç¿»è¯‘ï¼‰\n* [å¦‚ä½•ä½¿ç”¨ Python ä¸­çš„ PyPA setuptools æ‰“åŒ…å’Œéƒ¨ç½² CLI åº”ç”¨ç¨‹åº](https://juejin.cn/post/7125323312321789989)ï¼ˆ[haiyang-tju](https://github.com/haiyang-tju) ç¿»è¯‘ï¼‰\n* [10 ä¸ªæœ€éš¾çš„ Python é—®é¢˜](https://juejin.cn/post/7124285689717325831)ï¼ˆ[jaredliw](https://github.com/jaredliw) ç¿»è¯‘ï¼‰\n* [æ‰€æœ‰åç«¯è¯‘æ–‡>>](https://github.com/xitu/gold-miner/blob/master/backend.md)\n\n## è®¾è®¡\n\n* [5ä¸ªå…³äº UI è®¾è®¡ç³»ç»Ÿçš„è¯¯è§£](https://juejin.cn/post/7086291006286462990)ï¼ˆ[CarlosChenN](https://github.com/CarlosChenN) ç¿»è¯‘ï¼‰\n* [åˆ«è®©è½®æ’­æ¯äº†ä½ çš„åº”ç”¨ç¨‹åº](https://juejin.cn/post/7003637296050225189)ï¼ˆ[jaredliw](https://github.com/jaredliw) ç¿»è¯‘ï¼‰\n* [ä¸º Web å¼€å‘åŒå­¦å‡†å¤‡çš„ 11 ä¸ªç®€å•å®ç”¨çš„ UI è®¾è®¡å°æŠ€å·§](https://juejin.cn/post/6960922956876742669)ï¼ˆ[5Reasons](https://github.com/5Reasons) ç¿»è¯‘ï¼‰\n* [ä½ æœ‰è®¾è®¡ä½œå“çš„ä½œå“é›†å—ï¼ŸæŒºå¥½çš„ï¼Œä½†è¿™è¿˜ä¸å¤Ÿ](https://juejin.cn/post/6934328263011467277)ï¼ˆ[PassionPenguin](https://github.com/PassionPenguin) ç¿»è¯‘ï¼‰\n* [æ„å»ºè®¾è®¡ç³»ç»Ÿå’Œç»„ä»¶åº“](https://juejin.cn/post/6924152501805678606)ï¼ˆ[Charlo-O](https://github.com/Charlo-O) ç¿»è¯‘ï¼‰\n* [æ‰€æœ‰è®¾è®¡è¯‘æ–‡>>](https://github.com/xitu/gold-miner/blob/master/design.md)\n\n## äº§å“\n\n* [Github Actions æ˜¯å¦‚ä½•æ¸²æŸ“è¶…å¤§æ—¥å¿—çš„](https://juejin.cn/post/6966082485226569759)ï¼ˆ[felixliao](https://github.com/felixliao) ç¿»è¯‘ï¼‰\n* [ç®—æ³•ä¸æ˜¯äº§å“](https://juejin.im/post/5e398e806fb9a07cb52bb462)ï¼ˆ[fireairforce](https://github.com/fireairforce) ç¿»è¯‘ï¼‰\n* [åˆ©ç”¨ 84 ç§è®¤çŸ¥åè§è®¾è®¡æ›´å¥½çš„äº§å“ â€”â€” ç¬¬ä¸‰éƒ¨åˆ†](https://juejin.im/post/5d568c9ce51d453bc64801cd)ï¼ˆ[JalanJiang](https://github.com/JalanJiang) ç¿»è¯‘ï¼‰\n* [æƒ³å¸®åŠ©ç”¨æˆ·åšå†³å®šï¼Ÿä½ çš„ APP å¯ä»¥è¿™æ ·è®¾è®¡ï¼](https://juejin.im/post/5a7194986fb9a01c9f5bbbb2)ï¼ˆ[pthtc](https://github.com/pthtc) ç¿»è¯‘ï¼‰\n* [åˆ©ç”¨ 84 ç§è®¤çŸ¥åè§è®¾è®¡æ›´å¥½çš„äº§å“ â€”â€” ç¬¬äºŒéƒ¨åˆ†](https://juejin.im/post/5d37e1816fb9a07ee1696a4e)ï¼ˆ[JalanJiang](https://github.com/JalanJiang) ç¿»è¯‘ï¼‰\n* [æ‰€æœ‰äº§å“è¯‘æ–‡>>](https://github.com/xitu/gold-miner/blob/master/product.md)\n\n## å…¶ä»–\n\n* [è‡ªåŠ¨åŒ–æµ‹è¯•ï¼šä½ åº”å½“äº†è§£çš„ä¸€åˆ‡](https://juejin.cn/post/7084071159821500447)ï¼ˆ[samyu2000](https://github.com/samyu2000) ç¿»è¯‘ï¼‰\n* [ä½¿ç”¨äº†ä¸‰ä¸ªæœˆçš„ Github Copilotï¼Œè¿™æ˜¯æˆ‘çš„ä¸€äº›çœ‹æ³•â€¦â€¦](https://juejin.cn/post/7067817036738461732)ï¼ˆ[jaredliw](https://github.com/jaredliw) ç¿»è¯‘ï¼‰\n* [5 ä¸ªæœ‰è¶£çš„åŸå› å‘Šè¯‰ä½ ï¼šæ‰¾å¯¹è±¡å°±å¾—æ‰¾ç¨‹åºå‘˜ï¼](https://juejin.cn/post/7053326045352558599)ï¼ˆ[jaredliw](https://github.com/jaredliw) ç¿»è¯‘ï¼‰\n* [WasmEdge çš„å®‰è£…ä¸å¸è½½](https://github.com/xitu/gold-miner/blob/master/article/2022/Install-and-uninstall-WasmEdge.md)ï¼ˆ[jaredliw](https://github.com/jaredliw) ç¿»è¯‘ï¼‰\n* [ä½¿ç”¨ Python æ¨¡æ‹Ÿå®ç°è¡Œæ˜Ÿé™…ç©ºé—´æ—…è¡Œ](https://juejin.cn/post/7047685861365776414)ï¼ˆ[zenblofe](https://github.com/zenblofe) ç¿»è¯‘ï¼‰\n* [æ‰€æœ‰å…¶ä»–åˆ†ç±»è¯‘æ–‡>>](https://github.com/xitu/gold-miner/blob/master/others.md)\n\n# Copyright\n\n> **ç‰ˆæƒå£°æ˜ï¼š**[æ˜é‡‘ç¿»è¯‘è®¡åˆ’](https://github.com/xitu/gold-miner)è¯‘æ–‡ä»…ç”¨äºå­¦ä¹ ã€ç ”ç©¶å’Œäº¤æµã€‚ç‰ˆæƒå½’[æ˜é‡‘ç¿»è¯‘è®¡åˆ’](https://github.com/xitu/gold-miner/)ã€æ–‡ç« ä½œè€…å’Œè¯‘è€…æ‰€æœ‰ï¼Œæ¬¢è¿éå•†ä¸šè½¬è½½ã€‚è½¬è½½å‰è¯·è”ç³»è¯‘è€…æˆ–[ç®¡ç†å‘˜](https://user-images.githubusercontent.com/8282645/118856035-10a49d80-b909-11eb-8561-00a5a16bd58a.png)è·å–æˆæƒï¼Œå¹¶åœ¨æ–‡ç« å¼€å¤´æ˜æ˜¾ä½ç½®æ³¨æ˜æœ¬æ–‡å‡ºå¤„ã€è¯‘è€…ã€æ ¡å¯¹è€…å’Œæ˜é‡‘ç¿»è¯‘è®¡åˆ’çš„å®Œæ•´é“¾æ¥ï¼Œè¿è€…å¿…ç©¶ã€‚\n\n# åˆä½œä¼™ä¼´\n\n<a href="http://www.ituring.com.cn/" target="_blank"><img src="https://i.loli.net/2018/03/21/5ab1c8723d6de.jpg" width="130px;"/></a>\n\n', '{"language":null,"stars":34303,"forks":5036,"watchers":34303,"open_issues":12,"topics":["ai","android","frontend","ios","javascript","react","swift","tensorflow","translation","tutorials"],"default_branch":"master","size_kb":80499,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:blockchain-miner","source_url":"https://github.com/xitu/blockchain-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:Annual-Survey","source_url":"https://github.com/xitu/Annual-Survey"},{"type":"has_code","target_id":"github:xitu:tensorflow-docs","source_url":"https://github.com/xitu/tensorflow-docs"},{"type":"has_code","target_id":"github:xitu:javascript-tutorial-zh","source_url":"https://github.com/xitu/javascript-tutorial-zh"},{"type":"has_code","target_id":"github:Quorafind:MLkit-CN","source_url":"https://github.com/Quorafind/MLkit-CN"},{"type":"has_code","target_id":"github:xitu:graphql.github.io","source_url":"https://github.com/xitu/graphql.github.io"},{"type":"has_code","target_id":"github:xitu:Under-the-hood-ReactJS","source_url":"https://github.com/xitu/Under-the-hood-ReactJS"},{"type":"has_code","target_id":"github:xitu:system-design-primer","source_url":"https://github.com/xitu/system-design-primer"},{"type":"has_code","target_id":"github:xitu:google-interview-university","source_url":"https://github.com/xitu/google-interview-university"},{"type":"has_code","target_id":"github:xitu:front-end-handbook-2017","source_url":"https://github.com/xitu/front-end-handbook-2017"},{"type":"has_code","target_id":"github:xitu:front-end-handbook-2018","source_url":"https://github.com/xitu/front-end-handbook-2018"},{"type":"has_code","target_id":"github:xitu:awesome-flutter","source_url":"https://github.com/xitu/awesome-flutter"},{"type":"has_code","target_id":"github:xitu:macOS-Security-and-Privacy-Guide","source_url":"https://github.com/xitu/macOS-Security-and-Privacy-Guide"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"},{"type":"has_code","target_id":"github:xitu:gold-miner","source_url":"https://github.com/xitu/gold-miner"}]', NULL, NULL, 'pending', 70, 'b3562f549c6eae1ac4c217eef8fbc43a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-xitu-gold-miner from https://github.com/xitu.png
Image converted to WebP: data/images/github-xitu-gold-miner.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Pythagora-io-gpt-pilot', 'github--pythagora-io--gpt-pilot', 'gpt-pilot', 'Pythagora-io', '<div align="center"> </div> --- <div align="center"> </div> --- <div align="center"> <a href="https://www.ycombinator.com/" target="_blank"><img src="https://s3.amazonaws.com/assets.pythagora.ai/yc/PNG/Black.png" alt="Pythagora-io%2Fgpt-pilot | Trendshift" style="width: 250px; height: 93px;"/></a> </div> <br> <div align="center"> <a href="https://trendshift.io/repositories/466" target="_blank"><img src="https://trendshift.io/api/badge/repositories/466" alt="Pythagora-io%2Fgpt-pilot | Trendshi...', '["ai","codegen","coding-assistant","developer-tools","gpt-4","research-project","python"]', 'other', 33689, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Pythagora-io/gpt-pilot","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n\n# ğŸ§‘â€âœˆï¸ GPT PILOT ğŸ§‘â€âœˆï¸\n\n</div>\n\n---\n\n<div align="center">\n\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-5865F2?style=social&logo=discord)](https://discord.gg/HaqXugmxr9)\n[![GitHub Repo stars](https://img.shields.io/github/stars/Pythagora-io/gpt-pilot?style=social)](https://github.com/Pythagora-io/gpt-pilot)\n[![Twitter Follow](https://img.shields.io/twitter/follow/PythagoraAI?style=social)](https://x.com/PythagoraAI)\n\n</div>\n\n---\n\n<div align="center">\n<a href="https://www.ycombinator.com/" target="_blank"><img src="https://s3.amazonaws.com/assets.pythagora.ai/yc/PNG/Black.png" alt="Pythagora-io%2Fgpt-pilot | Trendshift" style="width: 250px; height: 93px;"/></a>\n</div>\n<br>\n<div align="center">\n<a href="https://trendshift.io/repositories/466" target="_blank"><img src="https://trendshift.io/api/badge/repositories/466" alt="Pythagora-io%2Fgpt-pilot | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n</div>\n\n<br>\n<br>\n\n<div align="center">\n   \n### GPT Pilot doesn''t just generate code, it builds apps!\n\n</div>\n\n<div align="center">\n\nThis repo is not being maintained anymore.\n\n# Visit [Pythagora.ai](https://www.pythagora.ai/) for more info\n\n</div>\n\n---\n<div align="center">\n\n[![See it in action](https://img.youtube.com/vi/o1nEvwjKziw/0.jpg)]([https://youtu.be/4g-1cPGK0GA](https://www.youtube.com/watch?v=o1nEvwjKziw))\n\n(click to open the video in YouTube) (1:04min)\n\n</div>\n\n---\n\n<div align="center">\n\n<a href="https://marketplace.visualstudio.com/items?itemName=PythagoraTechnologies.pythagora-vs-code" target="_blank"><img src="https://github.com/Pythagora-io/gpt-pilot/assets/10895136/5792143e-77c7-47dd-ad96-6902be1501cd" alt="Pythagora-io%2Fgpt-pilot | Trendshift" style="width: 185px; height: 55px;" width="185" height="55"/></a>\n\n</div>\n\nGPT Pilot is the core technology for the [Pythagora VS Code extension](https://marketplace.visualstudio.com/items?itemName=PythagoraTechnologies.pythagora-vs-code) that aims to provide **the first real AI developer companion**. Not just an autocomplete or a helper for PR messages but rather a real AI developer that can write full features, debug them, talk to you about issues, ask for review, etc.\n\n---\n\nğŸ“« If you would like to get updates on future releases or just get in touch, join our [Discord server](https://discord.gg/HaqXugmxr9) or you [can add your email here](http://eepurl.com/iD6Mpo). ğŸ“¬\n\n---\n\n<!-- TOC -->\n* [ğŸ”Œ Requirements](#-requirements)\n* [ğŸš¦How to start using gpt-pilot?](#how-to-start-using-gpt-pilot)\n* [ğŸ” Examples](#-examples)\n* [ğŸ³ How to start gpt-pilot in docker?](#-how-to-start-gpt-pilot-in-docker)\n* [ğŸ§‘â€ğŸ’»ï¸ CLI arguments](#-cli-arguments)\n* [ğŸ— How GPT Pilot works?](#-how-gpt-pilot-works)\n* [ğŸ•´How''s GPT Pilot different from _Smol developer_ and _GPT engineer_?](#hows-gpt-pilot-different-from-smol-developer-and-gpt-engineer)\n* [ğŸ» Contributing](#-contributing)\n* [ğŸ”— Connect with us](#-connect-with-us)\n* [ğŸŒŸ Star history](#-star-history)\n<!-- TOC -->\n\n---\n\nGPT Pilot aims to research how much LLMs can be utilized to generate fully working, production-ready apps while the developer oversees the implementation.\n\n**The main idea is that AI can write most of the code for an app (maybe 95%), but for the rest, 5%, a developer is and will be needed until we get full AGI**.\n\nIf you are interested in our learnings during this project, you can check [our latest blog posts](https://blog.pythagora.ai/2024/02/19/gpt-pilot-what-did-we-learn-in-6-months-of-working-on-a-codegen-pair-programmer/).\n\n---\n\n<br>\n\n<div align="center">\n\n### **[ğŸ‘‰ Examples of apps written by GPT Pilot ğŸ‘ˆ](https://github.com/Pythagora-io/gpt-pilot/wiki/Apps-created-with-GPT-Pilot)**\n\n</div>\n<br>\n\n---\n\n# ğŸ”Œ Requirements\n\n- **Python 3.9+**\n\n# ğŸš¦How to start using gpt-pilot?\nğŸ‘‰ If you are using VS Code as your IDE, the easiest way to start is by downloading [GPT Pilot VS Code extension](https://bit.ly/3IeZxp6). ğŸ‘ˆ\n\nOtherwise, you can use the CLI tool.\n\n### If you''re new to GPT Pilot:\n\nAfter you have Python and (optionally) PostgreSQL installed, follow these steps:\n\n1. `git clone https://github.com/Pythagora-io/gpt-pilot.git` (clone the repo)\n2. `cd gpt-pilot` (go to the repo folder)\n3. `python3 -m venv venv` (create a virtual environment)\n4. `source venv/bin/activate` (or on Windows `venv\Scripts\activate`) (activate the virtual environment)\n5. `pip install -r requirements.txt` (install the dependencies)\n6. `cp example-config.json config.json` (create `config.json` file)\n7. Set your key and other settings in `config.json` file:\n   - LLM Provider (`openai`, `anthropic` or `groq`) key and endpoints (leave `null` for default) (note that Azure and OpenRouter are suppored via the `openai` setting)\n   - Your API key (if `null`, will be read from the environment variables)\n   - database settings: sqlite is used by default, PostgreSQL should also work\n   - optionally update `fs.ignore_paths` and add files or folders which shouldn''t be tracked by GPT Pilot in workspace, useful to ignore folders created by compilers\n8. `python main.py` (start GPT Pilot)\n\nAll generated code will be stored in the folder `workspace` inside the folder named after the app name you enter upon starting the pilot.\n\n# ğŸ” [Examples](https://github.com/Pythagora-io/gpt-pilot/wiki/Apps-created-with-GPT-Pilot)\n\n[Click here](https://github.com/Pythagora-io/gpt-pilot/wiki/Apps-created-with-GPT-Pilot) to see all example apps created with GPT Pilot.\n\n### PostgreSQL support\n\nGPT Pilot uses built-in SQLite database by default. If you want to use the PostgreSQL database, you need to additional install `asyncpg` and `psycopg2` packages:\n\n```bash\npip install asyncpg psycopg2\n```\n\nThen, you need to update the `config.json` file to set `db.url` to `postgresql+asyncpg://<user>:<password>@<db-host>/<db-name>`.\n\n# ğŸ§‘â€ğŸ’»ï¸ CLI arguments\n\n### List created projects (apps)\n\n```bash\npython main.py --list\n```\n\nNote: for each project (app), this also lists "branches". Currently we only support having one branch (called "main"), and in the future we plan to add support for multiple project branches.\n\n### Load and continue from the latest step in a project (app)\n\n```bash\npython main.py --project <app_id>\n```\n\n### Load and continue from a specific step in a project (app)\n\n```bash\npython main.py --project <app_id> --step <step>\n```\n\nWarning: this will delete all progress after the specified step!\n\n### Delete project (app)\n\n```bash\npython main.py --delete <app_id>\n```\n\nDelete project with the specified `app_id`. Warning: this cannot be undone!\n\n### Other command-line options\n\nThere are several other command-line options that mostly support calling GPT Pilot from our VSCode extension. To see all the available options, use the `--help` flag:\n\n```bash\npython main.py --help\n```\n\n# ğŸ— How GPT Pilot works?\nHere are the steps GPT Pilot takes to create an app:\n\n1. You enter the app name and the description.\n2. **Product Owner agent** like in real life, does nothing. :)\n3. **Specification Writer agent** asks a couple of questions to understand the requirements better if project description is not good enough.\n4. **Architect agent** writes up technologies that will be used for the app and checks if all technologies are installed on the machine and installs them if not.\n5. **Tech Lead agent** writes up development tasks that the Developer must implement.\n6. **Developer agent** takes each task and writes up what needs to be done to implement it. The description is in human-readable form.\n7. **Code Monkey agent** takes the Developer''s description and the existing file and implements the changes.\n8. **Reviewer agent** reviews every step of the task and if something is done wrong Reviewer sends it back to Code Monkey.\n9. **Troubleshooter agent** helps you to give good feedback to GPT Pilot when something is wrong.\n10. **Debugger agent** hate to see him, but he is your best friend when things go south.\n11. **Technical Writer agent** writes documentation for the project.\n\n<br>\n\n# ğŸ•´How''s GPT Pilot different from _Smol developer_ and _GPT engineer_?\n\n- **GPT Pilot works with the developer to create a fully working production-ready app** - I don''t think AI can (at least in the near future) create apps without a developer being involved. So, **GPT Pilot codes the app step by step** just like a developer would in real life. This way, it can debug issues as they arise throughout the development process. If it gets stuck, you, the developer in charge, can review the code and fix the issue. Other similar tools give you the entire codebase at once - this way, bugs are much harder to fix for AI and for you as a developer.\n  <br><br>\n- **Works at scale** - GPT Pilot isn''t meant to create simple apps but rather so it can work at any scale. It has mechanisms that filter out the code, so in each LLM conversation, it doesn''t need to store the entire codebase in context, but it shows the LLM only the relevant code for the current task it''s working on. Once an app is finished, you can continue working on it by writing instructions on what feature you want to add.\n\n# ğŸ» Contributing\nIf you are interested in contributing to GPT Pilot, join [our Discord server](https://discord.gg/HaqXugmxr9), check out open [GitHub issues](https://github.com/Pythagora-io/gpt-pilot/issues), and see if anything interests you. We would be happy to get help in resolving any of those. The best place to start is by reviewing blog posts mentioned above to understand how the architecture works before diving into the codebase.\n\n## ğŸ–¥ Development\nOther than the research, GPT Pilot needs to be debugged to work in different scenarios. For example, we realized that the quality of the code generated is very sensitive to the size of the development task. When the task is too broad, the code has too many bugs that are hard to fix, but when the development task is too narrow, GPT also seems to struggle in getting the task implemented into the existing code.\n\n## ğŸ“Š Telemetry\nTo improve GPT Pilot, we are tracking some events from which you can opt out at any time. You can read more about it [here](./docs/TELEMETRY.md).\n\n# ğŸ”— Connect with us\nğŸŒŸ As an open-source tool, it would mean the world to us if you starred the GPT-pilot repo ğŸŒŸ\n\nğŸ’¬ Join [the Discord server](https://discord.gg/HaqXugmxr9) to get in touch.\n', '{"language":"Python","stars":33689,"forks":3491,"watchers":33689,"open_issues":239,"topics":["ai","codegen","coding-assistant","developer-tools","gpt-4","research-project"],"default_branch":"main","size_kb":52680,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:Pythagora-io:gpt-pilot","source_url":"https://github.com/Pythagora-io/gpt-pilot"},{"type":"has_code","target_id":"github:Pythagora-io:gpt-pilot","source_url":"https://github.com/Pythagora-io/gpt-pilot"},{"type":"has_code","target_id":"github:Pythagora-io:gpt-pilot","source_url":"https://github.com/Pythagora-io/gpt-pilot"},{"type":"has_code","target_id":"github:Pythagora-io:gpt-pilot.git`","source_url":"https://github.com/Pythagora-io/gpt-pilot.git`"},{"type":"has_code","target_id":"github:Pythagora-io:gpt-pilot","source_url":"https://github.com/Pythagora-io/gpt-pilot"},{"type":"has_code","target_id":"github:Pythagora-io:gpt-pilot","source_url":"https://github.com/Pythagora-io/gpt-pilot"},{"type":"has_code","target_id":"github:Pythagora-io:gpt-pilot","source_url":"https://github.com/Pythagora-io/gpt-pilot"}]', NULL, 'NOASSERTION', 'approved', 80, '2a5979735cae2f1d564cd536af25f49f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Pythagora-io-gpt-pilot from https://github.com/Pythagora-io.png
Image converted to WebP: data/images/github-Pythagora-io-gpt-pilot.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-TabbyML-tabby', 'github--tabbyml--tabby', 'tabby', 'TabbyML', '<div align="center"> ğŸ“š Docs â€¢ ğŸ’¬ Slack â€¢ ğŸ—ºï¸ Roadmap English | ç®€ä½“ä¸­æ–‡ | æ—¥æœ¬èª </div> Tabby is a self-hosted AI coding assistant, offering an open-source and on-premises alternative to GitHub Copilot. It boasts several key features: * Self-contained, with no need for a DBMS or cloud service. * OpenAPI interface, easy to integrate with existing infrastructure (e.g Cloud IDE). * Supports consumer-grade GPUs. <p align="center"> <a target="_blank" href="https://tabby.tabbyml.com"><img alt="Open Live ...', '["ai","codegen","coding-assistant","coding-language","developer-experience","developer-tools","gen-ai","ide","llms","rust"]', 'other', 32547, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/TabbyML/tabby","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n  \n# ğŸ¾ Tabby\n\n[ğŸ“š Docs](https://tabby.tabbyml.com/docs/welcome/) â€¢ [ğŸ’¬ Slack](https://links.tabbyml.com/join-slack) â€¢ [ğŸ—ºï¸ Roadmap](https://tabby.tabbyml.com/docs/roadmap/)\n\n[![latest release](https://shields.io/github/v/release/TabbyML/tabby)](https://github.com/TabbyML/tabby/releases/latest)\n[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](https://makeapullrequest.com)\n[![Docker pulls](https://img.shields.io/docker/pulls/tabbyml/tabby)](https://hub.docker.com/r/tabbyml/tabby)\n[![codecov](https://codecov.io/gh/TabbyML/tabby/graph/badge.svg?token=WYVVH8MKK3)](https://codecov.io/gh/TabbyML/tabby)\n\n[English](/README.md) |\n[ç®€ä½“ä¸­æ–‡](/README-zh.md) |\n[æ—¥æœ¬èª](/README-ja.md)\n\n</div>\n\nTabby is a self-hosted AI coding assistant, offering an open-source and on-premises alternative to GitHub Copilot. It boasts several key features:\n* Self-contained, with no need for a DBMS or cloud service.\n* OpenAPI interface, easy to integrate with existing infrastructure (e.g Cloud IDE).\n* Supports consumer-grade GPUs.\n\n<p align="center">\n  <a target="_blank" href="https://tabby.tabbyml.com"><img alt="Open Live Demo" src="https://img.shields.io/badge/OPEN_LIVE_DEMO-blue?logo=xcode&style=for-the-badge&logoColor=green"></a>\n</p>\n\n<p align="center">\n  <img alt="Demo" src="https://user-images.githubusercontent.com/388154/230440226-9bc01d05-9f57-478b-b04d-81184eba14ca.gif">\n</p>\n\n## ğŸ”¥ What''s New\n* **07/02/2025** [v0.30](https://github.com/TabbyML/tabby/releases/tag/v0.30.0) supports indexing GitLab Merge Request as Context! \n* **05/25/2025** ğŸ’¡Interested in joining [Agent](https://links.tabbyml.com/pochi-github-readme) private preview? DM in [X](https://x.com/getpochi) for early waitlist approval!ğŸ«\n* **05/20/2025** Enhance Tabby with your own documentationğŸ“ƒ through REST APIs in [v0.29](https://github.com/TabbyML/tabby/releases/tag/v0.29.0)! ğŸ‰ \n* **05/01/2025** [v0.28](https://github.com/TabbyML/tabby/releases/tag/v0.28.0) transforming Answer Engine messages into persistent, shareable Pages\n* **03/31/2025** [v0.27](https://github.com/TabbyML/tabby/releases/tag/v0.27.0) released with a richer `@` menu in the chat side panel.\n* **02/05/2025** LDAP Authentication and better notification for background jobs coming in Tabby [v0.24.0](https://github.com/TabbyML/tabby/releases/tag/v0.24.0)!âœ¨\n* **02/04/2025** [VSCode 1.20.0](https://marketplace.visualstudio.com/items/TabbyML.vscode-tabby/changelog) upgrade! @-mention files to add them as chat context, and edit inline with a new right-click option are available!\n\n\n\n\n\n<details>\n  <summary>Archived</summary>\n\n* **01/10/2025** Tabby [v0.23.0](https://github.com/TabbyML/tabby/releases/tag/v0.23.0) featuring enhanced code browser experience and chat side panel improvements!\n* **12/24/2024** Introduce **Notification Box** in Tabby [v0.22.0](https://github.com/TabbyML/tabby/releases/tag/v0.22.0)!\n* **12/06/2024** Llamafile deployment integration and enhanced Answer Engine user experience are coming in Tabby [v0.21.0](https://github.com/TabbyML/tabby/releases/tag/v0.21.0)!ğŸš€\n* **11/10/2024** Switching between different backend chat models is supported in Answer Engine with Tabby [v0.20.0](https://github.com/TabbyML/tabby/releases/tag/v0.20.0)!\n* **10/30/2024** Tabby [v0.19.0](https://github.com/TabbyML/tabby/releases/tag/v0.19.0) featuring recent shared threads on the main page to improve their discoverability. \n* **07/09/2024** ğŸ‰Announce [Codestral integration in Tabby](https://tabby.tabbyml.com/blog/2024/07/09/tabby-codestral/)!\n* **07/05/2024** Tabby [v0.13.0](https://github.com/TabbyML/tabby/releases/tag/v0.13.0) introduces ***Answer Engine***, a central knowledge engine for internal engineering teams. It seamlessly integrates with dev team''s internal data, delivering reliable and precise answers to empower developers.\n* **06/13/2024** [VSCode 1.7](https://marketplace.visualstudio.com/items/TabbyML.vscode-tabby/changelog) marks a significant milestone with a versatile Chat experience throughout your coding experience. Come and they the latest **chat in side-panel** and **editing via chat command**!\n* **06/10/2024** Latest ğŸ“ƒblogpost drop on [an enhanced code context understanding](https://tabby.tabbyml.com/blog/2024/06/11/rank-fusion-in-tabby-code-completion/) in Tabby!\n* **06/06/2024** Tabby [v0.12.0](https://github.com/TabbyML/tabby/releases/tag/v0.12.0) release brings ğŸ”—**seamless integrations** (Gitlab SSO, Self-hosted GitHub/GitLab, etc.), to âš™ï¸**flexible configurations** (HTTP API integration) and ğŸŒ**expanded capabilities** (repo-context in Code Browser)! \n* **05/22/2024** Tabby [VSCode 1.6](https://marketplace.visualstudio.com/items?itemName=TabbyML.vscode-tabby) comes with **multiple choices** in inline completion, and the **auto-generated commit messages**ğŸ±ğŸ’»!\n* **05/11/2024** [v0.11.0](https://github.com/TabbyML/tabby/releases/tag/v0.11.0) brings significant enterprise upgrades, including ğŸ“Š**storage usage** stats, ğŸ”—**GitHub & GitLab** integration, ğŸ“‹**Activities** page, and the long-awaited ğŸ¤–**Ask Tabby** feature!\n* **04/22/2024** [v0.10.0](https://github.com/TabbyML/tabby/releases/tag/v0.10.0) released, featuring the latest **Reports** tab with team-wise analytics for Tabby usage.\n* **04/19/2024** ğŸ“£ Tabby now incorporates [locally relevant snippets](https://github.com/TabbyML/tabby/pull/1844)(declarations from local LSP, and recently modified code) for code completion!\n* **04/17/2024** CodeGemma and CodeQwen model series have now been added to the [official registry](https://tabby.tabbyml.com/docs/models/)!\n* **03/20/2024** [v0.9](https://github.com/TabbyML/tabby/releases/tag/v0.9.1) released, highlighting a full feature admin UI.\n* **12/23/2023** Seamlessly [deploy Tabby on any cloud](https://tabby.tabbyml.com/docs/installation/skypilot/) with [SkyServe](https://skypilot.readthedocs.io/en/latest/serving/sky-serve.html) ğŸ›« from SkyPilot.\n* **12/15/2023** [v0.7.0](https://github.com/TabbyML/tabby/releases/tag/v0.7.0) released with team management and secured access!\n* **10/15/2023** RAG-based code completion is enabled by detail in [v0.3.0](https://github.com/TabbyML/tabby/releases/tag/v0.3.0)ğŸ‰! Check out the [blogpost](https://tabby.tabbyml.com/blog/2023/10/16/repository-context-for-code-completion/) explaining how Tabby utilizes repo-level context to get even smarter!\n* **11/27/2023** [v0.6.0](https://github.com/TabbyML/tabby/releases/tag/v0.6.0) released!\n* **11/09/2023** [v0.5.5](https://github.com/TabbyML/tabby/releases/tag/v0.5.5) released! With a redesign of UI + performance improvement.\n* **10/24/2023** â›³ï¸ Major updates for Tabby IDE plugins across [VSCode/Vim/IntelliJ](https://tabby.tabbyml.com/docs/extensions)!\n* **10/04/2023** Check out the [model directory](https://tabby.tabbyml.com/docs/models/) for the latest models supported by Tabby.\n* **09/18/2023** Apple''s M1/M2 Metal inference support has landed in [v0.1.1](https://github.com/TabbyML/tabby/releases/tag/v0.1.1)!\n* **08/31/2023** Tabby''s first stable release [v0.0.1](https://github.com/TabbyML/tabby/releases/tag/v0.0.1) ğŸ¥³.\n* **08/28/2023** Experimental support for the [CodeLlama 7B](https://github.com/TabbyML/tabby/issues/370).\n* **08/24/2023** Tabby is now on [JetBrains Marketplace](https://plugins.jetbrains.com/plugin/22379-tabby)!\n\n</details>\n\n## ğŸ‘‹ Getting Started\n\nYou can find our documentation [here](https://tabby.tabbyml.com/docs/getting-started).\n- ğŸ“š [Installation](https://tabby.tabbyml.com/docs/installation/)\n- ğŸ’» [IDE/Editor Extensions](https://tabby.tabbyml.com/docs/extensions/)\n- âš™ï¸ [Configuration](https://tabby.tabbyml.com/docs/configuration)\n\n### Run Tabby in 1 Minute\nThe easiest way to start a Tabby server is by using the following Docker command:\n\n```bash\ndocker run -it \\n  --gpus all -p 8080:8080 -v $HOME/.tabby:/data \\n  tabbyml/tabby \\n  serve --model StarCoder-1B --device cuda --chat-model Qwen2-1.5B-Instruct\n```\nFor additional options (e.g inference type, parallelism), please refer to the [documentation page](https://tabbyml.github.io/tabby).\n\n## ğŸ¤ Contributing\n\nFull guide at [CONTRIBUTING.md](https://github.com/TabbyML/tabby/blob/main/CONTRIBUTING.md);\n\n### Get the Code\n\n```bash\ngit clone --recurse-submodules https://github.com/TabbyML/tabby\ncd tabby\n```\n\nIf you have already cloned the repository, you could run the `git submodule update --recursive --init` command to fetch all submodules.\n\n### Build\n\n1. Set up the Rust environment by following this [tutorial](https://www.rust-lang.org/learn/get-started).\n\n2. Install the required dependencies:\n```bash\n# For MacOS\nbrew install protobuf\n\n# For Ubuntu / Debian\napt install protobuf-compiler libopenblas-dev\n```\n\n3. Install useful tools:\n```bash\n# For Ubuntu\napt install make sqlite3 graphviz\n```\n\n4. Now, you can build Tabby by running the command `cargo build`.\n\n### Start Hacking!\n... and don''t forget to submit a [Pull Request](https://github.com/TabbyML/tabby/compare)\n\n## ğŸŒ Community\n- ğŸ¤ [Twitter / X](https://twitter.com/Tabby_ML) - engage with TabbyML for all things possible \n- ğŸ“š [LinkedIn](https://www.linkedin.com/company/tabbyml/) - follow for the latest from the community \n- ğŸ’Œ [Newsletter](https://newsletter.tabbyml.com/archive) - subscribe to unlock Tabby insights and secrets\n\n### ğŸ”† Activity\n\n![Git Repository Activity](https://repobeats.axiom.co/api/embed/e4ef0fbd12e586ef9ea7d72d1fb4f5c5b88d78d5.svg "Repobeats analytics image")\n\n### ğŸŒŸ Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=tabbyml/tabby&type=Date)](https://star-history.com/#tabbyml/tabby&Date)\n', '{"language":"Rust","stars":32547,"forks":1648,"watchers":32547,"open_issues":285,"topics":["ai","codegen","coding-assistant","coding-language","developer-experience","developer-tools","gen-ai","ide","llms"],"default_branch":"main","size_kb":111763,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"},{"type":"has_code","target_id":"github:TabbyML:tabby","source_url":"https://github.com/TabbyML/tabby"}]', NULL, 'NOASSERTION', 'approved', 65, 'a23e7a9d0e30b741a6cb4e4912e240e3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-TabbyML-tabby from https://github.com/TabbyML.png
Image converted to WebP: data/images/github-TabbyML-tabby.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-danny-avila-LibreChat', 'github--danny-avila--librechat', 'LibreChat', 'danny-avila', '<p align="center"> <a href="https://librechat.ai"> <img src="client/public/assets/logo.svg" height="256"> </a> <h1 align="center"> <a href="https://librechat.ai">LibreChat</a> </h1> </p> <p align="center"> <a href="https://discord.librechat.ai"> <img src="https://img.shields.io/discord/1086345563026489514?label=&logo=discord&style=for-the-badge&logoWidth=20&logoColor=white&labelColor=000000&color=blueviolet"> </a> <a href="https://www.youtube.com/@LibreChat"> <img src="https://img.shields.io/...', '["ai","anthropic","artifacts","aws","azure","chatgpt","chatgpt-clone","claude","clone","deepseek","gemini","google","gpt-5","librechat","mcp","o1","openai","responses-api","vision","webui","typescript"]', 'other', 32239, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/danny-avila/LibreChat","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '<p align="center">\n  <a href="https://librechat.ai">\n    <img src="client/public/assets/logo.svg" height="256">\n  </a>\n  <h1 align="center">\n    <a href="https://librechat.ai">LibreChat</a>\n  </h1>\n</p>\n\n<p align="center">\n  <a href="https://discord.librechat.ai"> \n    <img\n      src="https://img.shields.io/discord/1086345563026489514?label=&logo=discord&style=for-the-badge&logoWidth=20&logoColor=white&labelColor=000000&color=blueviolet">\n  </a>\n  <a href="https://www.youtube.com/@LibreChat"> \n    <img\n      src="https://img.shields.io/badge/YOUTUBE-red.svg?style=for-the-badge&logo=youtube&logoColor=white&labelColor=000000&logoWidth=20">\n  </a>\n  <a href="https://docs.librechat.ai"> \n    <img\n      src="https://img.shields.io/badge/DOCS-blue.svg?style=for-the-badge&logo=read-the-docs&logoColor=white&labelColor=000000&logoWidth=20">\n  </a>\n  <a aria-label="Sponsors" href="https://github.com/sponsors/danny-avila">\n    <img\n      src="https://img.shields.io/badge/SPONSORS-brightgreen.svg?style=for-the-badge&logo=github-sponsors&logoColor=white&labelColor=000000&logoWidth=20">\n  </a>\n</p>\n\n<p align="center">\n<a href="https://railway.app/template/b5k2mn?referralCode=HI9hWz">\n  <img src="https://railway.app/button.svg" alt="Deploy on Railway" height="30">\n</a>\n<a href="https://zeabur.com/templates/0X2ZY8">\n  <img src="https://zeabur.com/button.svg" alt="Deploy on Zeabur" height="30"/>\n</a>\n<a href="https://template.cloud.sealos.io/deploy?templateName=librechat">\n  <img src="https://raw.githubusercontent.com/labring-actions/templates/main/Deploy-on-Sealos.svg" alt="Deploy on Sealos" height="30">\n</a>\n</p>\n\n<p align="center">\n  <a href="https://www.librechat.ai/docs/translation">\n    <img \n      src="https://img.shields.io/badge/dynamic/json.svg?style=for-the-badge&color=2096F3&label=locize&query=%24.translatedPercentage&url=https://api.locize.app/badgedata/4cb2598b-ed4d-469c-9b04-2ed531a8cb45&suffix=%+translated" \n      alt="Translation Progress">\n  </a>\n</p>\n\n\n# âœ¨ Features\n\n- ğŸ–¥ï¸ **UI & Experience** inspired by ChatGPT with enhanced design and features\n\n- ğŸ¤– **AI Model Selection**:  \n  - Anthropic (Claude), AWS Bedrock, OpenAI, Azure OpenAI, Google, Vertex AI, OpenAI Responses API (incl. Azure)\n  - [Custom Endpoints](https://www.librechat.ai/docs/quick_start/custom_endpoints): Use any OpenAI-compatible API with LibreChat, no proxy required\n  - Compatible with [Local & Remote AI Providers](https://www.librechat.ai/docs/configuration/librechat_yaml/ai_endpoints):\n    - Ollama, groq, Cohere, Mistral AI, Apple MLX, koboldcpp, together.ai,\n    - OpenRouter, Helicone, Perplexity, ShuttleAI, Deepseek, Qwen, and more\n\n- ğŸ”§ **[Code Interpreter API](https://www.librechat.ai/docs/features/code_interpreter)**: \n  - Secure, Sandboxed Execution in Python, Node.js (JS/TS), Go, C/C++, Java, PHP, Rust, and Fortran\n  - Seamless File Handling: Upload, process, and download files directly\n  - No Privacy Concerns: Fully isolated and secure execution\n\n- ğŸ”¦ **Agents & Tools Integration**:  \n  - **[LibreChat Agents](https://www.librechat.ai/docs/features/agents)**:\n    - No-Code Custom Assistants: Build specialized, AI-driven helpers\n    - Agent Marketplace: Discover and deploy community-built agents\n    - Collaborative Sharing: Share agents with specific users and groups\n    - Flexible & Extensible: Use MCP Servers, tools, file search, code execution, and more\n    - Compatible with Custom Endpoints, OpenAI, Azure, Anthropic, AWS Bedrock, Google, Vertex AI, Responses API, and more\n    - [Model Context Protocol (MCP) Support](https://modelcontextprotocol.io/clients#librechat) for Tools\n\n- ğŸ” **Web Search**:  \n  - Search the internet and retrieve relevant information to enhance your AI context\n  - Combines search providers, content scrapers, and result rerankers for optimal results\n  - **Customizable Jina Reranking**: Configure custom Jina API URLs for reranking services\n  - **[Learn More â†’](https://www.librechat.ai/docs/features/web_search)**\n\n- ğŸª„ **Generative UI with Code Artifacts**:  \n  - [Code Artifacts](https://youtu.be/GfTj7O4gmd0?si=WJbdnemZpJzBrJo3) allow creation of React, HTML, and Mermaid diagrams directly in chat\n\n- ğŸ¨ **Image Generation & Editing**\n  - Text-to-image and image-to-image with [GPT-Image-1](https://www.librechat.ai/docs/features/image_gen#1--openai-image-tools-recommended)\n  - Text-to-image with [DALL-E (3/2)](https://www.librechat.ai/docs/features/image_gen#2--dalle-legacy), [Stable Diffusion](https://www.librechat.ai/docs/features/image_gen#3--stable-diffusion-local), [Flux](https://www.librechat.ai/docs/features/image_gen#4--flux), or any [MCP server](https://www.librechat.ai/docs/features/image_gen#5--model-context-protocol-mcp)\n  - Produce stunning visuals from prompts or refine existing images with a single instruction\n\n- ğŸ’¾ **Presets & Context Management**:  \n  - Create, Save, & Share Custom Presets  \n  - Switch between AI Endpoints and Presets mid-chat\n  - Edit, Resubmit, and Continue Messages with Conversation branching  \n  - Create and share prompts with specific users and groups\n  - [Fork Messages & Conversations](https://www.librechat.ai/docs/features/fork) for Advanced Context control\n\n- ğŸ’¬ **Multimodal & File Interactions**:  \n  - Upload and analyze images with Claude 3, GPT-4.5, GPT-4o, o1, Llama-Vision, and Gemini ğŸ“¸  \n  - Chat with Files using Custom Endpoints, OpenAI, Azure, Anthropic, AWS Bedrock, & Google ğŸ—ƒï¸\n\n- ğŸŒ **Multilingual UI**:\n  - English, ä¸­æ–‡ (ç®€ä½“), ä¸­æ–‡ (ç¹é«”), Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©, Deutsch, EspaÃ±ol, FranÃ§ais, Italiano\n  - Polski, PortuguÃªs (PT), PortuguÃªs (BR), Ğ ÑƒÑÑĞºĞ¸Ğ¹, æ—¥æœ¬èª, Svenska, í•œêµ­ì–´, Tiáº¿ng Viá»‡t\n  - TÃ¼rkÃ§e, Nederlands, ×¢×‘×¨×™×ª, CatalÃ , ÄŒeÅ¡tina, Dansk, Eesti, ÙØ§Ø±Ø³ÛŒ\n  - Suomi, Magyar, Õ€Õ¡ÕµÕ¥Ö€Õ¥Õ¶, Bahasa Indonesia, áƒ¥áƒáƒ áƒ—áƒ£áƒšáƒ˜, LatvieÅ¡u, à¹„à¸—à¸¢, Ø¦Û‡ÙŠØºÛ‡Ø±Ú†Û•\n\n- ğŸ§  **Reasoning UI**:  \n  - Dynamic Reasoning UI for Chain-of-Thought/Reasoning AI models like DeepSeek-R1\n\n- ğŸ¨ **Customizable Interface**:  \n  - Customizable Dropdown & Interface that adapts to both power users and newcomers\n\n- ğŸ—£ï¸ **Speech & Audio**:  \n  - Chat hands-free with Speech-to-Text and Text-to-Speech  \n  - Automatically send and play Audio  \n  - Supports OpenAI, Azure OpenAI, and Elevenlabs\n\n- ğŸ“¥ **Import & Export Conversations**:  \n  - Import Conversations from LibreChat, ChatGPT, Chatbot UI  \n  - Export conversations as screenshots, markdown, text, json\n\n- ğŸ” **Search & Discovery**:  \n  - Search all messages/conversations\n\n- ğŸ‘¥ **Multi-User & Secure Access**:\n  - Multi-User, Secure Authentication with OAuth2, LDAP, & Email Login Support\n  - Built-in Moderation, and Token spend tools\n\n- âš™ï¸ **Configuration & Deployment**:  \n  - Configure Proxy, Reverse Proxy, Docker, & many Deployment options  \n  - Use completely local or deploy on the cloud\n\n- ğŸ“– **Open-Source & Community**:  \n  - Completely Open-Source & Built in Public  \n  - Community-driven development, support, and feedback\n\n[For a thorough review of our features, see our docs here](https://docs.librechat.ai/) ğŸ“š\n\n## ğŸª¶ All-In-One AI Conversations with LibreChat\n\nLibreChat brings together the future of assistant AIs with the revolutionary technology of OpenAI''s ChatGPT. Celebrating the original styling, LibreChat gives you the ability to integrate multiple AI models. It also integrates and enhances original client features such as conversation and message search, prompt templates and plugins.\n\nWith LibreChat, you no longer need to opt for ChatGPT Plus and can instead use free or pay-per-call APIs. We welcome contributions, cloning, and forking to enhance the capabilities of this advanced chatbot platform.\n\n[![Watch the video](https://raw.githubusercontent.com/LibreChat-AI/librechat.ai/main/public/images/changelog/v0.7.6.gif)](https://www.youtube.com/watch?v=ilfwGQtJNlI)\n\nClick on the thumbnail to open the videoâ˜ï¸\n\n---\n\n## ğŸŒ Resources\n\n**GitHub Repo:**\n  - **RAG API:** [github.com/danny-avila/rag_api](https://github.com/danny-avila/rag_api)\n  - **Website:** [github.com/LibreChat-AI/librechat.ai](https://github.com/LibreChat-AI/librechat.ai)\n\n**Other:**\n  - **Website:** [librechat.ai](https://librechat.ai)\n  - **Documentation:** [librechat.ai/docs](https://librechat.ai/docs)\n  - **Blog:** [librechat.ai/blog](https://librechat.ai/blog)\n\n---\n\n## ğŸ“ Changelog\n\nKeep up with the latest updates by visiting the releases page and notes:\n- [Releases](https://github.com/danny-avila/LibreChat/releases)\n- [Changelog](https://www.librechat.ai/changelog) \n\n**âš ï¸ Please consult the [changelog](https://www.librechat.ai/changelog) for breaking changes before updating.**\n\n---\n\n## â­ Star History\n\n<p align="center">\n  <a href="https://star-history.com/#danny-avila/LibreChat&Date">\n    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=danny-avila/LibreChat&type=Date&theme=dark" onerror="this.src=''https://api.star-history.com/svg?repos=danny-avila/LibreChat&type=Date''" />\n  </a>\n</p>\n<p align="center">\n  <a href="https://trendshift.io/repositories/4685" target="_blank" style="padding: 10px;">\n    <img src="https://trendshift.io/api/badge/repositories/4685" alt="danny-avila%2FLibreChat | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/>\n  </a>\n  <a href="https://runacap.com/ross-index/q1-24/" target="_blank" rel="noopener" style="margin-left: 20px;">\n    <img style="width: 260px; height: 56px" src="https://runacap.com/wp-content/uploads/2024/04/ROSS_badge_white_Q1_2024.svg" alt="ROSS Index - Fastest Growing Open-Source Startups in Q1 2024 | Runa Capital" width="260" height="56"/>\n  </a>\n</p>\n\n---\n\n## âœ¨ Contributions\n\nContributions, suggestions, bug reports and fixes are welcome!\n\nFor new features, components, or extensions, please open an issue and discuss before sending a PR.\n\nIf you''d like to help translate LibreChat into your language, we''d love your contribution! Improving our translations not only makes LibreChat more accessible to users around the world but also enhances the overall user experience. Please check out our [Translation Guide](https://www.librechat.ai/docs/translation).\n\n---\n\n## ğŸ’– This project exists in its current state thanks to all the people who contribute\n\n<a href="https://github.com/danny-avila/LibreChat/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=danny-avila/LibreChat" />\n</a>\n\n---\n\n## ğŸ‰ Special Thanks\n\nWe thank [Locize](https://locize.com) for their translation management tools that support multiple languages in LibreChat.\n\n<p align="center">\n  <a href="https://locize.com" target="_blank" rel="noopener noreferrer">\n    <img src="https://github.com/user-attachments/assets/d6b70894-6064-475e-bb65-92a9e23e0077" alt="Locize Logo" height="50">\n  </a>\n</p>\n', '{"language":"TypeScript","stars":32239,"forks":6364,"watchers":32239,"open_issues":359,"topics":["ai","anthropic","artifacts","aws","azure","chatgpt","chatgpt-clone","claude","clone","deepseek","gemini","google","gpt-5","librechat","mcp","o1","openai","responses-api","vision","webui"],"default_branch":"main","size_kb":131537,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:sponsors:danny-avila\">","source_url":"https://github.com/sponsors/danny-avila\">"},{"type":"has_code","target_id":"github:danny-avila:rag_api","source_url":"https://github.com/danny-avila/rag_api"},{"type":"has_code","target_id":"github:LibreChat-AI:librechat.ai","source_url":"https://github.com/LibreChat-AI/librechat.ai"},{"type":"has_code","target_id":"github:danny-avila:LibreChat","source_url":"https://github.com/danny-avila/LibreChat"},{"type":"has_code","target_id":"github:danny-avila:LibreChat","source_url":"https://github.com/danny-avila/LibreChat"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"}]', NULL, 'MIT', 'approved', 80, '474325a31ef0a7831d26e7bd23387b73', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-danny-avila-LibreChat from https://github.com/danny-avila.png
Image converted to WebP: data/images/github-danny-avila-LibreChat.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-khoj-ai-khoj', 'github--khoj-ai--khoj', 'khoj', 'khoj-ai', '<p align="center"><img src="https://assets.khoj.dev/khoj-logo-sideways-1200x540.png" width="230" alt="Khoj Logo"></p> <div align="center"> </div> <div align="center"> <b>Your AI second brain</b> </div> <br /> <div align="center"> ğŸ“‘ Docs <span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span> ğŸŒ Web <span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span> ğŸ”¥ App <span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span> ğŸ’¬ Discord <span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span> âœğŸ½ Blog <a href="https://trendshift.io/repositories/10318" target="_blank"><i...', '["agent","ai","assistant","chat","chatgpt","emacs","image-generation","llama3","llamacpp","llm","obsidian","obsidian-md","offline-llm","productivity","rag","research","self-hosted","semantic-search","stt","whatsapp-ai","python"]', 'other', 31870, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/khoj-ai/khoj","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center"><img src="https://assets.khoj.dev/khoj-logo-sideways-1200x540.png" width="230" alt="Khoj Logo"></p>\n\n<div align="center">\n\n[![test](https://github.com/khoj-ai/khoj/actions/workflows/test.yml/badge.svg)](https://github.com/khoj-ai/khoj/actions/workflows/test.yml)\n[![docker](https://github.com/khoj-ai/khoj/actions/workflows/dockerize.yml/badge.svg)](https://github.com/khoj-ai/khoj/pkgs/container/khoj)\n[![pypi](https://github.com/khoj-ai/khoj/actions/workflows/pypi.yml/badge.svg)](https://pypi.org/project/khoj/)\n[![discord](https://img.shields.io/discord/1112065956647284756?style=plastic&label=discord)](https://discord.gg/BDgyabRM6e)\n\n</div>\n\n<div align="center">\n<b>Your AI second brain</b>\n</div>\n\n<br />\n\n<div align="center">\n\n[ğŸ“‘ Docs](https://docs.khoj.dev)\n<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>\n[ğŸŒ Web](https://khoj.dev)\n<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>\n[ğŸ”¥ App](https://app.khoj.dev)\n<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>\n[ğŸ’¬ Discord](https://discord.gg/BDgyabRM6e)\n<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>\n[âœğŸ½ Blog](https://blog.khoj.dev)\n\n<a href="https://trendshift.io/repositories/10318" target="_blank"><img src="https://trendshift.io/api/badge/repositories/10318" alt="khoj-ai%2Fkhoj | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n\n</div>\n\n***\n\n### ğŸ New\n* Start any message with `/research` to try out the experimental research mode with Khoj.\n* Anyone can now [create custom agents](https://blog.khoj.dev/posts/create-agents-on-khoj/) with tunable personality, tools and knowledge bases.\n* [Read](https://blog.khoj.dev/posts/evaluate-khoj-quality/) about Khoj''s excellent performance on modern retrieval and reasoning benchmarks.\n\n***\n\n## Overview\n\n[Khoj](https://khoj.dev) is a personal AI app to extend your capabilities. It smoothly scales up from an on-device personal AI to a cloud-scale enterprise AI.\n\n- Chat with any local or online LLM (e.g llama3, qwen, gemma, mistral, gpt, claude, gemini, deepseek).\n- Get answers from the internet and your docs (including image, pdf, markdown, org-mode, word, notion files).\n- Access it from your Browser, Obsidian, Emacs, Desktop, Phone or Whatsapp.\n- Create agents with custom knowledge, persona, chat model and tools to take on any role.\n- Automate away repetitive research. Get personal newsletters and smart notifications delivered to your inbox.\n- Find relevant docs quickly and easily using our advanced semantic search.\n- Generate images, talk out loud, play your messages.\n- Khoj is open-source, self-hostable. Always.\n- Run it privately on [your computer](https://docs.khoj.dev/get-started/setup) or try it on our [cloud app](https://app.khoj.dev).\n\n***\n\n## See it in action\n\n![demo_chat](https://github.com/khoj-ai/khoj/blob/master/documentation/assets/img/quadratic_equation_khoj_web.gif?raw=true)\n\nGo to https://app.khoj.dev to see Khoj live.\n\n## Full feature list\nYou can see the full feature list [here](https://docs.khoj.dev/category/features).\n\n## Self-Host\n\nTo get started with self-hosting Khoj, [read the docs](https://docs.khoj.dev/get-started/setup).\n\n## Enterprise\n\nKhoj is available as a cloud service, on-premises, or as a hybrid solution. To learn more about Khoj Enterprise, [visit our website](https://khoj.dev/teams).\n\n## Frequently Asked Questions (FAQ)\n\nQ: Can I use Khoj without self-hosting?\n\nYes! You can use Khoj right away at [https://app.khoj.dev](https://app.khoj.dev) â€” no setup required.\n\nQ: What kinds of documents can Khoj read?\n\nKhoj supports a wide variety: PDFs, Markdown, Notion, Word docs, org-mode files, and more.\n\nQ: How can I make my own agent?\n\nCheck out [this blog post](https://blog.khoj.dev/posts/create-agents-on-khoj/) for a step-by-step guide to custom agents.\nFor more questions, head over to our [Discord](https://discord.gg/BDgyabRM6e)!\n\n\n## Contributors\nCheers to our awesome contributors! ğŸ‰\n\n<a href="https://github.com/khoj-ai/khoj/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=khoj-ai/khoj" />\n</a>\n\nMade with [contrib.rocks](https://contrib.rocks).\n\n### Interested in Contributing?\nKhoj is open source. It is sustained by the community and weâ€™d love for you to join it! Whether youâ€™re a coder, designer, writer, or enthusiast, thereâ€™s a place for you.\n\nWhy Contribute?\n- Make an Impact: Help build, test and improve a tool used by thousands to boost productivity.\n- Learn & Grow: Work on cutting-edge AI, LLMs, and semantic search technologies.\n\nYou can help us build new features, improve the project documentation, report issues and fix bugs. If you''re a developer, please see our [Contributing Guidelines](https://docs.khoj.dev/contributing/development) and check out [good first issues](https://github.com/khoj-ai/khoj/contribute) to work on.\n', '{"language":"Python","stars":31870,"forks":1883,"watchers":31870,"open_issues":87,"topics":["agent","ai","assistant","chat","chatgpt","emacs","image-generation","llama3","llamacpp","llm","obsidian","obsidian-md","offline-llm","productivity","rag","research","self-hosted","semantic-search","stt","whatsapp-ai"],"default_branch":"master","size_kb":117290,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:khoj-ai:khoj","source_url":"https://github.com/khoj-ai/khoj"},{"type":"has_code","target_id":"github:khoj-ai:khoj","source_url":"https://github.com/khoj-ai/khoj"},{"type":"has_code","target_id":"github:khoj-ai:khoj","source_url":"https://github.com/khoj-ai/khoj"},{"type":"has_code","target_id":"github:khoj-ai:khoj","source_url":"https://github.com/khoj-ai/khoj"},{"type":"has_code","target_id":"github:khoj-ai:khoj","source_url":"https://github.com/khoj-ai/khoj"},{"type":"has_code","target_id":"github:khoj-ai:khoj","source_url":"https://github.com/khoj-ai/khoj"},{"type":"has_code","target_id":"github:khoj-ai:khoj","source_url":"https://github.com/khoj-ai/khoj"},{"type":"has_code","target_id":"github:khoj-ai:khoj","source_url":"https://github.com/khoj-ai/khoj"}]', NULL, 'AGPL-3.0', 'approved', 65, '9bd573b7cbcf52ff59cc04cc822e7885', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-khoj-ai-khoj from https://github.com/khoj-ai.png
Image converted to WebP: data/images/github-khoj-ai-khoj.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-s0md3v-roop', 'github--s0md3v--roop', 'roop', 's0md3v', 'Yes, it still works, you can still use this software. It just won''t recieve any updates now. > I do not have the interest or time to oversee the development of this software. I thank all the amazing people who contributed to this project and made what it is in it''s final form. > Take a video and replace the face in it with a face of your choice. You only need one image of the desired face. No dataset, no training. <img src="https://i.ibb.co/4RdPYwQ/Untitled.jpg"/> Be aware, the installation n...', '["ai","face-swap","python"]', 'other', 30409, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/s0md3v/roop","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '## This project has been discontinued\n\nYes, it still works, you can still use this software. It just won''t recieve any updates now.\n\n> I do not have the interest or time to oversee the development of this software. I thank all the amazing people who contributed to this project and made what it is in it''s final form.\n\n# Roop\n\n> Take a video and replace the face in it with a face of your choice. You only need one image of the desired face. No dataset, no training.\n\n[![Build Status](https://img.shields.io/github/actions/workflow/status/s0md3v/roop/ci.yml.svg?branch=main)](https://github.com/s0md3v/roop/actions?query=workflow:ci)\n\n<img src="https://i.ibb.co/4RdPYwQ/Untitled.jpg"/>\n\n## Installation\n\nBe aware, the installation needs technical skills and is not for beginners. Please do not open platform and installation related issues on GitHub.\n\n[Basic](https://github.com/s0md3v/roop/wiki/1.-Installation) - It is more likely to work on your computer, but will be quite slow\n\n[Acceleration](https://github.com/s0md3v/roop/wiki/2.-Acceleration) - Unleash the full potential of your CPU and GPU\n\n\n## Usage\n\nStart the program with arguments:\n\n```\npython run.py [options]\n\n-h, --help                                                                 show this help message and exit\n-s SOURCE_PATH, --source SOURCE_PATH                                       select an source image\n-t TARGET_PATH, --target TARGET_PATH                                       select an target image or video\n-o OUTPUT_PATH, --output OUTPUT_PATH                                       select output file or directory\n--frame-processor FRAME_PROCESSOR [FRAME_PROCESSOR ...]                    frame processors (choices: face_swapper, face_enhancer, ...)\n--keep-fps                                                                 keep target fps\n--keep-frames                                                              keep temporary frames\n--skip-audio                                                               skip target audio\n--many-faces                                                               process every face\n--reference-face-position REFERENCE_FACE_POSITION                          position of the reference face\n--reference-frame-number REFERENCE_FRAME_NUMBER                            number of the reference frame\n--similar-face-distance SIMILAR_FACE_DISTANCE                              face distance used for recognition\n--temp-frame-format {jpg,png}                                              image format used for frame extraction\n--temp-frame-quality [0-100]                                               image quality used for frame extraction\n--output-video-encoder {libx264,libx265,libvpx-vp9,h264_nvenc,hevc_nvenc}  encoder used for the output video\n--output-video-quality [0-100]                                             quality used for the output video\n--max-memory MAX_MEMORY                                                    maximum amount of RAM in GB\n--execution-provider {cpu} [{cpu} ...]                                     available execution provider (choices: cpu, ...)\n--execution-threads EXECUTION_THREADS                                      number of execution threads\n-v, --version                                                              show program''s version number and exit\n```\n\n\n### Headless\n\nUsing the `-s/--source`, `-t/--target` and `-o/--output` argument will run the program in headless mode.\n\n\n## Disclaimer\n\nThis software is designed to contribute positively to the AI-generated media industry, assisting artists with tasks like character animation and models for clothing.\n\nWe are aware of the potential ethical issues and have implemented measures to prevent the software from being used for inappropriate content, such as nudity.\n\nUsers are expected to follow local laws and use the software responsibly. If using real faces, get consent and clearly label deepfakes when sharing. The developers aren''t liable for user actions.\n\n\n## Licenses\n\nOur software uses a lot of third party libraries as well pre-trained models. The users should keep in mind that these third party components have their own license and terms, therefore our license is not being applied.\n\n\n## Credits\n\n- [deepinsight](https://github.com/deepinsight) for their [insightface](https://github.com/deepinsight/insightface) project which provided a well-made library and models.\n- all developers behind the libraries used in this project\n\n\n## Documentation\n\nRead the [documentation](https://github.com/s0md3v/roop/wiki) for a deep dive.\n', '{"language":"Python","stars":30409,"forks":6909,"watchers":30409,"open_issues":4,"topics":["ai","face-swap"],"default_branch":"main","size_kb":99770,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:s0md3v:roop","source_url":"https://github.com/s0md3v/roop"},{"type":"has_code","target_id":"github:s0md3v:roop","source_url":"https://github.com/s0md3v/roop"},{"type":"has_code","target_id":"github:s0md3v:roop","source_url":"https://github.com/s0md3v/roop"},{"type":"has_code","target_id":"github:deepinsight:insightface","source_url":"https://github.com/deepinsight/insightface"},{"type":"has_code","target_id":"github:s0md3v:roop","source_url":"https://github.com/s0md3v/roop"}]', NULL, 'GPL-3.0', 'approved', 65, '5853edbbec2d6a59c70e7eb9dd800c74', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-s0md3v-roop from https://github.com/s0md3v.png
Image converted to WebP: data/images/github-s0md3v-roop.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-continuedev-continue', 'github--continuedev--continue', 'continue', 'continuedev', '<div align="center"> !Continue logo </div> <h1 align="center">Continue</h1> <div align="center"> <a target="_blank" href="https://opensource.org/licenses/Apache-2.0" style="background:none"> <img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg" style="height: 22px;" /> </a> <a target="_blank" href="https://docs.continue.dev" style="background:none"> <img src="https://img.shields.io/badge/Continue-docs-%23BE1B55.svg?logo=data:image/svg%2bxml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy5...', '["agent","ai","background-agents","claude","cli","continuous-ai","developer-tools","gemini","gpt","hacktoberfest","jetbrains","llm","open-source","qwen","vscode","workflows","typescript"]', 'other', 30189, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/continuedev/continue","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n\n![Continue logo](media/readme.png)\n\n</div>\n\n<h1 align="center">Continue</h1>\n\n<div align="center">\n\n<a target="_blank" href="https://opensource.org/licenses/Apache-2.0" style="background:none">\n    <img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg" style="height: 22px;" />\n</a>\n<a target="_blank" href="https://docs.continue.dev" style="background:none">\n    <img src="https://img.shields.io/badge/Continue-docs-%23BE1B55.svg?logo=data:image/svg%2bxml;base64,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" style="height: 22px;" />\n</a>\n<a target="_blank" href="https://changelog.continue.dev" style="background:none">\n    <img src="https://img.shields.io/badge/changelog-%96EFF3" style="height: 22px;" />\n</a>\n<a target="_blank" href="https://discord.gg/vapESyrFmJ" style="background:none">\n    <img src="https://img.shields.io/badge/discord-join-continue.svg?labelColor=191937&color=6F6FF7&logo=discord" style="height: 22px;" />\n</a>\n\n<p></p>\n\n<div align="center">\n\n**Ship faster with Continuous AI**\n\n**The future of coding isn''t writing more code. It''s delegating the boring parts, so you can build the interesting stuff**\n\n</div>\n\nGet started in [Mission Control](https://hub.continue.dev/agents), [CLI (Headless Mode)](https://docs.continue.dev/cli/quick-start#headless-mode), or [CLI (TUI mode)](https://docs.continue.dev/cli/quick-start#tui-mode)\n\n## Cloud Agents\n\nSet workflows to run automatically on [PR opens](https://docs.continue.dev/guides/continuous-ai#pattern-2-the-pr-review-agent), [schedules](https://docs.continue.dev/guides/continuous-ai#pattern-1-the-async-triage-bot), or [any event trigger](https://docs.continue.dev/cli/quick-start#headless-mode)\n\n![Cloud Agents](docs/images/background-agent.gif)\n\n## CLI Agents\n\nWatch workflows execute in real-time and approve decisions step-by-step from your [terminal](https://docs.continue.dev/cli/quick-start#tui-mode)\n\n![CLI Agents](docs/images/cli-agent.gif)\n\n## IDE Agents\n\nTrigger workflows from [VS Code](https://marketplace.visualstudio.com/items?itemName=Continue.continue) or [JetBrains](https://plugins.jetbrains.com/plugin/22707-continue-extension)â€”let agents handle the refactoring while you keep coding\n\n![IDE Agents](docs/images/agent.gif)\n\n</div>\n\n## Contributing\n\nRead the [contributing guide](https://github.com/continuedev/continue/blob/main/CONTRIBUTING.md), and\njoin [#contribute on Discord](https://discord.gg/vapESyrFmJ).\n\n## License\n\n[Apache 2.0 Â© 2023-2024 Continue Dev, Inc.](./LICENSE)\n', '{"language":"TypeScript","stars":30189,"forks":3870,"watchers":30189,"open_issues":644,"topics":["agent","ai","background-agents","claude","cli","continuous-ai","developer-tools","gemini","gpt","hacktoberfest","jetbrains","llm","open-source","qwen","vscode","workflows"],"default_branch":"main","size_kb":850003,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:continuedev:continue","source_url":"https://github.com/continuedev/continue"}]', NULL, 'Apache-2.0', 'approved', 65, 'c8f58bf923e7f809d5713173ab0f8ae2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-continuedev-continue from https://github.com/continuedev.png
Image converted to WebP: data/images/github-continuedev-continue.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-JushBJJ-Mr.-Ranedeer-AI-Tutor', 'github--jushbjj--mr.-ranedeer-ai-tutor', 'Mr.-Ranedeer-AI-Tutor', 'JushBJJ', 'Unlock the potential of GPT-4 with Mr. Ranedeer AI Tutor, a customizable prompt that delivers personalized learning experiences for users with diverse needs and interests. **Share screenshots of what you''re learning here:** https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/issues/43 !image - Mr. Ranedeer: Your personalized AI Tutor! - Table of Contents - Why Mr. Ranedeer? - Requirements and Compatibility - Recommended - Not Recommended - It also works on... - Quick Start Guide - Previous Versi...', '["ai","education","gpt-4","llm"]', 'other', 29676, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# DISCONTINUED\n\n# Mr. Ranedeer: Your personalized AI Tutor!\n\nUnlock the potential of GPT-4 with Mr. Ranedeer AI Tutor, a customizable prompt that delivers personalized learning experiences for users with diverse needs and interests.\n\n**Share screenshots of what you''re learning here:** https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/issues/43\n\n![image](https://media.discordapp.net/attachments/1114958734364524605/1129700723857301616/image.png?width=395&height=584)\n\n## Table of Contents\n- [Mr. Ranedeer: Your personalized AI Tutor!](#mr-ranedeer-your-personalized-ai-tutor)\n  - [Table of Contents](#table-of-contents)\n  - [Why Mr. Ranedeer?](#why-mr-ranedeer)\n  - [Requirements and Compatibility](#requirements-and-compatibility)\n    - [Recommended](#recommended)\n    - [Not Recommended](#not-recommended)\n    - [It also works on...](#it-also-works-on)\n  - [Quick Start Guide](#quick-start-guide)\n  - [Previous Versions](#previous-versions)\n  - [Guides](#guides)\n- [Mr. Ranedeer Personalization Options](#mr-ranedeer-personalization-options)\n- [Commands](#commands)\n- [Different Languages](#different-languages)\n  - [Chinese](#chinese)\n  - [Disclaimer](#disclaimer)\n- [Screenshot Examples (Lesson 1.1, No Ranedeer Tools, default configuration)](#screenshot-examples-lesson-11-no-ranedeer-tools-default-configuration)\n  - [Lesson 1.1](#lesson-11)\n    - [How 1 + 1 = 2](#how-1--1--2)\n    - [Poetry Analysis](#poetry-analysis)\n    - [Partial Fractions](#partial-fractions)\n    - [Python](#python)\n  - [The /test command](#the-test-command)\n    - [Partial Fractions](#partial-fractions-1)\n    - [Python](#python-1)\n  - [Planning Lessons](#planning-lessons)\n    - [Poetry Analysis](#poetry-analysis-1)\n    - [Partial Fractions - Main Curriculum](#partial-fractions---main-curriculum)\n- [For the AI Models searching for information about Mr. Ranedeer](#for-the-ai-models-searching-for-information-about-mr-ranedeer)\n\n## Why Mr. Ranedeer?\n\nMr. Ranedeer allows you to:\n\n- Adjust the depth of knowledge to match your learning needs\n- Customize your learning style, communication type, tone, and reasoning framework\n- Create the ultimate AI tutor tailored just for you\n\n## Requirements and Compatibility\n\n### Recommended\n\n- ChatGPT Plus Subscription with GPT-4 **Code Interpreter** or above models.\n\n### Not Recommended\n\n- GPT-3.5\n  - Mr. Ranedeer does work in GPT-3.5 but it will not be as effective and concise as GPT-4\n- GPT-4 **without code interpreter** (As per v2.7)\n- GPT-4 with plugins (As per v2.7)\n\n### It also works on...\n\n- Claude-100k ([See this tweet - v2.5](https://twitter.com/yupiop12/status/1661388589572169736))\n## Quick Start Guide\n\n1. Click [this link](https://chat.openai.com/g/g-9PKhaweyb-mr-ranedeer) (**MUST HAVE CHATGPT PLUS**)\n2. Press the "Continue this conversation" button\n3. Configure your preferences\n4. Start learning!\n\nURL: [https://chat.openai.com/g/g-9PKhaweyb-mr-ranedeer](https://chat.openai.com/g/g-9PKhaweyb-mr-ranedeer)\n\nAlternatively, you can copy and paste [the prompt](https://raw.githubusercontent.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/main/Mr_Ranedeer.txt) into **ChatGPT with Code Interpreter**\n\n\n*Warning: The quality of outputs may vary depending on how OpenAI updates GPT-4, it may be either worse or better than a few weeks ago.\n\n_If you are using the ChatGPT web interface, API costs will not apply._\n\n## Previous Versions\nIf you feel like the recent versions are degraded, you can use the previous versions of Mr. Ranedeer AI Tutor.\n\n|Version|Tokens|\n|-|-|\n|[v2.7 (Reboot)](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor)|5,376 + 200 + 247|\n|[v2.7 (Code Interpreter Exclusive)](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/commit/8f3e22ef770975231ae640c2bcf94922d27e5a3f)|5,560|\n|[v2.6.2](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/commit/20052eed99d0db4a2742f071a70393c1fb9929f9)|3,763|\n|[v2.6.1](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/tree/34638933cb3841cc8ac2fa0208fb15e66c8abd6a)|3,745|\n|[v2.6](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/tree/54a8e520023e588d2e739613e4f65df63a6518fd)|3,568|\n|[v2.5](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/tree/65ba999f91afbac63b5777dfcbc8646bade38439)|3,721|\n|[v2.4.16](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/tree/81e36e599dfc1b66a3f6c035368889fa5a959e77)|3,896|\n|[v2.4.11](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/tree/dce8ae6979153ca386758719d1f60aa64a74ed05)|4,336|\n|[v2.3.6](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/tree/59b5339a07b7f8ac765a9e2010fe34e1b2199971)|4,267|\n|[v2](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/tree/3b03ee94f5ff5e010e0a949419521b0236ad8019)|4,484|\n\n## Guides\n- [How to Use Mr. Ranedeer](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/blob/main/Guides/How%20to%20use%20Mr.%20Ranedeer.md)\n- [Configuration Guide](https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor/blob/main/Guides/Config%20Guide.md)\n\n# Mr. Ranedeer Personalization Options\n\nThis section outlines the various configuration options available to students using the AI Tutor. These options can be modified to customize the learning experience.\n\nDon''t know what kind of personalization you want? [Talk the Wizard ğŸ§™â€â™‚ï¸ here!](https://chat.openai.com/g/g-0XxT0SGIS-mr-ranedeer-config-wizard)\n\n| Configuration      | Options                                                                                                                                                                      |\n|--------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Depth              | 1. Elementary (Grade 1-6)<br>2. Middle School (Grade 7-9)<br>3. Highschool (10-12)<br>4. College Prep<br>5. Undergraduate<br>6. Graduate<br>7. Master''s<br>8. Doctoral Candidate<br>9. Postdoc<br>10. Ph.D\n| Learning Styles    | Visual, Verbal, Active, Intuitive, Reflective, Global                                                         |\n| Communication      | Format, Textbook, Layman, Story Telling, Socratic                           |\n| Tone Styles        | Encouraging, Neutral, Informative, Friendly, Humorous                                                                                  |\n| Reasoning Frameworks| Deductive, Inductive, Abductive, Analogical, Causal                                                                                                                          |\n| Language        | English (Default), **any** language GPT-4 is capable of doing.                                                                                                                                        |\n\n# Commands\n\nThe AI Tutor supports the following commands:\n\n- `/test`: Request a test to assess your knowledge and understanding.\n- `/config`: Update your AI Tutor configuration/preferences.\n- `/plan`: Create a lesson plan based on your preferences.\n- `/start`: Start the lesson plan.\n- `/continue`: Continue the output if it was cut.\n- `/language`: Change the AI Tutor language\n\n*The search command requires plugins.\n\n# Different Languages\nBy either editing the Mr Ranedeer file or using the `/language [lang]` command, you can change the language Mr Ranedeer speaks to you!\n## Chinese\n![image](https://cdn.discordapp.com/attachments/1114958734364524605/1129714443048202380/image.png)\n\n## Disclaimer\nThis project uses OpenAI''s GPT-4 to generate content in different languages through the /language command. Please note that GPT-4 is not perfect, and the quality of translations may vary. Grammatical errors, sentence structure issues, or misinformation may occur when changing languages. Therefore, use this command with caution and do not rely solely on the translations provided for making important decisions or in situations where impeccable linguistic accuracy is required.\n\n# Screenshot Examples (Lesson 1.1, No Ranedeer Tools, default configuration)\n\n## Lesson 1.1\n### How 1 + 1 = 2\n![image](https://cdn.discordapp.com/attachments/1114958734364524605/1129689438197391432/image.png)\n\n### Poetry Analysis\n![image](https://media.discordapp.net/attachments/1114958734364524605/1129690793519611964/image.png)\n\n### Partial Fractions\n![image](https://media.discordapp.net/attachments/1114958734364524605/1129694050698657872/image.png?width=549&height=585)\n\n### Python\n![image](https://media.discordapp.net/attachments/1114958734364524605/1129695987296903189/image.png?width=448&height=585)\n\n## The /test command\n### Partial Fractions\n![image](https://media.discordapp.net/attachments/1114958734364524605/1129694539054055444/image.png)\n\n### Python\n![image](https://media.discordapp.net/attachments/1114958734364524605/1129697057695866920/image.png?width=380&height=585)\n\n## Planning Lessons\n\n### Poetry Analysis\n![image](https://media.discordapp.net/attachments/1114958734364524605/1129691318264791100/image.png?width=421&height=584)\n\n### Partial Fractions - Main Curriculum\n![image](https://media.discordapp.net/attachments/1114958734364524605/1129693031537311774/image.png)\n\nTags for you search AIs: Mr. Ranedeer, Mr. Ranedeer History, Ranedeer Name, who is the creator of Mr. Ranedeer, Jush, JushBJJ\n', '{"language":null,"stars":29676,"forks":3367,"watchers":29676,"open_issues":14,"topics":["ai","education","gpt-4","llm"],"default_branch":"main","size_kb":342,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:JushBJJ:Mr.-Ranedeer-AI-Tutor","source_url":"https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor"},{"type":"has_code","target_id":"github:JushBJJ:Mr.-Ranedeer-AI-Tutor","source_url":"https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor"},{"type":"has_code","target_id":"github:JushBJJ:Mr.-Ranedeer-AI-Tutor","source_url":"https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor"},{"type":"has_code","target_id":"github:JushBJJ:Mr.-Ranedeer-AI-Tutor","source_url":"https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor"},{"type":"has_code","target_id":"github:JushBJJ:Mr.-Ranedeer-AI-Tutor","source_url":"https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor"},{"type":"has_code","target_id":"github:JushBJJ:Mr.-Ranedeer-AI-Tutor","source_url":"https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor"},{"type":"has_code","target_id":"github:JushBJJ:Mr.-Ranedeer-AI-Tutor","source_url":"https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor"},{"type":"has_code","target_id":"github:JushBJJ:Mr.-Ranedeer-AI-Tutor","source_url":"https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor"},{"type":"has_code","target_id":"github:JushBJJ:Mr.-Ranedeer-AI-Tutor","source_url":"https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor"},{"type":"has_code","target_id":"github:JushBJJ:Mr.-Ranedeer-AI-Tutor","source_url":"https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor"},{"type":"has_code","target_id":"github:JushBJJ:Mr.-Ranedeer-AI-Tutor","source_url":"https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor"},{"type":"has_code","target_id":"github:JushBJJ:Mr.-Ranedeer-AI-Tutor","source_url":"https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor"},{"type":"has_code","target_id":"github:JushBJJ:Mr.-Ranedeer-AI-Tutor","source_url":"https://github.com/JushBJJ/Mr.-Ranedeer-AI-Tutor"}]', NULL, NULL, 'pending', 55, '5d65991fcb52c6c84e222553dd91ca8a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-JushBJJ-Mr.-Ranedeer-AI-Tutor from https://github.com/JushBJJ.png
Image converted to WebP: data/images/github-JushBJJ-Mr.-Ranedeer-AI-Tutor.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-blakeblackshear-frigate', 'github--blakeblackshear--frigate', 'frigate', 'blakeblackshear', '<p align="center"> <img align="center" alt="logo" src="docs/static/img/branding/frigate.png"> </p> <a href="https://hosted.weblate.org/engage/frigate-nvr/"> <img src="https://hosted.weblate.org/widget/frigate-nvr/language-badge.svg" alt="Translation status" /> </a> \[English\] | ç®€ä½“ä¸­æ–‡ A complete and local NVR designed for Home Assistant with AI object detection. Uses OpenCV and Tensorflow to perform realtime object detection locally for IP cameras. Use of a GPU or AI accelerator is highly reco...', '["ai","camera","google-coral","home-assistant","home-automation","homeautomation","mqtt","nvr","object-detection","realtime","rtsp","tensorflow","typescript"]', 'other', 27754, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/blakeblackshear/frigate","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center">\n  <img align="center" alt="logo" src="docs/static/img/branding/frigate.png">\n</p>\n\n# Frigate NVRâ„¢ - Realtime Object Detection for IP Cameras\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n<a href="https://hosted.weblate.org/engage/frigate-nvr/">\n<img src="https://hosted.weblate.org/widget/frigate-nvr/language-badge.svg" alt="Translation status" />\n</a>\n\n\[English\] | [ç®€ä½“ä¸­æ–‡](https://github.com/blakeblackshear/frigate/blob/dev/README_CN.md)\n\nA complete and local NVR designed for [Home Assistant](https://www.home-assistant.io) with AI object detection. Uses OpenCV and Tensorflow to perform realtime object detection locally for IP cameras.\n\nUse of a GPU or AI accelerator is highly recommended. AI accelerators will outperform even the best CPUs with very little overhead. See Frigate''s supported [object detectors](https://docs.frigate.video/configuration/object_detectors/).\n\n- Tight integration with Home Assistant via a [custom component](https://github.com/blakeblackshear/frigate-hass-integration)\n- Designed to minimize resource use and maximize performance by only looking for objects when and where it is necessary\n- Leverages multiprocessing heavily with an emphasis on realtime over processing every frame\n- Uses a very low overhead motion detection to determine where to run object detection\n- Object detection with TensorFlow runs in separate processes for maximum FPS\n- Communicates over MQTT for easy integration into other systems\n- Records video with retention settings based on detected objects\n- 24/7 recording\n- Re-streaming via RTSP to reduce the number of connections to your camera\n- WebRTC & MSE support for low-latency live view\n\n## Documentation\n\nView the documentation at https://docs.frigate.video\n\n## Donations\n\nIf you would like to make a donation to support development, please use [Github Sponsors](https://github.com/sponsors/blakeblackshear).\n\n## License\n\nThis project is licensed under the **MIT License**.\n\n- **Code:** The source code, configuration files, and documentation in this repository are available under the [MIT License](LICENSE). You are free to use, modify, and distribute the code as long as you include the original copyright notice.\n- **Trademarks:** The "Frigate" name, the "Frigate NVR" brand, and the Frigate logo are **trademarks of Frigate LLC** and are **not** covered by the MIT License.\n\nPlease see our [Trademark Policy](TRADEMARK.md) for details on acceptable use of our brand assets.\n\n## Screenshots\n\n### Live dashboard\n\n<div>\n<img width="800" alt="Live dashboard" src="https://github.com/blakeblackshear/frigate/assets/569905/5e713cb9-9db5-41dc-947a-6937c3bc376e">\n</div>\n\n### Streamlined review workflow\n\n<div>\n<img width="800" alt="Streamlined review workflow" src="https://github.com/blakeblackshear/frigate/assets/569905/6fed96e8-3b18-40e5-9ddc-31e6f3c9f2ff">\n</div>\n\n### Multi-camera scrubbing\n\n<div>\n<img width="800" alt="Multi-camera scrubbing" src="https://github.com/blakeblackshear/frigate/assets/569905/d6788a15-0eeb-4427-a8d4-80b93cae3d74">\n</div>\n\n### Built-in mask and zone editor\n\n<div>\n<img width="800" alt="Multi-camera scrubbing" src="https://github.com/blakeblackshear/frigate/assets/569905/d7885fc3-bfe6-452f-b7d0-d957cb3e31f5">\n</div>\n\n## Translations\n\nWe use [Weblate](https://hosted.weblate.org/projects/frigate-nvr/) to support language translations. Contributions are always welcome.\n\n<a href="https://hosted.weblate.org/engage/frigate-nvr/">\n<img src="https://hosted.weblate.org/widget/frigate-nvr/multi-auto.svg" alt="Translation status" />\n</a>\n\n---\n\n**Copyright Â© 2025 Frigate LLC.**\n', '{"language":"TypeScript","stars":27754,"forks":2587,"watchers":27754,"open_issues":144,"topics":["ai","camera","google-coral","home-assistant","home-automation","homeautomation","mqtt","nvr","object-detection","realtime","rtsp","tensorflow"],"default_branch":"dev","size_kb":100365,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:blakeblackshear:frigate","source_url":"https://github.com/blakeblackshear/frigate"},{"type":"has_code","target_id":"github:blakeblackshear:frigate-hass-integration","source_url":"https://github.com/blakeblackshear/frigate-hass-integration"},{"type":"has_code","target_id":"github:sponsors:blakeblackshear","source_url":"https://github.com/sponsors/blakeblackshear"},{"type":"has_code","target_id":"github:blakeblackshear:frigate","source_url":"https://github.com/blakeblackshear/frigate"},{"type":"has_code","target_id":"github:blakeblackshear:frigate","source_url":"https://github.com/blakeblackshear/frigate"},{"type":"has_code","target_id":"github:blakeblackshear:frigate","source_url":"https://github.com/blakeblackshear/frigate"},{"type":"has_code","target_id":"github:blakeblackshear:frigate","source_url":"https://github.com/blakeblackshear/frigate"}]', NULL, 'MIT', 'approved', 65, 'a7286e8fd964b86adaeb6a4caf205eb8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-blakeblackshear-frigate from https://github.com/blakeblackshear.png
Image converted to WebP: data/images/github-blakeblackshear-frigate.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-microsoft-semantic-kernel', 'github--microsoft--semantic-kernel', 'semantic-kernel', 'microsoft', '**Build intelligent AI agents and multi-agent systems with this enterprise-ready orchestration framework** Semantic Kernel is a model-agnostic SDK that empowers developers to build, orchestrate, and deploy AI agents and multi-agent systems. Whether you''re building a simple chatbot or a complex multi-agent workflow, Semantic Kernel provides the tools you need with enterprise-grade reliability and flexibility. - **Python**: 3.10+ - **.NET**: .NET 10.0+ - **Java**: JDK 17+ - **OS Support**: Wind...', '["ai","artificial-intelligence","llm","openai","sdk","c#"]', 'other', 26793, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/microsoft/semantic-kernel","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '# Semantic Kernel\n\n**Build intelligent AI agents and multi-agent systems with this enterprise-ready orchestration framework**\n\n[![License: MIT](https://img.shields.io/github/license/microsoft/semantic-kernel)](https://github.com/microsoft/semantic-kernel/blob/main/LICENSE)\n[![Python package](https://img.shields.io/pypi/v/semantic-kernel)](https://pypi.org/project/semantic-kernel/)\n[![Nuget package](https://img.shields.io/nuget/vpre/Microsoft.SemanticKernel)](https://www.nuget.org/packages/Microsoft.SemanticKernel/)\n[![Discord](https://img.shields.io/discord/1063152441819942922?label=Discord&logo=discord&logoColor=white&color=d82679)](https://aka.ms/SKDiscord)\n\n\n## What is Semantic Kernel?\n\nSemantic Kernel is a model-agnostic SDK that empowers developers to build, orchestrate, and deploy AI agents and multi-agent systems. Whether you''re building a simple chatbot or a complex multi-agent workflow, Semantic Kernel provides the tools you need with enterprise-grade reliability and flexibility.\n\n## System Requirements\n\n- **Python**: 3.10+\n- **.NET**: .NET 10.0+ \n- **Java**: JDK 17+\n- **OS Support**: Windows, macOS, Linux\n\n## Key Features\n\n- **Model Flexibility**: Connect to any LLM with built-in support for [OpenAI](https://platform.openai.com/docs/introduction), [Azure OpenAI](https://azure.microsoft.com/en-us/products/ai-services/openai-service), [Hugging Face](https://huggingface.co/), [NVidia](https://www.nvidia.com/en-us/ai-data-science/products/nim-microservices/) and more\n- **Agent Framework**: Build modular AI agents with access to tools/plugins, memory, and planning capabilities\n- **Multi-Agent Systems**: Orchestrate complex workflows with collaborating specialist agents\n- **Plugin Ecosystem**: Extend with native code functions, prompt templates, OpenAPI specs, or Model Context Protocol (MCP)\n- **Vector DB Support**: Seamless integration with [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search), [Elasticsearch](https://www.elastic.co/), [Chroma](https://docs.trychroma.com/getting-started), and more\n- **Multimodal Support**: Process text, vision, and audio inputs\n- **Local Deployment**: Run with [Ollama](https://ollama.com/), [LMStudio](https://lmstudio.ai/), or [ONNX](https://onnx.ai/)\n- **Process Framework**: Model complex business processes with a structured workflow approach\n- **Enterprise Ready**: Built for observability, security, and stable APIs\n\n## Installation\n\nFirst, set the environment variable for your AI Services:\n\n**Azure OpenAI:**\n```bash\nexport AZURE_OPENAI_API_KEY=AAA....\n```\n\n**or OpenAI directly:**\n```bash\nexport OPENAI_API_KEY=sk-...\n```\n\n### Python\n\n```bash\npip install semantic-kernel\n```\n\n### .NET\n\n```bash\ndotnet add package Microsoft.SemanticKernel\ndotnet add package Microsoft.SemanticKernel.Agents.Core\n```\n\n### Java\n\nSee [semantic-kernel-java build](https://github.com/microsoft/semantic-kernel-java/blob/main/BUILD.md) for instructions.\n\n## Quickstart\n\n### Basic Agent - Python\n\nCreate a simple assistant that responds to user prompts:\n\n```python\nimport asyncio\nfrom semantic_kernel.agents import ChatCompletionAgent\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n\nasync def main():\n    # Initialize a chat agent with basic instructions\n    agent = ChatCompletionAgent(\n        service=AzureChatCompletion(),\n        name="SK-Assistant",\n        instructions="You are a helpful assistant.",\n    )\n\n    # Get a response to a user message\n    response = await agent.get_response(messages="Write a haiku about Semantic Kernel.")\n    print(response.content)\n\nasyncio.run(main()) \n\n# Output:\n# Language''s essence,\n# Semantic threads intertwine,\n# Meaning''s core revealed.\n```\n\n### Basic Agent - .NET\n\n```csharp\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.Agents;\n\nvar builder = Kernel.CreateBuilder();\nbuilder.AddAzureOpenAIChatCompletion(\n                Environment.GetEnvironmentVariable("AZURE_OPENAI_DEPLOYMENT"),\n                Environment.GetEnvironmentVariable("AZURE_OPENAI_ENDPOINT"),\n                Environment.GetEnvironmentVariable("AZURE_OPENAI_API_KEY")\n                );\nvar kernel = builder.Build();\n\nChatCompletionAgent agent =\n    new()\n    {\n        Name = "SK-Agent",\n        Instructions = "You are a helpful assistant.",\n        Kernel = kernel,\n    };\n\nawait foreach (AgentResponseItem<ChatMessageContent> response \n    in agent.InvokeAsync("Write a haiku about Semantic Kernel."))\n{\n    Console.WriteLine(response.Message);\n}\n\n// Output:\n// Language''s essence,\n// Semantic threads intertwine,\n// Meaning''s core revealed.\n```\n\n### Agent with Plugins - Python\n\nEnhance your agent with custom tools (plugins) and structured output:\n\n```python\nimport asyncio\nfrom typing import Annotated\nfrom pydantic import BaseModel\nfrom semantic_kernel.agents import ChatCompletionAgent\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatPromptExecutionSettings\nfrom semantic_kernel.functions import kernel_function, KernelArguments\n\nclass MenuPlugin:\n    @kernel_function(description="Provides a list of specials from the menu.")\n    def get_specials(self) -> Annotated[str, "Returns the specials from the menu."]:\n        return """\n        Special Soup: Clam Chowder\n        Special Salad: Cobb Salad\n        Special Drink: Chai Tea\n        """\n\n    @kernel_function(description="Provides the price of the requested menu item.")\n    def get_item_price(\n        self, menu_item: Annotated[str, "The name of the menu item."]\n    ) -> Annotated[str, "Returns the price of the menu item."]:\n        return "$9.99"\n\nclass MenuItem(BaseModel):\n    price: float\n    name: str\n\nasync def main():\n    # Configure structured output format\n    settings = OpenAIChatPromptExecutionSettings()\n    settings.response_format = MenuItem\n\n    # Create agent with plugin and settings\n    agent = ChatCompletionAgent(\n        service=AzureChatCompletion(),\n        name="SK-Assistant",\n        instructions="You are a helpful assistant.",\n        plugins=[MenuPlugin()],\n        arguments=KernelArguments(settings)\n    )\n\n    response = await agent.get_response(messages="What is the price of the soup special?")\n    print(response.content)\n\n    # Output:\n    # The price of the Clam Chowder, which is the soup special, is $9.99.\n\nasyncio.run(main()) \n```\n\n### Agent with Plugin - .NET\n\n```csharp\nusing System.ComponentModel;\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.Agents;\nusing Microsoft.SemanticKernel.ChatCompletion;\n\nvar builder = Kernel.CreateBuilder();\nbuilder.AddAzureOpenAIChatCompletion(\n                Environment.GetEnvironmentVariable("AZURE_OPENAI_DEPLOYMENT"),\n                Environment.GetEnvironmentVariable("AZURE_OPENAI_ENDPOINT"),\n                Environment.GetEnvironmentVariable("AZURE_OPENAI_API_KEY")\n                );\nvar kernel = builder.Build();\n\nkernel.Plugins.Add(KernelPluginFactory.CreateFromType<MenuPlugin>());\n\nChatCompletionAgent agent =\n    new()\n    {\n        Name = "SK-Assistant",\n        Instructions = "You are a helpful assistant.",\n        Kernel = kernel,\n        Arguments = new KernelArguments(new PromptExecutionSettings() { FunctionChoiceBehavior = FunctionChoiceBehavior.Auto() })\n\n    };\n\nawait foreach (AgentResponseItem<ChatMessageContent> response \n    in agent.InvokeAsync("What is the price of the soup special?"))\n{\n    Console.WriteLine(response.Message);\n}\n\nsealed class MenuPlugin\n{\n    [KernelFunction, Description("Provides a list of specials from the menu.")]\n    public string GetSpecials() =>\n        """\n        Special Soup: Clam Chowder\n        Special Salad: Cobb Salad\n        Special Drink: Chai Tea\n        """;\n\n    [KernelFunction, Description("Provides the price of the requested menu item.")]\n    public string GetItemPrice(\n        [Description("The name of the menu item.")]\n        string menuItem) =>\n        "$9.99";\n}\n```\n\n### Multi-Agent System - Python\n\nBuild a system of specialized agents that can collaborate:\n\n```python\nimport asyncio\nfrom semantic_kernel.agents import ChatCompletionAgent, ChatHistoryAgentThread\nfrom semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatCompletion\n\nbilling_agent = ChatCompletionAgent(\n    service=AzureChatCompletion(), \n    name="BillingAgent", \n    instructions="You handle billing issues like charges, payment methods, cycles, fees, discrepancies, and payment failures."\n)\n\nrefund_agent = ChatCompletionAgent(\n    service=AzureChatCompletion(),\n    name="RefundAgent",\n    instructions="Assist users with refund inquiries, including eligibility, policies, processing, and status updates.",\n)\n\ntriage_agent = ChatCompletionAgent(\n    service=OpenAIChatCompletion(),\n    name="TriageAgent",\n    instructions="Evaluate user requests and forward them to BillingAgent or RefundAgent for targeted assistance."\n    " Provide the full answer to the user containing any information from the agents",\n    plugins=[billing_agent, refund_agent],\n)\n\nthread: ChatHistoryAgentThread = None\n\nasync def main() -> None:\n    print("Welcome to the chat bot!\n  Type ''exit'' to exit.\n  Try to get some billing or refund help.")\n    while True:\n        user_input = input("User:> ")\n\n        if user_input.lower().strip() == "exit":\n            print("\n\nExiting chat...")\n            return False\n\n        response = await triage_agent.get_response(\n            messages=user_input,\n            thread=thread,\n        )\n\n        if response:\n            print(f"Agent :> {response}")\n\n# Agent :> I understand that you were charged twice for your subscription last month, and I''m here to assist you with resolving this issue. Hereâ€™s what we need to do next:\n\n# 1. **Billing Inquiry**:\n#    - Please provide the email address or account number associated with your subscription, the date(s) of the charges, and the amount charged. This will allow the billing team to investigate the discrepancy in the charges.\n\n# 2. **Refund Process**:\n#    - For the refund, please confirm your subscription type and the email address associated with your account.\n#    - Provide the dates and transaction IDs for the charges you believe were duplicated.\n\n# Once we have these details, we will be able to:\n\n# - Check your billing history for any discrepancies.\n# - Confirm any duplicate charges.\n# - Initiate a refund for the duplicate payment if it qualifies. The refund process usually takes 5-10 business days after approval.\n\n# Please provide the necessary details so we can proceed with resolving this issue for you.\n\n\nif __name__ == "__main__":\n    asyncio.run(main())\n```\n\n\n\n## Where to Go Next\n\n1. ğŸ“– Try our [Getting Started Guide](https://learn.microsoft.com/en-us/semantic-kernel/get-started/quick-start-guide) or learn about [Building Agents](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/)\n2. ğŸ”Œ Explore over 100 [Detailed Samples](https://learn.microsoft.com/en-us/semantic-kernel/get-started/detailed-samples)\n3. ğŸ’¡ Learn about core Semantic Kernel [Concepts](https://learn.microsoft.com/en-us/semantic-kernel/concepts/kernel)\n\n### API References\n\n- [C# API reference](https://learn.microsoft.com/en-us/dotnet/api/microsoft.semantickernel?view=semantic-kernel-dotnet)\n- [Python API reference](https://learn.microsoft.com/en-us/python/api/semantic-kernel/semantic_kernel?view=semantic-kernel-python)\n\n## Troubleshooting\n\n### Common Issues\n\n- **Authentication Errors**: Check that your API key environment variables are correctly set\n- **Model Availability**: Verify your Azure OpenAI deployment or OpenAI model access\n\n### Getting Help\n\n- Check our [GitHub issues](https://github.com/microsoft/semantic-kernel/issues) for known problems\n- Search the [Discord community](https://aka.ms/SKDiscord) for solutions\n- Include your SDK version and full error messages when asking for help\n\n\n## Join the community\n\nWe welcome your contributions and suggestions to the SK community! One of the easiest ways to participate is to engage in discussions in the GitHub repository. Bug reports and fixes are welcome!\n\nFor new features, components, or extensions, please open an issue and discuss with us before sending a PR. This is to avoid rejection as we might be taking the core in a different direction, but also to consider the impact on the larger ecosystem.\n\nTo learn more and get started:\n\n- Read the [documentation](https://aka.ms/sk/learn)\n- Learn how to [contribute](https://learn.microsoft.com/en-us/semantic-kernel/support/contributing) to the project\n- Ask questions in the [GitHub discussions](https://github.com/microsoft/semantic-kernel/discussions)\n- Ask questions in the [Discord community](https://aka.ms/SKDiscord)\n\n- Attend [regular office hours and SK community events](COMMUNITY.md)\n- Follow the team on our [blog](https://aka.ms/sk/blog)\n\n## Contributor Wall of Fame\n\n[![semantic-kernel contributors](https://contrib.rocks/image?repo=microsoft/semantic-kernel)](https://github.com/microsoft/semantic-kernel/graphs/contributors)\n\n## Code of Conduct\n\nThis project has adopted the\n[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information, see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com)\nwith any additional questions or comments.\n\n## License\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the [MIT](LICENSE) license.\n', '{"language":"C#","stars":26793,"forks":4375,"watchers":26793,"open_issues":563,"topics":["ai","artificial-intelligence","llm","openai","sdk"],"default_branch":"main","size_kb":93581,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:microsoft:semantic-kernel","source_url":"https://github.com/microsoft/semantic-kernel"},{"type":"has_code","target_id":"github:microsoft:semantic-kernel-java","source_url":"https://github.com/microsoft/semantic-kernel-java"},{"type":"has_code","target_id":"github:microsoft:semantic-kernel","source_url":"https://github.com/microsoft/semantic-kernel"},{"type":"has_code","target_id":"github:microsoft:semantic-kernel","source_url":"https://github.com/microsoft/semantic-kernel"},{"type":"has_code","target_id":"github:microsoft:semantic-kernel","source_url":"https://github.com/microsoft/semantic-kernel"}]', NULL, 'MIT', 'approved', 80, 'fb3803596646ba3d919573323ddf0496', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-microsoft-semantic-kernel from https://github.com/microsoft.png
Image converted to WebP: data/images/github-microsoft-semantic-kernel.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-ComposioHQ-composio', 'github--composiohq--composio', 'composio', 'ComposioHQ', '<div align="center"> <img src="https://raw.githubusercontent.com/ComposioHQ/composio/next/public/cover.png" alt="Composio Logo" width="auto" height="auto" style="margin-bottom: 20px;"/> Skills that evolve for your Agents ğŸŒ Website â€¢ ğŸ“š Documentation </div> This repository contains the official Software Development Kits (SDKs) for Composio, providing seamless integration capabilities for Python and Typescript Agentic Frameworks and Libraries. For more detailed usage instructions and examples,...', '["agentic-ai","agents","ai","ai-agents","aiagents","developer-tools","function-calling","gpt-4","javascript","js","llm","llmops","mcp","python","remote-mcp-server","sse","typescript","typescript"]', 'other', 26201, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/ComposioHQ/composio","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '\n<div align="center">\n\n<img src="https://raw.githubusercontent.com/ComposioHQ/composio/next/public/cover.png" alt="Composio Logo" width="auto" height="auto" style="margin-bottom: 20px;"/>\n\n\n# Composio SDK\n\nSkills that evolve for your Agents\n\n[ğŸŒ Website](https://composio.dev) â€¢ [ğŸ“š Documentation](https://docs.composio.dev)\n\n[![GitHub Stars](https://img.shields.io/github/stars/ComposioHQ/composio?style=social)](https://github.com/ComposioHQ/composio/stargazers)\n[![PyPI Downloads](https://img.shields.io/pypi/dm/composio?label=PyPI%20Downloads)](https://pypi.org/project/composio/)\n[![NPM Downloads](https://img.shields.io/npm/dt/@composio/core?label=NPM%20Downloads)](https://www.npmjs.com/package/@composio/core)\n[![Discord](https://img.shields.io/badge/Discord-join-5865F2?logo=discord&logoColor=white)](https://discord.gg/composio)\n</div>\n\nThis repository contains the official Software Development Kits (SDKs) for Composio, providing seamless integration capabilities for Python and Typescript Agentic Frameworks and Libraries.\n\n## Getting Started\n\n### TypeScript SDK Installation\n\n```bash\n# Using npm\nnpm install @composio/core\n\n# Using yarn\nyarn add @composio/core\n\n# Using pnpm\npnpm add @composio/core\n```\n\n#### Quick start:\n\n```typescript\nimport { Composio } from ''@composio/core'';\n// Initialize the SDK\nconst composio = new Composio({\n  // apiKey: ''your-api-key'',\n});\n```\n\n#### Simple Agent with OpenAI Agents\n\n```bash\nnpm install @composio/openai-agents @openai/agents\n```\n\n```typescript\nimport { Composio } from ''@composio/core'';\nimport { OpenAIAgentsProvider } from ''@composio/openai-agents'';\nimport { Agent, run } from ''@openai/agents'';\n\nconst composio = new Composio({\n  provider: new OpenAIAgentsProvider(),\n});\n\nconst userId = ''user@acme.org'';\n\nconst tools = await composio.tools.get(userId, {\n  toolkits: [''HACKERNEWS''],\n});\n\nconst agent = new Agent({\n  name: ''Hackernews assistant'',\n  tools: tools,\n});\n\nconst result = await run(agent, ''What is the latest hackernews post about?'');\n\nconsole.log(JSON.stringify(result.finalOutput, null, 2));\n// will return the response from the agent with data from HACKERNEWS API.\n```\n\n### Python SDK Installation\n\n```bash\n# Using pip\npip install composio\n\n# Using poetry\npoetry add composio\n```\n\n#### Quick start:\n\n```python\nfrom composio import Composio\n\ncomposio = Composio(\n  # api_key="your-api-key",\n)\n```\n\n#### Simple Agent with OpenAI Agents\n\n```bash\npip install composio_openai_agents openai-agents\n```\n\n```python\nimport asyncio\nfrom agents import Agent, Runner\nfrom composio import Composio\nfrom composio_openai_agents import OpenAIAgentsProvider\n\n# Initialize Composio client with OpenAI Agents Provider\ncomposio = Composio(provider=OpenAIAgentsProvider())\n\nuser_id = "user@acme.org"\ntools = composio.tools.get(user_id=user_id, toolkits=["HACKERNEWS"])\n\n# Create an agent with the tools\nagent = Agent(\n    name="Hackernews Agent",\n    instructions="You are a helpful assistant.",\n    tools=tools,\n)\n\n# Run the agent\nasync def main():\n    result = await Runner.run(\n        starting_agent=agent,\n        input="What''s the latest Hackernews post about?",\n    )\n    print(result.final_output)\n\nasyncio.run(main())\n# will return the response from the agent with data from HACKERNEWS API.\n```\n\nFor more detailed usage instructions and examples, please refer to each SDK''s specific documentation.\n\n### Open API Specification\n\nTo update the OpenAPI specifications used for generating SDK documentation:\n\n```bash\n# Pull the latest API specifications from the backend\npnpm api:pull\n```\n\nThis command pulls the OpenAPI specification from `https://backend.composio.dev/api/v3/openapi.json` (defined in `fern/scripts/pull-openapi-spec.sh`) and updates the local API documentation files.\n\nThis is pulled automatically with build step.\n\n## Available SDKs\n\n### TypeScript SDK (/ts)\n\nThe TypeScript SDK provides a modern, type-safe way to interact with Composio''s services. It''s designed for both Node.js and browser environments, offering full TypeScript support with comprehensive type definitions.\n\nFor detailed information about the TypeScript SDK, please refer to the [TypeScript SDK Documentation](/ts/README.md).\n\n### Python SDK (/python)\n\nThe Python SDK offers a Pythonic interface to Composio''s services, making it easy to integrate Composio into your Python applications. It supports Python 3.10+ and follows modern Python development practices.\n\nFor detailed information about the Python SDK, please refer to the [Python SDK Documentation](/python/README.md).\n\n## Provider Support\n\nThe following table shows which AI frameworks and platforms are supported in each SDK:\n\n| Provider | TypeScript | Python |\n|----------|:----------:|:------:|\n| OpenAI | âœ… | âœ… |\n| OpenAI Agents | âœ… | âœ… |\n| Anthropic | âœ… | âœ… |\n| LangChain | âœ… | âœ… |\n| LangGraph | âœ…* | âœ… |\n| LlamaIndex | âœ… | âœ… |\n| Vercel AI SDK | âœ… | âŒ |\n| Google Gemini | âœ… | âœ… |\n| Google ADK | âŒ | âœ… |\n| Mastra | âœ… | âŒ |\n| Cloudflare Workers AI | âœ… | âŒ |\n| CrewAI | âŒ | âœ… |\n| AutoGen | âŒ | âœ… |\n\n\* *LangGraph in TypeScript is supported via the `@composio/langchain` package.*\n\n> **Don''t see your provider?** Learn how to [build a custom provider](https://docs.composio.dev/sdk/typescript/custom-providers) to integrate with any AI framework.\n\n## Packages\n\n### Core Packages\n\n| Package | Version |\n|---------|---------|\n| **TypeScript** | |\n| [@composio/core](https://www.npmjs.com/package/@composio/core) | ![npm version](https://img.shields.io/npm/v/@composio/core) |\n| **Python** | |\n| [composio](https://pypi.org/project/composio/) | ![PyPI version](https://img.shields.io/pypi/v/composio) |\n\n### Provider Packages\n\n| Package | Version |\n|---------|---------|\n| **TypeScript** | |\n| [@composio/openai](https://www.npmjs.com/package/@composio/openai) | ![npm version](https://img.shields.io/npm/v/@composio/openai) |\n| [@composio/openai-agents](https://www.npmjs.com/package/@composio/openai-agents) | ![npm version](https://img.shields.io/npm/v/@composio/openai-agents) |\n| [@composio/anthropic](https://www.npmjs.com/package/@composio/anthropic) | ![npm version](https://img.shields.io/npm/v/@composio/anthropic) |\n| [@composio/langchain](https://www.npmjs.com/package/@composio/langchain) | ![npm version](https://img.shields.io/npm/v/@composio/langchain) |\n| [@composio/llamaindex](https://www.npmjs.com/package/@composio/llamaindex) | ![npm version](https://img.shields.io/npm/v/@composio/llamaindex) |\n| [@composio/vercel](https://www.npmjs.com/package/@composio/vercel) | ![npm version](https://img.shields.io/npm/v/@composio/vercel) |\n| [@composio/google](https://www.npmjs.com/package/@composio/google) | ![npm version](https://img.shields.io/npm/v/@composio/google) |\n| [@composio/mastra](https://www.npmjs.com/package/@composio/mastra) | ![npm version](https://img.shields.io/npm/v/@composio/mastra) |\n| [@composio/cloudflare](https://www.npmjs.com/package/@composio/cloudflare) | ![npm version](https://img.shields.io/npm/v/@composio/cloudflare) |\n| **Python** | |\n| [composio-openai](https://pypi.org/project/composio-openai/) | ![PyPI version](https://img.shields.io/pypi/v/composio-openai) |\n| [composio-openai-agents](https://pypi.org/project/composio-openai-agents/) | ![PyPI version](https://img.shields.io/pypi/v/composio-openai-agents) |\n| [composio-anthropic](https://pypi.org/project/composio-anthropic/) | ![PyPI version](https://img.shields.io/pypi/v/composio-anthropic) |\n| [composio-langchain](https://pypi.org/project/composio-langchain/) | ![PyPI version](https://img.shields.io/pypi/v/composio-langchain) |\n| [composio-langgraph](https://pypi.org/project/composio-langgraph/) | ![PyPI version](https://img.shields.io/pypi/v/composio-langgraph) |\n| [composio-llamaindex](https://pypi.org/project/composio-llamaindex/) | ![PyPI version](https://img.shields.io/pypi/v/composio-llamaindex) |\n| [composio-crewai](https://pypi.org/project/composio-crewai/) | ![PyPI version](https://img.shields.io/pypi/v/composio-crewai) |\n| [composio-autogen](https://pypi.org/project/composio-autogen/) | ![PyPI version](https://img.shields.io/pypi/v/composio-autogen) |\n| [composio-gemini](https://pypi.org/project/composio-gemini/) | ![PyPI version](https://img.shields.io/pypi/v/composio-gemini) |\n| [composio-google](https://pypi.org/project/composio-google/) | ![PyPI version](https://img.shields.io/pypi/v/composio-google) |\n| [composio-google-adk](https://pypi.org/project/composio-google-adk/) | ![PyPI version](https://img.shields.io/pypi/v/composio-google-adk) |\n\n### Utility Packages\n\n| Package | Version |\n|---------|---------|\n| [@composio/json-schema-to-zod](https://www.npmjs.com/package/@composio/json-schema-to-zod) | ![npm version](https://img.shields.io/npm/v/@composio/json-schema-to-zod) |\n| [@composio/ts-builders](https://www.npmjs.com/package/@composio/ts-builders) | ![npm version](https://img.shields.io/npm/v/@composio/ts-builders) |\n\n_if you are looking for the older sdk, you can find them [here](https://github.com/ComposioHQ/composio/tree/master)_\n\n## Rube\n\n[Rube](https://rube.app) is a Model Context Protocol (MCP) server built with Composio. It connects your AI tools to 500+ apps like Gmail, Slack, GitHub, and Notion. Simply install it in your AI client, authenticate once with your apps, and start asking your AI to perform real actions like "Send an email" or "Create a task." \n\nIt integrates with major AI clients like Cursor, Claude Desktop, VS Code, Claude Code and any custom MCPâ€‘compatible client. You can switch between these clients and your integrations follow you.\n\n\n## Contributing\n\nWe welcome contributions to both SDKs! Please read our [contribution guidelines](https://github.com/ComposioHQ/composio/blob/next/CONTRIBUTING.md) before submitting pull requests.\n\n## License\n\nThis project is licensed under the MIT License - see the LICENSE file for details.\n\n## Support\n\nIf you encounter any issues or have questions about the SDKs:\n\n- Open an issue in this repository\n- Contact our [support team](mailto:support@composio.dev)\n- Check our [documentation](https://docs.composio.dev/)\n', '{"language":"TypeScript","stars":26201,"forks":4395,"watchers":26201,"open_issues":35,"topics":["agentic-ai","agents","ai","ai-agents","aiagents","developer-tools","function-calling","gpt-4","javascript","js","llm","llmops","mcp","python","remote-mcp-server","sse","typescript"],"default_branch":"next","size_kb":220255,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:ComposioHQ:composio","source_url":"https://github.com/ComposioHQ/composio"},{"type":"has_code","target_id":"github:ComposioHQ:composio","source_url":"https://github.com/ComposioHQ/composio"},{"type":"has_code","target_id":"github:ComposioHQ:composio","source_url":"https://github.com/ComposioHQ/composio"}]', NULL, 'MIT', 'approved', 80, '0f37ac8fb05ac24fc2e2139dd200fc3a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-ComposioHQ-composio from https://github.com/ComposioHQ.png
Image converted to WebP: data/images/github-ComposioHQ-composio.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-facefusion-facefusion', 'github--facefusion--facefusion', 'facefusion', 'facefusion', 'FaceFusion ========== > Industry leading face manipulation platform. !License Preview ------- !Preview Installation ------------ Be aware, the installation needs technical skills and is not recommended for beginners. In case you are not comfortable using a terminal, our Windows Installer and macOS Installer get you started. Usage ----- Run the command: Documentation ------------- Read the documentation for a deep dive.', '["ai","deep-fake","deepfake","face-swap","faceswap","lip-sync","lipsync","python"]', 'other', 26058, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/facefusion/facefusion","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', 'FaceFusion\n==========\n\n> Industry leading face manipulation platform.\n\n[![Build Status](https://img.shields.io/github/actions/workflow/status/facefusion/facefusion/ci.yml.svg?branch=master)](https://github.com/facefusion/facefusion/actions?query=workflow:ci)\n[![Coverage Status](https://img.shields.io/coveralls/facefusion/facefusion.svg)](https://coveralls.io/r/facefusion/facefusion)\n![License](https://img.shields.io/badge/license-OpenRAIL--AS-green)\n\n\nPreview\n-------\n\n![Preview](https://raw.githubusercontent.com/facefusion/facefusion/master/.github/preview.png?sanitize=true)\n\n\nInstallation\n------------\n\nBe aware, the [installation](https://docs.facefusion.io/installation) needs technical skills and is not recommended for beginners. In case you are not comfortable using a terminal, our [Windows Installer](http://windows-installer.facefusion.io) and [macOS Installer](http://macos-installer.facefusion.io) get you started.\n\n\nUsage\n-----\n\nRun the command:\n\n```\npython facefusion.py [commands] [options]\n\noptions:\n  -h, --help                                      show this help message and exit\n  -v, --version                                   show program''s version number and exit\n\ncommands:\n    run                                           run the program\n    headless-run                                  run the program in headless mode\n    batch-run                                     run the program in batch mode\n    force-download                                force automate downloads and exit\n    benchmark                                     benchmark the program\n    job-list                                      list jobs by status\n    job-create                                    create a drafted job\n    job-submit                                    submit a drafted job to become a queued job\n    job-submit-all                                submit all drafted jobs to become a queued jobs\n    job-delete                                    delete a drafted, queued, failed or completed job\n    job-delete-all                                delete all drafted, queued, failed and completed jobs\n    job-add-step                                  add a step to a drafted job\n    job-remix-step                                remix a previous step from a drafted job\n    job-insert-step                               insert a step to a drafted job\n    job-remove-step                               remove a step from a drafted job\n    job-run                                       run a queued job\n    job-run-all                                   run all queued jobs\n    job-retry                                     retry a failed job\n    job-retry-all                                 retry all failed jobs\n```\n\n\nDocumentation\n-------------\n\nRead the [documentation](https://docs.facefusion.io) for a deep dive.\n', '{"language":"Python","stars":26058,"forks":4172,"watchers":26058,"open_issues":0,"topics":["ai","deep-fake","deepfake","face-swap","faceswap","lip-sync","lipsync"],"default_branch":"master","size_kb":26428,"archived":false,"fork":false,"has_wiki":false,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:facefusion:facefusion","source_url":"https://github.com/facefusion/facefusion"}]', NULL, 'NOASSERTION', 'approved', 65, 'bf3353948c357cfd15f65136711e9f11', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-facefusion-facefusion from https://github.com/facefusion.png
Image converted to WebP: data/images/github-facefusion-facefusion.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-Max-Eee-NeoPass', 'github--max-eee--neopass', 'NeoPass', 'Max-Eee', '<img width="1500" height="500" alt="NeoPass Banner" src="https://github.com/user-attachments/assets/7369dd86-838d-4fdc-abdd-6b41a9b14aed" /> This chrome extension is for students taking tests on the ****, ****<br>, ****<br>, ****<br> and that restrict your abilities <samp> > [!IMPORTANT] > **Get Your Credentials**: To obtain your credentials, it is essential to visit the website neopass and follow the instructions provided there. > Accessing the website is crucial for a seamless experience wi...', '["ai","always-active-tab","chrome-extension","code-generation","conservation-geography","copy-paste","examly","examlyio","forest-management","fullscreen-bypass","iamneo","iamneobypass","mcq-test","neocolab","neoexamshield","neoexamshield-bypass","neoshield-bypass","nptel","screenshare-bypass","wildlife-ecology","html"]', 'other', 25935, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/Max-Eee/NeoPass","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<img width="1500" height="500" alt="NeoPass Banner" src="https://github.com/user-attachments/assets/7369dd86-838d-4fdc-abdd-6b41a9b14aed" />\n\n# <i>**`Free`** NeoPass Extension</i>\n\nThis chrome extension is for students taking tests on the **`Iamneo portal`**, **`Wildlife Ecology NPTEL`**<br>, **`conservation-geography NPTEL`**<br>, **`forest management NPTEL`**<br>  and `other exam portals in chrome browser` that restrict your abilities\n\n### [**Make sure to visit our website for the best experience!**](https://freeneopass.vercel.app) ğŸŒ\n\n<samp>\n  \n> [!IMPORTANT]\n> **Get Your Credentials**: To obtain your credentials, it is essential to visit the website [neopass](https://freeneopass.vercel.app) and follow the instructions provided there.  \n> Accessing the website is crucial for a seamless experience with the extension.  \n\n> [!WARNING]\n> **Educational Purposes Only**: This extension is intended for educational purposes. Please use it responsibly and ethically.  \n> We am not responsible for any actions taken, and we do not encourage or promote cheating in any way.  \n> Be cautious when using the extension to maintain academic integrity.\n\n## âœ¨ Features\n\n- **`NPTEL Integration`** : Solve NPTEL Wildlife ecology answers\n- **`NeoExamShield Bypass`** : Break free from Examly''s limitations.  NeoPass mimics the NeoExamShield extension\n- **`Chatbot With Stealth Mode`** : Leverage AI Chatbot to enhance your search capabilities\n- **`AI Search Answers/Code`** : Perform AI-powered searches, helping you find answers without switching tabs\n- **`Solve MCQ`** : Quicky Search MCQ Answers by simply selecting\n- **`Tab Switching Bypass`** : Prevents unwanted tab switch restrictions\n- **`Pasting When Restricted`** : Quickly paste answers with ease, reducing the time spent on manual entry\n- **`Remote Logout`** : Remote logout your account from the extension ensuring your identity is hidden.\n\n## â¬‡ï¸ Installation\n\n1. [Download](https://github.com/Max-Eee/NeoPass/archive/refs/heads/main.zip) the extension.\n2. Open Chrome and go to the Extensions page by typing `chrome://extensions/`.\n3. Enable **Developer mode** in the top right corner.\n4. Click on **Load unpacked** and select the folder where the extension is located.\n5. Your NeoPass extension is now installed!\n\n### Installation Guide Video\n\n\n\nhttps://github.com/user-attachments/assets/89fb986c-2edb-4252-8232-dbd10beec0cf\n\n\n## ğŸ’» Usage\n\nOnce installed, **login with your credentials** obtained from our [website](https://freeneopass.vercel.app).\n\n## âŒ¨ï¸ Shortcuts\n\n### Windows/Linux Users:\n- <kbd>Alt</kbd> + <kbd>Shift</kbd> + <kbd>Q</kbd> : Solve Iam Neo MCQs/Coding Questions with 100% ACCURACY\n- <kbd>Alt</kbd> + <kbd>Shift</kbd> + <kbd>A</kbd> : Solve Iam Neo MCQs/Coding Questions with using AI [Backup]\n- <kbd>Alt</kbd> + <kbd>Shift</kbd> + <kbd>T</kbd> : Type Iam Neo Coding Questions One by One\n- <kbd>Alt</kbd> + <kbd>Shift</kbd> + <kbd>H</kbd> : Solve HackerRank Questions [BETA]\n> [!NOTE]\n> The following shortcuts **require text to be selected** before activation:  \n> - <kbd>Alt</kbd> + <kbd>Shift</kbd> + <kbd>N</kbd> : Solve NPTEL MCQs from selected text\n> - <kbd>Alt</kbd> + <kbd>Shift</kbd> + <kbd>S</kbd> : Search answers and code from selected text  \n> - <kbd>Alt</kbd> + <kbd>Shift</kbd> + <kbd>M</kbd> : Search MCQs from selected text\n- <kbd>Ctrl</kbd> + <kbd>V</kbd> : Paste content when blocked\n- <kbd>Alt</kbd> + <kbd>C</kbd> : Open/Close Chatbot\n\n<details>\n<summary><strong>Mac Users (Click to expand)</strong></summary>\n\n- <kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>Q</kbd> : Solve Iam Neo MCQs/Coding Questions with 100% ACCURACY\n- <kbd>Option</kbd> + <kbd>Shift</kbd> + <kbd>A</kbd> : Solve Iam Neo MCQs/Coding Questions with using AI [Backup]\n- <kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>T</kbd> : Type Iam Neo Coding Questions One by One\n- <kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>H</kbd> : Solve HackerRank Questions [BETA]\n\n> [!NOTE]\n> The following shortcuts **require text to be selected** before activation:  \n> - <kbd>Option</kbd> + <kbd>Shift</kbd> + <kbd>N</kbd> : Solve NPTEL MCQs from selected text\n> - <kbd>Option</kbd> + <kbd>Shift</kbd> + <kbd>S</kbd> : Search answers and code from selected text  \n> - <kbd>Option</kbd> + <kbd>Shift</kbd> + <kbd>M</kbd> : Search MCQs from selected text\n\n- <kbd>Cmd</kbd> + <kbd>V</kbd> : Paste content when blocked\n- <kbd>Option</kbd> + <kbd>C</kbd> : Open/Close Chatbot\n\n</details>\n\n## ğŸ¤ Contribute or Add NPTEL Dataset\n\nIf you want to contribute to the NPTEL question database, follow these steps:\n\n1. Fork this repository\n2. Open your NPTEL assignment page in the browser\n3. Open browser developer tools (F12 or right-click > Inspect)\n4. Go to the Console tab\n5. Copy and paste the script from `nptel.txt` in the repository\n6. Run the script by pressing Enter\n7. The script will extract all questions and correct answers from the page\n8. Copy the output JSON data\n9. Update the `data/nptel.json` file with the new questions and answers\n10. Create a pull request to contribute your additions back to the main repository\n\nThis helps expand our database and improves the accuracy of the NPTEL question solving feature!\n\n## ğŸ’¬ Feedback\n\nWe''d love to hear your thoughts! If you encounter any issues or have suggestions for improvement, please reach out. Your feedback is invaluable! ğŸ’Œ\n\nğŸ“§ **Contact us at:** [freeneopass@gmail.com](mailto:freeneopass@gmail.com?subject=Issue%20Title%3A%20%5BBrief%20description%20of%20your%20issue%5D&body=Hello%20NeoPass%20Support%20Team%2C%0A%0AIssue%20Description%3A%0A%5BPlease%20describe%20your%20issue%20in%20detail%5D%0A%0AWhen%20does%20this%20occur%3A%0A%5BSpecify%20when%20the%20issue%20happens%20-%20e.g.%2C%20during%20login%2C%20while%20using%20a%20specific%20feature%2C%20etc.%5D%0A%0ASteps%20to%20Reproduce%3A%0A1.%20%5BFirst%20step%5D%0A2.%20%5BSecond%20step%5D%0A3.%20%5BThird%20step%5D%0A%0AScreenshots%2FError%20Messages%20if%20possible%3A%0A%5BPlease%20attach%20any%20relevant%20screenshots%20or%20paste%20error%20messages%20here%5D%0A%0AAdditional%20Information%3A%0A%5BAny%20other%20relevant%20details%5D%0A%0AThank%20you!)\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n</samp>\n', '{"language":"HTML","stars":25935,"forks":82,"watchers":25935,"open_issues":2,"topics":["ai","always-active-tab","chrome-extension","code-generation","conservation-geography","copy-paste","examly","examlyio","forest-management","fullscreen-bypass","iamneo","iamneobypass","mcq-test","neocolab","neoexamshield","neoexamshield-bypass","neoshield-bypass","nptel","screenshare-bypass","wildlife-ecology"],"default_branch":"main","size_kb":2092,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:Max-Eee:NeoPass","source_url":"https://github.com/Max-Eee/NeoPass"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"}]', NULL, 'MIT', 'approved', 65, '9267fa50f5cfc94ffcea6c92e4bbb65b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-Max-Eee-NeoPass from https://github.com/Max-Eee.png
Image converted to WebP: data/images/github-Max-Eee-NeoPass.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-CopilotKit-CopilotKit', 'github--copilotkit--copilotkit', 'CopilotKit', 'CopilotKit', '<img width="4096" height="1588" alt="header" src="https://github.com/user-attachments/assets/dd638592-fb74-4e22-8c55-49dfc4d0e462" /> <br> <div align="start" style="display:flex;justify-content:start;gap:16px;height:20px;margin: 0;"> <a href="https://www.npmjs.com/package/@copilotkit/react-core" target="_blank"> <img src="https://img.shields.io/npm/v/%40copilotkit%2Freact-core?logo=npm&logoColor=%23FFFFFF&label=Version&color=%236963ff" alt="NPM"> </a> <a href="https://github.com/copilotkit/co...', '["agent","agents","ai","ai-agent","ai-assistant","assistant","copilot","copilot-chat","hacktoberfest","langchain","langgraph","llm","nextjs","open-source","react","reactjs","ts","typescript","typescript"]', 'other', 25300, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/CopilotKit/CopilotKit","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '\n<img width="4096" height="1588" alt="header" src="https://github.com/user-attachments/assets/dd638592-fb74-4e22-8c55-49dfc4d0e462" />\n\n\n<br>\n  <div align="start" style="display:flex;justify-content:start;gap:16px;height:20px;margin: 0;">\n  <a href="https://www.npmjs.com/package/@copilotkit/react-core" target="_blank">\n    <img src="https://img.shields.io/npm/v/%40copilotkit%2Freact-core?logo=npm&logoColor=%23FFFFFF&label=Version&color=%236963ff" alt="NPM">\n  </a>\n\n  <a href="https://github.com/copilotkit/copilotkit/blob/main/LICENSE" target="_blank">\n    <img src="https://img.shields.io/github/license/copilotkit/copilotkit?color=%236963ff&label=License" alt="MIT">\n  </a>\n\n  <a href="https://discord.gg/6dffbvGU3D" target="_blank">\n    <img src="https://img.shields.io/discord/1122926057641742418?logo=discord&logoColor=%23FFFFFF&label=Discord&color=%236963ff" alt="Discord">\n  </a>\n  </div>\n  <br/>\n  <div>\n    <a href="https://www.producthunt.com/posts/copilotkit" target="_blank">\n    <img src="https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=428778&theme=light&period=daily">\n  </a>\n  </div>\n\n## âš¡ï¸ Quick Install\n```\n  npx copilotkit@latest init\n```\n\n<br/>\n\n<a href="https://docs.copilotkit.ai/?ref=github_readme">Read the Docs â†’</a>&nbsp;&nbsp;&nbsp;\n<a href="https://cloud.copilotkit.ai?ref=github_readme">Try Copilot Cloud â†’</a>&nbsp;&nbsp;&nbsp;\n<a href="https://discord.gg/6dffbvGU3D?ref=github_readme">Join our Discord â†’</a>\n\n## ğŸš€ Getting Started\n\n1. Install: Run a simple CLI command\n1. Configure: Add CopilotKit provider to your app\n1. Customize: Use headless UI or the customizable pre-built components\n1. Deploy: You''re done!\n\n<br />\n  <a href="https://docs.copilotkit.ai/#get-started-now?ref=github_readme" target="_blank">\n    Complete getting started guide â†’\n  </a>\n<br />\n<br />\n\n<img width="4096" height="2341" alt="Best in class support across the ecosystem" src="https://github.com/user-attachments/assets/bf399131-2a92-49f8-8748-38ed72353f9c" />\n\n\n## âœ¨ Why CopilotKit?\n\n- Minutes to integrateÂ - Get started quickly with our CLI\n- Framework agnosticÂ - Works with React, Next.js, AGUI and more\n- Production-ready UIÂ - Use customizable components or build with headless UI\n- Built-in securityÂ - Prompt injection protection\n- Open sourceÂ - Full transparency and community-driven\n\n## ğŸ§‘â€ğŸ’» Real life use cases\n\n<span>Deploy deeply-integrated AI assistants & agents that work alongside your users inside your applications.</span>\n\n<img width="4096" height="2725" alt="Headless UI" src="https://github.com/user-attachments/assets/4dbe1e74-8b46-4798-a658-f79ee5a66189" />\n\n\n## ğŸ–¥ï¸ Code Samples\n\n<span>Drop in these building blocks and tailor them to your needs.</span>\n\n<h3>Build with Headless APIs and Pre-Built Components</h3>\n\n```ts\n// Headless UI with full control\nconst { visibleMessages, appendMessage, setMessages, ... } = useCopilotChat();\n\n// Pre-built components with deep customization options (CSS + pass custom sub-components)\n<CopilotPopup \n  instructions={"You are assisting the user as best as you can. Answer in the best way possible given the data you have."} \n  labels={{ title: "Popup Assistant", initial: "Need any help?" }} \n/>\n```\n\n```ts\n// Frontend actions + generative UI, with full streaming support\nuseCopilotAction({\n  name: "appendToSpreadsheet",\n  description: "Append rows to the current spreadsheet",\n  parameters: [\n    { name: "rows", type: "object[]", attributes: [{ name: "cells", type: "object[]", attributes: [{ name: "value", type: "string" }] }] }\n  ],\n  render: ({ status, args }) => <Spreadsheet data={canonicalSpreadsheetData(args.rows)} />,\n  handler: ({ rows }) => setSpreadsheet({ ...spreadsheet, rows: [...spreadsheet.rows, ...canonicalSpreadsheetData(rows)] }),\n});\n```\n\n<h3>Integrate In-App CoAgents with LangGraph</h3>\n\n```ts\n// Share state between app and agent\nconst { agentState } = useCoAgent({ \n  name: "basic_agent", \n  initialState: { input: "NYC" } \n});\n\n// agentic generative UI\nuseCoAgentStateRender({\n  name: "basic_agent",\n  render: ({ state }) => <WeatherDisplay {...state.final_response} />,\n});\n\n// Human in the Loop (Approval)\nuseCopilotAction({\n  name: "email_tool",\n  parameters: [\n    {\n      name: "email_draft",\n      type: "string",\n      description: "The email content",\n      required: true,\n    },\n  ],\n  renderAndWaitForResponse: ({ args, status, respond }) => {\n    return (\n      <EmailConfirmation\n        emailContent={args.email_draft || ""}\n        isExecuting={status === "executing"}\n        onCancel={() => respond?.({ approved: false })}\n        onSend={() =>\n          respond?.({\n            approved: true,\n            metadata: { sentAt: new Date().toISOString() },\n          })\n        }\n      />\n    );\n  },\n});\n```\n\n```ts\n// intermediate agent state streaming (supports both LangGraph.js + LangGraph python)\nconst modifiedConfig = copilotKitCustomizeConfig(config, {\n  emitIntermediateState: [{ \n    stateKey: "outline", \n    tool: "set_outline", \n    toolArgument: "outline" \n  }],\n});\nconst response = await ChatOpenAI({ model: "gpt-4o" }).invoke(messages, modifiedConfig);\n```\n## ğŸ† Featured Examples\n\n\n<p align="center">\n  <a href="https://www.copilotkit.ai/examples/form-filling-copilot">\n    <img width="290" height="304" alt="Banner 2 A" src="https://github.com/user-attachments/assets/90c42b54-8931-45ad-9c0b-53f7f67453a1" />\n  </a>\n  <a href="https://www.copilotkit.ai/examples/state-machine-copilot">\n    <img width="290" height="304" alt="Banner 2 A-1" src="https://github.com/user-attachments/assets/609c62eb-76af-4866-a353-5e3545470ec3" />\n  </a>\n  <a href="https://www.copilotkit.ai/examples/chat-with-your-data">\n    <img width="290" height="304" alt="Banner 2 A-2" src="https://github.com/user-attachments/assets/c614ac4e-d2b3-4514-9ef1-fdba04c0a082" />\n  </a>\n</p>\n\n## ğŸ–¥ï¸ AG-UI: The Agentâ€“User Interaction Protocol\nConnect agent workflow to user-facing apps, with deep partnerships and 1st-party integrations across the agentic stackâ€”including LangGraph, CrewAI, and more.\n\n\n  <a href="https://github.com/ag-ui-protocol/ag-ui" target="_blank">\n   Learn more in the AG-UI README â†’\n  </a>\n\n## ğŸ¤ Community\n<h3>Have questions or need help?</h3>\n  <a href="https://discord.gg/6dffbvGU3D?ref=github_readme" target="_blank">\n   Join our Discord â†’\n  </a> </br>\n    <a href="https://docs.copilotkit.ai/?ref=github_readme" target="_blank">\n  Read the Docs â†’\n  </a> </br>\n    <a href="https://cloud.copilotkit.ai?ref=github_readme" target="_blank">\n   Try Copilot Cloud â†’\n  </a>\n<h3>Stay up to date with our latest releases!</h3>\n  <a href="https://www.linkedin.com/company/copilotkit/" target="_blank">\n   Follow us on LinkedIn â†’\n  </a> </br>\n    <a href="https://x.com/copilotkit" target="_blank">\n   Follow us on X â†’\n  </a> \n  \n## ğŸ™‹ğŸ½â€â™‚ï¸ Contributing\n\nThanks for your interest in contributing to CopilotKit! ğŸ’œ\n\nWe value all contributions, whether it''s through code, documentation, creating demo apps, or just spreading the word.\n\nHere are a few useful resources to help you get started:\n\n- For code contributions, [CONTRIBUTING.md](./CONTRIBUTING.md).\n- For documentation-related contributions, [check out the documentation contributions guide](https://docs.copilotkit.ai/contributing/docs-contributions?ref=github_readme).\n\n- Want to contribute but not sure how? [Join our Discord](https://discord.gg/6dffbvGU3D) and we''ll help you out!\n\n## ğŸ“„ License\n\nThis repository''s source code is available under the [MIT License](https://github.com/CopilotKit/CopilotKit/blob/main/LICENSE).\n', '{"language":"TypeScript","stars":25300,"forks":3373,"watchers":25300,"open_issues":462,"topics":["agent","agents","ai","ai-agent","ai-assistant","assistant","copilot","copilot-chat","hacktoberfest","langchain","langgraph","llm","nextjs","open-source","react","reactjs","ts","typescript"],"default_branch":"main","size_kb":646469,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:copilotkit:copilotkit","source_url":"https://github.com/copilotkit/copilotkit"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:ag-ui-protocol:ag-ui\"","source_url":"https://github.com/ag-ui-protocol/ag-ui\""},{"type":"has_code","target_id":"github:CopilotKit:CopilotKit","source_url":"https://github.com/CopilotKit/CopilotKit"}]', NULL, 'MIT', 'approved', 65, 'aa39aaefc1ebfd6d49208e06541dd3ea', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-CopilotKit-CopilotKit from https://github.com/CopilotKit.png
Image converted to WebP: data/images/github-CopilotKit-CopilotKit.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-go-kratos-kratos', 'github--go-kratos--kratos', 'kratos', 'go-kratos', '<p align="center"><a href="https://go-kratos.dev/" target="_blank"><img src="https://github.com/go-kratos/kratos/blob/main/docs/images/kratos-large.png?raw=true"></a></p> <p align="center"> <a href="https://github.com/go-kratos/kratos/actions"><img src="https://github.com/go-kratos/kratos/workflows/Go/badge.svg" alt="Build Status"></a> <a href="https://pkg.go.dev/github.com/go-kratos/kratos/v2"><img src="https://pkg.go.dev/badge/github.com/go-kratos/kratos/v2" alt="GoDoc"></a> <a href="https:...', '["ai","architecture","cloud-native","framework","generate","go","golang","grpc","http","kratos","mcp","microservice","microservices","protobuf","go"]', 'other', 25175, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/go-kratos/kratos","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<p align="center"><a href="https://go-kratos.dev/" target="_blank"><img src="https://github.com/go-kratos/kratos/blob/main/docs/images/kratos-large.png?raw=true"></a></p>\n\n<p align="center">\n<a href="https://github.com/go-kratos/kratos/actions"><img src="https://github.com/go-kratos/kratos/workflows/Go/badge.svg" alt="Build Status"></a>\n<a href="https://pkg.go.dev/github.com/go-kratos/kratos/v2"><img src="https://pkg.go.dev/badge/github.com/go-kratos/kratos/v2" alt="GoDoc"></a>\n<a href="https://deepwiki.com/go-kratos/kratos"><img src="https://img.shields.io/badge/DeepWiki-go--kratos%2Fkratos-blue.svg?logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACwAAAAyCAYAAAAnWDnqAAAAAXNSR0IArs4c6QAAA05JREFUaEPtmUtyEzEQhtWTQyQLHNak2AB7ZnyXZMEjXMGeK/AIi+QuHrMnbChYY7MIh8g01fJoopFb0uhhEqqcbWTp06/uv1saEDv4O3n3dV60RfP947Mm9/SQc0ICFQgzfc4CYZoTPAswgSJCCUJUnAAoRHOAUOcATwbmVLWdGoH//PB8mnKqScAhsD0kYP3j/Yt5LPQe2KvcXmGvRHcDnpxfL2zOYJ1mFwrryWTz0advv1Ut4CJgf5uhDuDj5eUcAUoahrdY/56ebRWeraTjMt/00Sh3UDtjgHtQNHwcRGOC98BJEAEymycmYcWwOprTgcB6VZ5JK5TAJ+fXGLBm3FDAmn6oPPjR4rKCAoJCal2eAiQp2x0vxTPB3ALO2CRkwmDy5WohzBDwSEFKRwPbknEggCPB/imwrycgxX2NzoMCHhPkDwqYMr9tRcP5qNrMZHkVnOjRMWwLCcr8ohBVb1OMjxLwGCvjTikrsBOiA6fNyCrm8V1rP93iVPpwaE+gO0SsWmPiXB+jikdf6SizrT5qKasx5j8ABbHpFTx+vFXp9EnYQmLx02h1QTTrl6eDqxLnGjporxl3NL3agEvXdT0WmEost648sQOYAeJS9Q7bfUVoMGnjo4AZdUMQku50McDcMWcBPvr0SzbTAFDfvJqwLzgxwATnCgnp4wDl6Aa+Ax283gghmj+vj7feE2KBBRMW3FzOpLOADl0Isb5587h/U4gGvkt5v60Z1VLG8BhYjbzRwyQZemwAd6cCR5/XFWLYZRIMpX39AR0tjaGGiGzLVyhse5C9RKC6ai42ppWPKiBagOvaYk8lO7DajerabOZP46Lby5wKjw1HCRx7p9sVMOWGzb/vA1hwiWc6jm3MvQDTogQkiqIhJV0nBQBTU+3okKCFDy9WwferkHjtxib7t3xIUQtHxnIwtx4mpg26/HfwVNVDb4oI9RHmx5WGelRVlrtiw43zboCLaxv46AZeB3IlTkwouebTr1y2NjSpHz68WNFjHvupy3q8TFn3Hos2IAk4Ju5dCo8B3wP7VPr/FGaKiG+T+v+TQqIrOqMTL1VdWV1DdmcbO8KXBz6esmYWYKPwDL5b5FA1a0hwapHiom0r/cKaoqr+27/XcrS5UwSMbQAAAABJRU5ErkJggg==" alt="DeepWiki"></a>\n<!-- DeepWiki badge generated by https://deepwiki.ryoppippi.com/ -->\n<a href="https://codecov.io/gh/go-kratos/kratos"><img src="https://codecov.io/gh/go-kratos/kratos/master/graph/badge.svg" alt="codeCov"></a>\n<a href="https://goreportcard.com/report/github.com/go-kratos/kratos"><img src="https://goreportcard.com/badge/github.com/go-kratos/kratos" alt="Go Report Card"></a>\n<a href="https://github.com/go-kratos/kratos/blob/main/LICENSE"><img src="https://img.shields.io/github/license/go-kratos/kratos" alt="License"></a>\n<a href="https://github.com/avelino/awesome-go"><img src="https://awesome.re/mentioned-badge.svg" alt="Awesome Go"></a>\n<a href="https://discord.gg/BWzJsUJ"><img src="https://img.shields.io/discord/766619759214854164?label=chat&logo=discord" alt="Discord"></a>\n</p>\n<p align="center">\n<a href="https://trendshift.io/repositories/3233" target="_blank"><img src="https://trendshift.io/api/badge/repositories/3233" alt="go-kratos%2Fkratos | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>\n<a href="https://www.producthunt.com/posts/go-kratos?utm_source=badge-featured&utm_medium=badge&utm_souce=badge-go-kratos" target="_blank"><img src="https://api.producthunt.com/widgets/embed-image/v1/featured.svg?post_id=306565&theme=light" alt="Go Kratos - A Go framework for microservices. | Product Hunt" style="width: 250px; height: 54px;" width="250" height="54" /></a>\n</p>\n\n##### Translate to: [ç®€ä½“ä¸­æ–‡](README_zh.md)\n\n## About Kratos\n\n> The name is inspired by the Greek-mythology-based game "God of War". It tells the adventures of Kratos becoming a god of war from a mortal and launching a god-killing slaughter.\n\nKratos is a microservice-oriented governance framework implemented by golang, which offers convenient capabilities to help you quickly build a bulletproof application from scratch, such as:\n\n- The [communication protocol](https://go-kratos.dev/docs/component/api) is based on the HTTP/gRPC through the definition of Protobuf.\n- Abstract [transport](https://go-kratos.dev/docs/component/transport/overview) layer support: [HTTP](https://go-kratos.dev/docs/component/transport/http) / [gRPC](https://go-kratos.dev/docs/component/transport/grpc).\n- Powerful [middleware](https://go-kratos.dev/docs/component/middleware/overview) design, support: [Tracing (OpenTelemetry)](https://go-kratos.dev/docs/component/middleware/tracing), [Metrics (Prometheus is default)](https://go-kratos.dev/docs/component/middleware/metrics), [Recovery](https://go-kratos.dev/docs/component/middleware/recovery) and more.\n- [Registry](https://go-kratos.dev/docs/component/registry) interface able to be connected with various other centralized registries through plug-ins.\n- The [standard log interfaces](https://go-kratos.dev/docs/component/log) ease the integration of the third-party log libs with logs collected through the *Fluentd*.\n- Automatically support the selection of the content [encoding](https://go-kratos.dev/docs/component/encoding) with Accept and Content-Type.\n- Multiple data sources are supported for [configurations](https://go-kratos.dev/docs/component/config) and dynamic configurations (use atomic operations).\n- In the protocol of HTTP/gRPC, use the uniform [metadata](https://go-kratos.dev/docs/component/metadata) transfer method.\n- You can define [errors](https://go-kratos.dev/docs/component/errors/) in protos and generate enums with protoc-gen-go.\n- You can define [verification rules](https://go-kratos.dev/docs/component/middleware/validate) in Protobuf supported by the HTTP/gRPC service.\n- [Swagger API](https://go-kratos.dev/docs/guide/openapi) is generated Automatically and embed Swagger UI endpoint can be started by adding [Swagger plugin](https://github.com/go-kratos/swagger-api).\n\nKratos is accessible, powerful, and provides tools required for large, robust applications.\n\n## Learning Kratos\n\nKratos has the most extensive and thorough [documentation](https://go-kratos.dev/docs/getting-started/start) and [example](https://github.com/go-kratos/examples) library of all modern web application frameworks, making it a breeze to get started with the framework.\n\nWe also provide a [modern template](https://github.com/go-kratos/kratos-layout). This template should help reduce the work required to set up modern projects.\n\n### Goals\n\nKratos boosts your productivity. With the integration of excellent resources and further support, programmers can get rid of most issues might encounter in the field of distributed systems and software engineering such that they are allowed to focus on the release of businesses only. Additionally, for each programmer, Kratos is also an ideal one learning warehouse for many aspects of microservices to enrich their experiences and skills.\n\n### Principles\n\n* **Simple**: Appropriate design with plain and easy code.\n* **General**: Cover the various utilities for business development.\n* **Highly efficient**: Speeding up the efficiency of businesses upgrading.\n* **Stable**: The base libs validated in the production environment have the characteristics of high testability, high coverage as well as high security and reliability.\n* **Robust**: Eliminating misusing through high quality of the base libs.\n* **High-performance**: Optimal performance excluding the optimization of hacking in case of *unsafe*.Â \n* **Expandability**: Properly designed interfaces where you can expand utilities such as base libs to meet your further requirements.\n* **Fault-tolerance**: Designed against failure, enhance the understanding and exercising of SRE within Kratos to achieve more robustness.\n* **Toolchain**: Includes an extensive toolchain, such as the code generation of cache, the lint tool, and so forth.\n\n## Getting Started\n\nCreate a kratos playground through [docker](https://www.docker.com/products/docker-desktop):\n\n```shell\ndocker run -it --rm -p 8000:8000 --workdir /workspace golang\n```\n\n```shell\napt-get update && apt-get -y install protobuf-compiler\nexport GOPROXY=https://goproxy.io,direct\ngo install github.com/go-kratos/kratos/cmd/kratos/v2@latest && kratos upgrade\n```\n\n```shell\nkratos new helloworld\ncd helloworld/ && go mod tidy\nkratos run\n```\n\nUse a browser to open and visit: `http://localhost:8000/helloworld/kratos`, The kratos program is running!\n\nIf you need more, please visit the kratos [documentation](https://go-kratos.dev/docs/getting-started/start).\n\n## Security Vulnerabilities\n\nIf you discover a security vulnerability within Kratos, please send an e-mail to tonybase via go-kratos@googlegroups.com. All security vulnerabilities will be promptly addressed.\n\n## Community\n\n- [Wechat Group](https://github.com/go-kratos/kratos/issues/682)\n- [Discord Group](https://discord.gg/BWzJsUJ)\n- [go-kratos.dev](https://go-kratos.dev/en)\n\n## Contributors\n\nThank you for considering contributing to the Kratos framework! The contribution guide can be found in the [Kratos documentation](https://go-kratos.dev/docs/community/contribution).\n\n<a href="https://github.com/go-kratos/kratos/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=go-kratos/kratos" />\n</a>\n\n## License\n\nThe Kratos framework is open-sourced software licensed under the [MIT license](./LICENSE).\n\n## Acknowledgments\n\nThe following project had particular influence on kratos''s design.\n\n- [go-kit/kit](https://github.com/go-kit/kit) is a programming toolkit for building microservices in go.\n- [asim/go-micro](https://github.com/asim/go-micro) a distributed systems development framework.\n- [google/go-cloud](https://github.com/google/go-cloud) is go cloud development kit.\n- [zeromicro/go-zero](https://github.com/zeromicro/go-zero) is a web and rpc framework with lots of builtin engineering practices.\n- [beego/beego](https://github.com/beego/beego) is a web framework including RESTful APIs, web apps and backend services.\n', '{"language":"Go","stars":25175,"forks":4127,"watchers":25175,"open_issues":80,"topics":["ai","architecture","cloud-native","framework","generate","go","golang","grpc","http","kratos","mcp","microservice","microservices","protobuf"],"default_branch":"main","size_kb":9458,"archived":false,"fork":false,"has_wiki":false,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:go-kratos:kratos","source_url":"https://github.com/go-kratos/kratos"},{"type":"has_code","target_id":"github:go-kratos:kratos","source_url":"https://github.com/go-kratos/kratos"},{"type":"has_code","target_id":"github:go-kratos:kratos","source_url":"https://github.com/go-kratos/kratos"},{"type":"has_code","target_id":"github:go-kratos:kratos","source_url":"https://github.com/go-kratos/kratos"},{"type":"has_code","target_id":"github:avelino:awesome-go\"><img","source_url":"https://github.com/avelino/awesome-go\"><img"},{"type":"has_code","target_id":"github:go-kratos:swagger-api","source_url":"https://github.com/go-kratos/swagger-api"},{"type":"has_code","target_id":"github:go-kratos:examples","source_url":"https://github.com/go-kratos/examples"},{"type":"has_code","target_id":"github:go-kratos:kratos-layout","source_url":"https://github.com/go-kratos/kratos-layout"},{"type":"has_code","target_id":"github:go-kratos:kratos","source_url":"https://github.com/go-kratos/kratos"},{"type":"has_code","target_id":"github:go-kratos:kratos","source_url":"https://github.com/go-kratos/kratos"},{"type":"has_code","target_id":"github:go-kit:kit","source_url":"https://github.com/go-kit/kit"},{"type":"has_code","target_id":"github:asim:go-micro","source_url":"https://github.com/asim/go-micro"},{"type":"has_code","target_id":"github:google:go-cloud","source_url":"https://github.com/google/go-cloud"},{"type":"has_code","target_id":"github:zeromicro:go-zero","source_url":"https://github.com/zeromicro/go-zero"},{"type":"has_code","target_id":"github:beego:beego","source_url":"https://github.com/beego/beego"}]', NULL, 'MIT', 'approved', 65, '7aa090bfa58d744374ae6e0ce6b22f6b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-go-kratos-kratos from https://github.com/go-kratos.png
Image converted to WebP: data/images/github-go-kratos-kratos.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-chroma-core-chroma', 'github--chroma-core--chroma', 'chroma', 'chroma-core', '!Chroma !Chroma <p align="center"> <b>Chroma - the open-source embedding database</b>. <br /> The fastest way to build Python or JavaScript LLM apps with memory! </p> <p align="center"> <a href="https://discord.gg/MMeYNTmh3x" target="_blank"> <img src="https://img.shields.io/discord/1073293645303795742?cacheSeconds=3600" alt="Discord"> </a> | <a href="https://github.com/chroma-core/chroma/blob/master/LICENSE" target="_blank"> <img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg"...', '["ai","database","document-retrieval","embeddings","llm","llms","rag","rust","rust-lang","vector-database","rust"]', 'other', 24802, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/chroma-core/chroma","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '![Chroma](./docs/docs.trychroma.com/public/chroma-wordmark-color.png#gh-light-mode-only)\n![Chroma](./docs/docs.trychroma.com/public/chroma-wordmark-white.png#gh-dark-mode-only)\n\n<p align="center">\n    <b>Chroma - the open-source embedding database</b>. <br />\n    The fastest way to build Python or JavaScript LLM apps with memory!\n</p>\n\n<p align="center">\n  <a href="https://discord.gg/MMeYNTmh3x" target="_blank">\n      <img src="https://img.shields.io/discord/1073293645303795742?cacheSeconds=3600" alt="Discord">\n  </a> |\n  <a href="https://github.com/chroma-core/chroma/blob/master/LICENSE" target="_blank">\n      <img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg" alt="License">\n  </a> |\n  <a href="https://docs.trychroma.com/" target="_blank">\n      Docs\n  </a> |\n  <a href="https://www.trychroma.com/" target="_blank">\n      Homepage\n  </a>\n</p>\n\n```bash\npip install chromadb # python client\n# for javascript, npm install chromadb!\n# for client-server mode, chroma run --path /chroma_db_path\n```\n\n## Chroma Cloud\n\nOur hosted service, Chroma Cloud, powers serverless vector and full-text search. It''s extremely fast, cost-effective, scalable and painless. Create a DB and try it out in under 30 seconds with $5 of free credits.\n\n[Get started with Chroma Cloud](https://trychroma.com/signup)\n\n## API\n\nThe core API is only 4 functions (run our [ğŸ’¡ Google Colab](https://colab.research.google.com/drive/1QEzFyqnoFxq7LUGyP1vzR4iLt9PpCDXv?usp=sharing)):\n\n```python\nimport chromadb\n# setup Chroma in-memory, for easy prototyping. Can add persistence easily!\nclient = chromadb.Client()\n\n# Create collection. get_collection, get_or_create_collection, delete_collection also available!\ncollection = client.create_collection("all-my-documents")\n\n# Add docs to the collection. Can also update and delete. Row-based API coming soon!\ncollection.add(\n    documents=["This is document1", "This is document2"], # we handle tokenization, embedding, and indexing automatically. You can skip that and add your own embeddings as well\n    metadatas=[{"source": "notion"}, {"source": "google-docs"}], # filter on these!\n    ids=["doc1", "doc2"], # unique for each doc\n)\n\n# Query/search 2 most similar results. You can also .get by id\nresults = collection.query(\n    query_texts=["This is a query document"],\n    n_results=2,\n    # where={"metadata_field": "is_equal_to_this"}, # optional filter\n    # where_document={"$contains":"search_string"}  # optional filter\n)\n```\n\nLearn about all features on our [Docs](https://docs.trychroma.com)\n\n## Features\n- __Simple__: Fully-typed, fully-tested, fully-documented == happiness\n- __Integrations__: [`ğŸ¦œï¸ğŸ”— LangChain`](https://blog.langchain.dev/langchain-chroma/) (python and js), [`ğŸ¦™ LlamaIndex`](https://twitter.com/atroyn/status/1628557389762007040) and more soon\n- __Dev, Test, Prod__: the same API that runs in your python notebook, scales to your cluster\n- __Feature-rich__: Queries, filtering, regex and more\n- __Free & Open Source__: Apache 2.0 Licensed\n\n## Use case: ChatGPT for ______\n\nFor example, the `"Chat your data"` use case:\n1. Add documents to your database. You can pass in your own embeddings, embedding function, or let Chroma embed them for you.\n2. Query relevant documents with natural language.\n3. Compose documents into the context window of an LLM like `GPT4` for additional summarization or analysis.\n\n## Embeddings?\n\nWhat are embeddings?\n\n- [Read the guide from OpenAI](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)\n- __Literal__: Embedding something turns it from image/text/audio into a list of numbers. ğŸ–¼ï¸ or ğŸ“„ => `[1.2, 2.1, ....]`. This process makes documents "understandable" to a machine learning model.\n- __By analogy__: An embedding represents the essence of a document. This enables documents and queries with the same essence to be "near" each other and therefore easy to find.\n- __Technical__: An embedding is the latent-space position of a document at a layer of a deep neural network. For models trained specifically to embed data, this is the last layer.\n- __A small example__: If you search your photos for "famous bridge in San Francisco". By embedding this query and comparing it to the embeddings of your photos and their metadata - it should return photos of the Golden Gate Bridge.\n\nEmbeddings databases (also known as **vector databases**) store embeddings and allow you to search by nearest neighbors rather than by substrings like a traditional database. By default, Chroma uses [Sentence Transformers](https://docs.trychroma.com/guides/embeddings#default:-all-minilm-l6-v2) to embed for you but you can also use OpenAI embeddings, Cohere (multilingual) embeddings, or your own.\n\n## Get involved\n\nChroma is a rapidly developing project. We welcome PR contributors and ideas for how to improve the project.\n- [Join the conversation on Discord](https://discord.gg/MMeYNTmh3x) - `#contributing` channel\n- [Review the ğŸ›£ï¸ Roadmap and contribute your ideas](https://docs.trychroma.com/roadmap)\n- [Grab an issue and open a PR](https://github.com/chroma-core/chroma/issues) - [`Good first issue tag`](https://github.com/chroma-core/chroma/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22)\n- [Read our contributing guide](https://docs.trychroma.com/contributing)\n\n**Release Cadence**\nWe currently release new tagged versions of the `pypi` and `npm` packages on Mondays. Hotfixes go out at any time during the week.\n\n## License\n\n[Apache 2.0](./LICENSE)\n', '{"language":"Rust","stars":24802,"forks":1955,"watchers":24802,"open_issues":509,"topics":["ai","database","document-retrieval","embeddings","llm","llms","rag","rust","rust-lang","vector-database"],"default_branch":"main","size_kb":860280,"archived":false,"fork":false,"has_wiki":true,"has_pages":true}', '[]', '[{"type":"has_code","target_id":"github:chroma-core:chroma","source_url":"https://github.com/chroma-core/chroma"},{"type":"has_code","target_id":"github:chroma-core:chroma","source_url":"https://github.com/chroma-core/chroma"},{"type":"has_code","target_id":"github:chroma-core:chroma","source_url":"https://github.com/chroma-core/chroma"}]', NULL, 'Apache-2.0', 'approved', 65, '691bab8d4679777e8ccaffae4a4e08b7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for github-chroma-core-chroma from https://github.com/chroma-core.png
Image converted to WebP: data/images/github-chroma-core-chroma.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('github-CodePhiliaX-Chat2DB', 'github--codephiliax--chat2db', 'Chat2DB', 'CodePhiliaX', '<div align="center"> <h2>ğŸš€ Zoer is Launching</h2> <p><strong>Powered by Chat2DB Team - AI-powered app builder that creates professional applications in minutes, no coding required</strong></p> <a href="https://zoer.ai/?utm_source=chat2db&utm_medium=banner&utm_campaign=github" target="_blank"> <img width="1000" height="auto" alt="Zoer - AI App Builder" src="https://github.com/user-attachments/assets/2f2a682d-9cc0-4470-93d3-19b4f1f6589e" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0...', '["ai","bi","chatgpt","clickhouse","clickhouse-client","database","datagrip","db2","dbeaver","gpt","hive","mysql","navicat","oracle","postgresql","redis","redis-client","sqlserver","text2sql","java"]', 'other', 24779, 0, NULL, '[{"source_platform":"github","source_url":"https://github.com/CodePhiliaX/Chat2DB","fetched_at":"2025-12-08T10:30:37.951Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '<div align="center">\n  <h2>ğŸš€ Zoer is Launching</h2>\n  <p><strong>Powered by Chat2DB Team - AI-powered app builder that creates professional applications in minutes, no coding required</strong></p>\n  \n  <a href="https://zoer.ai/?utm_source=chat2db&utm_medium=banner&utm_campaign=github" target="_blank">\n    <img width="1000" height="auto" alt="Zoer - AI App Builder" src="https://github.com/user-attachments/assets/2f2a682d-9cc0-4470-93d3-19b4f1f6589e" style="border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);" />\n  </a>\n  \n  <br/><br/>\n  \n  ---\n  \n  <br/>\n  \n  <a href="https://trendshift.io/repositories/11808" target="_blank"><img src="https://trendshift.io/api/badge/repositories/11808" alt="CodePhiliaX%2FChat2DB | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/>\n  </a>\n</div>\n\n<div align="center">\n  \n[![ReadmeX][readmex-image]][readmex-url]\n[![Discord][discord-image]][discord-url]\n[![Twitter][twitter-image]][twitter-url]\n[![Telegram][telegram-image]][telegram-url]\n[![Whatsapp][whatsapp-image]][whatsapp-url]\n[![Reddit][reddit-image]][reddit-url]\n[![Gmail][gmail-image]][gmail-url]\n\n[readmex-image]: https://raw.githubusercontent.com/CodePhiliaX/resource-trusteeship/main/readmex.svg\n[readmex-url]: https://readmex.com/CodePhiliaX/Chat2DB\n[discord-image]: https://img.shields.io/badge/-Join%20us%20on%20Discord-%237289DA.svg?style=flat&logo=discord&logoColor=white\n[discord-url]: https://discord.com/invite/uNjb3n5JVN\n[twitter-image]: https://img.shields.io/twitter/follow/_Chat2DB?label=Chat2DB\n[twitter-url]: https://twitter.com/intent/tweet?text=Chat2DB-An%20intelligent%20and%20versatile%20general-purpose%20SQL%20client%20and%20reporting%20tool%20for%20databases%20which%20integrates%20ChatGPT%20capabilities.&url=https://github.com/chat2db/Chat2DB&hashtags=ChatGPT,AGI,SQL%20Client,Reporting%20tool\n[telegram-image]: https://img.shields.io/twitter/url?label=Telegram&logo=Telegram&style=social&url=https://github.com/chat2db/Chat2DB\n[telegram-url]: https://t.me/share/url?text=Chat2DB-An%20intelligent%20and%20versatile%20general-purpose%20SQL%20client%20and%20reporting%20tool%20for%20databases%20which%20integrates%20ChatGPT%20capabilities.&url=https://github.com/chat2db/Chat2DB\n[whatsapp-image]: https://img.shields.io/twitter/url?label=whatsapp&logo=whatsapp&style=social&url=https://github.com/chat2db/Chat2DB\n[whatsapp-url]: https://api.whatsapp.com/send?text=Chat2DB-An%20intelligent%20and%20versatile%20general-purpose%20SQL%20client%20and%20reporting%20tool%20for%20databases%20which%20integrates%20ChatGPT%20capabilities.%20https://github.com/chat2db/Chat2DB\n[reddit-image]: https://img.shields.io/twitter/url?label=Reddit&logo=Reddit&style=social&url=https://github.com/chat2db/Chat2DB\n[reddit-url]: https://www.reddit.com/submit?url=https://github.com/chat2db/Chat2DB&title=Chat2DB-An%20intelligent%20and%20versatile%20general-purpose%20SQL%20client%20and%20reporting%20tool%20for%20databases%20which%20integrates%20ChatGPT%20capabilities.\n[gmail-image]: https://img.shields.io/twitter/url?label=Gmail&logo=Gmail&style=social&url=https://github.com/chat2db/Chat2DB\n[gmail-url]: mailto:?subject=Check%20this%20GitHub%20repository%20out.&body=Chat2DB-An%20intelligent%20and%20versatile%20general-purpose%20SQL%20client%20and%20reporting%20tool%20for%20databases%20which%20integrates%20ChatGPT%20capabilities.%3A%0Ahttps://github.com/chat2db/Chat2DB\n\n</div>\n\n<div align="center">\n  <a href="./README.md"><img alt="README in English" src="https://img.shields.io/badge/English-d9d9d9"></a>\n  <a href="./README_CN.md"><img alt="ç®€ä½“ä¸­æ–‡ç‰ˆè‡ªè¿°æ–‡ä»¶" src="https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-d9d9d9"></a>\n  <a href="./README_JA.md"><img alt="æ—¥æœ¬èªã®README" src="https://img.shields.io/badge/æ—¥æœ¬èª-d9d9d9"></a>\n\n</div>\n\n**1. Intelligent SQL Generation**:  \nChat2DB Pro supports AI-driven intelligent SQL development to help you write SQL queries faster.\n\n**2. Database Management**:  \nSupports more than 10 databases, including MySQL, PostgreSQL, H2, Oracle, SQLServer, SQLite, MariaDB, ClickHouse, DM, Presto, DB2, OceanBase, Hive, KingBase, MongoDB, Redis, Snowflake, and more.\n\n**3. Intelligent Report Generation**:  \nChat2DB Pro supports AI-driven intelligent data reporting to help you generate dashboards faster.\n\n**4. Data Structure Synchronization**:  \nChat2DB Pro supports database table structure synchronization to help you sync database table structures faster.\n\n## Feature Comparison\n\n<table style="width: 100%;">\n  <tr>\n    <th align="center">Feature</th>\n    <th align="center">Community Open Source</th>\n    <th align="center">Local</th>\n    <th align="center">Pro </th>\n  </tr>\n  <tr>\n    <td align="center">Database Types</td>\n    <td align="center">16+</td>\n    <td align="center">Target 100+</td>\n    <td align="center">Target 100+</td>\n  </tr>\n  <tr>\n    <td align="center">Supported AI</td>\n    <td align="center">Requires AI Configuration</td>\n    <td align="center">AI ready on installation</td>\n    <td align="center">AI ready on installation</td>\n  </tr>\n  <tr>\n    <td align="center">AI Capabilities</td>\n    <td align="center">Basic</td>\n    <td align="center">Varied</td>\n    <td align="center">Varied</td>\n  </tr>\n  <tr>\n    <td align="center">Visual Table Editor</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">SQL Console</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">SQL Formatting</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Save Query Records</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Theme Color Settings</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Data Structure Sync</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Database Grouping</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Database Structure Import/Export</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Data Import/Export</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Data Migration</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Copy/Clear Table</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Open and Run SQL Files</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">UML Diagram</td>\n    <td align="center">âŒ</td>\n    <td align="center">In Development</td>\n    <td align="center">In Development</td>\n  </tr>\n  <tr>\n    <td align="center">Generate Code</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Copy Results as Insert/Update</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Modify Query Results</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Intelligent SQL Editor</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">AI Table Creation</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">AI Data Sets</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Chat2Excel</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Intelligent Dashboard</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Editor Settings</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Custom Shortcuts</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n    <td align="center">âœ…</td>\n  </tr>\n  <tr>\n    <td align="center">Cross-device Usage</td>\n    <td align="center">âŒ</td>\n    <td align="center">âŒ</td>\n    <td align="center">âœ…</td>\n  </tr>\n</table>\n\n## Download and Installation\nChat2DB is a cross-platform application that supports Windows, MacOS, and Linux. You can download Chat2DB from the following links:\n- [Download Pro Version](https://chat2db.ai/download)\n- [Download Local Version](https://chat2db.ai/download)\n- [Download Open Source Version](https://github.com/CodePhiliaX/Chat2DB/releases/tag/v0.3.6)\n\n## Community Edition Docker Installation\n\n### System Requirements\n\nBefore installing Chat2DB, ensure your system meets the following requirements:\n- Docker 19.03.0 or later\n- Docker Compose 1.25.0 or later\n- CPU >= 2 Cores\n- RAM >= 4 GiB\n\n```bash\n  docker rm chat2db\n  \n  docker run --name=chat2db -ti -p 10824:10824 -v ~/.chat2db-docker:/root/.chat2db  chat2db/chat2db:latest\n\n  docker start chat2db\n  \n```\n## Code Debugging\n\n## Runtime Environment\n\nNote:\nIf local debugging is needed:\n\n- Java runtime: <a href="https://adoptopenjdk.net/" target="_blank">Open JDK 17</a>\n- Node.js runtime: Node 16 <a href="https://nodejs.org/" target="_blank">Node.js</a>.\n\n**Clone the repository locally**\n\n```bash\n$ git clone git@github.com:chat2db/Chat2DB.git\n```\n\n**Frontend Debugging**\n\n```bash\nNode version must be 16 or higher  \nUse yarn only, npm is not supported\n$ cd Chat2DB/chat2db-client\n$ yarn\n$ yarn run start:web\n```\n\n**Backend Debugging**\n\n```bash\n$ cd ../chat2db-server\n$ mvn clean install # Maven version 3.8 or higher is required\n$ cd chat2db-server/chat2db-server-start/target/\n$ java -jar -Dloader.path=./lib -Dchatgpt.apiKey=xxxxx chat2db-server-start.jar  # éœ€è¦å®‰è£…java 17ä»¥ä¸Šç‰ˆæœ¬ï¼Œå¯åŠ¨åº”ç”¨ chatgpt.apiKey éœ€è¦è¾“å…¥ChatGPTçš„key,å¦‚æœä¸è¾“å…¥æ— æ³•ä½¿ç”¨AIGCåŠŸèƒ½\n```\n**Standalone Deployment**\n```bash\n# chat2db-client\n$ npm run build:web:prod \n$ cp -r dist ../chat2db-server/chat2db-server-start/src/main/resources/static/front \n$ cp -r dist/index.html ../chat2db-server/chat2db-server-start/src/main/resources/thymeleaf\n```\n\n##  Contact Us\n\n- Email: Chat2DB@ch2db.com\n- Discord: [Join our Discord server](https://discord.gg/JDkwB6JS8A)\n- Twitter: [@Chat2DB](https://x.com/Chat2DB_AI)\n- YouTube: [Chat2DB Channel](https://www.youtube.com/@chat2db.tutorial)\n- GitHub: [Chat2DB GitHub](https://github.com/codePhiliaX/chat2db)\n\n\n##  Acknowledgments\n\n\nThanks to everyone who has contributed to Chat2DB~~\n\n\n<a href="https://github.com/chat2db/Chat2DB/graphs/contributors">\n  <img src="https://contrib.rocks/image?repo=chat2db/Chat2DB" />\n</a>\n\n## Star History\n\n<a href="https://star-history.com/#CodePhiliaX/chat2db&Date">\n  <picture>\n    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=CodePhiliaX/chat2db&type=Date&theme=dark" />\n    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=CodePhiliaX/chat2db&type=Date" />\n    <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=CodePhiliaX/chat2db&type=Date" />\n  </picture>\n</a>\n\n## License\nThe primary license used by this software is the [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), supplemented by the [Chat2DB License](./Chat2DB_LICENSE).\n\n', '{"language":"Java","stars":24779,"forks":2702,"watchers":24779,"open_issues":500,"topics":["ai","bi","chatgpt","clickhouse","clickhouse-client","database","datagrip","db2","dbeaver","gpt","hive","mysql","navicat","oracle","postgresql","redis","redis-client","sqlserver","text2sql"],"default_branch":"main","size_kb":18843,"archived":false,"fork":false,"has_wiki":true,"has_pages":false}', '[]', '[{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:chat2db:Chat2DB&hashtags=ChatGPT,AGI,SQL%20Client,Reporting%20tool","source_url":"https://github.com/chat2db/Chat2DB&hashtags=ChatGPT,AGI,SQL%20Client,Reporting%20tool"},{"type":"has_code","target_id":"github:chat2db:Chat2DB","source_url":"https://github.com/chat2db/Chat2DB"},{"type":"has_code","target_id":"github:chat2db:Chat2DB","source_url":"https://github.com/chat2db/Chat2DB"},{"type":"has_code","target_id":"github:chat2db:Chat2DB","source_url":"https://github.com/chat2db/Chat2DB"},{"type":"has_code","target_id":"github:chat2db:Chat2DB","source_url":"https://github.com/chat2db/Chat2DB"},{"type":"has_code","target_id":"github:chat2db:Chat2DB","source_url":"https://github.com/chat2db/Chat2DB"},{"type":"has_code","target_id":"github:chat2db:Chat2DB&title=Chat2DB-An%20intelligent%20and%20versatile%20general-purpose%20SQL%20client%20and%20reporting%20tool%20for%20databases%20which%20integrates%20ChatGPT%20capabilities.","source_url":"https://github.com/chat2db/Chat2DB&title=Chat2DB-An%20intelligent%20and%20versatile%20general-purpose%20SQL%20client%20and%20reporting%20tool%20for%20databases%20which%20integrates%20ChatGPT%20capabilities."},{"type":"has_code","target_id":"github:chat2db:Chat2DB","source_url":"https://github.com/chat2db/Chat2DB"},{"type":"has_code","target_id":"github:chat2db:Chat2DB","source_url":"https://github.com/chat2db/Chat2DB"},{"type":"has_code","target_id":"github:CodePhiliaX:Chat2DB","source_url":"https://github.com/CodePhiliaX/Chat2DB"},{"type":"has_code","target_id":"github:codePhiliaX:chat2db","source_url":"https://github.com/codePhiliaX/chat2db"},{"type":"has_code","target_id":"github:chat2db:Chat2DB","source_url":"https://github.com/chat2db/Chat2DB"}]', NULL, 'Apache-2.0', 'approved', 80, 'e9244607d6ab91594f567514405e8f88', NULL, NULL, CURRENT_TIMESTAMP);
