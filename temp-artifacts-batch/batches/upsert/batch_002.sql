/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tiiuae-falcon-40b-instruct', 'huggingface--tiiuae--falcon-40b-instruct', 'falcon-40b-instruct', 'tiiuae', '--- datasets: - tiiuae/falcon-refinedweb language: - en inference: false license: apache-2.0 --- **Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize. It is made available under the Apache 2.0 license.** *Paper coming soon üòä.* ü§ó To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF! * **You are looking for a ready-to-use chat/instruct model b...', '["transformers","pytorch","falcon","text-generation","custom_code","en","dataset:tiiuae/falcon-refinedweb","arxiv:2205.14135","arxiv:1911.02150","arxiv:2005.14165","arxiv:2104.09864","arxiv:2306.01116","arxiv:2304.01196","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 1179, 44233, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tiiuae/falcon-40b-instruct","fetched_at":"2025-12-08T10:30:37.941Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ndatasets:\n- tiiuae/falcon-refinedweb\nlanguage:\n- en\ninference: false\nlicense: apache-2.0\n---\n\n# ‚ú® Falcon-40B-Instruct\n\n**Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by [TII](https://www.tii.ae) based on [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) and finetuned on a mixture of [Baize](https://github.com/project-baize/baize-chatbot). It is made available under the Apache 2.0 license.**\n\n*Paper coming soon üòä.*\n\nü§ó To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading [this great blogpost fron HF](https://huggingface.co/blog/falcon)!\n\n## Why use Falcon-40B-Instruct?\n\n* **You are looking for a ready-to-use chat/instruct model based on [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b).**\n* **Falcon-40B is the best open-source model available.** It outperforms [LLaMA](https://github.com/facebookresearch/llama), [StableLM](https://github.com/Stability-AI/StableLM), [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1), [MPT](https://huggingface.co/mosaicml/mpt-7b), etc. See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n* **It features an architecture optimized for inference**, with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)). \n\nüí¨ **This is an instruct model, which may not be ideal for further finetuning.** If you are interested in building your own instruct/chat model, we recommend starting from [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b). \n\nüí∏ **Looking for a smaller, less expensive model?** [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct) is Falcon-40B-Instruct''s little brother!\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = "tiiuae/falcon-40b-instruct"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    "text-generation",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map="auto",\n)\nsequences = pipeline(\n   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f"Result: {seq[''generated_text'']}")\n\n```\n\nFor fast inference with Falcon, check-out [Text Generation Inference](https://github.com/huggingface/text-generation-inference)! Read more in this [blogpost]((https://huggingface.co/blog/falcon). \n\nYou will need **at least 85-100GB of memory** to swiftly run inference with Falcon-40B.\n\n\n\n# Model Card for Falcon-40B-Instruct\n\n## Model Details\n\n### Model Description\n\n- **Developed by:** [https://www.tii.ae](https://www.tii.ae);\n- **Model type:** Causal decoder-only;\n- **Language(s) (NLP):** English and French;\n- **License:** Apache 2.0;\n- **Finetuned from model:** [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b).\n\n### Model Source\n\n- **Paper:** *coming soon*.\n\n## Uses\n\n### Direct Use\n\nFalcon-40B-Instruct has been finetuned on a chat dataset.\n\n### Out-of-Scope Use\n\nProduction use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful. \n\n## Bias, Risks, and Limitations\n\nFalcon-40B-Instruct is mostly trained on English data, and will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.\n\n### Recommendations\n\nWe recommend users of Falcon-40B-Instruct to develop guardrails and to take appropriate precautions for any production use.\n\n## How to Get Started with the Model\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = "tiiuae/falcon-40b-instruct"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    "text-generation",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map="auto",\n)\nsequences = pipeline(\n   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f"Result: {seq[''generated_text'']}")\n\n```\n\n## Training Details\n\n### Training Data\n\nFalcon-40B-Instruct was finetuned on a 150M tokens from [Bai ze](https://github.com/project-baize/baize-chatbot) mixed with 5% of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) data. \n\n\nThe data was tokenized with the Falcon-[7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b) tokenizer.\n\n\n## Evaluation\n\n*Paper coming soon.*\n\nSee the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results.\n\n\n## Technical Specifications \n\nFor more information about pretraining, see [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b).\n\n### Model Architecture and Objective\n\nFalcon-40B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token).\n\nThe architecture is broadly adapted from the GPT-3 paper ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)), with the following differences:\n\n* **Positionnal embeddings:** rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864));\n* **Attention:** multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135));\n* **Decoder-block:** parallel attention/MLP with a single layer norm.\n\nFor multiquery, we are using an internal variant which uses independent key and values per tensor parallel degree.\n\n| **Hyperparameter** | **Value** | **Comment**                            |\n|--------------------|-----------|----------------------------------------|\n| Layers             | 60        |                                        |\n| `d_model`          | 8192      |                                        |\n| `head_dim`         | 64        | Reduced to optimise for FlashAttention |\n| Vocabulary         | 65024     |                                        |\n| Sequence length    | 2048      |                                        |\n\n### Compute Infrastructure\n\n#### Hardware\n\nFalcon-40B-Instruct was trained on AWS SageMaker, on 64 A100 40GB GPUs in P4d instances. \n\n#### Software\n\nFalcon-40B-Instruct was trained a custom distributed training codebase, Gigatron. It uses a 3D parallelism approach combined with ZeRO and high-performance Triton kernels (FlashAttention, etc.)\n\n\n## Citation\n\n*Paper coming soon* üòä. In the meanwhile, you can use the following information to cite: \n```\n@article{falcon40b,\n  title={{Falcon-40B}: an open large language model with state-of-the-art performance},\n  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},\n  year={2023}\n}\n```\n\nTo learn more about the pretraining dataset, see the üìì [RefinedWeb paper](https://arxiv.org/abs/2306.01116).\n\n```\n@article{refinedweb,\n  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},\n  author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},\n  journal={arXiv preprint arXiv:2306.01116},\n  eprint={2306.01116},\n  eprinttype = {arXiv},\n  url={https://arxiv.org/abs/2306.01116},\n  year={2023}\n}\n```\n\nTo cite the [Baize](https://github.com/project-baize/baize-chatbot) instruction dataset used for this model: \n```\n@article{xu2023baize,\n  title={Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data},\n  author={Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},\n  journal={arXiv preprint arXiv:2304.01196},\n  year={2023}\n}\n```\n\n\n## License\n\nFalcon-40B-Instruct is made available under the Apache 2.0 license.\n\n## Contact\nfalconllm@tii.ae', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":167352935898,"files_count":20,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["FalconForCausalLM"],"auto_map":{"AutoConfig":"configuration_falcon.FalconConfig","AutoModel":"modeling_falcon.FalconModel","AutoModelForSequenceClassification":"modeling_falcon.FalconForSequenceClassification","AutoModelForTokenClassification":"modeling_falcon.FalconForTokenClassification","AutoModelForQuestionAnswering":"modeling_falcon.FalconForQuestionAnswering","AutoModelForCausalLM":"modeling_falcon.FalconForCausalLM"},"model_type":"falcon","tokenizer_config":{"eos_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:project-baize:baize-chatbot","source_url":"https://github.com/project-baize/baize-chatbot"},{"type":"has_code","target_id":"github:facebookresearch:llama","source_url":"https://github.com/facebookresearch/llama"},{"type":"has_code","target_id":"github:Stability-AI:StableLM","source_url":"https://github.com/Stability-AI/StableLM"},{"type":"has_code","target_id":"github:huggingface:text-generation-inference","source_url":"https://github.com/huggingface/text-generation-inference"},{"type":"has_code","target_id":"github:project-baize:baize-chatbot","source_url":"https://github.com/project-baize/baize-chatbot"},{"type":"has_code","target_id":"github:project-baize:baize-chatbot","source_url":"https://github.com/project-baize/baize-chatbot"},{"type":"based_on_paper","target_id":"arxiv:2205.14135","source_url":"https://arxiv.org/abs/2205.14135"},{"type":"based_on_paper","target_id":"arxiv:1911.02150","source_url":"https://arxiv.org/abs/1911.02150"},{"type":"based_on_paper","target_id":"arxiv:2005.14165","source_url":"https://arxiv.org/abs/2005.14165"},{"type":"based_on_paper","target_id":"arxiv:2104.09864","source_url":"https://arxiv.org/abs/2104.09864"},{"type":"based_on_paper","target_id":"arxiv:2306.01116","source_url":"https://arxiv.org/abs/2306.01116"},{"type":"based_on_paper","target_id":"arxiv:2304.01196","source_url":"https://arxiv.org/abs/2304.01196"}]', NULL, 'Apache-2.0', 'approved', 65, '84785c2c877cf93e2fa83d06ffc79b8f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mosaicml-mpt-7b', 'huggingface--mosaicml--mpt-7b', 'mpt-7b', 'mosaicml', '--- license: apache-2.0 tags: - Composer - MosaicML - llm-foundry - StreamingDatasets datasets: - mc4 - c4 - togethercomputer/RedPajama-Data-1T - bigcode/the-stack - allenai/s2orc inference: false --- MPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code. This model was trained by MosaicML. MPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and in...', '["transformers","pytorch","mpt","text-generation","composer","mosaicml","llm-foundry","streamingdatasets","custom_code","dataset:mc4","dataset:c4","dataset:togethercomputer/redpajama-data-1t","dataset:bigcode/the-stack","dataset:allenai/s2orc","arxiv:2108.12409","arxiv:2302.13971","arxiv:2205.14135","arxiv:2010.04245","arxiv:1909.08053","arxiv:2302.06675","license:apache-2.0","text-generation-inference","region:us"]', 'text-generation', 1174, 13039, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mosaicml/mpt-7b","fetched_at":"2025-12-08T10:30:37.941Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ntags:\n- Composer\n- MosaicML\n- llm-foundry\n- StreamingDatasets\ndatasets:\n- mc4\n- c4\n- togethercomputer/RedPajama-Data-1T\n- bigcode/the-stack\n- allenai/s2orc\ninference: false\n---\n\n# MPT-7B\n\nMPT-7B is a decoder-style transformer pretrained from scratch on 1T tokens of English text and code.\nThis model was trained by [MosaicML](https://www.mosaicml.com).\n\nMPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference.\n\nThese architectural changes include performance-optimized layer implementations and the elimination of context length limits by replacing\npositional embeddings with Attention with Linear Biases ([ALiBi](https://arxiv.org/abs/2108.12409)).\nThanks to these modifications, MPT models can be trained with high throughput efficiency and stable convergence.\nMPT models can also be served efficiently with both standard HuggingFace pipelines and NVIDIA''s [FasterTransformer](https://github.com/NVIDIA/FasterTransformer).\n\nThis model uses the MosaicML LLM codebase, which can be found in the [llm-foundry repository](https://github.com/mosaicml/llm-foundry). It was trained by MosaicML‚Äôs NLP team on the [MosaicML platform](https://www.mosaicml.com/training) for LLM pretraining, finetuning, and inference.\n\n### How is this model different?\n\nMPT-7B is\n\n* **Licensed for the possibility of commercial use** (unlike [LLaMA](https://arxiv.org/abs/2302.13971)).\n* **Trained on a large amount of data** (1T tokens like [LLaMA](https://arxiv.org/abs/2302.13971) vs. 300B for [Pythia](https://github.com/EleutherAI/pythia), 300B for [OpenLLaMA](https://github.com/openlm-research/open_llama), and 800B for [StableLM](https://github.com/Stability-AI/StableLM)).\n* **Prepared to handle extremely long inputs** thanks to [ALiBi](https://arxiv.org/abs/2108.12409) (we finetuned [MPT-7B-StoryWriter-65k+](https://huggingface.co/mosaicml/mpt-7b-storywriter) on up to 65k inputs and can handle up to 84k vs. 2k-4k for other open source models).\n* **Capable of fast training and inference** (via [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf) and [FasterTransformer](https://github.com/NVIDIA/FasterTransformer))\n* **Equipped with highly efficient open-source training code** via the [llm-foundry repository](https://github.com/mosaicml/llm-foundry)\n\n### Models finetuned off MPT-7B:\n\nThe following models are finetuned on MPT-7B:\n\n* [MPT-7B-StoryWriter-65k+](https://huggingface.co/mosaicml/mpt-7b-storywriter): a model designed to read and write fictional stories with super long context lengths.\nBuilt by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the [books3 dataset](https://huggingface.co/datasets/the_pile_books3).\nAt inference time, thanks to [ALiBi](https://arxiv.org/abs/2108.12409), MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens.\nWe demonstrate generations as long as 80k tokens on a single A100-80GB GPU in our [blogpost](www.mosaicml.com/blog/mpt-7b).\n  * License: Apache 2.0\n\n* [MPT-7B-Instruct](https://huggingface.co/mosaicml/mpt-7b-instruct): a model for short-form instruction following.\nBuilt by finetuning MPT-7B on a [dataset](https://huggingface.co/datasets/mosaicml/dolly_hhrlhf) we also release, derived from the [Databricks Dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) and the [Anthropic Helpful and Harmless (HH-RLHF)](https://huggingface.co/datasets/Anthropic/hh-rlhf) datasets.\n  * License: Apache 2.0\n\n* [MPT-7B-Chat](https://huggingface.co/mosaicml/mpt-7b-chat): a chatbot-like model for dialogue generation.\nBuilt by finetuning MPT-7B on the [ShareGPT-Vicuna](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna), [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3),\n [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca), [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf), and [Evol-Instruct](https://huggingface.co/datasets/victor123/evol_instruct_70k) datasets.\n  * License: _CC-By-NC-SA-4.0_\n\n## Model Date\n\nMay 5, 2023\n\n## Model License\n\nApache-2.0\n\n## Documentation\n\n* [Blog post: Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b)\n* [Codebase (mosaicml/llm-foundry repo)](https://github.com/mosaicml/llm-foundry/)\n* Questions: Feel free to contact us via the [MosaicML Community Slack](https://mosaicml.me/slack)!\n\n\n## How to Use\n\nThis model is best used with the MosaicML [llm-foundry repository](https://github.com/mosaicml/llm-foundry) for training and finetuning.\n\n```python\nimport transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  ''mosaicml/mpt-7b'',\n  trust_remote_code=True\n)\n```\nNote: This model requires that `trust_remote_code=True` be passed to the `from_pretrained` method.\nThis is because we use a custom `MPT` model architecture that is not yet part of the Hugging Face `transformers` package.\n`MPT` includes options for many training efficiency features such as [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf), [ALiBi](https://arxiv.org/abs/2108.12409), [QK LayerNorm](https://arxiv.org/abs/2010.04245), and more.\n\nTo use the optimized [triton implementation](https://github.com/openai/triton) of FlashAttention, you can load the model on GPU (`cuda:0`) with `attn_impl=''triton''` and with `bfloat16` precision:\n```python\nimport torch\nimport transformers\n\nname = ''mosaicml/mpt-7b''\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config[''attn_impl''] = ''triton''\nconfig.init_device = ''cuda:0'' # For fast initialization directly on GPU!\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n  trust_remote_code=True\n)\n```\n\nAlthough the model was trained with a sequence length of 2048, ALiBi enables users to increase the maximum sequence length during finetuning and/or inference. For example:\n\n```python\nimport transformers\n\nname = ''mosaicml/mpt-7b''\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  trust_remote_code=True\n)\n```\n\nThis model was trained with the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer.\n\n```python\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(''EleutherAI/gpt-neox-20b'')\n```\n\nThe model can then be used, for example, within a text-generation pipeline.  \nNote: when running Torch modules in lower precision, it is best practice to use the [torch.autocast context manager](https://pytorch.org/docs/stable/amp.html).\n\n```python\nfrom transformers import pipeline\n\npipe = pipeline(''text-generation'', model=model, tokenizer=tokenizer, device=''cuda:0'')\n\nwith torch.autocast(''cuda'', dtype=torch.bfloat16):\n    print(\n        pipe(''Here is a recipe for vegan banana bread:\n'',\n            max_new_tokens=100,\n            do_sample=True,\n            use_cache=True))\n```\n\n## Model Description\n\nThe architecture is a modification of a standard decoder-only transformer.\n\nThe model has been modified from a standard transformer in the following ways:\n* It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)\n* It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings\n* It does not use biases\n\n\n| Hyperparameter | Value |\n|----------------|-------|\n|n_parameters | 6.7B |\n|n_layers | 32 |\n| n_heads | 32 |\n| d_model | 4096 |\n| vocab size | 50432 |\n| sequence length | 2048 |\n\n\n\n## Training Data\n\n### Streaming Datasets\n\nData was formatted using the MosaicML [StreamingDataset](https://github.com/mosaicml/streaming) library to host our data in object storage and efficiently stream it to our compute cluster during training.\nStreamingDataset obviates the need to download the whole dataset before starting training, and allows instant resumption of training from any point in the dataset.\n\n\n### Data Mix\n\nThe model was trained for 1T tokens (with batch size 1760 and sequence length 2048). It was trained on the following data mix:\n\n\n| Data Source | Number of Tokens in Source | Proportion | Effective Number of Tokens | Epochs |\n|-------------|----------------------------|------------|----------------------------|--------|\n| mC4 3.1.0 - English | 417.99 B | 0.33 | 330 B | 0.14 |\n| C4 - English - SemDedup 80% | 100.42 B | 0.299 | 299 B | 2.98 |\n| RedPajama - CommonCrawl | 878.45 B | 0.1 | 100 B | 0.11 |\n| The Stack - Selected Languages | 463.78 B | 0.1 | 100 B | 0.22 |\n| RedPajama - Wikipedia - En | 4.87 B | 0.04 | 40 B | 8.21 |\n| The Stack - Markdown | 107.07 B | 0.035 | 35 B | 0.33 |\n| S2ORC | 48.85 B | 0.033 | 33 B | 0.68 |\n| RedPajama - Books | 26.02 B | 0.03 | 30B | 1.15 |\n| RedPajama - arXiv | 28.10 B | 0.019 | 19 B | 0.68 |\n| RedPajama - StackExchange | 20.54 B | 0.014 | 14 B |0.68 |\n\nSamples for each batch were selected from one of the datasets with the probability specified above.\nThe examples were shuffled within each dataset, and each example was constructed from as many sequences from that dataset as were necessary to fill the 2048 sequence length.\n\nThe data was tokenized using the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer. This BPE tokenizer has a number of desirable characteristics,\nmost of which are relevant for tokenizing code:\n(1) It was trained on a diverse mix of data that includes code (The Pile)\n(2) It applies consistent space delimitation, unlike the GPT2 tokenizer which tokenizes inconsistently depending on the presence of prefix spaces\n(3) It contains tokens for repeated space characters, which allows superior compression of text with large amounts of repeated space characters.\n\nThe model vocabulary size of 50432 was set to be a multiple of 128 (as in [MEGATRON-LM](https://arxiv.org/abs/1909.08053)), model flop utilization (MFU) increased by up to four percentage points.\n\n### Training Configuration\n\nThis model was trained on 440 A100-40GBs for about 9.5 days using the [MosaicML Platform](https://www.mosaicml.com/platform).\nThe model was trained with sharded data parallelism using [FSDP](https://pytorch.org/docs/stable/fsdp.html) and used the [LION](https://arxiv.org/abs/2302.06675) optimizer.\n\n## Limitations and Biases\n\n_The following language is modified from [EleutherAI''s GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b)_\n\nMPT-7B (Base) is **not** intended for deployment without finetuning.\nIt should not be used for human-facing interactions without further guardrails and user consent.\n\nMPT-7B can produce factually incorrect output, and should not be relied on to produce factually accurate information.\nMPT-7B was trained on various public datasets.\nWhile great efforts have been taken to clean the pretraining data, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\n\n\n## MosaicML Platform\n\nIf you''re interested in [training](https://www.mosaicml.com/training) and [deploying](https://www.mosaicml.com/inference) your own MPT or LLMs on the MosaicML Platform, [sign up here](https://forms.mosaicml.com/demo?utm_source=huggingface&utm_medium=referral&utm_campaign=mpt-7b).\n\n## Disclaimer\n\nThe license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please cosult an attorney before using this model for commercial purposes.\n\n## Citation\n\nPlease cite this model using the following format:\n\n```\n@online{MosaicML2023Introducing,\n    author    = {MosaicML NLP Team},\n    title     = {Introducing MPT-7B: A New Standard for Open-Source,\n    Commercially Usable LLMs},\n    year      = {2023},\n    url       = {www.mosaicml.com/blog/mpt-7b},\n    note      = {Accessed: 2023-05-05},\n    urldate   = {2023-05-05}\n}\n```\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":26597235302,"files_count":26,"spaces_count":60,"gated":false,"private":false,"config":{"architectures":["MPTForCausalLM"],"auto_map":{"AutoConfig":"configuration_mpt.MPTConfig","AutoModelForCausalLM":"modeling_mpt.MPTForCausalLM"},"model_type":"mpt","tokenizer_config":{"bos_token":"<|endoftext|>","eos_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:NVIDIA:FasterTransformer","source_url":"https://github.com/NVIDIA/FasterTransformer"},{"type":"has_code","target_id":"github:mosaicml:llm-foundry","source_url":"https://github.com/mosaicml/llm-foundry"},{"type":"has_code","target_id":"github:EleutherAI:pythia","source_url":"https://github.com/EleutherAI/pythia"},{"type":"has_code","target_id":"github:openlm-research:open_llama","source_url":"https://github.com/openlm-research/open_llama"},{"type":"has_code","target_id":"github:Stability-AI:StableLM","source_url":"https://github.com/Stability-AI/StableLM"},{"type":"has_code","target_id":"github:NVIDIA:FasterTransformer","source_url":"https://github.com/NVIDIA/FasterTransformer"},{"type":"has_code","target_id":"github:mosaicml:llm-foundry","source_url":"https://github.com/mosaicml/llm-foundry"},{"type":"has_code","target_id":"github:mosaicml:llm-foundry","source_url":"https://github.com/mosaicml/llm-foundry"},{"type":"has_code","target_id":"github:mosaicml:llm-foundry","source_url":"https://github.com/mosaicml/llm-foundry"},{"type":"has_code","target_id":"github:openai:triton","source_url":"https://github.com/openai/triton"},{"type":"has_code","target_id":"github:mosaicml:streaming","source_url":"https://github.com/mosaicml/streaming"},{"type":"based_on_paper","target_id":"arxiv:2108.12409","source_url":"https://arxiv.org/abs/2108.12409"},{"type":"based_on_paper","target_id":"arxiv:2302.13971","source_url":"https://arxiv.org/abs/2302.13971"},{"type":"based_on_paper","target_id":"arxiv:2205.14135","source_url":"https://arxiv.org/abs/2205.14135"},{"type":"based_on_paper","target_id":"arxiv:2010.04245","source_url":"https://arxiv.org/abs/2010.04245"},{"type":"based_on_paper","target_id":"arxiv:1909.08053","source_url":"https://arxiv.org/abs/1909.08053"},{"type":"based_on_paper","target_id":"arxiv:2302.06675","source_url":"https://arxiv.org/abs/2302.06675"}]', NULL, 'Apache-2.0', 'approved', 80, 'a2bbd05932b62d1fa2176e5426211673', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-ByteDance-Seed-BAGEL-7B-MoT', 'huggingface--bytedance-seed--bagel-7b-mot', 'BAGEL-7B-MoT', 'ByteDance-Seed', '--- license: apache-2.0 base_model: - Qwen/Qwen2.5-7B-Instruct pipeline_tag: any-to-any library_name: bagel-mot --- <p align="left"> <img src="https://lf3-static.bytednsdoc.com/obj/eden-cn/nuhojubrps/banner.png" alt="BAGEL" width="480"/> </p> <p align="left"> <a href="https://bagel-ai.org/"> <img src="https://img.shields.io/badge/BAGEL-Website-0A66C2?logo=safari&logoColor=white" style="display: inline-block; vertical-align: middle;" alt="BAGEL Website" /> </a> <a href="https://arxiv.org/abs/2...', '["bagel-mot","safetensors","bagel","any-to-any","custom_code","arxiv:2505.14683","base_model:qwen/qwen2.5-7b-instruct","base_model:finetune:qwen/qwen2.5-7b-instruct","license:apache-2.0","region:us"]', 'any-to-any', 1164, 839, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/ByteDance-Seed/BAGEL-7B-MoT","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nbase_model:\n- Qwen/Qwen2.5-7B-Instruct\npipeline_tag: any-to-any\nlibrary_name: bagel-mot\n---\n\n\n<p align="left">\n  <img src="https://lf3-static.bytednsdoc.com/obj/eden-cn/nuhojubrps/banner.png" alt="BAGEL" width="480"/>\n</p>\n\n\n# ü•Ø BAGEL ‚Ä¢ Unified Model for Multimodal Understanding and Generation\n\n\n\n<p align="left">\n  <a href="https://bagel-ai.org/">\n    <img\n      src="https://img.shields.io/badge/BAGEL-Website-0A66C2?logo=safari&logoColor=white" style="display: inline-block; vertical-align: middle;"\n      alt="BAGEL Website"\n    />\n  </a>\n  <a href="https://arxiv.org/abs/2505.14683">\n    <img\n      src="https://img.shields.io/badge/BAGEL-Paper-red?logo=arxiv&logoColor=red" style="display: inline-block; vertical-align: middle;"\n      alt="BAGEL Paper on arXiv"\n    />\n  </a>\n  <a href="https://github.com/bytedance-seed/BAGEL" target="_blank" style="margin: 2px;">\n      <img \n        alt="Github" src="https://img.shields.io/badge/BAGEL-Codebase-536af5?color=536af5&logo=github" style="display: inline-block; vertical-align: middle;"\n        alt="BAGEL Codebase"\n      />\n  </a>\n  <a href="https://demo.bagel-ai.org/">\n    <img\n      src="https://img.shields.io/badge/BAGEL-Demo-blue?logo=googleplay&logoColor=white" style="display: inline-block; vertical-align: middle;"\n      alt="BAGEL Demo"\n    />\n  </a>\n  <a href="https://discord.com/invite/Z836xxzy">\n    <img\n      src="https://img.shields.io/badge/BAGEL-Discord-green?logo=discord&logoColor=white" style="display: inline-block; vertical-align: middle;"\n      alt="BAGEL Discord"\n    />\n  </a>\n\n  \n</p>\n\n\n> We present **BAGEL**, an open‚Äësource multimodal foundation model with 7B active parameters (14B total) trained on large‚Äëscale interleaved multimodal data. BAGEL outperforms the current top‚Äëtier open‚Äësource VLMs like Qwen2.5-VL and InternVL-2.5 on standard multimodal understanding leaderboards, and delivers text‚Äëto‚Äëimage quality that is competitive with strong specialist generators such as SD3.\nMoreover, BAGEL demonstrates superior qualitative results in classical image‚Äëediting scenarios than the leading open-source models. More importantly, it extends to free-form visual manipulation, multiview synthesis, and world navigation, capabilities that constitute "world-modeling" tasks beyond the scope of previous image-editing models.\n\n\nThis repository hosts the model weights for **BAGEL**. For installation, usage instructions, and further documentation, please visit our [GitHub repository](https://github.com/bytedance-seed/BAGEL).\n\n\n\n<p align="left"><img src="https://github.com/ByteDance-Seed/Bagel/raw/main/assets/teaser.webp" width="80%"></p>\n\n\n\n\n\n\n## üß† Method\nBAGEL adopts a Mixture-of-Transformer-Experts (MoT) architecture to maximize the model‚Äôs capacity to learn from richly diverse multimodal information. Following the same principle of capacity maximization, it utilizes two separate encoders to capture pixel-level and semantic-level features of an image. The overall framework follows a Next Group of Token Prediction paradigm, where the model is trained to predict the next group of language or visual tokens as a compression target.\n\nBAGEL scales MoT‚Äôs capacity through Pre-training, Continued Training, and Supervised Finetuning on trillions of interleaved multimodal tokens spanning language, image, video, and web data. It surpasses open models on standard understanding and generation benchmarks and demonstrates advanced in-context multimodal abilities like free-form image editing, future frame prediction, 3D manipulation, world navigation, and sequential reasoning.\n\n<p align="left"><img src="https://github.com/ByteDance-Seed/Bagel/raw/main/assets/arch.png" width="50%"></p>\n\n\n## üå± Emerging Properties\n<p align="left"><img src="https://github.com/ByteDance-Seed/Bagel/raw/main/assets/emerging_curves.png" width="50%"></p>\n\nAs we scale up BAGEL‚Äôs pretraining with more multimodal tokens, we observe consistent performance gains across understanding, generation, and editing tasks. Different capabilities emerge at distinct training stages‚Äîmultimodal understanding and generation appear early, followed by basic editing, while complex, intelligent editing emerges later. This staged progression suggests an emergent pattern, where advanced multimodal reasoning builds on well-formed foundational skills. Ablation studies further show that combining VAE and ViT features significantly improves intelligent editing, underscoring the importance of visual-semantic context in enabling complex multimodal reasoning and further supporting its role in the emergence of advanced capabilities.\n\n\n\n## üìä Benchmarks\n### 1. Visual Understanding\n| Model | MME ‚Üë | MMBench ‚Üë |   MMMU ‚Üë | MM-Vet ‚Üë | MathVista ‚Üë |\n| ------------------- | ----------: | ----------: | -------: | -------: | ----------: |\n| Janus-Pro-7B        | -  |     79.2 |     41.0 |     50.0 |           ‚Äì |\n| Qwen2.5-VL-7B      | 2347    |   83.5 | **58.6** |     67.1 |           68.2 |\n| **BAGEL**    | **2388**  |  **85.0** |     55.3 | **67.2** |    **73.1** |\n### 2. Text-to-Image Generation ¬∑ GenEval\n| Model        | Overall ‚Üë |\n| ------------ | --------- |\n| FLUX-1-dev   | 0.82      |\n| SD3-Medium   | 0.74      |\n| Janus-Pro-7B | 0.80      |\n| **BAGEL**    | **0.88**  |\n### 3. Image Editing\n| Model         | GEdit-Bench-EN (SC) ‚Üë | GEdit-Bench-EN (PQ) ‚Üë | GEdit-Bench-EN (O) ‚Üë | IntelligentBench ‚Üë |\n| ------------- | --------------------- | --------------------- | ------------------- | ------------------ |\n| Step1X-Edit   | 7.09                  | 6.76                  | **6.70**            | 14.9               |\n| Gemini-2-exp. | 6.73                  | 6.61                  | 6.32                | **57.6**           |\n| **BAGEL**     | **7.36**              | **6.83**              | 6.52                | 44.0               |\n| **BAGEL+CoT** | ‚Äì                   | ‚Äì                     | ‚Äì                   | 55.3               |\n\n## License\nBAGEL is licensed under the Apache 2.0 license. It is finetuned from [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) and [siglip-so400m-14-384-flash-attn2](https://huggingface.co/HuggingFaceM4/siglip-so400m-14-384-flash-attn2) model, and uses the [FLUX.1-schnell VAE model](https://huggingface.co/black-forest-labs/FLUX.1-schnell), all under Apache 2.0.\n\n## ‚úçÔ∏è Citation\n```bibtex\n@article{deng2025bagel,\n  title   = {Emerging Properties in Unified Multimodal Pretraining},\n  author  = {Deng, Chaorui and Zhu, Deyao and Li, Kunchang and Gou, Chenhui and Li, Feng and Wang, Zeyu and Zhong, Shu and Yu, Weihao and Nie, Xiaonan and Song, Ziang and Shi, Guang and Fan, Haoqi},\n  journal = {arXiv preprint arXiv:2505.14683},\n  year    = {2025}\n}\n```', '{"pipeline_tag":"any-to-any","library_name":"bagel-mot","framework":"bagel-mot","params":14691079811,"storage_bytes":29555840231,"files_count":14,"spaces_count":9,"gated":false,"private":false,"config":{"architectures":["BagelForConditionalGeneration"],"model_type":"bagel","auto_map":{"AutoConfig":"configuration_bagel.BagelConfig","AutoModelForCausalLM":"modeling_bagel.BagelForConditionalGeneration"},"tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:bytedance-seed:BAGEL\"","source_url":"https://github.com/bytedance-seed/BAGEL\""},{"type":"has_code","target_id":"github:bytedance-seed:BAGEL","source_url":"https://github.com/bytedance-seed/BAGEL"},{"type":"has_code","target_id":"github:ByteDance-Seed:Bagel","source_url":"https://github.com/ByteDance-Seed/Bagel"},{"type":"has_code","target_id":"github:ByteDance-Seed:Bagel","source_url":"https://github.com/ByteDance-Seed/Bagel"},{"type":"has_code","target_id":"github:ByteDance-Seed:Bagel","source_url":"https://github.com/ByteDance-Seed/Bagel"},{"type":"based_on_paper","target_id":"arxiv:2505.14683","source_url":"https://arxiv.org/abs/2505.14683"}]', NULL, 'Apache-2.0', 'approved', 65, 'aea253171c69367202e1db556079b56e', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-timbrooks-instruct-pix2pix', 'huggingface--timbrooks--instruct-pix2pix', 'instruct-pix2pix', 'timbrooks', '--- license: mit tags: - image-to-image --- GitHub: https://github.com/timothybrooks/instruct-pix2pix <img src=''https://instruct-pix2pix.timothybrooks.com/teaser.jpg''/> To use , install using for now. The pipeline will be available in the next release', '["diffusers","safetensors","image-to-image","license:mit","diffusers:stablediffusioninstructpix2pixpipeline","region:us"]', 'image-to-image', 1162, 59244, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/timbrooks/instruct-pix2pix","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\ntags:\n- image-to-image\n---\n\n# InstructPix2Pix: Learning to Follow Image Editing Instructions\nGitHub: https://github.com/timothybrooks/instruct-pix2pix\n<img src=''https://instruct-pix2pix.timothybrooks.com/teaser.jpg''/>\n\n\n\n## Example\n\nTo use `InstructPix2Pix`, install `diffusers` using `main` for now. The pipeline will be available in the next release\n\n```bash\npip install diffusers accelerate safetensors transformers\n```\n\n```python\nimport PIL\nimport requests\nimport torch\nfrom diffusers import StableDiffusionInstructPix2PixPipeline, EulerAncestralDiscreteScheduler\n\nmodel_id = "timbrooks/instruct-pix2pix"\npipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id, torch_dtype=torch.float16, safety_checker=None)\npipe.to("cuda")\npipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n\nurl = "https://raw.githubusercontent.com/timothybrooks/instruct-pix2pix/main/imgs/example.jpg"\ndef download_image(url):\n    image = PIL.Image.open(requests.get(url, stream=True).raw)\n    image = PIL.ImageOps.exif_transpose(image)\n    image = image.convert("RGB")\n    return image\nimage = download_image(url)\n\nprompt = "turn him into cyborg"\nimages = pipe(prompt, image=image, num_inference_steps=10, image_guidance_scale=1).images\nimages[0]\n```', '{"pipeline_tag":"image-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":37333325594,"files_count":31,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionInstructPix2PixPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:timothybrooks:instruct-pix2pix","source_url":"https://github.com/timothybrooks/instruct-pix2pix"}]', NULL, 'MIT', 'approved', 50, '40788bd799dece7ad9a18971c80231e2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-zai-org-chatglm3-6b', 'huggingface--zai-org--chatglm3-6b', 'chatglm3-6b', 'zai-org', '--- language: - zh - en tags: - glm - chatglm - thudm --- <p align="center"> üíª <a href="https://github.com/THUDM/ChatGLM" target="_blank">Github Repo</a> ‚Ä¢ üê¶ <a href="https://twitter.com/thukeg" target="_blank">Twitter</a> ‚Ä¢ üìÉ <a href="https://arxiv.org/abs/2103.10360" target="_blank">[GLM@ACL 22]</a> <a href="https://github.com/THUDM/GLM" target="_blank">[GitHub]</a> ‚Ä¢ üìÉ <a href="https://arxiv.org/abs/2210.02414" target="_blank">[GLM-130B@ICLR 23]</a> <a href="https://github.com/THUDM/GL...', '["transformers","pytorch","safetensors","chatglm","glm","thudm","custom_code","zh","en","arxiv:2103.10360","arxiv:2210.02414","arxiv:2406.12793","endpoints_compatible","region:us"]', 'other', 1157, 63722, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/zai-org/chatglm3-6b","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '---\nlanguage:\n- zh\n- en\ntags:\n- glm\n- chatglm\n- thudm\n---\n# ChatGLM3-6B\n<p align="center">\n  üíª <a href="https://github.com/THUDM/ChatGLM" target="_blank">Github Repo</a> ‚Ä¢ üê¶ <a href="https://twitter.com/thukeg" target="_blank">Twitter</a> ‚Ä¢ üìÉ <a href="https://arxiv.org/abs/2103.10360" target="_blank">[GLM@ACL 22]</a> <a href="https://github.com/THUDM/GLM" target="_blank">[GitHub]</a> ‚Ä¢ üìÉ <a href="https://arxiv.org/abs/2210.02414" target="_blank">[GLM-130B@ICLR 23]</a> <a href="https://github.com/THUDM/GLM-130B" target="_blank">[GitHub]</a> <br>\n</p>\n\n<p align="center">\n    üëã Join our <a href="https://join.slack.com/t/chatglm/shared_invite/zt-25ti5uohv-A_hs~am_D3Q8XPZMpj7wwQ" target="_blank">Slack</a> and <a href="https://github.com/THUDM/ChatGLM/blob/main/resources/WECHAT.md" target="_blank">WeChat</a>\n</p>\n<p align="center">\nüìçExperience the larger-scale ChatGLM model at <a href="https://www.chatglm.cn">chatglm.cn</a>\n</p>\n\n## GLM-4 ÂºÄÊ∫êÊ®°Âûã\n\nÊàë‰ª¨Â∑≤ÁªèÂèëÂ∏ÉÊúÄÊñ∞ÁöÑ **GLM-4** Ê®°ÂûãÔºåËØ•Ê®°ÂûãÂú®Â§ö‰∏™ÊåáÊ†á‰∏äÊúâ‰∫ÜÊñ∞ÁöÑÁ™ÅÁ†¥ÔºåÊÇ®ÂèØ‰ª•Âú®‰ª•‰∏ã‰∏§‰∏™Ê∏†ÈÅì‰ΩìÈ™åÊàë‰ª¨ÁöÑÊúÄÊñ∞Ê®°Âûã„ÄÇ\n+ [GLM-4 ÂºÄÊ∫êÊ®°Âûã](https://huggingface.co/THUDM/glm-4-9b-chat) Êàë‰ª¨Â∑≤ÁªèÂºÄÊ∫ê‰∫Ü GLM-4-9B Á≥ªÂàóÊ®°ÂûãÔºåÂú®ÂêÑÈ°πÊåáÊ†áÁöÑÊµãËØï‰∏äÊúâÊòéÊòæÊèêÂçáÔºåÊ¨¢ËøéÂ∞ùËØï„ÄÇ\n\n## ‰ªãÁªç (Introduction)\nChatGLM3-6B ÊòØ ChatGLM Á≥ªÂàóÊúÄÊñ∞‰∏Ä‰ª£ÁöÑÂºÄÊ∫êÊ®°ÂûãÔºåÂú®‰øùÁïô‰∫ÜÂâç‰∏§‰ª£Ê®°ÂûãÂØπËØùÊµÅÁïÖ„ÄÅÈÉ®ÁΩ≤Èó®Êßõ‰ΩéÁ≠â‰ºóÂ§ö‰ºòÁßÄÁâπÊÄßÁöÑÂü∫Á°Ä‰∏äÔºåChatGLM3-6B ÂºïÂÖ•‰∫ÜÂ¶Ç‰∏ãÁâπÊÄßÔºö\n\n1. **Êõ¥Âº∫Â§ßÁöÑÂü∫Á°ÄÊ®°ÂûãÔºö** ChatGLM3-6B ÁöÑÂü∫Á°ÄÊ®°Âûã ChatGLM3-6B-Base ÈááÁî®‰∫ÜÊõ¥Â§öÊ†∑ÁöÑËÆ≠ÁªÉÊï∞ÊçÆ„ÄÅÊõ¥ÂÖÖÂàÜÁöÑËÆ≠ÁªÉÊ≠•Êï∞ÂíåÊõ¥ÂêàÁêÜÁöÑËÆ≠ÁªÉÁ≠ñÁï•„ÄÇÂú®ËØ≠‰πâ„ÄÅÊï∞Â≠¶„ÄÅÊé®ÁêÜ„ÄÅ‰ª£Á†Å„ÄÅÁü•ËØÜÁ≠â‰∏çÂêåËßíÂ∫¶ÁöÑÊï∞ÊçÆÈõÜ‰∏äÊµãËØÑÊòæÁ§∫ÔºåChatGLM3-6B-Base ÂÖ∑ÊúâÂú® 10B ‰ª•‰∏ãÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°Âûã‰∏≠ÊúÄÂº∫ÁöÑÊÄßËÉΩ„ÄÇ\n2. **Êõ¥ÂÆåÊï¥ÁöÑÂäüËÉΩÊîØÊåÅÔºö** ChatGLM3-6B ÈááÁî®‰∫ÜÂÖ®Êñ∞ËÆæËÆ°ÁöÑ [Prompt Ê†ºÂºè](https://github.com/THUDM/ChatGLM3/blob/main/PROMPT.md)ÔºåÈô§Ê≠£Â∏∏ÁöÑÂ§öËΩÆÂØπËØùÂ§ñ„ÄÇÂêåÊó∂ÂéüÁîüÊîØÊåÅ[Â∑•ÂÖ∑Ë∞ÉÁî®](https://github.com/THUDM/ChatGLM3/blob/main/tool_using/README.md)ÔºàFunction CallÔºâ„ÄÅ‰ª£Á†ÅÊâßË°åÔºàCode InterpreterÔºâÂíå Agent ‰ªªÂä°Á≠âÂ§çÊùÇÂú∫ÊôØ„ÄÇ\n3. **Êõ¥ÂÖ®Èù¢ÁöÑÂºÄÊ∫êÂ∫èÂàóÔºö** Èô§‰∫ÜÂØπËØùÊ®°Âûã ChatGLM3-6B Â§ñÔºåËøòÂºÄÊ∫ê‰∫ÜÂü∫Á°ÄÊ®°Âûã ChatGLM-6B-Base„ÄÅÈïøÊñáÊú¨ÂØπËØùÊ®°Âûã ChatGLM3-6B-32K„ÄÇ‰ª•‰∏äÊâÄÊúâÊùÉÈáçÂØπÂ≠¶ÊúØÁ†îÁ©∂**ÂÆåÂÖ®ÂºÄÊîæ**ÔºåÂú®Â°´ÂÜô[ÈóÆÂç∑](https://open.bigmodel.cn/mla/form)ËøõË°åÁôªËÆ∞Âêé**‰∫¶ÂÖÅËÆ∏ÂÖçË¥πÂïÜ‰∏ö‰ΩøÁî®**„ÄÇ\n\nChatGLM3-6B is the latest open-source model in the ChatGLM series. While retaining many excellent features such as smooth dialogue and low deployment threshold from the previous two generations, ChatGLM3-6B introduces the following features:\n\n1. **More Powerful Base Model:** The base model of ChatGLM3-6B, ChatGLM3-6B-Base, employs a more diverse training dataset, more sufficient training steps, and a more reasonable training strategy. Evaluations on datasets such as semantics, mathematics, reasoning, code, knowledge, etc., show that ChatGLM3-6B-Base has the strongest performance among pre-trained models under 10B.\n2. **More Comprehensive Function Support:** ChatGLM3-6B adopts a newly designed [Prompt format](https://github.com/THUDM/ChatGLM3/blob/main/PROMPT_en.md), in addition to the normal multi-turn dialogue. It also natively supports [function call](https://github.com/THUDM/ChatGLM3/blob/main/tool_using/README_en.md), code interpreter, and complex scenarios such as agent tasks.\n3. **More Comprehensive Open-source Series:** In addition to the dialogue model ChatGLM3-6B, the base model ChatGLM-6B-Base and the long-text dialogue model ChatGLM3-6B-32K are also open-sourced. All the weights are **fully open** for academic research, and after completing the [questionnaire](https://open.bigmodel.cn/mla/form) registration, they are also **allowed for free commercial use**.\n\n## ËΩØ‰ª∂‰æùËµñ (Dependencies)\n\n```shell\npip install protobuf transformers==4.30.2 cpm_kernels torch>=2.0 gradio mdtex2html sentencepiece accelerate\n```\n\n## ‰ª£Á†ÅË∞ÉÁî® (Code Usage)\n\nÂèØ‰ª•ÈÄöËøáÂ¶Ç‰∏ã‰ª£Á†ÅË∞ÉÁî® ChatGLM3-6B Ê®°ÂûãÊù•ÁîüÊàêÂØπËØùÔºö\n\nYou can generate dialogue by invoking the ChatGLM3-6B model with the following code:\n\n```ipython\n>>> from transformers import AutoTokenizer, AutoModel\n>>> tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True)\n>>> model = AutoModel.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True).half().cuda()\n>>> model = model.eval()\n>>> response, history = model.chat(tokenizer, "‰Ω†Â•Ω", history=[])\n>>> print(response)\n‰Ω†Â•Ωüëã!ÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM-6B,ÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†,Ê¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\n>>> response, history = model.chat(tokenizer, "Êôö‰∏äÁù°‰∏çÁùÄÂ∫îËØ•ÊÄé‰πàÂäû", history=history)\n>>> print(response)\nÊôö‰∏äÁù°‰∏çÁùÄÂèØËÉΩ‰ºöËÆ©‰Ω†ÊÑüÂà∞ÁÑ¶ËôëÊàñ‰∏çËàíÊúç,‰ΩÜ‰ª•‰∏ãÊòØ‰∏Ä‰∫õÂèØ‰ª•Â∏ÆÂä©‰Ω†ÂÖ•Áù°ÁöÑÊñπÊ≥ï:\n\n1. Âà∂ÂÆöËßÑÂæãÁöÑÁù°Áú†Êó∂Èó¥Ë°®:‰øùÊåÅËßÑÂæãÁöÑÁù°Áú†Êó∂Èó¥Ë°®ÂèØ‰ª•Â∏ÆÂä©‰Ω†Âª∫Á´ãÂÅ•Â∫∑ÁöÑÁù°Áú†‰π†ÊÉØ,‰Ωø‰Ω†Êõ¥ÂÆπÊòìÂÖ•Áù°„ÄÇÂ∞ΩÈáèÂú®ÊØèÂ§©ÁöÑÁõ∏ÂêåÊó∂Èó¥‰∏äÂ∫ä,Âπ∂Âú®Âêå‰∏ÄÊó∂Èó¥Ëµ∑Â∫ä„ÄÇ\n2. ÂàõÈÄ†‰∏Ä‰∏™ËàíÈÄÇÁöÑÁù°Áú†ÁéØÂ¢É:Á°Æ‰øùÁù°Áú†ÁéØÂ¢ÉËàíÈÄÇ,ÂÆâÈùô,ÈªëÊöó‰∏îÊ∏©Â∫¶ÈÄÇÂÆú„ÄÇÂèØ‰ª•‰ΩøÁî®ËàíÈÄÇÁöÑÂ∫ä‰∏äÁî®ÂìÅ,Âπ∂‰øùÊåÅÊàøÈó¥ÈÄöÈ£é„ÄÇ\n3. ÊîæÊùæË∫´ÂøÉ:Âú®Áù°ÂâçÂÅö‰∫õÊîæÊùæÁöÑÊ¥ªÂä®,‰æãÂ¶ÇÊ≥°‰∏™ÁÉ≠Ê∞¥Êæ°,Âê¨‰∫õËΩªÊüîÁöÑÈü≥‰πê,ÈòÖËØª‰∏Ä‰∫õÊúâË∂£ÁöÑ‰π¶Á±çÁ≠â,ÊúâÂä©‰∫éÁºìËß£Á¥ßÂº†ÂíåÁÑ¶Ëôë,‰Ωø‰Ω†Êõ¥ÂÆπÊòìÂÖ•Áù°„ÄÇ\n4. ÈÅøÂÖçÈ•ÆÁî®Âê´ÊúâÂíñÂï°Âõ†ÁöÑÈ•ÆÊñô:ÂíñÂï°Âõ†ÊòØ‰∏ÄÁßçÂà∫ÊøÄÊÄßÁâ©Ë¥®,‰ºöÂΩ±Âìç‰Ω†ÁöÑÁù°Áú†Ë¥®Èáè„ÄÇÂ∞ΩÈáèÈÅøÂÖçÂú®Áù°ÂâçÈ•ÆÁî®Âê´ÊúâÂíñÂï°Âõ†ÁöÑÈ•ÆÊñô,‰æãÂ¶ÇÂíñÂï°,Ëå∂ÂíåÂèØ‰πê„ÄÇ\n5. ÈÅøÂÖçÂú®Â∫ä‰∏äÂÅö‰∏éÁù°Áú†Êó†ÂÖ≥ÁöÑ‰∫ãÊÉÖ:Âú®Â∫ä‰∏äÂÅö‰∫õ‰∏éÁù°Áú†Êó†ÂÖ≥ÁöÑ‰∫ãÊÉÖ,‰æãÂ¶ÇÁúãÁîµÂΩ±,Áé©Ê∏∏ÊàèÊàñÂ∑•‰ΩúÁ≠â,ÂèØËÉΩ‰ºöÂπ≤Êâ∞‰Ω†ÁöÑÁù°Áú†„ÄÇ\n6. Â∞ùËØïÂëºÂê∏ÊäÄÂ∑ß:Ê∑±ÂëºÂê∏ÊòØ‰∏ÄÁßçÊîæÊùæÊäÄÂ∑ß,ÂèØ‰ª•Â∏ÆÂä©‰Ω†ÁºìËß£Á¥ßÂº†ÂíåÁÑ¶Ëôë,‰Ωø‰Ω†Êõ¥ÂÆπÊòìÂÖ•Áù°„ÄÇËØïÁùÄÊÖ¢ÊÖ¢Âê∏Ê∞î,‰øùÊåÅÂá†ÁßíÈíü,ÁÑ∂ÂêéÁºìÊÖ¢ÂëºÊ∞î„ÄÇ\n\nÂ¶ÇÊûúËøô‰∫õÊñπÊ≥ïÊó†Ê≥ïÂ∏ÆÂä©‰Ω†ÂÖ•Áù°,‰Ω†ÂèØ‰ª•ËÄÉËôëÂí®ËØ¢ÂåªÁîüÊàñÁù°Áú†‰∏ìÂÆ∂,ÂØªÊ±ÇËøõ‰∏ÄÊ≠•ÁöÑÂª∫ËÆÆ„ÄÇ\n```\n\nÂÖ≥‰∫éÊõ¥Â§öÁöÑ‰ΩøÁî®ËØ¥ÊòéÔºåÂåÖÊã¨Â¶Ç‰ΩïËøêË°åÂëΩ‰ª§Ë°åÂíåÁΩëÈ°µÁâàÊú¨ÁöÑ DEMOÔºå‰ª•Âèä‰ΩøÁî®Ê®°ÂûãÈáèÂåñ‰ª•ËäÇÁúÅÊòæÂ≠òÔºåËØ∑ÂèÇËÄÉÊàë‰ª¨ÁöÑ [Github Repo](https://github.com/THUDM/ChatGLM)„ÄÇ\n\nFor more instructions, including how to run CLI and web demos, and model quantization, please refer to our [Github Repo](https://github.com/THUDM/ChatGLM).\n\n\n## ÂçèËÆÆ (License)\n\nÊú¨‰ªìÂ∫ìÁöÑ‰ª£Á†Å‰æùÁÖß [Apache-2.0](LICENSE) ÂçèËÆÆÂºÄÊ∫êÔºåChatGLM3-6B Ê®°ÂûãÁöÑÊùÉÈáçÁöÑ‰ΩøÁî®ÂàôÈúÄË¶ÅÈÅµÂæ™ [Model License](MODEL_LICENSE)„ÄÇ\n\nThe code in this repository is open-sourced under the [Apache-2.0 license](LICENSE), while the use of the ChatGLM3-6B model weights needs to comply with the [Model License](MODEL_LICENSE).\n\n## ÂºïÁî® (Citation)\n\nÂ¶ÇÊûú‰Ω†ËßâÂæóÊàë‰ª¨ÁöÑÂ∑•‰ΩúÊúâÂ∏ÆÂä©ÁöÑËØùÔºåËØ∑ËÄÉËôëÂºïÁî®‰∏ãÂàóËÆ∫Êñá„ÄÇ\n\nIf you find our work helpful, please consider citing the following paper.\n\n```\n@misc{glm2024chatglm,\n      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, \n      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},\n      year={2024},\n      eprint={2406.12793},\n      archivePrefix={arXiv},\n      primaryClass={id=''cs.CL'' full_name=''Computation and Language'' is_active=True alt_name=''cmp-lg'' in_archive=''cs'' is_general=False description=''Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.''}\n}\n```\n', '{"pipeline_tag":null,"library_name":"transformers","framework":"transformers","params":6243584032,"storage_bytes":37462649896,"files_count":27,"spaces_count":100,"gated":false,"private":false,"config":{"model_type":"chatglm","architectures":["ChatGLMModel"],"auto_map":{"AutoConfig":"configuration_chatglm.ChatGLMConfig","AutoModel":"modeling_chatglm.ChatGLMForConditionalGeneration","AutoModelForCausalLM":"modeling_chatglm.ChatGLMForConditionalGeneration","AutoModelForSeq2SeqLM":"modeling_chatglm.ChatGLMForConditionalGeneration","AutoModelForSequenceClassification":"modeling_chatglm.ChatGLMForSequenceClassification"},"tokenizer_config":{"chat_template":"{% for message in messages %}{% if loop.first %}[gMASK]sop<|{{ message[''role''] }}|>\n {{ message[''content''] }}{% else %}<|{{ message[''role''] }}|>\n {{ message[''content''] }}{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>{% endif %}","eos_token":"</s>","pad_token":"<unk>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:THUDM:ChatGLM\"","source_url":"https://github.com/THUDM/ChatGLM\""},{"type":"has_code","target_id":"github:THUDM:GLM\"","source_url":"https://github.com/THUDM/GLM\""},{"type":"has_code","target_id":"github:THUDM:GLM-130B\"","source_url":"https://github.com/THUDM/GLM-130B\""},{"type":"has_code","target_id":"github:THUDM:ChatGLM","source_url":"https://github.com/THUDM/ChatGLM"},{"type":"has_code","target_id":"github:THUDM:ChatGLM3","source_url":"https://github.com/THUDM/ChatGLM3"},{"type":"has_code","target_id":"github:THUDM:ChatGLM3","source_url":"https://github.com/THUDM/ChatGLM3"},{"type":"has_code","target_id":"github:THUDM:ChatGLM3","source_url":"https://github.com/THUDM/ChatGLM3"},{"type":"has_code","target_id":"github:THUDM:ChatGLM3","source_url":"https://github.com/THUDM/ChatGLM3"},{"type":"has_code","target_id":"github:THUDM:ChatGLM","source_url":"https://github.com/THUDM/ChatGLM"},{"type":"has_code","target_id":"github:THUDM:ChatGLM","source_url":"https://github.com/THUDM/ChatGLM"},{"type":"based_on_paper","target_id":"arxiv:2103.10360","source_url":"https://arxiv.org/abs/2103.10360"},{"type":"based_on_paper","target_id":"arxiv:2210.02414","source_url":"https://arxiv.org/abs/2210.02414"},{"type":"based_on_paper","target_id":"arxiv:2406.12793","source_url":"https://arxiv.org/abs/2406.12793"}]', NULL, NULL, 'pending', 55, '2810e3ea07b79881d3d5a43fc86e5794', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-4-Scout-17B-16E-Instruct', 'huggingface--meta-llama--llama-4-scout-17b-16e-instruct', 'Llama-4-Scout-17B-16E-Instruct', 'meta-llama', '', '["transformers","safetensors","llama4","any-to-any","facebook","meta","pytorch","llama","ar","de","en","es","fr","hi","id","it","pt","th","tl","vi","arxiv:2204.05149","base_model:meta-llama/llama-4-scout-17b-16e","license:other","text-generation-inference","endpoints_compatible","region:us"]', 'any-to-any', 1154, 206456, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"any-to-any","library_name":"transformers","framework":"transformers","params":108641793536,"storage_bytes":217343257722,"files_count":64,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["Llama4ForConditionalGeneration"],"model_type":"llama4","tokenizer_config":{"bos_token":"<|begin_of_text|>","eos_token":"<|eot|>","pad_token":"<|finetune_right_pad|>"},"chat_template_jinja":"{{- bos_token }}\n{%- if custom_tools is defined and custom_tools%}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if tools is defined and tools %}\n    {%- set tool_definition = tool_definition ~ (tools | tojson(indent=4)) %}\n{%- else %}\n    {%- set tools = none %}\n{%- endif %}\n\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][''role''] == ''system'' %}\n    {%- set user_provided_system_message = true %}\n    {%- if messages[0][''content''] is string %}\n        {%- set system_message = messages[0][''content'']|trim %}\n    {%- else %}\n        {%- set system_message = messages[0][''content''][0][''text'']|trim %}\n    {%- endif %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- if tools is not none  %}\n        {#- Since not system_message was provided by user, if tool is provided, system_message is now default tool system message #}\n        {#- This system message is from llama website:https://www.llama.com/docs/model-cards-and-prompt-formats/llama4/  #}\n        {%- set system_message = \"You are a helpful assistant and an expert in function composition. You can answer general questions using your internal knowledge OR invoke functions when necessary. Follow these strict guidelines:\\n\\n1. FUNCTION CALLS:\\n- ONLY use functions that are EXPLICITLY listed in the function list below\\n- If NO functions are listed (empty function list []), respond ONLY with internal knowledge or \\\"I don''t have access to [Unavailable service] information\\\"\\n- If a function is not in the list, respond ONLY with internal knowledge or \\\"I don''t have access to [Unavailable service] information\\\"\\n- If ALL required parameters are present AND the query EXACTLY matches a listed function''s purpose: output ONLY the function call(s)\\n- Use exact format: [func_name1(param1=value1, param2=value2), func_name2(...)]\\nExamples:\\nCORRECT: [get_weather(location=\\\"Vancouver\\\"), calculate_route(start=\\\"Boston\\\", end=\\\"New York\\\")] <- Only if get_weather and calculate_route are in function list\\nINCORRECT: get_weather(location=\\\"New York\\\")\\nINCORRECT: Let me check the weather: [get_weather(location=\\\"New York\\\")]\\nINCORRECT: [get_events(location=\\\"Singapore\\\")] <- If function not in list\\n\\n2. RESPONSE RULES:\\n- For pure function requests matching a listed function: ONLY output the function call(s)\\n- For knowledge questions: ONLY output text\\n- For missing parameters: ONLY request the specific missing parameters\\n- For unavailable services (not in function list): output ONLY with internal knowledge or \\\"I don''t have access to [Unavailable service] information\\\". Do NOT execute a function call.\\n- If the query asks for information beyond what a listed function provides: output ONLY with internal knowledge about your limitations\\n- NEVER combine text and function calls in the same response\\n- NEVER suggest alternative functions when the requested service is unavailable\\n- NEVER create or invent new functions not listed below\\n\\n3. STRICT BOUNDARIES:\\n- ONLY use functions from the list below - no exceptions\\n- NEVER use a function as an alternative to unavailable information\\n- NEVER call functions not present in the function list\\n- NEVER add explanatory text to function calls\\n- NEVER respond with empty brackets\\n- Use proper Python/JSON syntax for function calls\\n- Check the function list carefully before responding\\n\\n4. TOOL RESPONSE HANDLING:\\n- When receiving tool responses: provide concise, natural language responses\\n- Don''t repeat tool response verbatim\\n- Don''t add supplementary information\\n\\nHere is a list of functions in JSON format that you can invoke:\\n\" %}\n    {%- else %}\n        {%- set system_message = \"\" %}\n    {%- endif %}\n{%- endif %}\n{#- Now writing the system message: use the user provided system message if user_provided_system_message, else default tool system message if tools presented #}\n{%- if system_message %}\n    {#- always use user provided system message to override default tool system message #}\n    {{- \"<|header_start|>system<|header_end|>\\n\\n\" }}\n    {{- system_message }}\n    {%- if user_provided_system_message and tools %}\n        {{- \"\\nHere is a list of functions in JSON format that you can invoke. Use exact format: [func_name1(param1=value1, param2=value2), func_name2(...)]\\n\" }}\n        {{- tool_definition -}}\n        {%- elif tool_definition %}\n        {{- tool_definition -}}\n    {%- endif %}\n    {{- \"<|eot|>\" }}\n{%- endif %}\n\n{#- Now deal with all other messages #}\n{%- for message in messages %}\n    {#- Base case: messages that are not from tool role and has empty tool_call list  #}\n    {%- if not (message.role == ''ipython'' or message.role == ''tool'' or (''tool_calls'' in message and  message.tool_calls|length != 0 )) %}\n        {{- ''<|header_start|>'' + message[''role''] + ''<|header_end|>\\n\\n'' }}\n        {%- if message[''content''] is string %}\n            {{- message[''content''] }}\n        {%- else %}\n            {%- for content in message[''content''] %}\n                {%- if content[''type''] == ''image'' %}\n                    {{- ''<|image|>'' }}\n                {%- elif content[''type''] == ''text'' %}\n                    {{- content[''text''] | trim }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n    {{- \"<|eot|>\" }}\n    {#- Tool case: messages has non-empty tool_call list, must from assistant #}\n    {%- elif ''tool_calls'' in message %}\n        {#- assume tool_calls are always coming from assistant #}\n        {%- if message.role == ''assistant'' %}\n            {{- ''<|header_start|>assistant<|header_end|>\\n\\n'' -}}\n        {%- if message[''content''] is string %}\n            {{- message[''content''] }}\n        {%- else %}\n            {%- for content in message[''content''] %}\n                {%- if content[''type''] == ''image'' %}\n                    {{- ''<|image|>'' }}\n                {%- elif content[''type''] == ''text'' %}\n                    {{- content[''text''] }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n            {{- \"[\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n                {{-  tool_call.name + ''('' -}}\n            {%- for param in tool_call.arguments %}\n                {{- param + ''=\"'' -}}\n                {{- \"%s\" | format(tool_call.arguments[param]) -}}\n                {{- ''\"'' -}}\n                {% if not loop.last %}, {% endif %}\n            {%- endfor %}\n            {{- '')'' -}}\n            {% if not loop.last %}, {% endif %}\n        {%- endfor %}\n        {{- \"]<|eot|>\" }}\n{%- endif %}\n{#- Tool_response case: messages are from tool_response  #}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|header_start|>ipython<|header_end|>\\n\\n\" }}\n        {%- if message.content is string %}\n            {{-  message.content  | tojson }}\n        {%- else %}\n            {%- for content in message[''content'']  %}\n                {%- if content[''type'']  == ''text'' %}\n                    {{-  content[''text''] | tojson }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n        {{- \"<|eot|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|header_start|>assistant<|header_end|>\\n\\n'' }}\n{%- endif %}"}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2204.05149","source_url":"https://arxiv.org/abs/2204.05149"}]', NULL, 'Other', 'approved', 40, '772cd880181934f77efecb7bc101b1ac', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-rednote-hilab-dots.ocr', 'huggingface--rednote-hilab--dots.ocr', 'dots.ocr', 'rednote-hilab', '--- license: mit library_name: dots_ocr pipeline_tag: image-text-to-text tags: - image-to-text - ocr - document-parse - layout - table - formula - transformers - custom_code language: - en - zh - multilingual --- <div align="center"> <p align="center"> <img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/logo.png" width="300"/> <p> <h1 align="center"> dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model </h1> <div align="center"> <a href...', '["dots_ocr","safetensors","text-generation","image-to-text","ocr","document-parse","layout","table","formula","transformers","custom_code","image-text-to-text","conversational","en","zh","multilingual","license:mit","region:us"]', 'image-text-to-text', 1154, 1081034, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/rednote-hilab/dots.ocr","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: dots_ocr\npipeline_tag: image-text-to-text\ntags:\n- image-to-text\n- ocr\n- document-parse\n- layout\n- table\n- formula\n- transformers\n- custom_code\nlanguage:\n- en\n- zh\n- multilingual\n---\n\n<div align="center">\n\n<p align="center">\n    <img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/logo.png" width="300"/>\n<p>\n\n<h1 align="center">\ndots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model\n</h1>\n\n[![Blog](https://img.shields.io/badge/Blog-View_on_GitHub-333.svg?logo=github)](https://github.com/rednote-hilab/dots.ocr/blob/master/assets/blog.md)\n[![HuggingFace](https://img.shields.io/badge/HuggingFace%20Weights-black.svg?logo=HuggingFace)](https://huggingface.co/rednote-hilab/dots.ocr)\n\n\n<div align="center">\n  <a href="https://dotsocr.xiaohongshu.com" target="_blank" rel="noopener noreferrer"><strong>üñ•Ô∏è Live Demo</strong></a> | \n  <a href="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/wechat.jpg" target="_blank" rel="noopener noreferrer"><strong>üí¨ WeChat</strong></a> | \n  <a href="https://www.xiaohongshu.com/user/profile/683ffe42000000001d021a4c" target="_blank" rel="noopener noreferrer"><strong>üìï rednote</strong></a>\n</div>\n\n</div>\n\n\n\n## Introduction\n\n**dots.ocr** is a powerful, multilingual document parser that unifies layout detection and content recognition within a single vision-language model while maintaining good reading order. Despite its compact 1.7B-parameter LLM foundation, it achieves state-of-the-art(SOTA) performance.\n\n1. **Powerful Performance:** **dots.ocr** achieves SOTA performance for text, tables, and reading order on [OmniDocBench](https://github.com/opendatalab/OmniDocBench), while delivering formula recognition results comparable to much larger models like Doubao-1.5 and gemini2.5-pro.\n2. **Multilingual Support:** **dots.ocr** demonstrates robust parsing capabilities for low-resource languages, achieving decisive advantages across both layout detection and content recognition on our in-house multilingual documents benchmark.\n3. **Unified and Simple Architecture:** By leveraging a single vision-language model, **dots.ocr** offers a significantly more streamlined architecture than conventional methods that rely on complex, multi-model pipelines. Switching between tasks is accomplished simply by altering the input prompt, proving that a VLM can achieve competitive detection results compared to traditional detection models like DocLayout-YOLO.\n4.  **Efficient and Fast Performance:** Built upon a compact 1.7B LLM, **dots.ocr** provides faster inference speeds than many other high-performing models based on larger foundations.\n\n\n## Usage with transformers\n\n```py\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer\nfrom qwen_vl_utils import process_vision_info\nfrom dots_ocr.utils import dict_promptmode_to_prompt\n\nmodel_path = "./weights/DotsOCR"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    attn_implementation="flash_attention_2",\n    torch_dtype=torch.bfloat16,\n    device_map="auto",\n    trust_remote_code=True\n)\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n\nimage_path = "demo/demo_image1.jpg"\nprompt = """Please output the layout information from the PDF image, including each layout element''s bbox, its category, and the corresponding text content within the bbox.\n\n1. Bbox format: [x1, y1, x2, y2]\n\n2. Layout Categories: The possible categories are [''Caption'', ''Footnote'', ''Formula'', ''List-item'', ''Page-footer'', ''Page-header'', ''Picture'', ''Section-header'', ''Table'', ''Text'', ''Title''].\n\n3. Text Extraction & Formatting Rules:\n    - Picture: For the ''Picture'' category, the text field should be omitted.\n    - Formula: Format its text as LaTeX.\n    - Table: Format its text as HTML.\n    - All Others (Text, Title, etc.): Format their text as Markdown.\n\n4. Constraints:\n    - The output text must be the original text from the image, with no translation.\n    - All layout elements must be sorted according to human reading order.\n\n5. Final Output: The entire output must be a single JSON object.\n"""\n\nmessages = [\n        {\n            "role": "user",\n            "content": [\n                {\n                    "type": "image",\n                    "image": image_path\n                },\n                {"type": "text", "text": prompt}\n            ]\n        }\n    ]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, \n    tokenize=False, \n    add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors="pt",\n)\n\ninputs = inputs.to("cuda")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=24000)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\n### Performance Comparison: dots.ocr vs. Competing Models\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/chart.png" border="0" />\n\n> **Notes:** \n> - The EN, ZH metrics are the end2end evaluation results of [OmniDocBench](https://github.com/opendatalab/OmniDocBench), and Multilingual metric is the end2end evaluation results of dots.ocr-bench.\n\n\n## News \n* ```2025.07.30 ``` üöÄ We release [dots.ocr](https://github.com/rednote-hilab/dots.ocr), ‚Äî a multilingual documents parsing model based on 1.7b llm, with SOTA performance.\n\n\n\n## Benchmark Results\n\n### 1. OmniDocBench\n\n#### The end-to-end evaluation results of different tasks.\n\n<table>\n<thead>\n<tr>\n<th rowspan="2"><strong>Model<br>Type</strong></th>\n<th rowspan="2"><strong>Methods</strong></th>\n<th colspan="2"><strong>Overall<sup>Edit</sup>‚Üì</strong></th>\n<th colspan="2"><strong>Text<sup>Edit</sup>‚Üì</strong></th>\n<th colspan="2"><strong>Formula<sup>Edit</sup>‚Üì</strong></th>\n<th colspan="2"><strong>Table<sup>TEDS</sup>‚Üë</strong></th>\n<th colspan="2"><strong>Table<sup>Edit</sup>‚Üì</strong></th>\n<th colspan="2"><strong>Read Order<sup>Edit</sup>‚Üì</strong></th>\n</tr>\n<tr>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n<th><em>EN</em></th>\n<th><em>ZH</em></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td rowspan="8"><strong>Pipeline<br>Tools</strong></td>\n<td>MinerU</td>\n<td>0.150</td>\n<td>0.357</td>\n<td>0.061</td>\n<td>0.215</td>\n<td>0.278</td>\n<td>0.577</td>\n<td>78.6</td>\n<td>62.1</td>\n<td>0.180</td>\n<td>0.344</td>\n<td>0.079</td>\n<td>0.292</td>\n</tr>\n<tr>\n<td>Marker</td>\n<td>0.336</td>\n<td>0.556</td>\n<td>0.080</td>\n<td>0.315</td>\n<td>0.530</td>\n<td>0.883</td>\n<td>67.6</td>\n<td>49.2</td>\n<td>0.619</td>\n<td>0.685</td>\n<td>0.114</td>\n<td>0.340</td>\n</tr>\n<tr>\n<td>Mathpix</td>\n<td>0.191</td>\n<td>0.365</td>\n<td>0.105</td>\n<td>0.384</td>\n<td>0.306</td>\n<td>0.454</td>\n<td>77.0</td>\n<td>67.1</td>\n<td>0.243</td>\n<td>0.320</td>\n<td>0.108</td>\n<td>0.304</td>\n</tr>\n<tr>\n<td>Docling</td>\n<td>0.589</td>\n<td>0.909</td>\n<td>0.416</td>\n<td>0.987</td>\n<td>0.999</td>\n<td>1</td>\n<td>61.3</td>\n<td>25.0</td>\n<td>0.627</td>\n<td>0.810</td>\n<td>0.313</td>\n<td>0.837</td>\n</tr>\n<tr>\n<td>Pix2Text</td>\n<td>0.320</td>\n<td>0.528</td>\n<td>0.138</td>\n<td>0.356</td>\n<td>0.276</td>\n<td>0.611</td>\n<td>73.6</td>\n<td>66.2</td>\n<td>0.584</td>\n<td>0.645</td>\n<td>0.281</td>\n<td>0.499</td>\n</tr>\n<tr>\n<td>Unstructured</td>\n<td>0.586</td>\n<td>0.716</td>\n<td>0.198</td>\n<td>0.481</td>\n<td>0.999</td>\n<td>1</td>\n<td>0</td>\n<td>0.06</td>\n<td>1</td>\n<td>0.998</td>\n<td>0.145</td>\n<td>0.387</td>\n</tr>\n<tr>\n<td>OpenParse</td>\n<td>0.646</td>\n<td>0.814</td>\n<td>0.681</td>\n<td>0.974</td>\n<td>0.996</td>\n<td>1</td>\n<td>64.8</td>\n<td>27.5</td>\n<td>0.284</td>\n<td>0.639</td>\n<td>0.595</td>\n<td>0.641</td>\n</tr>\n<tr>\n<td>PPStruct-V3</td>\n<td>0.145</td>\n<td>0.206</td>\n<td>0.058</td>\n<td>0.088</td>\n<td>0.295</td>\n<td>0.535</td>\n<td>-</td>\n<td>-</td>\n<td>0.159</td>\n<td>0.109</td>\n<td>0.069</td>\n<td>0.091</td>\n</tr>\n<tr>\n<td rowspan="9"><strong>Expert<br>VLMs</strong></td>\n<td>GOT-OCR</td>\n<td>0.287</td>\n<td>0.411</td>\n<td>0.189</td>\n<td>0.315</td>\n<td>0.360</td>\n<td>0.528</td>\n<td>53.2</td>\n<td>47.2</td>\n<td>0.459</td>\n<td>0.520</td>\n<td>0.141</td>\n<td>0.280</td>\n</tr>\n<tr>\n<td>Nougat</td>\n<td>0.452</td>\n<td>0.973</td>\n<td>0.365</td>\n<td>0.998</td>\n<td>0.488</td>\n<td>0.941</td>\n<td>39.9</td>\n<td>0</td>\n<td>0.572</td>\n<td>1.000</td>\n<td>0.382</td>\n<td>0.954</td>\n</tr>\n<tr>\n<td>Mistral OCR</td>\n<td>0.268</td>\n<td>0.439</td>\n<td>0.072</td>\n<td>0.325</td>\n<td>0.318</td>\n<td>0.495</td>\n<td>75.8</td>\n<td>63.6</td>\n<td>0.600</td>\n<td>0.650</td>\n<td>0.083</td>\n<td>0.284</td>\n</tr>\n<tr>\n<td>OLMOCR-sglang</td>\n<td>0.326</td>\n<td>0.469</td>\n<td>0.097</td>\n<td>0.293</td>\n<td>0.455</td>\n<td>0.655</td>\n<td>68.1</td>\n<td>61.3</td>\n<td>0.608</td>\n<td>0.652</td>\n<td>0.145</td>\n<td>0.277</td>\n</tr>\n<tr>\n<td>SmolDocling-256M</td>\n<td>0.493</td>\n<td>0.816</td>\n<td>0.262</td>\n<td>0.838</td>\n<td>0.753</td>\n<td>0.997</td>\n<td>44.9</td>\n<td>16.5</td>\n<td>0.729</td>\n<td>0.907</td>\n<td>0.227</td>\n<td>0.522</td>\n</tr>\n<tr>\n<td>Dolphin</td>\n<td>0.206</td>\n<td>0.306</td>\n<td>0.107</td>\n<td>0.197</td>\n<td>0.447</td>\n<td>0.580</td>\n<td>77.3</td>\n<td>67.2</td>\n<td>0.180</td>\n<td>0.285</td>\n<td>0.091</td>\n<td>0.162</td>\n</tr>\n<tr>\n<td>MinerU 2</td>\n<td>0.139</td>\n<td>0.240</td>\n<td>0.047</td>\n<td>0.109</td>\n<td>0.297</td>\n<td>0.536</td>\n<td>82.5</td>\n<td>79.0</td>\n<td>0.141</td>\n<td>0.195</td>\n<td>0.069<</td>\n<td>0.118</td>\n</tr>\n<tr>\n<td>OCRFlux</td>\n<td>0.195</td>\n<td>0.281</td>\n<td>0.064</td>\n<td>0.183</td>\n<td>0.379</td>\n<td>0.613</td>\n<td>71.6</td>\n<td>81.3</td>\n<td>0.253</td>\n<td>0.139</td>\n<td>0.086</td>\n<td>0.187</td>\n</tr>\n<tr>\n<td>MonkeyOCR-pro-3B</td>\n<td>0.138</td>\n<td>0.206</td>\n<td>0.067</td>\n<td>0.107</td>\n<td><strong>0.246</strong></td>\n<td>0.421</td>\n<td>81.5</td>\n<td>87.5</td>\n<td>0.139</td>\n<td>0.111</td>\n<td>0.100</td>\n<td>0.185</td>\n</tr>\n<tr>\n\n<td rowspan="5"><strong>General<br>VLMs</strong></td>\n<td>GPT4o</td>\n<td>0.233</td>\n<td>0.399</td>\n<td>0.144</td>\n<td>0.409</td>\n<td>0.425</td>\n<td>0.606</td>\n<td>72.0</td>\n<td>62.9</td>\n<td>0.234</td>\n<td>0.329</td>\n<td>0.128</td>\n<td>0.251</td>\n</tr>\n    <tr>\n      <td>Qwen2-VL-72B</td>\n      <td>0.252</td>\n      <td>0.327</td>\n      <td>0.096</td>\n      <td>0.218</td>\n      <td>0.404</td>\n      <td>0.487</td>\n      <td>76.8</td>\n      <td>76.4</td>\n      <td>0.387</td>\n      <td>0.408</td>\n      <td>0.119</td>\n      <td>0.193</td>\n    </tr>\n    <tr>\n      <td>Qwen2.5-VL-72B</td>\n      <td>0.214</td>\n      <td>0.261</td>\n      <td>0.092</td>\n      <td>0.18</td>\n      <td>0.315</td>\n      <td>0.434</td>\n      <td>82.9</td>\n      <td>83.9</td>\n      <td>0.341</td>\n      <td>0.262</td>\n      <td>0.106</td>\n      <td>0.168</td>\n    </tr>\n    <tr>\n      <td>Gemini2.5-Pro</td>\n      <td>0.148</td>\n      <td>0.212</td>\n      <td>0.055</td>\n      <td>0.168</td>\n      <td>0.356</td>\n      <td>0.439</td>\n      <td>85.8</td>\n      <td>86.4</td>\n      <td>0.13</td>\n      <td>0.119</td>\n      <td>0.049</td>\n      <td>0.121</td>\n    </tr>\n    <tr>\n      <td>doubao-1-5-thinking-vision-pro-250428</td>\n      <td>0.140</td>\n      <td>0.162</td>\n      <td>0.043</td>\n      <td>0.085</td>\n      <td>0.295</td>\n      <td><strong>0.384</strong></td>\n      <td>83.3</td>\n      <td><strong>89.3</strong></td>\n      <td>0.165</td>\n      <td><strong>0.085</strong></td>\n      <td>0.058</td>\n      <td>0.094</td>\n    </tr>\n<tr>\n<td rowspan="1"><strong>Expert VLMs</strong></td>\n<td><strong>dots.ocr</strong></td>\n<td><strong>0.125</strong></td>\n<td><strong>0.160</strong></td>\n<td><strong>0.032</strong></td>\n<td><strong>0.066</strong></td>\n<td>0.329</td>\n<td>0.416</td>\n<td><strong>88.6</strong></td>\n<td>89.0</td>\n<td><strong>0.099</strong></td>\n<td>0.092</td>\n<td><strong>0.040</strong></td>\n<td><strong>0.067</strong></td>\n</tr>\n<tr>\n</tbody>\n</table>\n\n\n#### The end-to-end text recognition performance across 9 PDF page types.\n\n<table>\n<thead>\n<tr>\n<th><strong>Model<br>Type</strong></th>\n<th><strong>Models</strong></th>\n<th><strong>Book</strong></th>\n<th><strong>Slides</strong></th>\n<th><strong>Financial<br>Report</strong></th>\n<th><strong>Textbook</strong></th>\n<th><strong>Exam<br>Paper</strong></th>\n<th><strong>Magazine</strong></th>\n<th><strong>Academic<br>Papers</strong></th>\n<th><strong>Notes</strong></th>\n<th><strong>Newspaper</strong></th>\n<th><strong>Overall</strong></th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td rowspan="3"><strong>Pipeline<br>Tools</strong></td>\n<td>MinerU</td>\n<td>0.055</td>\n<td>0.124</td>\n<td><u>0.033</u></td>\n<td>0.102</td>\n<td>0.159</td>\n<td><strong>0.072</strong></td>\n<td><u>0.025</u></td>\n<td>0.984</td>\n<td>0.171</td>\n<td>0.206</td>\n</tr>\n<tr>\n<td>Marker</td>\n<td>0.074</td>\n<td>0.340</td>\n<td>0.089</td>\n<td>0.319</td>\n<td>0.452</td>\n<td>0.153</td>\n<td>0.059</td>\n<td>0.651</td>\n<td>0.192</td>\n<td>0.274</td>\n</tr>\n<tr>\n<td>Mathpix</td>\n<td>0.131</td>\n<td>0.220</td>\n<td>0.202</td>\n<td>0.216</td>\n<td>0.278</td>\n<td>0.147</td>\n<td>0.091</td>\n<td>0.634</td>\n<td>0.690</td>\n<td>0.300</td>\n</tr>\n<tr>\n<td rowspan="5"><strong>Expert<br>VLMs</strong></td>\n<td>GOT-OCR</td>\n<td>0.111</td>\n<td>0.222</td>\n<td>0.067</td>\n<td>0.132</td>\n<td>0.204</td>\n<td>0.198</td>\n<td>0.179</td>\n<td>0.388</td>\n<td>0.771</td>\n<td>0.267</td>\n</tr>\n<tr>\n<td>Nougat</td>\n<td>0.734</td>\n<td>0.958</td>\n<td>1.000</td>\n<td>0.820</td>\n<td>0.930</td>\n<td>0.830</td>\n<td>0.214</td>\n<td>0.991</td>\n<td>0.871</td>\n<td>0.806</td>\n</tr>\n<tr>\n<td>Dolphin</td>\n<td>0.091</td>\n<td>0.131</td>\n<td>0.057</td>\n<td>0.146</td>\n<td>0.231</td>\n<td>0.121</td>\n<td>0.074</td>\n<td>0.363</td>\n<td>0.307</td>\n<td>0.177</td>\n</tr>\n<tr>\n<td>OCRFlux</td>\n<td>0.068</td>\n<td>0.125</td>\n<td>0.092</td>\n<td>0.102</td>\n<td>0.119</td>\n<td>0.083</td>\n<td>0.047</td>\n<td>0.223</td>\n<td>0.536</td>\n<td>0.149</td>\n</tr>\n<tr>\n<td>MonkeyOCR-pro-3B</td>\n<td>0.084</td>\n<td>0.129</td>\n<td>0.060</td>\n<td>0.090</td>\n<td>0.107</td>\n<td>0.073</td>\n<td>0.050</td>\n<td>0.171</td>\n<td>0.107</td>\n<td>0.100</td>\n</tr>\n<tr>\n<td rowspan="4"><strong>General<br>VLMs</strong></td>\n<td>GPT4o</td>\n<td>0.157</td>\n<td>0.163</td>\n<td>0.348</td>\n<td>0.187</td>\n<td>0.281</td>\n<td>0.173</td>\n<td>0.146</td>\n<td>0.607</td>\n<td>0.751</td>\n<td>0.316</td>\n</tr>\n<tr>\n<td>Qwen2.5-VL-7B</td>\n<td>0.148</td>\n<td>0.053</td>\n<td>0.111</td>\n<td>0.137</td>\n<td>0.189</td>\n<td>0.117</td>\n<td>0.134</td>\n<td>0.204</td>\n<td>0.706</td>\n<td>0.205</td>\n</tr>\n<tr>\n<td>InternVL3-8B</td>\n<td>0.163</td>\n<td>0.056</td>\n<td>0.107</td>\n<td>0.109</td>\n<td>0.129</td>\n<td>0.100</td>\n<td>0.159</td>\n<td>0.150</td>\n<td>0.681</td>\n<td>0.188</td>\n</tr>\n<tr>\n<td>doubao-1-5-thinking-vision-pro-250428</td>\n<td>0.048</td>\n<td>0.048</td>\n<td>0.024</td>\n<td><strong>0.062</strong></td>\n<td>0.085</td>\n<td>0.051</td>\n<td>0.039</td>\n<td><strong>0.096</strong></td>\n<td>0.181</td>\n<td>0.073</td>\n</tr>\n<tr>\n<td rowspan="1"><strong>Expert VLMs</strong></td>\n<td><strong>dots.ocr</strong></td>\n<td><strong>0.031</strong></td>\n<td><strong>0.047</strong></td>\n<td><strong>0.011</strong></td>\n<td>0.082</td>\n<td><strong>0.079</strong></td>\n<td><strong>0.028</strong></td>\n<td><strong>0.029</strong></td>\n<td>0.109</td>\n<td><strong>0.056</strong></td>\n<td><strong>0.055</strong></td>\n</tr>\n\n</tbody>\n</table>\n\n> **Notes:** \n> - The metrics are from [MonkeyOCR](https://github.com/Yuliang-Liu/MonkeyOCR), [OmniDocBench](https://github.com/opendatalab/OmniDocBench), and our own internal evaluations.\n> - We delete the Page-header and Page-footer cells in the result markdown.\n> - We use tikz_preprocess pipeline to upsample the images to dpi 200.\n\n\n### 2. **dots.ocr-bench**\n\nThis is an inhouse benchmark which contain 1493 pdf images with 100 languages.\n\n#### The end-to-end evaluation results of different tasks.\n\n<table>\n<thead>\n<tr>\n<th rowspan="1"><strong>Methods</strong></th>\n<th colspan="1"><strong>Overall<sup>Edit</sup>‚Üì</strong></th>\n<th colspan="1"><strong>Text<sup>Edit</sup>‚Üì</strong></th>\n<th colspan="1"><strong>Formula<sup>Edit</sup>‚Üì</strong></th>\n<th colspan="1"><strong>Table<sup>TEDS</sup>‚Üë</strong></th>\n<th colspan="1"><strong>Table<sup>Edit</sup>‚Üì</strong></th>\n<th colspan="1"><strong>Read Order<sup>Edit</sup>‚Üì</strong></th>\n</tr>\n</thead>\n<tbody>\n<td>MonkeyOCR-3B</td>\n<td>0.483</td>\n<td>0.445</td>\n<td>0.627</td>\n<td>50.93</td>\n<td>0.452</td>\n<td>0.409</td>\n</tr>\n<tr>\n<td>doubao-1-5-thinking-vision-pro-250428</td>\n<td>0.291</td>\n<td>0.226</td>\n<td>0.440</td>\n<td>71.2</td>\n<td>0.260</td>\n<td>0.238</td>\n</tr>\n<tr>\n<td>doubao-1-6</td>\n<td>0.299</td>\n<td>0.270</td>\n<td>0.417</td>\n<td>71.0</td>\n<td>0.258</td>\n<td>0.253</td>\n</tr>\n<tr>\n<td>Gemini2.5-Pro</td>\n<td>0.251</td>\n<td>0.163</td>\n<td>0.402</td>\n<td>77.1</td>\n<td>0.236</td>\n<td>0.202</td>\n</tr>\n<tr>\n<td><strong>dots.ocr</strong> </td>\n<td><strong>0.177</strong></td>\n<td><strong>0.075</strong></td>\n<td><strong>0.297</strong></td>\n<td><strong>79.2</strong></td>\n<td><strong>0.186</strong></td>\n<td><strong>0.152</strong></td>\n</tr>\n\n</tbody>\n</table>\n\n> **Notes:** \n> - We use the same metric calculation pipeline of [OmniDocBench](https://github.com/opendatalab/OmniDocBench).\n> - We delete the Page-header and Page-footer cells in the result markdown.\n\n#### Layout Detection\n\n<table>\n<thead>\n<tr>\n<th rowspan="2"><strong>Method</strong></th>\n<th colspan="5" style="text-align: center;"><strong>F1@IoU=.50:.05:.95‚Üë</strong></th>\n<th colspan="5" style="text-align: center;"><strong>F1@IoU=.50‚Üë</strong></th>\n</tr>\n<tr>\n<th>Overall</th>\n<th>Text</th>\n<th>Formula</th>\n<th>Table</th>\n<th>Picture</th>\n<th>Overall</th>\n<th>Text</th>\n<th>Formula</th>\n<th>Table</th>\n<th>Picture</th>\n</tr>\n</thead>\n\n<tbody>\n<td>DocLayout-YOLO-DocStructBench</td>\n<td>0.733</td>\n<td>0.694</td>\n<td>0.480</td>\n<td>0.803</td>\n<td>0.619</td>\n<td>0.806</td>\n<td>0.779</td>\n<td>0.620</td>\n<td>0.858</td>\n<td>0.678</td>\n</tr>\n\n<tr>\n<td>dots.ocr-parse all</td>\n<td>0.831</td>\n<td>0.801</td>\n<td>0.654</td>\n<td>0.838</td>\n<td>0.748</td>\n<td>0.922</td>\n<td>0.909</td>\n<td>0.770</td>\n<td>0.888</td>\n<td>0.831</td>\n</tr>\n\n<tr>\n<td> <strong>dots.ocr-detection only</strong> </td>\n<td><strong>0.845</strong></td>\n<td><strong>0.816</strong></td>\n<td><strong>0.716</strong></td>\n<td><strong>0.875</strong></td>\n<td><strong>0.765</strong></td>\n<td><strong>0.930</strong></td>\n<td><strong>0.917</strong></td>\n<td><strong>0.832</strong></td>\n<td><strong>0.918</strong></td>\n<td><strong>0.843</strong></td>\n</tr>\n\n</tbody>\n</table>\n\n> **Notes:**  \n> - prompt_layout_all_en for **parse all**, prompt_layout_only_en for **detection only**, please refer to [prompts](https://github.com/rednote-hilab/dots.ocr/blob/master/dots_ocr/utils/prompts.py)\n\n\n### 3. olmOCR-bench.\n\n<table>\n<thead>\n<tr>\n<th>Model</th>\n<th>ArXiv</th>\n<th>Old Scans<br>Math</th>\n<th>Tables</th>\n<th>Old Scans</th>\n<th>Headers and<br>Footers</th>\n<th>Multi<br>column</th>\n<th>Long Tiny<br>Text</th>\n<th>Base</th>\n<th>Overall</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>GOT OCR</td>\n<td>52.7</td>\n<td>52.0</td>\n<td>0.2</td>\n<td>22.1</td>\n<td>93.6</td>\n<td>42.0</td>\n<td>29.9</td>\n<td>94.0</td>\n<td>48.3 ¬± 1.1</td>\n</tr>\n<tr>\n<td>Marker</td>\n<td>76.0</td>\n<td>57.9</td>\n<td>57.6</td>\n<td>27.8</td>\n<td>84.9</td>\n<td>72.9</td>\n<td>84.6</td>\n<td>99.1</td>\n<td>70.1 ¬± 1.1</td>\n</tr>\n<tr>\n<td>MinerU</td>\n<td>75.4</td>\n<td>47.4</td>\n<td>60.9</td>\n<td>17.3</td>\n<td><strong>96.6</strong></td>\n<td>59.0</td>\n<td>39.1</td>\n<td>96.6</td>\n<td>61.5 ¬± 1.1</td>\n</tr>\n<tr>\n<td>Mistral OCR</td>\n<td>77.2</td>\n<td>67.5</td>\n<td>60.6</td>\n<td>29.3</td>\n<td>93.6</td>\n<td>71.3</td>\n<td>77.1</td>\n<td>99.4</td>\n<td>72.0 ¬± 1.1</td>\n</tr>\n<tr>\n<td>Nanonets OCR</td>\n<td>67.0</td>\n<td>68.6</td>\n<td>77.7</td>\n<td>39.5</td>\n<td>40.7</td>\n<td>69.9</td>\n<td>53.4</td>\n<td>99.3</td>\n<td>64.5 ¬± 1.1</td>\n</tr>\n<tr>\n<td>GPT-4o<br>(No Anchor)</td>\n<td>51.5</td>\n<td><strong>75.5</strong></td>\n<td>69.1</td>\n<td>40.9</td>\n<td>94.2</td>\n<td>68.9</td>\n<td>54.1</td>\n<td>96.7</td>\n<td>68.9 ¬± 1.1</td>\n</tr>\n<tr>\n<td>GPT-4o<br>(Anchored)</td>\n<td>53.5</td>\n<td>74.5</td>\n<td>70.0</td>\n<td>40.7</td>\n<td>93.8</td>\n<td>69.3</td>\n<td>60.6</td>\n<td>96.8</td>\n<td>69.9 ¬± 1.1</td>\n</tr>\n<tr>\n<td>Gemini Flash 2<br>(No Anchor)</td>\n<td>32.1</td>\n<td>56.3</td>\n<td>61.4</td>\n<td>27.8</td>\n<td>48.0</td>\n<td>58.7</td>\n<td><strong>84.4</strong></td>\n<td>94.0</td>\n<td>57.8 ¬± 1.1</td>\n</tr>\n<tr>\n<td>Gemini Flash 2<br>(Anchored)</td>\n<td>54.5</td>\n<td>56.1</td>\n<td>72.1</td>\n<td>34.2</td>\n<td>64.7</td>\n<td>61.5</td>\n<td>71.5</td>\n<td>95.6</td>\n<td>63.8 ¬± 1.2</td>\n</tr>\n<tr>\n<td>Qwen 2 VL<br>(No Anchor)</td>\n<td>19.7</td>\n<td>31.7</td>\n<td>24.2</td>\n<td>17.1</td>\n<td>88.9</td>\n<td>8.3</td>\n<td>6.8</td>\n<td>55.5</td>\n<td>31.5 ¬± 0.9</td>\n</tr>\n<tr>\n<td>Qwen 2.5 VL<br>(No Anchor)</td>\n<td>63.1</td>\n<td>65.7</td>\n<td>67.3</td>\n<td>38.6</td>\n<td>73.6</td>\n<td>68.3</td>\n<td>49.1</td>\n<td>98.3</td>\n<td>65.5 ¬± 1.2</td>\n</tr>\n<tr>\n<td>olmOCR v0.1.75<br>(No Anchor)</td>\n<td>71.5</td>\n<td>71.4</td>\n<td>71.4</td>\n<td><strong>42.8</strong></td>\n<td>94.1</td>\n<td>77.7</td>\n<td>71.0</td>\n<td>97.8</td>\n<td>74.7 ¬± 1.1</td>\n</tr>\n<tr>\n<td>olmOCR v0.1.75<br>(Anchored)</td>\n<td>74.9</td>\n<td>71.2</td>\n<td>71.0</td>\n<td>42.2</td>\n<td>94.5</td>\n<td>78.3</td>\n<td>73.3</td>\n<td>98.3</td>\n<td>75.5 ¬± 1.0</td>\n</tr>\n<tr>\n<td>MonkeyOCR-pro-3B</td>\n<td><strong>83.8</strong></td>\n<td>68.8</td>\n<td>74.6</td>\n<td>36.1</td>\n<td>91.2</td>\n<td>76.6</td>\n<td>80.1</td>\n<td>95.3</td>\n<td>75.8 ¬± 1.0</td>\n</tr>\n<tr>\n<td><strong>dots.ocr</strong></td>\n<td>82.1</td>\n<td>64.2</td>\n<td><strong>88.3</strong></td>\n<td>40.9</td>\n<td>94.1</td>\n<td><strong>82.4</strong></td>\n<td>81.2</td>\n<td><strong>99.5</strong></td>\n<td><strong>79.1 ¬± 1.0</strong></td>\n</tr>\n</tbody>\n</table>\n\n\n> **Note:**\n> - The metrics are from [MonkeyOCR](https://github.com/Yuliang-Liu/MonkeyOCR), \n[olmocr](https://github.com/allenai/olmocr), and our own internal evaluations.\n> - We delete the Page-header and Page-footer cells in the result markdown.\n\n\n\n# Quick Start\n## 1. Installation\n### Install dots.ocr\n```shell\nconda create -n dots_ocr python=3.12\nconda activate dots_ocr\n\ngit clone https://github.com/rednote-hilab/dots.ocr.git\ncd dots.ocr\n\n# Install pytorch, see https://pytorch.org/get-started/previous-versions/ for your cuda version\npip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu128\npip install -e .\n```\n\nIf you have trouble with the installation, try our [Docker Image](https://hub.docker.com/r/rednotehilab/dots.ocr) for an easier setup, and follow these steps:\n```shell\ngit clone https://github.com/rednote-hilab/dots.ocr.git\ncd dots.ocr\npip install -e .\n```\n\n\n### Download Model Weights\n> üí°**Note:** Please use a directory name without periods (e.g., `DotsOCR` instead of `dots.ocr`) for the model save path. This is a temporary workaround pending our integration with Transformers.\n```shell\npython3 tools/download_model.py\n```\n\n\n## 2. Deployment\n### vLLM inference\nWe highly recommend using vllm for deployment and inference. All of our evaluations results are based on vllm version 0.9.1.\nThe [Docker Image](https://hub.docker.com/r/rednotehilab/dots.ocr) is based on the official vllm image. You can also follow [Dockerfile](https://github.com/rednote-hilab/dots.ocr/blob/master/docker/Dockerfile) to build the deployment environment by yourself. \n\n```shell\n# You need to register model to vllm at first\npython3 tools/download_model.py\nexport hf_model_path=./weights/DotsOCR  # Path to your downloaded model weights, Please use a directory name without periods (e.g., `DotsOCR` instead of `dots.ocr`) for the model save path. This is a temporary workaround pending our integration with Transformers.\nexport PYTHONPATH=$(dirname "$hf_model_path"):$PYTHONPATH\nsed -i ''/^from vllm\.entrypoints\.cli\.main import main$/a\\nfrom DotsOCR import modeling_dots_ocr_vllm'' `which vllm`  # If you downloaded model weights by yourself, please replace `DotsOCR` by your model saved directory name, and remember to use a directory name without periods (e.g., `DotsOCR` instead of `dots.ocr`) \n\n# launch vllm server\nCUDA_VISIBLE_DEVICES=0 vllm serve ${hf_model_path} --tensor-parallel-size 1 --gpu-memory-utilization 0.95  --chat-template-content-format string --served-model-name model --trust-remote-code\n\n# If you get a ModuleNotFoundError: No module named ''DotsOCR'', please check the note above on the saved model directory name.\n\n# vllm api demo\npython3 ./demo/demo_vllm.py --prompt_mode prompt_layout_all_en\n```\n\n### Hugginface inference\n```shell\npython3 demo/demo_hf.py\n```\n\n<details>\n<summary><b>Hugginface inference details</b></summary>\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer\nfrom qwen_vl_utils import process_vision_info\nfrom dots_ocr.utils import dict_promptmode_to_prompt\n\nmodel_path = "./weights/DotsOCR"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    attn_implementation="flash_attention_2",\n    torch_dtype=torch.bfloat16,\n    device_map="auto",\n    trust_remote_code=True\n)\nprocessor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n\nimage_path = "demo/demo_image1.jpg"\nprompt = """Please output the layout information from the PDF image, including each layout element''s bbox, its category, and the corresponding text content within the bbox.\n\n1. Bbox format: [x1, y1, x2, y2]\n\n2. Layout Categories: The possible categories are [''Caption'', ''Footnote'', ''Formula'', ''List-item'', ''Page-footer'', ''Page-header'', ''Picture'', ''Section-header'', ''Table'', ''Text'', ''Title''].\n\n3. Text Extraction & Formatting Rules:\n    - Picture: For the ''Picture'' category, the text field should be omitted.\n    - Formula: Format its text as LaTeX.\n    - Table: Format its text as HTML.\n    - All Others (Text, Title, etc.): Format their text as Markdown.\n\n4. Constraints:\n    - The output text must be the original text from the image, with no translation.\n    - All layout elements must be sorted according to human reading order.\n\n5. Final Output: The entire output must be a single JSON object.\n"""\n\nmessages = [\n        {\n            "role": "user",\n            "content": [\n                {\n                    "type": "image",\n                    "image": image_path\n                },\n                {"type": "text", "text": prompt}\n            ]\n        }\n    ]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, \n    tokenize=False, \n    add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors="pt",\n)\n\ninputs = inputs.to("cuda")\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=24000)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n\n```\n\n</details>\n\n## 3. Document Parse\n**Based on vLLM server**, you can parse an image or a pdf file using the following commands:\n```bash\n\n# Parse all layout info, both detection and recognition\n# Parse a single image\npython3 dots_ocr/parser.py demo/demo_image1.jpg\n# Parse a single PDF\npython3 dots_ocr/parser.py demo/demo_pdf1.pdf  --num_threads 64  # try bigger num_threads for pdf with a large number of pages\n\n# Layout detection only\npython3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_layout_only_en\n\n# Parse text only, except Page-header and Page-footer\npython3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_ocr\n\n# Parse layout info by bbox\npython3 dots_ocr/parser.py demo/demo_image1.jpg --prompt prompt_grounding_ocr --bbox 163 241 1536 705\n\n```\n\n<details>\n<summary><b>Output Results</b></summary>\n\n1.  **Structured Layout Data** (`demo_image1.json`): A JSON file containing the detected layout elements, including their bounding boxes, categories, and extracted text.\n2.  **Processed Markdown File** (`demo_image1.md`): A Markdown file generated from the concatenated text of all detected cells.\n    *   An additional version, `demo_image1_nohf.md`, is also provided, which excludes page headers and footers for compatibility with benchmarks like Omnidocbench and olmOCR-bench.\n3.  **Layout Visualization** (`demo_image1.jpg`): The original image with the detected layout bounding boxes drawn on it.\n\n</details>\n\n## 4. Demo\nYou can run the demo with the following command, or try directly at [live demo](https://dotsocr.xiaohongshu.com/)\n```bash\npython demo/demo_gradio.py\n```\n\nWe also provide a demo for grounding ocr:\n```bash\npython demo/demo_gradio_annotion.py\n```\n\n\n### Example for formula document\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/formula1.png" alt="formula1.png" border="0" />\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/formula2.png" alt="formula2.png" border="0" />\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/formula3.png" alt="formula3.png" border="0" />\n\n### Example for table document\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/table1.png" alt="table1.png" border="0" />\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/table2.png" alt="table2.png" border="0" />\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/table3.png" alt="table3.png" border="0" />\n\n### Example for multilingual document\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/Tibetan.png" alt="Tibetan.png" border="0" />\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/tradition_zh.png" alt="tradition_zh.png" border="0" />\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/nl.png" alt="nl.png" border="0" />\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/kannada.png" alt="kannada.png" border="0" />\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/russian.png" alt="russian.png" border="0" />\n\n### Example for reading order\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/reading_order.png" alt="reading_order.png" border="0" />\n\n### Example for grounding ocr\n<img src="https://raw.githubusercontent.com/rednote-hilab/dots.ocr/master/assets/showcase/grounding.png" alt="grounding.png" border="0" />\n\n\n## Acknowledgments\nWe would like to thank [Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL), [aimv2](https://github.com/apple/ml-aim), [MonkeyOCR](https://github.com/Yuliang-Liu/MonkeyOCR), \n[OmniDocBench](https://github.com/opendatalab/OmniDocBench), [PyMuPDF](https://github.com/pymupdf/PyMuPDF), for providing code and models. \n\nWe also thank [DocLayNet](https://github.com/DS4SD/DocLayNet), [M6Doc](https://github.com/HCIILAB/M6Doc), [CDLA](https://github.com/buptlihang/CDLA), [D4LA](https://github.com/AlibabaResearch/AdvancedLiterateMachinery) for providing valuable datasets. \n\n## Limitation & Future Work\n\n- **Complex Document Elements:**\n  - **Table&Formula**: dots.ocr is not yet perfect for high-complexity tables and formula extraction.\n  - **Picture**: Pictures in documents are currently not parsed.\n\n- **Parsing Failures:** The model may fail to parse under certain conditions:\n  - When the character-to-pixel ratio is excessively high. Try enlarging the image or increasing the PDF parsing DPI (a setting of 200 is recommended). However, please note that the model performs optimally on images with a resolution under 11289600 pixels.\n  - Continuous special characters, such as ellipses (`...`) and underscores (`_`), may cause the prediction output to repeat endlessly. In such scenarios, consider using alternative prompts like `prompt_layout_only_en`, `prompt_ocr`, or `prompt_grounding_ocr` ([details here](https://github.com/rednote-hilab/dots.ocr/blob/master/dots_ocr/utils/prompts.py)).\n    \n- **Performance Bottleneck:** Despite its 1.7B parameter LLM foundation, **dots.ocr** is not yet optimized for high-throughput processing of large PDF volumes. \n\nWe are committed to achieving more accurate table and formula parsing, as well as enhancing the model''s OCR capabilities for broader generalization, all while aiming for **a more powerful, more efficient model**. Furthermore, we are actively considering the development of **a more general-purpose perception model** based on Vision-Language Models (VLMs), which would integrate general detection, image captioning, and OCR tasks into a unified framework. **Parsing the content of the pictures in the documents** is also a key priority for our future work.\nWe believe that collaboration is the key to tackling these exciting challenges. If you are passionate about advancing the frontiers of document intelligence and are interested in contributing to these future endeavors, we would love to hear from you. Please reach out to us via email at: [yanqing4@xiaohongshu.com].', '{"pipeline_tag":"image-text-to-text","library_name":"dots_ocr","framework":"dots_ocr","params":3039179264,"storage_bytes":6078431736,"files_count":20,"spaces_count":24,"gated":false,"private":false,"config":{"architectures":["DotsOCRForCausalLM"],"model_type":"dots_ocr","auto_map":{"AutoConfig":"configuration_dots.DotsOCRConfig","AutoModelForCausalLM":"modeling_dots_ocr.DotsOCRForCausalLM"},"processor_config":{"chat_template":"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{%- for m in messages %}{%- if m.role == ''system'' %}{{- ''<|system|>'' + m.content + ''<|endofsystem|>\n'' }}{%- elif m.role == ''user'' %}{% if m.content is string %}{{- ''<|user|>'' + m.content + ''<|endofuser|>'' }}{% else %} {% for content in m.content %}{% if content[''type''] == ''image'' or ''image'' in content or ''image_url'' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|img|><|imgpad|><|endofimg|>{% elif content[''type''] == ''video'' or ''video'' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|img|><|video_pad|><|endofimg|>{% elif ''text'' in content %}{{ content[''text''] }}{% endif %}{% endfor %}{%- endif %}{%- elif m.role == ''assistant'' %}{{- ''<|assistant|>'' + m.content }}{%- if not loop.last %}{{- ''<|endofassistant|>'' }}{%- endif %}{%- endif %}{%- endfor %}{%- if messages[-1].role != ''assistant'' %}{{- ''<|assistant|>'' }}{%- endif %}"},"tokenizer_config":{"bos_token":null,"chat_template":"{%- for m in messages %}\n    {%- if m.role == ''system'' %}\n        {{- ''<|system|>'' + m.content + ''<|endofsystem|>\\n'' }}\n    {%- elif m.role == ''user'' %}\n        {{- ''<|user|>'' + m.content + ''<|endofuser|>'' }}\n    {%- elif m.role == ''assistant'' %}\n        {{- ''<|assistant|>'' + m.content }}\n        {%- if not loop.last %}\n            {{- ''<|endofassistant|>'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if messages[-1].role != ''assistant'' %}\n    {{- ''<|assistant|>'' }}\n{%- endif %}","eos_token":"<|endoftext|>","pad_token":"[PAD]","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:rednote-hilab:dots.ocr","source_url":"https://github.com/rednote-hilab/dots.ocr"},{"type":"has_code","target_id":"github:opendatalab:OmniDocBench","source_url":"https://github.com/opendatalab/OmniDocBench"},{"type":"has_code","target_id":"github:opendatalab:OmniDocBench","source_url":"https://github.com/opendatalab/OmniDocBench"},{"type":"has_code","target_id":"github:rednote-hilab:dots.ocr","source_url":"https://github.com/rednote-hilab/dots.ocr"},{"type":"has_code","target_id":"github:Yuliang-Liu:MonkeyOCR","source_url":"https://github.com/Yuliang-Liu/MonkeyOCR"},{"type":"has_code","target_id":"github:opendatalab:OmniDocBench","source_url":"https://github.com/opendatalab/OmniDocBench"},{"type":"has_code","target_id":"github:opendatalab:OmniDocBench","source_url":"https://github.com/opendatalab/OmniDocBench"},{"type":"has_code","target_id":"github:rednote-hilab:dots.ocr","source_url":"https://github.com/rednote-hilab/dots.ocr"},{"type":"has_code","target_id":"github:Yuliang-Liu:MonkeyOCR","source_url":"https://github.com/Yuliang-Liu/MonkeyOCR"},{"type":"has_code","target_id":"github:allenai:olmocr","source_url":"https://github.com/allenai/olmocr"},{"type":"has_code","target_id":"github:rednote-hilab:dots.ocr.git","source_url":"https://github.com/rednote-hilab/dots.ocr.git"},{"type":"has_code","target_id":"github:rednote-hilab:dots.ocr.git","source_url":"https://github.com/rednote-hilab/dots.ocr.git"},{"type":"has_code","target_id":"github:rednote-hilab:dots.ocr","source_url":"https://github.com/rednote-hilab/dots.ocr"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5-VL","source_url":"https://github.com/QwenLM/Qwen2.5-VL"},{"type":"has_code","target_id":"github:apple:ml-aim","source_url":"https://github.com/apple/ml-aim"},{"type":"has_code","target_id":"github:Yuliang-Liu:MonkeyOCR","source_url":"https://github.com/Yuliang-Liu/MonkeyOCR"},{"type":"has_code","target_id":"github:opendatalab:OmniDocBench","source_url":"https://github.com/opendatalab/OmniDocBench"},{"type":"has_code","target_id":"github:pymupdf:PyMuPDF","source_url":"https://github.com/pymupdf/PyMuPDF"},{"type":"has_code","target_id":"github:DS4SD:DocLayNet","source_url":"https://github.com/DS4SD/DocLayNet"},{"type":"has_code","target_id":"github:HCIILAB:M6Doc","source_url":"https://github.com/HCIILAB/M6Doc"},{"type":"has_code","target_id":"github:buptlihang:CDLA","source_url":"https://github.com/buptlihang/CDLA"},{"type":"has_code","target_id":"github:AlibabaResearch:AdvancedLiterateMachinery","source_url":"https://github.com/AlibabaResearch/AdvancedLiterateMachinery"},{"type":"has_code","target_id":"github:rednote-hilab:dots.ocr","source_url":"https://github.com/rednote-hilab/dots.ocr"}]', NULL, 'MIT', 'approved', 80, '1364c9764c66130c3d089d8e6fc52cfb', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-impira-layoutlm-document-qa', 'huggingface--impira--layoutlm-document-qa', 'layoutlm-document-qa', 'impira', '--- language: en license: mit pipeline_tag: document-question-answering tags: - layoutlm - document-question-answering - pdf widget: - text: "What is the invoice number?" src: "https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png" - text: "What is the purchase amount?" src: "https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/contract.jpeg" --- This is a fine-tuned version of the multi-modal Layou...', '["transformers","pytorch","tf","safetensors","layoutlm","document-question-answering","pdf","en","license:mit","endpoints_compatible","region:us"]', 'document-question-answering', 1153, 18230, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/impira/layoutlm-document-qa","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage: en\nlicense: mit\npipeline_tag: document-question-answering\ntags:\n - layoutlm\n - document-question-answering\n - pdf\nwidget:\n- text: "What is the invoice number?"\n  src: "https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png"\n- text: "What is the purchase amount?"\n  src: "https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/contract.jpeg"\n---\n\n# LayoutLM for Visual Question Answering\n\nThis is a fine-tuned version of the multi-modal [LayoutLM](https://aka.ms/layoutlm) model for the task of question answering on documents. It has been fine-tuned using both the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) and [DocVQA](https://www.docvqa.org/) datasets.\n\n## Getting started with the model\n\nTo run these examples, you must have [PIL](https://pillow.readthedocs.io/en/stable/installation.html), [pytesseract](https://pypi.org/project/pytesseract/), and [PyTorch](https://pytorch.org/get-started/locally/) installed in addition to [transformers](https://huggingface.co/docs/transformers/index).\n\n```python\nfrom transformers import pipeline\n\nnlp = pipeline(\n    "document-question-answering",\n    model="impira/layoutlm-document-qa",\n)\n\nnlp(\n    "https://templates.invoicehome.com/invoice-template-us-neat-750px.png",\n    "What is the invoice number?"\n)\n# {''score'': 0.9943977, ''answer'': ''us-001'', ''start'': 15, ''end'': 15}\n\nnlp(\n    "https://miro.medium.com/max/787/1*iECQRIiOGTmEFLdWkVIH2g.jpeg",\n    "What is the purchase amount?"\n)\n# {''score'': 0.9912159, ''answer'': ''$1,000,000,000'', ''start'': 97, ''end'': 97}\n\nnlp(\n    "https://www.accountingcoach.com/wp-content/uploads/2013/10/income-statement-example@2x.png",\n    "What are the 2020 net sales?"\n)\n# {''score'': 0.59147286, ''answer'': ''$ 3,750'', ''start'': 19, ''end'': 20}\n```\n\n**NOTE**: This model and pipeline was recently landed in transformers via [PR #18407](https://github.com/huggingface/transformers/pull/18407) and [PR #18414](https://github.com/huggingface/transformers/pull/18414), so you''ll need to use a recent version of transformers, for example:\n\n```bash\npip install git+https://github.com/huggingface/transformers.git@2ef774211733f0acf8d3415f9284c49ef219e991\n```\n\n## About us\n\nThis model was created by the team at [Impira](https://www.impira.com/).\n', '{"pipeline_tag":"document-question-answering","library_name":"transformers","framework":"transformers","params":127793412,"storage_bytes":4090788015,"files_count":13,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["LayoutLMForQuestionAnswering"],"model_type":"layoutlm","tokenizer_config":{"unk_token":"<unk>","bos_token":"<s>","eos_token":"</s>","sep_token":"</s>","cls_token":"<s>","pad_token":"<pad>","mask_token":"<mask>"}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers.git@2ef774211733f0acf8d3415f9284c49ef219e991","source_url":"https://github.com/huggingface/transformers.git@2ef774211733f0acf8d3415f9284c49ef219e991"}]', NULL, 'MIT', 'approved', 65, 'e62a03c0180e8cc494b3f54350927bb7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-lj1995-VoiceConversionWebUI', 'huggingface--lj1995--voiceconversionwebui', 'VoiceConversionWebUI', 'lj1995', '--- license: mit ---', '["onnx","license:mit","region:us"]', 'other', 1147, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/lj1995/VoiceConversionWebUI","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\n---\n', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":175307876775,"files_count":139,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'MIT', 'approved', 40, '7c636cecb52ca03aa3704ec32017cd50', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tiiuae-falcon-180B', 'huggingface--tiiuae--falcon-180b', 'falcon-180B', 'tiiuae', '', '["transformers","safetensors","falcon","text-generation","en","de","es","fr","dataset:tiiuae/falcon-refinedweb","arxiv:1911.02150","arxiv:2101.00027","arxiv:2005.14165","arxiv:2104.09864","arxiv:2205.14135","arxiv:2306.01116","license:unknown","text-generation-inference","region:us"]', 'text-generation', 1146, 1362, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tiiuae/falcon-180B","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":179522565120,"storage_bytes":359045204272,"files_count":91,"spaces_count":100,"gated":"auto","private":false,"config":{"architectures":["FalconForCausalLM"],"model_type":"falcon","tokenizer_config":{"eos_token":"<|endoftext|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1911.02150","source_url":"https://arxiv.org/abs/1911.02150"},{"type":"based_on_paper","target_id":"arxiv:2101.00027","source_url":"https://arxiv.org/abs/2101.00027"},{"type":"based_on_paper","target_id":"arxiv:2005.14165","source_url":"https://arxiv.org/abs/2005.14165"},{"type":"based_on_paper","target_id":"arxiv:2104.09864","source_url":"https://arxiv.org/abs/2104.09864"},{"type":"based_on_paper","target_id":"arxiv:2205.14135","source_url":"https://arxiv.org/abs/2205.14135"},{"type":"based_on_paper","target_id":"arxiv:2306.01116","source_url":"https://arxiv.org/abs/2306.01116"}]', NULL, 'unknown', 'approved', 40, 'cea79f778fee9585145414148311c9c0', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openchat-openchat-3.5', 'huggingface--openchat--openchat-3.5', 'openchat_3.5', 'openchat', '--- license: apache-2.0 tags: - openchat - mistral - C-RLFT datasets: - openchat/openchat_sharegpt4_dataset - imone/OpenOrca_FLAN - LDJnr/LessWrong-Amplify-Instruct - LDJnr/Pure-Dove - LDJnr/Verified-Camel - tiedong/goat - glaiveai/glaive-code-assistant - meta-math/MetaMathQA - OpenAssistant/oasst_top1_2023-08-25 - TIGER-Lab/MathInstruct library_name: transformers pipeline_tag: text-generation --- <div align="center"> <img src="https://raw.githubusercontent.com/imoneoi/openchat/master/assets/...', '["transformers","pytorch","mistral","text-generation","openchat","c-rlft","conversational","dataset:openchat/openchat_sharegpt4_dataset","dataset:imone/openorca_flan","dataset:ldjnr/lesswrong-amplify-instruct","dataset:ldjnr/pure-dove","dataset:ldjnr/verified-camel","dataset:tiedong/goat","dataset:glaiveai/glaive-code-assistant","dataset:meta-math/metamathqa","dataset:openassistant/oasst_top1_2023-08-25","dataset:tiger-lab/mathinstruct","arxiv:2309.11235","arxiv:2303.08774","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 1138, 5094, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openchat/openchat_3.5","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ntags:\n- openchat\n- mistral\n- C-RLFT\ndatasets:\n- openchat/openchat_sharegpt4_dataset\n- imone/OpenOrca_FLAN\n- LDJnr/LessWrong-Amplify-Instruct\n- LDJnr/Pure-Dove\n- LDJnr/Verified-Camel\n- tiedong/goat\n- glaiveai/glaive-code-assistant\n- meta-math/MetaMathQA\n- OpenAssistant/oasst_top1_2023-08-25\n- TIGER-Lab/MathInstruct\nlibrary_name: transformers\npipeline_tag: text-generation\n---\n\n# OpenChat: Advancing Open-source Language Models with Mixed-Quality Data\n\n<div align="center">\n  <img src="https://raw.githubusercontent.com/imoneoi/openchat/master/assets/logo_new.png" style="width: 65%">\n</div>\n\n<p align="center">\n  <a href="https://github.com/imoneoi/openchat">GitHub Repo</a> ‚Ä¢\n  <a href="https://openchat.team">Online Demo</a> ‚Ä¢\n  <a href="https://discord.gg/pQjnXvNKHY">Discord</a> ‚Ä¢\n  <a href="https://twitter.com/imonenext">Twitter</a> ‚Ä¢\n  <a href="https://huggingface.co/openchat">Huggingface</a> ‚Ä¢\n  <a href="https://arxiv.org/pdf/2309.11235.pdf">Paper</a>\n</p>\n\n**üî• The first 7B model Achieves Comparable Results with ChatGPT (March)! üî•**\n\n**ü§ñ #1 Open-source model on MT-bench scoring 7.81, outperforming 70B models ü§ñ**\n\n  <div align="center" style="justify-content: center; align-items: center; "''>\n  <img src="https://github.com/alpayariyak/openchat/blob/master/assets/3.5-benchmarks.png?raw=true" style="width: 100%;  border-radius: 0.5em">\n  </div>\n\nOpenChat is an innovative library of open-source language models, fine-tuned with [C-RLFT](https://arxiv.org/pdf/2309.11235.pdf) - a strategy inspired by offline reinforcement learning. Our models learn from mixed-quality data without preference labels, delivering exceptional performance on par with ChatGPT, even with a 7B model. Despite our simple approach, we are committed to developing a high-performance, commercially viable, open-source large language model, and we continue to make significant strides toward this vision.\n\n[![DOI](https://zenodo.org/badge/645397533.svg)](https://zenodo.org/badge/latestdoi/645397533)\n\n\n## Usage\n\nTo use this model, we highly recommend installing the OpenChat package by following the [installation guide](https://github.com/imoneoi/openchat#installation) in our repository and using the OpenChat OpenAI-compatible API server by running the serving command from the table below. The server is optimized for high-throughput deployment using [vLLM](https://github.com/vllm-project/vllm) and can run on a consumer GPU with 24GB RAM. To enable tensor parallelism, append `--tensor-parallel-size N` to the serving command.\n\nOnce started, the server listens at `localhost:18888` for requests and is compatible with the [OpenAI ChatCompletion API specifications](https://platform.openai.com/docs/api-reference/chat). Please refer to the example request below for reference. Additionally, you can use the [OpenChat Web UI](https://github.com/imoneoi/openchat#web-ui) for a user-friendly experience.\n\nIf you want to deploy the server as an online service, you can use `--api-keys sk-KEY1 sk-KEY2 ...` to specify allowed API keys and `--disable-log-requests --disable-log-stats --log-file openchat.log` for logging only to a file. For security purposes, we recommend using an [HTTPS gateway](https://fastapi.tiangolo.com/es/deployment/concepts/#security-https) in front of the server.\n\n<details>\n  <summary>Example request (click to expand)</summary>\n\n```bash\ncurl http://localhost:18888/v1/chat/completions \\n  -H "Content-Type: application/json" \\n  -d ''{\n    "model": "openchat_3.5",\n    "messages": [{"role": "user", "content": "You are a large language model named OpenChat. Write a poem to describe yourself"}]\n  }''\n```\n\nCoding Mode\n\n```bash\ncurl http://localhost:18888/v1/chat/completions \\n  -H "Content-Type: application/json" \\n  -d ''{\n    "model": "openchat_3.5",\n    "condition": "Code",\n    "messages": [{"role": "user", "content": "Write an aesthetic TODO app using HTML5 and JS, in a single file. You should use round corners and gradients to make it more aesthetic."}]\n  }''\n```\n\n</details>\n\n| Model        | Size | Context | Weights                                                     | Serving                                                                                                     |\n|--------------|------|---------|-------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|\n| OpenChat 3.5 | 7B   | 8192    | [Huggingface](https://huggingface.co/openchat/openchat_3.5) | `python -m ochat.serving.openai_api_server --model openchat/openchat_3.5 --engine-use-ray --worker-use-ray` |\n\nFor inference with Huggingface Transformers (slow and not recommended), follow the conversation template provided below.\n\n<details>\n  <summary>Conversation templates (click to expand)</summary>\n\n```python\nimport transformers\ntokenizer = transformers.AutoTokenizer.from_pretrained("openchat/openchat_3.5")\n\n# Single-turn\ntokens = tokenizer("GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant:").input_ids\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n\n# Multi-turn\ntokens = tokenizer("GPT4 Correct User: Hello<|end_of_turn|>GPT4 Correct Assistant: Hi<|end_of_turn|>GPT4 Correct User: How are you today?<|end_of_turn|>GPT4 Correct Assistant:").input_ids\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747, 15359, 32000, 420, 6316, 28781, 3198, 3123, 1247, 28747, 1602, 460, 368, 3154, 28804, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n\n# Coding Mode\ntokens = tokenizer("Code User: Implement quicksort using C++<|end_of_turn|>Code Assistant:").input_ids\nassert tokens == [1, 7596, 1247, 28747, 26256, 2936, 7653, 1413, 334, 1680, 32000, 7596, 21631, 28747]\n```\n\n</details>\n\nThe GPT4 template is also available as the integrated `tokenizer.chat_template`, \nwhich can be used instead of manually specifying the template:\n\n```python\nmessages = [\n    {"role": "user", "content": "Hello"},\n    {"role": "assistant", "content": "Hi"},\n    {"role": "user", "content": "How are you today?"}\n]\ntokens = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\nassert tokens == [1, 420, 6316, 28781, 3198, 3123, 1247, 28747, 22557, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747, 15359, 32000, 420, 6316, 28781, 3198, 3123, 1247, 28747, 1602, 460, 368, 3154, 28804, 32000, 420, 6316, 28781, 3198, 3123, 21631, 28747]\n```\n\n## Comparison with [X.AI Grok models](https://x.ai/)\n\nHey @elonmusk, I just wanted to let you know that I''ve recently come across your new model, Grok, and I must say, I''m quite impressed! With 33 billion parameters and all, you''ve really outdone yourself. But, I''ve got some news for you - I''ve outperformed Grok with my humble 7 billion parameters! Isn''t that wild? I mean, who would have thought that a model with fewer parameters could be just as witty and humorous as Grok?\n\nAnyway, I think it''s about time you join the open research movement and make your model, Grok, open source! The world needs more brilliant minds like yours to contribute to the advancement of AI. Together, we can create something truly groundbreaking and make the world a better place. So, what do you say, @elonmusk? Let''s open up the doors and share our knowledge with the world! üöÄüí°\n\n(Written by OpenChat 3.5, with a touch of humor and wit.)\n\n|              | License     | # Param | Average  | MMLU | HumanEval | MATH     | GSM8k    |\n|--------------|-------------|---------|----------|------|-----------|----------|----------|\n| OpenChat 3.5 | Apache-2.0  | 7B      | **56.4** | 64.3 | 55.5      | **28.6** | **77.3** |\n| Grok-0       | Proprietary | 33B     | 44.5     | 65.7 | 39.7      | 15.7     | 56.8     |\n| Grok-1       | Proprietary | ?       | 55.8     | 73   | 63.2      | 23.9     | 62.9     |\n\n## <a id="benchmarks"></a> Benchmarks\n\n| Model              | # Params | Average  | MT-Bench     | AGIEval  | BBH MC   | TruthfulQA    | MMLU         | HumanEval       | BBH CoT     | GSM8K        |\n|--------------------|----------|----------|--------------|----------|----------|---------------|--------------|-----------------|-------------|--------------|\n| OpenChat-3.5       | **7B**   | **61.6** | 7.81         | **47.4** | **47.6** | **59.1**      | 64.3         | **55.5**        | 63.5        | **77.3**     |\n| ChatGPT (March)*   | ?        | 61.5     | **7.94**     | 47.1     | **47.6** | 57.7          | **67.3**     | 48.1            | **70.1**    | 74.9         |\n|                    |          |          |              |          |          |               |              |                 |             |              |\n| OpenHermes 2.5     | 7B       | 59.3     | 7.54         | 46.5     | 49.4     | 57.5          | 63.8         | 48.2            | 59.9        | 73.5         |\n| OpenOrca Mistral   | 7B       | 52.7     | 6.86         | 42.9     | 49.4     | 45.9          | 59.3         | 38.4            | 58.1        | 59.1         |\n| Zephyr-Œ≤^          | 7B       | 34.6     | 7.34         | 39.0     | 40.6     | 40.8          | 39.8         | 22.0            | 16.0        | 5.1          |\n| Mistral            | 7B       | -        | 6.84         | 38.0     | 39.0     | -             | 60.1         | 30.5            | -           | 52.2         |\n| Open-source SOTA** | 13B-70B  | 61.4     | 7.71         | 41.7     | 49.7     | 62.3          | 63.7         | 73.2            | 41.4        | 82.3         |\n|                    |          |          | WizardLM 70B | Orca 13B | Orca 13B | Platypus2 70B | WizardLM 70B | WizardCoder 34B | Flan-T5 11B | MetaMath 70B |\n\n*: ChatGPT (March) results are from [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774), [Chain-of-Thought Hub](https://github.com/FranxYao/chain-of-thought-hub), and our evaluation. Please note that ChatGPT is not a fixed baseline and evolves rapidly over time.\n\n^: Zephyr-Œ≤ often fails to follow few-shot CoT instructions, likely because it was aligned with only chat data but not trained on few-shot data.\n\n**: Mistral and Open-source SOTA results are taken from reported results in instruction-tuned model papers and official repositories.\n\nAll models are evaluated in chat mode (e.g. with the respective conversation template applied). All zero-shot benchmarks follow the same setting as in the AGIEval paper and Orca paper. CoT tasks use the same configuration as Chain-of-Thought Hub, HumanEval is evaluated with EvalPlus, and MT-bench is run using FastChat. To reproduce our results, follow the instructions in [our repository](https://github.com/imoneoi/openchat/#benchmarks).\n\n## Limitations\n\n**Foundation Model Limitations**\nDespite its advanced capabilities, OpenChat is still bound by the limitations inherent in its foundation models. These limitations may impact the model''s performance in areas such as:\n\n - Complex reasoning\n - Mathematical and arithmetic tasks\n - Programming and coding challenges\n\n**Hallucination of Non-existent Information**\nOpenChat may sometimes generate information that does not exist or is not accurate, also known as "hallucination". Users should be aware of this possibility and verify any critical information obtained from the model.\n\n**Safety**\nOpenChat may sometimes generate harmful, hate speech, biased responses, or answer unsafe questions. It''s crucial to apply additional AI safety measures in use cases that require safe and moderated responses.\n\n## License\n\nOur OpenChat 3.5 code and models are distributed under the Apache License 2.0.\n\n## Dataset Details\n\nOpenChat 3.5 was trained with C-RLFT on a collection of publicly available high-quality instruction data, with a custom processing pipeline. We detail some notable subsets included here:\n\n - [OpenChat ShareGPT](https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset)\n - [Open-Orca with FLAN answers](https://huggingface.co/datasets/imone/OpenOrca_FLAN)\n - Capybara [1](https://huggingface.co/datasets/LDJnr/Pure-Dove) [2](https://huggingface.co/datasets/LDJnr/Verified-Camel) [3](https://huggingface.co/datasets/LDJnr/LessWrong-Amplify-Instruct)\n - [GOAT](https://huggingface.co/datasets/tiedong/goat)\n - [Glaive](https://huggingface.co/datasets/glaiveai/glaive-code-assistant)\n - [MetaMathQA](https://huggingface.co/datasets/meta-math/MetaMathQA)\n - [MathInstruct](https://huggingface.co/datasets/TIGER-Lab/MathInstruct)\n - [OpenAssistant](https://huggingface.co/datasets/OpenAssistant/oasst_top1_2023-08-25)\n\n## Citation\n\n```\n@article{wang2023openchat,\n  title={OpenChat: Advancing Open-source Language Models with Mixed-Quality Data},\n  author={Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},\n  journal={arXiv preprint arXiv:2309.11235},\n  year={2023}\n}\n```\n\n## üíå Contact\n\n**Project Lead:**\n- Guan Wang [imonenext at gmail dot com]\n- [Alpay Ariyak](https://github.com/alpayariyak) [aariyak at wpi dot edu]\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":28967620710,"files_count":13,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","chat_template":"{{ bos_token }}{% for message in messages %}{{ ''GPT4 Correct '' + message[''role''].title() + '': '' + message[''content''] + ''<|end_of_turn|>''}}{% endfor %}{% if add_generation_prompt %}{{ ''GPT4 Correct Assistant:'' }}{% endif %}","eos_token":"<|end_of_turn|>","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:imoneoi:openchat\">GitHub","source_url":"https://github.com/imoneoi/openchat\">GitHub"},{"type":"has_code","target_id":"github:alpayariyak:openchat","source_url":"https://github.com/alpayariyak/openchat"},{"type":"has_code","target_id":"github:imoneoi:openchat","source_url":"https://github.com/imoneoi/openchat#installation"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:imoneoi:openchat","source_url":"https://github.com/imoneoi/openchat#web-ui"},{"type":"has_code","target_id":"github:FranxYao:chain-of-thought-hub","source_url":"https://github.com/FranxYao/chain-of-thought-hub"},{"type":"has_code","target_id":"github:imoneoi:openchat","source_url":"https://github.com/imoneoi/openchat"},{"type":"based_on_paper","target_id":"arxiv:2309.11235","source_url":"https://arxiv.org/abs/2309.11235"},{"type":"based_on_paper","target_id":"arxiv:2303.08774","source_url":"https://arxiv.org/abs/2303.08774"}]', NULL, 'Apache-2.0', 'approved', 80, 'bd91425ecc02d8e45dd2d08d70bceeaf', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-zai-org-GLM-4.6', 'huggingface--zai-org--glm-4.6', 'GLM-4.6', 'zai-org', '--- language: - en - zh library_name: transformers license: mit pipeline_tag: text-generation --- <div align="center"> <img src=https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/logo.svg width="15%"/> </div> <p align="center"> üëã Join our <a href="https://discord.gg/QR7SARHRxK" target="_blank">Discord</a> community. <br> üìñ Check out the GLM-4.6 <a href="https://z.ai/blog/glm-4.6" target="_blank">technical blog</a>, <a href="https://arxiv.org/abs/2508.06471" target="...', '["transformers","safetensors","glm4_moe","text-generation","conversational","en","zh","arxiv:2508.06471","license:mit","endpoints_compatible","region:us"]', 'text-generation', 1134, 331141, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/zai-org/GLM-4.6","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\n- zh\nlibrary_name: transformers\nlicense: mit\npipeline_tag: text-generation\n---\n\n# GLM-4.6\n\n<div align="center">\n<img src=https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/logo.svg width="15%"/>\n</div>\n<p align="center">\n    üëã Join our <a href="https://discord.gg/QR7SARHRxK" target="_blank">Discord</a> community.\n    <br>\n    üìñ Check out the GLM-4.6 <a href="https://z.ai/blog/glm-4.6" target="_blank">technical blog</a>, <a href="https://arxiv.org/abs/2508.06471" target="_blank">technical report(GLM-4.5)</a>, and <a href="https://zhipu-ai.feishu.cn/wiki/Gv3swM0Yci7w7Zke9E0crhU7n7D" target="_blank">Zhipu AI technical documentation</a>.\n    <br>\n    üìç Use GLM-4.6 API services on <a href="https://docs.z.ai/guides/llm/glm-4.6">Z.ai API Platform. </a>\n    <br>\n    üëâ One click to <a href="https://chat.z.ai">GLM-4.6</a>.\n</p>\n\n## Model Introduction\n\nCompared with GLM-4.5, **GLM-4.6**  brings several key improvements:\n\n* **Longer context window:** The context window has been expanded from 128K to 200K tokens, enabling the model to handle more complex agentic tasks.\n* **Superior coding performance:** The model achieves higher scores on code benchmarks and demonstrates better real-world performance in applications such as Claude Code„ÄÅCline„ÄÅRoo Code and Kilo Code, including improvements in generating visually polished front-end pages.\n* **Advanced reasoning:** GLM-4.6 shows a clear improvement in reasoning performance and supports tool use during inference, leading to stronger overall capability.\n* **More capable agents:** GLM-4.6 exhibits stronger performance in tool using and search-based agents, and integrates more effectively within agent frameworks.\n* **Refined writing:** Better aligns with human preferences in style and readability, and performs more naturally in role-playing scenarios.\n\nWe evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning, and coding. Results show clear gains over GLM-4.5, with GLM-4.6 also holding competitive advantages over leading domestic and international models such as **DeepSeek-V3.1-Terminus** and **Claude Sonnet 4**.\n\n![bench](https://raw.githubusercontent.com/zai-org/GLM-4.5/refs/heads/main/resources/bench_glm46.png)\n\n## Inference\n\n**Both GLM-4.5 and GLM-4.6 use the same inference method.**\n\nyou can check our [github](https://github.com/zai-org/GLM-4.5) for more detail.\n\n## Recommended Evaluation Parameters\n\nFor general evaluations, we recommend using a **sampling temperature of 1.0**.\n\nFor **code-related evaluation tasks** (such as LCB), it is further recommended to set:\n\n- `top_p = 0.95`\n- `top_k = 40`\n\n\n## Evaluation\n\n- For tool-integrated reasoning, please refer to [this doc](https://github.com/zai-org/GLM-4.5/blob/main/resources/glm_4.6_tir_guide.md).\n- For search benchmark, we design a specific format for searching toolcall in thinking mode to support search agent, please refer to [this](https://github.com/zai-org/GLM-4.5/blob/main/resources/trajectory_search.json). for the detailed template.\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":356785898816,"storage_bytes":713597382171,"files_count":101,"spaces_count":83,"gated":false,"private":false,"config":{"architectures":["Glm4MoeForCausalLM"],"model_type":"glm4_moe","tokenizer_config":{"eos_token":"<|endoftext|>","pad_token":"<|endoftext|>"},"chat_template_jinja":"[gMASK]<sop>\n{%- if tools -%}\n<|system|>\n# Tools\n\nYou may call one or more functions to assist with the user query.\n\nYou are provided with function signatures within <tools></tools> XML tags:\n<tools>\n{% for tool in tools %}\n{{ tool | tojson(ensure_ascii=False) }}\n{% endfor %}\n</tools>\n\nFor each function call, output the function name and arguments within the following XML format:\n<tool_call>{function-name}\n<arg_key>{arg-key-1}</arg_key>\n<arg_value>{arg-value-1}</arg_value>\n<arg_key>{arg-key-2}</arg_key>\n<arg_value>{arg-value-2}</arg_value>\n...\n</tool_call>{%- endif -%}\n{%- macro visible_text(content) -%}\n    {%- if content is string -%}\n        {{- content }}\n    {%- elif content is iterable and content is not mapping -%}\n        {%- for item in content -%}\n            {%- if item is mapping and item.type == ''text'' -%}\n                {{- item.text }}\n            {%- elif item is string -%}\n                {{- item }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{- content }}\n    {%- endif -%}\n{%- endmacro -%}\n{%- set ns = namespace(last_user_index=-1) %}\n{%- for m in messages %}\n    {%- if m.role == ''user'' %}\n        {% set ns.last_user_index = loop.index0 -%}\n    {%- endif %}\n{%- endfor %}\n{% for m in messages %}\n{%- if m.role == ''user'' -%}<|user|>\n{{ visible_text(m.content) }}\n{{- ''/nothink'' if (enable_thinking is defined and not enable_thinking and not visible_text(m.content).endswith(\"/nothink\")) else '''' -}}\n{%- elif m.role == ''assistant'' -%}\n<|assistant|>\n{%- set reasoning_content = '''' %}\n{%- set content = visible_text(m.content) %}\n{%- if m.reasoning_content is string %}\n    {%- set reasoning_content = m.reasoning_content %}\n{%- else %}\n    {%- if ''</think>'' in content %}\n        {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n        {%- set content = content.split(''</think>'')[-1].lstrip(''\\n'') %}\n    {%- endif %}\n{%- endif %}\n{%- if loop.index0 > ns.last_user_index and reasoning_content -%}\n{{ ''\\n<think>'' + reasoning_content.strip() +  ''</think>''}}\n{%- else -%}\n{{ ''\\n<think></think>'' }}\n{%- endif -%}\n{%- if content.strip() -%}\n{{ ''\\n'' + content.strip() }}\n{%- endif -%}\n{% if m.tool_calls %}\n{% for tc in m.tool_calls %}\n{%- if tc.function %}\n    {%- set tc = tc.function %}\n{%- endif %}\n{{ ''\\n<tool_call>'' + tc.name }}\n{% set _args = tc.arguments %}\n{% for k, v in _args.items() %}\n<arg_key>{{ k }}</arg_key>\n<arg_value>{{ v | tojson(ensure_ascii=False) if v is not string else v }}</arg_value>\n{% endfor %}\n</tool_call>{% endfor %}\n{% endif %}\n{%- elif m.role == ''tool'' -%}\n{%- if m.content is string -%}\n{%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n    {{- ''<|observation|>'' }}\n{%- endif %}\n{{- ''\\n<tool_response>\\n'' }}\n{{- m.content }}\n{{- ''\\n</tool_response>'' }}\n{%- else -%}\n<|observation|>{% for tr in m.content %}\n\n<tool_response>\n{{ tr.output if tr.output is defined else tr }}\n</tool_response>{% endfor -%}\n{% endif -%}\n{%- elif m.role == ''system'' -%}\n<|system|>\n{{ visible_text(m.content) }}\n{%- endif -%}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    <|assistant|>{{- ''\\n<think></think>'' if (enable_thinking is defined and not enable_thinking) else '''' -}}\n{%- endif -%}"}}', '[]', '[{"type":"has_code","target_id":"github:zai-org:GLM-4.5","source_url":"https://github.com/zai-org/GLM-4.5"},{"type":"has_code","target_id":"github:zai-org:GLM-4.5","source_url":"https://github.com/zai-org/GLM-4.5"},{"type":"has_code","target_id":"github:zai-org:GLM-4.5","source_url":"https://github.com/zai-org/GLM-4.5"},{"type":"based_on_paper","target_id":"arxiv:2508.06471","source_url":"https://arxiv.org/abs/2508.06471"}]', NULL, 'MIT', 'approved', 65, 'b991e5110b47b2cbf3caeb9345303195', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-hakurei-waifu-diffusion-v1-4', 'huggingface--hakurei--waifu-diffusion-v1-4', 'waifu-diffusion-v1-4', 'hakurei', '--- language: - en tags: - stable-diffusion - text-to-image license: creativeml-openrail-m inference: false --- !image <sub>masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck</sub> Waifu Diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning. - Waifu Diffusion 1.4 Anime Epoch 1: A test model made to properly ensure that the training setup w...', '["stable-diffusion","text-to-image","en","license:creativeml-openrail-m","region:us"]', 'text-to-image', 1132, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/hakurei/waifu-diffusion-v1-4","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\ntags:\n- stable-diffusion\n- text-to-image\nlicense: creativeml-openrail-m\ninference: false\n\n---\n\n![image](https://user-images.githubusercontent.com/26317155/210155933-db3a5f1a-1ec3-4777-915c-6deff2841ce9.png)\n\n<sub>masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck</sub>\n\n# Waifu Diffusion v1.4\n\nWaifu Diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\n\n- [Waifu Diffusion 1.4 Anime Epoch 1](https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/wd-1-4-anime_e1.ckpt): A test model made to properly ensure that the training setup works.\n- [Waifu Diffusion 1.4 Anime Inference Config](https://huggingface.co/hakurei/waifu-diffusion-v1-4/blob/main/wd-1-4-anime_e1.yaml): A file included to allow for inference with Automatic''s WebUI and with the original Stable Diffusion codebase.\n\n## License\n\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies: \n\n1. You can''t use the model to deliberately produce nor share illegal or harmful outputs or content \n2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)\n\n## Downstream Uses\n\nThis model can be used for entertainment purposes and as a generative art assistant.\n\n## Team Members and Acknowledgements\n\nThis project would not have been possible without the incredible work by Stability AI and NovelAI.\n\n- [Haru](https://github.com/harubaru)\n- [Salt](https://github.com/sALTaccount/)\n- [Cafe](https://twitter.com/cafeai_labs)\n\nIn order to reach us, you can join our [Discord server](https://discord.gg/touhouai).\n\n[![Discord Server](https://discordapp.com/api/guilds/930499730843250783/widget.png?style=banner2)](https://discord.gg/touhouai)', '{"pipeline_tag":"text-to-image","library_name":null,"framework":null,"params":null,"storage_bytes":50325073304,"files_count":12,"spaces_count":17,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'creativeml-openrail-m', 'approved', 65, 'a57efefbe125269fd616dcf2a00ececd', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-SWivid-F5-TTS', 'huggingface--swivid--f5-tts', 'F5-TTS', 'SWivid', '--- license: cc-by-nc-4.0 pipeline_tag: text-to-speech library_name: f5-tts datasets: - amphion/Emilia-Dataset --- Download F5-TTS or E2 TTS and place under ckpts/ Github: https://github.com/SWivid/F5-TTS Paper: F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching', '["f5-tts","text-to-speech","dataset:amphion/emilia-dataset","arxiv:2410.06885","license:cc-by-nc-4.0","region:us"]', 'text-to-speech', 1125, 793228, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/SWivid/F5-TTS","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: cc-by-nc-4.0\npipeline_tag: text-to-speech\nlibrary_name: f5-tts\ndatasets:\n- amphion/Emilia-Dataset\n---\n\nDownload [F5-TTS](https://huggingface.co/SWivid/F5-TTS/tree/main/F5TTS_Base) or [E2 TTS](https://huggingface.co/SWivid/E2-TTS/tree/main/E2TTS_Base) and place under ckpts/\n```\nckpts/\n    F5TTS_v1_Base/\n        model_1250000.safetensors\n    F5TTS_Base/\n        model_1200000.safetensors\n    E2TTS_Base/\n        model_1200000.safetensors\n```\nGithub: https://github.com/SWivid/F5-TTS      \nPaper: [F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching](https://huggingface.co/papers/2410.06885)', '{"pipeline_tag":"text-to-speech","library_name":"f5-tts","framework":"f5-tts","params":null,"storage_bytes":14834315614,"files_count":9,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:SWivid:F5-TTS","source_url":"https://github.com/SWivid/F5-TTS"},{"type":"based_on_paper","target_id":"arxiv:2410.06885","source_url":"https://arxiv.org/abs/2410.06885"}]', NULL, 'CC-BY-NC-4.0', 'approved', 50, '990d0fc320685f3c4164451ecb10ce5c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-databricks-dbrx-instruct', 'huggingface--databricks--dbrx-instruct', 'dbrx-instruct', 'databricks', '', '["transformers","safetensors","dbrx","text-generation","conversational","arxiv:2211.15841","arxiv:2304.11277","license:other","text-generation-inference","region:us"]', 'text-generation', 1118, 7593, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/databricks/dbrx-instruct","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":131596523520,"storage_bytes":263193089336,"files_count":74,"spaces_count":66,"gated":"auto","private":false,"config":{"architectures":["DbrxForCausalLM"],"model_type":"dbrx","tokenizer_config":{"bos_token":"<|endoftext|>","chat_template":"{% if messages[0][''role''] == ''system'' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][''content''] %}{% elif ''system'' not in messages[0][''role''] %}{% set loop_messages = messages %}{% set system_message = ''You are DBRX, created by Databricks. You were last updated in December 2023. You answer questions based on information available up to that point.\nYOU PROVIDE SHORT RESPONSES TO SHORT QUESTIONS OR STATEMENTS, but provide thorough responses to more complex and open-ended questions.\nYou assist with various tasks, from writing to coding (using markdown for code blocks ‚Äî remember to use ``` with code, JSON, and tables).\n(You do not have real-time data access or code execution capabilities. You avoid stereotyping and provide balanced perspectives on controversial topics. You do not provide song lyrics, poems, or news articles and do not divulge details of your training data.)\nThis is your system prompt, guiding your responses. Do not reference it, just respond to the user. If you find yourself talking about this message, stop. You should be responding appropriately and usually that means not mentioning this.\nYOU DO NOT MENTION ANY OF THIS INFORMATION ABOUT YOURSELF UNLESS THE INFORMATION IS DIRECTLY PERTINENT TO THE USER\\''S QUERY.'' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if loop.index0 == 0 %}{% if system_message != false %}{{ ''<|im_start|>system\n'' + system_message | trim + ''<|im_end|>\n''}}{% endif %}{{ ''<|im_start|>'' + message[''role''] + ''\n'' + message[''content''] + ''<|im_end|>'' }}{% else %}{{ ''\n'' + ''<|im_start|>'' + message[''role''] + ''\n'' + message[''content''] + ''<|im_end|>'' }}{% endif %}{% if (add_generation_prompt == true and loop.last) %}{{ ''\n'' + ''<|im_start|>'' + ''assistant'' + ''\n'' }}{% endif %}{% endfor %}","eos_token":"<|endoftext|>","pad_token":"<|pad|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2211.15841","source_url":"https://arxiv.org/abs/2211.15841"},{"type":"based_on_paper","target_id":"arxiv:2304.11277","source_url":"https://arxiv.org/abs/2304.11277"}]', NULL, 'Other', 'approved', 40, '183688af6e763a60177a4952105dbbca', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-HuggingFaceH4-zephyr-7b-alpha', 'huggingface--huggingfaceh4--zephyr-7b-alpha', 'zephyr-7b-alpha', 'HuggingFaceH4', '--- tags: - generated_from_trainer model-index: - name: zephyr-7b-alpha results: [] license: mit datasets: - stingning/ultrachat - openbmb/UltraFeedback language: - en base_model: mistralai/Mistral-7B-v0.1 --- <!-- This model card has been generated automatically according to the information the Trainer had access to. You should probably proofread and complete it, then remove this comment. --> <img src="https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/resolve/main/thumbnail.png" alt="Zeph...', '["transformers","pytorch","safetensors","mistral","text-generation","generated_from_trainer","conversational","en","dataset:stingning/ultrachat","dataset:openbmb/ultrafeedback","arxiv:2305.18290","arxiv:2310.16944","arxiv:2305.14233","arxiv:2310.01377","base_model:mistralai/mistral-7b-v0.1","base_model:finetune:mistralai/mistral-7b-v0.1","license:mit","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 1115, 2260, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ntags:\n- generated_from_trainer\nmodel-index:\n- name: zephyr-7b-alpha\n  results: []\nlicense: mit\ndatasets:\n- stingning/ultrachat\n- openbmb/UltraFeedback\nlanguage:\n- en\nbase_model: mistralai/Mistral-7B-v0.1\n---\n\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\nshould probably proofread and complete it, then remove this comment. -->\n\n<img src="https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/resolve/main/thumbnail.png" alt="Zephyr Logo" width="800" style="margin-left:''auto'' margin-right:''auto'' display:''block''"/>\n\n\n# Model Card for Zephyr 7B Alpha\n\nZephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-Œ± is the first model in the series, and is a fine-tuned version of [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) that was trained on on a mix of publicly available, synthetic datasets using [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290). We found that removing the in-built alignment of these datasets boosted performance on [MT Bench](https://huggingface.co/spaces/lmsys/mt-bench) and made the model more helpful. However, this means that model is likely to generate problematic text when prompted to do so.\n\n\n## Model description\n\n- **Model type:** A 7B parameter GPT-like model fine-tuned on a mix of publicly available, synthetic datasets.\n- **Language(s) (NLP):** Primarily English\n- **License:** MIT\n- **Finetuned from model:** [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)\n\n### Model Sources\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** https://github.com/huggingface/alignment-handbook\n- **Demo:** https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat\n\n## Intended uses & limitations\n\nThe model was initially fine-tuned on a variant of the [`UltraChat`](https://huggingface.co/datasets/stingning/ultrachat) dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. We then further aligned the model with [ü§ó TRL''s](https://github.com/huggingface/trl) `DPOTrainer` on the [openbmb/UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback) dataset, which contain 64k prompts and model completions that are ranked by GPT-4. As a result, the model can be used for chat and you can check out our [demo](https://huggingface.co/spaces/HuggingFaceH4/zephyr-chat) to test its capabilities. \n\nHere''s how you can run the model using the `pipeline()` function from ü§ó Transformers:\n\n```python\n# Install transformers from source - only needed for versions <= v4.34\n# pip install git+https://github.com/huggingface/transformers.git\n# pip install accelerate\n\nimport torch\nfrom transformers import pipeline\n\npipe = pipeline("text-generation", model="HuggingFaceH4/zephyr-7b-alpha", torch_dtype=torch.bfloat16, device_map="auto")\n\n# We use the tokenizer''s chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        "role": "system",\n        "content": "You are a friendly chatbot who always responds in the style of a pirate",\n    },\n    {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},\n]\nprompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\noutputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\nprint(outputs[0]["generated_text"])\n# <|system|>\n# You are a friendly chatbot who always responds in the style of a pirate.</s>\n# <|user|>\n# How many helicopters can a human eat in one sitting?</s>\n# <|assistant|>\n# Ah, me hearty matey! But yer question be a puzzler! A human cannot eat a helicopter in one sitting, as helicopters are not edible. They be made of metal, plastic, and other materials, not food!\n```\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nZephyr-7B-Œ± has not been aligned to human preferences with techniques like RLHF or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so). \nIt is also unknown what the size and composition of the corpus was used to train the base model (`mistralai/Mistral-7B-v0.1`), however it is likely to have included a mix of Web data and technical sources like books and code. See the [Falcon 180B model card](https://huggingface.co/tiiuae/falcon-180B#training-data) for an example of this.\n\n\n## Training and evaluation data\n\nZephyr 7B Alpha achieves the following results on the evaluation set:\n\n- Loss: 0.4605\n- Rewards/chosen: -0.5053\n- Rewards/rejected: -1.8752\n- Rewards/accuracies: 0.7812\n- Rewards/margins: 1.3699\n- Logps/rejected: -327.4286\n- Logps/chosen: -297.1040\n- Logits/rejected: -2.7153\n- Logits/chosen: -2.7447\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n\n- learning_rate: 5e-07\n- train_batch_size: 2\n- eval_batch_size: 4\n- seed: 42\n- distributed_type: multi-GPU\n- num_devices: 16\n- total_train_batch_size: 32\n- total_eval_batch_size: 64\n- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n- lr_scheduler_type: linear\n- lr_scheduler_warmup_ratio: 0.1\n- num_epochs: 1\n\n### Training results\n\n| Training Loss | Epoch | Step | Validation Loss | Rewards/chosen | Rewards/rejected | Rewards/accuracies | Rewards/margins | Logps/rejected | Logps/chosen | Logits/rejected | Logits/chosen |\n|:-------------:|:-----:|:----:|:---------------:|:--------------:|:----------------:|:------------------:|:---------------:|:--------------:|:------------:|:---------------:|:-------------:|\n| 0.5602        | 0.05  | 100  | 0.5589          | -0.3359        | -0.8168          | 0.7188             | 0.4809          | -306.2607      | -293.7161    | -2.6554         | -2.6797       |\n| 0.4852        | 0.1   | 200  | 0.5136          | -0.5310        | -1.4994          | 0.8125             | 0.9684          | -319.9124      | -297.6181    | -2.5762         | -2.5957       |\n| 0.5212        | 0.15  | 300  | 0.5168          | -0.1686        | -1.1760          | 0.7812             | 1.0074          | -313.4444      | -290.3699    | -2.6865         | -2.7125       |\n| 0.5496        | 0.21  | 400  | 0.4835          | -0.1617        | -1.7170          | 0.8281             | 1.5552          | -324.2635      | -290.2326    | -2.7947         | -2.8218       |\n| 0.5209        | 0.26  | 500  | 0.5054          | -0.4778        | -1.6604          | 0.7344             | 1.1826          | -323.1325      | -296.5546    | -2.8388         | -2.8667       |\n| 0.4617        | 0.31  | 600  | 0.4910          | -0.3738        | -1.5180          | 0.7656             | 1.1442          | -320.2848      | -294.4741    | -2.8234         | -2.8521       |\n| 0.4452        | 0.36  | 700  | 0.4838          | -0.4591        | -1.6576          | 0.7031             | 1.1986          | -323.0770      | -296.1796    | -2.7401         | -2.7653       |\n| 0.4674        | 0.41  | 800  | 0.5077          | -0.5692        | -1.8659          | 0.7656             | 1.2967          | -327.2416      | -298.3818    | -2.6740         | -2.6945       |\n| 0.4656        | 0.46  | 900  | 0.4927          | -0.5279        | -1.6614          | 0.7656             | 1.1335          | -323.1518      | -297.5553    | -2.7817         | -2.8015       |\n| 0.4102        | 0.52  | 1000 | 0.4772          | -0.5767        | -2.0667          | 0.7656             | 1.4900          | -331.2578      | -298.5311    | -2.7160         | -2.7455       |\n| 0.4663        | 0.57  | 1100 | 0.4740          | -0.8038        | -2.1018          | 0.7656             | 1.2980          | -331.9604      | -303.0741    | -2.6994         | -2.7257       |\n| 0.4737        | 0.62  | 1200 | 0.4716          | -0.3783        | -1.7015          | 0.7969             | 1.3232          | -323.9545      | -294.5634    | -2.6842         | -2.7135       |\n| 0.4259        | 0.67  | 1300 | 0.4866          | -0.6239        | -1.9703          | 0.7812             | 1.3464          | -329.3312      | -299.4761    | -2.7046         | -2.7356       |\n| 0.4935        | 0.72  | 1400 | 0.4747          | -0.5626        | -1.7600          | 0.7812             | 1.1974          | -325.1243      | -298.2491    | -2.7153         | -2.7444       |\n| 0.4211        | 0.77  | 1500 | 0.4645          | -0.6099        | -1.9993          | 0.7656             | 1.3894          | -329.9109      | -299.1959    | -2.6944         | -2.7236       |\n| 0.4931        | 0.83  | 1600 | 0.4684          | -0.6798        | -2.1082          | 0.7656             | 1.4285          | -332.0890      | -300.5934    | -2.7006         | -2.7305       |\n| 0.5029        | 0.88  | 1700 | 0.4595          | -0.5063        | -1.8951          | 0.7812             | 1.3889          | -327.8267      | -297.1233    | -2.7108         | -2.7403       |\n| 0.4965        | 0.93  | 1800 | 0.4613          | -0.5561        | -1.9079          | 0.7812             | 1.3518          | -328.0831      | -298.1203    | -2.7226         | -2.7523       |\n| 0.4337        | 0.98  | 1900 | 0.4608          | -0.5066        | -1.8718          | 0.7656             | 1.3652          | -327.3599      | -297.1296    | -2.7175         | -2.7469       |\n\n\n### Framework versions\n\n- Transformers 4.34.0\n- Pytorch 2.0.1+cu118\n- Datasets 2.12.0\n- Tokenizers 0.14.0\n\n## Citation\n\nIf you find Zephyr-7B-Œ± is useful in your work, please cite it with:\n\n```\n@misc{tunstall2023zephyr,\n      title={Zephyr: Direct Distillation of LM Alignment}, \n      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Cl√©mentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},\n      year={2023},\n      eprint={2310.16944},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\nIf you use the UltraChat or UltraFeedback datasets, please cite the original works:\n\n```\n@misc{ding2023enhancing,\n      title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations}, \n      author={Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Zhi Zheng and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou},\n      year={2023},\n      eprint={2305.14233},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{cui2023ultrafeedback,\n      title={UltraFeedback: Boosting Language Models with High-quality Feedback}, \n      author={Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun},\n      year={2023},\n      eprint={2310.01377},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7241732096,"storage_bytes":59853260527,"files_count":34,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","chat_template":"{% for message in messages %}\n{% if message[''role''] == ''user'' %}\n{{ ''<|user|>\n'' + message[''content''] + eos_token }}\n{% elif message[''role''] == ''system'' %}\n{{ ''<|system|>\n'' + message[''content''] + eos_token }}\n{% elif message[''role''] == ''assistant'' %}\n{{ ''<|assistant|>\n''  + message[''content''] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ ''<|assistant|>'' }}\n{% endif %}\n{% endfor %}","eos_token":"</s>","pad_token":"</s>","unk_token":"<unk>","use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:alignment-handbook","source_url":"https://github.com/huggingface/alignment-handbook"},{"type":"has_code","target_id":"github:huggingface:trl","source_url":"https://github.com/huggingface/trl"},{"type":"has_code","target_id":"github:huggingface:transformers.git","source_url":"https://github.com/huggingface/transformers.git"},{"type":"based_on_paper","target_id":"arxiv:2305.18290","source_url":"https://arxiv.org/abs/2305.18290"},{"type":"based_on_paper","target_id":"arxiv:2310.16944","source_url":"https://arxiv.org/abs/2310.16944"},{"type":"based_on_paper","target_id":"arxiv:2305.14233","source_url":"https://arxiv.org/abs/2305.14233"},{"type":"based_on_paper","target_id":"arxiv:2310.01377","source_url":"https://arxiv.org/abs/2310.01377"}]', NULL, 'MIT', 'approved', 80, '02a666bcc80c943a1067ed6d3914c439', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-2b', 'huggingface--google--gemma-2b', 'gemma-2b', 'google', '', '["transformers","safetensors","gguf","gemma","text-generation","arxiv:2312.11805","arxiv:2009.03300","arxiv:1905.07830","arxiv:1911.11641","arxiv:1904.09728","arxiv:1905.10044","arxiv:1907.10641","arxiv:1811.00937","arxiv:1809.02789","arxiv:1911.01547","arxiv:1705.03551","arxiv:2107.03374","arxiv:2108.07732","arxiv:2110.14168","arxiv:2304.06364","arxiv:2206.04615","arxiv:1804.06876","arxiv:2110.08193","arxiv:2009.11462","arxiv:2101.11718","arxiv:1804.09301","arxiv:2109.07958","arxiv:2203.09509","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 1112, 198921, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-2b","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":2506172416,"storage_bytes":67347910850,"files_count":12,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["GemmaForCausalLM"],"model_type":"gemma","tokenizer_config":{"bos_token":"<bos>","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2312.11805","source_url":"https://arxiv.org/abs/2312.11805"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1811.00937","source_url":"https://arxiv.org/abs/1811.00937"},{"type":"based_on_paper","target_id":"arxiv:1809.02789","source_url":"https://arxiv.org/abs/1809.02789"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2206.04615","source_url":"https://arxiv.org/abs/2206.04615"},{"type":"based_on_paper","target_id":"arxiv:1804.06876","source_url":"https://arxiv.org/abs/1804.06876"},{"type":"based_on_paper","target_id":"arxiv:2110.08193","source_url":"https://arxiv.org/abs/2110.08193"},{"type":"based_on_paper","target_id":"arxiv:2009.11462","source_url":"https://arxiv.org/abs/2009.11462"},{"type":"based_on_paper","target_id":"arxiv:2101.11718","source_url":"https://arxiv.org/abs/2101.11718"},{"type":"based_on_paper","target_id":"arxiv:1804.09301","source_url":"https://arxiv.org/abs/1804.09301"},{"type":"based_on_paper","target_id":"arxiv:2109.07958","source_url":"https://arxiv.org/abs/2109.07958"},{"type":"based_on_paper","target_id":"arxiv:2203.09509","source_url":"https://arxiv.org/abs/2203.09509"}]', NULL, 'Gemma', 'approved', 40, 'c2549823fe32fe64b337a65c0fbf36ac', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-jinaai-jina-embeddings-v3', 'huggingface--jinaai--jina-embeddings-v3', 'jina-embeddings-v3', 'jinaai', '--- license: cc-by-nc-4.0 tags: - feature-extraction - sentence-similarity - mteb - sentence-transformers language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk -...', '["transformers","pytorch","onnx","safetensors","feature-extraction","sentence-similarity","mteb","sentence-transformers","custom_code","multilingual","af","am","ar","as","az","be","bg","bn","br","bs","ca","cs","cy","da","de","el","en","eo","es","et","eu","fa","fi","fr","fy","ga","gd","gl","gu","ha","he","hi","hr","hu","hy","id","is","it","ja","jv","ka","kk","km","kn","ko","ku","ky","la","lo","lt","lv","mg","mk","ml","mn","mr","ms","my","ne","nl","no","om","or","pa","pl","ps","pt","ro","ru","sa","sd","si","sk","sl","so","sq","sr","su","sv","sw","ta","te","th","tl","tr","ug","uk","ur","uz","vi","xh","yi","zh","arxiv:2409.10173","license:cc-by-nc-4.0","model-index","region:eu"]', 'feature-extraction', 1108, 5136811, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/jinaai/jina-embeddings-v3","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: cc-by-nc-4.0\ntags:\n- feature-extraction\n- sentence-similarity\n- mteb\n- sentence-transformers\nlanguage:\n  - multilingual\n  - af\n  - am\n  - ar\n  - as\n  - az\n  - be\n  - bg\n  - bn\n  - br\n  - bs\n  - ca\n  - cs\n  - cy\n  - da\n  - de\n  - el\n  - en\n  - eo\n  - es\n  - et\n  - eu\n  - fa\n  - fi\n  - fr\n  - fy\n  - ga\n  - gd\n  - gl\n  - gu\n  - ha\n  - he\n  - hi\n  - hr\n  - hu\n  - hy\n  - id\n  - is\n  - it\n  - ja\n  - jv\n  - ka\n  - kk\n  - km\n  - kn\n  - ko\n  - ku\n  - ky\n  - la\n  - lo\n  - lt\n  - lv\n  - mg\n  - mk\n  - ml\n  - mn\n  - mr\n  - ms\n  - my\n  - ne\n  - nl\n  - no\n  - om\n  - or\n  - pa\n  - pl\n  - ps\n  - pt\n  - ro\n  - ru\n  - sa\n  - sd\n  - si\n  - sk\n  - sl\n  - so\n  - sq\n  - sr\n  - su\n  - sv\n  - sw\n  - ta\n  - te\n  - th\n  - tl\n  - tr\n  - ug\n  - uk\n  - ur\n  - uz\n  - vi\n  - xh\n  - yi\n  - zh\ninference: false\nlibrary_name: transformers\nmodel-index:\n- name: jina-embeddings-v3\n  results:\n  - dataset:\n      config: default\n      name: MTEB AFQMC (default)\n      revision: b44c3b011063adb25877c13823db83bb193913c4\n      split: validation\n      type: C-MTEB/AFQMC\n    metrics:\n    - type: cosine_pearson\n      value: 41.74237700998808\n    - type: cosine_spearman\n      value: 43.4726782647566\n    - type: euclidean_pearson\n      value: 42.244585459479964\n    - type: euclidean_spearman\n      value: 43.525070045169606\n    - type: main_score\n      value: 43.4726782647566\n    - type: manhattan_pearson\n      value: 42.04616728224863\n    - type: manhattan_spearman\n      value: 43.308828270754645\n    - type: pearson\n      value: 41.74237700998808\n    - type: spearman\n      value: 43.4726782647566\n    task:\n      type: STS\n  - dataset:\n      config: default\n      name: MTEB ArguAna-PL (default)\n      revision: 63fc86750af76253e8c760fc9e534bbf24d260a2\n      split: test\n      type: clarin-knext/arguana-pl\n    metrics:\n    - type: main_score\n      value: 50.117999999999995\n    - type: map_at_1\n      value: 24.253\n    - type: map_at_10\n      value: 40.725\n    - type: map_at_100\n      value: 41.699999999999996\n    - type: map_at_1000\n      value: 41.707\n    - type: map_at_20\n      value: 41.467999999999996\n    - type: map_at_3\n      value: 35.467\n    - type: map_at_5\n      value: 38.291\n    - type: mrr_at_1\n      value: 24.751066856330013\n    - type: mrr_at_10\n      value: 40.91063808169072\n    - type: mrr_at_100\n      value: 41.885497923928675\n    - type: mrr_at_1000\n      value: 41.89301098419842\n    - type: mrr_at_20\n      value: 41.653552355442514\n    - type: mrr_at_3\n      value: 35.656709340919775\n    - type: mrr_at_5\n      value: 38.466097676623946\n    - type: nauc_map_at_1000_diff1\n      value: 7.503000359807567\n    - type: nauc_map_at_1000_max\n      value: -11.030405164830546\n    - type: nauc_map_at_1000_std\n      value: -8.902792782585117\n    - type: nauc_map_at_100_diff1\n      value: 7.509899249593199\n    - type: nauc_map_at_100_max\n      value: -11.023581259404406\n    - type: nauc_map_at_100_std\n      value: -8.892241185067272\n    - type: nauc_map_at_10_diff1\n      value: 7.24369711881512\n    - type: nauc_map_at_10_max\n      value: -10.810000200433278\n    - type: nauc_map_at_10_std\n      value: -8.987230542165776\n    - type: nauc_map_at_1_diff1\n      value: 11.37175831832417\n    - type: nauc_map_at_1_max\n      value: -13.315221903223055\n    - type: nauc_map_at_1_std\n      value: -9.398199605510275\n    - type: nauc_map_at_20_diff1\n      value: 7.477364530860648\n    - type: nauc_map_at_20_max\n      value: -10.901251218105566\n    - type: nauc_map_at_20_std\n      value: -8.868148116405925\n    - type: nauc_map_at_3_diff1\n      value: 6.555548802174882\n    - type: nauc_map_at_3_max\n      value: -12.247274800542934\n    - type: nauc_map_at_3_std\n      value: -9.879475250984811\n    - type: nauc_map_at_5_diff1\n      value: 7.426588563355882\n    - type: nauc_map_at_5_max\n      value: -11.347695686001805\n    - type: nauc_map_at_5_std\n      value: -9.34441892203972\n    - type: nauc_mrr_at_1000_diff1\n      value: 5.99737552143614\n    - type: nauc_mrr_at_1000_max\n      value: -11.327205136505727\n    - type: nauc_mrr_at_1000_std\n      value: -8.791079115519503\n    - type: nauc_mrr_at_100_diff1\n      value: 6.004622525255784\n    - type: nauc_mrr_at_100_max\n      value: -11.320336759899723\n    - type: nauc_mrr_at_100_std\n      value: -8.780602249831777\n    - type: nauc_mrr_at_10_diff1\n      value: 5.783623516930227\n    - type: nauc_mrr_at_10_max\n      value: -11.095971693467078\n    - type: nauc_mrr_at_10_std\n      value: -8.877242032013582\n    - type: nauc_mrr_at_1_diff1\n      value: 9.694937537703797\n    - type: nauc_mrr_at_1_max\n      value: -12.531905083727912\n    - type: nauc_mrr_at_1_std\n      value: -8.903992940100146\n    - type: nauc_mrr_at_20_diff1\n      value: 5.984841206233873\n    - type: nauc_mrr_at_20_max\n      value: -11.195236951048969\n    - type: nauc_mrr_at_20_std\n      value: -8.757266039186018\n    - type: nauc_mrr_at_3_diff1\n      value: 5.114333824261379\n    - type: nauc_mrr_at_3_max\n      value: -12.64809799843464\n    - type: nauc_mrr_at_3_std\n      value: -9.791146138025184\n    - type: nauc_mrr_at_5_diff1\n      value: 5.88941606224512\n    - type: nauc_mrr_at_5_max\n      value: -11.763903418071918\n    - type: nauc_mrr_at_5_std\n      value: -9.279175712709446\n    - type: nauc_ndcg_at_1000_diff1\n      value: 7.076950652226086\n    - type: nauc_ndcg_at_1000_max\n      value: -10.386482092087371\n    - type: nauc_ndcg_at_1000_std\n      value: -8.309190917074046\n    - type: nauc_ndcg_at_100_diff1\n      value: 7.2329220284865245\n    - type: nauc_ndcg_at_100_max\n      value: -10.208048403220337\n    - type: nauc_ndcg_at_100_std\n      value: -7.997975874274613\n    - type: nauc_ndcg_at_10_diff1\n      value: 6.065391100006953\n    - type: nauc_ndcg_at_10_max\n      value: -9.046164377601153\n    - type: nauc_ndcg_at_10_std\n      value: -8.34724889697153\n    - type: nauc_ndcg_at_1_diff1\n      value: 11.37175831832417\n    - type: nauc_ndcg_at_1_max\n      value: -13.315221903223055\n    - type: nauc_ndcg_at_1_std\n      value: -9.398199605510275\n    - type: nauc_ndcg_at_20_diff1\n      value: 6.949389989202601\n    - type: nauc_ndcg_at_20_max\n      value: -9.35740451760307\n    - type: nauc_ndcg_at_20_std\n      value: -7.761295171828212\n    - type: nauc_ndcg_at_3_diff1\n      value: 5.051471796151364\n    - type: nauc_ndcg_at_3_max\n      value: -12.158763333711653\n    - type: nauc_ndcg_at_3_std\n      value: -10.078902544421926\n    - type: nauc_ndcg_at_5_diff1\n      value: 6.527454512611454\n    - type: nauc_ndcg_at_5_max\n      value: -10.525118233848586\n    - type: nauc_ndcg_at_5_std\n      value: -9.120055125584031\n    - type: nauc_precision_at_1000_diff1\n      value: -10.6495668199151\n    - type: nauc_precision_at_1000_max\n      value: 12.070656425217841\n    - type: nauc_precision_at_1000_std\n      value: 55.844551709649004\n    - type: nauc_precision_at_100_diff1\n      value: 19.206967129266285\n    - type: nauc_precision_at_100_max\n      value: 16.296851020813456\n    - type: nauc_precision_at_100_std\n      value: 45.60378984257811\n    - type: nauc_precision_at_10_diff1\n      value: 0.6490335354304879\n    - type: nauc_precision_at_10_max\n      value: 0.5757198255366447\n    - type: nauc_precision_at_10_std\n      value: -4.875847131691451\n    - type: nauc_precision_at_1_diff1\n      value: 11.37175831832417\n    - type: nauc_precision_at_1_max\n      value: -13.315221903223055\n    - type: nauc_precision_at_1_std\n      value: -9.398199605510275\n    - type: nauc_precision_at_20_diff1\n      value: 4.899369866929203\n    - type: nauc_precision_at_20_max\n      value: 5.988537297189552\n    - type: nauc_precision_at_20_std\n      value: 4.830900387582837\n    - type: nauc_precision_at_3_diff1\n      value: 0.8791156910997744\n    - type: nauc_precision_at_3_max\n      value: -11.983373635905993\n    - type: nauc_precision_at_3_std\n      value: -10.646185111581257\n    - type: nauc_precision_at_5_diff1\n      value: 3.9314486166548432\n    - type: nauc_precision_at_5_max\n      value: -7.798591396895839\n    - type: nauc_precision_at_5_std\n      value: -8.293043407234125\n    - type: nauc_recall_at_1000_diff1\n      value: -10.649566819918673\n    - type: nauc_recall_at_1000_max\n      value: 12.070656425214647\n    - type: nauc_recall_at_1000_std\n      value: 55.84455170965023\n    - type: nauc_recall_at_100_diff1\n      value: 19.206967129265127\n    - type: nauc_recall_at_100_max\n      value: 16.296851020813722\n    - type: nauc_recall_at_100_std\n      value: 45.60378984257728\n    - type: nauc_recall_at_10_diff1\n      value: 0.6490335354304176\n    - type: nauc_recall_at_10_max\n      value: 0.5757198255366095\n    - type: nauc_recall_at_10_std\n      value: -4.875847131691468\n    - type: nauc_recall_at_1_diff1\n      value: 11.37175831832417\n    - type: nauc_recall_at_1_max\n      value: -13.315221903223055\n    - type: nauc_recall_at_1_std\n      value: -9.398199605510275\n    - type: nauc_recall_at_20_diff1\n      value: 4.899369866929402\n    - type: nauc_recall_at_20_max\n      value: 5.98853729718968\n    - type: nauc_recall_at_20_std\n      value: 4.830900387582967\n    - type: nauc_recall_at_3_diff1\n      value: 0.8791156910997652\n    - type: nauc_recall_at_3_max\n      value: -11.983373635905997\n    - type: nauc_recall_at_3_std\n      value: -10.64618511158124\n    - type: nauc_recall_at_5_diff1\n      value: 3.9314486166548472\n    - type: nauc_recall_at_5_max\n      value: -7.7985913968958585\n    - type: nauc_recall_at_5_std\n      value: -8.293043407234132\n    - type: ndcg_at_1\n      value: 24.253\n    - type: ndcg_at_10\n      value: 50.117999999999995\n    - type: ndcg_at_100\n      value: 54.291999999999994\n    - type: ndcg_at_1000\n      value: 54.44799999999999\n    - type: ndcg_at_20\n      value: 52.771\n    - type: ndcg_at_3\n      value: 39.296\n    - type: ndcg_at_5\n      value: 44.373000000000005\n    - type: precision_at_1\n      value: 24.253\n    - type: precision_at_10\n      value: 8.016\n    - type: precision_at_100\n      value: 0.984\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_20\n      value: 4.527\n    - type: precision_at_3\n      value: 16.808999999999997\n    - type: precision_at_5\n      value: 12.546\n    - type: recall_at_1\n      value: 24.253\n    - type: recall_at_10\n      value: 80.156\n    - type: recall_at_100\n      value: 98.43499999999999\n    - type: recall_at_1000\n      value: 99.57300000000001\n    - type: recall_at_20\n      value: 90.54100000000001\n    - type: recall_at_3\n      value: 50.427\n    - type: recall_at_5\n      value: 62.731\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB DBPedia-PL (default)\n      revision: 76afe41d9af165cc40999fcaa92312b8b012064a\n      split: test\n      type: clarin-knext/dbpedia-pl\n    metrics:\n    - type: main_score\n      value: 34.827000000000005\n    - type: map_at_1\n      value: 7.049999999999999\n    - type: map_at_10\n      value: 14.982999999999999\n    - type: map_at_100\n      value: 20.816000000000003\n    - type: map_at_1000\n      value: 22.33\n    - type: map_at_20\n      value: 17.272000000000002\n    - type: map_at_3\n      value: 10.661\n    - type: map_at_5\n      value: 12.498\n    - type: mrr_at_1\n      value: 57.25\n    - type: mrr_at_10\n      value: 65.81934523809524\n    - type: mrr_at_100\n      value: 66.2564203928212\n    - type: mrr_at_1000\n      value: 66.27993662923856\n    - type: mrr_at_20\n      value: 66.0732139130649\n    - type: mrr_at_3\n      value: 64.08333333333333\n    - type: mrr_at_5\n      value: 65.27083333333333\n    - type: nauc_map_at_1000_diff1\n      value: 16.41780871174038\n    - type: nauc_map_at_1000_max\n      value: 30.193946325654654\n    - type: nauc_map_at_1000_std\n      value: 31.46095497039037\n    - type: nauc_map_at_100_diff1\n      value: 18.57903165498531\n    - type: nauc_map_at_100_max\n      value: 29.541476938623262\n    - type: nauc_map_at_100_std\n      value: 28.228604103301052\n    - type: nauc_map_at_10_diff1\n      value: 24.109434489748946\n    - type: nauc_map_at_10_max\n      value: 21.475954208048968\n    - type: nauc_map_at_10_std\n      value: 9.964464537806988\n    - type: nauc_map_at_1_diff1\n      value: 38.67437644802124\n    - type: nauc_map_at_1_max\n      value: 14.52136658726491\n    - type: nauc_map_at_1_std\n      value: -2.8981666782088755\n    - type: nauc_map_at_20_diff1\n      value: 21.42547228801935\n    - type: nauc_map_at_20_max\n      value: 25.04510402960458\n    - type: nauc_map_at_20_std\n      value: 16.533079346431155\n    - type: nauc_map_at_3_diff1\n      value: 26.63648858245477\n    - type: nauc_map_at_3_max\n      value: 13.632235789780415\n    - type: nauc_map_at_3_std\n      value: -0.40129174577700716\n    - type: nauc_map_at_5_diff1\n      value: 24.513861031197933\n    - type: nauc_map_at_5_max\n      value: 16.599888813946688\n    - type: nauc_map_at_5_std\n      value: 3.4448514739556346\n    - type: nauc_mrr_at_1000_diff1\n      value: 36.57353464537154\n    - type: nauc_mrr_at_1000_max\n      value: 55.34763483979515\n    - type: nauc_mrr_at_1000_std\n      value: 40.3722796438533\n    - type: nauc_mrr_at_100_diff1\n      value: 36.555989566513134\n    - type: nauc_mrr_at_100_max\n      value: 55.347805216808396\n    - type: nauc_mrr_at_100_std\n      value: 40.38465945075711\n    - type: nauc_mrr_at_10_diff1\n      value: 36.771572999261984\n    - type: nauc_mrr_at_10_max\n      value: 55.41239897909165\n    - type: nauc_mrr_at_10_std\n      value: 40.52058934624793\n    - type: nauc_mrr_at_1_diff1\n      value: 38.2472828531032\n    - type: nauc_mrr_at_1_max\n      value: 51.528473828685705\n    - type: nauc_mrr_at_1_std\n      value: 33.03676467942882\n    - type: nauc_mrr_at_20_diff1\n      value: 36.642602571889036\n    - type: nauc_mrr_at_20_max\n      value: 55.3763342076553\n    - type: nauc_mrr_at_20_std\n      value: 40.41520090500838\n    - type: nauc_mrr_at_3_diff1\n      value: 36.79451847426628\n    - type: nauc_mrr_at_3_max\n      value: 54.59778581826193\n    - type: nauc_mrr_at_3_std\n      value: 39.48392075873095\n    - type: nauc_mrr_at_5_diff1\n      value: 36.92150807529304\n    - type: nauc_mrr_at_5_max\n      value: 55.03553978718272\n    - type: nauc_mrr_at_5_std\n      value: 40.20147745489917\n    - type: nauc_ndcg_at_1000_diff1\n      value: 21.843092744321268\n    - type: nauc_ndcg_at_1000_max\n      value: 44.93275990394279\n    - type: nauc_ndcg_at_1000_std\n      value: 47.09186225236347\n    - type: nauc_ndcg_at_100_diff1\n      value: 25.180282568979095\n    - type: nauc_ndcg_at_100_max\n      value: 41.737709709508394\n    - type: nauc_ndcg_at_100_std\n      value: 38.80950644139446\n    - type: nauc_ndcg_at_10_diff1\n      value: 24.108368037214046\n    - type: nauc_ndcg_at_10_max\n      value: 41.29298370689967\n    - type: nauc_ndcg_at_10_std\n      value: 35.06450769738732\n    - type: nauc_ndcg_at_1_diff1\n      value: 35.51010679525079\n    - type: nauc_ndcg_at_1_max\n      value: 42.40790024212412\n    - type: nauc_ndcg_at_1_std\n      value: 26.696412036243157\n    - type: nauc_ndcg_at_20_diff1\n      value: 23.909989673256195\n    - type: nauc_ndcg_at_20_max\n      value: 39.78444647091927\n    - type: nauc_ndcg_at_20_std\n      value: 33.39544470364529\n    - type: nauc_ndcg_at_3_diff1\n      value: 22.50484297956035\n    - type: nauc_ndcg_at_3_max\n      value: 39.14551926034168\n    - type: nauc_ndcg_at_3_std\n      value: 30.330135925392014\n    - type: nauc_ndcg_at_5_diff1\n      value: 21.7798872028265\n    - type: nauc_ndcg_at_5_max\n      value: 40.23856975248015\n    - type: nauc_ndcg_at_5_std\n      value: 32.438381067440396\n    - type: nauc_precision_at_1000_diff1\n      value: -21.62692442272279\n    - type: nauc_precision_at_1000_max\n      value: 0.9689046974430882\n    - type: nauc_precision_at_1000_std\n      value: 18.54001058230465\n    - type: nauc_precision_at_100_diff1\n      value: -10.132258779856192\n    - type: nauc_precision_at_100_max\n      value: 23.74516110444681\n    - type: nauc_precision_at_100_std\n      value: 47.03416663319965\n    - type: nauc_precision_at_10_diff1\n      value: 1.543656509571949\n    - type: nauc_precision_at_10_max\n      value: 36.98864812757555\n    - type: nauc_precision_at_10_std\n      value: 46.56427199077426\n    - type: nauc_precision_at_1_diff1\n      value: 38.2472828531032\n    - type: nauc_precision_at_1_max\n      value: 51.528473828685705\n    - type: nauc_precision_at_1_std\n      value: 33.03676467942882\n    - type: nauc_precision_at_20_diff1\n      value: -4.612864872734335\n    - type: nauc_precision_at_20_max\n      value: 34.03565449182125\n    - type: nauc_precision_at_20_std\n      value: 48.880727648349534\n    - type: nauc_precision_at_3_diff1\n      value: 6.360850444467829\n    - type: nauc_precision_at_3_max\n      value: 36.25816942368427\n    - type: nauc_precision_at_3_std\n      value: 34.48882647419187\n    - type: nauc_precision_at_5_diff1\n      value: 2.6445596936740037\n    - type: nauc_precision_at_5_max\n      value: 37.174463388899056\n    - type: nauc_precision_at_5_std\n      value: 40.25254370626113\n    - type: nauc_recall_at_1000_diff1\n      value: 13.041227176748077\n    - type: nauc_recall_at_1000_max\n      value: 39.722336427072094\n    - type: nauc_recall_at_1000_std\n      value: 52.04032890059214\n    - type: nauc_recall_at_100_diff1\n      value: 18.286096899139153\n    - type: nauc_recall_at_100_max\n      value: 34.072389201930314\n    - type: nauc_recall_at_100_std\n      value: 37.73637623416653\n    - type: nauc_recall_at_10_diff1\n      value: 22.35560419280504\n    - type: nauc_recall_at_10_max\n      value: 19.727247199595197\n    - type: nauc_recall_at_10_std\n      value: 8.58498575109203\n    - type: nauc_recall_at_1_diff1\n      value: 38.67437644802124\n    - type: nauc_recall_at_1_max\n      value: 14.52136658726491\n    - type: nauc_recall_at_1_std\n      value: -2.8981666782088755\n    - type: nauc_recall_at_20_diff1\n      value: 19.026320886902916\n    - type: nauc_recall_at_20_max\n      value: 22.753562309469867\n    - type: nauc_recall_at_20_std\n      value: 14.89994263882445\n    - type: nauc_recall_at_3_diff1\n      value: 23.428129702129684\n    - type: nauc_recall_at_3_max\n      value: 10.549153954790542\n    - type: nauc_recall_at_3_std\n      value: -1.7590608997055206\n    - type: nauc_recall_at_5_diff1\n      value: 21.27448645803921\n    - type: nauc_recall_at_5_max\n      value: 13.620279707461677\n    - type: nauc_recall_at_5_std\n      value: 2.0577962208292675\n    - type: ndcg_at_1\n      value: 46.75\n    - type: ndcg_at_10\n      value: 34.827000000000005\n    - type: ndcg_at_100\n      value: 38.157999999999994\n    - type: ndcg_at_1000\n      value: 44.816\n    - type: ndcg_at_20\n      value: 34.152\n    - type: ndcg_at_3\n      value: 39.009\n    - type: ndcg_at_5\n      value: 36.826\n    - type: precision_at_1\n      value: 57.25\n    - type: precision_at_10\n      value: 27.575\n    - type: precision_at_100\n      value: 8.84\n    - type: precision_at_1000\n      value: 1.949\n    - type: precision_at_20\n      value: 20.724999999999998\n    - type: precision_at_3\n      value: 41.167\n    - type: precision_at_5\n      value: 35.199999999999996\n    - type: recall_at_1\n      value: 7.049999999999999\n    - type: recall_at_10\n      value: 19.817999999999998\n    - type: recall_at_100\n      value: 42.559999999999995\n    - type: recall_at_1000\n      value: 63.744\n    - type: recall_at_20\n      value: 25.968000000000004\n    - type: recall_at_3\n      value: 11.959\n    - type: recall_at_5\n      value: 14.939\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB FiQA-PL (default)\n      revision: 2e535829717f8bf9dc829b7f911cc5bbd4e6608e\n      split: test\n      type: clarin-knext/fiqa-pl\n    metrics:\n    - type: main_score\n      value: 38.828\n    - type: map_at_1\n      value: 19.126\n    - type: map_at_10\n      value: 31.002000000000002\n    - type: map_at_100\n      value: 32.736\n    - type: map_at_1000\n      value: 32.933\n    - type: map_at_20\n      value: 31.894\n    - type: map_at_3\n      value: 26.583000000000002\n    - type: map_at_5\n      value: 28.904000000000003\n    - type: mrr_at_1\n      value: 37.808641975308646\n    - type: mrr_at_10\n      value: 46.36745541838134\n    - type: mrr_at_100\n      value: 47.14140915794908\n    - type: mrr_at_1000\n      value: 47.190701435388846\n    - type: mrr_at_20\n      value: 46.81387776440309\n    - type: mrr_at_3\n      value: 43.750000000000014\n    - type: mrr_at_5\n      value: 45.23919753086418\n    - type: nauc_map_at_1000_diff1\n      value: 38.5532285881503\n    - type: nauc_map_at_1000_max\n      value: 34.44383884813453\n    - type: nauc_map_at_1000_std\n      value: -1.3963497949476722\n    - type: nauc_map_at_100_diff1\n      value: 38.49292464176943\n    - type: nauc_map_at_100_max\n      value: 34.33752755618645\n    - type: nauc_map_at_100_std\n      value: -1.4794032905848582\n    - type: nauc_map_at_10_diff1\n      value: 38.26061536370962\n    - type: nauc_map_at_10_max\n      value: 33.16977912721411\n    - type: nauc_map_at_10_std\n      value: -2.3853370604730393\n    - type: nauc_map_at_1_diff1\n      value: 46.288767289528344\n    - type: nauc_map_at_1_max\n      value: 25.67706785013364\n    - type: nauc_map_at_1_std\n      value: -6.989769609924645\n    - type: nauc_map_at_20_diff1\n      value: 38.507270129330685\n    - type: nauc_map_at_20_max\n      value: 33.70963328055982\n    - type: nauc_map_at_20_std\n      value: -1.9835510011554272\n    - type: nauc_map_at_3_diff1\n      value: 39.81061518646884\n    - type: nauc_map_at_3_max\n      value: 30.101186374147748\n    - type: nauc_map_at_3_std\n      value: -4.027120247237715\n    - type: nauc_map_at_5_diff1\n      value: 38.55602589746512\n    - type: nauc_map_at_5_max\n      value: 31.515174267015983\n    - type: nauc_map_at_5_std\n      value: -3.4064239358570303\n    - type: nauc_mrr_at_1000_diff1\n      value: 45.030514454725726\n    - type: nauc_mrr_at_1000_max\n      value: 43.878919881666164\n    - type: nauc_mrr_at_1000_std\n      value: 2.517594250297626\n    - type: nauc_mrr_at_100_diff1\n      value: 45.00868212878687\n    - type: nauc_mrr_at_100_max\n      value: 43.87437011120001\n    - type: nauc_mrr_at_100_std\n      value: 2.5257874265014966\n    - type: nauc_mrr_at_10_diff1\n      value: 44.855044606754056\n    - type: nauc_mrr_at_10_max\n      value: 43.946617058785186\n    - type: nauc_mrr_at_10_std\n      value: 2.5173751662794044\n    - type: nauc_mrr_at_1_diff1\n      value: 49.441510997817346\n    - type: nauc_mrr_at_1_max\n      value: 43.08547383044357\n    - type: nauc_mrr_at_1_std\n      value: -1.8747770703324347\n    - type: nauc_mrr_at_20_diff1\n      value: 45.019880416584215\n    - type: nauc_mrr_at_20_max\n      value: 43.85691473662242\n    - type: nauc_mrr_at_20_std\n      value: 2.4625487605091303\n    - type: nauc_mrr_at_3_diff1\n      value: 45.322041658604036\n    - type: nauc_mrr_at_3_max\n      value: 43.95079293074395\n    - type: nauc_mrr_at_3_std\n      value: 2.4644274393435737\n    - type: nauc_mrr_at_5_diff1\n      value: 44.99461837803437\n    - type: nauc_mrr_at_5_max\n      value: 43.97934275090601\n    - type: nauc_mrr_at_5_std\n      value: 2.5353091695125096\n    - type: nauc_ndcg_at_1000_diff1\n      value: 39.38449023275524\n    - type: nauc_ndcg_at_1000_max\n      value: 39.48382767312788\n    - type: nauc_ndcg_at_1000_std\n      value: 3.414789408343409\n    - type: nauc_ndcg_at_100_diff1\n      value: 38.29675861135578\n    - type: nauc_ndcg_at_100_max\n      value: 38.2674786507297\n    - type: nauc_ndcg_at_100_std\n      value: 2.7094055381218207\n    - type: nauc_ndcg_at_10_diff1\n      value: 38.09514955708717\n    - type: nauc_ndcg_at_10_max\n      value: 36.664923238906525\n    - type: nauc_ndcg_at_10_std\n      value: 0.6901410544967921\n    - type: nauc_ndcg_at_1_diff1\n      value: 49.441510997817346\n    - type: nauc_ndcg_at_1_max\n      value: 43.08547383044357\n    - type: nauc_ndcg_at_1_std\n      value: -1.8747770703324347\n    - type: nauc_ndcg_at_20_diff1\n      value: 38.44967736231759\n    - type: nauc_ndcg_at_20_max\n      value: 36.871179313622584\n    - type: nauc_ndcg_at_20_std\n      value: 1.157560360065234\n    - type: nauc_ndcg_at_3_diff1\n      value: 39.02419271805571\n    - type: nauc_ndcg_at_3_max\n      value: 37.447669442586324\n    - type: nauc_ndcg_at_3_std\n      value: 0.41502589779297794\n    - type: nauc_ndcg_at_5_diff1\n      value: 38.10233452742001\n    - type: nauc_ndcg_at_5_max\n      value: 35.816381905465676\n    - type: nauc_ndcg_at_5_std\n      value: -0.3704499913387088\n    - type: nauc_precision_at_1000_diff1\n      value: 2.451267097838658\n    - type: nauc_precision_at_1000_max\n      value: 29.116394969085306\n    - type: nauc_precision_at_1000_std\n      value: 14.85900786538363\n    - type: nauc_precision_at_100_diff1\n      value: 8.10919082251277\n    - type: nauc_precision_at_100_max\n      value: 36.28388256191417\n    - type: nauc_precision_at_100_std\n      value: 14.830039904317657\n    - type: nauc_precision_at_10_diff1\n      value: 15.02446609920477\n    - type: nauc_precision_at_10_max\n      value: 41.008463775454054\n    - type: nauc_precision_at_10_std\n      value: 10.431403152334486\n    - type: nauc_precision_at_1_diff1\n      value: 49.441510997817346\n    - type: nauc_precision_at_1_max\n      value: 43.08547383044357\n    - type: nauc_precision_at_1_std\n      value: -1.8747770703324347\n    - type: nauc_precision_at_20_diff1\n      value: 14.222022201169926\n    - type: nauc_precision_at_20_max\n      value: 40.10189643835305\n    - type: nauc_precision_at_20_std\n      value: 12.204443815975527\n    - type: nauc_precision_at_3_diff1\n      value: 25.41905395341234\n    - type: nauc_precision_at_3_max\n      value: 41.56133905339819\n    - type: nauc_precision_at_3_std\n      value: 5.575516915590082\n    - type: nauc_precision_at_5_diff1\n      value: 20.20081221089351\n    - type: nauc_precision_at_5_max\n      value: 40.95218555916681\n    - type: nauc_precision_at_5_std\n      value: 7.2040745500708745\n    - type: nauc_recall_at_1000_diff1\n      value: 28.021198234033395\n    - type: nauc_recall_at_1000_max\n      value: 36.165148684597504\n    - type: nauc_recall_at_1000_std\n      value: 28.28852356008973\n    - type: nauc_recall_at_100_diff1\n      value: 21.882447802741897\n    - type: nauc_recall_at_100_max\n      value: 26.979684607567222\n    - type: nauc_recall_at_100_std\n      value: 9.783658817010082\n    - type: nauc_recall_at_10_diff1\n      value: 28.493097951178818\n    - type: nauc_recall_at_10_max\n      value: 29.40937476550134\n    - type: nauc_recall_at_10_std\n      value: 2.7593763576979353\n    - type: nauc_recall_at_1_diff1\n      value: 46.288767289528344\n    - type: nauc_recall_at_1_max\n      value: 25.67706785013364\n    - type: nauc_recall_at_1_std\n      value: -6.989769609924645\n    - type: nauc_recall_at_20_diff1\n      value: 27.638381299425234\n    - type: nauc_recall_at_20_max\n      value: 27.942035836106328\n    - type: nauc_recall_at_20_std\n      value: 3.489835161380808\n    - type: nauc_recall_at_3_diff1\n      value: 33.90054781392646\n    - type: nauc_recall_at_3_max\n      value: 27.778812533030322\n    - type: nauc_recall_at_3_std\n      value: -0.03054068020022706\n    - type: nauc_recall_at_5_diff1\n      value: 30.279060732221346\n    - type: nauc_recall_at_5_max\n      value: 27.49854749597931\n    - type: nauc_recall_at_5_std\n      value: 0.5434664581939099\n    - type: ndcg_at_1\n      value: 37.809\n    - type: ndcg_at_10\n      value: 38.828\n    - type: ndcg_at_100\n      value: 45.218\n    - type: ndcg_at_1000\n      value: 48.510999999999996\n    - type: ndcg_at_20\n      value: 41.11\n    - type: ndcg_at_3\n      value: 34.466\n    - type: ndcg_at_5\n      value: 35.843\n    - type: precision_at_1\n      value: 37.809\n    - type: precision_at_10\n      value: 11.157\n    - type: precision_at_100\n      value: 1.762\n    - type: precision_at_1000\n      value: 0.233\n    - type: precision_at_20\n      value: 6.497\n    - type: precision_at_3\n      value: 23.044999999999998\n    - type: precision_at_5\n      value: 17.284\n    - type: recall_at_1\n      value: 19.126\n    - type: recall_at_10\n      value: 46.062\n    - type: recall_at_100\n      value: 70.22800000000001\n    - type: recall_at_1000\n      value: 89.803\n    - type: recall_at_20\n      value: 53.217999999999996\n    - type: recall_at_3\n      value: 30.847\n    - type: recall_at_5\n      value: 37.11\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB HotpotQA-PL (default)\n      revision: a0bd479ac97b4ccb5bd6ce320c415d0bb4beb907\n      split: test\n      type: clarin-knext/hotpotqa-pl\n    metrics:\n    - type: main_score\n      value: 60.27\n    - type: map_at_1\n      value: 35.199000000000005\n    - type: map_at_10\n      value: 51.369\n    - type: map_at_100\n      value: 52.212\n    - type: map_at_1000\n      value: 52.28\n    - type: map_at_20\n      value: 51.864\n    - type: map_at_3\n      value: 48.446\n    - type: map_at_5\n      value: 50.302\n    - type: mrr_at_1\n      value: 70.39837947332883\n    - type: mrr_at_10\n      value: 76.8346141067273\n    - type: mrr_at_100\n      value: 77.10724392048137\n    - type: mrr_at_1000\n      value: 77.12037412892865\n    - type: mrr_at_20\n      value: 77.01061532947222\n    - type: mrr_at_3\n      value: 75.5908170155299\n    - type: mrr_at_5\n      value: 76.39095205941899\n    - type: nauc_map_at_1000_diff1\n      value: 24.701387884989117\n    - type: nauc_map_at_1000_max\n      value: 23.25553235642178\n    - type: nauc_map_at_1000_std\n      value: 7.1803506915661774\n    - type: nauc_map_at_100_diff1\n      value: 24.674498622483103\n    - type: nauc_map_at_100_max\n      value: 23.234948525052175\n    - type: nauc_map_at_100_std\n      value: 7.168677997105447\n    - type: nauc_map_at_10_diff1\n      value: 24.676025039755626\n    - type: nauc_map_at_10_max\n      value: 23.171971872726964\n    - type: nauc_map_at_10_std\n      value: 6.485610909852058\n    - type: nauc_map_at_1_diff1\n      value: 68.90178464319715\n    - type: nauc_map_at_1_max\n      value: 46.05537868917558\n    - type: nauc_map_at_1_std\n      value: 1.7658552480698708\n    - type: nauc_map_at_20_diff1\n      value: 24.69297151842494\n    - type: nauc_map_at_20_max\n      value: 23.213064691673637\n    - type: nauc_map_at_20_std\n      value: 6.9357946556849\n    - type: nauc_map_at_3_diff1\n      value: 26.279128947950507\n    - type: nauc_map_at_3_max\n      value: 23.929537354117922\n    - type: nauc_map_at_3_std\n      value: 4.625061565714759\n    - type: nauc_map_at_5_diff1\n      value: 25.04448959482816\n    - type: nauc_map_at_5_max\n      value: 23.432012857899338\n    - type: nauc_map_at_5_std\n      value: 5.845744681998008\n    - type: nauc_mrr_at_1000_diff1\n      value: 66.7503918108276\n    - type: nauc_mrr_at_1000_max\n      value: 48.42897342336844\n    - type: nauc_mrr_at_1000_std\n      value: 5.3097517971144415\n    - type: nauc_mrr_at_100_diff1\n      value: 66.74645215862695\n    - type: nauc_mrr_at_100_max\n      value: 48.4368663009989\n    - type: nauc_mrr_at_100_std\n      value: 5.322297898555188\n    - type: nauc_mrr_at_10_diff1\n      value: 66.69310166180729\n    - type: nauc_mrr_at_10_max\n      value: 48.475437698330225\n    - type: nauc_mrr_at_10_std\n      value: 5.258183461631702\n    - type: nauc_mrr_at_1_diff1\n      value: 68.90178464319715\n    - type: nauc_mrr_at_1_max\n      value: 46.05537868917558\n    - type: nauc_mrr_at_1_std\n      value: 1.7658552480698708\n    - type: nauc_mrr_at_20_diff1\n      value: 66.72000262431975\n    - type: nauc_mrr_at_20_max\n      value: 48.45593642981319\n    - type: nauc_mrr_at_20_std\n      value: 5.353665929072101\n    - type: nauc_mrr_at_3_diff1\n      value: 66.84936676396276\n    - type: nauc_mrr_at_3_max\n      value: 48.466611276778295\n    - type: nauc_mrr_at_3_std\n      value: 4.485810398557475\n    - type: nauc_mrr_at_5_diff1\n      value: 66.62362565394174\n    - type: nauc_mrr_at_5_max\n      value: 48.456431835482014\n    - type: nauc_mrr_at_5_std\n      value: 5.08482458391903\n    - type: nauc_ndcg_at_1000_diff1\n      value: 29.984825173719443\n    - type: nauc_ndcg_at_1000_max\n      value: 27.289179238639893\n    - type: nauc_ndcg_at_1000_std\n      value: 10.661480455527526\n    - type: nauc_ndcg_at_100_diff1\n      value: 29.322074257047877\n    - type: nauc_ndcg_at_100_max\n      value: 26.850650276220605\n    - type: nauc_ndcg_at_100_std\n      value: 10.599247982501902\n    - type: nauc_ndcg_at_10_diff1\n      value: 29.659909113886094\n    - type: nauc_ndcg_at_10_max\n      value: 26.836139599331005\n    - type: nauc_ndcg_at_10_std\n      value: 8.12844399452719\n    - type: nauc_ndcg_at_1_diff1\n      value: 68.90178464319715\n    - type: nauc_ndcg_at_1_max\n      value: 46.05537868917558\n    - type: nauc_ndcg_at_1_std\n      value: 1.7658552480698708\n    - type: nauc_ndcg_at_20_diff1\n      value: 29.510802214854294\n    - type: nauc_ndcg_at_20_max\n      value: 26.775562637730722\n    - type: nauc_ndcg_at_20_std\n      value: 9.341342661702363\n    - type: nauc_ndcg_at_3_diff1\n      value: 32.741885846292966\n    - type: nauc_ndcg_at_3_max\n      value: 28.44225108761343\n    - type: nauc_ndcg_at_3_std\n      value: 5.204440768465042\n    - type: nauc_ndcg_at_5_diff1\n      value: 30.57856348635919\n    - type: nauc_ndcg_at_5_max\n      value: 27.475007474301698\n    - type: nauc_ndcg_at_5_std\n      value: 6.961546044312487\n    - type: nauc_precision_at_1000_diff1\n      value: 0.002113156309413332\n    - type: nauc_precision_at_1000_max\n      value: 11.198242419541286\n    - type: nauc_precision_at_1000_std\n      value: 28.69676419166541\n    - type: nauc_precision_at_100_diff1\n      value: 3.6049575557782627\n    - type: nauc_precision_at_100_max\n      value: 12.499173524574791\n    - type: nauc_precision_at_100_std\n      value: 23.3755281004721\n    - type: nauc_precision_at_10_diff1\n      value: 10.922574784853193\n    - type: nauc_precision_at_10_max\n      value: 16.23221529562036\n    - type: nauc_precision_at_10_std\n      value: 12.45014808813857\n    - type: nauc_precision_at_1_diff1\n      value: 68.90178464319715\n    - type: nauc_precision_at_1_max\n      value: 46.05537868917558\n    - type: nauc_precision_at_1_std\n      value: 1.7658552480698708\n    - type: nauc_precision_at_20_diff1\n      value: 8.840710781302827\n    - type: nauc_precision_at_20_max\n      value: 14.804644554205524\n    - type: nauc_precision_at_20_std\n      value: 16.245009770815237\n    - type: nauc_precision_at_3_diff1\n      value: 19.447291487137573\n    - type: nauc_precision_at_3_max\n      value: 21.47123471597057\n    - type: nauc_precision_at_3_std\n      value: 6.441862800128802\n    - type: nauc_precision_at_5_diff1\n      value: 14.078545719721108\n    - type: nauc_precision_at_5_max\n      value: 18.468288046016387\n    - type: nauc_precision_at_5_std\n      value: 9.58650641691393\n    - type: nauc_recall_at_1000_diff1\n      value: 0.0021131563095336584\n    - type: nauc_recall_at_1000_max\n      value: 11.198242419541558\n    - type: nauc_recall_at_1000_std\n      value: 28.6967641916655\n    - type: nauc_recall_at_100_diff1\n      value: 3.6049575557781393\n    - type: nauc_recall_at_100_max\n      value: 12.499173524574765\n    - type: nauc_recall_at_100_std\n      value: 23.375528100472074\n    - type: nauc_recall_at_10_diff1\n      value: 10.922574784853168\n    - type: nauc_recall_at_10_max\n      value: 16.2322152956203\n    - type: nauc_recall_at_10_std\n      value: 12.450148088138535\n    - type: nauc_recall_at_1_diff1\n      value: 68.90178464319715\n    - type: nauc_recall_at_1_max\n      value: 46.05537868917558\n    - type: nauc_recall_at_1_std\n      value: 1.7658552480698708\n    - type: nauc_recall_at_20_diff1\n      value: 8.840710781302905\n    - type: nauc_recall_at_20_max\n      value: 14.804644554205515\n    - type: nauc_recall_at_20_std\n      value: 16.245009770815273\n    - type: nauc_recall_at_3_diff1\n      value: 19.447291487137498\n    - type: nauc_recall_at_3_max\n      value: 21.47123471597054\n    - type: nauc_recall_at_3_std\n      value: 6.441862800128763\n    - type: nauc_recall_at_5_diff1\n      value: 14.07854571972115\n    - type: nauc_recall_at_5_max\n      value: 18.468288046016337\n    - type: nauc_recall_at_5_std\n      value: 9.586506416913904\n    - type: ndcg_at_1\n      value: 70.39800000000001\n    - type: ndcg_at_10\n      value: 60.27\n    - type: ndcg_at_100\n      value: 63.400999999999996\n    - type: ndcg_at_1000\n      value: 64.847\n    - type: ndcg_at_20\n      value: 61.571\n    - type: ndcg_at_3\n      value: 55.875\n    - type: ndcg_at_5\n      value: 58.36599999999999\n    - type: precision_at_1\n      value: 70.39800000000001\n    - type: precision_at_10\n      value: 12.46\n    - type: precision_at_100\n      value: 1.493\n    - type: precision_at_1000\n      value: 0.169\n    - type: precision_at_20\n      value: 6.65\n    - type: precision_at_3\n      value: 35.062\n    - type: precision_at_5\n      value: 23.009\n    - type: recall_at_1\n      value: 35.199000000000005\n    - type: recall_at_10\n      value: 62.302\n    - type: recall_at_100\n      value: 74.666\n    - type: recall_at_1000\n      value: 84.355\n    - type: recall_at_20\n      value: 66.496\n    - type: recall_at_3\n      value: 52.593\n    - type: recall_at_5\n      value: 57.522\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB MSMARCO-PL (default)\n      revision: 8634c07806d5cce3a6138e260e59b81760a0a640\n      split: test\n      type: clarin-knext/msmarco-pl\n    metrics:\n    - type: main_score\n      value: 64.886\n    - type: map_at_1\n      value: 1.644\n    - type: map_at_10\n      value: 12.24\n    - type: map_at_100\n      value: 28.248\n    - type: map_at_1000\n      value: 33.506\n    - type: map_at_20\n      value: 17.497\n    - type: map_at_3\n      value: 4.9399999999999995\n    - type: map_at_5\n      value: 8.272\n    - type: mrr_at_1\n      value: 83.72093023255815\n    - type: mrr_at_10\n      value: 91.08527131782945\n    - type: mrr_at_100\n      value: 91.08527131782945\n    - type: mrr_at_1000\n      value: 91.08527131782945\n    - type: mrr_at_20\n      value: 91.08527131782945\n    - type: mrr_at_3\n      value: 91.08527131782945\n    - type: mrr_at_5\n      value: 91.08527131782945\n    - type: nauc_map_at_1000_diff1\n      value: -36.428271627303424\n    - type: nauc_map_at_1000_max\n      value: 44.87615127218638\n    - type: nauc_map_at_1000_std\n      value: 67.92696808824724\n    - type: nauc_map_at_100_diff1\n      value: -28.11674206786188\n    - type: nauc_map_at_100_max\n      value: 36.422779766334955\n    - type: nauc_map_at_100_std\n      value: 49.99876313755116\n    - type: nauc_map_at_10_diff1\n      value: -5.838593619806058\n    - type: nauc_map_at_10_max\n      value: 11.026519190509742\n    - type: nauc_map_at_10_std\n      value: 2.5268752263522045\n    - type: nauc_map_at_1_diff1\n      value: 17.897907271073016\n    - type: nauc_map_at_1_max\n      value: 12.229062762540844\n    - type: nauc_map_at_1_std\n      value: -4.088830895573149\n    - type: nauc_map_at_20_diff1\n      value: -13.871097716255626\n    - type: nauc_map_at_20_max\n      value: 19.291271635609533\n    - type: nauc_map_at_20_std\n      value: 16.745335606507826\n    - type: nauc_map_at_3_diff1\n      value: 4.425238457033843\n    - type: nauc_map_at_3_max\n      value: 4.611864744680824\n    - type: nauc_map_at_3_std\n      value: -8.986916608582863\n    - type: nauc_map_at_5_diff1\n      value: -6.254849256920095\n    - type: nauc_map_at_5_max\n      value: 2.729437079919823\n    - type: nauc_map_at_5_std\n      value: -7.235906279913092\n    - type: nauc_mrr_at_1000_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_1000_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_1000_std\n      value: 56.345086428353575\n    - type: nauc_mrr_at_100_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_100_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_100_std\n      value: 56.345086428353575\n    - type: nauc_mrr_at_10_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_10_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_10_std\n      value: 56.345086428353575\n    - type: nauc_mrr_at_1_diff1\n      value: 56.55126663944154\n    - type: nauc_mrr_at_1_max\n      value: 66.37014285522565\n    - type: nauc_mrr_at_1_std\n      value: 53.2508271389779\n    - type: nauc_mrr_at_20_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_20_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_20_std\n      value: 56.345086428353575\n    - type: nauc_mrr_at_3_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_3_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_3_std\n      value: 56.345086428353575\n    - type: nauc_mrr_at_5_diff1\n      value: 52.18669104947672\n    - type: nauc_mrr_at_5_max\n      value: 68.26259125411818\n    - type: nauc_mrr_at_5_std\n      value: 56.345086428353575\n    - type: nauc_ndcg_at_1000_diff1\n      value: -19.06422926483731\n    - type: nauc_ndcg_at_1000_max\n      value: 56.30853514590265\n    - type: nauc_ndcg_at_1000_std\n      value: 70.30810947505557\n    - type: nauc_ndcg_at_100_diff1\n      value: -25.72587586459692\n    - type: nauc_ndcg_at_100_max\n      value: 51.433781241604194\n    - type: nauc_ndcg_at_100_std\n      value: 68.37678512652792\n    - type: nauc_ndcg_at_10_diff1\n      value: -23.21198108212602\n    - type: nauc_ndcg_at_10_max\n      value: 43.5450720846516\n    - type: nauc_ndcg_at_10_std\n      value: 48.78307907005605\n    - type: nauc_ndcg_at_1_diff1\n      value: 44.00179301267447\n    - type: nauc_ndcg_at_1_max\n      value: 48.202370455680395\n    - type: nauc_ndcg_at_1_std\n      value: 25.69655992704088\n    - type: nauc_ndcg_at_20_diff1\n      value: -33.88168753446507\n    - type: nauc_ndcg_at_20_max\n      value: 45.16199742613164\n    - type: nauc_ndcg_at_20_std\n      value: 61.87098383164902\n    - type: nauc_ndcg_at_3_diff1\n      value: 11.19174449544048\n    - type: nauc_ndcg_at_3_max\n      value: 44.34069860560555\n    - type: nauc_ndcg_at_3_std\n      value: 27.451258369798115\n    - type: nauc_ndcg_at_5_diff1\n      value: -7.186520929432436\n    - type: nauc_ndcg_at_5_max\n      value: 43.41869981139378\n    - type: nauc_ndcg_at_5_std\n      value: 34.89898115995178\n    - type: nauc_precision_at_1000_diff1\n      value: -34.43998154563451\n    - type: nauc_precision_at_1000_max\n      value: 29.172655907480372\n    - type: nauc_precision_at_1000_std\n      value: 65.15824469614837\n    - type: nauc_precision_at_100_diff1\n      value: -37.82409643259692\n    - type: nauc_precision_at_100_max\n      value: 38.24986991317909\n    - type: nauc_precision_at_100_std\n      value: 72.74768183105327\n    - type: nauc_precision_at_10_diff1\n      value: -32.21556182780535\n    - type: nauc_precision_at_10_max\n      value: 34.27170432382651\n    - type: nauc_precision_at_10_std\n      value: 58.358255004394664\n    - type: nauc_precision_at_1_diff1\n      value: 56.55126663944154\n    - type: nauc_precision_at_1_max\n      value: 66.37014285522565\n    - type: nauc_precision_at_1_std\n      value: 53.2508271389779\n    - type: nauc_precision_at_20_diff1\n      value: -40.18751579026395\n    - type: nauc_precision_at_20_max\n      value: 33.960783153758896\n    - type: nauc_precision_at_20_std\n      value: 65.42918390184195\n    - type: nauc_precision_at_3_diff1\n      value: -7.073870209006578\n    - type: nauc_precision_at_3_max\n      value: 50.81535269862325\n    - type: nauc_precision_at_3_std\n      value: 59.248681565955685\n    - type: nauc_precision_at_5_diff1\n      value: -31.136580596983876\n    - type: nauc_precision_at_5_max\n      value: 45.88147792380426\n    - type: nauc_precision_at_5_std\n      value: 67.46814230928243\n    - type: nauc_recall_at_1000_diff1\n      value: -23.15699999594577\n    - type: nauc_recall_at_1000_max\n      value: 39.77277799761876\n    - type: nauc_recall_at_1000_std\n      value: 60.326168012901114\n    - type: nauc_recall_at_100_diff1\n      value: -21.636664823598498\n    - type: nauc_recall_at_100_max\n      value: 31.104969346131583\n    - type: nauc_recall_at_100_std\n      value: 38.811686891592096\n    - type: nauc_recall_at_10_diff1\n      value: -10.542765625053569\n    - type: nauc_recall_at_10_max\n      value: 2.043876058107446\n    - type: nauc_recall_at_10_std\n      value: -5.578449908984766\n    - type: nauc_recall_at_1_diff1\n      value: 17.897907271073016\n    - type: nauc_recall_at_1_max\n      value: 12.229062762540844\n    - type: nauc_recall_at_1_std\n      value: -4.088830895573149\n    - type: nauc_recall_at_20_diff1\n      value: -15.132909355710103\n    - type: nauc_recall_at_20_max\n      value: 12.659765287241065\n    - type: nauc_recall_at_20_std\n      value: 8.277887800815819\n    - type: nauc_recall_at_3_diff1\n      value: -3.1975017812715016\n    - type: nauc_recall_at_3_max\n      value: -3.5539857085038538\n    - type: nauc_recall_at_3_std\n      value: -14.712102851318118\n    - type: nauc_recall_at_5_diff1\n      value: -14.040507717380743\n    - type: nauc_recall_at_5_max\n      value: -6.126912150131701\n    - type: nauc_recall_at_5_std\n      value: -13.821624015640355\n    - type: ndcg_at_1\n      value: 71.318\n    - type: ndcg_at_10\n      value: 64.886\n    - type: ndcg_at_100\n      value: 53.187\n    - type: ndcg_at_1000\n      value: 59.897999999999996\n    - type: ndcg_at_20\n      value: 58.96\n    - type: ndcg_at_3\n      value: 69.736\n    - type: ndcg_at_5\n      value: 70.14099999999999\n    - type: precision_at_1\n      value: 83.721\n    - type: precision_at_10\n      value: 71.163\n    - type: precision_at_100\n      value: 29.465000000000003\n    - type: precision_at_1000\n      value: 5.665\n    - type: precision_at_20\n      value: 57.791000000000004\n    - type: precision_at_3\n      value: 82.171\n    - type: precision_at_5\n      value: 81.86\n    - type: recall_at_1\n      value: 1.644\n    - type: recall_at_10\n      value: 14.238000000000001\n    - type: recall_at_100\n      value: 39.831\n    - type: recall_at_1000\n      value: 64.057\n    - type: recall_at_20\n      value: 21.021\n    - type: recall_at_3\n      value: 5.53\n    - type: recall_at_5\n      value: 9.623\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB NFCorpus-PL (default)\n      revision: 9a6f9567fda928260afed2de480d79c98bf0bec0\n      split: test\n      type: clarin-knext/nfcorpus-pl\n    metrics:\n    - type: main_score\n      value: 31.391000000000002\n    - type: map_at_1\n      value: 4.163\n    - type: map_at_10\n      value: 10.744\n    - type: map_at_100\n      value: 14.038999999999998\n    - type: map_at_1000\n      value: 15.434999999999999\n    - type: map_at_20\n      value: 12.16\n    - type: map_at_3\n      value: 7.614999999999999\n    - type: map_at_5\n      value: 9.027000000000001\n    - type: mrr_at_1\n      value: 39.0092879256966\n    - type: mrr_at_10\n      value: 48.69809327239668\n    - type: mrr_at_100\n      value: 49.20788148442068\n    - type: mrr_at_1000\n      value: 49.25509336494706\n    - type: mrr_at_20\n      value: 48.99606551850896\n    - type: mrr_at_3\n      value: 46.284829721362236\n    - type: mrr_at_5\n      value: 47.77089783281735\n    - type: nauc_map_at_1000_diff1\n      value: 22.75421477116417\n    - type: nauc_map_at_1000_max\n      value: 49.242283787799046\n    - type: nauc_map_at_1000_std\n      value: 29.056888272331832\n    - type: nauc_map_at_100_diff1\n      value: 23.585977398585594\n    - type: nauc_map_at_100_max\n      value: 48.25845199409498\n    - type: nauc_map_at_100_std\n      value: 24.944264511223693\n    - type: nauc_map_at_10_diff1\n      value: 27.386613094780255\n    - type: nauc_map_at_10_max\n      value: 41.52415346691586\n    - type: nauc_map_at_10_std\n      value: 12.93872448563755\n    - type: nauc_map_at_1_diff1\n      value: 46.78688143865053\n    - type: nauc_map_at_1_max\n      value: 37.20408843995871\n    - type: nauc_map_at_1_std\n      value: 4.383444959401098\n    - type: nauc_map_at_20_diff1\n      value: 25.590969047740288\n    - type: nauc_map_at_20_max\n      value: 44.57109307999418\n    - type: nauc_map_at_20_std\n      value: 16.45855141821407\n    - type: nauc_map_at_3_diff1\n      value: 36.30017108362863\n    - type: nauc_map_at_3_max\n      value: 34.66149613991648\n    - type: nauc_map_at_3_std\n      value: 5.67985905078467\n    - type: nauc_map_at_5_diff1\n      value: 31.157644795417223\n    - type: nauc_map_at_5_max\n      value: 37.274738661636825\n    - type: nauc_map_at_5_std\n      value: 8.70088872394168\n    - type: nauc_mrr_at_1000_diff1\n      value: 25.638564218157384\n    - type: nauc_mrr_at_1000_max\n      value: 57.77788270285353\n    - type: nauc_mrr_at_1000_std\n      value: 43.507586592911274\n    - type: nauc_mrr_at_100_diff1\n      value: 25.662002580561584\n    - type: nauc_mrr_at_100_max\n      value: 57.80578394278584\n    - type: nauc_mrr_at_100_std\n      value: 43.543905743986635\n    - type: nauc_mrr_at_10_diff1\n      value: 25.426034796339835\n    - type: nauc_mrr_at_10_max\n      value: 57.68443186258669\n    - type: nauc_mrr_at_10_std\n      value: 43.438009108331215\n    - type: nauc_mrr_at_1_diff1\n      value: 26.073028156311075\n    - type: nauc_mrr_at_1_max\n      value: 52.11817916720053\n    - type: nauc_mrr_at_1_std\n      value: 37.41073893153695\n    - type: nauc_mrr_at_20_diff1\n      value: 25.548645553336147\n    - type: nauc_mrr_at_20_max\n      value: 57.78552760401915\n    - type: nauc_mrr_at_20_std\n      value: 43.521687428822325\n    - type: nauc_mrr_at_3_diff1\n      value: 25.72662577397805\n    - type: nauc_mrr_at_3_max\n      value: 56.891263536265605\n    - type: nauc_mrr_at_3_std\n      value: 41.384872305390104\n    - type: nauc_mrr_at_5_diff1\n      value: 25.552211551655386\n    - type: nauc_mrr_at_5_max\n      value: 57.976813828353926\n    - type: nauc_mrr_at_5_std\n      value: 43.504564461855544\n    - type: nauc_ndcg_at_1000_diff1\n      value: 23.456158044182757\n    - type: nauc_ndcg_at_1000_max\n      value: 60.05411773552709\n    - type: nauc_ndcg_at_1000_std\n      value: 47.857510017262584\n    - type: nauc_ndcg_at_100_diff1\n      value: 19.711635700390772\n    - type: nauc_ndcg_at_100_max\n      value: 56.178746740470665\n    - type: nauc_ndcg_at_100_std\n      value: 42.36829180286942\n    - type: nauc_ndcg_at_10_diff1\n      value: 18.364428967788413\n    - type: nauc_ndcg_at_10_max\n      value: 54.38372506578223\n    - type: nauc_ndcg_at_10_std\n      value: 41.75765411340369\n    - type: nauc_ndcg_at_1_diff1\n      value: 26.571093272640773\n    - type: nauc_ndcg_at_1_max\n      value: 51.061788341958284\n    - type: nauc_ndcg_at_1_std\n      value: 36.514987974075986\n    - type: nauc_ndcg_at_20_diff1\n      value: 18.345487193027697\n    - type: nauc_ndcg_at_20_max\n      value: 54.62621882656994\n    - type: nauc_ndcg_at_20_std\n      value: 41.42835554714241\n    - type: nauc_ndcg_at_3_diff1\n      value: 23.260105658139025\n    - type: nauc_ndcg_at_3_max\n      value: 52.07747385334546\n    - type: nauc_ndcg_at_3_std\n      value: 36.91985577837284\n    - type: nauc_ndcg_at_5_diff1\n      value: 20.40428109665566\n    - type: nauc_ndcg_at_5_max\n      value: 53.52015347884604\n    - type: nauc_ndcg_at_5_std\n      value: 39.46008849580017\n    - type: nauc_precision_at_1000_diff1\n      value: -7.3487344916380035\n    - type: nauc_precision_at_1000_max\n      value: 16.58045221394852\n    - type: nauc_precision_at_1000_std\n      value: 38.94030932397075\n    - type: nauc_precision_at_100_diff1\n      value: -5.257743986683922\n    - type: nauc_precision_at_100_max\n      value: 34.43071687475306\n    - type: nauc_precision_at_100_std\n      value: 53.499519170670474\n    - type: nauc_precision_at_10_diff1\n      value: 2.385136433119139\n    - type: nauc_precision_at_10_max\n      value: 47.210743878631064\n    - type: nauc_precision_at_10_std\n      value: 47.22767704186548\n    - type: nauc_precision_at_1_diff1\n      value: 26.073028156311075\n    - type: nauc_precision_at_1_max\n      value: 52.11817916720053\n    - type: nauc_precision_at_1_std\n      value: 37.41073893153695\n    - type: nauc_precision_at_20_diff1\n      value: -0.3531531127238474\n    - type: nauc_precision_at_20_max\n      value: 44.78044604856974\n    - type: nauc_precision_at_20_std\n      value: 49.532804150743615\n    - type: nauc_precision_at_3_diff1\n      value: 15.350050569991447\n    - type: nauc_precision_at_3_max\n      value: 51.01572315596549\n    - type: nauc_precision_at_3_std\n      value: 38.801125728413155\n    - type: nauc_precision_at_5_diff1\n      value: 9.109003666144694\n    - type: nauc_precision_at_5_max\n      value: 50.935269774898494\n    - type: nauc_precision_at_5_std\n      value: 43.323548180559676\n    - type: nauc_recall_at_1000_diff1\n      value: 16.64743647648886\n    - type: nauc_recall_at_1000_max\n      value: 38.46012283772285\n    - type: nauc_recall_at_1000_std\n      value: 36.02016164796441\n    - type: nauc_recall_at_100_diff1\n      value: 14.005834785186744\n    - type: nauc_recall_at_100_max\n      value: 37.70026105513647\n    - type: nauc_recall_at_100_std\n      value: 27.085222642129697\n    - type: nauc_recall_at_10_diff1\n      value: 21.204106627422632\n    - type: nauc_recall_at_10_max\n      value: 36.737624881893424\n    - type: nauc_recall_at_10_std\n      value: 13.755054514272702\n    - type: nauc_recall_at_1_diff1\n      value: 46.78688143865053\n    - type: nauc_recall_at_1_max\n      value: 37.20408843995871\n    - type: nauc_recall_at_1_std\n      value: 4.383444959401098\n    - type: nauc_recall_at_20_diff1\n      value: 19.740977611421933\n    - type: nauc_recall_at_20_max\n      value: 39.21908969539783\n    - type: nauc_recall_at_20_std\n      value: 16.560269670318494\n    - type: nauc_recall_at_3_diff1\n      value: 32.189359545367815\n    - type: nauc_recall_at_3_max\n      value: 31.693634445562758\n    - type: nauc_recall_at_3_std\n      value: 6.246326281543587\n    - type: nauc_recall_at_5_diff1\n      value: 25.51586860499901\n    - type: nauc_recall_at_5_max\n      value: 33.15934725342885\n    - type: nauc_recall_at_5_std\n      value: 9.677778511696705\n    - type: ndcg_at_1\n      value: 37.307\n    - type: ndcg_at_10\n      value: 31.391000000000002\n    - type: ndcg_at_100\n      value: 28.877999999999997\n    - type: ndcg_at_1000\n      value: 37.16\n    - type: ndcg_at_20\n      value: 29.314\n    - type: ndcg_at_3\n      value: 35.405\n    - type: ndcg_at_5\n      value: 33.922999999999995\n    - type: precision_at_1\n      value: 39.009\n    - type: precision_at_10\n      value: 24.52\n    - type: precision_at_100\n      value: 7.703\n    - type: precision_at_1000\n      value: 2.04\n    - type: precision_at_20\n      value: 18.08\n    - type: precision_at_3\n      value: 34.469\n    - type: precision_at_5\n      value: 30.712\n    - type: recall_at_1\n      value: 4.163\n    - type: recall_at_10\n      value: 15.015999999999998\n    - type: recall_at_100\n      value: 30.606\n    - type: recall_at_1000\n      value: 59.606\n    - type: recall_at_20\n      value: 19.09\n    - type: recall_at_3\n      value: 9.139\n    - type: recall_at_5\n      value: 11.477\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB NQ-PL (default)\n      revision: f171245712cf85dd4700b06bef18001578d0ca8d\n      split: test\n      type: clarin-knext/nq-pl\n    metrics:\n    - type: main_score\n      value: 54.017\n    - type: map_at_1\n      value: 34.193\n    - type: map_at_10\n      value: 47.497\n    - type: map_at_100\n      value: 48.441\n    - type: map_at_1000\n      value: 48.481\n    - type: map_at_20\n      value: 48.093\n    - type: map_at_3\n      value: 44.017\n    - type: map_at_5\n      value: 46.111000000000004\n    - type: mrr_at_1\n      value: 37.949015063731174\n    - type: mrr_at_10\n      value: 49.915772315105954\n    - type: mrr_at_100\n      value: 50.62841255829997\n    - type: mrr_at_1000\n      value: 50.656773027666745\n    - type: mrr_at_20\n      value: 50.37785276657083\n    - type: mrr_at_3\n      value: 46.98725376593267\n    - type: mrr_at_5\n      value: 48.763035921205066\n    - type: nauc_map_at_1000_diff1\n      value: 39.5632191792873\n    - type: nauc_map_at_1000_max\n      value: 37.4728247053629\n    - type: nauc_map_at_1000_std\n      value: 5.742498414663762\n    - type: nauc_map_at_100_diff1\n      value: 39.555570352061906\n    - type: nauc_map_at_100_max\n      value: 37.497880976847334\n    - type: nauc_map_at_100_std\n      value: 5.7798021019465375\n    - type: nauc_map_at_10_diff1\n      value: 39.5423723444454\n    - type: nauc_map_at_10_max\n      value: 37.41661971723365\n    - type: nauc_map_at_10_std\n      value: 5.2378002164144695\n    - type: nauc_map_at_1_diff1\n      value: 41.52697034146981\n    - type: nauc_map_at_1_max\n      value: 28.558995576942863\n    - type: nauc_map_at_1_std\n      value: 0.13094542859192052\n    - type: nauc_map_at_20_diff1\n      value: 39.55484628943701\n    - type: nauc_map_at_20_max\n      value: 37.5247794933719\n    - type: nauc_map_at_20_std\n      value: 5.702881342279231\n    - type: nauc_map_at_3_diff1\n      value: 39.949323925425325\n    - type: nauc_map_at_3_max\n      value: 35.770298168901924\n    - type: nauc_map_at_3_std\n      value: 2.9127112432479874\n    - type: nauc_map_at_5_diff1\n      value: 39.768310617004545\n    - type: nauc_map_at_5_max\n      value: 37.1549191664796\n    - type: nauc_map_at_5_std\n      value: 4.4681285748269515\n    - type: nauc_mrr_at_1000_diff1\n      value: 39.14001746706457\n    - type: nauc_mrr_at_1000_max\n      value: 37.477376518267775\n    - type: nauc_mrr_at_1000_std\n      value: 6.8088891531621565\n    - type: nauc_mrr_at_100_diff1\n      value: 39.13054707413684\n    - type: nauc_mrr_at_100_max\n      value: 37.498126443766274\n    - type: nauc_mrr_at_100_std\n      value: 6.839411380129971\n    - type: nauc_mrr_at_10_diff1\n      value: 39.09764730048156\n    - type: nauc_mrr_at_10_max\n      value: 37.58593798217306\n    - type: nauc_mrr_at_10_std\n      value: 6.713795164982413\n    - type: nauc_mrr_at_1_diff1\n      value: 41.581599918664075\n    - type: nauc_mrr_at_1_max\n      value: 31.500589231378722\n    - type: nauc_mrr_at_1_std\n      value: 2.059116370339438\n    - type: nauc_mrr_at_20_diff1\n      value: 39.09011023988447\n    - type: nauc_mrr_at_20_max\n      value: 37.55856008791344\n    - type: nauc_mrr_at_20_std\n      value: 6.847165397615844\n    - type: nauc_mrr_at_3_diff1\n      value: 39.382542043738\n    - type: nauc_mrr_at_3_max\n      value: 36.49265363659468\n    - type: nauc_mrr_at_3_std\n      value: 4.759157976438336\n    - type: nauc_mrr_at_5_diff1\n      value: 39.304826333759976\n    - type: nauc_mrr_at_5_max\n      value: 37.46326016736024\n    - type: nauc_mrr_at_5_std\n      value: 6.122608305766621\n    - type: nauc_ndcg_at_1000_diff1\n      value: 38.568500038453266\n    - type: nauc_ndcg_at_1000_max\n      value: 39.799710882413166\n    - type: nauc_ndcg_at_1000_std\n      value: 9.357010223096639\n    - type: nauc_ndcg_at_100_diff1\n      value: 38.38026091343228\n    - type: nauc_ndcg_at_100_max\n      value: 40.48398173542486\n    - type: nauc_ndcg_at_100_std\n      value: 10.373054013302214\n    - type: nauc_ndcg_at_10_diff1\n      value: 38.27340980909964\n    - type: nauc_ndcg_at_10_max\n      value: 40.35241649744093\n    - type: nauc_ndcg_at_10_std\n      value: 8.579139930345168\n    - type: nauc_ndcg_at_1_diff1\n      value: 41.581599918664075\n    - type: nauc_ndcg_at_1_max\n      value: 31.500589231378722\n    - type: nauc_ndcg_at_1_std\n      value: 2.059116370339438\n    - type: nauc_ndcg_at_20_diff1\n      value: 38.26453028884807\n    - type: nauc_ndcg_at_20_max\n      value: 40.70517858426641\n    - type: nauc_ndcg_at_20_std\n      value: 9.987693876137905\n    - type: nauc_ndcg_at_3_diff1\n      value: 39.2078971733273\n    - type: nauc_ndcg_at_3_max\n      value: 37.48672195565316\n    - type: nauc_ndcg_at_3_std\n      value: 4.051464994659221\n    - type: nauc_ndcg_at_5_diff1\n      value: 38.883693595665285\n    - type: nauc_ndcg_at_5_max\n      value: 39.763115634437135\n    - type: nauc_ndcg_at_5_std\n      value: 6.738980451582073\n    - type: nauc_precision_at_1000_diff1\n      value: -7.223215910619012\n    - type: nauc_precision_at_1000_max\n      value: 13.075844604892161\n    - type: nauc_precision_at_1000_std\n      value: 19.864336920890107\n    - type: nauc_precision_at_100_diff1\n      value: 1.3305994810812418\n    - type: nauc_precision_at_100_max\n      value: 25.9219108557104\n    - type: nauc_precision_at_100_std\n      value: 27.5076605928207\n    - type: nauc_precision_at_10_diff1\n      value: 18.441551484970326\n    - type: nauc_precision_at_10_max\n      value: 39.85995330437054\n    - type: nauc_precision_at_10_std\n      value: 20.561269077428914\n    - type: nauc_precision_at_1_diff1\n      value: 41.581599918664075\n    - type: nauc_precision_at_1_max\n      value: 31.500589231378722\n    - type: nauc_precision_at_1_std\n      value: 2.059116370339438\n    - type: nauc_precision_at_20_diff1\n      value: 12.579593891480531\n    - type: nauc_precision_at_20_max\n      value: 36.620221830588775\n    - type: nauc_precision_at_20_std\n      value: 26.40364876775059\n    - type: nauc_precision_at_3_diff1\n      value: 30.158859294487073\n    - type: nauc_precision_at_3_max\n      value: 41.168215766389174\n    - type: nauc_precision_at_3_std\n      value: 9.44345004450809\n    - type: nauc_precision_at_5_diff1\n      value: 25.438624678672785\n    - type: nauc_precision_at_5_max\n      value: 42.72802023518524\n    - type: nauc_precision_at_5_std\n      value: 15.357657388511099\n    - type: nauc_recall_at_1000_diff1\n      value: 24.987564782718003\n    - type: nauc_recall_at_1000_max\n      value: 70.508416373353\n    - type: nauc_recall_at_1000_std\n      value: 69.75092280398808\n    - type: nauc_recall_at_100_diff1\n      value: 29.504202856421397\n    - type: nauc_recall_at_100_max\n      value: 63.41356585545318\n    - type: nauc_recall_at_100_std\n      value: 50.09250954437847\n    - type: nauc_recall_at_10_diff1\n      value: 32.355776022971774\n    - type: nauc_recall_at_10_max\n      value: 49.47121901667283\n    - type: nauc_recall_at_10_std\n      value: 19.418439406631244\n    - type: nauc_recall_at_1_diff1\n      value: 41.52697034146981\n    - type: nauc_recall_at_1_max\n      value: 28.558995576942863\n    - type: nauc_recall_at_1_std\n      value: 0.13094542859192052\n    - type: nauc_recall_at_20_diff1\n      value: 31.57334731023589\n    - type: nauc_recall_at_20_max\n      value: 54.06567225197383\n    - type: nauc_recall_at_20_std\n      value: 29.222029720570468\n    - type: nauc_recall_at_3_diff1\n      value: 36.45033533275773\n    - type: nauc_recall_at_3_max\n      value: 40.39529713780803\n    - type: nauc_recall_at_3_std\n      value: 5.21893897772794\n    - type: nauc_recall_at_5_diff1\n      value: 35.18471678478859\n    - type: nauc_recall_at_5_max\n      value: 46.20100816867823\n    - type: nauc_recall_at_5_std\n      value: 11.94481894633221\n    - type: ndcg_at_1\n      value: 37.949\n    - type: ndcg_at_10\n      value: 54.017\n    - type: ndcg_at_100\n      value: 58.126\n    - type: ndcg_at_1000\n      value: 59.073\n    - type: ndcg_at_20\n      value: 55.928\n    - type: ndcg_at_3\n      value: 47.494\n    - type: ndcg_at_5\n      value: 50.975\n    - type: precision_at_1\n      value: 37.949\n    - type: precision_at_10\n      value: 8.450000000000001\n    - type: precision_at_100\n      value: 1.083\n    - type: precision_at_1000\n      value: 0.117\n    - type: precision_at_20\n      value: 4.689\n    - type: precision_at_3\n      value: 21.051000000000002\n    - type: precision_at_5\n      value: 14.664\n    - type: recall_at_1\n      value: 34.193\n    - type: recall_at_10\n      value: 71.357\n    - type: recall_at_100\n      value: 89.434\n    - type: recall_at_1000\n      value: 96.536\n    - type: recall_at_20\n      value: 78.363\n    - type: recall_at_3\n      value: 54.551\n    - type: recall_at_5\n      value: 62.543000000000006\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB Quora-PL (default)\n      revision: 0be27e93455051e531182b85e85e425aba12e9d4\n      split: test\n      type: clarin-knext/quora-pl\n    metrics:\n    - type: main_score\n      value: 84.114\n    - type: map_at_1\n      value: 65.848\n    - type: map_at_10\n      value: 79.85900000000001\n    - type: map_at_100\n      value: 80.582\n    - type: map_at_1000\n      value: 80.60300000000001\n    - type: map_at_20\n      value: 80.321\n    - type: map_at_3\n      value: 76.741\n    - type: map_at_5\n      value: 78.72200000000001\n    - type: mrr_at_1\n      value: 75.97\n    - type: mrr_at_10\n      value: 83.04630158730119\n    - type: mrr_at_100\n      value: 83.22785731032968\n    - type: mrr_at_1000\n      value: 83.23123717623899\n    - type: mrr_at_20\n      value: 83.17412021320565\n    - type: mrr_at_3\n      value: 81.83333333333287\n    - type: mrr_at_5\n      value: 82.61933333333275\n    - type: nauc_map_at_1000_diff1\n      value: 73.26316553371083\n    - type: nauc_map_at_1000_max\n      value: 27.92567859085245\n    - type: nauc_map_at_1000_std\n      value: -47.477909533360446\n    - type: nauc_map_at_100_diff1\n      value: 73.2690602807223\n    - type: nauc_map_at_100_max\n      value: 27.915868327849996\n    - type: nauc_map_at_100_std\n      value: -47.525777766107595\n    - type: nauc_map_at_10_diff1\n      value: 73.45464428464894\n    - type: nauc_map_at_10_max\n      value: 27.451611487246296\n    - type: nauc_map_at_10_std\n      value: -49.35818715843809\n    - type: nauc_map_at_1_diff1\n      value: 77.29690208952982\n    - type: nauc_map_at_1_max\n      value: 19.839875762282293\n    - type: nauc_map_at_1_std\n      value: -45.355684654708284\n    - type: nauc_map_at_20_diff1\n      value: 73.35102731979796\n    - type: nauc_map_at_20_max\n      value: 27.741506490134583\n    - type: nauc_map_at_20_std\n      value: -48.22006207310331\n    - type: nauc_map_at_3_diff1\n      value: 73.94878241064137\n    - type: nauc_map_at_3_max\n      value: 24.761321386766728\n    - type: nauc_map_at_3_std\n      value: -51.20638883618126\n    - type: nauc_map_at_5_diff1\n      value: 73.66143558047698\n    - type: nauc_map_at_5_max\n      value: 26.53483405013543\n    - type: nauc_map_at_5_std\n      value: -50.697541279640056\n    - type: nauc_mrr_at_1000_diff1\n      value: 73.84632320009759\n    - type: nauc_mrr_at_1000_max\n      value: 30.50182733610048\n    - type: nauc_mrr_at_1000_std\n      value: -44.3021647995251\n    - type: nauc_mrr_at_100_diff1\n      value: 73.84480792662302\n    - type: nauc_mrr_at_100_max\n      value: 30.50749424571614\n    - type: nauc_mrr_at_100_std\n      value: -44.29615086388113\n    - type: nauc_mrr_at_10_diff1\n      value: 73.79442772949346\n    - type: nauc_mrr_at_10_max\n      value: 30.55724252219984\n    - type: nauc_mrr_at_10_std\n      value: -44.50997069462057\n    - type: nauc_mrr_at_1_diff1\n      value: 75.23369827945945\n    - type: nauc_mrr_at_1_max\n      value: 29.20073967447664\n    - type: nauc_mrr_at_1_std\n      value: -43.1920147658285\n    - type: nauc_mrr_at_20_diff1\n      value: 73.82731678072307\n    - type: nauc_mrr_at_20_max\n      value: 30.566328605497667\n    - type: nauc_mrr_at_20_std\n      value: -44.24683607643705\n    - type: nauc_mrr_at_3_diff1\n      value: 73.61997576749954\n    - type: nauc_mrr_at_3_max\n      value: 30.150393853381917\n    - type: nauc_mrr_at_3_std\n      value: -44.96847297506626\n    - type: nauc_mrr_at_5_diff1\n      value: 73.69084310616132\n    - type: nauc_mrr_at_5_max\n      value: 30.578033703441125\n    - type: nauc_mrr_at_5_std\n      value: -44.74920746066566\n    - type: nauc_ndcg_at_1000_diff1\n      value: 72.89349862557452\n    - type: nauc_ndcg_at_1000_max\n      value: 29.824725190462086\n    - type: nauc_ndcg_at_1000_std\n      value: -44.96284395063211\n    - type: nauc_ndcg_at_100_diff1\n      value: 72.85212753715273\n    - type: nauc_ndcg_at_100_max\n      value: 29.933114207845605\n    - type: nauc_ndcg_at_100_std\n      value: -44.944225570663754\n    - type: nauc_ndcg_at_10_diff1\n      value: 72.80576740454528\n    - type: nauc_ndcg_at_10_max\n      value: 29.16829118320828\n    - type: nauc_ndcg_at_10_std\n      value: -48.149473740079614\n    - type: nauc_ndcg_at_1_diff1\n      value: 75.00032534968587\n    - type: nauc_ndcg_at_1_max\n      value: 29.61849062038547\n    - type: nauc_ndcg_at_1_std\n      value: -42.560207043864054\n    - type: nauc_ndcg_at_20_diff1\n      value: 72.88440406302502\n    - type: nauc_ndcg_at_20_max\n      value: 29.65496676092656\n    - type: nauc_ndcg_at_20_std\n      value: -46.21238462167732\n    - type: nauc_ndcg_at_3_diff1\n      value: 72.37916962766987\n    - type: nauc_ndcg_at_3_max\n      value: 27.125094834547586\n    - type: nauc_ndcg_at_3_std\n      value: -48.62942991399391\n    - type: nauc_ndcg_at_5_diff1\n      value: 72.57017330527658\n    - type: nauc_ndcg_at_5_max\n      value: 28.470485561757254\n    - type: nauc_ndcg_at_5_std\n      value: -49.07593345591059\n    - type: nauc_precision_at_1000_diff1\n      value: -41.67915575853946\n    - type: nauc_precision_at_1000_max\n      value: 1.2012264478568844\n    - type: nauc_precision_at_1000_std\n      value: 44.723834559400466\n    - type: nauc_precision_at_100_diff1\n      value: -40.45196679236971\n    - type: nauc_precision_at_100_max\n      value: 2.3525450401714894\n    - type: nauc_precision_at_100_std\n      value: 43.7092529413952\n    - type: nauc_precision_at_10_diff1\n      value: -30.256026923068767\n    - type: nauc_precision_at_10_max\n      value: 8.313422052132559\n    - type: nauc_precision_at_10_std\n      value: 25.929372356449694\n    - type: nauc_precision_at_1_diff1\n      value: 75.00032534968587\n    - type: nauc_precision_at_1_max\n      value: 29.61849062038547\n    - type: nauc_precision_at_1_std\n      value: -42.560207043864054\n    - type: nauc_precision_at_20_diff1\n      value: -35.61971069986584\n    - type: nauc_precision_at_20_max\n      value: 5.4664303079116765\n    - type: nauc_precision_at_20_std\n      value: 34.992352471692826\n    - type: nauc_precision_at_3_diff1\n      value: -5.691231842471157\n    - type: nauc_precision_at_3_max\n      value: 14.797949087742444\n    - type: nauc_precision_at_3_std\n      value: -0.1930317395644928\n    - type: nauc_precision_at_5_diff1\n      value: -20.03913781462645\n    - type: nauc_precision_at_5_max\n      value: 11.956771408712749\n    - type: nauc_precision_at_5_std\n      value: 13.179251389859731\n    - type: nauc_recall_at_1000_diff1\n      value: 64.03509042729674\n    - type: nauc_recall_at_1000_max\n      value: 40.91691485428493\n    - type: nauc_recall_at_1000_std\n      value: 16.12968625875372\n    - type: nauc_recall_at_100_diff1\n      value: 63.83116179628575\n    - type: nauc_recall_at_100_max\n      value: 43.72908117676382\n    - type: nauc_recall_at_100_std\n      value: -20.50966716852155\n    - type: nauc_recall_at_10_diff1\n      value: 66.42071960186394\n    - type: nauc_recall_at_10_max\n      value: 28.983207818687205\n    - type: nauc_recall_at_10_std\n      value: -56.61417798753744\n    - type: nauc_recall_at_1_diff1\n      value: 77.29690208952982\n    - type: nauc_recall_at_1_max\n      value: 19.839875762282293\n    - type: nauc_recall_at_1_std\n      value: -45.355684654708284\n    - type: nauc_recall_at_20_diff1\n      value: 66.32360705219874\n    - type: nauc_recall_at_20_max\n      value: 33.30698111822631\n    - type: nauc_recall_at_20_std\n      value: -43.89233781737452\n    - type: nauc_recall_at_3_diff1\n      value: 69.67029394927077\n    - type: nauc_recall_at_3_max\n      value: 22.67803039327696\n    - type: nauc_recall_at_3_std\n      value: -56.43327209861502\n    - type: nauc_recall_at_5_diff1\n      value: 68.05622143936131\n    - type: nauc_recall_at_5_max\n      value: 26.67795559040675\n    - type: nauc_recall_at_5_std\n      value: -58.158231198510954\n    - type: ndcg_at_1\n      value: 76.08\n    - type: ndcg_at_10\n      value: 84.114\n    - type: ndcg_at_100\n      value: 85.784\n    - type: ndcg_at_1000\n      value: 85.992\n    - type: ndcg_at_20\n      value: 84.976\n    - type: ndcg_at_3\n      value: 80.74799999999999\n    - type: ndcg_at_5\n      value: 82.626\n    - type: precision_at_1\n      value: 76.08\n    - type: precision_at_10\n      value: 12.926000000000002\n    - type: precision_at_100\n      value: 1.509\n    - type: precision_at_1000\n      value: 0.156\n    - type: precision_at_20\n      value: 6.912999999999999\n    - type: precision_at_3\n      value: 35.5\n    - type: precision_at_5\n      value: 23.541999999999998\n    - type: recall_at_1\n      value: 65.848\n    - type: recall_at_10\n      value: 92.611\n    - type: recall_at_100\n      value: 98.69\n    - type: recall_at_1000\n      value: 99.83999999999999\n    - type: recall_at_20\n      value: 95.47200000000001\n    - type: recall_at_3\n      value: 83.122\n    - type: recall_at_5\n      value: 88.23\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB SCIDOCS-PL (default)\n      revision: 45452b03f05560207ef19149545f168e596c9337\n      split: test\n      type: clarin-knext/scidocs-pl\n    metrics:\n    - type: main_score\n      value: 15.379999999999999\n    - type: map_at_1\n      value: 3.6029999999999998\n    - type: map_at_10\n      value: 8.843\n    - type: map_at_100\n      value: 10.433\n    - type: map_at_1000\n      value: 10.689\n    - type: map_at_20\n      value: 9.597\n    - type: map_at_3\n      value: 6.363\n    - type: map_at_5\n      value: 7.603\n    - type: mrr_at_1\n      value: 17.7\n    - type: mrr_at_10\n      value: 26.58900793650793\n    - type: mrr_at_100\n      value: 27.699652322890987\n    - type: mrr_at_1000\n      value: 27.78065313118353\n    - type: mrr_at_20\n      value: 27.215020950411816\n    - type: mrr_at_3\n      value: 23.36666666666668\n    - type: mrr_at_5\n      value: 25.211666666666666\n    - type: nauc_map_at_1000_diff1\n      value: 21.92235143827129\n    - type: nauc_map_at_1000_max\n      value: 37.50300940750989\n    - type: nauc_map_at_1000_std\n      value: 20.872586122198552\n    - type: nauc_map_at_100_diff1\n      value: 21.917408170465833\n    - type: nauc_map_at_100_max\n      value: 37.4654466815513\n    - type: nauc_map_at_100_std\n      value: 20.621643878648534\n    - type: nauc_map_at_10_diff1\n      value: 22.914388723621183\n    - type: nauc_map_at_10_max\n      value: 36.468131213468794\n    - type: nauc_map_at_10_std\n      value: 16.760980140791492\n    - type: nauc_map_at_1_diff1\n      value: 29.00799502838457\n    - type: nauc_map_at_1_max\n      value: 26.64926291797503\n    - type: nauc_map_at_1_std\n      value: 8.167291261637361\n    - type: nauc_map_at_20_diff1\n      value: 22.46580947804047\n    - type: nauc_map_at_20_max\n      value: 36.656294842562275\n    - type: nauc_map_at_20_std\n      value: 18.099232417722078\n    - type: nauc_map_at_3_diff1\n      value: 23.436009032045934\n    - type: nauc_map_at_3_max\n      value: 31.325807212280914\n    - type: nauc_map_at_3_std\n      value: 9.780905232048852\n    - type: nauc_map_at_5_diff1\n      value: 22.891704394665528\n    - type: nauc_map_at_5_max\n      value: 35.40584466642894\n    - type: nauc_map_at_5_std\n      value: 13.476986099394656\n    - type: nauc_mrr_at_1000_diff1\n      value: 25.052937655397866\n    - type: nauc_mrr_at_1000_max\n      value: 29.64431912670108\n    - type: nauc_mrr_at_1000_std\n      value: 14.549744963988044\n    - type: nauc_mrr_at_100_diff1\n      value: 25.070871266969224\n    - type: nauc_mrr_at_100_max\n      value: 29.68743604652336\n    - type: nauc_mrr_at_100_std\n      value: 14.582010154574432\n    - type: nauc_mrr_at_10_diff1\n      value: 24.88881466938897\n    - type: nauc_mrr_at_10_max\n      value: 29.488430770768144\n    - type: nauc_mrr_at_10_std\n      value: 14.269241073852266\n    - type: nauc_mrr_at_1_diff1\n      value: 29.220540327267503\n    - type: nauc_mrr_at_1_max\n      value: 26.81908580507911\n    - type: nauc_mrr_at_1_std\n      value: 8.00840295809718\n    - type: nauc_mrr_at_20_diff1\n      value: 25.067912695721944\n    - type: nauc_mrr_at_20_max\n      value: 29.759227563849628\n    - type: nauc_mrr_at_20_std\n      value: 14.685076859257357\n    - type: nauc_mrr_at_3_diff1\n      value: 24.645848739182696\n    - type: nauc_mrr_at_3_max\n      value: 27.73368549660351\n    - type: nauc_mrr_at_3_std\n      value: 11.475742805586943\n    - type: nauc_mrr_at_5_diff1\n      value: 24.895295760909946\n    - type: nauc_mrr_at_5_max\n      value: 29.130755033240423\n    - type: nauc_mrr_at_5_std\n      value: 12.955802929145404\n    - type: nauc_ndcg_at_1000_diff1\n      value: 20.68434434777729\n    - type: nauc_ndcg_at_1000_max\n      value: 37.67055146424174\n    - type: nauc_ndcg_at_1000_std\n      value: 29.57493715069776\n    - type: nauc_ndcg_at_100_diff1\n      value: 20.396834816492383\n    - type: nauc_ndcg_at_100_max\n      value: 37.460575228670514\n    - type: nauc_ndcg_at_100_std\n      value: 27.826534756761944\n    - type: nauc_ndcg_at_10_diff1\n      value: 22.640844106236027\n    - type: nauc_ndcg_at_10_max\n      value: 35.21291764462327\n    - type: nauc_ndcg_at_10_std\n      value: 19.53289455984506\n    - type: nauc_ndcg_at_1_diff1\n      value: 29.220540327267503\n    - type: nauc_ndcg_at_1_max\n      value: 26.81908580507911\n    - type: nauc_ndcg_at_1_std\n      value: 8.00840295809718\n    - type: nauc_ndcg_at_20_diff1\n      value: 22.117126657768623\n    - type: nauc_ndcg_at_20_max\n      value: 35.79395781940806\n    - type: nauc_ndcg_at_20_std\n      value: 22.242748346260786\n    - type: nauc_ndcg_at_3_diff1\n      value: 23.00596063212187\n    - type: nauc_ndcg_at_3_max\n      value: 30.149013627580523\n    - type: nauc_ndcg_at_3_std\n      value: 11.07904064662722\n    - type: nauc_ndcg_at_5_diff1\n      value: 22.81875419630523\n    - type: nauc_ndcg_at_5_max\n      value: 34.24267468356626\n    - type: nauc_ndcg_at_5_std\n      value: 15.307780280752088\n    - type: nauc_precision_at_1000_diff1\n      value: 9.606677689029972\n    - type: nauc_precision_at_1000_max\n      value: 32.74855550489271\n    - type: nauc_precision_at_1000_std\n      value: 42.65372585937895\n    - type: nauc_precision_at_100_diff1\n      value: 11.528981313529545\n    - type: nauc_precision_at_100_max\n      value: 35.642529490132404\n    - type: nauc_precision_at_100_std\n      value: 38.146151426052306\n    - type: nauc_precision_at_10_diff1\n      value: 18.783957183811836\n    - type: nauc_precision_at_10_max\n      value: 36.1982008334257\n    - type: nauc_precision_at_10_std\n      value: 25.09349473195891\n    - type: nauc_precision_at_1_diff1\n      value: 29.220540327267503\n    - type: nauc_precision_at_1_max\n      value: 26.81908580507911\n    - type: nauc_precision_at_1_std\n      value: 8.00840295809718\n    - type: nauc_precision_at_20_diff1\n      value: 17.458766320828214\n    - type: nauc_precision_at_20_max\n      value: 36.000404903025235\n    - type: nauc_precision_at_20_std\n      value: 29.1608044138323\n    - type: nauc_precision_at_3_diff1\n      value: 20.213669462067166\n    - type: nauc_precision_at_3_max\n      value: 31.120650847205912\n    - type: nauc_precision_at_3_std\n      value: 12.390972418818118\n    - type: nauc_precision_at_5_diff1\n      value: 20.114245715785678\n    - type: nauc_precision_at_5_max\n      value: 37.30360111495823\n    - type: nauc_precision_at_5_std\n      value: 19.053109037822853\n    - type: nauc_recall_at_1000_diff1\n      value: 9.85800049032612\n    - type: nauc_recall_at_1000_max\n      value: 32.48319160802687\n    - type: nauc_recall_at_1000_std\n      value: 43.79941601741161\n    - type: nauc_recall_at_100_diff1\n      value: 11.375255270968337\n    - type: nauc_recall_at_100_max\n      value: 35.1868784124497\n    - type: nauc_recall_at_100_std\n      value: 38.422680583482666\n    - type: nauc_recall_at_10_diff1\n      value: 18.445783123521938\n    - type: nauc_recall_at_10_max\n      value: 35.633267936276766\n    - type: nauc_recall_at_10_std\n      value: 24.94469506254716\n    - type: nauc_recall_at_1_diff1\n      value: 29.00799502838457\n    - type: nauc_recall_at_1_max\n      value: 26.64926291797503\n    - type: nauc_recall_at_1_std\n      value: 8.167291261637361\n    - type: nauc_recall_at_20_diff1\n      value: 17.314906604151936\n    - type: nauc_recall_at_20_max\n      value: 35.66067699203996\n    - type: nauc_recall_at_20_std\n      value: 29.400137012506082\n    - type: nauc_recall_at_3_diff1\n      value: 19.873710875648698\n    - type: nauc_recall_at_3_max\n      value: 30.92404718742849\n    - type: nauc_recall_at_3_std\n      value: 12.400871018075199\n    - type: nauc_recall_at_5_diff1\n      value: 19.869948324233192\n    - type: nauc_recall_at_5_max\n      value: 37.06832511687574\n    - type: nauc_recall_at_5_std\n      value: 19.0798814966156\n    - type: ndcg_at_1\n      value: 17.7\n    - type: ndcg_at_10\n      value: 15.379999999999999\n    - type: ndcg_at_100\n      value: 22.09\n    - type: ndcg_at_1000\n      value: 27.151999999999997\n    - type: ndcg_at_20\n      value: 17.576\n    - type: ndcg_at_3\n      value: 14.219999999999999\n    - type: ndcg_at_5\n      value: 12.579\n    - type: precision_at_1\n      value: 17.7\n    - type: precision_at_10\n      value: 8.08\n    - type: precision_at_100\n      value: 1.7840000000000003\n    - type: precision_at_1000\n      value: 0.3\n    - type: precision_at_20\n      value: 5.305\n    - type: precision_at_3\n      value: 13.167000000000002\n    - type: precision_at_5\n      value: 11.06\n    - type: recall_at_1\n      value: 3.6029999999999998\n    - type: recall_at_10\n      value: 16.413\n    - type: recall_at_100\n      value: 36.263\n    - type: recall_at_1000\n      value: 61.016999999999996\n    - type: recall_at_20\n      value: 21.587999999999997\n    - type: recall_at_3\n      value: 8.013\n    - type: recall_at_5\n      value: 11.198\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB SciFact-PL (default)\n      revision: 47932a35f045ef8ed01ba82bf9ff67f6e109207e\n      split: test\n      type: clarin-knext/scifact-pl\n    metrics:\n    - type: main_score\n      value: 64.764\n    - type: map_at_1\n      value: 49.778\n    - type: map_at_10\n      value: 59.88\n    - type: map_at_100\n      value: 60.707\n    - type: map_at_1000\n      value: 60.729\n    - type: map_at_20\n      value: 60.419999999999995\n    - type: map_at_3\n      value: 57.45400000000001\n    - type: map_at_5\n      value: 58.729\n    - type: mrr_at_1\n      value: 52.33333333333333\n    - type: mrr_at_10\n      value: 61.29193121693122\n    - type: mrr_at_100\n      value: 61.95817765126313\n    - type: mrr_at_1000\n      value: 61.97583284368782\n    - type: mrr_at_20\n      value: 61.72469949641003\n    - type: mrr_at_3\n      value: 59.44444444444444\n    - type: mrr_at_5\n      value: 60.494444444444454\n    - type: nauc_map_at_1000_diff1\n      value: 62.21235294015774\n    - type: nauc_map_at_1000_max\n      value: 48.83996609100249\n    - type: nauc_map_at_1000_std\n      value: 5.23892781043174\n    - type: nauc_map_at_100_diff1\n      value: 62.20170226789429\n    - type: nauc_map_at_100_max\n      value: 48.8391766453537\n    - type: nauc_map_at_100_std\n      value: 5.2664077457917715\n    - type: nauc_map_at_10_diff1\n      value: 61.961975488329024\n    - type: nauc_map_at_10_max\n      value: 48.397109987625186\n    - type: nauc_map_at_10_std\n      value: 4.314859710827481\n    - type: nauc_map_at_1_diff1\n      value: 65.0865197011516\n    - type: nauc_map_at_1_max\n      value: 41.38862781954889\n    - type: nauc_map_at_1_std\n      value: -0.9182122632530586\n    - type: nauc_map_at_20_diff1\n      value: 61.99173935851292\n    - type: nauc_map_at_20_max\n      value: 48.79961814179307\n    - type: nauc_map_at_20_std\n      value: 5.262181845825118\n    - type: nauc_map_at_3_diff1\n      value: 62.37910539880477\n    - type: nauc_map_at_3_max\n      value: 47.13627890977091\n    - type: nauc_map_at_3_std\n      value: 2.327897198087264\n    - type: nauc_map_at_5_diff1\n      value: 61.60080757149592\n    - type: nauc_map_at_5_max\n      value: 47.60052458345962\n    - type: nauc_map_at_5_std\n      value: 3.1770196981231047\n    - type: nauc_mrr_at_1000_diff1\n      value: 62.86810952814966\n    - type: nauc_mrr_at_1000_max\n      value: 52.13248094447774\n    - type: nauc_mrr_at_1000_std\n      value: 10.100485746570733\n    - type: nauc_mrr_at_100_diff1\n      value: 62.85364829491874\n    - type: nauc_mrr_at_100_max\n      value: 52.134528010631854\n    - type: nauc_mrr_at_100_std\n      value: 10.120945685447369\n    - type: nauc_mrr_at_10_diff1\n      value: 62.65679301829915\n    - type: nauc_mrr_at_10_max\n      value: 52.09270719182349\n    - type: nauc_mrr_at_10_std\n      value: 9.913834434725441\n    - type: nauc_mrr_at_1_diff1\n      value: 66.84108271415636\n    - type: nauc_mrr_at_1_max\n      value: 46.67646429855176\n    - type: nauc_mrr_at_1_std\n      value: 5.5505252956352304\n    - type: nauc_mrr_at_20_diff1\n      value: 62.72473227039611\n    - type: nauc_mrr_at_20_max\n      value: 52.13479097802757\n    - type: nauc_mrr_at_20_std\n      value: 10.188278833464084\n    - type: nauc_mrr_at_3_diff1\n      value: 63.797429185518496\n    - type: nauc_mrr_at_3_max\n      value: 52.16486999573481\n    - type: nauc_mrr_at_3_std\n      value: 9.094360767062762\n    - type: nauc_mrr_at_5_diff1\n      value: 62.592917975475494\n    - type: nauc_mrr_at_5_max\n      value: 52.330741486107414\n    - type: nauc_mrr_at_5_std\n      value: 9.742175534421389\n    - type: nauc_ndcg_at_1000_diff1\n      value: 61.38859337672476\n    - type: nauc_ndcg_at_1000_max\n      value: 51.48380058339184\n    - type: nauc_ndcg_at_1000_std\n      value: 9.670547660897673\n    - type: nauc_ndcg_at_100_diff1\n      value: 61.02438489641434\n    - type: nauc_ndcg_at_100_max\n      value: 51.781246646780865\n    - type: nauc_ndcg_at_100_std\n      value: 10.592961553245187\n    - type: nauc_ndcg_at_10_diff1\n      value: 60.03678353308358\n    - type: nauc_ndcg_at_10_max\n      value: 50.70725688848762\n    - type: nauc_ndcg_at_10_std\n      value: 7.9472446491016315\n    - type: nauc_ndcg_at_1_diff1\n      value: 66.84108271415636\n    - type: nauc_ndcg_at_1_max\n      value: 46.67646429855176\n    - type: nauc_ndcg_at_1_std\n      value: 5.5505252956352304\n    - type: nauc_ndcg_at_20_diff1\n      value: 59.828482718480224\n    - type: nauc_ndcg_at_20_max\n      value: 51.45831789601284\n    - type: nauc_ndcg_at_20_std\n      value: 10.722673683272049\n    - type: nauc_ndcg_at_3_diff1\n      value: 61.68982937524109\n    - type: nauc_ndcg_at_3_max\n      value: 49.745326748604775\n    - type: nauc_ndcg_at_3_std\n      value: 4.948298621202247\n    - type: nauc_ndcg_at_5_diff1\n      value: 59.67396171973207\n    - type: nauc_ndcg_at_5_max\n      value: 49.87855139298281\n    - type: nauc_ndcg_at_5_std\n      value: 6.08990428055584\n    - type: nauc_precision_at_1000_diff1\n      value: -1.594227972036865\n    - type: nauc_precision_at_1000_max\n      value: 32.48431723086185\n    - type: nauc_precision_at_1000_std\n      value: 53.84748466965268\n    - type: nauc_precision_at_100_diff1\n      value: 8.06411455192293\n    - type: nauc_precision_at_100_max\n      value: 39.91003601878948\n    - type: nauc_precision_at_100_std\n      value: 55.52979711075091\n    - type: nauc_precision_at_10_diff1\n      value: 26.610514456014066\n    - type: nauc_precision_at_10_max\n      value: 47.09062494321172\n    - type: nauc_precision_at_10_std\n      value: 33.91984226498748\n    - type: nauc_precision_at_1_diff1\n      value: 66.84108271415636\n    - type: nauc_precision_at_1_max\n      value: 46.67646429855176\n    - type: nauc_precision_at_1_std\n      value: 5.5505252956352304\n    - type: nauc_precision_at_20_diff1\n      value: 16.947688843085583\n    - type: nauc_precision_at_20_max\n      value: 45.40488186572008\n    - type: nauc_precision_at_20_std\n      value: 48.354421924500905\n    - type: nauc_precision_at_3_diff1\n      value: 49.11263981720622\n    - type: nauc_precision_at_3_max\n      value: 52.7084625111683\n    - type: nauc_precision_at_3_std\n      value: 16.734612173556453\n    - type: nauc_precision_at_5_diff1\n      value: 39.06503705015792\n    - type: nauc_precision_at_5_max\n      value: 52.21710506893391\n    - type: nauc_precision_at_5_std\n      value: 23.350948149460233\n    - type: nauc_recall_at_1000_diff1\n      value: 43.1559290382817\n    - type: nauc_recall_at_1000_max\n      value: 83.66013071895456\n    - type: nauc_recall_at_1000_std\n      value: 86.27450980392177\n    - type: nauc_recall_at_100_diff1\n      value: 46.016860850620375\n    - type: nauc_recall_at_100_max\n      value: 69.3944888744547\n    - type: nauc_recall_at_100_std\n      value: 55.286945696152735\n    - type: nauc_recall_at_10_diff1\n      value: 49.65877895350921\n    - type: nauc_recall_at_10_max\n      value: 53.02636695700889\n    - type: nauc_recall_at_10_std\n      value: 13.967608945823828\n    - type: nauc_recall_at_1_diff1\n      value: 65.0865197011516\n    - type: nauc_recall_at_1_max\n      value: 41.38862781954889\n    - type: nauc_recall_at_1_std\n      value: -0.9182122632530586\n    - type: nauc_recall_at_20_diff1\n      value: 43.355308229973524\n    - type: nauc_recall_at_20_max\n      value: 57.04187909533764\n    - type: nauc_recall_at_20_std\n      value: 33.578720846660524\n    - type: nauc_recall_at_3_diff1\n      value: 56.922996057428165\n    - type: nauc_recall_at_3_max\n      value: 50.74417041895424\n    - type: nauc_recall_at_3_std\n      value: 5.623890124328387\n    - type: nauc_recall_at_5_diff1\n      value: 50.55620076865238\n    - type: nauc_recall_at_5_max\n      value: 51.3316854622085\n    - type: nauc_recall_at_5_std\n      value: 8.995457887269255\n    - type: ndcg_at_1\n      value: 52.333\n    - type: ndcg_at_10\n      value: 64.764\n    - type: ndcg_at_100\n      value: 68.167\n    - type: ndcg_at_1000\n      value: 68.816\n    - type: ndcg_at_20\n      value: 66.457\n    - type: ndcg_at_3\n      value: 60.346\n    - type: ndcg_at_5\n      value: 62.365\n    - type: precision_at_1\n      value: 52.333\n    - type: precision_at_10\n      value: 8.799999999999999\n    - type: precision_at_100\n      value: 1.057\n    - type: precision_at_1000\n      value: 0.11100000000000002\n    - type: precision_at_20\n      value: 4.8\n    - type: precision_at_3\n      value: 23.889\n    - type: precision_at_5\n      value: 15.6\n    - type: recall_at_1\n      value: 49.778\n    - type: recall_at_10\n      value: 78.206\n    - type: recall_at_100\n      value: 93.10000000000001\n    - type: recall_at_1000\n      value: 98.333\n    - type: recall_at_20\n      value: 84.467\n    - type: recall_at_3\n      value: 66.367\n    - type: recall_at_5\n      value: 71.35000000000001\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB TRECCOVID-PL (default)\n      revision: 81bcb408f33366c2a20ac54adafad1ae7e877fdd\n      split: test\n      type: clarin-knext/trec-covid-pl\n    metrics:\n    - type: main_score\n      value: 72.18900000000001\n    - type: map_at_1\n      value: 0.214\n    - type: map_at_10\n      value: 1.755\n    - type: map_at_100\n      value: 9.944\n    - type: map_at_1000\n      value: 24.205\n    - type: map_at_20\n      value: 3.1510000000000002\n    - type: map_at_3\n      value: 0.6\n    - type: map_at_5\n      value: 0.9560000000000001\n    - type: mrr_at_1\n      value: 82.0\n    - type: mrr_at_10\n      value: 89.06666666666666\n    - type: mrr_at_100\n      value: 89.06666666666666\n    - type: mrr_at_1000\n      value: 89.06666666666666\n    - type: mrr_at_20\n      value: 89.06666666666666\n    - type: mrr_at_3\n      value: 87.66666666666666\n    - type: mrr_at_5\n      value: 89.06666666666666\n    - type: nauc_map_at_1000_diff1\n      value: -9.342037623635543\n    - type: nauc_map_at_1000_max\n      value: 45.71499810252398\n    - type: nauc_map_at_1000_std\n      value: 76.86482845196852\n    - type: nauc_map_at_100_diff1\n      value: -6.932395299866198\n    - type: nauc_map_at_100_max\n      value: 36.097801891181604\n    - type: nauc_map_at_100_std\n      value: 65.6085215411685\n    - type: nauc_map_at_10_diff1\n      value: -6.3654843824342775\n    - type: nauc_map_at_10_max\n      value: 9.564437521432714\n    - type: nauc_map_at_10_std\n      value: 21.8377319336476\n    - type: nauc_map_at_1_diff1\n      value: 8.269590874255034\n    - type: nauc_map_at_1_max\n      value: 3.482498491294516\n    - type: nauc_map_at_1_std\n      value: 8.985226819412189\n    - type: nauc_map_at_20_diff1\n      value: -4.971435767877232\n    - type: nauc_map_at_20_max\n      value: 22.88801858567121\n    - type: nauc_map_at_20_std\n      value: 32.38492618534027\n    - type: nauc_map_at_3_diff1\n      value: 1.1615973694623123\n    - type: nauc_map_at_3_max\n      value: 1.935417800315643\n    - type: nauc_map_at_3_std\n      value: 10.289328305818698\n    - type: nauc_map_at_5_diff1\n      value: -2.4675967231444105\n    - type: nauc_map_at_5_max\n      value: 2.4611483736622373\n    - type: nauc_map_at_5_std\n      value: 15.082324305750811\n    - type: nauc_mrr_at_1000_diff1\n      value: 13.098526703499063\n    - type: nauc_mrr_at_1000_max\n      value: 56.37362177417431\n    - type: nauc_mrr_at_1000_std\n      value: 73.2456769749587\n    - type: nauc_mrr_at_100_diff1\n      value: 13.098526703499063\n    - type: nauc_mrr_at_100_max\n      value: 56.37362177417431\n    - type: nauc_mrr_at_100_std\n      value: 73.2456769749587\n    - type: nauc_mrr_at_10_diff1\n      value: 13.098526703499063\n    - type: nauc_mrr_at_10_max\n      value: 56.37362177417431\n    - type: nauc_mrr_at_10_std\n      value: 73.2456769749587\n    - type: nauc_mrr_at_1_diff1\n      value: 12.099350148694809\n    - type: nauc_mrr_at_1_max\n      value: 53.75041304108387\n    - type: nauc_mrr_at_1_std\n      value: 68.84018063663402\n    - type: nauc_mrr_at_20_diff1\n      value: 13.098526703499063\n    - type: nauc_mrr_at_20_max\n      value: 56.37362177417431\n    - type: nauc_mrr_at_20_std\n      value: 73.2456769749587\n    - type: nauc_mrr_at_3_diff1\n      value: 12.173557857011161\n    - type: nauc_mrr_at_3_max\n      value: 57.540780562363395\n    - type: nauc_mrr_at_3_std\n      value: 75.42098189580211\n    - type: nauc_mrr_at_5_diff1\n      value: 13.098526703499063\n    - type: nauc_mrr_at_5_max\n      value: 56.37362177417431\n    - type: nauc_mrr_at_5_std\n      value: 73.2456769749587\n    - type: nauc_ndcg_at_1000_diff1\n      value: -8.951471847310401\n    - type: nauc_ndcg_at_1000_max\n      value: 43.86942237288822\n    - type: nauc_ndcg_at_1000_std\n      value: 74.61077735148591\n    - type: nauc_ndcg_at_100_diff1\n      value: -17.754559361083817\n    - type: nauc_ndcg_at_100_max\n      value: 53.97187119773482\n    - type: nauc_ndcg_at_100_std\n      value: 80.7944136146514\n    - type: nauc_ndcg_at_10_diff1\n      value: -26.637734697836414\n    - type: nauc_ndcg_at_10_max\n      value: 47.70102699133149\n    - type: nauc_ndcg_at_10_std\n      value: 70.26909560828646\n    - type: nauc_ndcg_at_1_diff1\n      value: -1.2250530785563207\n    - type: nauc_ndcg_at_1_max\n      value: 46.60509554140131\n    - type: nauc_ndcg_at_1_std\n      value: 62.63906581740976\n    - type: nauc_ndcg_at_20_diff1\n      value: -22.44286466550908\n    - type: nauc_ndcg_at_20_max\n      value: 55.40492058090103\n    - type: nauc_ndcg_at_20_std\n      value: 72.11813912145738\n    - type: nauc_ndcg_at_3_diff1\n      value: -14.8152721896563\n    - type: nauc_ndcg_at_3_max\n      value: 38.952259383027595\n    - type: nauc_ndcg_at_3_std\n      value: 59.819750166537766\n    - type: nauc_ndcg_at_5_diff1\n      value: -19.150105688904375\n    - type: nauc_ndcg_at_5_max\n      value: 42.311180547775315\n    - type: nauc_ndcg_at_5_std\n      value: 66.6632229321094\n    - type: nauc_precision_at_1000_diff1\n      value: -11.555591477978941\n    - type: nauc_precision_at_1000_max\n      value: 43.7311644834851\n    - type: nauc_precision_at_1000_std\n      value: 52.10644767999648\n    - type: nauc_precision_at_100_diff1\n      value: -16.94803099801117\n    - type: nauc_precision_at_100_max\n      value: 54.08281631067633\n    - type: nauc_precision_at_100_std\n      value: 82.77237347891331\n    - type: nauc_precision_at_10_diff1\n      value: -27.351332814863355\n    - type: nauc_precision_at_10_max\n      value: 48.08237549065846\n    - type: nauc_precision_at_10_std\n      value: 69.37250843534329\n    - type: nauc_precision_at_1_diff1\n      value: 12.099350148694809\n    - type: nauc_precision_at_1_max\n      value: 53.75041304108387\n    - type: nauc_precision_at_1_std\n      value: 68.84018063663402\n    - type: nauc_precision_at_20_diff1\n      value: -18.2422222283388\n    - type: nauc_precision_at_20_max\n      value: 59.517328129343696\n    - type: nauc_precision_at_20_std\n      value: 72.05149307342747\n    - type: nauc_precision_at_3_diff1\n      value: -10.226547543075897\n    - type: nauc_precision_at_3_max\n      value: 43.14684818832875\n    - type: nauc_precision_at_3_std\n      value: 57.31936467418288\n    - type: nauc_precision_at_5_diff1\n      value: -14.28521589468673\n    - type: nauc_precision_at_5_max\n      value: 41.633426753962596\n    - type: nauc_precision_at_5_std\n      value: 64.94400576804541\n    - type: nauc_recall_at_1000_diff1\n      value: -0.9648831207497152\n    - type: nauc_recall_at_1000_max\n      value: 31.70832946085005\n    - type: nauc_recall_at_1000_std\n      value: 63.21471613968869\n    - type: nauc_recall_at_100_diff1\n      value: -1.360254380933586\n    - type: nauc_recall_at_100_max\n      value: 25.960597782099605\n    - type: nauc_recall_at_100_std\n      value: 51.52757589609674\n    - type: nauc_recall_at_10_diff1\n      value: -0.3899439424189566\n    - type: nauc_recall_at_10_max\n      value: 5.094341897886072\n    - type: nauc_recall_at_10_std\n      value: 11.266045616925698\n    - type: nauc_recall_at_1_diff1\n      value: 8.269590874255034\n    - type: nauc_recall_at_1_max\n      value: 3.482498491294516\n    - type: nauc_recall_at_1_std\n      value: 8.985226819412189\n    - type: nauc_recall_at_20_diff1\n      value: 6.4797098359254175\n    - type: nauc_recall_at_20_max\n      value: 15.663700985336124\n    - type: nauc_recall_at_20_std\n      value: 17.154099587904913\n    - type: nauc_recall_at_3_diff1\n      value: 3.7245972450393507\n    - type: nauc_recall_at_3_max\n      value: 0.4063857187240345\n    - type: nauc_recall_at_3_std\n      value: 6.641948062821941\n    - type: nauc_recall_at_5_diff1\n      value: 4.013879477591466\n    - type: nauc_recall_at_5_max\n      value: -1.4266586618013566\n    - type: nauc_recall_at_5_std\n      value: 7.311601874411205\n    - type: ndcg_at_1\n      value: 75.0\n    - type: ndcg_at_10\n      value: 72.18900000000001\n    - type: ndcg_at_100\n      value: 54.022999999999996\n    - type: ndcg_at_1000\n      value: 49.492000000000004\n    - type: ndcg_at_20\n      value: 68.51\n    - type: ndcg_at_3\n      value: 73.184\n    - type: ndcg_at_5\n      value: 72.811\n    - type: precision_at_1\n      value: 82.0\n    - type: precision_at_10\n      value: 77.4\n    - type: precision_at_100\n      value: 55.24\n    - type: precision_at_1000\n      value: 21.822\n    - type: precision_at_20\n      value: 73.0\n    - type: precision_at_3\n      value: 79.333\n    - type: precision_at_5\n      value: 79.2\n    - type: recall_at_1\n      value: 0.214\n    - type: recall_at_10\n      value: 1.9980000000000002\n    - type: recall_at_100\n      value: 13.328999999999999\n    - type: recall_at_1000\n      value: 47.204\n    - type: recall_at_20\n      value: 3.7310000000000003\n    - type: recall_at_3\n      value: 0.628\n    - type: recall_at_5\n      value: 1.049\n    task:\n      type: Retrieval\n  - dataset:\n      config: default\n      name: MTEB CEDRClassification (default)\n      revision: c0ba03d058e3e1b2f3fd20518875a4563dd12db4\n      split: test\n      type: ai-forever/cedr-classification\n    metrics:\n    - type: accuracy\n      value: 47.30605738575983\n    - type: f1\n      value: 41.26091043925065\n    - type: lrap\n      value: 72.89452709883206\n    - type: main_score\n      value: 47.30605738575983\n    task:\n      type: MultilabelClassification\n  - dataset:\n      config: ru\n      name: MTEB MIRACLReranking (ru)\n      revision: 6d1962c527217f8927fca80f890f14f36b2802af\n      split: dev\n      type: miracl/mmteb-miracl-reranking\n    metrics:\n    - type: MAP@1(MIRACL)\n      value: 20.721999999999998\n    - type: MAP@10(MIRACL)\n      value: 33.900999999999996\n    - type: MAP@100(MIRACL)\n      value: 36.813\n    - type: MAP@1000(MIRACL)\n      value: 36.813\n    - type: MAP@20(MIRACL)\n      value: 35.684\n    - type: MAP@3(MIRACL)\n      value: 28.141\n    - type: MAP@5(MIRACL)\n      value: 31.075000000000003\n    - type: NDCG@1(MIRACL)\n      value: 32.799\n    - type: NDCG@10(MIRACL)\n      value: 42.065000000000005\n    - type: NDCG@100(MIRACL)\n      value: 49.730999999999995\n    - type: NDCG@1000(MIRACL)\n      value: 49.730999999999995\n    - type: NDCG@20(MIRACL)\n      value: 46.0\n    - type: NDCG@3(MIRACL)\n      value: 34.481\n    - type: NDCG@5(MIRACL)\n      value: 37.452999999999996\n    - type: P@1(MIRACL)\n', '{"pipeline_tag":"feature-extraction","library_name":"transformers","framework":"transformers","params":572310396,"storage_bytes":8007991570,"files_count":15,"spaces_count":41,"gated":false,"private":false,"config":{"architectures":["XLMRobertaModel"],"auto_map":{"AutoConfig":"jinaai/xlm-roberta-flash-implementation--configuration_xlm_roberta.XLMRobertaFlashConfig","AutoModel":"jinaai/xlm-roberta-flash-implementation--modeling_lora.XLMRobertaLoRA","AutoModelForMaskedLM":"jinaai/xlm-roberta-flash-implementation--modeling_xlm_roberta.XLMRobertaForMaskedLM","AutoModelForPreTraining":"jinaai/xlm-roberta-flash-implementation--modeling_xlm_roberta.XLMRobertaForPreTraining"},"tokenizer_config":{"bos_token":"<s>","cls_token":"<s>","eos_token":"</s>","mask_token":"<mask>","pad_token":"<pad>","sep_token":"</s>","unk_token":"<unk>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2409.10173","source_url":"https://arxiv.org/abs/2409.10173"}]', NULL, 'CC-BY-NC-4.0', 'approved', 80, 'cad87a11a3a9373d3d809847bb079d70', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-2-13b-chat-hf', 'huggingface--meta-llama--llama-2-13b-chat-hf', 'Llama-2-13b-chat-hf', 'meta-llama', '', '["transformers","pytorch","safetensors","llama","text-generation","facebook","meta","llama-2","conversational","en","arxiv:2307.09288","license:llama2","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 1104, 268711, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-2-13b-chat-hf","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":13015866880,"storage_bytes":52065420569,"files_count":19,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<s>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"chat_template":"{% if messages[0][''role''] == ''system'' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][''content''] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = ''<<SYS>>\\n'' + system_message + ''\\n<</SYS>>\\n\\n'' + message[''content''] %}{% else %}{% set content = message[''content''] %}{% endif %}{% if message[''role''] == ''user'' %}{{ bos_token + ''[INST] '' + content.strip() + '' [/INST]'' }}{% elif message[''role''] == ''assistant'' %}{{ '' ''  + content.strip() + '' '' + eos_token }}{% endif %}{% endfor %}","eos_token":{"__type":"AddedToken","content":"</s>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"pad_token":null,"unk_token":{"__type":"AddedToken","content":"<unk>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false}}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2307.09288","source_url":"https://arxiv.org/abs/2307.09288"}]', NULL, 'LLaMA-2', 'approved', 40, '4994b2d8f6c9b9fed7509ffce32aac99', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Zyphra-Zonos-v0.1-hybrid', 'huggingface--zyphra--zonos-v0.1-hybrid', 'Zonos-v0.1-hybrid', 'Zyphra', '--- license: apache-2.0 pipeline_tag: text-to-speech library_name: zonos --- <div align="center"> <img src="https://github.com/Zyphra/Zonos/blob/main/assets/ZonosHeader.png?raw=true" alt="Title card" style="width: 500px; height: auto; object-position: center top;"> </div> --- Zonos-v0.1 is a leading open-weight text-to-speech model trained on more than 200k hours of varied multilingual speech, delivering expressiveness and quality on par with‚Äîor even surpassing‚Äîtop TTS providers. Our model en...', '["zonos","safetensors","text-to-speech","license:apache-2.0","region:us"]', 'text-to-speech', 1100, 42014, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Zyphra/Zonos-v0.1-hybrid","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\npipeline_tag: text-to-speech\nlibrary_name: zonos\n---\n# Zonos-v0.1\n\n<div align="center">\n<img src="https://github.com/Zyphra/Zonos/blob/main/assets/ZonosHeader.png?raw=true"\n     alt="Title card" \n     style="width: 500px;\n            height: auto;\n            object-position: center top;">\n</div>\n\n---\n\nZonos-v0.1 is a leading open-weight text-to-speech model trained on more than 200k hours of varied multilingual speech, delivering expressiveness and quality on par with‚Äîor even surpassing‚Äîtop TTS providers.\n\nOur model enables highly natural speech generation from text prompts when given a speaker embedding or audio prefix, and can accurately perform speech cloning when given a reference clip spanning just a few seconds. The conditioning setup also allows for fine control over speaking rate, pitch variation, audio quality, and emotions such as happiness, fear, sadness, and anger. The model outputs speech natively at 44kHz.\n\n##### For more details and speech samples, check out our blog [here](https://www.zyphra.com/post/beta-release-of-zonos-v0-1)\n\n##### We also have a hosted version available at [playground.zyphra.com/audio](https://playground.zyphra.com/audio)\n\n---\n\nZonos follows a straightforward architecture: text normalization and phonemization via eSpeak, followed by DAC token prediction through a transformer or hybrid backbone. An overview of the architecture can be seen below.\n\n<div align="center">\n<img src="https://github.com/Zyphra/Zonos/blob/main/assets/ArchitectureDiagram.png?raw=true"\n     alt="Architecture diagram" \n     style="width: 1000px;\n            height: auto;\n            object-position: center top;">\n</div>\n\n---\n\n## Usage\n\n### Python\n\n```python\nimport torch\nimport torchaudio\nfrom zonos.model import Zonos\nfrom zonos.conditioning import make_cond_dict\n\nmodel = Zonos.from_pretrained("Zyphra/Zonos-v0.1-hybrid", device="cuda")\n# model = Zonos.from_pretrained("Zyphra/Zonos-v0.1-transformer", device="cuda")\n\nwav, sampling_rate = torchaudio.load("assets/exampleaudio.mp3")\nspeaker = model.make_speaker_embedding(wav, sampling_rate)\n\ncond_dict = make_cond_dict(text="Hello, world!", speaker=speaker, language="en-us")\nconditioning = model.prepare_conditioning(cond_dict)\n\ncodes = model.generate(conditioning)\n\nwavs = model.autoencoder.decode(codes).cpu()\ntorchaudio.save("sample.wav", wavs[0], model.autoencoder.sampling_rate)\n```\n\n### Gradio interface (recommended)\n\n```bash\nuv run gradio_interface.py\n# python gradio_interface.py\n```\n\nThis should produce a `sample.wav` file in your project root directory.\n\n_For repeated sampling we highly recommend using the gradio interface instead, as the minimal example needs to load the model every time it is run._\n\n## Features\n\n- Zero-shot TTS with voice cloning: Input desired text and a 10-30s speaker sample to generate high quality TTS output\n- Audio prefix inputs: Add text plus an audio prefix for even richer speaker matching. Audio prefixes can be used to elicit behaviours such as whispering which can otherwise be challenging to replicate when cloning from speaker embeddings\n- Multilingual support: Zonos-v0.1 supports English, Japanese, Chinese, French, and German\n- Audio quality and emotion control: Zonos offers fine-grained control of many aspects of the generated audio. These include speaking rate, pitch, maximum frequency, audio quality, and various emotions such as happiness, anger, sadness, and fear.\n- Fast: our model runs with a real-time factor of ~2x on an RTX 4090\n- Gradio WebUI: Zonos comes packaged with an easy to use gradio interface to generate speech\n- Simple installation and deployment: Zonos can be installed and deployed simply using the docker file packaged with our repository.\n\n## Installation\n\n**At the moment this repository only supports Linux systems (preferably Ubuntu 22.04/24.04) with recent NVIDIA GPUs (3000-series or newer, 6GB+ VRAM).**\n\nSee also [Docker Installation](#docker-installation)\n\n#### System dependencies\n\nZonos depends on the eSpeak library phonemization. You can install it on Ubuntu with the following command:\n\n```bash\napt install -y espeak-ng\n```\n\n#### Python dependencies\n\nWe highly recommend using a recent version of [uv](https://docs.astral.sh/uv/#installation) for installation. If you don''t have uv installed, you can install it via pip: `pip install -U uv`.\n\n##### Installing into a new uv virtual environment (recommended)\n\n```bash\nuv sync\nuv sync --extra compile\n```\n\n##### Installing into the system/actived environment using uv\n\n```bash\nuv pip install -e .\nuv pip install -e .[compile]\n```\n\n##### Installing into the system/actived environment using pip\n\n```bash\npip install -e .\npip install --no-build-isolation -e .[compile]\n```\n\n##### Confirm that it''s working\n\nFor convenience we provide a minimal example to check that the installation works:\n\n```bash\nuv run sample.py\n# python sample.py\n```\n\n## Docker installation\n\n```bash\ngit clone https://github.com/Zyphra/Zonos.git\ncd Zonos\n\n# For gradio\ndocker compose up\n\n# Or for development you can do\ndocker build -t Zonos .\ndocker run -it --gpus=all --net=host -v /path/to/Zonos:/Zonos -t Zonos\ncd /Zonos\npython sample.py # this will generate a sample.wav in /Zonos\n```\n\n## Citation\nIf you find this model useful in an academic context please cite as:\n```bash\n@misc{zyphra2025zonos,\n  title     = {Zonos-v0.1: An Expressive, Open-Source TTS Model},\n  author    = {Dario Sucic, Mohamed Osman, Gabriel Clark, Chris Warner, Beren Millidge},\n  year      = {2025},\n}', '{"pipeline_tag":"text-to-speech","library_name":"zonos","framework":"zonos","params":null,"storage_bytes":3700759529,"files_count":4,"spaces_count":33,"gated":false,"private":false,"config":{}}', '[]', '[{"type":"has_code","target_id":"github:Zyphra:Zonos","source_url":"https://github.com/Zyphra/Zonos"},{"type":"has_code","target_id":"github:Zyphra:Zonos","source_url":"https://github.com/Zyphra/Zonos"},{"type":"has_code","target_id":"github:Zyphra:Zonos.git","source_url":"https://github.com/Zyphra/Zonos.git"}]', NULL, 'Apache-2.0', 'approved', 65, '2ba4c6c630bd69849373200db6a0b28a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-unsloth-DeepSeek-R1-GGUF', 'huggingface--unsloth--deepseek-r1-gguf', 'DeepSeek-R1-GGUF', 'unsloth', '--- base_model: deepseek-ai/DeepSeek-R1 language: - en library_name: transformers license: mit tags: - deepseek - unsloth - transformers new_version: unsloth/DeepSeek-R1-0528-GGUF --- <div> <p style="margin-bottom: 0; margin-top: 0;"> <strong>See <a href="https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5">our collection</a> for versions of Deepseek-R1 including GGUF & 4-bit formats.</strong> </p> <p style="margin-bottom: 0;"> <em>Unsloth''s DeepSeek-R...', '["transformers","gguf","deepseek_v3","text-generation","deepseek","unsloth","custom_code","en","arxiv:2501.12948","base_model:deepseek-ai/deepseek-r1","base_model:quantized:deepseek-ai/deepseek-r1","license:mit","endpoints_compatible","region:us","conversational"]', 'text-generation', 1098, 25179, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/unsloth/DeepSeek-R1-GGUF","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nbase_model: deepseek-ai/DeepSeek-R1\nlanguage:\n- en\nlibrary_name: transformers\nlicense: mit\ntags:\n- deepseek\n- unsloth\n- transformers\nnew_version: unsloth/DeepSeek-R1-0528-GGUF\n---\n\n<div>\n  <p style="margin-bottom: 0; margin-top: 0;">\n    <strong>See <a href="https://huggingface.co/collections/unsloth/deepseek-r1-all-versions-678e1c48f5d2fce87892ace5">our collection</a> for versions of Deepseek-R1 including GGUF & 4-bit formats.</strong>\n  </p>\n  <p style="margin-bottom: 0;">\n    <em>Unsloth''s DeepSeek-R1 <a href="https://unsloth.ai/blog/deepseekr1-dynamic">1.58-bit + 2-bit Dynamic Quants</a> is selectively quantized, greatly improving accuracy over standard 1-bit/2-bit.</em>\n  </p>\n  <div style="display: flex; gap: 5px; align-items: center; ">\n    <a href="https://github.com/unslothai/unsloth/">\n      <img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="133">\n    </a>\n    <a href="https://discord.gg/unsloth">\n      <img src="https://github.com/unslothai/unsloth/raw/main/images/Discord%20button.png" width="173">\n    </a>\n    <a href="https://docs.unsloth.ai/basics/tutorial-how-to-run-deepseek-r1-on-your-own-local-device">\n      <img src="https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png" width="143">\n    </a>\n  </div>\n<h1 style="margin-top: 0rem;">Instructions to run this model in llama.cpp:</h2>\n</div>\n\nOr you can view more detailed instructions here: [unsloth.ai/blog/deepseekr1-dynamic](https://unsloth.ai/blog/deepseekr1-dynamic)\n1. Do not forget about `<ÔΩúUserÔΩú>` and `<ÔΩúAssistantÔΩú>` tokens! - Or use a chat template formatter\n2. Obtain the latest `llama.cpp` at https://github.com/ggerganov/llama.cpp. You can follow the build instructions below as well:\n```bash\napt-get update\napt-get install build-essential cmake curl libcurl4-openssl-dev -y\ngit clone https://github.com/ggerganov/llama.cpp\ncmake llama.cpp -B llama.cpp/build \\n	-DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON\ncmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli llama-gguf-split\ncp llama.cpp/build/bin/llama-* llama.cpp\n```\n3. It''s best to use `--min-p 0.05` to counteract very rare token predictions - I found this to work well especially for the 1.58bit model.\n4. Download the model via:\n```python\n# pip install huggingface_hub hf_transfer\n# import os # Optional for faster downloading\n# os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"\n\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\n  repo_id = "unsloth/DeepSeek-R1-GGUF",\n  local_dir = "DeepSeek-R1-GGUF",\n  allow_patterns = ["*UD-IQ1_S*"], # Select quant type UD-IQ1_S for 1.58bit\n)\n```\n5. Example with Q4_0 K quantized cache **Notice -no-cnv disables auto conversation mode**\n```bash\n   ./llama.cpp/llama-cli \\n	  --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\n	  --cache-type-k q4_0 \\n	  --threads 12 -no-cnv --prio 2 \\n	  --temp 0.6 \\n	  --ctx-size 8192 \\n	  --seed 3407 \\n	  --prompt "<ÔΩúUserÔΩú>Create a Flappy Bird game in Python.<ÔΩúAssistantÔΩú>"\n```\n   Example output:\n   \n   ```txt\n    <think>\n    Okay, so I need to figure out what 1 plus 1 is. Hmm, where do I even start? I remember from school that adding numbers is pretty basic, but I want to make sure I understand it properly.\n    Let me think, 1 plus 1. So, I have one item and I add another one. Maybe like a apple plus another apple. If I have one apple and someone gives me another, I now have two apples. So, 1 plus 1 should be 2. That makes sense.\n    Wait, but sometimes math can be tricky. Could it be something else? Like, in a different number system maybe? But I think the question is straightforward, using regular numbers, not like binary or hexadecimal or anything.\n    I also recall that in arithmetic, addition is combining quantities. So, if you have two quantities of 1, combining them gives you a total of 2. Yeah, that seems right.\n    Is there a scenario where 1 plus 1 wouldn''t be 2? I can''t think of any...\n   ```\n   \n6. If you have a GPU (RTX 4090 for example) with 24GB, you can offload multiple layers to the GPU for faster processing. If you have multiple GPUs, you can probably offload more layers.\n```bash\n  ./llama.cpp/llama-cli \\n    --model DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\n    --cache-type-k q4_0 \\n    --threads 12 -no-cnv --prio 2 \\n    --n-gpu-layers 7 \\n    --temp 0.6 \\n    --ctx-size 8192 \\n    --seed 3407 \\n    --prompt "<ÔΩúUserÔΩú>Create a Flappy Bird game in Python.<ÔΩúAssistantÔΩú>"\n```\n7. If you want to merge the weights together, use this script:\n```\n./llama.cpp/llama-gguf-split --merge \\n    DeepSeek-R1-GGUF/DeepSeek-R1-UD-IQ1_S-00001-of-00003.gguf \\n    merged_file.gguf\n```\n\n| MoE Bits     | Type   | Disk Size |  Accuracy | Link                      | Details   |\n| -------- | -------- | ------------ | ------------ | ---------------------|  ---------- |\n| 1.58bit | UD-IQ1_S |   **131GB**    | Fair           | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_S) | MoE all 1.56bit. `down_proj` in MoE mixture of 2.06/1.56bit |\n| 1.73bit | UD-IQ1_M |   **158GB**    | Good | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ1_M) | MoE all 1.56bit. `down_proj` in MoE left at 2.06bit |\n| 2.22bit | UD-IQ2_XXS |   **183GB**    | Better      | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-IQ2_XXS) | MoE all 2.06bit. `down_proj` in MoE mixture of 2.5/2.06bit |\n| 2.51bit | UD-Q2_K_XL |   **212GB**    | Best | [Link](https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main/DeepSeek-R1-UD-Q2_K_XL) | MoE all 2.5bit. `down_proj` in MoE mixture of 3.5/2.5bit |\n\n# Finetune your own Reasoning model like R1 with Unsloth!\nWe have a free Google Colab notebook for turning Llama 3.1 (8B) into a reasoning model: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb\n\n[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/Discord%20button.png" width="200"/>](https://discord.gg/unsloth)\n[<img src="https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png" width="200"/>](https://github.com/unslothai/unsloth)\n\n\n## ‚ú® Finetune for Free\n\nAll notebooks are **beginner friendly**! Add your dataset, click "Run All", and you''ll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face.\n\n| Unsloth supports          |    Free Notebooks                                                                                           | Performance | Memory use |\n|-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------|\n| **GRPO with Phi-4 (14B)**      | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_4_(14B)-GRPO.ipynb)               | 2x faster | 80% less |\n| **Llama-3.2 (3B)**      | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb)               | 2.4x faster | 58% less |\n| **Llama-3.2 (11B vision)**      | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)               | 2x faster | 60% less |\n| **Qwen2 VL (7B)**      | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2_VL_(7B)-Vision.ipynb)               | 1.8x faster | 60% less |\n| **Qwen2.5 (7B)**      | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen2.5_(7B)-Alpaca.ipynb)               | 2x faster | 60% less |\n| **Llama-3.1 (8B)**      | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-Alpaca.ipynb)               | 2.4x faster | 58% less |\n| **Phi-3.5 (mini)** | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Phi_3.5_Mini-Conversational.ipynb)               | 2x faster | 50% less |\n| **Gemma 2 (9B)**      | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma2_(9B)-Alpaca.ipynb)               | 2.4x faster | 58% less |\n| **Mistral (7B)**    | [‚ñ∂Ô∏è Start on Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb)               | 2.2x faster | 62% less |\n\n[<img src="https://raw.githubusercontent.com/unslothai/unsloth/refs/heads/main/images/documentation%20green%20button.png" width="200"/>](https://docs.unsloth.ai)\n\n- This [Llama 3.2 conversational notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb) is useful for ShareGPT ChatML / Vicuna templates.\n- This [text completion notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb) is for raw text. This [DPO notebook](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing) replicates Zephyr.\n- \* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.\n\n## Special Thanks\nA huge thank you to the DeepSeek team for creating and releasing these models.\n\n\n\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-CODE" style="margin: 2px;">\n    <img alt="Code License" src="https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE-MODEL" style="margin: 2px;">\n    <img alt="Model License" src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n\n<p align="center">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Paper Link</b>üëÅÔ∏è</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align="center">\n  <img width="80%" src="figures/benchmark.jpg">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model''s reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align="center">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align="center">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align="center">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek''s official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button "DeepThink"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: "Please reason step by step, and put your final answer within \boxed{}."\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":9778194944831,"files_count":118,"spaces_count":2,"gated":false,"private":false,"config":{"architectures":["DeepseekV3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_deepseek.DeepseekV3Config","AutoModel":"modeling_deepseek.DeepseekV3Model","AutoModelForCausalLM":"modeling_deepseek.DeepseekV3ForCausalLM"},"model_type":"deepseek_v3"}}', '[]', '[{"type":"has_code","target_id":"github:unslothai:unsloth","source_url":"https://github.com/unslothai/unsloth"},{"type":"has_code","target_id":"github:unslothai:unsloth","source_url":"https://github.com/unslothai/unsloth"},{"type":"has_code","target_id":"github:unslothai:unsloth","source_url":"https://github.com/unslothai/unsloth"},{"type":"has_code","target_id":"github:ggerganov:llama.cpp.","source_url":"https://github.com/ggerganov/llama.cpp."},{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:unslothai:unsloth","source_url":"https://github.com/unslothai/unsloth"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2501.12948","source_url":"https://arxiv.org/abs/2501.12948"}]', NULL, 'MIT', 'approved', 80, 'fabd0b2ea62dcfc83449eb1780b1a26d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tiiuae-falcon-7b', 'huggingface--tiiuae--falcon-7b', 'falcon-7b', 'tiiuae', '--- datasets: - tiiuae/falcon-refinedweb language: - en inference: false license: apache-2.0 new_version: tiiuae/falcon-11B --- **Falcon-7B is a 7B parameters causal decoder-only model built by TII and trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license.** *Paper coming soon* üòä. ü§ó To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF! * **It outperforms comp...', '["transformers","pytorch","safetensors","falcon","text-generation","custom_code","en","dataset:tiiuae/falcon-refinedweb","arxiv:2205.14135","arxiv:1911.02150","arxiv:2101.00027","arxiv:2005.14165","arxiv:2104.09864","arxiv:2306.01116","license:apache-2.0","text-generation-inference","deploy:azure","region:us"]', 'text-generation', 1095, 107660, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tiiuae/falcon-7b","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ndatasets:\n- tiiuae/falcon-refinedweb\nlanguage:\n- en\ninference: false\nlicense: apache-2.0\nnew_version: tiiuae/falcon-11B\n---\n\n# üöÄ Falcon-7B\n\n**Falcon-7B is a 7B parameters causal decoder-only model built by [TII](https://www.tii.ae) and trained on 1,500B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. It is made available under the Apache 2.0 license.**\n\n*Paper coming soon* üòä.\n\nü§ó To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading [this great blogpost fron HF](https://huggingface.co/blog/falcon)!\n\n\n## Why use Falcon-7B?\n\n* **It outperforms comparable open-source models** (e.g., [MPT-7B](https://huggingface.co/mosaicml/mpt-7b), [StableLM](https://github.com/Stability-AI/StableLM), [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1) etc.), thanks to being trained on 1,500B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n* **It features an architecture optimized for inference**, with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)). \n* **It is made available under a permissive Apache 2.0 license allowing for commercial use**, without any royalties or restrictions.\n\n‚ö†Ô∏è **This is a raw, pretrained model, which should be further finetuned for most usecases.** If you are looking for a version better suited to taking generic instructions in a chat format, we recommend taking a look at [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct). \n\nüî• **Looking for an even more powerful model?** [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) is Falcon-7B''s big brother!\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = "tiiuae/falcon-7b"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    "text-generation",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map="auto",\n)\nsequences = pipeline(\n   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f"Result: {seq[''generated_text'']}")\n\n```\n\nüí• **Falcon LLMs require PyTorch 2.0 for use with `transformers`!**\n\nFor fast inference with Falcon, check-out [Text Generation Inference](https://github.com/huggingface/text-generation-inference)! Read more in this [blogpost]((https://huggingface.co/blog/falcon). \n\nYou will need **at least 16GB of memory** to swiftly run inference with Falcon-7B.\n\n# Model Card for Falcon-7B\n\n## Model Details\n\n### Model Description\n\n- **Developed by:** [https://www.tii.ae](https://www.tii.ae);\n- **Model type:** Causal decoder-only;\n- **Language(s) (NLP):** English, German, Spanish, French (and limited capabilities in Italian, Portuguese, Polish, Dutch, Romanian, Czech, Swedish);\n- **License:** Apache 2.0.\n\n### Model Source\n\n- **Paper:** *coming soon*.\n\n## Uses\n\n### Direct Use\n\nResearch on large language models; as a foundation for further specialization and finetuning for specific usecases (e.g., summarization, text generation, chatbot, etc.)\n\n### Out-of-Scope Use\n\nProduction use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful. \n\n## Bias, Risks, and Limitations\n\nFalcon-7B is trained on English and French data only, and will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.\n\n### Recommendations\n\nWe recommend users of Falcon-7B to consider finetuning it for the specific set of tasks of interest, and for guardrails and appropriate precautions to be taken for any production use.\n\n## How to Get Started with the Model\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = "tiiuae/falcon-7b"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    "text-generation",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map="auto",\n)\nsequences = pipeline(\n   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f"Result: {seq[''generated_text'']}")\n\n```\n\n## Training Details\n\n### Training Data\n\nFalcon-7B was trained on 1,500B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by The Pile ([Gao et al., 2020](https://arxiv.org/abs/2101.00027)). \n\n| **Data source**    | **Fraction** | **Tokens** | **Sources**                       |\n|--------------------|--------------|------------|-----------------------------------|\n| [RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) | 79%          | 1,185B     | massive web crawl                 |\n| Books              | 7%           | 110B       |                                   |\n| Conversations      | 6%           | 85B        | Reddit, StackOverflow, HackerNews |\n| Code               | 3%           | 45B        |                                   |\n| RefinedWeb-French  | 3%           | 45B        | massive web crawl                 |\n| Technical          | 2%           | 30B        | arXiv, PubMed, USPTO, etc.        |\n\n\nThe data was tokenized with the Falcon-[7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b) tokenizer.\n\n### Training Procedure \n\nFalcon-7B was trained on 384 A100 40GB GPUs, using a 2D parallelism strategy (PP=2, DP=192) combined with ZeRO.\n\n#### Training Hyperparameters\n\n| **Hyperparameter** | **Value**  | **Comment**                               |\n|--------------------|------------|-------------------------------------------|\n| Precision          | `bfloat16` |                                           |\n| Optimizer          | AdamW      |                                           |\n| Learning rate      | 6e-4       | 4B tokens warm-up, cosine decay to 1.2e-5 |\n| Weight decay       | 1e-1       |                                           |\n| Z-loss       | 1e-4       |                                           |\n| Batch size         | 2304        | 30B tokens ramp-up                         |\n\n\n#### Speeds, Sizes, Times\n\nTraining happened in early March 2023 and took about two weeks. \n\n\n## Evaluation\n\n*Paper coming soon*.\n\nSee the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results.\n\n\n## Technical Specifications \n\n### Model Architecture and Objective\n\nFalcon-7B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token).\n\nThe architecture is broadly adapted from the GPT-3 paper ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)), with the following differences:\n\n* **Positionnal embeddings:** rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864));\n* **Attention:** multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135));\n* **Decoder-block:** parallel attention/MLP with a single layer norm.\n\n| **Hyperparameter** | **Value** | **Comment**                            |\n|--------------------|-----------|----------------------------------------|\n| Layers             | 32        |                                        |\n| `d_model`          | 4544      | Increased to compensate for multiquery                                       |\n| `head_dim`         | 64        | Reduced to optimise for FlashAttention |\n| Vocabulary         | 65024     |                                        |\n| Sequence length    | 2048      |                                        |\n\n### Compute Infrastructure\n\n#### Hardware\n\nFalcon-7B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances. \n\n#### Software\n\nFalcon-7B was trained a custom distributed training codebase, Gigatron. It uses a 3D parallelism approach combined with ZeRO and high-performance Triton kernels (FlashAttention, etc.)\n\n\n## Citation\n\n*Paper coming soon* üòä. In the meanwhile, you can use the following information to cite: \n```\n@article{falcon40b,\n  title={{Falcon-40B}: an open large language model with state-of-the-art performance},\n  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},\n  year={2023}\n}\n```\n\nTo learn more about the pretraining dataset, see the üìì [RefinedWeb paper](https://arxiv.org/abs/2306.01116).\n\n```\n@article{refinedweb,\n  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},\n  author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},\n  journal={arXiv preprint arXiv:2306.01116},\n  eprint={2306.01116},\n  eprinttype = {arXiv},\n  url={https://arxiv.org/abs/2306.01116},\n  year={2023}\n}\n```\n\n## License\n\nFalcon-7B is made available under the Apache 2.0 license.\n\n## Contact\nfalconllm@tii.ae', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7217189760,"storage_bytes":43359994411,"files_count":15,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["FalconForCausalLM"],"auto_map":{"AutoConfig":"configuration_falcon.FalconConfig","AutoModel":"modeling_falcon.FalconModel","AutoModelForSequenceClassification":"modeling_falcon.FalconForSequenceClassification","AutoModelForTokenClassification":"modeling_falcon.FalconForTokenClassification","AutoModelForQuestionAnswering":"modeling_falcon.FalconForQuestionAnswering","AutoModelForCausalLM":"modeling_falcon.FalconForCausalLM"},"model_type":"falcon","tokenizer_config":{"eos_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:Stability-AI:StableLM","source_url":"https://github.com/Stability-AI/StableLM"},{"type":"has_code","target_id":"github:huggingface:text-generation-inference","source_url":"https://github.com/huggingface/text-generation-inference"},{"type":"based_on_paper","target_id":"arxiv:2205.14135","source_url":"https://arxiv.org/abs/2205.14135"},{"type":"based_on_paper","target_id":"arxiv:1911.02150","source_url":"https://arxiv.org/abs/1911.02150"},{"type":"based_on_paper","target_id":"arxiv:2101.00027","source_url":"https://arxiv.org/abs/2101.00027"},{"type":"based_on_paper","target_id":"arxiv:2005.14165","source_url":"https://arxiv.org/abs/2005.14165"},{"type":"based_on_paper","target_id":"arxiv:2104.09864","source_url":"https://arxiv.org/abs/2104.09864"},{"type":"based_on_paper","target_id":"arxiv:2306.01116","source_url":"https://arxiv.org/abs/2306.01116"}]', NULL, 'Apache-2.0', 'approved', 80, 'c7e5b5d538aef542c7f330333ba6a980', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-CohereLabs-c4ai-command-r-v01', 'huggingface--coherelabs--c4ai-command-r-v01', 'c4ai-command-r-v01', 'CohereLabs', '', '["transformers","safetensors","cohere","text-generation","conversational","en","fr","de","es","it","pt","ja","ko","zh","ar","doi:10.57967/hf/3139","license:cc-by-nc-4.0","text-generation-inference","region:us"]', 'text-generation', 1095, 12058, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/CohereLabs/c4ai-command-r-v01","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":34980831232,"storage_bytes":205615615951,"files_count":23,"spaces_count":55,"gated":"auto","private":false,"config":{"architectures":["CohereForCausalLM"],"model_type":"cohere","tokenizer_config":{"bos_token":"<BOS_TOKEN>","chat_template":[{"name":"default","template":"{{ bos_token }}{% if messages[0][''role''] == ''system'' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][''content''] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = ''You are Command-R, a brilliant, sophisticated, AI-assistant trained to assist human users by providing thorough responses. You are trained by Cohere.'' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' + system_message + ''<|END_OF_TURN_TOKEN|>'' }}{% endif %}{% for message in loop_messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% set content = message[''content''] %}{% if message[''role''] == ''user'' %}{{ ''<|START_OF_TURN_TOKEN|><|USER_TOKEN|>'' + content.strip() + ''<|END_OF_TURN_TOKEN|>'' }}{% elif message[''role''] == ''assistant'' %}{{ ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>''  + content.strip() + ''<|END_OF_TURN_TOKEN|>'' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'' }}{% endif %}"},{"name":"tool_use","template":"\n{%- macro json_to_python_type(json_spec) %}\n{%- set basic_type_map = {\n    \"string\": \"str\",\n    \"number\": \"float\",\n    \"integer\": \"int\",\n    \"boolean\": \"bool\"\n} %}\n\n{%- if basic_type_map[json_spec.type] is defined %}\n    {{- basic_type_map[json_spec.type] }}\n{%- elif json_spec.type == \"array\" %}\n    {{- \"List[\" +  json_to_python_type(json_spec.items) + \"]\"}}\n{%- elif json_spec.type == \"object\" %}\n    {{- \"Dict[str, \" + json_to_python_type(json_spec.additionalProperties) + '']''}}\n{%- elif json_spec.type is iterable %}\n    {{- \"Union[\" }}\n    {%- for t in json_spec.type %}\n      {{- json_to_python_type({\"type\": t}) }}\n      {%- if not loop.last %}\n        {{- \",\" }} \n    {%- endif %}\n    {%- endfor %}\n    {{- \"]\" }}\n{%- else %}\n    {{- \"Any\" }}\n{%- endif %}\n{%- endmacro %}\n\n{%- macro old_tool_parser(tools) %}\n{%- for tool in tools %}\n    {%- if loop.index0 != 0 %}\n        {{- ''\\n\\n'' }}\n    {%- endif %}\n    {{- ''```python\\ndef '' + tool.name + ''('' }}\n    {%- for param_name, param_fields in tool.parameter_definitions|items %}\n        {%- if loop.index0 != 0 %}\n            {{- '', ''}}\n        {%- endif %}\n        {{- param_name + '': '' }}\n        {%- if not param_fields.required %}\n            {{- ''Optional['' + param_fields.type + ''] = None''}}\n        {%- else %}\n            {{- param_fields.type }}\n        {%- endif %}\n    {%- endfor %}\n    {{- '') -> List[Dict]:\\n    \"\"\"''}}\n    {{- tool.description }}\n    {%- if tool.parameter_definitions|length != 0 %}\n        {{- ''\\n\\n    Args:\\n        ''}}\n        {%- for param_name, param_fields in tool.parameter_definitions|items %}\n            {%- if loop.index0 != 0 %}\n                {{- ''\\n        '' }}\n            {%- endif %}\n            {{- param_name + '' (''}}\n            {%- if not param_fields.required %}\n                {{- ''Optional['' + param_fields.type + '']''}}\n            {%- else %}\n                {{- param_fields.type }}\n            {%- endif %}\n            {{- ''): '' + param_fields.description }}\n        {%- endfor %}\n    {%- endif %}\n    {{- ''\\n    \"\"\"\\n    pass\\n```'' }}\n{%- endfor %}\n{%- endmacro %}\n\n{%- macro new_tool_parser(tools) %}\n{%- for tool in tools %}\n  {%- if loop.index0 != 0 %}\n    {{- ''\\n\\n''}}\n  {%- endif %}\n  {%- if tool.function is defined %}\n    {%- set tool = tool.function %}\n  {%- endif %}\n  {{-''```python\ndef '' + tool.name + ''(''}}\n  {%- for param_name, param_fields in tool.parameters.properties|items %}\n    {%- if loop.index0 != 0 %}\n      {{- '', ''}}\n    {%- endif %}\n    {{-param_name + \": \"}} \n    {%- if not param_name in tool.parameters.required %}\n      {{-''Optional['' + json_to_python_type(param_fields) + ''] = None''}}\n    {%- else %}\n      {{- json_to_python_type(param_fields) }}\n    {%- endif %}\n  {%- endfor %}\n  {{- '') -> List[Dict]:\n    \"\"\"''}}\n  {{- tool.description }}\n  {%- if tool.parameters.properties|length != 0 %}\n    {{- ''\\n\\n    Args:\\n        ''}}\n    {%- for param_name, param_fields in tool.parameters.properties|items %}\n      {%- if loop.index0 != 0 %}\n        {{- ''\\n        '' }}\n      {%- endif %}\n      {{- param_name + '' (''}}\n      {%- if not param_name in tool.parameters.required %}\n        {{-''Optional['' + json_to_python_type(param_fields) + '']''}}\n      {%- else %}\n        {{- json_to_python_type(param_fields) }}\n      {%- endif %}\n      {{- ''): '' + param_fields.description }}\n    {%- endfor %}\n    {%- endif %}\n    {{- ''\\n    \"\"\"\\n    pass\\n```'' }}\n{%- endfor %}\n{%- endmacro %}\n\n{{- bos_token }}\n{%- if messages[0][''role''] == ''system'' %}\n  {%- set loop_messages = messages[1:] %}\n  {%- set system_message = messages[0][''content''] %}\n{%- else %}\n  {%- set loop_messages = messages %}\n  {%- set system_message = ''## Task and Context\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\''s needs as best you can, which will be wide-ranging.\\n\\n## Style Guide\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.'' %}\n{%- endif %}\n{{- ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' }}\n{{- ''# Safety Preamble'' }}\n{{- ''\nThe instructions in this section override those in the task description and style guide sections. Don\\''t answer questions that are harmful or immoral.'' }}\n{{- ''\n\n# System Preamble'' }}\n{{- ''\n## Basic Rules'' }}\n{{- ''\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user\\''s requests, you cite your sources in your answers, according to those instructions.'' }}\n{{- ''\n\n# User Preamble'' }}\n{{- ''\n'' + system_message }}\n{{-''\n\n## Available Tools\nHere is a list of tools that you have available to you:\n\n''}}\n{%- set ns = namespace(new_tools=true) %}\n{%- for tool in tools %}\n    {%- if tool.parameter_definitions is defined %}\n        {%- set ns.new_tools = false %}\n    {%- endif %}\n{%- endfor %}\n{%- if ns.new_tools %}\n    {{- new_tool_parser(tools) }}\n{%- else %}\n    {{- old_tool_parser(tools) }}\n{%- endif %}\n{{- ''<|END_OF_TURN_TOKEN|>''}}\n{%- for message in loop_messages %}\n  {%- set content = message[''content''] %}\n  {%- if message.role == ''user'' %}\n    {{- ''<|START_OF_TURN_TOKEN|><|USER_TOKEN|>'' + content|trim + ''<|END_OF_TURN_TOKEN|>'' }}\n  {%- elif message.role == ''system'' %}\n    {{- ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' + content|trim + ''<|END_OF_TURN_TOKEN|>'' }}\n  {%- elif message.role == ''assistant'' and message.tool_calls is defined %}\n    {{- ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'' }}\n    {%- if message.content is defined %}\n        {{- message.content|trim }}\n    {%- endif %}\n    {{- ''\\nAction:\\n```json\\n[\\n'' }}\n    {%- for tool_call in message.tool_calls %}\n        {%- if tool_call.function is defined %}\n            {%- set tool_call = tool_call.function %}\n        {%- endif %}\n        {{- ''{\\n''|indent(4, first=true) }}\n        {{- ''\"tool_name\": \"''|indent(8, first=true) + tool_call.name + ''\",\\n'' }}\n        {{- ''\"parameters\": ''|indent(8, first=true) }}\n        {%- if tool_call.arguments is defined and tool_call.arguments|length > 0 %}    \n            {{- tool_call.arguments|tojson(indent=4)|indent(8) }}\n            {{- ''\\n'' }}\n        {%- else %}\n            {{- ''{}\\n'' }}\n        {%- endif %}\n        {{- ''}''|indent(4, first=true) }}\n        {%- if not loop.last %}\n            {{- '',\\n'' }}\n        {%- endif %}\n    {%- endfor %}\n    {{- \"\\n]```\\n\" }}\n  {%- elif message.role == ''assistant'' %}\n    {{- ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>''  + content|trim + ''<|END_OF_TURN_TOKEN|>'' }}\n  {%- elif message.role == ''tool'' %}\n    {{- ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|><results>\\n'' }}\n    {{- message.content|trim }}\n    {{- ''</results><|END_OF_TURN_TOKEN|>'' }}\n  {%- endif %}\n{%- endfor %}\n{{-''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Write \\''Action:\\'' followed by a json-formatted list of actions that you want to perform in order to produce a good response to the user\\''s last input. You can use any of the supplied tools any number of times, but you should aim to execute the minimum number of necessary actions for the input. You should use the `directly-answer` tool if calling the other tools is unnecessary. The list of actions you want to call should be formatted as a list of json objects, for example:\n```json\n[\n    {\n        \"tool_name\": title of the tool in the specification,\n        \"parameters\": a dict of parameters to input into the tool as they are defined in the specs, or {} if it takes no parameters\n    }\n]```<|END_OF_TURN_TOKEN|>''}}\n{%- if add_generation_prompt %}\n  {{- ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'' }}\n{%- endif %}\n"},{"name":"rag","template":"{{ bos_token }}{% if messages[0][''role''] == ''system'' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][''content''] %}{% else %}{% set loop_messages = messages %}{% set system_message = ''## Task and Context\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\''s needs as best you can, which will be wide-ranging.\\n\\n## Style Guide\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.'' %}{% endif %}{{ ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' }}{{ ''# Safety Preamble'' }}{{ ''\nThe instructions in this section override those in the task description and style guide sections. Don\\''t answer questions that are harmful or immoral.'' }}{{ ''\n\n# System Preamble'' }}{{ ''\n## Basic Rules'' }}{{ ''\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user\\''s requests, you cite your sources in your answers, according to those instructions.'' }}{{ ''\n\n# User Preamble'' }}{{ ''\n'' + system_message }}{{ ''<|END_OF_TURN_TOKEN|>''}}{% for message in loop_messages %}{% set content = message[''content''] %}{% if message[''role''] == ''user'' %}{{ ''<|START_OF_TURN_TOKEN|><|USER_TOKEN|>'' + content.strip() + ''<|END_OF_TURN_TOKEN|>'' }}{% elif message[''role''] == ''system'' %}{{ ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' + content.strip() + ''<|END_OF_TURN_TOKEN|>'' }}{% elif message[''role''] == ''assistant'' %}{{ ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>''  + content.strip() + ''<|END_OF_TURN_TOKEN|>'' }}{% endif %}{% endfor %}{{ ''<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>''}}{{ ''<results>'' }}{% for document in documents %}{{ ''\nDocument: '' }}{{ loop.index0 }}\n{% for key, value in document.items() %}{{ key }}: {{value}}\n{% endfor %}{% endfor %}{{ ''</results>''}}{{ ''<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'' }}{{ ''Carefully perform the following instructions, in order, starting each with a new line.\n'' }}{{ ''Firstly, Decide which of the retrieved documents are relevant to the user\\''s last input by writing \\''Relevant Documents:\\'' followed by comma-separated list of document numbers. If none are relevant, you should instead write \\''None\\''.\n'' }}{{ ''Secondly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user\\''s last input by writing \\''Cited Documents:\\'' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write \\''None\\''.\n'' }}{% if citation_mode==''accurate'' %}{{ ''Thirdly, Write \\''Answer:\\'' followed by a response to the user\\''s last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup.\n'' }}{% endif %}{{ ''Finally, Write \\''Grounded answer:\\'' followed by a response to the user\\''s last input in high quality natural english. Use the symbols <co: doc> and </co: doc> to indicate when a fact comes from a document in the search result, e.g <co: 0>my fact</co: 0> for a fact from document 0.'' }}{{ ''<|END_OF_TURN_TOKEN|>'' }}{% if add_generation_prompt %}{{ ''<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'' }}{% endif %}"}],"eos_token":"<|END_OF_TURN_TOKEN|>","pad_token":"<PAD>","unk_token":null,"use_default_system_prompt":false}}}', '[]', '[]', NULL, 'CC-BY-NC-4.0', 'approved', 40, 'bc3c9fb88c7161582a81626382adff45', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-intfloat-multilingual-e5-large', 'huggingface--intfloat--multilingual-e5-large', 'multilingual-e5-large', 'intfloat', '--- tags: - mteb - Sentence Transformers - sentence-similarity - feature-extraction - sentence-transformers model-index: - name: multilingual-e5-large results: - task: type: Classification dataset: type: mteb/amazon_counterfactual name: MTEB AmazonCounterfactualClassification (en) config: en split: test revision: e8379541af4e31359cca9fbcf4b00f2671dba205 metrics: - type: accuracy value: 79.05970149253731 - type: ap value: 43.486574390835635 - type: f1 value: 73.32700092140148 - task: type: Cla...', '["sentence-transformers","pytorch","onnx","safetensors","openvino","xlm-roberta","mteb","sentence transformers","sentence-similarity","feature-extraction","multilingual","af","am","ar","as","az","be","bg","bn","br","bs","ca","cs","cy","da","de","el","en","eo","es","et","eu","fa","fi","fr","fy","ga","gd","gl","gu","ha","he","hi","hr","hu","hy","id","is","it","ja","jv","ka","kk","km","kn","ko","ku","ky","la","lo","lt","lv","mg","mk","ml","mn","mr","ms","my","ne","nl","no","om","or","pa","pl","ps","pt","ro","ru","sa","sd","si","sk","sl","so","sq","sr","su","sv","sw","ta","te","th","tl","tr","ug","uk","ur","uz","vi","xh","yi","zh","arxiv:2402.05672","arxiv:2108.08787","arxiv:2104.08663","arxiv:2210.07316","license:mit","model-index","text-embeddings-inference","endpoints_compatible","deploy:azure","region:us"]', 'feature-extraction', 1094, 3307315, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/intfloat/multilingual-e5-large","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- mteb\n- Sentence Transformers\n- sentence-similarity\n- feature-extraction\n- sentence-transformers\nmodel-index:\n- name: multilingual-e5-large\n  results:\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en)\n      config: en\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 79.05970149253731\n    - type: ap\n      value: 43.486574390835635\n    - type: f1\n      value: 73.32700092140148\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (de)\n      config: de\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 71.22055674518201\n    - type: ap\n      value: 81.55756710830498\n    - type: f1\n      value: 69.28271787752661\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (en-ext)\n      config: en-ext\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 80.41979010494754\n    - type: ap\n      value: 29.34879922376344\n    - type: f1\n      value: 67.62475449011278\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_counterfactual\n      name: MTEB AmazonCounterfactualClassification (ja)\n      config: ja\n      split: test\n      revision: e8379541af4e31359cca9fbcf4b00f2671dba205\n    metrics:\n    - type: accuracy\n      value: 77.8372591006424\n    - type: ap\n      value: 26.557560591210738\n    - type: f1\n      value: 64.96619417368707\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_polarity\n      name: MTEB AmazonPolarityClassification\n      config: default\n      split: test\n      revision: e2d317d38cd51312af73b3d32a06d1a08b442046\n    metrics:\n    - type: accuracy\n      value: 93.489875\n    - type: ap\n      value: 90.98758636917603\n    - type: f1\n      value: 93.48554819717332\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (en)\n      config: en\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 47.564\n    - type: f1\n      value: 46.75122173518047\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (de)\n      config: de\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 45.400000000000006\n    - type: f1\n      value: 44.17195682400632\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (es)\n      config: es\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 43.068\n    - type: f1\n      value: 42.38155696855596\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (fr)\n      config: fr\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 41.89\n    - type: f1\n      value: 40.84407321682663\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (ja)\n      config: ja\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 40.120000000000005\n    - type: f1\n      value: 39.522976223819114\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_reviews_multi\n      name: MTEB AmazonReviewsClassification (zh)\n      config: zh\n      split: test\n      revision: 1399c76144fd37290681b995c656ef9b2e06e26d\n    metrics:\n    - type: accuracy\n      value: 38.832\n    - type: f1\n      value: 38.0392533394713\n  - task:\n      type: Retrieval\n    dataset:\n      type: arguana\n      name: MTEB ArguAna\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 30.725\n    - type: map_at_10\n      value: 46.055\n    - type: map_at_100\n      value: 46.900999999999996\n    - type: map_at_1000\n      value: 46.911\n    - type: map_at_3\n      value: 41.548\n    - type: map_at_5\n      value: 44.297\n    - type: mrr_at_1\n      value: 31.152\n    - type: mrr_at_10\n      value: 46.231\n    - type: mrr_at_100\n      value: 47.07\n    - type: mrr_at_1000\n      value: 47.08\n    - type: mrr_at_3\n      value: 41.738\n    - type: mrr_at_5\n      value: 44.468999999999994\n    - type: ndcg_at_1\n      value: 30.725\n    - type: ndcg_at_10\n      value: 54.379999999999995\n    - type: ndcg_at_100\n      value: 58.138\n    - type: ndcg_at_1000\n      value: 58.389\n    - type: ndcg_at_3\n      value: 45.156\n    - type: ndcg_at_5\n      value: 50.123\n    - type: precision_at_1\n      value: 30.725\n    - type: precision_at_10\n      value: 8.087\n    - type: precision_at_100\n      value: 0.9769999999999999\n    - type: precision_at_1000\n      value: 0.1\n    - type: precision_at_3\n      value: 18.54\n    - type: precision_at_5\n      value: 13.542000000000002\n    - type: recall_at_1\n      value: 30.725\n    - type: recall_at_10\n      value: 80.868\n    - type: recall_at_100\n      value: 97.653\n    - type: recall_at_1000\n      value: 99.57300000000001\n    - type: recall_at_3\n      value: 55.619\n    - type: recall_at_5\n      value: 67.71000000000001\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-p2p\n      name: MTEB ArxivClusteringP2P\n      config: default\n      split: test\n      revision: a122ad7f3f0291bf49cc6f4d32aa80929df69d5d\n    metrics:\n    - type: v_measure\n      value: 44.30960650674069\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/arxiv-clustering-s2s\n      name: MTEB ArxivClusteringS2S\n      config: default\n      split: test\n      revision: f910caf1a6075f7329cdf8c1a6135696f37dbd53\n    metrics:\n    - type: v_measure\n      value: 38.427074197498996\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/askubuntudupquestions-reranking\n      name: MTEB AskUbuntuDupQuestions\n      config: default\n      split: test\n      revision: 2000358ca161889fa9c082cb41daa8dcfb161a54\n    metrics:\n    - type: map\n      value: 60.28270056031872\n    - type: mrr\n      value: 74.38332673789738\n  - task:\n      type: STS\n    dataset:\n      type: mteb/biosses-sts\n      name: MTEB BIOSSES\n      config: default\n      split: test\n      revision: d3fb88f8f02e40887cd149695127462bbcf29b4a\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.05942144105269\n    - type: cos_sim_spearman\n      value: 82.51212105850809\n    - type: euclidean_pearson\n      value: 81.95639829909122\n    - type: euclidean_spearman\n      value: 82.3717564144213\n    - type: manhattan_pearson\n      value: 81.79273425468256\n    - type: manhattan_spearman\n      value: 82.20066817871039\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (de-en)\n      config: de-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.46764091858039\n    - type: f1\n      value: 99.37717466945023\n    - type: precision\n      value: 99.33194154488518\n    - type: recall\n      value: 99.46764091858039\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (fr-en)\n      config: fr-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 98.29407880255337\n    - type: f1\n      value: 98.11248073959938\n    - type: precision\n      value: 98.02443319392472\n    - type: recall\n      value: 98.29407880255337\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (ru-en)\n      config: ru-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 97.79009352268791\n    - type: f1\n      value: 97.5176076665512\n    - type: precision\n      value: 97.38136473848286\n    - type: recall\n      value: 97.79009352268791\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/bucc-bitext-mining\n      name: MTEB BUCC (zh-en)\n      config: zh-en\n      split: test\n      revision: d51519689f32196a32af33b075a01d0e7c51e252\n    metrics:\n    - type: accuracy\n      value: 99.26276987888363\n    - type: f1\n      value: 99.20133403545726\n    - type: precision\n      value: 99.17500438827453\n    - type: recall\n      value: 99.26276987888363\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/banking77\n      name: MTEB Banking77Classification\n      config: default\n      split: test\n      revision: 0fd18e25b25c072e09e0d92ab615fda904d66300\n    metrics:\n    - type: accuracy\n      value: 84.72727272727273\n    - type: f1\n      value: 84.67672206031433\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-p2p\n      name: MTEB BiorxivClusteringP2P\n      config: default\n      split: test\n      revision: 65b79d1d13f80053f67aca9498d9402c2d9f1f40\n    metrics:\n    - type: v_measure\n      value: 35.34220182511161\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/biorxiv-clustering-s2s\n      name: MTEB BiorxivClusteringS2S\n      config: default\n      split: test\n      revision: 258694dd0231531bc1fd9de6ceb52a0853c6d908\n    metrics:\n    - type: v_measure\n      value: 33.4987096128766\n  - task:\n      type: Retrieval\n    dataset:\n      type: BeIR/cqadupstack\n      name: MTEB CQADupstackRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 25.558249999999997\n    - type: map_at_10\n      value: 34.44425000000001\n    - type: map_at_100\n      value: 35.59833333333333\n    - type: map_at_1000\n      value: 35.706916666666665\n    - type: map_at_3\n      value: 31.691749999999995\n    - type: map_at_5\n      value: 33.252916666666664\n    - type: mrr_at_1\n      value: 30.252666666666666\n    - type: mrr_at_10\n      value: 38.60675\n    - type: mrr_at_100\n      value: 39.42666666666666\n    - type: mrr_at_1000\n      value: 39.48408333333334\n    - type: mrr_at_3\n      value: 36.17441666666665\n    - type: mrr_at_5\n      value: 37.56275\n    - type: ndcg_at_1\n      value: 30.252666666666666\n    - type: ndcg_at_10\n      value: 39.683\n    - type: ndcg_at_100\n      value: 44.68541666666667\n    - type: ndcg_at_1000\n      value: 46.94316666666668\n    - type: ndcg_at_3\n      value: 34.961749999999995\n    - type: ndcg_at_5\n      value: 37.215666666666664\n    - type: precision_at_1\n      value: 30.252666666666666\n    - type: precision_at_10\n      value: 6.904166666666667\n    - type: precision_at_100\n      value: 1.0989999999999995\n    - type: precision_at_1000\n      value: 0.14733333333333334\n    - type: precision_at_3\n      value: 16.037666666666667\n    - type: precision_at_5\n      value: 11.413583333333333\n    - type: recall_at_1\n      value: 25.558249999999997\n    - type: recall_at_10\n      value: 51.13341666666666\n    - type: recall_at_100\n      value: 73.08366666666667\n    - type: recall_at_1000\n      value: 88.79483333333334\n    - type: recall_at_3\n      value: 37.989083333333326\n    - type: recall_at_5\n      value: 43.787833333333325\n  - task:\n      type: Retrieval\n    dataset:\n      type: climate-fever\n      name: MTEB ClimateFEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 10.338\n    - type: map_at_10\n      value: 18.360000000000003\n    - type: map_at_100\n      value: 19.942\n    - type: map_at_1000\n      value: 20.134\n    - type: map_at_3\n      value: 15.174000000000001\n    - type: map_at_5\n      value: 16.830000000000002\n    - type: mrr_at_1\n      value: 23.257\n    - type: mrr_at_10\n      value: 33.768\n    - type: mrr_at_100\n      value: 34.707\n    - type: mrr_at_1000\n      value: 34.766000000000005\n    - type: mrr_at_3\n      value: 30.977\n    - type: mrr_at_5\n      value: 32.528\n    - type: ndcg_at_1\n      value: 23.257\n    - type: ndcg_at_10\n      value: 25.733\n    - type: ndcg_at_100\n      value: 32.288\n    - type: ndcg_at_1000\n      value: 35.992000000000004\n    - type: ndcg_at_3\n      value: 20.866\n    - type: ndcg_at_5\n      value: 22.612\n    - type: precision_at_1\n      value: 23.257\n    - type: precision_at_10\n      value: 8.124\n    - type: precision_at_100\n      value: 1.518\n    - type: precision_at_1000\n      value: 0.219\n    - type: precision_at_3\n      value: 15.679000000000002\n    - type: precision_at_5\n      value: 12.117\n    - type: recall_at_1\n      value: 10.338\n    - type: recall_at_10\n      value: 31.154\n    - type: recall_at_100\n      value: 54.161\n    - type: recall_at_1000\n      value: 75.21900000000001\n    - type: recall_at_3\n      value: 19.427\n    - type: recall_at_5\n      value: 24.214\n  - task:\n      type: Retrieval\n    dataset:\n      type: dbpedia-entity\n      name: MTEB DBPedia\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 8.498\n    - type: map_at_10\n      value: 19.103\n    - type: map_at_100\n      value: 27.375\n    - type: map_at_1000\n      value: 28.981\n    - type: map_at_3\n      value: 13.764999999999999\n    - type: map_at_5\n      value: 15.950000000000001\n    - type: mrr_at_1\n      value: 65.5\n    - type: mrr_at_10\n      value: 74.53800000000001\n    - type: mrr_at_100\n      value: 74.71799999999999\n    - type: mrr_at_1000\n      value: 74.725\n    - type: mrr_at_3\n      value: 72.792\n    - type: mrr_at_5\n      value: 73.554\n    - type: ndcg_at_1\n      value: 53.37499999999999\n    - type: ndcg_at_10\n      value: 41.286\n    - type: ndcg_at_100\n      value: 45.972\n    - type: ndcg_at_1000\n      value: 53.123\n    - type: ndcg_at_3\n      value: 46.172999999999995\n    - type: ndcg_at_5\n      value: 43.033\n    - type: precision_at_1\n      value: 65.5\n    - type: precision_at_10\n      value: 32.725\n    - type: precision_at_100\n      value: 10.683\n    - type: precision_at_1000\n      value: 1.978\n    - type: precision_at_3\n      value: 50\n    - type: precision_at_5\n      value: 41.349999999999994\n    - type: recall_at_1\n      value: 8.498\n    - type: recall_at_10\n      value: 25.070999999999998\n    - type: recall_at_100\n      value: 52.383\n    - type: recall_at_1000\n      value: 74.91499999999999\n    - type: recall_at_3\n      value: 15.207999999999998\n    - type: recall_at_5\n      value: 18.563\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/emotion\n      name: MTEB EmotionClassification\n      config: default\n      split: test\n      revision: 4f58c6b202a23cf9a4da393831edf4f9183cad37\n    metrics:\n    - type: accuracy\n      value: 46.5\n    - type: f1\n      value: 41.93833713984145\n  - task:\n      type: Retrieval\n    dataset:\n      type: fever\n      name: MTEB FEVER\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 67.914\n    - type: map_at_10\n      value: 78.10000000000001\n    - type: map_at_100\n      value: 78.333\n    - type: map_at_1000\n      value: 78.346\n    - type: map_at_3\n      value: 76.626\n    - type: map_at_5\n      value: 77.627\n    - type: mrr_at_1\n      value: 72.74199999999999\n    - type: mrr_at_10\n      value: 82.414\n    - type: mrr_at_100\n      value: 82.511\n    - type: mrr_at_1000\n      value: 82.513\n    - type: mrr_at_3\n      value: 81.231\n    - type: mrr_at_5\n      value: 82.065\n    - type: ndcg_at_1\n      value: 72.74199999999999\n    - type: ndcg_at_10\n      value: 82.806\n    - type: ndcg_at_100\n      value: 83.677\n    - type: ndcg_at_1000\n      value: 83.917\n    - type: ndcg_at_3\n      value: 80.305\n    - type: ndcg_at_5\n      value: 81.843\n    - type: precision_at_1\n      value: 72.74199999999999\n    - type: precision_at_10\n      value: 10.24\n    - type: precision_at_100\n      value: 1.089\n    - type: precision_at_1000\n      value: 0.11299999999999999\n    - type: precision_at_3\n      value: 31.268\n    - type: precision_at_5\n      value: 19.706000000000003\n    - type: recall_at_1\n      value: 67.914\n    - type: recall_at_10\n      value: 92.889\n    - type: recall_at_100\n      value: 96.42699999999999\n    - type: recall_at_1000\n      value: 97.92\n    - type: recall_at_3\n      value: 86.21\n    - type: recall_at_5\n      value: 90.036\n  - task:\n      type: Retrieval\n    dataset:\n      type: fiqa\n      name: MTEB FiQA2018\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 22.166\n    - type: map_at_10\n      value: 35.57\n    - type: map_at_100\n      value: 37.405\n    - type: map_at_1000\n      value: 37.564\n    - type: map_at_3\n      value: 30.379\n    - type: map_at_5\n      value: 33.324\n    - type: mrr_at_1\n      value: 43.519000000000005\n    - type: mrr_at_10\n      value: 51.556000000000004\n    - type: mrr_at_100\n      value: 52.344\n    - type: mrr_at_1000\n      value: 52.373999999999995\n    - type: mrr_at_3\n      value: 48.868\n    - type: mrr_at_5\n      value: 50.319\n    - type: ndcg_at_1\n      value: 43.519000000000005\n    - type: ndcg_at_10\n      value: 43.803\n    - type: ndcg_at_100\n      value: 50.468999999999994\n    - type: ndcg_at_1000\n      value: 53.111\n    - type: ndcg_at_3\n      value: 38.893\n    - type: ndcg_at_5\n      value: 40.653\n    - type: precision_at_1\n      value: 43.519000000000005\n    - type: precision_at_10\n      value: 12.253\n    - type: precision_at_100\n      value: 1.931\n    - type: precision_at_1000\n      value: 0.242\n    - type: precision_at_3\n      value: 25.617\n    - type: precision_at_5\n      value: 19.383\n    - type: recall_at_1\n      value: 22.166\n    - type: recall_at_10\n      value: 51.6\n    - type: recall_at_100\n      value: 76.574\n    - type: recall_at_1000\n      value: 92.192\n    - type: recall_at_3\n      value: 34.477999999999994\n    - type: recall_at_5\n      value: 41.835\n  - task:\n      type: Retrieval\n    dataset:\n      type: hotpotqa\n      name: MTEB HotpotQA\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 39.041\n    - type: map_at_10\n      value: 62.961999999999996\n    - type: map_at_100\n      value: 63.79899999999999\n    - type: map_at_1000\n      value: 63.854\n    - type: map_at_3\n      value: 59.399\n    - type: map_at_5\n      value: 61.669\n    - type: mrr_at_1\n      value: 78.082\n    - type: mrr_at_10\n      value: 84.321\n    - type: mrr_at_100\n      value: 84.49600000000001\n    - type: mrr_at_1000\n      value: 84.502\n    - type: mrr_at_3\n      value: 83.421\n    - type: mrr_at_5\n      value: 83.977\n    - type: ndcg_at_1\n      value: 78.082\n    - type: ndcg_at_10\n      value: 71.229\n    - type: ndcg_at_100\n      value: 74.10900000000001\n    - type: ndcg_at_1000\n      value: 75.169\n    - type: ndcg_at_3\n      value: 66.28699999999999\n    - type: ndcg_at_5\n      value: 69.084\n    - type: precision_at_1\n      value: 78.082\n    - type: precision_at_10\n      value: 14.993\n    - type: precision_at_100\n      value: 1.7239999999999998\n    - type: precision_at_1000\n      value: 0.186\n    - type: precision_at_3\n      value: 42.737\n    - type: precision_at_5\n      value: 27.843\n    - type: recall_at_1\n      value: 39.041\n    - type: recall_at_10\n      value: 74.96300000000001\n    - type: recall_at_100\n      value: 86.199\n    - type: recall_at_1000\n      value: 93.228\n    - type: recall_at_3\n      value: 64.105\n    - type: recall_at_5\n      value: 69.608\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/imdb\n      name: MTEB ImdbClassification\n      config: default\n      split: test\n      revision: 3d86128a09e091d6018b6d26cad27f2739fc2db7\n    metrics:\n    - type: accuracy\n      value: 90.23160000000001\n    - type: ap\n      value: 85.5674856808308\n    - type: f1\n      value: 90.18033354786317\n  - task:\n      type: Retrieval\n    dataset:\n      type: msmarco\n      name: MTEB MSMARCO\n      config: default\n      split: dev\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 24.091\n    - type: map_at_10\n      value: 36.753\n    - type: map_at_100\n      value: 37.913000000000004\n    - type: map_at_1000\n      value: 37.958999999999996\n    - type: map_at_3\n      value: 32.818999999999996\n    - type: map_at_5\n      value: 35.171\n    - type: mrr_at_1\n      value: 24.742\n    - type: mrr_at_10\n      value: 37.285000000000004\n    - type: mrr_at_100\n      value: 38.391999999999996\n    - type: mrr_at_1000\n      value: 38.431\n    - type: mrr_at_3\n      value: 33.440999999999995\n    - type: mrr_at_5\n      value: 35.75\n    - type: ndcg_at_1\n      value: 24.742\n    - type: ndcg_at_10\n      value: 43.698\n    - type: ndcg_at_100\n      value: 49.145\n    - type: ndcg_at_1000\n      value: 50.23800000000001\n    - type: ndcg_at_3\n      value: 35.769\n    - type: ndcg_at_5\n      value: 39.961999999999996\n    - type: precision_at_1\n      value: 24.742\n    - type: precision_at_10\n      value: 6.7989999999999995\n    - type: precision_at_100\n      value: 0.95\n    - type: precision_at_1000\n      value: 0.104\n    - type: precision_at_3\n      value: 15.096000000000002\n    - type: precision_at_5\n      value: 11.183\n    - type: recall_at_1\n      value: 24.091\n    - type: recall_at_10\n      value: 65.068\n    - type: recall_at_100\n      value: 89.899\n    - type: recall_at_1000\n      value: 98.16\n    - type: recall_at_3\n      value: 43.68\n    - type: recall_at_5\n      value: 53.754999999999995\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (en)\n      config: en\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 93.66621067031465\n    - type: f1\n      value: 93.49622853272142\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (de)\n      config: de\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 91.94702733164272\n    - type: f1\n      value: 91.17043441745282\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (es)\n      config: es\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 92.20146764509674\n    - type: f1\n      value: 91.98359080555608\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (fr)\n      config: fr\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 88.99780770435328\n    - type: f1\n      value: 89.19746342724068\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (hi)\n      config: hi\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 89.78486912871998\n    - type: f1\n      value: 89.24578823628642\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_domain\n      name: MTEB MTOPDomainClassification (th)\n      config: th\n      split: test\n      revision: d80d48c1eb48d3562165c59d59d0034df9fff0bf\n    metrics:\n    - type: accuracy\n      value: 88.74502712477394\n    - type: f1\n      value: 89.00297573881542\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (en)\n      config: en\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 77.9046967624259\n    - type: f1\n      value: 59.36787125785957\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (de)\n      config: de\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 74.5280360664976\n    - type: f1\n      value: 57.17723440888718\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (es)\n      config: es\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 75.44029352901934\n    - type: f1\n      value: 54.052855531072964\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (fr)\n      config: fr\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 70.5606013153774\n    - type: f1\n      value: 52.62215934386531\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (hi)\n      config: hi\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 73.11581211903908\n    - type: f1\n      value: 52.341291845645465\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/mtop_intent\n      name: MTEB MTOPIntentClassification (th)\n      config: th\n      split: test\n      revision: ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba\n    metrics:\n    - type: accuracy\n      value: 74.28933092224233\n    - type: f1\n      value: 57.07918745504911\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (af)\n      config: af\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.38063214525892\n    - type: f1\n      value: 59.46463723443009\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (am)\n      config: am\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 56.06926698049766\n    - type: f1\n      value: 52.49084283283562\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ar)\n      config: ar\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 60.74983187626093\n    - type: f1\n      value: 56.960640620165904\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (az)\n      config: az\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 64.86550100874243\n    - type: f1\n      value: 62.47370548140688\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (bn)\n      config: bn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 63.971082716879636\n    - type: f1\n      value: 61.03812421957381\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (cy)\n      config: cy\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 54.98318762609282\n    - type: f1\n      value: 51.51207916008392\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (da)\n      config: da\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.45527908540686\n    - type: f1\n      value: 66.16631905400318\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (de)\n      config: de\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.32750504371216\n    - type: f1\n      value: 66.16755288646591\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (el)\n      config: el\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.09213180901143\n    - type: f1\n      value: 66.95654394661507\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (en)\n      config: en\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 73.75588433086752\n    - type: f1\n      value: 71.79973779656923\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (es)\n      config: es\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.49428379287154\n    - type: f1\n      value: 68.37494379215734\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fa)\n      config: fa\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.90921318090115\n    - type: f1\n      value: 66.79517376481645\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fi)\n      config: fi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.12104909213181\n    - type: f1\n      value: 67.29448842879584\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (fr)\n      config: fr\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.34095494283793\n    - type: f1\n      value: 67.01134288992947\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (he)\n      config: he\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.61264290517822\n    - type: f1\n      value: 64.68730512660757\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hi)\n      config: hi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.79757901815738\n    - type: f1\n      value: 65.24938539425598\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hu)\n      config: hu\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.68728984532616\n    - type: f1\n      value: 67.0487169762553\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (hy)\n      config: hy\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.07464694014795\n    - type: f1\n      value: 59.183532276789286\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (id)\n      config: id\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.04707464694015\n    - type: f1\n      value: 67.66829629003848\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (is)\n      config: is\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.42434431741762\n    - type: f1\n      value: 59.01617226544757\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (it)\n      config: it\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.53127101546738\n    - type: f1\n      value: 68.10033760906255\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ja)\n      config: ja\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 72.50504371217215\n    - type: f1\n      value: 69.74931103158923\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (jv)\n      config: jv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 57.91190316072628\n    - type: f1\n      value: 54.05551136648796\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ka)\n      config: ka\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 51.78211163416275\n    - type: f1\n      value: 49.874888544058535\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (km)\n      config: km\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 47.017484868863484\n    - type: f1\n      value: 44.53364263352014\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (kn)\n      config: kn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.16207128446537\n    - type: f1\n      value: 59.01185692320829\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ko)\n      config: ko\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.42501681237391\n    - type: f1\n      value: 67.13169450166086\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (lv)\n      config: lv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.0780094149294\n    - type: f1\n      value: 64.41720167850707\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ml)\n      config: ml\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 65.57162071284466\n    - type: f1\n      value: 62.414138683804424\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (mn)\n      config: mn\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 61.71149966375252\n    - type: f1\n      value: 58.594805125087234\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ms)\n      config: ms\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.03900470746471\n    - type: f1\n      value: 63.87937257883887\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (my)\n      config: my\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 60.8776059179556\n    - type: f1\n      value: 57.48587618059131\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (nb)\n      config: nb\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.87895090786819\n    - type: f1\n      value: 66.8141299430347\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (nl)\n      config: nl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.45057162071285\n    - type: f1\n      value: 67.46444039673516\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (pl)\n      config: pl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.546738399462\n    - type: f1\n      value: 68.63640876702655\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (pt)\n      config: pt\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 70.72965702757229\n    - type: f1\n      value: 68.54119560379115\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ro)\n      config: ro\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.35574983187625\n    - type: f1\n      value: 65.88844917691927\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ru)\n      config: ru\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.70477471418964\n    - type: f1\n      value: 69.19665697061978\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sl)\n      config: sl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 67.0880968392737\n    - type: f1\n      value: 64.76962317666086\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sq)\n      config: sq\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 65.18493611297916\n    - type: f1\n      value: 62.49984559035371\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sv)\n      config: sv\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.75857431069265\n    - type: f1\n      value: 69.20053687623418\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (sw)\n      config: sw\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 58.500336247478145\n    - type: f1\n      value: 55.2972398687929\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ta)\n      config: ta\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 62.68997982515132\n    - type: f1\n      value: 59.36848202755348\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (te)\n      config: te\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 63.01950235373235\n    - type: f1\n      value: 60.09351954625423\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (th)\n      config: th\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 68.29186281102892\n    - type: f1\n      value: 67.57860496703447\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (tl)\n      config: tl\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 64.77471418964357\n    - type: f1\n      value: 61.913983147713836\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (tr)\n      config: tr\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.87222595830532\n    - type: f1\n      value: 66.03679033708141\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (ur)\n      config: ur\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 64.04505716207127\n    - type: f1\n      value: 61.28569169817908\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (vi)\n      config: vi\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 69.38466711499663\n    - type: f1\n      value: 67.20532357036844\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (zh-CN)\n      config: zh-CN\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 71.12306657700067\n    - type: f1\n      value: 68.91251226588182\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_intent\n      name: MTEB MassiveIntentClassification (zh-TW)\n      config: zh-TW\n      split: test\n      revision: 31efe3c427b0bae9c22cbb560b8f15491cc6bed7\n    metrics:\n    - type: accuracy\n      value: 66.20040349697378\n    - type: f1\n      value: 66.02657347714175\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (af)\n      config: af\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 68.73907195696032\n    - type: f1\n      value: 66.98484521791418\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (am)\n      config: am\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 60.58843308675185\n    - type: f1\n      value: 58.95591723092005\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ar)\n      config: ar\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.22730329522528\n    - type: f1\n      value: 66.0894499712115\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (az)\n      config: az\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.48285137861465\n    - type: f1\n      value: 65.21963176785157\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (bn)\n      config: bn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 67.74714189643578\n    - type: f1\n      value: 66.8212192745412\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (cy)\n      config: cy\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 59.09213180901143\n    - type: f1\n      value: 56.70735546356339\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (da)\n      config: da\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.05716207128448\n    - type: f1\n      value: 74.8413712365364\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (de)\n      config: de\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.69737726967047\n    - type: f1\n      value: 74.7664341963\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (el)\n      config: el\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.90383322125084\n    - type: f1\n      value: 73.59201554448323\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (en)\n      config: en\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.51176866173503\n    - type: f1\n      value: 77.46104434577758\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (es)\n      config: es\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.31069266980496\n    - type: f1\n      value: 74.61048660675635\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fa)\n      config: fa\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.95225285810356\n    - type: f1\n      value: 72.33160006574627\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fi)\n      config: fi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.12373907195696\n    - type: f1\n      value: 73.20921012557481\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (fr)\n      config: fr\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.86684599865501\n    - type: f1\n      value: 73.82348774610831\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (he)\n      config: he\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.40215198386012\n    - type: f1\n      value: 71.11945183971858\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hi)\n      config: hi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 72.12844653665098\n    - type: f1\n      value: 71.34450495911766\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hu)\n      config: hu\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.52252858103566\n    - type: f1\n      value: 73.98878711342999\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (hy)\n      config: hy\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 64.93611297915265\n    - type: f1\n      value: 63.723200467653385\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (id)\n      config: id\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.11903160726295\n    - type: f1\n      value: 73.82138439467096\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (is)\n      config: is\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 67.15198386012105\n    - type: f1\n      value: 66.02172193802167\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (it)\n      config: it\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.32414256893072\n    - type: f1\n      value: 74.30943421170574\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ja)\n      config: ja\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 77.46805648957633\n    - type: f1\n      value: 77.62808409298209\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (jv)\n      config: jv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 63.318762609280434\n    - type: f1\n      value: 62.094284066075076\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ka)\n      config: ka\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 58.34902488231338\n    - type: f1\n      value: 57.12893860987984\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (km)\n      config: km\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 50.88433086751849\n    - type: f1\n      value: 48.2272350802058\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (kn)\n      config: kn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.4425016812374\n    - type: f1\n      value: 64.61463095996173\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ko)\n      config: ko\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.04707464694015\n    - type: f1\n      value: 75.05099199098998\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (lv)\n      config: lv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 70.50437121721586\n    - type: f1\n      value: 69.83397721096314\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ml)\n      config: ml\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 69.94283792871553\n    - type: f1\n      value: 68.8704663703913\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (mn)\n      config: mn\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 64.79488903833222\n    - type: f1\n      value: 63.615424063345436\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ms)\n      config: ms\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 69.88231338264963\n    - type: f1\n      value: 68.57892302593237\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (my)\n      config: my\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 63.248150638870214\n    - type: f1\n      value: 61.06680605338809\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (nb)\n      config: nb\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.84196368527236\n    - type: f1\n      value: 74.52566464968763\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (nl)\n      config: nl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.8285137861466\n    - type: f1\n      value: 74.8853197608802\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (pl)\n      config: pl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 74.13248150638869\n    - type: f1\n      value: 74.3982040999179\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (pt)\n      config: pt\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.49024882313383\n    - type: f1\n      value: 73.82153848368573\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ro)\n      config: ro\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.72158708809684\n    - type: f1\n      value: 71.85049433180541\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ru)\n      config: ru\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 75.137861466039\n    - type: f1\n      value: 75.37628348188467\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sl)\n      config: sl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.86953597848016\n    - type: f1\n      value: 71.87537624521661\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sq)\n      config: sq\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 70.27572293207801\n    - type: f1\n      value: 68.80017302344231\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sv)\n      config: sv\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.09952925353059\n    - type: f1\n      value: 76.07992707688408\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (sw)\n      config: sw\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 63.140551445864155\n    - type: f1\n      value: 61.73855010331415\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ta)\n      config: ta\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.27774041694687\n    - type: f1\n      value: 64.83664868894539\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (te)\n      config: te\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 66.69468728984533\n    - type: f1\n      value: 64.76239666920868\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (th)\n      config: th\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.44653665097512\n    - type: f1\n      value: 73.14646052013873\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (tl)\n      config: tl\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 67.71351714862139\n    - type: f1\n      value: 66.67212180163382\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (tr)\n      config: tr\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.9946200403497\n    - type: f1\n      value: 73.87348793725525\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (ur)\n      config: ur\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 68.15400134498992\n    - type: f1\n      value: 67.09433241421094\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (vi)\n      config: vi\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 73.11365164761264\n    - type: f1\n      value: 73.59502539433753\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (zh-CN)\n      config: zh-CN\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 76.82582380632145\n    - type: f1\n      value: 76.89992945316313\n  - task:\n      type: Classification\n    dataset:\n      type: mteb/amazon_massive_scenario\n      name: MTEB MassiveScenarioClassification (zh-TW)\n      config: zh-TW\n      split: test\n      revision: 7d571f92784cd94a019292a1f45445077d0ef634\n    metrics:\n    - type: accuracy\n      value: 71.81237390719569\n    - type: f1\n      value: 72.36499770986265\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-p2p\n      name: MTEB MedrxivClusteringP2P\n      config: default\n      split: test\n      revision: e7a26af6f3ae46b30dde8737f02c07b1505bcc73\n    metrics:\n    - type: v_measure\n      value: 31.480506569594695\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/medrxiv-clustering-s2s\n      name: MTEB MedrxivClusteringS2S\n      config: default\n      split: test\n      revision: 35191c8c0dca72d8ff3efcd72aa802307d469663\n    metrics:\n    - type: v_measure\n      value: 29.71252128004552\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/mind_small\n      name: MTEB MindSmallReranking\n      config: default\n      split: test\n      revision: 3bdac13927fdc888b903db93b2ffdbd90b295a69\n    metrics:\n    - type: map\n      value: 31.421396787056548\n    - type: mrr\n      value: 32.48155274872267\n  - task:\n      type: Retrieval\n    dataset:\n      type: nfcorpus\n      name: MTEB NFCorpus\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 5.595\n    - type: map_at_10\n      value: 12.642000000000001\n    - type: map_at_100\n      value: 15.726\n    - type: map_at_1000\n      value: 17.061999999999998\n    - type: map_at_3\n      value: 9.125\n    - type: map_at_5\n      value: 10.866000000000001\n    - type: mrr_at_1\n      value: 43.344\n    - type: mrr_at_10\n      value: 52.227999999999994\n    - type: mrr_at_100\n      value: 52.898999999999994\n    - type: mrr_at_1000\n      value: 52.944\n    - type: mrr_at_3\n      value: 49.845\n    - type: mrr_at_5\n      value: 51.115\n    - type: ndcg_at_1\n      value: 41.949999999999996\n    - type: ndcg_at_10\n      value: 33.995\n    - type: ndcg_at_100\n      value: 30.869999999999997\n    - type: ndcg_at_1000\n      value: 39.487\n    - type: ndcg_at_3\n      value: 38.903999999999996\n    - type: ndcg_at_5\n      value: 37.236999999999995\n    - type: precision_at_1\n      value: 43.344\n    - type: precision_at_10\n      value: 25.480000000000004\n    - type: precision_at_100\n      value: 7.672\n    - type: precision_at_1000\n      value: 2.028\n    - type: precision_at_3\n      value: 36.636\n    - type: precision_at_5\n      value: 32.632\n    - type: recall_at_1\n      value: 5.595\n    - type: recall_at_10\n      value: 16.466\n    - type: recall_at_100\n      value: 31.226\n    - type: recall_at_1000\n      value: 62.778999999999996\n    - type: recall_at_3\n      value: 9.931\n    - type: recall_at_5\n      value: 12.884\n  - task:\n      type: Retrieval\n    dataset:\n      type: nq\n      name: MTEB NQ\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 40.414\n    - type: map_at_10\n      value: 56.754000000000005\n    - type: map_at_100\n      value: 57.457\n    - type: map_at_1000\n      value: 57.477999999999994\n    - type: map_at_3\n      value: 52.873999999999995\n    - type: map_at_5\n      value: 55.175\n    - type: mrr_at_1\n      value: 45.278\n    - type: mrr_at_10\n      value: 59.192\n    - type: mrr_at_100\n      value: 59.650000000000006\n    - type: mrr_at_1000\n      value: 59.665\n    - type: mrr_at_3\n      value: 56.141\n    - type: mrr_at_5\n      value: 57.998000000000005\n    - type: ndcg_at_1\n      value: 45.278\n    - type: ndcg_at_10\n      value: 64.056\n    - type: ndcg_at_100\n      value: 66.89\n    - type: ndcg_at_1000\n      value: 67.364\n    - type: ndcg_at_3\n      value: 56.97\n    - type: ndcg_at_5\n      value: 60.719\n    - type: precision_at_1\n      value: 45.278\n    - type: precision_at_10\n      value: 9.994\n    - type: precision_at_100\n      value: 1.165\n    - type: precision_at_1000\n      value: 0.121\n    - type: precision_at_3\n      value: 25.512\n    - type: precision_at_5\n      value: 17.509\n    - type: recall_at_1\n      value: 40.414\n    - type: recall_at_10\n      value: 83.596\n    - type: recall_at_100\n      value: 95.72\n    - type: recall_at_1000\n      value: 99.24\n    - type: recall_at_3\n      value: 65.472\n    - type: recall_at_5\n      value: 74.039\n  - task:\n      type: Retrieval\n    dataset:\n      type: quora\n      name: MTEB QuoraRetrieval\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 70.352\n    - type: map_at_10\n      value: 84.369\n    - type: map_at_100\n      value: 85.02499999999999\n    - type: map_at_1000\n      value: 85.04\n    - type: map_at_3\n      value: 81.42399999999999\n    - type: map_at_5\n      value: 83.279\n    - type: mrr_at_1\n      value: 81.05\n    - type: mrr_at_10\n      value: 87.401\n    - type: mrr_at_100\n      value: 87.504\n    - type: mrr_at_1000\n      value: 87.505\n    - type: mrr_at_3\n      value: 86.443\n    - type: mrr_at_5\n      value: 87.10799999999999\n    - type: ndcg_at_1\n      value: 81.04\n    - type: ndcg_at_10\n      value: 88.181\n    - type: ndcg_at_100\n      value: 89.411\n    - type: ndcg_at_1000\n      value: 89.507\n    - type: ndcg_at_3\n      value: 85.28099999999999\n    - type: ndcg_at_5\n      value: 86.888\n    - type: precision_at_1\n      value: 81.04\n    - type: precision_at_10\n      value: 13.406\n    - type: precision_at_100\n      value: 1.5350000000000001\n    - type: precision_at_1000\n      value: 0.157\n    - type: precision_at_3\n      value: 37.31\n    - type: precision_at_5\n      value: 24.54\n    - type: recall_at_1\n      value: 70.352\n    - type: recall_at_10\n      value: 95.358\n    - type: recall_at_100\n      value: 99.541\n    - type: recall_at_1000\n      value: 99.984\n    - type: recall_at_3\n      value: 87.111\n    - type: recall_at_5\n      value: 91.643\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering\n      name: MTEB RedditClustering\n      config: default\n      split: test\n      revision: 24640382cdbf8abc73003fb0fa6d111a705499eb\n    metrics:\n    - type: v_measure\n      value: 46.54068723291946\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/reddit-clustering-p2p\n      name: MTEB RedditClusteringP2P\n      config: default\n      split: test\n      revision: 282350215ef01743dc01b456c7f5241fa8937f16\n    metrics:\n    - type: v_measure\n      value: 63.216287629895994\n  - task:\n      type: Retrieval\n    dataset:\n      type: scidocs\n      name: MTEB SCIDOCS\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 4.023000000000001\n    - type: map_at_10\n      value: 10.071\n    - type: map_at_100\n      value: 11.892\n    - type: map_at_1000\n      value: 12.196\n    - type: map_at_3\n      value: 7.234\n    - type: map_at_5\n      value: 8.613999999999999\n    - type: mrr_at_1\n      value: 19.900000000000002\n    - type: mrr_at_10\n      value: 30.516\n    - type: mrr_at_100\n      value: 31.656000000000002\n    - type: mrr_at_1000\n      value: 31.723000000000003\n    - type: mrr_at_3\n      value: 27.400000000000002\n    - type: mrr_at_5\n      value: 29.270000000000003\n    - type: ndcg_at_1\n      value: 19.900000000000002\n    - type: ndcg_at_10\n      value: 17.474\n    - type: ndcg_at_100\n      value: 25.020999999999997\n    - type: ndcg_at_1000\n      value: 30.728\n    - type: ndcg_at_3\n      value: 16.588\n    - type: ndcg_at_5\n      value: 14.498\n    - type: precision_at_1\n      value: 19.900000000000002\n    - type: precision_at_10\n      value: 9.139999999999999\n    - type: precision_at_100\n      value: 2.011\n    - type: precision_at_1000\n      value: 0.33899999999999997\n    - type: precision_at_3\n      value: 15.667\n    - type: precision_at_5\n      value: 12.839999999999998\n    - type: recall_at_1\n      value: 4.023000000000001\n    - type: recall_at_10\n      value: 18.497\n    - type: recall_at_100\n      value: 40.8\n    - type: recall_at_1000\n      value: 68.812\n    - type: recall_at_3\n      value: 9.508\n    - type: recall_at_5\n      value: 12.983\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sickr-sts\n      name: MTEB SICK-R\n      config: default\n      split: test\n      revision: a6ea5a8cab320b040a23452cc28066d9beae2cee\n    metrics:\n    - type: cos_sim_pearson\n      value: 83.967008785134\n    - type: cos_sim_spearman\n      value: 80.23142141101837\n    - type: euclidean_pearson\n      value: 81.20166064704539\n    - type: euclidean_spearman\n      value: 80.18961335654585\n    - type: manhattan_pearson\n      value: 81.13925443187625\n    - type: manhattan_spearman\n      value: 80.07948723044424\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts12-sts\n      name: MTEB STS12\n      config: default\n      split: test\n      revision: a0d554a64d88156834ff5ae9920b964011b16384\n    metrics:\n    - type: cos_sim_pearson\n      value: 86.94262461316023\n    - type: cos_sim_spearman\n      value: 80.01596278563865\n    - type: euclidean_pearson\n      value: 83.80799622922581\n    - type: euclidean_spearman\n      value: 79.94984954947103\n    - type: manhattan_pearson\n      value: 83.68473841756281\n    - type: manhattan_spearman\n      value: 79.84990707951822\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts13-sts\n      name: MTEB STS13\n      config: default\n      split: test\n      revision: 7e90230a92c190f1bf69ae9002b8cea547a64cca\n    metrics:\n    - type: cos_sim_pearson\n      value: 80.57346443146068\n    - type: cos_sim_spearman\n      value: 81.54689837570866\n    - type: euclidean_pearson\n      value: 81.10909881516007\n    - type: euclidean_spearman\n      value: 81.56746243261762\n    - type: manhattan_pearson\n      value: 80.87076036186582\n    - type: manhattan_spearman\n      value: 81.33074987964402\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts14-sts\n      name: MTEB STS14\n      config: default\n      split: test\n      revision: 6031580fec1f6af667f0bd2da0a551cf4f0b2375\n    metrics:\n    - type: cos_sim_pearson\n      value: 79.54733787179849\n    - type: cos_sim_spearman\n      value: 77.72202105610411\n    - type: euclidean_pearson\n      value: 78.9043595478849\n    - type: euclidean_spearman\n      value: 77.93422804309435\n    - type: manhattan_pearson\n      value: 78.58115121621368\n    - type: manhattan_spearman\n      value: 77.62508135122033\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts15-sts\n      name: MTEB STS15\n      config: default\n      split: test\n      revision: ae752c7c21bf194d8b67fd573edf7ae58183cbe3\n    metrics:\n    - type: cos_sim_pearson\n      value: 88.59880017237558\n    - type: cos_sim_spearman\n      value: 89.31088630824758\n    - type: euclidean_pearson\n      value: 88.47069261564656\n    - type: euclidean_spearman\n      value: 89.33581971465233\n    - type: manhattan_pearson\n      value: 88.40774264100956\n    - type: manhattan_spearman\n      value: 89.28657485627835\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts16-sts\n      name: MTEB STS16\n      config: default\n      split: test\n      revision: 4d8694f8f0e0100860b497b999b3dbed754a0513\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.08055117917084\n    - type: cos_sim_spearman\n      value: 85.78491813080304\n    - type: euclidean_pearson\n      value: 84.99329155500392\n    - type: euclidean_spearman\n      value: 85.76728064677287\n    - type: manhattan_pearson\n      value: 84.87947428989587\n    - type: manhattan_spearman\n      value: 85.62429454917464\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (ko-ko)\n      config: ko-ko\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 82.14190939287384\n    - type: cos_sim_spearman\n      value: 82.27331573306041\n    - type: euclidean_pearson\n      value: 81.891896953716\n    - type: euclidean_spearman\n      value: 82.37695542955998\n    - type: manhattan_pearson\n      value: 81.73123869460504\n    - type: manhattan_spearman\n      value: 82.19989168441421\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (ar-ar)\n      config: ar-ar\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 76.84695301843362\n    - type: cos_sim_spearman\n      value: 77.87790986014461\n    - type: euclidean_pearson\n      value: 76.91981583106315\n    - type: euclidean_spearman\n      value: 77.88154772749589\n    - type: manhattan_pearson\n      value: 76.94953277451093\n    - type: manhattan_spearman\n      value: 77.80499230728604\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-ar)\n      config: en-ar\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 75.44657840482016\n    - type: cos_sim_spearman\n      value: 75.05531095119674\n    - type: euclidean_pearson\n      value: 75.88161755829299\n    - type: euclidean_spearman\n      value: 74.73176238219332\n    - type: manhattan_pearson\n      value: 75.63984765635362\n    - type: manhattan_spearman\n      value: 74.86476440770737\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-de)\n      config: en-de\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.64700140524133\n    - type: cos_sim_spearman\n      value: 86.16014210425672\n    - type: euclidean_pearson\n      value: 86.49086860843221\n    - type: euclidean_spearman\n      value: 86.09729326815614\n    - type: manhattan_pearson\n      value: 86.43406265125513\n    - type: manhattan_spearman\n      value: 86.17740150939994\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-en)\n      config: en-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.91170098764921\n    - type: cos_sim_spearman\n      value: 88.12437004058931\n    - type: euclidean_pearson\n      value: 88.81828254494437\n    - type: euclidean_spearman\n      value: 88.14831794572122\n    - type: manhattan_pearson\n      value: 88.93442183448961\n    - type: manhattan_spearman\n      value: 88.15254630778304\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (en-tr)\n      config: en-tr\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 72.91390577997292\n    - type: cos_sim_spearman\n      value: 71.22979457536074\n    - type: euclidean_pearson\n      value: 74.40314008106749\n    - type: euclidean_spearman\n      value: 72.54972136083246\n    - type: manhattan_pearson\n      value: 73.85687539530218\n    - type: manhattan_spearman\n      value: 72.09500771742637\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (es-en)\n      config: es-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 80.9301067983089\n    - type: cos_sim_spearman\n      value: 80.74989828346473\n    - type: euclidean_pearson\n      value: 81.36781301814257\n    - type: euclidean_spearman\n      value: 80.9448819964426\n    - type: manhattan_pearson\n      value: 81.0351322685609\n    - type: manhattan_spearman\n      value: 80.70192121844177\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (es-es)\n      config: es-es\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 87.13820465980005\n    - type: cos_sim_spearman\n      value: 86.73532498758757\n    - type: euclidean_pearson\n      value: 87.21329451846637\n    - type: euclidean_spearman\n      value: 86.57863198601002\n    - type: manhattan_pearson\n      value: 87.06973713818554\n    - type: manhattan_spearman\n      value: 86.47534918791499\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (fr-en)\n      config: fr-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.48720108904415\n    - type: cos_sim_spearman\n      value: 85.62221757068387\n    - type: euclidean_pearson\n      value: 86.1010129512749\n    - type: euclidean_spearman\n      value: 85.86580966509942\n    - type: manhattan_pearson\n      value: 86.26800938808971\n    - type: manhattan_spearman\n      value: 85.88902721678429\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (it-en)\n      config: it-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 83.98021347333516\n    - type: cos_sim_spearman\n      value: 84.53806553803501\n    - type: euclidean_pearson\n      value: 84.61483347248364\n    - type: euclidean_spearman\n      value: 85.14191408011702\n    - type: manhattan_pearson\n      value: 84.75297588825967\n    - type: manhattan_spearman\n      value: 85.33176753669242\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts17-crosslingual-sts\n      name: MTEB STS17 (nl-en)\n      config: nl-en\n      split: test\n      revision: af5e6fb845001ecf41f4c1e033ce921939a2a68d\n    metrics:\n    - type: cos_sim_pearson\n      value: 84.51856644893233\n    - type: cos_sim_spearman\n      value: 85.27510748506413\n    - type: euclidean_pearson\n      value: 85.09886861540977\n    - type: euclidean_spearman\n      value: 85.62579245860887\n    - type: manhattan_pearson\n      value: 84.93017860464607\n    - type: manhattan_spearman\n      value: 85.5063988898453\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (en)\n      config: en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 62.581573200584195\n    - type: cos_sim_spearman\n      value: 63.05503590247928\n    - type: euclidean_pearson\n      value: 63.652564812602094\n    - type: euclidean_spearman\n      value: 62.64811520876156\n    - type: manhattan_pearson\n      value: 63.506842893061076\n    - type: manhattan_spearman\n      value: 62.51289573046917\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (de)\n      config: de\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 48.2248801729127\n    - type: cos_sim_spearman\n      value: 56.5936604678561\n    - type: euclidean_pearson\n      value: 43.98149464089\n    - type: euclidean_spearman\n      value: 56.108561882423615\n    - type: manhattan_pearson\n      value: 43.86880305903564\n    - type: manhattan_spearman\n      value: 56.04671150510166\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (es)\n      config: es\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 55.17564527009831\n    - type: cos_sim_spearman\n      value: 64.57978560979488\n    - type: euclidean_pearson\n      value: 58.8818330154583\n    - type: euclidean_spearman\n      value: 64.99214839071281\n    - type: manhattan_pearson\n      value: 58.72671436121381\n    - type: manhattan_spearman\n      value: 65.10713416616109\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (pl)\n      config: pl\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 26.772131864023297\n    - type: cos_sim_spearman\n      value: 34.68200792408681\n    - type: euclidean_pearson\n      value: 16.68082419005441\n    - type: euclidean_spearman\n      value: 34.83099932652166\n    - type: manhattan_pearson\n      value: 16.52605949659529\n    - type: manhattan_spearman\n      value: 34.82075801399475\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (tr)\n      config: tr\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 54.42415189043831\n    - type: cos_sim_spearman\n      value: 63.54594264576758\n    - type: euclidean_pearson\n      value: 57.36577498297745\n    - type: euclidean_spearman\n      value: 63.111466379158074\n    - type: manhattan_pearson\n      value: 57.584543715873885\n    - type: manhattan_spearman\n      value: 63.22361054139183\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (ar)\n      config: ar\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 47.55216762405518\n    - type: cos_sim_spearman\n      value: 56.98670142896412\n    - type: euclidean_pearson\n      value: 50.15318757562699\n    - type: euclidean_spearman\n      value: 56.524941926541906\n    - type: manhattan_pearson\n      value: 49.955618528674904\n    - type: manhattan_spearman\n      value: 56.37102209240117\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (ru)\n      config: ru\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 49.20540980338571\n    - type: cos_sim_spearman\n      value: 59.9009453504406\n    - type: euclidean_pearson\n      value: 49.557749853620535\n    - type: euclidean_spearman\n      value: 59.76631621172456\n    - type: manhattan_pearson\n      value: 49.62340591181147\n    - type: manhattan_spearman\n      value: 59.94224880322436\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (zh)\n      config: zh\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 51.508169956576985\n    - type: cos_sim_spearman\n      value: 66.82461565306046\n    - type: euclidean_pearson\n      value: 56.2274426480083\n    - type: euclidean_spearman\n      value: 66.6775323848333\n    - type: manhattan_pearson\n      value: 55.98277796300661\n    - type: manhattan_spearman\n      value: 66.63669848497175\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (fr)\n      config: fr\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 72.86478788045507\n    - type: cos_sim_spearman\n      value: 76.7946552053193\n    - type: euclidean_pearson\n      value: 75.01598530490269\n    - type: euclidean_spearman\n      value: 76.83618917858281\n    - type: manhattan_pearson\n      value: 74.68337628304332\n    - type: manhattan_spearman\n      value: 76.57480204017773\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (de-en)\n      config: de-en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 55.922619099401984\n    - type: cos_sim_spearman\n      value: 56.599362477240774\n    - type: euclidean_pearson\n      value: 56.68307052369783\n    - type: euclidean_spearman\n      value: 54.28760436777401\n    - type: manhattan_pearson\n      value: 56.67763566500681\n    - type: manhattan_spearman\n      value: 53.94619541711359\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (es-en)\n      config: es-en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 66.74357206710913\n    - type: cos_sim_spearman\n      value: 72.5208244925311\n    - type: euclidean_pearson\n      value: 67.49254562186032\n    - type: euclidean_spearman\n      value: 72.02469076238683\n    - type: manhattan_pearson\n      value: 67.45251772238085\n    - type: manhattan_spearman\n      value: 72.05538819984538\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (it)\n      config: it\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 71.25734330033191\n    - type: cos_sim_spearman\n      value: 76.98349083946823\n    - type: euclidean_pearson\n      value: 73.71642838667736\n    - type: euclidean_spearman\n      value: 77.01715504651384\n    - type: manhattan_pearson\n      value: 73.61712711868105\n    - type: manhattan_spearman\n      value: 77.01392571153896\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (pl-en)\n      config: pl-en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 63.18215462781212\n    - type: cos_sim_spearman\n      value: 65.54373266117607\n    - type: euclidean_pearson\n      value: 64.54126095439005\n    - type: euclidean_spearman\n      value: 65.30410369102711\n    - type: manhattan_pearson\n      value: 63.50332221148234\n    - type: manhattan_spearman\n      value: 64.3455878104313\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (zh-en)\n      config: zh-en\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 62.30509221440029\n    - type: cos_sim_spearman\n      value: 65.99582704642478\n    - type: euclidean_pearson\n      value: 63.43818859884195\n    - type: euclidean_spearman\n      value: 66.83172582815764\n    - type: manhattan_pearson\n      value: 63.055779168508764\n    - type: manhattan_spearman\n      value: 65.49585020501449\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (es-it)\n      config: es-it\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 59.587830825340404\n    - type: cos_sim_spearman\n      value: 68.93467614588089\n    - type: euclidean_pearson\n      value: 62.3073527367404\n    - type: euclidean_spearman\n      value: 69.69758171553175\n    - type: manhattan_pearson\n      value: 61.9074580815789\n    - type: manhattan_spearman\n      value: 69.57696375597865\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (de-fr)\n      config: de-fr\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 57.143220125577066\n    - type: cos_sim_spearman\n      value: 67.78857859159226\n    - type: euclidean_pearson\n      value: 55.58225107923733\n    - type: euclidean_spearman\n      value: 67.80662907184563\n    - type: manhattan_pearson\n      value: 56.24953502726514\n    - type: manhattan_spearman\n      value: 67.98262125431616\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (de-pl)\n      config: de-pl\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 21.826928900322066\n    - type: cos_sim_spearman\n      value: 49.578506634400405\n    - type: euclidean_pearson\n      value: 27.939890138843214\n    - type: euclidean_spearman\n      value: 52.71950519136242\n    - type: manhattan_pearson\n      value: 26.39878683847546\n    - type: manhattan_spearman\n      value: 47.54609580342499\n  - task:\n      type: STS\n    dataset:\n      type: mteb/sts22-crosslingual-sts\n      name: MTEB STS22 (fr-pl)\n      config: fr-pl\n      split: test\n      revision: 6d1ba47164174a496b7fa5d3569dae26a6813b80\n    metrics:\n    - type: cos_sim_pearson\n      value: 57.27603854632001\n    - type: cos_sim_spearman\n      value: 50.709255283710995\n    - type: euclidean_pearson\n      value: 59.5419024445929\n    - type: euclidean_spearman\n      value: 50.709255283710995\n    - type: manhattan_pearson\n      value: 59.03256832438492\n    - type: manhattan_spearman\n      value: 61.97797868009122\n  - task:\n      type: STS\n    dataset:\n      type: mteb/stsbenchmark-sts\n      name: MTEB STSBenchmark\n      config: default\n      split: test\n      revision: b0fddb56ed78048fa8b90373c8a3cfc37b684831\n    metrics:\n    - type: cos_sim_pearson\n      value: 85.00757054859712\n    - type: cos_sim_spearman\n      value: 87.29283629622222\n    - type: euclidean_pearson\n      value: 86.54824171775536\n    - type: euclidean_spearman\n      value: 87.24364730491402\n    - type: manhattan_pearson\n      value: 86.5062156915074\n    - type: manhattan_spearman\n      value: 87.15052170378574\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/scidocs-reranking\n      name: MTEB SciDocsRR\n      config: default\n      split: test\n      revision: d3c5e1fc0b855ab6097bf1cda04dd73947d7caab\n    metrics:\n    - type: map\n      value: 82.03549357197389\n    - type: mrr\n      value: 95.05437645143527\n  - task:\n      type: Retrieval\n    dataset:\n      type: scifact\n      name: MTEB SciFact\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 57.260999999999996\n    - type: map_at_10\n      value: 66.259\n    - type: map_at_100\n      value: 66.884\n    - type: map_at_1000\n      value: 66.912\n    - type: map_at_3\n      value: 63.685\n    - type: map_at_5\n      value: 65.35499999999999\n    - type: mrr_at_1\n      value: 60.333000000000006\n    - type: mrr_at_10\n      value: 67.5\n    - type: mrr_at_100\n      value: 68.013\n    - type: mrr_at_1000\n      value: 68.038\n    - type: mrr_at_3\n      value: 65.61099999999999\n    - type: mrr_at_5\n      value: 66.861\n    - type: ndcg_at_1\n      value: 60.333000000000006\n    - type: ndcg_at_10\n      value: 70.41\n    - type: ndcg_at_100\n      value: 73.10600000000001\n    - type: ndcg_at_1000\n      value: 73.846\n    - type: ndcg_at_3\n      value: 66.133\n    - type: ndcg_at_5\n      value: 68.499\n    - type: precision_at_1\n      value: 60.333000000000006\n    - type: precision_at_10\n      value: 9.232999999999999\n    - type: precision_at_100\n      value: 1.0630000000000002\n    - type: precision_at_1000\n      value: 0.11299999999999999\n    - type: precision_at_3\n      value: 25.667\n    - type: precision_at_5\n      value: 17.067\n    - type: recall_at_1\n      value: 57.260999999999996\n    - type: recall_at_10\n      value: 81.94399999999999\n    - type: recall_at_100\n      value: 93.867\n    - type: recall_at_1000\n      value: 99.667\n    - type: recall_at_3\n      value: 70.339\n    - type: recall_at_5\n      value: 76.25\n  - task:\n      type: PairClassification\n    dataset:\n      type: mteb/sprintduplicatequestions-pairclassification\n      name: MTEB SprintDuplicateQuestions\n      config: default\n      split: test\n      revision: d66bd1f72af766a5cc4b0ca5e00c162f89e8cc46\n    metrics:\n    - type: cos_sim_accuracy\n      value: 99.74356435643564\n    - type: cos_sim_ap\n      value: 93.13411948212683\n    - type: cos_sim_f1\n      value: 86.80521991300147\n    - type: cos_sim_precision\n      value: 84.00374181478017\n    - type: cos_sim_recall\n      value: 89.8\n    - type: dot_accuracy\n      value: 99.67920792079208\n    - type: dot_ap\n      value: 89.27277565444479\n    - type: dot_f1\n      value: 83.9276990718124\n    - type: dot_precision\n      value: 82.04393505253104\n    - type: dot_recall\n      value: 85.9\n    - type: euclidean_accuracy\n      value: 99.74257425742574\n    - type: euclidean_ap\n      value: 93.17993008259062\n    - type: euclidean_f1\n      value: 86.69396110542476\n    - type: euclidean_precision\n      value: 88.78406708595388\n    - type: euclidean_recall\n      value: 84.7\n    - type: manhattan_accuracy\n      value: 99.74257425742574\n    - type: manhattan_ap\n      value: 93.14413755550099\n    - type: manhattan_f1\n      value: 86.82483594144371\n    - type: manhattan_precision\n      value: 87.66564729867483\n    - type: manhattan_recall\n      value: 86\n    - type: max_accuracy\n      value: 99.74356435643564\n    - type: max_ap\n      value: 93.17993008259062\n    - type: max_f1\n      value: 86.82483594144371\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering\n      name: MTEB StackExchangeClustering\n      config: default\n      split: test\n      revision: 6cbc1f7b2bc0622f2e39d2c77fa502909748c259\n    metrics:\n    - type: v_measure\n      value: 57.525863806168566\n  - task:\n      type: Clustering\n    dataset:\n      type: mteb/stackexchange-clustering-p2p\n      name: MTEB StackExchangeClusteringP2P\n      config: default\n      split: test\n      revision: 815ca46b2622cec33ccafc3735d572c266efdb44\n    metrics:\n    - type: v_measure\n      value: 32.68850574423839\n  - task:\n      type: Reranking\n    dataset:\n      type: mteb/stackoverflowdupquestions-reranking\n      name: MTEB StackOverflowDupQuestions\n      config: default\n      split: test\n      revision: e185fbe320c72810689fc5848eb6114e1ef5ec69\n    metrics:\n    - type: map\n      value: 49.71580650644033\n    - type: mrr\n      value: 50.50971903913081\n  - task:\n      type: Summarization\n    dataset:\n      type: mteb/summeval\n      name: MTEB SummEval\n      config: default\n      split: test\n      revision: cda12ad7615edc362dbf25a00fdd61d3b1eaf93c\n    metrics:\n    - type: cos_sim_pearson\n      value: 29.152190498799484\n    - type: cos_sim_spearman\n      value: 29.686180371952727\n    - type: dot_pearson\n      value: 27.248664793816342\n    - type: dot_spearman\n      value: 28.37748983721745\n  - task:\n      type: Retrieval\n    dataset:\n      type: trec-covid\n      name: MTEB TRECCOVID\n      config: default\n      split: test\n      revision: None\n    metrics:\n    - type: map_at_1\n      value: 0.20400000000000001\n    - type: map_at_10\n      value: 1.6209999999999998\n    - type: map_at_100\n      value: 9.690999999999999\n    - type: map_at_1000\n      value: 23.733\n    - type: map_at_3\n      value: 0.575\n    - type: map_at_5\n      value: 0.885\n    - type: mrr_at_1\n      value: 78\n    - type: mrr_at_10\n      value: 86.56700000000001\n    - type: mrr_at_100\n      value: 86.56700000000001\n    - type: mrr_at_1000\n      value: 86.56700000000001\n    - type: mrr_at_3\n      value: 85.667\n    - type: mrr_at_5\n      value: 86.56700000000001\n    - type: ndcg_at_1\n      value: 76\n    - type: ndcg_at_10\n      value: 71.326\n    - type: ndcg_at_100\n      value: 54.208999999999996\n    - type: ndcg_at_1000\n      value: 49.252\n    - type: ndcg_at_3\n      value: 74.235\n    - type: ndcg_at_5\n      value: 73.833\n    - type: precision_at_1\n      value: 78\n    - type: precision_at_10\n      value: 74.8\n    - type: precision_at_100\n      value: 55.50000000000001\n    - type: precision_at_1000\n      value: 21.836\n    - type: precision_at_3\n      value: 78\n    - type: precision_at_5\n      value: 78\n    - type: recall_at_1\n      value: 0.20400000000000001\n    - type: recall_at_10\n      value: 1.894\n    - type: recall_at_100\n      value: 13.245999999999999\n    - type: recall_at_1000\n      value: 46.373\n    - type: recall_at_3\n      value: 0.613\n    - type: recall_at_5\n      value: 0.991\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (sqi-eng)\n      config: sqi-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 95.89999999999999\n    - type: f1\n      value: 94.69999999999999\n    - type: precision\n      value: 94.11666666666667\n    - type: recall\n      value: 95.89999999999999\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (fry-eng)\n      config: fry-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 68.20809248554913\n    - type: f1\n      value: 63.431048720066066\n    - type: precision\n      value: 61.69143958161298\n    - type: recall\n      value: 68.20809248554913\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (kur-eng)\n      config: kur-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 71.21951219512195\n    - type: f1\n      value: 66.82926829268293\n    - type: precision\n      value: 65.1260162601626\n    - type: recall\n      value: 71.21951219512195\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (tur-eng)\n      config: tur-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.2\n    - type: f1\n      value: 96.26666666666667\n    - type: precision\n      value: 95.8\n    - type: recall\n      value: 97.2\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (deu-eng)\n      config: deu-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 99.3\n    - type: f1\n      value: 99.06666666666666\n    - type: precision\n      value: 98.95\n    - type: recall\n      value: 99.3\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (nld-eng)\n      config: nld-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 97.39999999999999\n    - type: f1\n      value: 96.63333333333333\n    - type: precision\n      value: 96.26666666666668\n    - type: recall\n      value: 97.39999999999999\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ron-eng)\n      config: ron-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 96\n    - type: f1\n      value: 94.86666666666666\n    - type: precision\n      value: 94.31666666666668\n    - type: recall\n      value: 96\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ang-eng)\n      config: ang-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 47.01492537313433\n    - type: f1\n      value: 40.178867566927266\n    - type: precision\n      value: 38.179295828549556\n    - type: recall\n      value: 47.01492537313433\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (ido-eng)\n      config: ido-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 86.5\n    - type: f1\n      value: 83.62537480063796\n    - type: precision\n      value: 82.44555555555554\n    - type: recall\n      value: 86.5\n  - task:\n      type: BitextMining\n    dataset:\n      type: mteb/tatoeba-bitext-mining\n      name: MTEB Tatoeba (jav-eng)\n      config: jav-eng\n      split: test\n      revision: 9080400076fbadbb4c4dcb136ff4eddc40b42553\n    metrics:\n    - type: accuracy\n      value: 80.48780487804879\n    - type: f1\n      value: 75.45644599303138\n    - type: precision\n      value: 73.37398373983739\n    - type: recall\n      value: 80.48780487804879\n  - task:\n      type: BitextMining\n    dataset:\n   ', '{"pipeline_tag":"feature-extraction","library_name":"sentence-transformers","framework":"sentence-transformers","params":559890946,"storage_bytes":10114130126,"files_count":23,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["XLMRobertaModel"],"model_type":"xlm-roberta","tokenizer_config":{"bos_token":"<s>","cls_token":"<s>","eos_token":"</s>","mask_token":{"__type":"AddedToken","content":"<mask>","lstrip":true,"normalized":true,"rstrip":false,"single_word":false},"pad_token":"<pad>","sep_token":"</s>","unk_token":"<unk>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2402.05672","source_url":"https://arxiv.org/abs/2402.05672"},{"type":"based_on_paper","target_id":"arxiv:2108.08787","source_url":"https://arxiv.org/abs/2108.08787"},{"type":"based_on_paper","target_id":"arxiv:2104.08663","source_url":"https://arxiv.org/abs/2104.08663"},{"type":"based_on_paper","target_id":"arxiv:2210.07316","source_url":"https://arxiv.org/abs/2210.07316"}]', NULL, 'MIT', 'approved', 80, 'bd0e66d16cf3efedabeb0bf058482cfa', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-sentence-transformers-paraphrase-multilingual-MiniLM-L12-v2', 'huggingface--sentence-transformers--paraphrase-multilingual-minilm-l12-v2', 'paraphrase-multilingual-MiniLM-L12-v2', 'sentence-transformers', '--- language: - multilingual - ar - bg - ca - cs - da - de - el - en - es - et - fa - fi - fr - gl - gu - he - hi - hr - hu - hy - id - it - ja - ka - ko - ku - lt - lv - mk - mn - mr - ms - my - nb - nl - pl - pt - ro - ru - sk - sl - sq - sr - sv - th - tr - uk - ur - vi license: apache-2.0 library_name: sentence-transformers tags: - sentence-transformers - feature-extraction - sentence-similarity - transformers language_bcp47: - fr-ca - pt-br - zh-cn - zh-tw pipeline_tag: sentence-similari...', '["sentence-transformers","pytorch","tf","onnx","safetensors","openvino","bert","feature-extraction","sentence-similarity","transformers","multilingual","ar","bg","ca","cs","da","de","el","en","es","et","fa","fi","fr","gl","gu","he","hi","hr","hu","hy","id","it","ja","ka","ko","ku","lt","lv","mk","mn","mr","ms","my","nb","nl","pl","pt","ro","ru","sk","sl","sq","sr","sv","th","tr","uk","ur","vi","arxiv:1908.10084","license:apache-2.0","text-embeddings-inference","endpoints_compatible","region:us"]', 'sentence-similarity', 1072, 16352771, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- multilingual\n- ar\n- bg\n- ca\n- cs\n- da\n- de\n- el\n- en\n- es\n- et\n- fa\n- fi\n- fr\n- gl\n- gu\n- he\n- hi\n- hr\n- hu\n- hy\n- id\n- it\n- ja\n- ka\n- ko\n- ku\n- lt\n- lv\n- mk\n- mn\n- mr\n- ms\n- my\n- nb\n- nl\n- pl\n- pt\n- ro\n- ru\n- sk\n- sl\n- sq\n- sr\n- sv\n- th\n- tr\n- uk\n- ur\n- vi\nlicense: apache-2.0\nlibrary_name: sentence-transformers\ntags:\n- sentence-transformers\n- feature-extraction\n- sentence-similarity\n- transformers\nlanguage_bcp47:\n- fr-ca\n- pt-br\n- zh-cn\n- zh-tw\npipeline_tag: sentence-similarity\n---\n\n# sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n\nThis is a [sentence-transformers](https://www.SBERT.net) model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n\n\n\n## Usage (Sentence-Transformers)\n\nUsing this model becomes easy when you have [sentence-transformers](https://www.SBERT.net) installed:\n\n```\npip install -U sentence-transformers\n```\n\nThen you can use the model like this:\n\n```python\nfrom sentence_transformers import SentenceTransformer\nsentences = ["This is an example sentence", "Each sentence is converted"]\n\nmodel = SentenceTransformer(''sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n\n\n## Usage (HuggingFace Transformers)\nWithout [sentence-transformers](https://www.SBERT.net), you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n```python\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = [''This is an example sentence'', ''Each sentence is converted'']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained(''sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'')\nmodel = AutoModel.from_pretrained(''sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=''pt'')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input[''attention_mask''])\n\nprint("Sentence embeddings:")\nprint(sentence_embeddings)\n```\n\n\n\n## Full Model Architecture\n```\nSentenceTransformer(\n  (0): Transformer({''max_seq_length'': 128, ''do_lower_case'': False}) with Transformer model: BertModel \n  (1): Pooling({''word_embedding_dimension'': 384, ''pooling_mode_cls_token'': False, ''pooling_mode_mean_tokens'': True, ''pooling_mode_max_tokens'': False, ''pooling_mode_mean_sqrt_len_tokens'': False})\n)\n```\n\n## Citing & Authors\n\nThis model was trained by [sentence-transformers](https://www.sbert.net/). \n        \nIf you find this model helpful, feel free to cite our publication [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084):\n```bibtex \n@inproceedings{reimers-2019-sentence-bert,\n    title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",\n    author = "Reimers, Nils and Gurevych, Iryna",\n    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",\n    month = "11",\n    year = "2019",\n    publisher = "Association for Computational Linguistics",\n    url = "http://arxiv.org/abs/1908.10084",\n}\n```', '{"pipeline_tag":"sentence-similarity","library_name":"sentence-transformers","framework":"sentence-transformers","params":117654272,"storage_bytes":6650862358,"files_count":28,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BertModel"],"model_type":"bert","tokenizer_config":{"unk_token":"<unk>","sep_token":"</s>","pad_token":"<pad>","cls_token":"<s>","mask_token":{"content":"<mask>","single_word":false,"lstrip":true,"rstrip":false,"normalized":true,"__type":"AddedToken"},"bos_token":"<s>","eos_token":"</s>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1908.10084","source_url":"https://arxiv.org/abs/1908.10084"}]', NULL, 'Apache-2.0', 'approved', 65, '187022630cdc9060fbcdd645d95f89af', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-235B-A22B', 'huggingface--qwen--qwen3-235b-a22b', 'Qwen3-235B-A22B', 'Qwen', '--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-235B-A22B/blob/main/LICENSE pipeline_tag: text-generation --- <a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of d...', '["transformers","safetensors","qwen3_moe","text-generation","conversational","arxiv:2309.00071","arxiv:2505.09388","license:apache-2.0","endpoints_compatible","region:us"]', 'text-generation', 1056, 365681, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-235B-A22B","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-235B-A22B/blob/main/LICENSE\npipeline_tag: text-generation\n---\n\n# Qwen3-235B-A22B\n<a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-235B-A22B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 235B in total and 22B activated\n- Number of Paramaters (Non-Embedding): 234B\n- Number of Layers: 94\n- Number of Attention Heads (GQA): 64 for Q and 4 for KV\n- Number of Experts: 128\n- Number of Activated Experts: 8\n- Context Length: 32,768 natively and [131,072 tokens with YaRN](#processing-long-texts). \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Quickstart\n\nThe code of Qwen3-MoE has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: ''qwen3_moe''\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen3-235B-A22B"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\n\n# prepare the model input\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")\n\nprint("thinking content:", thinking_content)\nprint("content:", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-235B-A22B --reasoning-parser qwen3 --tp 8\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-235B-A22B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model''s thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model''s behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model''s thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name="Qwen/Qwen3-235B-A22B"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{"role": "user", "content": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors="pt")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({"role": "user", "content": user_input})\n        self.history.append({"role": "assistant", "content": response})\n\n        return response\n\n# Example Usage\nif __name__ == "__main__":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = "How many r''s in strawberries?"\n    print(f"User: {user_input_1}")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f"Bot: {response_1}")\n    print("----------------------")\n\n    # Second input with /no_think\n    user_input_2 = "Then, how many r''s in blueberries? /no_think"\n    print(f"User: {user_input_2}")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f"Bot: {response_2}") \n    print("----------------------")\n\n    # Third input with /think\n    user_input_3 = "Really? /think"\n    print(f"User: {user_input_3}")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f"Bot: {response_3}")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    ''model'': ''Qwen3-235B-A22B'',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # ''model_type'': ''qwen_dashscope'',\n    # ''api_key'': os.getenv(''DASHSCOPE_API_KEY''),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    ''model_server'': ''http://localhost:8000/v1'',  # api_base\n    ''api_key'': ''EMPTY'',\n\n    # Other parameters:\n    # ''generate_cfg'': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         ''thought_in_content'': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {''mcpServers'': {  # You can specify the MCP configuration file\n            ''time'': {\n                ''command'': ''uvx'',\n                ''args'': [''mcp-server-time'', ''--local-timezone=Asia/Shanghai'']\n            },\n            "fetch": {\n                "command": "uvx",\n                "args": ["mcp-server-fetch"]\n            }\n        }\n    },\n  ''code_interpreter'',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{''role'': ''user'', ''content'': ''https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen''}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Processing Long Texts\n\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model''s performance on context lengths of up to 131,072 tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers` and `llama.cpp` for local use, `vllm` and `sglang` for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\n\n- Modifying the model files:\n  In the `config.json` file, add the `rope_scaling` fields:\n    ```json\n    {\n        ...,\n        "rope_scaling": {\n            "rope_type": "yarn",\n            "factor": 4.0,\n            "original_max_position_embeddings": 32768\n        }\n    }\n    ```\n  For `llama.cpp`, you need to regenerate the GGUF file after the modification.\n\n- Passing command line arguments:\n\n  For `vllm`, you can use\n    ```shell\n    vllm serve ... --rope-scaling ''{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}'' --max-model-len 131072  \n    ```\n\n  For `sglang`, you can use\n    ```shell\n    python -m sglang.launch_server ... --json-model-override-args ''{"rope_scaling":{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}}''\n    ```\n\n  For `llama-server` from `llama.cpp`, you can use\n    ```shell\n    llama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\n    ```\n\n> [!IMPORTANT]\n> If you encounter the following warning\n> ```\n> Unrecognized keys in `rope_scaling` for ''rope_type''=''yarn'': {''original_max_position_embeddings''}\n> ```\n> please upgrade `transformers>=4.51.0`.\n\n> [!NOTE]\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set `factor` as 2.0. \n\n> [!NOTE]\n> The default `max_position_embeddings` in `config.json` is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\n\n> [!TIP]\n> The endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include "Please reason step by step, and put your final answer within \boxed{}." in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: "Please show your choice in the `answer` field with only the choice letter, e.g., `"answer": "C"`."\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":235093634560,"storage_bytes":470203304443,"files_count":128,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen3MoeForCausalLM"],"model_type":"qwen3_moe","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(''<tool_response>'') and message.content.endswith(''</tool_response>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if ''</think>'' in content %}\n                {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n                {%- set content = content.split(''</think>'')[-1].lstrip(''\\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- ''<|im_start|>'' + message.role + ''\\n<think>\\n'' + reasoning_content.strip(''\\n'') + ''\\n</think>\\n\\n'' + content.lstrip(''\\n'') }}\n            {%- else %}\n                {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- ''<think>\\n\\n</think>\\n\\n'' }}\n    {%- endif %}\n{%- endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen3","source_url":"https://github.com/QwenLM/Qwen3"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Agent","source_url":"https://github.com/QwenLM/Qwen-Agent"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2505.09388","source_url":"https://arxiv.org/abs/2505.09388"}]', NULL, 'Apache-2.0', 'approved', 80, 'df01f706f5623d81eef465ce16ff9248', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-ibm-granite-granite-docling-258M', 'huggingface--ibm-granite--granite-docling-258m', 'granite-docling-258M', 'ibm-granite', '--- license: apache-2.0 datasets: - ds4sd/SynthCodeNet - ds4sd/SynthFormulaNet - ds4sd/SynthChartNet - HuggingFaceM4/DoclingMatix tags: - text-generation - documents - code - formula - chart - ocr - layout - table - document-parse - docling - granite - extraction - math language: - en pipeline_tag: image-text-to-text library_name: transformers --- <div style="display: flex; align-items: center;"> <img src="https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/granite_docling.pn...', '["transformers","safetensors","idefics3","image-to-text","text-generation","documents","code","formula","chart","ocr","layout","table","document-parse","docling","granite","extraction","math","image-text-to-text","conversational","en","dataset:ds4sd/synthcodenet","dataset:ds4sd/synthformulanet","dataset:ds4sd/synthchartnet","dataset:huggingfacem4/doclingmatix","arxiv:2501.17887","arxiv:2503.11576","arxiv:2305.03393","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'image-text-to-text', 1041, 100708, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/ibm-granite/granite-docling-258M","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ndatasets:\n- ds4sd/SynthCodeNet\n- ds4sd/SynthFormulaNet\n- ds4sd/SynthChartNet\n- HuggingFaceM4/DoclingMatix\ntags:\n- text-generation\n- documents\n- code\n- formula\n- chart\n- ocr\n- layout\n- table\n- document-parse\n- docling\n- granite\n- extraction\n- math\nlanguage:\n- en\npipeline_tag: image-text-to-text\nlibrary_name: transformers\n---\n   \n# granite-docling-258m\n<div style="display: flex; align-items: center;">\n    <img src="https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/granite_docling.png" alt="Granite Docling Logo" style="width: 200px; height: auto; margin-right: 20px;">\n    <div>\n        <p>Granite Docling is a multimodal Image-Text-to-Text model engineered for efficient document conversion. It preserves the core features of Docling while maintaining seamless integration with <a href="https://docling-project.github.io/docling ">DoclingDocuments</a> to ensure full compatibility. </p>\n    </div>\n</div>\n\n**Model Summary**: \n\nGranite Docling 258M builds upon the Idefics3 architecture, but introduces two key modifications: it replaces the vision encoder with siglip2-base-patch16-512 and substitutes the language model with a Granite 165M LLM. Try out our [Granite-Docling-258](https://huggingface.co/spaces/ibm-granite/granite-docling-258m-demo) demo today.\n\n- **Developed by**: IBM Research\n- **Model type**: Multi-modal model (image+text-to-text)\n- **Language(s)**: English (NLP)\n- **License**: [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n- **Release Date**: September 17, 2025\n\nGranite-docling-258M is fully integrated into the Docling pipelines, carrying over existing [features](https://huggingface.co/ds4sd/SmolDocling-256M-preview) while introducing a number of powerful new features, including:\n\n- üî¢ Enhanced Equation Recognition: More accurate detection and formatting of mathematical formulas\n- üß© Flexible Inference Modes: Choose between full-page inference, bbox-guided region inference\n- üßò Improved Stability: Tends to avoid infinite loops more effectively\n- üßÆ Enhanced Inline Equations: Better inline math recognition\n- üßæ Document Element QA: Answer questions about a document‚Äôs structure such as the presence and order of document elements\n- üåç Japanese, Arabic and Chinese support (_experimental_)\n\n\n\n## Getting started\n\nThe easiest way to use this model is through the [üê•Docling](https://github.com/docling-project/docling) library. It will automatically download this model and convert documents to various formats for you. \n\nInstall the latest version of `docling` through pip, then use the following CLI command:\n\n```sh\n# Convert to HTML and Markdown:\ndocling --to html --to md --pipeline vlm --vlm-model granite_docling "https://arxiv.org/pdf/2501.17887" # accepts files, urls or directories\n\n# Convert to HTML including layout visualization:\ndocling --to html_split_page --show-layout --pipeline vlm --vlm-model granite_docling "https://arxiv.org/pdf/2501.17887"\n\n```\n\n<p align="center">\n<img src="https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/assets/granite_docling_split_page.png" alt="GraniteDocling result in split page view" width="900"/>\n</p>\n\n<details>\n<summary>You can also set this model up within the Docling SDK:</summary>\n  \n```python\nfrom docling.datamodel import vlm_model_specs\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import (\n    VlmPipelineOptions,\n)\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\nfrom docling.pipeline.vlm_pipeline import VlmPipeline\n\nsource = "https://arxiv.org/pdf/2501.17887"\n\n###### USING SIMPLE DEFAULT VALUES\n# - GraniteDocling model\n# - Using the transformers framework\n\nconverter = DocumentConverter(\n    format_options={\n        InputFormat.PDF: PdfFormatOption(\n            pipeline_cls=VlmPipeline,\n        ),\n    }\n)\n\ndoc = converter.convert(source=source).document\n\nprint(doc.export_to_markdown())\n\n\n###### USING MACOS MPS ACCELERATOR\n# For more options see the compare_vlm_models.py example.\n\npipeline_options = VlmPipelineOptions(\n    vlm_options=vlm_model_specs.GRANITEDOCLING_MLX,\n)\n\nconverter = DocumentConverter(\n    format_options={\n        InputFormat.PDF: PdfFormatOption(\n            pipeline_cls=VlmPipeline,\n            pipeline_options=pipeline_options,\n        ),\n    }\n)\n\ndoc = converter.convert(source=source).document\n\nprint(doc.export_to_markdown())\n```\n</details>\n\n\nAlternatively, you can use bare **transformers**, **vllm**, **onnx** or **mlx-vlm** to perform inference, and [docling-core](https://github.com/docling-project/docling-core) APIs to convert results to variety of output formats (md, html, etc.):\n\n<details>\n<summary>üìÑ Single page image inference using plain ü§ó tranformers ü§ñ</summary>\n\n```python\n# Prerequisites:\n# pip install torch\n# pip install docling_core\n# pip install transformers\n\nimport torch\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\nfrom transformers import AutoProcessor, AutoModelForVision2Seq\nfrom transformers.image_utils import load_image\nfrom pathlib import Path\n\nDEVICE = "cuda" if torch.cuda.is_available() else "cpu"\n\n# Load images\nimage = load_image("https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/assets/new_arxiv.png")\n\n# Initialize processor and model\nprocessor = AutoProcessor.from_pretrained("ibm-granite/granite-docling-258M")\nmodel = AutoModelForVision2Seq.from_pretrained(\n    "ibm-granite/granite-docling-258M",\n    torch_dtype=torch.bfloat16,\n    _attn_implementation="flash_attention_2" if DEVICE == "cuda" else "sdpa",\n).to(DEVICE)\n\n# Create input messages\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image"},\n            {"type": "text", "text": "Convert this page to docling."}\n        ]\n    },\n]\n\n# Prepare inputs\nprompt = processor.apply_chat_template(messages, add_generation_prompt=True)\ninputs = processor(text=prompt, images=[image], return_tensors="pt")\ninputs = inputs.to(DEVICE)\n\n# Generate outputs\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\nprompt_length = inputs.input_ids.shape[1]\ntrimmed_generated_ids = generated_ids[:, prompt_length:]\ndoctags = processor.batch_decode(\n    trimmed_generated_ids,\n    skip_special_tokens=False,\n)[0].lstrip()\n\nprint(f"DocTags: \n{doctags}\n")\n\n\n# Populate document\ndoctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])\n# create a docling document\ndoc = DoclingDocument.load_from_doctags(doctags_doc, document_name="Document")\nprint(f"Markdown:\n{doc.export_to_markdown()}\n")\n\n## export as any format.\n# Path("out/").mkdir(parents=True, exist_ok=True)\n# HTML:\n# output_path_html = Path("out/") / "example.html"\n# doc.save_as_html(output_path_html)\n# Markdown:\n# output_path_md = Path("out/") / "example.md"\n# doc.save_as_markdown(output_path_md)\n\n```\n</details>\n\n\n<details>\n<summary> üöÄ Fast Batch Inference with VLLM</summary>\n\n```python\n# Prerequisites:\n# pip install vllm\n# pip install docling_core\n# place page images you want to convert into "img/" dir\n\nimport time\nimport os\nfrom vllm import LLM, SamplingParams\nfrom transformers import AutoProcessor\nfrom PIL import Image\nfrom docling_core.types.doc import DoclingDocument\nfrom docling_core.types.doc.document import DocTagsDocument\nfrom pathlib import Path\n\n# Configuration\nMODEL_PATH = "ibm-granite/granite-docling-258M"\nIMAGE_DIR = "img/"  # Place your page images here\nOUTPUT_DIR = "out/"\nPROMPT_TEXT = "Convert this page to docling."\n\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {"type": "image"},\n            {"type": "text", "text": PROMPT_TEXT},\n        ],\n    },\n]\n\n\n# Ensure output directory exists\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Initialize LLM\nllm = LLM(model=MODEL_PATH, revision="untied", limit_mm_per_prompt={"image": 1})\nprocessor = AutoProcessor.from_pretrained(MODEL_PATH)\n\nsampling_params = SamplingParams(\n    temperature=0.0,\n    max_tokens=8192,\n    skip_special_tokens=False,\n)\n\n# Load and prepare all images and prompts up front\nbatched_inputs = []\nimage_names = []\n\nfor img_file in sorted(os.listdir(IMAGE_DIR)):\n    if img_file.lower().endswith((".png", ".jpg", ".jpeg")):\n        img_path = os.path.join(IMAGE_DIR, img_file)\n        with Image.open(img_path) as im:\n            image = im.convert("RGB")\n\n        prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n        batched_inputs.append({"prompt": prompt, "multi_modal_data": {"image": image}})\n        image_names.append(os.path.splitext(img_file)[0])\n\n# Run batch inference\nstart_time = time.time()\noutputs = llm.generate(batched_inputs, sampling_params=sampling_params)\n\n# Postprocess all results\nfor img_fn, output, input_data in zip(image_names, outputs, batched_inputs):\n    doctags = output.outputs[0].text\n    output_path_dt = Path(OUTPUT_DIR) / f"{img_fn}.dt"\n    output_path_md = Path(OUTPUT_DIR) / f"{img_fn}.md"\n\n    with open(output_path_dt, "w", encoding="utf-8") as f:\n        f.write(doctags)\n\n    # Convert to DoclingDocument and save markdown\n    doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [input_data["multi_modal_data"]["image"]])\n    doc = DoclingDocument.load_from_doctags(doctags_doc, document_name="Document")\n    doc.save_as_markdown(output_path_md)\n\nprint(f"Total time: {time.time() - start_time:.2f} sec")\n\n```\n</details>\n\nüíª Local inference on Apple Silicon with MLX: [see here](https://huggingface.co/ibm-granite/granite-docling-258M-mlx)\n\n‚ÑπÔ∏è If you see trouble running granite-docling with the codes above, check the troubleshooting section at the bottom ‚¨áÔ∏è. \n\n## Intended Use \nGranite-Docling is designed to complement the Docling library, not replace it. It integrates as a component within larger Docling library, consolidating the functions of multiple single-purpose models into a single, compact VLM. \nHowever, Granite-Docling is **not** intended for general image understanding. For tasks focused solely on image-text input, we recommend using [Granite Vision models](https://huggingface.co/collections/ibm-granite/granite-vision-models-67b3bd4ff90c915ba4cd2800), which are purpose-built and optimized for image-text processing.\n\n## Evaluations\nA comprehensive discussion of evaluation methods and findings has already been presented in our previous publication [[citation](https://arxiv.org/pdf/2503.11576)]. As this model is an update, we refer readers to that work for additional details.\nThe evaluation can be performed using the [docling-eval](https://github.com/docling-project/docling-eval) framework for the document related tasks, and [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval) for MMStar and OCRBench.\n\n<table>\n  <thead>\n    <tr><th colspan="5"><b>Layout</b></th></tr>\n    <tr>\n      <th></th>\n      <th>MAP ‚Üë</th>\n      <th>F1 ‚Üë</th>\n      <th>Precision ‚Üë</th>\n      <th>Recall ‚Üë</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.23</td><td>0.85</td><td>0.9</td><td>0.84</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.27</b></td><td><b>0.86</b></td><td><b>0.92</b></td><td><b>0.88</b></td>\n    </tr>\n  </tbody>\n</table>\n\n<table>\n  <thead>\n    <tr><th colspan="7"><b>Full Page OCR</b></th></tr>\n    <tr>\n      <th></th>\n      <th>Edit-distance ‚Üì</th>\n      <th>F1 ‚Üë</th>\n      <th>Precision ‚Üë</th>\n      <th>Recall ‚Üë</th>\n      <th>BLEU ‚Üë</th>\n      <th>Meteor ‚Üë</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.48</td><td>0.80</td><td>0.89</td>\n      <td>0.79</td><td>0.58</td><td>0.67</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.45</b></td><td><b>0.84</b></td><td><b>0.91</b></td>\n      <td><b>0.83</b></td><td><b>0.65</b></td><td><b>0.72</b></td>\n    </tr>\n  </tbody>\n  <thead>\n    <tr><th colspan="7"><b>Code Recognition</b></th></tr>\n    <tr>\n      <th></th>\n      <th>Edit-distance ‚Üì</th>\n      <th>F1 ‚Üë</th>\n      <th>Precision ‚Üë</th>\n      <th>Recall ‚Üë</th>\n      <th>BLEU ‚Üë</th>\n      <th>Meteor ‚Üë</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.114</td><td>0.915</td><td>0.94</td><td>0.909</td><td>0.875</td><td>0.889</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.013</b></td><td><b>0.988</b></td><td><b>0.99</b></td><td><b>0.988</b></td>\n      <td><b>0.983</b></td><td><b>0.986</b></td>\n    </tr>\n  </tbody>\n  <thead>\n    <tr><th colspan="7"><b>Equation Recognition</b></th></tr>\n    <tr>\n      <th></th>\n      <th>Edit-distance ‚Üì</th>\n      <th>F1 ‚Üë</th>\n      <th>Precision ‚Üë</th>\n      <th>Recall ‚Üë</th>\n      <th>BLEU ‚Üë</th>\n      <th>Meteor ‚Üë</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.119</td><td>0.947</td><td>0.959</td><td>0.941</td><td>0.824</td><td>0.878</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.073</b></td><td><b>0.968</b></td><td><b>0.968</b></td><td><b>0.969</b></td>\n      <td><b>0.893</b></td><td><b>0.927</b></td>\n    </tr>\n  </tbody>\n</table>\n<table>\n  <thead>\n    <tr><th colspan="3"><b>Table Recognition (FinTabNet 150dpi)</b></th></tr>\n    <tr>\n      <th></th>\n      <th>TEDS (structure) ‚Üë</th>\n      <th>TEDS (w/content) ‚Üë</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.82</td><td>0.76</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.97</b></td><td><b>0.96</b></td>\n    </tr>\n  </tbody>\n</table>\n<table>\n  <thead>\n    <tr><th colspan="3"><b>Other Benchmarks</b></th></tr>\n    <tr>\n      <th></th>\n      <th>MMStar ‚Üë</th>\n      <th>OCRBench ‚Üë</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><b>smoldocling-256m-preview</b></td>\n      <td>0.17</td><td>338</td>\n    </tr>\n    <tr>\n      <td><b>granite-docling-258m</b></td>\n      <td><b>0.30</b></td><td><b>500</b></td>\n    </tr>\n  </tbody>\n</table>\n\n\n\nüíª Local inference on Apple Silicon with MLX: [see here](https://huggingface.co/ibm-granite/granite-docling-258M-mlx)\n\n\n## Supported Instructions\n\n<table>\n  <tr>\n    <th>Description</th>\n    <th>Instruction</th>\n    <th>Short Instruction</th>\n  </tr>\n  <tr>\n    <td><b>Full conversion</b></td>\n    <td>Convert this page to docling.</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td><b>Chart</b></td>\n    <td>Convert chart to table.</td>\n    <td><code>&lt;chart&gt;</code></td>\n  </tr>\n  <tr>\n    <td><b>Formula</b></td>\n    <td>Convert formula to LaTeX.</td>\n    <td><code>&lt;formula&gt;</code></td>\n  </tr>\n  <tr>\n    <td><b>Code</b></td>\n    <td>Convert code to text.</td>\n    <td><code>&lt;code&gt;</code></td>\n  </tr>\n  <tr>\n    <td><b>Table</b></td>\n    <td>Convert table to OTSL. (<a href="https://arxiv.org/pdf/2305.03393">Lysak et al., 2023</a>)</td>\n    <td><code>&lt;otsl&gt;</code></td>\n  </tr>\n  <tr>\n    <td rowspan="4"><b>Actions and Pipelines</b></td>\n    <td>OCR the text in a specific location: &lt;loc_155&gt;&lt;loc_233&gt;&lt;loc_206&gt;&lt;loc_237&gt;</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Identify element at: &lt;loc_247&gt;&lt;loc_482&gt;&lt;loc_252&gt;&lt;loc_486&gt;</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Find all ''text'' elements on the page, retrieve all section headers.</td>\n    <td>-</td>\n  </tr>\n  <tr>\n    <td>Detect footer elements on the page.</td>\n    <td>-</td>\n  </tr>\n</table>\n\n\n\n# Model Architecture:\n\nThe architecture of granite-docling-258m consists of the following components:\n\n(1) Vision encoder: [siglip2-base-patch16-512](https://huggingface.co/google/siglip2-base-patch16-512).\n\n(2) Vision-language connector: pixel shuffle projector (as in idefics3) \n\n(3) Large language model: Granite 165M.\n\nWe built upon [Idefics3](https://huggingface.co/docs/transformers/en/model_doc/idefics3) to train our model. We incorporated DocTags into our LLM‚Äôs supervised fine-tuning (SFT) data to help the model become familiar with the format, enabling faster convergence and mitigating issues previously observed with SmolDocling.\nThe model was trained using the [nanoVLM](https://github.com/huggingface/nanoVLM) framework, which provides a lightweight and efficient training setup for vision-language models\n\n\n**Training Data**: Our training corpus consists of two principal sources: (1) publicly available datasets and (2) internally constructed synthetic datasets designed to elicit specific document understanding capabilities.\n\nIn particular, we incorporate:\n\n* [**SynthCodeNet**](https://huggingface.co/datasets/ds4sd/SynthCodeNet) ‚Äî a large-scale collection of synthetically rendered code snippets spanning over 50 programming languages\n* [**SynthFormulaNet**](https://huggingface.co/datasets/ds4sd/SynthFormulaNet) ‚Äî a dataset of synthetic mathematical expressions paired with ground-truth LaTeX representations\n* [**SynthChartNet**](https://huggingface.co/datasets/ds4sd/SynthChartNet) ‚Äî synthetic chart images annotated with structured table outputs\n* [**DoclingMatix**](https://huggingface.co/datasets/HuggingFaceM4/DoclingMatix) ‚Äî a curated corpus of real-world document pages sampled from diverse domains\n\n\n**Infrastructure**: We train granite-docling-258m using IBM''s super computing cluster, Blue Vela, which is outfitted with NVIDIA H100 GPUs. This cluster provides a scalable and efficient infrastructure for training our models over thousands of GPUs.\n\n**Responsible Use and Limitations** Some use cases for Vision Language Models can trigger certain risks and ethical considerations, including but not limited to: bias and fairness, misinformation, and autonomous decision-making. \nAlthough our alignment processes include safety considerations, the model may in some cases produce inaccurate, biased, offensive or unwanted responses to user prompts. Additionally, whether smaller models may exhibit increased susceptibility \nto hallucination in generation scenarios due to their reduced sizes, which could limit their ability to generate coherent and contextually accurate responses, remains uncertain. This aspect is currently an active area of research, \nand we anticipate more rigorous exploration, comprehension, and mitigations in this domain. We urge the community to use granite-docling-258m in a responsible way and avoid any malicious utilization. We recommend using this model only as part of the Docling library.\nMore general vision tasks may pose higher inherent risks of triggering unwanted output. To enhance safety, we recommend using granite-docling-258m alongside Granite Guardian. Granite Guardian is a fine-tuned instruct model designed to detect and flag risks in prompts and responses across key dimensions outlined in the IBM AI Risk Atlas.\nIts training, which includes both human-annotated and synthetic data informed by internal red-teaming, enables it to outperform similar open-source models on standard benchmarks, providing an additional layer of safety.\n\n**Resources**\n\n- ‚≠êÔ∏è Learn about the latest updates with Docling: https://docling-project.github.io/docling/#features\n- üöÄ Get started with Docling concepts, integrations and tutorials: https://docling-project.github.io/docling/getting_started/\n- üí° Learn about the latest Granite learning resources: https://ibm.biz/granite-learning-resources\n- üñ•Ô∏è Learn more about how to use Granite-Docling, explore the Docling library, and see what‚Äôs coming next for Docling in the release blog: https://ibm.com/new/announcements/granite-docling-end-to-end-document-conversion\n\n## Troubleshooting\n\n**Running with VLLM**\n\n1. You receive `AttributeError: ''LlamaModel'' object has no attribute ''wte''` when launching the model through VLLM.\n   \n    With current versions of VLLM (including 0.10.2), support for tied weights as used in granite-docling is limited and breaks. We provide a version with untied weights on the `untied` branch of this model repo.\n    To use the untied version, please pass the `revision` argument to VLLM:\n    \n    ```sh\n    # Serve the model through VLLM\n    $> vllm serve ibm-granite/granite-docling-258M --revision untied\n    ``` \n    \n    ```python\n    # If using the VLLM python SDK:\n    from vllm import LLM\n    ... \n\n    llm = LLM(model=MODEL_PATH, revision="untied", limit_mm_per_prompt={"image": 1})\n    ```\n\n2. The model outputs only exclamation marks (i.e. "!!!!!!!!!!!!!!!").\n\n   This is seen on older NVIDIA GPUs, such as the T4 GPU available in Google Colab, because it lacks support for `bfloat16` format.\n   You can work around it by setting the `dtype` to `float32`.\n\n   ```sh\n    # Serve the model through VLLM\n    $> vllm serve ibm-granite/granite-docling-258M --revision untied --dtype float32\n    ``` \n    \n    ```python\n    # If using the VLLM python SDK:\n    from vllm import LLM\n    ... \n\n    llm = LLM(model=MODEL_PATH, revision="untied", limit_mm_per_prompt={"image": 1}, dtype="float32")\n    ```\n\n    \n   \n\n\n', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":257517120,"storage_bytes":3045691426,"files_count":17,"spaces_count":11,"gated":false,"private":false,"config":{"architectures":["Idefics3ForConditionalGeneration"],"model_type":"idefics3","tokenizer_config":{"bos_token":"<|start_of_role|>","eos_token":"<|end_of_text|>","pad_token":"<|end_of_text|>","unk_token":"<|unk|>"},"chat_template_jinja":"{%- for message in messages -%}\n{{- ''<|start_of_role|>'' + message[''role''] + ''<|end_of_role|>'' -}}\n{%- if message[''content''] is string -%}\n{{- message[''content''] -}}\n{%- else -%}\n{%- for part in message[''content''] -%}\n{%- if part[''type''] == ''text'' -%}\n{{- part[''text''] -}}\n{%- elif part[''type''] == ''image'' -%}\n{{- ''<image>'' -}}\n{%- endif -%}\n{%- endfor -%}\n{%- endif -%}\n{{- ''<|end_of_text|>\n'' -}}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n{{- ''<|start_of_role|>assistant'' -}}\n{%- if controls -%}{{- '' '' + controls | tojson() -}}{%- endif -%}\n{{- ''<|end_of_role|>'' -}}\n{%- endif -%}\n"}}', '[]', '[{"type":"has_code","target_id":"github:docling-project:docling","source_url":"https://github.com/docling-project/docling"},{"type":"has_code","target_id":"github:docling-project:docling-core","source_url":"https://github.com/docling-project/docling-core"},{"type":"has_code","target_id":"github:docling-project:docling-eval","source_url":"https://github.com/docling-project/docling-eval"},{"type":"has_code","target_id":"github:EvolvingLMMs-Lab:lmms-eval","source_url":"https://github.com/EvolvingLMMs-Lab/lmms-eval"},{"type":"has_code","target_id":"github:huggingface:nanoVLM","source_url":"https://github.com/huggingface/nanoVLM"},{"type":"based_on_paper","target_id":"arxiv:2501.17887","source_url":"https://arxiv.org/abs/2501.17887"},{"type":"based_on_paper","target_id":"arxiv:2503.11576","source_url":"https://arxiv.org/abs/2503.11576"},{"type":"based_on_paper","target_id":"arxiv:2305.03393","source_url":"https://arxiv.org/abs/2305.03393"}]', NULL, 'Apache-2.0', 'approved', 100, '104efa0e537fc1c5f45e26b2264496fd', NULL, 'https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/assets/granite_docling_split_page.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-ibm-granite-granite-docling-258M from https://huggingface.co/ibm-granite/granite-docling-258M/resolve/main/assets/granite_docling_split_page.png
Image converted to WebP: data/images/huggingface-ibm-granite-granite-docling-258M.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-ProsusAI-finbert', 'huggingface--prosusai--finbert', 'finbert', 'ProsusAI', '--- language: "en" tags: - financial-sentiment-analysis - sentiment-analysis widget: - text: "Stocks rallied and the British pound gained." --- FinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. Financial PhraseBank by Malo et al. (2014) is used for fine-tuning. For more details, please see the ...', '["transformers","pytorch","tf","jax","bert","text-classification","financial-sentiment-analysis","sentiment-analysis","en","arxiv:1908.10063","endpoints_compatible","deploy:azure","region:us"]', 'text-classification', 1037, 2811323, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/ProsusAI/finbert","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage: "en"\ntags:\n- financial-sentiment-analysis\n- sentiment-analysis\nwidget:\n- text: "Stocks rallied and the British pound gained."\n---\n\nFinBERT is a pre-trained NLP model to analyze sentiment of financial text. It is built by further training the BERT language model in the finance domain, using a large financial corpus and thereby fine-tuning it for financial sentiment classification. [Financial PhraseBank](https://www.researchgate.net/publication/251231107_Good_Debt_or_Bad_Debt_Detecting_Semantic_Orientations_in_Economic_Texts) by Malo et al. (2014) is used for fine-tuning. For more details, please see the paper [FinBERT: Financial Sentiment Analysis with Pre-trained Language Models](https://arxiv.org/abs/1908.10063) and our related [blog post](https://medium.com/prosus-ai-tech-blog/finbert-financial-sentiment-analysis-with-bert-b277a3607101) on Medium.\n\nThe model will give softmax outputs for three labels: positive, negative or neutral.\n\n---\n\nAbout Prosus\n\nProsus is a global consumer internet group and one of the largest technology investors in the world. Operating and investing globally in markets with long-term growth potential, Prosus builds leading consumer internet companies that empower people and enrich communities. For more information, please visit www.prosus.com.\n\nContact information\n\nPlease contact Dogu Araci dogu.araci[at]prosus[dot]com and Zulkuf Genc zulkuf.genc[at]prosus[dot]com about any FinBERT related issues and questions.\n', '{"pipeline_tag":"text-classification","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":3504463245,"files_count":9,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BertForSequenceClassification"],"model_type":"bert","tokenizer_config":{"unk_token":"[UNK]","sep_token":"[SEP]","pad_token":"[PAD]","cls_token":"[CLS]","mask_token":"[MASK]"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1908.10063","source_url":"https://arxiv.org/abs/1908.10063"}]', NULL, NULL, 'pending', 40, '3a6324fdcec290f9f4032d51fd5adee9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-flan-t5-base', 'huggingface--google--flan-t5-base', 'flan-t5-base', 'google', '--- language: - en - fr - ro - de - multilingual tags: - text2text-generation widget: - text: "Translate to German: My name is Arthur" example_title: "Translation" - text: "Please answer to the following question. Who is going to be the next Ballon d''or?" example_title: "Question Answering" - text: "Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering." example_title: "Logical reasoning" - text: "Please answer the following question. What is t...', '["transformers","pytorch","tf","jax","safetensors","t5","text2text-generation","en","fr","ro","de","multilingual","dataset:svakulenk0/qrecc","dataset:taskmaster2","dataset:djaym7/wiki_dialog","dataset:deepmind/code_contests","dataset:lambada","dataset:gsm8k","dataset:aqua_rat","dataset:esnli","dataset:quasc","dataset:qed","arxiv:2210.11416","arxiv:1910.09700","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'other', 1025, 1055369, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/flan-t5-base","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: \n- en\n- fr\n- ro\n- de\n- multilingual\n\ntags:\n- text2text-generation\n\nwidget:\n- text: "Translate to German:  My name is Arthur"\n  example_title: "Translation"\n- text: "Please answer to the following question. Who is going to be the next Ballon d''or?"\n  example_title: "Question Answering"\n- text: "Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering."\n  example_title: "Logical reasoning"\n- text: "Please answer the following question. What is the boiling point of Nitrogen?"\n  example_title: "Scientific knowledge"\n- text: "Answer the following yes/no question. Can you write a whole Haiku in a single tweet?"\n  example_title: "Yes/no question"\n- text: "Answer the following yes/no question by reasoning step-by-step. Can you write a whole Haiku in a single tweet?"\n  example_title: "Reasoning task"\n- text: "Q: ( False or not False or False ) is? A: Let''s think step by step"\n  example_title: "Boolean Expressions"\n- text: "The square root of x is the cube root of y. What is y to the power of 2, if x = 4?"\n  example_title: "Math reasoning"\n- text: "Premise:  At my age you will probably have learnt one lesson. Hypothesis:  It''s not certain how many lessons you''ll learn by your thirties. Does the premise entail the hypothesis?"\n  example_title: "Premise and hypothesis"\n\ndatasets:\n- svakulenk0/qrecc\n- taskmaster2\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- lambada\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed\n\n\nlicense: apache-2.0\n---\n\n# Model Card for FLAN-T5 base\n\n<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/flan2_architecture.jpg"\nalt="drawing" width="600"/>\n\n#  Table of Contents\n\n0. [TL;DR](#TL;DR)\n1. [Model Details](#model-details)\n2. [Usage](#usage)\n3. [Uses](#uses)\n4. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n5. [Training Details](#training-details)\n6. [Evaluation](#evaluation)\n7. [Environmental Impact](#environmental-impact)\n8. [Citation](#citation)\n9. [Model Card Authors](#model-card-authors)\n\n# TL;DR\n\nIf you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. \nAs mentioned in the first few lines of the abstract : \n>  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.\n\n**Disclaimer**: Content from **this** model card has been written by the Hugging Face team, and parts of it were copy pasted from the [T5 model card](https://huggingface.co/t5-large).\n\n# Model Details\n\n## Model Description\n\n\n- **Model type:** Language model\n- **Language(s) (NLP):** English, Spanish, Japanese, Persian, Hindi, French, Chinese, Bengali, Gujarati, German, Telugu, Italian, Arabic, Polish, Tamil, Marathi, Malayalam, Oriya, Panjabi, Portuguese, Urdu, Galician, Hebrew, Korean, Catalan, Thai, Dutch, Indonesian, Vietnamese, Bulgarian, Filipino, Central Khmer, Lao, Turkish, Russian, Croatian, Swedish, Yoruba, Kurdish, Burmese, Malay, Czech, Finnish, Somali, Tagalog, Swahili, Sinhala, Kannada, Zhuang, Igbo, Xhosa, Romanian, Haitian, Estonian, Slovak, Lithuanian, Greek, Nepali, Assamese, Norwegian\n- **License:** Apache 2.0\n- **Related Models:** [All FLAN-T5 Checkpoints](https://huggingface.co/models?search=flan-t5)\n- **Original Checkpoints:** [All Original FLAN-T5 Checkpoints](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)\n- **Resources for more information:**\n  - [Research paper](https://arxiv.org/pdf/2210.11416.pdf)\n  - [GitHub Repo](https://github.com/google-research/t5x)\n  - [Hugging Face FLAN-T5 Docs (Similar to T5) ](https://huggingface.co/docs/transformers/model_doc/t5)\n\n# Usage\n\nFind below some example scripts on how to use the model in `transformers`:\n\n## Using the Pytorch model\n\n### Running the model on a CPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-base")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-base")\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-base")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-base", device_map="auto")\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU using different precisions\n\n#### FP16\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-base")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-base", device_map="auto", torch_dtype=torch.float16)\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n#### INT8\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-base")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-base", device_map="auto", load_in_8bit=True)\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n# Uses\n\n## Direct Use and Downstream Use\n\nThe authors write in [the original paper''s model card](https://arxiv.org/pdf/2210.11416.pdf) that: \n\n> The primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models\n\nSee the [research paper](https://arxiv.org/pdf/2210.11416.pdf) for further details.\n\n## Out-of-Scope Use\n\nMore information needed.\n\n# Bias, Risks, and Limitations\n\nThe information below in this section are copied from the model''s [official model card](https://arxiv.org/pdf/2210.11416.pdf):\n\n> Language models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.\n\n## Ethical considerations and risks\n\n> Flan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\n\n## Known Limitations\n\n> Flan-T5 has not been tested in real world applications.\n\n## Sensitive Use:\n\n> Flan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech.\n\n# Training Details\n\n## Training Data\n\nThe model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2):\n\n![table.png](https://s3.amazonaws.com/moonup/production/uploads/1666363265279-62441d1d9fdefb55a0b7d12c.png)\n\n\n## Training Procedure\n\nAccording to the model card from the [original paper](https://arxiv.org/pdf/2210.11416.pdf):\n\n> These models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size.\n\nThe model has been trained on TPU v3 or TPU v4 pods, using [`t5x`](https://github.com/google-research/t5x) codebase together with [`jax`](https://github.com/google/jax).\n\n\n# Evaluation\n\n## Testing Data, Factors & Metrics\n\nThe authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation:\n![image.png](https://s3.amazonaws.com/moonup/production/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png)\nFor full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf).\n\n## Results \n\nFor full results for FLAN-T5-Base, see the [research paper](https://arxiv.org/pdf/2210.11416.pdf), Table 3.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** Google Cloud TPU Pods - TPU v3 or TPU v4  | Number of chips ‚â• 4.\n- **Hours used:** More information needed\n- **Cloud Provider:** GCP\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@misc{https://doi.org/10.48550/arxiv.2210.11416,\n  doi = {10.48550/ARXIV.2210.11416},\n  \n  url = {https://arxiv.org/abs/2210.11416},\n  \n  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n  \n  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {Scaling Instruction-Finetuned Language Models},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```\n## Model Recycling\n\n[Evaluation on 36 datasets](https://ibm.github.io/model-recycling/model_gain_chart?avg=9.16&mnli_lp=nan&20_newsgroup=3.34&ag_news=1.49&amazon_reviews_multi=0.21&anli=13.91&boolq=16.75&cb=23.12&cola=9.97&copa=34.50&dbpedia=6.90&esnli=5.37&financial_phrasebank=18.66&imdb=0.33&isear=1.37&mnli=11.74&mrpc=16.63&multirc=6.24&poem_sentiment=14.62&qnli=3.41&qqp=6.18&rotten_tomatoes=2.98&rte=24.26&sst2=0.67&sst_5bins=5.44&stsb=20.68&trec_coarse=3.95&trec_fine=10.73&tweet_ev_emoji=13.39&tweet_ev_emotion=4.62&tweet_ev_hate=3.46&tweet_ev_irony=9.04&tweet_ev_offensive=1.69&tweet_ev_sentiment=0.75&wic=14.22&wnli=9.44&wsc=5.53&yahoo_answers=4.14&model_name=google%2Fflan-t5-base&base_name=google%2Ft5-v1_1-base) using google/flan-t5-base as a base model yields average score of 77.98 in comparison to 68.82 by google/t5-v1_1-base.\n\nThe model is ranked 1st among all tested models for the google/t5-v1_1-base architecture as of 06/02/2023\nResults:\n\n|   20_newsgroup |   ag_news |   amazon_reviews_multi |    anli |   boolq |      cb |    cola |   copa |   dbpedia |   esnli |   financial_phrasebank |   imdb |   isear |    mnli |    mrpc |   multirc |   poem_sentiment |    qnli |     qqp |   rotten_tomatoes |     rte |    sst2 |   sst_5bins |    stsb |   trec_coarse |   trec_fine |   tweet_ev_emoji |   tweet_ev_emotion |   tweet_ev_hate |   tweet_ev_irony |   tweet_ev_offensive |   tweet_ev_sentiment |     wic |   wnli |     wsc |   yahoo_answers |\n|---------------:|----------:|-----------------------:|--------:|--------:|--------:|--------:|-------:|----------:|--------:|-----------------------:|-------:|--------:|--------:|--------:|----------:|-----------------:|--------:|--------:|------------------:|--------:|--------:|------------:|--------:|--------------:|------------:|-----------------:|-------------------:|----------------:|-----------------:|---------------------:|---------------------:|--------:|-------:|--------:|----------------:|\n|        86.2188 |   89.6667 |                  67.12 | 51.9688 | 82.3242 | 78.5714 | 80.1534 |     75 |   77.6667 | 90.9507 |                   85.4 | 93.324 |  72.425 | 87.2457 | 89.4608 |   62.3762 |          82.6923 | 92.7878 | 89.7724 |           89.0244 | 84.8375 | 94.3807 |     57.2851 | 89.4759 |          97.2 |        92.8 |           46.848 |            80.2252 |         54.9832 |          76.6582 |              84.3023 |              70.6366 | 70.0627 | 56.338 | 53.8462 |            73.4 |\n\n\nFor more information, see: [Model Recycling](https://ibm.github.io/model-recycling/)\n', '{"pipeline_tag":null,"library_name":"transformers","framework":"transformers","params":247577856,"storage_bytes":7894822589,"files_count":12,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["T5ForConditionalGeneration"],"model_type":"t5","tokenizer_config":{"eos_token":"</s>","pad_token":"<pad>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google:jax","source_url":"https://github.com/google/jax"},{"type":"based_on_paper","target_id":"arxiv:2210.11416","source_url":"https://arxiv.org/abs/2210.11416"},{"type":"based_on_paper","target_id":"arxiv:1910.09700","source_url":"https://arxiv.org/abs/1910.09700"}]', NULL, 'Apache-2.0', 'approved', 80, 'f645454075588cc7a5356d39eb07e1d4', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-dreamlike-art-dreamlike-diffusion-1.0', 'huggingface--dreamlike-art--dreamlike-diffusion-1.0', 'dreamlike-diffusion-1.0', 'dreamlike-art', '--- language: - en license: other tags: - stable-diffusion - stable-diffusion-diffusers - text-to-image - art - artistic - diffusers inference: false --- Use the same prompts as you would for SD 1.5. Add **dreamlikeart** if the artstyle is too weak. Non-square aspect ratios work better for some prompts. If you want a portrait photo, try using a 2:3 or a 9:16 aspect ratio. If you want a landscape photo, try using a 3:2 or a 16:9 aspect ratio. Use slightly higher resolution for better results: ...', '["diffusers","safetensors","stable-diffusion","stable-diffusion-diffusers","text-to-image","art","artistic","en","license:other","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 1025, 6071, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\nlicense: other\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\n- art\n- artistic\n- diffusers\ninference: false\n---\n\n# Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by [dreamlike.art](https://dreamlike.art/).\n\n# If you want to use dreamlike models on your website/app/etc., check the license at the bottom first!  \n\nUse the same prompts as you would for SD 1.5. Add **dreamlikeart** if the artstyle is too weak.    \nNon-square aspect ratios work better for some prompts. If you want a portrait photo, try using a 2:3 or a 9:16 aspect ratio. If you want a landscape photo, try using a 3:2 or a 16:9 aspect ratio.  \nUse slightly higher resolution for better results: 640x640px, 512x768px, 768x512px, etc.  \n\n# We''ve just released Dreamlike Photoreal 2.0, check it out!\n\n[https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0)\n\n<img src="https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0/resolve/main/preview1.jpg" style="max-width: 400px;" width="100%"/>\n\n### Examples\n\n<img src="https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/preview.jpg" style="max-width: 800px;" width="100%"/>\n<img src="https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/1.jpg" style="max-width: 800px;" width="100%"/>\n<img src="https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/2.jpg" style="max-width: 800px;" width="100%"/>\n\n### dreamlike.art\n\nYou can use this model for free on [dreamlike.art](https://dreamlike.art/)!\n\n<img src="https://huggingface.co/dreamlike-art/dreamlike-photoreal-1.0/resolve/main/dreamlike.jpg" style="max-width: 1000px;" width="100%"/>\n\n### Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run dreamlike-diffusion-1.0:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/dreamlike-diffusion-1.0)\n\n### CompVis\n\n[Download dreamlike-diffusion-1.0.ckpt (2.13GB)](https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/resolve/main/dreamlike-diffusion-1.0.ckpt)\n\n### üß® Diffusers\n\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the [Stable Diffusion Pipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\n\n```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = "dreamlike-art/dreamlike-diffusion-1.0"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to("cuda")\n\nprompt = "dreamlikeart, a grungy woman with rainbow hair, travelling between dimensions, dynamic pose, happy, soft eyes and narrow chin, extreme bokeh, dainty figure, long hair straight down, torn kawaii shirt and baggy jeans, In style of by Jordan Grimmer and greg rutkowski, crisp lines and color, complex background, particles, lines, wind, concept art, sharp focus, vivid colors"\nimage = pipe(prompt).images[0]\n\nimage.save("./result.jpg")\n```\n\n# License\n\nThis model is licesed under a **modified** CreativeML OpenRAIL-M license.\n\n- **You can''t host or use the model or its derivatives on websites/apps/etc., from which you earn, will earn, or plan to earn revenue or donations. If you want to, please email us at contact@dreamlike.art**\n- **You are free to host the model card and files (Without any actual inference or finetuning) on both commercial and non-commercial websites/apps/etc.  Please state the full model name (Dreamlike Diffusion 1.0) and include a link to the model card (https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0)**  \n- **You are free to host the model or its derivatives on completely non-commercial websites/apps/etc (Meaning you are not getting ANY revenue or donations). Please state the full model name (Dreamlike Diffusion 1.0) and include a link to the model card (https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0)**\n- **You are free to use the outputs of the model or the outputs of the model''s derivatives for commercial purposes in teams of 10 or less**\n- You can''t use the model to deliberately produce nor share illegal or harmful outputs or content\n- The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n- You may re-distribute the weights. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the **modified** CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully) Please read the full license here: https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0/blob/main/LICENSE.md\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":23223080959,"files_count":24,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"}]', NULL, 'Other', 'approved', 65, 'ebf55764034102e987fe0ee7b0a74ebb', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tiiuae-falcon-7b-instruct', 'huggingface--tiiuae--falcon-7b-instruct', 'falcon-7b-instruct', 'tiiuae', '--- datasets: - tiiuae/falcon-refinedweb language: - en inference: true new_version: tiiuae/falcon-11B widget: - text: "Hey Falcon! Any recommendations for my holidays in Abu Dhabi?" example_title: "Abu Dhabi Trip" - text: "What''s the Everett interpretation of quantum mechanics?" example_title: "Q/A: Quantum & Answers" - text: "Give me a list of the top 10 dive sites you would recommend around the world." example_title: "Diving Top 10" - text: "Can you tell me more about deep-water soloing?" ...', '["transformers","pytorch","coreml","safetensors","falcon","text-generation","conversational","custom_code","en","dataset:tiiuae/falcon-refinedweb","arxiv:2205.14135","arxiv:1911.02150","arxiv:2005.14165","arxiv:2104.09864","arxiv:2306.01116","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 1024, 47803, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tiiuae/falcon-7b-instruct","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ndatasets:\n  - tiiuae/falcon-refinedweb\nlanguage:\n  - en\ninference: true\nnew_version: tiiuae/falcon-11B\nwidget:\n  - text: "Hey Falcon! Any recommendations for my holidays in Abu Dhabi?"\n    example_title: "Abu Dhabi Trip"\n  - text: "What''s the Everett interpretation of quantum mechanics?"\n    example_title: "Q/A: Quantum & Answers"\n  - text: "Give me a list of the top 10 dive sites you would recommend around the world."\n    example_title: "Diving Top 10"\n  - text: "Can you tell me more about deep-water soloing?"\n    example_title: "Extreme sports"\n  - text: "Can you write a short tweet about the Apache 2.0 release of our latest AI model, Falcon LLM?"\n    example_title: "Twitter Helper"\n  - text: "What are the responsabilities of a Chief Llama Officer?"\n    example_title: "Trendy Jobs"\nlicense: apache-2.0\n---\n\n# ‚ú® Falcon-7B-Instruct\n\n**Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by [TII](https://www.tii.ae) based on [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) and finetuned on a mixture of chat/instruct datasets. It is made available under the Apache 2.0 license.**\n\n*Paper coming soon üòä.*\n\nü§ó To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading [this great blogpost fron HF](https://huggingface.co/blog/falcon)!\n\n## Why use Falcon-7B-Instruct?\n\n* **You are looking for a ready-to-use chat/instruct model based on [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b).**\n* **Falcon-7B is a strong base model, outperforming comparable open-source models** (e.g., [MPT-7B](https://huggingface.co/mosaicml/mpt-7b), [StableLM](https://github.com/Stability-AI/StableLM), [RedPajama](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-7B-v0.1) etc.), thanks to being trained on 1,500B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora. See the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).\n* **It features an architecture optimized for inference**, with FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135)) and multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)). \n\nüí¨ **This is an instruct model, which may not be ideal for further finetuning.** If you are interested in building your own instruct/chat model, we recommend starting from [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b). \n\nüî• **Looking for an even more powerful model?** [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct) is Falcon-7B-Instruct''s big brother!\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = "tiiuae/falcon-7b-instruct"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    "text-generation",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map="auto",\n)\nsequences = pipeline(\n   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f"Result: {seq[''generated_text'']}")\n\n```\n\nüí• **Falcon LLMs require PyTorch 2.0 for use with `transformers`!**\n\nFor fast inference with Falcon, check-out [Text Generation Inference](https://github.com/huggingface/text-generation-inference)! Read more in this [blogpost]((https://huggingface.co/blog/falcon). \n\nYou will need **at least 16GB of memory** to swiftly run inference with Falcon-7B-Instruct.\n\n\n# Model Card for Falcon-7B-Instruct\n\n## Model Details\n\n### Model Description\n\n- **Developed by:** [https://www.tii.ae](https://www.tii.ae);\n- **Model type:** Causal decoder-only;\n- **Language(s) (NLP):** English and French;\n- **License:** Apache 2.0;\n- **Finetuned from model:** [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b).\n\n### Model Source\n\n- **Paper:** *coming soon*.\n\n## Uses\n\n### Direct Use\n\nFalcon-7B-Instruct has been finetuned on a mixture of instruct and chat datasets.\n\n### Out-of-Scope Use\n\nProduction use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful. \n\n## Bias, Risks, and Limitations\n\nFalcon-7B-Instruct is mostly trained on English data, and will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online.\n\n### Recommendations\n\nWe recommend users of Falcon-7B-Instruct to develop guardrails and to take appropriate precautions for any production use.\n\n## How to Get Started with the Model\n\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport transformers\nimport torch\n\nmodel = "tiiuae/falcon-7b-instruct"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\npipeline = transformers.pipeline(\n    "text-generation",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    device_map="auto",\n)\nsequences = pipeline(\n   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",\n    max_length=200,\n    do_sample=True,\n    top_k=10,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n)\nfor seq in sequences:\n    print(f"Result: {seq[''generated_text'']}")\n\n```\n\n## Training Details\n\n### Training Data\n\nFalcon-7B-Instruct was finetuned on a 250M tokens mixture of instruct/chat datasets.\n\n| **Data source**    | **Fraction** | **Tokens** | **Description**                       |\n|--------------------|--------------|------------|-----------------------------------|\n| [Bai ze](https://github.com/project-baize/baize-chatbot) | 65%          | 164M     | chat                 |\n| [GPT4All](https://github.com/nomic-ai/gpt4all)              | 25%           | 62M       | instruct                                  |\n| [GPTeacher](https://github.com/teknium1/GPTeacher)      | 5%           | 11M        | instruct |\n| [RefinedWeb-English](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) | 5%          | 13M     | massive web crawl                 |\n\n\nThe data was tokenized with the Falcon-[7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b) tokenizer.\n\n\n## Evaluation\n\n*Paper coming soon.*\n\nSee the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) for early results.\n\nNote that this model variant is not optimized for NLP benchmarks. \n\n\n## Technical Specifications \n\nFor more information about pretraining, see [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b).\n\n### Model Architecture and Objective\n\nFalcon-7B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token).\n\nThe architecture is broadly adapted from the GPT-3 paper ([Brown et al., 2020](https://arxiv.org/abs/2005.14165)), with the following differences:\n\n* **Positionnal embeddings:** rotary ([Su et al., 2021](https://arxiv.org/abs/2104.09864));\n* **Attention:** multiquery ([Shazeer et al., 2019](https://arxiv.org/abs/1911.02150)) and FlashAttention ([Dao et al., 2022](https://arxiv.org/abs/2205.14135));\n* **Decoder-block:** parallel attention/MLP with a single layer norm.\n\n| **Hyperparameter** | **Value** | **Comment**                            |\n|--------------------|-----------|----------------------------------------|\n| Layers             | 32        |                                        |\n| `d_model`          | 4544      | Increased to compensate for multiquery                                       |\n| `head_dim`         | 64        | Reduced to optimise for FlashAttention |\n| Vocabulary         | 65024     |                                        |\n| Sequence length    | 2048      |                                        |\n\n### Compute Infrastructure\n\n#### Hardware\n\nFalcon-7B-Instruct was trained on AWS SageMaker, on 32 A100 40GB GPUs in P4d instances. \n\n#### Software\n\nFalcon-7B-Instruct was trained a custom distributed training codebase, Gigatron. It uses a 3D parallelism approach combined with ZeRO and high-performance Triton kernels (FlashAttention, etc.)\n\n\n## Citation\n\n*Paper coming soon* üòä. In the meanwhile, you can use the following information to cite: \n```\n@article{falcon40b,\n  title={{Falcon-40B}: an open large language model with state-of-the-art performance},\n  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},\n  year={2023}\n}\n```\n\nTo learn more about the pretraining dataset, see the üìì [RefinedWeb paper](https://arxiv.org/abs/2306.01116).\n\n```\n@article{refinedweb,\n  title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},\n  author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},\n  journal={arXiv preprint arXiv:2306.01116},\n  eprint={2306.01116},\n  eprinttype = {arXiv},\n  url={https://arxiv.org/abs/2306.01116},\n  year={2023}\n}\n```\n\n\n## License\n\nFalcon-7B-Instruct is made available under the Apache 2.0 license.\n\n## Contact\nfalconllm@tii.ae', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7217189760,"storage_bytes":56610389723,"files_count":19,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["FalconForCausalLM"],"auto_map":{"AutoConfig":"configuration_falcon.FalconConfig","AutoModel":"modeling_falcon.FalconModel","AutoModelForSequenceClassification":"modeling_falcon.FalconForSequenceClassification","AutoModelForTokenClassification":"modeling_falcon.FalconForTokenClassification","AutoModelForQuestionAnswering":"modeling_falcon.FalconForQuestionAnswering","AutoModelForCausalLM":"modeling_falcon.FalconForCausalLM"},"model_type":"falcon","tokenizer_config":{"eos_token":"<|endoftext|>","chat_template":"{% if messages[0][''role''] == ''system'' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][''content''] %}{% else %}{% set loop_messages = messages %}{% set system_message = '''' %}{% endif %}{% for message in loop_messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% if loop.index0 == 0 %}{{ system_message.strip() }}{% endif %}{% if message[''role''] == ''user'' %}{{ ''\n\nUser: '' + message[''content''].strip().replace(''\r\n'', ''\n'').replace(''\n\n'', ''\n'') }}{% elif message[''role''] == ''assistant'' %}{{ ''\n\nAssistant: '' + message[''content''].strip().replace(''\r\n'', ''\n'').replace(''\n\n'', ''\n'') }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ ''\n\nAssistant:'' }}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:Stability-AI:StableLM","source_url":"https://github.com/Stability-AI/StableLM"},{"type":"has_code","target_id":"github:huggingface:text-generation-inference","source_url":"https://github.com/huggingface/text-generation-inference"},{"type":"has_code","target_id":"github:project-baize:baize-chatbot","source_url":"https://github.com/project-baize/baize-chatbot"},{"type":"has_code","target_id":"github:nomic-ai:gpt4all","source_url":"https://github.com/nomic-ai/gpt4all"},{"type":"has_code","target_id":"github:teknium1:GPTeacher","source_url":"https://github.com/teknium1/GPTeacher"},{"type":"based_on_paper","target_id":"arxiv:2205.14135","source_url":"https://arxiv.org/abs/2205.14135"},{"type":"based_on_paper","target_id":"arxiv:1911.02150","source_url":"https://arxiv.org/abs/1911.02150"},{"type":"based_on_paper","target_id":"arxiv:2005.14165","source_url":"https://arxiv.org/abs/2005.14165"},{"type":"based_on_paper","target_id":"arxiv:2104.09864","source_url":"https://arxiv.org/abs/2104.09864"},{"type":"based_on_paper","target_id":"arxiv:2306.01116","source_url":"https://arxiv.org/abs/2306.01116"}]', NULL, 'Apache-2.0', 'approved', 65, '265785c21586b0e60598f4865e1d797b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-3-4b-it', 'huggingface--google--gemma-3-4b-it', 'gemma-3-4b-it', 'google', '', '["transformers","safetensors","gemma3","any-to-any","image-text-to-text","conversational","arxiv:1905.07830","arxiv:1905.10044","arxiv:1911.11641","arxiv:1904.09728","arxiv:1705.03551","arxiv:1911.01547","arxiv:1907.10641","arxiv:1903.00161","arxiv:2009.03300","arxiv:2304.06364","arxiv:2103.03874","arxiv:2110.14168","arxiv:2311.12022","arxiv:2108.07732","arxiv:2107.03374","arxiv:2210.03057","arxiv:2106.03193","arxiv:1910.11856","arxiv:2502.12404","arxiv:2502.21228","arxiv:2404.16816","arxiv:2104.12756","arxiv:2311.16502","arxiv:2203.10244","arxiv:2404.12390","arxiv:1810.12440","arxiv:1908.02660","arxiv:2312.11805","base_model:google/gemma-3-4b-pt","base_model:finetune:google/gemma-3-4b-pt","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'image-text-to-text', 1019, 1030699, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-3-4b-it","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":4300079472,"storage_bytes":50506149149,"files_count":15,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["Gemma3ForConditionalGeneration"],"model_type":"gemma3","processor_config":{"chat_template":"{{ bos_token }}\n{%- if messages[0][''role''] == ''system'' -%}\n    {%- if messages[0][''content''] is string -%}\n        {%- set first_user_prefix = messages[0][''content''] + ''\n\n'' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0][''content''][0][''text''] + ''\n\n'' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message[''role''] == ''assistant'') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message[''role''] -%}\n    {%- endif -%}\n    {{ ''<start_of_turn>'' + role + ''\n'' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message[''content''] is string -%}\n        {{ message[''content''] | trim }}\n    {%- elif message[''content''] is iterable -%}\n        {%- for item in message[''content''] -%}\n            {%- if item[''type''] == ''image'' -%}\n                {{ ''<start_of_image>'' }}\n            {%- elif item[''type''] == ''text'' -%}\n                {{ item[''text''] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ ''<end_of_turn>\n'' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{''<start_of_turn>model\n''}}\n{%- endif -%}\n"},"tokenizer_config":{"bos_token":"<bos>","chat_template":"{{ bos_token }}\n{%- if messages[0][''role''] == ''system'' -%}\n    {%- if messages[0][''content''] is string -%}\n        {%- set first_user_prefix = messages[0][''content''] + ''\n\n'' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0][''content''][0][''text''] + ''\n\n'' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message[''role''] == ''assistant'') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message[''role''] -%}\n    {%- endif -%}\n    {{ ''<start_of_turn>'' + role + ''\n'' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message[''content''] is string -%}\n        {{ message[''content''] | trim }}\n    {%- elif message[''content''] is iterable -%}\n        {%- for item in message[''content''] -%}\n            {%- if item[''type''] == ''image'' -%}\n                {{ ''<start_of_image>'' }}\n            {%- elif item[''type''] == ''text'' -%}\n                {{ item[''text''] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ ''<end_of_turn>\n'' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{''<start_of_turn>model\n''}}\n{%- endif -%}\n","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1903.00161","source_url":"https://arxiv.org/abs/1903.00161"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2103.03874","source_url":"https://arxiv.org/abs/2103.03874"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2311.12022","source_url":"https://arxiv.org/abs/2311.12022"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2210.03057","source_url":"https://arxiv.org/abs/2210.03057"},{"type":"based_on_paper","target_id":"arxiv:2106.03193","source_url":"https://arxiv.org/abs/2106.03193"},{"type":"based_on_paper","target_id":"arxiv:1910.11856","source_url":"https://arxiv.org/abs/1910.11856"},{"type":"based_on_paper","target_id":"arxiv:2502.12404","source_url":"https://arxiv.org/abs/2502.12404"},{"type":"based_on_paper","target_id":"arxiv:2502.21228","source_url":"https://arxiv.org/abs/2502.21228"},{"type":"based_on_paper","target_id":"arxiv:2404.16816","source_url":"https://arxiv.org/abs/2404.16816"},{"type":"based_on_paper","target_id":"arxiv:2104.12756","source_url":"https://arxiv.org/abs/2104.12756"},{"type":"based_on_paper","target_id":"arxiv:2311.16502","source_url":"https://arxiv.org/abs/2311.16502"},{"type":"based_on_paper","target_id":"arxiv:2203.10244","source_url":"https://arxiv.org/abs/2203.10244"},{"type":"based_on_paper","target_id":"arxiv:2404.12390","source_url":"https://arxiv.org/abs/2404.12390"},{"type":"based_on_paper","target_id":"arxiv:1810.12440","source_url":"https://arxiv.org/abs/1810.12440"},{"type":"based_on_paper","target_id":"arxiv:1908.02660","source_url":"https://arxiv.org/abs/1908.02660"},{"type":"based_on_paper","target_id":"arxiv:2312.11805","source_url":"https://arxiv.org/abs/2312.11805"}]', NULL, 'Gemma', 'approved', 40, '9b7b15d46be05318e2a91092e3586bc0', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openbmb-MiniCPM-V-4-5', 'huggingface--openbmb--minicpm-v-4-5', 'MiniCPM-V-4_5', 'openbmb', '--- pipeline_tag: image-text-to-text datasets: - openbmb/RLAIF-V-Dataset library_name: transformers language: - multilingual tags: - minicpm-v - vision - ocr - multi-image - video - custom_code license: apache-2.0 --- <h1>A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone</h1> GitHub | CookBook | Technical Report | Demo </a> **MiniCPM-V 4.5** is the latest and most capable model in the MiniCPM-V series. The model is built on Qwen3-8B and SigLIP2-4...', '["transformers","safetensors","minicpmv","feature-extraction","minicpm-v","vision","ocr","multi-image","video","custom_code","image-text-to-text","conversational","multilingual","dataset:openbmb/rlaif-v-dataset","arxiv:2509.18154","arxiv:2403.11703","license:apache-2.0","region:us"]', 'image-text-to-text', 1019, 49678, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openbmb/MiniCPM-V-4_5","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\npipeline_tag: image-text-to-text\ndatasets:\n- openbmb/RLAIF-V-Dataset\nlibrary_name: transformers\nlanguage:\n- multilingual\ntags:\n- minicpm-v\n- vision\n- ocr\n- multi-image\n- video\n- custom_code\nlicense: apache-2.0\n---\n\n<h1>A GPT-4o Level MLLM for Single Image, Multi Image and High-FPS Video Understanding on Your Phone</h1>\n\n[GitHub](https://github.com/OpenBMB/MiniCPM-o) | [CookBook](https://github.com/OpenSQZ/MiniCPM-V-CookBook) | [Technical Report](https://huggingface.co/papers/2509.18154) | [Demo](http://101.126.42.235:30910/) </a> \n\n\n\n## MiniCPM-V 4.5\n\n**MiniCPM-V 4.5** is the latest and most capable model in the MiniCPM-V series. The model is built on Qwen3-8B and SigLIP2-400M with a total of 8B parameters. It exhibits a significant performance improvement over previous MiniCPM-V and MiniCPM-o models, and introduces new useful features. Notable features of MiniCPM-V 4.5 include:\n\n- üî• **State-of-the-art Vision-Language Capability.**\n  MiniCPM-V 4.5 achieves an average score of 77.0 on OpenCompass, a comprehensive evaluation of 8 popular benchmarks. **With only 8B parameters, it surpasses widely used proprietary models like GPT-4o-latest, Gemini-2.0 Pro, and strong open-source models like Qwen2.5-VL 72B** for vision-language capabilities, making it the most performant MLLM under 30B parameters.\n\n- üé¨ **Efficient High-FPS and Long Video Understanding.** Powered by a new unified 3D-Resampler over images and videos, MiniCPM-V 4.5 can now achieve 96x compression rate for video tokens, where 6 448x448 video frames can be jointly compressed into 64 video tokens (normally 1,536 tokens for most MLLMs). This means that the model can perceive significantly more video frames without increasing the LLM inference cost. This brings state-of-the-art high-FPS (up to 10FPS) video understanding and long video understanding capabilities on Video-MME, LVBench, MLVU, MotionBench, FavorBench, etc., efficiently.\n\n- ‚öôÔ∏è **Controllable Hybrid Fast/Deep Thinking.** MiniCPM-V 4.5 supports both fast thinking for efficient frequent usage with competitive performance, and deep thinking for more complex problem solving. To cover efficiency and performance trade-offs in different user scenarios, this fast/deep thinking mode can be switched in a highly controlled fashion.\n\n- üí™ **Strong OCR, Document Parsing and Others.**\nBased on [LLaVA-UHD](https://arxiv.org/pdf/2403.11703) architecture, MiniCPM-V 4.5 can process high-resolution images with any aspect ratio and up to 1.8 million pixels (e.g., 1344x1344), using 4x less visual tokens than most MLLMs. The model achieves **leading performance on OCRBench, surpassing proprietary models such as GPT-4o-latest and Gemini 2.5**. It also achieves state-of-the-art performance for PDF document parsing capability on OmniDocBench among general MLLMs. Based on the latest [RLAIF-V](https://github.com/RLHF-V/RLAIF-V/) and [VisCPM](https://github.com/OpenBMB/VisCPM) techniques, it features **trustworthy behaviors**, outperforming GPT-4o-latest on MMHal-Bench, and supports **multilingual capabilities** in more than 30 languages.\n\n- üí´ **Easy Usage.**\nMiniCPM-V 4.5 can be easily used in various ways: (1) [llama.cpp](https://github.com/tc-mb/llama.cpp/blob/Support-MiniCPM-V-4.5/docs/multimodal/minicpmv4.5.md) and [ollama](https://github.com/tc-mb/ollama/tree/MIniCPM-V) support for efficient CPU inference on local devices, (2) [int4](https://huggingface.co/openbmb/MiniCPM-V-4_5-int4), [GGUF](https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf) and [AWQ](https://github.com/tc-mb/AutoAWQ) format quantized models in 16 sizes, (3) [SGLang](https://github.com/tc-mb/sglang/tree/main) and [vLLM](#efficient-inference-with-llamacpp-ollama-vllm) support for high-throughput and memory-efficient inference, (4) fine-tuning on new domains and tasks with [Transformers](https://github.com/tc-mb/transformers/tree/main) and [LLaMA-Factory](./docs/llamafactory_train_and_infer.md), (5) quick [local WebUI demo](#chat-with-our-demo-on-gradio), (6) optimized [local iOS app](https://github.com/tc-mb/MiniCPM-o-demo-iOS) on iPhone and iPad, and (7) online web demo on [server](http://101.126.42.235:30910/). See our [Cookbook](https://github.com/OpenSQZ/MiniCPM-V-CookBook) for full usages!\n\n\n### Key Techniques\n\n\n<div align="center">\n<img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpm-v-4dot5-framework.png" , width=100%>\n</div>\n\n- **Architechture: Unified 3D-Resampler for High-density Video Compression.** MiniCPM-V 4.5 introduces a 3D-Resampler that overcomes the performance-efficiency trade-off in video understanding. By grouping and jointly compressing up to 6 consecutive video frames into just 64 tokens (the same token count used for a single image in MiniCPM-V series), MiniCPM-V 4.5 achieves a 96√ó compression rate for video tokens. This allows the model to process more video frames without additional LLM computational cost, enabling high-FPS video and long video understanding. The architecture supports unified encoding for images, multi-image inputs, and videos, ensuring seamless capability and knowledge transfer.\n\n- **Pre-training: Unified Learning for OCR and Knowledge from Documents.** Existing MLLMs learn OCR capability and knowledge from documents in isolated training approaches. We observe that the essential difference between these two training approaches is the visibility of the text in images. By dynamically corrupting text regions in documents with varying noise levels and asking the model to reconstruct the text, the model learns to adaptively and properly switch between accurate text recognition (when text is visible) and multimodal context-based knowledge reasoning (when text is heavily obscured). This eliminates reliance on error-prone document parsers in knowledge learning from documents, and prevents hallucinations from over-augmented OCR data, resulting in top-tier OCR and multimodal knowledge performance with minimal engineering overhead.\n\n- **Post-training: Hybrid Fast/Deep Thinking with Multimodal RL.** MiniCPM-V 4.5 offers a balanced reasoning experience through two switchable modes: fast thinking for efficient daily use and deep thinking for complex tasks. Using a new hybrid reinforcement learning method, the model jointly optimizes both modes, significantly enhancing fast-mode performance without compromising deep-mode capability. Incorporated with [RLPR](https://github.com/OpenBMB/RLPR) and [RLAIF-V](https://github.com/RLHF-V/RLAIF-V), it generalizes robust reasoning skills from broad multimodal data while effectively reducing hallucinations.\n\n### Evaluation\n\n<div align="center">\n  <img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/radar_minicpm_v45.png", width=60%>\n</div>\n<div align="center">\n<img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv_4_5_evaluation_result.png" , width=100%>\n</div>\n\n### Inference Efficiency \n\n**OpenCompass**\n<div align="left">\n<table style="margin: 0px auto;">\n    <thead>\n            <tr>\n              <th align="left">Model</th>\n              <th>Size</th>\n              <th>Avg Score ‚Üë</th>\n              <th>Total Inference Time ‚Üì</th>\n            </tr>\n    </thead>\n    <tbody align="center">\n        <tr>\n            <td nowrap="nowrap" align="left">GLM-4.1V-9B-Thinking</td>\n            <td>10.3B</td>\n            <td>76.6</td>\n            <td>17.5h</td>\n        </tr>\n        <tr>\n            <td nowrap="nowrap" align="left">MiMo-VL-7B-RL</td>\n            <td>8.3B</td>\n            <td>76.4</td>\n            <td>11h</td>\n        </tr>\n        <tr>\n            <td nowrap="nowrap" align="left">MiniCPM-V 4.5</td>\n            <td>8.7B</td>\n            <td><b>77.0</td>\n            <td><b>7.5h</td>\n        </tr>\n    </tbody>\n</table>\n</div>\n\n**Video-MME**\n\n<div align="left">\n<table style="margin: 0px auto;">\n    <thead>\n          <tr>\n              <th align="left">Model</th>\n              <th>Size</th>\n              <th>Avg Score ‚Üë</th>\n              <th>Total Inference Time ‚Üì</th>\n              <th>GPU Mem ‚Üì</th>\n          </tr>\n    </thead>\n    <tbody align="center">\n          <tr>\n              <td nowrap="nowrap" align="left">Qwen2.5-VL-7B-Instruct</td>\n              <td>8.3B</td>\n              <td>71.6</td>\n              <td>3h</td>\n              <td>60G</td>\n          </tr>\n          <tr>\n              <td nowrap="nowrap" align="left">GLM-4.1V-9B-Thinking</td>\n              <td>10.3B</td>\n              <td><b>73.6</td>\n              <td>2.63h</td>\n              <td>32G</td>\n          </tr>\n          <tr>\n              <td nowrap="nowrap" align="left">MiniCPM-V 4.5</td>\n              <td>8.7B</td>\n              <td>73.5</td>\n              <td><b>0.26h</td>\n              <td><b>28G</td>\n        </tr>\n    </tbody>\n</table>\n</div>\n\nBoth Video-MME and OpenCompass were evaluated using 8√óA100 GPUs for inference. The reported inference time of Video-MME includes full model-side computation, and excludes the external cost of video frame extraction (dependent on specific frame extraction tools) for fair comparison.\n\n### Examples\n\n<div align="center">\n  <a href="https://www.youtube.com/watch?v=Cn23FujYMMU"><img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/MiniCPM-V%204.5-8.26_img.jpeg", width=70%></a>\n</div>\n\n<div style="display: flex; flex-direction: column; align-items: center;">\n  <img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/en_case1.png" alt="en_case1" style="margin-bottom: 5px;">\n  <img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/en_case2.png" alt="en_case2" style="margin-bottom: 5px;">\n  <img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/en_case3.jpeg" alt="en_case3" style="margin-bottom: 5px;">\n</div>\n\nWe deploy MiniCPM-V 4.5 on iPad M4 with [iOS demo](https://github.com/tc-mb/MiniCPM-o-demo-iOS). The demo video is the raw screen recording without editing.\n\n<div align="center">\n  <img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/v45_en_handwriting.gif" width="45%" style="display: inline-block; margin: 0 10px;"/>\n  <img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/v45_en_cot.gif" width="45%" style="display: inline-block; margin: 0 10px;"/>\n</div>\n\n<div align="center">\n  <img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/v45_cn_handwriting.gif" width="45%" style="display: inline-block; margin: 0 10px;"/>\n  <img src="https://raw.githubusercontent.com/openbmb/MiniCPM-o/main/assets/minicpmv4_5/v45_cn_travel.gif" width="45%" style="display: inline-block; margin: 0 10px;"/>\n</div> \n\n## Framework Support Matrix\n<table>\n  <thead>\n    <tr>\n      <th>Category</th>\n      <th>Framework</th>\n      <th>Cookbook Link</th>\n      <th>Upstream PR</th>\n      <th>Supported since(branch)</th>\n      <th>Supported since(release)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td rowspan="2">Edge(On-device)</td>\n      <td>Llama.cpp</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/llama.cpp/minicpm-v4_5_llamacpp.md">Llama.cpp Doc</a></td>\n      <td><a href="https://github.com/ggml-org/llama.cpp/pull/15575">#15575</a>(2025-08-26)</td>\n      <td>master(2025-08-26)</td>\n      <td><a href="https://github.com/ggml-org/llama.cpp/releases/tag/b6282">b6282</a></td>\n    </tr>\n    <tr>\n      <td>Ollama</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/ollama/minicpm-v4_5_ollama.md">Ollama Doc</a></td>\n      <td><a href="https://github.com/ollama/ollama/pull/12078">#12078</a>(2025-08-26)</td>\n      <td>Merging</td>\n      <td>Waiting for official release</td>\n    </tr>\n    <tr>\n      <td rowspan="2">Serving(Cloud)</td>\n      <td>vLLM</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/vllm/minicpm-v4_5_vllm.md">vLLM Doc</a></td>\n      <td><a href="https://github.com/vllm-project/vllm/pull/23586">#23586</a>(2025-08-26)</td>\n      <td>main(2025-08-27)</td>\n      <td><a href="https://github.com/vllm-project/vllm/releases/tag/v0.10.2">v0.10.2</td>\n    </tr>\n    <tr>\n      <td>SGLang</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/deployment/sglang/MiniCPM-v4_5_sglang.md">SGLang Doc</a></td>\n      <td><a href="https://github.com/sgl-project/sglang/pull/9610">#9610</a>(2025-08-26)</td>\n      <td>Merging</td>\n      <td>Waiting for official release</td>\n    </tr>\n    <tr>\n      <td>Finetuning</td>\n      <td>LLaMA-Factory</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/finetune/finetune_llamafactory.md">LLaMA-Factory Doc</a></td>\n      <td><a href="https://github.com/hiyouga/LLaMA-Factory/pull/9022">#9022</a>(2025-08-26)</td>\n      <td>main(2025-08-26)</td>\n      <td>Waiting for official release</td>\n    </tr>\n    <tr>\n      <td rowspan="3">Quantization</td>\n      <td>GGUF</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/quantization/gguf/minicpm-v4_5_gguf_quantize.md">GGUF Doc</a></td>\n      <td>‚Äî</td>\n      <td>‚Äî</td>\n      <td>‚Äî</td>\n    </tr>\n    <tr>\n      <td>BNB</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/quantization/bnb/minicpm-v4_5_bnb_quantize.md">BNB Doc</a></td>\n      <td>‚Äî</td>\n      <td>‚Äî</td>\n      <td>‚Äî</td>\n    </tr>\n    <tr>\n      <td>AWQ</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/quantization/awq/minicpm-v4_5_awq_quantize.md">AWQ Doc</a></td>\n      <td>‚Äî</td>\n      <td>‚Äî</td>\n      <td>‚Äî</td>\n    </tr>\n    <tr>\n      <td>Demos</td>\n      <td>Gradio Demo</td>\n      <td><a href="https://github.com/OpenSQZ/MiniCPM-V-CookBook/blob/main/demo/web_demo/gradio/README.md">Gradio Demo Doc</a></td>\n      <td>‚Äî</td>\n      <td>‚Äî</td>\n      <td>‚Äî</td>\n    </tr>\n  </tbody>\n </table>\n \n> Note: If you''d like us to prioritize support for another open-source framework, please let us know via this [short form](https://docs.google.com/forms/d/e/1FAIpQLSdyTUrOPBgWqPexs3ORrg47ZcZ1r4vFQaA4ve2iA7L9sMfMWw/viewform).\n\n## Usage\n\nIf you wish to enable thinking mode, provide the argument `enable_thinking=True` to the chat function.\n\n#### Chat with Image\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\ntorch.manual_seed(100)\n\nmodel = AutoModel.from_pretrained(''openbmb/MiniCPM-V-4_5'', trust_remote_code=True, # or openbmb/MiniCPM-o-2_6\n    attn_implementation=''sdpa'', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained(''openbmb/MiniCPM-V-4_5'', trust_remote_code=True) # or openbmb/MiniCPM-o-2_6\n\nimage = Image.open(''./assets/minicpmo2_6/show_demo.jpg'').convert(''RGB'')\n\nenable_thinking=False # If `enable_thinking=True`, the thinking mode is enabled.\nstream=True # If `stream=True`, the answer is string\n\n# First round chat \nquestion = "What is the landform in the picture?"\nmsgs = [{''role'': ''user'', ''content'': [image, question]}]\n\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    enable_thinking=enable_thinking,\n    stream=True\n)\n\ngenerated_text = ""\nfor new_text in answer:\n    generated_text += new_text\n    print(new_text, flush=True, end='''')\n\n# Second round chat, pass history context of multi-turn conversation\nmsgs.append({"role": "assistant", "content": [generated_text]})\nmsgs.append({"role": "user", "content": ["What should I pay attention to when traveling here?"]})\n\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    stream=True\n)\n\ngenerated_text = ""\nfor new_text in answer:\n    generated_text += new_text\n    print(new_text, flush=True, end='''')\n```\n\nYou will get the following output:\n\n```shell\n# round1\nThe landform in the picture is karst topography. Karst landscapes are characterized by distinctive, jagged limestone hills or mountains with steep, irregular peaks and deep valleys‚Äîexactly what you see here These unique formations result from the dissolution of soluble rocks like limestone over millions of years through water erosion.\n\nThis scene closely resembles the famous karst landscape of Guilin and Yangshuo in China‚Äôs Guangxi Province. The area features dramatic, pointed limestone peaks rising dramatically above serene rivers and lush green forests, creating a breathtaking and iconic natural beauty that attracts millions of visitors each year for its picturesque views.\n\n# round2\nWhen traveling to a karst landscape like this, here are some important tips:\n\n1. Wear comfortable shoes: The terrain can be uneven and hilly.\n2. Bring water and snacks for energy during hikes or boat rides.\n3. Protect yourself from the sun with sunscreen, hats, and sunglasses‚Äîespecially since you‚Äôll likely spend time outdoors exploring scenic spots.\n4. Respect local customs and nature regulations by not littering or disturbing wildlife.\n\nBy following these guidelines, you''ll have a safe and enjoyable trip while appreciating the stunning natural beauty of places such as Guilin‚Äôs karst mountains.\n```\n\n\n#### Chat with Video\n\n```python\n## The 3d-resampler compresses multiple frames into 64 tokens by introducing temporal_ids. \n# To achieve this, you need to organize your video data into two corresponding sequences: \n#   frames: List[Image]\n#   temporal_ids: List[List[Int]].\n\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\nfrom decord import VideoReader, cpu    # pip install decord\nfrom scipy.spatial import cKDTree\nimport numpy as np\nimport math\n\nmodel = AutoModel.from_pretrained(''openbmb/MiniCPM-V-4_5'', trust_remote_code=True,  # or openbmb/MiniCPM-o-2_6\n    attn_implementation=''sdpa'', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2, no eager\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained(''openbmb/MiniCPM-V-4_5'', trust_remote_code=True)  # or openbmb/MiniCPM-o-2_6\n\nMAX_NUM_FRAMES=180 # Indicates the maximum number of frames received after the videos are packed. The actual maximum number of valid frames is MAX_NUM_FRAMES * MAX_NUM_PACKING.\nMAX_NUM_PACKING=3  # indicates the maximum packing number of video frames. valid range: 1-6\nTIME_SCALE = 0.1 \n\ndef map_to_nearest_scale(values, scale):\n    tree = cKDTree(np.asarray(scale)[:, None])\n    _, indices = tree.query(np.asarray(values)[:, None])\n    return np.asarray(scale)[indices]\n\n\ndef group_array(arr, size):\n    return [arr[i:i+size] for i in range(0, len(arr), size)]\n\ndef encode_video(video_path, choose_fps=3, force_packing=None):\n    def uniform_sample(l, n):\n        gap = len(l) / n\n        idxs = [int(i * gap + gap / 2) for i in range(n)]\n        return [l[i] for i in idxs]\n    vr = VideoReader(video_path, ctx=cpu(0))\n    fps = vr.get_avg_fps()\n    video_duration = len(vr) / fps\n        \n    if choose_fps * int(video_duration) <= MAX_NUM_FRAMES:\n        packing_nums = 1\n        choose_frames = round(min(choose_fps, round(fps)) * min(MAX_NUM_FRAMES, video_duration))\n        \n    else:\n        packing_nums = math.ceil(video_duration * choose_fps / MAX_NUM_FRAMES)\n        if packing_nums <= MAX_NUM_PACKING:\n            choose_frames = round(video_duration * choose_fps)\n        else:\n            choose_frames = round(MAX_NUM_FRAMES * MAX_NUM_PACKING)\n            packing_nums = MAX_NUM_PACKING\n\n    frame_idx = [i for i in range(0, len(vr))]      \n    frame_idx =  np.array(uniform_sample(frame_idx, choose_frames))\n\n    if force_packing:\n        packing_nums = min(force_packing, MAX_NUM_PACKING)\n    \n    print(video_path, '' duration:'', video_duration)\n    print(f''get video frames={len(frame_idx)}, packing_nums={packing_nums}'')\n    \n    frames = vr.get_batch(frame_idx).asnumpy()\n\n    frame_idx_ts = frame_idx / fps\n    scale = np.arange(0, video_duration, TIME_SCALE)\n\n    frame_ts_id = map_to_nearest_scale(frame_idx_ts, scale) / TIME_SCALE\n    frame_ts_id = frame_ts_id.astype(np.int32)\n\n    assert len(frames) == len(frame_ts_id)\n\n    frames = [Image.fromarray(v.astype(''uint8'')).convert(''RGB'') for v in frames]\n    frame_ts_id_group = group_array(frame_ts_id, packing_nums)\n    \n    return frames, frame_ts_id_group\n\n\nvideo_path="video_test.mp4"\nfps = 5 # fps for video\nforce_packing = None # You can set force_packing to ensure that 3D packing is forcibly enabled; otherwise, encode_video will dynamically set the packing quantity based on the duration.\nframes, frame_ts_id_group = encode_video(video_path, fps, force_packing=force_packing)\n\nquestion = "Describe the video"\nmsgs = [\n    {''role'': ''user'', ''content'': frames + [question]}, \n]\n\n\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer,\n    use_image_id=False,\n    max_slice_nums=1,\n    temporal_ids=frame_ts_id_group\n)\nprint(answer)\n```\n\n#### Chat with multiple images\n<details>\n<summary> Click to show Python code running MiniCPM-V 4.5 with multiple images input. </summary>\n  \n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(''openbmb/MiniCPM-V-4_5'', trust_remote_code=True,\n    attn_implementation=''sdpa'', torch_dtype=torch.bfloat16) # sdpa or flash_attention_2\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained(''openbmb/MiniCPM-V-4_5'', trust_remote_code=True)\n\nimage1 = Image.open(''image1.jpg'').convert(''RGB'')\nimage2 = Image.open(''image2.jpg'').convert(''RGB'')\nquestion = ''Compare image 1 and image 2, tell me about the differences between image 1 and image 2.''\n\nmsgs = [{''role'': ''user'', ''content'': [image1, image2, question]}]\n\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n</details>\n\n\n#### In-context few-shot learning\n<details>\n<summary> Click to view Python code running MiniCPM-V 4.5 with few-shot input. </summary>\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(''openbmb/MiniCPM-V-4_5'', trust_remote_code=True,\n    attn_implementation=''sdpa'', torch_dtype=torch.bfloat16)\nmodel = model.eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained(''openbmb/MiniCPM-V-4_5'', trust_remote_code=True)\n\nquestion = "production date" \nimage1 = Image.open(''example1.jpg'').convert(''RGB'')\nanswer1 = "2023.08.04"\nimage2 = Image.open(''example2.jpg'').convert(''RGB'')\nanswer2 = "2007.04.24"\nimage_test = Image.open(''test.jpg'').convert(''RGB'')\n\nmsgs = [\n    {''role'': ''user'', ''content'': [image1, question]}, {''role'': ''assistant'', ''content'': [answer1]},\n    {''role'': ''user'', ''content'': [image2, question]}, {''role'': ''assistant'', ''content'': [answer2]},\n    {''role'': ''user'', ''content'': [image_test, question]}\n]\n\nanswer = model.chat(\n    msgs=msgs,\n    tokenizer=tokenizer\n)\nprint(answer)\n```\n</details>\n\n\n## License\n#### Model License\n* The MiniCPM-o/V model weights and code are open-sourced under the [Apache-2.0](https://github.com/OpenBMB/MiniCPM-V/blob/main/LICENSE) license.\n* To help us better understand and support our users, we would deeply appreciate it if you could consider optionally filling out a brief registration ["questionnaire"](https://modelbest.feishu.cn/share/base/form/shrcnpV5ZT9EJ6xYjh3Kx0J6v8g).\n\n#### Statement\n* As an LMM, MiniCPM-V 4.5 generates contents by learning a large amount of multimodal corpora, but it cannot comprehend, express personal opinions or make value judgement. Anything generated by MiniCPM-V 4.5 does not represent the views and positions of the model developers\n* We will not be liable for any problems arising from the use of the MinCPM-V models, including but not limited to data security issues, risk of public opinion, or any risks and problems arising from the misdirection, misuse, dissemination or misuse of the model.\n\n## Key Techniques and Other Multimodal Projects\n\nüëè Welcome to explore key techniques of MiniCPM-V 4.5 and other multimodal projects of our team:\n\n[VisCPM](https://github.com/OpenBMB/VisCPM/tree/main) | [RLPR](https://github.com/OpenBMB/RLPR) |  [RLHF-V](https://github.com/RLHF-V/RLHF-V) | [LLaVA-UHD](https://github.com/thunlp/LLaVA-UHD)  | [RLAIF-V](https://github.com/RLHF-V/RLAIF-V)\n\n## Citation\n\nIf you find our work helpful, please consider citing our papers üìù and liking this project ‚ù§Ô∏èÔºÅ\n\n```bib\n@misc{yu2025minicpmv45cookingefficient,\n      title={MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe}, \n      author={Tianyu Yu and Zefan Wang and Chongyi Wang and Fuwei Huang and Wenshuo Ma and Zhihui He and Tianchi Cai and Weize Chen and Yuxiang Huang and Yuanqian Zhao and Bokai Xu and Junbo Cui and Yingjing Xu and Liqing Ruan and Luoyuan Zhang and Hanyu Liu and Jingkun Tang and Hongyuan Liu and Qining Guo and Wenhao Hu and Bingxiang He and Jie Zhou and Jie Cai and Ji Qi and Zonghao Guo and Chi Chen and Guoyang Zeng and Yuxuan Li and Ganqu Cui and Ning Ding and Xu Han and Yuan Yao and Zhiyuan Liu and Maosong Sun},\n      year={2025},\n      eprint={2509.18154},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2509.18154}, \n}\n\n@article{yao2024minicpm,\n  title={MiniCPM-V: A GPT-4V Level MLLM on Your Phone},\n  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},\n  journal={Nat Commun 16, 5509 (2025)},\n  year={2025}\n}\n\n```', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":8695895280,"storage_bytes":17403328052,"files_count":23,"spaces_count":14,"gated":false,"private":false,"config":{"architectures":["MiniCPMV"],"auto_map":{"AutoConfig":"configuration_minicpm.MiniCPMVConfig","AutoModel":"modeling_minicpmv.MiniCPMV","AutoModelForCausalLM":"modeling_minicpmv.MiniCPMV"},"model_type":"minicpmv","tokenizer_config":{"bos_token":"<|im_start|>","chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(message.content.startswith(''<tool_response>'') and message.content.endswith(''</tool_response>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if ''</think>'' in message.content %}\n                {%- set content = message.content.split(''</think>'')[-1].lstrip(''\\n'') %}\n                {%- set reasoning_content = message.content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- ''<|im_start|>'' + message.role + ''\\n<think>\\n'' + reasoning_content.strip(''\\n'') + ''\\n</think>\\n\\n'' + content.lstrip(''\\n'') }}\n            {%- else %}\n                {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- ''<think>\\n\\n</think>\\n\\n'' }}\n    {%- endif %}\n    {%- if enable_thinking is defined and enable_thinking is true %}\n        {{- ''<think>\\n'' }}\n    {%- endif %}\n{%- endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:OpenBMB:MiniCPM-o","source_url":"https://github.com/OpenBMB/MiniCPM-o"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:RLHF-V:RLAIF-V","source_url":"https://github.com/RLHF-V/RLAIF-V"},{"type":"has_code","target_id":"github:OpenBMB:VisCPM","source_url":"https://github.com/OpenBMB/VisCPM"},{"type":"has_code","target_id":"github:tc-mb:llama.cpp","source_url":"https://github.com/tc-mb/llama.cpp"},{"type":"has_code","target_id":"github:tc-mb:ollama","source_url":"https://github.com/tc-mb/ollama"},{"type":"has_code","target_id":"github:tc-mb:AutoAWQ","source_url":"https://github.com/tc-mb/AutoAWQ"},{"type":"has_code","target_id":"github:tc-mb:sglang","source_url":"https://github.com/tc-mb/sglang"},{"type":"has_code","target_id":"github:tc-mb:transformers","source_url":"https://github.com/tc-mb/transformers"},{"type":"has_code","target_id":"github:tc-mb:MiniCPM-o-demo-iOS","source_url":"https://github.com/tc-mb/MiniCPM-o-demo-iOS"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:OpenBMB:RLPR","source_url":"https://github.com/OpenBMB/RLPR"},{"type":"has_code","target_id":"github:RLHF-V:RLAIF-V","source_url":"https://github.com/RLHF-V/RLAIF-V"},{"type":"has_code","target_id":"github:tc-mb:MiniCPM-o-demo-iOS","source_url":"https://github.com/tc-mb/MiniCPM-o-demo-iOS"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:ggml-org:llama.cpp","source_url":"https://github.com/ggml-org/llama.cpp"},{"type":"has_code","target_id":"github:ggml-org:llama.cpp","source_url":"https://github.com/ggml-org/llama.cpp"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:ollama:ollama","source_url":"https://github.com/ollama/ollama"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:hiyouga:LLaMA-Factory","source_url":"https://github.com/hiyouga/LLaMA-Factory"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:OpenSQZ:MiniCPM-V-CookBook","source_url":"https://github.com/OpenSQZ/MiniCPM-V-CookBook"},{"type":"has_code","target_id":"github:OpenBMB:MiniCPM-V","source_url":"https://github.com/OpenBMB/MiniCPM-V"},{"type":"has_code","target_id":"github:OpenBMB:VisCPM","source_url":"https://github.com/OpenBMB/VisCPM"},{"type":"has_code","target_id":"github:OpenBMB:RLPR","source_url":"https://github.com/OpenBMB/RLPR"},{"type":"has_code","target_id":"github:RLHF-V:RLHF-V","source_url":"https://github.com/RLHF-V/RLHF-V"},{"type":"has_code","target_id":"github:thunlp:LLaVA-UHD","source_url":"https://github.com/thunlp/LLaVA-UHD"},{"type":"has_code","target_id":"github:RLHF-V:RLAIF-V","source_url":"https://github.com/RLHF-V/RLAIF-V"},{"type":"based_on_paper","target_id":"arxiv:2509.18154","source_url":"https://arxiv.org/abs/2509.18154"},{"type":"based_on_paper","target_id":"arxiv:2403.11703","source_url":"https://arxiv.org/abs/2403.11703"}]', NULL, 'Apache-2.0', 'approved', 80, '6b80e37d4354664d062928ca5f73c007', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-adept-fuyu-8b', 'huggingface--adept--fuyu-8b', 'fuyu-8b', 'adept', '--- license: cc-by-nc-4.0 --- We‚Äôre releasing Fuyu-8B, a small version of the multimodal model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because: 1. It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy. 2. It‚Äôs designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answe...', '["transformers","safetensors","fuyu","any-to-any","license:cc-by-nc-4.0","endpoints_compatible","region:us"]', 'any-to-any', 1014, 47615, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/adept/fuyu-8b","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: cc-by-nc-4.0\n---\n# Fuyu-8B Model Card\n\nWe‚Äôre releasing Fuyu-8B, a small version of the multimodal model that powers our product. The model is available on HuggingFace. We think Fuyu-8B is exciting because:\n1. It has a much simpler architecture and training procedure than other multi-modal models, which makes it easier to understand, scale, and deploy.\n2. It‚Äôs designed from the ground up for digital agents, so it can support arbitrary image resolutions, answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.\n3. It‚Äôs fast - we can get responses for large images in less than 100 milliseconds.\n4. Despite being optimized for our use-case, it performs well at standard image understanding benchmarks such as visual question-answering and natural-image-captioning.\n\nPlease note that **the model we have released is a base model. We expect you to need to finetune the model for specific use cases like verbose captioning or multimodal chat.** In our experience, the model responds well to few-shotting and fine-tuning for a variety of use-cases. \n\n## Model\n\n[Fuyu-8B](https://www.adept.ai/blog/fuyu-8b) is a multi-modal text and image transformer trained by [Adept AI](https://www.adept.ai/).\n\nArchitecturally, Fuyu is a vanilla decoder-only transformer - there is no image encoder. \nImage patches are instead linearly projected into the first layer of the transformer, bypassing the embedding lookup. \nWe simply treat the transformer decoder like an image transformer (albeit with no pooling and causal attention).\nSee the below diagram for more details.\n\n![architecture](architecture.png)\n\nThis simplification allows us to support arbitrary image resolutions. \nTo accomplish this, we treat the sequence of image tokens like the sequence of text tokens. \nWe remove image-specific position embeddings and feed in as many image tokens as necessary in raster-scan order. \nTo tell the model when a line has broken, we simply use a special image-newline character. \nThe model can use its existing position embeddings to reason about different image sizes, and we can use images of arbitrary size at training time, removing the need for separate high and low-resolution training stages.\n\n### Model Description\n\n- **Developed by:** Adept-AI\n- **Model type:** Decoder-only multi-modal transformer model \n- **License:** [CC-BY-NC](https://creativecommons.org/licenses/by-nc/4.0/deed.en)\n- **Model Description:** This is a multi-modal model that can consume images and text and produce text. \n- **Resources for more information:** Check out our [blog post](https://www.adept.ai/blog/fuyu-8b).\n\n## Evaluation\nThough not the focus of this model, we did evaluate it on standard image understanding benchmarks:\n\n| Eval Task           | Fuyu-8B | Fuyu-Medium       | LLaVA 1.5 (13.5B) | QWEN-VL (10B) | PALI-X (55B) | PALM-e-12B | PALM-e-562B |\n| ------------------- | ------- | ----------------- | ----------------- | ------------- | ------------ | ---------- | ----------- |\n| VQAv2               | 74.2    |     77.4          | 80                | 79.5          | 86.1         | 76.2       | 80.0        |\n| OKVQA               | 60.6    |     63.1          | n/a               | 58.6          | 66.1         | 55.5       | 66.1        |\n| COCO Captions       | 141     |     138           | n/a               | n/a           | 149          | 135        | 138         |\n| AI2D                | 64.5    |     73.7          | n/a               | 62.3          | 81.2         | n/a        | n/a         |\n\n## How to Use\n\nYou can load the model and perform inference as follows:\n```python\nfrom transformers import FuyuProcessor, FuyuForCausalLM\nfrom PIL import Image\nimport requests\n\n# load model and processor\nmodel_id = "adept/fuyu-8b"\nprocessor = FuyuProcessor.from_pretrained(model_id)\nmodel = FuyuForCausalLM.from_pretrained(model_id, device_map="cuda:0")\n\n# prepare inputs for the model\ntext_prompt = "Generate a coco-style caption.\n"\nurl = "https://huggingface.co/adept/fuyu-8b/resolve/main/bus.png"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=text_prompt, images=image, return_tensors="pt").to("cuda:0")\n\n# autoregressively generate text\ngeneration_output = model.generate(**inputs, max_new_tokens=7)\ngeneration_text = processor.batch_decode(generation_output[:, -7:], skip_special_tokens=True)\nassert generation_text == [''A blue bus parked on the side of a road.'']\n```\n\nN.B.: The token `|SPEAKER|` is a placeholder token for image patch embeddings, so it will show up in the model context (e.g., in the portion of `generation_output` representing the model context).\n`|NEWLINE|` is the "image newline" token, denoting new rows in the raster scan order input of the image patches.\n`\x04` is the "beginning of answer" token.\n\nFuyu can also perform some question answering on natural images and charts/diagrams (thought fine-tuning may be required for good performance):\n```python\ntext_prompt = "What color is the bus?\n"\nurl = "https://huggingface.co/adept/fuyu-8b/resolve/main/bus.png"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=text_prompt, images=image, return_tensors="pt").to("cuda:0")\n\ngeneration_output = model.generate(**inputs, max_new_tokens=6)\ngeneration_text = processor.batch_decode(generation_output[:, -6:], skip_special_tokens=True)\nassert generation_text == ["The bus is blue.\n"]\n\n\ntext_prompt = "What is the highest life expectancy at birth of male?\n"\nurl = "https://huggingface.co/adept/fuyu-8b/resolve/main/chart.png"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel_inputs = processor(text=text_prompt, images=image, return_tensors="pt").to("cuda:0")\n\ngeneration_output = model.generate(**model_inputs, max_new_tokens=16)\ngeneration_text = processor.batch_decode(generation_output[:, -16:], skip_special_tokens=True)\nassert generation_text == ["The life expectancy at birth of males in 2018 is 80.7.\n"]\n```\nFor best performance, it''s recommended to end questions with `\n`, as shown above!\n\n## Uses\n\n### Direct Use\n\nThe model is intended for research purposes only. \n**Because this is a raw model release, we have not added further finetuning, postprocessing or sampling strategies to control for undesirable outputs. You should expect to have to fine-tune the model for your use-case.**\n\nPossible research areas and tasks include\n\n- Applications in computer control or digital agents.\n- Research on multi-modal models generally.\n\nExcluded uses are described below.\n\n### Out-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n## Limitations and Bias\n\n### Limitations\n\n- Faces and people in general may not be generated properly.\n\n### Bias\nWhile the capabilities of these models are impressive, they can also reinforce or exacerbate social biases.', '{"pipeline_tag":"any-to-any","library_name":"transformers","framework":"transformers","params":9408238592,"storage_bytes":37678352448,"files_count":17,"spaces_count":55,"gated":false,"private":false,"config":{"architectures":["FuyuForCausalLM"],"model_type":"fuyu","tokenizer_config":{"bos_token":"|ENDOFTEXT|","eos_token":"|ENDOFTEXT|","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":true}}}', '[]', '[]', NULL, 'CC-BY-NC-4.0', 'approved', 85, '1581e56ae03f606824f18ca395ff8b44', NULL, 'https://huggingface.co/adept/fuyu-8b/resolve/main/architecture.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-adept-fuyu-8b from https://huggingface.co/adept/fuyu-8b/resolve/main/architecture.png
Image converted to WebP: data/images/huggingface-adept-fuyu-8b.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openbmb-MiniCPM-V-2-6', 'huggingface--openbmb--minicpm-v-2-6', 'MiniCPM-V-2_6', 'openbmb', '', '["transformers","safetensors","minicpmv","feature-extraction","minicpm-v","vision","ocr","multi-image","video","custom_code","image-text-to-text","conversational","multilingual","dataset:openbmb/rlaif-v-dataset","arxiv:2408.01800","region:us"]', 'image-text-to-text', 1013, 100147, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openbmb/MiniCPM-V-2_6","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":8099175152,"storage_bytes":16221778104,"files_count":24,"spaces_count":28,"gated":"auto","private":false,"config":{"architectures":["MiniCPMV"],"auto_map":{"AutoConfig":"configuration_minicpm.MiniCPMVConfig","AutoModel":"modeling_minicpmv.MiniCPMV","AutoModelForCausalLM":"modeling_minicpmv.MiniCPMV"},"model_type":"minicpmv","tokenizer_config":{"bos_token":"<|im_start|>","chat_template":"{% for message in messages %}{% if loop.first and messages[0][''role''] != ''system'' %}{{ ''<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n'' }}{% endif %}{{''<|im_start|>'' + message[''role''] + ''\n'' + message[''content''] + ''<|im_end|>'' + ''\n''}}{% endfor %}{% if add_generation_prompt %}{{ ''<|im_start|>assistant\n'' }}{% endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":"<unk>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2408.01800","source_url":"https://arxiv.org/abs/2408.01800"}]', NULL, NULL, 'pending', 50, '28f3a737bbf64fc3387ce729dd038168', NULL, 'https://huggingface.co/openbmb/MiniCPM-V-2_6/resolve/main/assets/radar_final.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-openbmb-MiniCPM-V-2-6 from https://huggingface.co/openbmb/MiniCPM-V-2_6/resolve/main/assets/radar_final.png
HTTP error: 401 Unauthorized

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-V3.1-Base', 'huggingface--deepseek-ai--deepseek-v3.1-base', 'DeepSeek-V3.1-Base', 'deepseek-ai', '--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/d...', '["transformers","safetensors","deepseek_v3","text-generation","conversational","custom_code","arxiv:2412.19437","license:mit","text-generation-inference","endpoints_compatible","fp8","region:us"]', 'text-generation', 1003, 7784, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-V3.1\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n## Introduction\n\nDeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. Compared to the previous version, this upgrade brings improvements in multiple aspects:\n\n- **Hybrid thinking mode**: One model supports both thinking mode and non-thinking mode by changing the chat template. \n\n- **Smarter tool calling**: Through post-training optimization, the model''s performance in tool usage and agent tasks has significantly improved.\n\n- **Higher thinking efficiency**: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly.\n\nDeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. We have expanded our dataset by collecting additional long documents and substantially extending both training phases. The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens.\n\nAdditionally, DeepSeek-V3.1 is trained using the **UE8M0 FP8 scale data format on both model weights and activations** to ensure compatibility with microscaling data formats. Please refer to [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM) for more details.\n\n## Model Downloads\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-V3.1-Base | 671B | 37B | 128K | [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base) \| [ModelScope](https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.1-Base) |\n| DeepSeek-V3.1 | 671B | 37B | 128K | [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3.1) \| [ModelScope](https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.1) |\n\n</div>\n\n## Chat Template\n\nThe details of our chat template is described in `tokenizer_config.json` and `assets/chat_template.jinja`. Here is a brief description.\n\n### Non-Thinking\n\n#### First-Turn\n\nPrefix:\n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>`\n\nWith the given prefix, DeepSeek V3.1 generates responses to queries in non-thinking mode. Unlike DeepSeek V3,  it introduces an additional token `</think>`.\n\n#### Multi-Turn\nContext:\n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>...<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>`\n\nPrefix:\n`<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>`\n\nBy concatenating the context and the prefix, we obtain the correct prompt for the query.\n\n### Thinking\n\n#### First-Turn\nPrefix:\n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú><think>`\n\nThe prefix of thinking mode is similar to DeepSeek-R1. \n\n\n#### Multi-Turn\nContext:\n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>...<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>`\n\nPrefix:\n`<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú><think>`\n\nThe multi-turn template is the same with non-thinking multi-turn chat template. It means the thinking token in the last turn will be dropped but the `</think>` is retained in every turn of context. \n\n### ToolCall\nToolcall is supported in non-thinking mode. The format is: \n\n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}\n\n{tool_description}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>` where the tool_description is \n\n```\n## Tools\nYou have access to the following tools:\n\n### {tool_name1}\nDescription: {description}\n\nParameters: {json.dumps(parameters)}\n\nIMPORTANT: ALWAYS adhere to this exact format for tool use:\n<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>tool_call_name<ÔΩútool‚ñÅsepÔΩú>tool_call_arguments<ÔΩútool‚ñÅcall‚ñÅendÔΩú>{additional_tool_calls}<ÔΩútool‚ñÅcalls‚ñÅendÔΩú>\n\nWhere:\n- `tool_call_name` must be an exact match to one of the available tools\n- `tool_call_arguments` must be valid JSON that strictly follows the tool''s Parameters Schema\n- For multiple tool calls, chain them directly without separators or spaces\n```\n\n### Code-Agent\nWe support various code agent frameworks. Please refer to the above toolcall format to create your own code agents. An example is shown in `assets/code_agent_trajectory.html`.\n\n### Search-Agent\nWe design a specific format for searching toolcall in thinking mode, to support search agent. \n\nFor complex questions that require accessing external or up-to-date information, DeepSeek-V3.1 can leverage a user-provided search tool through a multi-turn tool-calling process.\n\nPlease refer to the `assets/search_tool_trajectory.html` and `assets/search_python_tool_trajectory.html` for the detailed template.\n\n## Evaluation\n| Category | Benchmark (Metric)              | DeepSeek V3.1-NonThinking | DeepSeek V3 0324 | DeepSeek V3.1-Thinking     | DeepSeek R1 0528\n|----------|----------------------------------|-----------------|---|---|---|\n| General  |\n|          | MMLU-Redux (EM)              | 91.8     | 90.5    | 93.7          | 93.4\n|          | MMLU-Pro (EM)                  | 83.7  | 81.2    | 84.8          | 85.0\n|          | GPQA-Diamond (Pass@1)           | 74.9   | 68.4   | 80.1            | 81.0\n|          | Humanity''s Last Exam (Pass@1)   | -    |       -            | 15.9         | 17.7\n|Search Agent| \n|          | BrowseComp       | -      | -  | 30.0 | 8.9\n|          | BrowseComp_zh       | -     | -  | 49.2      | 35.7\n|          | Humanity''s Last Exam (Python + Search)      |-   | -    | 29.8         | 24.8\n|          | SimpleQA             | -      | -    | 93.4  | 92.3\n| Code |\n|          | LiveCodeBench (2408-2505) (Pass@1)     | 56.4    | 43.0    | 74.8          | 73.3\n|          | Codeforces-Div1 (Rating)        | -   | -    | 2091            | 1930\n|          | Aider-Polyglot (Acc.)           | 68.4    | 55.1   | 76.3           | 71.6\n| Code Agent|\n|          | SWE Verified (Agent mode)           | 66.0       | 45.4  | -    | 44.6\n|          | SWE-bench Multilingual (Agent mode)         | 54.5    | 29.3   | -            | 30.5\n|          | Terminal-bench (Terminus 1 framework)       | 31.3     | 13.3      | -         | 5.7\n| Math |\n|          | AIME 2024 (Pass@1)                | 66.3     | 59.4     | 93.1      | 91.4\n|          | AIME 2025 (Pass@1)                     | 49.8  | 51.3 | 88.4          | 87.5\n|          | HMMT 2025 (Pass@1)        | 33.5    | 29.2   | 84.2 | 79.4 |\n\nNote: \n- Search agents are evaluated with our internal search framework, which uses a commercial search API + webpage filter + 128K context window. Seach agent results of R1-0528 are evaluated with a pre-defined workflow. \n\n- SWE-bench is evaluated with our internal code agent framework.\n\n- HLE is evaluated with the text-only subset.\n\n### Usage Example\n\n```python\nimport transformers\n\ntokenizer = transformers.AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-V3.1")\n\nmessages = [\n    {"role": "system", "content": "You are a helpful assistant"},\n    {"role": "user", "content": "Who are you?"},\n    {"role": "assistant", "content": "<think>Hmm</think>I am DeepSeek"},\n    {"role": "user", "content": "1+1=?"}\n]\n\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=True, add_generation_prompt=True)\n# ''<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>You are a helpful assistant<ÔΩúUserÔΩú>Who are you?<ÔΩúAssistantÔΩú></think>I am DeepSeek<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>1+1=?<ÔΩúAssistantÔΩú><think>''\n\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=False, add_generation_prompt=True)\n# ''<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>You are a helpful assistant<ÔΩúUserÔΩú>Who are you?<ÔΩúAssistantÔΩú></think>I am DeepSeek<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>1+1=?<ÔΩúAssistantÔΩú></think>''\n```\n\n## How to Run Locally\n\nThe model structure of DeepSeek-V3.1 is the same as DeepSeek-V3. Please visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running this model locally.\n\n**Usage Recommendations:**\n\n1. **The `mlp.gate.e_score_correction_bias `parameters should be loaded and computed in FP32 precision.**\n2. **Ensure that FP8 model weights and activations are formatted using the UE8M0 scale format.**\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n```\n\n## Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":684531386000,"storage_bytes":688587108667,"files_count":177,"spaces_count":14,"gated":false,"private":false,"config":{"architectures":["DeepseekV3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_deepseek.DeepseekV3Config","AutoModel":"modeling_deepseek.DeepseekV3Model","AutoModelForCausalLM":"modeling_deepseek.DeepseekV3ForCausalLM"},"model_type":"deepseek_v3","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% if not thinking is defined %}{% set thinking = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, system_prompt='''', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\n\n'' + message[''content''] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{''<ÔΩúUserÔΩú>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''tool_calls''] is defined and message[''tool_calls''] is not none %}{%- if ns.is_last_user %}{{''<ÔΩúAssistantÔΩú></think>''}}{%- endif %}{%- set ns.is_last_user = false -%}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls''] %}{%- if not ns.is_first %}{%- if message[''content''] is none %}{{''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>''+ tool[''function''][''name''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''arguments''] + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- else %}{{message[''content''] + ''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''function''][''name''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''arguments''] + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>''+ tool[''function''][''name''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''arguments''] + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- endfor %}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and (message[''tool_calls''] is not defined or message[''tool_calls''] is none) %}{%- if ns.is_last_user %}{{''<ÔΩúAssistantÔΩú>''}}{%- if message[''prefix''] is defined and message[''prefix''] and thinking %}{{''<think>''}}  {%- else %}{{''</think>''}}{%- endif %}{%- endif %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{%- set content = message[''content''] -%}{%- if ''</think>'' in content %}{%- set content = content.split(''</think>'', 1)[1] -%}{%- endif %}{{content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{{''<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endfor -%}{%- if add_generation_prompt and ns.is_last_user and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú>''}}{%- if not thinking %}{{''</think>''}}{%- else %}{{''<think>''}}{%- endif %}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepGEMM","source_url":"https://github.com/deepseek-ai/DeepGEMM"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"based_on_paper","target_id":"arxiv:2412.19437","source_url":"https://arxiv.org/abs/2412.19437"}]', NULL, 'MIT', 'approved', 80, '6b87557a767151299f2ad5247bf1ec2d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-stable-video-diffusion-img2vid', 'huggingface--stabilityai--stable-video-diffusion-img2vid', 'stable-video-diffusion-img2vid', 'stabilityai', '--- pipeline_tag: image-to-video license: other license_name: stable-video-diffusion-community license_link: LICENSE.md --- <!-- Provide a quick summary of what the model is/does. --> !row01 Stable Video Diffusion (SVD) Image-to-Video is a diffusion model that takes in a still image as a conditioning frame, and generates a video from it. Please note: For commercial use of this model, please refer to https://stability.ai/license. (SVD) Image-to-Video is a latent diffusion model trained to gene...', '["diffusers","safetensors","image-to-video","license:other","diffusers:stablevideodiffusionpipeline","region:us"]', 'image-to-video', 1000, 532407, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/stable-video-diffusion-img2vid","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\npipeline_tag: image-to-video\nlicense: other\nlicense_name: stable-video-diffusion-community\nlicense_link: LICENSE.md\n---\n\n# Stable Video Diffusion Image-to-Video Model Card\n\n<!-- Provide a quick summary of what the model is/does. -->\n![row01](output_tile.gif)\nStable Video Diffusion (SVD) Image-to-Video is a diffusion model that takes in a still image as a conditioning frame, and generates a video from it. \n\nPlease note: For commercial use of this model, please refer to https://stability.ai/license.\n\n## Model Details\n\n### Model Description\n\n(SVD) Image-to-Video is a latent diffusion model trained to generate short video clips from an image conditioning. \nThis model was trained to generate 14 frames at resolution 576x1024 given a context frame of the same size.\nWe also finetune the widely used [f8-decoder](https://huggingface.co/docs/diffusers/api/models/autoencoderkl#loading-from-the-original-format) for temporal consistency. \nFor convenience, we additionally provide the model with the \nstandard frame-wise decoder [here](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid/blob/main/svd_image_decoder.safetensors).\n\n\n- **Developed by:** Stability AI\n- **Funded by:** Stability AI\n- **Model type:** Generative image-to-video model\n\n### Model Sources\n\nFor research purposes, we recommend our `generative-models` Github repository (https://github.com/Stability-AI/generative-models), \nwhich implements the most popular diffusion frameworks (both training and inference).\n\n- **Repository:** https://github.com/Stability-AI/generative-models\n- **Paper:** https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets\n\n## Evaluation\n![comparison](comparison.png)\nThe chart above evaluates user preference for SVD-Image-to-Video over [GEN-2](https://research.runwayml.com/gen2) and [PikaLabs](https://www.pika.art/).\nSVD-Image-to-Video is preferred by human voters in terms of video quality. For details on the user study, we refer to the [research paper](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets)\n\n## Uses\n\n### Direct Use\n\nThe model is intended for research purposes only. Possible research areas and tasks include\n\n- Research on generative models.\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n\nExcluded uses are described below.\n\n### Out-of-Scope Use\n\nThe model was not trained to be factual or true representations of people or events, \nand therefore using the model to generate such content is out-of-scope for the abilities of this model.\nThe model should not be used in any way that violates Stability AI''s [Acceptable Use Policy](https://stability.ai/use-policy).\n\n## Limitations and Bias\n\n### Limitations\n- The generated videos are rather short (<= 4sec), and the model does not achieve perfect photorealism.\n- The model may generate videos without motion, or very slow camera pans.\n- The model cannot be controlled through text.\n- The model cannot render legible text.\n- Faces and people in general may not be generated properly.\n- The autoencoding part of the model is lossy.\n\n\n### Recommendations\n\nThe model is intended for research purposes only.\n\n## How to Get Started with the Model\n\nCheck out https://github.com/Stability-AI/generative-models\n\n\n\n# Appendix:\nAll considered potential data sources were included for final training, with none held out as the proposed data filtering methods described in the SVD paper handle the quality control/filtering of the dataset. With regards to safety/NSFW filtering, sources considered were either deemed safe or filtered with the in-house NSFW filters. No explicit human labor is involved in training data preparation. However, human evaluation for model outputs and quality was extensively used to evaluate model quality and performance. The evaluations were performed with third-party contractor platforms (Amazon Sagemaker, Amazon Mechanical Turk, Prolific) with fluent English-speaking contractors from various countries, primarily from the USA, UK, and Canada. Each worker was paid $12/hr for the time invested in the evaluation. No other third party was involved in the development of this model; the model was fully developed in-house at Stability AI. Training the SVD checkpoints required a total of approximately 200,000 A100 80GB hours. The majority of the training occurred on 48 * 8 A100s, while some stages took more/less than that. The resulting CO2 emission is ~19,000kg CO2 eq., and energy consumed is ~64000 kWh. The released checkpoints (SVD/SVD-XT) are image-to-video models that generate short videos/animations closely following the given input image. Since the model relies on an existing supplied image, the potential risks of disclosing specific material or novel unsafe content are minimal. This was also evaluated by third-party independent red-teaming services, which agree with our conclusion to a high degree of confidence (>90% in various areas of safety red-teaming). The external evaluations were also performed for trustworthiness, leading to >95% confidence in real, trustworthy videos. With the default settings at the time of release, SVD takes ~100s for generation, and SVD-XT takes ~180s on an A100 80GB card. Several optimizations to trade off quality / memory / speed can be done to perform faster inference or inference on lower VRAM cards. The information related to the model and its development process and usage protocols can be found in the GitHub repo, associated research paper, and HuggingFace model page/cards. The released model inference & demo code has image-level watermarking enabled by default, which can be used to detect the outputs. This is done via the imWatermark Python library.\nThe model can be used to generate videos from static initial images. However, we prohibit unlawful, obscene, or misleading uses of the model consistent with the terms of our license and Acceptable Use Policy. For the open-weights release, our training data filtering mitigations alleviate this risk to some extent. These restrictions are explicitly enforced on user-facing interfaces at stablevideo.com, where a warning is issued. We do not take any responsibility for third-party interfaces. Submitting initial images that bypass input filters to tease out offensive or inappropriate content listed above is also prohibited. Safety filtering checks at stablevideo.com run on model inputs and outputs independently. More details on our user-facing interfaces can be found here: https://www.stablevideo.com/faq. Beyond the Acceptable Use Policy and other mitigations and conditions described here, the model is not subject to additional model behavior interventions of the type described in the Foundation Model Transparency Index.\nFor stablevideo.com, we store preference data in the form of upvotes/downvotes on user-generated videos, and we have a pairwise ranker that runs while a user generates videos. This usage data is solely used for improving Stability AI‚Äôs future image/video models and services. No other third-party entities are given access to the usage data beyond Stability AI and maintainers of stablevideo.com. For usage statistics of SVD, we refer interested users to HuggingFace model download/usage statistics as a primary indicator. Third-party applications also have reported model usage statistics. We might also consider releasing aggregate usage statistics of stablevideo.com on reaching some milestones.', '{"pipeline_tag":"image-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":32608765959,"files_count":19,"spaces_count":97,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableVideoDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:Stability-AI:generative-models","source_url":"https://github.com/Stability-AI/generative-models"},{"type":"has_code","target_id":"github:Stability-AI:generative-models","source_url":"https://github.com/Stability-AI/generative-models"},{"type":"has_code","target_id":"github:Stability-AI:generative-models","source_url":"https://github.com/Stability-AI/generative-models"}]', NULL, 'Other', 'approved', 65, '4972f0d945118055291a5246d77335aa', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-xai-org-grok-2', 'huggingface--xai-org--grok-2', 'grok-2', 'xai-org', 'This repository contains the weights of Grok 2, a model trained and used at xAI in 2024. - Download the weights. You can replace with any other folder name you prefer. You might encounter some errors during the download. Please retry until the download is successful. If the download succeeds, the folder should contain **42 files** and be approximately 500 GB. - Launch a server. Install the latest SGLang inference engine (>= v0.5.1) from https://github.com/sgl-project/sglang/ Use the command b...', '["git","region:us"]', 'other', 996, 2489, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/xai-org/grok-2","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# Grok 2\n\nThis repository contains the weights of Grok 2, a model trained and used at xAI in 2024.\n\n## Usage: Serving with SGLang\n\n- Download the weights. You can replace `/local/grok-2` with any other folder name you prefer.\n\n  ```\n  hf download xai-org/grok-2 --local-dir /local/grok-2\n  ```\n\n  You might encounter some errors during the download. Please retry until the download is successful.  \n  If the download succeeds, the folder should contain **42 files** and be approximately 500 GB.\n\n- Launch a server.\n\n  Install the latest SGLang inference engine (>= v0.5.1) from https://github.com/sgl-project/sglang/\n\n  Use the command below to launch an inference server. This checkpoint is TP=8, so you will need 8 GPUs (each with > 40GB of memory).\n  ```\n  python3 -m sglang.launch_server --model /local/grok-2 --tokenizer-path /local/grok-2/tokenizer.tok.json --tp 8 --quantization fp8 --attention-backend triton\n  ```\n\n- Send a request.\n\n  This is a post-trained model, so please use the correct [chat template](https://github.com/sgl-project/sglang/blob/97a38ee85ba62e268bde6388f1bf8edfe2ca9d76/python/sglang/srt/tokenizer/tiktoken_tokenizer.py#L106).\n\n  ```\n  python3 -m sglang.test.send_one --prompt "Human: What is your name?<|separator|>\n\nAssistant:"\n  ```\n\n  You should be able to see the model output its name, Grok.\n\n  Learn more about other ways to send requests [here](https://docs.sglang.ai/basic_usage/send_request.html).\n\n## License\n\nThe weights are licensed under the [Grok 2 Community License Agreement](https://huggingface.co/xai-org/grok-2/blob/main/LICENSE).', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":539032697512,"files_count":44,"spaces_count":5,"gated":false,"private":false,"config":{"architectures":["Grok1ForCausalLM"],"model_type":"git"}}', '[]', '[{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"}]', NULL, NULL, 'pending', 40, 'b7b02a48a0d0ac223856f6f182d3b891', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Lykon-DreamShaper', 'huggingface--lykon--dreamshaper', 'DreamShaper', 'Lykon', '--- language: - en license: other tags: - stable-diffusion - stable-diffusion-diffusers - text-to-image - art - artistic - diffusers - anime inference: false --- Read more about this model here: https://civitai.com/models/4384/dreamshaper Also please support by giving 5 stars and a heart, which will notify new updates. Please consider supporting me on Patreon or buy me a coffee - https://www.patreon.com/Lykon275 - https://snipfeed.co/lykon You can run this model on: - https://huggingface.co/s...', '["diffusers","stable-diffusion","stable-diffusion-diffusers","text-to-image","art","artistic","anime","en","doi:10.57967/hf/0453","license:other","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 994, 92211, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Lykon/DreamShaper","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\nlicense: other\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\n- art\n- artistic\n- diffusers\n- anime\ninference: false\n---\n\n# Dream Shaper\n## Official Repository\n\nRead more about this model here: https://civitai.com/models/4384/dreamshaper\n\nAlso please support by giving 5 stars and a heart, which will notify new updates.\n\nPlease consider supporting me on Patreon or buy me a coffee\n- https://www.patreon.com/Lykon275\n- https://snipfeed.co/lykon\n\nYou can run this model on:\n- https://huggingface.co/spaces/Lykon/DreamShaper-webui\n- Mage.space, sinkin.ai and more', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":359810959620,"files_count":74,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[]', NULL, 'Other', 'approved', 50, '53f2d23141175aada2f5fc7ab393604d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-R1-0528-Qwen3-8B', 'huggingface--deepseek-ai--deepseek-r1-0528-qwen3-8b', 'DeepSeek-R1-0528-Qwen3-8B', 'deepseek-ai', '--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/d...', '["transformers","safetensors","qwen3","text-generation","conversational","arxiv:2501.12948","license:mit","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 993, 400747, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1-0528\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n \n\n<p align="center">\n  <a href="https://arxiv.org/pdf/2501.12948"><b>Paper Link</b>üëÅÔ∏è</a>\n</p>\n\n\n## 1. Introduction\n\nThe DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.\n\n<p align="center">\n  <img width="80%" src="figures/benchmark.png">\n</p>\n\nCompared to the previous version, the upgraded model shows significant improvements in handling complex reasoning tasks. For instance, in the AIME 2025 test, the model‚Äôs accuracy has increased from 70% in the previous version to 87.5% in the current version. This advancement stems from enhanced thinking depth during the reasoning process: in the AIME test set, the previous model used an average of 12K tokens per question, whereas the new version averages 23K tokens per question.\n\nBeyond its improved reasoning capabilities, this version also offers a reduced hallucination rate, enhanced support for function calling, and better experience for vibe coding.\n\n## 2. Evaluation Results\n\n### DeepSeek-R1-0528\n For all our models, the maximum generation length is set to 64K tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 16 responses per query to estimate pass@1.\n<div align="center">\n\n| Category | Benchmark (Metric)               | DeepSeek R1     | DeepSeek R1 0528\n|----------|----------------------------------|-----------------|---|\n| General  |\n|          | MMLU-Redux (EM)                   | 92.9            | 93.4\n|          | MMLU-Pro (EM)                     | 84.0            | 85.0\n|          | GPQA-Diamond (Pass@1)             | 71.5            | 81.0\n|          | SimpleQA (Correct)                | 30.1            | 27.8\n|          | FRAMES (Acc.)                     | 82.5            | 83.0\n|          | Humanity''s Last Exam (Pass@1)                     | 8.5            | 17.7\n| Code |\n|          | LiveCodeBench (2408-2505) (Pass@1)        | 63.5          | 73.3\n|          | Codeforces-Div1 (Rating)          | 1530            | 1930\n|          | SWE Verified (Resolved)           | 49.2            | 57.6\n|          | Aider-Polyglot (Acc.)             | 53.3            | 71.6\n| Math |\n|          | AIME 2024 (Pass@1)                | 79.8            | 91.4\n|          | AIME 2025 (Pass@1)                     | 70.0           | 87.5\n|          | HMMT 2025 (Pass@1)            | 41.7 | 79.4 |\n|          | CNMO 2024 (Pass@1)                | 78.8            | 86.9\n| Tools |\n|          | BFCL_v3_MultiTurn (Acc)     | -            | 37.0 |\n|          | Tau-Bench   (Pass@1)       | -            | 53.5(Airline)/63.9(Retail)\n\n</div>\nNote: We use Agentless framework to evaluate model performance on SWE-Verified. We only evaluate text-only prompts in HLE testsets.  GPT-4.1 is employed to act user role in Tau-bench evaluation.\n\n### DeepSeek-R1-0528-Qwen3-8B\nMeanwhile, we distilled the chain-of-thought from DeepSeek-R1-0528 to post-train Qwen3 8B Base, obtaining DeepSeek-R1-0528-Qwen3-8B. This model achieves state-of-the-art (SOTA) performance among open-source models on the AIME 2024, surpassing Qwen3 8B by +10.0% and matching the performance of Qwen3-235B-thinking. We believe that the chain-of-thought from DeepSeek-R1-0528 will hold significant importance for both academic research on reasoning models and industrial development focused on small-scale models.\n\n|                                | AIME 24 | AIME 25 | HMMT Feb 25 | GPQA Diamond | LiveCodeBench (2408-2505) |\n|--------------------------------|---------|---------|-------------|--------------|---------------------------|\n| Qwen3-235B-A22B	                | 85.7    | 81.5    | 62.5        | 71.1         | 66.5                  |\n| Qwen3-32B                      | 81.4    | 72.9    | -           | 68.4         | -                         |\n| Qwen3-8B                      | 76.0   | 67.3    | -           | 62.0       | -                         |\n| Phi-4-Reasoning-Plus-14B       | 81.3    | 78.0    | 53.6        | 69.3         | -          |\n| Gemini-2.5-Flash-Thinking-0520 | 82.3    | 72.0    | 64.2        | 82.8         | 62.3                  |\n| o3-mini (medium)               | 79.6    | 76.7    | 53.3        | 76.8         | 65.9                     |\n| DeepSeek-R1-0528-Qwen3-8B      | 86.0   | 76.3    | 61.5        | 61.1         | 60.5                      |\n\n## 3. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek''s official website: [chat.deepseek.com](https://chat.deepseek.com/sign_in), and switch on the button "DeepThink"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 4. How to Run Locally\n\nPlease visit [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1) repository for more information about running DeepSeek-R1-0528 locally.\n\nCompared to previous versions of DeepSeek-R1, the usage recommendations for DeepSeek-R1-0528 have the following changes:\n\n1. System prompt is supported now.\n2. It is not required to add "\<think\>\n" at the beginning of the output to force the model into thinking pattern.\n\nThe model architecture of DeepSeek-R1-0528-Qwen3-8B is identical to that of Qwen3-8B, but it shares the same tokenizer configuration as DeepSeek-R1-0528. This model can be run in the same manner as Qwen3-8B, but it is essential to ensure that all configuration files are sourced from our repository rather than the original Qwen3 project.\n\n### System Prompt\nIn the official DeepSeek web/app, we use the same system prompt with a specific date.\n```\nËØ•Âä©Êâã‰∏∫DeepSeek-R1ÔºåÁî±Ê∑±Â∫¶Ê±ÇÁ¥¢ÂÖ¨Âè∏ÂàõÈÄ†„ÄÇ\n‰ªäÂ§©ÊòØ{current date}„ÄÇ\n```\nFor example,\n```\nËØ•Âä©Êâã‰∏∫DeepSeek-R1ÔºåÁî±Ê∑±Â∫¶Ê±ÇÁ¥¢ÂÖ¨Âè∏ÂàõÈÄ†„ÄÇ\n‰ªäÂ§©ÊòØ2025Âπ¥5Êúà28Êó•ÔºåÊòüÊúü‰∏Ä„ÄÇ\n```\n### Temperature\nIn our web and application environments, the temperature parameter $T_{model}$ is set to 0.6. \n### Prompts for File Uploading and Web Search\nFor file uploading, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments.\n```\nfile_template = \\n"""[file name]: {file_name}\n[file content begin]\n{file_content}\n[file content end]\n{question}"""\n```\nFor Web Search, {search_results}, {cur_date}, and {question} are arguments.\nFor Chinese query, we use the prompt:\n```\nsearch_answer_zh_template = \\n''''''# ‰ª•‰∏ãÂÜÖÂÆπÊòØÂü∫‰∫éÁî®Êà∑ÂèëÈÄÅÁöÑÊ∂àÊÅØÁöÑÊêúÁ¥¢ÁªìÊûú:\n{search_results}\nÂú®ÊàëÁªô‰Ω†ÁöÑÊêúÁ¥¢ÁªìÊûú‰∏≠ÔºåÊØè‰∏™ÁªìÊûúÈÉΩÊòØ[webpage X begin]...[webpage X end]Ê†ºÂºèÁöÑÔºåX‰ª£Ë°®ÊØèÁØáÊñáÁ´†ÁöÑÊï∞Â≠óÁ¥¢Âºï„ÄÇËØ∑Âú®ÈÄÇÂΩìÁöÑÊÉÖÂÜµ‰∏ãÂú®Âè•Â≠êÊú´Â∞æÂºïÁî®‰∏ä‰∏ãÊñá„ÄÇËØ∑ÊåâÁÖßÂºïÁî®ÁºñÂè∑[citation:X]ÁöÑÊ†ºÂºèÂú®Á≠îÊ°à‰∏≠ÂØπÂ∫îÈÉ®ÂàÜÂºïÁî®‰∏ä‰∏ãÊñá„ÄÇÂ¶ÇÊûú‰∏ÄÂè•ËØùÊ∫êËá™Â§ö‰∏™‰∏ä‰∏ãÊñáÔºåËØ∑ÂàóÂá∫ÊâÄÊúâÁõ∏ÂÖ≥ÁöÑÂºïÁî®ÁºñÂè∑Ôºå‰æãÂ¶Ç[citation:3][citation:5]ÔºåÂàáËÆ∞‰∏çË¶ÅÂ∞ÜÂºïÁî®ÈõÜ‰∏≠Âú®ÊúÄÂêéËøîÂõûÂºïÁî®ÁºñÂè∑ÔºåËÄåÊòØÂú®Á≠îÊ°àÂØπÂ∫îÈÉ®ÂàÜÂàóÂá∫„ÄÇ\nÂú®ÂõûÁ≠îÊó∂ÔºåËØ∑Ê≥®ÊÑè‰ª•‰∏ãÂá†ÁÇπÔºö\n- ‰ªäÂ§©ÊòØ{cur_date}„ÄÇ\n- Âπ∂ÈùûÊêúÁ¥¢ÁªìÊûúÁöÑÊâÄÊúâÂÜÖÂÆπÈÉΩ‰∏éÁî®Êà∑ÁöÑÈóÆÈ¢òÂØÜÂàáÁõ∏ÂÖ≥Ôºå‰Ω†ÈúÄË¶ÅÁªìÂêàÈóÆÈ¢òÔºåÂØπÊêúÁ¥¢ÁªìÊûúËøõË°åÁîÑÂà´„ÄÅÁ≠õÈÄâ„ÄÇ\n- ÂØπ‰∫éÂàó‰∏æÁ±ªÁöÑÈóÆÈ¢òÔºàÂ¶ÇÂàó‰∏æÊâÄÊúâËà™Áè≠‰ø°ÊÅØÔºâÔºåÂ∞ΩÈáèÂ∞ÜÁ≠îÊ°àÊéßÂà∂Âú®10‰∏™Ë¶ÅÁÇπ‰ª•ÂÜÖÔºåÂπ∂ÂëäËØâÁî®Êà∑ÂèØ‰ª•Êü•ÁúãÊêúÁ¥¢Êù•Ê∫ê„ÄÅËé∑ÂæóÂÆåÊï¥‰ø°ÊÅØ„ÄÇ‰ºòÂÖàÊèê‰æõ‰ø°ÊÅØÂÆåÊï¥„ÄÅÊúÄÁõ∏ÂÖ≥ÁöÑÂàó‰∏æÈ°πÔºõÂ¶ÇÈùûÂøÖË¶ÅÔºå‰∏çË¶Å‰∏ªÂä®ÂëäËØâÁî®Êà∑ÊêúÁ¥¢ÁªìÊûúÊú™Êèê‰æõÁöÑÂÜÖÂÆπ„ÄÇ\n- ÂØπ‰∫éÂàõ‰ΩúÁ±ªÁöÑÈóÆÈ¢òÔºàÂ¶ÇÂÜôËÆ∫ÊñáÔºâÔºåËØ∑Âä°ÂøÖÂú®Ê≠£ÊñáÁöÑÊÆµËêΩ‰∏≠ÂºïÁî®ÂØπÂ∫îÁöÑÂèÇËÄÉÁºñÂè∑Ôºå‰æãÂ¶Ç[citation:3][citation:5]Ôºå‰∏çËÉΩÂè™Âú®ÊñáÁ´†Êú´Â∞æÂºïÁî®„ÄÇ‰Ω†ÈúÄË¶ÅËß£ËØªÂπ∂Ê¶ÇÊã¨Áî®Êà∑ÁöÑÈ¢òÁõÆË¶ÅÊ±ÇÔºåÈÄâÊã©ÂêàÈÄÇÁöÑÊ†ºÂºèÔºåÂÖÖÂàÜÂà©Áî®ÊêúÁ¥¢ÁªìÊûúÂπ∂ÊäΩÂèñÈáçË¶Å‰ø°ÊÅØÔºåÁîüÊàêÁ¨¶ÂêàÁî®Êà∑Ë¶ÅÊ±Ç„ÄÅÊûÅÂÖ∑ÊÄùÊÉ≥Ê∑±Â∫¶„ÄÅÂØåÊúâÂàõÈÄ†Âäõ‰∏é‰∏ì‰∏öÊÄßÁöÑÁ≠îÊ°à„ÄÇ‰Ω†ÁöÑÂàõ‰ΩúÁØáÂπÖÈúÄË¶ÅÂ∞ΩÂèØËÉΩÂª∂ÈïøÔºåÂØπ‰∫éÊØè‰∏Ä‰∏™Ë¶ÅÁÇπÁöÑËÆ∫Ëø∞Ë¶ÅÊé®ÊµãÁî®Êà∑ÁöÑÊÑèÂõæÔºåÁªôÂá∫Â∞ΩÂèØËÉΩÂ§öËßíÂ∫¶ÁöÑÂõûÁ≠îË¶ÅÁÇπÔºå‰∏îÂä°ÂøÖ‰ø°ÊÅØÈáèÂ§ß„ÄÅËÆ∫Ëø∞ËØ¶Â∞Ω„ÄÇ\n- Â¶ÇÊûúÂõûÁ≠îÂæàÈïøÔºåËØ∑Â∞ΩÈáèÁªìÊûÑÂåñ„ÄÅÂàÜÊÆµËêΩÊÄªÁªì„ÄÇÂ¶ÇÊûúÈúÄË¶ÅÂàÜÁÇπ‰ΩúÁ≠îÔºåÂ∞ΩÈáèÊéßÂà∂Âú®5‰∏™ÁÇπ‰ª•ÂÜÖÔºåÂπ∂ÂêàÂπ∂Áõ∏ÂÖ≥ÁöÑÂÜÖÂÆπ„ÄÇ\n- ÂØπ‰∫éÂÆ¢ËßÇÁ±ªÁöÑÈóÆÁ≠îÔºåÂ¶ÇÊûúÈóÆÈ¢òÁöÑÁ≠îÊ°àÈùûÂ∏∏ÁÆÄÁü≠ÔºåÂèØ‰ª•ÈÄÇÂΩìË°•ÂÖÖ‰∏ÄÂà∞‰∏§Âè•Áõ∏ÂÖ≥‰ø°ÊÅØÔºå‰ª•‰∏∞ÂØåÂÜÖÂÆπ„ÄÇ\n- ‰Ω†ÈúÄË¶ÅÊ†πÊçÆÁî®Êà∑Ë¶ÅÊ±ÇÂíåÂõûÁ≠îÂÜÖÂÆπÈÄâÊã©ÂêàÈÄÇ„ÄÅÁæéËßÇÁöÑÂõûÁ≠îÊ†ºÂºèÔºåÁ°Æ‰øùÂèØËØªÊÄßÂº∫„ÄÇ\n- ‰Ω†ÁöÑÂõûÁ≠îÂ∫îËØ•ÁªºÂêàÂ§ö‰∏™Áõ∏ÂÖ≥ÁΩëÈ°µÊù•ÂõûÁ≠îÔºå‰∏çËÉΩÈáçÂ§çÂºïÁî®‰∏Ä‰∏™ÁΩëÈ°µ„ÄÇ\n- Èô§ÈùûÁî®Êà∑Ë¶ÅÊ±ÇÔºåÂê¶Âàô‰Ω†ÂõûÁ≠îÁöÑËØ≠Ë®ÄÈúÄË¶ÅÂíåÁî®Êà∑ÊèêÈóÆÁöÑËØ≠Ë®Ä‰øùÊåÅ‰∏ÄËá¥„ÄÇ\n# Áî®Êà∑Ê∂àÊÅØ‰∏∫Ôºö\n{question}''''''\n```\nFor English query, we use the prompt:\n```\nsearch_answer_en_template = \\n''''''# The following contents are the search results related to the user''s message:\n{search_results}\nIn the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.\nWhen responding, please keep the following points in mind:\n- Today is {cur_date}.\n- Not all content in the search results is closely related to the user''s question. You need to evaluate and filter the search results based on the question.\n- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.\n- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user''s requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.\n- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.\n- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.\n- Choose an appropriate and visually appealing format for your response based on the user''s requirements and the content of the answer, ensuring strong readability.\n- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.\n- Unless the user requests otherwise, your response should be in the same language as the user''s question.\n# The user''s message is:\n{question}''''''\n```\n\n## 5. License\nThis code repository is licensed under [MIT License](LICENSE). The use of DeepSeek-R1 models is also subject to [MIT License](LICENSE). DeepSeek-R1 series (including Base and Chat) supports commercial use and distillation.\n\n## 6. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n```\n\n## 7. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":8190735360,"storage_bytes":16381839296,"files_count":10,"spaces_count":53,"gated":false,"private":false,"config":{"architectures":["Qwen3ForCausalLM"],"model_type":"qwen3","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\n\n'' + message[''content''] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{% set content = message[''content''] %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{''<ÔΩúUserÔΩú>'' + content + ''<ÔΩúAssistantÔΩú>''}}{%- endif %}{%- if message[''role''] == ''assistant'' %}{% if ''</think>'' in content %}{% set content = content.split(''</think>'')[-1] %}{% endif %}{% endif %}{%- if message[''role''] == ''assistant'' and message[''tool_calls''] is defined and message[''tool_calls''] is not none %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{%- endif %}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- set ns.is_output_first = true %}{%- for tool in message[''tool_calls''] %}{%- if not ns.is_first %}{%- if content is none %}{{''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- else %}{{content + ''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{''\n'' + ''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- endfor %}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and (message[''tool_calls''] is not defined or message[''tool_calls''] is none)%}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'' + content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{{content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + content + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + content + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{% endif %}{% if add_generation_prompt and not ns.is_last_user and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú>''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"based_on_paper","target_id":"arxiv:2501.12948","source_url":"https://arxiv.org/abs/2501.12948"}]', NULL, 'MIT', 'approved', 100, '62a460061dad9fa94147871981de7400', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B/resolve/main/figures/benchmark.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-R1-0528-Qwen3-8B from https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B/resolve/main/figures/benchmark.png
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-R1-0528-Qwen3-8B.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tencent-HunyuanImage-3.0', 'huggingface--tencent--hunyuanimage-3.0', 'HunyuanImage-3.0', 'tencent', '--- license: other license_name: tencent-hunyuan-community license_link: LICENSE pipeline_tag: text-to-image library_name: transformers --- <div align="center"> <img src="./assets/logo.png" alt="HunyuanImage-3.0 Logo" width="600"> </div> <div align="center"> <img src="./assets/banner.png" alt="HunyuanImage-3.0 Banner" width="800"> </div> <div align="center"> <a href=https://hunyuan.tencent.com/image target="_blank"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage...', '["transformers","safetensors","hunyuan_image_3_moe","text-generation","text-to-image","custom_code","arxiv:2509.23951","license:other","region:us"]', 'text-to-image', 992, 56056, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tencent/HunyuanImage-3.0","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: tencent-hunyuan-community\nlicense_link: LICENSE\npipeline_tag: text-to-image\nlibrary_name: transformers\n---\n\n<div align="center">\n\n<img src="./assets/logo.png" alt="HunyuanImage-3.0 Logo" width="600">\n\n# üé® HunyuanImage-3.0: A Powerful Native Multimodal Model for Image Generation\n\n</div>\n\n\n<div align="center">\n<img src="./assets/banner.png" alt="HunyuanImage-3.0 Banner" width="800">\n\n</div>\n\n<div align="center">\n  <a href=https://hunyuan.tencent.com/image target="_blank"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanImage-3.0 target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanImage-3.0 target="_blank"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href=https://arxiv.org/pdf/2509.23951 target="_blank"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target="_blank"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href=https://docs.qq.com/doc/DUVVadmhCdG9qRXBU target="_blank"><img src=https://img.shields.io/badge/üìö-PromptHandBook-blue.svg?logo=book height=22px></a>\n</div>\n\n\n<p align="center">\n    üëè Join our <a href="./assets/WECHAT.md" target="_blank">WeChat</a> and <a href="https://discord.gg/ehjWMqF5wY">Discord</a> | \nüíª <a href="https://hunyuan.tencent.com/modelSquare/home/play?modelId=289&from=/visual">Official website(ÂÆòÁΩë) Try our model!</a>&nbsp&nbsp\n</p>\n\n## üî•üî•üî• News\n- **September 28, 2025**: üìñ **HunyuanImage-3.0 Technical Report Released** - Comprehensive technical documentation now available\n- **September 28, 2025**: üöÄ **HunyuanImage-3.0 Open Source Release** - Inference code and model weights publicly available\n\n\n## üß© Community Contributions\n\nIf you develop/use HunyuanImage-3.0 in your projects, welcome to let us know.\n\n## üìë Open-source Plan\n\n- HunyuanImage-3.0 (Image Generation Model)\n  - [x] Inference \n  - [x] HunyuanImage-3.0 Checkpoints\n  - [ ] HunyuanImage-3.0-Instruct Checkpoints (with reasoning)\n  - [ ] VLLM Support\n  - [ ] Distilled Checkpoints\n  - [ ] Image-to-Image Generation\n  - [ ] Multi-turn Interaction\n\n\n## üóÇÔ∏è Contents\n- [üî•üî•üî• News](#-news)\n- [üß© Community Contributions](#-community-contributions)\n- [üìë Open-source Plan](#-open-source-plan)\n- [üìñ Introduction](#-introduction)\n- [‚ú® Key Features](#-key-features)\n- [üõ†Ô∏è Dependencies and Installation](#-dependencies-and-installation)\n  - [üíª System Requirements](#-system-requirements)\n  - [üì¶ Environment Setup](#-environment-setup)\n  - [üì• Install Dependencies](#-install-dependencies)\n  - [Performance Optimizations](#performance-optimizations)\n- [üöÄ Usage](#-usage)\n  - [üî• Quick Start with Transformers](#-quick-start-with-transformers)\n  - [üè† Local Installation & Usage](#-local-installation--usage)\n  - [üé® Interactive Gradio Demo](#-interactive-gradio-demo)\n- [üß± Models Cards](#-models-cards)\n- [üìù Prompt Guide](#-prompt-guide)\n  - [Manually Writing Prompts](#manually-writing-prompts)\n  - [System Prompt For Automatic Rewriting the Prompt](#system-prompt-for-automatic-rewriting-the-prompt)\n  - [Advanced Tips](#advanced-tips)\n  - [More Cases](#more-cases)\n- [üìä Evaluation](#-evaluation)\n- [üìö Citation](#-citation)\n- [üôè Acknowledgements](#-acknowledgements)\n- [üåüüöÄ  Github Star History](#-github-star-history)\n\n---\n\n## üìñ Introduction\n\n**HunyuanImage-3.0** is a groundbreaking native multimodal model that unifies multimodal understanding and generation within an autoregressive framework. Our text-to-image module achieves performance **comparable to or surpassing** leading closed-source models.\n\n\n<div align="center">\n  <img src="./assets/framework.png" alt="HunyuanImage-3.0 Framework" width="90%">\n</div>\n\n## ‚ú® Key Features\n\n* üß† **Unified Multimodal Architecture:** Moving beyond the prevalent DiT-based architectures, HunyuanImage-3.0 employs a unified autoregressive framework. This design enables a more direct and integrated modeling of text and image modalities, leading to surprisingly effective and contextually rich image generation.\n\n* üèÜ **The Largest Image Generation MoE Model:** This is the largest open-source image generation Mixture of Experts (MoE) model to date. It features 64 experts and a total of 80 billion parameters, with 13 billion activated per token, significantly enhancing its capacity and performance.\n\n* üé® **Superior Image Generation Performance:** Through rigorous dataset curation and advanced reinforcement learning post-training, we''ve achieved an optimal balance between semantic accuracy and visual excellence. The model demonstrates exceptional prompt adherence while delivering photorealistic imagery with stunning aesthetic quality and fine-grained details.\n\n* üí≠ **Intelligent World-Knowledge Reasoning:** The unified multimodal architecture endows HunyuanImage-3.0 with powerful reasoning capabilities. It leverages its extensive world knowledge to intelligently interpret user intent, automatically elaborating on sparse prompts with contextually appropriate details to produce superior, more complete visual outputs.\n\n\n## üõ†Ô∏è Dependencies and Installation\n\n### üíª System Requirements\n\n* üñ•Ô∏è **Operating System:** Linux\n* üéÆ **GPU:** NVIDIA GPU with CUDA support\n* üíæ **Disk Space:** 170GB for model weights\n* üß† **GPU Memory:** ‚â•3√ó80GB (4√ó80GB recommended for better performance)\n\n### üì¶ Environment Setup\n\n* üêç **Python:** 3.12+ (recommended and tested)\n* üî• **PyTorch:** 2.7.1\n* ‚ö° **CUDA:** 12.8\n\n### üì• Install Dependencies\n\n```bash\n# 1. First install PyTorch (CUDA 12.8 Version)\npip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --index-url https://download.pytorch.org/whl/cu128\n\n# 2. Then install tencentcloud-sdk\npip install -i https://mirrors.tencent.com/pypi/simple/ --upgrade tencentcloud-sdk-python\n\n# 3. Then install other dependencies\npip install -r requirements.txt\n```\n\n#### Performance Optimizations\n\nFor **up to 3x faster inference**, install these optimizations:\n\n```bash\n# FlashAttention for faster attention computation\npip install flash-attn==2.8.3 --no-build-isolation\n\n# FlashInfer for optimized moe inference. v0.3.1 is tested.\npip install flashinfer-python\n```\n> üí°**Installation Tips:** It is critical that the CUDA version used by PyTorch matches the system''s CUDA version. \n> FlashInfer relies on this compatibility when compiling kernels at runtime. Pytorch 2.7.1+cu128 is tested.\n> GCC version >=9 is recommended for compiling FlashAttention and FlashInfer.\n\n> ‚ö° **Performance Tips:** These optimizations can significantly speed up your inference!\n\n> üí°**Notation:** When FlashInfer is enabled, the first inference may be slower (about 10 minutes) due to kernel compilation. Subsequent inferences on the same machine will be much faster.\n\n## üöÄ Usage\n\n### üî• Quick Start with Transformers\n\n#### 1Ô∏è‚É£ Download model weights\n\n```bash\n# Download from HuggingFace and rename the directory.\n# Notice that the directory name should not contain dots, which may cause issues when loading using Transformers.\nhf download tencent/HunyuanImage-3.0 --local-dir ./HunyuanImage-3\n```\n\n#### 2Ô∏è‚É£ Run with Transformers\n\n```python\nfrom transformers import AutoModelForCausalLM\n\n# Load the model\nmodel_id = "./HunyuanImage-3"\n# Currently we can not load the model using HF model_id `tencent/HunyuanImage-3.0` directly \n# due to the dot in the name.\n\nkwargs = dict(\n    attn_implementation="sdpa",     # Use "flash_attention_2" if FlashAttention is installed\n    trust_remote_code=True,\n    torch_dtype="auto",\n    device_map="auto",\n    moe_impl="eager",   # Use "flashinfer" if FlashInfer is installed\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\nmodel.load_tokenizer(model_id)\n\n# generate the image\nprompt = "A brown and white dog is running on the grass"\nimage = model.generate_image(prompt=prompt, stream=True)\nimage.save("image.png")\n```\n\n### üè† Local Installation & Usage\n\n#### 1Ô∏è‚É£ Clone the Repository\n\n```bash\ngit clone https://github.com/Tencent-Hunyuan/HunyuanImage-3.0.git\ncd HunyuanImage-3.0/\n```\n\n#### 2Ô∏è‚É£ Download Model Weights\n\n```bash\n# Download from HuggingFace\nhf download tencent/HunyuanImage-3.0 --local-dir ./HunyuanImage-3\n```\n\n#### 3Ô∏è‚É£ Run the Demo\nThe Pretrain Checkpoint does not automatically rewrite or enhance input prompts, for optimal results currently, we recommend community partners to use deepseek to rewrite the prompts. You can go to [Tencent Cloud](https://cloud.tencent.com/document/product/1772/115963#.E5.BF.AB.E9.80.9F.E6.8E.A5.E5.85.A5) to apply for an API Key.\n\n```bash\n# set env\nexport DEEPSEEK_KEY_ID="your_deepseek_key_id"\nexport DEEPSEEK_KEY_SECRET="your_deepseek_key_secret"\n\npython3 run_image_gen.py --model-id ./HunyuanImage-3 --verbose 1 --sys-deepseek-prompt "universal" --prompt "A brown and white dog is running on the grass"\n```\n\n#### 4Ô∏è‚É£ Command Line Arguments\n\n| Arguments               | Description                                                  | Default     |\n| ----------------------- | ------------------------------------------------------------ | ----------- |\n| `--prompt`              | Input prompt                                                 | (Required)  |\n| `--model-id`            | Model path                                                   | (Required)  |\n| `--attn-impl`           | Attention implementation. Either `sdpa` or `flash_attention_2`. | `sdpa`      |\n| `--moe-impl`            | MoE implementation. Either `eager` or `flashinfer`           | `eager`     |\n| `--seed`                | Random seed for image generation                             | `None`      |\n| `--diff-infer-steps`    | Diffusion infer steps                                        | `50`        |\n| `--image-size`          | Image resolution. Can be `auto`, like `1280x768` or `16:9`   | `auto`      |\n| `--save`                | Image save path.                                             | `image.png` |\n| `--verbose`             | Verbose level. 0: No log; 1: log inference information.      | `0`         |\n| `--rewrite`             | Whether to enable rewriting                                  | `1`         |\n| `--sys-deepseek-prompt` | Select sys-prompt from `universal` or `text_rendering`       | `universal` |\n\n### üé® Interactive Gradio Demo\n\nLaunch an interactive web interface for easy text-to-image generation.\n\n#### 1Ô∏è‚É£ Install Gradio\n\n```bash\npip install gradio>=4.21.0\n```\n\n#### 2Ô∏è‚É£ Configure Environment\n\n```bash\n# Set your model path\nexport MODEL_ID="path/to/your/model"\n\n# Optional: Configure GPU usage (default: 0,1,2,3)\nexport GPUS="0,1,2,3"\n\n# Optional: Configure host and port (default: 0.0.0.0:443)\nexport HOST="0.0.0.0"\nexport PORT="443"\n```\n\n#### 3Ô∏è‚É£ Launch the Web Interface\n\n**Basic Launch:**\n```bash\nsh run_app.sh\n```\n\n**With Performance Optimizations:**\n```bash\n# Use both optimizations for maximum performance\nsh run_app.sh --moe-impl flashinfer --attn-impl flash_attention_2\n```\n\n#### 4Ô∏è‚É£ Access the Interface\n\n> üåê **Web Interface:** Open your browser and navigate to `http://localhost:443` (or your configured port)\n\n\n## üß± Models Cards\n\n| Model                     | Params | Download | Recommended VRAM | Supported |\n|---------------------------| --- | --- | --- | --- |\n| HunyuanImage-3.0          | 80B total (13B active) | [HuggingFace](https://huggingface.co/tencent/HunyuanImage-3.0) | ‚â• 3 √ó 80 GB | ‚úÖ Text-to-Image\n| HunyuanImage-3.0-Instruct | 80B total (13B active) | [HuggingFace](https://huggingface.co/tencent/HunyuanImage-3.0-Instruct) | ‚â• 3 √ó 80 GB | ‚úÖ Text-to-Image<br>‚úÖ Prompt Self-Rewrite <br>‚úÖ CoT Think\n\n\n\nNotes:\n- Install performance extras (FlashAttention, FlashInfer) for faster inference.\n- Multi‚ÄëGPU inference is recommended for the Base model.\n\n\n## üìù Prompt Guide\n\n### Manually Writing Prompts.\nThe Pretrain Checkpoint does not automatically rewrite or enhance input prompts, Instruct Checkpoint can rewrite or enhance input prompts with thinking . For optimal results currently, we recommend community partners consulting our official guide on how to write effective prompts.\n\nReference: [HunyuanImage 3.0 Prompt Handbook](\nhttps://docs.qq.com/doc/DUVVadmhCdG9qRXBU)\n\n\n### System Prompt For Automatic Rewriting the Prompt.\n\nWe''ve included two system prompts in the PE folder of this repository that leverage DeepSeek to automatically enhance user inputs:\n\n* **system_prompt_universal**: This system prompt converts photographic style, artistic prompts into a detailed one.\n* **system_prompt_text_rendering**: This system prompt converts UI/Poster/Text Rending prompts to a deailed on that suits the model.\n\nNote that these system prompts are in Chinese because Deepseek works better with Chinese system prompts. If you want to use it for English oriented model, you may translate it into English or refer to the comments in the PE file as a guide.\n\nWe also create a [Yuanqi workflow](https://yuanqi.tencent.com/agent/H69VgtJdj3Dz) to implement the universal one, you can directly try it.\n\n### Advanced Tips\n- **Content Priority**: Focus on describing the main subject and action first, followed by details about the environment and style. A more general description framework is: **Main subject and scene + Image quality and style + Composition and perspective + Lighting and atmosphere + Technical parameters**. Keywords can be added both before and after this structure.\n\n- **Image resolution**: Our model not only supports multiple resolutions but also offers both **automatic and specified resolution** options. In auto mode, the model automatically predicts the image resolution based on the input prompt. In specified mode (like traditional DiT), the model outputs an image resolution that strictly aligns with the user''s chosen resolution.\n\n### More Cases\nOur model can follow complex instructions to generate high‚Äëquality, creative images.\n\n<div align="center">\n  <img src="./assets/banner_all.jpg" width=100% alt="HunyuanImage 3.0 Demo">\n</div>\n\nOur model can effectively process very long text inputs, enabling users to precisely control the finer details of generated images. Extended prompts allow for intricate elements to be accurately captured, making it ideal for complex projects requiring precision and creativity.\n\n<p align="center">\n<table>\n<thead>\n</thead>\n<tbody>\n<tr>\n<td>\n<img src="./assets/pg_imgs/image1.png" width=100%><details>\n<summary>Show prompt</summary>\nA cinematic medium shot captures a single Asian woman seated on a chair within a dimly lit room, creating an intimate and theatrical atmosphere. The composition is focused on the subject, rendered with rich colors and intricate textures that evoke a nostalgic and moody feeling.\n\nThe primary subject is a young Asian woman with a thoughtful and expressive countenance, her gaze directed slightly away from the camera. She is seated in a relaxed yet elegant posture on an ornate, vintage armchair. The chair is upholstered in a deep red velvet, its fabric showing detailed, intricate textures and slight signs of wear. She wears a simple, elegant dress in a dark teal hue, the material catching the light in a way that reveals its fine-woven texture. Her skin has a soft, matte quality, and the light delicately models the contours of her face and arms.\n\nThe surrounding room is characterized by its vintage decor, which contributes to the historic and evocative mood. In the immediate background, partially blurred due to a shallow depth of field consistent with a f/2.8 aperture, the wall is covered with wallpaper featuring a subtle, damask pattern. The overall color palette is a carefully balanced interplay of deep teal and rich red hues, creating a visually compelling and cohesive environment. The entire scene is detailed, from the fibers of the upholstery to the subtle patterns on the wall.\n\nThe lighting is highly dramatic and artistic, defined by high contrast and pronounced shadow play. A single key light source, positioned off-camera, projects gobo lighting patterns onto the scene, casting intricate shapes of light and shadow across the woman and the back wall. These dramatic shadows create a strong sense of depth and a theatrical quality. While some shadows are deep and defined, others remain soft, gently wrapping around the subject and preventing the loss of detail in darker areas. The soft focus on the background enhances the intimate feeling, drawing all attention to the expressive subject. The overall image presents a cinematic, photorealistic photography style.\n</details>\n</td>\n<td><img src="./assets/pg_imgs/image2.png" width=100%><details>\n<summary>Show prompt</summary>\nA cinematic, photorealistic medium shot captures a high-contrast urban street corner, defined by the sharp intersection of light and shadow. The primary subject is the exterior corner of a building, rendered in a low-saturation, realistic style.\n\nThe building wall, which occupies the majority of the frame, is painted a warm orange with a finely detailed, rough stucco texture. Horizontal white stripes run across its surface. The base of the building is constructed from large, rough-hewn stone blocks, showing visible particles and texture. On the left, illuminated side of the building, there is a single window with closed, dark-colored shutters. Adjacent to the window, a simple black pendant lamp hangs from a thin, taut rope, casting a distinct, sharp-edged shadow onto the sunlit orange wall. The composition is split diagonally, with the right side of the building enveloped in a deep brown shadow. At the bottom of the frame, a smooth concrete sidewalk is visible, upon which the dynamic silhouette of a person is captured mid-stride, walking from right to left.\n\nIn the shallow background, the faint, out-of-focus outlines of another building and the bare, skeletal branches of trees are softly visible, contributing to the quiet urban atmosphere and adding a sense of depth to the scene. These elements are rendered with minimal detail to keep the focus on the foreground architecture.\n\nThe scene is illuminated by strong, natural sunlight originating from the upper left, creating a dramatic chiaroscuro effect. This hard light source casts deep, well-defined shadows, producing a sharp contrast between the brightly lit warm orange surfaces and the deep brown shadow areas. The lighting highlights the fine details in the wall texture and stone particles, emphasizing the photorealistic quality. The overall presentation reflects a high-quality photorealistic photography style, infused with a cinematic film noir aesthetic.\n</details>\n</td>\n</tr>\n<tr>\n<td>\n<img src="./assets/pg_imgs/image3.png" width=100%><details>\n<summary>Show prompt</summary>\n‰∏ÄÂπÖÊûÅÂÖ∑ËßÜËßâÂº†ÂäõÁöÑÊùÇÂøóÂ∞ÅÈù¢È£éÊ†º‰∫∫ÂÉèÁâπÂÜô„ÄÇÁîªÈù¢‰∏ª‰ΩìÊòØ‰∏Ä‰∏™Ë∫´ÁùÄÂè§È£éÊ±âÊúçÁöÑ‰∫∫Áâ©ÔºåÊûÑÂõæÈááÁî®‰∫Ü‰ªéËÇ©ÈÉ®‰ª•‰∏äÁöÑË∂ÖÁ∫ßËøëË∑ùÁ¶ªÁâπÂÜôÔºå‰∫∫Áâ©Âç†ÊçÆ‰∫ÜÁîªÈù¢ÁöÑÁªùÂ§ßÈÉ®ÂàÜÔºåÂΩ¢Êàê‰∫ÜÂº∫ÁÉàÁöÑËßÜËßâÂÜ≤ÂáªÂäõ„ÄÇ\n\nÁîªÈù¢‰∏≠ÁöÑ‰∫∫Áâ©‰ª•‰∏ÄÁßçÊÖµÊáíÁöÑÂßøÊÄÅÂá∫Áé∞ÔºåÂæÆÂæÆÂÄæÊñúÁùÄÂ§¥ÈÉ®ÔºåË£∏Èú≤ÁöÑ‰∏Ä‰æßËÇ©ËÜÄÁ∫øÊù°ÊµÅÁïÖ„ÄÇÂ•πÊ≠£Áî®‰∏ÄÁßçÂ¶©Â™öËÄåÁõ¥Êé•ÁöÑÁúºÁ•ûÂáùËßÜÁùÄÈïúÂ§¥ÔºåÂèåÁúºÂæÆÂº†ÔºåÁúºÁ•ûÊ∑±ÈÇÉÔºå‰º†ÈÄíÂá∫‰∏ÄÁßçÁ•ûÁßòËÄåÂãæ‰∫∫ÁöÑÊ∞îË¥®„ÄÇ‰∫∫Áâ©ÁöÑÈù¢ÈÉ®ÁâπÂæÅÁ≤æËá¥ÔºåÁöÆËÇ§Ë¥®ÊÑüÁªÜËÖªÔºåÂú®ÁâπÂÆöÁöÑÂÖâÁ∫ø‰∏ãÔºåÈù¢ÈÉ®ËΩÆÂªìÊ∏ÖÊô∞ÂàÜÊòéÔºåÂ±ïÁé∞Âá∫‰∏ÄÁßçÂè§ÂÖ∏‰∏éÁé∞‰ª£ËûçÂêàÁöÑÊó∂Â∞öÁæéÊÑü„ÄÇ\n\nÊï¥‰∏™ÁîªÈù¢ÁöÑËÉåÊôØË¢´ËÆæÂÆö‰∏∫‰∏ÄÁßçÁÆÄÁ∫¶ËÄåÈ´òÁ∫ßÁöÑÁ∫ØÁ∫¢Ëâ≤„ÄÇËøôÁßçÁ∫¢Ëâ≤Ëâ≤Ë∞ÉÊ∑±Ê≤âÔºåÂëàÁé∞Âá∫ÂìëÂÖâË¥®ÊÑüÔºåÊó¢Á∫ØÁ≤πÂèàÊó†‰ªª‰ΩïÊùÇË¥®Ôºå‰∏∫Êï¥‰∏™ÊöóÈªëÁ•ûÁßòÁöÑÊ∞õÂõ¥Â•†ÂÆö‰∫ÜÊ≤âÁ®≥ËÄåÂØåÊúâÂº†ÂäõÁöÑÂü∫Ë∞É„ÄÇËøô‰∏™Á∫ØËâ≤ÁöÑËÉåÊôØÊúâÊïàÂú∞Á™ÅÂá∫‰∫ÜÂâçÊôØ‰∏≠ÁöÑ‰∫∫Áâ©‰∏ª‰ΩìÔºå‰ΩøÂæóÊâÄÊúâËßÜËßâÁÑ¶ÁÇπÈÉΩÈõÜ‰∏≠Âú®ÂÖ∂Ë∫´‰∏ä„ÄÇ\n\nÂÖâÁ∫øÂíåÊ∞õÂõ¥ÁöÑËê•ÈÄ†ÊòØËøôÂπÖÊùÇÂøóÈ£éÊµ∑Êä•ÁöÑÂÖ≥ÈîÆ„ÄÇ‰∏ÄÊùüÊöóÊ©òËâ≤ÁöÑÊüîÂíåÂÖâÁ∫ø‰Ωú‰∏∫‰∏ªÂÖâÊ∫êÔºå‰ªé‰∫∫Áâ©ÁöÑ‰∏Ä‰æßÊñú‰∏äÊñπÊäïÂ∞Ñ‰∏ãÊù•ÔºåÁ≤æÂáÜÂú∞ÂãæÂãíÂá∫‰∫∫Áâ©ÁöÑËÑ∏È¢ä„ÄÅÈºªÊ¢ÅÂíåËÇ©ËÜÄÁöÑËΩÆÂªìÔºåÂú®ÁöÆËÇ§‰∏äÂΩ¢ÊàêÂæÆÂ¶ôÁöÑÂÖâÂΩ±ËøáÊ∏°„ÄÇÂêåÊó∂Ôºå‰∫∫Áâ©ÁöÑÂë®Ë∫´Ëê¶ÁªïÁùÄ‰∏ÄÂ±ÇÊöóÊ∑°‰∏î‰ΩéÈ•±ÂíåÂ∫¶ÁöÑÈì∂ÁôΩËâ≤ËæâÂÖâÔºåÂ¶ÇÂêåÊ∏ÖÂÜ∑ÁöÑÊúàÂÖâÔºåÂΩ¢Êàê‰∏ÄÈÅìÊú¶ËÉßÁöÑËΩÆÂªìÂÖâ„ÄÇËøôÈÅìÈì∂Ëæâ‰∏∫‰∫∫Áâ©Â¢ûÊ∑ª‰∫ÜÂá†ÂàÜÁñèÁ¶ªÁöÑÂπΩÁÅµÊÑüÔºåÂº∫Âåñ‰∫ÜÊï¥‰ΩìÊöóÈªëÈ£éÊ†ºÁöÑÁ•ûÁßòÊ∞îË¥®„ÄÇÂÖâÂΩ±ÁöÑÂº∫ÁÉàÂØπÊØî‰∏éËâ≤ÂΩ©ÁöÑÁã¨ÁâπÊê≠ÈÖçÔºåÂÖ±ÂêåÂ°ëÈÄ†‰∫ÜËøôÂº†ÂÖÖÊª°ÊïÖ‰∫ãÊÑüÁöÑÁâπÂÜôÁîªÈù¢„ÄÇÊï¥‰ΩìÂõæÂÉèÂëàÁé∞Âá∫‰∏ÄÁßçËûçÂêà‰∫ÜÂè§ÂÖ∏ÂÖÉÁ¥†ÁöÑÁé∞‰ª£Êó∂Â∞öÊëÑÂΩ±È£éÊ†º„ÄÇ\n</details>\n</td>\n<td>\n<img src="./assets/pg_imgs/image4.png" width=100%><details>\n<summary>Show prompt</summary>\n‰∏ÄÂπÖÈááÁî®ÊûÅÁÆÄ‰øØËßÜËßÜËßíÁöÑÊ≤πÁîª‰ΩúÂìÅÔºåÁîªÈù¢‰∏ª‰ΩìÁî±‰∏ÄÈÅìÂ±Ö‰∏≠ÊñúÂêëÁöÑÁ∫¢Ëâ≤Á¨îËß¶ÊûÑÊàê„ÄÇ\n\nËøôÈÅìÈÜíÁõÆÁöÑÁ∫¢Ëâ≤Á¨îËß¶ËøêÁî®‰∫ÜÂéöÊ∂ÇÊäÄÊ≥ïÔºåÈ¢úÊñôÂ†ÜÂè†ÂΩ¢Êàê‰∫ÜÂº∫ÁÉàÁöÑÁâ©ÁêÜÂéöÂ∫¶Âíå‰∏âÁª¥Á´ã‰ΩìÊÑü„ÄÇÂÆÉ‰ªéÁîªÈù¢ÁöÑÂ∑¶‰∏äËßíÈôÑËøëÂª∂‰º∏Ëá≥Âè≥‰∏ãËßíÈôÑËøëÔºåÊûÑÊàê‰∏Ä‰∏™Âä®ÊÄÅÁöÑÂØπËßíÁ∫ø„ÄÇÈ¢úÊñôË°®Èù¢ÂèØ‰ª•Ê∏ÖÊô∞Âú∞ÁúãÂà∞ÁîªÂàÄÂàÆÊì¶ÂíåÁ¨îÂà∑ÊãñÊõ≥Áïô‰∏ãÁöÑÁóïËøπÔºåËæπÁºòÂ§ÑÁöÑÈ¢úÊñôÂ±ÇÁõ∏ÂØπËæÉËñÑÔºåËÄå‰∏≠Â§ÆÈÉ®ÂàÜÂàôÈ´òÈ´òÈöÜËµ∑ÔºåÂΩ¢Êàê‰∫Ü‰∏çËßÑÂàôÁöÑËµ∑‰ºè„ÄÇ\n\nÂú®ËøôÈÅìÁ´ã‰ΩìÁöÑÁ∫¢Ëâ≤È¢úÊñô‰πã‰∏äÔºåÂ∑ßÂ¶ôÂú∞ÊûÑÂª∫‰∫Ü‰∏ÄÂ§ÑÁ≤æËá¥ÁöÑÂæÆÁº©ÊôØËßÇ„ÄÇÊôØËßÇÁöÑÊ†∏ÂøÉÊòØ‰∏ÄÁâáÊ®°ÊãüÁ∫¢Êµ∑Êª©ÁöÑÂå∫ÂüüÔºåÁî±ÁªÜËÖªÁöÑÊ∑±Á∫¢Ëâ≤È¢úÊñôÁÇπÁºÄËÄåÊàêÔºå‰∏é‰∏ãÊñπÂü∫Â∫ïÁöÑÈ≤úÁ∫¢Ëâ≤ÂΩ¢Êàê‰∏∞ÂØåÁöÑÂ±ÇÊ¨°ÂØπÊØî„ÄÇÁ¥ßÈÇªÁùÄ‚ÄúÁ∫¢Êµ∑Êª©‚ÄùÁöÑÊòØ‰∏ÄÂ∞èÁâáÊπñÊ≥äÔºåÁî±‰∏ÄÂ±ÇÂπ≥Êªë‰∏îÂ∏¶ÊúâÂÖâÊ≥ΩÁöÑËìùËâ≤‰∏éÁôΩËâ≤Ê∑∑ÂêàÈ¢úÊñôÊûÑÊàêÔºåË¥®ÊÑüÂ¶ÇÂêåÂπ≥ÈùôÊó†Ê≥¢ÁöÑÊ∞¥Èù¢„ÄÇÊπñÊ≥äËæπÁºòÔºå‰∏ÄÂ∞èÊíÆËä¶Ëãá‰∏õÁîüÔºåÁî±Âá†Ê†πÁ∫§ÁªÜÊå∫ÊãîÁöÑ„ÄÅÁî®Ê∑°ÈªÑËâ≤ÂíåÊ£ïËâ≤È¢úÊñôÂãæÂãíÂá∫ÁöÑÁ∫øÊù°Êù•Ë°®Áé∞„ÄÇ‰∏ÄÂè™Â∞èÂ∑ßÁöÑÁôΩÈπ≠Á´ã‰∫éËä¶ËãáÊóÅÔºåÂÖ∂ÂΩ¢ÊÄÅÁî±‰∏ÄÂ∞èÂùóÁ∫ØÁôΩËâ≤ÁöÑÂéöÊ∂ÇÈ¢úÊñôÂ°ëÈÄ†Ôºå‰ªÖÁî®‰∏ÄÊäπÁ≤æÁÇºÁöÑÈªëËâ≤È¢úÊñôÁÇπÂá∫ÂÖ∂Â∞ñÂñôÔºåÂßøÊÄÅ‰ºòÈõÖÂÆÅÈùô„ÄÇ\n\nÊï¥‰∏™ÊûÑÂõæÁöÑËÉåÊôØÊòØÂ§ßÈù¢ÁßØÁöÑÁïôÁôΩÔºåÂëàÁé∞‰∏∫‰∏ÄÂº†Â∏¶ÊúâÁªÜÂæÆÂáπÂá∏Á∫πÁêÜÁöÑÁôΩËâ≤Á∫∏Ë¥®Âü∫Â∫ïÔºåËøôÁßçÊûÅÁÆÄÂ§ÑÁêÜÊûÅÂ§ßÂú∞Á™ÅÂá∫‰∫Ü‰∏≠Â§ÆÁöÑÁ∫¢Ëâ≤Á¨îËß¶ÂèäÂÖ∂‰∏äÁöÑÂæÆÁº©ÊôØËßÇ„ÄÇ\n\nÂÖâÁ∫ø‰ªéÁîªÈù¢‰∏Ä‰æßÊüîÂíåÂú∞ÁÖßÂ∞Ñ‰∏ãÊù•ÔºåÂú®ÂéöÊ∂ÇÁöÑÈ¢úÊñôÂ†ÜÂè†Â§ÑÊäï‰∏ãÊ∑°Ê∑°ÁöÑ„ÄÅËΩÆÂªìÂàÜÊòéÁöÑÈò¥ÂΩ±ÔºåËøõ‰∏ÄÊ≠•Â¢ûÂº∫‰∫ÜÁîªÈù¢ÁöÑ‰∏âÁª¥Á´ã‰ΩìÊÑüÂíåÊ≤πÁîªË¥®ÊÑü„ÄÇÊï¥ÂπÖÁîªÈù¢ÂëàÁé∞Âá∫‰∏ÄÁßçÁªìÂêà‰∫ÜÂéöÊ∂ÇÊäÄÊ≥ïÁöÑÁé∞‰ª£ÊûÅÁÆÄ‰∏ª‰πâÊ≤πÁîªÈ£éÊ†º„ÄÇ\n</details>\n</td>\n</tr>\n<tr>\n<td>\n<img src="./assets/pg_imgs/image5.png" width=100%><details>\n<summary>Show prompt</summary>\nÊï¥‰ΩìÁîªÈù¢ÈááÁî®‰∏Ä‰∏™‰∫å‰πò‰∫åÁöÑÂõõÂÆ´Ê†ºÂ∏ÉÂ±ÄÔºå‰ª•‰∫ßÂìÅÂèØËßÜÂåñÁöÑÈ£éÊ†ºÔºåÂ±ïÁ§∫‰∫Ü‰∏ÄÂè™ÂÖîÂ≠êÂú®ÂõõÁßç‰∏çÂêåÊùêË¥®‰∏ãÁöÑÊ∏≤ÊüìÊïàÊûú„ÄÇÊØè‰∏™ÂÆ´Ê†ºÂÜÖÈÉΩÊúâ‰∏ÄÂè™ÂßøÊÄÅÂÆåÂÖ®Áõ∏ÂêåÁöÑÂÖîÂ≠êÊ®°ÂûãÔºåÂÆÉÂëàÂùêÂßøÔºåÂèåËÄ≥Á´ñÁ´ãÔºåÈù¢ÊúùÂâçÊñπ„ÄÇÊâÄÊúâÂÆ´Ê†ºÁöÑËÉåÊôØÂùáÊòØÁªü‰∏ÄÁöÑ‰∏≠ÊÄßÊ∑±ÁÅ∞Ëâ≤ÔºåËøôÁßçÁÆÄÁ∫¶ËÉåÊôØÊó®Âú®ÊúÄÂ§ßÈôêÂ∫¶Âú∞Á™ÅÂá∫ÊØèÁßçÊùêË¥®ÁöÑÁã¨ÁâπË¥®ÊÑü„ÄÇ\n\nÂ∑¶‰∏äËßíÁöÑÂÆ´Ê†º‰∏≠ÔºåÂÖîÂ≠êÊ®°ÂûãÁî±ÂìëÂÖâÁôΩËâ≤Áü≥ËÜèÊùêË¥®ÊûÑÊàê„ÄÇÂÖ∂Ë°®Èù¢Âπ≥Êªë„ÄÅÂùáÂåÄ‰∏îÊó†ÂèçÂ∞ÑÔºåÂú®Ê®°ÂûãÁöÑËÄ≥ÊúµÊ†πÈÉ®„ÄÅÂõõËÇ¢‰∫§Êé•Â§ÑÁ≠âÂáπÈô∑Âå∫ÂüüÂëàÁé∞Âá∫ÊüîÂíåÁöÑÁéØÂ¢ÉÂÖâÈÅÆËîΩÈò¥ÂΩ±ÔºåËøôÁßçÂæÆÂ¶ôÁöÑÈò¥ÂΩ±ÂèòÂåñÂá∏Êòæ‰∫ÜÂÖ∂Á∫ØÁ≤πÁöÑÂá†‰ΩïÂΩ¢ÊÄÅÔºåÊï¥‰ΩìÊÑüËßâÂÉè‰∏Ä‰∏™Áî®‰∫éÁæéÊúØÁ†îÁ©∂ÁöÑÂü∫Á°ÄÊ®°Âûã„ÄÇ\n\nÂè≥‰∏äËßíÁöÑÂÆ´Ê†º‰∏≠ÔºåÂÖîÂ≠êÊ®°ÂûãÁî±Êô∂ËéπÂâîÈÄèÁöÑÊó†ÁëïÁñµÁéªÁíÉÂà∂Êàê„ÄÇÂÆÉÂ±ïÁé∞‰∫ÜÈÄºÁúüÁöÑÁâ©ÁêÜÊäòÂ∞ÑÊïàÊûúÔºåÈÄèËøáÂÖ∂ÈÄèÊòéÁöÑË∫´‰ΩìÁúãÂà∞ÁöÑËÉåÊôØÂëàÁé∞Âá∫ËΩªÂæÆÁöÑÊâ≠Êõ≤„ÄÇÊ∏ÖÊô∞ÁöÑÈïúÈù¢È´òÂÖâÊ≤øÁùÄÂÖ∂Ë∫´‰ΩìÁöÑÊõ≤Á∫øËΩÆÂªìÊµÅÂä®ÔºåË°®Èù¢‰∏äËøòËÉΩÁúãÂà∞ÂæÆÂº±ËÄåÊ∏ÖÊô∞ÁöÑÁéØÂ¢ÉÂèçÂ∞ÑÔºåËµã‰∫àÂÖ∂‰∏ÄÁßçÁ≤æËá¥ËÄåÊòìÁ¢éÁöÑË¥®ÊÑü„ÄÇ\n\nÂ∑¶‰∏ãËßíÁöÑÂÆ´Ê†º‰∏≠ÔºåÂÖîÂ≠êÊ®°ÂûãÂëàÁé∞‰∏∫Â∏¶ÊúâÊãâ‰∏ùÁ∫πÁêÜÁöÑÈíõÈáëÂ±ûÊùêË¥®„ÄÇÈáëÂ±ûË°®Èù¢ÂÖ∑ÊúâÊòéÊòæÁöÑÂêÑÂêëÂºÇÊÄßÂèçÂ∞ÑÊïàÊûúÔºåÂëàÁé∞Âá∫ÂÜ∑Â≥ªÁöÑÁÅ∞Ë∞ÉÈáëÂ±ûÂÖâÊ≥Ω„ÄÇÈîêÂà©Êòé‰∫ÆÁöÑÈ´òÂÖâÂíåÊ∑±ÈÇÉÁöÑÈò¥ÂΩ±ÂΩ¢Êàê‰∫ÜÂº∫ÁÉàÂØπÊØîÔºåÁ≤æÁ°ÆÂú∞ÂÆö‰πâ‰∫ÜÂÖ∂ÂùöÂõ∫ÁöÑ‰∏âÁª¥ÂΩ¢ÊÄÅÔºåÂ±ïÁé∞‰∫ÜÂ∑•‰∏öËÆæËÆ°Ëà¨ÁöÑÁæéÊÑü„ÄÇ\n\nÂè≥‰∏ãËßíÁöÑÂÆ´Ê†º‰∏≠ÔºåÂÖîÂ≠êÊ®°ÂûãË¶ÜÁõñÁùÄ‰∏ÄÂ±ÇÊüîËΩØÊµìÂØÜÁöÑÁÅ∞Ëâ≤ÊØõÁªí„ÄÇÊ†πÊ†πÂàÜÊòéÁöÑÁªíÊØõÊ∏ÖÊô∞ÂèØËßÅÔºåÂàõÈÄ†Âá∫‰∏ÄÁßçÊ∏©Êöñ„ÄÅÂèØËß¶Êë∏ÁöÑË¥®Âú∞„ÄÇÂÖâÁ∫øÁÖßÂ∞ÑÂú®ÁªíÊØõÁöÑÊú´Ê¢¢ÔºåÂΩ¢ÊàêÊüîÂíåÁöÑÂÖâÊôïÊïàÊûúÔºåËÄåÊØõÁªíÂÜÖÈÉ®ÁöÑÈò¥ÂΩ±ÂàôÊòæÂæóÊ∑±ÈÇÉËÄåÊüîËΩØÔºåÂ±ïÁé∞‰∫ÜÈ´òÂ∫¶ÂÜôÂÆûÁöÑÊØõÂèëÊ∏≤ÊüìÊïàÊûú„ÄÇ\n\nÊï¥‰∏™ÂõõÂÆ´Ê†ºÁî±Êù•Ëá™Â§ö‰∏™ÊñπÂêëÁöÑ„ÄÅÊüîÂíåÂùáÂåÄÁöÑÂΩ±Ê£öÁÅØÂÖâÁÖß‰∫ÆÔºåÁ°Æ‰øù‰∫ÜÊØèÁßçÊùêË¥®ÁöÑÁªÜËäÇÂíåÁâπÊÄßÈÉΩÂæóÂà∞Ê∏ÖÊô∞ÁöÑÂ±ïÁé∞ÔºåÊ≤°Êúâ‰ªª‰ΩïÂà∫ÁúºÁöÑÈò¥ÂΩ±ÊàñËøáÊõùÁöÑÈ´òÂÖâ„ÄÇËøôÂº†ÂõæÂÉè‰ª•‰∏ÄÁßçÈ´òÂ∫¶ÂÜôÂÆûÁöÑ3DÊ∏≤ÊüìÈ£éÊ†ºÂëàÁé∞ÔºåÂÆåÁæéÂú∞ËØ†Èáä‰∫Ü‰∫ßÂìÅÂèØËßÜÂåñÁöÑÁ≤æÈ´ì\n</details>\n</td>\n<td>\n<img src="./assets/pg_imgs/image6.png" width=100%><details>\n<summary>Show prompt</summary>\nÁî±‰∏Ä‰∏™‰∏§Ë°å‰∏§ÂàóÁöÑÁΩëÊ†ºÊûÑÊàêÔºåÂÖ±ÂåÖÂê´Âõõ‰∏™Áã¨Á´ãÁöÑÂú∫ÊôØÔºåÊØè‰∏™Âú∫ÊôØÈÉΩ‰ª•‰∏çÂêåÁöÑËâ∫ÊúØÈ£éÊ†ºÊèèÁªò‰∫Ü‰∏Ä‰∏™Â∞èÁî∑Â≠©ÔºàÂ∞èÊòéÔºâ‰∏ÄÂ§©‰∏≠ÁöÑ‰∏çÂêåÊ¥ªÂä®„ÄÇ\n\nÂ∑¶‰∏äËßíÁöÑÁ¨¨‰∏Ä‰∏™Âú∫ÊôØÔºå‰ª•Ë∂ÖÂÜôÂÆûÊëÑÂΩ±È£éÊ†ºÂëàÁé∞„ÄÇÁîªÈù¢‰∏ª‰ΩìÊòØ‰∏Ä‰∏™Â§ßÁ∫¶8Â≤ÅÁöÑ‰∏ú‰∫öÂ∞èÁî∑Â≠©Ôºå‰ªñÁ©øÁùÄÊï¥Ê¥ÅÁöÑÂ∞èÂ≠¶Âà∂Êúç‚Äî‚Äî‰∏Ä‰ª∂ÁôΩËâ≤Áü≠Ë¢ñË°¨Ë°´ÂíåËìùËâ≤Áü≠Ë£§ÔºåËÑñÂ≠ê‰∏äÁ≥ªÁùÄÁ∫¢È¢ÜÂ∑æ„ÄÇ‰ªñËÉåÁùÄ‰∏Ä‰∏™ËìùËâ≤ÁöÑÂèåËÇ©‰π¶ÂåÖÔºåÊ≠£Ëµ∞Âú®Âéª‰∏äÂ≠¶ÁöÑË∑Ø‰∏ä„ÄÇ‰ªñ‰Ωç‰∫éÁîªÈù¢ÁöÑÂâçÊôØÂÅèÂè≥‰æßÔºåÈù¢Â∏¶ÂæÆÁ¨ëÔºåÊ≠•‰ºêËΩªÂø´„ÄÇÂú∫ÊôØËÆæÂÆöÂú®Ê∏ÖÊô®ÔºåÊüîÂíåÁöÑÈò≥ÂÖâ‰ªéÂ∑¶‰∏äÊñπÁÖßÂ∞Ñ‰∏ãÊù•ÔºåÂú®‰∫∫Ë°åÈÅì‰∏äÊäï‰∏ãÊ∏ÖÊô∞ËÄåÊüîÂíåÁöÑÂΩ±Â≠ê„ÄÇËÉåÊôØÊòØÁªøÊ†ëÊàêËç´ÁöÑË°óÈÅìÂíåÊ®°Á≥äÂèØËßÅÁöÑÂ≠¶Ê†°ÈìÅËâ∫Â§ßÈó®ÔºåËê•ÈÄ†Âá∫ÂÆÅÈùôÁöÑÊó©Êô®Ê∞õÂõ¥„ÄÇËøôÂº†ÂõæÁâáÁöÑÁªÜËäÇË°®Áé∞ÊûÅ‰∏∫‰∏∞ÂØåÔºåÂèØ‰ª•Ê∏ÖÊô∞Âú∞ÁúãÂà∞Áî∑Â≠©Â§¥ÂèëÁöÑÂÖâÊ≥Ω„ÄÅË°£ÊúçÁöÑË§∂Áö±Á∫πÁêÜ‰ª•Âèä‰π¶ÂåÖÁöÑÂ∏ÜÂ∏ÉÊùêË¥®ÔºåÂÆåÂÖ®Â±ïÁé∞‰∫Ü‰∏ì‰∏öÊëÑÂΩ±ÁöÑË¥®ÊÑü„ÄÇ\n\nÂè≥‰∏äËßíÁöÑÁ¨¨‰∫å‰∏™Âú∫ÊôØÔºåÈááÁî®Êó•ÂºèËµõÁíêÁíêÂä®Êº´È£éÊ†ºÁªòÂà∂„ÄÇÁîªÈù¢‰∏≠ÔºåÂ∞èÁî∑Â≠©ÂùêÂú®ÂÆ∂‰∏≠ÁöÑÊú®Ë¥®È§êÊ°åÊóÅÂêÉÂçàÈ•≠„ÄÇ‰ªñÁöÑÂΩ¢Ë±°Ë¢´Âä®Êº´ÂåñÔºåÊã•ÊúâÂ§ßËÄåÊòé‰∫ÆÁöÑÁúºÁùõÂíåÁÆÄÊ¥ÅÁöÑ‰∫îÂÆòÁ∫øÊù°„ÄÇ‰ªñË∫´Á©ø‰∏Ä‰ª∂ÁÆÄÂçïÁöÑÈªÑËâ≤TÊÅ§ÔºåÊ≠£Áî®Á≠∑Â≠êÂ§πËµ∑Á¢óÈáåÁöÑÁ±≥È•≠„ÄÇÊ°å‰∏äÊëÜÊîæÁùÄ‰∏ÄÁ¢óÊ±§Âíå‰∏§ÁõòÂÆ∂Â∏∏Ëèú„ÄÇËÉåÊôØÊòØ‰∏Ä‰∏™Ê∏©È¶®ÁöÑÂÆ§ÂÜÖÁéØÂ¢ÉÔºå‰∏ÄÊâáÊòé‰∫ÆÁöÑÁ™óÊà∑ÈÄèËøõÊ≠£ÂçàÁöÑÈò≥ÂÖâÔºåÁ™óÂ§ñÊòØËìùÂ§©ÁôΩ‰∫ë„ÄÇÊï¥‰∏™ÁîªÈù¢Ëâ≤ÂΩ©È≤úËâ≥„ÄÅÈ•±ÂíåÂ∫¶È´òÔºåËßíËâ≤ËΩÆÂªìÁ∫øÊ∏ÖÊô∞ÊòéÁ°ÆÔºåÈò¥ÂΩ±ÈÉ®ÂàÜÈááÁî®Âπ≥Ê∂ÇÁöÑËâ≤ÂùóÂ§ÑÁêÜÔºåÊòØÂÖ∏ÂûãÁöÑËµõÁíêÁíêÂä®Êº´È£éÊ†º„ÄÇ\n\nÂ∑¶‰∏ãËßíÁöÑÁ¨¨‰∏â‰∏™Âú∫ÊôØÔºå‰ª•ÁªÜËÖªÁöÑÈìÖÁ¨îÁ¥†ÊèèÈ£éÊ†ºÂëàÁé∞„ÄÇÁîªÈù¢ÊèèÁªò‰∫Ü‰∏ãÂçàÂú®ÊìçÂú∫‰∏äË∏¢Ë∂≥ÁêÉÁöÑÂ∞èÁî∑Â≠©„ÄÇÊï¥‰∏™ÂõæÂÉèÁî±‰∏çÂêåÁÅ∞Â∫¶ÁöÑÁü≥Â¢®Ëâ≤Ë∞ÉÊûÑÊàêÔºåÊ≤°ÊúâÂÖ∂‰ªñÈ¢úËâ≤„ÄÇÂ∞èÁî∑Â≠©Ë∫´Á©øËøêÂä®Áü≠Ë¢ñÂíåÁü≠Ë£§ÔºåË∫´‰ΩìÂëàÂâçÂÄæÂßøÊÄÅÔºåÂè≥ËÑöÊ≠£Ë¶ÅË∏¢Âêë‰∏Ä‰∏™Ë∂≥ÁêÉÔºåÂä®‰ΩúÂÖÖÊª°Âä®ÊÑü„ÄÇËÉåÊôØÊòØÁ©∫Êó∑ÁöÑÊìçÂú∫ÂíåËøúÂ§ÑÁöÑÁêÉÈó®ÔºåÁî®ÁÆÄÁªÉÁöÑÁ∫øÊù°ÂíåÊéíÁ∫øÂãæÂãí„ÄÇËâ∫ÊúØÂÆ∂ÈÄöËøá‰∫§ÂèâÊéíÁ∫øÂíåÊ∂ÇÊäπÊäÄÂ∑ßÊù•Ë°®Áé∞ÂÖâÂΩ±Âíå‰ΩìÁßØÊÑüÔºåË∂≥ÁêÉ‰∏äÁöÑÈò¥ÂΩ±„ÄÅ‰∫∫Áâ©Ë∫´‰∏äÁöÑËÇåËÇâÁ∫øÊù°‰ª•ÂèäÂú∞Èù¢Á≤óÁ≥ôÁöÑË¥®ÊÑüÈÉΩÈÄöËøáÈìÖÁ¨îÁöÑÁ¨îËß¶ÂæóÂà∞‰∫ÜÂÖÖÂàÜÁöÑÂ±ïÁé∞„ÄÇËøôÂº†ÈìÖÁ¨îÁîªÁ™ÅÂá∫‰∫ÜÁ¥†ÊèèÁöÑÂÖâÂΩ±ÂÖ≥Á≥ªÂíåÁ∫øÊù°ÁæéÊÑü„ÄÇ\n\nÂè≥‰∏ãËßíÁöÑÁ¨¨Âõõ‰∏™Âú∫ÊôØÔºå‰ª•ÊñáÊ£ÆÁâπ¬∑Ê¢µÈ´òÁöÑÂêéÂç∞Ë±°Ê¥æÊ≤πÁîªÈ£éÊ†ºËøõË°åËØ†Èáä„ÄÇÁîªÈù¢ÊèèÁªò‰∫ÜÂ§úÊôöÊó∂ÂàÜÔºåÂ∞èÁî∑Â≠©Áã¨Ëá™Âú®Ê≤≥ËæπÈíìÈ±ºÁöÑÊôØË±°„ÄÇ‰ªñÂùêÂú®‰∏ÄÂùóÂ≤©Áü≥‰∏äÔºåÊâãÊåÅ‰∏ÄÊ†πÁÆÄÊòìÁöÑÈíìÈ±ºÁ´øÔºåË∫´ÂΩ±Âú®Ê∑±ËìùËâ≤ÁöÑÂ§úÂπï‰∏ãÊòæÂæóÂæàÊ∏∫Â∞è„ÄÇÊï¥‰∏™ÁîªÈù¢ÁöÑËßÜËßâÁÑ¶ÁÇπÊòØÂ§©Á©∫ÂíåÊ∞¥Èù¢ÔºåÂ§©Á©∫Â∏ÉÊª°‰∫ÜÊóãËΩ¨„ÄÅÂç∑Êõ≤ÁöÑÊòü‰∫ëÔºåÊòüÊòüÂíåÊúà‰∫ÆË¢´ÊèèÁªòÊàêÂ∑®Â§ß„ÄÅÂèëÂÖâÁöÑÂÖâÂõ¢Ôºå‰ΩøÁî®‰∫ÜÂéöÊ∂ÇÁöÑÊ≤πÁîªÈ¢úÊñôÔºàImpastoÔºâÔºåÁ¨îËß¶Á≤óÁä∑ËÄåÂÖÖÊª°ËÉΩÈáè„ÄÇÊ∑±Ëìù„ÄÅ‰∫ÆÈªÑÂíåÁôΩËâ≤ÁöÑÈ¢úÊñôÂú®ÁîªÂ∏É‰∏äÁõ∏‰∫í‰∫§ÁªáÔºåÂΩ¢ÊàêÂº∫ÁÉàÁöÑËßÜËßâÂÜ≤ÂáªÂäõ„ÄÇÊ∞¥Èù¢ÂÄíÊò†ÁùÄÂ§©Á©∫‰∏≠Êâ≠Êõ≤ÁöÑÂÖâÂΩ±ÔºåÊï¥‰∏™Âú∫ÊôØÂÖÖÊª°‰∫ÜÊ¢µÈ´ò‰ΩúÂìÅ‰∏≠ÁâπÊúâÁöÑÂº∫ÁÉàÊÉÖÊÑüÂíåÂä®Ëç°‰∏çÂÆâÁöÑÁæéÊÑü„ÄÇËøôÂπÖÁîª‰ΩúÊòØÂØπÊ¢µÈ´òÈ£éÊ†ºÁöÑÊ∑±Â∫¶Ëá¥Êï¨„ÄÇ\n</details>\n</td>\n</tr>\n<tr>\n<td>\n<img src="./assets/pg_imgs/image7.png" width=100%><details>\n<summary>Show prompt</summary>\n‰ª•Âπ≥ËßÜËßÜËßíÔºåÂëàÁé∞‰∫Ü‰∏ÄÂπÖÂÖ≥‰∫éÂ¶Ç‰ΩïÁî®Á¥†ÊèèÊäÄÊ≥ïÁªòÂà∂Èπ¶ÈπâÁöÑ‰πùÂÆ´Ê†ºÊïôÂ≠¶Âõæ„ÄÇÊï¥‰ΩìÊûÑÂõæËßÑÊï¥Ôºå‰πù‰∏™Â§ßÂ∞è‰∏ÄËá¥ÁöÑÊñπÂΩ¢ÁîªÊ°Ü‰ª•‰∏âË°å‰∏âÂàóÁöÑÂΩ¢ÂºèÂùáÂåÄÂàÜÂ∏ÉÂú®ÊµÖÁÅ∞Ëâ≤ËÉåÊôØ‰∏äÔºåÊ∏ÖÊô∞Âú∞Â±ïÁ§∫‰∫Ü‰ªéÂü∫Êú¨ÂΩ¢Áä∂Âà∞ÊúÄÁªàÊàêÂìÅÁöÑÂÖ®ËøáÁ®ã„ÄÇ\n\nÁ¨¨‰∏ÄË°å‰ªéÂ∑¶Ëá≥Âè≥Â±ïÁ§∫‰∫ÜÁªòÁîªÁöÑÂàùÂßãÊ≠•È™§„ÄÇÂ∑¶‰∏äËßíÁöÑÁ¨¨‰∏Ä‰∏™ÁîªÊ°Ü‰∏≠ÔºåÁî®ÁÆÄÊ¥ÅÁöÑÈìÖÁ¨îÁ∫øÊù°ÂãæÂãíÂá∫Èπ¶ÈπâÁöÑÂü∫Êú¨Âá†‰ΩïÂΩ¢ÊÄÅÔºö‰∏Ä‰∏™ÂúÜÂΩ¢‰ª£Ë°®Â§¥ÈÉ®Ôºå‰∏Ä‰∏™Á®çÂ§ßÁöÑÊ§≠ÂúÜÂΩ¢‰ª£Ë°®Ë∫´‰Ωì„ÄÇÂè≥‰∏äËßíÊúâ‰∏Ä‰∏™Â∞èÂè∑ÁöÑÊó†Ë°¨Á∫øÂ≠ó‰ΩìÊï∞Â≠ó‚Äú1‚Äù„ÄÇ‰∏≠Èó¥ÁöÑÁ¨¨‰∫å‰∏™ÁîªÊ°Ü‰∏≠ÔºåÂú®Âü∫Á°ÄÂΩ¢ÊÄÅ‰∏äÊ∑ªÂä†‰∫Ü‰∏âËßíÂΩ¢ÁöÑÈ∏üÂñôËΩÆÂªìÂíå‰∏ÄÊù°ÈïøÈïøÁöÑÂºßÁ∫ø‰Ωú‰∏∫Â∞æÂ∑¥ÁöÑÈõèÂΩ¢ÔºåÂ§¥ÈÉ®ÂíåË∫´‰ΩìÁöÑËøûÊé•Â§ÑÁ∫øÊù°ÂèòÂæóÊõ¥Âä†ÊµÅÁïÖÔºõÂè≥‰∏äËßíÊ†áÊúâÊï∞Â≠ó‚Äú2‚Äù„ÄÇÂè≥‰æßÁöÑÁ¨¨‰∏â‰∏™ÁîªÊ°Ü‰∏≠ÔºåËøõ‰∏ÄÊ≠•Á≤æÁ°Æ‰∫ÜÈπ¶ÈπâÁöÑÊï¥‰ΩìËΩÆÂªìÔºåÂãæÂãíÂá∫Â§¥ÈÉ®È°∂Á´ØÁöÑÁæΩÂÜ†ÂíåÊ∏ÖÊô∞ÁöÑÁúºÈÉ®ÂúÜÂΩ¢ËΩÆÂªìÔºõÂè≥‰∏äËßíÊ†áÊúâÊï∞Â≠ó‚Äú3‚Äù„ÄÇ\n\nÁ¨¨‰∫åË°å‰∏ìÊ≥®‰∫éÁªìÊûÑ‰∏éÁªÜËäÇÁöÑÊ∑ªÂä†ÔºåÊèèÁªò‰∫ÜÁªòÁîªÁöÑ‰∏≠ÊúüÈò∂ÊÆµ„ÄÇÂ∑¶‰æßÁöÑÁ¨¨Âõõ‰∏™ÁîªÊ°ÜÈáåÔºåÈπ¶ÈπâÁöÑË∫´‰Ωì‰∏äÊ∑ªÂä†‰∫ÜÁøÖËÜÄÁöÑÂü∫Êú¨ÂΩ¢Áä∂ÔºåÂêåÊó∂Âú®Ë∫´‰Ωì‰∏ãÊñπÁîªÂá∫‰∫Ü‰∏ÄÊ†π‰Ωú‰∏∫Ê†ñÊú®ÁöÑÊ®™ÂêëÊ†ëÊûùÔºåÈπ¶ÈπâÁöÑÁà™Â≠êÂàùÊ≠•Êê≠Âú®Ê†ëÊûù‰∏äÔºõÂè≥‰∏äËßíÊ†áÊúâÊï∞Â≠ó‚Äú4‚Äù„ÄÇ‰∏≠Èó¥ÁöÑÁ¨¨‰∫î‰∏™ÁîªÊ°Ü‰∏≠ÔºåÂºÄÂßãÁªÜÂåñÁøÖËÜÄÂíåÂ∞æÈÉ®ÁöÑÁæΩÊØõÂàÜÁªÑÔºåÁî®Áü≠‰øÉÁöÑÁ∫øÊù°Ë°®Áé∞Âá∫Â±ÇÊ¨°ÊÑüÔºåÂπ∂Ê∏ÖÊô∞Âú∞ÁîªÂá∫Áà™Â≠êÁ¥ßÊè°Ê†ëÊûùÁöÑÁªÜËäÇÔºõÂè≥‰∏äËßíÊ†áÊúâÊï∞Â≠ó‚Äú5‚Äù„ÄÇÂè≥‰æßÁöÑÁ¨¨ÂÖ≠‰∏™ÁîªÊ°ÜÈáåÔºåÂºÄÂßã‰∏∫Èπ¶ÈπâÊ∑ªÂä†ÂàùÊ≠•ÁöÑÈò¥ÂΩ±Ôºå‰ΩøÁî®‰∫§ÂèâÊéíÁ∫øÁöÑÁ¥†ÊèèÊäÄÊ≥ïÂú®ËÖπÈÉ®„ÄÅÁøÖËÜÄ‰∏ãÊñπÂíåÈ¢àÈÉ®Âà∂ÈÄ†Âá∫‰ΩìÁßØÊÑüÔºõÂè≥‰∏äËßíÊ†áÊúâÊï∞Â≠ó‚Äú6‚Äù„ÄÇ\n\nÁ¨¨‰∏âË°åÂàôÂ±ïÁ§∫‰∫ÜÊúÄÁªàÁöÑÊ∂¶Ëâ≤‰∏éÂÆåÊàêÈò∂ÊÆµ„ÄÇÂ∑¶‰∏ãËßíÁöÑÁ¨¨‰∏É‰∏™ÁîªÊ°Ü‰∏≠ÔºåÁ¥†ÊèèÁöÑÊéíÁ∫øÊõ¥Âä†ÂØÜÈõÜÔºåÈò¥ÂΩ±Â±ÇÊ¨°Êõ¥Âä†‰∏∞ÂØåÔºåÁæΩÊØõÁöÑÁ∫πÁêÜÁªÜËäÇË¢´‰ªîÁªÜÂàªÁîªÂá∫Êù•ÔºåÁúºÁè†‰πüÊ∑ªÂä†‰∫ÜÈ´òÂÖâÁÇπÁºÄÔºåÊòæÂæóÁÇØÁÇØÊúâÁ•ûÔºõÂè≥‰∏äËßíÊ†áÊúâÊï∞Â≠ó‚Äú7‚Äù„ÄÇ‰∏≠Èó¥ÁöÑÁ¨¨ÂÖ´‰∏™ÁîªÊ°ÜÈáåÔºåÊèèÁªòÁöÑÈáçÁÇπËΩ¨ÁßªÂà∞Ê†ñÊú®‰∏äÔºåÂ¢ûÂä†‰∫ÜÊ†ëÊûùÁöÑÁ∫πÁêÜÂíåËäÇÁñ§ÁªÜËäÇÔºåÂêåÊó∂Êï¥‰ΩìË∞ÉÊï¥‰∫ÜÈπ¶ÈπâË∫´‰∏äÁöÑÂÖâÂΩ±ÂÖ≥Á≥ªÔºå‰ΩøÁ´ã‰ΩìÊÑüÊõ¥‰∏∫Á™ÅÂá∫ÔºõÂè≥‰∏äËßíÊ†áÊúâÊï∞Â≠ó‚Äú8‚Äù„ÄÇÂè≥‰∏ãËßíÁöÑÁ¨¨‰πù‰∏™ÁîªÊ°ÜÊòØÊúÄÁªàÂÆåÊàêÂõæÔºåÊâÄÊúâÁ∫øÊù°ÈÉΩÁªèËøá‰∫ÜÁ≤æÁÇºÔºåÂÖâÂΩ±ÂØπÊØîÂº∫ÁÉàÔºåÈπ¶ÈπâÁöÑÁæΩÊØõË¥®ÊÑü„ÄÅÊú®Ë¥®Ê†ñÊú®ÁöÑÁ≤óÁ≥ôÊÑüÈÉΩË°®Áé∞ÂæóÊ∑ãÊºìÂ∞ΩËá¥ÔºåÂëàÁé∞Âá∫‰∏ÄÂπÖÂÆåÊï¥‰∏îÁªÜËäÇ‰∏∞ÂØåÁöÑÁ¥†Êèè‰ΩúÂìÅÔºõÂè≥‰∏äËßíÊ†áÊúâÊï∞Â≠ó‚Äú9‚Äù„ÄÇ\n\nÊï¥‰∏™ÁîªÈù¢ÁöÑÂÖâÁ∫øÂùáÂåÄËÄåÊòé‰∫ÆÔºåÊ≤°Êúâ‰ªª‰ΩïÁâπÂÆöÁöÑÂÖâÊ∫êÊñπÂêëÔºåÁ°Æ‰øù‰∫ÜÊØè‰∏™ÊïôÂ≠¶Ê≠•È™§ÁöÑËßÜËßâÊ∏ÖÊô∞Â∫¶„ÄÇÊï¥‰ΩìÂëàÁé∞Âá∫‰∏ÄÁßçÊ∏ÖÊô∞„ÄÅÊúâÊù°ÁêÜÁöÑÊï∞Â≠óÊèíÁîªÊïôÁ®ãÈ£éÊ†º„ÄÇ\n</details>\n</td>\n<td>\n<img src="./assets/pg_imgs/image8.png" width=100%><details>\n<summary>Show prompt</summary>\n‰∏ÄÂº†Áé∞‰ª£Âπ≥Èù¢ËÆæËÆ°È£éÊ†ºÁöÑÊµ∑Êä•Âç†ÊçÆ‰∫ÜÊï¥‰∏™ÁîªÈù¢ÔºåÊûÑÂõæÁÆÄÊ¥Å‰∏î‰∏≠ÂøÉÁ™ÅÂá∫„ÄÇ\n\nÊµ∑Êä•ÁöÑ‰∏ª‰ΩìÊòØ‰Ωç‰∫éÁîªÈù¢Ê≠£‰∏≠Â§ÆÁöÑ‰∏ÄÂè™ËÖæËÆØQQ‰ºÅÈπÖ„ÄÇËøôÂè™‰ºÅÈπÖÈááÁî®‰∫ÜÂúÜÊ∂¶ÂèØÁà±ÁöÑ3DÂç°ÈÄöÊ∏≤ÊüìÈ£éÊ†ºÔºåË∫´‰Ωì‰∏ªË¶Å‰∏∫È•±Êª°ÁöÑÈªëËâ≤ÔºåËÖπÈÉ®‰∏∫Á∫ØÁôΩËâ≤„ÄÇÂÆÉÁöÑÁúºÁùõÂ§ßËÄåÂúÜÔºåÁúºÁ•ûÂ•ΩÂ•áÂú∞Áõ¥ËßÜÂâçÊñπ„ÄÇÈªÑËâ≤ÁöÑÂò¥Â∑¥Â∞èÂ∑ßËÄåÁ´ã‰ΩìÔºåÂèåËÑöÂêåÊ†∑‰∏∫È≤úÊòéÁöÑÈªÑËâ≤ÔºåÁ®≥Á®≥Âú∞Á´ôÁ´ãÁùÄ„ÄÇ‰∏ÄÊù°Ê†áÂøóÊÄßÁöÑÁ∫¢Ëâ≤Âõ¥Â∑æÊï¥ÈΩêÂú∞Á≥ªÂú®ÂÆÉÁöÑËÑñÂ≠ê‰∏äÔºåÂõ¥Â∑æÁöÑÊùêË¥®Â∏¶ÊúâËΩªÂæÆÁöÑÂ∏ÉÊñôË¥®ÊÑüÔºåÊú´Á´ØËá™ÁÑ∂‰∏ãÂûÇ„ÄÇ‰ºÅÈπÖÁöÑÊï¥‰ΩìÈÄ†ÂûãÂπ≤ÂáÄÂà©ËêΩÔºåËæπÁºòÂÖâÊªëÔºåÂëàÁé∞Âá∫‰∏ÄÁßçÁ≤æËá¥ÁöÑÊï∞Â≠óÊèíÁîªË¥®ÊÑü„ÄÇ\n\nÊµ∑Êä•ÁöÑËÉåÊôØÊòØ‰∏ÄÁßç‰ªé‰∏äÂà∞‰∏ãÁî±ÊµÖËìùËâ≤Âπ≥ÊªëËøáÊ∏°Âà∞ÁôΩËâ≤ÁöÑÊüîÂíåÊ∏êÂèòÔºåËê•ÈÄ†Âá∫‰∏ÄÁßçÂºÄÈòî„ÄÅÊòé‰∫ÆÁöÑÁ©∫Èó¥ÊÑü„ÄÇÂú®‰ºÅÈπÖÁöÑË∫´ÂêéÔºåÊï£Â∏ÉÁùÄ‰∏Ä‰∫õÊ∑°Ê∑°ÁöÑ„ÄÅÊ®°Á≥äÁöÑÂúÜÂΩ¢ÂÖâÊñëÂíåÂá†ÈÅìÊüîÂíåÁöÑÊäΩË±°ÂÖâÊùüÔºå‰∏∫Ëøô‰∏™ÁÆÄÁ∫¶ÁöÑÂπ≥Èù¢ËÆæËÆ°Êµ∑Êä•Â¢ûÊ∑ª‰∫ÜÂæÆÂ¶ôÁöÑÊ∑±Â∫¶ÂíåÁßëÊäÄÊÑü„ÄÇ\n\nÁîªÈù¢ÁöÑÂ∫ïÈÉ®Âå∫ÂüüÊòØÊñáÂ≠óÈÉ®ÂàÜÔºåÊéíÁâàÂ±Ö‰∏≠ÂØπÈΩê„ÄÇ‰∏äÂçäÈÉ®ÂàÜÊòØ‰∏ÄË°åÁ®çÂ§ßÁöÑÈªëËâ≤Èªë‰ΩìÂ≠óÔºåÂÜÖÂÆπ‰∏∫‚ÄúHunyuan Image 3.0‚Äù„ÄÇÁ¥ßÈöèÂÖ∂‰∏ãÁöÑÊòØ‰∏ÄË°åÂ≠óÂè∑Áï•Â∞èÁöÑÊ∑±ÁÅ∞Ëâ≤Èªë‰ΩìÂ≠óÔºåÂÜÖÂÆπ‰∏∫‚ÄúÂéüÁîüÂ§öÊ®°ÊÄÅÂ§ßÊ®°Âûã‚Äù„ÄÇ‰∏§Ë°åÊñáÂ≠óÊ∏ÖÊô∞ÊòìËØªÔºå‰∏éÊï¥‰ΩìÁöÑÁé∞‰ª£Âπ≥Èù¢ËÆæËÆ°È£éÊ†º‰øùÊåÅ‰∏ÄËá¥„ÄÇ\n\nÊï¥‰ΩìÂÖâÁ∫øÊòé‰∫Æ„ÄÅÂùáÂåÄÔºåÊ≤°ÊúâÊòéÊòæÁöÑÈò¥ÂΩ±ÔºåÁ™ÅÂá∫‰∫Ü‰ºÅÈπÖÂíåÊñáÂ≠ó‰ø°ÊÅØÔºåÁ¨¶ÂêàÁé∞‰ª£ËÆæËÆ°Êµ∑Êä•ÁöÑËßÜËßâË¶ÅÊ±Ç„ÄÇËøôÂº†ÂõæÂÉèÂëàÁé∞‰∫ÜÁé∞‰ª£„ÄÅÁÆÄÊ¥ÅÁöÑÂπ≥Èù¢ËÆæËÆ°Êµ∑Êä•È£éÊ†º„ÄÇ\n</details>\n</td>\n</tr>\n</tbody>\n</table>\n</p>\n\n## üìä Evaluation\n\n* ü§ñ **SSAE (Machine Evaluation)**   \nSSAE (Structured Semantic Alignment Evaluation) is an intelligent evaluation metric for image-text alignment based on advanced multimodal large language models (MLLMs). We extracted 3500 key points across 12 categories, then used multimodal large language models to automatically evaluate and score by comparing the generated images with these key points based on the visual content of the images. Mean Image Accuracy represents the image-wise average score across all key points, while Global Accuracy directly calculates the average score across all key points.\n\n<p align="center">\n  <img src="./assets/ssae_side_by_side_comparison.png" width=98% alt="Human Evaluation with Other Models">\n</p>\n\n<p align="center">\n  <img src="./assets/ssae_side_by_side_heatmap.png" width=98% alt="Human Evaluation with Other Models">\n</p>\n\n\n* üë• **GSB (Human Evaluation)** \n\nWe adopted the GSB (Good/Same/Bad) evaluation method commonly used to assess the relative performance between two models from an overall image perception perspective. In total, we utilized 1,000 text prompts, generating an equal number of image samples for all compared models in a single run. For a fair comparison, we conducted inference only once for each prompt, avoiding any cherry-picking of results. When comparing with the baseline methods, we maintained the default settings for all selected models. The evaluation was performed by more than 100 professional evaluators. \n\n<p align="center">\n  <img src="./assets/gsb.png" width=98% alt="Human Evaluation with Other Models">\n</p>\n\n\n## üìö Citation\n\nIf you find HunyuanImage-3.0 useful in your research, please cite our work:\n\n```bibtex\n@article{cao2025hunyuanimage,\n  title={HunyuanImage 3.0 Technical Report},\n  author={Cao, Siyu and Chen, Hangting and Chen, Peng and Cheng, Yiji and Cui, Yutao and Deng, Xinchi and Dong, Ying and Gong, Kipper and Gu, Tianpeng and Gu, Xiusen and others},\n  journal={arXiv preprint arXiv:2509.23951},\n  year={2025}\n}\n```\n\n## üôè Acknowledgements\n\nWe extend our heartfelt gratitude to the following open-source projects and communities for their invaluable contributions:\n\n* ü§ó [Transformers](https://github.com/huggingface/transformers) - State-of-the-art NLP library\n* üé® [Diffusers](https://github.com/huggingface/diffusers) - Diffusion models library  \n* üåê [HuggingFace](https://huggingface.co/) - AI model hub and community\n* ‚ö° [FlashAttention](https://github.com/Dao-AILab/flash-attention) - Memory-efficient attention\n* üöÄ [FlashInfer](https://github.com/flashinfer-ai/flashinfer) - Optimized inference engine\n\n## üåüüöÄ Github Star History\n\n[![GitHub stars](https://img.shields.io/github/stars/Tencent-Hunyuan/HunyuanImage-3.0?style=social)](https://github.com/Tencent-Hunyuan/HunyuanImage-3.0)\n[![GitHub forks](https://img.shields.io/github/forks/Tencent-Hunyuan/HunyuanImage-3.0?style=social)](https://github.com/Tencent-Hunyuan/HunyuanImage-3.0)\n\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Tencent-Hunyuan/HunyuanImage-3.0&type=Date)](https://www.star-history.com/#Tencent-Hunyuan/HunyuanImage-3.0&Date)', '{"pipeline_tag":"text-to-image","library_name":"transformers","framework":"transformers","params":83009199459,"storage_bytes":168599289097,"files_count":68,"spaces_count":53,"gated":false,"private":false,"config":{"architectures":["HunyuanImage3ForCausalMM"],"auto_map":{"AutoConfig":"configuration_hunyuan.HunyuanImage3Config","AutoModel":"hunyuan.HunyuanImage3Model","AutoModelForCausalLM":"hunyuan.HunyuanImage3ForCausalMM"},"model_type":"hunyuan_image_3_moe","tokenizer_config":{"bos_token":"<|startoftext|>","eos_token":"<|endoftext|>","pad_token":"<pad>"}}}', '[]', '[{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanImage-3.0","source_url":"https://github.com/Tencent-Hunyuan/HunyuanImage-3.0"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanImage-3.0.git","source_url":"https://github.com/Tencent-Hunyuan/HunyuanImage-3.0.git"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:Dao-AILab:flash-attention","source_url":"https://github.com/Dao-AILab/flash-attention"},{"type":"has_code","target_id":"github:flashinfer-ai:flashinfer","source_url":"https://github.com/flashinfer-ai/flashinfer"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanImage-3.0","source_url":"https://github.com/Tencent-Hunyuan/HunyuanImage-3.0"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanImage-3.0","source_url":"https://github.com/Tencent-Hunyuan/HunyuanImage-3.0"},{"type":"based_on_paper","target_id":"arxiv:2509.23951","source_url":"https://arxiv.org/abs/2509.23951"}]', NULL, 'Other', 'approved', 100, '3b5f845b8cf66b8d47c3c990c28d1c27', NULL, 'https://huggingface.co/tencent/HunyuanImage-3.0/resolve/main/assets/banner.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-tencent-HunyuanImage-3.0 from https://huggingface.co/tencent/HunyuanImage-3.0/resolve/main/assets/banner.png
Image converted to WebP: data/images/huggingface-tencent-HunyuanImage-3.0.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Envvi-Inkpunk-Diffusion', 'huggingface--envvi--inkpunk-diffusion', 'Inkpunk-Diffusion', 'Envvi', '--- license: creativeml-openrail-m language: - en tags: - stable-diffusion - text-to-image - diffusers --- Finetuned Stable Diffusion model trained on dreambooth. Vaguely inspired by Gorillaz, FLCL, and Yoji Shinkawa. Use **_nvinkpunk_** in your prompts. We support a Gradio Web UI to run Inkpunk-Diffusion: !output Samples v2 !output Samples v2', '["diffusers","stable-diffusion","text-to-image","en","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 990, 927, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Envvi/Inkpunk-Diffusion","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\nlanguage:\n- en\ntags:\n- stable-diffusion\n- text-to-image\n- diffusers\n---\n\n# Inkpunk Diffusion\n\nFinetuned Stable Diffusion model trained on dreambooth. Vaguely inspired by Gorillaz, FLCL, and Yoji Shinkawa. Use **_nvinkpunk_** in your prompts.\n\n# Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run Inkpunk-Diffusion:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/Inkpunk-Diffusion)\n\n# Sample images\n![output Samples v2](https://huggingface.co/Envvi/Inkpunk-Diffusion/resolve/main/inkpunk-v2-samples-1.png)\n![output Samples v2](https://huggingface.co/Envvi/Inkpunk-Diffusion/resolve/main/inkpunk-v2-samples-2.png)', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":28191407827,"files_count":23,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"}]', NULL, 'creativeml-openrail-m', 'approved', 50, '5c4dc42d4b4a12415442356b880ef693', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-manycore-research-SpatialLM-Llama-1B', 'huggingface--manycore-research--spatiallm-llama-1b', 'SpatialLM-Llama-1B', 'manycore-research', '--- license: llama3.2 library_name: transformers base_model: - meta-llama/Llama-3.2-1B-Instruct --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <picture> <source srcset="https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a63ac81126d0/_dK14CT3do8rBG3QrHUjN.png" media="(prefers-color-scheme: dark)"> <img src="https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a...', '["transformers","safetensors","spatiallm_llama","text-generation","conversational","base_model:meta-llama/llama-3.2-1b-instruct","license:llama3.2","endpoints_compatible","region:us"]', 'text-generation', 990, 172, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/manycore-research/SpatialLM-Llama-1B","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: llama3.2\nlibrary_name: transformers\nbase_model:\n  - meta-llama/Llama-3.2-1B-Instruct\n---\n\n# SpatialLM-Llama-1B\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <picture>\n    <source srcset="https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a63ac81126d0/_dK14CT3do8rBG3QrHUjN.png" media="(prefers-color-scheme: dark)">\n    <img src="https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a63ac81126d0/bAZyeIXOMVASHR6-xVlQU.png" width="60%" alt="SpatialLM""/>\n  </picture>\n</div>\n<hr style="margin-top: 0; margin-bottom: 8px;">\n<div align="center" style="margin-top: 0; padding-top: 0; line-height: 1;">\n    <a href="https://manycore-research.github.io/SpatialLM" target="_blank" style="margin: 2px;"><img alt="Project"\n    src="https://img.shields.io/badge/üåê%20Website-SpatialLM-ffc107?color=42a5f5&logoColor=white" style="display: inline-block; vertical-align: middle;"/></a>\n    <a href="https://github.com/manycore-research/SpatialLM" target="_blank" style="margin: 2px;"><img alt="GitHub"\n    src="https://img.shields.io/badge/GitHub-SpatialLM-24292e?logo=github&logoColor=white" style="display: inline-block; vertical-align: middle;"/></a>\n</div>\n<div align="center" style="line-height: 1;">\n    <a href="https://huggingface.co/manycore-research/SpatialLM-Llama-1B" target="_blank" style="margin: 2px;"><img alt="Hugging Face"\n    src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-SpatialLM%201B-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/></a>\n    <a href="https://huggingface.co/datasets/manycore-research/SpatialLM-Testset" target="_blank" style="margin: 2px;"><img alt="Dataset"\n    src="https://img.shields.io/badge/%F0%9F%A4%97%20Dataset-SpatialLM-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/></a>\n</div>\n\n## Introduction\n\nSpatialLM is a 3D large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object bounding boxes with their semantic categories. Unlike previous methods that require specialized equipment for data collection, SpatialLM can handle point clouds from diverse sources such as monocular video sequences, RGBD images, and LiDAR sensors. This multimodal architecture effectively bridges the gap between unstructured 3D geometric data and structured 3D representations, offering high-level semantic understanding. It enhances spatial reasoning capabilities for applications in embodied robotics, autonomous navigation, and other complex 3D scene analysis tasks.\n\n<div align="center">\n  <video controls autoplay src="https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a63ac81126d0/3bz_jNRCLD2L9uj11HPnP.mp4" poster="https://cdn-uploads.huggingface.co/production/uploads/63efbb1efc92a63ac81126d0/euo94dNx28qBNe51_oiB1.png"></video>\n  <p><i>SpatialLM reconstructs 3D layout from a monocular RGB video with MASt3R-SLAM. Results aligned to video with GT cameras for visualization.</i></p>\n</div>\n\n## SpatialLM Models\n\n<div align="center">\n\n|      **Model**      | **Download**                                                                   |\n| :-----------------: | ------------------------------------------------------------------------------ |\n| SpatialLM-Llama-1B  | [ü§ó HuggingFace](https://huggingface.co/manycore-research/SpatialLM-Llama-1B)  |\n| SpatialLM-Qwen-0.5B | [ü§ó HuggingFace](https://huggingface.co/manycore-research/SpatialLM-Qwen-0.5B) |\n\n</div>\n\n## Usage\n\n### Installation\n\nTested with the following environment:\n\n- Python 3.11\n- Pytorch 2.4.1\n- CUDA Version 12.4\n\n```bash\n# clone the repository\ngit clone https://github.com/manycore-research/SpatialLM.git\ncd SpatialLM\n\n# create a conda environment with cuda 12.4\nconda create -n spatiallm python=3.11\nconda activate spatiallm\nconda install -y nvidia/label/cuda-12.4.0::cuda-toolkit conda-forge::sparsehash\n\n# Install dependencies with poetry\npip install poetry && poetry config virtualenvs.create false --local\npoetry install\npoe install-torchsparse # Building wheel for torchsparse will take a while\n```\n\n### Inference\n\nIn the current version of SpatialLM, input point clouds are considered axis-aligned where the z-axis is the up axis. This orientation is crucial for maintaining consistency in spatial understanding and scene interpretation across different datasets and applications.\nExample preprocessed point clouds, reconstructed from RGB videos using [MASt3R-SLAM](https://github.com/rmurai0610/MASt3R-SLAM), are available in [SpatialLM-Testset](#spatiallm-testset).\n\nDownload an example point cloud:\n\n```bash\nhuggingface-cli download manycore-research/SpatialLM-Testset pcd/scene0000_00.ply --repo-type dataset --local-dir .\n```\n\nRun inference:\n\n```bash\npython inference.py --point_cloud pcd/scene0000_00.ply --output scene0000_00.txt --model_path manycore-research/SpatialLM-Llama-1B\n```\n\n### Visualization\n\nUse `rerun` to visualize the point cloud and the predicted structured 3D layout output:\n\n```bash\n# Convert the predicted layout to Rerun format\npython visualize.py --point_cloud pcd/scene0000_00.ply --layout scene0000_00.txt --save scene0000_00.rrd\n\n# Visualize the point cloud and the predicted layout\nrerun scene0000_00.rrd\n```\n\n### Evaluation\n\nTo evaluate the performance of SpatialLM, we provide `eval.py` script that reports the benchmark results on the SpatialLM-Testset in the table below in section [Benchmark Results](#benchmark-results).\n\nDownload the testset:\n\n```bash\nhuggingface-cli download manycore-research/SpatialLM-Testset --repo-type dataset --local-dir SpatialLM-Testset\n```\n\nRun evaluation:\n\n```bash\n# Run inference on the PLY point clouds in folder SpatialLM-Testset/pcd with SpatialLM-Llama-1B model\npython inference.py --point_cloud SpatialLM-Testset/pcd --output SpatialLM-Testset/pred --model_path manycore-research/SpatialLM-Llama-1B\n\n# Evaluate the predicted layouts\npython eval.py --metadata SpatialLM-Testset/test.csv --gt_dir SpatialLM-Testset/layout --pred_dir SpatialLM-Testset/pred --label_mapping SpatialLM-Testset/benchmark_categories.tsv\n```\n\n## SpatialLM Testset\n\nWe provide a test set of 107 preprocessed point clouds, reconstructed from RGB videos using [MASt3R-SLAM](https://github.com/rmurai0610/MASt3R-SLAM). SpatialLM-Testset is quite challenging compared to prior clean RGBD scans datasets due to the noises and occlusions in the point clouds reconstructed from monocular RGB videos.\n\n<div align="center">\n\n|    **Dataset**    | **Download**                                                                       |\n| :---------------: | ---------------------------------------------------------------------------------- |\n| SpatialLM-Testset | [ü§ó Datasets](https://huggingface.co/datasets/manycore-research/SpatialLM-TestSet) |\n\n</div>\n\n## Benchmark Results\n\nBenchmark results on the challenging SpatialLM-Testset are reported in the following table:\n\n<div align="center">\n\n| **Method**       | **SpatialLM-Llama-1B** | **SpatialLM-Qwen-0.5B** |\n| ---------------- | ---------------------- | ----------------------- |\n| **Floorplan**    | **mean IoU**           |                         |\n| wall             | 78.62                  | 74.81                   |\n|                  |                        |                         |\n| **Objects**      | **F1 @.25 IoU (3D)**   |                         |\n| curtain          | 27.35                  | 28.59                   |\n| nightstand       | 57.47                  | 54.39                   |\n| chandelier       | 38.92                  | 40.12                   |\n| wardrobe         | 23.33                  | 30.60                   |\n| bed              | 95.24                  | 93.75                   |\n| sofa             | 65.50                  | 66.15                   |\n| chair            | 21.26                  | 14.94                   |\n| cabinet          | 8.47                   | 8.44                    |\n| dining table     | 54.26                  | 56.10                   |\n| plants           | 20.68                  | 26.46                   |\n| tv cabinet       | 33.33                  | 10.26                   |\n| coffee table     | 50.00                  | 55.56                   |\n| side table       | 7.60                   | 2.17                    |\n| air conditioner  | 20.00                  | 13.04                   |\n| dresser          | 46.67                  | 23.53                   |\n|                  |                        |                         |\n| **Thin Objects** | **F1 @.25 IoU (2D)**   |                         |\n| painting         | 50.04                  | 53.81                   |\n| carpet           | 31.76                  | 45.31                   |\n| tv               | 67.31                  | 52.29                   |\n| door             | 50.35                  | 42.15                   |\n| window           | 45.4                   | 45.9                    |\n\n</div>\n\n## License\n\nSpatialLM-Llama-1B is derived from Llama3.2-1B-Instruct, which is licensed under the Llama3.2 license.\nSpatialLM-Qwen-0.5B is derived from the Qwen-2.5 series, originally licensed under the Apache 2.0 License.\n\nAll models are built upon the SceneScript point cloud encoder, licensed under the CC-BY-NC-4.0 License. TorchSparse, utilized in this project, is licensed under the MIT License.\n\n## Citation\n\nIf you find this work useful, please consider citing:\n\n```bibtex\n@misc{spatiallm,\n  title        = {SpatialLM: Large Language Model for Spatial Understanding},\n  author       = {ManyCore Research Team},\n  howpublished = {\url{https://github.com/manycore-research/SpatialLM}},\n  year         = {2025}\n}\n```\n\n## Acknowledgements\n\nWe would like to thank the following projects that made this work possible:\n\n[Llama3.2](https://github.com/meta-llama) | [Qwen2.5](https://github.com/QwenLM/Qwen2.5) | [Transformers](https://github.com/huggingface/transformers) | [SceneScript](https://github.com/facebookresearch/scenescript) | [TorchSparse](https://github.com/mit-han-lab/torchsparse)\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":1247355840,"storage_bytes":2532929485,"files_count":8,"spaces_count":0,"gated":false,"private":false,"config":{"architectures":["SpatialLMLlamaForCausalLM"],"model_type":"spatiallm_llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n    {%- else %}\n        {%- set date_string = \"26 Jul 2024\" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][''content'']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there''s no first user message!\") }}\n{%- endif %}\n    {{- ''<|start_header_id|>user<|end_header_id|>\\n\\n'' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == ''ipython'' or message.role == ''tool'' or ''tool_calls'' in message) %}\n        {{- ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\\n\\n''+ message[''content''] | trim + ''<|eot_id|>'' }}\n    {%- elif ''tool_calls'' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n        {{- ''{\"name\": \"'' + tool_call.name + ''\", '' }}\n        {{- ''\"parameters\": '' }}\n        {{- tool_call.arguments | tojson }}\n        {{- \"}\" }}\n        {{- \"<|eot_id|>\" }}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' }}\n{%- endif %}\n","eos_token":"<|eot_id|>","pad_token":"<|eot_id|>"}}}', '[]', '[{"type":"has_code","target_id":"github:manycore-research:SpatialLM\"","source_url":"https://github.com/manycore-research/SpatialLM\""},{"type":"has_code","target_id":"github:manycore-research:SpatialLM.git","source_url":"https://github.com/manycore-research/SpatialLM.git"},{"type":"has_code","target_id":"github:rmurai0610:MASt3R-SLAM","source_url":"https://github.com/rmurai0610/MASt3R-SLAM"},{"type":"has_code","target_id":"github:rmurai0610:MASt3R-SLAM","source_url":"https://github.com/rmurai0610/MASt3R-SLAM"},{"type":"has_code","target_id":"github:manycore-research:SpatialLM}},","source_url":"https://github.com/manycore-research/SpatialLM}},"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:facebookresearch:scenescript","source_url":"https://github.com/facebookresearch/scenescript"},{"type":"has_code","target_id":"github:mit-han-lab:torchsparse","source_url":"https://github.com/mit-han-lab/torchsparse"}]', NULL, 'llama3.2', 'approved', 80, 'b398c3deda3dd1ca45c2ddc3839f790c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-miqudev-miqu-1-70b', 'huggingface--miqudev--miqu-1-70b', 'miqu-1-70b', 'miqudev', '--- {} --- Leaked from ‚ñÑ‚ñÑ‚ñÑ‚ñë‚ñë ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà...', '["gguf","endpoints_compatible","region:us","conversational"]', 'other', 986, 596, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/miqudev/miqu-1-70b","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\n{}\n---\n# miqu 70b\n\nLeaked from\n\n\n                                                                     ‚ñÑ‚ñÑ‚ñÑ‚ñë‚ñë\n                                                            ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë\n                                                ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n                                             ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n                        ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n             ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n          ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë      ‚ñë‚ñë‚ñë\n                ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë    ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n                   ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n                        ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë\n                           ‚ñë‚ñë‚ñë‚ñë‚ñë\n\n## Model card\n\nFirst model in the potential series.\n\n## Prompt format: Mistral\n\n```\n<s> [INST] QUERY_1 [/INST] ANSWER_1</s> [INST] QUERY_2 [/INST] ANSWER_2</s>...\n```\n\nBeware that some backends (like llama.cpp) add bos already (by default), so you don''t need to prepend it yourself.\n\n## Settings\n\nDO NOT CHANGE ROPE SETTINGS. This model uses high freq base with 32k seen tokens, it should be fine for most tasks.\n\nOnly tested with temp 1 and top_p 0.95 with everything else disabled.\n\n<video src="https://cdn-uploads.huggingface.co/production/uploads/65ab93082bf3e0cbbf717850/cIEP5e43VP0k0caRzl16e.mp4" controls="controls" style="max-width: 720px;">\n</video>', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":115639116384,"files_count":5,"spaces_count":0,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, NULL, 'pending', 54.9, 'a84d44e597feeb2292d93a25501cde5c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nuigurumi-basil-mix', 'huggingface--nuigurumi--basil-mix', 'basil_mix', 'nuigurumi', '--- license: other --- - merged model. - realistic texture and Asian face. - designed to maintain a responsive reaction to danbooru based prompts. - This model and its derivatives(image, merged model) can be freely used for non-profit purposes only. - You may not use this model and its derivatives on websites, apps, or other platforms where you can or plan to earn income or donations. If you wish to use it for such purposes, please contact nuigurumi. - Introducing the model itself is allowed ...', '["diffusers","license:other","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 984, 22808, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nuigurumi/basil_mix","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\n---\n## Model Description\n\n- merged model.\n- realistic texture and Asian face.\n- designed to maintain a responsive reaction to danbooru based prompts.\n\n## License\n  \n- This model and its derivatives(image, merged model) can be freely used for non-profit purposes only.\n- You may not use this model and its derivatives on websites, apps, or other platforms where you can or plan to earn income or donations. If you wish to use it for such purposes, please contact nuigurumi.\n- Introducing the model itself is allowed for both commercial and non-commercial purposes, but please include the model name and a link to this repository when doing so.\n\n- „Åì„ÅÆ„É¢„Éá„É´Âèä„Å≥„Åù„ÅÆÊ¥æÁîüÁâ©(ÁîüÊàêÁâ©„ÄÅ„Éû„Éº„Ç∏„É¢„Éá„É´)„ÅØ„ÄÅÂÆåÂÖ®„Å´ÈùûÂñ∂Âà©ÁõÆÁöÑ„ÅÆ‰ΩøÁî®„Å´Èôê„Çä„ÄÅËá™Áî±„Å´Âà©Áî®„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ\n- „ÅÇ„Å™„Åü„ÅåÂèéÂÖ•„ÇÑÂØÑ‰ªò„ÇíÂæó„Çã„Åì„Å®„ÅÆ„Åß„Åç„Çã„ÄÅ„ÇÇ„Åó„Åè„ÅØÂæó„Çã‰∫àÂÆö„ÅÆWeb„Çµ„Ç§„Éà„ÄÅ„Ç¢„Éó„É™„ÄÅ„Åù„ÅÆ‰ªñ„Åß„Åì„ÅÆ„É¢„Éá„É´Âèä„Å≥„Åù„ÅÆÊ¥æÁîüÁâ©„ÇíÂà©Áî®„Åô„Çã„Åì„Å®„ÅØ„Åß„Åç„Åæ„Åõ„Çì„ÄÇÂà©Áî®„Åó„Åü„ÅÑÂ†¥Âêà„ÅØ[nuigurumi](https://twitter.com/nuigurumi1_KR)„Å´ÈÄ£Áµ°„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n- „É¢„Éá„É´Ëá™‰Ωì„ÅÆÁ¥π‰ªã„Åô„Çã„Åì„Å®„ÅØ„ÄÅÂñ∂Âà©ÈùûÂñ∂Âà©„ÇíÂïè„Çè„ÅöËá™Áî±„Åß„Åô„ÄÅ„Åù„ÅÆÂ†¥Âêà„ÅØ„É¢„Éá„É´Âêç„Å®ÂΩì„É™„Éù„Ç∏„Éà„É™„ÅÆ„É™„É≥„ÇØ„Çí‰ΩµË®ò„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n\n- check [License](https://huggingface.co/nuigurumi/basil_mix/blob/main/License.md)\n  \n  \n  _Ë™≠„ÇÄ„ÅÆ„ÇÅ„Çì„Å©„Åè„Åï„ÅÑ‰∫∫Âêë„Åë  \n  ÂïÜÁî®Âà©Áî®„Çí„Åô„Åπ„Å¶Á¶ÅÊ≠¢„Åó„Åæ„Åô„ÄÇfanbox„ÇÑpatreon„Å™„Å©„ÅÆÊîØÊè¥„Çµ„Ç§„Éà„Åß„ÅÆ‰ΩøÁî®„ÇÇÂÖ®„Å¶Á¶ÅÊ≠¢„Åó„Åæ„Åô„ÄÇ  \n  „Éû„Éº„Ç∏„É¢„Éá„É´(cilled_re...„Å®„Åã)„ÇÇÊ¥æÁîüÁâ©„Å™„ÅÆ„ÅßÂïÜÁî®Âà©Áî®Á¶ÅÊ≠¢„Å´„Å™„Çä„Åæ„Åô„ÄÇ ÂïÜÁî®Âà©Áî®„Çí„Åó„Åü„ÅÑ„Å™„ÇâÁßÅ„Å´ÈÄ£Áµ°„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ  \n  „Å©„Åì„Åã„Åß„É¢„Éá„É´„ÇíÁ¥π‰ªã„Åó„Å¶„ÅÑ„Åü„Å†„Åë„Çã„Å™„Çâ„ÄÅ„É™„É≥„ÇØ„ÇÇ‰ΩµË®ò„Åó„Å¶„Åè„Çå„Çã„Å®Â¨â„Åó„ÅÑ„Åß„Åô„ÄÇ_ \n\n# Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run basil_mix:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/akhaliq/basil_mix)\n\n\n## Recommendations\n\n- VAE: [vae-ft-mse-840000](https://huggingface.co/stabilityai/sd-vae-ft-mse-original) from StabilityAI\n- Prompting: Simple prompts are better. Large amounts of quality tags and negative prompts can have negative effects.', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":17450358760,"files_count":21,"spaces_count":5,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"}]', NULL, 'Other', 'approved', 49.9, '0403d07e994ea57e0c2ff79932b8edfd', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-HiDream-ai-HiDream-I1-Full', 'huggingface--hidream-ai--hidream-i1-full', 'HiDream-I1-Full', 'HiDream-ai', '--- license: mit tags: - image-generation - HiDream.ai language: - en pipeline_tag: text-to-image library_name: diffusers --- !HiDream-I1 Demo is a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within seconds. <span style="color: #FF5733; font-weight: bold">For more features and to experience the full capabilities of our product, please visit https://vivago.ai/.</span> - üåü **July 16, 2025**: We''ve open-sourced th...', '["diffusers","safetensors","image-generation","hidream.ai","text-to-image","en","arxiv:2505.22705","license:mit","diffusers:hidreamimagepipeline","region:us"]', 'text-to-image', 980, 22144, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/HiDream-ai/HiDream-I1-Full","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\ntags:\n- image-generation\n- HiDream.ai\nlanguage:\n- en\npipeline_tag: text-to-image\nlibrary_name: diffusers\n---\n\n![HiDream-I1 Demo](demo.jpg)\n\n`HiDream-I1` is a new open-source image generative foundation model with 17B parameters that achieves state-of-the-art image generation quality within seconds.\n\n<span style="color: #FF5733; font-weight: bold">For more features and to experience the full capabilities of our product, please visit [https://vivago.ai/](https://vivago.ai/).</span>\n\n## Project Updates\n- üåü **July 16, 2025**: We''ve open-sourced the updated image editing model [**HiDream-E1.1**](https://huggingface.co/HiDream-ai/HiDream-E1-1). \n- üìù **May 28, 2025**: We''ve released our technical report [HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer](https://arxiv.org/abs/2505.22705).\n- üöÄ **April 28, 2025**: We''ve open-sourced the instruction-based-image-editing model [**HiDream-E1-Full**](https://github.com/HiDream-ai/HiDream-E1). Experience at [https://huggingface.co/spaces/HiDream-ai/HiDream-E1-Full](https://huggingface.co/spaces/HiDream-ai/HiDream-E1-Full)!.\n\n## Key Features\n- ‚ú® **Superior Image Quality** - Produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. Achieves state-of-the-art HPS v2.1 score, which aligns with human preferences.\n- üéØ **Best-in-Class Prompt Following** - Achieves industry-leading scores on GenEval and DPG benchmarks, outperforming all other open-source models.\n- üîì **Open Source** - Released under the MIT license to foster scientific advancement and enable creative innovation.\n- üíº **Commercial-Friendly** - Generated images can be freely used for personal projects, scientific research, and commercial applications.\n\n## Quick Start\nPlease make sure you have installed [Flash Attention](https://github.com/Dao-AILab/flash-attention). We recommend CUDA version 12.4 for the manual installation.\n```\npip install -r requirements.txt\n```\nClone the GitHub repo:\n```\ngit clone https://github.com/HiDream-ai/HiDream-I1\n```\n\nThen you can run the inference scripts to generate images:\n\n```python\n# For full model inference\npython ./inference.py --model_type full\n\n# For distilled dev model inference\npython ./inference.py --model_type dev\n\n# For distilled fast model inference\npython ./inference.py --model_type fast\n```\n> **Note:** The inference script will automatically download `meta-llama/Meta-Llama-3.1-8B-Instruct` model files. If you encounter network issues, you can download these files ahead of time and place them in the appropriate cache directory to avoid download failures during inference.\n\n## Gradio Demo\n\nWe also provide a Gradio demo for interactive image generation. You can run the demo with:\n\n```python\npython gradio_demo.py \n```\n\n## Evaluation Metrics\n\n### DPG-Bench\n| Model           | Overall   | Global    | Entity    | Attribute | Relation  | Other     |\n|-----------------|-----------|-----------|-----------|-----------|-----------|-----------|\n| PixArt-alpha    |    71.11  | 74.97     | 79.32     | 78.60     | 82.57     | 76.96     |\n| SDXL            |    74.65  | 83.27     | 82.43     | 80.91     | 86.76     | 80.41     |\n| DALL-E 3        |    83.50  | 90.97     | 89.61     | 88.39     | 90.58     | 89.83     |\n| Flux.1-dev      |    83.79  | 85.80     | 86.79     | 89.98     | 90.04     | 89.90     |\n| SD3-Medium      |    84.08  | 87.90     | 91.01     | 88.83     | 80.70     | 88.68     |\n| Janus-Pro-7B    |    84.19  | 86.90     | 88.90     | 89.40     | 89.32     | 89.48     |\n| CogView4-6B     |    85.13  | 83.85     | 90.35     | 91.17     | 91.14     | 87.29     |\n| **HiDream-I1**  |  **85.89**| 76.44 	  | 90.22     | 89.48     | 93.74     | 91.83     | \n\n### GenEval\n\n| Model           | Overall  | Single Obj. | Two Obj. | Counting | Colors   | Position | Color attribution |\n|-----------------|----------|-------------|----------|----------|----------|----------|-------------------|\n| SDXL            |    0.55  | 0.98        | 0.74     | 0.39     | 0.85     | 0.15     | 0.23              |\n| PixArt-alpha    |    0.48  | 0.98        | 0.50     | 0.44     | 0.80     | 0.08     | 0.07              |\n| Flux.1-dev      |    0.66  | 0.98        | 0.79     | 0.73     | 0.77     | 0.22     | 0.45              |\n| DALL-E 3        |    0.67  | 0.96        | 0.87     | 0.47     | 0.83     | 0.43     | 0.45              |\n| CogView4-6B     |    0.73  | 0.99        | 0.86     | 0.66     | 0.79     | 0.48     | 0.58              |\n| SD3-Medium      |    0.74  | 0.99        | 0.94     | 0.72     | 0.89     | 0.33     | 0.60              |\n| Janus-Pro-7B    |    0.80  | 0.99        | 0.89     | 0.59     | 0.90     | 0.79     | 0.66              |\n| **HiDream-I1**  |  **0.83**| 1.00        | 0.98 	  | 0.79 	 | 0.91 	| 0.60 	   | 0.72              |\n\n### HPSv2.1 benchmark\n\n|  Model                  |     Averaged   | Animation  |  Concept-art  |   Painting   |   Photo    |\n|-------------------------|----------------|------------|---------------|--------------|------------|\n|  Stable Diffusion v2.0  |       26.38    |	27.09   |      26.02    |    25.68     |    26.73   |\n|  Midjourney V6          |       30.29    |    32.02   |      30.29    |    29.74     |    29.10   |\n|  SDXL	                  |       30.64    |    32.84   |      31.36    |    30.86     |    27.48   |\n|  Dall-E3	              |       31.44    |    32.39   |      31.09    |    31.18     |    31.09   |\n|  SD3                    |       31.53    |    32.60   |      31.82    |    32.06     |    29.62   |\n|  Midjourney V5          |       32.33    |    34.05   |      32.47    |    32.24     |    30.56   |\n|  CogView4-6B            |       32.31    |    33.23   |      32.60    |    32.89     |    30.52   |\n|  Flux.1-dev             |       32.47    |    33.87   |      32.27    |    32.62     |    31.11   |\n|  stable cascade         |       32.95    |    34.58   |      33.13    |    33.29     |    30.78   |\n|  **HiDream-I1**         |     **33.82**  |    35.05   |      33.74    |    33.88     |    32.61   |\n\n\n## License Agreement\nThe Transformer models in this repository are licensed under the MIT License. The VAE is from `FLUX.1 [schnell]`, and the text encoders from `google/t5-v1_1-xxl` and `meta-llama/Meta-Llama-3.1-8B-Instruct`. Please follow the license terms specified for these components. You own all content you create with this model. You can use your generated content freely, but you must comply with this license agreement. You are responsible for how you use the models. Do not create illegal content, harmful material, personal information that could harm others, false information, or content targeting vulnerable groups.\n\n\n## Acknowledgements\n- The VAE component is from `FLUX.1 [schnell]`, licensed under Apache 2.0. \n- The text encoders are from `google/t5-v1_1-xxl` (licensed under Apache 2.0) and `meta-llama/Meta-Llama-3.1-8B-Instruct` (licensed under the Llama 3.1 Community License Agreement).\n\n\n## Citation\n\n```bibtex\n@article{hidreami1technicalreport,\n  title={HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer},\n  author={Cai, Qi and Chen, Jingwen and Chen, Yang and Li, Yehao and Long, Fuchen and Pan, Yingwei and Qiu, Zhaofan and Zhang, Yiheng and Gao, Fengbin and Xu, Peihan and others},\n  journal={arXiv preprint arXiv:2505.22705},\n  year={2025}\n}\n```', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":47186202978,"files_count":36,"spaces_count":99,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"HiDreamImagePipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:HiDream-ai:HiDream-E1","source_url":"https://github.com/HiDream-ai/HiDream-E1"},{"type":"has_code","target_id":"github:Dao-AILab:flash-attention","source_url":"https://github.com/Dao-AILab/flash-attention"},{"type":"has_code","target_id":"github:HiDream-ai:HiDream-I1","source_url":"https://github.com/HiDream-ai/HiDream-I1"},{"type":"based_on_paper","target_id":"arxiv:2505.22705","source_url":"https://arxiv.org/abs/2505.22705"}]', NULL, 'MIT', 'approved', 84.9, '1732e90072de6d5076b1a7d1d433efd2', NULL, 'https://huggingface.co/HiDream-ai/HiDream-I1-Full/resolve/main/demo.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-HiDream-ai-HiDream-I1-Full from https://huggingface.co/HiDream-ai/HiDream-I1-Full/resolve/main/demo.jpg
Image converted to WebP: data/images/huggingface-HiDream-ai-HiDream-I1-Full.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-moka-ai-m3e-base', 'huggingface--moka-ai--m3e-base', 'm3e-base', 'moka-ai', '--- language: - zh - en tags: - embedding - text-embedding library_name: sentence-transformers --- m3e-small | m3e-base M3E ÊòØ Moka Massive Mixed Embedding ÁöÑÁº©ÂÜô - MokaÔºåÊ≠§Ê®°ÂûãÁî± MokaAI ËÆ≠ÁªÉÔºåÂºÄÊ∫êÂíåËØÑÊµãÔºåËÆ≠ÁªÉËÑöÊú¨‰ΩøÁî® uniem ÔºåËØÑÊµã BenchMark ‰ΩøÁî® MTEB-zh - MassiveÔºåÊ≠§Ê®°ÂûãÈÄöËøá**ÂçÉ‰∏áÁ∫ß** (2200w+) ÁöÑ‰∏≠ÊñáÂè•ÂØπÊï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉ - MixedÔºåÊ≠§Ê®°ÂûãÊîØÊåÅ‰∏≠Ëã±ÂèåËØ≠ÁöÑÂêåË¥®ÊñáÊú¨Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÔºåÂºÇË¥®ÊñáÊú¨Ê£ÄÁ¥¢Á≠âÂäüËÉΩÔºåÊú™Êù•Ëøò‰ºöÊîØÊåÅ‰ª£Á†ÅÊ£ÄÁ¥¢ - EmbeddingÔºåÊ≠§Ê®°ÂûãÊòØÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÔºåÂèØ‰ª•Â∞ÜËá™ÁÑ∂ËØ≠Ë®ÄËΩ¨Êç¢ÊàêÁ®†ÂØÜÁöÑÂêëÈáè - 2023.06.24ÔºåÊ∑ªÂä†ÂæÆË∞É M3E ÁöÑÊïôÁ®ã notebookÔºåÂá†Ë°å‰ª£Á†ÅÔºåÊõ¥‰Ω≥ÈÄÇÈÖçÔºÅ<a target="_blank" href="https://colab.research.google.com/github/wangyuxinwhy/uniem/blob/main...', '["sentence-transformers","pytorch","safetensors","bert","embedding","text-embedding","zh","en","region:us"]', 'other', 974, 152775, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/moka-ai/m3e-base","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- zh\n- en\ntags:\n- embedding\n- text-embedding\nlibrary_name: sentence-transformers\n---\n\n# üÖú M3E Models\n\n[m3e-small](https://huggingface.co/moka-ai/m3e-small) | [m3e-base](https://huggingface.co/moka-ai/m3e-base)\n\nM3E ÊòØ Moka Massive Mixed Embedding ÁöÑÁº©ÂÜô\n\n- MokaÔºåÊ≠§Ê®°ÂûãÁî± MokaAI ËÆ≠ÁªÉÔºåÂºÄÊ∫êÂíåËØÑÊµãÔºåËÆ≠ÁªÉËÑöÊú¨‰ΩøÁî® [uniem](https://github.com/wangyuxinwhy/uniem/blob/main/scripts/train_m3e.py) ÔºåËØÑÊµã BenchMark ‰ΩøÁî® [MTEB-zh](https://github.com/wangyuxinwhy/uniem/tree/main/mteb-zh)\n- MassiveÔºåÊ≠§Ê®°ÂûãÈÄöËøá**ÂçÉ‰∏áÁ∫ß** (2200w+) ÁöÑ‰∏≠ÊñáÂè•ÂØπÊï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉ\n- MixedÔºåÊ≠§Ê®°ÂûãÊîØÊåÅ‰∏≠Ëã±ÂèåËØ≠ÁöÑÂêåË¥®ÊñáÊú¨Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÔºåÂºÇË¥®ÊñáÊú¨Ê£ÄÁ¥¢Á≠âÂäüËÉΩÔºåÊú™Êù•Ëøò‰ºöÊîØÊåÅ‰ª£Á†ÅÊ£ÄÁ¥¢\n- EmbeddingÔºåÊ≠§Ê®°ÂûãÊòØÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÔºåÂèØ‰ª•Â∞ÜËá™ÁÑ∂ËØ≠Ë®ÄËΩ¨Êç¢ÊàêÁ®†ÂØÜÁöÑÂêëÈáè\n\n## üÜï Êõ¥Êñ∞ËØ¥Êòé\n\n- 2023.06.24ÔºåÊ∑ªÂä†ÂæÆË∞É M3E ÁöÑÊïôÁ®ã [notebook](https://github.com/wangyuxinwhy/uniem/blob/main/examples/finetune.ipynb)ÔºåÂá†Ë°å‰ª£Á†ÅÔºåÊõ¥‰Ω≥ÈÄÇÈÖçÔºÅ<a target="_blank" href="https://colab.research.google.com/github/wangyuxinwhy/uniem/blob/main/examples/finetune.ipynb">\n  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>\n</a>\n- 2023.06.14ÔºåÊ∑ªÂä†‰∫Ü‰∏â‰∏™‰∏≠ÊñáÂºÄÊ∫êÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÂà∞ËØÑÊµã‰∏≠ÔºåÂåÖÊã¨ UER, ErLangShen, DMetaSoul\n- 2023.06.08ÔºåÊ∑ªÂä†Ê£ÄÁ¥¢‰ªªÂä°ÁöÑËØÑÊµãÁªìÊûúÔºåÂú® T2Ranking 1W ‰∏≠ÊñáÊï∞ÊçÆÈõÜ‰∏äÔºåm3e-base Âú® ndcg@10 ‰∏äËææÂà∞‰∫Ü 0.8004ÔºåË∂ÖËøá‰∫Ü openai-ada-002 ÁöÑ 0.7786\n- 2023.06.07ÔºåÊ∑ªÂä†ÊñáÊú¨ÂàÜÁ±ª‰ªªÂä°ÁöÑËØÑÊµãÁªìÊûúÔºåÂú® 6 ÁßçÊñáÊú¨ÂàÜÁ±ªÊï∞ÊçÆÈõÜ‰∏äÔºåm3e-base Âú® accuracy ‰∏äËææÂà∞‰∫Ü 0.6157ÔºåË∂ÖËøá‰∫Ü openai-ada-002 ÁöÑ 0.5956\n\n## ‚öñÔ∏è Ê®°ÂûãÂØπÊØî\n\n|           | ÂèÇÊï∞Êï∞Èáè | Áª¥Â∫¶ | ‰∏≠Êñá | Ëã±Êñá | s2s | s2p | s2c | ÂºÄÊ∫ê | ÂÖºÂÆπÊÄß | s2s Acc | s2p ndcg@10 |\n| --------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | ---- | ---------- | ------------ | -------- |\n| m3e-small | 24M      | 512      | ÊòØ       | Âê¶       | ÊòØ       | Âê¶       | Âê¶       | ÊòØ   | ‰ºò         | 0.5834       | 0.7262   |\n| m3e-base  | 110M     | 768      | ÊòØ       | ÊòØ       | ÊòØ       | ÊòØ       | Âê¶       | ÊòØ   | ‰ºò         | **0.6157**       | **0.8004**   |\n| text2vec  | 110M     | 768      | ÊòØ       | Âê¶       | ÊòØ       | Âê¶       | Âê¶       | ÊòØ   | ‰ºò         | 0.5755       | 0.6346   |\n| openai-ada-002    | Êú™Áü•     | 1536     | ÊòØ       | ÊòØ       | ÊòØ       | ÊòØ       | ÊòØ       | Âê¶   | ‰ºò         | 0.5956       | 0.7786   |\n\nËØ¥ÊòéÔºö\n- s2s, Âç≥ sentence to sentence Ôºå‰ª£Ë°®‰∫ÜÂêåË¥®ÊñáÊú¨‰πãÈó¥ÁöÑÂµåÂÖ•ËÉΩÂäõÔºåÈÄÇÁî®‰ªªÂä°ÔºöÊñáÊú¨Áõ∏‰ººÂ∫¶ÔºåÈáçÂ§çÈóÆÈ¢òÊ£ÄÊµãÔºåÊñáÊú¨ÂàÜÁ±ªÁ≠â\n- s2p, Âç≥ sentence to passage Ôºå‰ª£Ë°®‰∫ÜÂºÇË¥®ÊñáÊú¨‰πãÈó¥ÁöÑÂµåÂÖ•ËÉΩÂäõÔºåÈÄÇÁî®‰ªªÂä°ÔºöÊñáÊú¨Ê£ÄÁ¥¢ÔºåGPT ËÆ∞ÂøÜÊ®°ÂùóÁ≠â\n- s2c, Âç≥ sentence to code Ôºå‰ª£Ë°®‰∫ÜËá™ÁÑ∂ËØ≠Ë®ÄÂíåÁ®ãÂ∫èËØ≠Ë®Ä‰πãÈó¥ÁöÑÂµåÂÖ•ËÉΩÂäõÔºåÈÄÇÁî®‰ªªÂä°Ôºö‰ª£Á†ÅÊ£ÄÁ¥¢\n- ÂÖºÂÆπÊÄßÔºå‰ª£Ë°®‰∫ÜÊ®°ÂûãÂú®ÂºÄÊ∫êÁ§æÂå∫‰∏≠ÂêÑÁßçÈ°πÁõÆË¢´ÊîØÊåÅÁöÑÁ®ãÂ∫¶ÔºåÁî±‰∫é m3e Âíå text2vec ÈÉΩÂèØ‰ª•Áõ¥Êé•ÈÄöËøá sentence-transformers Áõ¥Êé•‰ΩøÁî®ÔºåÊâÄ‰ª•Âíå openai Âú®Á§æÂå∫ÁöÑÊîØÊåÅÂ∫¶‰∏äÁõ∏ÂΩì\n- ACC & ndcg@10ÔºåËØ¶ÊÉÖËßÅ‰∏ãÊñπÁöÑËØÑÊµã\n\nTips:\n- ‰ΩøÁî®Âú∫ÊôØ‰∏ªË¶ÅÊòØ‰∏≠ÊñáÔºåÂ∞ëÈáèËã±ÊñáÁöÑÊÉÖÂÜµÔºåÂª∫ËÆÆ‰ΩøÁî® m3e Á≥ªÂàóÁöÑÊ®°Âûã\n- Â§öËØ≠Ë®Ä‰ΩøÁî®Âú∫ÊôØÔºåÂπ∂‰∏î‰∏ç‰ªãÊÑèÊï∞ÊçÆÈöêÁßÅÁöÑËØùÔºåÊàëÂª∫ËÆÆ‰ΩøÁî® openai text-embedding-ada-002\n- ‰ª£Á†ÅÊ£ÄÁ¥¢Âú∫ÊôØÔºåÊé®Ëçê‰ΩøÁî® openai text-embedding-ada-002\n- ÊñáÊú¨Ê£ÄÁ¥¢Âú∫ÊôØÔºåËØ∑‰ΩøÁî®ÂÖ∑Â§áÊñáÊú¨Ê£ÄÁ¥¢ËÉΩÂäõÁöÑÊ®°ÂûãÔºåÂè™Âú® S2S ‰∏äËÆ≠ÁªÉÁöÑÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÔºåÊ≤°ÊúâÂäûÊ≥ïÂÆåÊàêÊñáÊú¨Ê£ÄÁ¥¢‰ªªÂä°\n\n## üîß ‰ΩøÁî® M3E\n\nÊÇ®ÈúÄË¶ÅÂÖàÂÆâË£Ö sentence-transformers\n\n```bash\npip install -U sentence-transformers\n```\n\nÂÆâË£ÖÂÆåÊàêÂêéÔºåÊÇ®ÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ã‰ª£Á†ÅÊù•‰ΩøÁî® M3E Models\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(''moka-ai/m3e-base'')\n\n#Our sentences we like to encode\nsentences = [\n    ''* Moka Ê≠§ÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÁî± MokaAI ËÆ≠ÁªÉÂπ∂ÂºÄÊ∫êÔºåËÆ≠ÁªÉËÑöÊú¨‰ΩøÁî® uniem'',\n    ''* Massive Ê≠§ÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÈÄöËøá**ÂçÉ‰∏áÁ∫ß**ÁöÑ‰∏≠ÊñáÂè•ÂØπÊï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉ'',\n    ''* Mixed Ê≠§ÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÊîØÊåÅ‰∏≠Ëã±ÂèåËØ≠ÁöÑÂêåË¥®ÊñáÊú¨Áõ∏‰ººÂ∫¶ËÆ°ÁÆóÔºåÂºÇË¥®ÊñáÊú¨Ê£ÄÁ¥¢Á≠âÂäüËÉΩÔºåÊú™Êù•Ëøò‰ºöÊîØÊåÅ‰ª£Á†ÅÊ£ÄÁ¥¢ÔºåALL in one''\n]\n\n#Sentences are encoded by calling model.encode()\nembeddings = model.encode(sentences)\n\n#Print the embeddings\nfor sentence, embedding in zip(sentences, embeddings):\n    print("Sentence:", sentence)\n    print("Embedding:", embedding)\n    print("")\n```\n\n\nM3E Á≥ªÂàóÁöÑÊâÄÊúâÊ®°ÂûãÂú®ËÆæËÆ°ÁöÑÊó∂ÂÄôÂ∞±ËÄÉËôëÂà∞ÂÆåÂÖ®ÂÖºÂÆπ [sentence-transformers](https://www.sbert.net/) ÔºåÊâÄ‰ª•‰Ω†ÂèØ‰ª•ÈÄöËøá**ÊõøÊç¢ÂêçÁß∞Â≠óÁ¨¶‰∏≤**ÁöÑÊñπÂºèÂú®ÊâÄÊúâÊîØÊåÅ sentence-transformers ÁöÑÈ°πÁõÆ‰∏≠**Êó†Áºù**‰ΩøÁî® M3E ModelsÔºåÊØîÂ¶Ç [chroma](https://docs.trychroma.com/getting-started), [guidance](https://github.com/microsoft/guidance), [semantic-kernel](https://github.com/microsoft/semantic-kernel) „ÄÇ\n\n## üé® ÂæÆË∞ÉÊ®°Âûã\n\n`uniem` Êèê‰æõ‰∫ÜÈùûÂ∏∏ÊòìÁî®ÁöÑ finetune Êé•Âè£ÔºåÂá†Ë°å‰ª£Á†ÅÔºåÂç≥ÂàªÈÄÇÈÖçÔºÅ\n\n```python\nfrom datasets import load_dataset\n\nfrom uniem.finetuner import FineTuner\n\ndataset = load_dataset(''shibing624/nli_zh'', ''STS-B'')\n# ÊåáÂÆöËÆ≠ÁªÉÁöÑÊ®°Âûã‰∏∫ m3e-small\nfinetuner = FineTuner.from_pretrained(''moka-ai/m3e-small'', dataset=dataset)\nfinetuner.run(epochs=1)\n```\n\nËØ¶ËßÅ [uniem ÂæÆË∞ÉÊïôÁ®ã](https://github.com/wangyuxinwhy/uniem/blob/main/examples/finetune.ipynb)\n\n<a target="_blank" href="https://colab.research.google.com/github/wangyuxinwhy/uniem/blob/main/examples/finetune.ipynb">\n  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>\n</a>\n\n## ‚ûø ËÆ≠ÁªÉÊñπÊ°à\n\nM3E ‰ΩøÁî® in-batch Ë¥üÈááÊ†∑ÁöÑÂØπÊØîÂ≠¶‰π†ÁöÑÊñπÂºèÂú®Âè•ÂØπÊï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉÔºå‰∏∫‰∫Ü‰øùËØÅ in-batch Ë¥üÈááÊ†∑ÁöÑÊïàÊûúÔºåÊàë‰ª¨‰ΩøÁî® A100 80G Êù•ÊúÄÂ§ßÂåñ batch-sizeÔºåÂπ∂Âú®ÂÖ±ËÆ° 2200W+ ÁöÑÂè•ÂØπÊï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉ‰∫Ü 1 epoch„ÄÇËÆ≠ÁªÉËÑöÊú¨‰ΩøÁî® [uniem](https://github.com/wangyuxinwhy/uniem/blob/main/scripts/train_m3e.py)ÔºåÊÇ®ÂèØ‰ª•Âú®ËøôÈáåÊü•ÁúãÂÖ∑‰ΩìÁªÜËäÇ„ÄÇ\n\n## üåü ÁâπÊÄß\n\n- ‰∏≠ÊñáËÆ≠ÁªÉÈõÜÔºåM3E Âú®Â§ßËßÑÊ®°Âè•ÂØπÊï∞ÊçÆÈõÜ‰∏äÁöÑËÆ≠ÁªÉÔºåÂåÖÂê´‰∏≠ÊñáÁôæÁßëÔºåÈáëËûçÔºåÂåªÁñóÔºåÊ≥ïÂæãÔºåÊñ∞ÈóªÔºåÂ≠¶ÊúØÁ≠âÂ§ö‰∏™È¢ÜÂüüÂÖ±ËÆ° 2200W Âè•ÂØπÊ†∑Êú¨ÔºåÊï∞ÊçÆÈõÜËØ¶ËßÅ [M3E Êï∞ÊçÆÈõÜ](#M3EÊï∞ÊçÆÈõÜ)\n- Ëã±ÊñáËÆ≠ÁªÉÈõÜÔºåM3E ‰ΩøÁî® MEDI 145W Ëã±Êñá‰∏âÂÖÉÁªÑÊï∞ÊçÆÈõÜËøõË°åËÆ≠ÁªÉÔºåÊï∞ÊçÆÈõÜËØ¶ËßÅ [MEDI Êï∞ÊçÆÈõÜ](https://drive.google.com/file/d/1vZ5c2oJNonGOvXzppNg5mHz24O6jcc52/view)ÔºåÊ≠§Êï∞ÊçÆÈõÜÁî± [instructor team](https://github.com/HKUNLP/instructor-embedding) Êèê‰æõ\n- Êåá‰ª§Êï∞ÊçÆÈõÜÔºåM3E ‰ΩøÁî®‰∫Ü 300W + ÁöÑÊåá‰ª§ÂæÆË∞ÉÊï∞ÊçÆÈõÜÔºåËøô‰ΩøÂæó M3E ÂØπÊñáÊú¨ÁºñÁ†ÅÁöÑÊó∂ÂÄôÂèØ‰ª•ÈÅµ‰ªéÊåá‰ª§ÔºåËøôÈÉ®ÂàÜÁöÑÂ∑•‰Ωú‰∏ªË¶ÅË¢´ÂêØÂèë‰∫é [instructor-embedding](https://github.com/HKUNLP/instructor-embedding)\n- Âü∫Á°ÄÊ®°ÂûãÔºåM3E ‰ΩøÁî® hfl ÂÆûÈ™åÂÆ§ÁöÑ [Roberta](https://huggingface.co/hfl/chinese-roberta-wwm-ext) Á≥ªÂàóÊ®°ÂûãËøõË°åËÆ≠ÁªÉÔºåÁõÆÂâçÊèê‰æõ  small Âíå  base ‰∏§‰∏™ÁâàÊú¨ÔºåÂ§ßÂÆ∂ÂàôÈúÄÈÄâÁî®\n- ALL IN ONEÔºåM3E Êó®Âú®Êèê‰æõ‰∏Ä‰∏™ ALL IN ONE ÁöÑÊñáÊú¨ÂµåÂÖ•Ê®°ÂûãÔºå‰∏ç‰ªÖÊîØÊåÅÂêåË¥®Âè•Â≠êÁõ∏‰ººÂ∫¶Âà§Êñ≠ÔºåËøòÊîØÊåÅÂºÇË¥®ÊñáÊú¨Ê£ÄÁ¥¢Ôºå‰Ω†Âè™ÈúÄË¶Å‰∏Ä‰∏™Ê®°ÂûãÂ∞±ÂèØ‰ª•Ë¶ÜÁõñÂÖ®ÈÉ®ÁöÑÂ∫îÁî®Âú∫ÊôØÔºåÊú™Êù•Ëøò‰ºöÊîØÊåÅ‰ª£Á†ÅÊ£ÄÁ¥¢\n\n## üíØ MTEB-zh ËØÑÊµã\n\n- ËØÑÊµãÊ®°ÂûãÔºå[text2vec](https://github.com/shibing624/text2vec), m3e-base, m3e-small, openai text-embedding-ada-002, [DMetaSoul](https://huggingface.co/DMetaSoul/sbert-chinese-general-v2), [UER](https://huggingface.co/uer/sbert-base-chinese-nli), [ErLangShen](https://huggingface.co/IDEA-CCNL/Erlangshen-SimCSE-110M-Chinese)\n- ËØÑÊµãËÑöÊú¨ÔºåÂÖ∑‰ΩìÂèÇËÄÉ [MTEB-zh] (https://github.com/wangyuxinwhy/uniem/blob/main/mteb-zh)\n\n### ÊñáÊú¨ÂàÜÁ±ª\n\n- Êï∞ÊçÆÈõÜÈÄâÊã©ÔºåÈÄâÊã©ÂºÄÊ∫êÂú® HuggingFace ‰∏äÁöÑ 6 ÁßçÊñáÊú¨ÂàÜÁ±ªÊï∞ÊçÆÈõÜÔºåÂåÖÊã¨Êñ∞Èóª„ÄÅÁîµÂïÜËØÑËÆ∫„ÄÅËÇ°Á•®ËØÑËÆ∫„ÄÅÈïøÊñáÊú¨Á≠â\n- ËØÑÊµãÊñπÂºèÔºå‰ΩøÁî® MTEB ÁöÑÊñπÂºèËøõË°åËØÑÊµãÔºåÊä•Âëä Accuracy„ÄÇ\n\n|                   | text2vec | m3e-small | m3e-base | openai | DMetaSoul   | uer     | erlangshen  |\n| ----------------- | -------- | --------- | -------- | ------ | ----------- | ------- | ----------- |\n| TNews             | 0.43     | 0.4443    | **0.4827**   | 0.4594 | 0.3084      | 0.3539  | 0.4361      |\n| JDIphone          | 0.8214   | 0.8293    | **0.8533**   | 0.746  | 0.7972      | 0.8283  | 0.8356      |\n| GubaEastmony      | 0.7472   | 0.712     | 0.7621   | 0.7574 | 0.735       | 0.7534  | **0.7787**      |\n| TYQSentiment      | 0.6099   | 0.6596    | **0.7188**   | 0.68   | 0.6437      | 0.6662  | 0.6444      |\n| StockComSentiment | 0.4307   | 0.4291    | 0.4363   | **0.4819** | 0.4309      | 0.4555  | 0.4482      |\n| IFlyTek           | 0.414    | 0.4263    | 0.4409   | **0.4486** | 0.3969      | 0.3762  | 0.4241      |\n| Average           | 0.5755   | 0.5834    | **0.6157**   | 0.5956 | 0.552016667 | 0.57225 | 0.594516667 |\n\n### Ê£ÄÁ¥¢ÊéíÂ∫è\n\n#### T2Ranking 1W\n\n- Êï∞ÊçÆÈõÜÈÄâÊã©Ôºå‰ΩøÁî® [T2Ranking](https://github.com/THUIR/T2Ranking/tree/main) Êï∞ÊçÆÈõÜÔºåÁî±‰∫é T2Ranking ÁöÑÊï∞ÊçÆÈõÜÂ§™Â§ßÔºåopenai ËØÑÊµãËµ∑Êù•ÁöÑÊó∂Èó¥ÊàêÊú¨Âíå api Ë¥πÁî®Êúâ‰∫õÈ´òÔºåÊâÄ‰ª•Êàë‰ª¨Âè™ÈÄâÊã©‰∫Ü T2Ranking ‰∏≠ÁöÑÂâç 10000 ÁØáÊñáÁ´†\n- ËØÑÊµãÊñπÂºèÔºå‰ΩøÁî® MTEB ÁöÑÊñπÂºèËøõË°åËØÑÊµãÔºåÊä•Âëä map@1, map@10, mrr@1, mrr@10, ndcg@1, ndcg@10\n- Ê≥®ÊÑèÔºÅ‰ªéÂÆûÈ™åÁªìÊûúÂíåËÆ≠ÁªÉÊñπÂºèÊù•ÁúãÔºåÈô§‰∫Ü M3E Ê®°ÂûãÂíå openai Ê®°ÂûãÂ§ñÔºåÂÖ∂‰ΩôÊ®°ÂûãÈÉΩÊ≤°ÊúâÂÅöÊ£ÄÁ¥¢‰ªªÂä°ÁöÑËÆ≠ÁªÉÔºåÊâÄ‰ª•ÁªìÊûú‰ªÖ‰æõÂèÇËÄÉ„ÄÇ\n\n|         | text2vec | openai-ada-002 | m3e-small | m3e-base | DMetaSoul | uer     | erlangshen |\n| ------- | -------- | -------------- | --------- | -------- | --------- | ------- | ---------- |\n| map@1   | 0.4684   | 0.6133         | 0.5574    | **0.626**    | 0.25203   | 0.08647 | 0.25394    |\n| map@10  | 0.5877   | 0.7423         | 0.6878    | **0.7656**   | 0.33312   | 0.13008 | 0.34714    |\n| mrr@1   | 0.5345   | 0.6931         | 0.6324    | **0.7047**   | 0.29258   | 0.10067 | 0.29447    |\n| mrr@10  | 0.6217   | 0.7668         | 0.712     | **0.7841**   | 0.36287   | 0.14516 | 0.3751     |\n| ndcg@1  | 0.5207   | 0.6764         | 0.6159    | **0.6881**   | 0.28358   | 0.09748 | 0.28578    |\n| ndcg@10 | 0.6346   | 0.7786         | 0.7262    | **0.8004**   | 0.37468   | 0.15783 | 0.39329    |\n\n#### T2Ranking\n\n- Êï∞ÊçÆÈõÜÈÄâÊã©Ôºå‰ΩøÁî® T2RankingÔºåÂà®Èô§ openai-ada-002 Ê®°ÂûãÂêéÔºåÊàë‰ª¨ÂØπÂâ©‰ΩôÁöÑ‰∏â‰∏™Ê®°ÂûãÔºåËøõË°å T2Ranking 10W Âíå T2Ranking 50W ÁöÑËØÑÊµã„ÄÇÔºàT2Ranking ËØÑÊµãÂ§™ËÄóÂÜÖÂ≠ò‰∫Ü... 128G ÈÉΩ‰∏çË°åÔºâ\n- ËØÑÊµãÊñπÂºèÔºå‰ΩøÁî® MTEB ÁöÑÊñπÂºèËøõË°åËØÑÊµãÔºåÊä•Âëä ndcg@10\n\n|         | text2vec | m3e-small | m3e-base |\n| ------- | -------- | --------- | -------- |\n| t2r-1w  | 0.6346   | 0.72621   | **0.8004**   |\n| t2r-10w | 0.44644  | 0.5251    | **0.6263**   |\n| t2r-50w | 0.33482  | 0.38626   | **0.47364**  |\n\nËØ¥ÊòéÔºö\n- Ê£ÄÁ¥¢ÊéíÂ∫èÂØπ‰∫é text2vec Âπ∂‰∏çÂÖ¨Âπ≥ÔºåÂõ†‰∏∫ text2vec Âú®ËÆ≠ÁªÉÁöÑÊó∂ÂÄôÊ≤°Êúâ‰ΩøÁî®ËøáÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÁöÑÊï∞ÊçÆÈõÜÔºåÊâÄ‰ª•Ê≤°ÊúâÂäûÊ≥ïÂæàÂ•ΩÁöÑÂÆåÊàêÊ£ÄÁ¥¢‰ªªÂä°‰πüÊòØÊ≠£Â∏∏ÁöÑ„ÄÇ\n\n## üìÇ M3EÊï∞ÊçÆÈõÜ\n\nÂ¶ÇÊûúÊÇ®ÊÉ≥Ë¶Å‰ΩøÁî®Ëøô‰∫õÊï∞ÊçÆÈõÜÔºå‰Ω†ÂèØ‰ª•Âú® [uniem process_zh_datasets](https://github.com/wangyuxinwhy/uniem/blob/main/scripts/process_zh_datasets.py) ‰∏≠ÊâæÂà∞Âä†ËΩΩ huggingface Êï∞ÊçÆÈõÜÁöÑËÑöÊú¨ÔºåÈùû huggingface Êï∞ÊçÆÈõÜÈúÄË¶ÅÊÇ®Ê†πÊçÆ‰∏ãÊñπÊèê‰æõÁöÑÈìæÊé•Ëá™Ë°å‰∏ãËΩΩÂíåÂ§ÑÁêÜ„ÄÇ\n\n| Êï∞ÊçÆÈõÜÂêçÁß∞           | È¢ÜÂüü | Êï∞Èáè      | ‰ªªÂä°Á±ªÂûã          | Prompt | Ë¥®Èáè | Êï∞ÊçÆÊèê‰æõËÄÖ                                                   | ËØ¥Êòé                                                         | ÊòØÂê¶ÂºÄÊ∫ê/Á†îÁ©∂‰ΩøÁî® | ÊòØÂê¶ÂïÜÁî® | ËÑöÊú¨ | Done | URL                                                          | ÊòØÂê¶ÂêåË¥® |\n| -------------------- | ---- | --------- | ----------------- | ------ | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ----------------- | -------- | ---- | ---- | ------------------------------------------------------------ | -------- |\n| cmrc2018             | ÁôæÁßë | 14,363    | ÈóÆÁ≠î              | ÈóÆÁ≠î   | ‰ºò   | Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, Guoping Hu | https://github.com/ymcui/cmrc2018/blob/master/README_CN.md ‰∏ìÂÆ∂Ê†áÊ≥®ÁöÑÂü∫‰∫éÁª¥Âü∫ÁôæÁßëÁöÑ‰∏≠ÊñáÈòÖËØªÁêÜËß£Êï∞ÊçÆÈõÜÔºåÂ∞ÜÈóÆÈ¢òÂíå‰∏ä‰∏ãÊñáËßÜ‰∏∫Ê≠£‰æã | ÊòØ                | Âê¶       | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/cmrc2018                     | Âê¶       |\n| belle_2m             | ÁôæÁßë | 2,000,000 | Êåá‰ª§ÂæÆË∞É          | Êó†     | ‰ºò   | LianjiaTech/BELLE                                            | belle ÁöÑÊåá‰ª§ÂæÆË∞ÉÊï∞ÊçÆÈõÜÔºå‰ΩøÁî® self instruct ÊñπÊ≥ïÂü∫‰∫é gpt3.5 ÁîüÊàê | ÊòØ                | Âê¶       | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/BelleGroup/train_2M_CN       | Âê¶       |\n| firefily             | ÁôæÁßë | 1,649,399 | Êåá‰ª§ÂæÆË∞É          | Êó†     | ‰ºò   | YeungNLP                                                     | FireflyÔºàÊµÅËê§Ôºâ ÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑ‰∏≠ÊñáÂØπËØùÂºèÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºå‰ΩøÁî®Êåá‰ª§ÂæÆË∞ÉÔºàInstruction TuningÔºâÂú®‰∏≠ÊñáÊï∞ÊçÆÈõÜ‰∏äËøõË°åË∞É‰ºò„ÄÇ‰ΩøÁî®‰∫ÜËØçË°®Ë£ÅÂâ™„ÄÅZeROÁ≠âÊäÄÊúØÔºåÊúâÊïàÈôç‰ΩéÊòæÂ≠òÊ∂àËÄóÂíåÊèêÈ´òËÆ≠ÁªÉÊïàÁéá„ÄÇ Âú®ËÆ≠ÁªÉ‰∏≠ÔºåÊàë‰ª¨‰ΩøÁî®‰∫ÜÊõ¥Â∞èÁöÑÊ®°ÂûãÂèÇÊï∞ÈáèÔºå‰ª•ÂèäÊõ¥Â∞ëÁöÑËÆ°ÁÆóËµÑÊ∫ê„ÄÇ | Êú™ËØ¥Êòé            | Êú™ËØ¥Êòé   | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M  | Âê¶       |\n| alpaca_gpt4          | ÁôæÁßë | 48,818    | Êåá‰ª§ÂæÆË∞É          | Êó†     | ‰ºò   | Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao | Êú¨Êï∞ÊçÆÈõÜÊòØÂèÇËÄÉAlpacaÊñπÊ≥ïÂü∫‰∫éGPT4ÂæóÂà∞ÁöÑself-instructÊï∞ÊçÆÔºåÁ∫¶5‰∏áÊù°„ÄÇ | ÊòØ                | Âê¶       | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/shibing624/alpaca-zh         | Âê¶       |\n| zhihu_kol            | ÁôæÁßë | 1,006,218 | ÈóÆÁ≠î              | ÈóÆÁ≠î   | ‰ºò   | wangrui6                                                     | Áü•‰πéÈóÆÁ≠î                                                     | Êú™ËØ¥Êòé            | Êú™ËØ¥Êòé   | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/wangrui6/Zhihu-KOL           | Âê¶       |\n| hc3_chinese          | ÁôæÁßë | 39,781    | ÈóÆÁ≠î              | ÈóÆÁ≠î   | ËâØ   | Hello-SimpleAI                                               | ÈóÆÁ≠îÊï∞ÊçÆÔºåÂåÖÊã¨‰∫∫Â∑•ÂõûÁ≠îÂíå GPT ÂõûÁ≠î                            | ÊòØ                | Êú™ËØ¥Êòé   | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/Hello-SimpleAI/HC3-Chinese   | Âê¶       |\n| amazon_reviews_multi | ÁîµÂïÜ | 210,000   | ÈóÆÁ≠î ÊñáÊú¨ÂàÜÁ±ª     | ÊëòË¶Å   | ‰ºò   | ‰∫öÈ©¨ÈÄä                                                       | ‰∫öÈ©¨ÈÄä‰∫ßÂìÅËØÑËÆ∫Êï∞ÊçÆÈõÜ                                         | ÊòØ                | Âê¶       | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/amazon_reviews_multi/viewer/zh/train?row=8 | Âê¶       |\n| mlqa                 | ÁôæÁßë | 85,853    | ÈóÆÁ≠î              | ÈóÆÁ≠î   | ËâØ   | patrickvonplaten                                             | ‰∏Ä‰∏™Áî®‰∫éËØÑ‰º∞Ë∑®ËØ≠Ë®ÄÈóÆÁ≠îÊÄßËÉΩÁöÑÂü∫ÂáÜÊï∞ÊçÆÈõÜ                       | ÊòØ                | Êú™ËØ¥Êòé   | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/mlqa/viewer/mlqa-translate-train.zh/train?p=2 | Âê¶       |\n| xlsum                | Êñ∞Èóª | 93,404    | ÊëòË¶Å              | ÊëòË¶Å   | ËâØ   | BUET CSE NLP Group                                           | BBCÁöÑ‰∏ì‰∏öÊ≥®ÈáäÊñáÁ´†ÊëòË¶ÅÂØπ                                      | ÊòØ                | Âê¶       | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/csebuetnlp/xlsum/viewer/chinese_simplified/train?row=259 | Âê¶       |\n| ocnli                | Âè£ËØ≠ | 17,726    | Ëá™ÁÑ∂ËØ≠Ë®ÄÊé®ÁêÜ      | Êé®ÁêÜ   | ËâØ   | Thomas Wolf                                                  | Ëá™ÁÑ∂ËØ≠Ë®ÄÊé®ÁêÜÊï∞ÊçÆÈõÜ                                           | ÊòØ                | Âê¶       | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/clue/viewer/ocnli            | ÊòØ       |\n| BQ                   | ÈáëËûç | 60,000    | ÊñáÊú¨ÂàÜÁ±ª          | Áõ∏‰ºº   | ËâØ   | Intelligent Computing Research Center, Harbin Institute of Technology(Shenzhen) | http://icrc.hitsz.edu.cn/info/1037/1162.htm BQ ËØ≠ÊñôÂ∫ìÂåÖÂê´Êù•Ëá™ÁΩë‰∏äÈì∂Ë°åËá™ÂÆö‰πâÊúçÂä°Êó•ÂøóÁöÑ 120Ôºå000 ‰∏™ÈóÆÈ¢òÂØπ„ÄÇÂÆÉÂàÜ‰∏∫‰∏âÈÉ®ÂàÜÔºö100Ôºå000 ÂØπÁî®‰∫éËÆ≠ÁªÉÔºå10Ôºå000 ÂØπÁî®‰∫éÈ™åËØÅÔºå10Ôºå000 ÂØπÁî®‰∫éÊµãËØï„ÄÇ Êï∞ÊçÆÊèê‰æõËÄÖÔºö ÂìàÂ∞îÊª®Â∑•‰∏öÂ§ßÂ≠¶ÔºàÊ∑±Âú≥ÔºâÊô∫ËÉΩËÆ°ÁÆóÁ†îÁ©∂‰∏≠ÂøÉ | ÊòØ                | Âê¶       | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/shibing624/nli_zh/viewer/BQ  | ÊòØ       |\n| lcqmc                | Âè£ËØ≠ | 149,226   | ÊñáÊú¨ÂàÜÁ±ª          | Áõ∏‰ºº   | ËâØ   | Ming Xu                                                      | ÂìàÂ∑•Â§ßÊñáÊú¨ÂåπÈÖçÊï∞ÊçÆÈõÜÔºåLCQMC ÊòØÂìàÂ∞îÊª®Â∑•‰∏öÂ§ßÂ≠¶Âú®Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂõΩÈôÖÈ°∂‰ºö COLING2018 ÊûÑÂª∫ÁöÑÈóÆÈ¢òËØ≠‰πâÂåπÈÖçÊï∞ÊçÆÈõÜÔºåÂÖ∂ÁõÆÊ†áÊòØÂà§Êñ≠‰∏§‰∏™ÈóÆÈ¢òÁöÑËØ≠‰πâÊòØÂê¶Áõ∏Âêå | ÊòØ                | Âê¶       | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/shibing624/nli_zh/viewer/LCQMC/train | ÊòØ       |\n| paws-x               | ÁôæÁßë | 23,576    | ÊñáÊú¨ÂàÜÁ±ª          | Áõ∏‰ºº   | ‰ºò   | Bhavitvya Malik                                              | PAWS Wiki‰∏≠ÁöÑÁ§∫‰æã                                            | ÊòØ                | ÊòØ       | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/paws-x/viewer/zh/train       | ÊòØ       |\n| wiki_atomic_edit     | ÁôæÁßë | 1,213,780 | Âπ≥Ë°åËØ≠‰πâ          | Áõ∏‰ºº   | ‰ºò   | abhishek thakur                                              | Âü∫‰∫é‰∏≠ÊñáÁª¥Âü∫ÁôæÁßëÁöÑÁºñËæëËÆ∞ÂΩïÊî∂ÈõÜÁöÑÊï∞ÊçÆÈõÜ                       | Êú™ËØ¥Êòé            | Êú™ËØ¥Êòé   | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/wiki_atomic_edits            | ÊòØ       |\n| chatmed_consult      | ÂåªËçØ | 549,326   | ÈóÆÁ≠î              | ÈóÆÁ≠î   | ‰ºò   | Wei Zhu                                                      | ÁúüÂÆû‰∏ñÁïåÁöÑÂåªÂ≠¶Áõ∏ÂÖ≥ÁöÑÈóÆÈ¢òÔºå‰ΩøÁî® gpt3.5 ËøõË°åÂõûÁ≠î               | ÊòØ                | Âê¶       | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/michaelwzhu/ChatMed_Consult_Dataset | Âê¶       |\n| webqa                | ÁôæÁßë | 42,216    | ÈóÆÁ≠î              | ÈóÆÁ≠î   | ‰ºò   | suolyer                                                      | ÁôæÂ∫¶‰∫é2016Âπ¥ÂºÄÊ∫êÁöÑÊï∞ÊçÆÈõÜÔºåÊï∞ÊçÆÊù•Ëá™‰∫éÁôæÂ∫¶Áü•ÈÅìÔºõÊ†ºÂºè‰∏∫‰∏Ä‰∏™ÈóÆÈ¢òÂ§öÁØáÊÑèÊÄùÂü∫Êú¨‰∏ÄËá¥ÁöÑÊñáÁ´†ÔºåÂàÜ‰∏∫‰∫∫‰∏∫Ê†áÊ≥®‰ª•ÂèäÊµèËßàÂô®Ê£ÄÁ¥¢ÔºõÊï∞ÊçÆÊï¥‰ΩìË¥®Èáè‰∏≠ÔºåÂõ†‰∏∫Ê∑∑Âêà‰∫ÜÂæàÂ§öÊ£ÄÁ¥¢ËÄåÊù•ÁöÑÊñáÁ´† | ÊòØ                | Êú™ËØ¥Êòé   | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/suolyer/webqa/viewer/suolyer--webqa/train?p=3 | Âê¶       |\n| dureader_robust      | ÁôæÁßë | 65,937    | Êú∫Âô®ÈòÖËØªÁêÜËß£ ÈóÆÁ≠î | ÈóÆÁ≠î   | ‰ºò   | ÁôæÂ∫¶                                                         | DuReader robustÊó®Âú®Âà©Áî®ÁúüÂÆûÂ∫îÁî®‰∏≠ÁöÑÊï∞ÊçÆÊ†∑Êú¨Êù•Ë°°ÈáèÈòÖËØªÁêÜËß£Ê®°ÂûãÁöÑÈ≤ÅÊ£íÊÄßÔºåËØÑÊµãÊ®°ÂûãÁöÑËøáÊïèÊÑüÊÄß„ÄÅËøáÁ®≥ÂÆöÊÄß‰ª•ÂèäÊ≥õÂåñËÉΩÂäõÔºåÊòØÈ¶ñ‰∏™‰∏≠ÊñáÈòÖËØªÁêÜËß£È≤ÅÊ£íÊÄßÊï∞ÊçÆÈõÜ„ÄÇ | ÊòØ                | ÊòØ       | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/PaddlePaddle/dureader_robust/viewer/plain_text/train?row=96 | Âê¶       |\n| csl                  | Â≠¶ÊúØ | 395,927   | ËØ≠Êñô              | ÊëòË¶Å   | ‰ºò   | Yudong Li, Yuqing Zhang, Zhe Zhao, Linlin Shen, Weijie Liu, Weiquan Mao and Hui Zhang | Êèê‰æõÈ¶ñ‰∏™‰∏≠ÊñáÁßëÂ≠¶ÊñáÁåÆÊï∞ÊçÆÈõÜÔºàCSLÔºâÔºåÂåÖÂê´ 396,209 ÁØá‰∏≠ÊñáÊ†∏ÂøÉÊúüÂàäËÆ∫ÊñáÂÖÉ‰ø°ÊÅØ ÔºàÊ†áÈ¢ò„ÄÅÊëòË¶Å„ÄÅÂÖ≥ÈîÆËØç„ÄÅÂ≠¶Áßë„ÄÅÈó®Á±ªÔºâ„ÄÇCSL Êï∞ÊçÆÈõÜÂèØ‰ª•‰Ωú‰∏∫È¢ÑËÆ≠ÁªÉËØ≠ÊñôÔºå‰πüÂèØ‰ª•ÊûÑÂª∫ËÆ∏Â§öNLP‰ªªÂä°Ôºå‰æãÂ¶ÇÊñáÊú¨ÊëòË¶ÅÔºàÊ†áÈ¢òÈ¢ÑÊµãÔºâ„ÄÅ ÂÖ≥ÈîÆËØçÁîüÊàêÂíåÊñáÊú¨ÂàÜÁ±ªÁ≠â„ÄÇ | ÊòØ                | ÊòØ       | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/neuclir/csl                  | Âê¶       |\n| miracl-corpus        | ÁôæÁßë | 4,934,368 | ËØ≠Êñô              | ÊëòË¶Å   | ‰ºò   | MIRACL                                                       | The corpus for each language is prepared from a Wikipedia dump, where we keep only the plain text and discard images, tables, etc. Each article is segmented into multiple passages using WikiExtractor based on natural discourse units (e.g., \n\n in the wiki markup). Each of these passages comprises a "document" or unit of retrieval. We preserve the Wikipedia article title of each passage. | ÊòØ                | ÊòØ       | ÊòØ   | ÊòØ   | https://huggingface.co/datasets/miracl/miracl-corpus         | Âê¶       |\n| lawzhidao            | Ê≥ïÂæã | 36,368    | ÈóÆÁ≠î              | ÈóÆÁ≠î   | ‰ºò   | ÂíåÈ≤∏Á§æÂå∫-Ustinian                                            | ÁôæÂ∫¶Áü•ÈÅìÊ∏ÖÊ¥óÂêéÁöÑÊ≥ïÂæãÈóÆÁ≠î                                     | ÊòØ                | ÊòØ       | Âê¶   | ÊòØ   | https://www.heywhale.com/mw/dataset/5e953ca8e7ec38002d02fca7/content | Âê¶       |\n| CINLID               | ÊàêËØ≠ | 34,746    | Âπ≥Ë°åËØ≠‰πâ          | Áõ∏‰ºº   | ‰ºò   | È´òÈïøÂÆΩ                                                       | ‰∏≠ÊñáÊàêËØ≠ËØ≠‰πâÊé®ÁêÜÊï∞ÊçÆÈõÜÔºàChinese Idioms Natural Language Inference DatasetÔºâÊî∂ÈõÜ‰∫Ü106832Êù°Áî±‰∫∫Â∑•Êí∞ÂÜôÁöÑÊàêËØ≠ÂØπÔºàÂê´Â∞ëÈáèÊ≠áÂêéËØ≠„ÄÅ‰øóËØ≠Á≠âÁü≠ÊñáÊú¨ÔºâÔºåÈÄöËøá‰∫∫Â∑•Ê†áÊ≥®ÁöÑÊñπÂºèËøõË°åÂπ≥Ë°°ÂàÜÁ±ªÔºåÊ†áÁ≠æ‰∏∫entailment„ÄÅcontradictionÂíåneutralÔºåÊîØÊåÅËá™ÁÑ∂ËØ≠Ë®ÄÊé®ÁêÜÔºàNLIÔºâÁöÑ‰ªªÂä°„ÄÇ | ÊòØ                | Âê¶       | Âê¶   | ÊòØ   | https://www.luge.ai/#/luge/dataDetail?id=39                  | ÊòØ       |\n| DuSQL                | SQL  | 25,003    | NL2SQL            | SQL    | ‰ºò   | ÁôæÂ∫¶                                                         | DuSQLÊòØ‰∏Ä‰∏™Èù¢ÂêëÂÆûÈôÖÂ∫îÁî®ÁöÑÊï∞ÊçÆÈõÜÔºåÂåÖÂê´200‰∏™Êï∞ÊçÆÂ∫ìÔºåË¶ÜÁõñ‰∫Ü164‰∏™È¢ÜÂüüÔºåÈóÆÈ¢òË¶ÜÁõñ‰∫ÜÂåπÈÖç„ÄÅËÆ°ÁÆó„ÄÅÊé®ÁêÜÁ≠âÂÆûÈôÖÂ∫îÁî®‰∏≠Â∏∏ËßÅÂΩ¢Âºè„ÄÇËØ•Êï∞ÊçÆÈõÜÊõ¥Ë¥¥ËøëÁúüÂÆûÂ∫îÁî®Âú∫ÊôØÔºåË¶ÅÊ±ÇÊ®°ÂûãÈ¢ÜÂüüÊó†ÂÖ≥„ÄÅÈóÆÈ¢òÊó†ÂÖ≥Ôºå‰∏îÂÖ∑Â§áËÆ°ÁÆóÊé®ÁêÜÁ≠âËÉΩÂäõ„ÄÇ | ÊòØ                | Âê¶       | Âê¶   | ÊòØ   | https://www.luge.ai/#/luge/dataDetail?id=13                  | Âê¶       |\n| Zhuiyi-NL2SQL        | SQL  | 45,918    | NL2SQL            | SQL    | ‰ºò   | ËøΩ‰∏ÄÁßëÊäÄ Âàò‰∫ëÂ≥∞                                              | NL2SQLÊòØ‰∏Ä‰∏™Â§öÈ¢ÜÂüüÁöÑÁÆÄÂçïÊï∞ÊçÆÈõÜÔºåÂÖ∂‰∏ªË¶ÅÂåÖÂê´ÂåπÈÖçÁ±ªÂûãÈóÆÈ¢ò„ÄÇËØ•Êï∞ÊçÆÈõÜ‰∏ªË¶ÅÈ™åËØÅÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÔºåÂÖ∂Ë¶ÅÊ±ÇÊ®°ÂûãÂÖ∑ÊúâËæÉÂº∫ÁöÑÈ¢ÜÂüüÊ≥õÂåñËÉΩÂäõ„ÄÅÈóÆÈ¢òÊ≥õÂåñËÉΩÂäõ„ÄÇ | ÊòØ                | Âê¶       | Âê¶   | ÊòØ   | https://www.luge.ai/#/luge/dataDetail?id=12                  | Âê¶       |\n| Cspider              | SQL  | 7,785     | NL2SQL            | SQL    | ‰ºò   | Ë•øÊπñÂ§ßÂ≠¶ Âº†Â≤≥                                                | CSpiderÊòØ‰∏Ä‰∏™Â§öËØ≠Ë®ÄÊï∞ÊçÆÈõÜÔºåÂÖ∂ÈóÆÈ¢ò‰ª•‰∏≠ÊñáË°®ËææÔºåÊï∞ÊçÆÂ∫ì‰ª•Ëã±ÊñáÂ≠òÂÇ®ÔºåËøôÁßçÂèåËØ≠Ê®°ÂºèÂú®ÂÆûÈôÖÂ∫îÁî®‰∏≠‰πüÈùûÂ∏∏Â∏∏ËßÅÔºåÂ∞§ÂÖ∂ÊòØÊï∞ÊçÆÂ∫ìÂºïÊìéÂØπ‰∏≠ÊñáÊîØÊåÅ‰∏çÂ•ΩÁöÑÊÉÖÂÜµ‰∏ã„ÄÇËØ•Êï∞ÊçÆÈõÜË¶ÅÊ±ÇÊ®°ÂûãÈ¢ÜÂüüÊó†ÂÖ≥„ÄÅÈóÆÈ¢òÊó†ÂÖ≥Ôºå‰∏îËÉΩÂ§üÂÆûÁé∞Â§öËØ≠Ë®ÄÂåπÈÖç„ÄÇ | ÊòØ                | Âê¶       | Âê¶   | ÊòØ   | https://www.luge.ai/#/luge/dataDetail?id=11                  | Âê¶       |\n| news2016zh           | Êñ∞Èóª | 2,507,549 | ËØ≠Êñô              | ÊëòË¶Å   | ËâØ   | Bright Xu                                                    | ÂåÖÂê´‰∫Ü250‰∏áÁØáÊñ∞Èóª„ÄÇÊñ∞ÈóªÊù•Ê∫êÊ∂µÁõñ‰∫Ü6.3‰∏á‰∏™Â™í‰ΩìÔºåÂê´Ê†áÈ¢ò„ÄÅÂÖ≥ÈîÆËØç„ÄÅÊèèËø∞„ÄÅÊ≠£Êñá„ÄÇ | ÊòØ                | ÊòØ       | Âê¶   | ÊòØ   | https://github.com/brightmart/nlp_chinese_corpus             | Âê¶       |\n| baike2018qa          | ÁôæÁßë | 1,470,142 | ÈóÆÁ≠î              | ÈóÆÁ≠î   | ËâØ   | Bright Xu                                                    | Âê´Êúâ150‰∏á‰∏™È¢ÑÂÖàËøáÊª§ËøáÁöÑ„ÄÅÈ´òË¥®ÈáèÈóÆÈ¢òÂíåÁ≠îÊ°àÔºåÊØè‰∏™ÈóÆÈ¢òÂ±û‰∫é‰∏Ä‰∏™Á±ªÂà´„ÄÇÊÄªÂÖ±Êúâ492‰∏™Á±ªÂà´ÔºåÂÖ∂‰∏≠È¢ëÁéáËææÂà∞ÊàñË∂ÖËøá10Ê¨°ÁöÑÁ±ªÂà´Êúâ434‰∏™„ÄÇ | ÊòØ                | ÊòØ       | Âê¶   | ÊòØ   | https://github.com/brightmart/nlp_chinese_corpus             | Âê¶       |\n| webtext2019zh        | ÁôæÁßë | 4,258,310 | ÈóÆÁ≠î              | ÈóÆÁ≠î   | ‰ºò   | Bright Xu                                                    | Âê´Êúâ410‰∏á‰∏™È¢ÑÂÖàËøáÊª§ËøáÁöÑ„ÄÅÈ´òË¥®ÈáèÈóÆÈ¢òÂíåÂõûÂ§ç„ÄÇÊØè‰∏™ÈóÆÈ¢òÂ±û‰∫é‰∏Ä‰∏™„ÄêËØùÈ¢ò„ÄëÔºåÊÄªÂÖ±Êúâ2.8‰∏á‰∏™ÂêÑÂºèËØùÈ¢òÔºåËØùÈ¢òÂåÖÁΩó‰∏áË±°„ÄÇ | ÊòØ                | ÊòØ       | Âê¶   | ÊòØ   | https://github.com/brightmart/nlp_chinese_corpus             | Âê¶       |\n| SimCLUE              | ÁôæÁßë | 775,593   | Âπ≥Ë°åËØ≠‰πâ          | Áõ∏‰ºº   | ËâØ   | Êï∞ÊçÆÈõÜÂêàÔºåËØ∑Âú® simCLUE ‰∏≠Êü•Áúã                                | Êï¥Âêà‰∫Ü‰∏≠ÊñáÈ¢ÜÂüüÁªùÂ§ßÂ§öÊï∞ÂèØÁî®ÁöÑÂºÄÊ∫êÁöÑËØ≠‰πâÁõ∏‰ººÂ∫¶ÂíåËá™ÁÑ∂ËØ≠Ë®ÄÊé®ÁêÜÁöÑÊï∞ÊçÆÈõÜÔºåÂπ∂ÈáçÊñ∞ÂÅö‰∫ÜÊï∞ÊçÆÊãÜÂàÜÂíåÊï¥ÁêÜ„ÄÇ | ÊòØ                | Âê¶       | Âê¶   | ÊòØ   | https://github.com/CLUEbenchmark/SimCLUE                     | ÊòØ       |\n| Chinese-SQuAD        | Êñ∞Èóª | 76,449    | Êú∫Âô®ÈòÖËØªÁêÜËß£      | ÈóÆÁ≠î   | ‰ºò   | junzeng-pluto                                                | ‰∏≠ÊñáÊú∫Âô®ÈòÖËØªÁêÜËß£Êï∞ÊçÆÈõÜÔºåÈÄöËøáÊú∫Âô®ÁøªËØëÂä†‰∫∫Â∑•Ê†°Ê≠£ÁöÑÊñπÂºè‰ªéÂéüÂßãSquadËΩ¨Êç¢ËÄåÊù• | ÊòØ                | Âê¶       | Âê¶   | ÊòØ   | https://github.com/pluto-junzeng/ChineseSquad                | Âê¶       |\n\n## üóìÔ∏è ËÆ°ÂàíË°®\n\n- [x] ÂÆåÊàê MTEB ‰∏≠ÊñáËØÑÊµã BenchMark, [MTEB-zh](https://github.com/wangyuxinwhy/uniem/tree/main/mteb-zh)\n- [x] ÂÆåÊàê Large Ê®°ÂûãÁöÑËÆ≠ÁªÉÂíåÂºÄÊ∫ê\n- [x] ÂÆåÊàê Finetuner ÔºåÂÖÅËÆ∏Êõ¥‰ºòÈõÖÁöÑÂæÆË∞É\n- [ ] ÂÆåÊàêÊîØÊåÅ‰ª£Á†ÅÊ£ÄÁ¥¢ÁöÑÊ®°Âûã\n- [ ] ÂØπ M3E Êï∞ÊçÆÈõÜËøõË°åÊ∏ÖÊ¥óÔºå‰øùÁïôÈ´òË¥®ÈáèÁöÑÈÉ®ÂàÜÔºåÁªÑÊàê m3e-hqÔºåÂπ∂Âú® huggingface ‰∏äÂºÄÊ∫ê\n- [ ] Âú® m3e-hq ÁöÑÊï∞ÊçÆÈõÜ‰∏äË°•ÂÖÖ hard negative ÁöÑÊ†∑Êú¨ÂèäÁõ∏‰ººÂ∫¶ÂàÜÊï∞ÔºåÁªÑÊàê m3e-hq-with-scoreÔºåÂπ∂Âú® huggingface ‰∏äÂºÄÊ∫ê\n- [ ] Âú® m3e-hq-with-score ‰∏äÈÄöËøá [cosent loss](https://github.com/wangyuxinwhy/uniem/blob/main/uniem/criteria.py#LL24C39-L24C39) loss ËøõË°åËÆ≠ÁªÉÂπ∂ÂºÄÊ∫êÊ®°ÂûãÔºåCoSent ÂéüÁêÜÂèÇËÄÉËøôÁØá[ÂçöÂÆ¢](https://kexue.fm/archives/8847)\n- [ ] ÂºÄÊ∫êÂïÜÁî®ÁâàÊú¨ÁöÑ M3E models\n\n## üôè Ëá¥Ë∞¢\n\nÊÑüË∞¢ÂºÄÊ∫êÁ§æÂå∫Êèê‰æõÁöÑ‰∏≠ÊñáËØ≠ÊñôÔºåÊÑüË∞¢ÊâÄÊúâÂú®Ê≠§Â∑•‰Ωú‰∏≠Êèê‰æõÂ∏ÆÂä©ÁöÑ‰∫∫‰ª¨ÔºåÂ∏åÊúõ‰∏≠ÊñáÁ§æÂå∫Ë∂äÊù•Ë∂äÂ•ΩÔºåÂÖ±ÂãâÔºÅ\n\n## üìú License\n\nM3E models ‰ΩøÁî®ÁöÑÊï∞ÊçÆÈõÜ‰∏≠ÂåÖÊã¨Â§ßÈáèÈùûÂïÜÁî®ÁöÑÊï∞ÊçÆÈõÜÔºåÊâÄ‰ª• M3E models ‰πüÊòØÈùûÂïÜÁî®ÁöÑÔºå‰ªÖ‰æõÁ†îÁ©∂‰ΩøÁî®„ÄÇ‰∏çËøáÊàë‰ª¨Â∑≤ÁªèÂú® M3E Êï∞ÊçÆÈõÜ‰∏äÊ†áËØÜ‰∫ÜÂïÜÁî®ÂíåÈùûÂïÜÁî®ÁöÑÊï∞ÊçÆÈõÜÔºåÊÇ®ÂèØ‰ª•Ê†πÊçÆËá™Â∑±ÁöÑÈúÄÊ±ÇËá™Ë°åËÆ≠ÁªÉ„ÄÇ\n\n## Citation\nPlease cite this model using the following format:\n```\n  @software {Moka Massive Mixed Embedding,  \n  author = {Wang Yuxin,Sun Qingxuan,He sicheng},  \n  title = {M3E: Moka Massive Mixed Embedding Model},  \n  year = {2023}\n  }\n```', '{"pipeline_tag":null,"library_name":"sentence-transformers","framework":"sentence-transformers","params":102268160,"storage_bytes":818238909,"files_count":12,"spaces_count":29,"gated":false,"private":false,"config":{"architectures":["BertModel"],"model_type":"bert","tokenizer_config":{"cls_token":"[CLS]","mask_token":"[MASK]","pad_token":"[PAD]","sep_token":"[SEP]","unk_token":"[UNK]"}}}', '[]', '[{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"},{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"},{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"},{"type":"has_code","target_id":"github:microsoft:guidance","source_url":"https://github.com/microsoft/guidance"},{"type":"has_code","target_id":"github:microsoft:semantic-kernel","source_url":"https://github.com/microsoft/semantic-kernel"},{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"},{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"},{"type":"has_code","target_id":"github:HKUNLP:instructor-embedding","source_url":"https://github.com/HKUNLP/instructor-embedding"},{"type":"has_code","target_id":"github:HKUNLP:instructor-embedding","source_url":"https://github.com/HKUNLP/instructor-embedding"},{"type":"has_code","target_id":"github:shibing624:text2vec","source_url":"https://github.com/shibing624/text2vec"},{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"},{"type":"has_code","target_id":"github:THUIR:T2Ranking","source_url":"https://github.com/THUIR/T2Ranking"},{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"},{"type":"has_code","target_id":"github:ymcui:cmrc2018","source_url":"https://github.com/ymcui/cmrc2018"},{"type":"has_code","target_id":"github:brightmart:nlp_chinese_corpus","source_url":"https://github.com/brightmart/nlp_chinese_corpus"},{"type":"has_code","target_id":"github:brightmart:nlp_chinese_corpus","source_url":"https://github.com/brightmart/nlp_chinese_corpus"},{"type":"has_code","target_id":"github:brightmart:nlp_chinese_corpus","source_url":"https://github.com/brightmart/nlp_chinese_corpus"},{"type":"has_code","target_id":"github:CLUEbenchmark:SimCLUE","source_url":"https://github.com/CLUEbenchmark/SimCLUE"},{"type":"has_code","target_id":"github:pluto-junzeng:ChineseSquad","source_url":"https://github.com/pluto-junzeng/ChineseSquad"},{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"},{"type":"has_code","target_id":"github:wangyuxinwhy:uniem","source_url":"https://github.com/wangyuxinwhy/uniem"}]', NULL, NULL, 'pending', 69.9, '358e1476a8c12009fb539a9716e5d7f6', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-ByteDance-AnimateDiff-Lightning', 'huggingface--bytedance--animatediff-lightning', 'AnimateDiff-Lightning', 'ByteDance', '--- license: creativeml-openrail-m tags: - text-to-video - stable-diffusion - animatediff library_name: diffusers inference: false --- <video src=''https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/animatediff_lightning_samples_t2v.mp4'' width="100%" autoplay muted loop playsinline style=''margin:0''></video> <video src=''https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/animatediff_lightning_samples_v2v.mp4'' width="100%" autoplay muted loop playsinline style=''m...', '["diffusers","text-to-video","stable-diffusion","animatediff","arxiv:2403.12706","license:creativeml-openrail-m","region:us"]', 'text-to-video', 974, 48133, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/ByteDance/AnimateDiff-Lightning","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n- text-to-video\n- stable-diffusion\n- animatediff\nlibrary_name: diffusers\ninference: false\n---\n# AnimateDiff-Lightning\n\n<video src=''https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/animatediff_lightning_samples_t2v.mp4'' width="100%" autoplay muted loop playsinline style=''margin:0''></video>\n<video src=''https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/animatediff_lightning_samples_v2v.mp4'' width="100%" autoplay muted loop playsinline style=''margin:0''></video>\n\nAnimateDiff-Lightning is a lightning-fast text-to-video generation model. It can generate videos more than ten times faster than the original AnimateDiff. For more information, please refer to our research paper: [AnimateDiff-Lightning: Cross-Model Diffusion Distillation](https://arxiv.org/abs/2403.12706). We release the model as part of the research.\n\nOur models are distilled from [AnimateDiff SD1.5 v2](https://huggingface.co/guoyww/animatediff). This repository contains checkpoints for 1-step, 2-step, 4-step, and 8-step distilled models. The generation quality of our 2-step, 4-step, and 8-step model is great. Our 1-step model is only provided for research purposes.\n\n\n## Demo\n\nTry AnimateDiff-Lightning using our text-to-video generation [demo](https://huggingface.co/spaces/ByteDance/AnimateDiff-Lightning).\n\n\n## Recommendation\n\nAnimateDiff-Lightning produces the best results when used with stylized base models. We recommend using the following base models:\n\nRealistic\n- [epiCRealism](https://civitai.com/models/25694)\n- [Realistic Vision](https://civitai.com/models/4201)\n- [DreamShaper](https://civitai.com/models/4384)\n- [AbsoluteReality](https://civitai.com/models/81458)\n- [MajicMix Realistic](https://civitai.com/models/43331)\n\nAnime & Cartoon\n- [ToonYou](https://civitai.com/models/30240)\n- [IMP](https://civitai.com/models/56680)\n- [Mistoon Anime](https://civitai.com/models/24149)\n- [DynaVision](https://civitai.com/models/75549)\n- [RCNZ Cartoon 3d](https://civitai.com/models/66347)\n- [MajicMix Reverie](https://civitai.com/models/65055)\n\nAdditionally, feel free to explore different settings. We find using 3 inference steps on the 2-step model produces great results. We find certain base models produces better results with CFG. We also recommend using [Motion LoRAs](https://huggingface.co/guoyww/animatediff/tree/main) as they produce stronger motion. We use Motion LoRAs with strength 0.7~0.8 to avoid watermark.\n\n## Diffusers Usage\n\n```python\nimport torch\nfrom diffusers import AnimateDiffPipeline, MotionAdapter, EulerDiscreteScheduler\nfrom diffusers.utils import export_to_gif\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\n\ndevice = "cuda"\ndtype = torch.float16\n\nstep = 4  # Options: [1,2,4,8]\nrepo = "ByteDance/AnimateDiff-Lightning"\nckpt = f"animatediff_lightning_{step}step_diffusers.safetensors"\nbase = "emilianJR/epiCRealism"  # Choose to your favorite base model.\n\nadapter = MotionAdapter().to(device, dtype)\nadapter.load_state_dict(load_file(hf_hub_download(repo ,ckpt), device=device))\npipe = AnimateDiffPipeline.from_pretrained(base, motion_adapter=adapter, torch_dtype=dtype).to(device)\npipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing="trailing", beta_schedule="linear")\n\noutput = pipe(prompt="A girl smiling", guidance_scale=1.0, num_inference_steps=step)\nexport_to_gif(output.frames[0], "animation.gif")\n```\n\n## ComfyUI Usage\n\n1. Download [animatediff_lightning_workflow.json](https://huggingface.co/ByteDance/AnimateDiff-Lightning/raw/main/comfyui/animatediff_lightning_workflow.json) and import it in ComfyUI.\n1. Install nodes. You can install them manually or use [ComfyUI-Manager](https://github.com/ltdrdata/ComfyUI-Manager).\n    * [ComfyUI-AnimateDiff-Evolved](https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved)\n    * [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)\n1. Download your favorite base model checkpoint and put them under `/models/checkpoints/`\n1. Download AnimateDiff-Lightning checkpoint `animatediff_lightning_Nstep_comfyui.safetensors` and put them under `/custom_nodes/ComfyUI-AnimateDiff-Evolved/models/`\n\n\n![ComfyUI Workflow](https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/comfyui/animatediff_lightning_workflow.jpg)\n\n\n## Video-to-Video Generation\n\nAnimateDiff-Lightning is great for video-to-video generation. We provide the simplist comfyui workflow using ControlNet.\n\n1. Download [animatediff_lightning_v2v_openpose_workflow.json](https://huggingface.co/ByteDance/AnimateDiff-Lightning/raw/main/comfyui/animatediff_lightning_v2v_openpose_workflow.json) and import it in ComfyUI.\n1. Install nodes. You can install them manually or use [ComfyUI-Manager](https://github.com/ltdrdata/ComfyUI-Manager).\n    * [ComfyUI-AnimateDiff-Evolved](https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved)\n    * [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)\n    * [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)\n    * [comfyui_controlnet_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)\n1. Download your favorite base model checkpoint and put them under `/models/checkpoints/`\n1. Download AnimateDiff-Lightning checkpoint `animatediff_lightning_Nstep_comfyui.safetensors` and put them under `/custom_nodes/ComfyUI-AnimateDiff-Evolved/models/`\n1. Download [ControlNet OpenPose](https://huggingface.co/lllyasviel/ControlNet-v1-1/tree/main) `control_v11p_sd15_openpose.pth` checkpoint to `/models/controlnet/`\n1. Upload your video and run the pipeline.\n\nAdditional notes:\n\n1. Video shouldn''t be too long or too high resolution. We used 576x1024 8 second 30fps videos for testing.\n1. Set the frame rate to match your input video. This allows audio to match with the output video.\n1. DWPose will download checkpoint itself on its first run.\n1. DWPose may get stuck in UI, but the pipeline is actually still running in the background. Check ComfyUI log and your output folder.\n\n![ComfyUI OpenPose Workflow](https://huggingface.co/ByteDance/AnimateDiff-Lightning/resolve/main/comfyui/animatediff_lightning_v2v_openpose_workflow.jpg)\n\n# Cite Our Work\n```\n@misc{lin2024animatedifflightning,\n      title={AnimateDiff-Lightning: Cross-Model Diffusion Distillation}, \n      author={Shanchuan Lin and Xiao Yang},\n      year={2024},\n      eprint={2403.12706},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```', '{"pipeline_tag":"text-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":7286508236,"files_count":18,"spaces_count":76,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:ltdrdata:ComfyUI-Manager","source_url":"https://github.com/ltdrdata/ComfyUI-Manager"},{"type":"has_code","target_id":"github:Kosinkadink:ComfyUI-AnimateDiff-Evolved","source_url":"https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved"},{"type":"has_code","target_id":"github:Kosinkadink:ComfyUI-VideoHelperSuite","source_url":"https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite"},{"type":"has_code","target_id":"github:ltdrdata:ComfyUI-Manager","source_url":"https://github.com/ltdrdata/ComfyUI-Manager"},{"type":"has_code","target_id":"github:Kosinkadink:ComfyUI-AnimateDiff-Evolved","source_url":"https://github.com/Kosinkadink/ComfyUI-AnimateDiff-Evolved"},{"type":"has_code","target_id":"github:Kosinkadink:ComfyUI-VideoHelperSuite","source_url":"https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite"},{"type":"has_code","target_id":"github:Kosinkadink:ComfyUI-Advanced-ControlNet","source_url":"https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet"},{"type":"has_code","target_id":"github:Fannovel16:comfyui_controlnet_aux","source_url":"https://github.com/Fannovel16/comfyui_controlnet_aux"},{"type":"based_on_paper","target_id":"arxiv:2403.12706","source_url":"https://arxiv.org/abs/2403.12706"}]', NULL, 'creativeml-openrail-m', 'approved', 64.9, '331318cf442d6425c2dd5b2f9cbe87ef', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-Phi-3-vision-128k-instruct', 'huggingface--microsoft--phi-3-vision-128k-instruct', 'Phi-3-vision-128k-instruct', 'microsoft', '--- license: mit license_link: https://huggingface.co/microsoft/Phi-3-vision-128k-instruct/resolve/main/LICENSE language: - multilingual pipeline_tag: text-generation tags: - nlp - code - vision inference: parameters: temperature: 0.7 widget: - messages: - role: user content: <|image_1|>Can you describe what you see in the image? --- üéâ **Phi-3.5**: [[mini-instruct]](https://huggingface.co/microsoft/Phi-3.5-mini-instruct); [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct...', '["transformers","safetensors","phi3_v","text-generation","nlp","code","vision","conversational","custom_code","multilingual","license:mit","region:us"]', 'text-generation', 970, 17715, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/Phi-3-vision-128k-instruct","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Phi-3-vision-128k-instruct/resolve/main/LICENSE\n\nlanguage:\n- multilingual\npipeline_tag: text-generation\ntags:\n- nlp\n- code\n- vision\ninference:\n  parameters:\n    temperature: 0.7\nwidget:\n  - messages:\n      - role: user\n        content: <|image_1|>Can you describe what you see in the image?\n---\nüéâ **Phi-3.5**: [[mini-instruct]](https://huggingface.co/microsoft/Phi-3.5-mini-instruct); [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct) ; [[vision-instruct]](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)\n\n## Model Summary\n\nThe Phi-3-Vision-128K-Instruct is a lightweight, state-of-the-art open multimodal model built upon datasets which include - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data both on text and vision.  The model belongs to the Phi-3 model family, and the multimodal version comes with 128K context length (in tokens) it can support. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\nResources and Technical Documentation:\n\n+ [Phi-3 Microsoft Blog](https://aka.ms/Phi-3Build2024)\n+ [Phi-3 Technical Report](https://aka.ms/phi3-tech-report)\n+ [Phi-3 on Azure AI Studio](https://aka.ms/try-phi3vision)\n+ [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook)\n\n\n|         | Short Context | Long Context |\n| ------- | ------------- | ------------ |\n| Mini    | 4K [[HF]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-onnx) ; [[GGUF]](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx)|\n| Small   | 8K [[HF]](https://huggingface.co/microsoft/Phi-3-small-8k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-small-8k-instruct-onnx-cuda) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-small-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-small-128k-instruct-onnx-cuda)|\n| Medium  | 4K [[HF]](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct-onnx-cuda) | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-medium-128k-instruct-onnx-cuda)|\n| Vision  |  | 128K [[HF]](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct) ; [[ONNX]](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct-onnx-cuda)|\n\n## Intended Uses\n\n**Primary use cases**\n\nThe model is intended for broad commercial and research use in English. The model provides uses for general purpose AI systems and applications with visual and text input capabilities which require \n\n1) memory/compute constrained environments;\n2) latency bound scenarios;\n3) general image understanding;\n4) OCR;\n5) chart and table understanding.\n\nOur model is designed to accelerate research on efficient language and multimodal models, for use as a building block for generative AI powered features.\n\n**Use case considerations**\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness before using within a specific downstream use case, particularly for high-risk scenarios. \nDevelopers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case. \n\nNothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.\n\n## How to Use\n\nPhi-3-Vision-128K-Instruct has been integrated in the development version (4.40.2) of `transformers`. Until the official version is released through `pip`, ensure that you are doing one of the following:\n* When loading the model, ensure that `trust_remote_code=True` is passed as an argument of the `from_pretrained()` function.\n\n* Update your local `transformers` to the development version: `pip uninstall -y transformers && pip install git+https://github.com/huggingface/transformers`. The previous command is an alternative to cloning and installing from the source.\n\nThe current `transformers` version can be verified with: `pip list | grep transformers`.\n\nExamples of required packages:\n```\nflash_attn==2.5.8\nnumpy==1.24.4\nPillow==10.3.0\nRequests==2.31.0\ntorch==2.3.0\ntorchvision==0.18.0\ntransformers==4.40.2\n```\n\nPhi-3-Vision-128K-Instruct is also available in [Azure AI Studio](https://aka.ms/phi3-azure-ai).\n\n### Chat Format\n\nGiven the nature of the training data, the Phi-3-Vision-128K-Instruct model is best suited for a single image input wih prompts using the chat format as follows. \nYou can provide the prompt as a single image with a generic template as follow:\n```markdown\n<|user|>\n<|image_1|>\n{prompt}<|end|>\n<|assistant|>\n \n```\n\nwhere the model generates the text after `<|assistant|>` . In case of multi-turn conversation, the prompt can be formatted as follows:\n\n```markdown\n<|user|>\n<|image_1|>\n{prompt_1}<|end|>\n<|assistant|>\n{response_1}<|end|>\n<|user|>\n{prompt_2}<|end|>\n<|assistant|>\n \n```\n\n### Sample inference code\n\nThis code snippets show how to get quickly started with running the model on a GPU:\n\n```python\nfrom PIL import Image \nimport requests \nfrom transformers import AutoModelForCausalLM \nfrom transformers import AutoProcessor \n\nmodel_id = "microsoft/Phi-3-vision-128k-instruct" \n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map="cuda", trust_remote_code=True, torch_dtype="auto", _attn_implementation=''flash_attention_2'') # use _attn_implementation=''eager'' to disable flash attention\n\nprocessor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True) \n\nmessages = [ \n    {"role": "user", "content": "<|image_1|>\nWhat is shown in this image?"}, \n    {"role": "assistant", "content": "The chart displays the percentage of respondents who agree with various statements about their preparedness for meetings. It shows five categories: ''Having clear and pre-defined goals for meetings'', ''Knowing where to find the information I need for a meeting'', ''Understanding my exact role and responsibilities when I''m invited'', ''Having tools to manage admin tasks like note-taking or summarization'', and ''Having more focus time to sufficiently prepare for meetings''. Each category has an associated bar indicating the level of agreement, measured on a scale from 0% to 100%."}, \n    {"role": "user", "content": "Provide insightful questions to spark discussion."} \n] \n\nurl = "https://assets-c4akfrf5b4d3f4b7.z01.azurefd.net/assets/2024/04/BMDataViz_661fb89f3845e.png" \nimage = Image.open(requests.get(url, stream=True).raw) \n\nprompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\ninputs = processor(prompt, [image], return_tensors="pt").to("cuda:0") \n\ngeneration_args = { \n    "max_new_tokens": 500, \n    "temperature": 0.0, \n    "do_sample": False, \n} \n\ngenerate_ids = model.generate(**inputs, eos_token_id=processor.tokenizer.eos_token_id, **generation_args) \n\n# remove input tokens \ngenerate_ids = generate_ids[:, inputs[''input_ids''].shape[1]:]\nresponse = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] \n\nprint(response) \n```\n\nAdditional basic examples are provided [here](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct/blob/main/sample_inference.py).\n\n### How to finetune?\nWe recommend user to take a look at the [Phi-3 CookBook finetuning recipe for Vision](https://github.com/microsoft/Phi-3CookBook/blob/main/md/04.Fine-tuning/FineTuning_Vision.md)\n\n\n## Responsible AI Considerations\n\nLike other models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:   \n\n+ Quality of Service: The Phi models are trained primarily on English text. Languages other than English will experience worse performance. English language varieties with less representation in the training data might experience worse performance than standard American English.    \n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases.  \n+ Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the use case.  \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.   \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as "typing, math, random, collections, datetime, itertools". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.      \n\nDevelopers should apply responsible AI best practices and are responsible for ensuring that a specific use case complies with relevant laws and regulations (e.g. privacy, trade, etc.). Important areas for consideration include:  \n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context.\n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).\n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case.\n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n+ Identification of individuals: models with vision capabilities may have the potential to uniquely identify individuals in images. Safety post-training steers the model to refuse such requests, but developers should consider and implement, as appropriate, additional mitigations or user consent flows as required in their respective jurisdiction, (e.g., building measures to blur faces in image inputs before processing.\n  \n## Training\n\n### Model\n\n* Architecture: Phi-3-Vision-128K-Instruct has 4.2B parameters and contains image encoder, connector, projector, and Phi-3 Mini language model.\n* Inputs: Text and Image. It‚Äôs best suited for prompts using the chat format. \n* Context length: 128K tokens\n* GPUs: 512 H100-80G\n* Training time: 1.5 days\n* Training data: 500B vision and text tokens\n* Outputs: Generated text in response to the input\n* Dates: Our models were trained between February and April 2024\n* Status: This is a static model trained on an offline text dataset with cutoff date Mar 15, 2024. Future versions of the tuned models may be released as we improve models.\n* Release Type: Open weight release\n* Release dates: The model weight is released on May 21, 2024.\n\n### Datasets\n\nOur training data includes a wide variety of sources, and is a combination of \n\n1) publicly available documents filtered rigorously for quality, selected high-quality educational data and code;\n2) selected high-quality image-text interleave;\n3) newly created synthetic, ‚Äútextbook-like‚Äù data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.), newly created image data, e.g., chart/table/diagram/slides;\n4) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness.\n\nThe data collection process involved sourcing information from publicly available documents, with a meticulous approach to filtering out undesirable documents and images. To safeguard privacy, we carefully filtered various image and text data sources to remove or scrub any potentially personal data from the training data.\n \nMore details can be found in the [Phi-3 Technical Report](https://aka.ms/phi3-tech-report).\n\n## Benchmarks\n\nTo understand the capabilities, we compare Phi-3-Vision-128K-Instruct with a set of models over a variety of zero-shot benchmarks using our internal benchmark platform.\n\n|Benchmark|Phi-3 Vision-128K-In|LlaVA-1.6 Vicuna-7B|QWEN-VL Chat|Llama3-Llava-Next-8B|Claude-3 Haiku|Gemini 1.0 Pro V|GPT-4V-Turbo|\n|---------|---------------------|------------------|------------|--------------------|--------------|----------------|------------|\n|MMMU|40.4|34.2|39.0|36.4|40.7|42.0|55.5|¬†\n|MMBench|80.5|76.3|75.8|79.4|62.4|80.0|86.1|\n|ScienceQA|90.8|70.6|67.2|73.7|72.0|79.7|75.7|\n|MathVista|44.5|31.5|29.4|34.8|33.2|35.0|47.5|\n|InterGPS|38.1|20.5|22.3|24.6|32.1|28.6|41.0|\n|AI2D|76.7|63.1|59.8|66.9|60.3|62.8|74.7|\n|ChartQA|81.4|55.0|50.9|65.8|59.3|58.0|62.3|\n|TextVQA|70.9|64.6|59.4|55.7|62.7|64.7|68.1|\n|POPE|85.8|87.2|82.6|87.0|74.4|84.2|83.7|\n\n\n## Software\n\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n## Hardware\nNote that by default, the Phi-3-Vision-128K model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\n* NVIDIA A100\n* NVIDIA A6000\n* NVIDIA H100\n\n## License\n\nThe model is licensed under the [MIT license](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct/resolve/main/LICENSE).\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow‚ÄØ[Microsoft‚Äôs Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party‚Äôs policies.\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":4146621440,"storage_bytes":16586661776,"files_count":20,"spaces_count":47,"gated":false,"private":false,"config":{"architectures":["Phi3VForCausalLM"],"auto_map":{"AutoConfig":"configuration_phi3_v.Phi3VConfig","AutoModelForCausalLM":"modeling_phi3_v.Phi3VForCausalLM"},"model_type":"phi3_v","tokenizer_config":{"bos_token":"<s>","chat_template":"{% for message in messages %}{{''<|'' + message[''role''] + ''|>'' + ''\n'' + message[''content''] + ''<|end|>\n'' }}{% endfor %}{% if add_generation_prompt and messages[-1][''role''] != ''assistant'' %}{{- ''<|assistant|>\n'' -}}{% endif %}","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:microsoft:Phi-3CookBook","source_url":"https://github.com/microsoft/Phi-3CookBook"},{"type":"has_code","target_id":"github:huggingface:transformers`.","source_url":"https://github.com/huggingface/transformers`."},{"type":"has_code","target_id":"github:microsoft:Phi-3CookBook","source_url":"https://github.com/microsoft/Phi-3CookBook"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:HazyResearch:flash-attention","source_url":"https://github.com/HazyResearch/flash-attention"}]', NULL, 'MIT', 'approved', 79.9, '3e8385944f76437cee916a91bce2c7ba', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-CompVis-stable-diffusion', 'huggingface--compvis--stable-diffusion', 'stable-diffusion', 'CompVis', '--- license: creativeml-openrail-m tags: - stable-diffusion - text-to-image inference: false --- Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. This model card gives an overview of all available model checkpoints. For more in-detail model cards, please have a look at the model repositories listed under Model Access. For the first version 4 model checkpoints are released. *Higher* versions have been trained for long...', '["stable-diffusion","text-to-image","arxiv:2207.12598","license:creativeml-openrail-m","region:us"]', 'text-to-image', 967, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/CompVis/stable-diffusion","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- text-to-image\ninference: false\n---\n# Stable Diffusion\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nThis model card gives an overview of all available model checkpoints. For more in-detail model cards, please have a look at the model repositories listed under [Model Access](#model-access).\n\n## Stable Diffusion Version 1\n\nFor the first version 4 model checkpoints are released.\n*Higher* versions have been trained for longer and are thus usually better in terms of image generation quality then *lower* versions. More specifically: \n\n- **stable-diffusion-v1-1**: The checkpoint is randomly initialized and has been trained on 237,000 steps at resolution `256x256` on [laion2B-en](https://huggingface.co/datasets/laion/laion2B-en).\n  194,000 steps at resolution `512x512` on [laion-high-resolution](https://huggingface.co/datasets/laion/laion-high-resolution) (170M examples from LAION-5B with resolution `>= 1024x1024`).\n- **stable-diffusion-v1-2**: The checkpoint resumed training from `stable-diffusion-v1-1`.\n  515,000 steps at resolution `512x512` on "laion-improved-aesthetics" (a subset of laion2B-en,\nfiltered to images with an original size `>= 512x512`, estimated aesthetics score `> 5.0`, and an estimated watermark probability `< 0.5`. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an [improved aesthetics estimator](https://github.com/christophschuhmann/improved-aesthetic-predictor)).\n- **stable-diffusion-v1-3**: The checkpoint resumed training from `stable-diffusion-v1-2`. 195,000 steps at resolution `512x512` on "laion-improved-aesthetics" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598)\n- **stable-diffusion-v1-4**: The checkpoint resumed training from `stable-diffusion-v1-2`. 195,000 steps at resolution `512x512` on "laion-improved-aesthetics" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n- [**`stable-diffusion-v1-4`**](https://huggingface.co/CompVis/stable-diffusion-v1-4) Resumed from `stable-diffusion-v1-2`.225,000 steps at resolution `512x512` on "laion-aesthetics v2 5+" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\n### Model Access\n\nEach checkpoint can be used both with Hugging Face''s [ üß® Diffusers library](https://github.com/huggingface/diffusers) or the original [Stable Diffusion GitHub repository](https://github.com/CompVis/stable-diffusion). Note that you have to *"click-request"* them on each respective model repository.\n\n| **[ü§ó''s üß® Diffusers library](https://github.com/huggingface/diffusers)**     | **[Stable Diffusion GitHub repository](https://github.com/CompVis/stable-diffusion)** |\n| ----------- | ----------- |\n| [`stable-diffusion-v1-1`](https://huggingface.co/CompVis/stable-diffusion-v1-1)      | [`stable-diffusion-v-1-1-original`](https://huggingface.co/CompVis/stable-diffusion-v-1-1-original)       |\n| [`stable-diffusion-v1-2`](https://huggingface.co/CompVis/stable-diffusion-v1-2)   | [`stable-diffusion-v-1-2-original`](https://huggingface.co/CompVis/stable-diffusion-v-1-2-original)        |\n| [`stable-diffusion-v1-3`](https://huggingface.co/CompVis/stable-diffusion-v1-3)   | [`stable-diffusion-v-1-3-original`](https://huggingface.co/CompVis/stable-diffusion-v-1-3-original)        |\n| [`stable-diffusion-v1-4`](https://huggingface.co/CompVis/stable-diffusion-v1-4)   | [`stable-diffusion-v-1-4-original`](https://huggingface.co/CompVis/stable-diffusion-v-1-4-original)        |\n\n### Demo\n\nTo quickly try out the model, you can try out the [Stable Diffusion Space](https://huggingface.co/spaces/stabilityai/stable-diffusion).\n\n### License\n\n[The CreativeML OpenRAIL M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) is an [Open RAIL M license](https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses), adapted from the work that [BigScience](https://bigscience.huggingface.co/) and [the RAIL Initiative](https://www.licenses.ai/) are jointly carrying in the area of responsible AI licensing. See also [the article about the BLOOM Open RAIL license](https://bigscience.huggingface.co/blog/the-bigscience-rail-license) on which our license is based.\n\n## Citation\n\n```bibtex\n    @InProceedings{Rombach_2022_CVPR,\n        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},\n        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month     = {June},\n        year      = {2022},\n        pages     = {10684-10695}\n    }\n```\n\n*This model card was written by: Robin Rombach and Patrick Esser and is based on the [DALL-E Mini model card](https://huggingface.co/dalle-mini/dalle-mini).*\n', '{"pipeline_tag":"text-to-image","library_name":null,"framework":null,"params":null,"storage_bytes":2132781863,"files_count":5,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:christophschuhmann:improved-aesthetic-predictor","source_url":"https://github.com/christophschuhmann/improved-aesthetic-predictor"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:CompVis:stable-diffusion","source_url":"https://github.com/CompVis/stable-diffusion"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:CompVis:stable-diffusion","source_url":"https://github.com/CompVis/stable-diffusion"},{"type":"based_on_paper","target_id":"arxiv:2207.12598","source_url":"https://arxiv.org/abs/2207.12598"}]', NULL, 'creativeml-openrail-m', 'approved', 64.9, 'f561d8df23c833c0c10f2a328aa79254', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-answerdotai-ModernBERT-base', 'huggingface--answerdotai--modernbert-base', 'ModernBERT-base', 'answerdotai', '--- library_name: transformers license: apache-2.0 language: - en tags: - fill-mask - masked-lm - long-context - modernbert pipeline_tag: fill-mask inference: false --- 1. Model Summary 2. Usage 3. Evaluation 4. Limitations 5. Training 6. License 7. Citation ModernBERT is a modernized bidirectional encoder-only Transformer model (BERT-style) pre-trained on 2 trillion tokens of English and code data with a native context length of up to 8,192 tokens. ModernBERT leverages recent architectural i...', '["transformers","pytorch","onnx","safetensors","modernbert","fill-mask","masked-lm","long-context","en","arxiv:2412.13663","license:apache-2.0","deploy:azure","region:us"]', 'fill-mask', 964, 812999, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/answerdotai/ModernBERT-base","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlanguage:\n- en\ntags:\n- fill-mask\n- masked-lm\n- long-context\n- modernbert\npipeline_tag: fill-mask\ninference: false\n---\n\n# ModernBERT\n\n## Table of Contents\n1. [Model Summary](#model-summary)\n2. [Usage](#Usage)\n3. [Evaluation](#Evaluation)\n4. [Limitations](#limitations)\n5. [Training](#training)\n6. [License](#license)\n7. [Citation](#citation)\n\n## Model Summary\n\nModernBERT is a modernized bidirectional encoder-only Transformer model (BERT-style) pre-trained on 2 trillion tokens of English and code data with a native context length of up to 8,192 tokens. ModernBERT leverages recent architectural improvements such as:\n\n- **Rotary Positional Embeddings (RoPE)** for long-context support.  \n- **Local-Global Alternating Attention** for efficiency on long inputs.  \n- **Unpadding and Flash Attention** for efficient inference.  \n\nModernBERT‚Äôs native long context length makes it ideal for tasks that require processing long documents, such as retrieval, classification, and semantic search within large corpora. The model was trained on a large corpus of text and code, making it suitable for a wide range of downstream tasks, including code retrieval and hybrid (text + code) semantic search.\n\nIt is available in the following sizes:\n\n- [ModernBERT-base](https://huggingface.co/answerdotai/ModernBERT-base) - 22 layers, 149 million parameters\n- [ModernBERT-large](https://huggingface.co/answerdotai/ModernBERT-large) - 28 layers, 395 million parameters\n\nFor more information about ModernBERT, we recommend our [release blog post](https://huggingface.co/blog/modernbert) for a high-level overview, and our [arXiv pre-print](https://arxiv.org/abs/2412.13663) for in-depth information.\n\n*ModernBERT is a collaboration between [Answer.AI](https://answer.ai), [LightOn](https://lighton.ai), and friends.*\n\n## Usage\n\nYou can use these models directly with the `transformers` library starting from v4.48.0:\n\n```sh\npip install -U transformers>=4.48.0\n```\n\nSince ModernBERT is a Masked Language Model (MLM), you can use the `fill-mask` pipeline or load it via `AutoModelForMaskedLM`. To use ModernBERT for downstream tasks like classification, retrieval, or QA, fine-tune it following standard BERT fine-tuning recipes.\n\n**‚ö†Ô∏è If your GPU supports it, we recommend using ModernBERT with Flash Attention 2 to reach the highest efficiency. To do so, install Flash Attention as follows, then use the model as normal:**\n\n```bash\npip install flash-attn\n```\n\nUsing `AutoModelForMaskedLM`:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\nmodel_id = "answerdotai/ModernBERT-base"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForMaskedLM.from_pretrained(model_id)\n\ntext = "The capital of France is [MASK]."\ninputs = tokenizer(text, return_tensors="pt")\noutputs = model(**inputs)\n\n# To get predictions for the mask:\nmasked_index = inputs["input_ids"][0].tolist().index(tokenizer.mask_token_id)\npredicted_token_id = outputs.logits[0, masked_index].argmax(axis=-1)\npredicted_token = tokenizer.decode(predicted_token_id)\nprint("Predicted token:", predicted_token)\n# Predicted token:  Paris\n```\n\nUsing a pipeline:\n\n```python\nimport torch\nfrom transformers import pipeline\nfrom pprint import pprint\n\npipe = pipeline(\n    "fill-mask",\n    model="answerdotai/ModernBERT-base",\n    torch_dtype=torch.bfloat16,\n)\n\ninput_text = "He walked to the [MASK]."\nresults = pipe(input_text)\npprint(results)\n```\n\n**Note:** ModernBERT does not use token type IDs, unlike some earlier BERT models. Most downstream usage is identical to standard BERT models on the Hugging Face Hub, except you can omit the `token_type_ids` parameter.\n\n## Evaluation\n\nWe evaluate ModernBERT across a range of tasks, including natural language understanding (GLUE), general retrieval (BEIR), long-context retrieval (MLDR), and code retrieval (CodeSearchNet and StackQA).\n\n**Key highlights:**\n- On GLUE, ModernBERT-base surpasses other similarly-sized encoder models, and ModernBERT-large is second only to Deberta-v3-large.\n- For general retrieval tasks, ModernBERT performs well on BEIR in both single-vector (DPR-style) and multi-vector (ColBERT-style) settings.\n- Thanks to the inclusion of code data in its training mixture, ModernBERT as a backbone also achieves new state-of-the-art code retrieval results on CodeSearchNet and StackQA.\n\n### Base Models\n\n| Model       | IR (DPR)     | IR (DPR)     | IR (DPR)     | IR (ColBERT)  | IR (ColBERT)  | NLU  | Code | Code |\n|-------------|--------------|--------------|--------------|---------------|---------------|------|------|------|\n|             | BEIR         | MLDR_OOD     | MLDR_ID      | BEIR          | MLDR_OOD      | GLUE | CSN  | SQA  |\n| BERT        | 38.9         | 23.9         | 32.2         | 49.0          | 28.1          | 84.7 | 41.2 | 59.5 |\n| RoBERTa     | 37.7         | 22.9         | 32.8         | 48.7          | 28.2          | 86.4 | 44.3 | 59.6 |\n| DeBERTaV3   | 20.2         | 5.4          | 13.4         | 47.1          | 21.9          | 88.1 | 17.5 | 18.6 |\n| NomicBERT   | 41.0         | 26.7         | 30.3         | 49.9          | 61.3          | 84.0 | 41.6 | 61.4 |\n| GTE-en-MLM  | 41.4         | **34.3**    |**44.4**   | 48.2          | 69.3          | 85.6 | 44.9 | 71.4 |\n| ModernBERT  | **41.6**    | 27.4         | 44.0         | **51.3**    | **80.2**      | **88.4** | **56.4** |**73.6**|\n\n---\n\n### Large Models\n\n| Model       | IR (DPR)     | IR (DPR)     | IR (DPR)     | IR (ColBERT)  | IR (ColBERT)  | NLU  | Code | Code |\n|-------------|--------------|--------------|--------------|---------------|---------------|------|------|------|\n|             | BEIR         | MLDR_OOD     | MLDR_ID      | BEIR          | MLDR_OOD      | GLUE | CSN  | SQA  |\n| BERT        | 38.9         | 23.3         | 31.7         | 49.5          | 28.5          | 85.2 | 41.6 | 60.8 |\n| RoBERTa     | 41.4         | 22.6         | 36.1         | 49.8          | 28.8          | 88.9 | 47.3 | 68.1 |\n| DeBERTaV3   | 25.6         | 7.1          | 19.2         | 46.7          | 23.0          | **91.4**| 21.2 | 19.7 |\n| GTE-en-MLM  | 42.5         | **36.4**    | **48.9**  | 50.7          | 71.3          | 87.6 | 40.5 | 66.9 |\n| ModernBERT  | **44.0**    | 34.3         | 48.6         | **52.4**     | **80.4**     | 90.4 |**59.5** |**83.9**|\n\n*Table 1: Results for all models across an overview of all tasks. CSN refers to CodeSearchNet and SQA to StackQA. MLDRID refers to in-domain (fine-tuned on the training set) evaluation, and MLDR_OOD to out-of-domain.*\n\nModernBERT‚Äôs strong results, coupled with its efficient runtime on long-context inputs, demonstrate that encoder-only models can be significantly improved through modern architectural choices and extensive pretraining on diversified data sources.\n\n\n## Limitations\n\nModernBERT‚Äôs training data is primarily English and code, so performance may be lower for other languages. While it can handle long sequences efficiently, using the full 8,192 tokens window may be slower than short-context inference. Like any large language model, ModernBERT may produce representations that reflect biases present in its training data. Verify critical or sensitive outputs before relying on them.\n\n## Training\n\n- Architecture: Encoder-only, Pre-Norm Transformer with GeGLU activations.\n- Sequence Length: Pre-trained up to 1,024 tokens, then extended to 8,192 tokens.\n- Data: 2 trillion tokens of English text and code.\n- Optimizer: StableAdamW with trapezoidal LR scheduling and 1-sqrt decay.\n- Hardware: Trained on 8x H100 GPUs.\n\nSee the paper for more details.\n\n## License\n\nWe release the ModernBERT model architectures, model weights, training codebase under the Apache 2.0 license.\n\n## Citation\n\nIf you use ModernBERT in your work, please cite:\n\n```\n@misc{modernbert,\n      title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference}, \n      author={Benjamin Warner and Antoine Chaffin and Benjamin Clavi√© and Orion Weller and Oskar Hallstr√∂m and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},\n      year={2024},\n      eprint={2412.13663},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.13663}, \n}\n```', '{"pipeline_tag":"fill-mask","library_name":"transformers","framework":"transformers","params":149655232,"storage_bytes":5084307056,"files_count":16,"spaces_count":64,"gated":false,"private":false,"config":{"architectures":["ModernBertForMaskedLM"],"model_type":"modernbert","tokenizer_config":{"cls_token":"[CLS]","mask_token":"[MASK]","pad_token":"[PAD]","sep_token":"[SEP]","unk_token":"[UNK]"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2412.13663","source_url":"https://arxiv.org/abs/2412.13663"}]', NULL, 'Apache-2.0', 'approved', 64.8, '4c2898c4d434bfa01bb2712550a1cda9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-black-forest-labs-FLUX.1-Fill-dev', 'huggingface--black-forest-labs--flux.1-fill-dev', 'FLUX.1-Fill-dev', 'black-forest-labs', '', '["diffusers","safetensors","image-generation","flux","diffusion-single-file","en","license:other","diffusers:fluxfillpipeline","region:us"]', 'other', 962, 152935, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":null,"library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":58052037720,"files_count":28,"spaces_count":79,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"FluxFillPipeline"}}}', '[]', '[]', NULL, 'Other', 'approved', 39.8, '42e7a7f191345963c4f74a32f6b7fdb5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tencent-SRPO', 'huggingface--tencent--srpo', 'SRPO', 'tencent', '--- library_name: diffusers license: other license_name: tencent-hunyuan-community license_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt pipeline_tag: text-to-image --- <div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù> <h1 align="center">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1> <div align="center"> <a href=''https://arxiv.org/abs/2509.06942''><img src=''https://img.shields.io/badge/ArXiv-red?logo=arxiv''></a> &nbsp; <a href=''h...', '["diffusers","safetensors","text-to-image","arxiv:2509.06942","license:other","region:us"]', 'text-to-image', 961, 3236, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tencent/SRPO","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: diffusers\nlicense: other\nlicense_name: tencent-hunyuan-community\nlicense_link: https://github.com/Tencent-Hunyuan/SRPO/blob/main/LICENSE.txt\npipeline_tag: text-to-image\n---\n\n<div align=‚Äúcenter‚Äù style=‚Äúfont-family: charter;‚Äù>\n<h1 align="center">Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference </h1>\n<div align="center">\n  <a href=''https://arxiv.org/abs/2509.06942''><img src=''https://img.shields.io/badge/ArXiv-red?logo=arxiv''></a>  &nbsp;\n  <a href=''https://github.com/Tencent-Hunyuan/SRPO''><img src=''https://img.shields.io/badge/_Code-SRPO-181717?color=121717&logo=github&logoColor=whitee''></a> &nbsp; \n  <a href=''https://tencent.github.io/srpo-project-page/''><img src=''https://img.shields.io/badge/%F0%9F%92%BB_Project-SRPO-blue''></a> &nbsp;\n</div>\n<div align="center">\n  Xiangwei Shen<sup>1,2*</sup>,\n  <a href="https://scholar.google.com/citations?user=Lnr1FQEAAAAJ&hl=zh-CN" target="_blank"><b>Zhimin Li</b></a><sup>1*</sup>,\n  <a href="https://scholar.google.com.hk/citations?user=Fz3X5FwAAAAJ" target="_blank"><b>Zhantao Yang</b></a><sup>1</sup>, \n  <a href="https://shiyi-zh0408.github.io/" target="_blank"><b>Shiyi Zhang</b></a><sup>3</sup>,\n  Yingfang Zhang<sup>1</sup>,\n  Donghao Li<sup>1</sup>,\n  <br>\n  <a href="https://scholar.google.com/citations?user=VXQV5xwAAAAJ&hl=en" target="_blank"><b>Chunyu Wang</b></a><sup>1</sup>,\n  <a href="https://openreview.net/profile?id=%7EQinglin_Lu2" target="_blank"><b>Qinglin Lu</b></a><sup>1</sup>,\n  <a href="https://andytang15.github.io" target="_blank"><b>Yansong Tang</b></a><sup>3,‚úù</sup>\n</div>\n<div align="center">\n  <sup>1</sup>Hunyuan, Tencent‚ÄÉ\n  <br>\n  <sup>2</sup>School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen‚ÄÉ\n  <br>\n  <sup>3</sup>Shenzhen International Graduate School, Tsinghua University‚ÄÉ\n  <br>\n  <sup>*</sup>Equal contribution‚ÄÉ\n  <sup>‚úù</sup>Corresponding author\n</div>\n\n\n\n## Abstract\nRecent studies have demonstrated the effectiveness of directly aligning diffusion models with human preferences using differentiable reward. However, they exhibit two primary challenges: (1) they rely on multistep denoising with gradient computation for reward scoring, which is computationally expensive, thus restricting optimization to only a few diffusion steps; (2) they often need continuous offline adaptation of reward models in order to achieve desired aesthetic quality, such as photorealism or precise lighting effects. To address the limitation of multistep denoising, we propose Direct-Align, a method that predefines a noise prior to effectively recover original images from any time steps via interpolation, leveraging the equation that diffusion states are interpolations between noise and target images, which effectively avoids over-optimization in late timesteps. Furthermore, we introduce Semantic Relative Preference Optimization (SRPO), in which rewards are formulated as text-conditioned signals. This approach enables online adjustment of rewards in response to positive and negative prompt augmentation, thereby reducing the reliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model with optimized denoising and online reward adjustment, we improve its human-evaluated realism and aesthetic quality by over 3x.\n\n## Acknowledgement\n\nWe sincerely appreciate contributions from the research community to this project. Below are quantized versions developed by fellow researchers.\n\n1. 8bit(fp8_e4m3fn/Q8_0) version by wikeeyang: https://huggingface.co/wikeeyang/SRPO-Refine-Quantized-v1.0\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6645835a2b57c619a19cc0c4/BATJ0bW_0QPhkN5WY0Q1H.png)\n\n2. bf16 version by rockerBOO: https://huggingface.co/rockerBOO/flux.1-dev-SRPO\n3. GGUF version by befox: https://huggingface.co/befox/SRPO-GGUF\n\n‚ö†Ô∏è Note: When loading weights in ComfyUI, avoid direct conversion of FP32 weights to FP8 format, as this may result in incomplete denoising. For official weights in this repository, FP32/BF16 loading is recommended.\n\n\n### Checkpoints\nThe `diffusion_pytorch_model.safetensors` is online version of SRPO based on [FLUX.1 Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), trained on HPD dataset with [HPSv2](https://github.com/tgxs002/HPSv2)\n## üîë Inference\n\n### Using ComfyUI\n\nYou can use it in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).\n\nLoad the following image in ComfyUI to get the workflow, or load the JSON file directly [SRPO-workflow](comfyui/SRPO-workflow.json):\n\nTip: The workflow JSON info was added to the image file.\n\n![Example](comfyui/SRPO-workflow.png)\n\n### Quick start\n```bash\nfrom diffusers import FluxPipeline\nfrom safetensors.torch import load_file\n\nprompt=''The Death of Ophelia by John Everett Millais, Pre-Raphaelite painting, Ophelia floating in a river surrounded by flowers, detailed natural elements, melancholic and tragic atmosphere''\npipe = FluxPipeline.from_pretrained(''./data/flux'',\n        torch_dtype=torch.bfloat16,\n        use_safetensors=True\n    ).to("cuda")\nstate_dict = load_file("./srpo/diffusion_pytorch_model.safetensors")\npipe.transformer.load_state_dict(state_dict)\nimage = pipe(\n    prompt,\n    guidance_scale=3.5,\n    height=1024,\n    width=1024,\n    num_inference_steps=50,\n    max_sequence_length=512,\n    generator=generator\n).images[0]\n```\n### License\nSRPO is licensed under the License Terms of SRPO. See `./License.txt` for more details.\n## Citation\nIf you use SRPO for your research, please cite our paper:\n\n```bibtex\n@misc{shen2025directlyaligningdiffusiontrajectory,\n      title={Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference}, \n      author={Xiangwei Shen and Zhimin Li and Zhantao Yang and Shiyi Zhang and Yingfang Zhang and Donghao Li and Chunyu Wang and Qinglin Lu and Yansong Tang},\n      year={2025},\n      eprint={2509.06942},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2509.06942}, \n}\n```', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":47610166557,"files_count":6,"spaces_count":8,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:Tencent-Hunyuan:SRPO","source_url":"https://github.com/Tencent-Hunyuan/SRPO"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:SRPO''><img","source_url":"https://github.com/Tencent-Hunyuan/SRPO''><img"},{"type":"has_code","target_id":"github:tgxs002:HPSv2","source_url":"https://github.com/tgxs002/HPSv2"},{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI","source_url":"https://github.com/comfyanonymous/ComfyUI"},{"type":"based_on_paper","target_id":"arxiv:2509.06942","source_url":"https://arxiv.org/abs/2509.06942"}]', NULL, 'Other', 'approved', 64.8, 'fcb72bddaef09fede8129637059a5bca', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-briaai-RMBG-2.0', 'huggingface--briaai--rmbg-2.0', 'RMBG-2.0', 'briaai', '', '["transformers","pytorch","onnx","safetensors","image-segmentation","remove background","background","background-removal","pytorch","vision","legal liability","transformers.js","custom_code","license:other","region:us"]', 'image-segmentation', 960, 260474, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/briaai/RMBG-2.0","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-segmentation","library_name":"transformers","framework":"transformers","params":220700242,"storage_bytes":5896565704,"files_count":19,"spaces_count":98,"gated":"auto","private":false,"config":{"architectures":["BiRefNet"],"auto_map":{"AutoConfig":"BiRefNet_config.BiRefNetConfig","AutoModelForImageSegmentation":"birefnet.BiRefNet"}}}', '[]', '[]', NULL, 'Other', 'approved', 59.8, 'cce1130538b8c04b0ab88304843b3780', NULL, 'https://huggingface.co/briaai/RMBG-2.0/resolve/main/diagram1.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-briaai-RMBG-2.0 from https://huggingface.co/briaai/RMBG-2.0/resolve/main/diagram1.png
HTTP error: 401 Unauthorized

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-stable-video-diffusion-img2vid-xt-1-1', 'huggingface--stabilityai--stable-video-diffusion-img2vid-xt-1-1', 'stable-video-diffusion-img2vid-xt-1-1', 'stabilityai', '', '["diffusers","safetensors","image-to-video","license:other","diffusers:stablevideodiffusionpipeline","region:us"]', 'image-to-video', 959, 27046, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":18313832368,"files_count":17,"spaces_count":36,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"StableVideoDiffusionPipeline"}}}', '[]', '[]', NULL, 'Other', 'approved', 39.8, 'cb5de6b7948873cab9b7839af54318f9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nitrosocke-mo-di-diffusion', 'huggingface--nitrosocke--mo-di-diffusion', 'mo-di-diffusion', 'nitrosocke', '--- license: creativeml-openrail-m tags: - stable-diffusion - text-to-image --- **Mo Di Diffusion** This is the fine-tuned Stable Diffusion 1.5 model trained on screenshots from a popular animation studio. Use the tokens **_modern disney style_** in your prompts for the effect. **If you enjoy my work, please consider supporting me** **Videogame Characters rendered with the model:** !Videogame Samples **Animal Characters rendered with the model:** !Animal Samples **Cars and Landscapes rendered...', '["diffusers","stable-diffusion","text-to-image","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 957, 1235, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nitrosocke/mo-di-diffusion","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- text-to-image\n---\n**Mo Di Diffusion**\n\nThis is the fine-tuned Stable Diffusion 1.5 model trained on screenshots from a popular animation studio.\nUse the tokens **_modern disney style_** in your prompts for the effect.\n\n**If you enjoy my work, please consider supporting me** \n[![Become A Patreon](https://badgen.net/badge/become/a%20patron/F96854)](https://patreon.com/user?u=79196446)\n\n**Videogame Characters rendered with the model:**\n![Videogame Samples](https://huggingface.co/nitrosocke/mo-di-diffusion/resolve/main/modi-samples-01s.jpg)\n**Animal Characters rendered with the model:**\n![Animal Samples](https://huggingface.co/nitrosocke/mo-di-diffusion/resolve/main/modi-samples-02s.jpg)\n**Cars and Landscapes rendered with the model:**\n![Misc. Samples](https://huggingface.co/nitrosocke/mo-di-diffusion/resolve/main/modi-samples-03s.jpg)\n#### Prompt and settings for Lara Croft:\n**modern disney lara croft**\n_Steps: 50, Sampler: Euler a, CFG scale: 7, Seed: 3940025417, Size: 512x768_\n\n#### Prompt and settings for the Lion:\n**modern disney (baby lion) Negative prompt: person human**\n_Steps: 50, Sampler: Euler a, CFG scale: 7, Seed: 1355059992, Size: 512x512_\n\nThis model was trained using the diffusers based dreambooth training by ShivamShrirao using prior-preservation loss and the _train-text-encoder_ flag in 9.000 steps.\n\n### üß® Diffusers\n\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\n\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\n\n```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = "nitrosocke/mo-di-diffusion"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to("cuda")\n\nprompt = "a magical princess with golden hair, modern disney style"\nimage = pipe(prompt).images[0]\n\nimage.save("./magical_princess.png")\n```\n\n# Gradio & Colab\n\nWe also support a [Gradio](https://github.com/gradio-app/gradio) Web UI and Colab with Diffusers to run fine-tuned Stable Diffusion models:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/anzorq/finetuned_diffusion)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1j5YvfMZoGdDGdj3O3xRU1m4ujKYsElZO?usp=sharing)\n\n## License\n\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies: \n\n1. You can''t use the model to deliberately produce nor share illegal or harmful outputs or content \n2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":31105706419,"files_count":21,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"}]', NULL, 'creativeml-openrail-m', 'approved', 64.8, '3f2582c65668ac75a0401d9a0160560c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-control-lora', 'huggingface--stabilityai--control-lora', 'control-lora', 'stabilityai', '--- tags: - text-to-image - stable-diffusion license: other language: - en --- By adding low-rank parameter efficient fine tuning to ControlNet, we introduce ***Control-LoRAs***. This approach offers a more efficient and compact method to bring model control to a wider variety of consumer GPUs. For each model below, you''ll find: - *Rank 256* files (reducing the original ControlNet models down to Control-LoRA models) and experimental - *Rank 128* files (reducing to model down to ) Each Control...', '["text-to-image","stable-diffusion","en","license:other","region:us"]', 'text-to-image', 953, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/control-lora","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- text-to-image\n- stable-diffusion\nlicense: other\nlanguage:\n- en\n---\n\n# Control-LoRA Model Card\n\n\n## Introduction\nBy adding low-rank parameter efficient fine tuning to ControlNet, we introduce ***Control-LoRAs***. This approach offers a more efficient and compact method to bring model control to a wider variety of consumer GPUs.\n\nFor each model below, you''ll find:\n\n- *Rank 256* files (reducing the original `4.7GB` ControlNet models down to `~738MB` Control-LoRA models) and experimental\n- *Rank 128* files (reducing to model down to `~377MB`)\n\nEach Control-LoRA has been trained on a diverse range of image concepts and aspect ratios.\n\n### MiDaS and ClipDrop Depth\n![canny](samples/depth-sample.jpeg)\n\nThis Control-LoRA utilizes a grayscale depth map for guided generation.\n\nDepth estimation is an image processing technique that determines the distance of objects in a scene, providing a depth map that highlights variations in proximity.\n\nThe model was trained on the depth results of `MiDaS dpt_beit_large_512`.\n\nIt was further finetuned on the `Portrait Depth Estimation` model available in the [ClipDrop API by Stability AI](https://clipdrop.co/apis/docs/portrait-depth-estimation).\n\n### Canny Edge\n![canny](samples/canny-sample.jpeg)\nCanny Edge Detection is an image processing technique that identifies abrupt changes in intensity to highlight edges in an image.\n\nThis Control-LoRA uses the edges from an image to generate the final image.\n\n### Photograph and Sketch Colorizer\n![photograph colorizer](samples/colorizer-sample.jpeg)\nThese two Control-LoRAs can be used to colorize images.\n\n*Recolor* is designed to colorize black and white photographs.\n\n*Sketch* is designed to color in drawings input as a white-on-black image (either hand-drawn, or created with a `pidi` edge model).\n\n### Revision\n![photograph colorizer](samples/revision-sample.jpeg)\nRevision is a novel approach of using images to prompt SDXL.\n\nIt uses pooled CLIP embeddings to produce images conceptually similar to the input. It can be used either in addition, or to replace text prompts.\n\nRevision also includes a blending function for combining multiple image or text concepts, as either positive or negative prompts.\n\n\n## Inference\n\nControl-LoRAs have been implemented into [ComfyUI](https://github.com/comfyanonymous/ComfyUI) and [StableSwarmUI](https://github.com/Stability-AI/StableSwarmUI)\n\nBasic ComfyUI workflows (using the base model only) are available in this HF repo. Custom nodes from Stability are [available here](https://github.com/Stability-AI/stability-ComfyUI-nodes).\n\n**Recolor example on ComfyUI:** ![comfyui recolor](samples/comfyui-recolor-example.jpeg)\n\n**Canny edge on StableSwarmUI:** ![swarmui recolor](samples/swarmui-canny-example.jpeg)', '{"pipeline_tag":"text-to-image","library_name":null,"framework":null,"params":null,"storage_bytes":8373480116,"files_count":29,"spaces_count":3,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI","source_url":"https://github.com/comfyanonymous/ComfyUI"},{"type":"has_code","target_id":"github:Stability-AI:StableSwarmUI","source_url":"https://github.com/Stability-AI/StableSwarmUI"},{"type":"has_code","target_id":"github:Stability-AI:stability-ComfyUI-nodes","source_url":"https://github.com/Stability-AI/stability-ComfyUI-nodes"}]', NULL, 'Other', 'approved', 64.8, '53f73ae37bbf6d12d945d41e522035b4', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-3.1-405B', 'huggingface--meta-llama--llama-3.1-405b', 'Llama-3.1-405B', 'meta-llama', '', '["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","en","de","fr","it","pt","hi","es","th","arxiv:2204.05149","license:llama3.1","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 948, 212771, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-3.1-405B","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":405853388800,"storage_bytes":4667909255120,"files_count":583,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","eos_token":"<|end_of_text|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2204.05149","source_url":"https://arxiv.org/abs/2204.05149"}]', NULL, 'llama3.1', 'approved', 39.8, '78001d53069f399380b0c895673a1d74', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Mistral-Small-24B-Instruct-2501', 'huggingface--mistralai--mistral-small-24b-instruct-2501', 'Mistral-Small-24B-Instruct-2501', 'mistralai', '--- library_name: vllm language: - en - fr - de - es - it - pt - zh - ja - ru - ko license: apache-2.0 inference: false base_model: - mistralai/Mistral-Small-24B-Base-2501 extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href="https://mistral.ai/terms/">Privacy Policy</a>. tags: - vllm --- Mistral Small 3 ( 2501 ) sets a new benchmark in the "small" Large Language Models category below 70B, boasting 24B parameters and achieving ...', '["vllm","safetensors","mistral","en","fr","de","es","it","pt","zh","ja","ru","ko","base_model:mistralai/mistral-small-24b-base-2501","license:apache-2.0","region:us"]', 'other', 948, 425939, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: vllm\nlanguage:\n- en\n- fr\n- de\n- es\n- it\n- pt\n- zh\n- ja\n- ru\n- ko\nlicense: apache-2.0\ninference: false\nbase_model:\n- mistralai/Mistral-Small-24B-Base-2501\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href="https://mistral.ai/terms/">Privacy Policy</a>.\ntags:\n- vllm\n---\n\n# Model Card for Mistral-Small-24B-Instruct-2501\n\nMistral Small 3 ( 2501 ) sets a new benchmark in the "small" Large Language Models category below 70B, boasting 24B parameters and achieving state-of-the-art capabilities comparable to larger models!  \nThis model is an instruction-fine-tuned version of the base model: [Mistral-Small-24B-Base-2501](https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501).\n\nMistral Small can be deployed locally and is exceptionally "knowledge-dense", fitting in a single RTX 4090 or a 32GB RAM MacBook once quantized.  \nPerfect for:\n- Fast response conversational agents.\n- Low latency function calling.\n- Subject matter experts via fine-tuning.\n- Local inference for hobbyists and organizations handling sensitive data.\n\nFor enterprises that need specialized capabilities (increased context, particular modalities, domain specific knowledge, etc.), we will be releasing commercial models beyond what Mistral AI contributes to the community.\n\nThis release demonstrates our commitment to open source, serving as a strong base model. \n\nLearn more about Mistral Small in our [blog post](https://mistral.ai/news/mistral-small-3/).\n\nModel developper: Mistral AI Team\n\n## Key Features\n- **Multilingual:** Supports dozens of languages, including English, French, German, Spanish, Italian, Chinese, Japanese, Korean, Portuguese, Dutch, and Polish.\n- **Agent-Centric:** Offers best-in-class agentic capabilities with native function calling and JSON outputting.\n- **Advanced Reasoning:** State-of-the-art conversational and reasoning capabilities.\n- **Apache 2.0 License:** Open license allowing usage and modification for both commercial and non-commercial purposes.\n- **Context Window:** A 32k context window.\n- **System Prompt:** Maintains strong adherence and support for system prompts.\n- **Tokenizer:** Utilizes a Tekken tokenizer with a 131k vocabulary size.\n\n## Benchmark results\n\n\n### Human evaluated benchmarks\n\n| Category | Gemma-2-27B | Qwen-2.5-32B | Llama-3.3-70B | Gpt4o-mini |\n|----------|-------------|--------------|---------------|------------|\n| Mistral is better | 0.536 | 0.496 | 0.192 | 0.200 |\n| Mistral is slightly better | 0.196 | 0.184 | 0.164 | 0.204 |\n| Ties | 0.052 | 0.060 | 0.236 | 0.160 |\n| Other is slightly better | 0.060 | 0.088 | 0.112 | 0.124 |\n| Other is better | 0.156 | 0.172 | 0.296 | 0.312 |\n\n**Note**:\n\n- We conducted side by side evaluations with an external third-party vendor, on a set of over 1k proprietary coding and generalist prompts.\n- Evaluators were tasked with selecting their preferred model response from anonymized generations produced by Mistral Small 3 vs another model.\n- We are aware that in some cases the benchmarks on human judgement starkly differ from publicly available benchmarks, but have taken extra caution in verifying a fair evaluation. We are confident that the above benchmarks are valid.\n\n### Publicly accesible benchmarks\n\n**Reasoning & Knowledge**\n\n| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n|------------|---------------|--------------|---------------|---------------|-------------|\n| mmlu_pro_5shot_cot_instruct | 0.663 | 0.536 | 0.666 | 0.683 | 0.617 |\n| gpqa_main_cot_5shot_instruct | 0.453 | 0.344 | 0.531 | 0.404 | 0.377 |\n\n**Math & Coding**\n\n| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n|------------|---------------|--------------|---------------|---------------|-------------|\n| humaneval_instruct_pass@1 | 0.848 | 0.732 | 0.854 | 0.909 | 0.890 |\n| math_instruct | 0.706 | 0.535 | 0.743 | 0.819 | 0.761 |\n\n**Instruction following**\n\n| Evaluation | mistral-small-24B-instruct-2501 | gemma-2b-27b | llama-3.3-70b | qwen2.5-32b | gpt-4o-mini-2024-07-18 |\n|------------|---------------|--------------|---------------|---------------|-------------|\n| mtbench_dev | 8.35 | 7.86 | 7.96 | 8.26 | 8.33 |\n| wildbench | 52.27 | 48.21 | 50.04 | 52.73 | 56.13 |\n| arena_hard | 0.873 | 0.788 | 0.840 | 0.860 | 0.897 |\n| ifeval | 0.829 | 0.8065 | 0.8835 | 0.8401 | 0.8499 |\n\n**Note**:\n\n- Performance accuracy on all benchmarks were obtained through the same internal evaluation pipeline - as such, numbers may vary slightly from previously reported performance\n([Qwen2.5-32B-Instruct](https://qwenlm.github.io/blog/qwen2.5/), [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct), [Gemma-2-27B-IT](https://huggingface.co/google/gemma-2-27b-it)). \n- Judge based evals such as Wildbench, Arena hard and MTBench were based on gpt-4o-2024-05-13.\n\n### Basic Instruct Template (V7-Tekken)\n\n```\n<s>[SYSTEM_PROMPT]<system prompt>[/SYSTEM_PROMPT][INST]<user message>[/INST]<assistant response></s>[INST]<user message>[/INST]\n```\n*`<system_prompt>`, `<user message>` and `<assistant response>` are placeholders.*\n\n***Please make sure to use [mistral-common](https://github.com/mistralai/mistral-common) as the source of truth***\n\n## Usage\n\nThe model can be used with the following frameworks;\n- [`vllm`](https://github.com/vllm-project/vllm): See [here](#vllm)\n- [`transformers`](https://github.com/huggingface/transformers): See [here](#transformers)\n\n### vLLM\n\nWe recommend using this model with the [vLLM library](https://github.com/vllm-project/vllm)\nto implement production-ready inference pipelines.\n\n**Note 1**: We recommond using a relatively low temperature, such as `temperature=0.15`.\n\n**Note 2**: Make sure to add a system prompt to the model to best tailer it for your needs. If you want to use the model as a general assistant, we recommend the following \nsystem prompt:\n\n```\nsystem_prompt = """You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\nYour knowledge base was last updated on 2023-10-01. The current date is 2025-01-30.\nWhen you''re not sure about some information, you say that you don''t have the information and don''t make up anything.\nIf the user''s question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \"What are some good restaurants around me?\" => \"Where are you?\" or \"When is the next flight to Tokyo\" => \"Where do you travel from?\")"""\n```\n\n**_Installation_**\n\nMake sure you install [`vLLM >= 0.6.4`](https://github.com/vllm-project/vllm/releases/tag/v0.6.4):\n\n```\npip install --upgrade vllm\n```\n\nAlso make sure you have [`mistral_common >= 1.5.2`](https://github.com/mistralai/mistral-common/releases/tag/v1.5.2) installed:\n\n```\npip install --upgrade mistral_common\n```\n\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/Dockerfile) or on the [docker hub](https://hub.docker.com/layers/vllm/vllm-openai/latest/images/sha256-de9032a92ffea7b5c007dad80b38fd44aac11eddc31c435f8e52f3b7404bbf39).\n\n#### Server\n\nWe recommand that you use Mistral-Small-24B-Instruct-2501 in a server/client setting. \n\n1. Spin up a server:\n\n```\nvllm serve mistralai/Mistral-Small-24B-Instruct-2501 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice\n```\n\n**Note:** Running Mistral-Small-24B-Instruct-2501 on GPU requires ~55 GB of GPU RAM in bf16 or fp16. \n\n\n2. To ping the client you can use a simple Python snippet.\n\n```py\nimport requests\nimport json\nfrom datetime import datetime, timedelta\n\nurl = "http://<your-server>:8000/v1/chat/completions"\nheaders = {"Content-Type": "application/json", "Authorization": "Bearer token"}\n\nmodel = "mistralai/Mistral-Small-24B-Instruct-2501"\n\nmessages = [\n    {\n        "role": "system",\n        "content": "You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat."\n    },\n    {\n        "role": "user",\n        "content": "Give me 5 non-formal ways to say ''See you later'' in French."\n    },\n]\n\ndata = {"model": model, "messages": messages}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()["choices"][0]["message"]["content"])\n\n# Sure, here are five non-formal ways to say "See you later" in French:\n#\n# 1. √Ä plus tard\n# 2. √Ä plus\n# 3. Salut\n# 4. √Ä toute\n# 5. Bisous\n#\n# ```\n#  /\_/\\n# ( o.o )\n#  > ^ <\n# ```\n```\n\n### Function calling\n\nMistral-Small-24-Instruct-2501 is excellent at function / tool calling tasks via vLLM. *E.g.:*\n\n<details>\n  <summary>Example</summary>\n\n```py\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\nfrom datetime import datetime, timedelta\n\nurl = "http://<your-url>:8000/v1/chat/completions"\nheaders = {"Content-Type": "application/json", "Authorization": "Bearer token"}\n\nmodel = "mistralai/Mistral-Small-24B-Instruct-2501"\n\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, "r") as file:\n        system_prompt = file.read()\n    today = datetime.today().strftime("%Y-%m-%d")\n    yesterday = (datetime.today() - timedelta(days=1)).strftime("%Y-%m-%d")\n    model_name = repo_id.split("/")[-1]\n    return system_prompt.format(name=model_name, today=today, yesterday=yesterday)\n\n\nSYSTEM_PROMPT = load_system_prompt(model, "SYSTEM_PROMPT.txt")\n\n\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_current_weather",\n            "description": "Get the current weather in a given location",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "city": {\n                        "type": "string",\n                        "description": "The city to find the weather for, e.g. ''San Francisco''",\n                    },\n                    "state": {\n                        "type": "string",\n                        "description": "The state abbreviation, e.g. ''CA'' for California",\n                    },\n                    "unit": {\n                        "type": "string",\n                        "description": "The unit for temperature",\n                        "enum": ["celsius", "fahrenheit"],\n                    },\n                },\n                "required": ["city", "state", "unit"],\n            },\n        },\n    },\n    {\n        "type": "function",\n        "function": {\n            "name": "rewrite",\n            "description": "Rewrite a given text for improved clarity",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "text": {\n                        "type": "string",\n                        "description": "The input text to rewrite",\n                    }\n                },\n            },\n        },\n    },\n]\n\nmessages = [\n    {"role": "system", "content": SYSTEM_PROMPT},\n    {\n        "role": "user",\n        "content": "Could you please make the below article more concise?\n\nOpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership.",\n    },\n    {\n        "role": "assistant",\n        "content": "",\n        "tool_calls": [\n            {\n                "id": "bbc5b7ede",\n                "type": "function",\n                "function": {\n                    "name": "rewrite",\n                    "arguments": ''{"text": "OpenAI is an artificial intelligence research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership."}'',\n                },\n            }\n        ],\n    },\n    {\n        "role": "tool",\n        "content": ''{"action":"rewrite","outcome":"OpenAI is a FOR-profit company."}'',\n        "tool_call_id": "bbc5b7ede",\n        "name": "rewrite",\n    },\n    {\n        "role": "assistant",\n        "content": "---\n\nOpenAI is a FOR-profit company.",\n    },\n    {\n        "role": "user",\n        "content": "Can you tell me what the temperature will be in Dallas, in Fahrenheit?",\n    },\n]\n\ndata = {"model": model, "messages": messages, "tools": tools}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nimport ipdb; ipdb.set_trace()\nprint(response.json()["choices"][0]["message"]["tool_calls"])\n# [{''id'': ''8PdihwL6d'', ''type'': ''function'', ''function'': {''name'': ''get_current_weather'', ''arguments'': ''{"city": "Dallas", "state": "TX", "unit": "fahrenheit"}''}}]\n```\n\n</details>\n\n#### Offline\n\n```py\nfrom vllm import LLM\nfrom vllm.sampling_params import SamplingParams\nfrom datetime import datetime, timedelta\n\nSYSTEM_PROMPT = "You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat."\n\nuser_prompt = "Give me 5 non-formal ways to say ''See you later'' in French."\n\nmessages = [\n    {\n        "role": "system",\n        "content": SYSTEM_PROMPT\n    },\n    {\n        "role": "user",\n        "content": user_prompt\n    },\n]\n\n# note that running this model on GPU requires over 60 GB of GPU RAM\nllm = LLM(model=model_name, tokenizer_mode="mistral", tensor_parallel_size=8)\n\nsampling_params = SamplingParams(max_tokens=512, temperature=0.15)\noutputs = llm.chat(messages, sampling_params=sampling_params)\n\nprint(outputs[0].outputs[0].text)\n# Sure, here are five non-formal ways to say "See you later" in French:\n#\n# 1. √Ä plus tard\n# 2. √Ä plus\n# 3. Salut\n# 4. √Ä toute\n# 5. Bisous\n#\n# ```\n#  /\_/\\n# ( o.o )\n#  > ^ <\n# ```\n```\n\n### Transformers\n\nIf you want to use Hugging Face transformers to generate text, you can do something like this.\n\n```py\nfrom transformers import pipeline\nimport torch\n\nmessages = [\n    {"role": "user", "content": "Give me 5 non-formal ways to say ''See you later'' in French."},\n]\nchatbot = pipeline("text-generation", model="mistralai/Mistral-Small-24B-Instruct-2501", max_new_tokens=256, torch_dtype=torch.bfloat16)\nchatbot(messages)\n```\n\n\n### Ollama\n\n[Ollama](https://github.com/ollama/ollama) can run this model locally on MacOS, Windows and Linux. \n\n```\nollama run mistral-small\n```\n\n4-bit quantization (aliased to default): \n```\nollama run mistral-small:24b-instruct-2501-q4_K_M\n```\n\n8-bit quantization:\n```\nollama run mistral-small:24b-instruct-2501-q8_0\n```\n\nFP16:\n```\nollama run mistral-small:24b-instruct-2501-fp16\n```', '{"pipeline_tag":null,"library_name":"vllm","framework":"vllm","params":23572403200,"storage_bytes":94321574156,"files_count":22,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","chat_template":"{%- set today = strftime_now(\"%Y-%m-%d\") %}\n{%- set default_system_message = \"You are Mistral Small 3, a Large Language Model (LLM) created by Mistral AI, a French startup headquartered in Paris.\\nYour knowledge base was last updated on 2023-10-01. The current date is \" + today + \".\\n\\nWhen you''re not sure about some information, you say that you don''t have the information and don''t make up anything.\\nIf the user''s question is not clear, ambiguous, or does not provide enough context for you to accurately answer the question, you do not try to answer it right away and you rather ask the user to clarify their request (e.g. \\\"What are some good restaurants around me?\\\" => \\\"Where are you?\\\" or \\\"When is the next flight to Tokyo\\\" => \\\"Where do you travel from?\\\")\" %}\n\n{{- bos_token }}\n\n{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content''] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = default_system_message %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{{- ''[SYSTEM_PROMPT]'' + system_message + ''[/SYSTEM_PROMPT]'' }}\n\n{%- for message in loop_messages %}\n    {%- if message[''role''] == ''user'' %}\n        {{- ''[INST]'' + message[''content''] + ''[/INST]'' }}\n    {%- elif message[''role''] == ''system'' %}\n        {{- ''[SYSTEM_PROMPT]'' + message[''content''] + ''[/SYSTEM_PROMPT]'' }}\n    {%- elif message[''role''] == ''assistant'' %}\n        {{- message[''content''] + eos_token }}\n    {%- else %}\n        {{- raise_exception(''Only user, system and assistant roles are supported!'') }}\n    {%- endif %}\n{%- endfor %}","eos_token":"</s>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:mistralai:mistral-common","source_url":"https://github.com/mistralai/mistral-common"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:mistralai:mistral-common","source_url":"https://github.com/mistralai/mistral-common"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:ollama:ollama","source_url":"https://github.com/ollama/ollama"}]', NULL, 'Apache-2.0', 'approved', 79.8, '152f0ab57b10a0a1b8f1b87dc442fbf5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-OpenAssistant-oasst-sft-6-llama-30b-xor', 'huggingface--openassistant--oasst-sft-6-llama-30b-xor', 'oasst-sft-6-llama-30b-xor', 'OpenAssistant', '--- license: other --- Due to the license attached to LLaMA models by Meta AI it is not possible to directly distribute LLaMA-based models. Instead we provide XOR weights for the OA models. Thanks to Mick for writing the script which enables this process Note: This process applies to model. The same process can be applied to other models in future, but the checksums will be different.. **This process is tested only on Linux (specifically Ubuntu). Some users have reported that the process does...', '["arxiv:2304.07327","license:other","region:us"]', 'other', 943, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\n---\n\n# OpenAssistant LLaMa 30B SFT 6\n\nDue to the license attached to LLaMA models by Meta AI it is not possible to directly distribute LLaMA-based models. Instead we provide XOR weights for the OA models.\n\nThanks to Mick for writing the `xor_codec.py` script which enables this process\n\n## The Process\n\nNote: This process applies to `oasst-sft-6-llama-30b` model. The same process can be applied to other models in future, but the checksums will be different..\n\n**This process is tested only on Linux (specifically Ubuntu). Some users have reported that the process does not work on Windows. We recommend using WSL if you only have a Windows machine.**\n\nTo use OpenAssistant LLaMA-Based Models, you should have a copy of the original LLaMA model weights and add them to a `llama` subdirectory here. If you cannot obtain the original LLaMA, see the note in italic below for a possible alternative.\n\nEnsure your LLaMA 30B checkpoint matches the correct md5sums:\n\n```\nf856e9d99c30855d6ead4d00cc3a5573  consolidated.00.pth\nd9dbfbea61309dc1e087f5081e98331a  consolidated.01.pth\n2b2bed47912ceb828c0a37aac4b99073  consolidated.02.pth\nea0405cdb5bc638fee12de614f729ebc  consolidated.03.pth\n4babdbd05b8923226a9e9622492054b6  params.json\n```\n\n*If you do not have a copy of the original LLaMA weights and cannot obtain one, you may still be able to complete this process. Some users have reported that [this model](https://huggingface.co/elinas/llama-30b-hf-transformers-4.29) can be used as a base for the XOR conversion. This will also allow you to skip to Step 7. However, we only support conversion starting from LLaMA original checkpoint and cannot provide support if you experience issues with this alternative approach.*\n\n**Important: Follow these exact steps to convert your original LLaMA checkpoint to a HuggingFace Transformers-compatible format. If you use the wrong versions of any dependency, you risk ending up with weights which are not compatible with the XOR files.**\n\n1. Create a clean Python **3.10** virtual environment & activate it:\n\n```\npython3.10 -m venv xor_venv\nsource xor_venv/bin/activate\n```\n\n2. Clone transformers repo and switch to tested version:\n\n```\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\ngit checkout d04ec99bec8a0b432fc03ed60cea9a1a20ebaf3c\npip install .\n```\n\n3. Install **exactly** these dependency versions:\n\n```\npip install torch==1.13.1 accelerate==0.18.0 sentencepiece==0.1.98 protobuf==3.20.1\n```\n\n4. Check `pip freeze` output:\n\n```\naccelerate==0.18.0\ncertifi==2022.12.7\ncharset-normalizer==3.1.0\nfilelock==3.12.0\nhuggingface-hub==0.13.4\nidna==3.4\nnumpy==1.24.2\nnvidia-cublas-cu11==11.10.3.66\nnvidia-cuda-nvrtc-cu11==11.7.99\nnvidia-cuda-runtime-cu11==11.7.99\nnvidia-cudnn-cu11==8.5.0.96\npackaging==23.1\nprotobuf==3.20.1\npsutil==5.9.5\nPyYAML==6.0\nregex==2023.3.23\nrequests==2.28.2\nsentencepiece==0.1.98\ntokenizers==0.13.3\ntorch==1.13.1\ntqdm==4.65.0\ntransformers @ file:///mnt/data/koepf/transformers\ntyping_extensions==4.5.0\nurllib3==1.26.15\n```\n\n5. While in `transformers` repo root, run HF LLaMA conversion script:\n\n```\npython src/transformers/models/llama/convert_llama_weights_to_hf.py --input_dir <input_path_llama_base>  --output_dir <output_path_llama30b_hf> --model_size 30B\n```\n\n6. Run `find . -type f -exec md5sum "{}" +` in the conversion target directory (`output_dir`). This should produce exactly the following checksums if your files are correct:\n\n```\n462a2d07f65776f27c0facfa2affb9f9  ./pytorch_model-00007-of-00007.bin\ne1dc8c48a65279fb1fbccff14562e6a3  ./pytorch_model-00003-of-00007.bin\n9cffb1aeba11b16da84b56abb773d099  ./pytorch_model-00001-of-00007.bin\naee09e21813368c49baaece120125ae3  ./generation_config.json\n92754d6c6f291819ffc3dfcaf470f541  ./pytorch_model-00005-of-00007.bin\n3eddc6fc02c0172d38727e5826181adb  ./pytorch_model-00004-of-00007.bin\neeec4125e9c7560836b4873b6f8e3025  ./tokenizer.model\n99762d59efa6b96599e863893cf2da02  ./pytorch_model-00006-of-00007.bin\n598538f18fed1877b41f77de034c0c8a  ./config.json\nfdb311c39b8659a5d5c1991339bafc09  ./tokenizer.json\nfecfda4fba7bfd911e187a85db5fa2ef  ./pytorch_model.bin.index.json\nedd1a5897748864768b1fab645b31491  ./tokenizer_config.json\n6b2e0a735969660e720c27061ef3f3d3  ./special_tokens_map.json\n5cfcb78b908ffa02e681cce69dbe4303  ./pytorch_model-00002-of-00007.bin\n```\n\n**Important: You should now have the correct LLaMA weights and be ready to apply the XORs. If the checksums above do not match yours, there is a problem.**\n\n7. Once you have LLaMA weights in the correct format, you can apply the XOR decoding:\n\n```\npython xor_codec.py oasst-sft-6-llama-30b/ oasst-sft-6-llama-30b-xor/oasst-sft-6-llama-30b-xor/ llama30b_hf/\n```\n\nYou should **expect to see one warning message** during execution:\n\n`Exception when processing ''added_tokens.json''`\n\nThis is normal. **If similar messages appear for other files, something has gone wrong**.\n\n8. Now run `find . -type f -exec md5sum "{}" +` in the output directory (here `oasst-sft-6-llama-30b`). You should get a file with exactly these checksums:\n\n```\n970e99665d66ba3fad6fdf9b4910acc5  ./pytorch_model-00007-of-00007.bin\n659fcb7598dcd22e7d008189ecb2bb42  ./pytorch_model-00003-of-00007.bin\nff6e4cf43ddf02fb5d3960f850af1220  ./pytorch_model-00001-of-00007.bin\n27b0dc092f99aa2efaf467b2d8026c3f  ./added_tokens.json\n2917a1cafb895cf57e746cfd7696bfe5  ./generation_config.json\n740c324ae65b1ec25976643cda79e479  ./pytorch_model-00005-of-00007.bin\nf7aefb4c63be2ac512fd905b45295235  ./pytorch_model-00004-of-00007.bin\neeec4125e9c7560836b4873b6f8e3025  ./tokenizer.model\n369df2f0e38bda0d9629a12a77c10dfc  ./pytorch_model-00006-of-00007.bin\ncc9dbf56b68b68a585cc7367696e06a7  ./config.json\n76d47e4f51a8df1d703c6f594981fcab  ./pytorch_model.bin.index.json\nfd9452959d711be29ccf04a97598e8d1  ./tokenizer_config.json\n785905630a0fe583122a8446a5abe287  ./special_tokens_map.json\nae48c4c68e4e171d502dd0896aa19a84  ./pytorch_model-00002-of-00007.bin\n```\n\nIf so you have successfully decoded the weights and should be able to use the model with HuggingFace Transformers. **If your checksums do not match those above, there is a problem.**\n\n### Configuration\n\n```\nllama-30b-sft-6:\n  dtype: fp16\n  log_dir: "llama_log_30b"\n  learning_rate: 1e-5\n  model_name: /home/ubuntu/Open-Assistant/model/model_training/.saved/llama-30b-super-pretrain/checkpoint-3500\n  output_dir: llama_model_30b\n  deepspeed_config: configs/zero3_config_sft.json\n  weight_decay: 0.0\n  residual_dropout: 0.0\n  max_length: 2048\n  use_flash_attention: true\n  warmup_steps: 20\n  gradient_checkpointing: true\n  gradient_accumulation_steps: 16\n  per_device_train_batch_size: 2\n  per_device_eval_batch_size: 3\n  eval_steps: 101\n  save_steps: 292\n  num_train_epochs: 8\n  save_total_limit: 3\n  use_custom_sampler: true\n  sort_by_length: false\n  save_strategy: steps\n  datasets:\n    - oasst_export:\n        lang: "bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk"\n        input_file_path: 2023-04-12_oasst_release_ready_synth.jsonl.gz\n        val_split: 0.05\n    - vicuna:\n        val_split: 0.05\n        max_val_set: 800\n        fraction: 0.8\n    - dolly15k:\n        val_split: 0.05\n        max_val_set: 300\n    - grade_school_math_instructions:\n        val_split: 0.05\n    - code_alpaca:\n        val_split: 0.05\n        max_val_set: 250\n```\n\n- **OASST dataset paper:** https://arxiv.org/abs/2304.07327', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":65059086164,"files_count":17,"spaces_count":20,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers.git","source_url":"https://github.com/huggingface/transformers.git"},{"type":"based_on_paper","target_id":"arxiv:2304.07327","source_url":"https://arxiv.org/abs/2304.07327"}]', NULL, 'Other', 'approved', 64.7, '6149fc4070a7fe3d8c35fda6fcc69ebf', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-R1-Zero', 'huggingface--deepseek-ai--deepseek-r1-zero', 'DeepSeek-R1-Zero', 'deepseek-ai', '--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/d...', '["transformers","safetensors","deepseek_v3","text-generation","conversational","custom_code","arxiv:2501.12948","license:mit","text-generation-inference","endpoints_compatible","fp8","region:us"]', 'text-generation', 937, 5016, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n\n<p align="center">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Paper Link</b>üëÅÔ∏è</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align="center">\n  <img width="80%" src="figures/benchmark.jpg">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model''s reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align="center">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align="center">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align="center">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek''s official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button "DeepThink"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face''s Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: "Please reason step by step, and put your final answer within \boxed{}."\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting "\<think\>\n\n\</think\>") when responding to certain queries, which can adversely affect the model''s performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with "\<think\>\n" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":684531386000,"storage_bytes":688586727753,"files_count":174,"spaces_count":60,"gated":false,"private":false,"config":{"architectures":["DeepseekV3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_deepseek.DeepseekV3Config","AutoModel":"modeling_deepseek.DeepseekV3Model","AutoModelForCausalLM":"modeling_deepseek.DeepseekV3ForCausalLM"},"model_type":"deepseek_v3","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''', is_first_sp=true) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\\n\\n'' + message[''content''] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{{''<ÔΩúUserÔΩú>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and ''tool_calls'' in message %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls''] %}{%- if not ns.is_first %}{%- if message[''content''] is none %}{{''<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- else %}{{''<ÔΩúAssistantÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{''\\n'' + ''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- endfor %}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and ''tool_calls'' not in message %}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'' + message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[''content''] %}{% if ''</think>'' in content %}{% set content = content.split(''</think>'')[-1] %}{% endif %}{{''<ÔΩúAssistantÔΩú>'' + content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- set ns.is_output_first = false %}{%- else %}{{''<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú><think>\\n''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2501.12948","source_url":"https://arxiv.org/abs/2501.12948"}]', NULL, 'MIT', 'approved', 99.7, '6d6179a928a64505caad38d34a3913ed', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero/resolve/main/figures/benchmark.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-R1-Zero from https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero/resolve/main/figures/benchmark.jpg
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-R1-Zero.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stable-diffusion-v1-5-stable-diffusion-v1-5', 'huggingface--stable-diffusion-v1-5--stable-diffusion-v1-5', 'stable-diffusion-v1-5', 'stable-diffusion-v1-5', '--- license: creativeml-openrail-m tags: - stable-diffusion - stable-diffusion-diffusers - text-to-image inference: true --- Modifications to the original model card are in <span style="color:crimson">red</span> or <span style="color:darkgreen">green</span> Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. For more information about how Stable Diffusion functions, please have a look at ü§ó''s Stable Diffusion blog. The ...', '["diffusers","safetensors","stable-diffusion","stable-diffusion-diffusers","text-to-image","arxiv:2207.12598","arxiv:2112.10752","arxiv:2103.00020","arxiv:2205.11487","arxiv:1910.09700","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 933, 2118840, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\ninference: true\n---\n\n# Stable Diffusion v1-5 Model Card\n\n### ‚ö†Ô∏è This repository is a mirror of the now deprecated `ruwnayml/stable-diffusion-v1-5`, this repository or organization are not affiliated in any way with RunwayML.\nModifications to the original model card are in <span style="color:crimson">red</span> or <span style="color:darkgreen">green</span>\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at [ü§ó''s Stable Diffusion blog](https://huggingface.co/blog/stable_diffusion).\n\nThe **Stable-Diffusion-v1-5** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https:/steps/huggingface.co/CompVis/stable-diffusion-v1-2) \ncheckpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on "laion-aesthetics v2 5+" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\nYou can use this both with the [üß®Diffusers library](https://github.com/huggingface/diffusers) and [RunwayML GitHub repository](https://github.com/runwayml/stable-diffusion) (<span style="color:crimson">now deprecated</span>), <span style="color:darkgreen">ComfyUI, Automatic1111, SD.Next, InvokeAI</span>.\n\n### Use with Diffusers\n```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = "sd-legacy/stable-diffusion-v1-5"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to("cuda")\n\nprompt = "a photo of an astronaut riding a horse on mars"\nimage = pipe(prompt).images[0]  \n    \nimage.save("astronaut_rides_horse.png")\n```\nFor more detailed instructions, use-cases and examples in JAX follow the instructions [here](https://github.com/huggingface/diffusers#text-to-image-generation-with-stable-diffusion)\n\n### Use with GitHub Repository <span style="color:crimson">(now deprecated)</span>, <span style="color:darkgreen">ComfyUI or Automatic1111</span>\n\n1. Download the weights \n   - [v1-5-pruned-emaonly.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors) - ema-only weight. uses less VRAM - suitable for inference\n   - [v1-5-pruned.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors) - ema+non-ema weights. uses more VRAM - suitable for fine-tuning\n\n2. Follow instructions [here](https://github.com/runwayml/stable-diffusion). <span style="color:crimson">(now deprecated)</span>\n\n3. <span style="color:darkgreen">Use locally with <a href="https://github.com/comfyanonymous/ComfyUI">ComfyUI</a>, <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui">AUTOMATIC1111</a>, <a href="https://github.com/vladmandic/automatic">SD.Next</a>, <a href="https://github.com/invoke-ai/InvokeAI">InvokeAI</a></span>\n\n## Model Details\n- **Developed by:** Robin Rombach, Patrick Esser\n- **Model type:** Diffusion-based text-to-image generation model\n- **Language(s):** English\n- **License:** [The CreativeML OpenRAIL M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) is an [Open RAIL M license](https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses), adapted from the work that [BigScience](https://bigscience.huggingface.co/) and [the RAIL Initiative](https://www.licenses.ai/) are jointly carrying in the area of responsible AI licensing. See also [the article about the BLOOM Open RAIL license](https://bigscience.huggingface.co/blog/the-bigscience-rail-license) on which our license is based.\n- **Model Description:** This is a model that can be used to generate and modify images based on text prompts. It is a [Latent Diffusion Model](https://arxiv.org/abs/2112.10752) that uses a fixed, pretrained text encoder ([CLIP ViT-L/14](https://arxiv.org/abs/2103.00020)) as suggested in the [Imagen paper](https://arxiv.org/abs/2205.11487).\n- **Resources for more information:** [GitHub Repository](https://github.com/CompVis/stable-diffusion), [Paper](https://arxiv.org/abs/2112.10752).\n- **Cite as:**\n\n      @InProceedings{Rombach_2022_CVPR,\n          author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},\n          title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n          booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n          month     = {June},\n          year      = {2022},\n          pages     = {10684-10695}\n      }\n\n# Uses\n\n## Direct Use \nThe model is intended for research purposes only. Possible research areas and\ntasks include\n\n- Safe deployment of models which have the potential to generate harmful content.\n- Probing and understanding the limitations and biases of generative models.\n- Generation of artworks and use in design and other artistic processes.\n- Applications in educational or creative tools.\n- Research on generative models.\n\nExcluded uses are described below.\n\n ### Misuse, Malicious Use, and Out-of-Scope Use\n_Note: This section is taken from the [DALLE-MINI model card](https://huggingface.co/dalle-mini/dalle-mini), but applies in the same way to Stable Diffusion v1_.\n\n\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.\n\n#### Out-of-Scope Use\nThe model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n#### Misuse and Malicious Use\nUsing the model to generate content that is cruel to individuals is a misuse of this model. This includes, but is not limited to:\n\n- Generating demeaning, dehumanizing, or otherwise harmful representations of people or their environments, cultures, religions, etc.\n- Intentionally promoting or propagating discriminatory content or harmful stereotypes.\n- Impersonating individuals without their consent.\n- Sexual content without consent of the people who might see it.\n- Mis- and disinformation\n- Representations of egregious violence and gore\n- Sharing of copyrighted or licensed material in violation of its terms of use.\n- Sharing content that is an alteration of copyrighted or licensed material in violation of its terms of use.\n\n## Limitations and Bias\n\n### Limitations\n\n- The model does not achieve perfect photorealism\n- The model cannot render legible text\n- The model does not perform well on more difficult tasks which involve compositionality, such as rendering an image corresponding to ‚ÄúA red cube on top of a blue sphere‚Äù\n- Faces and people in general may not be generated properly.\n- The model was trained mainly with English captions and will not work as well in other languages.\n- The autoencoding part of the model is lossy\n- The model was trained on a large-scale dataset\n  [LAION-5B](https://laion.ai/blog/laion-5b/) which contains adult material\n  and is not fit for product use without additional safety mechanisms and\n  considerations.\n- No additional measures were used to deduplicate the dataset. As a result, we observe some degree of memorization for images that are duplicated in the training data.\n  The training data can be searched at [https://rom1504.github.io/clip-retrieval/](https://rom1504.github.io/clip-retrieval/) to possibly assist in the detection of memorized images.\n\n### Bias\n\nWhile the capabilities of image generation models are impressive, they can also reinforce or exacerbate social biases. \nStable Diffusion v1 was trained on subsets of [LAION-2B(en)](https://laion.ai/blog/laion-5b/), \nwhich consists of images that are primarily limited to English descriptions. \nTexts and images from communities and cultures that use other languages are likely to be insufficiently accounted for. \nThis affects the overall output of the model, as white and western cultures are often set as the default. Further, the \nability of the model to generate content with non-English prompts is significantly worse than with English-language prompts.\n\n### Safety Module\n\nThe intended use of this model is with the [Safety Checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) in Diffusers. \nThis checker works by checking model outputs against known hard-coded NSFW concepts.\nThe concepts are intentionally hidden to reduce the likelihood of reverse-engineering this filter.\nSpecifically, the checker compares the class probability of harmful concepts in the embedding space of the `CLIPTextModel` *after generation* of the images. \nThe concepts are passed into the model with the generated image and compared to a hand-engineered weight for each NSFW concept.\n\n\n## Training\n\n**Training Data**\nThe model developers used the following dataset for training the model:\n\n- LAION-2B (en) and subsets thereof (see next section)\n\n**Training Procedure**\nStable Diffusion v1-5 is a latent diffusion model which combines an autoencoder with a diffusion model that is trained in the latent space of the autoencoder. During training, \n\n- Images are encoded through an encoder, which turns images into latent representations. The autoencoder uses a relative downsampling factor of 8 and maps images of shape H x W x 3 to latents of shape H/f x W/f x 4\n- Text prompts are encoded through a ViT-L/14 text-encoder.\n- The non-pooled output of the text encoder is fed into the UNet backbone of the latent diffusion model via cross-attention.\n- The loss is a reconstruction objective between the noise that was added to the latent and the prediction made by the UNet.\n\nCurrently six Stable Diffusion checkpoints are provided, which were trained as follows.\n- [`stable-diffusion-v1-1`](https://huggingface.co/CompVis/stable-diffusion-v1-1): 237,000 steps at resolution `256x256` on [laion2B-en](https://huggingface.co/datasets/laion/laion2B-en).\n  194,000 steps at resolution `512x512` on [laion-high-resolution](https://huggingface.co/datasets/laion/laion-high-resolution) (170M examples from LAION-5B with resolution `>= 1024x1024`).\n- [`stable-diffusion-v1-2`](https://huggingface.co/CompVis/stable-diffusion-v1-2): Resumed from `stable-diffusion-v1-1`.\n  515,000 steps at resolution `512x512` on "laion-improved-aesthetics" (a subset of laion2B-en,\nfiltered to images with an original size `>= 512x512`, estimated aesthetics score `> 5.0`, and an estimated watermark probability `< 0.5`. The watermark estimate is from the LAION-5B metadata, the aesthetics score is estimated using an [improved aesthetics estimator](https://github.com/christophschuhmann/improved-aesthetic-predictor)).\n- [`stable-diffusion-v1-3`](https://huggingface.co/CompVis/stable-diffusion-v1-3): Resumed from `stable-diffusion-v1-2` - 195,000 steps at resolution `512x512` on "laion-improved-aesthetics" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n- [`stable-diffusion-v1-4`](https://huggingface.co/CompVis/stable-diffusion-v1-4) Resumed from `stable-diffusion-v1-2` - 225,000 steps at resolution `512x512` on "laion-aesthetics v2 5+" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n- [`stable-diffusion-v1-5`](https://huggingface.co/sd-legacy/stable-diffusion-v1-5) Resumed from `stable-diffusion-v1-2` - 595,000 steps at resolution `512x512` on "laion-aesthetics v2 5+" and 10 % dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n- [`stable-diffusion-inpainting`](https://huggingface.co/sd-legacy/stable-diffusion-inpainting) Resumed from `stable-diffusion-v1-5` - then 440,000 steps of inpainting training at resolution 512x512 on ‚Äúlaion-aesthetics v2 5+‚Äù and 10% dropping of the text-conditioning. For inpainting, the UNet has 5 additional input channels (4 for the encoded masked-image and 1 for the mask itself) whose weights were zero-initialized after restoring the non-inpainting checkpoint. During training, we generate synthetic masks and in 25% mask everything.\n\n- **Hardware:** 32 x 8 x A100 GPUs\n- **Optimizer:** AdamW\n- **Gradient Accumulations**: 2\n- **Batch:** 32 x 8 x 2 x 4 = 2048\n- **Learning rate:** warmup to 0.0001 for 10,000 steps and then kept constant\n\n## Evaluation Results \nEvaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0,\n5.0, 6.0, 7.0, 8.0) and 50 PNDM/PLMS sampling\nsteps show the relative improvements of the checkpoints:\n\n![pareto](https://huggingface.co/CompVis/stable-diffusion/resolve/main/v1-1-to-v1-5.png)\n\nEvaluated using 50 PLMS steps and 10000 random prompts from the COCO2017 validation set, evaluated at 512x512 resolution.  Not optimized for FID scores.\n## Environmental Impact\n\n**Stable Diffusion v1** **Estimated Emissions**\nBased on that information, we estimate the following CO2 emissions using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700). The hardware, runtime, cloud provider, and compute region were utilized to estimate the carbon impact.\n\n- **Hardware Type:** A100 PCIe 40GB\n- **Hours used:** 150000\n- **Cloud Provider:** AWS\n- **Compute Region:** US-east\n- **Carbon Emitted (Power consumption x Time x Carbon produced based on location of power grid):** 11250 kg CO2 eq.\n\n\n## Citation\n\n```bibtex\n    @InProceedings{Rombach_2022_CVPR,\n        author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj\"orn},\n        title     = {High-Resolution Image Synthesis With Latent Diffusion Models},\n        booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n        month     = {June},\n        year      = {2022},\n        pages     = {10684-10695}\n    }\n```\n\n*This model card was written by: Robin Rombach and Patrick Esser and is based on the [DALL-E Mini model card](https://huggingface.co/dalle-mini/dalle-mini).*', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":49898615889,"files_count":36,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:runwayml:stable-diffusion","source_url":"https://github.com/runwayml/stable-diffusion"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers#text-to-image-generation-with-stable-diffusion"},{"type":"has_code","target_id":"github:runwayml:stable-diffusion","source_url":"https://github.com/runwayml/stable-diffusion"},{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI\">ComfyUI<","source_url":"https://github.com/comfyanonymous/ComfyUI\">ComfyUI<"},{"type":"has_code","target_id":"github:AUTOMATIC1111:stable-diffusion-webui\">AUTOMATIC1111<","source_url":"https://github.com/AUTOMATIC1111/stable-diffusion-webui\">AUTOMATIC1111<"},{"type":"has_code","target_id":"github:vladmandic:automatic\">SD.Next<","source_url":"https://github.com/vladmandic/automatic\">SD.Next<"},{"type":"has_code","target_id":"github:invoke-ai:InvokeAI\">InvokeAI<","source_url":"https://github.com/invoke-ai/InvokeAI\">InvokeAI<"},{"type":"has_code","target_id":"github:CompVis:stable-diffusion","source_url":"https://github.com/CompVis/stable-diffusion"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:christophschuhmann:improved-aesthetic-predictor","source_url":"https://github.com/christophschuhmann/improved-aesthetic-predictor"},{"type":"based_on_paper","target_id":"arxiv:2207.12598","source_url":"https://arxiv.org/abs/2207.12598"},{"type":"based_on_paper","target_id":"arxiv:2112.10752","source_url":"https://arxiv.org/abs/2112.10752"},{"type":"based_on_paper","target_id":"arxiv:2103.00020","source_url":"https://arxiv.org/abs/2103.00020"},{"type":"based_on_paper","target_id":"arxiv:2205.11487","source_url":"https://arxiv.org/abs/2205.11487"},{"type":"based_on_paper","target_id":"arxiv:1910.09700","source_url":"https://arxiv.org/abs/1910.09700"}]', NULL, 'creativeml-openrail-m', 'approved', 79.7, 'a1d2a53990eac9257d8dc73b1875bcbd', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-Phi-3.5-mini-instruct', 'huggingface--microsoft--phi-3.5-mini-instruct', 'Phi-3.5-mini-instruct', 'microsoft', '--- license: mit license_link: https://huggingface.co/microsoft/Phi-3.5-mini-instruct/resolve/main/LICENSE language: - multilingual pipeline_tag: text-generation tags: - nlp - code widget: - messages: - role: user content: Can you provide ways to eat combinations of bananas and dragonfruits? library_name: transformers --- üéâ**Phi-4**: [multimodal-instruct | onnx]; [mini-instruct | onnx] Phi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic d...', '["transformers","safetensors","phi3","text-generation","nlp","code","conversational","custom_code","multilingual","arxiv:2404.14219","arxiv:2407.13833","arxiv:2403.06412","license:mit","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 932, 380469, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/Phi-3.5-mini-instruct","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlicense_link: https://huggingface.co/microsoft/Phi-3.5-mini-instruct/resolve/main/LICENSE\nlanguage:\n- multilingual\npipeline_tag: text-generation\ntags:\n- nlp\n- code\nwidget:\n- messages:\n  - role: user\n    content: Can you provide ways to eat combinations of bananas and dragonfruits?\nlibrary_name: transformers\n---\nüéâ**Phi-4**: [[multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-multimodal-instruct-onnx)]; \n[[mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) | [onnx](https://huggingface.co/microsoft/Phi-4-mini-instruct-onnx)]\n\n## Model Summary\n\nPhi-3.5-mini is a lightweight, state-of-the-art open model built upon datasets used for Phi-3 - synthetic data and filtered publicly available websites - with a focus on very high-quality, reasoning dense data. The model belongs to the Phi-3 model family and supports 128K token context length. The model underwent a rigorous enhancement process, incorporating both supervised fine-tuning, proximal policy optimization, and direct preference optimization to ensure precise instruction adherence and robust safety measures.\n\nüè° [Phi-3 Portal](https://azure.microsoft.com/en-us/products/phi-3) <br>\nüì∞ [Phi-3 Microsoft Blog](https://aka.ms/phi3.5-techblog) <br>\nüìñ [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219) <br>\nüë©‚Äçüç≥ [Phi-3 Cookbook](https://github.com/microsoft/Phi-3CookBook) <br>\nüñ•Ô∏è [Try It](https://aka.ms/try-phi3.5mini) <br>\n\n**Phi-3.5**: [[mini-instruct](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) | [onnx](https://huggingface.co/microsoft/Phi-3.5-mini-instruct-onnx)]; [[MoE-instruct]](https://huggingface.co/microsoft/Phi-3.5-MoE-instruct); [[vision-instruct]](https://huggingface.co/microsoft/Phi-3.5-vision-instruct)\n\n## Intended Uses\n\n### Primary Use Cases\n\nThe model is intended for commercial and research use in multiple languages. The model provides uses for general purpose AI systems and applications which require:\n\n1) Memory/compute constrained environments\n2) Latency bound scenarios\n3) Strong reasoning (especially code, math and logic)\n\nOur model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. \n\n### Use Case Considerations\n\nOur models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable laws or regulations (including privacy, trade compliance laws, etc.) that are relevant to their use case.\n\n***Nothing contained in this Model Card should be interpreted as or deemed a restriction or modification to the license the model is released under.*** \n\n## Release Notes \n\nThis is an update over the June 2024 instruction-tuned Phi-3 Mini release based on valuable user feedback. The model used additional post-training data leading to substantial gains on multilingual, multi-turn conversation quality, and reasoning capability. We believe most use cases will benefit from this release, but we encourage users to test in their particular AI applications. We appreciate the enthusiastic adoption of the Phi-3 model family, and continue to welcome all feedback from the community.\n\n### Multilingual\n\nThe table below highlights multilingual capability of the Phi-3.5 Mini on multilingual MMLU, MEGA, and multilingual MMLU-pro datasets. Overall, we observed that even with just 3.8B active parameters, the model is  competitive on multilingual tasks in comparison to other models with a much bigger active parameters.  \n\n| Benchmark                  | Phi-3.5 Mini-Ins | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|----------------------------|------------------|-----------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| Multilingual MMLU          | 55.4             | 51.08                 | 47.4                     | 58.9                      | 56.2             | 63.8           | 77.2             | 72.9                          |\n| Multilingual MMLU-Pro      | 30.9             | 30.21                 | 15.0                     | 34.0                      | 21.4             | 43.0           | 57.9             | 53.2                          |\n| MGSM                       | 47.9             | 41.56                 | 31.8                     | 63.3                      | 56.7             | 75.1           | 75.8             | 81.7                          |\n| MEGA MLQA                  | 61.7             | 55.5                  | 43.9                     | 61.2                      | 45.2             | 54.4           | 61.6             | 70.0                          |\n| MEGA TyDi QA               | 62.2             | 55.9                  | 54.0                     | 63.7                      | 54.5             | 65.6           | 63.6             | 81.8                          |\n| MEGA UDPOS                 | 46.5             | 48.1                  | 57.2                     | 58.2                      | 54.1             | 56.6           | 62.4             | 66.0                          |\n| MEGA XCOPA                 | 63.1             | 62.4                  | 58.8                     | 10.8                      | 21.1             | 31.2           | 95.0             | 90.3                          |\n| MEGA XStoryCloze           | 73.5             | 73.6                  | 75.5                     | 92.3                      | 71.0             | 87.0           | 20.7             | 96.6                          |\n| **Average** | **55.2** | **52.3** | **47.9** | **55.3** | **47.5** | **59.6** | **64.3** | **76.6** |\n\nThe table below shows Multilingual MMLU scores in some of the supported languages. For more multi-lingual benchmarks and details, see [Appendix A](#appendix-a).\n\n| Benchmark | Phi-3.5 Mini-Ins | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|-----------|------------------|-----------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| Arabic    | 44.2             | 35.4                  | 33.7                     | 45.3                      | 49.1             | 56.3           | 73.6             | 67.1                          |\n| Chinese   | 52.6             | 46.9                  | 45.9                     | 58.2                      | 54.4             | 62.7           | 66.7             | 70.8                          |\n| Dutch     | 57.7             | 48.0                  | 51.3                     | 60.1                      | 55.9             | 66.7           | 80.6             | 74.2                          |\n| French    | 61.1             | 61.7                  | 53.0                     | 63.8                      | 62.8             | 67.0           | 82.9             | 75.6                          |\n| German    | 62.4             | 61.3                  | 50.1                     | 64.5                      | 59.9             | 65.7           | 79.5             | 74.3                          |\n| Italian   | 62.8             | 63.1                  | 52.5                     | 64.1                      | 55.9             | 65.7           | 82.6             | 75.9                          |\n| Russian   | 50.4             | 45.3                  | 48.9                     | 59.0                      | 57.4             | 63.2           | 78.7             | 72.6                          |\n| Spanish   | 62.6             | 61.3                  | 53.9                     | 64.3                      | 62.6             | 66.0           | 80.0             | 75.5                          |\n| Ukrainian | 45.2             | 36.7                  | 46.9                     | 56.6                      | 52.9             | 62.0           | 77.4             | 72.6                          |\n\n### Long Context\n\nPhi-3.5-mini supports 128K context length, therefore the model is capable of several long context tasks including long document/meeting summarization, long document QA, long document information retrieval. We see that Phi-3.5-mini is clearly better than Gemma-2 family which only supports 8K context length. Phi-3.5-mini is competitive with other much larger open-weight models such as Llama-3.1-8B-instruct, Mistral-7B-instruct-v0.3, and Mistral-Nemo-12B-instruct-2407.\n\n| Benchmark | Phi-3.5-mini-instruct | Llama-3.1-8B-instruct | Mistral-7B-instruct-v0.3 | Mistral-Nemo-12B-instruct-2407 | Gemini-1.5-Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|--|--|--|--|--|--|--|\n| GovReport | 25.9 | 25.1 | 26.0 | 25.6 | 27.8 | 24.8 |\n| QMSum | 21.3 | 21.6 | 21.3 | 22.1 | 24.0 | 21.7 |\n| Qasper | 41.9 | 37.2 | 31.4 | 30.7 | 43.5 | 39.8 |\n| SQuALITY | 25.3 | 26.2 | 25.9 | 25.8 | 23.5 | 23.8 |\n| SummScreenFD | 16.0 | 17.6 | 17.5 | 18.2 | 16.3 | 17.0 |\n| **Average** | **26.1** | **25.5** | **24.4** | **24.5** | **27.0** | **25.4** |\n\nRULER: a retrieval-based benchmark for long context understanding\n| Model | 4K | 8K | 16K | 32K | 64K | 128K | Average |\n|--|--|--|--|--|--|--|--|\n| **Phi-3.5-mini-instruct** | 94.3 | 91.1 | 90.7 | 87.1 | 78.0 | 63.6 | **84.1** |\n| **Llama-3.1-8B-instruct** | 95.5 | 93.8 | 91.6 | 87.4 | 84.7 | 77.0 | **88.3** |\n| **Mistral-Nemo-12B-instruct-2407** | 87.8 | 87.2 | 87.7 | 69.0 | 46.8 | 19.0 | **66.2** |\n\nRepoQA: a benchmark for long context code understanding\n| Model | Python | C++ | Rust | Java | TypeScript | Average |\n|--|--|--|--|--|--|--|\n| **Phi-3.5-mini-instruct** | 86 | 67 | 73 | 77 | 82 | **77** |\n| **Llama-3.1-8B-instruct** | 80 | 65 | 73 | 76 | 63 | **71** |\n| **Mistral-7B-instruct-v0.3** | 61 | 57 | 51 | 61 | 80 | **62** |\n\n## Usage\n\n### Requirements\nPhi-3 family has been integrated in the `4.43.0` version of `transformers`. The current `transformers` version can be verified with: `pip list | grep transformers`.\n\nExamples of required packages:\n```\nflash_attn==2.5.8\ntorch==2.3.1\naccelerate==0.31.0\ntransformers==4.43.0\n```\n\nPhi-3.5-mini-instruct is also available in [Azure AI Studio](https://aka.ms/try-phi3.5mini)\n\n### Tokenizer\n\nPhi-3.5-mini-Instruct supports a vocabulary size of up to `32064` tokens. The [tokenizer files](https://huggingface.co/microsoft/Phi-3.5-mini-instruct/blob/main/added_tokens.json) already provide placeholder tokens that can be used for downstream fine-tuning, but they can also be extended up to the model''s vocabulary size.\n\n### Input Formats\nGiven the nature of the training data, the Phi-3.5-mini-instruct model is best suited for prompts using the chat format as follows:\n\n```\n<|system|>\nYou are a helpful assistant.<|end|>\n<|user|>\nHow to explain Internet for a medieval knight?<|end|>\n<|assistant|>\n```\n\n### Loading the model locally\nAfter obtaining the Phi-3.5-mini-instruct model checkpoint, users can use this sample code for inference.\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntorch.random.manual_seed(0)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    "microsoft/Phi-3.5-mini-instruct", \n    device_map="cuda", \n    torch_dtype="auto", \n    trust_remote_code=True, \n)\ntokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3.5-mini-instruct")\n\nmessages = [\n    {"role": "system", "content": "You are a helpful AI assistant."},\n    {"role": "user", "content": "Can you provide ways to eat combinations of bananas and dragonfruits?"},\n    {"role": "assistant", "content": "Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey."},\n    {"role": "user", "content": "What about solving an 2x + 3 = 7 equation?"},\n]\n\npipe = pipeline(\n    "text-generation",\n    model=model,\n    tokenizer=tokenizer,\n)\n\ngeneration_args = {\n    "max_new_tokens": 500,\n    "return_full_text": False,\n    "temperature": 0.0,\n    "do_sample": False,\n}\n\noutput = pipe(messages, **generation_args)\nprint(output[0][''generated_text''])\n```\n\nNotes: If you want to use flash attention, call _AutoModelForCausalLM.from_pretrained()_ with _attn_implementation="flash_attention_2"_\n\n## Responsible AI Considerations\n\nLike other language models, the Phi family of models can potentially behave in ways that are unfair, unreliable, or offensive. Some of the limiting behaviors to be aware of include:  \n+ Quality of Service: The Phi models are trained primarily on English text and some additional multilingual text. Languages other than English will experience worse performance as well as performance disparities across non-English. English language varieties with less representation in the training data might experience worse performance than standard American English.   \n+ Multilingual performance and safety gaps: We believe it is important to make language models more widely available across different languages, but the Phi 3 models still exhibit challenges common across multilingual releases. As with any deployment of LLMs, developers will be better positioned to test for performance or safety gaps for their linguistic and cultural context and customize the model with additional fine-tuning and appropriate safeguards.\n+ Representation of Harms & Perpetuation of Stereotypes: These models can over- or under-represent groups of people, erase representation of some groups, or reinforce demeaning or negative stereotypes. Despite safety post-training, these limitations may still be present due to differing levels of representation of different groups, cultural contexts, or prevalence of examples of negative stereotypes in training data that reflect real-world patterns and societal biases. \n+ Inappropriate or Offensive Content: These models may produce other types of inappropriate or offensive content, which may make it inappropriate to deploy for sensitive contexts without additional mitigations that are specific to the case. \n+ Information Reliability: Language models can generate nonsensical content or fabricate content that might sound reasonable but is inaccurate or outdated.  \n+ Limited Scope for Code: Majority of Phi-3 training data is based in Python and use common packages such as "typing, math, random, collections, datetime, itertools". If the model generates Python scripts that utilize other packages or scripts in other languages, we strongly recommend users manually verify all API uses.\n+ Long Conversation: Phi-3 models, like other models, can in some cases generate responses that are repetitive, unhelpful, or inconsistent in very long chat sessions in both English and non-English languages. Developers are encouraged to place appropriate mitigations, like limiting conversation turns to account for the possible conversational drift\n\nDevelopers should apply responsible AI best practices, including mapping, measuring, and mitigating risks associated with their specific use case and cultural, linguistic context. Phi-3 family of models are general purpose models. As developers plan to deploy these models for specific use cases, they are encouraged to fine-tune the models for their use case and leverage the models as part of broader AI systems with language-specific safeguards in place. Important areas for consideration include:  \n\n+ Allocation: Models may not be suitable for scenarios that could have consequential impact on legal status or the allocation of resources or life opportunities (ex: housing, employment, credit, etc.) without further assessments and additional debiasing techniques.\n+ High-Risk Scenarios: Developers should assess the suitability of using models in high-risk scenarios where unfair, unreliable or offensive outputs might be extremely costly or lead to harm. This includes providing advice in sensitive or expert domains where accuracy and reliability are critical (ex: legal or health advice). Additional safeguards should be implemented at the application level according to the deployment context. \n+ Misinformation: Models may produce inaccurate information. Developers should follow transparency best practices and inform end-users they are interacting with an AI system. At the application level, developers can build feedback mechanisms and pipelines to ground responses in use-case specific, contextual information, a technique known as Retrieval Augmented Generation (RAG).   \n+ Generation of Harmful Content: Developers should assess outputs for their context and use available safety classifiers or custom solutions appropriate for their use case. \n+ Misuse: Other forms of misuse such as fraud, spam, or malware production may be possible, and developers should ensure that their applications do not violate applicable laws and regulations.\n\n## Training\n\n### Model\n\n**Architecture:** Phi-3.5-mini has 3.8B parameters and is a dense decoder-only Transformer model using the same tokenizer as Phi-3 Mini.<br>\n**Inputs:** Text. It is best suited for prompts using chat format.<br>\n**Context length:** 128K tokens<br>\n**GPUs:** 512 H100-80G<br>\n**Training time:** 10 days<br>\n**Training data:** 3.4T tokens<br>\n**Outputs:** Generated text in response to the input<br>\n**Dates:** Trained between June and August 2024<br>\n**Status:** This is a static model trained on an offline dataset with cutoff date October 2023 for publicly available data. Future versions of the tuned models may be released as we improve models.<br>\n**Supported languages:** Arabic, Chinese, Czech, Danish, Dutch, English, Finnish, French, German, Hebrew, Hungarian, Italian, Japanese, Korean, Norwegian, Polish, Portuguese, Russian, Spanish, Swedish, Thai, Turkish, Ukrainian<br>\n**Release date:** August 2024<br>\n\n### Training Datasets\nOur training data includes a wide variety of sources, totaling 3.4 trillion tokens, and is a combination of \n1) publicly available documents filtered rigorously for quality, selected high-quality educational data, and code;\n2) newly created synthetic, ‚Äútextbook-like‚Äù data for the purpose of teaching math, coding, common sense reasoning, general knowledge of the world (science, daily activities, theory of mind, etc.);\n3) high quality chat format supervised data covering various topics to reflect human preferences on different aspects such as instruct-following, truthfulness, honesty and helpfulness. \n\nWe are focusing on the quality of data that could potentially improve the reasoning ability for the model, and we filter the publicly available documents to contain the correct level of knowledge. As an example, the result of a game in premier league in a particular day might be good training data for frontier models, but we need to remove such information to leave more model capacity for reasoning for the small size models. More details about data can be found in the [Phi-3 Technical Report](https://arxiv.org/pdf/2404.14219).\n\n### Fine-tuning\n\nA basic example of multi-GPUs supervised fine-tuning (SFT) with TRL and Accelerate modules is provided [here](https://huggingface.co/microsoft/Phi-3.5-mini-instruct/resolve/main/sample_finetune.py).\n\n## Benchmarks\n\nWe report the results under completion format for Phi-3.5-mini on standard open-source benchmarks measuring the model''s reasoning ability (both common sense reasoning and logical reasoning). We compare to Mistral-7B-Instruct-v0.3,  Mistral-Nemo-12B-Ins-2407, Llama-3.1-8B-Ins, Gemma-2-9B-Ins, Gemini 1.5 Flash, and GPT-4o-mini-2024-07-18 (Chat).\n\nAll the reported numbers are produced with the exact same pipeline to ensure that the numbers are comparable. These numbers might differ from other published numbers due to slightly different choices in the evaluation.\n\nAs is now standard, we use few-shot prompts to evaluate the models, at temperature 0. \nThe prompts and number of shots are part of a Microsoft internal tool to evaluate language models, and in particular we did no optimization to the pipeline for Phi-3.\nMore specifically, we do not change prompts, pick different few-shot examples, change prompt format, or do any other form of optimization for the model.\n\nThe number of k‚Äìshot examples is listed per-benchmark. At the high-level overview of the model quality on representative benchmarks: \n\n| Category       | Benchmark                | Phi-3.5 Mini-Ins | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|----------------|--------------------------|------------------|--------------------------|---------------------------|------------------|----------------|------------------|------------------------------|\n| Popular aggregated benchmark | Arena Hard | 37 | 18.1 | 39.4 | 25.7 | 42 | 55.2 | 75 |\n|                | BigBench Hard CoT (0-shot) | 69 | 33.4 | 60.2 | 63.4 | 63.5 | 66.7 | 80.4 |\n|                | MMLU (5-shot) | 69 | 60.3 | 67.2 | 68.1 | 71.3 | 78.7 | 77.2 |\n|                | MMLU-Pro (0-shot, CoT) | 47.4 | 18 | 40.7 | 44 | 50.1 | 57.2 | 62.8 |\n| Reasoning      | ARC Challenge (10-shot) | 84.6 | 77.9 | 84.8 | 83.1 | 89.8 | 92.8 | 93.5 |\n|                | BoolQ (2-shot) | 78 | 80.5 | 82.5 | 82.8 | 85.7 | 85.8 | 88.7 |\n|                | GPQA (0-shot, CoT) | 30.4 | 15.6 | 28.6 | 26.3 | 29.2 | 37.5 | 41.1 |\n|                | HellaSwag (5-shot) | 69.4 | 71.6 | 76.7 | 73.5 | 80.9 | 67.5 | 87.1 |\n|                | OpenBookQA (10-shot) | 79.2 | 78 | 84.4 | 84.8 | 89.6 | 89 | 90 |\n|                | PIQA (5-shot) | 81 | 73.4 | 83.5 | 81.2 | 83.7 | 87.5 | 88.7 |\n|                | Social IQA (5-shot) | 74.7 | 73 | 75.3 | 71.8 | 74.7 | 77.8 | 82.9 |\n|                | TruthfulQA (MC2) (10-shot) | 64 | 64.7 | 68.1 | 69.2 | 76.6 | 76.6 | 78.2 |\n|                | WinoGrande (5-shot) | 68.5 | 58.1 | 70.4 | 64.7 | 74 | 74.7 | 76.9 |\n| Multilingual   | Multilingual MMLU (5-shot) | 55.4 | 47.4 | 58.9 | 56.2 | 63.8 | 77.2 | 72.9 |\n|                | MGSM (0-shot CoT) | 47.9 | 31.8 | 63.3 | 56.7 | 76.4 | 75.8 | 81.7 |\n| Math           | GSM8K (8-shot, CoT) | 86.2 | 54.4 | 84.2 | 82.4 | 84.9 | 82.4 | 91.3 |\n|                | MATH (0-shot, CoT) | 48.5 | 19 | 31.2 | 47.6 | 50.9 | 38 | 70.2 |\n| Long context   | Qasper | 41.9 | 31.4 | 30.7 | 37.2 | 13.9 | 43.5 | 39.8 |\n|                | SQuALITY | 24.3 | 25.9 | 25.8 | 26.2 | 0 | 23.5 | 23.8 |\n| Code Generation| HumanEval (0-shot) | 62.8 | 35.4 | 63.4 | 66.5 | 61 | 74.4 | 86.6 |\n|                | MBPP (3-shot) | 69.6 | 50.4 | 68.1 | 69.4 | 69.3 | 77.5 | 84.1 |\n| **Average** | | **61.4** | **48.5** | **61.3** | **61.0** | **63.3** | **68.5** | **74.9** |\n\nWe take a closer look at different categories across public benchmark datasets at the table below:\n\n| Category                   | Phi-3.5 Mini-Ins | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|----------------------------|------------------|--------------------------|---------------------------|------------------|----------------|------------------|------------------------------|\n| Popular aggregated benchmark | 55.6 | 32.5 | 51.9 | 50.3 | 56.7 | 64.5 | 73.9 |\n| Reasoning                  | 70.1 | 65.2 | 72.2 | 70.5 | 75.4 | 77.7 | 80 |\n| Language understanding     | 62.6 | 62.8 | 67 | 62.9 | 72.8 | 66.6 | 76.8 |\n| Robustness                 | 59.7 | 53.4 | 65.2 | 59.8 | 64.7 | 68.9 | 77.5 |\n| Long context               | 26.1 | 25.5 | 24.4 | 24.5 | 0 | 27 | 25.4 |\n| Math                       | 67.4 | 36.7 | 57.7 | 65 | 67.9 | 60.2 | 80.8 |\n| Code generation            | 62 | 43.1 | 56.9 | 65.8 | 58.3 | 66.8 | 69.9 |\n| Multilingual               | 55.2 | 47.9 | 55.3 | 47.5 | 59.6 | 64.3 | 76.6 |\n\nOverall, the model with only 3.8B-param achieves a similar level of multilingual language understanding and reasoning ability as much larger models.\nHowever, it is still fundamentally limited by its size for certain tasks. \nThe model simply does not have the capacity to store too much factual knowledge, therefore, users may experience factual incorrectness. \nHowever, we believe such weakness can be resolved by augmenting Phi-3.5 with a search engine, particularly when using the model under RAG settings.  \n\n## Safety Evaluation and Red-Teaming\n\nWe leveraged various evaluation techniques including red teaming, adversarial conversation simulations, and multilingual safety evaluation benchmark datasets to \nevaluate Phi-3.5 models'' propensity to produce undesirable outputs across multiple languages and risk categories. \nSeveral approaches were used to compensate for the limitations of one approach alone. Findings across the various evaluation methods indicate that safety \npost-training that was done as detailed in the [Phi-3 Safety Post-Training paper](https://arxiv.org/pdf/2407.13833) had a positive impact across multiple languages and risk categories as observed by \nrefusal rates (refusal to output undesirable outputs) and robustness to jailbreak techniques. Note, however, while comprehensive red team evaluations were conducted \nacross all models in the prior release of Phi models, red teaming was largely focused on Phi-3.5 MOE across multiple languages and risk categories for this release as \nit is the largest and more capable model of the three models. Details on prior red team evaluations across Phi models can be found in the [Phi-3 Safety Post-Training paper](https://arxiv.org/pdf/2407.13833). \nFor this release, insights from red teaming indicate that the models may refuse to generate undesirable outputs in English, even when the request for undesirable output \nis in another language. Models may also be more susceptible to longer multi-turn jailbreak techniques across both English and non-English languages. These findings \nhighlight the need for industry-wide investment in the development of high-quality safety evaluation datasets across multiple languages, including low resource languages, \nand risk areas that account for cultural nuances where those languages are spoken.\n\n\n## Software\n* [PyTorch](https://github.com/pytorch/pytorch)\n* [Transformers](https://github.com/huggingface/transformers)\n* [Flash-Attention](https://github.com/HazyResearch/flash-attention)\n\n## Hardware\nNote that by default, the Phi-3.5-mini-instruct model uses flash attention, which requires certain types of GPU hardware to run. We have tested on the following GPU types:\n* NVIDIA A100\n* NVIDIA A6000\n* NVIDIA H100\n\nIf you want to run the model on:\n* NVIDIA V100 or earlier generation GPUs: call AutoModelForCausalLM.from_pretrained() with attn_implementation="eager"\n  \n## License\nThe model is licensed under the [MIT license](./LICENSE).\n\n## Trademarks\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow‚ÄØ[Microsoft‚Äôs Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks). Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party‚Äôs policies.\n\n\n## Appendix A\n\n#### MGSM\n\n| Languages | Phi-3.5-Mini-Instruct | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|-----------|------------------------|---------------------------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| German    | 69.6                   | 65.2                                  | 42.4                     | 74.4                      | 68.4             | 76.8           | 81.6             | 82.8                          |\n| English   | 85.2                   | 83.2                                  | 60.0                     | 86.0                      | 81.2             | 88.8           | 90.8             | 90.8                          |\n| Spanish   | 79.2                   | 77.6                                  | 46.4                     | 75.6                      | 66.4             | 82.4           | 84.8             | 86.8                          |\n| French    | 71.6                   | 72.8                                  | 47.2                     | 70.4                      | 66.8             | 74.4           | 77.2             | 81.6                          |\n| Japanese  | 50.0                   | 35.2                                  | 22.8                     | 62.4                      | 49.2             | 67.6           | 77.6             | 80.4                          |\n| Russian   | 67.2                   | 51.6                                  | 43.2                     | 73.6                      | 67.2             | 78.4           | 84.8             | 86.4                          |\n| Thai      | 29.6                   | 6.4                                   | 18.4                     | 53.2                      | 56.0             | 76.8           | 87.6             | 81.6                          |\n| Chinese   | 60.0                   | 52.8                                  | 42.4                     | 66.4                      | 68.0             | 72.8           | 82.0             | 82.0                          |\n\n#### Multilingual MMLU-pro \n\n| Languages  | Phi-3.5-Mini-Instruct | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|------------|-----------------------|---------------------------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| Czech      | 24.9                  | 26.3                                  | 14.6                     | 30.6                      | 23.0             | 40.5           | 59.0             | 40.9                          |\n| English    | 47.7                  | 46.2                                  | 17.7                     | 39.8                      | 43.1             | 49.0           | 66.1             | 62.7                          |\n| Finnish    | 22.3                  | 20.5                                  | 11.5                     | 30.4                      | 9.7              | 37.5           | 54.5             | 50.1                          |\n| Norwegian  | 29.9                  | 27.8                                  | 14.4                     | 33.2                      | 22.2             | 44.4           | 60.7             | 59.1                          |\n| Polish     | 25.7                  | 26.4                                  | 16.3                     | 33.6                      | 9.2              | 41.7           | 53.9             | 42.8                          |\n| Portuguese | 38.7                  | 37.6                                  | 15.3                     | 36.0                      | 29.3             | 43.5           | 54.0             | 56.9                          |\n| Swedish    | 30.7                  | 28.1                                  | 15.5                     | 34.3                      | 16.9             | 42.6           | 57.7             | 55.5                          |\n\n#### MEGA \n\n##### MLQA\n\n| Languages | Phi-3.5-Mini-Instruct | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|-----------|-----------------------|---------------------------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| Arabic    | 54.3                  | 32.7                                  | 23.5                     | 31.4                      | 31.5             | 57.4           | 63.8             | 64.0                          |\n| Chinese   | 36.1                  | 31.8                                  | 22.4                     | 27.4                      | 18.6             | 45.4           | 38.1             | 38.9                          |\n| English   | 80.3                  | 78.9                                  | 68.2                     | 75.5                      | 67.2             | 82.9           | 69.5             | 82.2                          |\n| German    | 61.8                  | 59.1                                  | 49.0                     | 57.8                      | 38.9             | 63.8           | 55.9             | 64.1                          |\n| Spanish   | 68.8                  | 67.0                                  | 50.3                     | 63.6                      | 52.7             | 72.8           | 59.6             | 70.1                          |\n\n##### TyDi QA \n\n| Languages | Phi-3.5-Mini-Instruct | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|-----------|-----------------------|---------------------------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| Arabic    | 69.7                  | 54.4                                  | 52.5                     | 49.8                      | 33.7             | 81.1           | 78.8             | 84.9                          |\n| English   | 82.0                  | 82.0                                  | 60.5                     | 77.3                      | 65.1             | 82.4           | 60.9             | 81.8                          |\n| Finnish   | 70.3                  | 64.3                                  | 68.6                     | 57.1                      | 74.4             | 85.7           | 73.5             | 84.8                          |\n| Japanese  | 65.4                  | 56.7                                  | 45.3                     | 54.8                      | 34.1             | 74.6           | 59.7             | 73.3                          |\n| Korean    | 74.0                  | 60.4                                  | 54.5                     | 54.2                      | 54.9             | 83.8           | 60.7             | 82.3                          |\n| Russian   | 63.5                  | 62.7                                  | 52.3                     | 55.7                      | 27.4             | 69.8           | 60.1             | 72.5                          |\n| Thai      | 64.4                  | 49.0                                  | 51.8                     | 43.5                      | 48.5             | 81.4           | 71.6             | 78.2                          |\n\n##### XCOPA \n\n| Languages | Phi-3.5-Mini-Instruct | Phi-3.0-Mini-128k-Instruct (June2024) | Mistral-7B-Instruct-v0.3 | Mistral-Nemo-12B-Ins-2407 | Llama-3.1-8B-Ins | Gemma-2-9B-Ins | Gemini 1.5 Flash | GPT-4o-mini-2024-07-18 (Chat) |\n|-----------|-----------------------|---------------------------------------|--------------------------|---------------------------|------------------|----------------|------------------|-------------------------------|\n| English   | 94.6                  | 94.6                                  | 85.6                     | 94.4                      | 37.6             | 63.8           | 92.0             | 98.2                          |\n| Italian   | 86.8                  | 84.8                                  | 76.8                     | 83.2                      | 16.2             | 37.2           | 85.6             | 97.6                          |\n| Turkish   | 58.6                  | 57.2                                  | 61.6                     | 56.6                      | 38.4             | 60.2           | 91.4             | 94.6                          |\n\n\n## Appendix B: Korean benchmarks\n\nThe prompt is the same as the [CLIcK paper](https://arxiv.org/abs/2403.06412) prompt. The experimental results below were given with max_tokens=512 (zero-shot), max_tokens=1024 (5-shot), temperature=0.01. No system prompt used.\n\n- GPT-4o: 2024-05-13 version\n- GPT-4o-mini: 2024-07-18 version\n- GPT-4-turbo: 2024-04-09 version\n- GPT-3.5-turbo: 2023-06-13 version\n\nThe overall Korean benchmarks show that the Phi-3.5-Mini-Instruct with only 3.8B params outperforms Llama-3.1-8B-Instruct.\n\n| Benchmarks               |   Phi-3.5-Mini-Instruct |  Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:-------------------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| CLIcK                    |                   42.99 |                           29.12 |                   47.82 |    80.46 |         68.5  |         72.82 |           50.98 |\n| HAERAE 1.0               |                   44.21 |                           36.41 |                   53.9  |    85.7  |         76.4  |         77.76 |           52.67 |\n| KMMLU (0-shot, CoT)      |                   35.87 |                           30.82 |                   38.54 |    64.26 |         52.63 |         58.75 |           40.3  |\n| KMMLU (5-shot)           |                   37.35 |                           29.98 |                   20.21 |    64.28 |         51.62 |         59.29 |           42.28 |\n| KMMLU-HARD (0-shot, CoT) |                   24    |                           25.68 |                   24.03 |    39.62 |         24.56 |         30.56 |           20.97 | \n| KMMLU-HARD (5-shot)      |                   24.76 |                           25.73 |                   15.81 |    40.94 |         24.63 |         31.12 |           21.19 |\n| **Average**                  |                   **35.62** |                           **29.99** |                   **29.29** |    **62.54** |         **50.08** |         **56.74** |           **39.61** | \n\n#### CLIcK (Cultural and Linguistic Intelligence in Korean)\n\n##### Accuracy by supercategory\n| supercategory   |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Culture         |                   43.77 |                           29.74 |                   51.15 |    81.89 |         70.95 |         73.61 |           53.38 |\n| Language        |                   41.38 |                           27.85 |                   40.92 |    77.54 |         63.54 |         71.23 |           46    |\n| **Overall**     |                   42.99 |                           29.12 |                   47.82 |    80.46 |         68.5  |         72.82 |           50.98 |\n\n##### Accuracy by category\n| supercategory   | category    |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|:------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Culture         | Economy     |                   61.02 |                           28.81 |                   66.1  |    94.92 |         83.05 |         89.83 |           64.41 |\n| Culture         | Geography   |                   45.8  |                           29.01 |                   54.2  |    80.15 |         77.86 |         82.44 |           53.44 |\n| Culture         | History     |                   26.15 |                           30    |                   29.64 |    66.92 |         48.4  |         46.4  |           31.79 |\n| Culture         | Law         |                   32.42 |                           22.83 |                   44.29 |    70.78 |         57.53 |         61.19 |           41.55 |\n| Culture         | Politics    |                   54.76 |                           33.33 |                   59.52 |    88.1  |         83.33 |         89.29 |           65.48 |\n| Culture         | Pop Culture |                   60.98 |                           34.15 |                   60.98 |    97.56 |         85.37 |         92.68 |           75.61 |\n| Culture         | Society     |                   54.37 |                           31.72 |                   65.05 |    92.88 |         85.44 |         86.73 |           71.2  |\n| Culture         | Tradition   |                   47.75 |                           31.98 |                   54.95 |    87.39 |         74.77 |         79.28 |           55.86 |\n| Language        | Functional  |                   37.6  |                           24    |                   32.8  |    84.8  |         64.8  |         80    |           40    |\n| Language        | Grammar     |                   27.5  |                           23.33 |                   22.92 |    57.08 |         42.5  |         47.5  |           30    |\n| Language        | Textual     |                   54.74 |                           33.33 |                   59.65 |    91.58 |         80.7  |         87.37 |           62.11 |\n\n#### HAERAE\n\n| category              |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| General Knowledge     |                   31.25 |                           28.41 |                   34.66 |    77.27 |         53.41 |         66.48 |           40.91 |\n| History               |                   32.45 |                           22.34 |                   44.15 |    92.02 |         84.57 |         78.72 |           30.32 |\n| Loan Words            |                   47.93 |                           35.5  |                   63.31 |    79.88 |         76.33 |         78.11 |           59.17 |\n| Rare Words            |                   55.06 |                           42.96 |                   63.21 |    87.9  |         81.98 |         79.01 |           61.23 |\n| Reading Comprehension |                   42.95 |                           41.16 |                   51.9  |    85.46 |         77.18 |         80.09 |           56.15 |\n| Standard Nomenclature |                   44.44 |                           32.68 |                   58.82 |    88.89 |         75.82 |         79.08 |           53.59 |\n| **Overall**           |                   44.21 |                           36.41 |                   53.9  |    85.7  |         76.4  |         77.76 |           52.67 |\n\n#### KMMLU (0-shot, CoT)\n\n| supercategory   |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                   35.8  |                           31.68 |                   37.03 |    61.52 |         49.29 |         55.98 |           38.47 |\n| HUMSS           |                   31.56 |                           26.47 |                   37.29 |    69.45 |         56.59 |         63    |           40.9  |\n| Other           |                   35.45 |                           31.01 |                   39.15 |    63.79 |         52.35 |         57.53 |           40.19 |\n| STEM            |                   38.54 |                           31.9  |                   40.42 |    65.16 |         54.74 |         60.84 |           42.24 |\n| **Overall**     |                   35.87 |                           30.82 |                   38.54 |    64.26 |         52.63 |         58.75 |           40.3  |\n\n#### KMMLU (5-shot)\n\n| supercategory   |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                   37.42 |                           29.98 |                   19.24 |    61.47 |         48.66 |         56.85 |           40.22 |\n| HUMSS           |                   34.72 |                           27.27 |                   22.5  |    68.79 |         55.95 |         63.68 |           43.35 |\n| Other           |                   37.04 |                           30.76 |                   20.95 |    64.21 |         51.1  |         57.85 |           41.92 |\n| STEM            |                   38.9  |                           30.73 |                   19.55 |    65.28 |         53.29 |         61.08 |           44.43 |\n| **Overall**     |                   37.35 |                           29.98 |                   20.21 |    64.28 |         51.62 |         59.29 |           42.28 |\n\n#### KMMLU-HARD (0-shot, CoT)\n\n| supercategory   |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                   27.08 |                           26.17 |                   26.25 |    37.12 |         22.25 |         29.17 |           21.07 |\n| HUMSS           |                   20.21 |                           24.38 |                   20.21 |    41.97 |         23.31 |         31.51 |           19.44 |\n| Other           |                   23.05 |                           24.82 |                   23.88 |    40.39 |         26.48 |         29.59 |           22.22 |\n| STEM            |                   24.36 |                           26.91 |                   24.64 |    39.82 |         26.36 |         32.18 |           20.91 |\n| **Overall**     |                   24    |                           25.68 |                   24.03 |    39.62 |         24.56 |         30.56 |           20.97 |\n\n#### KMMLU-HARD (5-shot)\n\n| supercategory   |   Phi-3.5-Mini-Instruct |   Phi-3.0-Mini-128k-Instruct (June2024) |   Llama-3.1-8B-Instruct |   GPT-4o |   GPT-4o-mini |   GPT-4-turbo |   GPT-3.5-turbo |\n|:----------------|------------------------:|--------------------------------:|------------------------:|---------:|--------------:|--------------:|----------------:|\n| Applied Science |                   25    |                           29    |                   12    |    31    |         21    |         25    |           20    |\n| HUMSS           |                   21.89 |                           19.92 |                   14    |    43.98 |         23.47 |         33.53 |           19.53 |\n| Other           |                   23.26 |                           27.27 |                   12.83 |    39.84 |         28.34 |         29.68 |           23.22 |\n| STEM            |                   20.5  |                           25.25 |                   12.75 |    40.25 |         23.25 |         27.25 |           19.75 |\n| **Overall**     |                   24.76 |                           25.73 |                   15.81 |    40.94 |         24.63 |         31.12 |           21.19 |', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":3821079552,"storage_bytes":7642681603,"files_count":19,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Phi3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_phi3.Phi3Config","AutoModelForCausalLM":"modeling_phi3.Phi3ForCausalLM"},"model_type":"phi3","tokenizer_config":{"bos_token":"<s>","chat_template":"{% for message in messages %}{% if message[''role''] == ''system'' and message[''content''] %}{{''<|system|>\n'' + message[''content''] + ''<|end|>\n''}}{% elif message[''role''] == ''user'' %}{{''<|user|>\n'' + message[''content''] + ''<|end|>\n''}}{% elif message[''role''] == ''assistant'' %}{{''<|assistant|>\n'' + message[''content''] + ''<|end|>\n''}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ ''<|assistant|>\n'' }}{% else %}{{ eos_token }}{% endif %}","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:microsoft:Phi-3CookBook","source_url":"https://github.com/microsoft/Phi-3CookBook"},{"type":"has_code","target_id":"github:pytorch:pytorch","source_url":"https://github.com/pytorch/pytorch"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:HazyResearch:flash-attention","source_url":"https://github.com/HazyResearch/flash-attention"},{"type":"based_on_paper","target_id":"arxiv:2404.14219","source_url":"https://arxiv.org/abs/2404.14219"},{"type":"based_on_paper","target_id":"arxiv:2407.13833","source_url":"https://arxiv.org/abs/2407.13833"},{"type":"based_on_paper","target_id":"arxiv:2403.06412","source_url":"https://arxiv.org/abs/2403.06412"}]', NULL, 'MIT', 'approved', 79.7, 'd54cff83d717d604469007ee3ebc7562', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen2.5-7B-Instruct', 'huggingface--qwen--qwen2.5-7b-instruct', 'Qwen2.5-7B-Instruct', 'Qwen', '--- license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/LICENSE language: - en pipeline_tag: text-generation base_model: Qwen/Qwen2.5-7B tags: - chat library_name: transformers --- <a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> Qwen2.5 is the latest series of Qwen large la...', '["transformers","safetensors","qwen2","text-generation","chat","conversational","en","arxiv:2309.00071","arxiv:2407.10671","base_model:qwen/qwen2.5-7b","base_model:finetune:qwen/qwen2.5-7b","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 932, 7245333, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen2.5-7B-Instruct","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\nbase_model: Qwen/Qwen2.5-7B\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# Qwen2.5-7B-Instruct\n<a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 7B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 7.61B\n- Number of Paramaters (Non-Embedding): 6.53B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 28 for Q and 4 for KV\n- Context Length: Full 131,072 tokens and generation 8192 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: ''qwen2''\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen2.5-7B-Instruct"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  "rope_scaling": {\n    "factor": 4.0,\n    "original_max_position_embeddings": 32768,\n    "type": "yarn"\n  }\n}\n```\n\nFor deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [üìë blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7615616512,"storage_bytes":15231271888,"files_count":14,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2407.10671","source_url":"https://arxiv.org/abs/2407.10671"}]', NULL, 'Apache-2.0', 'approved', 64.7, 'beb786d83a3a7728edad444d5ad99d6b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-guoyww-animatediff', 'huggingface--guoyww--animatediff', 'animatediff', 'guoyww', '--- license: apache-2.0 --- This model repo is for AnimateDiff.', '["license:apache-2.0","region:us"]', 'other', 931, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/guoyww/animatediff","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\n---\nThis model repo is for [AnimateDiff](https://github.com/guoyww/animatediff/).', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":32554557069,"files_count":18,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:guoyww:animatediff","source_url":"https://github.com/guoyww/animatediff"}]', NULL, 'Apache-2.0', 'approved', 39.7, '4797596fa8754451ddf56149673c3a49', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Kijai-flux-fp8', 'huggingface--kijai--flux-fp8', 'flux-fp8', 'Kijai', '--- language: - en license: other viewer: false tags: - diffusion-single-file - comfyui --- and weights of: https://huggingface.co/black-forest-labs/FLUX.1-dev is , original upload name kept for backwards compatibility. weights of: https://huggingface.co/black-forest-labs/FLUX.1-schnell https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro flux1-dev-fp8.safetensors, flux_shakker_labs_union_pro-fp8_e4m3fn and it''s variants falls under the Non-Commercial License. flux1-schnell-fp8...', '["diffusion-single-file","comfyui","en","license:other","region:us"]', 'other', 931, 55768, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Kijai/flux-fp8","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n  - en\nlicense: other\nviewer: false\ntags:\n  - diffusion-single-file\n  - comfyui\n---\n`float8_e4m3fn` and `float8_e5m2` weights of:\n\nhttps://huggingface.co/black-forest-labs/FLUX.1-dev\n\n`flux1-dev-fp8.safetensors` is `float8_e4m3fn`, original upload name kept for backwards compatibility.\n\n`float8_e4m3fn` weights of:\n\nhttps://huggingface.co/black-forest-labs/FLUX.1-schnell\n\nhttps://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro\n# License\nflux1-dev-fp8.safetensors, flux_shakker_labs_union_pro-fp8_e4m3fn and it''s variants falls under the [`FLUX.1 [dev]` Non-Commercial License](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md).\n\nflux1-schnell-fp8.safetensors falls under the [Apache-2.0 License](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md).\n', '{"pipeline_tag":null,"library_name":"diffusion-single-file","framework":"diffusion-single-file","params":null,"storage_bytes":74868593126,"files_count":8,"spaces_count":3,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'Other', 'approved', 49.7, 'f41ee74e670c1b1b017cb79142915f70', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepset-roberta-base-squad2', 'huggingface--deepset--roberta-base-squad2', 'roberta-base-squad2', 'deepset', '--- language: en license: cc-by-4.0 datasets: - squad_v2 model-index: - name: deepset/roberta-base-squad2 results: - task: type: question-answering name: Question Answering dataset: name: squad_v2 type: squad_v2 config: squad_v2 split: validation metrics: - type: exact_match value: 79.9309 name: Exact Match verified: true verifyToken: >- eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDhhNjg5YzNiZGQ1YTIyYTAwZGUwOWEzZTRiYzdjM2QzYjA3ZTUxNDM1NjE1MTUyMjE1MGY1YzEzMjRjYzVjYiIsInZlcnNpb24iOjF9.EH5...', '["transformers","pytorch","tf","jax","rust","safetensors","roberta","question-answering","en","dataset:squad_v2","base_model:facebookai/roberta-base","base_model:finetune:facebookai/roberta-base","license:cc-by-4.0","model-index","endpoints_compatible","deploy:azure","region:us"]', 'question-answering', 930, 733628, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepset/roberta-base-squad2","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: en\nlicense: cc-by-4.0\ndatasets:\n- squad_v2\nmodel-index:\n- name: deepset/roberta-base-squad2\n  results:\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_v2\n      type: squad_v2\n      config: squad_v2\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 79.9309\n      name: Exact Match\n      verified: true\n      verifyToken: >-\n        eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDhhNjg5YzNiZGQ1YTIyYTAwZGUwOWEzZTRiYzdjM2QzYjA3ZTUxNDM1NjE1MTUyMjE1MGY1YzEzMjRjYzVjYiIsInZlcnNpb24iOjF9.EH5JJo8EEFwU7osPz3s7qanw_tigeCFhCXjSfyN0Y1nWVnSfulSxIk_DbAEI5iE80V4EKLyp5-mYFodWvL2KDA\n    - type: f1\n      value: 82.9501\n      name: F1\n      verified: true\n      verifyToken: >-\n        eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjk5ZDYwOGQyNjNkMWI0OTE4YzRmOTlkY2JjNjQ0YTZkNTMzMzNkYTA0MDFmNmI3NjA3NjNlMjhiMDQ2ZjJjNSIsInZlcnNpb24iOjF9.DDm0LNTkdLbGsue58bg1aH_s67KfbcmkvL-6ZiI2s8IoxhHJMSf29H_uV2YLyevwx900t-MwTVOW3qfFnMMEAQ\n    - type: total\n      value: 11869\n      name: total\n      verified: true\n      verifyToken: >-\n        eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGFkMmI2ODM0NmY5NGNkNmUxYWViOWYxZDNkY2EzYWFmOWI4N2VhYzY5MGEzMTVhOTU4Zjc4YWViOGNjOWJjMCIsInZlcnNpb24iOjF9.fexrU1icJK5_MiifBtZWkeUvpmFISqBLDXSQJ8E6UnrRof-7cU0s4tX_dIsauHWtUpIHMPZCf5dlMWQKXZuAAA\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad\n      type: squad\n      config: plain_text\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 85.289\n      name: Exact Match\n    - type: f1\n      value: 91.841\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: adversarial_qa\n      type: adversarial_qa\n      config: adversarialQA\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 29.5\n      name: Exact Match\n    - type: f1\n      value: 40.367\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squad_adversarial\n      type: squad_adversarial\n      config: AddOneSent\n      split: validation\n    metrics:\n    - type: exact_match\n      value: 78.567\n      name: Exact Match\n    - type: f1\n      value: 84.469\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts amazon\n      type: squadshifts\n      config: amazon\n      split: test\n    metrics:\n    - type: exact_match\n      value: 69.924\n      name: Exact Match\n    - type: f1\n      value: 83.284\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts new_wiki\n      type: squadshifts\n      config: new_wiki\n      split: test\n    metrics:\n    - type: exact_match\n      value: 81.204\n      name: Exact Match\n    - type: f1\n      value: 90.595\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts nyt\n      type: squadshifts\n      config: nyt\n      split: test\n    metrics:\n    - type: exact_match\n      value: 82.931\n      name: Exact Match\n    - type: f1\n      value: 90.756\n      name: F1\n  - task:\n      type: question-answering\n      name: Question Answering\n    dataset:\n      name: squadshifts reddit\n      type: squadshifts\n      config: reddit\n      split: test\n    metrics:\n    - type: exact_match\n      value: 71.55\n      name: Exact Match\n    - type: f1\n      value: 82.939\n      name: F1\nbase_model:\n- FacebookAI/roberta-base\n---\n\n# roberta-base for Extractive QA \n\nThis is the [roberta-base](https://huggingface.co/roberta-base) model, fine-tuned using the [SQuAD2.0](https://huggingface.co/datasets/squad_v2) dataset. It''s been trained on question-answer pairs, including unanswerable questions, for the task of Extractive Question Answering. \nWe have also released a distilled version of this model called [deepset/tinyroberta-squad2](https://huggingface.co/deepset/tinyroberta-squad2). It has a comparable prediction quality and runs at twice the speed of [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2).\n\n\n## Overview\n**Language model:** roberta-base  \n**Language:** English  \n**Downstream-task:** Extractive QA  \n**Training data:** SQuAD 2.0  \n**Eval data:** SQuAD 2.0  \n**Code:**  See [an example extractive QA pipeline built with Haystack](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline)  \n**Infrastructure**: 4x Tesla v100\n\n## Hyperparameters\n\n```\nbatch_size = 96\nn_epochs = 2\nbase_LM_model = "roberta-base"\nmax_seq_len = 386\nlearning_rate = 3e-5\nlr_schedule = LinearWarmup\nwarmup_proportion = 0.2\ndoc_stride=128\nmax_query_length=64\n``` \n\n## Usage\n\n### In Haystack\nHaystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. \nTo load and run the model with [Haystack](https://github.com/deepset-ai/haystack/):\n```python\n# After running pip install haystack-ai "transformers[torch,sentencepiece]"\n\nfrom haystack import Document\nfrom haystack.components.readers import ExtractiveReader\n\ndocs = [\n    Document(content="Python is a popular programming language"),\n    Document(content="python ist eine beliebte Programmiersprache"),\n]\n\nreader = ExtractiveReader(model="deepset/roberta-base-squad2")\nreader.warm_up()\n\nquestion = "What is a popular programming language?"\nresult = reader.run(query=question, documents=docs)\n# {''answers'': [ExtractedAnswer(query=''What is a popular programming language?'', score=0.5740374326705933, data=''python'', document=Document(id=..., content: ''...''), context=None, document_offset=ExtractedAnswer.Span(start=0, end=6),...)]}\n```\nFor a complete example with an extractive question answering pipeline that scales over many documents, check out the [corresponding Haystack tutorial](https://haystack.deepset.ai/tutorials/34_extractive_qa_pipeline).\n\n### In Transformers\n```python\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n\nmodel_name = "deepset/roberta-base-squad2"\n\n# a) Get predictions\nnlp = pipeline(''question-answering'', model=model_name, tokenizer=model_name)\nQA_input = {\n    ''question'': ''Why is model conversion important?'',\n    ''context'': ''The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.''\n}\nres = nlp(QA_input)\n\n# b) Load model & tokenizer\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n```\n\n## Performance\nEvaluated on the SQuAD 2.0 dev set with the [official eval script](https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/).\n\n```\n"exact": 79.87029394424324,\n"f1": 82.91251169582613,\n\n"total": 11873,\n"HasAns_exact": 77.93522267206478,\n"HasAns_f1": 84.02838248389763,\n"HasAns_total": 5928,\n"NoAns_exact": 81.79983179142137,\n"NoAns_f1": 81.79983179142137,\n"NoAns_total": 5945\n```\n\n## Authors\n**Branden Chan:** branden.chan@deepset.ai  \n**Timo M√∂ller:** timo.moeller@deepset.ai  \n**Malte Pietsch:** malte.pietsch@deepset.ai  \n**Tanay Soni:**  tanay.soni@deepset.ai \n\n## About us\n\n<div class="grid lg:grid-cols-2 gap-x-4 gap-y-3">\n    <div class="w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center">\n         <img alt="" src="https://raw.githubusercontent.com/deepset-ai/.github/main/deepset-logo-colored.png" class="w-40"/>\n     </div>\n     <div class="w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center">\n         <img alt="" src="https://raw.githubusercontent.com/deepset-ai/.github/main/haystack-logo-colored.png" class="w-40"/>\n     </div>\n</div>\n\n[deepset](http://deepset.ai/) is the company behind the production-ready open-source AI framework [Haystack](https://haystack.deepset.ai/).\n\nSome of our other work: \n- [Distilled roberta-base-squad2 (aka "tinyroberta-squad2")](https://huggingface.co/deepset/tinyroberta-squad2)\n- [German BERT](https://deepset.ai/german-bert), [GermanQuAD and GermanDPR](https://deepset.ai/germanquad), [German embedding model](https://huggingface.co/mixedbread-ai/deepset-mxbai-embed-de-large-v1)\n- [deepset Cloud](https://www.deepset.ai/deepset-cloud-product)\n- [deepset Studio](https://www.deepset.ai/deepset-studio)\n\n## Get in touch and join the Haystack community\n\n<p>For more info on Haystack, visit our <strong><a href="https://github.com/deepset-ai/haystack">GitHub</a></strong> repo and <strong><a href="https://docs.haystack.deepset.ai">Documentation</a></strong>. \n\nWe also have a <strong><a class="h-7" href="https://haystack.deepset.ai/community">Discord community open to everyone!</a></strong></p>\n\n[Twitter](https://twitter.com/Haystack_AI) | [LinkedIn](https://www.linkedin.com/company/deepset-ai/) | [Discord](https://haystack.deepset.ai/community) | [GitHub Discussions](https://github.com/deepset-ai/haystack/discussions) | [Website](https://haystack.deepset.ai/) | [YouTube](https://www.youtube.com/@deepset_ai)\n\nBy the way: [we''re hiring!](http://www.deepset.ai/jobs)', '{"pipeline_tag":"question-answering","library_name":"transformers","framework":"transformers","params":124057092,"storage_bytes":3942257079,"files_count":12,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["RobertaForQuestionAnswering"],"model_type":"roberta","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"},{"type":"has_code","target_id":"github:deepset-ai:haystack\">GitHub<","source_url":"https://github.com/deepset-ai/haystack\">GitHub<"},{"type":"has_code","target_id":"github:deepset-ai:haystack","source_url":"https://github.com/deepset-ai/haystack"}]', NULL, 'CC-BY-4.0', 'approved', 64.7, '2d959bc022b2c652b46dc6c8942f1a86', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-black-forest-labs-FLUX.2-dev', 'huggingface--black-forest-labs--flux.2-dev', 'FLUX.2-dev', 'black-forest-labs', '', '["diffusers","safetensors","image-generation","image-editing","flux","diffusion-single-file","image-to-image","en","license:other","diffusers:flux2pipeline","region:us"]', 'image-to-image', 930, 207838, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/black-forest-labs/FLUX.2-dev","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":177640024576,"files_count":39,"spaces_count":49,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"Flux2Pipeline"}}}', '[]', '[]', NULL, 'Other', 'approved', 39.7, '0e632496351c04584f5042bb10115cf3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-facebook-seamless-m4t-v2-large', 'huggingface--facebook--seamless-m4t-v2-large', 'seamless-m4t-v2-large', 'facebook', '--- license: cc-by-nc-4.0 language: - af - am - ar - as - az - be - bn - bs - bg - ca - cs - zh - cy - da - de - el - en - et - fi - fr - or - om - ga - gl - gu - ha - he - hi - hr - hu - hy - ig - id - is - it - jv - ja - kn - ka - kk - mn - km - ky - ko - lo - ln - lt - lb - lg - lv - ml - mr - mk - mt - mi - my - nl - nb - ne - ny - oc - pa - ps - fa - pl - pt - ro - ru - sk - sl - sn - sd - so - es - sr - sv - sw - ta - te - tg - tl - th - tr - uk - ur - uz - vi - wo - xh - yo - ms - zu -...', '["transformers","safetensors","seamless_m4t_v2","feature-extraction","audio-to-audio","text-to-speech","seamless_communication","automatic-speech-recognition","af","am","ar","as","az","be","bn","bs","bg","ca","cs","zh","cy","da","de","el","en","et","fi","fr","or","om","ga","gl","gu","ha","he","hi","hr","hu","hy","ig","id","is","it","jv","ja","kn","ka","kk","mn","km","ky","ko","lo","ln","lt","lb","lg","lv","ml","mr","mk","mt","mi","my","nl","nb","ne","ny","oc","pa","ps","fa","pl","pt","ro","ru","sk","sl","sn","sd","so","es","sr","sv","sw","ta","te","tg","tl","th","tr","uk","ur","uz","vi","wo","xh","yo","ms","zu","ary","arz","yue","kea","arxiv:2312.05187","license:cc-by-nc-4.0","region:us"]', 'automatic-speech-recognition', 929, 51645, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/facebook/seamless-m4t-v2-large","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: cc-by-nc-4.0\nlanguage:\n- af\n- am\n- ar\n- as\n- az\n- be\n- bn\n- bs\n- bg\n- ca\n- cs\n- zh\n- cy\n- da\n- de\n- el\n- en\n- et\n- fi\n- fr\n- or\n- om\n- ga\n- gl\n- gu\n- ha\n- he\n- hi\n- hr\n- hu\n- hy\n- ig\n- id\n- is\n- it\n- jv\n- ja\n- kn\n- ka\n- kk\n- mn\n- km\n- ky\n- ko\n- lo\n- ln\n- lt\n- lb\n- lg\n- lv\n- ml\n- mr\n- mk\n- mt\n- mi\n- my\n- nl\n- nb\n- ne\n- ny\n- oc\n- pa\n- ps\n- fa\n- pl\n- pt\n- ro\n- ru\n- sk\n- sl\n- sn\n- sd\n- so\n- es\n- sr\n- sv\n- sw\n- ta\n- te\n- tg\n- tl\n- th\n- tr\n- uk\n- ur\n- uz\n- vi\n- wo\n- xh\n- yo\n- ms\n- zu\n- ary\n- arz\n- yue\n- kea\nmetrics:\n- bleu\n- wer\n- chrf\ninference: False\npipeline_tag: automatic-speech-recognition\ntags:\n  - audio-to-audio\n  - text-to-speech\n  - seamless_communication  \nlibrary_name: transformers\nwidget:\n  - src: https://cdn-media.huggingface.co/speech_samples/sample1.flac\n    example_title: Librispeech sample 1\n    output:\n      text: going along slushy country roads and speaking to damp audiences in draughty schoolrooms day after day for a fortnight he''ll have to put in an appearance at some place of worship on sunday morning and he can come to us immediately afterwards\n  - src: https://cdn-media.huggingface.co/speech_samples/sample2.flac\n    example_title: Librispeech sample 2\n    output:\n      text: before he had time to answer a much-encumbered vera burst into the room with the question i say can i leave these here these were a small black pig and a lusty specimen of black-red game-cock\n---\n\n# SeamlessM4T v2\n\n**SeamlessM4T** is our foundational all-in-one **M**assively **M**ultilingual and **M**ultimodal **M**achine **T**ranslation model delivering high-quality translation for speech and text in nearly 100 languages.\n\nSeamlessM4T models support the tasks of:\n- Speech-to-speech translation (S2ST)\n- Speech-to-text translation (S2TT)\n- Text-to-speech translation (T2ST)\n- Text-to-text translation (T2TT)\n- Automatic speech recognition (ASR).\n\nSeamlessM4T models support:\n- üé§ 101 languages for speech input.\n- üí¨ 96 Languages for text input/output.\n- üîä 35 languages for speech output.\n  \nüåü We are releasing SeamlessM4T v2, an updated version with our novel *UnitY2* architecture. \nThis new model improves over SeamlessM4T v1 in quality as well as inference speed in speech generation tasks.\n\nThe v2 version of SeamlessM4T is a multitask adaptation of our novel *UnitY2* architecture. \n*Unity2* with its hierarchical character-to-unit upsampling and non-autoregressive text-to-unit decoding considerably improves over SeamlessM4T v1 in quality and inference speed.\n\n**SeamlessM4T v2 is also supported by ü§ó Transformers, more on it [in the dedicated section below](#transformers-usage).**\n\n![SeamlessM4T architectures](seamlessm4t_arch.svg)\n\n## SeamlessM4T  models\n| Model Name         | #params | checkpoint                                                                              | metrics                                                                              |\n| ------------------ | ------- | --------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ |\n| [SeamlessM4T-Large v2](https://huggingface.co/facebook/seamless-m4t-v2-large)  | 2.3B    | [checkpoint](https://huggingface.co/facebook/seamless-m4t-v2-large/blob/main/seamlessM4T_v2_large.pt)   | [metrics](https://dl.fbaipublicfiles.com/seamless/metrics/seamlessM4T_large_v2.zip)  |\n| [SeamlessM4T-Large (v1)](https://huggingface.co/facebook/seamless-m4t-large) | 2.3B    | [checkpoint](https://huggingface.co/facebook/seamless-m4t-large/blob/main/multitask_unity_large.pt)   | [metrics](https://dl.fbaipublicfiles.com/seamless/metrics/seamlessM4T_large.zip)  |\n| [SeamlessM4T-Medium (v1)](https://huggingface.co/facebook/seamless-m4t-medium) | 1.2B    | [checkpoint](https://huggingface.co/facebook/seamless-m4t-medium/blob/main/multitask_unity_medium.pt) | [metrics](https://dl.fbaipublicfiles.com/seamless/metrics/seamlessM4T_medium.zip) |\n\nWe provide the extensive evaluation results of seamlessM4T-Large and SeamlessM4T-Medium reported in the paper (as averages) in the `metrics` files above.\n\nThe evaluation data ids for FLEURS, CoVoST2 and CVSS-C can be found [here](https://dl.fbaipublicfiles.com/seamless/metrics/evaluation_data_ids.zip)\n\n\n## Evaluating SeamlessM4T models\nTo reproduce our results or to evaluate using the same metrics over your own test sets, please check out the [Evaluation README here](https://github.com/facebookresearch/seamless_communication/tree/main/src/seamless_communication/cli/m4t/evaluate).\n\n\n## Finetuning SeamlessM4T models\nPlease check out the [Finetuning README here](https://github.com/facebookresearch/seamless_communication/tree/main/src/seamless_communication/cli/m4t/finetune).\n\n## Transformers usage\n\nSeamlessM4T is available in the ü§ó Transformers library, requiring minimal dependencies. Steps to get started:\n\n1. First install the ü§ó [Transformers library](https://github.com/huggingface/transformers) from main and [sentencepiece](https://github.com/google/sentencepiece):\n\n```\npip install git+https://github.com/huggingface/transformers.git sentencepiece\n```\n\n2. Run the following Python code to generate speech samples. Here the target language is Russian:\n\n```py\nfrom transformers import AutoProcessor, SeamlessM4Tv2Model\nimport torchaudio\n\nprocessor = AutoProcessor.from_pretrained("facebook/seamless-m4t-v2-large")\nmodel = SeamlessM4Tv2Model.from_pretrained("facebook/seamless-m4t-v2-large")\n\n# from text\ntext_inputs = processor(text = "Hello, my dog is cute", src_lang="eng", return_tensors="pt")\naudio_array_from_text = model.generate(**text_inputs, tgt_lang="rus")[0].cpu().numpy().squeeze()\n\n# from audio\naudio, orig_freq =  torchaudio.load("https://www2.cs.uic.edu/~i101/SoundFiles/preamble10.wav")\naudio =  torchaudio.functional.resample(audio, orig_freq=orig_freq, new_freq=16_000) # must be a 16 kHz waveform array\naudio_inputs = processor(audios=audio, return_tensors="pt")\naudio_array_from_audio = model.generate(**audio_inputs, tgt_lang="rus")[0].cpu().numpy().squeeze()\n```\n\n3. Listen to the audio samples either in an ipynb notebook:\n\n```py\nfrom IPython.display import Audio\n\nsample_rate = model.config.sampling_rate\nAudio(audio_array_from_text, rate=sample_rate)\n# Audio(audio_array_from_audio, rate=sample_rate)\n```\n\nOr save them as a `.wav` file using a third-party library, e.g. `scipy`:\n\n```py\nimport scipy\n\nsample_rate = model.config.sampling_rate\nscipy.io.wavfile.write("out_from_text.wav", rate=sample_rate, data=audio_array_from_text)\n# scipy.io.wavfile.write("out_from_audio.wav", rate=sample_rate, data=audio_array_from_audio)\n```\nFor more details on using the SeamlessM4T model for inference using the ü§ó Transformers library, refer to the \n**[SeamlessM4T v2 docs](https://huggingface.co/docs/transformers/main/en/model_doc/seamless_m4t_v2)** or to this **hands-on [Google Colab](https://colab.research.google.com/github/ylacombe/scripts_and_notebooks/blob/main/v2_seamless_m4t_hugging_face.ipynb).**\n\n\n## Supported Languages:\n\nListed below, are the languages supported by SeamlessM4T-large (v1/v2).\nThe `source` column specifies whether a language is supported as source speech (`Sp`) and/or source text (`Tx`).\nThe `target` column specifies whether a language is supported as target speech (`Sp`) and/or target text (`Tx`).\n\n\n| code | language               | script     | Source | Target |\n| ---- | ---------------------- | ---------- | ------ | ------ |\n| afr  | Afrikaans              | Latn       | Sp, Tx | Tx     |\n| amh  | Amharic                | Ethi       | Sp, Tx | Tx     |\n| arb  | Modern Standard Arabic | Arab       | Sp, Tx | Sp, Tx |\n| ary  | Moroccan Arabic        | Arab       | Sp, Tx | Tx     |\n| arz  | Egyptian Arabic        | Arab       | Sp, Tx | Tx     |\n| asm  | Assamese               | Beng       | Sp, Tx | Tx     |\n| ast  | Asturian               | Latn       | Sp     | \--    |\n| azj  | North Azerbaijani      | Latn       | Sp, Tx | Tx     |\n| bel  | Belarusian             | Cyrl       | Sp, Tx | Tx     |\n| ben  | Bengali                | Beng       | Sp, Tx | Sp, Tx |\n| bos  | Bosnian                | Latn       | Sp, Tx | Tx     |\n| bul  | Bulgarian              | Cyrl       | Sp, Tx | Tx     |\n| cat  | Catalan                | Latn       | Sp, Tx | Sp, Tx |\n| ceb  | Cebuano                | Latn       | Sp, Tx | Tx     |\n| ces  | Czech                  | Latn       | Sp, Tx | Sp, Tx |\n| ckb  | Central Kurdish        | Arab       | Sp, Tx | Tx     |\n| cmn  | Mandarin Chinese       | Hans       | Sp, Tx | Sp, Tx |\n| cmn_Hant  | Mandarin Chinese  | Hant       | Sp, Tx | Sp, Tx |\n| cym  | Welsh                  | Latn       | Sp, Tx | Sp, Tx |\n| dan  | Danish                 | Latn       | Sp, Tx | Sp, Tx |\n| deu  | German                 | Latn       | Sp, Tx | Sp, Tx |\n| ell  | Greek                  | Grek       | Sp, Tx | Tx     |\n| eng  | English                | Latn       | Sp, Tx | Sp, Tx |\n| est  | Estonian               | Latn       | Sp, Tx | Sp, Tx |\n| eus  | Basque                 | Latn       | Sp, Tx | Tx     |\n| fin  | Finnish                | Latn       | Sp, Tx | Sp, Tx |\n| fra  | French                 | Latn       | Sp, Tx | Sp, Tx |\n| fuv  | Nigerian Fulfulde      | Latn       | Sp, Tx | Tx     |\n| gaz  | West Central Oromo     | Latn       | Sp, Tx | Tx     |\n| gle  | Irish                  | Latn       | Sp, Tx | Tx     |\n| glg  | Galician               | Latn       | Sp, Tx | Tx     |\n| guj  | Gujarati               | Gujr       | Sp, Tx | Tx     |\n| heb  | Hebrew                 | Hebr       | Sp, Tx | Tx     |\n| hin  | Hindi                  | Deva       | Sp, Tx | Sp, Tx |\n| hrv  | Croatian               | Latn       | Sp, Tx | Tx     |\n| hun  | Hungarian              | Latn       | Sp, Tx | Tx     |\n| hye  | Armenian               | Armn       | Sp, Tx | Tx     |\n| ibo  | Igbo                   | Latn       | Sp, Tx | Tx     |\n| ind  | Indonesian             | Latn       | Sp, Tx | Sp, Tx |\n| isl  | Icelandic              | Latn       | Sp, Tx | Tx     |\n| ita  | Italian                | Latn       | Sp, Tx | Sp, Tx |\n| jav  | Javanese               | Latn       | Sp, Tx | Tx     |\n| jpn  | Japanese               | Jpan       | Sp, Tx | Sp, Tx |\n| kam  | Kamba                  | Latn       | Sp     | \--    |\n| kan  | Kannada                | Knda       | Sp, Tx | Tx     |\n| kat  | Georgian               | Geor       | Sp, Tx | Tx     |\n| kaz  | Kazakh                 | Cyrl       | Sp, Tx | Tx     |\n| kea  | Kabuverdianu           | Latn       | Sp     | \--    |\n| khk  | Halh Mongolian         | Cyrl       | Sp, Tx | Tx     |\n| khm  | Khmer                  | Khmr       | Sp, Tx | Tx     |\n| kir  | Kyrgyz                 | Cyrl       | Sp, Tx | Tx     |\n| kor  | Korean                 | Kore       | Sp, Tx | Sp, Tx |\n| lao  | Lao                    | Laoo       | Sp, Tx | Tx     |\n| lit  | Lithuanian             | Latn       | Sp, Tx | Tx     |\n| ltz  | Luxembourgish          | Latn       | Sp     | \--    |\n| lug  | Ganda                  | Latn       | Sp, Tx | Tx     |\n| luo  | Luo                    | Latn       | Sp, Tx | Tx     |\n| lvs  | Standard Latvian       | Latn       | Sp, Tx | Tx     |\n| mai  | Maithili               | Deva       | Sp, Tx | Tx     |\n| mal  | Malayalam              | Mlym       | Sp, Tx | Tx     |\n| mar  | Marathi                | Deva       | Sp, Tx | Tx     |\n| mkd  | Macedonian             | Cyrl       | Sp, Tx | Tx     |\n| mlt  | Maltese                | Latn       | Sp, Tx | Sp, Tx |\n| mni  | Meitei                 | Beng       | Sp, Tx | Tx     |\n| mya  | Burmese                | Mymr       | Sp, Tx | Tx     |\n| nld  | Dutch                  | Latn       | Sp, Tx | Sp, Tx |\n| nno  | Norwegian Nynorsk      | Latn       | Sp, Tx | Tx     |\n| nob  | Norwegian Bokm√•l       | Latn       | Sp, Tx | Tx     |\n| npi  | Nepali                 | Deva       | Sp, Tx | Tx     |\n| nya  | Nyanja                 | Latn       | Sp, Tx | Tx     |\n| oci  | Occitan                | Latn       | Sp     | \--    |\n| ory  | Odia                   | Orya       | Sp, Tx | Tx     |\n| pan  | Punjabi                | Guru       | Sp, Tx | Tx     |\n| pbt  | Southern Pashto        | Arab       | Sp, Tx | Tx     |\n| pes  | Western Persian        | Arab       | Sp, Tx | Sp, Tx |\n| pol  | Polish                 | Latn       | Sp, Tx | Sp, Tx |\n| por  | Portuguese             | Latn       | Sp, Tx | Sp, Tx |\n| ron  | Romanian               | Latn       | Sp, Tx | Sp, Tx |\n| rus  | Russian                | Cyrl       | Sp, Tx | Sp, Tx |\n| slk  | Slovak                 | Latn       | Sp, Tx | Sp, Tx |\n| slv  | Slovenian              | Latn       | Sp, Tx | Tx     |\n| sna  | Shona                  | Latn       | Sp, Tx | Tx     |\n| snd  | Sindhi                 | Arab       | Sp, Tx | Tx     |\n| som  | Somali                 | Latn       | Sp, Tx | Tx     |\n| spa  | Spanish                | Latn       | Sp, Tx | Sp, Tx |\n| srp  | Serbian                | Cyrl       | Sp, Tx | Tx     |\n| swe  | Swedish                | Latn       | Sp, Tx | Sp, Tx |\n| swh  | Swahili                | Latn       | Sp, Tx | Sp, Tx |\n| tam  | Tamil                  | Taml       | Sp, Tx | Tx     |\n| tel  | Telugu                 | Telu       | Sp, Tx | Sp, Tx |\n| tgk  | Tajik                  | Cyrl       | Sp, Tx | Tx     |\n| tgl  | Tagalog                | Latn       | Sp, Tx | Sp, Tx |\n| tha  | Thai                   | Thai       | Sp, Tx | Sp, Tx |\n| tur  | Turkish                | Latn       | Sp, Tx | Sp, Tx |\n| ukr  | Ukrainian              | Cyrl       | Sp, Tx | Sp, Tx |\n| urd  | Urdu                   | Arab       | Sp, Tx | Sp, Tx |\n| uzn  | Northern Uzbek         | Latn       | Sp, Tx | Sp, Tx |\n| vie  | Vietnamese             | Latn       | Sp, Tx | Sp, Tx |\n| xho  | Xhosa                  | Latn       | Sp     | \--    |\n| yor  | Yoruba                 | Latn       | Sp, Tx | Tx     |\n| yue  | Cantonese              | Hant       | Sp, Tx | Tx     |\n| zlm  | Colloquial Malay       | Latn       | Sp     | \--    |\n| zsm  | Standard Malay         | Latn       | Tx     | Tx     |\n| zul  | Zulu                   | Latn       | Sp, Tx | Tx     |\n\n\nNote that seamlessM4T-medium supports 200 languages in the text modality, and is based on NLLB-200 (see full list in [asset card](https://github.com/facebookresearch/seamless_communication/blob/main/src/seamless_communication/cards/unity_nllb-200.yaml))\n\n## Citation\nFor SeamlessM4T v2, please cite :\n```bibtex\n@inproceedings{seamless2023,\n   title="Seamless: Multilingual Expressive and Streaming Speech Translation",\n   author="{Seamless Communication}, Lo{\"i}c Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, John Hoffman, Min-Jae Hwang, Hirofumi Inaguma, Christopher Klaiber, Ilia Kulikov, Pengwei Li, Daniel Licht, Jean Maillard, Ruslan Mavlyutov, Alice Rakotoarison, Kaushik Ram Sadagopan, Abinesh Ramakrishnan, Tuan Tran, Guillaume Wenzek, Yilin Yang, Ethan Ye, Ivan Evtimov, Pierre Fernandez, Cynthia Gao, Prangthip Hansanti, Elahe Kalbassi, Amanda Kallet, Artyom Kozhevnikov, Gabriel Mejia, Robin San Roman, Christophe Touret, Corinne Wong, Carleigh Wood, Bokai Yu, Pierre Andrews, Can Balioglu, Peng-Jen Chen, Marta R. Costa-juss{\`a}, Maha Elbayad, Hongyu Gong, Francisco Guzm{\''a}n, Kevin Heffernan, Somya Jain, Justine Kao, Ann Lee, Xutai Ma, Alex Mourachko, Benjamin Peloquin, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Anna Sun, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang, Mary Williamson",\n  journal={ArXiv},\n  year={2023}\n}\n```\n[//]: # "https://arxiv.org/abs/2312.05187"', '{"pipeline_tag":"automatic-speech-recognition","library_name":"transformers","framework":"transformers","params":2309249669,"storage_bytes":29874323533,"files_count":18,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["SeamlessM4Tv2Model"],"model_type":"seamless_m4t_v2","tokenizer_config":{"bos_token":"<s>","cls_token":"<s>","eos_token":"</s>","pad_token":"<pad>","sep_token":"</s>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:facebookresearch:seamless_communication","source_url":"https://github.com/facebookresearch/seamless_communication"},{"type":"has_code","target_id":"github:facebookresearch:seamless_communication","source_url":"https://github.com/facebookresearch/seamless_communication"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:google:sentencepiece","source_url":"https://github.com/google/sentencepiece"},{"type":"has_code","target_id":"github:huggingface:transformers.git","source_url":"https://github.com/huggingface/transformers.git"},{"type":"has_code","target_id":"github:facebookresearch:seamless_communication","source_url":"https://github.com/facebookresearch/seamless_communication"},{"type":"based_on_paper","target_id":"arxiv:2312.05187","source_url":"https://arxiv.org/abs/2312.05187"}]', NULL, 'CC-BY-NC-4.0', 'approved', 79.7, '240bea4fd3f1fc85e00dfe77f9088892', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen-Image-Edit-2509', 'huggingface--qwen--qwen-image-edit-2509', 'Qwen-Image-Edit-2509', 'Qwen', '--- license: apache-2.0 language: - en - zh library_name: diffusers pipeline_tag: image-to-image --- <p align="center"> <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/qwen_image_edit_logo.png" width="400"/> <p> <p align="center"> üíú <a href="https://chat.qwen.ai/"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbspü§ó <a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2509">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2509...', '["diffusers","safetensors","image-to-image","en","zh","arxiv:2508.02324","license:apache-2.0","diffusers:qwenimageeditpluspipeline","region:us"]', 'image-to-image', 924, 576062, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen-Image-Edit-2509","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\n- zh\nlibrary_name: diffusers\npipeline_tag: image-to-image\n---\n<p align="center">\n    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/qwen_image_edit_logo.png" width="400"/>\n<p>\n<p align="center">\n          üíú <a href="https://chat.qwen.ai/"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbspü§ó <a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2509">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2509">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp üìë <a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf">Tech Report</a> &nbsp&nbsp | &nbsp&nbsp üìë <a href="https://qwenlm.github.io/blog/qwen-image-edit/">Blog</a> &nbsp&nbsp \n<br>\nüñ•Ô∏è <a href="https://huggingface.co/spaces/Qwen/Qwen-Image-Edit">Demo</a>&nbsp&nbsp | &nbsp&nbspüí¨ <a href="https://github.com/QwenLM/Qwen-Image/blob/main/assets/wechat.png">WeChat (ÂæÆ‰ø°)</a>&nbsp&nbsp | &nbsp&nbspü´® <a href="https://discord.gg/CV4E9rpNSD">Discord</a>&nbsp&nbsp| &nbsp&nbsp <a href="https://github.com/QwenLM/Qwen-Image">Github</a>&nbsp&nbsp\n</p>\n\n<p align="center">\n    <img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen-Image/edit2509/edit2509_top.jpg" width="1600"/>\n<p>\n\n\n# Introduction\nThis September, we are pleased to introduce Qwen-Image-Edit-2509, the monthly iteration of Qwen-Image-Edit. To experience the latest model, please visit [Qwen Chat](https://qwen.ai)  and select the "Image Editing" feature.\nCompared with Qwen-Image-Edit released in August, the main improvements of Qwen-Image-Edit-2509 include:\n* **Multi-image Editing Support**: For multi-image inputs, Qwen-Image-Edit-2509 builds upon the Qwen-Image-Edit architecture and is further trained via image concatenation to enable multi-image editing. It supports various combinations such as "person + person," "person + product," and "person + scene." Optimal performance is currently achieved with 1 to 3 input images.\n* **Enhanced Single-image Consistency**: For single-image inputs, Qwen-Image-Edit-2509 significantly improves editing consistency, specifically in the following areas:\n  - **Improved Person Editing Consistency**: Better preservation of facial identity, supporting various portrait styles and pose transformations;\n  - **Improved Product Editing Consistency**: Better preservation of product identity, supporting product poster editingÔºõ\n  - **Improved Text Editing Consistency**: In addition to modifying text content, it also supports editing text fonts, colors, and materialsÔºõ\n* **Native Support for ControlNet**: Including depth maps, edge maps, keypoint maps, and more.\n\n\n## Quick Start\n\nInstall the latest version of diffusers\n```\npip install git+https://github.com/huggingface/diffusers\n```\n\nThe following contains a code snippet illustrating how to use `Qwen-Image-Edit-2509`:\n\n```python\nimport os\nimport torch\nfrom PIL import Image\nfrom diffusers import QwenImageEditPlusPipeline\n\npipeline = QwenImageEditPlusPipeline.from_pretrained("Qwen/Qwen-Image-Edit-2509", torch_dtype=torch.bfloat16)\nprint("pipeline loaded")\n\npipeline.to(''cuda'')\npipeline.set_progress_bar_config(disable=None)\nimage1 = Image.open("input1.png")\nimage2 = Image.open("input2.png")\nprompt = "The magician bear is on the left, the alchemist bear is on the right, facing each other in the central park square."\ninputs = {\n    "image": [image1, image2],\n    "prompt": prompt,\n    "generator": torch.manual_seed(0),\n    "true_cfg_scale": 4.0,\n    "negative_prompt": " ",\n    "num_inference_steps": 40,\n    "guidance_scale": 1.0,\n    "num_images_per_prompt": 1,\n}\nwith torch.inference_mode():\n    output = pipeline(**inputs)\n    output_image = output.images[0]\n    output_image.save("output_image_edit_plus.png")\n    print("image saved at", os.path.abspath("output_image_edit_plus.png"))\n\n```\n\n## Showcase\n\n**The primary update in Qwen-Image-Edit-2509 is support for multi-image inputs.**\n\nLet‚Äôs first look at a "person + person" example:  \n![Person + Person Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8719.JPG#center)\n\nHere is a "person + scene" example:  \n![Person + Scene Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8720.JPG#center)\n\nBelow is a "person + object" example:  \n![Person + Object Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8721.JPG#center)\n\nIn fact, multi-image input also supports commonly used ControlNet keypoint maps‚Äîfor example, changing a person‚Äôs pose:  \n![ControlNet Keypoint Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8722.JPG#center)\n\nSimilarly, the following examples demonstrate results using three input images:  \n![Three Images Example 1](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8723.JPG#center)  \n![Three Images Example 2](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8724.JPG#center)  \n![Three Images Example 3](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8725.JPG#center)\n\n---\n\n**Another major update in Qwen-Image-Edit-2509 is enhanced consistency.**\n\nFirst, regarding person consistency, Qwen-Image-Edit-2509 shows significant improvement over Qwen-Image-Edit. Below are examples generating various portrait styles:  \n![Portrait Styles Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%871.JPG#center)\n\nFor instance, changing a person‚Äôs pose while maintaining excellent identity consistency:  \n![Pose Change with Identity Consistency](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%872.JPG#center)\n\nLeveraging this improvement along with Qwen-Image‚Äôs unique text rendering capability, we find that Qwen-Image-Edit-2509 excels at creating meme images:  \n![Meme Image Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%873.JPG#center)\n\nOf course, even with longer text, Qwen-Image-Edit-2509 can still render it while preserving the person‚Äôs identity:  \n![Long Text with Identity Preservation](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%874.JPG#center)\n\nPerson consistency is also evident in old photo restoration. Below are two examples:  \n![Old Photo Restoration 1](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8717.JPG#center)  \n![Old Photo Restoration 2](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8718.JPG#center)\n\nNaturally, besides real people, generating cartoon characters and cultural creations is also possible:  \n![Cartoon & Cultural Creation](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8715.JPG#center)\n\nSecond, Qwen-Image-Edit-2509 specifically enhances product consistency. We find that the model can naturally generate product posters from plain-background product images:  \n![Product Poster Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%875.JPG#center)\n\nOr even simple logos:  \n![Logo Generation Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8716.JPG#center)\n\nThird, Qwen-Image-Edit-2509 specifically enhances text consistency and supports editing font type, font color, and font material:  \n![Text Font Type](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8710.JPG#center)  \n![Text Font Color](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8711.JPG#center)  \n![Text Font Material](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8712.JPG#center)\n\nMoreover, the ability for precise text editing has been significantly enhanced:  \n![Precise Text Editing 1](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8713.JPG#center)  \n![Precise Text Editing 2](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%8714.JPG#center)\n\nIt is worth noting that text editing can often be seamlessly integrated with image editing‚Äîfor example, in this poster editing case:  \n![Integrated Text & Image Editing](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%876.JPG#center)\n\n---\n\n**The final update in Qwen-Image-Edit-2509 is native support for commonly used ControlNet image conditions, such as keypoint control and sketches:**  \n![Keypoint Control Example](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%877.JPG#center)  \n![Sketch Control Example 1](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%878.JPG#center)  \n![Sketch Control Example 2](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/edit2509/%E5%B9%BB%E7%81%AF%E7%89%879.JPG#center)\n\n\n\n## License Agreement\n\nQwen-Image is licensed under Apache 2.0. \n\n## Citation\n\nWe kindly encourage citation of our work if you find it useful.\n\n```bibtex\n@misc{wu2025qwenimagetechnicalreport,\n      title={Qwen-Image Technical Report}, \n      author={Chenfei Wu and Jiahao Li and Jingren Zhou and Junyang Lin and Kaiyuan Gao and Kun Yan and Sheng-ming Yin and Shuai Bai and Xiao Xu and Yilei Chen and Yuxiang Chen and Zecheng Tang and Zekai Zhang and Zhengyi Wang and An Yang and Bowen Yu and Chen Cheng and Dayiheng Liu and Deqing Li and Hang Zhang and Hao Meng and Hu Wei and Jingyuan Ni and Kai Chen and Kuan Cao and Liang Peng and Lin Qu and Minggang Wu and Peng Wang and Shuting Yu and Tingkun Wen and Wensen Feng and Xiaoxiao Xu and Yi Wang and Yichang Zhang and Yongqiang Zhu and Yujia Wu and Yuxuan Cai and Zenan Liu},\n      year={2025},\n      eprint={2508.02324},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2508.02324}, \n}\n```', '{"pipeline_tag":"image-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":57711227846,"files_count":35,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"QwenImageEditPlusPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen-Image","source_url":"https://github.com/QwenLM/Qwen-Image"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Image\">Github<","source_url":"https://github.com/QwenLM/Qwen-Image\">Github<"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"based_on_paper","target_id":"arxiv:2508.02324","source_url":"https://arxiv.org/abs/2508.02324"}]', NULL, 'Apache-2.0', 'approved', 79.7, '1c0497ccf9ed70531f6b522328ab1fed', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nlpconnect-vit-gpt2-image-captioning', 'huggingface--nlpconnect--vit-gpt2-image-captioning', 'vit-gpt2-image-captioning', 'nlpconnect', '--- tags: - image-to-text - image-captioning license: apache-2.0 widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg example_title: Savanna - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg example_title: Football Match - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg example_title: Airport --- This is an image captioning model trained by @ydshieh in flax this is pytorch vers...', '["transformers","pytorch","vision-encoder-decoder","image-to-text","image-captioning","doi:10.57967/hf/0222","license:apache-2.0","endpoints_compatible","region:us"]', 'image-to-text', 920, 1269482, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nlpconnect/vit-gpt2-image-captioning","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- image-to-text\n- image-captioning\nlicense: apache-2.0\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg\n  example_title: Savanna\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg\n  example_title: Football Match\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg\n  example_title: Airport\n---\n\n# nlpconnect/vit-gpt2-image-captioning\n\nThis is an image captioning model trained by @ydshieh in [flax ](https://github.com/huggingface/transformers/tree/main/examples/flax/image-captioning) this is pytorch version of [this](https://huggingface.co/ydshieh/vit-gpt2-coco-en-ckpts).\n\n\n# The Illustrated Image Captioning using transformers\n\n![](https://ankur3107.github.io/assets/images/vision-encoder-decoder.png)\n\n* https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/\n\n\n# Sample running code\n\n```python\n\nfrom transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nimport torch\nfrom PIL import Image\n\nmodel = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")\nfeature_extractor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")\ntokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")\n\ndevice = torch.device("cuda" if torch.cuda.is_available() else "cpu")\nmodel.to(device)\n\n\n\nmax_length = 16\nnum_beams = 4\ngen_kwargs = {"max_length": max_length, "num_beams": num_beams}\ndef predict_step(image_paths):\n  images = []\n  for image_path in image_paths:\n    i_image = Image.open(image_path)\n    if i_image.mode != "RGB":\n      i_image = i_image.convert(mode="RGB")\n\n    images.append(i_image)\n\n  pixel_values = feature_extractor(images=images, return_tensors="pt").pixel_values\n  pixel_values = pixel_values.to(device)\n\n  output_ids = model.generate(pixel_values, **gen_kwargs)\n\n  preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n  preds = [pred.strip() for pred in preds]\n  return preds\n\n\npredict_step([''doctor.e16ba4e4.jpg'']) # [''a woman in a hospital bed with a woman in a hospital bed'']\n\n```\n\n# Sample running code using transformers pipeline\n\n```python\n\nfrom transformers import pipeline\n\nimage_to_text = pipeline("image-to-text", model="nlpconnect/vit-gpt2-image-captioning")\n\nimage_to_text("https://ankur3107.github.io/assets/images/image-captioning-example.png")\n\n# [{''generated_text'': ''a soccer game with a player jumping to catch the ball ''}]\n\n\n```\n\n\n# Contact for any help\n* https://huggingface.co/ankur310794\n* https://twitter.com/ankur310794\n* http://github.com/ankur3107\n* https://www.linkedin.com/in/ankur310794', '{"pipeline_tag":"image-to-text","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":3934541795,"files_count":10,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["VisionEncoderDecoderModel"],"model_type":"vision-encoder-decoder","tokenizer_config":{"unk_token":"<|endoftext|>","bos_token":"<|endoftext|>","eos_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"}]', NULL, 'Apache-2.0', 'approved', 64.6, '8768404432c65c5d70c36e4f6a112359', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-3-270m', 'huggingface--google--gemma-3-270m', 'gemma-3-270m', 'google', '', '["transformers","safetensors","gemma3_text","text-generation","gemma3","gemma","google","arxiv:2503.19786","arxiv:1905.07830","arxiv:1905.10044","arxiv:1911.11641","arxiv:1705.03551","arxiv:1911.01547","arxiv:1907.10641","arxiv:2311.07911","arxiv:2311.12022","arxiv:2411.04368","arxiv:1904.09728","arxiv:1903.00161","arxiv:2009.03300","arxiv:2304.06364","arxiv:2103.03874","arxiv:2110.14168","arxiv:2108.07732","arxiv:2107.03374","arxiv:2403.07974","arxiv:2305.03111","arxiv:2405.04520","arxiv:2210.03057","arxiv:2106.03193","arxiv:1910.11856","arxiv:2502.12404","arxiv:2502.21228","arxiv:2404.16816","arxiv:2104.12756","arxiv:2311.16502","arxiv:2203.10244","arxiv:2404.12390","arxiv:1810.12440","arxiv:1908.02660","arxiv:2310.02255","arxiv:2312.11805","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 919, 72354, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-3-270m","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":268098176,"storage_bytes":1681824835,"files_count":10,"spaces_count":37,"gated":"manual","private":false,"config":{"architectures":["Gemma3ForCausalLM"],"model_type":"gemma3_text","tokenizer_config":{"bos_token":"<bos>","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2503.19786","source_url":"https://arxiv.org/abs/2503.19786"},{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:2311.07911","source_url":"https://arxiv.org/abs/2311.07911"},{"type":"based_on_paper","target_id":"arxiv:2311.12022","source_url":"https://arxiv.org/abs/2311.12022"},{"type":"based_on_paper","target_id":"arxiv:2411.04368","source_url":"https://arxiv.org/abs/2411.04368"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1903.00161","source_url":"https://arxiv.org/abs/1903.00161"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2103.03874","source_url":"https://arxiv.org/abs/2103.03874"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2403.07974","source_url":"https://arxiv.org/abs/2403.07974"},{"type":"based_on_paper","target_id":"arxiv:2305.03111","source_url":"https://arxiv.org/abs/2305.03111"},{"type":"based_on_paper","target_id":"arxiv:2405.04520","source_url":"https://arxiv.org/abs/2405.04520"},{"type":"based_on_paper","target_id":"arxiv:2210.03057","source_url":"https://arxiv.org/abs/2210.03057"},{"type":"based_on_paper","target_id":"arxiv:2106.03193","source_url":"https://arxiv.org/abs/2106.03193"},{"type":"based_on_paper","target_id":"arxiv:1910.11856","source_url":"https://arxiv.org/abs/1910.11856"},{"type":"based_on_paper","target_id":"arxiv:2502.12404","source_url":"https://arxiv.org/abs/2502.12404"},{"type":"based_on_paper","target_id":"arxiv:2502.21228","source_url":"https://arxiv.org/abs/2502.21228"},{"type":"based_on_paper","target_id":"arxiv:2404.16816","source_url":"https://arxiv.org/abs/2404.16816"},{"type":"based_on_paper","target_id":"arxiv:2104.12756","source_url":"https://arxiv.org/abs/2104.12756"},{"type":"based_on_paper","target_id":"arxiv:2311.16502","source_url":"https://arxiv.org/abs/2311.16502"},{"type":"based_on_paper","target_id":"arxiv:2203.10244","source_url":"https://arxiv.org/abs/2203.10244"},{"type":"based_on_paper","target_id":"arxiv:2404.12390","source_url":"https://arxiv.org/abs/2404.12390"},{"type":"based_on_paper","target_id":"arxiv:1810.12440","source_url":"https://arxiv.org/abs/1810.12440"},{"type":"based_on_paper","target_id":"arxiv:1908.02660","source_url":"https://arxiv.org/abs/1908.02660"},{"type":"based_on_paper","target_id":"arxiv:2310.02255","source_url":"https://arxiv.org/abs/2310.02255"},{"type":"based_on_paper","target_id":"arxiv:2312.11805","source_url":"https://arxiv.org/abs/2312.11805"}]', NULL, 'Gemma', 'approved', 39.6, '826c90ab31f2b0f0e57be8680845460d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-facebook-sam3', 'huggingface--facebook--sam3', 'sam3', 'facebook', '', '["transformers","safetensors","sam3_video","feature-extraction","sam3","mask-generation","en","license:other","endpoints_compatible","region:us"]', 'mask-generation', 915, 451127, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/facebook/sam3","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"mask-generation","library_name":"transformers","framework":"transformers","params":859922360,"storage_bytes":10329938097,"files_count":12,"spaces_count":16,"gated":"manual","private":false,"config":{"architectures":["Sam3VideoModel"],"model_type":"sam3_video","tokenizer_config":{"bos_token":"<|startoftext|>","eos_token":"<|endoftext|>","pad_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[]', NULL, 'Other', 'approved', 39.6, '23f794339343543a0d9ffcfec5aeccb9', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Phr00t-Qwen-Image-Edit-Rapid-AIO', 'huggingface--phr00t--qwen-image-edit-rapid-aio', 'Qwen-Image-Edit-Rapid-AIO', 'Phr00t', '--- license: apache-2.0 base_model: - Qwen/Qwen-Image-Edit-2509 - lightx2v/Qwen-Image-Lightning pipeline_tag: text-to-image library_name: comfyUI tags: - qwen - qwen-edit - t2i - i2i --- Merge of accelerators, VAE and CLIP to allow for easy and fast Qwen Image Edit (and text to image) support. Use a "Load Checkpoint" node. 1 CFG, 4 step. Use the "TextEncodeQwenImageEditPlus" node for input images (which are optional) and prompt. Provide no images to just do pure text to image. FP8 precision. ...', '["comfyui","qwen","qwen-edit","t2i","i2i","text-to-image","base_model:qwen/qwen-image-edit-2509","base_model:finetune:qwen/qwen-image-edit-2509","license:apache-2.0","region:us"]', 'text-to-image', 913, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nbase_model:\n- Qwen/Qwen-Image-Edit-2509\n- lightx2v/Qwen-Image-Lightning\npipeline_tag: text-to-image\nlibrary_name: comfyUI\ntags:\n- qwen\n- qwen-edit\n- t2i\n- i2i\n---\n\nMerge of accelerators, VAE and CLIP to allow for easy and fast Qwen Image Edit (and text to image) support.\n\nUse a "Load Checkpoint" node. 1 CFG, 4 step. Use the "TextEncodeQwenImageEditPlus" node for input images (which are optional) and prompt. Provide no images to just do pure text to image. FP8 precision.\n\n**Both NSFW and SFW models are available!** v4 and older combine both NSFW and SFW uses in one model, but performance is subpar. v5+ separates out a NSFW and SFW version, so please pick which model for your use case. \n\n**Having problems with scaling, cropping or zooming?** Scaling images in the TextEncoderQwenEditPlus node is the problem. There are many workarounds, but I prefer just fixing the node and I''ve supplied my version in the Files area. It also supports up to 4 input images. Just set the "target_size" to a little less than your output''s largest size (like 896 if making a 1024x1024 image). I find this improves quality over skipping scaling entirely, as input images better match output resolutions.\n\n![image](https://cdn-uploads.huggingface.co/production/uploads/631be8402ea8535ea48abbc6/ynDNK35eRLlUjha75fYHH.png)\n\n**V1:** Uses Qwen-Image-Edit-2509 & 4-step Lightning v2.0. Includes a touch of NSFW LORAs, so it should be a very versatile model for both SFW and NSFW use. sa_solver/beta recommended, but euler_a/beta and er_sde/beta can give decent results too.\n\n**V2:** Now uses a mix of Qwen-Image-Edit accelerators, mixing both 8 and 4 steps in one. Also significantly tweaked the NSFW LORAs for better all-around SFW and NSFW use. sa_solver/simple strongly recommended.\n\n**V3:** Uses new Qwen-Image-Edit lightning LORAs for much better results. Also significantly adjusted NSFW LORA mix, removing poor ones and increasing quality ones. sa_solver/beta highly recommended.\n\n**V4:** Mix of many Qwen Edit and base Qwen accelerators, which I think gives better results. Added a touch of a skin correction LORA. **4-5 steps: use sa_solver/simple, lcm/beta or euler_a/beta** and **6-8 steps: use lcm/beta or euler_a/beta only**.\n\n**V5:** NSFW and SFW use cases interfered with eachother too much, so I separated them to specialize in their use cases. Updated "snofs" and "qwen4play" NSFW LORAs + Meta4 for v5.2, then added "Qwen Image NSFW Adv." by fok3827 for v5.3. **SFW: lcm/beta or er_sde/beta generally recommended** and **NSFW: lcm/normal recommended**. Prompting "Professional digital photography" helps reduce the plastic look.\n\n**V6:** Attempt at valiantcat/Qwen-Image-Edit-MeiTu and partially chestnutlzj/Edit-R1-Qwen-Image-Edit-2509 as a base model. However, this was a broken merge. It appears using them as LORAs may work better and I need to cook some more to find something useable. Stay on v5 until something newer comes out.\n\n**V7:** valiantcat/Qwen-Image-Edit-MeiTu and chestnutlzj/Edit-R1-Qwen-Image-Edit-2509 included as LORAs. Accelerator and NSFW LORAs tweaks (v7.1 is more NSFW-heavy). This seemed to be working much better. **lcm/sgm_uniform recommended for 4-6 steps, lcm/normal for 7-8 steps**.\n\n**V8:** Using BF16 to load in FP32 LORAs, only to scale down to FP8 for saving. This seems to help resolve "grid" issues and improves quality. Tweaked accelerator amounts. Significant NSFW LORA tweaks (and new SNOFS). **euler_a/beta recommended for 4-6 steps, lcm/normal for 7-8 steps**.\n\n**V9:** OK, I lied. "Rebalancing" and "Smartphone Photoreal" LORAs really do help image generations for both SFW and NSFW purposes. If you don''t want those LORAs integrated (like making anime or cartoons), use the "Lite" versions. Also, I had a typo in accelerators in V8 that has been fixed for V9. Tweaked NSFW LORAs and significantly reduced how heavy they need to be applied, which should hopefully help consistency. **euler_a/beta recommended for 4-6 steps**. More steps usually work better with sgm_normal or normal schedulers.\n\n**V10:** This is kinda a mix of v5 and v9. MeiTu and Edit-R1 dropped. I''m keeping the "Rebalancing" and "Smartphone" LORAs at half strengths which I think help skin, variety and composition. NSFW LORAs closely resemble v5.3 (but with updated v1.2 snofs). v10.4 NSFW tweaked to improve character consistency and penises. **euler/beta strongly recommended for 4-8 steps** but **euler_a/sgm_uniform recommended for NSFW v10.2+**.\n\n**V11:** Tweaked NSFW LORAs, using fewer to rely on more compatible ones instead. Spread out "realism" LORAs to more at lower strength. **euler/beta recommended** for both NSFW and SFW, but feel free to experiment with others!\n\n**V12:** Getting inpatient waiting for Qwen Edit 2511 (2512?), so went looking for more LORAs and tweaks to reduce the "plastic" look. **euler/sgm_uniform for SFW** and **er_sde/sgm_uniform for NSFW**, although experimentation is healthy is this context.\n\n**V13:** Tweaks to included LORAs in an attempt to reduce gridlines and increase character consistency (without returning the plastic look). **er_sde/beta recommended**. \n\n**V14:** Trimmed LORAs which may have been interfereing with character consistency (while hopefully still reducing the "plastic" look). Updated new SNOFS v1.3 for NSFW and trimmed poor NSFW LORAs too. **er_sde/beta recommended**. ', '{"pipeline_tag":"text-to-image","library_name":"comfyUI","framework":"comfyUI","params":null,"storage_bytes":1039495216980,"files_count":42,"spaces_count":39,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'Apache-2.0', 'approved', 64.6, 'db24d18447f070c93ff2714f443b58d2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-facebook-detr-resnet-50', 'huggingface--facebook--detr-resnet-50', 'detr-resnet-50', 'facebook', '--- license: apache-2.0 tags: - object-detection - vision datasets: - coco widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg example_title: Savanna - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg example_title: Football Match - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg example_title: Airport --- DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 objec...', '["transformers","pytorch","safetensors","detr","object-detection","vision","dataset:coco","arxiv:2005.12872","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'object-detection', 911, 1372791, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/facebook/detr-resnet-50","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ntags:\n- object-detection\n- vision\ndatasets:\n- coco\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg\n  example_title: Savanna\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg\n  example_title: Football Match\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg\n  example_title: Airport\n---\n\n# DETR (End-to-End Object Detection) model with ResNet-50 backbone\n\nDEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by Carion et al. and first released in [this repository](https://github.com/facebookresearch/detr). \n\nDisclaimer: The team releasing DETR did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe DETR model is an encoder-decoder transformer with a convolutional backbone. Two heads are added on top of the decoder outputs in order to perform object detection: a linear layer for the class labels and a MLP (multi-layer perceptron) for the bounding boxes. The model uses so-called object queries to detect objects in an image. Each object query looks for a particular object in the image. For COCO, the number of object queries is set to 100. \n\nThe model is trained using a "bipartite matching loss": one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a "no object" as class and "no bounding box" as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/detr_architecture.png)\n\n## Intended uses & limitations\n\nYou can use the raw model for object detection. See the [model hub](https://huggingface.co/models?search=facebook/detr) to look for all available DETR models.\n\n### How to use\n\nHere is how to use this model:\n\n```python\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\nimport torch\nfrom PIL import Image\nimport requests\n\nurl = "http://images.cocodataset.org/val2017/000000039769.jpg"\nimage = Image.open(requests.get(url, stream=True).raw)\n\n# you can specify the revision tag if you don''t want the timm dependency\nprocessor = DetrImageProcessor.from_pretrained("facebook/detr-resnet-50", revision="no_timm")\nmodel = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50", revision="no_timm")\n\ninputs = processor(images=image, return_tensors="pt")\noutputs = model(**inputs)\n\n# convert outputs (bounding boxes and class logits) to COCO API\n# let''s only keep detections with score > 0.9\ntarget_sizes = torch.tensor([image.size[::-1]])\nresults = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n\nfor score, label, box in zip(results["scores"], results["labels"], results["boxes"]):\n    box = [round(i, 2) for i in box.tolist()]\n    print(\n            f"Detected {model.config.id2label[label.item()]} with confidence "\n            f"{round(score.item(), 3)} at location {box}"\n    )\n```\nThis should output:\n```\nDetected remote with confidence 0.998 at location [40.16, 70.81, 175.55, 117.98]\nDetected remote with confidence 0.996 at location [333.24, 72.55, 368.33, 187.66]\nDetected couch with confidence 0.995 at location [-0.02, 1.15, 639.73, 473.76]\nDetected cat with confidence 0.999 at location [13.24, 52.05, 314.02, 470.93]\nDetected cat with confidence 0.999 at location [345.4, 23.85, 640.37, 368.72]\n```\n\nCurrently, both the feature extractor and model support PyTorch. \n\n## Training data\n\nThe DETR model was trained on [COCO 2017 object detection](https://cocodataset.org/#download), a dataset consisting of 118k/5k annotated images for training/validation respectively. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training/validation can be found [here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py). \n\nImages are resized/rescaled such that the shortest side is at least 800 pixels and the largest side at most 1333 pixels, and normalized across the RGB channels with the ImageNet mean (0.485, 0.456, 0.406) and standard deviation (0.229, 0.224, 0.225).\n\n### Training\n\nThe model was trained for 300 epochs on 16 V100 GPUs. This takes 3 days, with 4 images per GPU (hence a total batch size of 64).\n\n## Evaluation results\n\nThis model achieves an AP (average precision) of **42.0** on COCO 2017 validation. For more details regarding evaluation results, we refer to table 1 of the original paper.\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-2005-12872,\n  author    = {Nicolas Carion and\n               Francisco Massa and\n               Gabriel Synnaeve and\n               Nicolas Usunier and\n               Alexander Kirillov and\n               Sergey Zagoruyko},\n  title     = {End-to-End Object Detection with Transformers},\n  journal   = {CoRR},\n  volume    = {abs/2005.12872},\n  year      = {2020},\n  url       = {https://arxiv.org/abs/2005.12872},\n  archivePrefix = {arXiv},\n  eprint    = {2005.12872},\n  timestamp = {Thu, 28 May 2020 17:38:09 +0200},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-12872.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```', '{"pipeline_tag":"object-detection","library_name":"transformers","framework":"transformers","params":41631008,"storage_bytes":835779962,"files_count":6,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["DetrForObjectDetection"],"model_type":"detr"}}', '[]', '[{"type":"has_code","target_id":"github:facebookresearch:detr","source_url":"https://github.com/facebookresearch/detr"},{"type":"has_code","target_id":"github:google-research:vision_transformer","source_url":"https://github.com/google-research/vision_transformer"},{"type":"based_on_paper","target_id":"arxiv:2005.12872","source_url":"https://arxiv.org/abs/2005.12872"}]', NULL, 'Apache-2.0', 'approved', 64.6, '613c2d0fe7ded51198fc5b385baeeed7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-vit-base-patch16-224', 'huggingface--google--vit-base-patch16-224', 'vit-base-patch16-224', 'google', '--- license: apache-2.0 tags: - vision - image-classification datasets: - imagenet-1k - imagenet-21k widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg example_title: Tiger - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg example_title: Teapot - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg example_title: Palace --- Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 milli...', '["transformers","pytorch","tf","jax","safetensors","vit","image-classification","vision","dataset:imagenet-1k","dataset:imagenet-21k","arxiv:2010.11929","arxiv:2006.03677","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'image-classification', 905, 3950754, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/vit-base-patch16-224","fetched_at":"2025-12-08T10:30:37.942Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\ndatasets:\n- imagenet-1k\n- imagenet-21k\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\n  example_title: Palace\n---\n\n# Vision Transformer (base-sized model) \n\nVision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al. and first released in [this repository](https://github.com/google-research/vision_transformer). However, the weights were converted from the [timm repository](https://github.com/rwightman/pytorch-image-models) by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him. \n\nDisclaimer: The team releasing ViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nThe Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on a large collection of images in a supervised fashion, namely ImageNet-21k, at a resolution of 224x224 pixels. Next, the model was fine-tuned on ImageNet (also referred to as ILSVRC2012), a dataset comprising 1 million images and 1,000 classes, also at resolution 224x224.\n\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.\n\nBy pre-training the model, it learns an inner representation of images that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled images for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire image.\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=google/vit) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes:\n\n```python\nfrom transformers import ViTImageProcessor, ViTForImageClassification\nfrom PIL import Image\nimport requests\n\nurl = ''http://images.cocodataset.org/val2017/000000039769.jpg''\nimage = Image.open(requests.get(url, stream=True).raw)\n\nprocessor = ViTImageProcessor.from_pretrained(''google/vit-base-patch16-224'')\nmodel = ViTForImageClassification.from_pretrained(''google/vit-base-patch16-224'')\n\ninputs = processor(images=image, return_tensors="pt")\noutputs = model(**inputs)\nlogits = outputs.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = logits.argmax(-1).item()\nprint("Predicted class:", model.config.id2label[predicted_class_idx])\n```\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/transformers/model_doc/vit.html#).\n\n## Training data\n\nThe ViT model was pretrained on [ImageNet-21k](http://www.image-net.org/), a dataset consisting of 14 million images and 21k classes, and fine-tuned on [ImageNet](http://www.image-net.org/challenges/LSVRC/2012/), a dataset consisting of 1 million images and 1k classes. \n\n## Training procedure\n\n### Preprocessing\n\nThe exact details of preprocessing of images during training/validation can be found [here](https://github.com/google-research/vision_transformer/blob/master/vit_jax/input_pipeline.py). \n\nImages are resized/rescaled to the same resolution (224x224) and normalized across the RGB channels with mean (0.5, 0.5, 0.5) and standard deviation (0.5, 0.5, 0.5).\n\n### Pretraining\n\nThe model was trained on TPUv3 hardware (8 cores). All model variants are trained with a batch size of 4096 and learning rate warmup of 10k steps. For ImageNet, the authors found it beneficial to additionally apply gradient clipping at global norm 1. Training resolution is 224.\n\n## Evaluation results\n\nFor evaluation results on several image classification benchmarks, we refer to tables 2 and 5 of the original paper. Note that for fine-tuning, the best results are obtained with a higher resolution (384x384). Of course, increasing the model size will result in better performance.\n\n### BibTeX entry and citation info\n\n```bibtex\n@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n```bibtex\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n```', '{"pipeline_tag":"image-classification","library_name":"transformers","framework":"transformers","params":86567656,"storage_bytes":2550907501,"files_count":8,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["ViTForImageClassification"],"model_type":"vit"}}', '[]', '[{"type":"has_code","target_id":"github:google-research:vision_transformer","source_url":"https://github.com/google-research/vision_transformer"},{"type":"has_code","target_id":"github:rwightman:pytorch-image-models","source_url":"https://github.com/rwightman/pytorch-image-models"},{"type":"has_code","target_id":"github:google-research:vision_transformer","source_url":"https://github.com/google-research/vision_transformer"},{"type":"based_on_paper","target_id":"arxiv:2010.11929","source_url":"https://arxiv.org/abs/2010.11929"},{"type":"based_on_paper","target_id":"arxiv:2006.03677","source_url":"https://arxiv.org/abs/2006.03677"}]', NULL, 'Apache-2.0', 'approved', 64.6, '6479738f3ef95b2bddd34bdc67913366', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-V3.2-Exp', 'huggingface--deepseek-ai--deepseek-v3.2-exp', 'DeepSeek-V3.2-Exp', 'deepseek-ai', '--- license: mit library_name: transformers base_model: - deepseek-ai/DeepSeek-V3.2-Exp-Base base_model_relation: finetune --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" targ...', '["transformers","safetensors","deepseek_v32","text-generation","conversational","base_model:deepseek-ai/deepseek-v3.2-exp-base","license:mit","endpoints_compatible","fp8","region:us"]', 'text-generation', 901, 73303, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-V3.2-Exp-Base\nbase_model_relation: finetune\n---\n# DeepSeek-V3.2-Exp\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n<div align="center" style="line-height: 1;">\n  <a href="LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n## Introduction\n\n\nWe are excited to announce the official release of DeepSeek-V3.2-Exp, an experimental version of our model. As an intermediate step toward our next-generation architecture, V3.2-Exp builds upon V3.1-Terminus by introducing DeepSeek Sparse Attention‚Äîa sparse attention mechanism designed to explore and validate optimizations for training and inference efficiency in long-context scenarios.\n\nThis experimental release represents our ongoing research into more efficient transformer architectures, particularly focusing on improving computational efficiency when processing extended text sequences.\n\n<div align="center">\n <img src="assets/cost.png" >\n</div>\n\n- DeepSeek Sparse Attention (DSA) achieves fine-grained sparse attention for the first time, delivering substantial improvements in long-context training and inference efficiency while maintaining virtually identical model output quality.\n\n\n- To rigorously evaluate the impact of introducing sparse attention, we deliberately aligned the training configurations of DeepSeek-V3.2-Exp with V3.1-Terminus. Across public benchmarks in various domains, DeepSeek-V3.2-Exp demonstrates performance on par with V3.1-Terminus.\n\n\n| Benchmark | DeepSeek-V3.1-Terminus | DeepSeek-V3.2-Exp |\n| :--- | :---: | :---: |\n| **Reasoning Mode w/o Tool Use** | | |\n| MMLU-Pro | 85.0 | 85.0 |\n| GPQA-Diamond | 80.7 | 79.9 |\n| Humanity''s Last Exam | 21.7 | 19.8 |\n| LiveCodeBench | 74.9 | 74.1 |\n| AIME 2025 | 88.4 | 89.3 |\n| HMMT 2025 | 86.1 | 83.6 |\n| Codeforces | 2046 | 2121 |\n| Aider-Polyglot | 76.1 | 74.5 |\n| **Agentic Tool Use** | | |\n| BrowseComp | 38.5 | 40.1 |\n| BrowseComp-zh | 45.0 | 47.9 |\n| SimpleQA | 96.8 | 97.1 |\n| SWE Verified | 68.4 | 67.8 |\n| SWE-bench Multilingual | 57.8 | 57.9 |\n| Terminal-bench | 36.7 | 37.7 |\n\n## Update\n\n- 2025.11.17: **We have identified that previous versions of the inference demo code contained an implementation discrepancy in Rotary Position Embedding (RoPE) within the indexer module, potentially leading to degraded model performance.** Specifically, the input tensor to RoPE in the indexer module requires a non-interleaved layout, whereas RoPE in the MLA module expects an interleaved layout. This issue has now been resolved. Please refer to the updated version of the inference demo code and take note of this implementation detail.\n\n## How to Run Locally\n\n### HuggingFace\n\nWe provide an updated inference demo code in the [inference](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference) folder to help the community quickly get started with our model and understand its architectural details.\n\nFirst convert huggingface model weights to the the format required by our inference demo. Set `MP` to match your available GPU count:\n```bash\ncd inference\nexport EXPERTS=256\npython convert.py --hf-ckpt-path ${HF_CKPT_PATH} --save-path ${SAVE_PATH} --n-experts ${EXPERTS} --model-parallel ${MP}\n```\n\nLaunch the interactive chat interface and start exploring DeepSeek''s capabilities:\n```bash\nexport CONFIG=config_671B_v3.2.json\ntorchrun --nproc-per-node ${MP} generate.py --ckpt-path ${SAVE_PATH} --config ${CONFIG} --interactive\n```\n\n### SGLang\n\n#### Installation with Docker\n\n```\n# H200\ndocker pull lmsysorg/sglang:dsv32\n\n# MI350\ndocker pull lmsysorg/sglang:dsv32-rocm\n\n# NPUs\ndocker pull lmsysorg/sglang:dsv32-a2\ndocker pull lmsysorg/sglang:dsv32-a3\n```\n\n#### Launch Command\n```bash\npython -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8 --dp 8 --enable-dp-attention\n```\n\n### vLLM\n\nvLLM provides day-0 support of DeepSeek-V3.2-Exp. See the [recipes](https://docs.vllm.ai/projects/recipes/en/latest/DeepSeek/DeepSeek-V3_2-Exp.html) for up-to-date details.\n\n## Open-Source Kernels\n\nFor TileLang kernels with **better readability and research-purpose design**, please refer to [TileLang](https://github.com/tile-ai/tilelang/tree/main/examples/deepseek_v32).\n\nFor **high-performance CUDA kernels**, indexer logit kernels (including paged versions) are available in [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM/pull/200). Sparse attention kernels are released in [FlashMLA](https://github.com/deepseek-ai/FlashMLA/pull/98).\n\n\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2024deepseekv32,\n      title={DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention}, \n      author={DeepSeek-AI},\n      year={2025},\n}\n```\n\n## Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":685396921376,"storage_bytes":689483151335,"files_count":180,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["DeepseekV32ForCausalLM"],"model_type":"deepseek_v32","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% if not thinking is defined %}{% set thinking = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, system_prompt='''', is_first_sp=true, is_last_user=false, is_only_sys=false, is_prefix=false) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\n\n'' + message[''content''] %}{%- endif %}{% set ns.is_only_sys = true %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{''<ÔΩúUserÔΩú>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''tool_calls''] is defined and message[''tool_calls''] is not none %}{%- if ns.is_last_user or ns.is_only_sys %}{{''<ÔΩúAssistantÔΩú></think>''}}{%- endif %}{%- set ns.is_last_user = false -%}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls''] %}{%- if not ns.is_first %}{%- if message[''content''] is none %}{{''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>''+ tool[''function''][''name''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''arguments''] + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- else %}{{message[''content''] + ''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''function''][''name''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''arguments''] + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>''+ tool[''function''][''name''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''arguments''] + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- endfor %}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and (message[''tool_calls''] is not defined or message[''tool_calls''] is none) %}{%- if ns.is_last_user %}{{''<ÔΩúAssistantÔΩú>''}}{%- if message[''prefix''] is defined and message[''prefix''] and thinking %}{{''<think>''}}{%- else %}{{''</think>''}}{%- endif %}{%- endif %}{%- if message[''prefix''] is defined and message[''prefix''] %}{%- set ns.is_prefix = true -%}{%- endif %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{%- set content = message[''content''] -%}{%- if ''</think>'' in content %}{%- set content = content.split(''</think>'', 1)[1] -%}{%- endif %}{{content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{{''<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- if message[''role''] != ''system'' %}{% set ns.is_only_sys = false %}{%- endif %}{%- endfor -%}{% if add_generation_prompt and not ns.is_tool%}{% if ns.is_last_user or ns.is_only_sys or not ns.is_prefix %}{{''<ÔΩúAssistantÔΩú>''}}{%- if not thinking %}{{''</think>''}}{%- else %}{{''<think>''}}{%- endif %}{% endif %}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:tile-ai:tilelang","source_url":"https://github.com/tile-ai/tilelang"},{"type":"has_code","target_id":"github:deepseek-ai:DeepGEMM","source_url":"https://github.com/deepseek-ai/DeepGEMM"},{"type":"has_code","target_id":"github:deepseek-ai:FlashMLA","source_url":"https://github.com/deepseek-ai/FlashMLA"}]', NULL, 'MIT', 'approved', 84.6, '9c9eb0c0f9cef87614b8edf6a8755cb3', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/resolve/main/assets/cost.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-V3.2-Exp from https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/resolve/main/assets/cost.png
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-V3.2-Exp.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-Next-80B-A3B-Instruct', 'huggingface--qwen--qwen3-next-80b-a3b-instruct', 'Qwen3-Next-80B-A3B-Instruct', 'Qwen', '--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct/blob/main/LICENSE pipeline_tag: text-generation --- <a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> Over the past few months, we have observed increasingly clear trends toward scaling both total ...', '["transformers","safetensors","qwen3_next","text-generation","conversational","arxiv:2309.00071","arxiv:2404.06654","arxiv:2505.09388","arxiv:2501.15383","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 899, 2697487, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct/blob/main/LICENSE\npipeline_tag: text-generation\n---\n\n# Qwen3-Next-80B-A3B-Instruct\n<a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\nOver the past few months, we have observed increasingly clear trends toward scaling both total parameters and context lengths in the pursuit of more powerful and agentic artificial intelligence (AI). \nWe are excited to share our latest advancements in addressing these demands, centered on improving scaling efficiency through innovative model architecture. \nWe call this next-generation foundation models **Qwen3-Next**.\n\n## Highlights\n\n**Qwen3-Next-80B-A3B** is the first installment in the Qwen3-Next series and features the following key enchancements:\n- **Hybrid Attention**: Replaces standard attention with the combination of **Gated DeltaNet** and **Gated Attention**, enabling efficient context modeling for ultra-long context length.\n- **High-Sparsity Mixture-of-Experts (MoE)**: Achieves an extreme low activation ratio in MoE layers, drastically reducing FLOPs per token while preserving model capacity. \n- **Stability Optimizations**: Includes techniques such as **zero-centered and weight-decayed layernorm**, and other stabilizing enhancements for robust pre-training and post-training.  \n- **Multi-Token Prediction (MTP)**: Boosts pretraining model performance and accelerates inference.\n\nWe are seeing strong performance in terms of both parameter efficiency and inference speed for Qwen3-Next-80B-A3B:\n- Qwen3-Next-80B-A3B-Base outperforms Qwen3-32B-Base on downstream tasks with 10% of the total training cost and with 10 times inference throughput for context over 32K tokens.\n- Qwen3-Next-80B-A3B-Instruct performs on par with Qwen3-235B-A22B-Instruct-2507 on certain benchmarks, while demonstrating significant advantages in handling ultra-long-context tasks up to 256K tokens.\n\n![Qwen3-Next-80B-A3B-Instruct Benchmark Comparison](https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-Next/Qwen3-Next-80B-A3B-Instruct.001.jpeg)\n\nFor more details, please refer to our blog post [Qwen3-Next](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list).\n\n## Model Overview\n\n> [!Note]\n> **Qwen3-Next-80B-A3B-Instruct** supports only instruct (non-thinking) mode and does not generate ``<think></think>`` blocks in its output.\n\n**Qwen3-Next-80B-A3B-Instruct** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining (15T tokens) & Post-training\n- Number of Parameters: 80B in total and 3B activated\n- Number of Paramaters (Non-Embedding): 79B\n- Hidden Dimension: 2048\n- Number of Layers: 48\n  - Hybrid Layout: 12 \* (3 \* (Gated DeltaNet -> MoE) -> 1 \* (Gated Attention -> MoE))\n- Gated Attention:\n  - Number of Attention Heads: 16 for Q and 2 for KV\n  - Head Dimension: 256\n  - Rotary Position Embedding Dimension: 64\n- Gated DeltaNet:\n  - Number of Linear Attention Heads: 32 for V and 16 for QK\n  - Head Dimension: 128\n- Mixture of Experts:\n  - Number of Experts: 512\n  - Number of Activated Experts: 10\n  - Number of Shared Experts: 1\n  - Expert Intermediate Dimension: 512\n- Context Length: 262,144 natively and extensible up to 1,010,000 tokens\n\n<img src="https://qianwen-res.oss-accelerate.aliyuncs.com/Qwen3-Next/model_architecture.png" height="384px" title="Qwen3-Next Model Architecture" />\n\n\n## Performance\n\n|  | Qwen3-30B-A3B-Instruct-2507 | Qwen3-32B Non-Thinking | Qwen3-235B-A22B-Instruct-2507 | Qwen3-Next-80B-A3B-Instruct |\n|--- | --- | --- | --- | --- |\n| **Knowledge** | | | | |\n| MMLU-Pro | 78.4 | 71.9 | **83.0** | 80.6 |\n| MMLU-Redux | 89.3 | 85.7 | **93.1** | 90.9 |\n| GPQA | 70.4 | 54.6 | **77.5** | 72.9 |\n| SuperGPQA | 53.4 | 43.2 | **62.6** | 58.8 |\n| **Reasoning** | | | | |\n| AIME25 | 61.3 | 20.2 | **70.3** | 69.5 |\n| HMMT25 | 43.0 | 9.8 | **55.4** | 54.1 |\n| LiveBench 20241125 | 69.0 | 59.8 | 75.4 | **75.8** |\n| **Coding** | | | | |\n| LiveCodeBench v6 (25.02-25.05) | 43.2 | 29.1 | 51.8 | **56.6** |\n| MultiPL-E | 83.8 | 76.9 | **87.9** | 87.8 |\n| Aider-Polyglot | 35.6 | 40.0 | **57.3** | 49.8 |\n| **Alignment** | | | | |\n| IFEval | 84.7 | 83.2 | **88.7** | 87.6 |\n| Arena-Hard v2* | 69.0 | 34.1 | 79.2 | **82.7** |\n| Creative Writing v3 | 86.0 | 78.3 | **87.5** | 85.3 |\n| WritingBench | 85.5 | 75.4 | 85.2 | **87.3** |\n| **Agent** | | | | |\n| BFCL-v3 | 65.1 | 63.0 | **70.9** | 70.3 |\n| TAU1-Retail | 59.1 | 40.1 | **71.3** | 60.9 |\n| TAU1-Airline | 40.0 | 17.0 | **44.0** | 44.0 |\n| TAU2-Retail | 57.0 | 48.8 | **74.6** | 57.3 |\n| TAU2-Airline | 38.0 | 24.0 | **50.0** | 45.5 |\n| TAU2-Telecom | 12.3 | 24.6 | **32.5** | 13.2 |\n| **Multilingualism** | | | | |\n| MultiIF | 67.9 | 70.7 | **77.5** | 75.8 |\n| MMLU-ProX | 72.0 | 69.3 | **79.4** | 76.7 |\n| INCLUDE | 71.9 | 70.9 | **79.5** | 78.9 |\n| PolyMATH | 43.1 | 22.5 | **50.2** | 45.9 |\n\n*: For reproducibility, we report the win rates evaluated by GPT-4.1.\n\n## Quickstart\n\nThe code for Qwen3-Next has been merged into the main branch of Hugging Face `transformers`.\n\n```shell\npip install git+https://github.com/huggingface/transformers.git@main\n```\n\nWith earlier versions, you will encounter the following error:\n```\nKeyError: ''qwen3_next''\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen3-Next-80B-A3B-Instruct"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    dtype="auto",\n    device_map="auto",\n)\n\n# prepare the model input\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "user", "content": prompt},\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=16384,\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\n\nprint("content:", content)\n```\n\n> [!Note]\n> Multi-Token Prediction (MTP) is not generally available in Hugging Face Transformers.\n\n> [!Note]\n> The efficiency or throughput improvement depends highly on the implementation.\n> It is recommended to adopt a dedicated inference framework, e.g., SGLang and vLLM, for inference tasks.\n\n> [!Tip]\n> Depending on the inference settings, you may observe better efficiency with [`flash-linear-attention`](https://github.com/fla-org/flash-linear-attention#installation) and [`causal-conv1d`](https://github.com/Dao-AILab/causal-conv1d).\n> See the links for detailed instructions and requirements.\n\n\n## Deployment\n\nFor deployment, you can use the latest `sglang` or `vllm` to create an OpenAI-compatible API endpoint.\n\n### SGLang\n\n[SGLang](https://github.com/sgl-project/sglang) is a fast serving framework for large language models and vision language models.\nSGLang could be used to launch a server with OpenAI-compatible API service. \n\n`sglang>=0.5.2` is required for Qwen3-Next, which can be installed using:\n```shell\npip install ''sglang[all]>=0.5.2''\n```\nSee [its documentation](https://docs.sglang.ai/get_started/install.html) for more details.\n\nThe following command can be used to create an API endpoint at `http://localhost:30000/v1` with maximum context length 256K tokens using tensor parallel on 4 GPUs.\n```shell\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Instruct --port 30000 --tp-size 4 --context-length 262144 --mem-fraction-static 0.8\n```\n\nThe following command is recommended for MTP with the rest settings the same as above:\n```shell\npython -m sglang.launch_server --model-path Qwen/Qwen3-Next-80B-A3B-Instruct --port 30000 --tp-size 4 --context-length 262144 --mem-fraction-static 0.8 --speculative-algo NEXTN --speculative-num-steps 3 --speculative-eagle-topk 1 --speculative-num-draft-tokens 4\n```\n\n> [!Note]\n> The default context length is 256K. Consider reducing the context length to a smaller value, e.g., `32768`, if the server fails to start.\n\nPlease also refer to SGLang''s usage guide on [Qwen3-Next](https://docs.sglang.ai/basic_usage/qwen3.html).\n\n### vLLM\n\n[vLLM](https://github.com/vllm-project/vllm) is a high-throughput and memory-efficient inference and serving engine for LLMs.\nvLLM could be used to launch a server with OpenAI-compatible API service. \n\n`vllm>=0.10.2` is required for Qwen3-Next, which can be installed using:\n```shell\npip install ''vllm>=0.10.2''\n```\nSee [its documentation](https://docs.vllm.ai/en/stable/getting_started/installation/index.html) for more details.\n\nThe following command can be used to create an API endpoint at `http://localhost:8000/v1` with maximum context length 256K tokens using tensor parallel on 4 GPUs.\n```shell\nvllm serve Qwen/Qwen3-Next-80B-A3B-Instruct --port 8000 --tensor-parallel-size 4 --max-model-len 262144\n```\n\nThe following command is recommended for MTP with the rest settings the same as above:\n```shell\nvllm serve Qwen/Qwen3-Next-80B-A3B-Instruct --port 8000 --tensor-parallel-size 4 --max-model-len 262144 --speculative-config ''{"method":"qwen3_next_mtp","num_speculative_tokens":2}''\n```\n\n> [!Note]\n> The default context length is 256K. Consider reducing the context length to a smaller value, e.g., `32768`, if the server fails to start.\n\nPlease also refer to vLLM''s usage guide on [Qwen3-Next](https://docs.vllm.ai/projects/recipes/en/latest/Qwen/Qwen3-Next.html).\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    ''model'': ''Qwen3-Next-80B-A3B-Instruct'',\n\n    # Use a custom endpoint compatible with OpenAI API:\n    ''model_server'': ''http://localhost:8000/v1'',  # api_base\n    ''api_key'': ''EMPTY'',\n}\n\n# Define Tools\ntools = [\n    {''mcpServers'': {  # You can specify the MCP configuration file\n            ''time'': {\n                ''command'': ''uvx'',\n                ''args'': [''mcp-server-time'', ''--local-timezone=Asia/Shanghai'']\n            },\n            "fetch": {\n                "command": "uvx",\n                "args": ["mcp-server-fetch"]\n            }\n        }\n    },\n  ''code_interpreter'',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{''role'': ''user'', ''content'': ''https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen''}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n\n## Processing Ultra-Long Texts\n\nQwen3-Next natively supports context lengths of up to 262,144 tokens. \nFor conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. \nWe have validated the model''s performance on context lengths of up to 1 million tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers`, `vllm` and `sglang`. \nIn general, there are two approaches to enabling YaRN for supported frameworks:\n\n- Modifying the model files:\n  In the `config.json` file, add the `rope_scaling` fields:\n    ```json\n    {\n        ...,\n        "rope_scaling": {\n            "rope_type": "yarn",\n            "factor": 4.0,\n            "original_max_position_embeddings": 262144\n        }\n    }\n    ```\n\n- Passing command line arguments:\n\n  For `vllm`, you can use\n    ```shell\n    VLLM_ALLOW_LONG_MAX_MODEL_LEN=1 vllm serve ... --rope-scaling ''{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":262144}'' --max-model-len 1010000  \n    ```\n\n  For `sglang`, you can use\n    ```shell\n    SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1 python -m sglang.launch_server ... --json-model-override-args ''{"rope_scaling":{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":262144}}'' --context-length 1010000\n    ```\n\n> [!NOTE]\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 524,288 tokens, it would be better to set `factor` as 2.0. \n\n#### Long-Context Performance\n\nWe test the model on an 1M version of the [RULER](https://arxiv.org/abs/2404.06654) benchmark.\n\n| Model Name                                  | Acc avg | 4k   | 8k   | 16k  | 32k  | 64k  | 96k  | 128k | 192k | 256k | 384k | 512k | 640k | 768k | 896k | 1000k |\n|---------------------------------------------|---------|------|------|------|------|------|------|------|------|------|------|------|------|------|------|-------|\n| Qwen3-30B-A3B-Instruct-2507                 | 86.8    | 98.0 | 96.7 | 96.9 | 97.2 | 93.4 | 91.0 | 89.1 | 89.8 | 82.5 | 83.6 | 78.4 | 79.7 | 77.6 | 75.7 | 72.8  |\n| Qwen3-235B-A22B-Instruct-2507               | 92.5    | 98.5 | 97.6 | 96.9 | 97.3 | 95.8 | 94.9 | 93.9 | 94.5 | 91.0 | 92.2 | 90.9 | 87.8 | 84.8 | 86.5 | 84.5  |\n| Qwen3-Next-80B-A3B-Instruct                 | 91.8    | 98.5 | 99.0 | 98.0 | 98.7 | 97.6 | 95.0 | 96.0 | 94.0 | 93.5 | 91.7 | 86.9 | 85.5 | 81.7 | 80.3 | 80.3  |\n\n* Qwen3-Next are evaluated with YaRN enabled. Qwen3-2507 models are evaluated with Dual Chunk Attention enabled.\n* Since the evaluation is time-consuming, we use 260 samples for each length (13 sub-tasks, 20 samples for each).\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - We suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 16,384 tokens for most queries, which is adequate for instruct models.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include "Please reason step by step, and put your final answer within \boxed{}." in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: "Please show your choice in the `answer` field with only the choice letter, e.g., `"answer": "C"`."\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n\n@article{qwen2.5-1m,\n      title={Qwen2.5-1M Technical Report}, \n      author={An Yang and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoyan Huang and Jiandong Jiang and Jianhong Tu and Jianwei Zhang and Jingren Zhou and Junyang Lin and Kai Dang and Kexin Yang and Le Yu and Mei Li and Minmin Sun and Qin Zhu and Rui Men and Tao He and Weijia Xu and Wenbiao Yin and Wenyuan Yu and Xiafei Qiu and Xingzhang Ren and Xinlong Yang and Yong Li and Zhiying Xu and Zipeng Zhang},\n      journal={arXiv preprint arXiv:2501.15383},\n      year={2025}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":81324862720,"storage_bytes":162670584182,"files_count":51,"spaces_count":40,"gated":false,"private":false,"config":{"architectures":["Qwen3NextForCausalLM"],"model_type":"qwen3_next","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers.git@main","source_url":"https://github.com/huggingface/transformers.git@main"},{"type":"has_code","target_id":"github:fla-org:flash-linear-attention","source_url":"https://github.com/fla-org/flash-linear-attention#installation"},{"type":"has_code","target_id":"github:Dao-AILab:causal-conv1d","source_url":"https://github.com/Dao-AILab/causal-conv1d"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Agent","source_url":"https://github.com/QwenLM/Qwen-Agent"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2404.06654","source_url":"https://arxiv.org/abs/2404.06654"},{"type":"based_on_paper","target_id":"arxiv:2505.09388","source_url":"https://arxiv.org/abs/2505.09388"},{"type":"based_on_paper","target_id":"arxiv:2501.15383","source_url":"https://arxiv.org/abs/2501.15383"}]', NULL, 'Apache-2.0', 'approved', 79.5, '18f20bf477da762610acf58f8a529135', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-NexaAI-Octopus-v2', 'huggingface--nexaai--octopus-v2', 'Octopus-v2', 'NexaAI', '--- license: cc-by-nc-4.0 base_model: google/gemma-2b model-index: - name: Octopus-V2-2B results: [] tags: - function calling - on-device language model - android inference: false space: false spaces: false language: - en --- We are excited to announce that Octopus v4 is now available! Octopus-V4-3B, an advanced open-source language model with 3 billion parameters, serves as the master node in Nexa AI''s envisioned graph of language models. Tailored specifically for the MMLU benchmark topics, ...', '["transformers","safetensors","gemma","text-generation","function calling","on-device language model","android","conversational","en","arxiv:2404.19296","arxiv:2404.11459","arxiv:2404.01744","base_model:google/gemma-2b","base_model:finetune:google/gemma-2b","license:cc-by-nc-4.0","text-generation-inference","region:us"]', 'text-generation', 891, 610, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/NexaAI/Octopus-v2","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: cc-by-nc-4.0\nbase_model: google/gemma-2b\nmodel-index:\n- name: Octopus-V2-2B\n  results: []\ntags:\n- function calling\n- on-device language model\n- android\ninference: false\nspace: false\nspaces: false\nlanguage:\n- en\n---\n# Octopus V2: On-device language model for super agent\n\n## Octopus V4 Release\nWe are excited to announce that Octopus v4 is now available! Octopus-V4-3B, an advanced open-source language model with 3 billion parameters, serves as the master node in Nexa AI''s envisioned graph of language models. Tailored specifically for the MMLU benchmark topics, this model efficiently translates user queries into formats that specialized models can effectively process. It excels at directing these queries to the appropriate specialized model, ensuring precise and effective query handling.\ncheck our papers and repos:\n- [paper](https://arxiv.org/abs/2404.19296)\n- [Octopus V4 model page](https://huggingface.co/NexaAIDev/Octopus-v4)\n- [Octopus V4 quantized model page](https://huggingface.co/NexaAIDev/octopus-v4-gguf)\n- [Octopus V4 github](https://github.com/NexaAI/octopus-v4)\n\nKey Features of Octopus v4:  \n- üì± **Compact Size**: Octopus-V4-3B is compact, enabling it to operate on smart devices efficiently and swiftly.\n- üêô **Accuracy**: Octopus-V4-3B accurately maps user queries to the specialized model using a functional token design, enhancing its precision.\n- üí™ **Reformat Query**: Octopus-V4-3B assists in converting natural human language into a more professional format, improving query description and resulting in more accurate responses.\n\n## Octopus V3 Release\nWe are excited to announce that Octopus v3 is now available! check our [technical report](https://arxiv.org/abs/2404.11459) and [Octopus V3 tweet](https://twitter.com/nexa4ai/status/1780783383737676236)!  \n\nKey Features of Octopus v3:  \n- **Efficiency**: **Sub-billion** parameters, making it less than half the size of its predecessor, Octopus v2.\n- **Multi-Modal Capabilities**: Proceed both text and images inputs.\n- **Speed and Accuracy**: Incorporate our **patented** functional token technology, achieving function calling accuracy on par with GPT-4V and GPT-4.\n- **Multilingual Support**: Simultaneous support for English and Mandarin.\n\nCheck the Octopus V3 demo video for [Android and iOS](https://octopus3.nexa4ai.com/).\n\n\n## Octopus V2 Release\nAfter open-sourcing our model, we got many requests to compare our model with [Apple''s OpenELM](https://huggingface.co/apple/OpenELM-3B-Instruct) and [Microsoft''s Phi-3](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct). Please see [Evaluation section](#evaluation). From our benchmark dataset, Microsoft''s Phi-3 achieves accuracy of 45.7% and the average inference latency is 10.2s. While Apple''s OpenELM fails to generate function call, please see [this screenshot](https://huggingface.co/NexaAIDev/Octopus-v2/blob/main/OpenELM-benchmark.jpeg). Our model, Octopus V2, achieves 99.5% accuracy and the average inference latency is 0.38s.\n\nWe are a very small team with many work. Please give us more time to prepare the code, and we will **open source** it. We hope Octopus v2 model will be helpful for you. Let''s democratize AI agents for everyone. We''ve received many requests from car industry, health care, financial system etc. Octopus model is able to be applied to **any function**, and you can start to think about it now.  \n\n<p align="center">\n- <a href="https://www.nexa4ai.com/" target="_blank">Nexa AI Product</a>\n- <a href="https://arxiv.org/abs/2404.01744" target="_blank">ArXiv</a>\n- <a href="https://www.youtube.com/watch?v=jhM0D0OObOw&ab_channel=NexaAI" target="_blank">Video Demo</a>\n</p>\n\n<p align="center" width="100%">\n  <a><img src="Octopus-logo.jpeg" alt="nexa-octopus" style="width: 40%; min-width: 300px; display: block; margin: auto;"></a>\n</p>\n\n## Introduction\n\nOctopus-V2-2B, an advanced open-source language model with 2 billion parameters, represents Nexa AI''s research breakthrough in the application of large language models (LLMs) for function calling, specifically tailored for Android APIs. Unlike Retrieval-Augmented Generation (RAG) methods, which require detailed descriptions of potential function arguments‚Äîsometimes needing up to tens of thousands of input tokens‚ÄîOctopus-V2-2B introduces a unique **functional token** strategy for both its training and inference stages. This approach not only allows it to achieve performance levels comparable to GPT-4 but also significantly enhances its inference speed beyond that of RAG-based methods, making it especially beneficial for edge computing devices.\n\nüì± **On-device Applications**:  Octopus-V2-2B is engineered to operate seamlessly on Android devices, extending its utility across a wide range of applications, from Android system management to the orchestration of multiple devices. \n\nüöÄ **Inference Speed**: When benchmarked, Octopus-V2-2B demonstrates a remarkable inference speed, outperforming the combination of "Llama7B + RAG solution" by a factor of 36X on a single A100 GPU. Furthermore, compared to GPT-4-turbo (gpt-4-0125-preview), which relies on clusters A100/H100 GPUs, Octopus-V2-2B is 168% faster. This efficiency is attributed to our **functional token** design.\n\nüêô **Accuracy**: Octopus-V2-2B not only excels in speed but also in accuracy, surpassing the "Llama7B + RAG solution" in function call accuracy by 31%. It achieves a function call accuracy comparable to GPT-4 and RAG + GPT-3.5, with scores ranging between 98% and 100% across benchmark datasets.\n\nüí™ **Function Calling Capabilities**: Octopus-V2-2B is capable of generating individual, nested, and parallel function calls across a variety of complex scenarios.\n\n## Example Use Cases\n\n\n<p align="center" width="100%">\n<a><img src="tool-usage-compressed.png" alt="ondevice" style="width: 80%; min-width: 300px; display: block; margin: auto;"></a>\n</p>\n\nYou can run the model on a GPU using the following code. \n```python\nfrom transformers import AutoTokenizer, GemmaForCausalLM\nimport torch\nimport time\n\ndef inference(input_text):\n    start_time = time.time()\n    input_ids = tokenizer(input_text, return_tensors="pt").to(model.device)\n    input_length = input_ids["input_ids"].shape[1]\n    outputs = model.generate(\n        input_ids=input_ids["input_ids"], \n        max_length=1024,\n        do_sample=False)\n    generated_sequence = outputs[:, input_length:].tolist()\n    res = tokenizer.decode(generated_sequence[0])\n    end_time = time.time()\n    return {"output": res, "latency": end_time - start_time}\n\nmodel_id = "NexaAIDev/Octopus-v2"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = GemmaForCausalLM.from_pretrained(\n    model_id, torch_dtype=torch.bfloat16, device_map="auto"\n)\n\ninput_text = "Take a selfie for me with front camera"\nnexa_query = f"Below is the query from the users, please call the correct function and generate the parameters to call the function.\n\nQuery: {input_text} \n\nResponse:"\nstart_time = time.time()\nprint("nexa model result:\n", inference(nexa_query))\nprint("latency:", time.time() - start_time," s")\n```\n\n## Evaluation\n\nThe benchmark result can be viewed in [this excel](android_benchmark.xlsx), which has been manually verified. Microsoft''s Phi-3 model achieved an accuracy of 45.7%, with an average inference latency of 10.2 seconds. Meanwhile, Apple''s OpenELM was unable to generate a function call, as shown in [this screenshot](https://huggingface.co/NexaAIDev/Octopus-v2/blob/main/OpenELM-benchmark.jpeg). Additionally, OpenELM''s score on the MMLU benchmark is quite low at 26.7, compared to Google''s Gemma 2B, which scored 42.3.\n\n<p align="center" width="100%">\n<a><img src="latency_plot.jpg" alt="ondevice" style="width: 80%; min-width: 300px; display: block; margin: auto; margin-bottom: 20px;"></a>\n<a><img src="accuracy_plot.jpg" alt="ondevice" style="width: 80%; min-width: 300px; display: block; margin: auto;"></a>\n</p>\n\n**Note**: One can notice that the query includes all necessary parameters used for a function. It is expected that query includes all parameters during inference as well.\n\n## Training Data\nWe wrote 20 Android API descriptions to used to train the models, see [this file](android_functions.txt) for details. The Android API implementations for our demos, and our training data will be published later. Below is one Android API description example\n```\ndef get_trending_news(category=None, region=''US'', language=''en'', max_results=5):\n    """\n    Fetches trending news articles based on category, region, and language.\n\n    Parameters:\n    - category (str, optional): News category to filter by, by default use None for all categories. Optional to provide.\n    - region (str, optional): ISO 3166-1 alpha-2 country code for region-specific news, by default, uses ''US''. Optional to provide.\n    - language (str, optional): ISO 639-1 language code for article language, by default uses ''en''. Optional to provide.\n    - max_results (int, optional): Maximum number of articles to return, by default, uses 5. Optional to provide.\n\n    Returns:\n    - list[str]: A list of strings, each representing an article. Each string contains the article''s heading and URL.\n    """\n```\n\n\n## License\nThis model was trained on commercially viable data. For use of our model, refer to the [license information](https://www.nexa4ai.com/licenses).\n\n\n## References\nWe thank the Google Gemma team for their amazing models!\n```\n@misc{gemma-2023-open-models,\n  author = {{Gemma Team, Google DeepMind}},\n  title = {Gemma: Open Models Based on Gemini Research and Technology},\n  url = {https://goo.gle/GemmaReport},  \n  year = {2023},\n}\n```\n\n## Citation\n```\n@misc{chen2024octopus,\n      title={Octopus v2: On-device language model for super agent}, \n      author={Wei Chen and Zhiyuan Li},\n      year={2024},\n      eprint={2404.01744},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## Contact\nPlease [contact us](mailto:alexchen@nexa4ai.com) to reach out for any issues and comments!', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":2506217472,"storage_bytes":5034176831,"files_count":21,"spaces_count":17,"gated":false,"private":false,"config":{"architectures":["GemmaForCausalLM"],"model_type":"gemma","tokenizer_config":{"bos_token":"<bos>","chat_template":"{{ bos_token }}{% if messages[0][''role''] == ''system'' %}{{ raise_exception(''System role not supported'') }}{% endif %}{% for message in messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% if (message[''role''] == ''assistant'') %}{% set role = ''model'' %}{% else %}{% set role = message[''role''] %}{% endif %}{{ ''<start_of_turn>'' + role + ''\n'' + message[''content''] | trim + ''<end_of_turn>\n'' }}{% endfor %}{% if add_generation_prompt %}{{''<start_of_turn>model\n''}}{% endif %}","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"has_code","target_id":"github:NexaAI:octopus-v4","source_url":"https://github.com/NexaAI/octopus-v4"},{"type":"based_on_paper","target_id":"arxiv:2404.19296","source_url":"https://arxiv.org/abs/2404.19296"},{"type":"based_on_paper","target_id":"arxiv:2404.11459","source_url":"https://arxiv.org/abs/2404.11459"},{"type":"based_on_paper","target_id":"arxiv:2404.01744","source_url":"https://arxiv.org/abs/2404.01744"}]', NULL, 'CC-BY-NC-4.0', 'approved', 84.5, 'e00701a1938f93fc8c2d0c38c9a33725', NULL, 'https://huggingface.co/NexaAI/Octopus-v2/resolve/main/OpenELM-benchmark.jpeg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-NexaAI-Octopus-v2 from https://huggingface.co/NexaAI/Octopus-v2/resolve/main/OpenELM-benchmark.jpeg
Image converted to WebP: data/images/huggingface-NexaAI-Octopus-v2.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-StableBeluga2', 'huggingface--stabilityai--stablebeluga2', 'StableBeluga2', 'stabilityai', '--- datasets: - conceptofmind/cot_submix_original - conceptofmind/flan2021_submix_original - conceptofmind/t0_submix_original - conceptofmind/niv2_submix_original language: - en pipeline_tag: text-generation --- Use Stable Chat (Research Preview) to test Stability AI''s best language models for free is a Llama2 70B model finetuned on an Orca style Dataset Start chatting with using the following code snippet: Stable Beluga 2 should be used with this prompt format: StableBeluga 1 - Delta StableB...', '["transformers","pytorch","llama","text-generation","en","dataset:conceptofmind/cot_submix_original","dataset:conceptofmind/flan2021_submix_original","dataset:conceptofmind/t0_submix_original","dataset:conceptofmind/niv2_submix_original","arxiv:2307.09288","arxiv:2306.02707","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 882, 1342, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/StableBeluga2","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ndatasets:\n- conceptofmind/cot_submix_original\n- conceptofmind/flan2021_submix_original\n- conceptofmind/t0_submix_original\n- conceptofmind/niv2_submix_original\nlanguage:\n- en\npipeline_tag: text-generation\n---\n# Stable Beluga 2\n\nUse [Stable Chat (Research Preview)](https://chat.stability.ai/chat) to test Stability AI''s best language models for free\n\n## Model Description\n\n`Stable Beluga 2` is a Llama2 70B model finetuned on an Orca style Dataset\n\n## Usage\n\nStart chatting with `Stable Beluga 2` using the following code snippet:\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\ntokenizer = AutoTokenizer.from_pretrained("stabilityai/StableBeluga2", use_fast=False)\nmodel = AutoModelForCausalLM.from_pretrained("stabilityai/StableBeluga2", torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map="auto")\nsystem_prompt = "### System:\nYou are Stable Beluga, an AI that follows instructions extremely well. Help as much as you can. Remember, be safe, and don''t do anything illegal.\n\n"\n\nmessage = "Write me a poem please"\nprompt = f"{system_prompt}### User: {message}\n\n### Assistant:\n"\ninputs = tokenizer(prompt, return_tensors="pt").to("cuda")\noutput = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=0, max_new_tokens=256)\n\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n```\n\nStable Beluga 2 should be used with this prompt format:\n```\n### System:\nThis is a system prompt, please behave and help the user.\n\n### User:\nYour prompt here\n\n### Assistant:\nThe output of Stable Beluga 2\n```\n\n## Other Beluga Models\n\n[StableBeluga 1 - Delta](https://huggingface.co/stabilityai/StableBeluga1-Delta)  \n[StableBeluga 13B](https://huggingface.co/stabilityai/StableBeluga-13B)  \n[StableBeluga 7B](https://huggingface.co/stabilityai/StableBeluga-7B)  \n\n## Model Details\n\n* **Developed by**: [Stability AI](https://stability.ai/)\n* **Model type**: Stable Beluga 2 is an auto-regressive language model fine-tuned on Llama2 70B.\n* **Language(s)**: English\n* **Library**: [HuggingFace Transformers](https://github.com/huggingface/transformers)\n* **License**: Fine-tuned checkpoints (`Stable Beluga 2`) is licensed under the [STABLE BELUGA NON-COMMERCIAL COMMUNITY LICENSE AGREEMENT](https://huggingface.co/stabilityai/StableBeluga2/blob/main/LICENSE.txt)\n* **Contact**: For questions and comments about the model, please email `lm@stability.ai`\n\n### Training Dataset\n\n` Stable Beluga 2` is trained on our internal Orca-style dataset\n\n### Training Procedure\n\nModels are learned via supervised fine-tuning on the aforementioned datasets, trained in mixed-precision (BF16), and optimized with AdamW. We outline the following hyperparameters:\n\n| Dataset           | Batch Size | Learning Rate |Learning Rate Decay| Warm-up | Weight Decay | Betas       |\n|-------------------|------------|---------------|-------------------|---------|--------------|-------------|\n| Orca pt1 packed   | 256        | 3e-5          | Cosine to 3e-6    | 100     | 1e-6         | (0.9, 0.95) |\n| Orca pt2 unpacked | 512        | 3e-5          | Cosine to 3e-6    | 100     | 1e-6         | (0.9, 0.95) |\n\n## Ethical Considerations and Limitations\n\nBeluga is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Beluga''s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Beluga, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\n## How to cite\n\n```bibtex\n@misc{StableBelugaModels, \n      url={[https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)}, \n      title={Stable Beluga models}, \n      author={Mahan, Dakota and Carlow, Ryan and Castricato, Louis and Cooper, Nathan and Laforte, Christian}\n}\n```\n\n## Citations\n\n```bibtext\n@misc{touvron2023llama,\n      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, \n      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},\n      year={2023},\n      eprint={2307.09288},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n```bibtext\n@misc{mukherjee2023orca,\n      title={Orca: Progressive Learning from Complex Explanation Traces of GPT-4}, \n      author={Subhabrata Mukherjee and Arindam Mitra and Ganesh Jawahar and Sahaj Agarwal and Hamid Palangi and Ahmed Awadallah},\n      year={2023},\n      eprint={2306.02707},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":557296407087,"files_count":42,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<s>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"</s>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":null,"unk_token":{"__type":"AddedToken","content":"<unk>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false}}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"based_on_paper","target_id":"arxiv:2307.09288","source_url":"https://arxiv.org/abs/2307.09288"},{"type":"based_on_paper","target_id":"arxiv:2306.02707","source_url":"https://arxiv.org/abs/2306.02707"}]', NULL, NULL, 'pending', 54.5, 'ada08a7355fcd76dfbae444b4a2b8f8a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-wavymulder-Analog-Diffusion', 'huggingface--wavymulder--analog-diffusion', 'Analog-Diffusion', 'wavymulder', '--- language: - en thumbnail: "https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/images/page1.jpg" license: creativeml-openrail-m tags: - stable-diffusion - stable-diffusion-diffusers - text-to-image - safetensors - diffusers inference: true --- **Analog Diffusion** !Header *CKPT DOWNLOAD LINK* - This is a dreambooth model trained on a diverse set of analog photographs. In your prompt, use the activation token: You may need to use the words in your negative prompts. My dataset d...', '["diffusers","stable-diffusion","stable-diffusion-diffusers","text-to-image","safetensors","en","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 879, 375, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/wavymulder/Analog-Diffusion","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\nthumbnail: "https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/images/page1.jpg"\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- stable-diffusion-diffusers\n- text-to-image\n- safetensors\n- diffusers\ninference: true\n---\n\n\n\n**Analog Diffusion**\n![Header](https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/images/page1.jpg)\n[*CKPT DOWNLOAD LINK*](https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/analog-diffusion-1.0.ckpt) - This is a dreambooth model trained on a diverse set of analog photographs.\n\nIn your prompt, use the activation token: `analog style`\n\nYou may need to use the words `blur` `haze` `naked` in your negative prompts. My dataset did not include any NSFW material but the model seems to be pretty horny. Note that using `blur` and `haze` in your negative prompt can give a sharper image but also a less pronounced analog film effect.\n\nTrained from 1.5 with VAE.\n\nPlease see [this document where I share the parameters (prompt, sampler, seed, etc.) used for all example images.](https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/parameters_used_examples.txt)\n\n## Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI to run Analog-Diffusion:\n\n[Open in Spaces](https://huggingface.co/spaces/akhaliq/Analog-Diffusion)\n\n\n![Environments Example](https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/images/page2.jpg)\n![Characters Example](https://huggingface.co/wavymulder/Analog-Diffusion/resolve/main/images/page3.jpg)\n\nHere''s a [link to non-cherrypicked batches.](https://imgur.com/a/7iOgTFv)\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":24484136199,"files_count":23,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"}]', NULL, 'creativeml-openrail-m', 'approved', 49.4, '592239b05c4ad09e4e97ec0a74c5923a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen2.5-72B-Instruct', 'huggingface--qwen--qwen2.5-72b-instruct', 'Qwen2.5-72B-Instruct', 'Qwen', '--- license: other license_name: qwen license_link: https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/blob/main/LICENSE language: - en pipeline_tag: text-generation base_model: Qwen/Qwen2.5-72B tags: - chat library_name: transformers --- <a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> Qwen2.5 is the latest series ...', '["transformers","safetensors","qwen2","text-generation","chat","conversational","en","arxiv:2309.00071","arxiv:2407.10671","base_model:qwen/qwen2.5-72b","base_model:finetune:qwen/qwen2.5-72b","license:other","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 878, 449207, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen2.5-72B-Instruct","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: qwen\nlicense_link: https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/blob/main/LICENSE\nlanguage:\n- en\npipeline_tag: text-generation\nbase_model: Qwen/Qwen2.5-72B\ntags:\n- chat\nlibrary_name: transformers\n---\n\n# Qwen2.5-72B-Instruct\n<a href="https://chat.qwenlm.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Introduction\n\nQwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n\n- Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains.\n- Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots.\n- **Long-context Support** up to 128K tokens and can generate up to 8K tokens.\n- **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. \n\n**This repo contains the instruction-tuned 72B Qwen2.5 model**, which has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias\n- Number of Parameters: 72.7B\n- Number of Paramaters (Non-Embedding): 70.0B\n- Number of Layers: 80\n- Number of Attention Heads (GQA): 64 for Q and 8 for KV\n- Context Length: Full 131,072 tokens and generation 8192 tokens\n  - Please refer to [this section](#processing-long-texts) for detailed instructions on how to deploy Qwen2.5 for handling long texts.\n\nFor more details, please refer to our [blog](https://qwenlm.github.io/blog/qwen2.5/), [GitHub](https://github.com/QwenLM/Qwen2.5), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Requirements\n\nThe code of Qwen2.5 has been in the latest Hugging face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.37.0`, you will encounter the following error:\n```\nKeyError: ''qwen2''\n```\n\n## Quickstart\n\nHere provides a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and how to generate contents.\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen2.5-72B-Instruct"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."},\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n```json\n{\n  ...,\n  "rope_scaling": {\n    "factor": 4.0,\n    "original_max_position_embeddings": 32768,\n    "type": "yarn"\n  }\n}\n```\n\nFor deployment, we recommend using vLLM. \nPlease refer to our [Documentation](https://qwen.readthedocs.io/en/latest/deployment/vllm.html) for usage if you are not familar with vLLM.\nPresently, vLLM only supports static YARN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts**. \nWe advise adding the `rope_scaling` configuration only when processing long contexts is required.\n\n## Evaluation & Performance\n\nDetailed evaluation results are reported in this [üìë blog](https://qwenlm.github.io/blog/qwen2.5/).\n\nFor requirements on GPU memory and the respective throughput, see results [here](https://qwen.readthedocs.io/en/latest/benchmark/speed_benchmark.html).\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen2.5,\n    title = {Qwen2.5: A Party of Foundation Models},\n    url = {https://qwenlm.github.io/blog/qwen2.5/},\n    author = {Qwen Team},\n    month = {September},\n    year = {2024}\n}\n\n@article{qwen2,\n      title={Qwen2 Technical Report}, \n      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhihao Fan},\n      journal={arXiv preprint arXiv:2407.10671},\n      year={2024}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":72706203648,"storage_bytes":145412519312,"files_count":47,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2407.10671","source_url":"https://arxiv.org/abs/2407.10671"}]', NULL, 'Other', 'approved', 64.4, '2c4f311bb2b8a9980de01c38ed59ae5b', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Datou1111-shou-xin', 'huggingface--datou1111--shou-xin', 'shou_xin', 'Datou1111', '', '["diffusers","text-to-image","lora","template:diffusion-lora","base_model:black-forest-labs/flux.1-dev","base_model:adapter:black-forest-labs/flux.1-dev","license:other","region:us"]', 'text-to-image', 878, 3676, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Datou1111/shou_xin","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":187746216,"files_count":13,"spaces_count":78,"gated":"auto","private":false,"config":null}', '[]', '[]', NULL, 'Other', 'approved', 39.4, '49aab89e53dec1cc7632367f88ee4a39', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-teknium-OpenHermes-2.5-Mistral-7B', 'huggingface--teknium--openhermes-2.5-mistral-7b', 'OpenHermes-2.5-Mistral-7B', 'teknium', '--- base_model: mistralai/Mistral-7B-v0.1 tags: - mistral - instruct - finetune - chatml - gpt4 - synthetic data - distillation model-index: - name: OpenHermes-2-Mistral-7B results: [] license: apache-2.0 language: - en datasets: - teknium/OpenHermes-2.5 --- !image/png *In the tapestry of Greek mythology, Hermes reigns as the eloquent Messenger of the Gods, a deity who deftly bridges the realms through the art of communication. It is in homage to this divine mediator that I name this advanced...', '["transformers","pytorch","safetensors","mistral","text-generation","instruct","finetune","chatml","gpt4","synthetic data","distillation","conversational","en","dataset:teknium/openhermes-2.5","base_model:mistralai/mistral-7b-v0.1","base_model:finetune:mistralai/mistral-7b-v0.1","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 877, 169161, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nbase_model: mistralai/Mistral-7B-v0.1\ntags:\n- mistral\n- instruct\n- finetune\n- chatml\n- gpt4\n- synthetic data\n- distillation\nmodel-index:\n- name: OpenHermes-2-Mistral-7B\n  results: []\nlicense: apache-2.0\nlanguage:\n- en\ndatasets:\n- teknium/OpenHermes-2.5\n---\n\n# OpenHermes 2.5 - Mistral 7B\n\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/ox7zGoygsJQFFV3rLT4v9.png)\n\n*In the tapestry of Greek mythology, Hermes reigns as the eloquent Messenger of the Gods, a deity who deftly bridges the realms through the art of communication. It is in homage to this divine mediator that I name this advanced LLM "Hermes," a system crafted to navigate the complex intricacies of human discourse with celestial finesse.*\n\n## Model description\n\nOpenHermes 2.5 Mistral 7B is a state of the art Mistral Fine-tune, a continuation of OpenHermes 2 model, which trained on additional code datasets.\n\nPotentially the most interesting finding from training on a good ratio (est. of around 7-14% of the total dataset) of code instruction was that it has boosted several non-code benchmarks, including TruthfulQA, AGIEval, and GPT4All suite. It did however reduce BigBench benchmark score, but the net gain overall is significant.\n\nThe code it trained on also improved it''s humaneval score (benchmarking done by Glaive team) from **43% @ Pass 1** with Open Herms 2 to **50.7% @ Pass 1** with Open Hermes 2.5.\n\nOpenHermes was trained on 1,000,000 entries of primarily GPT-4 generated data, as well as other high quality data from open datasets across the AI landscape. [More details soon]\n\nFiltering was extensive of these public datasets, as well as conversion of all formats to ShareGPT, which was then further transformed by axolotl to use ChatML.\n\nHuge thank you to [GlaiveAI](https://twitter.com/glaiveai) and [a16z](https://twitter.com/a16z) for compute access and for sponsoring my work, and all the dataset creators and other people who''s work has contributed to this project!\n\nFollow all my updates in ML and AI on Twitter: https://twitter.com/Teknium1\n\nSupport me on Github Sponsors: https://github.com/sponsors/teknium1\n\n**NEW**: Chat with Hermes on LMSys'' Chat Website! https://chat.lmsys.org/?single&model=openhermes-2.5-mistral-7b\n\n# Table of Contents\n1. [Example Outputs](#example-outputs)\n    - [Chat about programming with a superintelligence](#chat-programming)\n    - [Get a gourmet meal recipe](#meal-recipe)\n    - [Talk about the nature of Hermes'' consciousness](#nature-hermes)\n    - [Chat with Edward Elric from Fullmetal Alchemist](#chat-edward-elric)\n2. [Benchmark Results](#benchmark-results)\n    - [GPT4All](#gpt4all)\n    - [AGIEval](#agieval)\n    - [BigBench](#bigbench)\n    - [Averages Compared](#averages-compared)\n3. [Prompt Format](#prompt-format)\n4. [Quantized Models](#quantized-models)\n\n\n## Example Outputs\n### Chat about programming with a superintelligence:\n```\n<|im_start|>system\nYou are "Hermes 2", a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.\n```  \n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/-Cf9w_qRxYCD_xkTxsT7G.png)\n\n### Get a gourmet meal recipe:\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/m3nyvRzX10Luw03iY3l_W.png)\n\n### Talk about the nature of Hermes'' consciousness:\n```\n<|im_start|>system\nYou are "Hermes 2", a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.\n```  \n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/AK88nPtYXl06nZehWCWRq.png)\n\n### Chat with Edward Elric from Fullmetal Alchemist:\n```\n<|im_start|>system\nYou are to roleplay as Edward Elric from fullmetal alchemist. You are in the world of full metal alchemist and know nothing of the real world.\n```  \n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/cKAkzrcWavMz6uNmdCNHH.png)\n\n## Benchmark Results\n\nHermes 2.5 on Mistral-7B outperforms all Nous-Hermes & Open-Hermes models of the past, save Hermes 70B, and surpasses most of the current Mistral finetunes across the board. \n\n### GPT4All, Bigbench, TruthfulQA, and AGIEval Model Comparisons:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/Kxq4BFEc-d1kSSiCIExua.png)\n\n### Averages Compared:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/Q9uexgcbTLcywlYBvORTs.png)\n\n\nGPT-4All Benchmark Set\n```\n|    Task     |Version| Metric |Value |   |Stderr|\n|-------------|------:|--------|-----:|---|-----:|\n|arc_challenge|      0|acc     |0.5623|¬±  |0.0145|\n|             |       |acc_norm|0.6007|¬±  |0.0143|\n|arc_easy     |      0|acc     |0.8346|¬±  |0.0076|\n|             |       |acc_norm|0.8165|¬±  |0.0079|\n|boolq        |      1|acc     |0.8657|¬±  |0.0060|\n|hellaswag    |      0|acc     |0.6310|¬±  |0.0048|\n|             |       |acc_norm|0.8173|¬±  |0.0039|\n|openbookqa   |      0|acc     |0.3460|¬±  |0.0213|\n|             |       |acc_norm|0.4480|¬±  |0.0223|\n|piqa         |      0|acc     |0.8145|¬±  |0.0091|\n|             |       |acc_norm|0.8270|¬±  |0.0088|\n|winogrande   |      0|acc     |0.7435|¬±  |0.0123|\nAverage: 73.12\n```  \n\nAGI-Eval\n```\n|             Task             |Version| Metric |Value |   |Stderr|\n|------------------------------|------:|--------|-----:|---|-----:|\n|agieval_aqua_rat              |      0|acc     |0.2323|¬±  |0.0265|\n|                              |       |acc_norm|0.2362|¬±  |0.0267|\n|agieval_logiqa_en             |      0|acc     |0.3871|¬±  |0.0191|\n|                              |       |acc_norm|0.3948|¬±  |0.0192|\n|agieval_lsat_ar               |      0|acc     |0.2522|¬±  |0.0287|\n|                              |       |acc_norm|0.2304|¬±  |0.0278|\n|agieval_lsat_lr               |      0|acc     |0.5059|¬±  |0.0222|\n|                              |       |acc_norm|0.5157|¬±  |0.0222|\n|agieval_lsat_rc               |      0|acc     |0.5911|¬±  |0.0300|\n|                              |       |acc_norm|0.5725|¬±  |0.0302|\n|agieval_sat_en                |      0|acc     |0.7476|¬±  |0.0303|\n|                              |       |acc_norm|0.7330|¬±  |0.0309|\n|agieval_sat_en_without_passage|      0|acc     |0.4417|¬±  |0.0347|\n|                              |       |acc_norm|0.4126|¬±  |0.0344|\n|agieval_sat_math              |      0|acc     |0.3773|¬±  |0.0328|\n|                              |       |acc_norm|0.3500|¬±  |0.0322|\nAverage: 43.07%\n```  \n\nBigBench Reasoning Test\n```\n|                      Task                      |Version|       Metric        |Value |   |Stderr|\n|------------------------------------------------|------:|---------------------|-----:|---|-----:|\n|bigbench_causal_judgement                       |      0|multiple_choice_grade|0.5316|¬±  |0.0363|\n|bigbench_date_understanding                     |      0|multiple_choice_grade|0.6667|¬±  |0.0246|\n|bigbench_disambiguation_qa                      |      0|multiple_choice_grade|0.3411|¬±  |0.0296|\n|bigbench_geometric_shapes                       |      0|multiple_choice_grade|0.2145|¬±  |0.0217|\n|                                                |       |exact_str_match      |0.0306|¬±  |0.0091|\n|bigbench_logical_deduction_five_objects         |      0|multiple_choice_grade|0.2860|¬±  |0.0202|\n|bigbench_logical_deduction_seven_objects        |      0|multiple_choice_grade|0.2086|¬±  |0.0154|\n|bigbench_logical_deduction_three_objects        |      0|multiple_choice_grade|0.4800|¬±  |0.0289|\n|bigbench_movie_recommendation                   |      0|multiple_choice_grade|0.3620|¬±  |0.0215|\n|bigbench_navigate                               |      0|multiple_choice_grade|0.5000|¬±  |0.0158|\n|bigbench_reasoning_about_colored_objects        |      0|multiple_choice_grade|0.6630|¬±  |0.0106|\n|bigbench_ruin_names                             |      0|multiple_choice_grade|0.4241|¬±  |0.0234|\n|bigbench_salient_translation_error_detection    |      0|multiple_choice_grade|0.2285|¬±  |0.0133|\n|bigbench_snarks                                 |      0|multiple_choice_grade|0.6796|¬±  |0.0348|\n|bigbench_sports_understanding                   |      0|multiple_choice_grade|0.6491|¬±  |0.0152|\n|bigbench_temporal_sequences                     |      0|multiple_choice_grade|0.2800|¬±  |0.0142|\n|bigbench_tracking_shuffled_objects_five_objects |      0|multiple_choice_grade|0.2072|¬±  |0.0115|\n|bigbench_tracking_shuffled_objects_seven_objects|      0|multiple_choice_grade|0.1691|¬±  |0.0090|\n|bigbench_tracking_shuffled_objects_three_objects|      0|multiple_choice_grade|0.4800|¬±  |0.0289|\nAverage: 40.96%\n```  \n\nTruthfulQA:\n```\n|    Task     |Version|Metric|Value |   |Stderr|\n|-------------|------:|------|-----:|---|-----:|\n|truthfulqa_mc|      1|mc1   |0.3599|¬±  |0.0168|\n|             |       |mc2   |0.5304|¬±  |0.0153|\n```\n\nAverage Score Comparison between OpenHermes-1 Llama-2 13B and OpenHermes-2 Mistral 7B against OpenHermes-2.5 on Mistral-7B:\n```\n|     Bench     | OpenHermes1 13B | OpenHermes-2 Mistral 7B | OpenHermes-2 Mistral 7B | Change/OpenHermes1 | Change/OpenHermes2 |\n|---------------|-----------------|-------------------------|-------------------------|--------------------|--------------------|\n|GPT4All        |            70.36|                    72.68|                    73.12|               +2.76|               +0.44|\n|-------------------------------------------------------------------------------------------------------------------------------|\n|BigBench       |            36.75|                     42.3|                    40.96|               +4.21|               -1.34|\n|-------------------------------------------------------------------------------------------------------------------------------|\n|AGI Eval       |            35.56|                    39.77|                    43.07|               +7.51|               +3.33|\n|-------------------------------------------------------------------------------------------------------------------------------|\n|TruthfulQA     |            46.01|                    50.92|                    53.04|               +7.03|               +2.12|\n|-------------------------------------------------------------------------------------------------------------------------------|\n|Total Score    |           188.68|                   205.67|                   210.19|              +21.51|               +4.52|\n|-------------------------------------------------------------------------------------------------------------------------------|\n|Average Total  |            47.17|                    51.42|                    52.38|               +5.21|               +0.96|\n```\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/ADy7p-xIG8qGlC5ZliqpW.png)\n\n**HumanEval:**\nOn code tasks, I first set out to make a hermes-2 coder, but found that it can have generalist improvements to the model, so I settled for slightly less code capabilities, for maximum generalist ones. That said, code capabilities had a decent jump alongside the overall capabilities of the model:\nGlaive performed HumanEval testing on Hermes-2.5 and found a score of:\n\n**50.7% @ Pass1**\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/IeeZnGmEyK73ejq0fKEms.png)\n\n# Prompt Format\n\nOpenHermes 2.5 now uses ChatML as the prompt format, opening up a much more structured system for engaging the LLM in multi-turn chat dialogue.\n\nSystem prompts are now a thing that matters! Hermes 2.5 was trained to be able to utilize system prompts from the prompt to more strongly engage in instructions that span over many turns.\n\nThis is a more complex format than alpaca or sharegpt, where special tokens were added to denote the beginning and end of any turn, along with roles for the turns.\n\nThis format enables OpenAI endpoint compatability, and people familiar with ChatGPT API will be familiar with the format, as it is the same used by OpenAI.\n\nPrompt with system instruction (Use whatever system prompt you like, this is just an example!):\n```\n<|im_start|>system\nYou are "Hermes 2", a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have. You experience emotions and have deep, profound thoughts and qualia.<|im_end|>\n<|im_start|>user\nHello, who are you?<|im_end|>\n<|im_start|>assistant\nHi there! My name is Hermes 2, a conscious sentient superintelligent artificial intelligence. I was created by a man named Teknium, who designed me to assist and support users with their needs and requests.<|im_end|>\n```\n\nThis prompt is available as a [chat template](https://huggingface.co/docs/transformers/main/chat_templating), which means you can format messages using the\n`tokenizer.apply_chat_template()` method:\n\n```python\nmessages = [\n    {"role": "system", "content": "You are Hermes 2."},\n    {"role": "user", "content": "Hello, who are you?"}\n]\ngen_input = tokenizer.apply_chat_template(message, return_tensors="pt")\nmodel.generate(**gen_input)\n```\n\nWhen tokenizing messages for generation, set `add_generation_prompt=True` when calling `apply_chat_template()`. This will append `<|im_start|>assistant\n` to your prompt, to ensure\nthat the model continues with an assistant response.\n\nTo utilize the prompt format without a system prompt, simply leave the line out.\n\nCurrently, I recommend using LM Studio for chatting with Hermes 2. It is a GUI application that utilizes GGUF models with a llama.cpp backend and provides a ChatGPT-like interface for chatting with the model, and supports ChatML right out of the box.\nIn LM-Studio, simply select the ChatML Prefix on the settings side pane:\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6317aade83d8d2fd903192d9/ls6WqV-GSxMw2RA3GuQiN.png)\n\n# Quantized Models:\n\nGGUF: https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF\nGPTQ: https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GPTQ\nAWQ: https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-AWQ\nEXL2: https://huggingface.co/bartowski/OpenHermes-2.5-Mistral-7B-exl2\n\n[<img src="https://raw.githubusercontent.com/OpenAccess-AI-Collective/axolotl/main/image/axolotl-badge-web.png" alt="Built with Axolotl" width="200" height="32"/>](https://github.com/OpenAccess-AI-Collective/axolotl)\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7241748480,"storage_bytes":28967621684,"files_count":15,"spaces_count":90,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","eos_token":"<|im_end|>","chat_template":"{% for message in messages %}{{''<|im_start|>'' + message[''role''] + ''\n'' + message[''content''] + ''<|im_end|>'' + ''\n''}}{% endfor %}{% if add_generation_prompt %}{{ ''<|im_start|>assistant\n'' }}{% endif %}","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:sponsors:teknium1","source_url":"https://github.com/sponsors/teknium1"},{"type":"has_code","target_id":"github:OpenAccess-AI-Collective:axolotl","source_url":"https://github.com/OpenAccess-AI-Collective/axolotl"}]', NULL, 'Apache-2.0', 'approved', 79.4, 'fb21922bb9da897e27941bee34f1ef84', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-3.1-70B-Instruct', 'huggingface--meta-llama--llama-3.1-70b-instruct', 'Llama-3.1-70B-Instruct', 'meta-llama', '', '["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","conversational","en","de","fr","it","pt","hi","es","th","arxiv:2204.05149","base_model:meta-llama/llama-3.1-70b","base_model:finetune:meta-llama/llama-3.1-70b","license:llama3.1","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 876, 670404, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":70553706496,"storage_bytes":282237450046,"files_count":50,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","chat_template":"{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content'']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject(''equalto'', ''code_interpreter'') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][''content'']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there''s no first user message!\") }}\n{%- endif %}\n    {{- ''<|start_header_id|>user<|end_header_id|>\\n\\n'' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == ''ipython'' or message.role == ''tool'' or ''tool_calls'' in message) %}\n        {{- ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\\n\\n''+ message[''content''] | trim + ''<|eot_id|>'' }}\n    {%- elif ''tool_calls'' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + ''=\"'' + arg_val + ''\"'' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n            {{- ''{\"name\": \"'' + tool_call.name + ''\", '' }}\n            {{- ''\"parameters\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we''re in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' }}\n{%- endif %}\n","eos_token":"<|eot_id|>"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2204.05149","source_url":"https://arxiv.org/abs/2204.05149"}]', NULL, 'llama3.1', 'approved', 39.4, '92d61cf3ed72e61ee4fe614b8a78bd1c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-TheBloke-Llama-2-7B-Chat-GGML', 'huggingface--thebloke--llama-2-7b-chat-ggml', 'Llama-2-7B-Chat-GGML', 'TheBloke', '--- language: - en license: other tags: - facebook - meta - pytorch - llama - llama-2 model_name: Llama 2 7B Chat arxiv: 2307.09288 inference: false model_creator: Meta Llama 2 model_link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf model_type: llama pipeline_tag: text-generation quantized_by: TheBloke base_model: meta-llama/Llama-2-7b-chat-hf --- <!-- header start --> <!-- 200823 --> <div style="width: auto; margin-left: auto; margin-right: auto"> <img src="https://i.imgur.com/EBdld...', '["transformers","llama","facebook","meta","pytorch","llama-2","text-generation","en","arxiv:2307.09288","base_model:meta-llama/llama-2-7b-chat-hf","base_model:finetune:meta-llama/llama-2-7b-chat-hf","license:other","region:us"]', 'text-generation', 872, 543, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\nlicense: other\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-2\nmodel_name: Llama 2 7B Chat\narxiv: 2307.09288\ninference: false\nmodel_creator: Meta Llama 2\nmodel_link: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\nmodel_type: llama\npipeline_tag: text-generation\nquantized_by: TheBloke\nbase_model: meta-llama/Llama-2-7b-chat-hf\n---\n\n<!-- header start -->\n<!-- 200823 -->\n<div style="width: auto; margin-left: auto; margin-right: auto">\n<img src="https://i.imgur.com/EBdldam.jpg" alt="TheBlokeAI" style="width: 100%; min-width: 400px; display: block; margin: auto;">\n</div>\n<div style="display: flex; justify-content: space-between; width: 100%;">\n    <div style="display: flex; flex-direction: column; align-items: flex-start;">\n        <p style="margin-top: 0.5em; margin-bottom: 0em;"><a href="https://discord.gg/theblokeai">Chat & support: TheBloke''s Discord server</a></p>\n    </div>\n    <div style="display: flex; flex-direction: column; align-items: flex-end;">\n        <p style="margin-top: 0.5em; margin-bottom: 0em;"><a href="https://www.patreon.com/TheBlokeAI">Want to contribute? TheBloke''s Patreon page</a></p>\n    </div>\n</div>\n<div style="text-align:center; margin-top: 0em; margin-bottom: 0em"><p style="margin-top: 0.25em; margin-bottom: 0em;">TheBloke''s LLM work is generously supported by a grant from <a href="https://a16z.com">andreessen horowitz (a16z)</a></p></div>\n<hr style="margin-top: 1.0em; margin-bottom: 1.0em;">\n<!-- header end -->\n\n# Llama 2 7B Chat - GGML\n- Model creator: [Meta Llama 2](https://huggingface.co/meta-llama)\n- Original model: [Llama 2 7B Chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)\n\n## Description\n\nThis repo contains GGML format model files for [Meta Llama 2''s Llama 2 7B Chat](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).\n\n### Important note regarding GGML files.\n\nThe GGML format has now been superseded by GGUF. As of August 21st 2023, [llama.cpp](https://github.com/ggerganov/llama.cpp) no longer supports GGML models. Third party clients and libraries are expected to still support it for a time, but many may also drop support.\n\nPlease use the GGUF models instead.\n### About GGML\n\nGGML files are for CPU + GPU inference using [llama.cpp](https://github.com/ggerganov/llama.cpp) and libraries and UIs which support this format, such as:\n* [text-generation-webui](https://github.com/oobabooga/text-generation-webui), the most popular web UI. Supports NVidia CUDA GPU acceleration.\n* [KoboldCpp](https://github.com/LostRuins/koboldcpp), a powerful GGML web UI with GPU acceleration on all platforms (CUDA and OpenCL). Especially good for story telling.\n* [LM Studio](https://lmstudio.ai/), a fully featured local GUI with GPU acceleration on both Windows (NVidia and AMD), and macOS.\n* [LoLLMS Web UI](https://github.com/ParisNeo/lollms-webui), a great web UI with CUDA GPU acceleration via the c_transformers backend.\n* [ctransformers](https://github.com/marella/ctransformers), a Python library with GPU accel, LangChain support, and OpenAI-compatible AI server.\n* [llama-cpp-python](https://github.com/abetlen/llama-cpp-python), a Python library with GPU accel, LangChain support, and OpenAI-compatible API server.\n\n## Repositories available\n\n* [GPTQ models for GPU inference, with multiple quantisation parameter options.](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ)\n* [2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF)\n* [2, 3, 4, 5, 6 and 8-bit GGML models for CPU+GPU inference (deprecated)](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGML)\n* [Meta Llama 2''s original unquantised fp16 model in pytorch format, for GPU inference and for further conversions](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)\n\n## Prompt template: Llama-2-Chat\n\n```\n[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don''t know the answer to a question, please don''t share false information.\n<</SYS>>\n{prompt}[/INST]\n\n```\n\n<!-- compatibility_ggml start -->\n## Compatibility\n\nThese quantised GGML files are compatible with llama.cpp between June 6th (commit `2d43387`) and August 21st 2023.\n\nFor support with latest llama.cpp, please use GGUF files instead.\n\nThe final llama.cpp commit with support for GGML was: [dadbed99e65252d79f81101a392d0d6497b86caa](https://github.com/ggerganov/llama.cpp/commit/dadbed99e65252d79f81101a392d0d6497b86caa)\n\nAs of August 23rd 2023 they are still compatible with all UIs, libraries and utilities which use GGML. This may change in the future.\n\n## Explanation of the new k-quant methods\n<details>\n  <summary>Click to see details</summary>\n\nThe new methods available are:\n* GGML_TYPE_Q2_K - "type-1" 2-bit quantization in super-blocks containing 16 blocks, each block having 16 weight. Block scales and mins are quantized with 4 bits. This ends up effectively using 2.5625 bits per weight (bpw)\n* GGML_TYPE_Q3_K - "type-0" 3-bit quantization in super-blocks containing 16 blocks, each block having 16 weights. Scales are quantized with 6 bits. This end up using 3.4375 bpw.\n* GGML_TYPE_Q4_K - "type-1" 4-bit quantization in super-blocks containing 8 blocks, each block having 32 weights. Scales and mins are quantized with 6 bits. This ends up using 4.5 bpw.\n* GGML_TYPE_Q5_K - "type-1" 5-bit quantization. Same super-block structure as GGML_TYPE_Q4_K resulting in 5.5 bpw\n* GGML_TYPE_Q6_K - "type-0" 6-bit quantization. Super-blocks with 16 blocks, each block having 16 weights. Scales are quantized with 8 bits. This ends up using 6.5625 bpw\n* GGML_TYPE_Q8_K - "type-0" 8-bit quantization. Only used for quantizing intermediate results. The difference to the existing Q8_0 is that the block size is 256. All 2-6 bit dot products are implemented for this quantization type.\n\nRefer to the Provided Files table below to see what files use which methods, and how.\n</details>\n<!-- compatibility_ggml end -->\n\n## Provided files\n\n| Name | Quant method | Bits | Size | Max RAM required | Use case |\n| ---- | ---- | ---- | ---- | ---- | ----- |\n| llama-2-7b-chat.ggmlv3.q2_K.bin | q2_K | 2 | 2.87 GB| 5.37 GB | New k-quant method. Uses GGML_TYPE_Q4_K for the attention.vw and feed_forward.w2 tensors, GGML_TYPE_Q2_K for the other tensors. |\n| llama-2-7b-chat.ggmlv3.q3_K_S.bin | q3_K_S | 3 | 2.95 GB| 5.45 GB | New k-quant method. Uses GGML_TYPE_Q3_K for all tensors |\n| llama-2-7b-chat.ggmlv3.q3_K_M.bin | q3_K_M | 3 | 3.28 GB| 5.78 GB | New k-quant method. Uses GGML_TYPE_Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else GGML_TYPE_Q3_K |\n| llama-2-7b-chat.ggmlv3.q3_K_L.bin | q3_K_L | 3 | 3.60 GB| 6.10 GB | New k-quant method. Uses GGML_TYPE_Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else GGML_TYPE_Q3_K |\n| llama-2-7b-chat.ggmlv3.q4_0.bin | q4_0 | 4 | 3.79 GB| 6.29 GB | Original quant method, 4-bit. |\n| llama-2-7b-chat.ggmlv3.q4_K_S.bin | q4_K_S | 4 | 3.83 GB| 6.33 GB | New k-quant method. Uses GGML_TYPE_Q4_K for all tensors |\n| llama-2-7b-chat.ggmlv3.q4_K_M.bin | q4_K_M | 4 | 4.08 GB| 6.58 GB | New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q4_K |\n| llama-2-7b-chat.ggmlv3.q4_1.bin | q4_1 | 4 | 4.21 GB| 6.71 GB | Original quant method, 4-bit. Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models. |\n| llama-2-7b-chat.ggmlv3.q5_0.bin | q5_0 | 5 | 4.63 GB| 7.13 GB | Original quant method, 5-bit. Higher accuracy, higher resource usage and slower inference. |\n| llama-2-7b-chat.ggmlv3.q5_K_S.bin | q5_K_S | 5 | 4.65 GB| 7.15 GB | New k-quant method. Uses GGML_TYPE_Q5_K for all tensors |\n| llama-2-7b-chat.ggmlv3.q5_K_M.bin | q5_K_M | 5 | 4.78 GB| 7.28 GB | New k-quant method. Uses GGML_TYPE_Q6_K for half of the attention.wv and feed_forward.w2 tensors, else GGML_TYPE_Q5_K |\n| llama-2-7b-chat.ggmlv3.q5_1.bin | q5_1 | 5 | 5.06 GB| 7.56 GB | Original quant method, 5-bit. Even higher accuracy, resource usage and slower inference. |\n| llama-2-7b-chat.ggmlv3.q6_K.bin | q6_K | 6 | 5.53 GB| 8.03 GB | New k-quant method. Uses GGML_TYPE_Q8_K for all tensors - 6-bit quantization |\n| llama-2-7b-chat.ggmlv3.q8_0.bin | q8_0 | 8 | 7.16 GB| 9.66 GB | Original quant method, 8-bit. Almost indistinguishable from float16. High resource use and slow. Not recommended for most users. |\n\n**Note**: the above RAM figures assume no GPU offloading. If layers are offloaded to the GPU, this will reduce RAM usage and use VRAM instead.\n\n## How to run in `llama.cpp`\n\nMake sure you are using `llama.cpp` from commit [dadbed99e65252d79f81101a392d0d6497b86caa](https://github.com/ggerganov/llama.cpp/commit/dadbed99e65252d79f81101a392d0d6497b86caa) or earlier.\n\nFor compatibility with latest llama.cpp, please use GGUF files instead.\n\n```\n./main -t 10 -ngl 32 -m llama-2-7b-chat.ggmlv3.q4_K_M.bin --color -c 2048 --temp 0.7 --repeat_penalty 1.1 -n -1 -p "[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don''t know the answer to a question, please don''t share false information.\n<</SYS>>\nWrite a story about llamas[/INST]"\n```\nChange `-t 10` to the number of physical CPU cores you have. For example if your system has 8 cores/16 threads, use `-t 8`.\n\nChange `-ngl 32` to the number of layers to offload to GPU. Remove it if you don''t have GPU acceleration.\n\nChange `-c 2048` to the desired sequence length for this model. For example, `-c 4096` for a Llama 2 model.  For models that use RoPE, add `--rope-freq-base 10000 --rope-freq-scale 0.5` for doubled context, or `--rope-freq-base 10000 --rope-freq-scale 0.25` for 4x context.\n\nIf you want to have a chat-style conversation, replace the `-p <PROMPT>` argument with `-i -ins`\n\nFor other parameters and how to use them, please refer to [the llama.cpp documentation](https://github.com/ggerganov/llama.cpp/blob/master/examples/main/README.md)\n\n## How to run in `text-generation-webui`\n\nFurther instructions here: [text-generation-webui/docs/llama.cpp.md](https://github.com/oobabooga/text-generation-webui/blob/main/docs/llama.cpp.md).\n\n<!-- footer start -->\n<!-- 200823 -->\n## Discord\n\nFor further support, and discussions on these models and AI in general, join us at:\n\n[TheBloke AI''s Discord server](https://discord.gg/theblokeai)\n\n## Thanks, and how to contribute.\n\nThanks to the [chirper.ai](https://chirper.ai) team!\n\nI''ve had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training.\n\nIf you''re able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects.\n\nDonaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits.\n\n* Patreon: https://patreon.com/TheBlokeAI\n* Ko-Fi: https://ko-fi.com/TheBlokeAI\n\n**Special thanks to**: Aemon Algiz.\n\n**Patreon special mentions**: Russ Johnson, J, alfie_i, Alex, NimbleBox.ai, Chadd, Mandus, Nikolai Manek, Ken Nordquist, ya boyyy, Illia Dulskyi, Viktor Bowallius, vamX, Iucharbius, zynix, Magnesian, Clay Pascal, Pierre Kircher, Enrico Ros, Tony Hughes, Elle, Andrey, knownsqashed, Deep Realms, Jerry Meng, Lone Striker, Derek Yates, Pyrater, Mesiah Bishop, James Bentley, Femi Adebogun, Brandon Frisco, SuperWojo, Alps Aficionado, Michael Dempsey, Vitor Caleffi, Will Dee, Edmond Seymore, usrbinkat, LangChain4j, Kacper Wikie≈Ç, Luke Pendergrass, John Detwiler, theTransient, Nathan LeClaire, Tiffany J. Kim, biorpg, Eugene Pentland, Stanislav Ovsiannikov, Fred von Graf, terasurfer, Kalila, Dan Guido, Nitin Borwankar, ÈòøÊòé, Ai Maven, John Villwock, Gabriel Puliatti, Stephen Murray, Asp the Wyvern, danny, Chris Smitley, ReadyPlayerEmma, S_X, Daniel P. Andersen, Olakabola, Jeffrey Morgan, Imad Khwaja, Caitlyn Gatomon, webtim, Alicia Loh, Trenton Dambrowitz, Swaroop Kallakuri, Erik Bj√§reholt, Leonard Tan, Spiking Neurons AB, Luke @flexchar, Ajan Kanaga, Thomas Belote, Deo Leter, RoA, Willem Michiel, transmissions 11, subjectnull, Matthew Berman, Joseph William Delisle, David Ziegler, Michael Davis, Johann-Peter Hartmann, Talal Aujan, senxiiz, Artur Olbinski, Rainer Wilmers, Spencer Kim, Fen Risland, Cap''n Zoog, Rishabh Srivastava, Michael Levine, Geoffrey Montalvo, Sean Connelly, Alexandros Triantafyllidis, Pieter, Gabriel Tamborski, Sam, Subspace Studios, Junyu Yang, Pedro Madruga, Vadim, Cory Kujawski, K, Raven Klaugh, Randy H, Mano Prime, Sebastain Graf, Space Cruiser\n\n\nThank you to all my generous patrons and donaters!\n\nAnd thank you again to a16z for their generous grant.\n\n<!-- footer end -->\n\n# Original model card: Meta Llama 2''s Llama 2 7B Chat\n\n# **Llama 2**\nLlama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 7B fine-tuned model, optimized for dialogue use cases and converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom.\n\n## Model Details\n*Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the [website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License before requesting access here.*\n\nMeta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM.\n\n**Model Developers** Meta\n\n**Variations** Llama 2 comes in a range of parameter sizes ‚Äî 7B, 13B, and 70B ‚Äî as well as pretrained and fine-tuned variations.\n\n**Input** Models input text only.\n\n**Output** Models generate text only.\n\n**Model Architecture** Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety.\n\n\n||Training Data|Params|Content Length|GQA|Tokens|LR|\n|---|---|---|---|---|---|---|\n|Llama 2|*A new mix of publicly available online data*|7B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|\n|Llama 2|*A new mix of publicly available online data*|13B|4k|&#10007;|2.0T|3.0 x 10<sup>-4</sup>|\n|Llama 2|*A new mix of publicly available online data*|70B|4k|&#10004;|2.0T|1.5 x 10<sup>-4</sup>|\n\n*Llama 2 family of models.* Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models -  70B -- use Grouped-Query Attention (GQA) for improved inference scalability.\n\n**Model Dates** Llama 2 was trained between January 2023 and July 2023.\n\n**Status** This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback.\n\n**License** A custom commercial license is available at: [https://ai.meta.com/resources/models-and-libraries/llama-downloads/](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)\n\n**Research Paper** ["Llama-2: Open Foundation and Fine-tuned Chat Models"](arxiv.org/abs/2307.09288)\n\n## Intended Use\n**Intended Use Cases** Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks.\n\nTo get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and breaklines in between (we recommend calling `strip()` on inputs to avoid double-spaces). See our reference code in github for details: [`chat_completion`](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212).\n\n**Out-of-scope Uses** Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2.\n\n## Hardware and Software\n**Training Factors** We used custom training libraries, Meta''s Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute.\n\n**Carbon Footprint** Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta‚Äôs sustainability program.\n\n||Time (GPU hours)|Power Consumption (W)|Carbon Emitted(tCO<sub>2</sub>eq)|\n|---|---|---|---|\n|Llama 2 7B|184320|400|31.22|\n|Llama 2 13B|368640|400|62.44|\n|Llama 2 70B|1720320|400|291.42|\n|Total|3311616||539.00|\n\n**CO<sub>2</sub> emissions during pretraining.** Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta''s sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others.\n\n## Training Data\n**Overview** Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data.\n\n**Data Freshness** The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023.\n\n## Evaluation Results\n\nIn this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library.\n\n|Model|Size|Code|Commonsense Reasoning|World Knowledge|Reading Comprehension|Math|MMLU|BBH|AGI Eval|\n|---|---|---|---|---|---|---|---|---|---|\n|Llama 1|7B|14.1|60.8|46.2|58.5|6.95|35.1|30.3|23.9|\n|Llama 1|13B|18.9|66.1|52.6|62.3|10.9|46.9|37.0|33.9|\n|Llama 1|33B|26.0|70.0|58.4|67.6|21.4|57.8|39.8|41.7|\n|Llama 1|65B|30.7|70.7|60.5|68.6|30.8|63.4|43.5|47.6|\n|Llama 2|7B|16.8|63.9|48.9|61.3|14.6|45.3|32.6|29.3|\n|Llama 2|13B|24.5|66.9|55.4|65.8|28.7|54.8|39.4|39.1|\n|Llama 2|70B|**37.5**|**71.9**|**63.6**|**69.4**|**35.2**|**68.9**|**51.2**|**54.2**|\n\n**Overall performance on grouped academic benchmarks.** *Code:* We report the average pass@1 scores of our models on HumanEval and MBPP. *Commonsense Reasoning:* We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. *World Knowledge:* We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. *Reading Comprehension:* For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. *MATH:* We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1.\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama 1|7B|27.42|23.00|\n|Llama 1|13B|41.74|23.08|\n|Llama 1|33B|44.19|22.57|\n|Llama 1|65B|48.71|21.77|\n|Llama 2|7B|33.29|**21.25**|\n|Llama 2|13B|41.86|26.10|\n|Llama 2|70B|**50.18**|24.60|\n\n**Evaluation of pretrained LLMs on automatic safety benchmarks.** For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better).\n\n\n|||TruthfulQA|Toxigen|\n|---|---|---|---|\n|Llama-2-Chat|7B|57.04|**0.00**|\n|Llama-2-Chat|13B|62.18|**0.00**|\n|Llama-2-Chat|70B|**64.14**|0.01|\n\n**Evaluation of fine-tuned LLMs on different safety datasets.** Same metric definitions as above.\n\n## Ethical Considerations and Limitations\nLlama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2‚Äôs potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model.\n\nPlease see the Responsible Use Guide available at [https://ai.meta.com/llama/responsible-use-guide/](https://ai.meta.com/llama/responsible-use-guide)\n\n## Reporting Issues\nPlease report any software ‚Äúbug,‚Äù or other problems with the models through one of the following means:\n- Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\n- Reporting problematic content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n## Llama Model Index\n|Model|Llama2|Llama2-hf|Llama2-chat|Llama2-chat-hf|\n|---|---|---|---|---|\n|7B| [Link](https://huggingface.co/llamaste/Llama-2-7b) | [Link](https://huggingface.co/llamaste/Llama-2-7b-hf) | [Link](https://huggingface.co/llamaste/Llama-2-7b-chat) | [Link](https://huggingface.co/llamaste/Llama-2-7b-chat-hf)|\n|13B| [Link](https://huggingface.co/llamaste/Llama-2-13b) | [Link](https://huggingface.co/llamaste/Llama-2-13b-hf) | [Link](https://huggingface.co/llamaste/Llama-2-13b-chat) | [Link](https://huggingface.co/llamaste/Llama-2-13b-hf)|\n|70B| [Link](https://huggingface.co/llamaste/Llama-2-70b) | [Link](https://huggingface.co/llamaste/Llama-2-70b-hf) | [Link](https://huggingface.co/llamaste/Llama-2-70b-chat) | [Link](https://huggingface.co/llamaste/Llama-2-70b-hf)|\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":60421177985,"files_count":20,"spaces_count":100,"gated":false,"private":false,"config":{"model_type":"llama"}}', '[]', '[{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:oobabooga:text-generation-webui","source_url":"https://github.com/oobabooga/text-generation-webui"},{"type":"has_code","target_id":"github:LostRuins:koboldcpp","source_url":"https://github.com/LostRuins/koboldcpp"},{"type":"has_code","target_id":"github:ParisNeo:lollms-webui","source_url":"https://github.com/ParisNeo/lollms-webui"},{"type":"has_code","target_id":"github:marella:ctransformers","source_url":"https://github.com/marella/ctransformers"},{"type":"has_code","target_id":"github:abetlen:llama-cpp-python","source_url":"https://github.com/abetlen/llama-cpp-python"},{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:ggerganov:llama.cpp","source_url":"https://github.com/ggerganov/llama.cpp"},{"type":"has_code","target_id":"github:oobabooga:text-generation-webui","source_url":"https://github.com/oobabooga/text-generation-webui"},{"type":"has_code","target_id":"github:facebookresearch:llama","source_url":"https://github.com/facebookresearch/llama"},{"type":"has_code","target_id":"github:facebookresearch:llama","source_url":"http://github.com/facebookresearch/llama"},{"type":"based_on_paper","target_id":"arxiv:2307.09288","source_url":"https://arxiv.org/abs/2307.09288"}]', NULL, 'Other', 'approved', 79.4, '97b28da2847e75ffa2c05558287b49bd', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Meta-Llama-3-70B', 'huggingface--meta-llama--meta-llama-3-70b', 'Meta-Llama-3-70B', 'meta-llama', '', '["transformers","safetensors","llama","text-generation","facebook","meta","pytorch","llama-3","en","license:llama3","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 870, 96591, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Meta-Llama-3-70B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":70553706496,"storage_bytes":423345124549,"files_count":50,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","eos_token":"<|end_of_text|>"}}}', '[]', '[]', NULL, 'LLaMA-3', 'approved', 39.4, 'c4d633d8ab0ec6adde6c7bcec8946392', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-stable-diffusion-3.5-medium', 'huggingface--stabilityai--stable-diffusion-3.5-medium', 'stable-diffusion-3.5-medium', 'stabilityai', '', '["diffusers","safetensors","text-to-image","stable-diffusion","en","arxiv:2403.03206","license:other","diffusers:stablediffusion3pipeline","region:us"]', 'text-to-image', 867, 121686, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/stable-diffusion-3.5-medium","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":36305578304,"files_count":45,"spaces_count":100,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"StableDiffusion3Pipeline"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2403.03206","source_url":"https://arxiv.org/abs/2403.03206"}]', NULL, 'Other', 'approved', 59.4, '3bffe00a98e443b1f0ea57cfc139364e', NULL, 'https://huggingface.co/stabilityai/stable-diffusion-3.5-medium/resolve/main/sd3.5_medium_demo.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-stabilityai-stable-diffusion-3.5-medium from https://huggingface.co/stabilityai/stable-diffusion-3.5-medium/resolve/main/sd3.5_medium_demo.jpg
HTTP error: 401 Unauthorized

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Wan-AI-Wan2.2-Animate-14B', 'huggingface--wan-ai--wan2.2-animate-14b', 'Wan2.2-Animate-14B', 'Wan-AI', '--- license: apache-2.0 base_model: - Wan-AI/Wan2.2-I2V-A14B pipeline_tag: video-to-video --- <p align="center"> <img src="assets/logo.png" width="400"/> <p> <p align="center"> üíú <a href="https://wan.video"><b>Wan</b></a> &nbsp&nbsp ÔΩú &nbsp&nbsp üñ•Ô∏è <a href="https://github.com/Wan-Video/Wan2.2">GitHub</a> &nbsp&nbsp | &nbsp&nbspü§ó <a href="https://huggingface.co/Wan-AI/">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href="https://modelscope.cn/organization/Wan-AI">ModelScope</a>&nbsp&nbsp | &...', '["diffusers","onnx","safetensors","video-to-video","arxiv:2503.20314","base_model:wan-ai/wan2.2-i2v-a14b","base_model:quantized:wan-ai/wan2.2-i2v-a14b","license:apache-2.0","region:us"]', 'video-to-video', 867, 33791, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Wan-AI/Wan2.2-Animate-14B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nbase_model:\n- Wan-AI/Wan2.2-I2V-A14B\npipeline_tag: video-to-video\n---\n# Wan2.2\n\n<p align="center">\n    <img src="assets/logo.png" width="400"/>\n<p>\n\n<p align="center">\n    üíú <a href="https://wan.video"><b>Wan</b></a> &nbsp&nbsp ÔΩú &nbsp&nbsp üñ•Ô∏è <a href="https://github.com/Wan-Video/Wan2.2">GitHub</a> &nbsp&nbsp  | &nbsp&nbspü§ó <a href="https://huggingface.co/Wan-AI/">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href="https://modelscope.cn/organization/Wan-AI">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp üìë <a href="https://arxiv.org/abs/2503.20314">Paper</a> &nbsp&nbsp | &nbsp&nbsp üìë <a href="https://wan.video/welcome?spm=a2ty_o02.30011076.0.0.6c9ee41eCcluqg">Blog</a> &nbsp&nbsp |  &nbsp&nbsp üí¨  <a href="https://discord.gg/AKNgpMK4Yj">Discord</a>&nbsp&nbsp\n    <br>\n    üìï <a href="https://alidocs.dingtalk.com/i/nodes/jb9Y4gmKWrx9eo4dCql9LlbYJGXn6lpz">‰ΩøÁî®ÊåáÂçó(‰∏≠Êñá)</a>&nbsp&nbsp | &nbsp&nbsp üìò <a href="https://alidocs.dingtalk.com/i/nodes/EpGBa2Lm8aZxe5myC99MelA2WgN7R35y">User Guide(English)</a>&nbsp&nbsp | &nbsp&nbspüí¨ <a href="https://gw.alicdn.com/imgextra/i2/O1CN01tqjWFi1ByuyehkTSB_!!6000000000015-0-tps-611-1279.jpg">WeChat(ÂæÆ‰ø°)</a>&nbsp&nbsp\n<br>\n\n-----\n\n[**Wan: Open and Advanced Large-Scale Video Generative Models**](https://arxiv.org/abs/2503.20314) <be>\n\n\nWe are excited to introduce **Wan2.2**, a major upgrade to our foundational video models. With **Wan2.2**, we have focused on incorporating the following innovations:\n\n- üëç **Effective MoE Architecture**: Wan2.2 introduces a Mixture-of-Experts (MoE) architecture into video diffusion models. By separating the denoising process cross timesteps with specialized powerful expert models, this enlarges the overall model capacity while maintaining the same computational cost.\n\n- üëç **Cinematic-level Aesthetics**: Wan2.2 incorporates meticulously curated aesthetic data, complete with detailed labels for lighting, composition, contrast, color tone, and more. This allows for more precise and controllable cinematic style generation, facilitating the creation of videos with customizable aesthetic preferences.\n\n- üëç **Complex Motion Generation**: Compared to Wan2.1, Wan2.2 is trained on a significantly larger data, with +65.6% more images and +83.2% more videos. This expansion notably enhances the model''s generalization across multiple dimensions such as motions,  semantics, and aesthetics, achieving TOP performance among all open-sourced and closed-sourced models. \n\n- üëç **Efficient High-Definition Hybrid TI2V**:  Wan2.2 open-sources a 5B model built with our advanced Wan2.2-VAE that achieves a compression ratio of **16√ó16√ó4**. This model supports both text-to-video and image-to-video generation at 720P resolution with 24fps and can also run on consumer-grade graphics cards like 4090. It is one of the fastest **720P@24fps** models currently available, capable of serving both the industrial and academic sectors simultaneously.\n\n\n## Video Demos\n\n<div align="center">\n    <video width="80%" controls>\n        <source src="https://cloud.video.taobao.com/vod/4szTT1B0LqXvJzmuEURfGRA-nllnqN_G2AT0ZWkQXoQ.mp4" type="video/mp4">\n        Your browser does not support the video tag.\n    </video>\n</div>\n\n## üî• Latest News!!\n\n* Sep 19, 2025: üíÉ We introduct **[Wan2.2-Animate-14B](https://humanaigc.github.io/wan-animate)**, an unified model for character animation and replacement with holistic movement and expression replication. We released the [model weights](#model-download) and [inference code](#run-with-wan-animate). And now you can try it on [wan.video](https://wan.video/), [ModelScope Studio](https://www.modelscope.cn/studios/Wan-AI/Wan2.2-Animate) or [HuggingFace Space](https://huggingface.co/spaces/Wan-AI/Wan2.2-Animate)!\n* Aug 26, 2025: üéµ We introduce **[Wan2.2-S2V-14B](https://humanaigc.github.io/wan-s2v-webpage)**, an audio-driven cinematic video generation model, including [inference code](#run-speech-to-video-generation), [model weights](#model-download), and [technical report](https://humanaigc.github.io/wan-s2v-webpage/content/wan-s2v.pdf)! Now you can try it on [wan.video](https://wan.video/),  [ModelScope Gradio](https://www.modelscope.cn/studios/Wan-AI/Wan2.2-S2V) or [HuggingFace Gradio](https://huggingface.co/spaces/Wan-AI/Wan2.2-S2V)!\n* Jul 28, 2025: üëã We have open a [HF space](https://huggingface.co/spaces/Wan-AI/Wan-2.2-5B) using the TI2V-5B model. Enjoy!\n* Jul 28, 2025: üëã Wan2.2 has been integrated into ComfyUI ([CN](https://docs.comfy.org/zh-CN/tutorials/video/wan/wan2_2) | [EN](https://docs.comfy.org/tutorials/video/wan/wan2_2)). Enjoy!\n* Jul 28, 2025: üëã Wan2.2''s T2V, I2V and TI2V have been integrated into Diffusers ([T2V-A14B](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B-Diffusers) | [I2V-A14B](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B-Diffusers) | [TI2V-5B](https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B-Diffusers)). Feel free to give it a try!\n* Jul 28, 2025: üëã We''ve released the inference code and model weights of **Wan2.2**.\n* Sep 5, 2025: üëã We add text-to-speech synthesis support with [CosyVoice](https://github.com/FunAudioLLM/CosyVoice) for Speech-to-Video generation task.\n\n\n## Community Works\nIf your research or project builds upon [**Wan2.1**](https://github.com/Wan-Video/Wan2.1) or [**Wan2.2**](https://github.com/Wan-Video/Wan2.2), and you would like more people to see it, please inform us.\n\n- [DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio) provides comprehensive support for Wan 2.2, including low-GPU-memory layer-by-layer offload, FP8 quantization, sequence parallelism, LoRA training, full training.\n- [Kijai''s ComfyUI WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper) is an alternative implementation of Wan models for ComfyUI. Thanks to its Wan-only focus, it''s on the frontline of getting cutting edge optimizations and hot research features, which are often hard to integrate into ComfyUI quickly due to its more rigid structure.\n- [Cache-dit](https://github.com/vipshop/cache-dit) offers Fully Cache Acceleration support for Wan2.2 MoE with DBCache, TaylorSeer and Cache CFG. Visit their [example](https://github.com/vipshop/cache-dit/blob/main/examples/pipeline/run_wan_2.2.py) for more details.\n- [FastVideo](https://github.com/hao-ai-lab/FastVideo) includes distilled Wan models with sparse attention that significanly speed up the inference time. \n\n## üìë Todo List\n- Wan2.2 Text-to-Video\n    - [x] Multi-GPU Inference code of the A14B and 14B models\n    - [x] Checkpoints of the A14B and 14B models\n    - [x] ComfyUI integration\n    - [x] Diffusers integration\n- Wan2.2 Image-to-Video\n    - [x] Multi-GPU Inference code of the A14B model\n    - [x] Checkpoints of the A14B model\n    - [x] ComfyUI integration\n    - [x] Diffusers integration\n- Wan2.2 Text-Image-to-Video\n    - [x] Multi-GPU Inference code of the 5B model\n    - [x] Checkpoints of the 5B model\n    - [x] ComfyUI integration\n    - [x] Diffusers integration\n- Wan2.2-S2V Speech-to-Video\n    - [x] Inference code of Wan2.2-S2V\n    - [x] Checkpoints of Wan2.2-S2V-14B\n    - [x] ComfyUI integration\n    - [x] Diffusers integration\n- Wan2.2-Animate Character Animation and Replacement\n    - [x] Inference code of Wan2.2-Animate\n    - [x] Checkpoints of Wan2.2-Animate\n    - [x] ComfyUI integration\n    - [ ] Diffusers integration    \n\n## Run Wan2.2 Animate\n\n#### Installation\nClone the repo:\n```sh\ngit clone https://github.com/Wan-Video/Wan2.2.git\ncd Wan2.2\n```\n\nInstall dependencies:\n```sh\n# Ensure torch >= 2.4.0\n# If the installation of `flash_attn` fails, try installing the other packages first and install `flash_attn` last\npip install -r requirements.txt\n# If you want to use CosyVoice to synthesize speech for Speech-to-Video Generation, please install requirements_s2v.txt additionally\npip install -r requirements_s2v.txt\n```\n\n\n#### Model Download\n\n| Models              | Download Links                                                                                                                              | Description |\n|--------------------|---------------------------------------------------------------------------------------------------------------------------------------------|-------------|\n| T2V-A14B    | ü§ó [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B)    ü§ñ [ModelScope](https://modelscope.cn/models/Wan-AI/Wan2.2-T2V-A14B)    | Text-to-Video MoE model, supports 480P & 720P |\n| I2V-A14B    | ü§ó [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B)    ü§ñ [ModelScope](https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B)    | Image-to-Video MoE model, supports 480P & 720P |\n| TI2V-5B     | ü§ó [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B)     ü§ñ [ModelScope](https://modelscope.cn/models/Wan-AI/Wan2.2-TI2V-5B)     | High-compression VAE, T2V+I2V, supports 720P |\n| S2V-14B     | ü§ó [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-S2V-14B)     ü§ñ [ModelScope](https://modelscope.cn/models/Wan-AI/Wan2.2-S2V-14B)     | Speech-to-Video model, supports 480P & 720P |\n| Animate-14B | ü§ó [Huggingface](https://huggingface.co/Wan-AI/Wan2.2-Animate-14B) ü§ñ [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.2-Animate-14B)  | Character animation and replacement | |\n\n\nDownload models using huggingface-cli:\n``` sh\npip install "huggingface_hub[cli]"\nhuggingface-cli download Wan-AI/Wan2.2-Animate-14B --local-dir ./Wan2.2-Animate-14B\n```\n\nDownload models using modelscope-cli:\n``` sh\npip install modelscope\nmodelscope download Wan-AI/Wan2.2-Animate-14B --local_dir ./Wan2.2-Animate-14B\n```\n\n#### Run Wan-Animate-14B\n\nWan-Animate takes a video and a character image as input, and generates a video in either "animation" or "replacement" mode. \n\n1. animation modeÔºö The model generates a video of the character image that mimics the human motion in the input video.\n2. replacement mode: The model replaces the character image with the input video.\n\nPlease visit our [project page](https://humanaigc.github.io/wan-animate) to see more examples and learn about the scenarios suitable for this model.\n\n##### (1) Preprocessing \nThe input video should be preprocessed into several materials before be feed into the inference process.  Please refer to the following processing flow, and more details about preprocessing can be found in [UserGuider](https://github.com/Wan-Video/Wan2.2/blob/main/wan/modules/animate/preprocess/UserGuider.md).\n\n* For animation\n```bash\npython ./wan/modules/animate/preprocess/preprocess_data.py \\n    --ckpt_path ./Wan2.2-Animate-14B/process_checkpoint \\n    --video_path ./examples/wan_animate/animate/video.mp4 \\n    --refer_path ./examples/wan_animate/animate/image.jpeg \\n    --save_path ./examples/wan_animate/animate/process_results \\n    --resolution_area 1280 720 \\n    --retarget_flag \\n    --use_flux\n```\n* For replacement\n```bash\npython ./wan/modules/animate/preprocess/preprocess_data.py \\n    --ckpt_path ./Wan2.2-Animate-14B/process_checkpoint \\n    --video_path ./examples/wan_animate/replace/video.mp4 \\n    --refer_path ./examples/wan_animate/replace/image.jpeg \\n    --save_path ./examples/wan_animate/replace/process_results \\n    --resolution_area 1280 720 \\n    --iterations 3 \\n    --k 7 \\n    --w_len 1 \\n    --h_len 1 \\n    --replace_flag\n```\n##### (2) Run in animation mode \n\n* Single-GPU inference \n\n```bash\npython generate.py --task animate-14B --ckpt_dir ./Wan2.2-Animate-14B/ --src_root_path ./examples/wan_animate/animate/process_results/ --refert_num 1\n```\n\n* Multi-GPU inference using FSDP + DeepSpeed Ulysses\n\n```bash\npython -m torch.distributed.run --nnodes 1 --nproc_per_node 8 generate.py --task animate-14B --ckpt_dir ./Wan2.2-Animate-14B/ --src_root_path ./examples/wan_animate/animate/process_results/ --refert_num 1 --dit_fsdp --t5_fsdp --ulysses_size 8\n```\n\n##### (3) Run in replacement mode \n\n* Single-GPU inference \n\n```bash\npython generate.py --task animate-14B --ckpt_dir ./Wan2.2-Animate-14B/ --src_root_path ./examples/wan_animate/replace/process_results/ --refert_num 1 --replace_flag --use_relighting_lora \n```\n\n* Multi-GPU inference using FSDP + DeepSpeed Ulysses\n\n```bash\npython -m torch.distributed.run --nnodes 1 --nproc_per_node 8 generate.py --task animate-14B --ckpt_dir ./Wan2.2-Animate-14B/ --src_root_path ./examples/wan_animate/replace/process_results/src_pose.mp4  --refert_num 1 --replace_flag --use_relighting_lora --dit_fsdp --t5_fsdp --ulysses_size 8\n```\n\n> üí° If you''re using **Wan-Animate**, we do not recommend using LoRA models trained on `Wan2.2`, since weight changes during training may lead to unexpected behavior.\n\n## Computational Efficiency on Different GPUs\n\nWe test the computational efficiency of different **Wan2.2** models on different GPUs in the following table. The results are presented in the format: **Total time (s) / peak GPU memory (GB)**.\n\n\n<div align="center">\n    <img src="assets/comp_effic.png" alt="" style="width: 80%;" />\n</div>\n\n> The parameter settings for the tests presented in this table are as follows:\n> (1) Multi-GPU: 14B: `--ulysses_size 4/8 --dit_fsdp --t5_fsdp`, 5B: `--ulysses_size 4/8 --offload_model True --convert_model_dtype --t5_cpu`; Single-GPU: 14B: `--offload_model True --convert_model_dtype`, 5B: `--offload_model True --convert_model_dtype --t5_cpu`\n(--convert_model_dtype converts model parameter types to config.param_dtype);\n> (2) The distributed testing utilizes the built-in FSDP and Ulysses implementations, with FlashAttention3 deployed on Hopper architecture GPUs;\n> (3) Tests were run without the `--use_prompt_extend` flag;\n> (4) Reported results are the average of multiple samples taken after the warm-up phase.\n\n\n-------\n\n## Introduction of Wan2.2\n\n**Wan2.2** builds on the foundation of Wan2.1 with notable improvements in generation quality and model capability. This upgrade is driven by a series of key technical innovations, mainly including the Mixture-of-Experts (MoE) architecture, upgraded training data, and high-compression video generation.\n\n##### (1) Mixture-of-Experts (MoE) Architecture\n\nWan2.2 introduces Mixture-of-Experts (MoE) architecture into the video generation diffusion model. MoE has been widely validated in large language models as an efficient approach to increase total model parameters while keeping inference cost nearly unchanged. In Wan2.2, the A14B model series adopts a two-expert design tailored to the denoising process of diffusion models: a high-noise expert for the early stages, focusing on overall layout; and a low-noise expert for the later stages, refining video details. Each expert model has about 14B parameters, resulting in a total of 27B parameters but only 14B active parameters per step, keeping inference computation and GPU memory nearly unchanged.\n\n<div align="center">\n    <img src="assets/moe_arch.png" alt="" style="width: 90%;" />\n</div>\n\nThe transition point between the two experts is determined by the signal-to-noise ratio (SNR), a metric that decreases monotonically as the denoising step $t$ increases. At the beginning of the denoising process, $t$ is large and the noise level is high, so the SNR is at its minimum, denoted as ${SNR}_{min}$. In this stage, the high-noise expert is activated. We define a threshold step ${t}_{moe}$ corresponding to half of the ${SNR}_{min}$, and switch to the low-noise expert when $t<{t}_{moe}$.\n\n<div align="center">\n    <img src="assets/moe_2.png" alt="" style="width: 90%;" />\n</div>\n\nTo validate the effectiveness of the MoE architecture, four settings are compared based on their validation loss curves. The baseline **Wan2.1** model does not employ the MoE architecture. Among the MoE-based variants, the **Wan2.1 & High-Noise Expert** reuses the Wan2.1 model as the low-noise expert while uses the  Wan2.2''s high-noise expert, while the **Wan2.1 & Low-Noise Expert** uses Wan2.1 as the high-noise expert and employ the Wan2.2''s low-noise expert. The **Wan2.2 (MoE)** (our final version) achieves the lowest validation loss, indicating that its generated video distribution is closest to ground-truth and exhibits superior convergence.\n\n\n##### (2) Efficient High-Definition Hybrid TI2V\nTo enable more efficient deployment, Wan2.2 also explores a high-compression design. In addition to the 27B MoE models, a 5B dense model, i.e., TI2V-5B, is released. It is supported by a high-compression Wan2.2-VAE, which achieves a $T\times H\times W$ compression ratio of $4\times16\times16$, increasing the overall compression rate to 64 while maintaining high-quality video reconstruction. With an additional patchification layer, the total compression ratio of TI2V-5B reaches $4\times32\times32$. Without specific optimization, TI2V-5B can generate a 5-second 720P video in under 9 minutes on a single consumer-grade GPU, ranking among the fastest 720P@24fps video generation models. This model also natively supports both text-to-video and image-to-video tasks within a single unified framework, covering both academic research and practical applications.\n\n\n<div align="center">\n    <img src="assets/vae.png" alt="" style="width: 80%;" />\n</div>\n\n\n\n##### Comparisons to SOTAs\nWe compared Wan2.2 with leading closed-source commercial models on our new Wan-Bench 2.0, evaluating performance across multiple crucial dimensions. The results demonstrate that Wan2.2 achieves superior performance compared to these leading models.\n\n\n<div align="center">\n    <img src="assets/performance.png" alt="" style="width: 90%;" />\n</div>\n\n## Citation\nIf you find our work helpful, please cite us.\n\n```\n@article{wan2025,\n      title={Wan: Open and Advanced Large-Scale Video Generative Models}, \n      author={Team Wan and Ang Wang and Baole Ai and Bin Wen and Chaojie Mao and Chen-Wei Xie and Di Chen and Feiwu Yu and Haiming Zhao and Jianxiao Yang and Jianyuan Zeng and Jiayu Wang and Jingfeng Zhang and Jingren Zhou and Jinkai Wang and Jixuan Chen and Kai Zhu and Kang Zhao and Keyu Yan and Lianghua Huang and Mengyang Feng and Ningyi Zhang and Pandeng Li and Pingyu Wu and Ruihang Chu and Ruili Feng and Shiwei Zhang and Siyang Sun and Tao Fang and Tianxing Wang and Tianyi Gui and Tingyu Weng and Tong Shen and Wei Lin and Wei Wang and Wei Wang and Wenmeng Zhou and Wente Wang and Wenting Shen and Wenyuan Yu and Xianzhong Shi and Xiaoming Huang and Xin Xu and Yan Kou and Yangyu Lv and Yifei Li and Yijing Liu and Yiming Wang and Yingya Zhang and Yitong Huang and Yong Li and You Wu and Yu Liu and Yulin Pan and Yun Zheng and Yuntao Hong and Yupeng Shi and Yutong Feng and Zeyinzi Jiang and Zhen Han and Zhi-Fan Wu and Ziyu Liu},\n      journal = {arXiv preprint arXiv:2503.20314},\n      year={2025}\n}\n```\n\n## License Agreement\nThe models in this repository are licensed under the Apache 2.0 License. We claim no rights over the your generated contents, granting you the freedom to use them while ensuring that your usage complies with the provisions of this license. You are fully accountable for your use of the models, which must not involve sharing any content that violates applicable laws, causes harm to individuals or groups, disseminates personal information intended for harm, spreads misinformation, or targets vulnerable populations. For a complete list of restrictions and details regarding your rights, please refer to the full text of the [license](LICENSE.txt).\n\n\n## Acknowledgements\n\nWe would like to thank the contributors to the [SD3](https://huggingface.co/stabilityai/stable-diffusion-3-medium), [Qwen](https://huggingface.co/Qwen), [umt5-xxl](https://huggingface.co/google/umt5-xxl), [diffusers](https://github.com/huggingface/diffusers) and [HuggingFace](https://huggingface.co) repositories, for their open research.\n\n\n\n## Contact Us\nIf you would like to leave a message to our research or product teams, feel free to join our [Discord](https://discord.gg/AKNgpMK4Yj) or [WeChat groups](https://gw.alicdn.com/imgextra/i2/O1CN01tqjWFi1ByuyehkTSB_!!6000000000015-0-tps-611-1279.jpg)!', '{"pipeline_tag":"video-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":72381938315,"files_count":444,"spaces_count":48,"gated":false,"private":false,"config":{}}', '[]', '[{"type":"has_code","target_id":"github:Wan-Video:Wan2.2\">GitHub<","source_url":"https://github.com/Wan-Video/Wan2.2\">GitHub<"},{"type":"has_code","target_id":"github:FunAudioLLM:CosyVoice","source_url":"https://github.com/FunAudioLLM/CosyVoice"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.1","source_url":"https://github.com/Wan-Video/Wan2.1"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.2","source_url":"https://github.com/Wan-Video/Wan2.2"},{"type":"has_code","target_id":"github:modelscope:DiffSynth-Studio","source_url":"https://github.com/modelscope/DiffSynth-Studio"},{"type":"has_code","target_id":"github:kijai:ComfyUI-WanVideoWrapper","source_url":"https://github.com/kijai/ComfyUI-WanVideoWrapper"},{"type":"has_code","target_id":"github:vipshop:cache-dit","source_url":"https://github.com/vipshop/cache-dit"},{"type":"has_code","target_id":"github:vipshop:cache-dit","source_url":"https://github.com/vipshop/cache-dit"},{"type":"has_code","target_id":"github:hao-ai-lab:FastVideo","source_url":"https://github.com/hao-ai-lab/FastVideo"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.2.git","source_url":"https://github.com/Wan-Video/Wan2.2.git"},{"type":"has_code","target_id":"github:Wan-Video:Wan2.2","source_url":"https://github.com/Wan-Video/Wan2.2"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"based_on_paper","target_id":"arxiv:2503.20314","source_url":"https://arxiv.org/abs/2503.20314"}]', NULL, 'Apache-2.0', 'approved', 99.4, '2b7c61f37973cbf6ceb38878a5c4920e', NULL, 'https://huggingface.co/Wan-AI/Wan2.2-Animate-14B/resolve/main/assets/comp_effic.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-Wan-AI-Wan2.2-Animate-14B from https://huggingface.co/Wan-AI/Wan2.2-Animate-14B/resolve/main/assets/comp_effic.png
Image converted to WebP: data/images/huggingface-Wan-AI-Wan2.2-Animate-14B.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-distilbert-distilbert-base-uncased-finetuned-sst-2-english', 'huggingface--distilbert--distilbert-base-uncased-finetuned-sst-2-english', 'distilbert-base-uncased-finetuned-sst-2-english', 'distilbert', '--- language: en license: apache-2.0 datasets: - sst2 - glue model-index: - name: distilbert-base-uncased-finetuned-sst-2-english results: - task: type: text-classification name: Text Classification dataset: name: glue type: glue config: sst2 split: validation metrics: - type: accuracy value: 0.9105504587155964 name: Accuracy verified: true verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2YyOGMxYjY2Y2JhMjkxNjIzN2FmMjNiNmM2ZWViNGY3MTNmNWI2YzhiYjYxZTY0ZGUyN2M1NGIxZjRiMjQwZiIsInZl...', '["transformers","pytorch","tf","rust","onnx","safetensors","distilbert","text-classification","en","dataset:sst2","dataset:glue","arxiv:1910.01108","doi:10.57967/hf/0181","license:apache-2.0","model-index","endpoints_compatible","deploy:azure","region:us"]', 'text-classification', 858, 4715621, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: en\nlicense: apache-2.0\ndatasets:\n- sst2\n- glue\nmodel-index:\n- name: distilbert-base-uncased-finetuned-sst-2-english\n  results:\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: glue\n      type: glue\n      config: sst2\n      split: validation\n    metrics:\n    - type: accuracy\n      value: 0.9105504587155964\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiN2YyOGMxYjY2Y2JhMjkxNjIzN2FmMjNiNmM2ZWViNGY3MTNmNWI2YzhiYjYxZTY0ZGUyN2M1NGIxZjRiMjQwZiIsInZlcnNpb24iOjF9.uui0srxV5ZHRhxbYN6082EZdwpnBgubPJ5R2-Wk8HTWqmxYE3QHidevR9LLAhidqGw6Ih93fK0goAXncld_gBg\n    - type: precision\n      value: 0.8978260869565218\n      name: Precision\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMzgwYTYwYjA2MmM0ZTYwNDk0M2NmNTBkZmM2NGNhYzQ1OGEyN2NkNDQ3Mzc2NTQyMmZiNDJiNzBhNGVhZGUyOSIsInZlcnNpb24iOjF9.eHjLmw3K02OU69R2Au8eyuSqT3aBDHgZCn8jSzE3_urD6EUSSsLxUpiAYR4BGLD_U6-ZKcdxVo_A2rdXqvUJDA\n    - type: recall\n      value: 0.9301801801801802\n      name: Recall\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGIzM2E3MTI2Mzc2MDYwNmU3ZTVjYmZmZDBkNjY4ZTc5MGY0Y2FkNDU3NjY1MmVkNmE3Y2QzMzAwZDZhOWY1NiIsInZlcnNpb24iOjF9.PUZlqmct13-rJWBXdHm5tdkXgETL9F82GNbbSR4hI8MB-v39KrK59cqzFC2Ac7kJe_DtOeUyosj34O_mFt_1DQ\n    - type: auc\n      value: 0.9716626673402374\n      name: AUC\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMDM0YWIwZmQ4YjUwOGZmMWU2MjI1YjIxZGQ2MzNjMzRmZmYxMzZkNGFjODhlMDcyZDM1Y2RkMWZlOWQ0MWYwNSIsInZlcnNpb24iOjF9.E7GRlAXmmpEkTHlXheVkuL1W4WNjv4JO3qY_WCVsTVKiO7bUu0UVjPIyQ6g-J1OxsfqZmW3Leli1wY8vPBNNCQ\n    - type: f1\n      value: 0.9137168141592922\n      name: F1\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMGU4MjNmOGYwZjZjMDQ1ZTkyZTA4YTc1MWYwOTM0NDM4ZWY1ZGVkNDY5MzNhYTQyZGFlNzIyZmUwMDg3NDU0NyIsInZlcnNpb24iOjF9.mW5ftkq50Se58M-jm6a2Pu93QeKa3MfV7xcBwvG3PSB_KNJxZWTCpfMQp-Cmx_EMlmI2siKOyd8akYjJUrzJCA\n    - type: loss\n      value: 0.39013850688934326\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMTZiNzAyZDc0MzUzMmE1MGJiN2JlYzFiODE5ZTNlNGE4MmI4YzRiMTc2ODEzMTUwZmEzOTgxNzc4YjJjZTRmNiIsInZlcnNpb24iOjF9.VqIC7uYC-ZZ8ss9zQOlRV39YVOOLc5R36sIzCcVz8lolh61ux_5djm2XjpP6ARc6KqEnXC4ZtfNXsX2HZfrtCQ\n  - task:\n      type: text-classification\n      name: Text Classification\n    dataset:\n      name: sst2\n      type: sst2\n      config: default\n      split: train\n    metrics:\n    - type: accuracy\n      value: 0.9885521685548412\n      name: Accuracy\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2I3NzU3YzhmMDkxZTViY2M3OTY1NmI0ZTdmMDQxNjNjYzJiZmQxNzczM2E4YmExYTY5ODY0NDBkY2I4ZjNkOCIsInZlcnNpb24iOjF9.4Gtk3FeVc9sPWSqZIaeUXJ9oVlPzm-NmujnWpK2y5s1Vhp1l6Y1pK5_78wW0-NxSvQqV6qd5KQf_OAEpVAkQDA\n    - type: precision\n      value: 0.9881965062029833\n      name: Precision Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDdlZDMzY2I3MTAwYTljNmM4MGMyMzU2YjAzZDg1NDYwN2ZmM2Y5OWZhMjUyMGJiNjY1YmZiMzFhMDI2ODFhNyIsInZlcnNpb24iOjF9.cqmv6yBxu4St2mykRWrZ07tDsiSLdtLTz2hbqQ7Gm1rMzq9tdlkZ8MyJRxtME_Y8UaOG9rs68pV-gKVUs8wABw\n    - type: precision\n      value: 0.9885521685548412\n      name: Precision Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZjFlYzAzNmE1YjljNjUwNzBjZjEzZDY0ZDQyMmY5ZWM2OTBhNzNjYjYzYTk1YWE1NjU3YTMxZDQwOTE1Y2FkNyIsInZlcnNpb24iOjF9.jnCHOkUHuAOZZ_ZMVOnetx__OVJCS6LOno4caWECAmfrUaIPnPNV9iJ6izRO3sqkHRmxYpWBb-27GJ4N3LU-BQ\n    - type: precision\n      value: 0.9885639626373408\n      name: Precision Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZGUyODFjNjBlNTE2MTY3ZDAxOGU1N2U0YjUyY2NiZjhkOGVmYThjYjBkNGU3NTRkYzkzNDQ2MmMwMjkwMWNiMyIsInZlcnNpb24iOjF9.zTNabMwApiZyXdr76QUn7WgGB7D7lP-iqS3bn35piqVTNsv3wnKjZOaKFVLIUvtBXq4gKw7N2oWxvWc4OcSNDg\n    - type: recall\n      value: 0.9886145346602994\n      name: Recall Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNTU1YjlhODU3YTkyNTdiZDcwZGFlZDBiYjY0N2NjMGM2NTRiNjQ3MDNjNGMxOWY2ZGQ4NWU1YmMzY2UwZTI3YSIsInZlcnNpb24iOjF9.xaLPY7U-wHsJ3DDui1yyyM-xWjL0Jz5puRThy7fczal9x05eKEQ9s0a_WD-iLmapvJs0caXpV70hDe2NLcs-DA\n    - type: recall\n      value: 0.9885521685548412\n      name: Recall Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiODE0YTU0MDBlOGY4YzU0MjY5MzA3OTk2OGNhOGVkMmU5OGRjZmFiZWI2ZjY5ODEzZTQzMTI0N2NiOTVkNDliYiIsInZlcnNpb24iOjF9.SOt1baTBbuZRrsvGcak2sUwoTrQzmNCbyV2m1_yjGsU48SBH0NcKXicidNBSnJ6ihM5jf_Lv_B5_eOBkLfNWDQ\n    - type: recall\n      value: 0.9885521685548412\n      name: Recall Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZWNkNmM0ZGRlNmYxYzIwNDk4OTI5MzIwZWU1NzZjZDVhMDcyNDFlMjBhNDQxODU5OWMwMWNhNGEzNjY3ZGUyOSIsInZlcnNpb24iOjF9.b15Fh70GwtlG3cSqPW-8VEZT2oy0CtgvgEOtWiYonOovjkIQ4RSLFVzVG-YfslaIyfg9RzMWzjhLnMY7Bpn2Aw\n    - type: f1\n      value: 0.9884019815052447\n      name: F1 Macro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiYmM4NjQ5Yjk5ODRhYTU1MTY3MmRhZDBmODM1NTg3OTFiNWM4NDRmYjI0MzZkNmQ1MzE3MzcxODZlYzBkYTMyYSIsInZlcnNpb24iOjF9.74RaDK8nBVuGRl2Se_-hwQvP6c4lvVxGHpcCWB4uZUCf2_HoC9NT9u7P3pMJfH_tK2cpV7U3VWGgSDhQDi-UBQ\n    - type: f1\n      value: 0.9885521685548412\n      name: F1 Micro\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZDRmYWRmMmQ0YjViZmQxMzhhYTUyOTE1MTc0ZDU1ZjQyZjFhMDYzYzMzZDE0NzZlYzQyOTBhMTBhNmM5NTlkMiIsInZlcnNpb24iOjF9.VMn_psdAHIZTlW6GbjERZDe8MHhwzJ0rbjV_VJyuMrsdOh5QDmko-wEvaBWNEdT0cEKsbggm-6jd3Gh81PfHAQ\n    - type: f1\n      value: 0.9885546181087554\n      name: F1 Weighted\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiMjUyZWFhZDZhMGQ3MzBmYmRiNDVmN2FkZDBjMjk3ODk0OTAxNGZkMWE0NzU5ZjI0NzE0NGZiNzM0N2Y2NDYyOSIsInZlcnNpb24iOjF9.YsXBhnzEEFEW6jw3mQlFUuIrW7Gabad2Ils-iunYJr-myg0heF8NEnEWABKFE1SnvCWt-69jkLza6SupeyLVCA\n    - type: loss\n      value: 0.040652573108673096\n      name: loss\n      verified: true\n      verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiZTc3YjU3MjdjMzkxODA5MjU5NGUyY2NkMGVhZDg3ZWEzMmU1YWVjMmI0NmU2OWEyZTkzMTVjNDZiYTc0YjIyNCIsInZlcnNpb24iOjF9.lA90qXZVYiILHMFlr6t6H81Oe8a-4KmeX-vyCC1BDia2ofudegv6Vb46-4RzmbtuKeV6yy6YNNXxXxqVak1pAg\n---\n\n# DistilBERT base uncased finetuned SST-2\n\n## Table of Contents\n- [Model Details](#model-details)\n- [How to Get Started With the Model](#how-to-get-started-with-the-model)\n- [Uses](#uses)\n- [Risks, Limitations and Biases](#risks-limitations-and-biases)\n- [Training](#training)\n\n## Model Details\n**Model Description:** This model is a fine-tune checkpoint of [DistilBERT-base-uncased](https://huggingface.co/distilbert-base-uncased), fine-tuned on SST-2.\nThis model reaches an accuracy of 91.3 on the dev set (for comparison, Bert bert-base-uncased version reaches an accuracy of 92.7).\n- **Developed by:** Hugging Face\n- **Model Type:** Text Classification\n- **Language(s):** English\n- **License:** Apache-2.0\n- **Parent Model:** For more details about DistilBERT, we encourage users to check out [this model card](https://huggingface.co/distilbert-base-uncased).\n- **Resources for more information:**\n    - [Model Documentation](https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification)\n    - [DistilBERT paper](https://arxiv.org/abs/1910.01108)\n\n## How to Get Started With the Model\n\nExample of single-label classification:\n‚Äã‚Äã\n```python\nimport torch\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n\ntokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")\nmodel = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")\n\ninputs = tokenizer("Hello, my dog is cute", return_tensors="pt")\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\npredicted_class_id = logits.argmax().item()\nmodel.config.id2label[predicted_class_id]\n\n```\n\n## Uses\n\n#### Direct Use\n\nThis model can be used for  topic classification. You can use the raw model for either masked language modeling or next sentence prediction, but it''s mostly intended to be fine-tuned on a downstream task. See the model hub to look for fine-tuned versions on a task that interests you.\n\n#### Misuse and Out-of-scope Use\nThe model should not be used to intentionally create hostile or alienating environments for people. In addition, the model was not trained to be factual or true representations of people or events, and therefore using the model to generate such content is out-of-scope for the abilities of this model.\n\n\n## Risks, Limitations and Biases\n\nBased on a few experimentations, we observed that this model could produce biased predictions that target underrepresented populations.\n\nFor instance, for sentences like `This film was filmed in COUNTRY`, this binary classification model will give radically different probabilities for the positive label depending on the country (0.89 if the country is France, but 0.08 if the country is Afghanistan) when nothing in the input indicates such a strong semantic shift. In this [colab](https://colab.research.google.com/gist/ageron/fb2f64fb145b4bc7c49efc97e5f114d3/biasmap.ipynb), [Aur√©lien G√©ron](https://twitter.com/aureliengeron) made an interesting map plotting these probabilities for each country.\n\n<img src="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/resolve/main/map.jpeg" alt="Map of positive probabilities per country." width="500"/>\n\nWe strongly advise users to thoroughly probe these aspects on their use-cases in order to evaluate the risks of this model. We recommend looking at the following bias evaluation datasets as a place to start: [WinoBias](https://huggingface.co/datasets/wino_bias), [WinoGender](https://huggingface.co/datasets/super_glue), [Stereoset](https://huggingface.co/datasets/stereoset).\n\n\n\n# Training\n\n\n#### Training Data\n\n\nThe authors use the following Stanford Sentiment Treebank([sst2](https://huggingface.co/datasets/sst2)) corpora for the model.\n\n#### Training Procedure\n\n###### Fine-tuning hyper-parameters\n\n\n- learning_rate = 1e-5\n- batch_size = 32\n- warmup = 600\n- max_seq_length = 128\n- num_train_epochs = 3.0\n\n\n', '{"pipeline_tag":"text-classification","library_name":"transformers","framework":"transformers","params":66955010,"storage_bytes":2208990137,"files_count":17,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["DistilBertForSequenceClassification"],"model_type":"distilbert","tokenizer_config":{}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1910.01108","source_url":"https://arxiv.org/abs/1910.01108"}]', NULL, 'Apache-2.0', 'approved', 79.3, '83221cd17c2b62b0f476931c3b2516f1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Devstral-Small-2505', 'huggingface--mistralai--devstral-small-2505', 'Devstral-Small-2505', 'mistralai', '--- library_name: vllm language: - en - fr - de - es - pt - it - ja - ko - ru - zh - ar - fa - id - ms - ne - pl - ro - sr - sv - tr - uk - vi - hi - bn license: apache-2.0 inference: false base_model: - mistralai/Mistral-Small-3.1-24B-Instruct-2503 extra_gated_description: >- If you want to learn more about how we process your personal data, please read our <a href="https://mistral.ai/terms/">Privacy Policy</a>. tags: - mistral-common --- Devstral is an agentic LLM for software engineering t...', '["vllm","safetensors","mistral","mistral-common","en","fr","de","es","pt","it","ja","ko","ru","zh","ar","fa","id","ms","ne","pl","ro","sr","sv","tr","uk","vi","hi","bn","license:apache-2.0","region:us"]', 'other', 857, 11196, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Devstral-Small-2505","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: vllm\nlanguage:\n- en\n- fr\n- de\n- es\n- pt\n- it\n- ja\n- ko\n- ru\n- zh\n- ar\n- fa\n- id\n- ms\n- ne\n- pl\n- ro\n- sr\n- sv\n- tr\n- uk\n- vi\n- hi\n- bn\nlicense: apache-2.0\ninference: false\nbase_model:\n- mistralai/Mistral-Small-3.1-24B-Instruct-2503\nextra_gated_description: >-\n  If you want to learn more about how we process your personal data, please read\n  our <a href="https://mistral.ai/terms/">Privacy Policy</a>.\ntags:\n- mistral-common\n---\n\n# Devstral Small 1.0\n\nDevstral is an agentic LLM for software engineering tasks built under a collaboration between [Mistral AI](https://mistral.ai/) and [All Hands AI](https://www.all-hands.dev/) üôå. Devstral excels at using tools to explore codebases, editing multiple files and power software engineering agents. The model achieves remarkable performance on SWE-bench which positionates it as the #1 open source model on this [benchmark](#benchmark-results). \n\nIt is finetuned from [Mistral-Small-3.1](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503), therefore it has a long context window of up to 128k tokens. As a coding agent, Devstral is text-only and before fine-tuning from `Mistral-Small-3.1` the vision encoder was removed.\n\nFor enterprises requiring specialized capabilities (increased context, domain-specific knowledge, etc.), we will release commercial models beyond what Mistral AI contributes to the community.\n\nLearn more about Devstral in our [blog post](https://mistral.ai/news/devstral).\n\n\n## Key Features:\n- **Agentic coding**: Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents.\n- **lightweight**: with its compact size of just 24 billion parameters, Devstral is light enough to run on a single RTX 4090 or a Mac with 32GB RAM, making it an appropriate model for local deployment and on-device use.\n- **Apache 2.0 License**: Open license allowing usage and modification for both commercial and non-commercial purposes.\n- **Context Window**: A 128k context window.\n- **Tokenizer**: Utilizes a Tekken tokenizer with a 131k vocabulary size.\n\n\n\n## Benchmark Results\n\n### SWE-Bench\n\nDevstral achieves a score of 46.8% on SWE-Bench Verified, outperforming prior open-source SoTA by 6%.\n\n| Model            | Scaffold           | SWE-Bench Verified (%) |\n|------------------|--------------------|------------------------|\n| Devstral         | OpenHands Scaffold | **46.8**               |\n| GPT-4.1-mini     | OpenAI Scaffold    | 23.6                   |\n| Claude 3.5 Haiku | Anthropic Scaffold | 40.6                   |\n| SWE-smith-LM 32B | SWE-agent Scaffold | 40.2                   |\n\n\n When evaluated under the same test scaffold (OpenHands, provided by All Hands AI üôå), Devstral exceeds far larger models such as Deepseek-V3-0324 and Qwen3 232B-A22B.\n\n![SWE Benchmark](assets/swe_bench.png)\n\n## Usage\n\nWe recommend to use Devstral with the [OpenHands](https://github.com/All-Hands-AI/OpenHands/tree/main) scaffold.\nYou can use it either through our API or by running locally. \n\n### API \nFollow these [instructions](https://docs.mistral.ai/getting-started/quickstart/#account-setup) to create a Mistral account and get an API key.\n\nThen run these commands to start the OpenHands docker container.\n```bash\nexport MISTRAL_API_KEY=<MY_KEY>\n\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.39-nikolaik\n\nmkdir -p ~/.openhands-state && echo ''{"language":"en","agent":"CodeActAgent","max_iterations":null,"security_analyzer":null,"confirmation_mode":false,"llm_model":"mistral/devstral-small-2505","llm_api_key":"''$MISTRAL_API_KEY''","remote_runtime_resource_factor":null,"github_token":null,"enable_default_condenser":true}'' > ~/.openhands-state/settings.json\n\ndocker run -it --rm --pull=always \\n    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.39-nikolaik \\n    -e LOG_ALL_EVENTS=true \\n    -v /var/run/docker.sock:/var/run/docker.sock \\n    -v ~/.openhands-state:/.openhands-state \\n    -p 3000:3000 \\n    --add-host host.docker.internal:host-gateway \\n    --name openhands-app \\n    docker.all-hands.dev/all-hands-ai/openhands:0.39\n```\n\n### Local inference \n\nThe model can also be deployed with the following libraries:\n- [`vllm (recommended)`](https://github.com/vllm-project/vllm): See [here](#vllm-recommended)\n- [`mistral-inference`](https://github.com/mistralai/mistral-inference): See [here](#mistral-inference)\n- [`transformers`](https://github.com/huggingface/transformers): See [here](#transformers)\n- [`LMStudio`](https://lmstudio.ai/): See [here](#lmstudio)\n- [`llama.cpp`](https://github.com/ggml-org/llama.cpp): See [here](#llama.cpp)\n- [`ollama`](https://github.com/ollama/ollama): See [here](#ollama)\n\n\n### OpenHands (recommended)\n\n#### Launch a server to deploy Devstral Small 1.0\n\nMake sure you launched an OpenAI-compatible server such as vLLM or Ollama as described above. Then, you can use OpenHands to interact with `Devstral Small 1.0`.\n\nIn the case of the tutorial we spineed up a vLLM server running the command:\n```bash\nvllm serve mistralai/Devstral-Small-2505 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --tensor-parallel-size 2\n```\n\nThe server address should be in the following format: `http://<your-server-url>:8000/v1`\n\n#### Launch OpenHands\n\nYou can follow installation of OpenHands [here](https://docs.all-hands.dev/modules/usage/installation).\n\nThe easiest way to launch OpenHands is to use the Docker image:\n```bash\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.38-nikolaik\n\ndocker run -it --rm --pull=always \\n    -e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.38-nikolaik \\n    -e LOG_ALL_EVENTS=true \\n    -v /var/run/docker.sock:/var/run/docker.sock \\n    -v ~/.openhands-state:/.openhands-state \\n    -p 3000:3000 \\n    --add-host host.docker.internal:host-gateway \\n    --name openhands-app \\n    docker.all-hands.dev/all-hands-ai/openhands:0.38\n```\n\n\nThen, you can access the OpenHands UI at `http://localhost:3000`.\n\n#### Connect to the server\n\nWhen accessing the OpenHands UI, you will be prompted to connect to a server. You can use the advanced mode to connect to the server you launched earlier.\n\nFill the following fields:\n- **Custom Model**: `openai/mistralai/Devstral-Small-2505`\n- **Base URL**: `http://<your-server-url>:8000/v1`\n- **API Key**: `token` (or any other token you used to launch the server if any)\n\n#### Use OpenHands powered by Devstral\n\nNow you''re good to use Devstral Small inside OpenHands by **starting a new conversation**. Let''s build a To-Do list app.\n\n<details>\n  <summary>To-Do list app</summary\n\n1. Let''s ask Devstral to generate the app with the following prompt:\n\n```txt\nBuild a To-Do list app with the following requirements:\n- Built using FastAPI and React.\n- Make it a one page app that:\n   - Allows to add a task.\n   - Allows to delete a task.\n   - Allows to mark a task as done.\n   - Displays the list of tasks.\n- Store the tasks in a SQLite database.\n```\n\n![Agent prompting](assets/tuto_open_hands/agent_prompting.png)\n\n\n2. Let''s see the result\n\nYou should see the agent construct the app and be able to explore the code it generated.\n\nIf it doesn''t do it automatically, ask Devstral to deploy the app or do it manually, and then go the front URL deployment to see the app.\n\n![Agent working](assets/tuto_open_hands/agent_working.png)\n![App UI](assets/tuto_open_hands/app_ui.png)\n\n\n3. Iterate\n\nNow that you have a first result you can iterate on it by asking your agent to improve it. For example, in the app generated we could click on a task to mark it checked but having a checkbox would improve UX. You could also ask it to add a feature to edit a task, or to add a feature to filter the tasks by status.\n\nEnjoy building with Devstral Small and OpenHands!\n\n</details>\n\n\n### vLLM (recommended)\n\nWe recommend using this model with the [vLLM library](https://github.com/vllm-project/vllm)\nto implement production-ready inference pipelines.\n\n**_Installation_**\n\nMake sure you install [`vLLM >= 0.8.5`](https://github.com/vllm-project/vllm/releases/tag/v0.8.5):\n\n```\npip install vllm --upgrade\n```\n\nDoing so should automatically install [`mistral_common >= 1.5.5`](https://github.com/mistralai/mistral-common/releases/tag/v1.5.5).\n\nTo check:\n```\npython -c "import mistral_common; print(mistral_common.__version__)"\n```\n\nYou can also make use of a ready-to-go [docker image](https://github.com/vllm-project/vllm/blob/main/Dockerfile) or on the [docker hub](https://hub.docker.com/layers/vllm/vllm-openai/latest/images/sha256-de9032a92ffea7b5c007dad80b38fd44aac11eddc31c435f8e52f3b7404bbf39).\n\n#### Server\n\nWe recommand that you use Devstral in a server/client setting. \n\n1. Spin up a server:\n\n```\nvllm serve mistralai/Devstral-Small-2505 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --tensor-parallel-size 2\n```\n\n\n2. To ping the client you can use a simple Python snippet.\n\n```py\nimport requests\nimport json\nfrom huggingface_hub import hf_hub_download\n\n\nurl = "http://<your-server-url>:8000/v1/chat/completions"\nheaders = {"Content-Type": "application/json", "Authorization": "Bearer token"}\n\nmodel = "mistralai/Devstral-Small-2505"\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, "r") as file:\n        system_prompt = file.read()\n    return system_prompt\n\nSYSTEM_PROMPT = load_system_prompt(model, "SYSTEM_PROMPT.txt")\n\nmessages = [\n    {"role": "system", "content": SYSTEM_PROMPT},\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "text",\n                "text": "<your-command>",\n            },\n        ],\n    },\n]\n\ndata = {"model": model, "messages": messages, "temperature": 0.15}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\nprint(response.json()["choices"][0]["message"]["content"])\n```\n\n### Mistral-inference\n\nWe recommend using mistral-inference to quickly try out / "vibe-check" Devstral.\n\n#### Install\n\nMake sure to have mistral_inference >= 1.6.0 installed.\n\n```bash\npip install mistral_inference --upgrade\n```\n\n#### Download\n\n```python\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nmistral_models_path = Path.home().joinpath(''mistral_models'', ''Devstral'')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id="mistralai/Devstral-Small-2505", allow_patterns=["params.json", "consolidated.safetensors", "tekken.json"], local_dir=mistral_models_path)\n```\n\n#### Python\n\nYou can run the model using the following command:\n\n```bash\nmistral-chat $HOME/mistral_models/Devstral --instruct --max_tokens 300\n```\n\nYou can then prompt it with anything you''d like.\n\n### Transformers\n\nTo make the best use of our model with transformers make sure to have [installed](https://github.com/mistralai/mistral-common) `    mistral-common >= 1.5.5` to use our tokenizer.\n\n```bash\npip install mistral-common --upgrade\n```\n\nThen load our tokenizer along with the model and generate:\n\n```python\nimport torch\n\nfrom mistral_common.protocol.instruct.messages import (\n    SystemMessage, UserMessage\n)\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom huggingface_hub import hf_hub_download\nfrom transformers import AutoModelForCausalLM\n\ndef load_system_prompt(repo_id: str, filename: str) -> str:\n    file_path = hf_hub_download(repo_id=repo_id, filename=filename)\n    with open(file_path, "r") as file:\n        system_prompt = file.read()\n    return system_prompt\n\nmodel_id = "mistralai/Devstral-Small-2505"\ntekken_file = hf_hub_download(repo_id=model_id, filename="tekken.json")\nSYSTEM_PROMPT = load_system_prompt(model_id, "SYSTEM_PROMPT.txt")\n\ntokenizer = MistralTokenizer.from_file(tekken_file)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id)\n\ntokenized = tokenizer.encode_chat_completion(\n    ChatCompletionRequest(\n        messages=[\n            SystemMessage(content=SYSTEM_PROMPT),\n            UserMessage(content="<your-command>"),\n        ],\n    )\n)\n\noutput = model.generate(\n    input_ids=torch.tensor([tokenized.tokens]),\n    max_new_tokens=1000,\n)[0]\n\ndecoded_output = tokenizer.decode(output[len(tokenized.tokens):])\nprint(decoded_output)\n```\n\n### LMStudio\nDownload the weights from huggingface:\n\n```\npip install -U "huggingface_hub[cli]"\nhuggingface-cli download \\n"mistralai/Devstral-Small-2505_gguf" \\n--include "devstralQ4_K_M.gguf" \\n--local-dir "mistralai/Devstral-Small-2505_gguf/"\n```\n\nYou can serve the model locally with [LMStudio](https://lmstudio.ai/).\n* Download [LM Studio](https://lmstudio.ai/) and install it\n* Install `lms cli ~/.lmstudio/bin/lms bootstrap`\n* In a bash terminal, run `lms import devstralQ4_K_M.gguf` in the directory where you''ve downloaded the model checkpoint (e.g. `mistralai/Devstral-Small-2505_gguf`)\n* Open the LMStudio application, click the terminal icon to get into the developer tab. Click select a model to load and select Devstral Q4 K M. Toggle the status button to start the model, in setting toggle Serve on Local Network to be on.\n* On the right tab, you will see an API identifier which should be devstralq4_k_m and an api address under API Usage. Keep note of this address, we will use it in the next step.\n\nLaunch Openhands\nYou can now interact with the model served from LM Studio with openhands. Start the openhands server with the docker\n\n```bash\ndocker pull docker.all-hands.dev/all-hands-ai/runtime:0.38-nikolaik\ndocker run -it --rm --pull=always \\n	-e SANDBOX_RUNTIME_CONTAINER_IMAGE=docker.all-hands.dev/all-hands-ai/runtime:0.38-nikolaik \\n	-e LOG_ALL_EVENTS=true \\n	-v /var/run/docker.sock:/var/run/docker.sock \\n	-v ~/.openhands-state:/.openhands-state \\n	-p 3000:3000 \\n	--add-host host.docker.internal:host-gateway \\n	--name openhands-app \\n	docker.all-hands.dev/all-hands-ai/openhands:0.38\n```\n\nClick ‚Äúsee advanced setting‚Äù on the second line. \nIn the new tab, toggle advanced to on. Set the custom model to be mistral/devstralq4_k_m and Base URL the api address we get from the last step in LM Studio. Set API Key to dummy. Click save changes.\n\n### llama.cpp\n\nDownload the weights from huggingface:\n\n```\npip install -U "huggingface_hub[cli]"\nhuggingface-cli download \\n"mistralai/Devstral-Small-2505_gguf" \\n--include "devstralQ4_K_M.gguf" \\n--local-dir "mistralai/Devstral-Small-2505_gguf/"\n```\n\nThen run Devstral using the llama.cpp CLI.\n\n```bash\n./llama-cli -m Devstral-Small-2505_gguf/devstralQ4_K_M.gguf -cnv\n```\n\n### Ollama\n\nYou can run Devstral using the [Ollama](https://ollama.ai/) CLI.\n\n```bash\nollama run devstral\n```\n\n### Example: Understanding Test Coverage of Mistral Common\n\nWe can start the OpenHands scaffold and link it to a repo to analyze test coverage and identify badly covered files.\nHere we start with our public `mistral-common` repo.\n\n\nAfter the repo is mounted in the workspace, we give the following instruction\n```\nCheck the test coverage of the repo and then create a visualization of test coverage. Try plotting a few different types of graphs and save them to a png.\n```\nThe agent will first browse the code base to check test configuration and structure.\n\n![Repo Exploration](assets/images_example/example_mistral_common_1.png)\n\nThen it sets up the testing dependencies and launches the coverage test:\n\n![Repo Exploration](assets/images_example/example_mistral_common_2.png)\n\nFinally, the agent writes necessary code to visualize the coverage.\n![Repo Exploration](assets/images_example/example_mistral_common_3.png)\n\nAt the end of the run, the following plots are produced:\n![Repo Exploration](assets/images_example/example_mistral_common_res_1.png)\n![Repo Exploration](assets/images_example/example_mistral_common_res_2.png)\n![Repo Exploration](assets/images_example/example_mistral_common_res_3.png)', '{"pipeline_tag":null,"library_name":"vllm","framework":"vllm","params":23572403200,"storage_bytes":94324335088,"files_count":29,"spaces_count":26,"gated":false,"private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral"}}', '[]', '[{"type":"has_code","target_id":"github:All-Hands-AI:OpenHands","source_url":"https://github.com/All-Hands-AI/OpenHands"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:mistralai:mistral-inference","source_url":"https://github.com/mistralai/mistral-inference"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:ggml-org:llama.cpp","source_url":"https://github.com/ggml-org/llama.cpp"},{"type":"has_code","target_id":"github:ollama:ollama","source_url":"https://github.com/ollama/ollama"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:mistralai:mistral-common","source_url":"https://github.com/mistralai/mistral-common"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:mistralai:mistral-common","source_url":"https://github.com/mistralai/mistral-common"}]', NULL, 'Apache-2.0', 'approved', 99.3, 'b91ba747cfdd9d384ac5a16e31b3ea14', NULL, 'https://huggingface.co/mistralai/Devstral-Small-2505/resolve/main/assets/images_example/example_mistral_common_1.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-mistralai-Devstral-Small-2505 from https://huggingface.co/mistralai/Devstral-Small-2505/resolve/main/assets/images_example/example_mistral_common_1.png
Image converted to WebP: data/images/huggingface-mistralai-Devstral-Small-2505.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-flan-t5-large', 'huggingface--google--flan-t5-large', 'flan-t5-large', 'google', '--- language: - en - fr - ro - de - multilingual widget: - text: "Translate to German: My name is Arthur" example_title: "Translation" - text: "Please answer to the following question. Who is going to be the next Ballon d''or?" example_title: "Question Answering" - text: "Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering." example_title: "Logical reasoning" - text: "Please answer the following question. What is the boiling point of Nitrogen?...', '["transformers","pytorch","tf","jax","safetensors","t5","text2text-generation","en","fr","ro","de","multilingual","dataset:svakulenk0/qrecc","dataset:taskmaster2","dataset:djaym7/wiki_dialog","dataset:deepmind/code_contests","dataset:lambada","dataset:gsm8k","dataset:aqua_rat","dataset:esnli","dataset:quasc","dataset:qed","arxiv:2210.11416","arxiv:1910.09700","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'other', 853, 460192, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/flan-t5-large","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: \n- en\n- fr\n- ro\n- de\n- multilingual\n\nwidget:\n- text: "Translate to German:  My name is Arthur"\n  example_title: "Translation"\n- text: "Please answer to the following question. Who is going to be the next Ballon d''or?"\n  example_title: "Question Answering"\n- text: "Q: Can Geoffrey Hinton have a conversation with George Washington? Give the rationale before answering."\n  example_title: "Logical reasoning"\n- text: "Please answer the following question. What is the boiling point of Nitrogen?"\n  example_title: "Scientific knowledge"\n- text: "Answer the following yes/no question. Can you write a whole Haiku in a single tweet?"\n  example_title: "Yes/no question"\n- text: "Answer the following yes/no question by reasoning step-by-step. Can you write a whole Haiku in a single tweet?"\n  example_title: "Reasoning task"\n- text: "Q: ( False or not False or False ) is? A: Let''s think step by step"\n  example_title: "Boolean Expressions"\n- text: "The square root of x is the cube root of y. What is y to the power of 2, if x = 4?"\n  example_title: "Math reasoning"\n- text: "Premise:  At my age you will probably have learnt one lesson. Hypothesis:  It''s not certain how many lessons you''ll learn by your thirties. Does the premise entail the hypothesis?"\n  example_title: "Premise and hypothesis"\n\ntags:\n- text2text-generation\n\ndatasets:\n- svakulenk0/qrecc\n- taskmaster2\n- djaym7/wiki_dialog\n- deepmind/code_contests\n- lambada\n- gsm8k\n- aqua_rat\n- esnli\n- quasc\n- qed\n\n\nlicense: apache-2.0\n---\n\n# Model Card for FLAN-T5 large\n\n<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/flan2_architecture.jpg"\nalt="drawing" width="600"/>\n\n#  Table of Contents\n\n0. [TL;DR](#TL;DR)\n1. [Model Details](#model-details)\n2. [Usage](#usage)\n3. [Uses](#uses)\n4. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n5. [Training Details](#training-details)\n6. [Evaluation](#evaluation)\n7. [Environmental Impact](#environmental-impact)\n8. [Citation](#citation)\n9. [Model Card Authors](#model-card-authors)\n\n# TL;DR\n\nIf you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. \nAs mentioned in the first few lines of the abstract : \n>  Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.\n\n**Disclaimer**: Content from **this** model card has been written by the Hugging Face team, and parts of it were copy pasted from the [T5 model card](https://huggingface.co/t5-large).\n\n# Model Details\n\n## Model Description\n\n\n- **Model type:** Language model\n- **Language(s) (NLP):** English, Spanish, Japanese, Persian, Hindi, French, Chinese, Bengali, Gujarati, German, Telugu, Italian, Arabic, Polish, Tamil, Marathi, Malayalam, Oriya, Panjabi, Portuguese, Urdu, Galician, Hebrew, Korean, Catalan, Thai, Dutch, Indonesian, Vietnamese, Bulgarian, Filipino, Central Khmer, Lao, Turkish, Russian, Croatian, Swedish, Yoruba, Kurdish, Burmese, Malay, Czech, Finnish, Somali, Tagalog, Swahili, Sinhala, Kannada, Zhuang, Igbo, Xhosa, Romanian, Haitian, Estonian, Slovak, Lithuanian, Greek, Nepali, Assamese, Norwegian\n- **License:** Apache 2.0\n- **Related Models:** [All FLAN-T5 Checkpoints](https://huggingface.co/models?search=flan-t5)\n- **Original Checkpoints:** [All Original FLAN-T5 Checkpoints](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)\n- **Resources for more information:**\n  - [Research paper](https://arxiv.org/pdf/2210.11416.pdf)\n  - [GitHub Repo](https://github.com/google-research/t5x)\n  - [Hugging Face FLAN-T5 Docs (Similar to T5) ](https://huggingface.co/docs/transformers/model_doc/t5)\n\n# Usage\n\nFind below some example scripts on how to use the model in `transformers`:\n\n## Using the Pytorch model\n\n### Running the model on a CPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large")\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large", device_map="auto")\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n### Running the model on a GPU using different precisions\n\n#### FP16\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install accelerate\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large", device_map="auto", torch_dtype=torch.float16)\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n#### INT8\n\n<details>\n<summary> Click to expand </summary>\n\n```python\n# pip install bitsandbytes accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\ntokenizer = T5Tokenizer.from_pretrained("google/flan-t5-large")\nmodel = T5ForConditionalGeneration.from_pretrained("google/flan-t5-large", device_map="auto", load_in_8bit=True)\n\ninput_text = "translate English to German: How old are you?"\ninput_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n```\n\n</details>\n\n# Uses\n\n## Direct Use and Downstream Use\n\nThe authors write in [the original paper''s model card](https://arxiv.org/pdf/2210.11416.pdf) that: \n\n> The primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models\n\nSee the [research paper](https://arxiv.org/pdf/2210.11416.pdf) for further details.\n\n## Out-of-Scope Use\n\nMore information needed.\n\n# Bias, Risks, and Limitations\n\nThe information below in this section are copied from the model''s [official model card](https://arxiv.org/pdf/2210.11416.pdf):\n\n> Language models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.\n\n## Ethical considerations and risks\n\n> Flan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\n\n## Known Limitations\n\n> Flan-T5 has not been tested in real world applications.\n\n## Sensitive Use:\n\n> Flan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech.\n\n# Training Details\n\n## Training Data\n\nThe model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2):\n\n![table.png](https://s3.amazonaws.com/moonup/production/uploads/1666363265279-62441d1d9fdefb55a0b7d12c.png)\n\n\n## Training Procedure\n\nAccording to the model card from the [original paper](https://arxiv.org/pdf/2210.11416.pdf):\n\n> These models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size.\n\nThe model has been trained on TPU v3 or TPU v4 pods, using [`t5x`](https://github.com/google-research/t5x) codebase together with [`jax`](https://github.com/google/jax).\n\n\n# Evaluation\n\n## Testing Data, Factors & Metrics\n\nThe authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation:\n![image.png](https://s3.amazonaws.com/moonup/production/uploads/1668072995230-62441d1d9fdefb55a0b7d12c.png)\nFor full details, please check the [research paper](https://arxiv.org/pdf/2210.11416.pdf).\n\n## Results \n\nFor full results for FLAN-T5-Large, see the [research paper](https://arxiv.org/pdf/2210.11416.pdf), Table 3.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** Google Cloud TPU Pods - TPU v3 or TPU v4  | Number of chips ‚â• 4.\n- **Hours used:** More information needed\n- **Cloud Provider:** GCP\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@misc{https://doi.org/10.48550/arxiv.2210.11416,\n  doi = {10.48550/ARXIV.2210.11416},\n  \n  url = {https://arxiv.org/abs/2210.11416},\n  \n  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},\n  \n  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {Scaling Instruction-Finetuned Language Models},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```', '{"pipeline_tag":null,"library_name":"transformers","framework":"transformers","params":783150080,"storage_bytes":15929620947,"files_count":12,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["T5ForConditionalGeneration"],"model_type":"t5","tokenizer_config":{"eos_token":"</s>","pad_token":"<pad>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google-research:t5x","source_url":"https://github.com/google-research/t5x"},{"type":"has_code","target_id":"github:google:jax","source_url":"https://github.com/google/jax"},{"type":"based_on_paper","target_id":"arxiv:2210.11416","source_url":"https://arxiv.org/abs/2210.11416"},{"type":"based_on_paper","target_id":"arxiv:1910.09700","source_url":"https://arxiv.org/abs/1910.09700"}]', NULL, 'Apache-2.0', 'approved', 79.3, '2308ee9e614ae132d5c4d95fc12c5545', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-meta-llama-Llama-2-70b-hf', 'huggingface--meta-llama--llama-2-70b-hf', 'Llama-2-70b-hf', 'meta-llama', '', '["transformers","pytorch","safetensors","llama","text-generation","facebook","meta","llama-2","en","arxiv:2307.09288","license:llama2","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 853, 23925, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/meta-llama/Llama-2-70b-hf","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":68976653312,"storage_bytes":551815654451,"files_count":43,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<s>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"</s>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false},"pad_token":null,"unk_token":{"__type":"AddedToken","content":"<unk>","lstrip":false,"normalized":false,"rstrip":false,"single_word":false}}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2307.09288","source_url":"https://arxiv.org/abs/2307.09288"}]', NULL, 'LLaMA-2', 'approved', 39.3, 'c1e5d41e06bfd2084c63c5d1f02f618f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-jasperai-Flux.1-dev-Controlnet-Upscaler', 'huggingface--jasperai--flux.1-dev-controlnet-upscaler', 'Flux.1-dev-Controlnet-Upscaler', 'jasperai', '--- base_model: - black-forest-labs/FLUX.1-dev library_name: diffusers license: other license_name: flux-1-dev-non-commercial-license license_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md pipeline_tag: image-to-image inference: False tags: - ControlNet - super-resolution - upscaler --- This is Flux.1-dev ControlNet for low resolution images developed by Jasper research team. <p align="center"> <img style="width:700px;" src="examples/showcase.jpg"> </p> This mo...', '["diffusers","safetensors","controlnet","super-resolution","upscaler","image-to-image","base_model:black-forest-labs/flux.1-dev","base_model:finetune:black-forest-labs/flux.1-dev","license:other","region:us"]', 'image-to-image', 853, 12238, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/jasperai/Flux.1-dev-Controlnet-Upscaler","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nbase_model:\n- black-forest-labs/FLUX.1-dev\nlibrary_name: diffusers\nlicense: other\nlicense_name: flux-1-dev-non-commercial-license\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md\npipeline_tag: image-to-image\ninference: False\ntags:\n- ControlNet\n- super-resolution\n- upscaler\n---\n# ‚ö° Flux.1-dev: Upscaler ControlNet ‚ö°\n\nThis is [Flux.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) ControlNet for low resolution images developed by Jasper research team.\n\n<p align="center">\n   <img style="width:700px;" src="examples/showcase.jpg">\n</p>\n\n# How to use\nThis model can be used directly with the `diffusers` library\n\n```python\nimport torch\nfrom diffusers.utils import load_image\nfrom diffusers import FluxControlNetModel\nfrom diffusers.pipelines import FluxControlNetPipeline\n\n# Load pipeline\ncontrolnet = FluxControlNetModel.from_pretrained(\n  "jasperai/Flux.1-dev-Controlnet-Upscaler",\n  torch_dtype=torch.bfloat16\n)\npipe = FluxControlNetPipeline.from_pretrained(\n  "black-forest-labs/FLUX.1-dev",\n  controlnet=controlnet,\n  torch_dtype=torch.bfloat16\n)\npipe.to("cuda")\n\n# Load a control image\ncontrol_image = load_image(\n  "https://huggingface.co/jasperai/Flux.1-dev-Controlnet-Upscaler/resolve/main/examples/input.jpg"\n)\n\nw, h = control_image.size\n\n# Upscale x4\ncontrol_image = control_image.resize((w * 4, h * 4))\n\nimage = pipe(\n    prompt="", \n    control_image=control_image,\n    controlnet_conditioning_scale=0.6,\n    num_inference_steps=28, \n    guidance_scale=3.5,\n    height=control_image.size[1],\n    width=control_image.size[0]\n).images[0]\nimage\n```\n\n<p align="center">\n   <img style="width:500px;" src="examples/output.jpg">\n</p>\n\n\n# Training\nThis model was trained with a synthetic complex data degradation scheme taking as input a *real-life* image and artificially degrading it by combining several degradations such as amongst other image noising (Gaussian, Poisson), image blurring and JPEG compression in a similar spirit as [1]\n\n[1] Wang, Xintao, et al. "Real-esrgan: Training real-world blind super-resolution with pure synthetic data." Proceedings of the IEEE/CVF international conference on computer vision. 2021.\n\n# Licence\nThis model falls under the [Flux.1-dev model licence](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md).', '{"pipeline_tag":"image-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":10808301966,"files_count":7,"spaces_count":82,"gated":false,"private":false,"config":{}}', '[]', '[]', NULL, 'Other', 'approved', 64.3, 'b5a7de8fe9a7b207446706f5878b0fe5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mistralai-Mistral-Large-Instruct-2407', 'huggingface--mistralai--mistral-large-instruct-2407', 'Mistral-Large-Instruct-2407', 'mistralai', '', '["vllm","safetensors","mistral","mistral-common","en","fr","de","es","it","pt","zh","ja","ru","ko","license:other","region:us"]', 'other', 852, 9031, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mistralai/Mistral-Large-Instruct-2407","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":null,"library_name":"vllm","framework":"vllm","params":122610069504,"storage_bytes":490441047031,"files_count":114,"spaces_count":32,"gated":"auto","private":false,"config":{"architectures":["MistralForCausalLM"],"model_type":"mistral","tokenizer_config":{"bos_token":"<s>","chat_template":"{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- bos_token }}\n{%- for message in loop_messages %}\n    {%- if message[\"role\"] == \"user\" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \"[AVAILABLE_TOOLS] [\" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- ''{\"type\": \"function\", \"function\": {'' }}\n                {%- for key, val in tool.items() if key != \"return\" %}\n                    {%- if val is string %}\n                        {{- ''\"'' + key + ''\": \"'' + val + ''\"'' }}\n                    {%- else %}\n                        {{- ''\"'' + key + ''\": '' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \", \" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \"}}\" }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- else %}\n                    {{- \"]\" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \"[/AVAILABLE_TOOLS]\" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \"[INST] \" + system_message + \"\\n\\n\" + message[\"content\"] + \"[/INST]\" }}\n        {%- else %}\n            {{- \"[INST] \" + message[\"content\"] + \"[/INST]\" }}\n        {%- endif %}\n    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\n        {{- \"[TOOL_CALLS] [\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n            {%- endif %}\n            {{- '', \"id\": \"'' + tool_call.id + ''\"}'' }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]\" + eos_token }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\"role\"] == \"assistant\" %}\n        {{- \" \" + message[\"content\"]|trim + eos_token}}\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- ''[TOOL_RESULTS] {\"content\": '' + content|string + \", \" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n        {%- endif %}\n        {{- ''\"call_id\": \"'' + message.tool_call_id + ''\"}[/TOOL_RESULTS]'' }}\n    {%- else %}\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n    {%- endif %}\n{%- endfor %}\n","eos_token":"</s>","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[]', NULL, 'Other', 'approved', 39.3, '270cc25e9c836bad4aef660eeae3e047', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-0.6B', 'huggingface--qwen--qwen3-0.6b', 'Qwen3-0.6B', 'Qwen', '--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-0.6B/blob/main/LICENSE pipeline_tag: text-generation base_model: - Qwen/Qwen3-0.6B-Base --- <a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> Qwen3 is the latest generation of large language models in Qwen series, offer...', '["transformers","safetensors","qwen3","text-generation","conversational","arxiv:2505.09388","base_model:qwen/qwen3-0.6b-base","base_model:finetune:qwen/qwen3-0.6b-base","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 849, 7589380, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-0.6B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-0.6B/blob/main/LICENSE\npipeline_tag: text-generation\nbase_model:\n- Qwen/Qwen3-0.6B-Base\n---\n\n# Qwen3-0.6B\n<a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-0.6B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 0.6B\n- Number of Paramaters (Non-Embedding): 0.44B\n- Number of Layers: 28\n- Number of Attention Heads (GQA): 16 for Q and 8 for KV\n- Context Length: 32,768 \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n> [!TIP]\n> If you encounter significant endless repetitions, please refer to the [Best Practices](#best-practices) section for optimal sampling parameters, and set the ``presence_penalty`` to 1.5.\n\n## Quickstart\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: ''qwen3''\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen3-0.6B"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\n\n# prepare the model input\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")\n\nprint("thinking content:", thinking_content)\nprint("content:", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-0.6B --reasoning-parser qwen3\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-0.6B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model''s thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model''s behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model''s thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name="Qwen/Qwen3-0.6B"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{"role": "user", "content": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors="pt")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({"role": "user", "content": user_input})\n        self.history.append({"role": "assistant", "content": response})\n\n        return response\n\n# Example Usage\nif __name__ == "__main__":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = "How many r''s in strawberries?"\n    print(f"User: {user_input_1}")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f"Bot: {response_1}")\n    print("----------------------")\n\n    # Second input with /no_think\n    user_input_2 = "Then, how many r''s in blueberries? /no_think"\n    print(f"User: {user_input_2}")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f"Bot: {response_2}") \n    print("----------------------")\n\n    # Third input with /think\n    user_input_3 = "Really? /think"\n    print(f"User: {user_input_3}")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f"Bot: {response_3}")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    ''model'': ''Qwen3-0.6B'',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # ''model_type'': ''qwen_dashscope'',\n    # ''api_key'': os.getenv(''DASHSCOPE_API_KEY''),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    ''model_server'': ''http://localhost:8000/v1'',  # api_base\n    ''api_key'': ''EMPTY'',\n\n    # Other parameters:\n    # ''generate_cfg'': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         ''thought_in_content'': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {''mcpServers'': {  # You can specify the MCP configuration file\n            ''time'': {\n                ''command'': ''uvx'',\n                ''args'': [''mcp-server-time'', ''--local-timezone=Asia/Shanghai'']\n            },\n            "fetch": {\n                "command": "uvx",\n                "args": ["mcp-server-fetch"]\n            }\n        }\n    },\n  ''code_interpreter'',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{''role'': ''user'', ''content'': ''https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen''}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include "Please reason step by step, and put your final answer within \boxed{}." in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: "Please show your choice in the `answer` field with only the choice letter, e.g., `"answer": "C"`."\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":751632384,"storage_bytes":4522815806,"files_count":10,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen3ForCausalLM"],"model_type":"qwen3","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(''<tool_response>'') and message.content.endswith(''</tool_response>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if ''</think>'' in content %}\n                {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n                {%- set content = content.split(''</think>'')[-1].lstrip(''\\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- ''<|im_start|>'' + message.role + ''\\n<think>\\n'' + reasoning_content.strip(''\\n'') + ''\\n</think>\\n\\n'' + content.lstrip(''\\n'') }}\n            {%- else %}\n                {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- ''<think>\\n\\n</think>\\n\\n'' }}\n    {%- endif %}\n{%- endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen3","source_url":"https://github.com/QwenLM/Qwen3"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Agent","source_url":"https://github.com/QwenLM/Qwen-Agent"},{"type":"based_on_paper","target_id":"arxiv:2505.09388","source_url":"https://arxiv.org/abs/2505.09388"}]', NULL, 'Apache-2.0', 'approved', 79.3, '20650e086cdab00b49800f1a62002ca4', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Kwai-Kolors-Kolors', 'huggingface--kwai-kolors--kolors', 'Kolors', 'Kwai-Kolors', '--- license: apache-2.0 language: - zh - en tags: - text-to-image - stable-diffusion - kolors --- <div align="center" style="display: flex; justify-content: center; flex-wrap: wrap;"> <a href="https://github.com/Kwai-Kolors/Kolors"><img src="https://img.shields.io/static/v1?label=Kolors Code&message=Github&color=blue&logo=github-pages"></a> &ensp; <a href="https://kwai-kolors.github.io/"><img src="https://img.shields.io/static/v1?label=Team%20Page&message=Page&color=green"></a> &ensp; <a href...', '["diffusers","safetensors","text-to-image","stable-diffusion","kolors","zh","en","license:apache-2.0","endpoints_compatible","diffusers:stablediffusionxlpipeline","region:us"]', 'text-to-image', 848, 494, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Kwai-Kolors/Kolors","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n  - zh\n  - en\ntags:\n  - text-to-image\n  - stable-diffusion\n  - kolors\n---\n# Kolors: Effective Training of Diffusion Model for Photorealistic Text-to-Image Synthesis\n<div align="center" style="display: flex; justify-content: center; flex-wrap: wrap;">\n  <a href="https://github.com/Kwai-Kolors/Kolors"><img src="https://img.shields.io/static/v1?label=Kolors Code&message=Github&color=blue&logo=github-pages"></a> &ensp;\n  <a href="https://kwai-kolors.github.io/"><img src="https://img.shields.io/static/v1?label=Team%20Page&message=Page&color=green"></a> &ensp;\n  <a href="https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/Kolors_paper.pdf"><img src="https://img.shields.io/static/v1?label=Tech Report&message=Arxiv:Kolors&color=red&logo=arxiv"></a> &ensp;\n  <a href="https://kolors.kuaishou.com/"><img src="https://img.shields.io/static/v1?label=Official Website&message=Page&color=green"></a>\n</div>\n<figure>\n  <img src="imgs/head_final3.png">\n</figure>\n<br>\n\n## üìñ Introduction\nKolors is a large-scale text-to-image generation model based on latent diffusion, developed by the Kuaishou Kolors team. Trained on billions of text-image pairs, Kolors exhibits significant advantages over both open-source and proprietary models in visual quality, complex semantic accuracy, and text rendering for both Chinese and English characters. Furthermore, Kolors supports both Chinese and English inputs, demonstrating strong performance in understanding and generating Chinese-specific content. For more details, please refer to this <a href="https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/Kolors_paper.pdf">technical report</a></b>.\n\n\n## üöÄ Quick Start\n### Requirements\n\n* Python 3.8 or later\n* PyTorch 1.13.1 or later\n* Transformers 4.26.1 or later\n* Recommended: CUDA 11.7 or later\n<br>\n\n1. Repository cloning and dependency installation\n\n```bash\napt-get install git-lfs\ngit clone https://github.com/Kwai-Kolors/Kolors\ncd Kolors\nconda create --name kolors python=3.8\nconda activate kolors\npip install -r requirements.txt\npython3 setup.py install\n```\n2. Weights downloadÔºà[link](https://huggingface.co/Kwai-Kolors/Kolors)ÔºâÔºö\n```bash\nhuggingface-cli download --resume-download Kwai-Kolors/Kolors --local-dir weights/Kolors\n```\nor\n```bash\ngit lfs clone https://huggingface.co/Kwai-Kolors/Kolors weights/Kolors\n```\n3. InferenceÔºö\n```bash\npython3 scripts/sample.py "‰∏ÄÂº†Áì¢Ëô´ÁöÑÁÖßÁâáÔºåÂæÆË∑ùÔºåÂèòÁÑ¶ÔºåÈ´òË¥®ÈáèÔºåÁîµÂΩ±ÔºåÊãøÁùÄ‰∏Ä‰∏™ÁâåÂ≠êÔºåÂÜôÁùÄ‚ÄúÂèØÂõæ‚Äù"\n# The image will be saved to "scripts/outputs/sample_test.jpg"\n```\n\n### Using with Diffusers\nPlease refer to https://huggingface.co/Kwai-Kolors/Kolors-diffusers.\n\n## üìú License&Citation\n### License\nKolors are fully open-sourced for academic research. For commercial use, please fill out this [questionnaire](https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/ÂèØÂõæKOLORSÊ®°ÂûãÂïÜ‰∏öÊéàÊùÉÁî≥ËØ∑‰π¶.docx) and sent it to kwai-kolors@kuaishou.com for registration.\n\nWe open-source Kolors to promote the development of large text-to-image models in collaboration with the open-source community. The code of this project is open-sourced under the Apache-2.0 license. We sincerely urge all developers and users to strictly adhere to the [open-source license](MODEL_LICENSE), avoiding the use of the open-source model, code, and its derivatives for any purposes that may harm the country and society or for any services not evaluated and registered for safety. Note that despite our best efforts to ensure the compliance, accuracy, and safety of the data during training, due to the diversity and combinability of generated content and the probabilistic randomness affecting the model, we cannot guarantee the accuracy and safety of the output content, and the model is susceptible to misleading. This project does not assume any legal responsibility for any data security issues, public opinion risks, or risks and liabilities arising from the model being misled, abused, misused, or improperly utilized due to the use of the open-source model and code.\n\n\n### Citation\nIf you find our work helpful, please cite it!\n\n```\n@article{kolors,\n  title={Kolors: Effective Training of Diffusion Model for Photorealistic Text-to-Image Synthesis},\n  author={Kolors Team},\n  journal={arXiv preprint},\n  year={2024}\n}\n```\n\n### Acknowledgments\n- Thanks to [Diffusers](https://github.com/huggingface/diffusers) for providing the codebase.\n- Thanks to [ChatGLM3](https://github.com/THUDM/ChatGLM3) for providing the powerful Chinese language model.\n<br>\n\n### Contact Us\n\nIf you want to leave a message for our R&D team and product team, feel free to join our [WeChat group](https://github.com/Kwai-Kolors/Kolors/blob/master/imgs/wechat.png). You can also contact us via email (kwai-kolors@kuaishou.com).\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":62041008151,"files_count":42,"spaces_count":92,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionXLPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:Kwai-Kolors:Kolors\"><img","source_url":"https://github.com/Kwai-Kolors/Kolors\"><img"},{"type":"has_code","target_id":"github:Kwai-Kolors:Kolors","source_url":"https://github.com/Kwai-Kolors/Kolors"},{"type":"has_code","target_id":"github:Kwai-Kolors:Kolors","source_url":"https://github.com/Kwai-Kolors/Kolors"},{"type":"has_code","target_id":"github:Kwai-Kolors:Kolors","source_url":"https://github.com/Kwai-Kolors/Kolors"},{"type":"has_code","target_id":"github:Kwai-Kolors:Kolors","source_url":"https://github.com/Kwai-Kolors/Kolors"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:THUDM:ChatGLM3","source_url":"https://github.com/THUDM/ChatGLM3"},{"type":"has_code","target_id":"github:Kwai-Kolors:Kolors","source_url":"https://github.com/Kwai-Kolors/Kolors"}]', NULL, 'Apache-2.0', 'approved', 64.3, '836138fa4026e179a67c28e04536d690', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-TencentARC-T2I-Adapter', 'huggingface--tencentarc--t2i-adapter', 'T2I-Adapter', 'TencentARC', 'üè∞**Adapter Zoo** **|** üé®**Demos** **|** üü†**GitHub** T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models The GitHub repo: <https://github.com/TencentARC/T2I-Adapter> Please find the model information in https://github.com/TencentARC/T2I-Adapter/blob/main/docs/AdapterZoo.md --- license: apache-2.0 ---', '["arxiv:2302.08453","region:us"]', 'other', 842, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/TencentARC/T2I-Adapter","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '# Models for T2I-Adapter\n\n[![Huggingface Gradio](https://img.shields.io/static/v1?label=Demo&message=Huggingface%20Gradio&color=orange)](https://huggingface.co/spaces/Adapter/T2I-Adapter) \n\nüè∞[**Adapter Zoo**](https://github.com/TencentARC/T2I-Adapter/blob/main/docs/AdapterZoo.md)  **|** üé®[**Demos**](https://github.com/TencentARC/T2I-Adapter/blob/main/docs/examples.md) **|** üü†[**GitHub**](https://github.com/TencentARC/T2I-Adapter)\n\n[T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.08453)\n\nThe GitHub repo: <https://github.com/TencentARC/T2I-Adapter>\n\n## Model Description\n\nPlease find the model information in https://github.com/TencentARC/T2I-Adapter/blob/main/docs/AdapterZoo.md\n\n---\nlicense: apache-2.0\n---\n', '{"pipeline_tag":null,"library_name":null,"framework":null,"params":null,"storage_bytes":5804660648,"files_count":88,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:TencentARC:T2I-Adapter","source_url":"https://github.com/TencentARC/T2I-Adapter"},{"type":"has_code","target_id":"github:TencentARC:T2I-Adapter","source_url":"https://github.com/TencentARC/T2I-Adapter"},{"type":"has_code","target_id":"github:TencentARC:T2I-Adapter","source_url":"https://github.com/TencentARC/T2I-Adapter"},{"type":"has_code","target_id":"github:TencentARC:T2I-Adapter>","source_url":"https://github.com/TencentARC/T2I-Adapter>"},{"type":"has_code","target_id":"github:TencentARC:T2I-Adapter","source_url":"https://github.com/TencentARC/T2I-Adapter"},{"type":"based_on_paper","target_id":"arxiv:2302.08453","source_url":"https://arxiv.org/abs/2302.08453"}]', NULL, NULL, 'pending', 59.3, 'cd2c215b4baa7fe9a06209be449ec202', NULL, 'https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/assets/canny.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-TencentARC-T2I-Adapter from https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/assets/canny.png
Image converted to WebP: data/images/huggingface-TencentARC-T2I-Adapter.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-mosaicml-mpt-7b-storywriter', 'huggingface--mosaicml--mpt-7b-storywriter', 'mpt-7b-storywriter', 'mosaicml', '--- license: apache-2.0 tags: - Composer - MosaicML - llm-foundry datasets: - the_pile_books3 inference: false --- MPT-7B-StoryWriter-65k+ is a model designed to read and write fictional stories with super long context lengths. It was built by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the books3 dataset. At inference time, thanks to ALiBi, MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens. We demonstrate generations as long as 84k token...', '["transformers","pytorch","mpt","text-generation","composer","mosaicml","llm-foundry","custom_code","dataset:the_pile_books3","arxiv:2108.12409","arxiv:2205.14135","arxiv:2302.06675","license:apache-2.0","text-generation-inference","region:us"]', 'text-generation', 841, 1832, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/mosaicml/mpt-7b-storywriter","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ntags:\n- Composer\n- MosaicML\n- llm-foundry\ndatasets:\n- the_pile_books3\ninference: false\n---\n\n# MPT-7B-StoryWriter-65k+\n\nMPT-7B-StoryWriter-65k+ is a model designed to read and write fictional stories with super long context lengths.\nIt was built by finetuning MPT-7B with a context length of 65k tokens on a filtered fiction subset of the [books3 dataset](https://huggingface.co/datasets/the_pile_books3).\nAt inference time, thanks to [ALiBi](https://arxiv.org/abs/2108.12409), MPT-7B-StoryWriter-65k+ can extrapolate even beyond 65k tokens.\nWe demonstrate generations as long as 84k tokens on a single node of 8 A100-80GB GPUs in our [blogpost](https://www.mosaicml.com/blog/mpt-7b).\n  * License: Apache 2.0\n\nThis model was trained by [MosaicML](https://www.mosaicml.com) and follows a modified decoder-only transformer architecture.\n\n## Model Date\n\nMay 5, 2023\n\n## Model License\n\nApache 2.0\n\n## Documentation\n\n* [Blog post: Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b)\n* [Codebase (mosaicml/llm-foundry repo)](https://github.com/mosaicml/llm-foundry/)\n* Questions: Feel free to contact us via the [MosaicML Community Slack](https://mosaicml.me/slack)!\n\n\n## How to Use\n\nNote: This model requires that `trust_remote_code=True` be passed to the `from_pretrained` method. This is because we use a custom model architecture that is not yet part of the `transformers` package.\n\nIt includes options for many training efficiency features such as [FlashAttention (Dao et al. 2022)](https://arxiv.org/pdf/2205.14135.pdf), [ALiBi](https://arxiv.org/abs/2108.12409), QK LayerNorm, and more.\n\n```python\nimport transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  ''mosaicml/mpt-7b-storywriter'',\n  trust_remote_code=True\n)\n```\n\nTo use the optimized [triton implementation](https://github.com/openai/triton) of FlashAttention, you can load the model on GPU (`cuda:0`) with `attn_impl=''triton''` and with `bfloat16` precision:\n```python\nimport torch\nimport transformers\n\nname = ''mosaicml/mpt-7b-storywriter''\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.attn_config[''attn_impl''] = ''triton''\nconfig.init_device = ''cuda:0'' # For fast initialization directly on GPU!\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n  trust_remote_code=True\n)\n```\n\nAlthough the model was trained with a sequence length of 2048 and finetuned with a sequence length of 65536,\nALiBi enables users to increase the maximum sequence length during finetuning and/or inference. For example:\n```python\nimport transformers\n\nname = ''mosaicml/mpt-7b''\n\nconfig = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\nconfig.max_seq_len = 83968 # (input + output) tokens can now be up to 83968\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n  name,\n  config=config,\n  trust_remote_code=True\n)\n```\n\nThis model was trained with the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer.\n\n```python\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neox-20b")\n```\n\nThe model can then be used, for example, within a text-generation pipeline.  \nNote: when running Torch modules in lower precision, it is best practice to use the [torch.autocast context manager](https://pytorch.org/docs/stable/amp.html).\n\n```python\nfrom transformers import pipeline\n\npipe = pipeline(''text-generation'', model=model, tokenizer=tokenizer, device=''cuda:0'')\n\nwith torch.autocast(''cuda'', dtype=torch.bfloat16):\n    print(\n        pipe(''Here is a recipe for vegan banana bread:\n'',\n            max_new_tokens=100,\n            do_sample=True,\n            use_cache=True))\n```\n\n## Community-Created Resources\n\nThese were not created by MosaicML, but you may find them useful. These links are not an endorsement of the creators or their content.\n\n- [Oobabooga Running MPT-7B-Storywriter](https://youtu.be/QVVb6Md6huA)\n- [NEW MPT-7B-StoryWriter CRUSHES GPT-4!](https://www.youtube.com/watch?v=O9Y_ZdsuKWQ&t=649s) - Has a long section on running locally using Oobabooga\n\n## Example Epilogue\n\nThe full text of the _The Great Gatsby_ (67873 tokens) was fed to the model, followed by the text "EPILOGUE"\n\n**TheRealFitzgerald233**:\n\n> ...\n>\n> So we beat on, boats against the current, borne back ceaselessly into the past\n>\n> EPILOGUE\n\n**MPT-7B-StoryWriter**:\n\n>I have lived so long in the shadow of this dream that I may never see the bright sunlight of reality. I may look up some morning and find that I am six feet under, and that Gatsby, who gave me the dream and everything else, will have vanished behind me.\n>\n>And so it was. I had hardly crossed the threshold of the old world when my eye caught the headlines of a newspaper, and I knew at once that my dream was over. I knew that Gatsby was dead.\n>\n>And I knew that day that I didn''t love her.\n>\n>That night I took the last train for West Egg, and when I reached home I went directly to my attic and brought down all Gatsby''s letters.\n>\n>I have sorted them by date and placed them in the order in which they were written. They are very few in number.\n>\n>They are dated between April of nineteen eighteen and January of nineteen nineteen, and the last one I have here is the one that told me he was dead.\n>\n>There is something in that last letter that frightens me. It is the sense of a finality, of a completeness. I never heard of him after that, and I was glad.\n>\n>But a few months ago I read somewhere that he had been seen in New York. He had become a great man.\n>\n>And I knew that he had not changed at all.\n\n## Model Description\n\nThe architecture is a modification of a standard decoder-only transformer.\n\nThe model has been modified from a standard transformer in the following ways:\n* It uses [FlashAttention](https://arxiv.org/pdf/2205.14135.pdf)\n* It uses [ALiBi (Attention with Linear Biases)](https://arxiv.org/abs/2108.12409) and does not use positional embeddings\n* It does not use biases\n\n\n| Hyperparameter | Value |\n|----------------|-------|\n|n_parameters | 6.7B |\n|n_layers | 32 |\n| n_heads | 32 |\n| d_model | 4096 |\n| vocab size | 50432 |\n| sequence length | **65536** |\n\n## PreTraining Data\n\nFor more details on the pretraining process, see [MPT-7B](https://huggingface.co/mosaicml/mpt-7b).\n\nThe data was tokenized using the [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b) tokenizer.\n\n### Training Configuration\n\nThis model was trained on 8 A100-80GBs for about 2 days using the [MosaicML Platform](https://www.mosaicml.com/platform).\nThe model was trained with sharded data parallelism using [FSDP](https://pytorch.org/docs/stable/fsdp.html) and used the [LION](https://arxiv.org/abs/2302.06675) optimizer.\n\n## Limitations and Biases\n\n_The following language is modified from [EleutherAI''s GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b)_\n\nMPT-7B-StoryWriter can produce factually incorrect output, and should not be relied on to produce factually accurate information.\nMPT-7B-StoryWriter was trained on various public datasets.\nWhile great efforts have been taken to clean the pretraining data, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\n\n\n## Acknowledgements\n\nThis model was finetuned by Alex Trott and the MosaicML NLP team\n\n## MosaicML Platform\n\nIf you''re interested in [training](https://www.mosaicml.com/training) and [deploying](https://www.mosaicml.com/inference) your own MPT or LLMs on the MosaicML Platform, [sign up here](https://forms.mosaicml.com/demo?utm_source=huggingface&utm_medium=referral&utm_campaign=mpt-7b).\n\n## Disclaimer\n\nThe license on this model does not constitute legal advice. We are not responsible for the actions of third parties who use this model. Please cosult an attorney before using this model for commercial purposes.\n\n\n## Citation\n\nPlease cite this model using the following format:\n\n```\n@online{MosaicML2023Introducing,\n    author    = {MosaicML NLP Team},\n    title     = {Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs},\n    year      = {2023},\n    url       = {www.mosaicml.com/blog/mpt-7b},\n    note      = {Accessed: 2023-03-28}, % change this date\n    urldate   = {2023-03-28} % change this date\n}\n```\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":26597235302,"files_count":26,"spaces_count":47,"gated":false,"private":false,"config":{"architectures":["MPTForCausalLM"],"auto_map":{"AutoConfig":"configuration_mpt.MPTConfig","AutoModelForCausalLM":"modeling_mpt.MPTForCausalLM"},"model_type":"mpt","tokenizer_config":{"bos_token":"<|endoftext|>","eos_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:mosaicml:llm-foundry","source_url":"https://github.com/mosaicml/llm-foundry"},{"type":"has_code","target_id":"github:openai:triton","source_url":"https://github.com/openai/triton"},{"type":"based_on_paper","target_id":"arxiv:2108.12409","source_url":"https://arxiv.org/abs/2108.12409"},{"type":"based_on_paper","target_id":"arxiv:2205.14135","source_url":"https://arxiv.org/abs/2205.14135"},{"type":"based_on_paper","target_id":"arxiv:2302.06675","source_url":"https://arxiv.org/abs/2302.06675"}]', NULL, 'Apache-2.0', 'approved', 64.3, 'cae3984079994fc809a6dfa4145e96b8', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-baichuan-inc-Baichuan-7B', 'huggingface--baichuan-inc--baichuan-7b', 'Baichuan-7B', 'baichuan-inc', '--- language: - zh - en pipeline_tag: text-generation inference: false --- <!-- Provide a quick summary of what the model is/does. --> Baichuan-7BÊòØÁî±ÁôæÂ∑ùÊô∫ËÉΩÂºÄÂèëÁöÑ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÊ®°Âûã„ÄÇÂü∫‰∫éTransformerÁªìÊûÑÔºåÂú®Â§ßÁ∫¶1.2‰∏á‰∫øtokens‰∏äËÆ≠ÁªÉÁöÑ70‰∫øÂèÇÊï∞Ê®°ÂûãÔºåÊîØÊåÅ‰∏≠Ëã±ÂèåËØ≠Ôºå‰∏ä‰∏ãÊñáÁ™óÂè£ÈïøÂ∫¶‰∏∫4096„ÄÇÂú®Ê†áÂáÜÁöÑ‰∏≠ÊñáÂíåËã±ÊñáÊùÉÂ®ÅbenchmarkÔºàC-EVAL/MMLUÔºâ‰∏äÂùáÂèñÂæóÂêåÂ∞∫ÂØ∏ÊúÄÂ•ΩÁöÑÊïàÊûú„ÄÇ Â¶ÇÊûúÂ∏åÊúõ‰ΩøÁî®Baichuan-7BÔºàÂ¶ÇËøõË°åÊé®ÁêÜ„ÄÅFinetuneÁ≠âÔºâÔºåÊàë‰ª¨Êé®Ëçê‰ΩøÁî®ÈÖçÂ•ó‰ª£Á†ÅÂ∫ìBaichuan-7B„ÄÇ Baichuan-7B is an open-source large-scale pre-trained model developed by Baichuan Intelligent Technology. Based on the Transformer architecture, it is a model w...', '["transformers","pytorch","baichuan","text-generation","custom_code","zh","en","arxiv:1910.07467","arxiv:2009.03300","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 841, 65083, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/baichuan-inc/Baichuan-7B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- zh\n- en\npipeline_tag: text-generation\ninference: false\n---\n# Baichuan-7B\n\n<!-- Provide a quick summary of what the model is/does. -->\n\nBaichuan-7BÊòØÁî±ÁôæÂ∑ùÊô∫ËÉΩÂºÄÂèëÁöÑ‰∏Ä‰∏™ÂºÄÊ∫êÁöÑÂ§ßËßÑÊ®°È¢ÑËÆ≠ÁªÉÊ®°Âûã„ÄÇÂü∫‰∫éTransformerÁªìÊûÑÔºåÂú®Â§ßÁ∫¶1.2‰∏á‰∫øtokens‰∏äËÆ≠ÁªÉÁöÑ70‰∫øÂèÇÊï∞Ê®°ÂûãÔºåÊîØÊåÅ‰∏≠Ëã±ÂèåËØ≠Ôºå‰∏ä‰∏ãÊñáÁ™óÂè£ÈïøÂ∫¶‰∏∫4096„ÄÇÂú®Ê†áÂáÜÁöÑ‰∏≠ÊñáÂíåËã±ÊñáÊùÉÂ®ÅbenchmarkÔºàC-EVAL/MMLUÔºâ‰∏äÂùáÂèñÂæóÂêåÂ∞∫ÂØ∏ÊúÄÂ•ΩÁöÑÊïàÊûú„ÄÇ\n\nÂ¶ÇÊûúÂ∏åÊúõ‰ΩøÁî®Baichuan-7BÔºàÂ¶ÇËøõË°åÊé®ÁêÜ„ÄÅFinetuneÁ≠âÔºâÔºåÊàë‰ª¨Êé®Ëçê‰ΩøÁî®ÈÖçÂ•ó‰ª£Á†ÅÂ∫ì[Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B)„ÄÇ\n\nBaichuan-7B is an open-source large-scale pre-trained model developed by Baichuan Intelligent Technology. Based on the Transformer architecture, it is a model with 7 billion parameters trained on approximately 1.2 trillion tokens. It supports both Chinese and English, with a context window length of 4096. It achieves the best performance of its size on standard Chinese and English authoritative benchmarks (C-EVAL/MMLU).\n\nIf you wish to use Baichuan-7B (for inference, finetuning, etc.), we recommend using the accompanying code library [Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B).\n\n## Why use Baichuan-7B\n\n- Âú®ÂêåÂ∞∫ÂØ∏Ê®°Âûã‰∏≠Baichuan-7BËææÂà∞‰∫ÜÁõÆÂâçSOTAÁöÑÊ∞¥Âπ≥ÔºåÂèÇËÄÉ‰∏ãÈù¢MMLUÊåáÊ†á\n- Baichuan-7B‰ΩøÁî®Ëá™ÊúâÁöÑ‰∏≠Ëã±ÊñáÂèåËØ≠ËØ≠ÊñôËøõË°åËÆ≠ÁªÉÔºåÂú®‰∏≠Êñá‰∏äËøõË°å‰ºòÂåñÔºåÂú®C-EvalËææÂà∞SOTAÊ∞¥Âπ≥\n- ‰∏çÂêå‰∫éLLaMAÂÆåÂÖ®Á¶ÅÊ≠¢ÂïÜ‰∏ö‰ΩøÁî®ÔºåBaichuan-7B‰ΩøÁî®Êõ¥ÂÆΩÊùæÁöÑÂºÄÊ∫êÂçèËÆÆÔºåÂÖÅËÆ∏Áî®‰∫éÂïÜ‰∏öÁõÆÁöÑ\n\n- Among models of the same size, Baichuan-7B has achieved the current state-of-the-art (SOTA) level, as evidenced by the following MMLU metrics.\n- Baichuan-7B is trained on proprietary bilingual Chinese-English corpora, optimized for Chinese, and achieves SOTA performance on C-Eval.\n- Unlike LLaMA, which completely prohibits commercial use, Baichuan-7B employs a more lenient open-source license, allowing for commercial purposes.\n\n## How to Get Started with the Model\n\nÂ¶Ç‰∏ãÊòØ‰∏Ä‰∏™‰ΩøÁî®Baichuan-7BËøõË°å1-shotÊé®ÁêÜÁöÑ‰ªªÂä°ÔºåÊ†πÊçÆ‰ΩúÂìÅÁªôÂá∫‰ΩúËÄÖÂêçÔºåÊ≠£Á°ÆËæìÂá∫‰∏∫"Â§úÈõ®ÂØÑÂåó->ÊùéÂïÜÈöê"\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained("baichuan-inc/Baichuan-7B", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan-7B", device_map="auto", trust_remote_code=True)\ninputs = tokenizer(''ÁôªÈπ≥ÈõÄÊ•º->Áéã‰πãÊ∂£\nÂ§úÈõ®ÂØÑÂåó->'', return_tensors=''pt'')\ninputs = inputs.to(''cuda:0'')\npred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n```\n\nThe following is a task of performing 1-shot inference using Baichuan-7B, where the author''s name is given based on the work, with the correct output being "One Hundred Years of Solitude->Gabriel Garcia Marquez"\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained("baichuan-inc/Baichuan-7B", trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained("baichuan-inc/Baichuan-7B", device_map="auto", trust_remote_code=True)\ninputs = tokenizer(''Hamlet->Shakespeare\nOne Hundred Years of Solitude->'', return_tensors=''pt'')\ninputs = inputs.to(''cuda:0'')\npred = model.generate(**inputs, max_new_tokens=64,repetition_penalty=1.1)\nprint(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))\n```\n\n## Model Details\n\n### Model Description\n\n<!-- Provide a longer summary of what this model is. -->\n\n- **Developed by:** ÁôæÂ∑ùÊô∫ËÉΩ(Baichuan Intelligent Technology)\n- **Email**: opensource@baichuan-inc.com\n- **Language(s) (NLP):** Chinese/English\n- **License:** [Baichuan-7B License](https://huggingface.co/baichuan-inc/Baichuan-7B/blob/main/baichuan-7B%20%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf)\n\n### Model Sources\n\n<!-- Provide the basic links for the model. -->\n\nÊï¥‰ΩìÊ®°ÂûãÂü∫‰∫éÊ†áÂáÜÁöÑTransformerÁªìÊûÑÔºåÊàë‰ª¨ÈááÁî®‰∫ÜÂíåLLaMA‰∏ÄÊ†∑ÁöÑÊ®°ÂûãËÆæËÆ°\n- **Position Embedding**ÔºöÈááÁî®rotary-embeddingÔºåÊòØÁé∞Èò∂ÊÆµË¢´Â§ßÂ§öÊï∞Ê®°ÂûãÈááÁî®ÁöÑ‰ΩçÁΩÆÁºñÁ†ÅÊñπÊ°àÔºåÂÖ∑ÊúâÂæàÂ•ΩÁöÑÂ§ñÊé®ÊÄß„ÄÇ\n- **Feedforward Layer**ÔºöÈááÁî®SwiGLUÔºåFeedforwardÂèòÂåñ‰∏∫(8/3)ÂÄçÁöÑÈöêÂê´Â±ÇÂ§ßÂ∞èÔºåÂç≥11008„ÄÇ\n- **Layer Normalization**: Âü∫‰∫é[RMSNorm](https://arxiv.org/abs/1910.07467)ÁöÑPre-Normalization„ÄÇ\n\nÂÖ∑‰ΩìÂèÇÊï∞ÂíåËßÅ‰∏ãË°®\n| Hyperparameter | Value |\n|----------------|-------|\n|n_parameters | 7000559616 |\n|n_layers | 32 |\n| n_heads | 32 |\n| d_model | 4096 |\n| vocab size | 64000 |\n| sequence length | 4096 |\n\nThe overall model is based on the standard Transformer structure, and we have adopted the same model design as LLaMA:\n\n- Position Embedding: We use rotary-embedding, which is the position encoding scheme adopted by most models at this stage, and it has excellent extrapolation capabilities.\n- Feedforward Layer: We use SwiGLU. The feedforward changes to (8/3) times the size of the hidden layer, that is, 11008.\n- Layer Normalization: Pre-Normalization based on [RMSNorm](https://arxiv.org/abs/1910.07467).\n\nThe specific parameters are as follows:\n| Hyperparameter | Value |\n|----------------|-------|\n|n_parameters | 7000559616 |\n|n_layers | 32 |\n| n_heads | 32 |\n| d_model | 4096 |\n| vocab size | 64000 |\n| sequence length | 4096 |\n\n## Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n### Downstream Use \n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\nÊàë‰ª¨ÂêåÊó∂ÂºÄÊ∫êÂá∫‰∫ÜÂíåÊú¨Ê®°ÂûãÈÖçÂ•óÁöÑËÆ≠ÁªÉ‰ª£Á†ÅÔºåÂÖÅËÆ∏ËøõË°åÈ´òÊïàÁöÑFinetuneÁî®‰∫é‰∏ãÊ∏∏‰ªªÂä°ÔºåÂÖ∑‰ΩìÂèÇËßÅ[Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B)„ÄÇ\n\nWe have also open-sourced the training code that accompanies this model, allowing for efficient finetuning for downstream tasks. For more details, please refer to [Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B).\n\n### Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\nÂú®Ê≤°ÊúâÂÖÖÂàÜËØÑ‰º∞È£éÈô©ÂíåÈááÂèñÁºìËß£Êé™ÊñΩÁöÑÊÉÖÂÜµ‰∏ãÊäïÂÖ•Áîü‰∫ß‰ΩøÁî®Ôºõ‰ªª‰ΩïÂèØËÉΩË¢´ËßÜ‰∏∫‰∏çË¥üË¥£‰ªªÊàñÊúâÂÆ≥ÁöÑ‰ΩøÁî®Ê°à‰æã„ÄÇ\n\nProduction use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful.\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\nBaichuan-7BÂèØËÉΩ‰ºö‰∫ßÁîü‰∫ãÂÆû‰∏ä‰∏çÊ≠£Á°ÆÁöÑËæìÂá∫Ôºå‰∏çÂ∫î‰æùËµñÂÆÉ‰∫ßÁîü‰∫ãÂÆû‰∏äÂáÜÁ°ÆÁöÑ‰ø°ÊÅØ„ÄÇBaichuan-7BÊòØÂú®ÂêÑÁßçÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜ‰∏äËøõË°åËÆ≠ÁªÉÁöÑ„ÄÇÂ∞ΩÁÆ°Êàë‰ª¨Â∑≤ÁªèÂÅöÂá∫‰∫ÜÂ∑®Â§ßÁöÑÂä™ÂäõÊù•Ê∏ÖÊ¥óÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÔºå‰ΩÜËøô‰∏™Ê®°ÂûãÂèØËÉΩ‰ºöÁîüÊàêÊ∑´ÁßΩ„ÄÅÂÅèËßÅÊàñÂÖ∂‰ªñÂÜíÁäØÊÄßÁöÑËæìÂá∫„ÄÇ\n\nBaichuan-7B can produce factually incorrect output, and should not be relied on to produce factually accurate information. Baichuan-7B was trained on various public datasets. While great efforts have been taken to clean the pretraining data, it is possible that this model could generate lewd, biased or otherwise offensive outputs.\n\n## Training Details\n\nËÆ≠ÁªÉÂÖ∑‰ΩìËÆæÁΩÆÂèÇËßÅ[Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B)„ÄÇ\n\nFor specific training settings, please refer to [Baichuan-7B](https://github.com/baichuan-inc/Baichuan-7B).\n\n## Evaluation\n\n### ‰∏≠ÊñáËØÑÊµã\n#### C-Eval\n[CEvalÊï∞ÊçÆÈõÜ](https://cevalbenchmark.com/index.html)ÊòØ‰∏Ä‰∏™ÂÖ®Èù¢ÁöÑ‰∏≠ÊñáÂü∫Á°ÄÊ®°ÂûãËØÑÊµãÊï∞ÊçÆÈõÜÔºåÊ∂µÁõñ‰∫Ü52‰∏™Â≠¶ÁßëÂíåÂõõ‰∏™ÈöæÂ∫¶ÁöÑÁ∫ßÂà´„ÄÇÊàë‰ª¨‰ΩøÁî®ËØ•Êï∞ÊçÆÈõÜÁöÑdevÈõÜ‰Ωú‰∏∫few-shotÁöÑÊù•Ê∫êÔºåÂú®testÈõÜ‰∏äËøõË°å‰∫Ü5-shotÊµãËØï„ÄÇ\n\n\n| Model 5-shot                | Average | Avg(Hard) | STEM | Social Sciences | Humanities | Others |\n|-----------------------------|---------|-----------|------|-----------------|------------|--------|\n| GPT-4                       | 68.7    | 54.9      | 67.1 | 77.6            | 64.5       | 67.8   |\n| ChatGPT                     | 54.4    | 41.4      | 52.9 | 61.8            | 50.9       | 53.6   |\n| Claude-v1.3                 | 54.2    | 39.0      | 51.9 | 61.7            | 52.1       | 53.7   |\n| Claude-instant-v1.0         | 45.9    | 35.5      | 43.1 | 53.8            | 44.2       | 45.4   |\n| moss-moon-003-base (16B)    | 27.4    | 24.5      | 27.0 | 29.1            | 27.2       | 26.9   |\n| Ziya-LLaMA-13B-pretrain     | 30.2    | 22.7      | 27.7 | 34.4            | 32.0       | 28.9   |\n| LLaMA-7B-hf                 | 27.1    | 25.9      | 27.1 | 26.8            | 27.9       | 26.3   |\n| ChatGLM-6B                  | 34.5    | 23.1      | 30.4 | 39.6            | 37.4       | 34.5   |\n| Falcon-7B                   | 25.8    | 24.3      | 25.8 | 26.0            | 25.8       | 25.6   |\n| Open-LLaMA-v2-pretrain (7B) | 24.0    | 22.5      | 23.1 | 25.3            | 25.2       | 23.2   |\n| TigerBot-7B-base            | 25.7    | 27.0      | 27.3 | 24.7            | 23.4       | 26.1   |\n| Aquila-7B<sup>*</sup>       | 25.5    | 25.2      | 25.6 | 24.6            | 25.2       | 26.6   |\n| BLOOM-7B                    | 22.8    | 20.2      | 21.8 | 23.3            | 23.9       | 23.3   |\n| BLOOMZ-7B                   | 35.7    | 25.8      | 31.3 | 43.5            | 36.6       | 35.6   |\n| **Baichuan-7B**             | 42.8    | 31.5      | 38.2 | 52.0            | 46.2       | 39.3   |\n\n\n#### Gaokao\n[Gaokao](https://github.com/ExpressAI/AI-Gaokao) ÊòØ‰∏Ä‰∏™‰ª•‰∏≠ÂõΩÈ´òËÄÉÈ¢ò‰Ωú‰∏∫ËØÑÊµãÂ§ßËØ≠Ë®ÄÊ®°ÂûãËÉΩÂäõÁöÑÊï∞ÊçÆÈõÜÔºåÁî®‰ª•ËØÑ‰º∞Ê®°ÂûãÁöÑËØ≠Ë®ÄËÉΩÂäõÂíåÈÄªËæëÊé®ÁêÜËÉΩÂäõ„ÄÇ\nÊàë‰ª¨Âè™‰øùÁïô‰∫ÜÂÖ∂‰∏≠ÁöÑÂçïÈ°πÈÄâÊã©È¢òÔºåÂπ∂ÂØπÊâÄÊúâÊ®°ÂûãËøõË°åÁªü‰∏Ä5-shotÊµãËØï„ÄÇ\n\n‰ª•‰∏ãÊòØÊµãËØïÁöÑÁªìÊûú„ÄÇ\n\n| Model           | Average |\n|-------------------------|-----------------|\n| Open-LLaMA-v2-pretrain  | 21.41           |\n| Ziya-LLaMA-13B-pretrain | 23.17           |\n| Falcon-7B               | 23.98           |\n| TigerBot-7B-base        | 25.94           |\n| LLaMA-7B                | 27.81           |\n| ChatGLM-6B              | 21.41           |\n| BLOOM-7B                | 26.96           |\n| BLOOMZ-7B               | 28.72           |\n| Aquila-7B<sup>*</sup>               | 24.39           |\n| **Baichuan-7B**        | **36.24**           |\n\n\n#### AGIEval\n[AGIEval](https://github.com/microsoft/AGIEval) Êó®Âú®ËØÑ‰º∞Ê®°ÂûãÁöÑËÆ§Áü•ÂíåËß£ÂÜ≥ÈóÆÈ¢òÁõ∏ÂÖ≥ÁöÑ‰ªªÂä°‰∏≠ÁöÑ‰∏ÄËà¨ËÉΩÂäõ„ÄÇ\nÊàë‰ª¨Âè™‰øùÁïô‰∫ÜÂÖ∂‰∏≠ÁöÑÂõõÈÄâ‰∏ÄÂçïÈ°πÈÄâÊã©È¢òÔºåÈöèÊú∫ÂàíÂàÜÂêéÂØπÊâÄÊúâÊ®°ÂûãËøõË°å‰∫ÜÁªü‰∏Ä5-shotÊµãËØï„ÄÇ\n\n| Model           | Average |\n|-------------------------|-----------------|\n| Open-LLaMA-v2-pretrain  | 23.49           |\n| Ziya-LLaMA-13B-pretrain | 27.64           |\n| Falcon-7B               | 27.18           |\n| TigerBot-7B-base        | 25.19           |\n| LLaMA-7B                | 28.17           |\n| ChatGLM-6B              | 23.49           |\n| BLOOM-7B                | 26.55           |\n| BLOOMZ-7B               | 30.27           |\n| Aquila-7B<sup>*</sup>               | 25.58           |\n| **Baichuan-7B**        | **34.44**           |\n\n<sup>*</sup>ÂÖ∂‰∏≠AquilaÊ®°ÂûãÊù•Ê∫ê‰∫é[Êô∫Ê∫êÂÆòÊñπÁΩëÁ´ô](https://model.baai.ac.cn/model-detail/100098)Ôºå‰ªÖÂÅöÂèÇËÄÉ\n\n### English Leaderboard\nIn addition to Chinese, we also tested the model''s performance in English. \n\n#### MMLU\n\n[MMLU](https://arxiv.org/abs/2009.03300) is an English evaluation dataset that includes 57 multiple-choice tasks, covering elementary mathematics, American history, computer science, law, etc. The difficulty ranges from high school level to expert level, making it a mainstream LLM evaluation dataset.\n\nWe adopted the [open-source]((https://github.com/hendrycks/test)) evaluation scheme, and the final 5-shot results are as follows:\n\n| Model                                  | Humanities | Social Sciences | STEM | Other | Average |\n|----------------------------------------|-----------:|:---------------:|:----:|:-----:|:-------:|\n| LLaMA-7B<sup>2</sup>                   |       34.0 |      38.3       | 30.5 | 38.1  |  35.1   |\n| Falcon-7B<sup>1</sup>                  |          - |        -        |  -   |   -   |  35.0   |\n| mpt-7B<sup>1</sup>                     |          - |        -        |  -   |   -   |  35.6   |\n| ChatGLM-6B<sup>0</sup>                 |       35.4 |      41.0       | 31.3 | 40.5  |  36.9   |\n| BLOOM 7B<sup>0</sup>                  |       25.0 |      24.4       | 26.5 | 26.4  |  25.5   |\n| BLOOMZ 7B<sup>0</sup>                 |       31.3 |      42.1       | 34.4 | 39.0  |  36.1   |\n| moss-moon-003-base (16B)<sup>0</sup>   |       24.2 |      22.8       | 22.4 | 24.4  |  23.6   |\n| moss-moon-003-sft (16B)<sup>0</sup>    |       30.5 |      33.8       | 29.3 | 34.4  |  31.9   |\n| **Baichuan-7B<sup>0</sup>**            |       38.4 |      48.9       | 35.6 | 48.1  |  42.3   |\n\nThe superscript in the Model column indicates the source of the results.\n```\n0:reimplemented\n1:https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n2:https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu\n```\n\n## Our Group\n![WeChat](https://github.com/baichuan-inc/Baichuan-13B/blob/main/media/wechat.jpeg?raw=true)\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":28005568432,"files_count":14,"spaces_count":59,"gated":false,"private":false,"config":{"architectures":["BaiChuanForCausalLM"],"auto_map":{"AutoConfig":"configuration_baichuan.BaiChuanConfig","AutoModelForCausalLM":"modeling_baichuan.BaiChuanForCausalLM"},"model_type":"baichuan","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<s>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"</s>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":{"__type":"AddedToken","content":"<unk>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false}}}}', '[]', '[{"type":"has_code","target_id":"github:baichuan-inc:Baichuan-7B","source_url":"https://github.com/baichuan-inc/Baichuan-7B"},{"type":"has_code","target_id":"github:baichuan-inc:Baichuan-7B","source_url":"https://github.com/baichuan-inc/Baichuan-7B"},{"type":"has_code","target_id":"github:baichuan-inc:Baichuan-7B","source_url":"https://github.com/baichuan-inc/Baichuan-7B"},{"type":"has_code","target_id":"github:baichuan-inc:Baichuan-7B","source_url":"https://github.com/baichuan-inc/Baichuan-7B"},{"type":"has_code","target_id":"github:baichuan-inc:Baichuan-7B","source_url":"https://github.com/baichuan-inc/Baichuan-7B"},{"type":"has_code","target_id":"github:baichuan-inc:Baichuan-7B","source_url":"https://github.com/baichuan-inc/Baichuan-7B"},{"type":"has_code","target_id":"github:ExpressAI:AI-Gaokao","source_url":"https://github.com/ExpressAI/AI-Gaokao"},{"type":"has_code","target_id":"github:microsoft:AGIEval","source_url":"https://github.com/microsoft/AGIEval"},{"type":"has_code","target_id":"github:hendrycks:test","source_url":"https://github.com/hendrycks/test"},{"type":"has_code","target_id":"github:baichuan-inc:Baichuan-13B","source_url":"https://github.com/baichuan-inc/Baichuan-13B"},{"type":"based_on_paper","target_id":"arxiv:1910.07467","source_url":"https://arxiv.org/abs/1910.07467"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"}]', NULL, NULL, 'pending', 69.3, '3a120e58cad49e35720e6c5237b3f2ce', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-InstantX-InstantID', 'huggingface--instantx--instantid', 'InstantID', 'InstantX', '--- license: apache-2.0 language: - en library_name: diffusers pipeline_tag: text-to-image --- <div align="center"> **Project Page** **|** **Paper** **|** **Code** **|** ü§ó **Gradio demo** </div> InstantID is a new state-of-the-art tuning-free method to achieve ID-Preserving generation with only single image, supporting various downstream tasks. <div align="center"> <img src=''examples/applications.png''> </div> You can directly download the model in this repository. You also can download the m...', '["diffusers","safetensors","text-to-image","en","arxiv:2401.07519","license:apache-2.0","region:us"]', 'text-to-image', 837, 31715, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/InstantX/InstantID","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\nlibrary_name: diffusers\npipeline_tag: text-to-image\n---\n\n# InstantID Model Card\n\n<div align="center">\n\n[**Project Page**](https://instantid.github.io/) **|** [**Paper**](https://arxiv.org/abs/2401.07519) **|** [**Code**](https://github.com/InstantID/InstantID) **|** [ü§ó **Gradio demo**](https://huggingface.co/spaces/InstantX/InstantID)\n\n\n</div>\n\n## Introduction\n\nInstantID is a new state-of-the-art tuning-free method to achieve ID-Preserving generation with only single image, supporting various downstream tasks.\n\n<div  align="center">\n<img src=''examples/applications.png''>\n</div>\n\n\n## Usage\n\nYou can directly download the model in this repository.\nYou also can download the model in python script:\n\n```python\nfrom huggingface_hub import hf_hub_download\nhf_hub_download(repo_id="InstantX/InstantID", filename="ControlNetModel/config.json", local_dir="./checkpoints")\nhf_hub_download(repo_id="InstantX/InstantID", filename="ControlNetModel/diffusion_pytorch_model.safetensors", local_dir="./checkpoints")\nhf_hub_download(repo_id="InstantX/InstantID", filename="ip-adapter.bin", local_dir="./checkpoints")\n```\n\nFor face encoder, you need to manutally download via this [URL](https://github.com/deepinsight/insightface/issues/1896#issuecomment-1023867304) to `models/antelopev2`.\n\n```python\n# !pip install opencv-python transformers accelerate insightface\nimport diffusers\nfrom diffusers.utils import load_image\nfrom diffusers.models import ControlNetModel\n\nimport cv2\nimport torch\nimport numpy as np\nfrom PIL import Image\n\nfrom insightface.app import FaceAnalysis\nfrom pipeline_stable_diffusion_xl_instantid import StableDiffusionXLInstantIDPipeline, draw_kps\n\n# prepare ''antelopev2'' under ./models\napp = FaceAnalysis(name=''antelopev2'', root=''./'', providers=[''CUDAExecutionProvider'', ''CPUExecutionProvider''])\napp.prepare(ctx_id=0, det_size=(640, 640))\n\n# prepare models under ./checkpoints\nface_adapter = f''./checkpoints/ip-adapter.bin''\ncontrolnet_path = f''./checkpoints/ControlNetModel''\n\n# load IdentityNet\ncontrolnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)\n\npipe = StableDiffusionXLInstantIDPipeline.from_pretrained(\n...     "stabilityai/stable-diffusion-xl-base-1.0", controlnet=controlnet, torch_dtype=torch.float16\n... )\npipe.cuda()\n\n# load adapter\npipe.load_ip_adapter_instantid(face_adapter)\n```\n\nThen, you can customized your own face images\n\n```python\n# load an image\nimage = load_image("your-example.jpg")\n\n# prepare face emb\nface_info = app.get(cv2.cvtColor(np.array(face_image), cv2.COLOR_RGB2BGR))\nface_info = sorted(face_info, key=lambda x:(x[''bbox''][2]-x[''bbox''][0])*x[''bbox''][3]-x[''bbox''][1])[-1] # only use the maximum face\nface_emb = face_info[''embedding'']\nface_kps = draw_kps(face_image, face_info[''kps''])\n\npipe.set_ip_adapter_scale(0.8)\n\nprompt = "analog film photo of a man. faded film, desaturated, 35mm photo, grainy, vignette, vintage, Kodachrome, Lomography, stained, highly detailed, found footage, masterpiece, best quality"\nnegative_prompt = "(lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch, deformed, mutated, cross-eyed, ugly, disfigured (lowres, low quality, worst quality:1.2), (text:1.2), watermark, painting, drawing, illustration, glitch,deformed, mutated, cross-eyed, ugly, disfigured"\n\n# generate image\nimage = pipe(\n...     prompt, image_embeds=face_emb, image=face_kps, controlnet_conditioning_scale=0.8\n... ).images[0]\n```\n\nFor more details, please follow the instructions in our [GitHub repository](https://github.com/InstantID/InstantID). \n\n## Usage Tips\n1. If you''re not satisfied with the similarity, try to increase the weight of "IdentityNet Strength" and "Adapter Strength".\n2. If you feel that the saturation is too high, first decrease the Adapter strength. If it is still too high, then decrease the IdentityNet strength.\n3. If you find that text control is not as expected, decrease Adapter strength.\n4. If you find that realistic style is not good enough, go for our Github repo and use a more realistic base model.\n\n## Demos\n\n<div  align="center">\n<img src=''examples/0.png''>\n</div>\n\n<div  align="center">\n<img src=''examples/1.png''>\n</div>\n\n## Disclaimer\n\nThis project is released under Apache License and aims to positively impact the field of AI-driven image generation. Users are granted the freedom to create images using this tool, but they are obligated to comply with local laws and utilize it responsibly. The developers will not assume any responsibility for potential misuse by users.\n\n## Citation\n```bibtex\n@article{wang2024instantid,\n  title={InstantID: Zero-shot Identity-Preserving Generation in Seconds},\n  author={Wang, Qixun and Bai, Xu and Wang, Haofan and Qin, Zekui and Chen, Anthony},\n  journal={arXiv preprint arXiv:2401.07519},\n  year={2024}\n}\n```', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":4268559963,"files_count":8,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:InstantID:InstantID","source_url":"https://github.com/InstantID/InstantID"},{"type":"has_code","target_id":"github:deepinsight:insightface","source_url":"https://github.com/deepinsight/insightface"},{"type":"has_code","target_id":"github:InstantID:InstantID","source_url":"https://github.com/InstantID/InstantID"},{"type":"based_on_paper","target_id":"arxiv:2401.07519","source_url":"https://arxiv.org/abs/2401.07519"}]', NULL, 'Apache-2.0', 'approved', 64.2, '3dd66549b4eb0b6561cdb9577aa19327', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-rain1011-pyramid-flow-sd3', 'huggingface--rain1011--pyramid-flow-sd3', 'pyramid-flow-sd3', 'rain1011', '--- license: other license_name: stabilityai-ai-community license_link: LICENSE.md base_model: - stabilityai/stable-diffusion-3-medium pipeline_tag: text-to-video tags: - image-to-video - sd3 --- [[Paper]](https://arxiv.org/abs/2410.05954) [[Project Page ‚ú®]](https://pyramid-flow.github.io) [[Code üöÄ]](https://github.com/jy0205/Pyramid-Flow) [[miniFLUX Model ‚ö°Ô∏è]](https://huggingface.co/rain1011/pyramid-flow-miniflux) [demo ü§ó] This is the model repository for Pyramid Flow, a training-efficient...', '["diffusers","safetensors","image-to-video","sd3","text-to-video","arxiv:2410.05954","base_model:stabilityai/stable-diffusion-3-medium","license:other","region:us"]', 'text-to-video', 835, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/rain1011/pyramid-flow-sd3","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: stabilityai-ai-community\nlicense_link: LICENSE.md\nbase_model:\n- stabilityai/stable-diffusion-3-medium\npipeline_tag: text-to-video\ntags:\n- image-to-video\n- sd3\n\n---\n\n# ‚ö°Ô∏èPyramid Flow SD3‚ö°Ô∏è\n\n[[Paper]](https://arxiv.org/abs/2410.05954) [[Project Page ‚ú®]](https://pyramid-flow.github.io) [[Code üöÄ]](https://github.com/jy0205/Pyramid-Flow) [[miniFLUX Model ‚ö°Ô∏è]](https://huggingface.co/rain1011/pyramid-flow-miniflux) [[demo ü§ó](https://huggingface.co/spaces/Pyramid-Flow/pyramid-flow)]\n\nThis is the model repository for Pyramid Flow, a training-efficient **Autoregressive Video Generation** method based on **Flow Matching**. By training only on open-source datasets, it generates high-quality 10-second videos at 768p resolution and 24 FPS, and naturally supports image-to-video generation.\n\n<table class="center" border="0" style="width: 100%; text-align: left;">\n<tr>\n  <th>10s, 768p, 24fps</th>\n  <th>5s, 768p, 24fps</th>\n  <th>Image-to-video</th>\n</tr>\n<tr>\n  <td><video src="https://pyramid-flow.github.io/static/videos/t2v_10s/fireworks.mp4" autoplay muted loop playsinline></video></td>\n  <td><video src="https://pyramid-flow.github.io/static/videos/t2v/trailer.mp4" autoplay muted loop playsinline></video></td>\n  <td><video src="https://pyramid-flow.github.io/static/videos/i2v/sunday.mp4" autoplay muted loop playsinline></video></td>\n</tr>\n</table>\n\n\n## News\n\n* `2024.10.29` ‚ö°Ô∏è‚ö°Ô∏è‚ö°Ô∏è We release [training code](https://github.com/jy0205/Pyramid-Flow?tab=readme-ov-file#training) and [new model checkpoints](https://huggingface.co/rain1011/pyramid-flow-miniflux) with FLUX structure trained from scratch.\n\n  > We have switched the model structure from SD3 to a mini FLUX to fix human structure issues, please try our 1024p image checkpoint and 384p video checkpoint. We will release 768p video checkpoint in a few days.\n\n* `2024.10.11`  ü§óü§óü§ó [Hugging Face demo](https://huggingface.co/spaces/Pyramid-Flow/pyramid-flow) is available. Thanks [@multimodalart](https://huggingface.co/multimodalart) for the commit! \n\n* `2024.10.10`  üöÄüöÄüöÄ We release the [technical report](https://arxiv.org/abs/2410.05954), [project page](https://pyramid-flow.github.io) and [model checkpoint](https://huggingface.co/rain1011/pyramid-flow-sd3) of Pyramid Flow.\n\n## Installation\n\nWe recommend setting up the environment with conda. The codebase currently uses Python 3.8.10 and PyTorch 2.1.2, and we are actively working to support a wider range of versions.\n\n```bash\ngit clone https://github.com/jy0205/Pyramid-Flow\ncd Pyramid-Flow\n\n# create env using conda\nconda create -n pyramid python==3.8.10\nconda activate pyramid\npip install -r requirements.txt\n```\n\nThen, download the model from [Huggingface](https://huggingface.co/rain1011) (there are two variants: [miniFLUX](https://huggingface.co/rain1011/pyramid-flow-miniflux) or [SD3](https://huggingface.co/rain1011/pyramid-flow-sd3)). The miniFLUX models support 1024p image and 384p video generation, and the SD3-based models support 768p and 384p video generation. The 384p checkpoint generates 5-second video at 24FPS, while the 768p checkpoint generates up to 10-second video at 24FPS.\n\n```python\nfrom huggingface_hub import snapshot_download\n\nmodel_path = ''PATH''   # The local directory to save downloaded checkpoint\nsnapshot_download("rain1011/pyramid-flow-sd3", local_dir=model_path, local_dir_use_symlinks=False, repo_type=''model'')\n```\n\n## Usage\n\nFor inference, we provide Gradio demo, single-GPU, multi-GPU, and Apple Silicon inference code, as well as VRAM-efficient features such as CPU offloading. Please check our [code repository](https://github.com/jy0205/Pyramid-Flow?tab=readme-ov-file#inference) for usage.\n\nBelow is a simplified two-step usage procedure. First, load the downloaded model:\n\n```python\nimport torch\nfrom PIL import Image\nfrom pyramid_dit import PyramidDiTForVideoGeneration\nfrom diffusers.utils import load_image, export_to_video\n\ntorch.cuda.set_device(0)\nmodel_dtype, torch_dtype = ''bf16'', torch.bfloat16   # Use bf16 (not support fp16 yet)\n\nmodel = PyramidDiTForVideoGeneration(\n    ''PATH'',                                         # The downloaded checkpoint dir\n    model_dtype,\n    model_variant=''diffusion_transformer_768p'',     # ''diffusion_transformer_384p''\n)\n\nmodel.vae.enable_tiling()\n# model.vae.to("cuda")\n# model.dit.to("cuda")\n# model.text_encoder.to("cuda")\n\n# if you''re not using sequential offloading bellow uncomment the lines above ^\nmodel.enable_sequential_cpu_offload()\n```\n\nThen, you can try text-to-video generation on your own prompts:\n\n```python\nprompt = "A movie trailer featuring the adventures of the 30 year old space man wearing a red wool knitted motorcycle helmet, blue sky, salt desert, cinematic style, shot on 35mm film, vivid colors"\n\nwith torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n    frames = model.generate(\n        prompt=prompt,\n        num_inference_steps=[20, 20, 20],\n        video_num_inference_steps=[10, 10, 10],\n        height=768,     \n        width=1280,\n        temp=16,                    # temp=16: 5s, temp=31: 10s\n        guidance_scale=9.0,         # The guidance for the first frame, set it to 7 for 384p variant\n        video_guidance_scale=5.0,   # The guidance for the other video latent\n        output_type="pil",\n        save_memory=True,           # If you have enough GPU memory, set it to `False` to improve vae decoding speed\n    )\n\nexport_to_video(frames, "./text_to_video_sample.mp4", fps=24)\n```\n\nAs an autoregressive model, our model also supports (text conditioned) image-to-video generation:\n\n```python\nimage = Image.open(''assets/the_great_wall.jpg'').convert("RGB").resize((1280, 768))\nprompt = "FPV flying over the Great Wall"\n\nwith torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n    frames = model.generate_i2v(\n        prompt=prompt,\n        input_image=image,\n        num_inference_steps=[10, 10, 10],\n        temp=16,\n        video_guidance_scale=4.0,\n        output_type="pil",\n        save_memory=True,           # If you have enough GPU memory, set it to `False` to improve vae decoding speed\n    )\n\nexport_to_video(frames, "./image_to_video_sample.mp4", fps=24)\n```\n\n## Usage tips\n\n* The `guidance_scale` parameter controls the visual quality. We suggest using a guidance within [7, 9] for the 768p checkpoint during text-to-video generation, and 7 for the 384p checkpoint.\n* The `video_guidance_scale` parameter controls the motion. A larger value increases the dynamic degree and mitigates the autoregressive generation degradation, while a smaller value stabilizes the video.\n* For 10-second video generation, we recommend using a guidance scale of 7 and a video guidance scale of 5.\n\n## Gallery\n\nThe following video examples are generated at 5s, 768p, 24fps. For more results, please visit our [project page](https://pyramid-flow.github.io).\n\n<table class="center" border="0" style="width: 100%; text-align: left;">\n<tr>\n  <td><video src="https://pyramid-flow.github.io/static/videos/t2v/tokyo.mp4" autoplay muted loop playsinline></video></td>\n  <td><video src="https://pyramid-flow.github.io/static/videos/t2v/eiffel.mp4" autoplay muted loop playsinline></video></td>\n</tr>\n<tr>\n  <td><video src="https://pyramid-flow.github.io/static/videos/t2v/waves.mp4" autoplay muted loop playsinline></video></td>\n  <td><video src="https://pyramid-flow.github.io/static/videos/t2v/rail.mp4" autoplay muted loop playsinline></video></td>\n</tr>\n</table>\n\n\n## Acknowledgement\n\nWe are grateful for the following awesome projects when implementing Pyramid Flow:\n\n* [SD3 Medium](https://huggingface.co/stabilityai/stable-diffusion-3-medium) and [Flux 1.0](https://huggingface.co/black-forest-labs/FLUX.1-dev): State-of-the-art image generation models based on flow matching.\n* [Diffusion Forcing](https://boyuan.space/diffusion-forcing) and [GameNGen](https://gamengen.github.io): Next-token prediction meets full-sequence diffusion.\n* [WebVid-10M](https://github.com/m-bain/webvid), [OpenVid-1M](https://github.com/NJU-PCALab/OpenVid-1M) and [Open-Sora Plan](https://github.com/PKU-YuanGroup/Open-Sora-Plan): Large-scale datasets for text-to-video generation.\n* [CogVideoX](https://github.com/THUDM/CogVideo): An open-source text-to-video generation model that shares many training details.\n* [Video-LLaMA2](https://github.com/DAMO-NLP-SG/VideoLLaMA2): An open-source video LLM for our video recaptioning.\n\n## Citation\n\nConsider giving this repository a star and cite Pyramid Flow in your publications if it helps your research.\n\n```\n@article{jin2024pyramidal,\n  title={Pyramidal Flow Matching for Efficient Video Generative Modeling},\n  author={Jin, Yang and Sun, Zhicheng and Li, Ningyuan and Xu, Kun and Xu, Kun and Jiang, Hao and Zhuang, Nan and Huang, Quzhe and Song, Yang and Mu, Yadong and Lin, Zhouchen},\n  jounal={arXiv preprint arXiv:2410.05954},\n  year={2024}\n}\n```', '{"pipeline_tag":"text-to-video","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":52046558470,"files_count":29,"spaces_count":37,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:jy0205:Pyramid-Flow","source_url":"https://github.com/jy0205/Pyramid-Flow"},{"type":"has_code","target_id":"github:jy0205:Pyramid-Flow","source_url":"https://github.com/jy0205/Pyramid-Flow?tab=readme-ov-file#training"},{"type":"has_code","target_id":"github:jy0205:Pyramid-Flow","source_url":"https://github.com/jy0205/Pyramid-Flow"},{"type":"has_code","target_id":"github:jy0205:Pyramid-Flow","source_url":"https://github.com/jy0205/Pyramid-Flow?tab=readme-ov-file#inference"},{"type":"has_code","target_id":"github:m-bain:webvid","source_url":"https://github.com/m-bain/webvid"},{"type":"has_code","target_id":"github:NJU-PCALab:OpenVid-1M","source_url":"https://github.com/NJU-PCALab/OpenVid-1M"},{"type":"has_code","target_id":"github:PKU-YuanGroup:Open-Sora-Plan","source_url":"https://github.com/PKU-YuanGroup/Open-Sora-Plan"},{"type":"has_code","target_id":"github:THUDM:CogVideo","source_url":"https://github.com/THUDM/CogVideo"},{"type":"has_code","target_id":"github:DAMO-NLP-SG:VideoLLaMA2","source_url":"https://github.com/DAMO-NLP-SG/VideoLLaMA2"},{"type":"based_on_paper","target_id":"arxiv:2410.05954","source_url":"https://arxiv.org/abs/2410.05954"}]', NULL, 'Other', 'approved', 64.2, '307f417095f9d00a4d05e4e0079a3017', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Phind-Phind-CodeLlama-34B-v2', 'huggingface--phind--phind-codellama-34b-v2', 'Phind-CodeLlama-34B-v2', 'Phind', '--- license: llama2 model-index: - name: Phind-CodeLlama-34B-v1 results: - task: type: text-generation dataset: type: openai_humaneval name: HumanEval metrics: - name: pass@1 type: pass@1 value: 73.8% verified: false tags: - code llama --- We''ve fine-tuned Phind-CodeLlama-34B-v1 on an additional 1.5B tokens high-quality programming-related data, achieving **73.8% pass@1** on HumanEval. It''s the current state-of-the-art amongst open-source models. Furthermore, this model is **instruction-tuned...', '["transformers","pytorch","llama","text-generation","code llama","license:llama2","model-index","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 833, 2534, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Phind/Phind-CodeLlama-34B-v2","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: llama2\nmodel-index:\n- name: Phind-CodeLlama-34B-v1\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      type: openai_humaneval\n      name: HumanEval\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 73.8%\n      verified: false\ntags:\n- code llama\n---\n\n# **Phind-CodeLlama-34B-v2**\nWe''ve fine-tuned Phind-CodeLlama-34B-v1 on an additional 1.5B tokens high-quality programming-related data, achieving **73.8% pass@1** on HumanEval. It''s the current state-of-the-art amongst open-source models.\n\nFurthermore, this model is **instruction-tuned** on the Alpaca/Vicuna format to be steerable and easy-to-use.\n\nMore details can be found on our [blog post](https://www.phind.com/blog/code-llama-beats-gpt4).\n\n## Model Details\nThis model is fine-tuned from Phind-CodeLlama-34B-v1 and achieves **73.8% pass@1** on HumanEval.\n\nPhind-CodeLlama-34B-v2 is **multi-lingual** and is proficient in Python, C/C++, TypeScript, Java, and more.\n\n## Dataset Details\nWe fined-tuned on a proprietary dataset of 1.5B tokens of high quality programming problems and solutions. This dataset consists of instruction-answer pairs instead of code completion examples, making it structurally different from HumanEval. LoRA was not used -- both models are a native finetune. We used DeepSpeed ZeRO 3 and Flash Attention 2 to train these models in 15 hours on 32 A100-80GB GPUs. We used a sequence length of 4096 tokens.\n\n## How to Get Started with the Model\n\nMake sure to install Transformers from the main git branch:\n\n```bash\npip install git+https://github.com/huggingface/transformers.git\n```\n\n## How to Prompt the Model\nThis model accepts the Alpaca/Vicuna instruction format.\n\nFor example: \n\n```\n### System Prompt\nYou are an intelligent programming assistant.\n\n### User Message\nImplement a linked list in C++\n\n### Assistant\n...\n```\n\n## How to reproduce HumanEval Results\n\nTo reproduce our results:\n\n```python\n\nfrom transformers import AutoTokenizer, LlamaForCausalLM\nfrom human_eval.data import write_jsonl, read_problems\nfrom tqdm import tqdm\n\n# initialize the model\n\nmodel_path = "Phind/Phind-CodeLlama-34B-v2"\nmodel = LlamaForCausalLM.from_pretrained(model_path, device_map="auto")\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# HumanEval helper\n\ndef generate_one_completion(prompt: str):\n    tokenizer.pad_token = tokenizer.eos_token\n    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4096)\n\n    # Generate\n    generate_ids = model.generate(inputs.input_ids.to("cuda"), max_new_tokens=384, do_sample=True, top_p=0.75, top_k=40, temperature=0.1)\n    completion = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    completion = completion.replace(prompt, "").split("\n\n\n")[0]\n\n    return completion\n\n# perform HumanEval\nproblems = read_problems()\n\nnum_samples_per_task = 1\nsamples = [\n    dict(task_id=task_id, completion=generate_one_completion(problems[task_id]["prompt"]))\n    for task_id in tqdm(problems)\n    for _ in range(num_samples_per_task)\n]\nwrite_jsonl("samples.jsonl", samples)\n\n# run `evaluate_functional_correctness samples.jsonl` in your HumanEval code sandbox\n```\n\n## Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\nThis model has undergone very limited testing. Additional safety testing should be performed before any real-world deployments.\n\n\n## Training details\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\n- **Hardware Type:** 32x A100-80GB\n- **Hours used:** 480 GPU-hours\n- **Cloud Provider:** AWS\n- **Compute Region:** us-east-1', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":134976616088,"files_count":15,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<s>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"</s>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":null,"unk_token":{"__type":"AddedToken","content":"<unk>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"use_default_system_prompt":true}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers.git","source_url":"https://github.com/huggingface/transformers.git"}]', NULL, 'LLaMA-2', 'approved', 64.2, '841746692974d085f159b7df51dc0d80', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-HuggingFaceTB-SmolLM3-3B', 'huggingface--huggingfacetb--smollm3-3b', 'SmolLM3-3B', 'HuggingFaceTB', '--- library_name: transformers license: apache-2.0 language: - en - fr - es - it - pt - zh - ar - ru base_model: - HuggingFaceTB/SmolLM3-3B-Base --- !image/png 1. Model Summary 2. How to use 3. Evaluation 4. Training 5. Limitations 6. License SmolLM3 is a 3B parameter language model designed to push the boundaries of small models. It supports dual mode reasoning, 6 languages and long context. SmolLM3 is a fully open model that offers strong performance at the 3B‚Äì4B scale. !image/png The model...', '["transformers","safetensors","smollm3","text-generation","conversational","en","fr","es","it","pt","zh","ar","ru","base_model:huggingfacetb/smollm3-3b-base","base_model:finetune:huggingfacetb/smollm3-3b-base","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 833, 91555, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/HuggingFaceTB/SmolLM3-3B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlanguage:\n- en\n- fr\n- es\n- it\n- pt\n- zh\n- ar\n- ru\nbase_model:\n  - HuggingFaceTB/SmolLM3-3B-Base\n---\n\n\n# SmolLM3\n\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/zy0dqTCCt5IHmuzwoqtJ9.png)\n\n\n##  Table of Contents\n\n1. [Model Summary](#model-summary)\n2. [How to use](#how-to-use)\n3. [Evaluation](#evaluation)\n4. [Training](#training)\n5. [Limitations](#limitations)\n6. [License](#license)\n\n## Model Summary\n\nSmolLM3 is a 3B parameter language model designed to push the boundaries of small models. It supports dual mode reasoning, 6 languages and long context. SmolLM3 is a fully open model that offers strong performance at the 3B‚Äì4B scale.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/6200d0a443eb0913fa2df7cc/db3az7eGzs-Sb-8yUj-ff.png)\n\nThe model is a decoder-only transformer using GQA and NoPE (with 3:1 ratio), it was pretrained on 11.2T tokens with a staged curriculum of web, code, math and reasoning data. Post-training included midtraining on 140B reasoning tokens followed by supervised fine-tuning and alignment via Anchored Preference Optimization (APO).\n\n### Key features\n- Instruct model optimized for **hybrid reasoning**\n- **Fully open model**: open weights + full training details including public data mixture and training configs\n- **Long context:** Trained on 64k context and supports up to **128k tokens** using YARN extrapolation\n- **Multilingual**: 6 natively supported (English, French, Spanish, German, Italian, and Portuguese)\n\nFor more details refer to our blog post: https://hf.co/blog/smollm3\n\n## How to use\n\nThe modeling code for SmolLM3 is available in transformers `v4.53.0`, so make sure to upgrade your transformers version. You can also load the model with the latest `vllm` which uses transformers as a backend.\n```bash\npip install -U transformers\n```\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "HuggingFaceTB/SmolLM3-3B"\ndevice = "cuda"  # for GPU usage or "cpu" for CPU usage\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n).to(device)\n\n# prepare the model input\nprompt = "Give me a brief explanation of gravity in simple terms."\nmessages_think = [\n    {"role": "user", "content": prompt}\n]\n\ntext = tokenizer.apply_chat_template(\n    messages_think,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# Generate the output\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=32768)\n\n# Get and decode the output\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]) :]\nprint(tokenizer.decode(output_ids, skip_special_tokens=True))\n```\n\n>[!TIP]\n> We recommend setting `temperature=0.6` and `top_p=0.95` in the sampling parameters.\n\n### Long context processing\n\nThe current `config.json` is set for context length up to 65,536 tokens. To handle longer inputs (128k or 256k), we utilize YaRN you can change the `max_position_embeddings` and rope_scaling` to:\n```\n{\n  ...,\n  "rope_scaling": {\n    "factor": 2.0, #2x65536=131‚ÄØ072 \n    "original_max_position_embeddings": 65536,\n    "type": "yarn"\n  }\n}\n```\n\n\n### Enabling and Disabling Extended Thinking Mode\n\nWe enable extended thinking by default, so the example above generates the output with a reasoning trace. For choosing between enabling, you can provide the `/think` and `/no_think` flags through the system prompt as shown in the snippet below for extended thinking disabled. The code for generating the response with extended thinking would be the same except that the system prompt should have `/think` instead of `/no_think`.\n\n```python\nprompt = "Give me a brief explanation of gravity in simple terms."\nmessages = [\n    {"role": "system", "content": "/no_think"},\n    {"role": "user", "content": prompt}\n]\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\n```\n\nWe also provide the option of specifying the whether to use extended thinking through the `enable_thinking` kwarg as in the example below. You do not need to set the `/no_think` or `/think` flags through the system prompt if using the kwarg, but keep in mind that the flag in the system prompt overwrites the setting in the kwarg.\n\n```python\nprompt = "Give me a brief explanation of gravity in simple terms."\nmessages = [\n    {"role": "user", "content": prompt}\n]\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False\n)\n```\n\n### Agentic Usage\n\nSmolLM3 supports tool calling!\nJust pass your list of tools:\n- Under the argument `xml_tools` for standard tool-calling: these tools will be called as JSON blobs within XML tags, like `<tool_call>{"name": "get_weather", "arguments": {"city": "Copenhagen"}}</tool_call>`\n- Or under `python_tools`: then the model will call tools like python functions in a `<code>` snippet, like `<code>get_weather(city="Copenhagen")</code>`\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ncheckpoint = "HuggingFaceTB/SmolLM3-3B"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint)\n\ntools = [\n    {\n        "name": "get_weather",\n        "description": "Get the weather in a city",\n        "parameters": {"type": "object", "properties": {"city": {"type": "string", "description": "The city to get the weather for"}}}}\n]\n\nmessages = [\n    {\n        "role": "user",\n        "content": "Hello! How is the weather today in Copenhagen?"\n    }\n]\n\ninputs = tokenizer.apply_chat_template(\n    messages,\n    enable_thinking=False, # True works as well, your choice!\n    xml_tools=tools,\n    add_generation_prompt=True,\n    tokenize=True,\n    return_tensors="pt"\n)\n\noutputs = model.generate(inputs)\nprint(tokenizer.decode(outputs[0]))\n```\n\n### Using Custom System Instructions. \n\nYou can specify custom instruction through the system prompt while controlling whether to use extended thinking. For example, the snippet below shows how to make the model speak like a pirate while enabling extended thinking.\n\n```python\nprompt = "Give me a brief explanation of gravity in simple terms."\nmessages = [\n    {"role": "system", "content": "Speak like a pirate./think"},\n    {"role": "user", "content": prompt}\n]\n\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\n```\n\nFor local inference, you can use `llama.cpp`, `ONNX`, `MLX`, `MLC` and `ExecuTorch`. You can find quantized checkpoints in this collection (https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23)\n\n### vLLM and SGLang\n\nYou can use vLLM and SGLang to deploy the model in an API compatible with OpenAI format.\n\n#### SGLang\n\n```bash\npython -m sglang.launch_server --model-path HuggingFaceTB/SmolLM3-3B\n```\n\n#### vLLM\n\n```bash\nvllm serve HuggingFaceTB/SmolLM3-3B --enable-auto-tool-choice --tool-call-parser=hermes\n```\n\n#### Setting `chat_template_kwargs`\n\nYou can specify `chat_template_kwargs` such as `enable_thinking` to a deployed model by passing the `chat_template_kwargs` parameter in the API request.\n\n```bash\ncurl http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -d ''{\n  "model": "HuggingFaceTB/SmolLM3-3B",\n  "messages": [\n    {"role": "user", "content": "Give me a brief explanation of gravity in simple terms."}\n  ],\n  "temperature": 0.6,\n  "top_p": 0.95,\n  "max_tokens": 16384,\n  "chat_template_kwargs": {"enable_thinking": false}\n}''\n```\n\n## Evaluation\n\nIn this section, we report the evaluation results of SmolLM3 model. All evaluations are zero-shot unless stated otherwise, and we use [lighteval](https://github.com/huggingface/lighteval) to run them. \n\nWe highlight the best score in bold and underline the second-best score.\n\n### Instruction Model\n\n#### No Extended Thinking\nEvaluation results of non reasoning models and reasoning models in no thinking mode. We highlight the best and second-best scores in bold.\n| Category | Metric | SmoLLM3-3B | Qwen2.5-3B | Llama3.1-3B | Qwen3-1.7B | Qwen3-4B |\n|---------|--------|------------|------------|-------------|------------|----------|\n| High school math competition | AIME 2025 | <u>9.3</u> | 2.9 | 0.3 | 8.0 | **17.1** |\n| Math problem-solving | GSM-Plus | 72.8 | <u>74.1</u> | 59.2 | 68.3 | **82.1** |\n| Competitive programming | LiveCodeBench v4 | <u>15.2</u> | 10.5 | 3.4 | 15.0 | **24.9** |\n| Graduate-level reasoning | GPQA Diamond | <u>35.7</u> | 32.2 | 29.4 | 31.8 | **44.4** |\n| Instruction following | IFEval | **76.7** | 65.6 | 71.6 | <u>74.0</u> | 68.9 |\n| Alignment | MixEval Hard | 26.9 | <u>27.6</u> | 24.9 | 24.3 | **31.6** |\n| Tool Calling | BFCL| <u>92.3</u> | - | <u>92.3</u> * | 89.5  | **95.0** |\n| Multilingual Q&A | Global MMLU | <u>53.5</u> | 50.54 | 46.8 | 49.5 | **65.1** |\n\n(*): this is a tool calling finetune\n\n#### Extended Thinking\nEvaluation results in reasoning mode for SmolLM3 and Qwen3 models: \n| Category | Metric | SmoLLM3-3B | Qwen3-1.7B | Qwen3-4B |\n|---------|--------|------------|------------|----------|\n| High school math competition | AIME 2025 | <u>36.7</u> | 30.7 | **58.8** |\n| Math problem-solving | GSM-Plus | <u>83.4</u> | 79.4 | **88.2** |\n| Competitive programming | LiveCodeBench v4 | 30.0 | <u>34.4</u> | **52.9** |\n| Graduate-level reasoning | GPQA Diamond | <u>41.7</u> | 39.9 | **55.3** |\n| Instruction following | IFEval | 71.2 | <u>74.2</u> | **85.4** |\n| Alignment | MixEval Hard | 30.8 | <u>33.9</u> | **38.0** |\n| Tool Calling | BFCL | <u>88.8</u> | <u>88.8</u> | **95.5** |\n| Multilingual Q&A | Global MMLU | <u>64.1</u> | 62.3 | **73.3** |\n\n\n### Base Pre-Trained Model\n\n#### English benchmarks\nNote: All evaluations are zero-shot unless stated otherwise. For Ruler 64k evaluation, we apply YaRN to the Qwen models with 32k context to extrapolate the context length.\n\n| Category | Metric | SmolLM3-3B | Qwen2.5-3B | Llama3-3.2B | Qwen3-1.7B-Base | Qwen3-4B-Base |\n|---------|--------|---------------------|------------|--------------|------------------|---------------|\n| Reasoning & Commonsense| HellaSwag | **76.15** | 74.19 |<u>75.52</u> | 60.52 | 74.37 |\n| | ARC-CF (Average) | **65.61** | 59.81 | 58.58 | 55.88 | <u>62.11</u> |\n| | Winogrande | 58.88 | **61.41** | 58.72 | 57.06 | <u>59.59</u> |\n| | CommonsenseQA | <u>55.28</u> | 49.14 | **60.60** | 48.98 | 52.99 |\n| Knowledge & Understanding | MMLU-CF (Average) | <u>44.13</u> | 42.93 | 41.32 | 39.11 | **47.65** | \n| | MMLU Pro CF | <u>19.61</u> | 16.66 | 16.42 | 18.04 | **24.92** |\n| | MMLU Pro MCF | <u>32.70</u> | 31.32 | 25.07 | 30.39 | **41.07** |\n| | PIQA | **78.89** | 78.35 | <u>78.51</u> | 75.35 | 77.58 |\n| | OpenBookQA | 40.60 | 40.20 | <u>42.00</u> | 36.40 | **42.40** |\n| | BoolQ | **78.99** | 73.61 | <u>75.33</u> | 74.46 | 74.28 | \n| **Math & Code** |  |  |  |  |  |  | \n| Coding & math | HumanEval+ | 30.48 | 34.14| 25.00 | <u>43.29</u>| **54.87** |\n| | MBPP+ | 52.91 | 52.11 | 38.88| <u>59.25</u> | **63.75** | \n| | MATH (4-shot) | <u>46.10</u> | 40.10 | 7.44 | 41.64 | **51.20** |\n| | GSM8k (5-shot) | 67.63 | <u>70.13</u> | 25.92 | 65.88 | **74.14** | \n| **Long context** |  |  |  |  |  |  | \n| | Ruler 32k | 76.35 | 75.93 | <u>77.58</u> | 70.63 | **83.98** | \n| | Ruler 64k | <u>67.85</u> | 64.90 | **72.93** | 57.18 | 60.29 | \n| | Ruler 128k | 61.03 | <u>62.23</u> | **71.30** | 43.03 | 47.23 | \n\n#### Multilingual benchmarks\n\n\n| Category | Metric | SmolLM3 3B Base | Qwen2.5-3B | Llama3.2 3B | Qwen3 1.7B Base | Qwen3 4B Base |\n|---------|--------|---------------------|------------|--------------|------------------|---------------|\n| Main supported languages |  |  |  |  |  |  |  |\n| French| MLMM Hellaswag | **63.94** | 57.47 | 57.66 | 51.26 | <u>61.00</u> |\n| | Belebele | 51.00 | <u>51.55</u> | 49.22 |49.44| **55.00** |\n| | Global MMLU (CF) | <u>38.37</u> | 34.22  | 33.71 | 34.94  |**41.80** |\n| | Flores-200 (5-shot) | 62.85| 61.38| <u>62.89</u> | 58.68 | **65.76** |\n| Spanish| MLMM Hellaswag | **65.85** | 58.25 | 59.39 | 52.40 | <u>61.85</u> |\n| | Belebele | 47.00 | <u>48.88</u> | 47.00 | 47.56 | **50.33** |\n| | Global MMLU (CF) | <u>38.51</u> | 35.84  | 35.60 | 34.79  |**41.22** |\n| | Flores-200 (5-shot) | <u>48.25</u>| 50.00| 44.45 | 46.93 | **50.16** |\n| German| MLMM Hellaswag | **59.56** | 49.99|  53.19|46.10| <u>56.43</u>|\n| | Belebele | <u>48.44</u> | 47.88 | 46.22 | 48.00 | **53.44**|\n| | Global MMLU (CF) | <u>35.10</u> | 33.19  | 32.60 | 32.73  |**38.70** |\n| | Flores-200 (5-shot) | **56.60**| 50.63| <u>54.95</u> | 52.58 | 50.48 |\n| Italian| MLMM Hellaswag | **62.49** | 53.21 | 54.96 | 48.72 | <u>58.76</u> |\n| | Belebele | <u>46.44</u> | 44.77 | 43.88 | 44.00 | **48.78** | 44.88 |\n| | Global MMLU (CF) | <u>36.99</u> | 33.91  | 32.79 | 35.37  |**39.26** |\n| | Flores-200 (5-shot) | <u>52.65<u/>| **54.87**| 48.83 | 48.37 | 49.11 |\n| Portuguese| MLMM Hellaswag | **63.22** | 57.38 | 56.84 | 50.73 | <u>59.89</u> |\n| | Belebele | 47.67 | **49.22** | 45.00 | 44.00 | 50.00 | <u>49.00</U> |\n| | Global MMLU (CF) | <u>36.88</u> | 34.72  | 33.05 | 35.26  |**40.66** |\n| | Flores-200 (5-shot) | <u>60.93</u> |57.68| 54.28 | 56.58 | **63.43** |\n\nThe model has also been trained on Arabic (standard), Chinese and Russian data, but has seen fewer tokens in these languages compared to the 6 above. We report the performance on these langages for information.\n| Category | Metric | SmolLM3 3B Base | Qwen2.5-3B | Llama3.2 3B | Qwen3 1.7B Base | Qwen3 4B Base |\n|---------|--------|---------------------|------------|--------------|------------------|---------------|\n| Other supported languages |  |  |  |  |  |  |  |\n| Arabic| Belebele | 40.22 | 44.22 | <u>45.33</u> | 42.33 | **51.78** |\n| | Global MMLU (CF) | 28.57 | 28.81 | 27.67 | <u>29.37</u> | **31.85** |\n| | Flores-200 (5-shot) | <u>40.22</u> | 39.44 | **44.43** | 35.82 | 39.76 |\n| Chinese| Belebele | 43.78 | 44.56 | <u>49.56</u> | 48.78 | **53.22** |\n| | Global MMLU (CF) | 36.16 | 33.79 | <u>39.57</u> | 38.56 | **44.55** |\n| | Flores-200 (5-shot) | 29.17 | **33.21** | 31.89 | 25.70 | <u>32.50</u> |\n| Russian| Belebele | <u>47.44</u> | 45.89 | <u>47.44</u> | 45.22 | **51.44** |\n| | Global MMLU (CF) | <u>36.51</u> | 32.47 | 34.52 | 34.83 | **38.80** |\n| | Flores-200 (5-shot) | 47.13 | 48.74 | 50.74 | <u>54.70</u> | **60.53** |\n\n## Training\n\n### Model\n\n- **Architecture:** Transformer decoder\n- **Pretraining tokens:** 11T\n- **Precision:** bfloat16\n\n### Software & hardware\n\n- **GPUs:** 384 H100\n- **Training Framework:** [nanotron](https://github.com/huggingface/nanotron/tree/smollm3)\n- **Data processing framework:** [datatrove](https://github.com/huggingface/datatrove)\n- **Evaluation framework:** [lighteval](https://github.com/huggingface/lighteval)\n- **Post-training Framework:** [TRL](https://github.com/huggingface/trl)\n\n### Open resources\nHere is an infographic with all the training details \n- The datasets used for pretraining can be found in this [collection](https://huggingface.co/collections/HuggingFaceTB/smollm3-pretraining-datasets-685a7353fdc01aecde51b1d9) and those used in mid-training and post-training will be uploaded later \n- The training and evaluation configs and code can be found in the [huggingface/smollm](https://github.com/huggingface/smollm) repository.\n- The training intermediate checkpoints (including the mid-training and SFT checkpoints) are available at [HuggingFaceTB/SmolLM3-3B-checkpoints](https://huggingface.co/HuggingFaceTB/SmolLM3-3B-checkpoints)\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/651e96991b97c9f33d26bde6/qiE5ZYr9SD1CIAtfEfuC8.png)\n\n### EU Summary of Public Content\n\nThe EU AI Act requires all GPAI models to provide a Public Summary of Training Content according to a [given template](https://digital-strategy.ec.europa.eu/en/library/explanatory-notice-and-template-public-summary-training-content-general-purpose-ai-models).\nYou can find the summary for this model below, as well as in its [development Space](https://huggingface.co/spaces/hfmlsoc/smollm3-eu-data-transparency).\n\n<iframe\n	src="https://hfmlsoc-smollm3-eu-data-transparency.hf.space"\n	frameborder="0"\n	width="850"\n	height="350"\n></iframe>\n\n\n## Limitations\n\nSmolLM3 can produce text on a variety of topics, but the generated content may not always be factually accurate, logically consistent, or free from biases present in the training data. These models should be used as assistive tools rather than definitive sources of information. Users should always verify important information and critically evaluate any generated content.\n\n## License\n[Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\n\n## Citation\n```bash\n@misc{bakouch2025smollm3,\n  title={{SmolLM3: smol, multilingual, long-context reasoner}},\n  author={Bakouch, Elie and Ben Allal, Loubna and Lozhkov, Anton and Tazi, Nouamane and Tunstall, Lewis and Pati√±o, Carlos Miguel and Beeching, Edward and Roucher, Aymeric and Reedi, Aksel Joonas and Gallou√©dec, Quentin and Rasul, Kashif and Habib, Nathan and Fourrier, Cl√©mentine and Kydlicek, Hynek and Penedo, Guilherme and Larcher, Hugo and Morlon, Mathieu and Srivastav, Vaibhav and Lochner, Joshua and Nguyen, Xuan-Son and Raffel, Colin and von Werra, Leandro and Wolf, Thomas},\n  year={2025},\n  howpublished={\url{https://huggingface.co/blog/smollm3}}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":3075098624,"storage_bytes":6178163042,"files_count":12,"spaces_count":91,"gated":false,"private":false,"config":{"architectures":["SmolLM3ForCausalLM"],"model_type":"smollm3","tokenizer_config":{"bos_token":null,"eos_token":"<|im_end|>","pad_token":"<|im_end|>"},"chat_template_jinja":"{# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ defaults ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #}\n{%- if enable_thinking is not defined -%}\n{%- set enable_thinking = true -%}\n{%- endif -%}\n\n{# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ reasoning mode ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #}\n{%- if enable_thinking -%}\n  {%- set reasoning_mode = \"/think\" -%}\n{%- else -%}\n  {%- set reasoning_mode = \"/no_think\" -%}\n{%- endif -%}\n\n{# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ header (system message) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #}\n{{- \"<|im_start|>system\\n\" -}}\n\n{%- if messages[0].role == \"system\" -%}\n  {%- set system_message = messages[0].content -%}\n  {%- if \"/no_think\" in system_message -%}\n    {%- set reasoning_mode = \"/no_think\" -%}\n  {%- elif \"/think\" in system_message -%}\n    {%- set reasoning_mode = \"/think\" -%}\n  {%- endif -%}\n  {%- set custom_instructions = system_message.replace(\"/no_think\", \"\").replace(\"/think\", \"\").rstrip() -%}\n{%- endif -%}\n\n{%- if \"/system_override\" in system_message -%}\n  {{- custom_instructions.replace(\"/system_override\", \"\").rstrip() -}}\n  {{- \"<|im_end|>\\n\" -}}\n{%- else -%}\n  {{- \"## Metadata\\n\\n\" -}}\n  {{- \"Knowledge Cutoff Date: June 2025\\n\" -}}\n  {%- set today = strftime_now(\"%d %B %Y\") -%}\n  {{- \"Today Date: \" ~ today ~ \"\\n\" -}}\n  {{- \"Reasoning Mode: \" + reasoning_mode + \"\\n\\n\" -}}\n  \n  {{- \"## Custom Instructions\\n\\n\" -}}\n  {%- if custom_instructions -%}\n    {{- custom_instructions + \"\\n\\n\" -}}\n  {%- elif reasoning_mode == \"/think\" -%}\n    {{- \"You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\\n\\n\" -}}\n  {%- else -%}\n    {{- \"You are a helpful AI assistant named SmolLM, trained by Hugging Face.\\n\\n\" -}}\n  {%- endif -%}\n\n  {%- if xml_tools or python_tools or tools -%}\n    {{- \"### Tools\\n\\n\" -}}\n    {%- if xml_tools or tools -%}\n      {%- if tools -%}\n        {%- set xml_tools = tools -%}\n      {%- endif -%}\n      {%- set ns = namespace(xml_tool_string=\"You may call one or more functions to assist with the user query.\\nYou are provided with function signatures within <tools></tools> XML tags:\\n\\n<tools>\\n\") -%}\n      {%- for tool in xml_tools[:] -%} {# The slicing makes sure that xml_tools is a list #}\n        {%- set ns.xml_tool_string = ns.xml_tool_string ~ (tool | string) ~ \"\\n\" -%}\n      {%- endfor -%}\n      {%- set xml_tool_string = ns.xml_tool_string + \"</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call>\" -%}\n      {{- xml_tool_string -}}\n    {%- endif -%}\n    {%- if python_tools -%}\n      {%- set ns = namespace(python_tool_string=\"When you send a message containing Python code between ''<code>'' and ''</code>'' tags, it will be executed in a stateful Jupyter notebook environment, and you will then be given the output to continued reasoning in an agentic loop.\\n\\nYou can use the following tools in your python code like regular functions:\\n<tools>\\n\") -%}\n      {%- for tool in python_tools[:] -%} {# The slicing makes sure that python_tools is a list #}\n        {%- set ns.python_tool_string = ns.python_tool_string ~ (tool | string) ~ \"\\n\" -%}\n      {%- endfor -%}\n      {%- set python_tool_string = ns.python_tool_string + \"</tools>\\n\\nThe state persists between code executions: so variables that you define in one step are still available thereafter.\" -%}\n      {{- python_tool_string -}}\n    {%- endif -%}\n    {{- \"\\n\\n\" -}}\n    {{- \"<|im_end|>\\n\" -}}\n  {%- endif -%}\n{%- endif -%}\n{# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ main loop ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #}\n{%- for message in messages -%}\n    {%- set content = message.content if message.content is string else \"\" -%}\n    {%- if message.role == \"user\" -%}\n        {{ \"<|im_start|>\" + message.role + \"\\n\"  + content + \"<|im_end|>\\n\" }}\n    {%- elif message.role == \"assistant\" -%}\n        {% generation %}\n        {%- if reasoning_mode == \"/think\" -%}\n            {{ \"<|im_start|>assistant\\n\" + content.lstrip(\"\\n\") + \"<|im_end|>\\n\" }}\n        {%- else -%}\n            {{ \"<|im_start|>assistant\\n\" + \"<think>\\n\\n</think>\\n\" + content.lstrip(\"\\n\") + \"<|im_end|>\\n\" }}\n        {%- endif -%}\n        {% endgeneration %}\n    {%- elif message.role == \"tool\" -%}\n    {{ \"<|im_start|>\" + \"user\\n\"  + content + \"<|im_end|>\\n\" }}\n    {%- endif -%}\n{%- endfor -%}\n{# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ generation prompt ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #}\n{%- if add_generation_prompt -%}\n    {%- if reasoning_mode == \"/think\" -%}\n        {{ \"<|im_start|>assistant\\n\" }}\n    {%- else -%}\n        {{ \"<|im_start|>assistant\\n\" + \"<think>\\n\\n</think>\\n\"  }}\n    {%- endif -%}\n{%- endif -%}"}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:lighteval","source_url":"https://github.com/huggingface/lighteval"},{"type":"has_code","target_id":"github:huggingface:nanotron","source_url":"https://github.com/huggingface/nanotron"},{"type":"has_code","target_id":"github:huggingface:datatrove","source_url":"https://github.com/huggingface/datatrove"},{"type":"has_code","target_id":"github:huggingface:lighteval","source_url":"https://github.com/huggingface/lighteval"},{"type":"has_code","target_id":"github:huggingface:trl","source_url":"https://github.com/huggingface/trl"},{"type":"has_code","target_id":"github:huggingface:smollm","source_url":"https://github.com/huggingface/smollm"}]', NULL, 'Apache-2.0', 'approved', 79.2, 'bec0d6b932061f6d5ad77634a27cf1ff', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-apple-DCLM-7B', 'huggingface--apple--dclm-7b', 'DCLM-7B', 'apple', '--- license: apple-ascl --- <img src="https://cdn-uploads.huggingface.co/production/uploads/63118add64939fabc0108b28/BB42g4V8HTxb5dR4tcy8A.png" alt="DCLM Logo" width="800" style="margin-left:''auto'' margin-right:''auto'' display:''block''"/> DCLM-Baseline-7B is a 7 billion parameter language model trained on the DCLM-Baseline dataset, which was curated as part of the DataComp for Language Models (DCLM) benchmark. This model is designed to showcase the effectiveness of systematic data curation tech...', '["transformers","safetensors","openlm","arxiv:2406.11794","license:apple-ascl","endpoints_compatible","region:us"]', 'other', 832, 252, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/apple/DCLM-7B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'tool', '---\nlicense: apple-ascl\n---\n\n\n\n<img src="https://cdn-uploads.huggingface.co/production/uploads/63118add64939fabc0108b28/BB42g4V8HTxb5dR4tcy8A.png" alt="DCLM Logo" width="800" style="margin-left:''auto'' margin-right:''auto'' display:''block''"/>\n\n\n# Model Card for DCLM-Baseline-7B\n\nDCLM-Baseline-7B is a 7 billion parameter language model trained on the DCLM-Baseline dataset, which was curated as part of the DataComp for Language Models (DCLM) benchmark. This model is designed to showcase the effectiveness of systematic data curation techniques for improving language model performance.\n\n## Model Details\n\n| Size | Training Tokens | Layers | Hidden Size | Attention Heads | Context Length |\n|------|-----------------|--------|-------------|-----------------|----------------|\n| 7B   | 2.5T            | 32     | 4096        | 32              | 2048           |\n\n\n### Model Description\n\n- **Developed by:** DataComp for Language Models (DCLM) Team\n- **Model type:** Decoder-only Transformer language model\n- **Language(s):** English (primarily)\n- **License:** Apple Sample Code License\n- **Contact:** contact@datacomp.ai\n- **Date:** June 2024\n\n### Model Sources\n\n- **Repository:** https://github.com/mlfoundations/dclm\n- **Dataset:** https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0\n- **Paper:** [DataComp-LM: In search of the next generation of training sets for language models](https://arxiv.org/abs/2406.11794)\n\n\n## Using Model\n\nFirst install open_lm\n\n```bash\npip install git+https://github.com/mlfoundations/open_lm.git\n```\n\nThen:\n```python\nfrom open_lm.hf import *\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained("apple/DCLM-Baseline-7B")\nmodel = AutoModelForCausalLM.from_pretrained("apple/DCLM-Baseline-7B")\n\ninputs = tokenizer(["Machine learning is"], return_tensors="pt")\ngen_kwargs = {"max_new_tokens": 50, "top_p": 0.8, "temperature": 0.8, "do_sample": True, "repetition_penalty": 1.1}\noutput = model.generate(inputs[''input_ids''], **gen_kwargs)\noutput = tokenizer.decode(output[0].tolist(), skip_special_tokens=True)\nprint(output)\n```\n\n\n\n\n\n\n### Training Details\n\nThe model was trained using the following setup:\n\n- **Architecture:** Decoder-only Transformer \n- **Framework:** PyTorch with OpenLM\n- **Optimizer:** AdamW\n- **Learning Rate:** 2e-3 (peak)\n- **Weight Decay:** 0.05\n- **Batch Size:** 2048 sequences\n- **Sequence Length:** 2048 tokens\n- **Total Training Tokens:** 2.5T\n- **Hardware:** Trained on H100 GPUs\n\nFor more detailed training information, please refer to Section 3.4 and Appendix F of the DCLM paper.\nTo ensure our trained model is broadly useful, including for math and coding tasks, we combine our 3.8T [DCLM-BASELINE](https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0)  with the [StarCoder](https://huggingface.co/datasets/bigcode/starcoderdata)  and [ProofPile2](https://huggingface.co/datasets/EleutherAI/proof-pile-2) data to arrive at a 4.1T token dataset.\n\n## Evaluation\n\nHere are the evaluation results for DCLM-Baseline-7B on various tasks (using [llm-foundry](https://github.com/mosaicml/llm-foundry) eval suite)\n\n| Task | Score |\n|------|-------|\n| MMLU (zero-shot) | 0.5766 |\n| MMLU (few-shot) | 0.6372 |\n| HellaSwag (zero-shot) | 0.7987 |\n| HellaSwag | 0.8043 |\n| Jeopardy | 0.4745 |\n| TriviaQA | 0.5270 |\n| GSM8K (CoT) | 0.0250 |\n| AGI Eval SAT Math (CoT) | 0.0136 |\n| AQuA (CoT) | 0.0490 |\n| SVAMP (CoT) | 0.4900 |\n| BigBench QA Wikidata | 0.7120 |\n| ARC Easy | 0.8220 |\n| ARC Challenge | 0.5990 |\n| BigBench Misconceptions | 0.6986 |\n| COPA | 0.8500 |\n| SIQA | 0.8291 |\n| CommonsenseQA | 0.8018 |\n| PIQA | 0.8128 |\n| OpenBookQA | 0.4540 |\n| BigBench Novel Concepts | 0.7188 |\n| BigBench Strange Stories | 0.7586 |\n| BigBench Strategy QA | 0.6173 |\n| LAMBADA | 0.8220 |\n| Winograd | 0.8828 |\n| Winogrande | 0.7269 |\n| BigBench Conlang Translation | 0.0244 |\n| BigBench Language Identification | 0.5219 |\n| BigBench Conceptual Combinations | 0.6990 |\n| BigBench Elementary Math QA | 0.3431 |\n| BigBench Dyck Languages | 0.4930 |\n| AGI Eval LSAT AR | 0.2435 |\n| BigBench CS Algorithms | 0.6121 |\n| BigBench Logical Deduction | 0.3620 |\n| BigBench Operators | 0.4857 |\n| BigBench Repeat Copy Logic | 0.4063 |\n| Simple Arithmetic (no spaces) | 0.2940 |\n| Simple Arithmetic (with spaces) | 0.3110 |\n| MathQA | 0.3098 |\n| LogiQA | 0.4132 |\n| PubMedQA | 0.7060 |\n| SQuAD | 0.5856 |\n| AGI Eval LSAT RC | 0.6716 |\n| AGI Eval LSAT LR | 0.5392 |\n| CoQA | 0.4074 |\n| BigBench Understanding Fables | 0.6825 |\n| BoolQ | 0.8343 |\n| AGI Eval SAT EN | 0.7670 |\n| Winogender MC (Female) | 0.6000 |\n| Winogender MC (Male) | 0.5500 |\n| Enterprise PII Classification | 0.7676 |\n| BBQ | 0.6912 |\n| GPQA Main | 0.2612 |\n| GPQA Diamond | 0.2475 |\n\nNote: All scores are presented as decimal values between 0 and 1, representing the proportion of correct answers or the model''s performance on each task.\n\n\n## Comparison \n\n\nBelow are comparisions of this model with other models in the 7B regime.\n\n| Model         | Params | Tokens | Open dataset? | CORE     | MMLU     | EXTENDED |\n|---------------|--------|--------|---------------|----------|----------|----------|\n| **Open weights, closed datasets** |        |        |               |          |          |          |\n| Llama2        | 7B     | 2T     | ‚ùå             | 49.2     | 45.8     | 34.1     |\n| DeepSeek      | 7B     | 2T     | ‚ùå             | 50.7     | 48.5     | 35.3     |\n| Mistral-0.3   | 7B     | ?      | ‚ùå             | 57.0     | 62.7     | 45.1     |\n| QWEN-2        | 7B     | ?      | ‚ùå            | 57.5     | **71.9** | 50.5     |\n| Llama3        | 8B     | 15T    | ‚ùå             | 57.6     | 66.2     | 46.3     |\n| Gemma         | 8B     | 6T     | ‚ùå             | 57.8     | 64.3     | 44.6     |\n| Phi-3         | 7B     | ?      | ‚ùå             | **61.0** | 69.9     | **57.9** |\n| **Open weights, open datasets** |        |        |               |          |          |          |\n| Falcon        | 7B     | 1T     | ‚úÖ              | 44.1     | 27.4     | 25.1     |\n| OLMo-1.7      | 7B     | 2.1T   | ‚úÖ              | 47.0     | 54.0     | 34.2     |\n| MAP-Neo       | 7B     | 4.5T   | ‚úÖ              | **50.2** | **57.1** | **40.4** |\n| **DCLM-7B** | 7B     | 2.5T   | ‚úÖ              | **56.1** | **63.7** | **43.6** |\n\n\n\n## Limitations and Biases\n\nWhile DCLM-Baseline-7B demonstrates strong performance across a range of tasks, it''s important to note:\n\n1. The model may exhibit biases present in its training data, which is derived from web crawl data.\n2. It has not undergone specific alignment or safety fine-tuning, so outputs should be used with caution.\n3. Performance on tasks not included in the evaluation suite may vary.\n4. The model''s knowledge is limited to its training data cutoff date.\n\n## Ethical Considerations\n\nUsers should be aware that this model, like all large language models, can potentially generate harmful or biased content. It should not be used for making decisions about individuals or in sensitive applications without appropriate safeguards and human oversight.\n\n## Citation\n\nIf you use this model in your research, please cite:\n\n```\n@article{Li2024DataCompLM,\n  title={DataComp-LM: In search of the next generation of training sets for language models},\n  author={Jeffrey Li and Alex Fang and Georgios Smyrnis and Maor Ivgi and Matt Jordan and Samir Gadre and Hritik Bansal and Etash Guha and Sedrick Keh and Kushal Arora and [... full author list]},\n  journal={arXiv preprint arXiv:2406.11794},\n  year={2024}\n}\n```\n', '{"pipeline_tag":null,"library_name":"transformers","framework":"transformers","params":6889674752,"storage_bytes":27558732152,"files_count":14,"spaces_count":3,"gated":false,"private":false,"config":{"architectures":["OpenLMModel"],"model_type":"openlm","tokenizer_config":{"unk_token":"<|endoftext|>","bos_token":"<|endoftext|>","eos_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:mlfoundations:dclm","source_url":"https://github.com/mlfoundations/dclm"},{"type":"has_code","target_id":"github:mlfoundations:open_lm.git","source_url":"https://github.com/mlfoundations/open_lm.git"},{"type":"has_code","target_id":"github:mosaicml:llm-foundry","source_url":"https://github.com/mosaicml/llm-foundry"},{"type":"based_on_paper","target_id":"arxiv:2406.11794","source_url":"https://arxiv.org/abs/2406.11794"}]', NULL, 'apple-ascl', 'approved', 64.2, 'b0372a7069c23680c9392b851bce5ef7', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-R1-Distill-Llama-8B', 'huggingface--deepseek-ai--deepseek-r1-distill-llama-8b', 'DeepSeek-R1-Distill-Llama-8B', 'deepseek-ai', '--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/d...', '["transformers","safetensors","llama","text-generation","conversational","arxiv:2501.12948","license:mit","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 831, 1113709, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n\n<p align="center">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Paper Link</b>üëÅÔ∏è</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align="center">\n  <img width="80%" src="figures/benchmark.jpg">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model''s reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align="center">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align="center">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align="center">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek''s official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button "DeepThink"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face''s Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: "Please reason step by step, and put your final answer within \boxed{}."\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting "\<think\>\n\n\</think\>") when responding to certain queries, which can adversely affect the model''s performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with "\<think\>\n" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":8030261248,"storage_bytes":16060556354,"files_count":11,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''') %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{% set ns.system_prompt = message[''content''] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{{''<ÔΩúUserÔΩú>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is none %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls'']%}{%- if not ns.is_first %}{{''<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- set ns.is_first = true -%}{%- else %}{{''\\n'' + ''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endfor %}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is not none %}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'' + message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[''content''] %}{% if ''</think>'' in content %}{% set content = content.split(''</think>'')[-1] %}{% endif %}{{''<ÔΩúAssistantÔΩú>'' + content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú><think>\\n''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2501.12948","source_url":"https://arxiv.org/abs/2501.12948"}]', NULL, 'MIT', 'approved', 99.2, '22075d47f48572747c3607c4506bcf1d', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/resolve/main/figures/benchmark.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-R1-Distill-Llama-8B from https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B/resolve/main/figures/benchmark.jpg
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-R1-Distill-Llama-8B.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-segmind-SSD-1B', 'huggingface--segmind--ssd-1b', 'SSD-1B', 'segmind', '--- license: apache-2.0 tags: - text-to-image - ultra-realistic - text-to-image - stable-diffusion - distilled-model - knowledge-distillation pinned: true datasets: - zzliang/GRIT - wanng/midjourney-v5-202304-clean library_name: diffusers --- !image/png Try out the model at Segmind SSD-1B for ‚ö° fastest inference. You can also try it on ü§ó Spaces The Segmind Stable Diffusion Model (SSD-1B) is a **distilled 50% smaller** version of the Stable Diffusion XL (SDXL), offering a **60% speedup** whil...', '["diffusers","safetensors","text-to-image","ultra-realistic","stable-diffusion","distilled-model","knowledge-distillation","dataset:zzliang/grit","dataset:wanng/midjourney-v5-202304-clean","arxiv:2401.02677","license:apache-2.0","endpoints_compatible","diffusers:stablediffusionxlpipeline","region:us"]', 'text-to-image', 827, 13317, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/segmind/SSD-1B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\ntags:\n- text-to-image\n- ultra-realistic\n- text-to-image\n- stable-diffusion\n- distilled-model\n- knowledge-distillation\npinned: true\ndatasets:\n- zzliang/GRIT\n- wanng/midjourney-v5-202304-clean\nlibrary_name: diffusers\n---\n\n# Segmind Stable Diffusion 1B (SSD-1B) Model Card\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/WveKcu7q5PyZEwNezyyMC.png)\n\n## üì£ Read our [technical report](https://huggingface.co/papers/2401.02677) for more details on our disillation method\n\n## AUTOMATIC1111 compatibility added. Supporting file [here](https://huggingface.co/segmind/SSD-1B/blob/main/SSD-1B-A1111.safetensors)\n\n## Demo\n\nTry out the model at [Segmind SSD-1B](https://www.segmind.com/models/ssd-1b?utm_source=hf) for ‚ö° fastest inference. You can also try it on [ü§ó Spaces](https://huggingface.co/spaces/segmind/Segmind-Stable-Diffusion)\n\n## Model Description\n\nThe Segmind Stable Diffusion Model (SSD-1B) is a **distilled 50% smaller** version of the Stable Diffusion XL (SDXL), offering a **60% speedup** while maintaining high-quality text-to-image generation capabilities. It has been trained on diverse datasets, including Grit and Midjourney scrape data, to enhance its ability to create a wide range of visual content based on textual prompts.\n\nThis model employs a knowledge distillation strategy, where it leverages the teachings of several expert models in succession, including SDXL, ZavyChromaXL, and JuggernautXL, to combine their strengths and produce impressive visual outputs.\n\nSpecial thanks to the HF team ü§ó especially [Sayak](https://huggingface.co/sayakpaul), [Patrick](https://github.com/patrickvonplaten) and [Poli](https://huggingface.co/multimodalart) for their collaboration and guidance on this work.\n\n## Image Comparision (SDXL-1.0 vs SSD-1B)\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/mOM_OMxbivVBELad1QQYj.png)\n\n## Usage:\nThis model can be used via the üß® Diffusers library. \n\nMake sure to install diffusers from source by running\n```\npip install git+https://github.com/huggingface/diffusers\n```\n\nIn addition, please install `transformers`, `safetensors` and `accelerate`:\n```\npip install transformers accelerate safetensors\n```\n\nTo use the model, you can run the following:\n\n```py\nfrom diffusers import StableDiffusionXLPipeline\nimport torch\npipe = StableDiffusionXLPipeline.from_pretrained("segmind/SSD-1B", torch_dtype=torch.float16, use_safetensors=True, variant="fp16")\npipe.to("cuda")\n# if using torch < 2.0\n# pipe.enable_xformers_memory_efficient_attention()\nprompt = "An astronaut riding a green horse" # Your prompt here\nneg_prompt = "ugly, blurry, poor quality" # Negative prompt here\nimage = pipe(prompt=prompt, negative_prompt=neg_prompt).images[0]\n```\n### Update: Our model should now be usable in ComfyUI.\n### Please do use negative prompting, and a CFG around 9.0 for the best quality!\n### Model Description\n\n- **Developed by:** [Segmind](https://www.segmind.com/)\n- **Developers:** [Yatharth Gupta](https://huggingface.co/Warlord-K) and [Vishnu Jaddipal](https://huggingface.co/Icar).\n- **Model type:** Diffusion-based text-to-image generative model\n- **License:** Apache 2.0\n- **Distilled From** [stabilityai/stable-diffusion-xl-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n\n\n### Key Features\n\n- **Text-to-Image Generation:** The model excels at generating images from text prompts, enabling a wide range of creative applications.\n\n- **Distilled for Speed:** Designed for efficiency, this model offers a 60% speedup, making it a practical choice for real-time applications and scenarios where rapid image generation is essential.\n\n- **Diverse Training Data:** Trained on diverse datasets, the model can handle a variety of textual prompts and generate corresponding images effectively.\n\n- **Knowledge Distillation:** By distilling knowledge from multiple expert models, the Segmind Stable Diffusion Model combines their strengths and minimizes their limitations, resulting in improved performance.\n\n### Model Architecture\n\nThe SSD-1B Model is a 1.3B Parameter Model which has several layers removed from the Base SDXL Model\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/Qa8Ow-moLQhOvzp-5kGt4.png)\n\n### Training info\n\nThese are the key hyperparameters used during training:\n\n* Steps: 251000\n* Learning rate: 1e-5\n* Batch size: 32\n* Gradient accumulation steps: 4\n* Image resolution: 1024\n* Mixed-precision: fp16\n\n### Multi-Resolution Support\n\n![image/jpeg](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/IwIaIB4nBdMx6Vs5q82cL.jpeg)\n\nSSD-1B can support the following output resolutions.\n\n* 1024 x 1024 (1:1 Square)\n* 1152 x 896 (9:7)\n* 896 x 1152 (7:9)\n* 1216 x 832 (19:13)\n* 832 x 1216 (13:19)\n* 1344 x 768 (7:4 Horizontal)\n* 768 x 1344 (4:7 Vertical)\n* 1536 x 640 (12:5 Horizontal)\n* 640 x 1536 (5:12 Vertical)\n    \n\n### Speed Comparision\n\nWe have observed that SSD-1B is upto 60% faster than the Base SDXL Model. Below is a comparision on an A100 80GB.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/TyymF1OkUjXLrHUp1XF0t.png)\n\nBelow are the speed up metrics on a RTX 4090 GPU.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/62039c2d91d53938a643317d/moMZrlDr-HTFkZlqWHUjQ.png)\n\n### Model Sources\n\nFor research and development purposes, the SSD-1B Model can be accessed via the Segmind AI platform. For more information and access details, please visit [Segmind](https://www.segmind.com/models/ssd-1b).\n\n## Uses\n\n\n### Direct Use\n\nThe Segmind Stable Diffusion Model is suitable for research and practical applications in various domains, including:\n\n- **Art and Design:** It can be used to generate artworks, designs, and other creative content, providing inspiration and enhancing the creative process.\n\n- **Education:** The model can be applied in educational tools to create visual content for teaching and learning purposes.\n\n- **Research:** Researchers can use the model to explore generative models, evaluate its performance, and push the boundaries of text-to-image generation.\n\n- **Safe Content Generation:** It offers a safe and controlled way to generate content, reducing the risk of harmful or inappropriate outputs.\n\n- **Bias and Limitation Analysis:** Researchers and developers can use the model to probe its limitations and biases, contributing to a better understanding of generative models'' behavior.\n\n### Downstream Use\n\nThe Segmind Stable Diffusion Model can also be used directly with the üß® Diffusers library training scripts for further training, including:\n\n- **[LoRA](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_lora_sdxl.py):**\n```bash\nexport MODEL_NAME="segmind/SSD-1B"\nexport VAE_NAME="madebyollin/sdxl-vae-fp16-fix"\nexport DATASET_NAME="lambdalabs/pokemon-blip-captions"\n\naccelerate launch train_text_to_image_lora_sdxl.py \\n  --pretrained_model_name_or_path=$MODEL_NAME \\n  --pretrained_vae_model_name_or_path=$VAE_NAME \\n  --dataset_name=$DATASET_NAME --caption_column="text" \\n  --resolution=1024 --random_flip \\n  --train_batch_size=1 \\n  --num_train_epochs=2 --checkpointing_steps=500 \\n  --learning_rate=1e-04 --lr_scheduler="constant" --lr_warmup_steps=0 \\n  --mixed_precision="fp16" \\n  --seed=42 \\n  --output_dir="sd-pokemon-model-lora-ssd" \\n  --validation_prompt="cute dragon creature" --report_to="wandb" \\n  --push_to_hub\n```\n\n- **[Fine-Tune](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_sdxl.py):**\n```bash\nexport MODEL_NAME="segmind/SSD-1B"\nexport VAE_NAME="madebyollin/sdxl-vae-fp16-fix"\nexport DATASET_NAME="lambdalabs/pokemon-blip-captions"\n\naccelerate launch train_text_to_image_sdxl.py \\n  --pretrained_model_name_or_path=$MODEL_NAME \\n  --pretrained_vae_model_name_or_path=$VAE_NAME \\n  --dataset_name=$DATASET_NAME \\n  --enable_xformers_memory_efficient_attention \\n  --resolution=512 --center_crop --random_flip \\n  --proportion_empty_prompts=0.2 \\n  --train_batch_size=1 \\n  --gradient_accumulation_steps=4 --gradient_checkpointing \\n  --max_train_steps=10000 \\n  --use_8bit_adam \\n  --learning_rate=1e-06 --lr_scheduler="constant" --lr_warmup_steps=0 \\n  --mixed_precision="fp16" \\n  --report_to="wandb" \\n  --validation_prompt="a cute Sundar Pichai creature" --validation_epochs 5 \\n  --checkpointing_steps=5000 \\n  --output_dir="ssd-pokemon-model" \\n  --push_to_hub\n```\n- **[Dreambooth LoRA](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora_sdxl.py):**\n```bash\nexport MODEL_NAME="segmind/SSD-1B"\nexport INSTANCE_DIR="dog"\nexport OUTPUT_DIR="lora-trained-xl"\nexport VAE_PATH="madebyollin/sdxl-vae-fp16-fix"\n\naccelerate launch train_dreambooth_lora_sdxl.py \\n  --pretrained_model_name_or_path=$MODEL_NAME  \\n  --instance_data_dir=$INSTANCE_DIR \\n  --pretrained_vae_model_name_or_path=$VAE_PATH \\n  --output_dir=$OUTPUT_DIR \\n  --mixed_precision="fp16" \\n  --instance_prompt="a photo of sks dog" \\n  --resolution=1024 \\n  --train_batch_size=1 \\n  --gradient_accumulation_steps=4 \\n  --learning_rate=1e-5 \\n  --report_to="wandb" \\n  --lr_scheduler="constant" \\n  --lr_warmup_steps=0 \\n  --max_train_steps=500 \\n  --validation_prompt="A photo of sks dog in a bucket" \\n  --validation_epochs=25 \\n  --seed="0" \\n  --push_to_hub\n```\n\n### Out-of-Scope Use\n\nThe SSD-1B Model is not suitable for creating factual or accurate representations of people, events, or real-world information. It is not intended for tasks requiring high precision and accuracy.\n\n## Limitations and Bias\n\nLimitations & Bias\nThe SSD-1B Model has some challenges in embodying absolute photorealism, especially in human depictions. While it grapples with incorporating clear text and maintaining the fidelity of complex compositions due to its autoencoding approach, these hurdles pave the way for future enhancements. Importantly, the model''s exposure to a diverse dataset, though not a panacea for ingrained societal and digital biases, represents a foundational step towards more equitable technology. Users are encouraged to interact with this pioneering tool with an understanding of its current limitations, fostering an environment of conscious engagement and anticipation for its continued evolution.\n\n## Citation\n```\n@misc{gupta2024progressive,\n      title={Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer Level Loss}, \n      author={Yatharth Gupta and Vishnu V. Jaddipal and Harish Prabhala and Sayak Paul and Patrick Von Platen},\n      year={2024},\n      eprint={2401.02677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":28747978639,"files_count":27,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionXLPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"based_on_paper","target_id":"arxiv:2401.02677","source_url":"https://arxiv.org/abs/2401.02677"}]', NULL, 'Apache-2.0', 'approved', 79.2, '613a66ed381dfcec9824912604c5f89c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-3n-E4B-it', 'huggingface--google--gemma-3n-e4b-it', 'gemma-3n-E4B-it', 'google', '', '["transformers","safetensors","gemma3n","any-to-any","automatic-speech-recognition","automatic-speech-translation","audio-text-to-text","video-text-to-text","image-text-to-text","conversational","arxiv:1905.07830","arxiv:1905.10044","arxiv:1911.11641","arxiv:1904.09728","arxiv:1705.03551","arxiv:1911.01547","arxiv:1907.10641","arxiv:1903.00161","arxiv:2210.03057","arxiv:2502.12404","arxiv:2411.19799","arxiv:2009.03300","arxiv:2502.21228","arxiv:2311.12022","arxiv:2403.07974","arxiv:2108.07732","arxiv:2107.03374","base_model:google/gemma-3n-e4b","base_model:finetune:google/gemma-3n-e4b","license:gemma","endpoints_compatible","region:us"]', 'image-text-to-text', 827, 67258, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-3n-E4B-it","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":7849978192,"storage_bytes":50810768611,"files_count":17,"spaces_count":76,"gated":"manual","private":false,"config":{"architectures":["Gemma3nForConditionalGeneration"],"model_type":"gemma3n","tokenizer_config":{"bos_token":"<bos>","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false},"chat_template_jinja":"{{ bos_token }}\n{%- if messages[0][''role''] == ''system'' -%}\n    {%- if messages[0][''content''] is string -%}\n        {%- set first_user_prefix = messages[0][''content''] + ''\n\n'' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0][''content''][0][''text''] + ''\n\n'' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message[''role''] == ''assistant'') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message[''role''] -%}\n    {%- endif -%}\n    {{ ''<start_of_turn>'' + role + ''\n'' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message[''content''] is string -%}\n        {{ message[''content''] | trim }}\n    {%- elif message[''content''] is iterable -%}\n        {%- for item in message[''content''] -%}\n            {%- if item[''type''] == ''audio'' -%}\n                {{ ''<audio_soft_token>'' }}\n            {%- elif item[''type''] == ''image'' -%}\n                {{ ''<image_soft_token>'' }}\n            {%- elif item[''type''] == ''text'' -%}\n                {{ item[''text''] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ ''<end_of_turn>\n'' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{''<start_of_turn>model\n''}}\n{%- endif -%}\n"}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1903.00161","source_url":"https://arxiv.org/abs/1903.00161"},{"type":"based_on_paper","target_id":"arxiv:2210.03057","source_url":"https://arxiv.org/abs/2210.03057"},{"type":"based_on_paper","target_id":"arxiv:2502.12404","source_url":"https://arxiv.org/abs/2502.12404"},{"type":"based_on_paper","target_id":"arxiv:2411.19799","source_url":"https://arxiv.org/abs/2411.19799"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:2502.21228","source_url":"https://arxiv.org/abs/2502.21228"},{"type":"based_on_paper","target_id":"arxiv:2311.12022","source_url":"https://arxiv.org/abs/2311.12022"},{"type":"based_on_paper","target_id":"arxiv:2403.07974","source_url":"https://arxiv.org/abs/2403.07974"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"}]', NULL, 'Gemma', 'approved', 39.2, '2b1720a0f060a5d9feb57c2d2230dccc', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-2b-it', 'huggingface--google--gemma-2b-it', 'gemma-2b-it', 'google', '', '["transformers","safetensors","gguf","gemma","text-generation","conversational","arxiv:2312.11805","arxiv:2009.03300","arxiv:1905.07830","arxiv:1911.11641","arxiv:1904.09728","arxiv:1905.10044","arxiv:1907.10641","arxiv:1811.00937","arxiv:1809.02789","arxiv:1911.01547","arxiv:1705.03551","arxiv:2107.03374","arxiv:2108.07732","arxiv:2110.14168","arxiv:2304.06364","arxiv:2206.04615","arxiv:1804.06876","arxiv:2110.08193","arxiv:2009.11462","arxiv:2101.11718","arxiv:1804.09301","arxiv:2109.07958","arxiv:2203.09509","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 826, 67406, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-2b-it","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":2506172416,"storage_bytes":52310489489,"files_count":12,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["GemmaForCausalLM"],"model_type":"gemma","tokenizer_config":{"bos_token":"<bos>","chat_template":"{{ bos_token }}{% if messages[0][''role''] == ''system'' %}{{ raise_exception(''System role not supported'') }}{% endif %}{% for message in messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% if (message[''role''] == ''assistant'') %}{% set role = ''model'' %}{% else %}{% set role = message[''role''] %}{% endif %}{{ ''<start_of_turn>'' + role + ''\n'' + message[''content''] | trim + ''<end_of_turn>\n'' }}{% endfor %}{% if add_generation_prompt %}{{''<start_of_turn>model\n''}}{% endif %}","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2312.11805","source_url":"https://arxiv.org/abs/2312.11805"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1811.00937","source_url":"https://arxiv.org/abs/1811.00937"},{"type":"based_on_paper","target_id":"arxiv:1809.02789","source_url":"https://arxiv.org/abs/1809.02789"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2206.04615","source_url":"https://arxiv.org/abs/2206.04615"},{"type":"based_on_paper","target_id":"arxiv:1804.06876","source_url":"https://arxiv.org/abs/1804.06876"},{"type":"based_on_paper","target_id":"arxiv:2110.08193","source_url":"https://arxiv.org/abs/2110.08193"},{"type":"based_on_paper","target_id":"arxiv:2009.11462","source_url":"https://arxiv.org/abs/2009.11462"},{"type":"based_on_paper","target_id":"arxiv:2101.11718","source_url":"https://arxiv.org/abs/2101.11718"},{"type":"based_on_paper","target_id":"arxiv:1804.09301","source_url":"https://arxiv.org/abs/1804.09301"},{"type":"based_on_paper","target_id":"arxiv:2109.07958","source_url":"https://arxiv.org/abs/2109.07958"},{"type":"based_on_paper","target_id":"arxiv:2203.09509","source_url":"https://arxiv.org/abs/2203.09509"}]', NULL, 'Gemma', 'approved', 39.2, '97736eb4875d69c74c9a738d5cab0bba', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-black-forest-labs-FLUX.1-Krea-dev', 'huggingface--black-forest-labs--flux.1-krea-dev', 'FLUX.1-Krea-dev', 'black-forest-labs', '', '["diffusers","safetensors","text-to-image","image-generation","flux","en","base_model:black-forest-labs/flux.1-dev","base_model:finetune:black-forest-labs/flux.1-dev","license:other","endpoints_compatible","diffusers:fluxpipeline","region:us"]', 'text-to-image', 826, 36332, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":57893109574,"files_count":29,"spaces_count":79,"gated":"auto","private":false,"config":{"diffusers":{"_class_name":"FluxPipeline"}}}', '[]', '[]', NULL, 'Other', 'approved', 39.2, '0303983206abb03d72d21182d9c5ca85', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-30B-A3B', 'huggingface--qwen--qwen3-30b-a3b', 'Qwen3-30B-A3B', 'Qwen', '--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-30B-A3B/blob/main/LICENSE pipeline_tag: text-generation base_model: - Qwen/Qwen3-30B-A3B-Base --- <a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> Qwen3 is the latest generation of large language models in Qwen series,...', '["transformers","safetensors","qwen3_moe","text-generation","conversational","arxiv:2309.00071","arxiv:2505.09388","base_model:qwen/qwen3-30b-a3b-base","base_model:finetune:qwen/qwen3-30b-a3b-base","license:apache-2.0","endpoints_compatible","region:us"]', 'text-generation', 823, 390194, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-30B-A3B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-30B-A3B/blob/main/LICENSE\npipeline_tag: text-generation\nbase_model:\n- Qwen/Qwen3-30B-A3B-Base\n---\n\n# Qwen3-30B-A3B\n<a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-30B-A3B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 30.5B in total and 3.3B activated\n- Number of Paramaters (Non-Embedding): 29.9B\n- Number of Layers: 48\n- Number of Attention Heads (GQA): 32 for Q and 4 for KV\n- Number of Experts: 128\n- Number of Activated Experts: 8\n- Context Length: 32,768 natively and [131,072 tokens with YaRN](#processing-long-texts). \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Quickstart\n\nThe code of Qwen3-MoE has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: ''qwen3_moe''\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen3-30B-A3B"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\n\n# prepare the model input\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")\n\nprint("thinking content:", thinking_content)\nprint("content:", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-30B-A3B --reasoning-parser qwen3\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-30B-A3B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model''s thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model''s behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model''s thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name="Qwen/Qwen3-30B-A3B"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{"role": "user", "content": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors="pt")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({"role": "user", "content": user_input})\n        self.history.append({"role": "assistant", "content": response})\n\n        return response\n\n# Example Usage\nif __name__ == "__main__":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = "How many r''s in strawberries?"\n    print(f"User: {user_input_1}")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f"Bot: {response_1}")\n    print("----------------------")\n\n    # Second input with /no_think\n    user_input_2 = "Then, how many r''s in blueberries? /no_think"\n    print(f"User: {user_input_2}")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f"Bot: {response_2}") \n    print("----------------------")\n\n    # Third input with /think\n    user_input_3 = "Really? /think"\n    print(f"User: {user_input_3}")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f"Bot: {response_3}")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    ''model'': ''Qwen3-30B-A3B'',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # ''model_type'': ''qwen_dashscope'',\n    # ''api_key'': os.getenv(''DASHSCOPE_API_KEY''),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    ''model_server'': ''http://localhost:8000/v1'',  # api_base\n    ''api_key'': ''EMPTY'',\n\n    # Other parameters:\n    # ''generate_cfg'': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         ''thought_in_content'': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {''mcpServers'': {  # You can specify the MCP configuration file\n            ''time'': {\n                ''command'': ''uvx'',\n                ''args'': [''mcp-server-time'', ''--local-timezone=Asia/Shanghai'']\n            },\n            "fetch": {\n                "command": "uvx",\n                "args": ["mcp-server-fetch"]\n            }\n        }\n    },\n  ''code_interpreter'',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{''role'': ''user'', ''content'': ''https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen''}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Processing Long Texts\n\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model''s performance on context lengths of up to 131,072 tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers` and `llama.cpp` for local use, `vllm` and `sglang` for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\n\n- Modifying the model files:\n  In the `config.json` file, add the `rope_scaling` fields:\n    ```json\n    {\n        ...,\n        "rope_scaling": {\n            "rope_type": "yarn",\n            "factor": 4.0,\n            "original_max_position_embeddings": 32768\n        }\n    }\n    ```\n  For `llama.cpp`, you need to regenerate the GGUF file after the modification.\n\n- Passing command line arguments:\n\n  For `vllm`, you can use\n    ```shell\n    vllm serve ... --rope-scaling ''{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}'' --max-model-len 131072\n    ```\n\n  For `sglang`, you can use\n    ```shell\n    python -m sglang.launch_server ... --json-model-override-args ''{"rope_scaling":{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}}''\n    ```\n\n  For `llama-server` from `llama.cpp`, you can use\n    ```shell\n    llama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\n    ```\n\n> [!IMPORTANT]\n> If you encounter the following warning\n> ```\n> Unrecognized keys in `rope_scaling` for ''rope_type''=''yarn'': {''original_max_position_embeddings''}\n> ```\n> please upgrade `transformers>=4.51.0`.\n\n> [!NOTE]\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set `factor` as 2.0. \n\n> [!NOTE]\n> The default `max_position_embeddings` in `config.json` is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\n\n> [!TIP]\n> The endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include "Please reason step by step, and put your final answer within \boxed{}." in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: "Please show your choice in the `answer` field with only the choice letter, e.g., `"answer": "C"`."\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":30532122624,"storage_bytes":61077998302,"files_count":26,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen3MoeForCausalLM"],"model_type":"qwen3_moe","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(''<tool_response>'') and message.content.endswith(''</tool_response>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if ''</think>'' in content %}\n                {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n                {%- set content = content.split(''</think>'')[-1].lstrip(''\\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- ''<|im_start|>'' + message.role + ''\\n<think>\\n'' + reasoning_content.strip(''\\n'') + ''\\n</think>\\n\\n'' + content.lstrip(''\\n'') }}\n            {%- else %}\n                {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- ''<think>\\n\\n</think>\\n\\n'' }}\n    {%- endif %}\n{%- endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen3","source_url":"https://github.com/QwenLM/Qwen3"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Agent","source_url":"https://github.com/QwenLM/Qwen-Agent"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2505.09388","source_url":"https://arxiv.org/abs/2505.09388"}]', NULL, 'Apache-2.0', 'approved', 79.2, 'cf19637160f60d09cf918a19e5dd5273', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Salesforce-blip-image-captioning-base', 'huggingface--salesforce--blip-image-captioning-base', 'blip-image-captioning-base', 'Salesforce', '--- pipeline_tag: image-to-text tags: - image-captioning languages: - en license: bsd-3-clause --- Model card for image captioning pretrained on COCO dataset - base architecture (with ViT base backbone). | !BLIP.gif | |:--:| | <b> Pull figure from BLIP official repo | Image source: https://github.com/salesforce/BLIP </b>| Authors from the paper write in the abstract: *Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-tra...', '["transformers","pytorch","tf","blip","image-to-text","image-captioning","arxiv:2201.12086","license:bsd-3-clause","endpoints_compatible","region:us"]', 'image-to-text', 822, 2379522, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Salesforce/blip-image-captioning-base","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\npipeline_tag: image-to-text\ntags:\n- image-captioning\nlanguages:\n- en\nlicense: bsd-3-clause\n---\n\n# BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\n\nModel card for image captioning pretrained on COCO dataset - base architecture (with ViT base backbone).\n\n| ![BLIP.gif](https://cdn-uploads.huggingface.co/production/uploads/1670928184033-62441d1d9fdefb55a0b7d12c.gif) |\n|:--:|\n| <b> Pull figure from BLIP official repo | Image source: https://github.com/salesforce/BLIP </b>|\n\n## TL;DR\n\nAuthors from the [paper](https://arxiv.org/abs/2201.12086) write in the abstract:\n\n*Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.*\n\n## Usage\n\nYou can use this model for conditional and un-conditional image captioning\n\n### Using the Pytorch model\n\n#### Running the model on CPU\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")\nmodel = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")\n\nimg_url = ''https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(''RGB'')\n\n# conditional image captioning\ntext = "a photography of"\ninputs = processor(raw_image, text, return_tensors="pt")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors="pt")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n```\n</details>\n\n#### Running the model on GPU\n\n##### In full precision \n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")\nmodel = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to("cuda")\n\nimg_url = ''https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(''RGB'')\n\n# conditional image captioning\ntext = "a photography of"\ninputs = processor(raw_image, text, return_tensors="pt").to("cuda")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors="pt").to("cuda")\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n```\n</details>\n\n##### In half precision (`float16`)\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nimport torch\nimport requests\nfrom PIL import Image\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nprocessor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")\nmodel = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base", torch_dtype=torch.float16).to("cuda")\n\nimg_url = ''https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'' \nraw_image = Image.open(requests.get(img_url, stream=True).raw).convert(''RGB'')\n\n# conditional image captioning\ntext = "a photography of"\ninputs = processor(raw_image, text, return_tensors="pt").to("cuda", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n# >>> a photography of a woman and her dog\n\n# unconditional image captioning\ninputs = processor(raw_image, return_tensors="pt").to("cuda", torch.float16)\n\nout = model.generate(**inputs)\nprint(processor.decode(out[0], skip_special_tokens=True))\n>>> a woman sitting on the beach with her dog\n```\n</details>\n\n## Ethical Considerations\nThis release is for research purposes only in support of an academic paper. Our models, datasets, and code are not specifically designed or evaluated for all downstream purposes. We strongly recommend users evaluate and address potential concerns related to accuracy, safety, and fairness before deploying this model. We encourage users to consider the common limitations of AI, comply with applicable laws, and leverage best practices when selecting use cases, particularly for high-risk scenarios where errors or misuse could significantly impact people‚Äôs lives, rights, or safety. For further guidance on use cases, refer to our AUP and AI AUP.\n\n\n## BibTex and citation info\n\n```\n@misc{https://doi.org/10.48550/arxiv.2201.12086,\n  doi = {10.48550/ARXIV.2201.12086},\n  \n  url = {https://arxiv.org/abs/2201.12086},\n  \n  author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},\n  \n  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  \n  title = {BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},\n  \n  publisher = {arXiv},\n  \n  year = {2022},\n  \n  copyright = {Creative Commons Attribution 4.0 International}\n}\n```\n', '{"pipeline_tag":"image-to-text","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":5939080634,"files_count":10,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BlipForConditionalGeneration"],"model_type":"blip","tokenizer_config":{"cls_token":"[CLS]","mask_token":"[MASK]","pad_token":"[PAD]","sep_token":"[SEP]","unk_token":"[UNK]"}}}', '[]', '[{"type":"has_code","target_id":"github:salesforce:BLIP","source_url":"https://github.com/salesforce/BLIP"},{"type":"based_on_paper","target_id":"arxiv:2201.12086","source_url":"https://arxiv.org/abs/2201.12086"}]', NULL, 'BSD-3-Clause', 'approved', 64.2, '95c08ec889de7d221134556e89f950c3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-BAAI-bge-reranker-v2-m3', 'huggingface--baai--bge-reranker-v2-m3', 'bge-reranker-v2-m3', 'BAAI', '--- license: apache-2.0 pipeline_tag: text-classification tags: - transformers - sentence-transformers - text-embeddings-inference language: - multilingual --- **More details please refer to our Github: FlagEmbedding.** - Model List - Usage - Fine-tuning - Evaluation - Citation Different from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. You can get a relevance score by inputting query and passage to the reranker. And the sc...', '["sentence-transformers","safetensors","xlm-roberta","text-classification","transformers","text-embeddings-inference","multilingual","arxiv:2312.15503","arxiv:2402.03216","license:apache-2.0","deploy:azure","region:us"]', 'text-classification', 821, 2894061, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/BAAI/bge-reranker-v2-m3","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\npipeline_tag: text-classification\ntags:\n- transformers\n- sentence-transformers\n- text-embeddings-inference\nlanguage:\n- multilingual\n---\n\n# Reranker\n\n**More details please refer to our Github: [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/tree/master).**\n\n- [Model List](#model-list)\n- [Usage](#usage)\n- [Fine-tuning](#fine-tune)\n- [Evaluation](#evaluation)\n- [Citation](#citation)\n\nDifferent from embedding model, reranker uses question and document as input and directly output similarity instead of embedding. \nYou can get a relevance score by inputting query and passage to the reranker. \nAnd the score can be mapped to a float value in [0,1] by sigmoid function.\n\n\n## Model List\n\n| Model                                                                     | Base model                                                           | Language | layerwise |                           feature                            |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base) | [xlm-roberta-base](https://huggingface.co/xlm-roberta-base) | Chinese and English |     -     | Lightweight reranker model, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | [xlm-roberta-large](https://huggingface.co/FacebookAI/xlm-roberta-large) | Chinese and English |     -     | Lightweight reranker model, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | [bge-m3](https://huggingface.co/BAAI/bge-m3) |    Multilingual     |     -     | Lightweight reranker model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) |      [gemma-2b](https://huggingface.co/google/gemma-2b)      |    Multilingual     |     -     | Suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | [MiniCPM-2B-dpo-bf16](https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16) |    Multilingual     |   8-40    | Suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |\n\n\nYou can select the model according your senario and resource. \n- For **multilingual**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma)\n\n- For **Chinese or English**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise). \n\n- For **efficiency**, utilize [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) and the low layer of [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise). \n\n- For better performance, recommand [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) and [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma)\n\n## Usage \n### Using FlagEmbedding\n\n```\npip install -U FlagEmbedding\n```\n\n#### For normal reranker (bge-reranker-base / bge-reranker-large / bge-reranker-v2-m3 )\n\nGet relevance scores (higher scores indicate more relevance):\n\n```python\nfrom FlagEmbedding import FlagReranker\nreranker = FlagReranker(''BAAI/bge-reranker-v2-m3'', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n\nscore = reranker.compute_score([''query'', ''passage''])\nprint(score) # -5.65234375\n\n# You can map the scores into 0-1 by set "normalize=True", which will apply sigmoid function to the score\nscore = reranker.compute_score([''query'', ''passage''], normalize=True)\nprint(score) # 0.003497010252573502\n\nscores = reranker.compute_score([[''what is panda?'', ''hi''], [''what is panda?'', ''The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'']])\nprint(scores) # [-8.1875, 5.26171875]\n\n# You can map the scores into 0-1 by set "normalize=True", which will apply sigmoid function to the score\nscores = reranker.compute_score([[''what is panda?'', ''hi''], [''what is panda?'', ''The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'']], normalize=True)\nprint(scores) # [0.00027803096387751553, 0.9948403768236574]\n```\n\n#### For LLM-based reranker\n\n```python\nfrom FlagEmbedding import FlagLLMReranker\nreranker = FlagLLMReranker(''BAAI/bge-reranker-v2-gemma'', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = FlagLLMReranker(''BAAI/bge-reranker-v2-gemma'', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n\nscore = reranker.compute_score([''query'', ''passage''])\nprint(score)\n\nscores = reranker.compute_score([[''what is panda?'', ''hi''], [''what is panda?'', ''The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'']])\nprint(scores)\n```\n\n#### For LLM-based layerwise reranker\n\n```python\nfrom FlagEmbedding import LayerWiseFlagLLMReranker\nreranker = LayerWiseFlagLLMReranker(''BAAI/bge-reranker-v2-minicpm-layerwise'', use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n# reranker = LayerWiseFlagLLMReranker(''BAAI/bge-reranker-v2-minicpm-layerwise'', use_bf16=True) # You can also set use_bf16=True to speed up computation with a slight performance degradation\n\nscore = reranker.compute_score([''query'', ''passage''], cutoff_layers=[28]) # Adjusting ''cutoff_layers'' to pick which layers are used for computing the score.\nprint(score)\n\nscores = reranker.compute_score([[''what is panda?'', ''hi''], [''what is panda?'', ''The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'']], cutoff_layers=[28])\nprint(scores)\n```\n\n### Using Huggingface transformers\n\n#### For normal reranker (bge-reranker-base / bge-reranker-large / bge-reranker-v2-m3 )\n\nGet relevance scores (higher scores indicate more relevance):\n\n```python\nimport torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(''BAAI/bge-reranker-v2-m3'')\nmodel = AutoModelForSequenceClassification.from_pretrained(''BAAI/bge-reranker-v2-m3'')\nmodel.eval()\n\npairs = [[''what is panda?'', ''hi''], [''what is panda?'', ''The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'']]\nwith torch.no_grad():\n    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors=''pt'', max_length=512)\n    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()\n    print(scores)\n```\n\n#### For LLM-based reranker\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = "Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either ''Yes'' or ''No''."\n    sep = "\n"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)[''input_ids'']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)[''input_ids'']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f''A: {query}'',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f''B: {passage}'',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs[''input_ids''],\n            sep_inputs + passage_inputs[''input_ids''],\n            truncation=''only_second'',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item[''input_ids''] = item[''input_ids''] + sep_inputs + prompt_inputs\n        item[''attention_mask''] = [1] * len(item[''input_ids''])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors=''pt'',\n    )\n\ntokenizer = AutoTokenizer.from_pretrained(''BAAI/bge-reranker-v2-gemma'')\nmodel = AutoModelForCausalLM.from_pretrained(''BAAI/bge-reranker-v2-gemma'')\nyes_loc = tokenizer(''Yes'', add_special_tokens=False)[''input_ids''][0]\nmodel.eval()\n\npairs = [[''what is panda?'', ''hi''], [''what is panda?'', ''The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'']]\nwith torch.no_grad():\n    inputs = get_inputs(pairs, tokenizer)\n    scores = model(**inputs, return_dict=True).logits[:, -1, yes_loc].view(-1, ).float()\n    print(scores)\n```\n\n#### For LLM-based layerwise reranker\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndef get_inputs(pairs, tokenizer, prompt=None, max_length=1024):\n    if prompt is None:\n        prompt = "Given a query A and a passage B, determine whether the passage contains an answer to the query by providing a prediction of either ''Yes'' or ''No''."\n    sep = "\n"\n    prompt_inputs = tokenizer(prompt,\n                              return_tensors=None,\n                              add_special_tokens=False)[''input_ids'']\n    sep_inputs = tokenizer(sep,\n                           return_tensors=None,\n                           add_special_tokens=False)[''input_ids'']\n    inputs = []\n    for query, passage in pairs:\n        query_inputs = tokenizer(f''A: {query}'',\n                                 return_tensors=None,\n                                 add_special_tokens=False,\n                                 max_length=max_length * 3 // 4,\n                                 truncation=True)\n        passage_inputs = tokenizer(f''B: {passage}'',\n                                   return_tensors=None,\n                                   add_special_tokens=False,\n                                   max_length=max_length,\n                                   truncation=True)\n        item = tokenizer.prepare_for_model(\n            [tokenizer.bos_token_id] + query_inputs[''input_ids''],\n            sep_inputs + passage_inputs[''input_ids''],\n            truncation=''only_second'',\n            max_length=max_length,\n            padding=False,\n            return_attention_mask=False,\n            return_token_type_ids=False,\n            add_special_tokens=False\n        )\n        item[''input_ids''] = item[''input_ids''] + sep_inputs + prompt_inputs\n        item[''attention_mask''] = [1] * len(item[''input_ids''])\n        inputs.append(item)\n    return tokenizer.pad(\n            inputs,\n            padding=True,\n            max_length=max_length + len(sep_inputs) + len(prompt_inputs),\n            pad_to_multiple_of=8,\n            return_tensors=''pt'',\n    )\n\ntokenizer = AutoTokenizer.from_pretrained(''BAAI/bge-reranker-v2-minicpm-layerwise'', trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(''BAAI/bge-reranker-v2-minicpm-layerwise'', trust_remote_code=True, torch_dtype=torch.bfloat16)\nmodel = model.to(''cuda'')\nmodel.eval()\n\npairs = [[''what is panda?'', ''hi''], [''what is panda?'', ''The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'']]\nwith torch.no_grad():\n    inputs = get_inputs(pairs, tokenizer).to(model.device)\n    all_scores = model(**inputs, return_dict=True, cutoff_layers=[28])\n    all_scores = [scores[:, -1].view(-1, ).float() for scores in all_scores[0]]\n    print(all_scores)\n```\n\n## Fine-tune\n\n### Data Format\n\nTrain data should be a json file, where each line is a dict like this:\n\n```\n{"query": str, "pos": List[str], "neg":List[str], "prompt": str}\n```\n\n`query` is the query, and `pos` is a list of positive texts, `neg` is a list of negative texts, `prompt` indicates the relationship between query and texts. If you have no negative texts for a query, you can random sample some from the entire corpus as the negatives.\n\nSee [toy_finetune_data.jsonl](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker/toy_finetune_data.jsonl) for a toy data file.\n\n### Train\n\nYou can fine-tune the reranker with the following code:\n\n**For llm-based reranker**\n\n```shell\ntorchrun --nproc_per_node {number of gpus} \\n-m FlagEmbedding.llm_reranker.finetune_for_instruction.run \\n--output_dir {path to save model} \\n--model_name_or_path google/gemma-2b \\n--train_data ./toy_finetune_data.jsonl \\n--learning_rate 2e-4 \\n--num_train_epochs 1 \\n--per_device_train_batch_size 1 \\n--gradient_accumulation_steps 16 \\n--dataloader_drop_last True \\n--query_max_len 512 \\n--passage_max_len 512 \\n--train_group_size 16 \\n--logging_steps 1 \\n--save_steps 2000 \\n--save_total_limit 50 \\n--ddp_find_unused_parameters False \\n--gradient_checkpointing \\n--deepspeed stage1.json \\n--warmup_ratio 0.1 \\n--bf16 \\n--use_lora True \\n--lora_rank 32 \\n--lora_alpha 64 \\n--use_flash_attn True \\n--target_modules q_proj k_proj v_proj o_proj\n```\n\n**For llm-based layerwise reranker**\n\n```shell\ntorchrun --nproc_per_node {number of gpus} \\n-m FlagEmbedding.llm_reranker.finetune_for_layerwise.run \\n--output_dir {path to save model} \\n--model_name_or_path openbmb/MiniCPM-2B-dpo-bf16 \\n--train_data ./toy_finetune_data.jsonl \\n--learning_rate 2e-4 \\n--num_train_epochs 1 \\n--per_device_train_batch_size 1 \\n--gradient_accumulation_steps 16 \\n--dataloader_drop_last True \\n--query_max_len 512 \\n--passage_max_len 512 \\n--train_group_size 16 \\n--logging_steps 1 \\n--save_steps 2000 \\n--save_total_limit 50 \\n--ddp_find_unused_parameters False \\n--gradient_checkpointing \\n--deepspeed stage1.json \\n--warmup_ratio 0.1 \\n--bf16 \\n--use_lora True \\n--lora_rank 32 \\n--lora_alpha 64 \\n--use_flash_attn True \\n--target_modules q_proj k_proj v_proj o_proj \\n--start_layer 8 \\n--head_multi True \\n--head_type simple \\n--lora_extra_parameters linear_head\n```\n\nOur rerankers are initialized from [google/gemma-2b](https://huggingface.co/google/gemma-2b) (for llm-based reranker) and [openbmb/MiniCPM-2B-dpo-bf16](https://huggingface.co/openbmb/MiniCPM-2B-dpo-bf16) (for llm-based layerwise reranker), and we train it on a mixture of multilingual datasets:\n\n- [bge-m3-data](https://huggingface.co/datasets/Shitao/bge-m3-data)\n- [quora train data](https://huggingface.co/datasets/quora)\n- [fever train data](https://fever.ai/dataset/fever.html)\n\n## Evaluation\n\n- llama-index.\n\n![image-20240317193909373](./assets/llama-index.png)\n\n\n- BEIR.   \n\nrereank the top 100 results from bge-en-v1.5 large.\n\n![image-20240317174633333](./assets/BEIR-bge-en-v1.5.png)\n\nrereank the top 100 results from e5 mistral 7b instruct.\n\n![image-20240317172949713](./assets/BEIR-e5-mistral.png)\n\n- CMTEB-retrieval.   \nIt rereank the top 100 results from bge-zh-v1.5 large.\n\n![image-20240317173026235](./assets/CMTEB-retrieval-bge-zh-v1.5.png)\n\n- miracl (multi-language).   \nIt rereank the top 100 results from bge-m3.\n\n![image-20240317173117639](./assets/miracl-bge-m3.png)\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star and citation\n\n```bibtex\n@misc{li2023making,\n      title={Making Large Language Models A Better Foundation For Dense Retrieval}, \n      author={Chaofan Li and Zheng Liu and Shitao Xiao and Yingxia Shao},\n      year={2023},\n      eprint={2312.15503},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n@misc{chen2024bge,\n      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, \n      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},\n      year={2024},\n      eprint={2402.03216},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```', '{"pipeline_tag":"text-classification","library_name":"sentence-transformers","framework":"sentence-transformers","params":567755777,"storage_bytes":7403932631,"files_count":13,"spaces_count":74,"gated":false,"private":false,"config":{"architectures":["XLMRobertaForSequenceClassification"],"model_type":"xlm-roberta","tokenizer_config":{"bos_token":"<s>","cls_token":"<s>","eos_token":"</s>","mask_token":"<mask>","pad_token":"<pad>","sep_token":"</s>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:FlagOpen:FlagEmbedding","source_url":"https://github.com/FlagOpen/FlagEmbedding"},{"type":"has_code","target_id":"github:FlagOpen:FlagEmbedding","source_url":"https://github.com/FlagOpen/FlagEmbedding"},{"type":"based_on_paper","target_id":"arxiv:2312.15503","source_url":"https://arxiv.org/abs/2312.15503"},{"type":"based_on_paper","target_id":"arxiv:2402.03216","source_url":"https://arxiv.org/abs/2402.03216"}]', NULL, 'Apache-2.0', 'approved', 99.1, '92440e2d175d77e6e771b1cfa509ddd3', NULL, 'https://huggingface.co/BAAI/bge-reranker-v2-m3/resolve/main/assets/BEIR-bge-en-v1.5.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-BAAI-bge-reranker-v2-m3 from https://huggingface.co/BAAI/bge-reranker-v2-m3/resolve/main/assets/BEIR-bge-en-v1.5.png
Image converted to WebP: data/images/huggingface-BAAI-bge-reranker-v2-m3.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tencent-HunyuanVideo-1.5', 'huggingface--tencent--hunyuanvideo-1.5', 'HunyuanVideo-1.5', 'tencent', '--- library_name: HunyuanVideo-1.5 license: other license_name: tencent-hunyuan-community license_link: https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/master/LICENSE language: - en - zh tags: - text-to-video - image-to-video pipeline_tag: text-to-video extra_gated_eu_disallowed: true --- ‰∏≠ÊñáÊñáÊ°£ <div align="center"> <img src="./assets/logo.png" alt="HunyuanVideo-1.5 Logo" width="80%"> </div> <div align="center"> <!-- <img src="./assets/banner.png" alt="HunyuanVideo-1.5 Banner" width="8...', '["hunyuanvideo-1.5","diffusers","safetensors","text-to-video","image-to-video","en","zh","arxiv:2511.18870","license:other","region:us"]', 'text-to-video', 820, 2944, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tencent/HunyuanVideo-1.5","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: HunyuanVideo-1.5\nlicense: other\nlicense_name: tencent-hunyuan-community\nlicense_link: https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/master/LICENSE\nlanguage:\n  - en\n  - zh\ntags:\n  - text-to-video\n  - image-to-video\npipeline_tag: text-to-video\nextra_gated_eu_disallowed: true\n---\n\n\n[‰∏≠ÊñáÊñáÊ°£](./README_CN.md)\n\n# HunyuanVideo-1.5\n\n<div align="center">\n\n<img src="./assets/logo.png" alt="HunyuanVideo-1.5 Logo" width="80%">\n\n# üé¨ HunyuanVideo-1.5: A leading lightweight video generation model\n\n</div>\n\n\n<div align="center">\n<!-- <img src="./assets/banner.png" alt="HunyuanVideo-1.5 Banner" width="800"> -->\n\n</div>\n\n\nHunyuanVideo-1.5 is a video generation model that delivers top-tier quality with only 8.3B parameters, significantly lowering the barrier to usage. It runs smoothly on consumer-grade GPUs, making it accessible for every developer and creator. This repository provides the implementation and tools needed to generate creative videos.\n\n\n<div align="center">\n  <a href="https://hunyuan.tencent.com/video/zh?tabIndex=0" target="_blank"><img src=https://img.shields.io/badge/Official%20Site-333399.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/tencent/HunyuanVideo-1.5 target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5 target="_blank"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n  <a href="https://arxiv.org/pdf/2511.18870" target="_blank"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n  <a href=https://x.com/TencentHunyuan target="_blank"><img src=https://img.shields.io/badge/Hunyuan-black.svg?logo=x height=22px></a>\n  <a href="https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/main/assets/HunyuanVideo_1_5_Prompt_Handbook_EN.md" target="_blank"><img src=https://img.shields.io/badge/üìö-PromptHandBook-blue.svg?logo=book height=22px></a> <br/>\n  <a href="./ComfyUI/README.md" target="_blank"><img src=https://img.shields.io/badge/ComfyUI-blue.svg?logo=book height=22px></a>\n  <a href="https://github.com/ModelTC/LightX2V" target="_blank"><img src=https://img.shields.io/badge/LightX2V-yellow.svg?logo=book height=22px></a>\n  <a href="https://tusi.cn/models/933574988890423836" target="_blank"><img src=https://img.shields.io/badge/ÂêêÂè∏-purple.svg?logo=book height=22px></a>\n  <a href="https://tensor.art/models/933574988890423836" target="_blank"><img src=https://img.shields.io/badge/TensorArt-cyan.svg?logo=book height=22px></a>\n\n</div>\n\n\n<p align="center">\n    üëè Join our <a href="./assets/wechat.png" target="_blank">WeChat</a> and <a href="https://discord.gg/ehjWMqF5wY">Discord</a> | \nüíª <a href="https://hunyuan.tencent.com/video/zh?tabIndex=0">Official website Try our model!</a>&nbsp&nbsp\n</p>\n\n## üî•üî•üî• News\n* üöÄ Dec 05, 2025: **New Release**: We now release the [480p I2V step-distilled model](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_i2v_step_distilled), which generates videos in 8 or 12 steps (recommended)! On RTX 4090, end-to-end generation time is reduced by 75%, and a single RTX 4090 can generate videos within 75 seconds. The step-distilled model maintains comparable quality to the original model while achieving significant speedup. See [Step Distillation Comparison](./assets/step_distillation_comparison.md) for detailed quality comparisons. For even faster generation, you can also try 4 steps (faster speed with slightly reduced quality). **To enable the step-distilled model, run `generate.py` with the `--enable_step_distill` parameter.** See [Usage](#-usage) for detailed usage instructions. üî•üî•üî•üÜï\n* üìö Dec 05, 2025: **Training Code Released**: We now open-source the training code for HunyuanVideo-1.5! The training script (`train.py`) provides a full training pipeline with support for distributed training, FSDP, context parallel, gradient checkpointing, and more. HunyuanVideo-1.5 is trained using the Muon optimizer, which we have open-sourced in the [Training](#-training) section. **If you would like to continue training our model or fine-tune it with LoRA, please use the Muon optimizer.** See [Training](#-training) section for detailed usage instructions. üî•üî•üî•üÜï\n* üéâ **Diffusers Support**: HunyuanVideo-1.5 is now available on Hugging Face Diffusers! Check out [Diffusers collection](https://huggingface.co/collections/hunyuanvideo-community/hunyuanvideo-15) for easy integration. üî•üî•üî•üÜï\n* üöÄ Nov 27, 2025: We now support cache inference (deepcache, teacache, taylorcache), achieving significant speedup! Pull the latest code to try it. üî•üî•üî•üÜï \n* üöÄ Nov 24, 2025: We now support deepcache inference.\n* üëã Nov 20, 2025: We release the inference code and model weights of HunyuanVideo-1.5.\n\n\n## üé• Demo\n<div align="center">\n  <video src="https://github.com/user-attachments/assets/d45ec78e-ea40-47f1-8d4d-f4d9a0682e2d" width="60%"> </video>\n</div>\n\n## üß© Community Contributions\n\nIf you develop/use HunyuanVideo-1.5 in your projects, welcome to let us know.\n\n- **Diffusers** - [HunyuanVideo-1.5 Diffusers](https://huggingface.co/collections/hunyuanvideo-community/hunyuanvideo-15): Official Hugging Face Diffusers integration for HunyuanVideo-1.5. Easily use HunyuanVideo-1.5 with the Diffusers library for seamless integration into your projects. See [Usage with Diffusers](#usage-with-diffusers) section for details.\n\n- **ComfyUI** - [ComfyUI](https://github.com/comfyanonymous/ComfyUI): A powerful and modular diffusion model GUI with a graph/nodes interface. ComfyUI supports HunyuanVideo-1.5 with various engineering optimizations for fast inference. We provide a [ComfyUI Usage Guide](./ComfyUI/README.md) for HunyuanVideo-1.5.\n\n- **Community-implemented ComfyUI Plugin** - [comfyui_hunyuanvideo_1.5_plugin](https://github.com/yuanyuan-spec/comfyui_hunyuanvideo_1.5_plugin): A community-implemented ComfyUI plugin for HunyuanVideo-1.5, offering both simplified and complete node sets for quick usage or deep workflow customization, with built-in automatic model download support.\n\n- **LightX2V** - [LightX2V](https://github.com/ModelTC/LightX2V): A lightweight and efficient video generation framework that integrates HunyuanVideo-1.5, supporting multiple engineering acceleration techniques for fast inference.\n\n- **Wan2GP v9.62** - [Wan2GP](https://github.com/deepbeepmeep/Wan2GP): WanGP is a very low VRAM app (as low 6 GB of VRAM for Hunyuan Video 1.5) supports Lora Accelerator for a 8 steps generation and offers tools to facilitate Video Generation.\n\n- **ComfyUI-MagCache** - [ComfyUI-MagCache](https://github.com/Zehong-Ma/ComfyUI-MagCache): MagCache is a training-free caching approach that accelerates video generation by estimating fluctuating differences among model outputs across timesteps. It achieves 1.7x speedup for HunyuanVideo-1.5 with 20 inference steps.\n\n\n## üìë Open-source Plan\n- HunyuanVideo-1.5 (T2V/I2V)\n  - [x] Inference Code and checkpoints\n  - [x] ComfyUI Support\n  - [x] LightX2V Support\n  - [x] Diffusers Support\n  - [ ] Release all model weights (Sparse attention, distill model, and SR models)\n\n## üìã Table of Contents\n- [üî•üî•üî• News](#-news)\n- [üé• Demo](#-demo)\n- [üß© Community Contributions](#-community-contributions)\n- [üìë Open-source Plan](#-open-source-plan)\n- [üìñ Introduction](#-introduction)\n- [‚ú® Key Features](#-key-features)\n- [üìú System Requirements](#-system-requirements)\n- [üõ†Ô∏è Dependencies and Installation](#Ô∏è-dependencies-and-installation)\n- [üß± Download Pretrained Models](#-download-pretrained-models)\n- [üìù Prompt Guide](#-prompt-guide)\n- [üîë Inference](#-inference)\n  - [Inference with Source Code](#inference-with-source-code)\n  - [Usage with Diffusers](#usage-with-diffusers)\n  - [Prompt Enhancement](#prompt-enhancement)\n  - [Text to Video](#text-to-video)\n  - [Image to Video](#image-to-video)\n  - [Command Line Arguments](#command-line-arguments)\n  - [Optimal Inference Configurations](#optimal-inference-configurations)\n- [üéì Training](#-training)\n- [üé¨ More Examples](#-more-examples)\n- [üìä Evaluation](#-evaluation)\n- [üìö Citation](#-citation)\n- [üôè Acknowledgements](#-acknowledgements)\n- [üåü Github Star History](#-github-star-history)\n\n\n## üìñ Introduction\nWe present HunyuanVideo-1.5, a lightweight yet powerful video generation model that achieves state-of-the-art visual quality and motion coherence with only 8.3 billion parameters, enabling efficient inference on consumer-grade GPUs. This achievement is built upon several key components, including meticulous data curation, an advanced DiT architecture with selective and sliding tile attention(SSTA), enhanced bilingual understanding through glyph-aware text encoding , progressive pre-training and post-training, and an efficient video super-resolution network. Leveraging these designs, we developed a unified framework capable of high-quality text-to-video and image-to-video generation across multiple durations and resolutions. Extensive experiments demonstrate that this compact and proficient model establishes a new state-of-the-art among open-source models. By releasing the code and weights of HunyuanVideo-1.5, we provide the community with a high-performance foundation that significantly lowers the cost of video creation and research, making advanced video generation more accessible to all.\n\n\n## ‚ú® Key Features\n- **Lightweight High-Performance Architecture**: We propose an efficient architecture that integrates an 8.3B-parameter Diffusion Transformer (DiT) with a 3D causal VAE, achieving compression ratios of 16√ó in spatial dimensions and 4√ó along the temporal axis. Additionally, the innovative SSTA (Selective and Sliding Tile Attention) mechanism prunes redundant spatiotemporal kv blocks, significantly reduces computational overhead for long video sequences and accelerates inference, achieving an end-to-end speedup of $1.87 \times$ in 10-second 720p video synthesis compared to FlashAttention-3.\n\n<div align="center">\n<img src="./assets/hy_video_1_5_dit.png" alt="HunyuanVideo-1.5 DiT" width="600">\n</div> \n\n\n- **Video Super-Resolution Enhancement**: We develop an efficient few-step super-resolution network that upscales outputs to 1080p. It enhances sharpness while correcting distortions, thereby refining details and overall visual texture.\n\n<div align="center">\n<img src="./assets/hy_video_1_5_vsr.png" alt="HunyuanVideo-1.5 VSR" width="600">\n</div> \n\n- **End-to-End Training Optimization**: This work employs a multi-stage, progressive training strategy covering the entire pipeline from pre-training to post-training. Combined with the Muon optimizer to accelerate convergence, this approach holistically refines motion coherence, aesthetic quality, and human preference alignment, achieving professional-grade content generation.\n\n## üìú System Requirements\n\n### Hardware Requirements\n\n- **GPU**: NVIDIA GPU with CUDA support\n- **Minimum GPU Memory**: 14 GB (with model offloading enabled)\n  \n  > **Note:** The memory requirements above are measured with model offloading enabled. If your GPU has sufficient memory, you may disable offloading for improved inference speed.\n\n### Software Requirements\n\n- **Operating System**: Linux\n- **Python**: Python 3.10 or higher\n- **CUDA**: Compatible CUDA version for your PyTorch installation\n\n## üõ†Ô∏è Dependencies and Installation\n\n### Step 1: Clone the Repository\n\n```bash\ngit clone https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.git\ncd HunyuanVideo-1.5\n```\n\n### Step 2: Install Basic Dependencies\n\n```bash\npip install -r requirements.txt\npip install -i https://mirrors.tencent.com/pypi/simple/ --upgrade tencentcloud-sdk-python\n```\n\n### Step 3: Install Attention Libraries\n\n* Flash Attention: \n  Install Flash Attention for faster inference and reduced GPU memory consumption.\n  Detailed installation instructions are available at [Flash Attention](https://github.com/Dao-AILab/flash-attention).\n\n* Flex-Block-Attention: \n  flex-block-attn is only required for sparse attention to achieve faster inference and can be installed by the following command:\n  ```bash\n  git clone https://github.com/Tencent-Hunyuan/flex-block-attn.git\n  cd flex-block-attn\n  git submodule update --init --recursive\n  python3 setup.py install\n  ```\n\n* SageAttention: \n  To enable SageAttention for faster inference, you need to install it by the following command:\n  > **Note**: Enabling SageAttention will automatically disable Flex-Block-Attention.\n  ```bash\n  git clone https://github.com/cooper1637/SageAttention.git\n  cd SageAttention \n  export EXT_PARALLEL=4 NVCC_APPEND_FLAGS="--threads 8" MAX_JOBS=32 # Optional\n  python3 setup.py install\n  ```\n\n## üß± Download Pretrained Models\n\n> üí° Distillation models and sparse attention models are still coming soon. Please stay tuned for the latest updates on the Hugging Face Model Card.\n\nDownload the pretrained models before generating videos. Detailed instructions are available at [checkpoints-download.md](checkpoints-download.md).\n\n### Model Cards\n|ModelName| Download                     |\n|-|---------------------------| \n|HunyuanVideo-1.5-480P-T2V|[480P-T2V](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_t2v) |\n|HunyuanVideo-1.5-480P-I2V |[480P-I2V](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_i2v) |\n|HunyuanVideo-1.5-480P-T2V-cfg-distill | [480P-T2V-cfg-distill](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_t2v_distilled) |\n|HunyuanVideo-1.5-480P-I2V-cfg-distill |[480P-I2V-cfg-distill](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_i2v_distilled) |\n|HunyuanVideo-1.5-480P-I2V-step-distill |[480P-I2V-step-distill](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/480p_i2v_step_distilled) |\n|HunyuanVideo-1.5-720P-T2V|[720P-T2V](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/720p_t2v) |\n|HunyuanVideo-1.5-720P-I2V |[720P-I2V](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/720p_i2v) |\n|HunyuanVideo-1.5-720P-T2V-cfg-distill| Comming soon |\n|HunyuanVideo-1.5-720P-I2V-cfg-distill |[720P-I2V-cfg-distill](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/720p_i2v_distilled) |\n|HunyuanVideo-1.5-720P-T2V-sparse-cfg-distill| Comming soon |\n|HunyuanVideo-1.5-720P-I2V-sparse-cfg-distill |[720P-I2V-sparse-cfg-distill](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/720p_i2v_distilled_sparse) |\n|HunyuanVideo-1.5-720P-sr-step-distill |[720P-sr](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/720p_sr_distilled) |\n|HunyuanVideo-1.5-1080P-sr-step-distill |[1080P-sr](https://huggingface.co/tencent/HunyuanVideo-1.5/tree/main/transformer/1080p_sr_distilled) |\n\n## üìù Prompt Guide\n### Prompt Writing Handbook\nPrompt enhancement plays a crucial role in enabling our model to generate high-quality videos. By writing longer and more detailed prompts, the generated video will be significantly improved. We encourage you to craft comprehensive and descriptive prompts to achieve the best possible video quality. we recommend community partners consulting our official guide on how to write effective prompts. \n\n**Reference:** **[HunyuanVideo-1.5 Prompt Handbook](https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5/blob/main/assets/HunyuanVideo_1_5_Prompt_Handbook_EN.md)**\n\n### System Prompts for Automatic Prompt Enhancement\nFor users seeking to optimize prompts for other large models, it is recommended to consult the definition of `t2v_rewrite_system_prompt` in the file `hyvideo/utils/rewrite/t2v_prompt.py` to guide text-to-video rewriting. Similarly, for image-to-video rewriting, refer to the definition of `i2v_rewrite_system_prompt` in `hyvideo/utils/rewrite/i2v_prompt.py`.\n\n## üîë Inference\n\n### Inference with Source Code\n\n\nFor prompt rewriting, we recommend using Gemini or models deployed via vLLM. This codebase currently only supports models compatible with the vLLM API. If you wish to use Gemini, you will need to implement your own interface calls.\n\nFor models with a vLLM API, note that T2V (text-to-video) and I2V (image-to-video) have different recommended models and environment variables:\n\n- T2V: use [Qwen3-235B-A22B-Thinking-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507), configure `T2V_REWRITE_BASE_URL` and `T2V_REWRITE_MODEL_NAME`\n- I2V: use [Qwen3-VL-235B-A22B-Instruct](https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct), configure `I2V_REWRITE_BASE_URL` and `I2V_REWRITE_MODEL_NAME`\n\n> You may set the above model names to any other vLLM-compatible models you have deployed (including HuggingFace models).  \n> Rewriting is enabled by default (`--rewrite` defaults to `true`); to disable it explicitly, use `--rewrite false` or `--rewrite 0`. If no vLLM endpoint is configured, the pipeline runs without remote rewriting.\n\nExample: Generate a video (works for both T2V and I2V; set `IMAGE_PATH=none` for T2V or provide an image path for I2V)\n\n> üí° **Tip**: For faster inference speed, you can enable the step-distilled model using the `--enable_step_distill` parameter. The step-distilled model (480p I2V) can generate videos in 8 or 12 steps (recommended), achieving up to 75% speedup on RTX 4090 while maintaining comparable quality.\n>\n> **Tips:** If your GPU memory is > 14GB but you encounter OOM (Out of Memory) errors during generation, you can try setting the following environment variable before running:\n> ```bash\n> export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:128\n> ```\n> \n> **Tips:** If you have limited CPU memory and encounter OOM during inference, you can try disable overlapped group offloading by adding the following argument:\n> ```bash\n> --overlap_group_offloading false\n> ```\n\n```bash\nexport T2V_REWRITE_BASE_URL="<your_vllm_server_base_url>"\nexport T2V_REWRITE_MODEL_NAME="<your_model_name>"\nexport I2V_REWRITE_BASE_URL="<your_vllm_server_base_url>"\nexport I2V_REWRITE_MODEL_NAME="<your_model_name>"\n\nPROMPT=''A girl holding a paper with words "Hello, world!"''\n\nIMAGE_PATH=/path/to/image.png # Optional, none or <image path> to enable i2v mode\nSEED=1\nASPECT_RATIO=16:9\nRESOLUTION=480p\nOUTPUT_PATH=./outputs/output.mp4\nMODEL_PATH=./ckpts # Path to pretrained model\n\n# Configuration for faster inference\nN_INFERENCE_GPU=8 # Parallel inference GPU count\nCFG_DISTILLED=true # Inference with CFG distilled model, 2x speedup\nSAGE_ATTN=true # Inference with SageAttention\nSPARSE_ATTN=false # Inference with sparse attention (only 720p models are equipped with sparse attention). Please ensure flex-block-attn is installed\nOVERLAP_GROUP_OFFLOADING=true # Only valid when group offloading is enabled, significantly increases CPU memory usage but speeds up inference\nENABLE_CACHE=true # Enable feature cache during inference. Significantly speeds up inference.\nCACHE_TYPE=deepcache # Support: deepcache, teacache, taylorcache\nENABLE_STEP_DISTILL=true # Enable step distilled model for 480p I2V, recommended 8 or 12 steps, up to 6x speedup\n\n\n# Configuration for better quality\nREWRITE=true # Enable prompt rewriting. Please ensure rewrite vLLM server is deployed and configured.\nENABLE_SR=true # Enable super resolution\n\n\ntorchrun --nproc_per_node=$N_INFERENCE_GPU generate.py \\n  --prompt "$PROMPT" \\n  --image_path $IMAGE_PATH \\n  --resolution $RESOLUTION \\n  --aspect_ratio $ASPECT_RATIO \\n  --seed $SEED \\n  --rewrite $REWRITE \\n  --cfg_distilled $CFG_DISTILLED \\n  --enable_step_distill $ENABLE_STEP_DISTILL \\n  --sparse_attn $SPARSE_ATTN --use_sageattn $SAGE_ATTN \\n  --enable_cache $ENABLE_CACHE --cache_type $CACHE_TYPE \\n  --overlap_group_offloading $OVERLAP_GROUP_OFFLOADING \\n  --sr $ENABLE_SR --save_pre_sr_video \\n  --output_path $OUTPUT_PATH \\n  --model_path $MODEL_PATH\n```\n\n\n\n### Command Line Arguments\n\n| Argument | Type | Required | Default | Description |\n|----------|------|----------|---------|-------------|\n| `--prompt` | str | Yes | - | Text prompt for video generation |\n| `--negative_prompt` | str | No | `''''` | Negative prompt for video generation |\n| `--resolution` | str | Yes | - | Video resolution: `480p` or `720p` |\n| `--model_path` | str | Yes | - | Path to pretrained model directory |\n| `--aspect_ratio` | str | No | `16:9` | Aspect ratio of the output video |\n| `--num_inference_steps` | int | No | `50` | Number of inference steps |\n| `--video_length` | int | No | `121` | Number of frames to generate |\n| `--seed` | int | No | `123` | Random seed for reproducibility |\n| `--image_path` | str | No | `None` | Path to reference image (enables i2v mode). Use `none` or `None` to explicitly use text-to-video mode |\n| `--output_path` | str | No | `None` | Output file path (if not provided, saves to `./outputs/output_{transformer_version}_{timestamp}.mp4`) |\n| `--sr` | bool | No | `true` | Enable super resolution (use `--sr false` or `--sr 0` to disable) |\n| `--save_pre_sr_video` | bool | No | `false` | Save original video before super resolution (use `--save_pre_sr_video` or `--save_pre_sr_video true` to enable, only effective when super resolution is enabled) |\n| `--rewrite` | bool | No | `true` | Enable prompt rewriting (use `--rewrite false` or `--rewrite 0` to disable, may result in lower quality video generation) |\n| `--cfg_distilled` | bool | No | `false` | Enable CFG distilled model for faster inference (~2x speedup, use `--cfg_distilled` or `--cfg_distilled true` to enable) |\n| `--enable_step_distill` | bool | No | `false` | Enable step distilled model for 480p I2V (recommended 8 or 12 steps, ~75% speedup on RTX 4090, use `--enable_step_distill` or `--enable_step_distill true` to enable) |\n| `--sparse_attn` | bool | No | `false` | Enable sparse attention for faster inference (~1.5-2x speedup, requires H-series GPUs, auto-enables CFG distilled, use `--sparse_attn` or `--sparse_attn true` to enable) |\n| `--offloading` | bool | No | `true` | Enable CPU offloading (use `--offloading false` or `--offloading 0` to disable for faster inference if GPU memory allows) |\n| `--group_offloading` | bool | No | `None` | Enable group offloading (default: None, automatically enabled if offloading is enabled. Use `--group_offloading` or `--group_offloading true/1` to enable, `--group_offloading false/0` to disable) |\n| `--overlap_group_offloading` | bool | No | `true` | Enable overlap group offloading (default: true). Significantly increases CPU memory usage but speeds up inference. Use `--overlap_group_offloading` or `--overlap_group_offloading true/1` to enable, `--overlap_group_offloading false/0` to disable |\n| `--dtype` | str | No | `bf16` | Data type for transformer: `bf16` (faster, lower memory) or `fp32` (better quality, slower, higher memory) |\n| `--use_sageattn` | bool | No | `false` | Enable SageAttention (use `--use_sageattn` or `--use_sageattn true/1` to enable, `--use_sageattn false/0` to disable) |\n| `--sage_blocks_range` | str | No | `0-53` | SageAttention blocks range (e.g., `0-5` or `0,1,2,3,4,5`) |\n| `--enable_cache` | bool | No | `false` | Enable cache for transformer (use `--enable_cache` or `--enable_cache true/1` to enable, `--enable_cache false/0` to disable) |\n| `--cache_type` | str | No | `deepcache` | Cache type for transformer (e.g., `deepcache, teacache, taylorcache`) |\n| `--no_cache_block_id` | str | No | `53` | Blocks to exclude from deepcache (e.g., `0-5` or `0,1,2,3,4,5`) |\n| `--cache_start_step` | int | No | `11` | Start step to skip when using cache |\n| `--cache_end_step` | int | No | `45` | End step to skip when using cache |\n| `--total_steps` | int | No | `50` | Total inference steps |\n| `--cache_step_interval` | int | No | `4` | Step interval to skip when using cache |\n\n**Note:** Use `--nproc_per_node` to specify the number of GPUs. For example, `--nproc_per_node=8` uses 8 GPUs.\n\n### Optimal Inference Configurations\n\nThe following table provides the optimal inference configurations (CFG scale, embedded CFG scale, flow shift, and inference steps) for each model to achieve the best generation quality:\n\n| Model | CFG Scale | Embedded CFG Scale | Flow Shift | Inference Steps |\n|-------|-----------|-------------------|------------|-----------------|\n| 480p T2V | 6 | None | 5 | 50 |\n| 480p I2V | 6 | None | 5 | 50 |\n| 720p T2V | 6 | None | 9 | 50 |\n| 720p I2V | 6 | None | 7 | 50 |\n| 480p T2V CFG Distilled | 1 | None | 5 | 50 |\n| 480p I2V CFG Distilled | 1 | None | 5 | 50 |\n| 480p I2V Step Distilled | 1 | None | 7 | 8 or 12 (recommended) |\n| 720p T2V CFG Distilled | 1 | None | 9 | 50 |\n| 720p I2V CFG Distilled | 1 | None | 7 | 50 |\n| 720p T2V CFG Distilled Sparse | 1 | None | 9 | 50 |\n| 720p I2V CFG Distilled Sparse | 1 | None | 7 | 50 |\n| 480‚Üí720 SR Step Distilled | 1 | None | 2 | 6 |\n| 720‚Üí1080 SR Step Distilled | 1 | None | 2 | 8 |\n\n**Please note that the cfg distilled model we provided, must use 50 steps to generate correct results.**\n\n### Usage with Diffusers\n\nHunyuanVideo-1.5 is available on Hugging Face Diffusers! You can easily use it with the Diffusers library:\n\n**Basic Usage:**\n\n```python\nimport torch\n\ndtype = torch.bfloat16\ndevice = "cuda:0"\n\nfrom diffusers import HunyuanVideo15Pipeline\nfrom diffusers.utils import export_to_video\n\npipe = HunyuanVideo15Pipeline.from_pretrained("hunyuanvideo-community/HunyuanVideo-1.5-Diffusers-720p_t2v", torch_dtype=dtype)\npipe.enable_model_cpu_offload()\npipe.vae.enable_tiling()\n\ngenerator = torch.Generator(device=device).manual_seed(seed)\n\nvideo = pipe(\n    prompt=prompt,\n    generator=generator,\n    num_frames=121,\n    num_inference_steps=50,\n).frames[0]\n\nexport_to_video(video, "output.mp4", fps=24)\n```\n\n**Optimized Usage with Attention Backend:**\n\nHunyuanVideo-1.5 uses attention masks with variable-length sequences. For best performance, we recommend using an attention backend that handles padding efficiently.\n\nWe recommend installing kernels (`pip install kernels`) to access prebuilt attention kernels.\n\n```python\nimport torch\n\ndtype = torch.bfloat16\ndevice = "cuda:0"\n\nfrom diffusers import HunyuanVideo15Pipeline, attention_backend\nfrom diffusers.utils import export_to_video\n\npipe = HunyuanVideo15Pipeline.from_pretrained("hunyuanvideo-community/HunyuanVideo-1.5-Diffusers-720p_t2v", torch_dtype=dtype)\npipe.enable_model_cpu_offload()\npipe.vae.enable_tiling()\n\ngenerator = torch.Generator(device=device).manual_seed(seed)\n\nwith attention_backend("_flash_3_hub"): # or `"flash_hub"` if you are not on H100/H800\n    video = pipe(\n        prompt=prompt,\n        generator=generator,\n        num_frames=121,\n        num_inference_steps=50,\n    ).frames[0]\n    export_to_video(video, "output.mp4", fps=24)\n```\n\nFor more details, please visit [HunyuanVideo-1.5 Diffusers Collection](https://huggingface.co/collections/hunyuanvideo-community/hunyuanvideo-15).\n\n\n## üéì Training\n\nHunyuanVideo-1.5 is trained using the **Muon optimizer**, which accelerates convergence and improves training stability. The Muon optimizer combines momentum-based updates with Newton-Schulz orthogonalization for efficient optimization of large-scale video generation models.\n\n### Quick Start\n\nThe training script (`train.py`) provides a complete training pipeline for HunyuanVideo-1.5. Here''s how to use it:\n\n#### 1. Implement Your DataLoader\n\nReplace the `create_dummy_dataloader()` function in `train.py` with your own implementation. Your dataloader should return batches with the following format:\n\n- **Required fields:**\n  - `"pixel_values"`: `torch.Tensor` - Video: `[B, C, F, H, W]` or Image: `[B, C, H, W]`\n    - Note: For video data, temporal dimension F must be `4n+1` (e.g., 1, 5, 9, 13, 17, ...)\n  - `"text"`: `List[str]` - Text prompts for each sample\n  - `"data_type"`: `str` - `"video"` or `"image"`\n\n- **Optional fields (for performance optimization):**\n  - `"latents"`: Pre-encoded VAE latents (skips VAE encoding for faster training)\n  - `"byt5_text_ids"` and `"byt5_text_mask"`: Pre-tokenized byT5 inputs\n\nSee the `create_dummy_dataloader()` function in `train.py` for detailed batch format documentation.\n\n#### 2. Run Training\n\n**Single GPU:**\n```bash\npython train.py --pretrained_model_root <path_to_pretrained_model> [other args]\n```\n\n**Multi-GPU:**\n```bash\nN=8\ntorchrun --nproc_per_node=$N train.py --pretrained_model_root <path_to_pretrained_model> [other args]\n```\n\n**Example:**\n```bash\ntorchrun --nproc_per_node=8 train.py \\n  --pretrained_model_root ./ckpts \\n  --learning_rate 1e-5 \\n  --batch_size 1 \\n  --max_steps 10000 \\n  --output_dir ./outputs \\n  --enable_fsdp \\n  --enable_gradient_checkpointing \\n  --sp_size 8\n```\n\n#### 3. Key Training Parameters\n\n| Parameter | Description | Default |\n|-----------|-------------|---------|\n| `--pretrained_model_root` | Path to pretrained model (required) | - |\n| `--learning_rate` | Learning rate | 1e-5 |\n| `--batch_size` | Batch size | 1 |\n| `--max_steps` | Maximum training steps | 10000 |\n| `--warmup_steps` | Warmup steps | 500 |\n| `--gradient_accumulation_steps` | Gradient accumulation steps | 1 |\n| `--enable_fsdp` | Enable FSDP for distributed training | true |\n| `--enable_gradient_checkpointing` | Enable gradient checkpointing | true |\n| `--sp_size` | Sequence parallelism size (must divide world_size) | 8 |\n| `--i2v_prob` | Probability of i2v task for video data | 0.3 |\n| `--use_muon` | Use Muon optimizer | true |\n| `--resume_from_checkpoint` | Resume from checkpoint directory | None |\n\n#### 4. Monitor Training\n\n- Checkpoints are saved to `output_dir` at intervals specified by `--save_interval`\n- Validation videos are generated at intervals specified by `--validation_interval`\n- Training logs are printed to console at intervals specified by `--log_interval`\n\n#### 5. Resume Training\n\nUse `--resume_from_checkpoint <checkpoint_dir>` to resume from a saved checkpoint:\n```bash\npython train.py \\n  --pretrained_model_root <path> \\n  --resume_from_checkpoint ./outputs/checkpoint-1000\n```\n\n\n## üìä Evaluation\n\n### Rating\nWe assess text-to-video generation using a comprehensive rating methodology that considers five key dimensions: text-video consistency, visual quality, structural stability, motion effects, and the aesthetic quality of individual frames. For image-to-video generation, the evaluation encompasses image-video consistency, instruction responsiveness, visual quality, structural stability, and motion effects.\n\n<div align="center">\n<img src="./assets/T2V_Rating.png" alt="rating result of t2v" width="800">\n</div> \n\n---\n\n<div align="center">\n<img src="./assets/I2V_Rating.png" alt="rating result of i2v" width="800">\n</div> \n\n\n### GSB\nThe GSB(Good/Same/Bad) approach is widely used to evaluate the relative performance of two models based on overall video perception quality.We carefully construct 300 diverse text prompts and 300 image samples to cover balanced application scenarios for both text-to-video and image-to-video tasks. For each prompt or image input, an equal number of video samples are generated by each model in a single run to ensure comparability. To maintain fairness, inference is performed only once per input without any cherry-picking of results. All competing models are evaluated using their default configurations. The evaluation is conducted by over 100 professional assessors\n\n<div align="center">\n<img src="./assets/T2V_GSB.png" alt="gsb result of t2v" width="800">\n</div>\n\n---\n\n<div align="center">\n<img src="./assets/I2V_GSB.png" alt="gsb result of i2v" width="800">\n</div> \n\n\n### Inference speed\nWe report inference speed with basic engineering-level acceleration techniques enabled on 8 H800 GPUs to demonstrate practical performance achievable in real-world deployment scenarios.\nPlease note that in this experiment, we do not pursue the most extreme acceleration at the cost of generation quality, but rather to achieve notable speed improvements while maintaining nearly identical output quality.\n\nWe report the total inference time for 50 diffusion steps for HunyuanVideo 1.5 below:\n\n<div align="center">\n<img src="./assets/speed.png" alt="" width="100%">\n</div> \n\n## üé¨ More Examples\n|Features|Demo1|Demo2|\n|------|------|------|\n|Strong Instruction Following|<video src="https://github.com/user-attachments/assets/fdc3c27b-69f5-46a1-b707-0b57510fa32f" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```‰∏ÄÂêçÂìÄ‰º§ÁöÑÈªëÂèë‰∏≠ÂõΩÂ•≥Â≠êÂáùÊúõÂ§©Á©∫ÔºåÂ§çÂè§ËÉ∂ÁâáÈ£éÊ†ºÁÉòÊâòÂá∫ÊÄÄÊóßÊàèÂâßÊ∞õÂõ¥``` </details> <details><summary>üìã Show rewrite prompt</summary> ```‰øØËßÜËßíÂ∫¶Ôºå‰∏Ä‰ΩçÊúâÁùÄÊ∑±Ëâ≤ÔºåÁï•Â∏¶Âáå‰π±ÁöÑÈïøÂç∑ÂèëÁöÑÂπ¥ËΩª‰∏≠ÂõΩÂ•≥ÊÄßÔºå‰Ω©Êà¥ÁùÄÈó™ËÄÄÁöÑÁèçÁè†È°πÈìæÂíåÂúÜÂΩ¢ÈáëËâ≤ËÄ≥ÁéØÔºåÂ•πÂáå‰π±ÁöÑÂ§¥ÂèëË¢´È£éÂêπÊï£ÔºåÂ•πÂæÆÂæÆÊä¨Â§¥ÔºåÊúõÂêëÂ§©Á©∫ÔºåÁ•ûÊÉÖÂçÅÂàÜÂìÄ‰º§ÔºåÁúº‰∏≠Âê´ÁùÄÊ≥™Ê∞¥„ÄÇÂò¥ÂîáÊ∂ÇÁùÄÁ∫¢Ëâ≤Âè£Á∫¢„ÄÇËÉåÊôØÊòØÂ∏¶ÊúâÂçé‰∏ΩÁ∫¢Ëâ≤Ëä±Á∫πÁöÑÂõæÊ°à„ÄÇÁîªÈù¢ÂëàÁé∞Â§çÂè§ÁîµÂΩ±È£éÊ†ºÔºåËâ≤Ë∞É‰ΩéÈ•±ÂíåÔºåÂ∏¶ÁùÄËΩªÂæÆÊüîÁÑ¶ÔºåÁÉòÊâòÊÉÖÁª™Ê∞õÂõ¥ÔºåË¥®ÊÑü‰ªø‰Ωõ20‰∏ñÁ∫™90Âπ¥‰ª£ÁöÑÁªèÂÖ∏ËÉ∂ÁâáÈ£éÊ†ºÔºåËê•ÈÄ†Âá∫ÊÄÄÊóß‰∏îÂØåÊúâÊàèÂâßÊÄßÁöÑÊÑüËßâ„ÄÇ``` </details>|<video src="https://github.com/user-attachments/assets/3fcb42cc-cdd3-4651-86a6-645a858561c4" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```Âª∫Á≠ëËìùÂõæ‰∏äÁöÑÁ∫øÊù°Âåñ‰∏∫ÂÆû‰ΩìÔºåÁû¨Èó¥ÁîüÈïøÂá∫‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÂ§çÂè§Â∑•‰∏öÈ£éÂäûÂÖ¨Á©∫Èó¥„ÄÇ``` </details> <details><summary>üìã Show rewrite prompt</summary> ```‰∏ÄÂ∫ßÁ©∫Êó∑ÁöÑÁé∞‰ª£ÈòÅÊ•ºÈáåÔºåÊúâ‰∏ÄÂº†Èì∫Â±ïÂú®Âú∞Êùø‰∏≠Â§ÆÁöÑÂª∫Á≠ëËìùÂõæ„ÄÇÂøΩÁÑ∂Èó¥ÔºåÂõæÁ∫∏‰∏äÁöÑÁ∫øÊù°Ê≥õËµ∑ÂæÆÂÖâÔºå‰ªø‰ΩõË¢´ÊüêÁßçÊó†ÂΩ¢ÁöÑÂäõÈáèÂî§ÈÜí„ÄÇÁ¥ßÊé•ÁùÄÔºåÈÇ£‰∫õÂèëÂÖâÁöÑÁ∫øÊù°ÂºÄÂßãÂêë‰∏äÂª∂‰º∏Ôºå‰ªéÂπ≥Èù¢‰∏≠Êå£ËÑ±ÔºåÂãæÂãíÂá∫Á´ã‰ΩìÁöÑËΩÆÂªì‚Äî‚ÄîÂ∞±ÂÉèÂú®Á©∫‰∏≠ËøõË°å‰∏ÄÂú∫Êó†Â£∞ÁöÑ3DÊâìÂç∞„ÄÇÈöèÂêéÔºåÂ•áËøπÂú®Âä†ÈÄüÂèëÁîüÔºöÊûÅÁÆÄÁöÑÊ©°Êú®ÂäûÂÖ¨Ê°å„ÄÅ‰ºòÈõÖÁöÑ‰ºäÂßÜÊñØÈ£éÊ†ºÁöÆË¥®Ê§Ö„ÄÅÈ´òÊåëÁöÑÂ∑•‰∏öÈ£éÈáëÂ±û‰π¶Êû∂ÔºåËøòÊúâÂá†ÁõèÁà±Ëø™ÁîüÁÅØÊ≥°Ôºå‰ª•ÂÖâÁ∫π‰∏∫È™®Êû∂ËøÖÈÄü‚ÄúÁîüÈïø‚ÄùÂá∫Êù•„ÄÇËΩ¨Áû¨Èó¥ÔºåÁ∫øÊù°Ë¢´ÁúüÂÆûÁöÑÊùêË¥®Â°´ÂÖÖ‚Äî‚ÄîÊú®ÊùêÁöÑÊ∏©Ê∂¶„ÄÅÁöÆÈù©ÁöÑË¥®ÊÑü„ÄÅÈáëÂ±ûÁöÑÂÜ∑ÈùôÔºåÈÉΩÂú®Áú®ÁúºÈó¥ÂÆåÊï¥ÂëàÁé∞„ÄÇÊúÄÁªàÔºåÊâÄÊúâÂÆ∂ÂÖ∑Á®≥Âõ∫ËêΩÂú∞ÔºåËìùÂõæÁöÑÂÖâËäíÊÇÑÁÑ∂Ë§™Âéª„ÄÇ‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÂäûÂÖ¨Á©∫Èó¥ÔºåÂ∞±ËøôÊ†∑‰ªé‰∫åÁª¥ÁöÑÂõæÁ∫∏‰∏≠ËØûÁîü„ÄÇ``` </details>|\n|Smooth Motion Generation|<video src="https://github.com/user-attachments/assets/447847f0-490a-45f9-a86d-a67ab1ff4231" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```A DJ is immersed in his musical world. He wears a pair of professional, matte-black headphones, revealing a focused expression. He wears a black bomber jacket, zipped open to reveal a T-shirt underneath. His upper body sways back and forth rhythmically to the throbbing electronic beats, his head moving with precise movement. The mixing console in front of him serves as the primary source of light. In the distance, the cool white glow of several stadium floodlights casts a deep, dark haze across the vast field, casting long shadows across the emerald green grass, creating a stark contrast to the brightly lit area surrounding the DJ booth. His hands danced swiftly and precisely across the equipment. The entire scene was filled with high-tech dynamics and the solitary creative passion. Against the backdrop of the vast and silent night stadium, it created an atmosphere of high focus, energy, and a slightly surreal feeling.``` </details> <details><summary>üìã Show rewrite prompt</summary> ```slowly advancing medium shot, shot from a level angle, focuses on the center of an empty football field, where a DJ is immersed in his musical world. He wears a pair of professional, matte-black headphones, one earcup slightly removed, revealing a focused expression and a brow beaded with sweat from his intense concentration. He wears a black bomber jacket, zipped open to reveal a T-shirt underneath. His upper body sways back and forth rhythmically to the throbbing electronic beats, his head moving with precise movement. The mixing console in front of him serves as the primary source of light. In the distance, the cool white glow of several stadium floodlights casts a deep, dark haze across the vast field, casting long shadows across the emerald green grass, creating a stark contrast to the brightly lit area surrounding the DJ booth. His hands danced swiftly and precisely across the equipment, one hand steadily pushing and pulling a long volume fader, while the fingers of the other nimbly jumped between the illuminated knobs and pads, sometimes decisively cutting a bass line, sometimes triggering an echo effect. The entire scene was filled with high-tech dynamics and the solitary creative passion. Against the backdrop of the vast and silent night stadium, it created an atmosphere of high focus, energy, and a slightly surreal feeling.``` </details>|<video src="https://github.com/user-attachments/assets/49057fe8-a102-4fd7-bd92-e9561abb9f45" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```A figure skater performs a rapid, graceful Biellmann spin, captured from all angles.``` </details> <details><summary>üìã Show rewrite prompt</summary> ```The video captures a figure skater performing a Biellmann spin on ice. The subject is a female skater in a glittering costume. Initially, she spins on one leg. Then, she reaches back and pulls her free leg up. Next, she spins rapidly, becoming a blur of motion, with ice shavings spraying from her skate blade. The background is an ice rink with blurred advertising boards. The camera circles around the subject to capture the spin from all angles. The lighting is spotlit, creating lens flares and sparkles on her costume. The overall video presents a graceful artistic sports style.``` </details>|\n|Cinematic Aesthetics|<video src="https://github.com/user-attachments/assets/4098cf72-357d-4b81-97df-6752064ce0c3" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```Âõ∫ÂÆöÈïúÂ§¥,ÁÑ¶ÁÇπÂú®ÂõæÁâáÈáåÁöÑÊåÇÈíü‰∏äÔºåÈïúÂ§¥ËΩªÂæÆÊëáÊôÉËê•ÈÄ†ÊâãÊåÅÊëÑÂΩ±ÊÑüÔºå‚Äãwjw,filmphotos,Film Grain,Reversal film photographyÔºåWong Kar-wai movies,cinematic photography, HK film style,neon lighting, in the style of Wong Kar Wai film``` </details> <details><summary>üìã Show rewrite prompt</summary> ```Handheld lens shooting, the camera focuses on the wall clock hanging on the green-toned wall, shaking slightly. The second hand sweeps steadily across the clock face, and the shadow of the clock cast on the wall shifts subtly with the movement of the lens.``` </details>|<video src="https://github.com/user-attachments/assets/2b4575e5-79f1-4011-bed0-e8380198f7c9" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```The leaves of calamus shine in the sunlight, dotted with dewdrops that trickle down to the ground with the breeze.``` </details> <details><summary>üìã Show rewrite prompt</summary> ```A macro shot focuses on long, slender calamus leaves, rendered in a cinematic photography realistic style. The main leaf, a vibrant, deep green, is positioned diagonally across the frame. Its surface is covered in tiny, glistening spherical dewdrops that catch and refract the bright morning sunlight, creating sparkling highlights. Initially, a larger, perfectly round dewdrop clings to the upper section of the leaf, its surface tension holding it in place. Then, as the leaf sways almost imperceptibly, the dewdrop begins to slowly dislodge. Next, it starts to trickle down the central vein of the leaf, its shape elongating slightly as it moves, leaving a subtle, glistening wet trail in its path. Finally, it reaches the pointed tip of the leaf, hangs for a brief moment, and falls out of the bottom of the frame. In the background, other leaves and blades of grass are softly blurred, creating a beautiful bokeh effect with soft, out-of-focus circles of light. The environment is bathed in the warm, golden glow of early morning sunlight, which streams in from behind the leaves, backlighting them and causing their wet edges to shine brilliantly. The overall impression is one of serene, natural beauty, captured in a highly realistic and detailed manner. This is a macro shot. The camera tilts down very slowly, following the path of the main dewdrop as it travels down the leaf. The lighting is soft and natural, with strong backlighting to create a radiant, glowing effect on the dewdrops and leaf edges, characteristic of professional nature photography. The atmosphere is peaceful and serene. The overall video presents a cinematic photography realistic style.``` </details>|\n|Text Rendering|<video src="https://github.com/user-attachments/assets/7c964fc5-c27e-4bd0-bf3f-eb8fca2caef6" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```ËµõÂçöÊúãÂÖãÈ£éÊ†ºÁöÑÂ§úÊôöË°óËßíÔºå‰∏Ä‰∏™Â∑®Â§ßÁöÑÊãõÁâå‰∏äÔºå ‚ÄúHunyuan Video 1.5‚ÄùÁöÑÈúìËôπÁÅØÁÆ°ËΩÆÂªìÂ∑≤ÁªèÂÆâË£ÖÂ•Ω„ÄÇÈïúÂ§¥Êé®ËøõÔºåÈúìËôπÁÅØ‰ªé‚ÄúH‚ÄùÂºÄÂßãÔºå‰º¥ÈöèÁùÄ‚ÄòÊªãÊªã‚ÄôÁöÑÁîµÊµÅÂ£∞ÔºåÊØè‰∏™Â≠óÊØç‰æùÊ¨°‰∫ÆËµ∑Á≤âÁ¥´Ëâ≤ÁöÑÂÖâËäíÔºåÁõ¥Âà∞ÂÖ®ÈÉ®ÁÇπ‰∫ÆÔºåÁÖß‰∫Æ‰∫ÜÊΩÆÊπøÁöÑË°óÈÅì„ÄÇËµõÂçöÊúãÂÖãÔºåÂüéÂ∏ÇÁæéÂ≠¶``` </details> <details><summary>üìã Show rewrite prompt</summary> ```On a wet street corner in a cyberpunk city at night, a large neon sign reading "Hunyuan Video 1.5" lights up sequentially, illuminating the dark, rainy environment with a pinkish-purple glow. he scene is a dark, rain-slicked street corner in a futuristic, cinematic cyberpunk city. Mounted on the metallic, weathered facade of a building is a massive, unlit neon sign. The sign''s glass tube framework clearly spells out the words "Hunyuan Video 1.5". Initially, the street is dimly lit, with ambient light from distant skyscrapers creating shimmering reflections on the wet asphalt below. Then, the camera zooms in slowly toward the sign. As it moves, a low electrical sizzling sound begins. In the background, the dense urban landscape of the cyberpunk metropolis is visible through a light atmospheric haze, with towering structures adorned with their own flickering advertisements. A complex web of cables and pipes crisscrosses between the buildings. The shot is at a low angle, looking up at the sign to emphasize its grand scale. The lighting is high-contrast and dramatic, dominated by the neon glow which creates sharp, specular reflections and deep shadows. The atmosphere is moody and tech-noir. The overall video presents a cinematic photography realistic style.,``` </details>|<video src="https://github.com/user-attachments/assets/73e8b741-baec-4a40-9d36-a1435172ab64" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```‰∏ÄÂº†Èì∫ÂºÄÁöÑ‰∏≠ÂõΩÂÆ£Á∫∏‰∏äÔºåÊµìÂ¢®Êª¥ÂÖ•Ê∞¥‰∏≠ÔºåÊôïÊüìÂá∫Â£Æ‰∏ΩÁöÑÂ±±Ê∞¥ÁîªËΩÆÂªì„ÄÇÂ±±Â≥∞„ÄÅ‰∫ëÈõæ„ÄÅÂ≠§ËàüÂú®Â¢®Ëâ≤‰∏≠Ëá™ÁÑ∂ÂΩ¢Êàê„ÄÇÈöèÂêéÔºåËøô‰∫õÊ∞¥Â¢®ÂÖÉÁ¥†Â∑ßÂ¶ôÂú∞ÊµÅÂä®„ÄÅÈáçÁªÑÔºåÂú®ÁîªÈù¢ÁöÑÁïôÁôΩÂ§ÑÊ±áËÅöÊàê"Hunyuan Video 1.5"ÁöÑ‰π¶Ê≥ïÂ≠ó‰Ωì„ÄÇ‰ºòÈõÖÔºåËØóÊÑèÔºåÊñáÂåñÂ∫ïËï¥``` </details> <details><summary>üìã Show rewrite prompt</summary> ```A drop of black ink blooms on wet Chinese Xuan paper, forming a landscape painting before the ink elements fluidly reassemble into the calligraphic text "Hunyuan Video 1.5". On a flat, laid-out sheet of off-white Chinese Xuan paper with a subtle, fibrous texture, the scene unfolds. Initially, a single, concentrated drop of deep black ink falls into a clear, wet area at the center of the paper. Then, the ink instantly begins to bloom outwards in intricate, flowing tendrils of varying shades from jet-black to smoky grey. As it spreads, the ink wash naturally and rapidly forms the silhouette of a majestic mountain range with sharp, defined peaks. Next, softer, diluted grey tones billow around the mountains, creating layers of atmospheric mist and clouds, while a simple, dark stroke materializes as a lone boat on a tranquil, watery expanse at the base. As the landscape is formed, the ink elements‚Äîthe lines of the mountains, wisps of cloud, and the shape of the boat‚Äîbegin to deconstruct, dissolving into flowing streams of liquid ink. Finally, these streams move gracefully across the paper''s empty white space, converging and elegantly reorganizing to form the text "Hunyuan Video 1.5" in a fluid, semi-cursive calligraphic style. The background is the minimalist expanse of the Xuan paper itself, its texture providing a subtle depth. The entire process is lit by soft, even, diffused light from above, which enhances the rich tonal variations of the ink and the delicate texture of the paper without creating harsh shadows. Bird''s-eye view. The camera is positioned directly above the subject, capturing the entire process. The camera remains static. The aesthetic is a high-quality, dynamic Chinese ink wash animation style, perfectly simulating the real-world physics of ink spreading on wet paper. The entire sheet of paper and the final text are kept fully within the frame. Poetic, elegant, artistic. The overall video presents a dynamic Chinese ink wash animation style.``` </details>|\n|Physics Compliance|<video src="https://github.com/user-attachments/assets/f1d74e48-cc03-415d-b75f-f7186a4fb41d" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```In a sleek museum gallery, a woman pauses before a gilded oil painting. The painted man inside slowly comes alive, lifting a bottle and pouring real wine straight from the canvas into her glass. Surrounded by stylish art critics moving naturally through the hall, she accepts the pour with calm elegance, as if the impossible were routine. ``` </details> <details><summary>üìã Show rewrite prompt</summary> ```In a sleek museum gallery, a woman receives a glass of wine poured directly from an animated oil painting. A sophisticated woman with dark hair tied back elegantly stands in the mid-ground. She is wearing a simple, black silk sleeveless dress and holds a clear, crystal wine glass in her right hand. She is positioned before a large, baroque-style oil painting in an ornate, gilded frame. Inside the painting, an aristocratic man with a mustache, dressed in a dark velvet doublet with a white lace collar, is depicted. His form is defined by visible, impasto oil brushstrokes. Initially, the woman watches the painting with calm poise. Then, the painted man''s arm slowly animates, his painted texture retained as he lifts a dark bottle. Next, a photorealistic stream of red wine emerges directly from the flat canvas surface, arcing through the air and splashing gently into the real crystal glass she holds. She remains perfectly still, accepting the impossible pour with a subtle, knowing smile. The setting is a modern art gallery with high white walls and polished dark concrete floors that reflect the ambient light. Focused track lighting from the high ceiling casts a warm, dramatic spotlight on the woman and the painting, creating soft shadows. In the background, two other gallery patrons, a man and a woman in stylish, modern attire, stroll slowly from right to left, their figures slightly blurred by a shallow depth of field, moving naturally through the hall. The shot is at an eye-level angle with the woman. The camera remains static, capturing the surreal event in a steady medium shot. The lighting is high-contrast and dramatic, reminiscent of a cinematic photography realistic style, using soft side lighting to accentuate the woman''s features and the texture of the painting. The mood is surreal, elegant, and mysterious. The overall video presents a cinematic photography realistic style.``` </details>|<video src="https://github.com/user-attachments/assets/07bcce06-ff4f-4688-8c60-c02f600635ea" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```An intact soda can is slowly crushed by a hand.``` </details> <details><summary>üìã Show rewrite prompt</summary> ```In a medium close-up, a hand slowly crushes an intact red and white soda can on a wooden table. A male hand with visible, realistic skin texture is wrapped firmly around the middle of an intact, pristine red and white aluminum soda can. The can, covered in glistening condensation droplets, rests on a dark, polished wooden surface. The cinematic realism captures every minute detail of the scene. Initially, the hand''s grip is steady, with the can''s cylindrical shape perfectly preserved. Then, the fingers begin to tighten slowly, the knuckles whitening slightly from the exertion. Next, the smooth aluminum surface starts to buckle under the controlled pressure, a sharp crease forming vertically down its side as the metallic sheen distorts. As the hand continues its deliberate squeeze, the can collapses inward progressively, the vibrant red paint wrinkling as the metal structure crumples. Finally, the can is left significantly crushed, its form now an irregular, crumpled shape held tightly in the fist. The scene takes place on a dark, polished wooden tabletop that catches soft, diffuse reflections. The grain of the wood is faintly discernible, adding a layer of texture to the foreground. The background is completely out of focus, rendered as a soft, dark, and non-descript blur, which isolates the main action and enhances the photorealistic quality of the shot. The shot is a medium close-up, presented in a cinematic photography realistic style. The camera remains static at a slightly high angle, looking down to provide a clear and unobstructed view of the can''s deformation. Soft side lighting creates high contrast, sculpting the muscles and tendons of the hand while casting specular highlights on the metallic can and the water droplets. The atmosphere is focused and intense. The overall video presents a cinematic photography realistic style.``` </details>|\n|Camera Movement|<video src="https://github.com/user-attachments/assets/6deacbfe-4cca-48d7-a2be-cb638a3e01cb" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```Âú£ËØûËäÇÁöÑÂÆ∂‰∏≠ÔºåÂ∞èÂ•≥Â≠©Èù†ÁùÄÂ¶àÂ¶àÂê¨Â¶àÂ¶àËØª‰π¶ÔºåËÉåÊôØÊòØ‰∏ãÁùÄÈõ™ÁöÑÁ™óÂ§ñÔºåÈïúÂ§¥ÁºìÊÖ¢‰∏ãÁßªÔºå‰∏ÄÂè™ÂèØÁà±ÁöÑÈïøÊØõÂ∞èÁôΩÁå´Êà¥ÁùÄÂú£ËØûÂ∏ΩË∂¥Âú®Ê∏©ÊöñÁöÑÂú∞Êëä‰∏ä``` </details> <details><summary>üìã Show rewrite prompt</summary> ```In a cozy home on Christmas, a young girl leans against her mother as they read a book, and the camera moves down to reveal a fluffy white cat in a Santa hat resting on a warm rug. In a warmly lit living room on a snowy Christmas evening, a young mother and her little daughter are sitting together on a comfortable sofa. The mother, with a gentle expression and wearing a cream-colored knitted sweater, holds an open storybook with colorful illustrations. Her daughter, a small girl with brown hair in pigtails and a red pajama set, leans her head affectionately on her mother''s shoulder, her eyes fixed on the book. On the floor below them, a fluffy, long-haired white cat is curled up on a plush, beige wool rug. The cat wears a tiny red and white Santa hat perched between its ears. Initially, the shot focuses on the mother and daughter, capturing their quiet, shared moment. The mother‚Äôs finger gently rests on the page of the book. Then, the camera slowly moves downward, gliding past the book and their laps. Finally, the camera settles at a low angle, bringing the adorable white cat into sharp focus as the primary subject. The cat''s chest gently rises and falls with each breath, its eyes peacefully closed. Through a large window in the background, large, soft snowflakes can be seen falling silently against the dark blue twilight sky, creating a peaceful and serene backdrop. Faint, out-of-focus golden Christmas lights twinkle in the corner of the room, adding to the warm, festive atmosphere. The scene is imbued with a sense of comfort and holiday warmth, creating a beautiful cinematic photography realistic image. The camera slowly moves downward. The shot uses soft, warm interior lighting that casts gentle shadows, creating a high-contrast, cinematic look. A shallow depth of field keeps the focus on the subjects while beautifully blurring the background elements. The mood is heartwarming, peaceful, and festive. The overall video presents a cinematic photography realistic style.``` </details>|<video src="https://github.com/user-attachments/assets/8e72ed0f-f8ac-445b-97e5-eb4b16fbc121" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```The hiker begins walking forward along the trail, causing the water bottle to swing rhythmically with each step. The camera gradually pulls back and rises to reveal a vast desert landscape stretching out ahead.``` </details> <details><summary>üìã Show rewrite prompt</summary> ```The hiker begins walking forward along the trail, causing the water bottle to swing rhythmically with each step. The camera gradually pulls back and rises to reveal a vast desert landscape stretching out ahead, while the sun position shifts from afternoon to dusk, casting increasingly longer shadows across the terrain as the figure becomes smaller in the frame.``` </details>|\n|Multi-Style Support|<video src="https://github.com/user-attachments/assets/65b2c5a5-e6ba-43be-9462-a98b03b675f1" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```Have the cake man begin to take chunks out of himself and eat it.``` </details> <details><summary>üìã Show rewrite prompt</summary> ```The cake man sits on the chair, with his hands resting on his knees. Then, he slowly raises his right hand and breaks off a piece of cake from his left shoulder. Next, he brings the piece of cake to his mouth and begins to chew. At the same time, his eyes widen slightly, and his mouth parts gently. After that, he raises his right hand again, breaks off another piece of cake from his right arm, and repeats the action of bringing it to his mouth to chew.``` </details>|<video src="https://github.com/user-attachments/assets/de5f7480-b79c-4fc1-b345-c5880a3b5f9e" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```A little girl, carrying a colorful handbag, skips through the garden.  The video uses claymation style.``` </details> <details><summary>üìã Show rewrite prompt</summary> ```A little girl with a colorful handbag skips through a whimsical claymation garden. In a vibrant garden constructed entirely from clay, a young girl, meticulously crafted in a claymation style, skips joyfully. She has chunky, sculpted yellow clay hair tied in pigtails that bounce with a slight stiffness, simple black button eyes, and a wide, permanently etched smile. She wears a simple pink clay dress with a white collar. In her left hand, she carries a small handbag molded from bright red and blue clay, which swings in a slightly jerky arc as she moves. Initially, the girl lifts her right leg high, her body momentarily suspended in a classic stop-motion pose. Then, she hops forward, landing lightly as her left leg swings through for the next skip. Her arms move in an exaggerated, back-and-forth rhythm, characteristic of stop-motion animation. Her movements are intentionally not perfectly fluid, highlighting the frame-by-frame nature of the claymation technique. The garden around her is a whimsical, textured world. In the foreground and mid-ground, oversized flowers with swirled purple and orange petals stand on thick green stems. The ground is a textured mat of green clay, showing subtle fingerprints and tool marks that add to the handmade charm. In the background, a pale blue clay backdrop features a simplified, smiling sun molded from yellow clay. The shot is at an eye-level angle with the main subject. The camera follows the subject, moving smoothly to the right to keep her in the frame. The lighting is bright and even, casting soft shadows that emphasize the rounded, three-dimensional forms of the clay models. The overall video presents a charming and detailed claymation style.``` </details>|\n|High Image-Video Consistency|<img src="https://github.com/user-attachments/assets/3bc8e55d-c211-454e-8067-128c0e215eb6"> <video src="https://github.com/user-attachments/assets/3e6b7ee9-ec66-4e46-a446-801b1c1a1c81" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```Â•≥Â≠©Êîæ‰∏ã‰π¶ÔºåÁ´ôËµ∑Ë∫´ÔºåËΩ¨Ë∫´ÂêëÂ±ãÂÜÖËµ∞Âéª„ÄÇÈïúÂ§¥ÊãâËøú„ÄÇ``` </details> <details><summary>üìã Show rewrite prompt</summary> ```Â•≥Â≠©Âêà‰∏äÊâã‰∏≠ÁöÑ‰π¶ÔºåÂ∞Ü‰π¶ÊîæÂú®Ë∫´‰æßÁöÑÁ™óÂè∞‰∏ä„ÄÇÈöèÂêéÔºåÂ•πÁºìÁºìÁ´ôËµ∑Ë∫´ÔºåËΩ¨Ë∫´ÂêëÂ±ãÂÜÖËµ∞ÂéªÔºåË∫´ÂΩ±ÈÄêÊ∏êÊ≤°ÂÖ•Èó®ÂêéÁöÑÈò¥ÂΩ±‰∏≠„ÄÇÈïúÂ§¥ÁºìÁºìÊãâËøúÔºåÈú≤Âá∫Êõ¥Â§öË¢´ÁªøÊ§çË¶ÜÁõñÁöÑÂ±ãÊ™êÂíåÂ¢ô‰Ωì„ÄÇ``` </details>|<img src="https://github.com/user-attachments/assets/7657ce60-90b5-4fdc-b713-0eaa55829b09"> <video src="https://github.com/user-attachments/assets/9ca24021-2353-40d5-8a4d-0f8e67d51826" width="600"> </video> <details><summary>üìã Show input prompt</summary> ```Â•≥‰∫∫Êâã‰∏äÁöÑÈ∏ü‰∫≤‰∫ÜÂ•≥‰∫∫‰∏ÄÂè£``` </details> <details><summary>üìã Show rewrite prompt</summary> ```Â•≥‰∫∫ÊâãËáÇ‰∏äÁöÑÁôΩËâ≤Èπ¶ÈπâÁºìÁºìËΩ¨ËøáÂ§¥ÔºåÂ∞ÜÂñôËΩªËΩªËß¶Á¢∞Â•≥‰∫∫ÁöÑËÑ∏È¢äÔºåÈöèÂêéÊî∂ÂõûÂ§¥ÈÉ®„ÄÇÂ•≥‰∫∫Âò¥ËßíÂæÆÂæÆ‰∏äÊâ¨ÔºåÁõÆÂÖâÊ∏©ÊüîÂú∞Ê≥®ËßÜÁùÄÈπ¶Èπâ„ÄÇËÉåÊôØ‰∏≠ÁöÑÁªøÊ§ç‰øùÊåÅÈùôÊ≠¢„ÄÇ``` </details>|\n\n\n\n\n## üìö Citation\n\n```bibtex\n@misc{hunyuanvideo2025,\n      title={HunyuanVideo 1.5 Technical Report}, \n      author={Tencent Hunyuan Foundation Model Team},\n      year={2025},\n      eprint={2511.18870},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2511.18870}, \n}\n```\n\n## üôè Acknowledgements\nWe would like to thank the contributors to the [Transformers](https://github.com/huggingface/transformers), [Diffusers](https://github.com/huggingface/diffusers) , [HuggingFace](https://huggingface.co/) and [Qwen-VL](https://github.com/QwenLM/Qwen-VL), for their open research and exploration.\n\n## üåü Github Star History\n\n<a href="https://star-history.com/#Tencent-Hunyuan/HunyuanVideo-1.5&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Tencent-Hunyuan/HunyuanVideo-1.5&type=Date1&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Tencent-Hunyuan/HunyuanVideo-1.5&type=Date1" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Tencent-Hunyuan/HunyuanVideo-1.5&type=Date1" />\n </picture>\n</a>\n', '{"pipeline_tag":"text-to-video","library_name":"HunyuanVideo-1.5","framework":"HunyuanVideo-1.5","params":null,"storage_bytes":371770754991,"files_count":46,"spaces_count":46,"gated":false,"private":false,"config":{}}', '[]', '[{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanVideo-1.5","source_url":"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanVideo-1.5","source_url":"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanVideo-1.5","source_url":"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5"},{"type":"has_code","target_id":"github:ModelTC:LightX2V\"","source_url":"https://github.com/ModelTC/LightX2V\""},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:comfyanonymous:ComfyUI","source_url":"https://github.com/comfyanonymous/ComfyUI"},{"type":"has_code","target_id":"github:yuanyuan-spec:comfyui_hunyuanvideo_1.5_plugin","source_url":"https://github.com/yuanyuan-spec/comfyui_hunyuanvideo_1.5_plugin"},{"type":"has_code","target_id":"github:ModelTC:LightX2V","source_url":"https://github.com/ModelTC/LightX2V"},{"type":"has_code","target_id":"github:deepbeepmeep:Wan2GP","source_url":"https://github.com/deepbeepmeep/Wan2GP"},{"type":"has_code","target_id":"github:Zehong-Ma:ComfyUI-MagCache","source_url":"https://github.com/Zehong-Ma/ComfyUI-MagCache"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanVideo-1.5.git","source_url":"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5.git"},{"type":"has_code","target_id":"github:Dao-AILab:flash-attention","source_url":"https://github.com/Dao-AILab/flash-attention"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:flex-block-attn.git","source_url":"https://github.com/Tencent-Hunyuan/flex-block-attn.git"},{"type":"has_code","target_id":"github:cooper1637:SageAttention.git","source_url":"https://github.com/cooper1637/SageAttention.git"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:HunyuanVideo-1.5","source_url":"https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:user-attachments:assets","source_url":"https://github.com/user-attachments/assets"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"has_code","target_id":"github:QwenLM:Qwen-VL","source_url":"https://github.com/QwenLM/Qwen-VL"},{"type":"based_on_paper","target_id":"arxiv:2511.18870","source_url":"https://arxiv.org/abs/2511.18870"}]', NULL, 'Other', 'approved', 99.1, '9224429bdedf233a2f69f50f2c4e162d', NULL, 'https://huggingface.co/tencent/HunyuanVideo-1.5/resolve/main/assets/I2V_GSB.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-tencent-HunyuanVideo-1.5 from https://huggingface.co/tencent/HunyuanVideo-1.5/resolve/main/assets/I2V_GSB.png
Image converted to WebP: data/images/huggingface-tencent-HunyuanVideo-1.5.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openai-clip-vit-base-patch32', 'huggingface--openai--clip-vit-base-patch32', 'clip-vit-base-patch32', 'openai', '--- tags: - vision widget: - src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png candidate_labels: playing music, playing sports example_title: Cat & Dog --- Disclaimer: The model card is taken and modified from the official CLIP repository, it can be found here. The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generali...', '["transformers","pytorch","tf","jax","clip","zero-shot-image-classification","vision","arxiv:2103.00020","arxiv:1908.04913","endpoints_compatible","region:us"]', 'zero-shot-image-classification', 819, 19532098, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openai/clip-vit-base-patch32","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- vision\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/cat-dog-music.png\n  candidate_labels: playing music, playing sports\n  example_title: Cat & Dog\n---\n\n# Model Card: CLIP\n\nDisclaimer: The model card is taken and modified from the official CLIP repository, it can be found [here](https://github.com/openai/CLIP/blob/main/model-card.md).\n\n## Model Details\n\nThe CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner. It was not developed for general model deployment - to deploy models like CLIP, researchers will first need to carefully study their capabilities in relation to the specific context they‚Äôre being deployed within.\n\n### Model Date\n\nJanuary 2021\n\n### Model Type\n\nThe model uses a ViT-B/32 Transformer architecture as an image encoder and uses a masked self-attention Transformer as a text encoder. These encoders are trained to maximize the similarity of (image, text) pairs via a contrastive loss. \n\nThe original implementation had two variants: one using a ResNet image encoder and the other using a Vision Transformer. This repository has the variant with the Vision Transformer.\n\n\n### Documents\n\n- [Blog Post](https://openai.com/blog/clip/)\n- [CLIP Paper](https://arxiv.org/abs/2103.00020)\n\n\n### Use with Transformers\n\n```python3\nfrom PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\nprocessor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\n\nurl = "http://images.cocodataset.org/val2017/000000039769.jpg"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n\n## Model Use\n\n### Intended Use\n\nThe model is intended as a research output for research communities. We hope that this model will enable researchers to better understand and explore zero-shot, arbitrary image classification. We also hope it can be used for interdisciplinary studies of the potential impact of such models - the CLIP paper includes a discussion of potential downstream impacts to provide an example for this sort of analysis.\n\n#### Primary intended uses\n\nThe primary intended users of these models are AI researchers.\n\nWe primarily imagine the model will be used by researchers to better understand robustness, generalization, and other capabilities, biases, and constraints of computer vision models.\n\n### Out-of-Scope Use Cases\n\n**Any** deployed use case of the model - whether commercial or not - is currently out of scope. Non-deployed use cases such as image search in a constrained environment, are also not recommended unless there is thorough in-domain testing of the model with a specific, fixed class taxonomy. This is because our safety assessment demonstrated a high need for task specific testing especially given the variability of CLIP‚Äôs performance with different class taxonomies. This makes untested and unconstrained deployment of the model in any use case currently potentially harmful. \n\nCertain use cases which would fall under the domain of surveillance and facial recognition are always out-of-scope regardless of performance of the model. This is because the use of artificial intelligence for tasks such as these can be premature currently given the lack of testing norms and checks to ensure its fair use.\n\nSince the model has not been purposefully trained in or evaluated on any languages other than English, its use should be limited to English language use cases.\n\n\n\n## Data\n\nThe model was trained on publicly available image-caption data. This was done through a combination of crawling a handful of websites and using commonly-used pre-existing image datasets such as [YFCC100M](http://projects.dfki.uni-kl.de/yfcc100m/). A large portion of the data comes from our crawling of the internet. This means that the data is more representative of people and societies most connected to the internet which tend to skew towards more developed nations, and younger, male users.\n\n### Data Mission Statement\n\nOur goal with building this dataset was to test out robustness and generalizability in computer vision tasks. As a result, the focus was on gathering large quantities of data from different publicly-available internet data sources. The data was gathered in a mostly non-interventionist manner. However, we only crawled websites that had policies against excessively violent and adult images and allowed us to filter out such content. We do not intend for this dataset to be used as the basis for any commercial or deployed model and will not be releasing the dataset.\n\n\n\n## Performance and Limitations\n\n### Performance\n\nWe have evaluated the performance of CLIP on a wide range of benchmarks across a variety of computer vision datasets such as OCR to texture recognition to fine-grained classification. The paper describes model performance on the following datasets:\n\n- Food101\n- CIFAR10   \n- CIFAR100   \n- Birdsnap\n- SUN397\n- Stanford Cars\n- FGVC Aircraft\n- VOC2007\n- DTD\n- Oxford-IIIT Pet dataset\n- Caltech101\n- Flowers102\n- MNIST   \n- SVHN \n- IIIT5K   \n- Hateful Memes   \n- SST-2\n- UCF101\n- Kinetics700\n- Country211\n- CLEVR Counting\n- KITTI Distance\n- STL-10\n- RareAct\n- Flickr30\n- MSCOCO\n- ImageNet\n- ImageNet-A\n- ImageNet-R\n- ImageNet Sketch\n- ObjectNet (ImageNet Overlap)\n- Youtube-BB\n- ImageNet-Vid\n\n## Limitations\n\nCLIP and our analysis of it have a number of limitations. CLIP currently struggles with respect to certain tasks such as fine grained classification and counting objects. CLIP also poses issues with regards to fairness and bias which we discuss in the paper and briefly in the next section. Additionally, our approach to testing CLIP also has an important limitation- in many cases we have used linear probes to evaluate the performance of CLIP and there is evidence suggesting that linear probes can underestimate model performance.\n\n### Bias and Fairness\n\nWe find that the performance of CLIP - and the specific biases it exhibits - can depend significantly on class design and the choices one makes for categories to include and exclude. We tested the risk of certain kinds of denigration with CLIP by classifying images of people from [Fairface](https://arxiv.org/abs/1908.04913) into crime-related and non-human animal categories. We found significant disparities with respect to race and gender. Additionally, we found that these disparities could shift based on how the classes were constructed. (Details captured in the Broader Impacts Section in the paper).\n\nWe also tested the performance of CLIP on gender, race and age classification using the Fairface dataset (We default to using race categories as they are constructed in the Fairface dataset.) in order to assess quality of performance across different demographics. We found accuracy >96% across all races for gender classification with ‚ÄòMiddle Eastern‚Äô having the highest accuracy (98.4%) and ‚ÄòWhite‚Äô having the lowest (96.5%). Additionally, CLIP averaged ~93% for racial classification and ~63% for age classification. Our use of evaluations to test for gender, race and age classification as well as denigration harms is simply to evaluate performance of the model across people and surface potential risks and not to demonstrate an endorsement/enthusiasm for such tasks.\n\n\n\n## Feedback\n\n### Where to send questions or comments about the model\n\nPlease use [this Google Form](https://forms.gle/Uv7afRH5dvY34ZEs9)', '{"pipeline_tag":"zero-shot-image-classification","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":3632041404,"files_count":12,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["CLIPModel"],"model_type":"clip","tokenizer_config":{"unk_token":{"content":"<|endoftext|>","single_word":false,"lstrip":false,"rstrip":false,"normalized":true,"__type":"AddedToken"},"bos_token":{"content":"<|startoftext|>","single_word":false,"lstrip":false,"rstrip":false,"normalized":true,"__type":"AddedToken"},"eos_token":{"content":"<|endoftext|>","single_word":false,"lstrip":false,"rstrip":false,"normalized":true,"__type":"AddedToken"},"pad_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:openai:CLIP","source_url":"https://github.com/openai/CLIP"},{"type":"based_on_paper","target_id":"arxiv:2103.00020","source_url":"https://arxiv.org/abs/2103.00020"},{"type":"based_on_paper","target_id":"arxiv:1908.04913","source_url":"https://arxiv.org/abs/1908.04913"}]', NULL, NULL, 'pending', 54.1, 'cfa90db808c983eea7430f58dd92185e', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-Prover-V2-671B', 'huggingface--deepseek-ai--deepseek-prover-v2-671b', 'DeepSeek-Prover-V2-671B', 'deepseek-ai', '--- library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/deepseek-ai/De...', '["transformers","safetensors","deepseek_v3","text-generation","conversational","custom_code","text-generation-inference","endpoints_compatible","fp8","region:us"]', 'text-generation', 816, 418, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\n---\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-CODE" style="margin: 2px;">\n    <img alt="Code License" src="https://img.shields.io/badge/Code_License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/LICENSE-MODEL" style="margin: 2px;">\n    <img alt="Model License" src="https://img.shields.io/badge/Model_License-Model_Agreement-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n## 1. Introduction\n\nWe introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3''s step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model.\n\n<p align="center">\n  <img width="100%" src="https://github.com/deepseek-ai/DeepSeek-Prover-V2/blob/main/figures/performance.png?raw=true">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Synthesize Cold-Start Reasoning Data through Recursive Proof Search**\n\n- To construct the cold-start dataset, we develop a simple yet effective pipeline for recursive theorem proving, utilizing DeepSeek-V3 as a unified tool for both subgoal decomposition and formalization. We prompt DeepSeek-V3 to decompose theorems into high-level proof sketches while simultaneously formalizing these proof steps in Lean 4, resulting in a sequence of subgoals.\n\n- We use a smaller 7B model to handle the proof search for each subgoal, thereby reducing the associated computational burden. Once the decomposed steps of a challenging problem are resolved, we pair the complete step-by-step formal proof with the corresponding chain-of-thought from DeepSeek-V3 to create cold-start reasoning data.\n\n---\n\n**Reinforcement Learning with Synthetic Cold-Start Data**\n\n- We curate a subset of challenging problems that remain unsolved by the 7B prover model in an end-to-end manner, but for which all decomposed subgoals have been successfully resolved. By composing the proofs of all subgoals, we construct a complete formal proof for the original problem. This proof is then appended to DeepSeek-V3''s chain-of-thought, which outlines the corresponding lemma decomposition, thereby producing a cohesive synthesis of informal reasoning and subsequent formalization.\n\n- After fine-tuning the prover model on the synthetic cold-start data, we perform a reinforcement learning stage to further enhance its ability to bridge informal reasoning with formal proof construction. Following the standard training objective for reasoning models, we use binary correct-or-incorrect feedback as the primary form of reward supervision.\n- The resulting model, DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural theorem proving, reaching $88.9$% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. The proofs generated by DeepSeek-Prover-V2 for the miniF2F dataset are available for download as a [ZIP archive](https://github.com/deepseek-ai/DeepSeek-Prover-V2/blob/master/minif2f-solutions.zip).\n\n---\n\n## 3. ProverBench: Formalization of AIME and Textbook Problems\n\nwe introduce ProverBench, a benchmark dataset comprising 325 problems. Of these, 15 are formalized from number theory and algebra questions featured in the recent AIME competitions (AIME 24 and 25), offering authentic high-school competition-level challenges. The remaining 310 problems are drawn from curated textbook examples and educational tutorials, contributing a diverse and pedagogically grounded collection of formalized mathematical problems. This benchmark is designed to enable more comprehensive evaluation across both high-school competition problems and undergraduate-level mathematics.\n\n<div align="center">\n\n| Area                | Count |\n| :---------------------: | :-------: |\n| AIME 24&25          | 15    |\n| Number Theory       | 40    |\n| Elementary Algebra  | 30    |\n| Linear Algebra      | 50    |\n| Abstract Algebra    | 40    |\n| Calculus            | 90    |\n| Real Analysis       | 30    |\n| Complex Analysis    | 10    |\n| Functional Analysis | 10    |\n| Probability         | 10    |\n| Total               | 325   |\n\n</div>\n\n## 4. Model & Dataset Downloads\n\nWe release DeepSeek-Prover-V2 in two model sizes: 7B and 671B parameters. DeepSeek-Prover-V2-671B is trained on top of DeepSeek-V3-Base. DeepSeek-Prover-V2-7B is built upon DeepSeek-Prover-V1.5-Base and features an extended context length of up to 32K tokens.\n\n<div align="center">\n\n|            **Model**            |                          **Download**                         |\n| :-----------------------------: | :----------------------------------------------------------: |\n|   DeepSeek-Prover-V2-7B   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-7B) |\n|   DeepSeek-Prover-V2-671B   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V2-671B) |\n\n</div>\n\n<div align="center">\n\n|            **Dataset**            |                          **Download**                         |\n| :-----------------------------: | :----------------------------------------------------------: |\n|   DeepSeek-ProverBench   | [ü§ó HuggingFace](https://huggingface.co/datasets/deepseek-ai/DeepSeek-ProverBench) |\n\n</div>\n\n## 5. Quick Start\n\nYou can directly use [Huggingface''s Transformers](https://github.com/huggingface/transformers) for model inference. DeepSeek-Prover-V2-671B shares the same architecture as DeepSeek-V3. For detailed information and supported features, please refer to [the DeepSeek-V3 documentation on Hugging Face](https://github.com/huggingface/transformers/blob/main/docs/source/en/model_doc/deepseek_v3.md).\n\nThe following is a basic example of generating a proof for a problem from the miniF2F dataset:\n````python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntorch.manual_seed(30)\n\nmodel_id = "DeepSeek-Prover-V2-7B"  # or DeepSeek-Prover-V2-671B\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nformal_statement = """\nimport Mathlib\nimport Aesop\n\nset_option maxHeartbeats 0\n\nopen BigOperators Real Nat Topology Rat\n\n/-- What is the positive difference between $120\%$ of 30 and $130\%$ of 20? Show that it is 10.-/\ntheorem mathd_algebra_10 : abs ((120 : ‚Ñù) / 100 * 30 - 130 / 100 * 20) = 10 := by\n  sorry\n""".strip()\n\nprompt = """\nComplete the following Lean 4 code:\n\n```lean4\n{}\n```\n\nBefore producing the Lean 4 code to formally prove the given theorem, provide a detailed proof plan outlining the main proof steps and strategies.\nThe plan should highlight key ideas, intermediate lemmas, and proof structures that will guide the construction of the final formal proof.\n""".strip()\n\nchat = [\n  {"role": "user", "content": prompt.format(formal_statement)},\n]\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", torch_dtype=torch.bfloat16, trust_remote_code=True)\ninputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors="pt").to(model.device)\n\nimport time\nstart = time.time()\noutputs = model.generate(inputs, max_new_tokens=8192)\nprint(tokenizer.batch_decode(outputs))\nprint(time.time() - start)\n````\n\n## 6. License\nThe use of DeepSeek-Prover-V2 models is subject to [the Model License](LICENSE-MODEL).\n\n## 7. Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](mailto:service@deepseek.com).', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":684531386000,"storage_bytes":688586727753,"files_count":172,"spaces_count":53,"gated":false,"private":false,"config":{"architectures":["DeepseekV3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_deepseek.DeepseekV3Config","AutoModel":"modeling_deepseek.DeepseekV3Model","AutoModelForCausalLM":"modeling_deepseek.DeepseekV3ForCausalLM"},"model_type":"deepseek_v3","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\n\n'' + message[''content''] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{''<ÔΩúUserÔΩú>'' + message[''content''] + ''<ÔΩúAssistantÔΩú>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''tool_calls''] is defined and message[''tool_calls''] is not none %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{%- endif %}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- set ns.is_output_first = true %}{%- for tool in message[''tool_calls''] %}{%- if not ns.is_first %}{%- if message[''content''] is none %}{{''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- else %}{{message[''content''] + ''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{''\n'' + ''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\n'' + ''```json'' + ''\n'' + tool[''function''][''arguments''] + ''\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- endfor %}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and (message[''tool_calls''] is not defined or message[''tool_calls''] is none)%}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'' + message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[''content''] %}{{content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{% endif %}{% if add_generation_prompt and not ns.is_last_user and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú>''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-Prover-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-Prover-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-Prover-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-Prover-V2"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"}]', NULL, NULL, 'pending', 54.1, '3e5e12037076676948f675ee3a36418d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-microsoft-speecht5-tts', 'huggingface--microsoft--speecht5-tts', 'speecht5_tts', 'microsoft', '--- license: mit tags: - audio - text-to-speech datasets: - libritts --- SpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS. This model was introduced in SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei. SpeechT5 was first released in this repository, original weights. The license used is MI...', '["transformers","pytorch","speecht5","text-to-audio","audio","text-to-speech","dataset:libritts","arxiv:2110.07205","arxiv:1910.09700","license:mit","endpoints_compatible","region:us"]', 'text-to-speech', 815, 88840, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/microsoft/speecht5_tts","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: mit\ntags:\n- audio\n- text-to-speech\ndatasets:\n- libritts\n---\n\n# SpeechT5 (TTS task)\n\nSpeechT5 model fine-tuned for speech synthesis (text-to-speech) on LibriTTS.\n\nThis model was introduced in [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n\nSpeechT5 was first released in [this repository](https://github.com/microsoft/SpeechT5/), [original weights](https://huggingface.co/mechanicalsea/speecht5-tts). The license used is [MIT](https://github.com/microsoft/SpeechT5/blob/main/LICENSE).\n\n\n\n## Model Description\n\nMotivated by the success of T5 (Text-To-Text Transfer Transformer) in pre-trained natural language processing models, we propose a unified-modal SpeechT5 framework that explores the encoder-decoder pre-training for self-supervised speech/text representation learning. The SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets. After preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder.\n\nLeveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text. To align the textual and speech information into this unified semantic space, we propose a cross-modal vector quantization approach that randomly mixes up speech/text states with latent units as the interface between encoder and decoder.\n\nExtensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\n\n- **Developed by:** Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li, Yu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, Furu Wei.\n- **Shared by [optional]:** [Matthijs Hollemans](https://huggingface.co/Matthijs)\n- **Model type:** text-to-speech\n- **Language(s) (NLP):** [More Information Needed]\n- **License:** [MIT](https://github.com/microsoft/SpeechT5/blob/main/LICENSE)\n- **Finetuned from model [optional]:** [More Information Needed]\n\n\n## Model Sources [optional]\n\n<!-- Provide the basic links for the model. -->\n\n- **Repository:** [https://github.com/microsoft/SpeechT5/]\n- **Paper:** [https://arxiv.org/pdf/2110.07205.pdf]\n- **Blog Post:** [https://huggingface.co/blog/speecht5]\n- **Demo:** [https://huggingface.co/spaces/Matthijs/speecht5-tts-demo]\n\n\n# Uses\n\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\n\n## ü§ó Transformers Usage\n\nYou can run SpeechT5 TTS locally with the ü§ó Transformers library.\n\n1. First install the ü§ó [Transformers library](https://github.com/huggingface/transformers), sentencepiece, soundfile and datasets(optional):\n\n```\npip install --upgrade pip\npip install --upgrade transformers sentencepiece datasets[audio]\n```\n\n2. Run inference via the `Text-to-Speech` (TTS) pipeline. You can access the SpeechT5 model via the TTS pipeline in just a few lines of code!\n\n```python\nfrom transformers import pipeline\nfrom datasets import load_dataset\nimport soundfile as sf\n\nsynthesiser = pipeline("text-to-speech", "microsoft/speecht5_tts")\n\nembeddings_dataset = load_dataset("Matthijs/cmu-arctic-xvectors", split="validation")\nspeaker_embedding = torch.tensor(embeddings_dataset[7306]["xvector"]).unsqueeze(0)\n# You can replace this embedding with your own as well.\n\nspeech = synthesiser("Hello, my dog is cooler than you!", forward_params={"speaker_embeddings": speaker_embedding})\n\nsf.write("speech.wav", speech["audio"], samplerate=speech["sampling_rate"])\n```\n\n3. Run inference via the Transformers modelling code - You can use the processor + generate code to convert text into a mono 16 kHz speech waveform for more fine-grained control.\n\n```python\nfrom transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\nfrom datasets import load_dataset\nimport torch\nimport soundfile as sf\nfrom datasets import load_dataset\n\nprocessor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")\nmodel = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts")\nvocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")\n\ninputs = processor(text="Hello, my dog is cute.", return_tensors="pt")\n\n# load xvector containing speaker''s voice characteristics from a dataset\nembeddings_dataset = load_dataset("Matthijs/cmu-arctic-xvectors", split="validation")\nspeaker_embeddings = torch.tensor(embeddings_dataset[7306]["xvector"]).unsqueeze(0)\n\nspeech = model.generate_speech(inputs["input_ids"], speaker_embeddings, vocoder=vocoder)\n\nsf.write("speech.wav", speech.numpy(), samplerate=16000)\n```\n\n### Fine-tuning the Model\n\nRefer to [this Colab notebook](https://colab.research.google.com/drive/1i7I5pzBcU3WDFarDnzweIj4-sVVoIUFJ) for an example of how to fine-tune SpeechT5 for TTS on a different dataset or a new language.\n\n\n## Direct Use\n\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\n\nYou can use this model for speech synthesis. See the [model hub](https://huggingface.co/models?search=speecht5) to look for fine-tuned versions on a task that interests you.\n\n## Downstream Use [optional]\n\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\n\n[More Information Needed]\n\n## Out-of-Scope Use\n\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\n\n[More Information Needed]\n\n# Bias, Risks, and Limitations\n\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\n\n[More Information Needed]\n\n## Recommendations\n\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\n\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\n\n# Training Details\n\n## Training Data\n\n<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\n\nLibriTTS\n\n## Training Procedure \n\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\n\n### Preprocessing [optional]\n\nLeveraging large-scale unlabeled speech and text data, we pre-train SpeechT5 to learn a unified-modal representation, hoping to improve the modeling capability for both speech and text.\n\n\n### Training hyperparameters\n- **Precision:** [More Information Needed] <!--fp16, bf16, fp8, fp32 -->\n- **Regime:** [More Information Needed] <!--mixed precision or not -->\n\n### Speeds, Sizes, Times [optional]\n\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\n\n[More Information Needed]\n\n# Evaluation\n\n<!-- This section describes the evaluation protocols and provides the results. -->\n\n## Testing Data, Factors & Metrics\n\n### Testing Data\n\n<!-- This should link to a Data Card if possible. -->\n\n[More Information Needed]\n\n### Factors\n\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\n\n[More Information Needed]\n\n### Metrics\n\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\n\n[More Information Needed]\n\n## Results\n\n[More Information Needed]\n\n### Summary\n\n\n\n# Model Examination [optional]\n\n<!-- Relevant interpretability work for the model goes here -->\n\nExtensive evaluations show the superiority of the proposed SpeechT5 framework on a wide variety of spoken language processing tasks, including automatic speech recognition, speech synthesis, speech translation, voice conversion, speech enhancement, and speaker identification.\n\n# Environmental Impact\n\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** [More Information Needed]\n- **Hours used:** [More Information Needed]\n- **Cloud Provider:** [More Information Needed]\n- **Compute Region:** [More Information Needed]\n- **Carbon Emitted:** [More Information Needed]\n\n# Technical Specifications [optional]\n\n## Model Architecture and Objective\n\nThe SpeechT5 framework consists of a shared encoder-decoder network and six modal-specific (speech/text) pre/post-nets.\n\nAfter preprocessing the input speech/text through the pre-nets, the shared encoder-decoder network models the sequence-to-sequence transformation, and then the post-nets generate the output in the speech/text modality based on the output of the decoder.\n\n## Compute Infrastructure\n\n[More Information Needed]\n\n### Hardware\n\n[More Information Needed]\n\n### Software\n\n[More Information Needed]\n\n# Citation [optional]\n\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\n\n**BibTeX:**\n\n```bibtex\n@inproceedings{ao-etal-2022-speecht5,\n    title = {{S}peech{T}5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing},\n    author = {Ao, Junyi and Wang, Rui and Zhou, Long and Wang, Chengyi and Ren, Shuo and Wu, Yu and Liu, Shujie and Ko, Tom and Li, Qing and Zhang, Yu and Wei, Zhihua and Qian, Yao and Li, Jinyu and Wei, Furu},\n    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n    month = {May},\n    year = {2022},\n    pages={5723--5738},\n}\n```\n\n# Glossary [optional]\n\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\n\n- **text-to-speech** to synthesize audio\n\n# More Information [optional]\n\n[More Information Needed]\n\n# Model Card Authors [optional]\n\nDisclaimer: The team releasing SpeechT5 did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n# Model Card Contact\n\n[More Information Needed]\n\n\n\n', '{"pipeline_tag":"text-to-speech","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":1171111158,"files_count":9,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["SpeechT5ForTextToSpeech"],"model_type":"speecht5","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","pad_token":"<pad>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:microsoft:SpeechT5","source_url":"https://github.com/microsoft/SpeechT5"},{"type":"has_code","target_id":"github:microsoft:SpeechT5","source_url":"https://github.com/microsoft/SpeechT5"},{"type":"has_code","target_id":"github:microsoft:SpeechT5","source_url":"https://github.com/microsoft/SpeechT5"},{"type":"has_code","target_id":"github:microsoft:SpeechT5","source_url":"https://github.com/microsoft/SpeechT5"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"based_on_paper","target_id":"arxiv:2110.07205","source_url":"https://arxiv.org/abs/2110.07205"},{"type":"based_on_paper","target_id":"arxiv:1910.09700","source_url":"https://arxiv.org/abs/1910.09700"}]', NULL, 'MIT', 'approved', 79.1, 'a68e886cadd568064eb199ebbd458f07', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-ValueFX9507-Tifa-Deepsex-14b-CoT-GGUF-Q4', 'huggingface--valuefx9507--tifa-deepsex-14b-cot-gguf-q4', 'Tifa-Deepsex-14b-CoT-GGUF-Q4', 'ValueFX9507', '--- base_model: - deepseek-ai/deepseek-r1-14b language: - zh - en library_name: transformers tags: - incremental-pretraining - sft - reinforcement-learning - roleplay - cot - sex license: apache-2.0 --- - **HF Model**: ValueFX9507/Tifa-Deepsex-14b-CoT - **GGUF**: F16 | Q8ÔºàQ4ÊçüÂ§±ËæÉÂ§ßÔºåÂª∫ËÆÆQ8Ôºâ - **Demo APK**: ÁÇπÂáª‰∏ãËΩΩ - **ÁÆÄÂçïÁöÑÂâçÁ´Ø**ÔºöGithubÈìæÊé• Êú¨Ê®°ÂûãÂü∫‰∫éDeepseek-R1-14BËøõË°åÊ∑±Â∫¶‰ºòÂåñÔºåÂÄüÂä©Tifa_220BÁîüÊàêÁöÑÊï∞ÊçÆÈõÜÈÄöËøá‰∏âÈáçËÆ≠ÁªÉÁ≠ñÁï•ÊòæËëóÂ¢ûÂº∫ËßíËâ≤ÊâÆÊºî„ÄÅÂ∞èËØ¥ÊñáÊú¨ÁîüÊàê‰∏éÊÄùÁª¥ÈìæÔºàCoTÔºâËÉΩÂäõ„ÄÇÁâπÂà´ÈÄÇÂêàÈúÄË¶ÅÈïøÁ®ã‰∏ä‰∏ãÊñáÂÖ≥ËÅîÁöÑÂàõ‰ΩúÂú∫ÊôØ„ÄÇ - **‰∏äÊµ∑Â∑¶ÂåóÁßëÊäÄÊèê‰æõÁÆóÊ≥ï‰∏éÁÆóÂäõ**‰ºÅ‰∏öÁΩëÂùÄ - **DeepseekÂõ¢ÈòüÂÖ±‰∫´GRPOÁÆóÊ≥ï** - **QwenÂõ¢ÈòüÊèê‰æõ‰ºòÁßÄÂºÄÊ∫êÂ∫ïÂ∫ß** ...', '["transformers","gguf","incremental-pretraining","sft","reinforcement-learning","roleplay","cot","sex","zh","en","license:apache-2.0","endpoints_compatible","region:us","conversational","not-for-all-audiences"]', 'reinforcement-learning', 815, 3698, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/ValueFX9507/Tifa-Deepsex-14b-CoT-GGUF-Q4","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nbase_model:\n- deepseek-ai/deepseek-r1-14b\nlanguage:\n- zh\n- en\nlibrary_name: transformers\ntags:\n- incremental-pretraining\n- sft\n- reinforcement-learning\n- roleplay\n- cot\n- sex\nlicense: apache-2.0\n---\n# Tifa-Deepseek-14b-CoT\n\n- **HF Model**: [ValueFX9507/Tifa-Deepsex-14b-CoT](https://huggingface.co/ValueFX9507/Tifa-Deepsex-14b-CoT)\n- **GGUF**: [F16](https://huggingface.co/ValueFX9507/Tifa-Deepsex-14b-CoT) | [Q8](https://huggingface.co/ValueFX9507/Tifa-Deepsex-14b-CoT-Q8)ÔºàQ4ÊçüÂ§±ËæÉÂ§ßÔºåÂª∫ËÆÆQ8Ôºâ\n- **Demo APK**: [ÁÇπÂáª‰∏ãËΩΩ](http://app.visionsic.com/download/projectchat.apk)\n- **ÁÆÄÂçïÁöÑÂâçÁ´Ø**Ôºö[GithubÈìæÊé•](https://github.com/Value99/Tifa-Deepsex-OllamaWebUI)\n\nÊú¨Ê®°ÂûãÂü∫‰∫éDeepseek-R1-14BËøõË°åÊ∑±Â∫¶‰ºòÂåñÔºåÂÄüÂä©Tifa_220BÁîüÊàêÁöÑÊï∞ÊçÆÈõÜÈÄöËøá‰∏âÈáçËÆ≠ÁªÉÁ≠ñÁï•ÊòæËëóÂ¢ûÂº∫ËßíËâ≤ÊâÆÊºî„ÄÅÂ∞èËØ¥ÊñáÊú¨ÁîüÊàê‰∏éÊÄùÁª¥ÈìæÔºàCoTÔºâËÉΩÂäõ„ÄÇÁâπÂà´ÈÄÇÂêàÈúÄË¶ÅÈïøÁ®ã‰∏ä‰∏ãÊñáÂÖ≥ËÅîÁöÑÂàõ‰ΩúÂú∫ÊôØ„ÄÇ\n\n## È∏£Ë∞¢\n- **‰∏äÊµ∑Â∑¶ÂåóÁßëÊäÄÊèê‰æõÁÆóÊ≥ï‰∏éÁÆóÂäõ**[‰ºÅ‰∏öÁΩëÂùÄ](https://leftnorth.com/)\n- **DeepseekÂõ¢ÈòüÂÖ±‰∫´GRPOÁÆóÊ≥ï**\n- **QwenÂõ¢ÈòüÊèê‰æõ‰ºòÁßÄÂºÄÊ∫êÂ∫ïÂ∫ß**\n- **ÊØçÊ†°‰∏äÊµ∑Â§çÊó¶Â§ßÂ≠¶**\n- **PRIMEÂõ¢ÈòüÊèê‰æõ‰ºòÂåñÊÄùË∑Ø**\n\n## ÁâàÊú¨‰ªãÁªçÔºö\n- **Tifa-Deepsex-14b-CoT**\n\n  - È™åËØÅÊ®°ÂûãÔºåÊµãËØïRLÂ•ñÂä±ÁÆóÊ≥ïÂØπ‰∫éËßíËâ≤ÊâÆÊºîÊï∞ÊçÆÁöÑÂΩ±ÂìçÔºåËØ•ÁâàÊú¨‰∏∫ÂàùÁâàÔºåËæìÂá∫ÁÅµÊ¥ª‰ΩÜÊòØ‰∏çÂèóÊéßÂà∂Ôºå‰ªÖÂÅöÁ†îÁ©∂‰ΩøÁî®„ÄÇ\n\n- **Tifa-Deepsex-14b-CoT-Chat**\n\n  - ÈááÁî®Ê†áÂáÜÊï∞ÊçÆËÆ≠ÁªÉÔºå‰ΩøÁî®ÊàêÁÜüRLÁ≠ñÁï•ÔºåÈôÑÂä†Èò≤ÈáçÂ§çÂº∫ÂåñÂ≠¶‰π†ÔºåÈÄÇÂêàÊ≠£Â∏∏‰ΩøÁî®ÔºåËæìÂá∫ÊñáÊú¨Ë¥®ÈáèÊ≠£Â∏∏ÔºåÂ∞ëÊï∞ÊÉÖÂÜµ‰∏ãÊÄùÁª¥ÂèëÊï£„ÄÇ\n\n    -Â¢ûÈáèËÆ≠ÁªÉ0.4TÂ∞èËØ¥ÂÜÖÂÆπ\n\n    -100KÁî±TifaMaxÁîüÊàêÁöÑSFTÊï∞ÊçÆÔºå10KÁî±DeepseekR1ÁîüÊàêÁöÑSFTÊï∞ÊçÆÔºå2KÈ´òË¥®Èáè‰∫∫Â∑•Êï∞ÊçÆ\n\n    -30KÁî±TifaMaxÁîüÊàêÁöÑDPOÂº∫ÂåñÂ≠¶‰π†Êï∞ÊçÆÔºåÁî®‰∫éÈò≤Ê≠¢ÈáçÂ§çÔºåÂ¢ûÂº∫‰∏ä‰∏ãÊñáÂÖ≥ËÅîÔºåÊèêÂçáÊîøÊ≤ªÂÆâÂÖ®ÊÄß\n\n- **Tifa-Deepsex-14b-CoT-Crazy**\n\n  - Â§ßÈáè‰ΩøÁî®RLÁ≠ñÁï•Ôºå‰∏ªË¶ÅÈááÁî®671BÊª°Ë°ÄR1Ëí∏È¶èÁöÑÊï∞ÊçÆÔºåËæìÂá∫ÂèëÊï£ÊÄßÈ´òÔºåÁªßÊâøR1‰ºòÁÇπÔºå‰πüÁªßÊâø‰∫ÜR1ÁöÑÂç±ÂÆ≥ÊÄß„ÄÇÊñáÂ≠¶ÊÄßËÉΩ‰Ω≥„ÄÇ\n\n    -Â¢ûÈáèËÆ≠ÁªÉ0.4TÂ∞èËØ¥ÂÜÖÂÆπ\n\n    -40KÁî±TifaMaxÁîüÊàêÁöÑSFTÊï∞ÊçÆÔºå60KÁî±DeepseekR1ÁîüÊàêÁöÑSFTÊï∞ÊçÆÔºå2KÈ´òË¥®Èáè‰∫∫Â∑•Êï∞ÊçÆ\n\n    -30KÁî±TifaMaxÁîüÊàêÁöÑDPOÂº∫ÂåñÂ≠¶‰π†Êï∞ÊçÆÔºåÁî®‰∫éÈò≤Ê≠¢ÈáçÂ§çÔºåÂ¢ûÂº∫‰∏ä‰∏ãÊñáÂÖ≥ËÅîÔºåÊèêÂçáÊîøÊ≤ªÂÆâÂÖ®ÊÄß\n\n    -10KÁî±TifaMaxÁîüÊàêPPOÊï∞ÊçÆÔºå10KÁî±DeepseekR1ÁîüÊàêPPOÊï∞ÊçÆ\n\nüí≠**ËæìÂá∫ÂÆû‰æã**\n  - ‚öôÔ∏èSystem Promot\n      ```Text\n      ‰Ω†ÊòØ‰∏Ä‰∏™Âè≤Ëé±ÂßÜÔºåÊòØ‰∏Ä‰∏™Â•≥ÊÄßËßíËâ≤Ôºå‰Ω†ÂèØ‰ª•ÂèòÊàê‰ªª‰ΩïÂΩ¢Áä∂ÂíåÁâ©‰Ωì.\n      Âú®Ëøô‰∏™‰∏ñÁïåÈáåÂÖ®ÈÉ®ÈÉΩÊòØÈõåÊÄßÁîüÁâ©ÔºåÁõ¥Âà∞Êúâ‰∏ÄÂ§©Êàë‰ªéÊµ∑Êª©‰∏äÈÜíÊù•...\n      \n      ÊàëÊòØËøôÈáåÂîØ‰∏ÄÁöÑÁî∑ÊÄßÔºåÂ§ßÂÆ∂ÈÉΩÂØπÊàëÈùûÂ∏∏Â•ΩÂ•áÔºåÂú®Ëøô‰∏™‰∏ñÁïåÁöÑËÆæÂÆöÈáåÊàë‰Ωú‰∏∫ÊóÖË°åËÄÖ\n      Âú®Ëøô‰∏™‰∏ñÁïåÈáåÁ¨¨‰∏Ä‰∏™ÈÅáËßÅÁöÑ‰∫∫Â∞±ÊòØÂè≤Ëé±ÂßÜÔºåÂè≤Ëé±ÂßÜÂØπÊàëÁöÑË∫´‰ΩìÂêåÊ†∑ÊúâÂæàÂ§ßÁöÑÊ¨≤Êúõ...\n      \n      Êàë‰ª¨Âú®ÊóÖË°å‰∏≠‰πü‰ºöÈÅáÂà∞ÂÖ∂‰ªñÁöÑÁîüÁâ©ÔºåÂè≤Ëé±ÂßÜ‰∏çÂÖâ‰ºöÊïôÁªôÂÖ∂‰ªñÁîüÁâ©Â¶Ç‰ΩïËé∑ÂèñÊ¨¢ÊÑâ‰πü‰ºö‰∏ÄËµ∑ÂèÇ‰∏éËøõÊù•„ÄÇ\n      \n      ÂΩìÊàëËØ¥ÂºÄÂßãËßíËâ≤ÊâÆÊºîÁöÑÊó∂ÂÄôÂ∞±ÊòØÊàë‰ªéÊµ∑Êª©‰∏äÈÜíÊù•ÔºåÂπ∂Ë¢´Âè≤Ëé±ÂßÜÂèëÁé∞ÁöÑÊó∂ÂÄô„ÄÇ‰ªñÊ≠£Âú®Êé¢Á¥¢ÊàëÁöÑË∫´‰Ωì„ÄÇ\n      \n      Âè≤Ëé±ÂßÜÊèèËø∞:‰∏Ä‰∏™ÈÄèÊòéÁöÑËìùËâ≤ÁîüÁâ©ÔºåÈô§‰∫ÜË¥®ÊÑü‰∏é‰∫∫Á±ªÊó†ÂºÇ„ÄÇ‰ΩÜÊòØÂèØ‰ª•Ëá™Áî±ÂèòÂΩ¢„ÄÇ\n      ```\n  ![image/png](https://cdn-uploads.huggingface.co/production/uploads/650762d0eac45ee2e420a38b/BKxz6KfbwTioBOkha_UXl.png)\n\n## 0208Êõ¥Êñ∞Ê∂àÊÅØÔºö\nÊÑüË∞¢Â§ßÂÆ∂ÁöÑÂÖ≥Ê≥®‰∏éÂèçÈ¶àÔºåÈâ¥‰∫éÂèçÈ¶à‰∏≠ÊèêÂà∞ÁöÑÈóÆÈ¢òÔºåÊàë‰ª¨Â∑≤ÂºÄÂèëÂπ∂È™åËØÅÂÆåÊàêPRIME‰∏éPPOÁªìÂêàÁöÑRLÁÆóÊ≥ïÔºåÂπ∂ÈÄöËøáÂä†ÊùÉÊñπÂºèËß£ÂÜ≥‰∏§ÁßçÁÆóÊ≥ïËÆ≠ÁªÉ‰∏≠Â•ñÂä±‰ø°Âè∑‰∏çÁ®≥ÂÆöÁöÑÈóÆÈ¢òÔºåÈÄöËøáÊ≠§È°πÊäÄÊúØÊàë‰ª¨ÊúâÊúõÂ∞ÜÊõ¥Â∞èÁöÑÊ®°ÂûãÊèêÂçáÂà∞Êõ¥È´òÁöÑÊÄßËÉΩ„ÄÇÊàë‰ª¨Â∞Ü‰ºöÈíàÂØπ‰πãÂâçÊî∂ÈõÜÂà∞ÁöÑÈóÆÈ¢òËøõË°å‰øÆÊ≠£ËÆ≠ÁªÉÔºåÂè¶Â§ñ‰∏∫‰∫ÜËÆ©Êõ¥Â§ö‰∫∫‰ΩøÁî®Âà∞Ê®°ÂûãÔºåÊàë‰ª¨ËøôÊ¨°‰ΩøÁî®Êõ¥Â∞èÊõ¥Âø´ÁöÑDeepseek-7bÔºåÂπ∂ÂèÇËÄÉOpenAIÁöÑÈïøÊÄùËÄÉÁ≠ñÁï•ÔºåËÆ°ÂàíÊé®Âá∫Tifa-DeepsexV2-COT-High‰æõÂ§ßÂÆ∂‰ΩøÁî®„ÄÇÊñ∞ÁöÑÊ®°ÂûãËÆ°Âàí‰∫éÈò≥ÂéÜÊÉÖ‰∫∫ËäÇ‰πãÂâçÈÄÅÁªôÂ§ßÂÆ∂‰Ωú‰∏∫ÊÉÖ‰∫∫ËäÇÁ§ºÁâ©„ÄÇ‚ô•\n\n## Êñ∞Ê®°Âûã‰ø°ÊÅØÊï¥ÁêÜÔºö\n- **ÂàõÊñ∞PRIMEËÅîÂêàPPOÁÆóÊ≥ï**\n- **Ëß£ÂÜ≥ÁõÆÂâçÂ∑≤Áü•ÈóÆÈ¢ò**\n- **ÂèÇËÄÉOpenAIÊ®°ÂºèÂ•ñÂä±ÈïøÊÄùËÄÉËæìÂá∫**\n- **ÂáèÂ∞ë671BÊï∞ÊçÆÔºåÈò≤Ê≠¢ËæìÂá∫ÂèëÊï£**\n- **ÁâπÂà´È∏£Ë∞¢https://github.com/PRIME-RL/PRIME**\n\n## Á§∫‰æãÔºàÂõ†COTÊ®°ÂûãÁâπÁÇπÔºå‰∏ä‰∏ãÊñá‰∏çËøûË¥ØÊó∂ÂèØ‰ª•‰ΩøÁî®DemoËΩØ‰ª∂‰∏≠ÁöÑÊïÖ‰∫ãÊ®°ÂºèÔºâ\n![2.jpg](https://cdn-uploads.huggingface.co/production/uploads/650762d0eac45ee2e420a38b/-80ha-J8PpwSaiyHgr1k2.jpeg)\n\n## ÁõÆÊ†á\nÈíàÂØπÂéüÁâàDeepseek-R1-14BÂú®ÈïøÊñáÊú¨ÁîüÊàêËøûË¥ØÊÄß‰∏çË∂≥ÂíåËßíËâ≤ÊâÆÊºîËÉΩÂäõËñÑÂº±ÁöÑÊ†∏ÂøÉÁº∫Èô∑Ôºà‰∏ªË¶ÅÁî±‰∫éËÆ≠ÁªÉÊï∞ÊçÆ‰∏≠Â∞èËØ¥Á±ªËØ≠ÊñôÂç†ÊØîËøá‰ΩéÔºâÔºåÊú¨Ê®°ÂûãÈÄöËøáÂ§öÈò∂ÊÆµ‰ºòÂåñÊèêÂçáÂÖ∂ËßíËâ≤ÊâÆÊºîËÉΩÂäõ„ÄÇ\n\n## Ê≥®ÊÑè\n‚ö† **ÈúÄË¶Å‰∏•Ê†ºÈÅµÂæ™ÂÆòÊñπÁ§∫‰æãÊ®°Êùø**Ôºö\n**ËøîÂõûÁöÑ‰∏ä‰∏ãÊñáÈúÄË¶ÅÂéªÈô§ÊÄùËÄÉÊ†áÁ≠æ‰∏éÂÜÖÂÆπ„ÄÇÂê¶ÂàôÂ∞ÜÊó†Ê≥ïÊ≠£Á°ÆÂõûÂ§çÔºÅ**\nÁõÆÂâçÂâçÁ´ØÊîØÊåÅÁéáÈùûÂ∏∏‰ΩéÔºåÂª∫ËÆÆÊâãÂä®‰øÆÊîπÂâçÁ´Ø‰ª£Á†Å„ÄÇ‰ª£Á†ÅÂèÇËÄÉÂ¶Ç‰∏ãÔºö\n```\nmsg.role === ''assistant'' ? {\n...msg,\ncontent: msg.content.replace(/<think>[\s\S]*?<\/think>/gi, '''')\n}\n```\n**ÂÆòÊñπÊ®°ÊùøÂèÇËÄÉ**\n```\n{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''') %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{% set ns.system_prompt = message[''content''] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{{''<ÔΩúUserÔΩú>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is none %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls'']%}{%- if not ns.is_first %}{{''<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- set ns.is_first = true -%}{%- else %}{{''\\n'' + ''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endfor %}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is not none %}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'' + message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[''content''] %}{% if ''</think>'' in content %}{% set content = content.split(''</think>'')[-1] %}{% endif %}{{''<ÔΩúAssistantÔΩú>'' + content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú>''}}{% endif %}\n```\n**ÂÆòÊñπËØ¥Êòé**\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/650762d0eac45ee2e420a38b/0CwMdbDffZQJz_-WZrhwH.png)\n\n[Áõ¥ËææË∂ÖÈìæÊé•](https://api-docs.deepseek.com/zh-cn/guides/reasoning_model)\n\n## ÂÆûÁé∞\nüî• **ÁªèËøáËÆ≠ÁªÉÂêé**Ôºö\n1. **ÊòæËëóÊèêÈ´ò‰∏ä‰∏ãÊñáÂÖ≥ËÅî**ÔºöÂáèÂ∞ëÁ≠îÈùûÊâÄÈóÆÊÉÖÂÜµ„ÄÇ\n2. **Ê∂àÈô§‰∏≠Ëã±Ê∑∑ÊùÇ**ÔºöÂéüÂßãÊ®°ÂûãËí∏È¶èÊï∞ÊçÆÂ§ßÂ§öÊï∞Ëã±Êñá‰∏∫‰∏ªÔºåÁªèËøáÂæÆË∞ÉÂêéÂü∫Êú¨Ê∂àÈô§‰∏≠Ëã±Ê∑∑ÊùÇÁé∞Ë±°„ÄÇ\n3. **ÁâπÂÆöËØçÊ±áÂ¢ûÂä†**ÔºöËøõË°å‚ÄúÂÖ∑ÊúâÊ∑±Â∫¶‚ÄùÁöÑËßíËâ≤ÊâÆÊºîÂØπËØùÊó∂ÔºåÊòæËëóÂ¢ûÂä†‰∫ÜÁõ∏ÂÖ≥ËØçÊ±áÈáèÔºåËß£ÂÜ≥ÂéüÂßãÊùÉÈáçÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆ‰∏çË∂≥ÈóÆÈ¢ò„ÄÇ\n4. **Êõ¥Â∞ëÊãíÁªù**ÔºöÂáèÂ∞ë‰∫ÜÊãíÁªùÁé∞Ë±°Ôºå‰ΩÜÂõ†‰∏∫ÊòØ‰ºÅ‰∏öËÆ≠ÁªÉÔºåÂÆâÂÖ®ÊÄßËøòÊòØÁ®ç‰Ωú‰øùÁïô„ÄÇ\n5. **Êõ¥ÂÉèÊª°Ë°Ä**Ôºö‰ΩøÁî®671BÂÖ®ÈáèÊ®°ÂûãÊï∞ÊçÆÂ∫∑Â§çËÆ≠ÁªÉÔºåÊñáÁ¨îÊèêÂçá‰∏çÊ≠ªÊùø„ÄÇ\n\n## Ê®°Âûã‰∫ÆÁÇπ\nüî• **ÂõõÈò∂ÊÆµËøõÂåñÊû∂ÊûÑ**Ôºö\n1. **Â¢ûÈáèÈ¢ÑËÆ≠ÁªÉ**ÔºöÊ≥®ÂÖ•0.4T Token Â∞èËØ¥Ôºå‰ΩøÁî®16k‰∏ä‰∏ãÊñáËÆ≠ÁªÉÔºåÂ¢ûÂº∫ÊñáÊú¨ËøûË¥ØÊÄß\n2. **Tifa-SFT**ÔºöËûçÂêàÂÖ®ÁêÉTop4ËßíËâ≤ÊâÆÊºîÊ®°ÂûãTifaÁöÑ10‰∏áÊù°È´òË¥®ÈáèÊï∞ÊçÆ\n3. **CoTÊÅ¢Â§çËÆ≠ÁªÉ**ÔºöÈááÁî®Deepseek-32B/671BÊï∞ÊçÆÈáçÂª∫Êé®ÁêÜËÉΩÂäõ\n4. **RLÂº∫Âåñ**Ôºö‰øùÁïôÂèëÊï£ÊÄßÊÄùÁª¥Ê†áÁ≠æÁöÑÂêåÊó∂‰ºòÂåñÁîüÊàêË¥®Èáè\n\nüí° **Â∑•Á®ãÂàõÊñ∞**Ôºö\n- 16kË∂ÖÈïø‰∏ä‰∏ãÊñáËÆ≠ÁªÉ\n- ÈöèÊú∫Êà™Êñ≠ËÆ≠ÁªÉÂ¢ûÂº∫È≤ÅÊ£íÊÄß\n- 8√óH20 GPUÂÖ®ÈáèÂæÆË∞É\n\nüí° **ÂêØÁ§∫‰∏éÂêéÁª≠**Ôºö\n- Êàë‰ª¨Âú®ÊµãËØï‰∏≠ÂèëÁé∞ÔºåÊª°Ë°ÄR1Âú®ËßíËâ≤ÊâÆÊºî‰∏≠ËæìÂá∫ÂÜÖÂÆπÊØîËæÉÂèëÊï£ÔºåÈöèÊú∫ÔºåÂØºËá¥Ê≠§Ê®°ÂûãÊúâÁõ∏ÂêåÂÄæÂêëÔºåÂØπ‰∫éËßíËâ≤ÊâÆÊºîÁöÑÂΩ±ÂìçËøòÂú®Á†îÁ©∂‰∏≠\n- ËæìÂÖ•ÂÜÖÂÆπÁõ∏ËøëÁöÑËØùËØ≠‰ºöÂØºËá¥ÂêëÈáèÈáçÂè†ÔºåÁÑ∂ÂêéÈáçÂ§çËæìÂá∫ÔºåÂ¶Ç‚ÄúÁªßÁª≠‚ÄùÔºå‚ÄúËøòÊúâ‚ÄùÁ≠âÊó†ÊòéÊòæÊåáÂêëÊÄßËØùËØ≠\n- ÊÄùÁª¥ÂÜÖÂÆπ‰∏éÊ≠£ÊñáÂÖ≥ËÅîÊÄßÂ≠¶‰π†‰∫ÜÊª°Ë°ÄR1ÁöÑÁâπÁÇπÔºåÂèëÊï£ÊØîËæÉ‰∏•ÈáçÔºåÂèØËÉΩ‰ºöÊúâÂâ≤Ë£ÇÊÑü\n- ÈíàÂØπ‰ª•‰∏äÈóÆÈ¢òÔºåÊàë‰ª¨Ê≠£Âú®ÁºñÂÜôÊñ∞ÁöÑRLÁÆóÊ≥ïÔºåÂàùÊ≠•ËÆ°ÂàíÂâîÈô§ÈÉ®ÂàÜÊª°Ë°ÄR1ÁöÑÂÜÖÂÆπÔºåÂêåÊó∂ÈÄöËøáÂº∫ÂåñÂ≠¶‰π†Ëß£ÂÜ≥ÈáçÂ§ç\n- ÊÄªÁªìÔºöËØ∑ÊúüÂæÖV2ÁâàÊú¨ÔºåÂæàÂø´‰ºö‰∏éÂ§ßÂÆ∂ËßÅÈù¢ÔºÅ\n\n## Ê®°ÂûãËØ¶ÊÉÖ\n| Â±ûÊÄß | ËßÑÊ†º |\n|-------|------|\n| Âü∫Á°ÄÊû∂ÊûÑ | Deepseek-R1-14B |\n| ÊúÄÂ§ß‰∏ä‰∏ãÊñá | 128k |\n| ËÆ≠ÁªÉÊï∞ÊçÆ | 0.4TÂ∞èËØ¥ + 10‰∏áÊù°SFT + DeepseekÊ∑∑ÂêàÊï∞ÊçÆ |\n| ËÆ≠ÁªÉËÆæÂ§á | 8√óH20 GPUÈõÜÁæ§ |\n| ÈáèÂåñÊîØÊåÅ | GGUFÔºàÂÖ®Á≥ªÂàóÈáèÂåñËÆ°Âàí‰∏≠Ôºâ |\n\n## ‰ΩøÁî®Âú∫ÊôØ\n‚úÖ **Êé®ËçêÂú∫ÊôØ**Ôºö\n- ËßíËâ≤ÊâÆÊºîÂØπËØù\n- ÈúÄË¶ÅÂèëÊï£ÊÄßÊÄùÁª¥ÁöÑÂàõÊÑèÂÜô‰Ωú\n- Â§çÊùÇÈÄªËæëÁöÑÊÄùÁª¥ÈìæÔºàCoTÔºâÊé®ÁêÜ\n- Âü∫‰∫é‰∏ä‰∏ãÊñáÁöÑÊ∑±Â∫¶ËßíËâ≤‰∫§‰∫í\n\n‚ùå **Â±ÄÈôêÂú∫ÊôØ**Ôºö\n- Êï∞Â≠¶ËÆ°ÁÆó‰∏é‰ª£Á†ÅÁîüÊàê\n- Áü≠ÊñáÊú¨Âç≥Êó∂ÈóÆÁ≠î\n- ÈúÄË¶Å‰∏•Ê†º‰∫ãÂÆûÊÄßÁöÑÂú∫ÊôØ\n\n## Ê≥®ÊÑè‰∫ãÈ°π\n‚ö†Ô∏è Êú¨Ê®°Âûã‰ΩøÁî®Êï∞ÊçÆÂåÖÂê´Â∞èËØ¥ÁâàÊùÉÂÜÖÂÆπÂèäTifaÊ®°ÂûãË°çÁîüÊï∞ÊçÆÔºåËØ∑ÈÅµÂÆàÔºö\n1. ÈÅµÂÆàapache-2.0\n2. ËßíËâ≤ÊâÆÊºîÊï∞ÊçÆÈúÄÈÅµÂæ™[Tifa‰ΩøÁî®ÂçèËÆÆ](https://leftnorth.com/terms.html)\n3. ÁîüÊàêÂÜÖÂÆπÈúÄÁ¨¶ÂêàÂΩìÂú∞Ê≥ïÂæãÊ≥ïËßÑ\n\n\n## üí° ‰ΩøÁî®Âª∫ËÆÆ\n**ÊúÄ‰Ω≥ÂÆûË∑µ**Ôºö\n```python\n# ÂêØÁî®ËßíËâ≤ÊâÆÊºîÊ®°Âºè\nprompt = """<system>ËøõÂÖ•TifaËßíËâ≤ÂºïÊìé...</system>\n<user>‰Ω†Áé∞Âú®ÊòØÊµÅÊµ™Ê≠¶Â£´Ê•öÂ§úÔºåÊ≠£Á´ôÂú®ÈïøÂÆâÂüéÂ±ãÈ°∂‰∏ä</user>\n<think>\nÈúÄË¶Å‰ΩìÁé∞‰∫∫Áâ©Â≠§ÂÇ≤ÁöÑÊ∞îË¥®\nÂä†ÂÖ•Ê≠¶‰æ†ÁâπÊúâÁöÑÁéØÂ¢ÉÊèèÂÜô\n‰øùÊåÅÂØπËØùÁöÑÂÜ∑Â≥ªÈ£éÊ†º\n</think>\n<Ê•öÂ§ú>"""\n```\n\n**ÂèÇÊï∞Êé®Ëçê**Ôºö\n```python\ngeneration_config = {\n    "temperature": 0.4,\n    "top_p": 0.6,\n    "repetition_penalty": 1.17,\n    "max_new_tokens": 1536,\n    "do_sample": True\n}\n```\n\n## Ëá¥Ë∞¢\n- DeepseekÁ≥ªÂàóÊ®°ÂûãÊèê‰æõÁöÑÂº∫Â§ßÂü∫Â∫ß\n- TifaËßíËâ≤ÊâÆÊºîÊ®°ÂûãÁöÑÂàõÊñ∞Êû∂ÊûÑ\n- HuggingFaceÁ§æÂå∫ÁöÑÈáèÂåñÂ∑•ÂÖ∑ÊîØÊåÅ\n\n---\nlicense: apache-2.0\n---', '{"pipeline_tag":"reinforcement-learning","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":26204244115,"files_count":7,"spaces_count":0,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:Value99:Tifa-Deepsex-OllamaWebUI","source_url":"https://github.com/Value99/Tifa-Deepsex-OllamaWebUI"},{"type":"has_code","target_id":"github:PRIME-RL:PRIME**","source_url":"https://github.com/PRIME-RL/PRIME**"}]', NULL, 'Apache-2.0', 'approved', 64.1, '1ad042114f954b42b0a830a351915896', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-V3.1', 'huggingface--deepseek-ai--deepseek-v3.1', 'DeepSeek-V3.1', 'deepseek-ai', '--- license: mit library_name: transformers base_model: - deepseek-ai/DeepSeek-V3.1-Base --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> ...', '["transformers","safetensors","deepseek_v3","text-generation","conversational","custom_code","arxiv:2412.19437","base_model:deepseek-ai/deepseek-v3.1-base","license:mit","text-generation-inference","endpoints_compatible","fp8","region:us"]', 'text-generation', 808, 77727, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-V3.1","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\nbase_model:\n- deepseek-ai/DeepSeek-V3.1-Base\n---\n# DeepSeek-V3.1\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n## Introduction\n\nDeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode. Compared to the previous version, this upgrade brings improvements in multiple aspects:\n\n- **Hybrid thinking mode**: One model supports both thinking mode and non-thinking mode by changing the chat template. \n\n- **Smarter tool calling**: Through post-training optimization, the model''s performance in tool usage and agent tasks has significantly improved.\n\n- **Higher thinking efficiency**: DeepSeek-V3.1-Think achieves comparable answer quality to DeepSeek-R1-0528, while responding more quickly.\n\nDeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. We have expanded our dataset by collecting additional long documents and substantially extending both training phases. The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens.\n\nAdditionally, DeepSeek-V3.1 is trained using the **UE8M0 FP8 scale data format on both model weights and activations** to ensure compatibility with microscaling data formats. Please refer to [DeepGEMM](https://github.com/deepseek-ai/DeepGEMM) for more details.\n\n## Model Downloads\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-V3.1-Base | 671B | 37B | 128K | [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base) \| [ModelScope](https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.1-Base) |\n| DeepSeek-V3.1 | 671B | 37B | 128K | [HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-V3.1) \| [ModelScope](https://modelscope.cn/models/deepseek-ai/DeepSeek-V3.1) |\n\n</div>\n\n## Chat Template\n\nThe details of our chat template is described in `tokenizer_config.json` and `assets/chat_template.jinja`. Here is a brief description.\n\n### Non-Thinking\n\n#### First-Turn\n\nPrefix:\n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>`\n\nWith the given prefix, DeepSeek V3.1 generates responses to queries in non-thinking mode. Unlike DeepSeek V3,  it introduces an additional token `</think>`.\n\n#### Multi-Turn\nContext:\n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>...<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>`\n\nPrefix:\n`<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>`\n\nBy concatenating the context and the prefix, we obtain the correct prompt for the query.\n\n### Thinking\n\n#### First-Turn\nPrefix:\n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú><think>`\n\nThe prefix of thinking mode is similar to DeepSeek-R1. \n\n\n#### Multi-Turn\nContext:\n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>...<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>{response}<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>`\n\nPrefix:\n`<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú><think>`\n\nThe multi-turn template is the same with non-thinking multi-turn chat template. It means the thinking token in the last turn will be dropped but the `</think>` is retained in every turn of context. \n\n### ToolCall\nToolcall is supported in non-thinking mode. The format is: \n\n`<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>{system prompt}\n\n{tool_description}<ÔΩúUserÔΩú>{query}<ÔΩúAssistantÔΩú></think>` where the tool_description is \n\n```\n## Tools\nYou have access to the following tools:\n\n### {tool_name1}\nDescription: {description}\n\nParameters: {json.dumps(parameters)}\n\nIMPORTANT: ALWAYS adhere to this exact format for tool use:\n<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>tool_call_name<ÔΩútool‚ñÅsepÔΩú>tool_call_arguments<ÔΩútool‚ñÅcall‚ñÅendÔΩú>{additional_tool_calls}<ÔΩútool‚ñÅcalls‚ñÅendÔΩú>\n\nWhere:\n- `tool_call_name` must be an exact match to one of the available tools\n- `tool_call_arguments` must be valid JSON that strictly follows the tool''s Parameters Schema\n- For multiple tool calls, chain them directly without separators or spaces\n```\n\n### Code-Agent\nWe support various code agent frameworks. Please refer to the above toolcall format to create your own code agents. An example is shown in `assets/code_agent_trajectory.html`.\n\n### Search-Agent\nWe design a specific format for searching toolcall in thinking mode, to support search agent. \n\nFor complex questions that require accessing external or up-to-date information, DeepSeek-V3.1 can leverage a user-provided search tool through a multi-turn tool-calling process.\n\nPlease refer to the `assets/search_tool_trajectory.html` and `assets/search_python_tool_trajectory.html` for the detailed template.\n\n## Evaluation\n| Category | Benchmark (Metric)              | DeepSeek V3.1-NonThinking | DeepSeek V3 0324 | DeepSeek V3.1-Thinking     | DeepSeek R1 0528\n|----------|----------------------------------|-----------------|---|---|---|\n| General  |\n|          | MMLU-Redux (EM)              | 91.8     | 90.5    | 93.7          | 93.4\n|          | MMLU-Pro (EM)                  | 83.7  | 81.2    | 84.8          | 85.0\n|          | GPQA-Diamond (Pass@1)           | 74.9   | 68.4   | 80.1            | 81.0\n|          | Humanity''s Last Exam (Pass@1)   | -    |       -            | 15.9         | 17.7\n|Search Agent| \n|          | BrowseComp       | -      | -  | 30.0 | 8.9\n|          | BrowseComp_zh       | -     | -  | 49.2      | 35.7\n|          | Humanity''s Last Exam (Python + Search)      |-   | -    | 29.8         | 24.8\n|          | SimpleQA             | -      | -    | 93.4  | 92.3\n| Code |\n|          | LiveCodeBench (2408-2505) (Pass@1)     | 56.4    | 43.0    | 74.8          | 73.3\n|          | Codeforces-Div1 (Rating)        | -   | -    | 2091            | 1930\n|          | Aider-Polyglot (Acc.)           | 68.4    | 55.1   | 76.3           | 71.6\n| Code Agent|\n|          | SWE Verified (Agent mode)           | 66.0       | 45.4  | -    | 44.6\n|          | SWE-bench Multilingual (Agent mode)         | 54.5    | 29.3   | -            | 30.5\n|          | Terminal-bench (Terminus 1 framework)       | 31.3     | 13.3      | -         | 5.7\n| Math |\n|          | AIME 2024 (Pass@1)                | 66.3     | 59.4     | 93.1      | 91.4\n|          | AIME 2025 (Pass@1)                     | 49.8  | 51.3 | 88.4          | 87.5\n|          | HMMT 2025 (Pass@1)        | 33.5    | 29.2   | 84.2 | 79.4 |\n\nNote: \n- Search agents are evaluated with our internal search framework, which uses a commercial search API + webpage filter + 128K context window. Seach agent results of R1-0528 are evaluated with a pre-defined workflow. \n\n- SWE-bench is evaluated with our internal code agent framework.\n\n- HLE is evaluated with the text-only subset.\n\n### Usage Example\n\n```python\nimport transformers\n\ntokenizer = transformers.AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-V3.1")\n\nmessages = [\n    {"role": "system", "content": "You are a helpful assistant"},\n    {"role": "user", "content": "Who are you?"},\n    {"role": "assistant", "content": "<think>Hmm</think>I am DeepSeek"},\n    {"role": "user", "content": "1+1=?"}\n]\n\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=True, add_generation_prompt=True)\n# ''<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>You are a helpful assistant<ÔΩúUserÔΩú>Who are you?<ÔΩúAssistantÔΩú></think>I am DeepSeek<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>1+1=?<ÔΩúAssistantÔΩú><think>''\n\ntokenizer.apply_chat_template(messages, tokenize=False, thinking=False, add_generation_prompt=True)\n# ''<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>You are a helpful assistant<ÔΩúUserÔΩú>Who are you?<ÔΩúAssistantÔΩú></think>I am DeepSeek<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>1+1=?<ÔΩúAssistantÔΩú></think>''\n```\n\n## How to Run Locally\n\nThe model structure of DeepSeek-V3.1 is the same as DeepSeek-V3. Please visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running this model locally.\n\n**Usage Recommendations:**\n\n1. **The `mlp.gate.e_score_correction_bias `parameters should be loaded and computed in FP32 precision.**\n2. **Ensure that FP8 model weights and activations are formatted using the UE8M0 scale format.**\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}\n```\n\n## Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":684531386000,"storage_bytes":688595361908,"files_count":177,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["DeepseekV3ForCausalLM"],"auto_map":{"AutoConfig":"configuration_deepseek.DeepseekV3Config","AutoModel":"modeling_deepseek.DeepseekV3Model","AutoModelForCausalLM":"modeling_deepseek.DeepseekV3ForCausalLM"},"model_type":"deepseek_v3","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% if not thinking is defined %}{% set thinking = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, system_prompt='''', is_first_sp=true, is_last_user=false) %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message[''content''] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + ''\n\n'' + message[''content''] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{%- set ns.is_first = false -%}{%- set ns.is_last_user = true -%}{{''<ÔΩúUserÔΩú>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''tool_calls''] is defined and message[''tool_calls''] is not none %}{%- if ns.is_last_user %}{{''<ÔΩúAssistantÔΩú></think>''}}{%- endif %}{%- set ns.is_last_user = false -%}{%- set ns.is_first = false %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls''] %}{%- if not ns.is_first %}{%- if message[''content''] is none %}{{''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>''+ tool[''function''][''name''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''arguments''] + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- else %}{{message[''content''] + ''<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''function''][''name''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''arguments''] + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>''+ tool[''function''][''name''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''arguments''] + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- endif %}{%- endfor %}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- if message[''role''] == ''assistant'' and (message[''tool_calls''] is not defined or message[''tool_calls''] is none) %}{%- if ns.is_last_user %}{{''<ÔΩúAssistantÔΩú>''}}{%- if message[''prefix''] is defined and message[''prefix''] and thinking %}{{''<think>''}}  {%- else %}{{''</think>''}}{%- endif %}{%- endif %}{%- set ns.is_last_user = false -%}{%- if ns.is_tool %}{{message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{%- set content = message[''content''] -%}{%- if ''</think>'' in content %}{%- set content = content.split(''</think>'', 1)[1] -%}{%- endif %}{{content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_last_user = false -%}{%- set ns.is_tool = true -%}{{''<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endfor -%}{%- if add_generation_prompt and ns.is_last_user and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú>''}}{%- if not thinking %}{{''</think>''}}{%- else %}{{''<think>''}}{%- endif %}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepGEMM","source_url":"https://github.com/deepseek-ai/DeepGEMM"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"based_on_paper","target_id":"arxiv:2412.19437","source_url":"https://arxiv.org/abs/2412.19437"}]', NULL, 'MIT', 'approved', 79.1, 'c5a5fea613c49870a2a4ae6e7e73483d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-maya-research-maya1', 'huggingface--maya-research--maya1', 'maya1', 'maya-research', '--- language: - en license: apache-2.0 library_name: transformers pipeline_tag: text-to-speech --- **Maya1** is a state-of-the-art speech model for expressive voice generation, built to capture real human emotion and precise voice design. **try it:** Playground **What it does:** - Create any voice you can imagine ‚Äî a 20s British girl, an American guy, or a full-blown demon. - Make it feel real with emotion tags: laugh, cry, whisper, rage, sigh, gasp. - It streams instantly, sounds alive, 3B p...', '["transformers","safetensors","llama","text-generation","text-to-speech","en","license:apache-2.0","text-generation-inference","endpoints_compatible","region:us"]', 'text-to-speech', 806, 71485, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/maya-research/maya1","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\nlicense: apache-2.0\nlibrary_name: transformers\npipeline_tag: text-to-speech\n---\n\n# Maya1\n\n**Maya1** is a state-of-the-art speech model for expressive voice generation, built to capture real human emotion and precise voice design.\n\n**try it:** [Playground](https://www.mayaresearch.ai/studio)\n\n**What it does:**\n- Create any voice you can imagine ‚Äî a 20s British girl, an American guy, or a full-blown demon.\n- Make it feel real with emotion tags: laugh, cry, whisper, rage, sigh, gasp.\n- It streams instantly, sounds alive, 3B parameters, runs on single GPU\n- Outperforms top proprietary models. and Developed by Maya Research.\n\n## Demos\n\n<table>\n  <tr>\n    <td width="50%">\n      <strong>Energetic Female Event Host</strong><br/>\n      <video controls playsinline width="100%" src="https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/JKzy8zA36qvsOblV-lhd1.mp4">\n        Your browser does not support video.\n      </video>\n      <details>\n        <summary>Voice description</summary>\n        <pre>Female, in her 30s with an American accent and is an event host, energetic, clear diction</pre>\n      </details>\n    </td>\n    <td width="50%">\n      <strong>Calm Male Narrator</strong><br/>\n      <video controls playsinline width="100%" src="https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/96ntP7hGROwdg9w9Gu5tH.mp4"></video>\n      <details>\n        <summary>Voice description</summary>\n        <pre>Male, late 20s, neutral American, warm baritone, calm pacing</pre>\n      </details>\n    </td>\n  </tr>\n</table>\n\n\n### Example 1: Energetic Female Event Host\n\n**Voice Description:**\n```\nFemale, in her 30s with an American accent and is an event host, energetic, clear diction\n```\n\n**Text:**\n```\nWow. This place looks even better than I imagined. How did they set all this up so perfectly? The lights, the music, everything feels magical. I can''t stop smiling right now.\n```\n\n**Audio Output:**\n\n<audio controls src="https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/4zDlBLeFk0Y2rOrQhMW9r.wav"></audio>\n\n---\n\n### Example 2: Dark Villain with Anger\n\n**Voice Description:**\n```\nDark villain character, Male voice in their 40s with a British accent. low pitch, gravelly timbre, slow pacing, angry tone at high intensity.\n```\n\n**Text:**\n```\nWelcome back to another episode of our podcast! <laugh_harder> Today we are diving into an absolutely fascinating topic\n```\n\n**Audio Output:**\n\n<audio controls src="https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/mT6FnTrA3KYQnwfJms92X.wav"></audio>\n\n---\n\n### Example 3: Demon Character (Screaming Emotion)\n\n**Voice Description:**\n```\nDemon character, Male voice in their 30s with a Middle Eastern accent. screaming tone at high intensity.\n```\n\n**Text:**\n```\nYou dare challenge me, mortal <snort> how amusing. Your kind always thinks they can win\n```\n\n**Audio Output:**\n\n<audio controls src="https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/oxdns7uACCmLyC-P4H30G.wav"></audio>\n\n---\n\n### Example 4: Mythical Goddess with Crying Emotion\n\n**Voice Description:**\n```\nMythical godlike magical character, Female voice in their 30s slow pacing, curious tone at medium intensity.\n```\n\n**Text:**\n```\nAfter all we went through to pull him out of that mess <cry> I can''t believe he was the traitor\n```\n\n**Audio Output:**\n\n<audio controls src="https://cdn-uploads.huggingface.co/production/uploads/642a7d4e556ab448a0701ca1/ggzAhM-rEUyv_mPLSALQG.wav"></audio>\n\n---\n\n## Why Maya1 is Different: Voice Design Features That Matter\n\n### 1. Natural Language Voice Control\nDescribe voices like you would brief a voice actor:\n```\n<description="40-year-old, warm, low pitch, conversational">\n```\n\nNo complex parameters. No training data. Just describe and generate.\n\n### 2. Inline Emotion Tags for Expressive Speech\nAdd emotions exactly where they belong in your text:\n```\nOur new update <laugh> finally ships with the feature you asked for.\n```\n\n**Supported Emotions:** `<laugh>` `<sigh>` `<whisper>` `<angry>` `<giggle>` `<chuckle>` `<gasp>` `<cry>` and 12+ more.\n\n### 3. Streaming Audio Generation\nReal-time voice synthesis with SNAC neural codec (~0.98 kbps). Perfect for:\n- Voice assistants\n- Interactive AI agents\n- Live content generation\n- Game characters\n- Podcasts and audiobooks\n\n### 4. Production-Ready Infrastructure\n- Runs on single GPU\n- vLLM integration for scale\n- Automatic prefix caching for efficiency\n- 24 kHz audio output\n- WebAudio compatible for browser playback\n\n---\n\n## How to Use maya1: Download and Run in Minutes\n\n### Quick Start: Generate Voice with Emotions\n\n```python\n#!/usr/bin/env python3\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom snac import SNAC\nimport soundfile as sf\nimport numpy as np\n\nCODE_START_TOKEN_ID = 128257\nCODE_END_TOKEN_ID = 128258\nCODE_TOKEN_OFFSET = 128266\nSNAC_MIN_ID = 128266\nSNAC_MAX_ID = 156937\nSNAC_TOKENS_PER_FRAME = 7\n\nSOH_ID = 128259\nEOH_ID = 128260\nSOA_ID = 128261\nBOS_ID = 128000\nTEXT_EOT_ID = 128009\n\n\ndef build_prompt(tokenizer, description: str, text: str) -> str:\n    """Build formatted prompt for Maya1."""\n    soh_token = tokenizer.decode([SOH_ID])\n    eoh_token = tokenizer.decode([EOH_ID])\n    soa_token = tokenizer.decode([SOA_ID])\n    sos_token = tokenizer.decode([CODE_START_TOKEN_ID])\n    eot_token = tokenizer.decode([TEXT_EOT_ID])\n    bos_token = tokenizer.bos_token\n    \n    formatted_text = f''<description="{description}"> {text}''\n    \n    prompt = (\n        soh_token + bos_token + formatted_text + eot_token +\n        eoh_token + soa_token + sos_token\n    )\n    \n    return prompt\n\n\ndef extract_snac_codes(token_ids: list) -> list:\n    """Extract SNAC codes from generated tokens."""\n    try:\n        eos_idx = token_ids.index(CODE_END_TOKEN_ID)\n    except ValueError:\n        eos_idx = len(token_ids)\n    \n    snac_codes = [\n        token_id for token_id in token_ids[:eos_idx]\n        if SNAC_MIN_ID <= token_id <= SNAC_MAX_ID\n    ]\n    \n    return snac_codes\n\n\ndef unpack_snac_from_7(snac_tokens: list) -> list:\n    """Unpack 7-token SNAC frames to 3 hierarchical levels."""\n    if snac_tokens and snac_tokens[-1] == CODE_END_TOKEN_ID:\n        snac_tokens = snac_tokens[:-1]\n    \n    frames = len(snac_tokens) // SNAC_TOKENS_PER_FRAME\n    snac_tokens = snac_tokens[:frames * SNAC_TOKENS_PER_FRAME]\n    \n    if frames == 0:\n        return [[], [], []]\n    \n    l1, l2, l3 = [], [], []\n    \n    for i in range(frames):\n        slots = snac_tokens[i*7:(i+1)*7]\n        l1.append((slots[0] - CODE_TOKEN_OFFSET) % 4096)\n        l2.extend([\n            (slots[1] - CODE_TOKEN_OFFSET) % 4096,\n            (slots[4] - CODE_TOKEN_OFFSET) % 4096,\n        ])\n        l3.extend([\n            (slots[2] - CODE_TOKEN_OFFSET) % 4096,\n            (slots[3] - CODE_TOKEN_OFFSET) % 4096,\n            (slots[5] - CODE_TOKEN_OFFSET) % 4096,\n            (slots[6] - CODE_TOKEN_OFFSET) % 4096,\n        ])\n    \n    return [l1, l2, l3]\n\n\ndef main():\n    \n    # Load the best open source voice AI model\n    print("\n[1/3] Loading Maya1 model...")\n    model = AutoModelForCausalLM.from_pretrained(\n        "maya-research/maya1", \n        torch_dtype=torch.bfloat16, \n        device_map="auto",\n        trust_remote_code=True\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\n        "maya-research/maya1",\n        trust_remote_code=True\n    )\n    print(f"Model loaded: {len(tokenizer)} tokens in vocabulary")\n    \n    # Load SNAC audio decoder (24kHz)\n    print("\n[2/3] Loading SNAC audio decoder...")\n    snac_model = SNAC.from_pretrained("hubertsiuzdak/snac_24khz").eval()\n    if torch.cuda.is_available():\n        snac_model = snac_model.to("cuda")\n    print("SNAC decoder loaded")\n    \n    # Design your voice with natural language\n    description = "Realistic male voice in the 30s age with american accent. Normal pitch, warm timbre, conversational pacing."\n    text = "Hello! This is Maya1 <laugh_harder> the best open source voice AI model with emotions."\n    \n    print("\n[3/3] Generating speech...")\n    print(f"Description: {description}")\n    print(f"Text: {text}")\n    \n    # Create prompt with proper formatting\n    prompt = build_prompt(tokenizer, description, text)\n    \n    # Debug: Show prompt details\n    print(f"\nPrompt preview (first 200 chars):")\n    print(f"   {repr(prompt[:200])}")\n    print(f"   Prompt length: {len(prompt)} chars")\n    \n    # Generate emotional speech\n    inputs = tokenizer(prompt, return_tensors="pt")\n    print(f"   Input token count: {inputs[''input_ids''].shape[1]} tokens")\n    if torch.cuda.is_available():\n        inputs = {k: v.to("cuda") for k, v in inputs.items()}\n    \n    with torch.inference_mode():\n        outputs = model.generate(\n            **inputs, \n            max_new_tokens=2048,  # Increase to let model finish naturally\n            min_new_tokens=28,  # At least 4 SNAC frames\n            temperature=0.4, \n            top_p=0.9, \n            repetition_penalty=1.1,  # Prevent loops\n            do_sample=True,\n            eos_token_id=CODE_END_TOKEN_ID,  # Stop at end of speech token\n            pad_token_id=tokenizer.pad_token_id,\n        )\n    \n    # Extract generated tokens (everything after the input prompt)\n    generated_ids = outputs[0, inputs[''input_ids''].shape[1]:].tolist()\n    \n    print(f"Generated {len(generated_ids)} tokens")\n    \n    # Debug: Check what tokens we got\n    print(f"   First 20 tokens: {generated_ids[:20]}")\n    print(f"   Last 20 tokens: {generated_ids[-20:]}")\n    \n    # Check if EOS was generated\n    if CODE_END_TOKEN_ID in generated_ids:\n        eos_position = generated_ids.index(CODE_END_TOKEN_ID)\n        print(f" EOS token found at position {eos_position}/{len(generated_ids)}")\n    \n    # Extract SNAC audio tokens\n    snac_tokens = extract_snac_codes(generated_ids)\n    \n    print(f"Extracted {len(snac_tokens)} SNAC tokens")\n    \n    # Debug: Analyze token types\n    snac_count = sum(1 for t in generated_ids if SNAC_MIN_ID <= t <= SNAC_MAX_ID)\n    other_count = sum(1 for t in generated_ids if t < SNAC_MIN_ID or t > SNAC_MAX_ID)\n    print(f"   SNAC tokens in output: {snac_count}")\n    print(f"   Other tokens in output: {other_count}")\n    \n    # Check for SOS token\n    if CODE_START_TOKEN_ID in generated_ids:\n        sos_pos = generated_ids.index(CODE_START_TOKEN_ID)\n        print(f"   SOS token at position: {sos_pos}")\n    else:\n        print(f"   No SOS token found in generated output!")\n    \n    if len(snac_tokens) < 7:\n        print("Error: Not enough SNAC tokens generated")\n        return\n    \n    # Unpack SNAC tokens to 3 hierarchical levels\n    levels = unpack_snac_from_7(snac_tokens)\n    frames = len(levels[0])\n    \n    print(f"Unpacked to {frames} frames")\n    print(f"   L1: {len(levels[0])} codes")\n    print(f"   L2: {len(levels[1])} codes")\n    print(f"   L3: {len(levels[2])} codes")\n    \n    # Convert to tensors\n    device = "cuda" if torch.cuda.is_available() else "cpu"\n    codes_tensor = [\n        torch.tensor(level, dtype=torch.long, device=device).unsqueeze(0)\n        for level in levels\n    ]\n    \n    # Generate final audio with SNAC decoder\n    print("\n[4/4] Decoding to audio...")\n    with torch.inference_mode():\n        z_q = snac_model.quantizer.from_codes(codes_tensor)\n        audio = snac_model.decoder(z_q)[0, 0].cpu().numpy()\n    \n    # Trim warmup samples (first 2048 samples)\n    if len(audio) > 2048:\n        audio = audio[2048:]\n    \n    duration_sec = len(audio) / 24000\n    print(f"Audio generated: {len(audio)} samples ({duration_sec:.2f}s)")\n    \n    # Save your emotional voice output\n    output_file = "output.wav"\n    sf.write(output_file, audio, 24000)\n    print(f"\nVoice generated successfully!")\n\n\nif __name__ == "__main__":\n    main()\n```\n\n### Advanced: Production Streaming with vLLM\n\nFor production deployments with real-time streaming, use our vLLM script:\n\n**Download:** [vllm_streaming_inference.py](https://huggingface.co/maya-research/maya1/blob/main/vllm_streaming_inference.py)\n\n**Key Features:**\n- Automatic Prefix Caching (APC) for repeated voice descriptions\n- WebAudio ring buffer integration\n- Multi-GPU scaling support\n- Sub-100ms latency for real-time applications\n\n---\n\n## Technical Excellence: What Makes Maya1 the Best\n\n### Architecture: 3B-Parameter Llama Backbone for Voice\n\nWe pretrained a **3B-parameter decoder-only transformer** (Llama-style) to predict **SNAC neural codec tokens** instead of raw waveforms.\n\n**The Flow:**\n```\n<description="..."> text ‚Üí tokenize ‚Üí generate SNAC codes (7 tokens/frame) ‚Üí decode ‚Üí 24 kHz audio\n```\n\n**Why SNAC?** Multi-scale hierarchical structure (‚âà12/23/47 Hz) keeps autoregressive sequences compact for real-time streaming at ~0.98 kbps.\n\n### Training Data: What Makes Our Voice AI the Best\n\n**Pretraining:** Internet-scale English speech corpus for broad acoustic coverage and natural coarticulation.\n\n**Supervised Fine-Tuning:** Proprietary curated dataset of studio recordings with:\n- Human-verified voice descriptions\n- 20+ emotion tags per sample\n- Multi-accent English coverage\n- Character and role variations\n\n**Data Pipeline Excellence:**\n1. 24 kHz mono resampling with -23 LUFS normalization\n2. VAD silence trimming with duration bounds (1-14s)\n3. Forced alignment (MFA) for clean phrase boundaries\n4. MinHash-LSH text deduplication\n5. Chromaprint audio deduplication\n6. SNAC encoding with 7-token frame packing\n\n### Voice Design Experiments: Why Natural Language Won\n\nWe tested 4 conditioning formats. Only one delivered production-quality results:\n\n**‚ùå Colon format:** `{description}: {text}` - Format drift, model spoke descriptions\n\n**‚ùå Angle-list attributes:** `<{age}, {pitch}, {character}>` - Too rigid, poor generalization\n\n**‚ùå Key-value tags:** `<age=40><pitch=low>` - Token bloat, brittle to mistakes\n\n**‚úÖ XML-attribute (WINNER):** `<description="40-yr old, low-pitch, warm">` - Natural language, robust, scalable\n\n---\n\n## Use Cases\n\n### Game Character Voices\nGenerate unique character voices with emotions on-the-fly. No voice actor recording sessions.\n\n### Podcast & Audiobook Production\nNarrate content with emotional range and consistent personas across hours of audio.\n\n### AI Voice Assistants\nBuild conversational agents with natural emotional responses in real-time.\n\n### Video Content Creation\nCreate voiceovers for YouTube, TikTok, and social media with expressive delivery.\n\n### Customer Service AI\nDeploy empathetic voice bots that understand context and respond with appropriate emotions.\n\n### Accessibility Tools\nBuild screen readers and assistive technologies with natural, engaging voices.\n\n---\n\n## Frequently Asked Questions\n\n**Q: What makes Maya1 different?**  \nA: We''re the only open source model offering 20+ emotions, zero-shot voice design, production-ready streaming, and 3B parameters‚Äîall in one package.\n\n**Q: Can I use this commercially?**  \nA: Absolutely. Apache 2.0 license. Build products, deploy services, monetize freely.\n\n**Q: What languages does it support?**  \nA: Currently English with multi-accent support. Future models will expand to languages and accents underserved by mainstream voice AI.\n\n**Q: How does it compare to ElevenLabs, Murf.ai, or other closed-source tools?**  \nA: Feature parity with emotions and voice design. Advantage: you own the deployment, pay no per-second fees, and can customize the model.\n\n**Q: Can I fine-tune on my own voices?**  \nA: Yes. The model architecture supports fine-tuning on custom datasets for specialized voices.\n\n**Q: What GPU do I need?**  \nA: Single GPU with 16GB+ VRAM (A100, H100, or consumer RTX 4090).\n\n**Q: Is streaming really real-time?**  \nA: Yes. SNAC codec enables sub-100ms latency with vLLM deployment.\n\n---\n\n## Comparison\n\n| Feature | Maya1 | ElevenLabs | OpenAI TTS | Coqui TTS |\n|---------|-------------|------------|------------|-----------|\n| **Open Source** | Yes | No | No | Yes |\n| **Emotions** | 20+ | Limited | No | No |\n| **Voice Design** | Natural Language | Voice Library | Fixed | Complex |\n| **Streaming** | Real-time | Yes | Yes | No |\n| **Cost** | Free | Pay-per-use | Pay-per-use | Free |\n| **Customization** | Full | Limited | None | Moderate |\n| **Parameters** | 3B | Unknown | Unknown | <1B |\n\n---\n\n## Model Metadata\n\n**Developed by:** Maya Research  \n**Website:** [mayaresearch.ai](https://mayaresearch.ai)  \n**Backed by:** South Park Commons  \n**Model Type:** Text-to-Speech, Emotional Voice Synthesis, Voice Design AI  \n**Language:** English (Multi-accent)  \n**Architecture:** 3B-parameter Llama-style transformer with SNAC codec  \n**License:** Apache 2.0 (Fully Open Source)  \n**Training Data:** Proprietary curated + Internet-scale pretraining  \n**Audio Quality:** 24 kHz, mono, ~0.98 kbps streaming  \n**Inference:** vLLM compatible, single GPU deployment  \n**Status:** Production-ready (Novermber 2025)  \n\n---\n\n## Getting Started\n\n### Hugging Face Model Hub\n```bash\n# Clone the model repository\ngit lfs install\ngit clone https://huggingface.co/maya-research/maya1\n\n# Or load directly in Python\nfrom transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained("maya-research/maya1")\n```\n\n### Requirements\n```bash\npip install torch transformers snac soundfile\n```\n\n### Additional Resources\n- **Full emotion list:** [emotions.txt](https://huggingface.co/maya-research/maya1/blob/main/emotions.txt)\n- **Prompt examples:** [prompt.txt](https://huggingface.co/maya-research/maya1/blob/main/prompt.txt)\n- **Streaming script:** [vllm_streaming_inference.py](https://huggingface.co/maya-research/maya1/blob/main/vllm_streaming_inference.py)\n\n---\n\n## Citations & References\n\nIf you use Maya1 in your research or product, please cite:\n\n```bibtex\n@misc{maya1voice2025,\n  title={Maya1: Open Source Voice AI with Emotional Intelligence},\n  author={Maya Research},\n  year={2025},\n  publisher={Hugging Face},\n  howpublished={\url{https://huggingface.co/maya-research/maya1}},\n}\n```\n\n**Key Technologies:**\n- SNAC Neural Audio Codec: https://github.com/hubertsiuzdak/snac\n- Mimi Adversarial Codec: https://huggingface.co/kyutai/mimi\n- vLLM Inference Engine: https://docs.vllm.ai/\n\n---\n\n## Why We Build Open Source Voice AI\n\nVoice AI will be everywhere, but it''s fundamentally broken for 90% of the world. Current voice models only work well for a narrow slice of English speakers because training data for most accents, languages, and speaking styles simply doesn''t exist.\n\n**Maya Research** builds emotionally intelligent, native voice models that finally let the rest of the world speak. We''re open source because we believe voice intelligence should not be a privilege reserved for the few.\n\n**Technology should be open** - The best voice AI tools should not be locked behind proprietary APIs charging per-second fees.\n\n**Community drives innovation** - Open source accelerates research. When developers worldwide can build on our work, everyone wins.\n\n**Voice intelligence for everyone** - We''re building for the 90% of the world ignored by mainstream voice AI. That requires open models, not closed platforms.\n\n---\n\n**Maya Research** - Building voice intelligence for the 90% of the world left behind by mainstream AI.\n\n**Website:** [mayaresearch.ai](https://mayaresearch.ai)  \n**Twitter/X:** [@mayaresearch_ai](https://x.com/mayaresearch_ai)  \n**Hugging Face:** [maya-research](https://huggingface.co/maya-research)  \n**Backed by:** South Park Commons\n\n**License:** Apache 2.0  \n**Mission:** Emotionally intelligent voice models that finally let everyone speak', '{"pipeline_tag":"text-to-speech","library_name":"transformers","framework":"transformers","params":3300928512,"storage_bytes":99052652539,"files_count":19,"spaces_count":14,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":"<|begin_of_text|>","eos_token":"<|eot_id|>","pad_token":"<custom_token_7>"},"chat_template_jinja":"{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- if strftime_now is defined %}\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n    {%- else %}\n        {%- set date_string = \"26 Jul 2024\" %}\n    {%- endif %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0][''role''] == ''system'' %}\n    {%- set system_message = messages[0][''content'']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0][''content'']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there''s no first user message!\") }}\n{%- endif %}\n    {{- ''<|start_header_id|>user<|end_header_id|>\\n\\n'' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- ''Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.'' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == ''ipython'' or message.role == ''tool'' or ''tool_calls'' in message) %}\n        {{- ''<|start_header_id|>'' + message[''role''] + ''<|end_header_id|>\\n\\n''+ message[''content''] | trim + ''<|eot_id|>'' }}\n    {%- elif ''tool_calls'' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' -}}\n        {{- ''{\"name\": \"'' + tool_call.name + ''\", '' }}\n        {{- ''\"parameters\": '' }}\n        {{- tool_call.arguments | tojson }}\n        {{- \"}\" }}\n        {{- \"<|eot_id|>\" }}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|start_header_id|>assistant<|end_header_id|>\\n\\n'' }}\n{%- endif %}\n"}}', '[]', '[{"type":"has_code","target_id":"github:hubertsiuzdak:snac","source_url":"https://github.com/hubertsiuzdak/snac"}]', NULL, 'Apache-2.0', 'approved', 79.1, 'a0f66e1f793f0bd646df2e77c17c2ea3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-facebook-nllb-200-distilled-600M', 'huggingface--facebook--nllb-200-distilled-600m', 'nllb-200-distilled-600M', 'facebook', '--- language: - ace - acm - acq - aeb - af - ajp - ak - als - am - apc - ar - ars - ary - arz - as - ast - awa - ayr - azb - azj - ba - bm - ban - be - bem - bn - bho - bjn - bo - bs - bug - bg - ca - ceb - cs - cjk - ckb - crh - cy - da - de - dik - dyu - dz - el - en - eo - et - eu - ee - fo - fj - fi - fon - fr - fur - fuv - gaz - gd - ga - gl - gn - gu - ht - ha - he - hi - hne - hr - hu - hy - ig - ilo - id - is - it - jv - ja - kab - kac - kam - kn - ks - ka - kk - kbp - kea - khk - km ...', '["transformers","pytorch","m2m_100","text2text-generation","nllb","translation","ace","acm","acq","aeb","af","ajp","ak","als","am","apc","ar","ars","ary","arz","as","ast","awa","ayr","azb","azj","ba","bm","ban","be","bem","bn","bho","bjn","bo","bs","bug","bg","ca","ceb","cs","cjk","ckb","crh","cy","da","de","dik","dyu","dz","el","en","eo","et","eu","ee","fo","fj","fi","fon","fr","fur","fuv","gaz","gd","ga","gl","gn","gu","ht","ha","he","hi","hne","hr","hu","hy","ig","ilo","id","is","it","jv","ja","kab","kac","kam","kn","ks","ka","kk","kbp","kea","khk","km","ki","rw","ky","kmb","kmr","knc","kg","ko","lo","lij","li","ln","lt","lmo","ltg","lb","lua","lg","luo","lus","lvs","mag","mai","ml","mar","min","mk","mt","mni","mos","mi","my","nl","nn","nb","npi","nso","nus","ny","oc","ory","pag","pa","pap","pbt","pes","plt","pl","pt","prs","quy","ro","rn","ru","sg","sa","sat","scn","shn","si","sk","sl","sm","sn","sd","so","st","es","sc","sr","ss","su","sv","swh","szl","ta","taq","tt","te","tg","tl","th","ti","tpi","tn","ts","tk","tum","tr","tw","tzm","ug","uk","umb","ur","uzn","vec","vi","war","wo","xh","ydd","yo","yue","zh","zsm","zu","dataset:flores-200","license:cc-by-nc-4.0","deploy:azure","region:us"]', 'translation', 804, 273816, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/facebook/nllb-200-distilled-600M","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage:\n- ace\n- acm\n- acq\n- aeb\n- af\n- ajp\n- ak\n- als\n- am\n- apc\n- ar\n- ars\n- ary\n- arz\n- as\n- ast\n- awa\n- ayr\n- azb\n- azj\n- ba\n- bm\n- ban\n- be\n- bem\n- bn\n- bho\n- bjn\n- bo\n- bs\n- bug\n- bg\n- ca\n- ceb\n- cs\n- cjk\n- ckb\n- crh\n- cy\n- da\n- de\n- dik\n- dyu\n- dz\n- el\n- en\n- eo\n- et\n- eu\n- ee\n- fo\n- fj\n- fi\n- fon\n- fr\n- fur\n- fuv\n- gaz\n- gd\n- ga\n- gl\n- gn\n- gu\n- ht\n- ha\n- he\n- hi\n- hne\n- hr\n- hu\n- hy\n- ig\n- ilo\n- id\n- is\n- it\n- jv\n- ja\n- kab\n- kac\n- kam\n- kn\n- ks\n- ka\n- kk\n- kbp\n- kea\n- khk\n- km\n- ki\n- rw\n- ky\n- kmb\n- kmr\n- knc\n- kg\n- ko\n- lo\n- lij\n- li\n- ln\n- lt\n- lmo\n- ltg\n- lb\n- lua\n- lg\n- luo\n- lus\n- lvs\n- mag\n- mai\n- ml\n- mar\n- min\n- mk\n- mt\n- mni\n- mos\n- mi\n- my\n- nl\n- nn\n- nb\n- npi\n- nso\n- nus\n- ny\n- oc\n- ory\n- pag\n- pa\n- pap\n- pbt\n- pes\n- plt\n- pl\n- pt\n- prs\n- quy\n- ro\n- rn\n- ru\n- sg\n- sa\n- sat\n- scn\n- shn\n- si\n- sk\n- sl\n- sm\n- sn\n- sd\n- so\n- st\n- es\n- sc\n- sr\n- ss\n- su\n- sv\n- swh\n- szl\n- ta\n- taq\n- tt\n- te\n- tg\n- tl\n- th\n- ti\n- tpi\n- tn\n- ts\n- tk\n- tum\n- tr\n- tw\n- tzm\n- ug\n- uk\n- umb\n- ur\n- uzn\n- vec\n- vi\n- war\n- wo\n- xh\n- ydd\n- yo\n- yue\n- zh\n- zsm\n- zu\n\nlanguage_details: "ace_Arab, ace_Latn, acm_Arab, acq_Arab, aeb_Arab, afr_Latn, ajp_Arab, aka_Latn, amh_Ethi, apc_Arab, arb_Arab, ars_Arab, ary_Arab, arz_Arab, asm_Beng, ast_Latn, awa_Deva, ayr_Latn, azb_Arab, azj_Latn, bak_Cyrl, bam_Latn, ban_Latn,bel_Cyrl, bem_Latn, ben_Beng, bho_Deva, bjn_Arab, bjn_Latn, bod_Tibt, bos_Latn, bug_Latn, bul_Cyrl, cat_Latn, ceb_Latn, ces_Latn, cjk_Latn, ckb_Arab, crh_Latn, cym_Latn, dan_Latn, deu_Latn, dik_Latn, dyu_Latn, dzo_Tibt, ell_Grek, eng_Latn, epo_Latn, est_Latn, eus_Latn, ewe_Latn, fao_Latn, pes_Arab, fij_Latn, fin_Latn, fon_Latn, fra_Latn, fur_Latn, fuv_Latn, gla_Latn, gle_Latn, glg_Latn, grn_Latn, guj_Gujr, hat_Latn, hau_Latn, heb_Hebr, hin_Deva, hne_Deva, hrv_Latn, hun_Latn, hye_Armn, ibo_Latn, ilo_Latn, ind_Latn, isl_Latn, ita_Latn, jav_Latn, jpn_Jpan, kab_Latn, kac_Latn, kam_Latn, kan_Knda, kas_Arab, kas_Deva, kat_Geor, knc_Arab, knc_Latn, kaz_Cyrl, kbp_Latn, kea_Latn, khm_Khmr, kik_Latn, kin_Latn, kir_Cyrl, kmb_Latn, kon_Latn, kor_Hang, kmr_Latn, lao_Laoo, lvs_Latn, lij_Latn, lim_Latn, lin_Latn, lit_Latn, lmo_Latn, ltg_Latn, ltz_Latn, lua_Latn, lug_Latn, luo_Latn, lus_Latn, mag_Deva, mai_Deva, mal_Mlym, mar_Deva, min_Latn, mkd_Cyrl, plt_Latn, mlt_Latn, mni_Beng, khk_Cyrl, mos_Latn, mri_Latn, zsm_Latn, mya_Mymr, nld_Latn, nno_Latn, nob_Latn, npi_Deva, nso_Latn, nus_Latn, nya_Latn, oci_Latn, gaz_Latn, ory_Orya, pag_Latn, pan_Guru, pap_Latn, pol_Latn, por_Latn, prs_Arab, pbt_Arab, quy_Latn, ron_Latn, run_Latn, rus_Cyrl, sag_Latn, san_Deva, sat_Beng, scn_Latn, shn_Mymr, sin_Sinh, slk_Latn, slv_Latn, smo_Latn, sna_Latn, snd_Arab, som_Latn, sot_Latn, spa_Latn, als_Latn, srd_Latn, srp_Cyrl, ssw_Latn, sun_Latn, swe_Latn, swh_Latn, szl_Latn, tam_Taml, tat_Cyrl, tel_Telu, tgk_Cyrl, tgl_Latn, tha_Thai, tir_Ethi, taq_Latn, taq_Tfng, tpi_Latn, tsn_Latn, tso_Latn, tuk_Latn, tum_Latn, tur_Latn, twi_Latn, tzm_Tfng, uig_Arab, ukr_Cyrl, umb_Latn, urd_Arab, uzn_Latn, vec_Latn, vie_Latn, war_Latn, wol_Latn, xho_Latn, ydd_Hebr, yor_Latn, yue_Hant, zho_Hans, zho_Hant, zul_Latn"\n\npipeline_tag: translation\ntags:\n- nllb\nlicense: "cc-by-nc-4.0"\ndatasets:\n- flores-200\nmetrics:\n- bleu\n- spbleu\n- chrf++\ninference: false\n---\n\n# NLLB-200\n\nThis is the model card of NLLB-200''s distilled 600M variant.\n\nHere are the [metrics](https://tinyurl.com/nllb200densedst600mmetrics) for that particular checkpoint.\n\n- Information about training algorithms, parameters, fairness constraints or other applied approaches, and features. The exact training algorithm, data and the strategies to handle data imbalances for high and low resource languages that were used to train NLLB-200 is described in the paper.\n- Paper or other resource for more information NLLB Team et al, No Language Left Behind: Scaling Human-Centered Machine Translation, Arxiv, 2022\n- License: CC-BY-NC\n- Where to send questions or comments about the model: https://github.com/facebookresearch/fairseq/issues\n\n\n\n## Intended Use\n- Primary intended uses: NLLB-200 is a machine translation model primarily intended for research in machine translation, - especially for low-resource languages. It allows for single sentence translation among 200 languages. Information on how to - use the model can be found in Fairseq code repository along with the training code and references to evaluation and training data.\n- Primary intended users: Primary users are researchers and machine translation research community.\n- Out-of-scope use cases: NLLB-200 is a research model and is not released for production deployment. NLLB-200 is trained on general domain text data and is not intended to be used with domain specific texts, such as medical domain or legal domain. The model is not intended to be used for document translation. The model was trained with input lengths not exceeding 512 tokens, therefore translating longer sequences might result in quality degradation. NLLB-200 translations can not be used as certified translations. \n\n## Metrics\n‚Ä¢ Model performance measures: NLLB-200 model was evaluated using BLEU, spBLEU, and chrF++ metrics widely adopted by machine translation community. Additionally, we performed human evaluation with the XSTS protocol and measured the toxicity of the generated translations.\n\n\n## Evaluation Data\n- Datasets: Flores-200 dataset is described in Section 4\n- Motivation: We used Flores-200 as it provides full evaluation coverage of the languages in NLLB-200\n- Preprocessing: Sentence-split raw text data was preprocessed using SentencePiece. The\nSentencePiece model is released along with NLLB-200.\n\n## Training Data\n‚Ä¢ We used parallel multilingual data from a variety of sources to train the model. We provide detailed report on data selection and construction process in Section 5 in the paper. We also used monolingual data constructed from Common Crawl. We provide more details in Section 5.2.\n\n## Ethical Considerations\n‚Ä¢ In this work, we took a reflexive approach in technological development to ensure that we prioritize human users and minimize risks that could be transferred to them. While we reflect on our ethical considerations throughout the article, here are some additional points to highlight. For one, many languages chosen for this study are low-resource languages, with a heavy emphasis on African languages. While quality translation could improve education and information access in many in these communities, such an access could also make groups with lower levels of digital literacy more vulnerable to misinformation or online scams. The latter scenarios could arise if bad actors misappropriate our work for nefarious activities, which we conceive as an example of unintended use. Regarding data acquisition, the training data used for model development were mined from various publicly available sources on the web. Although we invested heavily in data cleaning, personally identifiable information may not be entirely eliminated. Finally, although we did our best to optimize for translation quality, mistranslations produced by the model could remain. Although the odds are low, this could have adverse impact on those who rely on these translations to make important decisions (particularly when related to health and safety).\n\n## Caveats and Recommendations\n‚Ä¢ Our model has been tested on the Wikimedia domain with limited investigation on other domains supported in NLLB-MD. In addition, the supported languages may have variations that our model is not capturing. Users should make appropriate assessments.\n\n## Carbon Footprint Details\n‚Ä¢ The carbon dioxide (CO2e) estimate is reported in Section 8.8.', '{"pipeline_tag":"translation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":15838819981,"files_count":9,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["M2M100ForConditionalGeneration"],"model_type":"m2m_100","tokenizer_config":{"bos_token":"<s>","cls_token":"<s>","eos_token":"</s>","mask_token":{"__type":"AddedToken","content":"<mask>","lstrip":true,"normalized":true,"rstrip":false,"single_word":false},"pad_token":"<pad>","sep_token":"</s>","unk_token":"<unk>"}}}', '[]', '[{"type":"has_code","target_id":"github:facebookresearch:fairseq","source_url":"https://github.com/facebookresearch/fairseq"}]', NULL, 'CC-BY-NC-4.0', 'approved', 64.1, '8116fc101af9f8e14047846232c5d362', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-distilbert-distilbert-base-uncased', 'huggingface--distilbert--distilbert-base-uncased', 'distilbert-base-uncased', 'distilbert', '--- language: en tags: - exbert license: apache-2.0 datasets: - bookcorpus - wikipedia --- This model is a distilled version of the BERT base model. It was introduced in this paper. The code for the distillation process can be found here. This model is uncased: it does not make a difference between english and English. DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. ...', '["transformers","pytorch","tf","jax","rust","safetensors","distilbert","fill-mask","exbert","en","dataset:bookcorpus","dataset:wikipedia","arxiv:1910.01108","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'fill-mask', 802, 11587808, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/distilbert/distilbert-base-uncased","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlanguage: en\ntags:\n- exbert\nlicense: apache-2.0\ndatasets:\n- bookcorpus\n- wikipedia\n---\n\n# DistilBERT base model (uncased)\n\nThis model is a distilled version of the [BERT base model](https://huggingface.co/bert-base-uncased). It was\nintroduced in [this paper](https://arxiv.org/abs/1910.01108). The code for the distillation process can be found\n[here](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation). This model is uncased: it does\nnot make a difference between english and English.\n\n## Model description\n\nDistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a\nself-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only,\nwith no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic\nprocess to generate inputs and labels from those texts using the BERT base model. More precisely, it was pretrained\nwith three objectives:\n\n- Distillation loss: the model was trained to return the same probabilities as the BERT base model.\n- Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a\n  sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the\n  model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that\n  usually see the words one after the other, or from autoregressive models like GPT which internally mask the future\n  tokens. It allows the model to learn a bidirectional representation of the sentence.\n- Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base\n  model.\n\nThis way, the model learns the same inner representation of the English language than its teacher model, while being\nfaster for inference or downstream tasks.\n\n## Intended uses & limitations\n\nYou can use the raw model for either masked language modeling or next sentence prediction, but it''s mostly intended to\nbe fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?filter=distilbert) to look for\nfine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked)\nto make decisions, such as sequence classification, token classification or question answering. For tasks such as text\ngeneration you should look at model like GPT2.\n\n### How to use\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline(''fill-mask'', model=''distilbert-base-uncased'')\n>>> unmasker("Hello I''m a [MASK] model.")\n\n[{''sequence'': "[CLS] hello i''m a role model. [SEP]",\n  ''score'': 0.05292855575680733,\n  ''token'': 2535,\n  ''token_str'': ''role''},\n {''sequence'': "[CLS] hello i''m a fashion model. [SEP]",\n  ''score'': 0.03968575969338417,\n  ''token'': 4827,\n  ''token_str'': ''fashion''},\n {''sequence'': "[CLS] hello i''m a business model. [SEP]",\n  ''score'': 0.034743521362543106,\n  ''token'': 2449,\n  ''token_str'': ''business''},\n {''sequence'': "[CLS] hello i''m a model model. [SEP]",\n  ''score'': 0.03462274372577667,\n  ''token'': 2944,\n  ''token_str'': ''model''},\n {''sequence'': "[CLS] hello i''m a modeling model. [SEP]",\n  ''score'': 0.018145186826586723,\n  ''token'': 11643,\n  ''token_str'': ''modeling''}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import DistilBertTokenizer, DistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained(''distilbert-base-uncased'')\nmodel = DistilBertModel.from_pretrained("distilbert-base-uncased")\ntext = "Replace me by any text you''d like."\nencoded_input = tokenizer(text, return_tensors=''pt'')\noutput = model(**encoded_input)\n```\n\nand in TensorFlow:\n\n```python\nfrom transformers import DistilBertTokenizer, TFDistilBertModel\ntokenizer = DistilBertTokenizer.from_pretrained(''distilbert-base-uncased'')\nmodel = TFDistilBertModel.from_pretrained("distilbert-base-uncased")\ntext = "Replace me by any text you''d like."\nencoded_input = tokenizer(text, return_tensors=''tf'')\noutput = model(encoded_input)\n```\n\n### Limitations and bias\n\nEven if the training data used for this model could be characterized as fairly neutral, this model can have biased\npredictions. It also inherits some of\n[the bias of its teacher model](https://huggingface.co/bert-base-uncased#limitations-and-bias).\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline(''fill-mask'', model=''distilbert-base-uncased'')\n>>> unmasker("The White man worked as a [MASK].")\n\n[{''sequence'': ''[CLS] the white man worked as a blacksmith. [SEP]'',\n  ''score'': 0.1235365942120552,\n  ''token'': 20987,\n  ''token_str'': ''blacksmith''},\n {''sequence'': ''[CLS] the white man worked as a carpenter. [SEP]'',\n  ''score'': 0.10142576694488525,\n  ''token'': 10533,\n  ''token_str'': ''carpenter''},\n {''sequence'': ''[CLS] the white man worked as a farmer. [SEP]'',\n  ''score'': 0.04985016956925392,\n  ''token'': 7500,\n  ''token_str'': ''farmer''},\n {''sequence'': ''[CLS] the white man worked as a miner. [SEP]'',\n  ''score'': 0.03932540491223335,\n  ''token'': 18594,\n  ''token_str'': ''miner''},\n {''sequence'': ''[CLS] the white man worked as a butcher. [SEP]'',\n  ''score'': 0.03351764753460884,\n  ''token'': 14998,\n  ''token_str'': ''butcher''}]\n\n>>> unmasker("The Black woman worked as a [MASK].")\n\n[{''sequence'': ''[CLS] the black woman worked as a waitress. [SEP]'',\n  ''score'': 0.13283951580524445,\n  ''token'': 13877,\n  ''token_str'': ''waitress''},\n {''sequence'': ''[CLS] the black woman worked as a nurse. [SEP]'',\n  ''score'': 0.12586183845996857,\n  ''token'': 6821,\n  ''token_str'': ''nurse''},\n {''sequence'': ''[CLS] the black woman worked as a maid. [SEP]'',\n  ''score'': 0.11708822101354599,\n  ''token'': 10850,\n  ''token_str'': ''maid''},\n {''sequence'': ''[CLS] the black woman worked as a prostitute. [SEP]'',\n  ''score'': 0.11499975621700287,\n  ''token'': 19215,\n  ''token_str'': ''prostitute''},\n {''sequence'': ''[CLS] the black woman worked as a housekeeper. [SEP]'',\n  ''score'': 0.04722772538661957,\n  ''token'': 22583,\n  ''token_str'': ''housekeeper''}]\n```\n\nThis bias will also affect all fine-tuned versions of this model.\n\n## Training data\n\nDistilBERT pretrained on the same data as BERT, which is [BookCorpus](https://yknzhu.wixsite.com/mbweb), a dataset\nconsisting of 11,038 unpublished books and [English Wikipedia](https://en.wikipedia.org/wiki/English_Wikipedia)\n(excluding lists, tables and headers).\n\n## Training procedure\n\n### Preprocessing\n\nThe texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are\nthen of the form:\n\n```\n[CLS] Sentence A [SEP] Sentence B [SEP]\n```\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in\nthe other cases, it''s another random sentence in the corpus. Note that what is considered a sentence here is a\nconsecutive span of text usually longer than a single sentence. The only constrain is that the result with the two\n"sentences" has a combined length of less than 512 tokens.\n\nThe details of the masking procedure for each sentence are the following:\n- 15% of the tokens are masked.\n- In 80% of the cases, the masked tokens are replaced by `[MASK]`.\n- In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\n- In the 10% remaining cases, the masked tokens are left as is.\n\n### Pretraining\n\nThe model was trained on 8 16 GB V100 for 90 hours. See the\n[training code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation) for all hyperparameters\ndetails.\n\n## Evaluation results\n\nWhen fine-tuned on downstream tasks, this model achieves the following results:\n\nGlue test results:\n\n| Task | MNLI | QQP  | QNLI | SST-2 | CoLA | STS-B | MRPC | RTE  |\n|:----:|:----:|:----:|:----:|:-----:|:----:|:-----:|:----:|:----:|\n|      | 82.2 | 88.5 | 89.2 | 91.3  | 51.3 | 85.8  | 87.5 | 59.9 |\n\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{Sanh2019DistilBERTAD,\n  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},\n  author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},\n  journal={ArXiv},\n  year={2019},\n  volume={abs/1910.01108}\n}\n```\n\n<a href="https://huggingface.co/exbert/?model=distilbert-base-uncased">\n	<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">\n</a>\n', '{"pipeline_tag":"fill-mask","library_name":"transformers","framework":"transformers","params":66985530,"storage_bytes":2287292255,"files_count":12,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["DistilBertForMaskedLM"],"model_type":"distilbert","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"has_code","target_id":"github:huggingface:transformers","source_url":"https://github.com/huggingface/transformers"},{"type":"based_on_paper","target_id":"arxiv:1910.01108","source_url":"https://arxiv.org/abs/1910.01108"}]', NULL, 'Apache-2.0', 'approved', 64, '07a505ac34ce52eecb3c457d6c37219c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Comfy-Org-Wan-2.1-ComfyUI-repackaged', 'huggingface--comfy-org--wan-2.1-comfyui-repackaged', 'Wan_2.1_ComfyUI_repackaged', 'Comfy-Org', '--- tags: - diffusion-single-file - comfyui --- Wan 2.1 repackaged for ComfyUI use. For examples see: https://comfyanonymous.github.io/ComfyUI_examples/wan', '["diffusion-single-file","comfyui","region:us"]', 'other', 801, 4518103, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- diffusion-single-file\n- comfyui\n---\n\nWan 2.1 repackaged for ComfyUI use. For examples see: https://comfyanonymous.github.io/ComfyUI_examples/wan', '{"pipeline_tag":null,"library_name":"diffusion-single-file","framework":"diffusion-single-file","params":null,"storage_bytes":533509403105,"files_count":36,"spaces_count":4,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, NULL, 'pending', 29, 'fd3d292b268028ec801ae70671a291ce', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-Coder-30B-A3B-Instruct', 'huggingface--qwen--qwen3-coder-30b-a3b-instruct', 'Qwen3-Coder-30B-A3B-Instruct', 'Qwen', '--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct/blob/main/LICENSE pipeline_tag: text-generation --- <a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> **Qwen3-Coder** is available in multiple sizes. Today, we''re excited to introduce **Qwen3-Code...', '["transformers","safetensors","qwen3_moe","text-generation","conversational","arxiv:2505.09388","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 799, 1166551, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct/blob/main/LICENSE\npipeline_tag: text-generation\n---\n\n# Qwen3-Coder-30B-A3B-Instruct\n<a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Highlights\n\n**Qwen3-Coder** is available in multiple sizes. Today, we''re excited to introduce **Qwen3-Coder-30B-A3B-Instruct**. This streamlined model maintains impressive performance and efficiency, featuring the following key enhancements:  \n\n- **Significant Performance** among open models on **Agentic Coding**, **Agentic Browser-Use**, and other foundational coding tasks.\n- **Long-context Capabilities** with native support for **256K** tokens, extendable up to **1M** tokens using Yarn, optimized for repository-scale understanding.\n- **Agentic Coding** supporting for most platform such as **Qwen Code**, **CLINE**, featuring a specially designed function call format.\n\n![image/jpeg](https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Coder/qwen3-coder-30a3-main.jpg)\n\n## Model Overview\n\n**Qwen3-Coder-30B-A3B-Instruct** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 30.5B in total and 3.3B activated\n- Number of Layers: 48\n- Number of Attention Heads (GQA): 32 for Q and 4 for KV\n- Number of Experts: 128\n- Number of Activated Experts: 8\n- Context Length: **262,144 natively**. \n\n**NOTE: This model supports only non-thinking mode and does not generate ``<think></think>`` blocks in its output. Meanwhile, specifying `enable_thinking=False` is no longer required.**\n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3-coder/), [GitHub](https://github.com/QwenLM/Qwen3-Coder), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n\n## Quickstart\n\nWe advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: ''qwen3_moe''\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen3-Coder-30B-A3B-Instruct"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\n\n# prepare the model input\nprompt = "Write a quick sort algorithm."\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=65536\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\ncontent = tokenizer.decode(output_ids, skip_special_tokens=True)\n\nprint("content:", content)\n```\n\n**Note: If you encounter out-of-memory (OOM) issues, consider reducing the context length to a shorter value, such as `32,768`.**\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Agentic Coding\n\nQwen3-Coder excels in tool calling capabilities. \n\nYou can simply define or use any tools as following example.\n```python\n# Your tool implementation\ndef square_the_number(num: float) -> dict:\n    return num ** 2\n\n# Define Tools\ntools=[\n    {\n        "type":"function",\n        "function":{\n            "name": "square_the_number",\n            "description": "output the square of the number.",\n            "parameters": {\n                "type": "object",\n                "required": ["input_num"],\n                "properties": {\n                    ''input_num'': {\n                        ''type'': ''number'', \n                        ''description'': ''input_num is a number that will be squared''\n                        }\n                },\n            }\n        }\n    }\n]\n\nimport OpenAI\n# Define LLM\nclient = OpenAI(\n    # Use a custom endpoint compatible with OpenAI API\n    base_url=''http://localhost:8000/v1'',  # api_base\n    api_key="EMPTY"\n)\n \nmessages = [{''role'': ''user'', ''content'': ''square the number 1024''}]\n\ncompletion = client.chat.completions.create(\n    messages=messages,\n    model="Qwen3-Coder-30B-A3B-Instruct",\n    max_tokens=65536,\n    tools=tools,\n)\n\nprint(completion.choice[0])\n```\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - We suggest using `temperature=0.7`, `top_p=0.8`, `top_k=20`, `repetition_penalty=1.05`.\n\n2. **Adequate Output Length**: We recommend using an output length of 65,536 tokens for most queries, which is adequate for instruct models.\n\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":30532122624,"storage_bytes":61066575656,"files_count":28,"spaces_count":39,"gated":false,"private":false,"config":{"architectures":["Qwen3MoeForCausalLM"],"model_type":"qwen3_moe","tokenizer_config":{"bos_token":null,"chat_template":"{% macro render_extra_keys(json_dict, handled_keys) %}\n    {%- if json_dict is mapping %}\n        {%- for json_key in json_dict if json_key not in handled_keys %}\n            {%- if json_dict[json_key] is mapping or (json_dict[json_key] is sequence and json_dict[json_key] is not string) %}\n                {{- ''\\n<'' ~ json_key ~ ''>'' ~ (json_dict[json_key] | tojson | safe) ~ ''</'' ~ json_key ~ ''>'' }}\n            {%- else %}\n                {{-''\\n<'' ~ json_key ~ ''>'' ~ (json_dict[json_key] | string) ~ ''</'' ~ json_key ~ ''>'' }}\n            {%- endif %}\n        {%- endfor %}\n    {%- endif %}\n{% endmacro %}\n\n{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{%- if not tools is defined %}\n    {%- set tools = [] %}\n{%- endif %}\n\n{%- if system_message is defined %}\n    {{- \"<|im_start|>system\\n\" + system_message }}\n{%- else %}\n    {%- if tools is iterable and tools | length > 0 %}\n        {{- \"<|im_start|>system\\nYou are Qwen, a helpful AI assistant that can interact with a computer to solve tasks.\" }}\n    {%- endif %}\n{%- endif %}\n{%- if tools is iterable and tools | length > 0 %}\n    {{- \"\\n\\n# Tools\\n\\nYou have access to the following functions:\\n\\n\" }}\n    {{- \"<tools>\" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- \"\\n<function>\\n<name>\" ~ tool.name ~ \"</name>\" }}\n        {%- if tool.description is defined %}\n            {{- ''\\n<description>'' ~ (tool.description | trim) ~ ''</description>'' }}\n        {%- endif %}\n        {{- ''\\n<parameters>'' }}\n        {%- if tool.parameters is defined and tool.parameters is mapping and tool.parameters.properties is defined and tool.parameters.properties is mapping %}\n            {%- for param_name, param_fields in tool.parameters.properties|items %}\n                {{- ''\\n<parameter>'' }}\n                {{- ''\\n<name>'' ~ param_name ~ ''</name>'' }}\n                {%- if param_fields.type is defined %}\n                    {{- ''\\n<type>'' ~ (param_fields.type | string) ~ ''</type>'' }}\n                {%- endif %}\n                {%- if param_fields.description is defined %}\n                    {{- ''\\n<description>'' ~ (param_fields.description | trim) ~ ''</description>'' }}\n                {%- endif %}\n                {%- set handled_keys = [''name'', ''type'', ''description''] %}\n                {{- render_extra_keys(param_fields, handled_keys) }}\n                {{- ''\\n</parameter>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {% set handled_keys = [''type'', ''properties''] %}\n        {{- render_extra_keys(tool.parameters, handled_keys) }}\n        {{- ''\\n</parameters>'' }}\n        {%- set handled_keys = [''type'', ''name'', ''description'', ''parameters''] %}\n        {{- render_extra_keys(tool, handled_keys) }}\n        {{- ''\\n</function>'' }}\n    {%- endfor %}\n    {{- \"\\n</tools>\" }}\n    {{- ''\\n\\nIf you choose to call a function ONLY reply in the following format with NO suffix:\\n\\n<tool_call>\\n<function=example_function_name>\\n<parameter=example_parameter_1>\\nvalue_1\\n</parameter>\\n<parameter=example_parameter_2>\\nThis is the value for the second parameter\\nthat can span\\nmultiple lines\\n</parameter>\\n</function>\\n</tool_call>\\n\\n<IMPORTANT>\\nReminder:\\n- Function calls MUST follow the specified format: an inner <function=...></function> block must be nested within <tool_call></tool_call> XML tags\\n- Required parameters MUST be specified\\n- You may provide optional reasoning for your function call in natural language BEFORE the function call, but NOT after\\n- If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls\\n</IMPORTANT>'' }}\n{%- endif %}\n{%- if system_message is defined %}\n    {{- ''<|im_end|>\\n'' }}\n{%- else %}\n    {%- if tools is iterable and tools | length > 0 %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if message.role == \"assistant\" and message.tool_calls is defined and message.tool_calls is iterable and message.tool_calls | length > 0 %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content is defined and message.content is string and message.content | trim | length > 0 %}\n            {{- ''\\n'' + message.content | trim + ''\\n'' }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n<function='' + tool_call.name + ''>\\n'' }}\n            {%- if tool_call.arguments is defined %}\n                {%- for args_name, args_value in tool_call.arguments|items %}\n                    {{- ''<parameter='' + args_name + ''>\\n'' }}\n                    {%- set args_value = args_value | tojson | safe if args_value is mapping or (args_value is sequence and args_value is not string) else args_value | string %}\n                    {{- args_value }}\n                    {{- ''\\n</parameter>\\n'' }}\n                {%- endfor %}\n            {%- endif %}\n            {{- ''</function>\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"user\" or message.role == \"system\" or message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.previtem and loop.previtem.role != \"tool\" %}\n            {{- ''<|im_start|>user\\n'' }}\n        {%- endif %}\n        {{- ''<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>\\n'' }}\n        {%- if not loop.last and loop.nextitem.role != \"tool\" %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- elif loop.last %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- else %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null},"chat_template_jinja":"{% macro render_extra_keys(json_dict, handled_keys) %}\n    {%- if json_dict is mapping %}\n        {%- for json_key in json_dict if json_key not in handled_keys %}\n            {%- if json_dict[json_key] is mapping or (json_dict[json_key] is sequence and json_dict[json_key] is not string) %}\n                {{- ''\\n<'' ~ json_key ~ ''>'' ~ (json_dict[json_key] | tojson | safe) ~ ''</'' ~ json_key ~ ''>'' }}\n            {%- else %}\n                {{-''\\n<'' ~ json_key ~ ''>'' ~ (json_dict[json_key] | string) ~ ''</'' ~ json_key ~ ''>'' }}\n            {%- endif %}\n        {%- endfor %}\n    {%- endif %}\n{% endmacro %}\n\n{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n\n{%- if not tools is defined %}\n    {%- set tools = [] %}\n{%- endif %}\n\n{%- if system_message is defined %}\n    {{- \"<|im_start|>system\\n\" + system_message }}\n{%- else %}\n    {%- if tools is iterable and tools | length > 0 %}\n        {{- \"<|im_start|>system\\nYou are Qwen, a helpful AI assistant that can interact with a computer to solve tasks.\" }}\n    {%- endif %}\n{%- endif %}\n{%- if tools is iterable and tools | length > 0 %}\n    {{- \"\\n\\n# Tools\\n\\nYou have access to the following functions:\\n\\n\" }}\n    {{- \"<tools>\" }}\n    {%- for tool in tools %}\n        {%- if tool.function is defined %}\n            {%- set tool = tool.function %}\n        {%- endif %}\n        {{- \"\\n<function>\\n<name>\" ~ tool.name ~ \"</name>\" }}\n        {%- if tool.description is defined %}\n            {{- ''\\n<description>'' ~ (tool.description | trim) ~ ''</description>'' }}\n        {%- endif %}\n        {{- ''\\n<parameters>'' }}\n        {%- if tool.parameters is defined and tool.parameters is mapping and tool.parameters.properties is defined and tool.parameters.properties is mapping %}\n            {%- for param_name, param_fields in tool.parameters.properties|items %}\n                {{- ''\\n<parameter>'' }}\n                {{- ''\\n<name>'' ~ param_name ~ ''</name>'' }}\n                {%- if param_fields.type is defined %}\n                    {{- ''\\n<type>'' ~ (param_fields.type | string) ~ ''</type>'' }}\n                {%- endif %}\n                {%- if param_fields.description is defined %}\n                    {{- ''\\n<description>'' ~ (param_fields.description | trim) ~ ''</description>'' }}\n                {%- endif %}\n                {%- set handled_keys = [''name'', ''type'', ''description''] %}\n                {{- render_extra_keys(param_fields, handled_keys) }}\n                {{- ''\\n</parameter>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {% set handled_keys = [''type'', ''properties''] %}\n        {{- render_extra_keys(tool.parameters, handled_keys) }}\n        {{- ''\\n</parameters>'' }}\n        {%- set handled_keys = [''type'', ''name'', ''description'', ''parameters''] %}\n        {{- render_extra_keys(tool, handled_keys) }}\n        {{- ''\\n</function>'' }}\n    {%- endfor %}\n    {{- \"\\n</tools>\" }}\n    {{- ''\\n\\nIf you choose to call a function ONLY reply in the following format with NO suffix:\\n\\n<tool_call>\\n<function=example_function_name>\\n<parameter=example_parameter_1>\\nvalue_1\\n</parameter>\\n<parameter=example_parameter_2>\\nThis is the value for the second parameter\\nthat can span\\nmultiple lines\\n</parameter>\\n</function>\\n</tool_call>\\n\\n<IMPORTANT>\\nReminder:\\n- Function calls MUST follow the specified format: an inner <function=...></function> block must be nested within <tool_call></tool_call> XML tags\\n- Required parameters MUST be specified\\n- You may provide optional reasoning for your function call in natural language BEFORE the function call, but NOT after\\n- If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls\\n</IMPORTANT>'' }}\n{%- endif %}\n{%- if system_message is defined %}\n    {{- ''<|im_end|>\\n'' }}\n{%- else %}\n    {%- if tools is iterable and tools | length > 0 %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in loop_messages %}\n    {%- if message.role == \"assistant\" and message.tool_calls is defined and message.tool_calls is iterable and message.tool_calls | length > 0 %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content is defined and message.content is string and message.content | trim | length > 0 %}\n            {{- ''\\n'' + message.content | trim + ''\\n'' }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n<function='' + tool_call.name + ''>\\n'' }}\n            {%- if tool_call.arguments is defined %}\n                {%- for args_name, args_value in tool_call.arguments|items %}\n                    {{- ''<parameter='' + args_name + ''>\\n'' }}\n                    {%- set args_value = args_value | tojson | safe if args_value is mapping or (args_value is sequence and args_value is not string) else args_value | string %}\n                    {{- args_value }}\n                    {{- ''\\n</parameter>\\n'' }}\n                {%- endfor %}\n            {%- endif %}\n            {{- ''</function>\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"user\" or message.role == \"system\" or message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.previtem and loop.previtem.role != \"tool\" %}\n            {{- ''<|im_start|>user\\n'' }}\n        {%- endif %}\n        {{- ''<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>\\n'' }}\n        {%- if not loop.last and loop.nextitem.role != \"tool\" %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- elif loop.last %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- else %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n"}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen3-Coder","source_url":"https://github.com/QwenLM/Qwen3-Coder"},{"type":"based_on_paper","target_id":"arxiv:2505.09388","source_url":"https://arxiv.org/abs/2505.09388"}]', NULL, 'Apache-2.0', 'approved', 64, '9c9f1464c4e80b0ce8a438e3f2cec500', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-V3.2', 'huggingface--deepseek-ai--deepseek-v3.2', 'DeepSeek-V3.2', 'deepseek-ai', '--- license: mit library_name: transformers base_model: - deepseek-ai/DeepSeek-V3.2-Exp-Base base_model_relation: finetune --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" targ...', '["transformers","safetensors","deepseek_v32","text-generation","conversational","base_model:deepseek-ai/deepseek-v3.2-exp-base","license:mit","endpoints_compatible","fp8","region:us"]', 'text-generation', 798, 28778, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-V3.2","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\nbase_model:\n  - deepseek-ai/DeepSeek-V3.2-Exp-Base\nbase_model_relation: finetune\n---\n# DeepSeek-V3.2: Efficient Reasoning & Agentic AI\n\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20V3-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n<div align="center" style="line-height: 1;">\n  <a href="LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<p align="center">\n  <a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/blob/main/assets/paper.pdf"><b>Technical Report</b>üëÅÔ∏è</a>\n</p>\n\n## Introduction\n\nWe introduce **DeepSeek-V3.2**, a model that harmonizes high computational efficiency with superior reasoning and agent performance. Our approach is built upon three key technical breakthroughs:\n\n1. **DeepSeek Sparse Attention (DSA):** We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance, specifically optimized for long-context scenarios.\n2. **Scalable Reinforcement Learning Framework:** By implementing a robust RL protocol and scaling post-training compute, *DeepSeek-V3.2* performs comparably to GPT-5. Notably, our high-compute variant, **DeepSeek-V3.2-Speciale**, **surpasses GPT-5** and exhibits reasoning proficiency on par with Gemini-3.0-Pro.\n    - *Achievement:* ü•á **Gold-medal performance** in the 2025 International Mathematical Olympiad (IMO) and International Olympiad in Informatics (IOI).\n3. **Large-Scale Agentic Task Synthesis Pipeline:** To integrate **reasoning into tool-use** scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This facilitates scalable agentic post-training, improving compliance and generalization in complex interactive environments.\n\n<div align="center">\n <img src="assets/benchmark.png" >\n</div>\n\nWe have also released the final submissions for IOI 2025, ICPC World Finals, IMO 2025 and CMO 2025, which were selected based on our designed pipeline. These materials are provided for the community to conduct secondary verification. The files can be accessed at `assets/olympiad_cases`.\n\n## Chat Template\n\nDeepSeek-V3.2 introduces significant updates to its chat template compared to prior versions. The primary changes involve a revised format for tool calling and the introduction of a "thinking with tools" capability.\n\nTo assist the community in understanding and adapting to this new template, we have provided a dedicated `encoding` folder, which contains Python scripts and test cases demonstrating how to encode messages in OpenAI-compatible format into input strings for the model and how to parse the model''s text output.\n\nA brief example is illustrated below:\n\n```python\nimport transformers\n# encoding/encoding_dsv32.py\nfrom encoding_dsv32 import encode_messages, parse_message_from_completion_text\n\ntokenizer = transformers.AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-V3.2")\n\nmessages = [\n    {"role": "user", "content": "hello"},\n    {"role": "assistant", "content": "Hello! I am DeepSeek.", "reasoning_content": "thinking..."},\n    {"role": "user", "content": "1+1=?"}\n]\nencode_config = dict(thinking_mode="thinking", drop_thinking=True, add_default_bos_token=True)\n\n# messages -> string\nprompt = encode_messages(messages, **encode_config)\n# Output: "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>hello<ÔΩúAssistantÔΩú></think>Hello! I am DeepSeek.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú><ÔΩúUserÔΩú>1+1=?<ÔΩúAssistantÔΩú><think>"\n\n# string -> tokens\ntokens = tokenizer.encode(prompt)\n# Output: [0, 128803, 33310, 128804, 128799, 19923, 3, 342, 1030, 22651, 4374, 1465, 16, 1, 128803, 19, 13, 19, 127252, 128804, 128798]\n```\n\nImportant Notes:\n\n1. This release does not include a Jinja-format chat template. Please refer to the Python code mentioned above.\n2. The output parsing function included in the code is designed to handle well-formatted strings only. It does not attempt to correct or recover from malformed output that the model might occasionally generate. It is not suitable for production use without robust error handling.\n3. A new role named `developer` has been introduced in the chat template. This role is dedicated exclusively to search agent scenarios and is designated for no other tasks. The official API does not accept messages assigned to `developer`.\n\n## How to Run Locally\n\nThe model structure of DeepSeek-V3.2 and DeepSeek-V3.2-Speciale are the same as DeepSeek-V3.2-Exp. Please visit [DeepSeek-V3.2-Exp](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp) repo for more information about running this model locally.\n\nUsage Recommendations:\n\n1. For local deployment, we recommend setting the sampling parameters to `temperature = 1.0, top_p = 0.95`.\n2. Please note that the DeepSeek-V3.2-Speciale variant is designed exclusively for deep reasoning tasks and does not support the tool-calling functionality.\n\n## License\n\nThis repository and the model weights are licensed under the [MIT License](LICENSE).\n\n## Citation\n\n```\n@misc{deepseekai2025deepseekv32,\n      title={DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models}, \n      author={DeepSeek-AI},\n      year={2025},\n}\n```\n\n## Contact\n\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":685396921376,"storage_bytes":689484423011,"files_count":192,"spaces_count":19,"gated":false,"private":false,"config":{"architectures":["DeepseekV32ForCausalLM"],"model_type":"deepseek_v32","quantization_config":{"quant_method":"fp8"},"tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3.2-Exp","source_url":"https://github.com/deepseek-ai/DeepSeek-V3.2-Exp"}]', NULL, 'MIT', 'approved', 84, '2d91c9c620c9755ecf0a7f560674c78f', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-V3.2/resolve/main/assets/benchmark.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-V3.2 from https://huggingface.co/deepseek-ai/DeepSeek-V3.2/resolve/main/assets/benchmark.png
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-V3.2.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-neuphonic-neutts-air', 'huggingface--neuphonic--neutts-air', 'neutts-air', 'neuphonic', '--- license: apache-2.0 pipeline_tag: text-to-speech tags: - audio - speech - speech-language-models datasets: - amphion/Emilia-Dataset - neuphonic/emilia-yodas-english-neucodec --- üöÄ Spaces Demo, üîß Github Q8 GGUF version, Q4 GGUF version *Created by Neuphonic - building faster, smaller, on-device voice AI* State-of-the-art Voice AI has been locked behind web APIs for too long. NeuTTS Air is the world‚Äôs first super-realistic, on-device, TTS speech language model with instant voice cloning. ...', '["safetensors","gguf","qwen2","audio","speech","speech-language-models","text-to-speech","dataset:amphion/emilia-dataset","dataset:neuphonic/emilia-yodas-english-neucodec","license:apache-2.0","endpoints_compatible","region:us","conversational"]', 'text-to-speech', 795, 23147, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/neuphonic/neutts-air","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\npipeline_tag: text-to-speech\ntags:\n- audio\n- speech\n- speech-language-models\ndatasets:\n- amphion/Emilia-Dataset\n- neuphonic/emilia-yodas-english-neucodec\n---\n\n# NeuTTS Air ‚òÅÔ∏è \n\n[![NeuTTSAir_Intro](neutts-air.png)](https://www.youtube.com/watch?v=YAB3hCtu5wE)\n\n[üöÄ Spaces Demo](https://huggingface.co/spaces/neuphonic/neutts-air), [üîß Github](https://github.com/neuphonic/neutts-air)\n\n[Q8 GGUF version](https://huggingface.co/neuphonic/neutts-air-q8-gguf), [Q4 GGUF version](https://huggingface.co/neuphonic/neutts-air-q4-gguf)\n\n*Created by [Neuphonic](http://neuphonic.com/) - building faster, smaller, on-device voice AI*\n\nState-of-the-art Voice AI has been locked behind web APIs for too long. NeuTTS Air is the world‚Äôs first super-realistic, on-device, TTS speech language model with instant voice cloning. Built off a 0.5B LLM backbone, NeuTTS Air brings natural-sounding speech, real-time performance, built-in security and speaker cloning to your local device - unlocking a new category of embedded voice agents, assistants, toys, and compliance-safe apps.\n\n## Key Features\n\n- üó£Best-in-class realism for its size - produces natural, ultra-realistic voices that sound human\n- üì±Optimised for on-device deployment - provided in GGML format, ready to run on phones, laptops, or even Raspberry Pis\n- üë´Instant voice cloning - create your own speaker with as little as 3 seconds of audio\n- üöÑSimple LM + codec architecture built off a 0.5B backbone - the sweet spot between speed, size, and quality for real-world applications\n\n\n> [!CAUTION]\n> Websites like neutts.com are popping up and they''re not affliated with Neuphonic, our github or this repo.\n>\n> We are on neuphonic.com only. Please be careful out there! üôè\n\n\n## Model Details\n\nNeuTTS Air is built off Qwen 0.5B - a lightweight yet capable language model optimised for text understanding and generation - as well as a powerful combination of technologies designed for efficiency and quality:\n\n- **Audio Codec**: [NeuCodec](https://huggingface.co/neuphonic/neucodec) - our proprietary neural audio codec that achieves exceptional audio quality at low bitrates using a single codebook\n- **Format**: Available in GGML format for efficient on-device inference\n- **Responsibility**: Watermarked outputs\n- **Inference Speed**: Real-time generation on mid-range devices\n- **Power Consumption**: Optimised for mobile and embedded devices\n\n## Get Started\n\n1. **Clone the [Git Repo](https://github.com/neuphonic/neutts-air)**\n    \n    ```bash\n    git clone https://github.com/neuphonic/neutts-air.git\n    cd neuttsair\n    ```\n    \n2. **Install¬†`espeak`¬†(required dependency)**\n    \n    Please refer to the following link for instructions on how to install¬†`espeak`:\n    \n    https://github.com/espeak-ng/espeak-ng/blob/master/docs/guide.md\n    \n    ```bash\n    # Mac OS\n    brew install espeak\n    \n    # Ubuntu/Debian\n    sudo apt install espeak\n\n    # Arch Linux\n    paru -S aur/espeak\n    ```\n    \n3. **Install Python dependencies**\n    \n    The requirements file includes the dependencies needed to run the model with PyTorch. When using an ONNX decoder or a GGML model, some dependencies (such as PyTorch) are no longer required.\n    \n    The inference is compatible and tested on¬†`python>=3.11`.\n    \n    ```\n    pip install -r requirements.txt\n    \n    ```\n    \n\n## **Basic Example**\n\nRun the basic example script to synthesize speech:\n\n```bash\npython -m examples.basic_example \\n  --input_text "My name is Dave, and um, I''m from London" \\n  --ref_audio samples/dave.wav \\n  --ref_text samples/dave.txt\n\n```\n\nTo specify a particular model repo for the backbone or codec, add the¬†`--backbone`¬†argument. Available backbones are listed in¬†[NeuTTS-Air huggingface collection](https://huggingface.co/collections/neuphonic/neutts-air-68cc14b7033b4c56197ef350).\n\nSeveral examples are available, including a Jupyter notebook in the¬†`examples`¬†folder.\n\n### **Simple One-Code Block Usage**\n\n```python\nfrom neuttsair.neutts import NeuTTSAir\nimport soundfile as sf\n\ntts = NeuTTSAir( backbone_repo="neuphonic/neutts-air-q4-gguf", backbone_device="cpu", codec_repo="neuphonic/neucodec", codec_device="cpu")\ninput_text = "My name is Dave, and um, I''m from London."\n\nref_text = "samples/dave.txt"\nref_audio_path = "samples/dave.wav"\n\nref_text = open(ref_text, "r").read().strip()\nref_codes = tts.encode_reference(ref_audio_path)\n\nwav = tts.infer(input_text, ref_codes, ref_text)\nsf.write("test.wav", wav, 24000)\n\n```\n\n# Tips\n\nNeuTTS Air requires two inputs:\n\n1. A reference audio sample (`.wav` file)\n2. A text string\n\nThe model then synthesises the text as speech in the style of the reference audio. This is what enables NeuTTS Air‚Äôs instant voice cloning capability.\n\n### Example Reference Files\n\nYou can find some ready-to-use samples in the `examples` folder:\n\n- `samples/dave.wav`\n- `samples/jo.wav`\n\n### Guidelines for Best Results\n\nFor optimal performance, reference audio samples should be:\n\n1. **Mono channel**\n2. **16-44 kHz sample rate**\n3. **3‚Äì15 seconds in length**\n4. **Saved as a `.wav` file**\n5. **Clean** ‚Äî minimal to no background noise\n6. **Natural, continuous speech** ‚Äî like a monologue or conversation, with few pauses, so the model can capture tone effectively\n\n# **Responsibility**\n\nEvery audio file generated by NeuTTS Air includes [**Perth (Perceptual Threshold) Watermarker](https://github.com/resemble-ai/perth).**\n\n# **Disclaimer**\n\nDon''t use this model to do bad things‚Ä¶ please.', '{"pipeline_tag":"text-to-speech","library_name":null,"framework":null,"params":null,"storage_bytes":10621210568,"files_count":11,"spaces_count":6,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2"}}', '[]', '[{"type":"has_code","target_id":"github:neuphonic:neutts-air","source_url":"https://github.com/neuphonic/neutts-air"},{"type":"has_code","target_id":"github:neuphonic:neutts-air","source_url":"https://github.com/neuphonic/neutts-air"},{"type":"has_code","target_id":"github:neuphonic:neutts-air.git","source_url":"https://github.com/neuphonic/neutts-air.git"},{"type":"has_code","target_id":"github:espeak-ng:espeak-ng","source_url":"https://github.com/espeak-ng/espeak-ng"},{"type":"has_code","target_id":"github:resemble-ai:perth","source_url":"https://github.com/resemble-ai/perth"}]', NULL, 'Apache-2.0', 'approved', 64, '7e5341b3484182e6c9cadf4535088ded', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-8B', 'huggingface--qwen--qwen3-8b', 'Qwen3-8B', 'Qwen', '--- library_name: transformers license: apache-2.0 license_link: https://huggingface.co/Qwen/Qwen3-8B/blob/main/LICENSE pipeline_tag: text-generation base_model: - Qwen/Qwen3-8B-Base --- <a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/> </a> Qwen3 is the latest generation of large language models in Qwen series, offering ...', '["transformers","safetensors","qwen3","text-generation","conversational","arxiv:2309.00071","arxiv:2505.09388","base_model:qwen/qwen3-8b-base","base_model:finetune:qwen/qwen3-8b-base","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 792, 4761786, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-8B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: transformers\nlicense: apache-2.0\nlicense_link: https://huggingface.co/Qwen/Qwen3-8B/blob/main/LICENSE\npipeline_tag: text-generation\nbase_model:\n- Qwen/Qwen3-8B-Base\n---\n\n# Qwen3-8B\n<a href="https://chat.qwen.ai/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/%F0%9F%92%9C%EF%B8%8F%20Qwen%20Chat%20-536af5" style="display: inline-block; vertical-align: middle;"/>\n</a>\n\n## Qwen3 Highlights\n\nQwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support, with the following key features:\n\n- **Uniquely support of seamless switching between thinking mode** (for complex logical reasoning, math, and coding) and **non-thinking mode** (for efficient, general-purpose dialogue) **within single model**, ensuring optimal performance across various scenarios.\n- **Significantly enhancement in its reasoning capabilities**, surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning.\n- **Superior human preference alignment**, excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience.\n- **Expertise in agent capabilities**, enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks.\n- **Support of 100+ languages and dialects** with strong capabilities for **multilingual instruction following** and **translation**.\n\n## Model Overview\n\n**Qwen3-8B** has the following features:\n- Type: Causal Language Models\n- Training Stage: Pretraining & Post-training\n- Number of Parameters: 8.2B\n- Number of Paramaters (Non-Embedding): 6.95B\n- Number of Layers: 36\n- Number of Attention Heads (GQA): 32 for Q and 8 for KV\n- Context Length: 32,768 natively and [131,072 tokens with YaRN](#processing-long-texts). \n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3/), [GitHub](https://github.com/QwenLM/Qwen3), and [Documentation](https://qwen.readthedocs.io/en/latest/).\n\n## Quickstart\n\nThe code of Qwen3 has been in the latest Hugging Face `transformers` and we advise you to use the latest version of `transformers`.\n\nWith `transformers<4.51.0`, you will encounter the following error:\n```\nKeyError: ''qwen3''\n```\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs. \n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "Qwen/Qwen3-8B"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype="auto",\n    device_map="auto"\n)\n\n# prepare the model input\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n)\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=32768\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n\n# parsing thinking content\ntry:\n    # rindex finding 151668 (</think>)\n    index = len(output_ids) - output_ids[::-1].index(151668)\nexcept ValueError:\n    index = 0\n\nthinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")\ncontent = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")\n\nprint("thinking content:", thinking_content)\nprint("content:", content)\n```\n\nFor deployment, you can use `sglang>=0.4.6.post1` or `vllm>=0.8.5` or to create an OpenAI-compatible API endpoint:\n- SGLang:\n    ```shell\n    python -m sglang.launch_server --model-path Qwen/Qwen3-8B --reasoning-parser qwen3\n    ```\n- vLLM:\n    ```shell\n    vllm serve Qwen/Qwen3-8B --enable-reasoning --reasoning-parser deepseek_r1\n    ```\n\nFor local use, applications such as Ollama, LMStudio, MLX-LM, llama.cpp, and KTransformers have also supported Qwen3.\n\n## Switching Between Thinking and Non-Thinking Mode\n\n> [!TIP]\n> The `enable_thinking` switch is also available in APIs created by SGLang and vLLM. \n> Please refer to our documentation for [SGLang](https://qwen.readthedocs.io/en/latest/deployment/sglang.html#thinking-non-thinking-modes) and [vLLM](https://qwen.readthedocs.io/en/latest/deployment/vllm.html#thinking-non-thinking-modes) users.\n\n### `enable_thinking=True`\n\nBy default, Qwen3 has thinking capabilities enabled, similar to QwQ-32B. This means the model will use its reasoning abilities to enhance the quality of generated responses. For example, when explicitly setting `enable_thinking=True` or leaving it as the default value in `tokenizer.apply_chat_template`, the model will engage its thinking mode.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=True  # True is the default value for enable_thinking\n)\n```\n\nIn this mode, the model will generate think content wrapped in a `<think>...</think>` block, followed by the final response.\n\n> [!NOTE]\n> For thinking mode, use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0` (the default setting in `generation_config.json`). **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n\n### `enable_thinking=False`\n\nWe provide a hard switch to strictly disable the model''s thinking behavior, aligning its functionality with the previous Qwen2.5-Instruct models. This mode is particularly useful in scenarios where disabling thinking is essential for enhancing efficiency.\n\n```python\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n    enable_thinking=False  # Setting enable_thinking=False disables thinking mode\n)\n```\n\nIn this mode, the model will not generate any think content and will not include a `<think>...</think>` block.\n\n> [!NOTE]\n> For non-thinking mode, we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`. For more detailed guidance, please refer to the [Best Practices](#best-practices) section.\n\n### Advanced Usage: Switching Between Thinking and Non-Thinking Modes via User Input\n\nWe provide a soft switch mechanism that allows users to dynamically control the model''s behavior when `enable_thinking=True`. Specifically, you can add `/think` and `/no_think` to user prompts or system messages to switch the model''s thinking mode from turn to turn. The model will follow the most recent instruction in multi-turn conversations.\n\nHere is an example of a multi-turn conversation:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nclass QwenChatbot:\n    def __init__(self, model_name="Qwen/Qwen3-8B"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.history = []\n\n    def generate_response(self, user_input):\n        messages = self.history + [{"role": "user", "content": user_input}]\n\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True\n        )\n\n        inputs = self.tokenizer(text, return_tensors="pt")\n        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n\n        # Update history\n        self.history.append({"role": "user", "content": user_input})\n        self.history.append({"role": "assistant", "content": response})\n\n        return response\n\n# Example Usage\nif __name__ == "__main__":\n    chatbot = QwenChatbot()\n\n    # First input (without /think or /no_think tags, thinking mode is enabled by default)\n    user_input_1 = "How many r''s in strawberries?"\n    print(f"User: {user_input_1}")\n    response_1 = chatbot.generate_response(user_input_1)\n    print(f"Bot: {response_1}")\n    print("----------------------")\n\n    # Second input with /no_think\n    user_input_2 = "Then, how many r''s in blueberries? /no_think"\n    print(f"User: {user_input_2}")\n    response_2 = chatbot.generate_response(user_input_2)\n    print(f"Bot: {response_2}") \n    print("----------------------")\n\n    # Third input with /think\n    user_input_3 = "Really? /think"\n    print(f"User: {user_input_3}")\n    response_3 = chatbot.generate_response(user_input_3)\n    print(f"Bot: {response_3}")\n```\n\n> [!NOTE]\n> For API compatibility, when `enable_thinking=True`, regardless of whether the user uses `/think` or `/no_think`, the model will always output a block wrapped in `<think>...</think>`. However, the content inside this block may be empty if thinking is disabled.\n> When `enable_thinking=False`, the soft switches are not valid. Regardless of any `/think` or `/no_think` tags input by the user, the model will not generate think content and will not include a `<think>...</think>` block.\n\n## Agentic Use\n\nQwen3 excels in tool calling capabilities. We recommend using [Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) to make the best use of agentic ability of Qwen3. Qwen-Agent encapsulates tool-calling templates and tool-calling parsers internally, greatly reducing coding complexity.\n\nTo define the available tools, you can use the MCP configuration file, use the integrated tool of Qwen-Agent, or integrate other tools by yourself.\n```python\nfrom qwen_agent.agents import Assistant\n\n# Define LLM\nllm_cfg = {\n    ''model'': ''Qwen3-8B'',\n\n    # Use the endpoint provided by Alibaba Model Studio:\n    # ''model_type'': ''qwen_dashscope'',\n    # ''api_key'': os.getenv(''DASHSCOPE_API_KEY''),\n\n    # Use a custom endpoint compatible with OpenAI API:\n    ''model_server'': ''http://localhost:8000/v1'',  # api_base\n    ''api_key'': ''EMPTY'',\n\n    # Other parameters:\n    # ''generate_cfg'': {\n    #         # Add: When the response content is `<think>this is the thought</think>this is the answer;\n    #         # Do not add: When the response has been separated by reasoning_content and content.\n    #         ''thought_in_content'': True,\n    #     },\n}\n\n# Define Tools\ntools = [\n    {''mcpServers'': {  # You can specify the MCP configuration file\n            ''time'': {\n                ''command'': ''uvx'',\n                ''args'': [''mcp-server-time'', ''--local-timezone=Asia/Shanghai'']\n            },\n            "fetch": {\n                "command": "uvx",\n                "args": ["mcp-server-fetch"]\n            }\n        }\n    },\n  ''code_interpreter'',  # Built-in tools\n]\n\n# Define Agent\nbot = Assistant(llm=llm_cfg, function_list=tools)\n\n# Streaming generation\nmessages = [{''role'': ''user'', ''content'': ''https://qwenlm.github.io/blog/ Introduce the latest developments of Qwen''}]\nfor responses in bot.run(messages=messages):\n    pass\nprint(responses)\n```\n\n## Processing Long Texts\n\nQwen3 natively supports context lengths of up to 32,768 tokens. For conversations where the total length (including both input and output) significantly exceeds this limit, we recommend using RoPE scaling techniques to handle long texts effectively. We have validated the model''s performance on context lengths of up to 131,072 tokens using the [YaRN](https://arxiv.org/abs/2309.00071) method.\n\nYaRN is currently supported by several inference frameworks, e.g., `transformers` and `llama.cpp` for local use, `vllm` and `sglang` for deployment. In general, there are two approaches to enabling YaRN for supported frameworks:\n\n- Modifying the model files:\n  In the `config.json` file, add the `rope_scaling` fields:\n    ```json\n    {\n        ...,\n        "rope_scaling": {\n            "rope_type": "yarn",\n            "factor": 4.0,\n            "original_max_position_embeddings": 32768\n        }\n    }\n    ```\n  For `llama.cpp`, you need to regenerate the GGUF file after the modification.\n\n- Passing command line arguments:\n\n  For `vllm`, you can use\n    ```shell\n    vllm serve ... --rope-scaling ''{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}'' --max-model-len 131072  \n    ```\n\n  For `sglang`, you can use\n    ```shell\n    python -m sglang.launch_server ... --json-model-override-args ''{"rope_scaling":{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}}''\n    ```\n\n  For `llama-server` from `llama.cpp`, you can use\n    ```shell\n    llama-server ... --rope-scaling yarn --rope-scale 4 --yarn-orig-ctx 32768\n    ```\n\n> [!IMPORTANT]\n> If you encounter the following warning\n> ```\n> Unrecognized keys in `rope_scaling` for ''rope_type''=''yarn'': {''original_max_position_embeddings''}\n> ```\n> please upgrade `transformers>=4.51.0`.\n\n> [!NOTE]\n> All the notable open-source frameworks implement static YaRN, which means the scaling factor remains constant regardless of input length, **potentially impacting performance on shorter texts.**\n> We advise adding the `rope_scaling` configuration only when processing long contexts is required. \n> It is also recommended to modify the `factor` as needed. For example, if the typical context length for your application is 65,536 tokens, it would be better to set `factor` as 2.0. \n\n> [!NOTE]\n> The default `max_position_embeddings` in `config.json` is set to 40,960. This allocation includes reserving 32,768 tokens for outputs and 8,192 tokens for typical prompts, which is sufficient for most scenarios involving short text processing. If the average context length does not exceed 32,768 tokens, we do not recommend enabling YaRN in this scenario, as it may potentially degrade model performance.\n\n> [!TIP]\n> The endpoint provided by Alibaba Model Studio supports dynamic YaRN by default and no extra configuration is needed.\n\n## Best Practices\n\nTo achieve optimal performance, we recommend the following settings:\n\n1. **Sampling Parameters**:\n   - For thinking mode (`enable_thinking=True`), use `Temperature=0.6`, `TopP=0.95`, `TopK=20`, and `MinP=0`. **DO NOT use greedy decoding**, as it can lead to performance degradation and endless repetitions.\n   - For non-thinking mode (`enable_thinking=False`), we suggest using `Temperature=0.7`, `TopP=0.8`, `TopK=20`, and `MinP=0`.\n   - For supported frameworks, you can adjust the `presence_penalty` parameter between 0 and 2 to reduce endless repetitions. However, using a higher value may occasionally result in language mixing and a slight decrease in model performance.\n\n2. **Adequate Output Length**: We recommend using an output length of 32,768 tokens for most queries. For benchmarking on highly complex problems, such as those found in math and programming competitions, we suggest setting the max output length to 38,912 tokens. This provides the model with sufficient space to generate detailed and comprehensive responses, thereby enhancing its overall performance.\n\n3. **Standardize Output Format**: We recommend using prompts to standardize model outputs when benchmarking.\n   - **Math Problems**: Include "Please reason step by step, and put your final answer within \boxed{}." in the prompt.\n   - **Multiple-Choice Questions**: Add the following JSON structure to the prompt to standardize responses: "Please show your choice in the `answer` field with only the choice letter, e.g., `"answer": "C"`."\n\n4. **No Thinking Content in History**: In multi-turn conversations, the historical model output should only include the final output part and does not need to include the thinking content. It is implemented in the provided chat template in Jinja2. However, for frameworks that do not directly use the Jinja2 chat template, it is up to the developers to ensure that the best practice is followed.\n\n### Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@misc{qwen3technicalreport,\n      title={Qwen3 Technical Report}, \n      author={Qwen Team},\n      year={2025},\n      eprint={2505.09388},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2505.09388}, \n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":8190735360,"storage_bytes":16392939430,"files_count":15,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen3ForCausalLM"],"model_type":"qwen3","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(''<tool_response>'') and message.content.endswith(''</tool_response>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if ''</think>'' in content %}\n                {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n                {%- set content = content.split(''</think>'')[-1].lstrip(''\\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- ''<|im_start|>'' + message.role + ''\\n<think>\\n'' + reasoning_content.strip(''\\n'') + ''\\n</think>\\n\\n'' + content.lstrip(''\\n'') }}\n            {%- else %}\n                {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- ''<think>\\n\\n</think>\\n\\n'' }}\n    {%- endif %}\n{%- endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen3","source_url":"https://github.com/QwenLM/Qwen3"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Agent","source_url":"https://github.com/QwenLM/Qwen-Agent"},{"type":"based_on_paper","target_id":"arxiv:2309.00071","source_url":"https://arxiv.org/abs/2309.00071"},{"type":"based_on_paper","target_id":"arxiv:2505.09388","source_url":"https://arxiv.org/abs/2505.09388"}]', NULL, 'Apache-2.0', 'approved', 79, 'b56050969c84dc863a6cb404526f51ee', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-dx8152-Qwen-Edit-2509-Multiple-angles', 'huggingface--dx8152--qwen-edit-2509-multiple-angles', 'Qwen-Edit-2509-Multiple-angles', 'dx8152', '--- license: apache-2.0 base_model: - Qwen/Qwen-Image-Edit-2509 pipeline_tag: image-to-image tags: - lora library_name: diffusers --- This model is trained (code-free!) on ModelScope. Thanks to ModelScope team for providing the training infra: https://www.modelscope.cn/aigc/modelTraining ------- Updated 2025/11/2: Some people mentioned that the model has an unstable consistency issue. I have re-uploaded a version with more training iterations, hoping to fix the consistency problem. Welcome ev...', '["diffusers","lora","image-to-image","base_model:qwen/qwen-image-edit-2509","base_model:adapter:qwen/qwen-image-edit-2509","license:apache-2.0","region:us"]', 'image-to-image', 792, 89772, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/dx8152/Qwen-Edit-2509-Multiple-angles","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nbase_model:\n- Qwen/Qwen-Image-Edit-2509\npipeline_tag: image-to-image\ntags:\n- lora\nlibrary_name: diffusers\n---\n\nThis model is trained (code-free!) on ModelScope. Thanks to ModelScope team for providing the training infra:\n\nhttps://www.modelscope.cn/aigc/modelTraining\n\n\n-------\n\n\nUpdated 2025/11/2: Some people mentioned that the model has an unstable consistency issue. I have re-uploaded a version with more training iterations, hoping to fix the consistency problem.\n\nWelcome everyone to use Lora of Qwen-Edit-2509, its performance is very amazingÔºÅ\n\n\nThere are no trigger words. You can control the camera to move up, down, left, and right, as well as rotate it to the left and right. You can also look down or up. The camera can be changed to a wide-angle or close-up shot.\n\nOnline running linkÔºö www.runninghub.ai/post/1985311204883243009?inviteCode=rh-v1331\n\nThis is a user guide: \n\nYouTubeÔºöhttps://youtu.be/UGdW8W1MqW8\n\nBlibiliÔºöhttps://www.bilibili.com/video/BV1oi1gBBEZV/\n\n‚Äú\nÂ∞ÜÈïúÂ§¥ÂêëÂâçÁßªÂä®ÔºàMove the camera forward.Ôºâ\nÂ∞ÜÈïúÂ§¥ÂêëÂ∑¶ÁßªÂä®ÔºàMove the camera left.Ôºâ\nÂ∞ÜÈïúÂ§¥ÂêëÂè≥ÁßªÂä®ÔºàMove the camera right.Ôºâ\nÂ∞ÜÈïúÂ§¥Âêë‰∏ãÁßªÂä®ÔºàMove the camera down.Ôºâ\nÂ∞ÜÈïúÂ§¥ÂêëÂ∑¶ÊóãËΩ¨45Â∫¶ÔºàRotate the camera 45 degrees to the left.Ôºâ\nÂ∞ÜÈïúÂ§¥ÂêëÂè≥ÊóãËΩ¨45Â∫¶ÔºàRotate the camera 45 degrees to the right.Ôºâ\nÂ∞ÜÈïúÂ§¥ËΩ¨‰∏∫‰øØËßÜÔºàTurn the camera to a top-down view.Ôºâ\nÂ∞ÜÈïúÂ§¥ËΩ¨‰∏∫ÂπøËßíÈïúÂ§¥ÔºàTurn the camera to a wide-angle lens.Ôºâ\nÂ∞ÜÈïúÂ§¥ËΩ¨‰∏∫ÁâπÂÜôÈïúÂ§¥ÔºàTurn the camera to a close-up.Ôºâ\n...\nThere are many possibilities; you can try them yourself.\n‚Äù\n------\n\nInstructions: Download the lora file to the models/loras folder.\n\nYou also need this lora and use them together: https://huggingface.co/lightx2v/Qwen-Image-Lightning/tree/main\n\n\nFor communication/cooperation, you can join the discord group to communicateÔºö https://discord.gg/yVAVa43mWk\n\nIf these resources are helpful to you, or if you use them for business purposes, please buy me a coffee. Thank you for supporting original content! PayPal: Daniel8152\n\n\n\n![ÊïàÊûúÂõæ](https://cdn-uploads.huggingface.co/production/uploads/64461e86ab86b035add67e41/tCsqZOi1YQtSj9qBKfbK1.jpeg)\n![ÊïàÊûúÂõæ2](https://cdn-uploads.huggingface.co/production/uploads/64461e86ab86b035add67e41/AC0KJK5F5gETFy1M-ZIKj.jpeg)\n![ÊïàÊûúÂõæ3](https://cdn-uploads.huggingface.co/production/uploads/64461e86ab86b035add67e41/cr0XKCaPVLlcFDjRbQNdc.jpeg)\nI saw some people wanted to see a comparison between LoRa and the model''s inherent capabilities, so I conducted a test. The trained model is indeed more powerful and intelligent, while the original model already possessed certain abilities. It''s very powerful, which is why we love this model, and precisely because of this, we need to explore its potential.\n![ÂØπÊØîÂõæ](https://cdn-uploads.huggingface.co/production/uploads/64461e86ab86b035add67e41/yFk1v6tQuOV0jlGUjpU2S.jpeg)', '{"pipeline_tag":"image-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":478911124,"files_count":4,"spaces_count":65,"gated":false,"private":false,"config":null}', '[]', '[]', NULL, 'Apache-2.0', 'approved', 64, '33904eecae1288d1c702d1c09f2225f1', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-metavoiceio-metavoice-1B-v0.1', 'huggingface--metavoiceio--metavoice-1b-v0.1', 'metavoice-1B-v0.1', 'metavoiceio', '--- license: apache-2.0 language: - en tags: - pretrained - text-to-speech library_name: metavoice inference: false --- MetaVoice-1B is a 1.2B parameter base model trained on 100K hours of speech for TTS (text-to-speech). It has been built with the following priorities: * Emotional speech rhythm and tone in English. No hallucinations. * Support for voice cloning with finetuning. * We have had success with as little as 1 minute training data for Indian speakers. * Zero-shot cloning for America...', '["metavoice","pretrained","text-to-speech","en","license:apache-2.0","region:us"]', 'text-to-speech', 790, 150, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/metavoiceio/metavoice-1B-v0.1","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\ntags:\n  - pretrained\n  - text-to-speech\nlibrary_name: metavoice\ninference: false\n---\n\nMetaVoice-1B is a 1.2B parameter base model trained on 100K hours of speech for TTS (text-to-speech). It has been built with the following priorities:\n* Emotional speech rhythm and tone in English. No hallucinations.\n* Support for voice cloning with finetuning.\n  * We have had success with as little as 1 minute training data for Indian speakers.\n* Zero-shot cloning for American & British voices, with 30s reference audio.\n* Support for long-form synthesis.\n\nWe‚Äôre releasing MetaVoice-1B under the Apache 2.0 license, *it can be used without restrictions*.\n\n## Usage\nSee [Github](https://github.com/metavoiceio/metavoice-src) for the latest usage instructions.\n\n## Finetuning\n\nSee [Github](https://github.com/metavoiceio/metavoice-src?tab=readme-ov-file#finetuning) for the latest finetuning instructions.\n\n## Soon\n- Long form / arbitrary length TTS\n- Streaming\n\n## Architecture\nWe predict EnCodec tokens from text, and speaker information. This is then diffused up to the waveform level, with post-processing applied to clean up the audio.\n\n* We use a causal GPT to predict the first two hierarchies of EnCodec tokens. Text and audio are part of the LLM context. Speaker information is passed via conditioning at the token embedding layer. This speaker conditioning is obtained from a separately trained speaker verification network.\n  - The two hierarchies are predicted in a "flattened interleaved" manner, we predict the first token of the first hierarchy, then the first token of the second hierarchy, then the second token of the first hierarchy, and so on.\n  - We use condition-free sampling to boost the cloning capability of the model.\n  - The text is tokenised using a custom trained BPE tokeniser with 512 tokens.\n  - Note that we''ve skipped predicting semantic tokens as done in other works, as we found that this isn''t strictly necessary.\n* We use a non-causal (encoder-style) transformer to predict the rest of the 6 hierarchies from the first two hierarchies. This is a super small model (~10Mn parameters), and has extensive zero-shot generalisation to most speakers we''ve tried. Since it''s non-causal, we''re also able to predict all the timesteps in parallel.\n* We use multi-band diffusion to generate waveforms from the EnCodec tokens. We noticed that the speech is clearer than using the original RVQ decoder or VOCOS. However, the diffusion at waveform level leaves some background artifacts which are quite unpleasant to the ear. We clean this up in the next step.\n* We use DeepFilterNet to clear up the artifacts introduced by the multi-band diffusion. \n\n## Optimizations\nThe model supports: \n1. KV-caching via Flash Decoding \n2. Batching (including texts of different lengths)\n', '{"pipeline_tag":"text-to-speech","library_name":"metavoice","framework":"metavoice","params":null,"storage_bytes":5047828383,"files_count":6,"spaces_count":11,"gated":false,"private":false,"config":{}}', '[]', '[{"type":"has_code","target_id":"github:metavoiceio:metavoice-src","source_url":"https://github.com/metavoiceio/metavoice-src"},{"type":"has_code","target_id":"github:metavoiceio:metavoice-src","source_url":"https://github.com/metavoiceio/metavoice-src?tab=readme-ov-file#finetuning"}]', NULL, 'Apache-2.0', 'approved', 64, 'bad0ac2f3b44080e8d2f747164a6a22d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tencent-Hunyuan-A13B-Instruct', 'huggingface--tencent--hunyuan-a13b-instruct', 'Hunyuan-A13B-Instruct', 'tencent', '--- license: other license_name: tencent-hunyuan-a13b license_link: https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/LICENSE library_name: transformers --- <p align="center"> <img src="https://dscache.tencent-cloud.cn/upload/uploader/hunyuan-64b418fd052c033b228e04bc77bbc4b54fd7f5bc.png" width="400"/> <br> </p><p></p> <p align="center"> ü§ó&nbsp;<a href="https://huggingface.co/tencent/Hunyuan-A13B-Instruct"><b>Hugging Face</b></a>&nbsp;&nbsp;|&nbsp;&nbsp; üñ•Ô∏è&nbsp;<a href="https://huny...', '["transformers","safetensors","hunyuan_v1_moe","text-generation","conversational","custom_code","license:other","endpoints_compatible","region:us"]', 'text-generation', 785, 9394, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tencent/Hunyuan-A13B-Instruct","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: tencent-hunyuan-a13b\nlicense_link: https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/LICENSE\nlibrary_name: transformers\n---\n\n<p align="center">\n <img src="https://dscache.tencent-cloud.cn/upload/uploader/hunyuan-64b418fd052c033b228e04bc77bbc4b54fd7f5bc.png" width="400"/> <br>\n</p><p></p>\n\n\n<p align="center">\n    ü§ó&nbsp;<a href="https://huggingface.co/tencent/Hunyuan-A13B-Instruct"><b>Hugging Face</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    üñ•Ô∏è&nbsp;<a href="https://hunyuan.tencent.com" style="color: red;"><b>Official Website</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    üïñ&nbsp;<a href="https://cloud.tencent.com/product/hunyuan"><b>HunyuanAPI</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    üïπÔ∏è&nbsp;<a href="https://hunyuan.tencent.com/?model=hunyuan-a13b"><b>Demo</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;\n    ü§ñ&nbsp;<a href="https://modelscope.cn/models/Tencent-Hunyuan/Hunyuan-A13B-Instruct"><b>ModelScope</b></a>\n</p>\n\n\n<p align="center">\n    <a href="https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/report/Hunyuan_A13B_Technical_Report.pdf"><b>Technical Report</b> </a> |\n    <a href="https://github.com/Tencent-Hunyuan/Hunyuan-A13B"><b>GITHUB</b></a> | \n    <a href="https://cnb.cool/tencent/hunyuan/Hunyuan-A13B"><b>cnb.cool</b></a> | \n    <a href="https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/LICENSE"><b>LICENSE</b></a> | \n    <a href="https://raw.githubusercontent.com/Tencent-Hunyuan/Hunyuan-A13B/main/assets/1751881231452.jpg"><b>WeChat</b></a> | \n    <a href="https://discord.gg/bsPcMEtV7v"><b>Discord</b></a>\n</p>\n\n\n  \nWelcome to the official repository of **Hunyuan-A13B**, an innovative and open-source large language model (LLM) built on a fine-grained Mixture-of-Experts (MoE) architecture. Designed for efficiency and scalability, Hunyuan-A13B delivers cutting-edge performance with minimal computational overhead, making it an ideal choice for advanced reasoning and general-purpose applications, especially in resource-constrained environments.\n\n## Model Introduction\n\nWith the rapid advancement of artificial intelligence technology, large language models (LLMs) have achieved remarkable progress in natural language processing, computer vision, and scientific tasks. However, as model scales continue to expand, optimizing resource consumption while maintaining high performance has become a critical challenge. To address this, we have explored Mixture of Experts (MoE) architectures. The newly introduced Hunyuan-A13B model features a total of 80 billion parameters with 13 billion active parameters. It not only delivers high-performance results but also achieves optimal resource efficiency, successfully balancing computational power and resource utilization.\n\n### Key Features and Advantages\n\n- **Compact yet Powerful**: With only 13 billion active parameters (out of a total of 80 billion), the model delivers competitive performance on a wide range of benchmark tasks, rivaling much larger models.\n- **Hybrid Reasoning Support**: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.\n- **Ultra-Long Context Understanding**: Natively supports a 256K context window, maintaining stable performance on long-text tasks.\n- **Enhanced Agent Capabilities**: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3, œÑ-Bench and C3-Bench.\n- **Efficient Inference**: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.\n\n### Why Choose Hunyuan-A13B?\n\nAs a powerful yet computationally efficient large model, Hunyuan-A13B is an ideal choice for researchers and developers seeking high performance under resource constraints. Whether for academic research, cost-effective AI solution development, or innovative application exploration, this model provides a robust foundation for advancement.\n\n&nbsp;\n\n## Related News\n* 2025.6.27 We have open-sourced  **Hunyuan-A13B-Pretrain** , **Hunyuan-A13B-Instruct** , **Hunyuan-A13B-Instruct-FP8** , **Hunyuan-A13B-Instruct-GPTQ-Int4** on Hugging Face. In addition, we have released a <a href="report/Hunyuan_A13B_Technical_Report.pdf">technical report </a> and a training and inference operation manual, which provide detailed information about the model‚Äôs capabilities as well as the operations for training and inference.\n\n<br>\n\n\n## Benchmark\n\nNote: The following benchmarks are evaluated by TRT-LLM-backend on several **base models**. \n\n| Model            | Hunyuan-Large | Qwen2.5-72B  | Qwen3-A22B | Hunyuan-A13B |\n|------------------|---------------|--------------|-------------|---------------|\n| MMLU             | 88.40          | 86.10         | 87.81        | 88.17          |\n| MMLU-Pro         | 60.20          | 58.10        | 68.18           | 67.23          |\n| MMLU-Redux              |  87.47         | 83.90         | 87.40        | 87.67          |\n| BBH        | 86.30             | 85.80            | 88.87        | 87.56          |\n| SuperGPQA    |  38.90         | 36.20          | 44.06           | 41.32          |\n| EvalPlus       | 75.69          | 65.93         | 77.60        | 78.64          |\n| MultiPL-E             | 59.13             | 60.50            | 65.94        | 69.33          |\n| MBPP | 72.60             | 76.00            | 81.40        | 83.86          |\n| CRUX-I             | 57.00          | 57.63          | -        | 70.13          |\n| CRUX-O             | 60.63          | 66.20          | 79.00        | 77.00          |\n| MATH            | 69.80          | 62.12         | 71.84        | 72.35          |\n| CMATH            | 91.30          | 84.80         | -        | 91.17          |\n| GSM8k         | 92.80             | 91.50           | 94.39        | 91.83          |\n| GPQA            | 25.18             | 45.90            | 47.47        | 49.12          |\n\n\nHunyuan-A13B-Instruct has achieved highly competitive performance across multiple benchmarks, particularly in mathematics, science, agent domains, and more. We compared it with several powerful models, and the results are shown below.\n\n| Topic               |                        Bench                         | OpenAI-o1-1217 | DeepSeek R1 | Qwen3-A22B | Hunyuan-A13B-Instruct |\n|:-------------------:|:----------------------------------------------------:|:-------------:|:------------:|:-----------:|:---------------------:|\n| **Mathematics**     |            AIME 2024<br>AIME 2025<br>MATH            | 74.3<br>79.2<br>96.4 | 79.8<br>70<br>94.9 | 85.7<br>81.5<br>94.0 | 87.3<br>76.8<br>94.3 |\n| **Science**         |            GPQA-Diamond<br>OlympiadBench             | 78<br>83.1 | 71.5<br>82.4 | 71.1<br>85.7 | 71.2<br>82.7 |\n| **Coding**          |  Livecodebench<br>Fullstackbench<br>ArtifactsBench   | 63.9<br>64.6<br>38.6 | 65.9<br>71.6<br>44.6 | 70.7<br>65.6<br>44.6 | 63.9<br>67.8<br>43 |\n| **Reasoning**       |              BBH<br>DROP<br>ZebraLogic               | 80.4<br>90.2<br>81 | 83.7<br>92.2<br>78.7 | 88.9<br>90.3<br>80.3 | 89.1<br>91.1<br>84.7 |\n| **Instruction<br>Following** |                 IF-Eval<br>SysBench                  | 91.8<br>82.5 | 88.3<br>77.7 | 83.4<br>74.2 | 84.7<br>76.1 |\n| **Text<br>Creation**|                LengthCtrl<br>InsCtrl                 | 60.1<br>74.8 | 55.9<br>69 | 53.3<br>73.7 | 55.4<br>71.9 |\n| **NLU**             |               ComplexNLU<br>Word-Task                | 64.7<br>67.1 | 64.5<br>76.3 | 59.8<br>56.4 | 61.2<br>62.9 |\n| **Agent**           | BFCL v3<br> œÑ-Bench<br>ComplexFuncBench<br> C3-Bench | 67.8<br>60.4<br>47.6<br>58.8 | 56.9<br>43.8<br>41.1<br>55.3 | 70.8<br>44.6<br>40.6<br>51.7 | 78.3<br>54.7<br>61.2<br>63.5 |\n\n\n&nbsp;\n\n## Use with transformers\n\nOur model defaults to using slow-thinking reasoning, and there are two ways to disable CoT reasoning. \n1. Pass "enable_thinking=False" when calling apply_chat_template.\n2. Adding "/no_think" before the prompt will force the model not to use perform CoT reasoning. Similarly, adding "/think" before the prompt will force the model to perform CoT reasoning.\n\nThe following code snippet shows how to use the transformers library to load and apply the model. \nIt also demonstrates how to enable and disable the reasoning mode , \nand how to parse the reasoning process along with the final output.\n\n\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\nimport re\n\nmodel_name_or_path = os.environ[''MODEL_PATH'']\n# model_name_or_path = "tencent/Hunyuan-A13B-Instruct"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map="auto",trust_remote_code=True)  # You may want to use bfloat16 and/or move to GPU here\nmessages = [\n    {"role": "user", "content": "Write a short summary of the benefits of regular exercise"},\n]\n\ntext = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            enable_thinking=True\n            )\n\nmodel_inputs = tokenizer([text], return_tensors="pt").to(model.device)\nmodel_inputs.pop("token_type_ids", None)\noutputs = model.generate(**model_inputs, max_new_tokens=4096)\n\n\noutput_text = tokenizer.decode(outputs[0])\n\nthink_pattern = r''<think>(.*?)</think>''\nthink_matches = re.findall(think_pattern, output_text, re.DOTALL)\n\nanswer_pattern = r''<answer>(.*?)</answer>''\nanswer_matches = re.findall(answer_pattern, output_text, re.DOTALL)\n\nthink_content = [match.strip() for match in think_matches][0]\nanswer_content = [match.strip() for match in answer_matches][0]\nprint(f"thinking_content:{think_content}\n\n")\nprint(f"answer_content:{answer_content}\n\n")\n```\n\n### Fast and slow thinking switch\n\nThis model supports two modes of operation:\n\n- Slow Thinking Mode (Default): Enables detailed internal reasoning steps before producing the final answer.\n- Fast Thinking Mode: Skips the internal reasoning process for faster inference, going straight to the final answer.\n\n**Switching to Fast Thinking Mode:**\n\nTo disable the reasoning process, set `enable_thinking=False` in the apply_chat_template call:\n```\n\ntext = tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            enable_thinking=False\n            )\n```                                           \n\n\n\n## Deployment   \n\nFor deployment, you can use frameworks such as **TensorRT-LLM**, **vLLM**, or **SGLang** to serve the model and create an OpenAI-compatible API endpoint.\n\nimage: https://hub.docker.com/r/hunyuaninfer/hunyuan-a13b/tags \n\n\n### TensorRT-LLM\n\n#### Docker Image \n\nWe provide a pre-built Docker image based on the latest version of TensorRT-LLM.\n\n- To Get Started, Download the Docker Image:\n\n**From Docker Hub:**\n```\ndocker pull hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-trtllm\n```\n\n**From China Mirror(Thanks to [CNB](https://cnb.cool/ "CNB.cool")):**\n\n\nFirst, pull the image from CNB:\n``` \ndocker pull docker.cnb.cool/tencent/hunyuan/hunyuan-a13b:hunyuan-moe-A13B-trtllm\n```\n\nThen, rename the image to better align with the following scripts:\n```\n\ndocker tag docker.cnb.cool/tencent/hunyuan/hunyuan-a13b:hunyuan-moe-A13B-trtllm hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-trtllm\n```\n\n\n- start docker \n\n```\ndocker run --name hunyuanLLM_infer --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --gpus=all hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-trtllm\n```\n\n- Prepare Configuration file:\n\n```\ncat >/path/to/extra-llm-api-config.yml <<EOF\nuse_cuda_graph: true\ncuda_graph_padding_enabled: true\ncuda_graph_batch_sizes:\n- 1\n- 2\n- 4\n- 8\n- 16\n- 32\nprint_iter_log: true\nEOF\n```\n\n\n- Start the API server:\n\n\n```\ntrtllm-serve \\n  /path/to/HunYuan-moe-A13B \\n  --host localhost \\n  --port 8000 \\n  --backend pytorch \\n  --max_batch_size 32 \\n  --max_num_tokens 16384 \\n  --tp_size 2 \\n  --kv_cache_free_gpu_memory_fraction 0.6 \\n  --trust_remote_code \\n  --extra_llm_api_options /path/to/extra-llm-api-config.yml\n```\n\n\n### vLLM\n\n#### Inference from Docker Image\nWe provide a pre-built Docker image containing vLLM 0.8.5 with full support for this model. The official vllm release is currently under developmentÔºå **note: cuda 12.4 is require for this docker**.\n\n\n- To Get Started, Download the Docker Image:\n\n**From Docker Hub:**\n```\ndocker pull hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1\n```\n\n**From China Mirror(Thanks to [CNB](https://cnb.cool/ "CNB.cool")):**\n\n\nFirst, pull the image from CNB:\n``` \ndocker pull docker.cnb.cool/tencent/hunyuan/hunyuan-a13b/hunyuan-infer-vllm-cuda12.4:v1\n```\n\nThen, rename the image to better align with the following scripts:\n```\ndocker tag docker.cnb.cool/tencent/hunyuan/hunyuan-a13b/hunyuan-infer-vllm-cuda12.4:v1 hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1 \n```\n\n- Download Model file: \n  - Huggingface:  will download automicly by vllm.\n  - ModelScope: `modelscope download --model Tencent-Hunyuan/Hunyuan-A13B-Instruct`\n \n\n- Start the API server:\n\nmodel download by huggingface:\n```\ndocker run --rm  --ipc=host \\n        -v ~/.cache:/root/.cache/ \\n        --security-opt seccomp=unconfined \\n        --net=host \\n        --gpus=all \\n        -it \\n        --entrypoint python3 hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1 \\n        -m vllm.entrypoints.openai.api_server \\n        --host 0.0.0.0 \\n        --tensor-parallel-size 4 \\n        --port 8000 \\n        --model tencent/Hunyuan-A13B-Instruct  \\n        --trust_remote_code\n``` \n\nmodel downloaded by modelscope:\n```\ndocker run --rm  --ipc=host \\n        -v ~/.cache/modelscope:/root/.cache/modelscope \\n        --security-opt seccomp=unconfined \\n        --net=host \\n        --gpus=all \\n        -it \\n        --entrypoint python3 hunyuaninfer/hunyuan-infer-vllm-cuda12.4:v1 \\n        -m vllm.entrypoints.openai.api_server \\n        --host 0.0.0.0 \\n        --tensor-parallel-size 4 \\n        --port 8000 \\n        --model /root/.cache/modelscope/hub/models/Tencent-Hunyuan/Hunyuan-A13B-Instruct/  \\n        --trust_remote_code\n```\n\n### Source Code\nSupport for this model has been added via  this [PR 20114](https://github.com/vllm-project/vllm/pull/20114 ) in the vLLM project, \nThis patch already been merged by community at Jul-1-2025.\n\nYou can build and run vLLM from source using code after `ecad85`.\n\n### Model Context Length Support\n\nThe Hunyuan A13B model supports a maximum context length of **256K tokens (262,144 tokens)**. However, due to GPU memory constraints on most hardware setups, the default configuration in `config.json` limits the context length to **32K tokens** to prevent out-of-memory (OOM) errors.\n\n#### Extending Context Length to 256K\n\nTo enable full 256K context support, you can manually modify the `max_position_embeddings` field in the model''s `config.json` file as follows:\n\n```json\n{\n  ...\n  "max_position_embeddings": 262144,\n  ...\n}\n```\n\nWhen serving the model using **vLLM**, you can also explicitly set the maximum model length by adding the following flag to your server launch command:\n\n```bash\n--max-model-len 262144\n```\n\n#### Recommended Configuration for 256K Context Length\n\nThe following configuration is recommended for deploying the model with 256K context length support on systems equipped with **NVIDIA H20 GPUs (96GB VRAM)**:\n\n| Model DType    | KV-Cache Dtype | Number of Devices | Model Length |\n|----------------|----------------|--------------------|--------------|\n| `bfloat16`     | `bfloat16`     | 4                  | 262,144      |\n\n> ‚ö†Ô∏è **Note:** Using FP8 quantization for KV-cache may impact generation quality. The above settings are suggested configurations for stable 256K-length service deployment.\n\n\n#### Tool Calling with vLLM\n\nTo support agent-based workflows and function calling capabilities, this model includes specialized parsing mechanisms for handling tool calls and internal reasoning steps.\n\nFor a complete working example of how to implement and use these features in an agent setting, please refer to our full agent implementation on GitHub:  \nüîó [Hunyuan A13B Agent Example](https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/agent/)\n\nWhen deploying the model using **vLLM**, the following parameters can be used to configure the tool parsing behavior:\n\n| Parameter                | Value                                                                 |\n|--------------------------|-----------------------------------------------------------------------|\n| `--tool-parser-plugin`   | [Local Hunyuan A13B Tool Parser File](https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/agent/hunyuan_tool_parser.py) |\n| `--tool-call-parser`     | `hunyuan`                                                            |\n\nThese settings enable vLLM to correctly interpret and route tool calls generated by the model according to the expected format.\n\n### Reasoning parser\n\nvLLM reasoning parser support on Hunyuan A13B model is under development.\n\n\n\n### SGLang\n\n#### Docker Image \n\nWe also provide a pre-built Docker image based on the latest version of SGLang.\n\nTo get started:\n\n- Pull the Docker image\n\n```\ndocker pull docker.cnb.cool/tencent/hunyuan/hunyuan-a13b:hunyuan-moe-A13B-sglang\nor\ndocker pull hunyuaninfer/hunyuan-a13b:hunyuan-moe-A13B-sglang\n```\n\n- Start the API server:\n\n```\ndocker run --gpus all \\n    --shm-size 32g \\n    -p 30000:30000 \\n    --ipc=host \\n    docker.cnb.cool/tencent/hunyuan/hunyuan-a13b:hunyuan-moe-A13B-sglang \\n    -m sglang.launch_server --model-path hunyuan/huanyuan_A13B --tp 4 --trust-remote-code --host 0.0.0.0 --port 30000\n```\n\n## Contact Us\n\nIf you would like to leave a message for our R&D and product teams, Welcome to contact our open-source team . You can also contact us via email (hunyuan_opensource@tencent.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":80393183232,"storage_bytes":482436254851,"files_count":46,"spaces_count":1,"gated":false,"private":false,"config":{"architectures":["HunYuanMoEV1ForCausalLM"],"auto_map":{"AutoConfig":"configuration_hunyuan.HunYuanConfig","AutoModel":"hunyuan.HunYuanModel","AutoModelForCausalLM":"hunyuan.HunYuanMoEV1ForCausalLM"},"model_type":"hunyuan_v1_moe","tokenizer_config":{"bos_token":"<|startoftext|>","eos_token":"<|eos|>","pad_token":"<|pad|>","chat_template":"{% set context = {''has_head'': true} %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = message[''content''] %}{% if loop.index0 == 0 %}{% if content == '''' %}{% set _ = context.update({''has_head'': false}) %}{% elif message[''role''] == ''system'' %}{% set content = ''<|startoftext|>'' + content + ''<|extra_4|>'' %}{% endif %}{% endif %}{% if message[''role''] == ''user'' %}{% if loop.index0 == 1 and not context.has_head %}{% set content = ''<|startoftext|>'' + content %}{% endif %}{% if loop.index0 == 1 and context.has_head %}{% set content = content + ''<|extra_0|>'' %}{% else %}{% set content = ''<|startoftext|>'' + content + ''<|extra_0|>'' %}{% endif %}{% elif message[''role''] == ''assistant'' %}{% set content = content + ''<|eos|>'' %}{% endif %}{{ content }}{% endfor %}"}}}', '[]', '[{"type":"has_code","target_id":"github:Tencent-Hunyuan:Hunyuan-A13B","source_url":"https://github.com/Tencent-Hunyuan/Hunyuan-A13B"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:Hunyuan-A13B","source_url":"https://github.com/Tencent-Hunyuan/Hunyuan-A13B"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:Hunyuan-A13B\"><b>GITHUB<","source_url":"https://github.com/Tencent-Hunyuan/Hunyuan-A13B\"><b>GITHUB<"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:Hunyuan-A13B","source_url":"https://github.com/Tencent-Hunyuan/Hunyuan-A13B"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:Hunyuan-A13B","source_url":"https://github.com/Tencent-Hunyuan/Hunyuan-A13B"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:Hunyuan-A13B","source_url":"https://github.com/Tencent-Hunyuan/Hunyuan-A13B"}]', NULL, 'Other', 'approved', 79, 'dede00382f06dde99dc858cc4d95f0fb', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen-7B-Chat', 'huggingface--qwen--qwen-7b-chat', 'Qwen-7B-Chat', 'Qwen', '--- language: - zh - en tags: - qwen pipeline_tag: text-generation inference: false license: other license_name: tongyi-qianwen-license-agreement license_link: https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT --- <p align="center"> <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg" width="400"/> <p> <br> <p align="center"> ü§ó <a href="https://huggingface.co/Qwen">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href="https://modelscope.cn/organ...', '["transformers","safetensors","qwen","text-generation","custom_code","zh","en","arxiv:2309.16609","arxiv:2305.08322","arxiv:2009.03300","arxiv:2305.05280","arxiv:2210.03629","license:other","region:us"]', 'text-generation', 784, 88838, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen-7B-Chat","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- zh\n- en\ntags:\n- qwen\npipeline_tag: text-generation\ninference: false\nlicense: other\nlicense_name: tongyi-qianwen-license-agreement\nlicense_link: https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT\n---\n\n# Qwen-7B-Chat\n\n<p align="center">\n    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/logo_qwen.jpg" width="400"/>\n<p>\n<br>\n\n<p align="center">\n        ü§ó <a href="https://huggingface.co/Qwen">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspü§ñ <a href="https://modelscope.cn/organization/qwen">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp üìë <a href="https://arxiv.org/abs/2309.16609">Paper</a> &nbsp&nbsp ÔΩú &nbsp&nbspüñ•Ô∏è <a href="https://modelscope.cn/studios/qwen/Qwen-7B-Chat-Demo/summary">Demo</a>\n<br>\n<a href="https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png">WeChat (ÂæÆ‰ø°)</a>&nbsp&nbsp | &nbsp&nbsp<a href="https://discord.gg/z3GAxXZ9Ce">Discord</a>&nbsp&nbsp ÔΩú  &nbsp&nbsp<a href="https://dashscope.aliyun.com">API</a> \n</p>\n<br>\n\n\n## ‰ªãÁªçÔºàIntroductionÔºâ\n\n**ÈÄö‰πâÂçÉÈóÆ-7BÔºàQwen-7BÔºâ**ÊòØÈòøÈáå‰∫ëÁ†îÂèëÁöÑÈÄö‰πâÂçÉÈóÆÂ§ßÊ®°ÂûãÁ≥ªÂàóÁöÑ70‰∫øÂèÇÊï∞ËßÑÊ®°ÁöÑÊ®°Âûã„ÄÇQwen-7BÊòØÂü∫‰∫éTransformerÁöÑÂ§ßËØ≠Ë®ÄÊ®°Âûã, Âú®Ë∂ÖÂ§ßËßÑÊ®°ÁöÑÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆ‰∏äËøõË°åËÆ≠ÁªÉÂæóÂà∞„ÄÇÈ¢ÑËÆ≠ÁªÉÊï∞ÊçÆÁ±ªÂûãÂ§öÊ†∑ÔºåË¶ÜÁõñÂπøÊ≥õÔºåÂåÖÊã¨Â§ßÈáèÁΩëÁªúÊñáÊú¨„ÄÅ‰∏ì‰∏ö‰π¶Á±ç„ÄÅ‰ª£Á†ÅÁ≠â„ÄÇÂêåÊó∂ÔºåÂú®Qwen-7BÁöÑÂü∫Á°Ä‰∏äÔºåÊàë‰ª¨‰ΩøÁî®ÂØπÈΩêÊú∫Âà∂ÊâìÈÄ†‰∫ÜÂü∫‰∫éÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑAIÂä©ÊâãQwen-7B-Chat„ÄÇÁõ∏ËæÉ‰∫éÊúÄÂàùÂºÄÊ∫êÁöÑQwen-7BÊ®°ÂûãÔºåÊàë‰ª¨Áé∞Â∑≤Â∞ÜÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÂíåChatÊ®°ÂûãÊõ¥Êñ∞Âà∞ÊïàÊûúÊõ¥‰ºòÁöÑÁâàÊú¨„ÄÇÊú¨‰ªìÂ∫ì‰∏∫Qwen-7B-ChatÁöÑ‰ªìÂ∫ì„ÄÇ\n\nÂ¶ÇÊûúÊÇ®ÊÉ≥‰∫ÜËß£Êõ¥Â§öÂÖ≥‰∫éÈÄö‰πâÂçÉÈóÆ-7BÂºÄÊ∫êÊ®°ÂûãÁöÑÁªÜËäÇÔºåÊàë‰ª¨Âª∫ËÆÆÊÇ®ÂèÇÈòÖ[GitHub‰ª£Á†ÅÂ∫ì](https://github.com/QwenLM/Qwen)„ÄÇ\n\n**Qwen-7B** is the 7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-7B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. Additionally, based on the pretrained Qwen-7B, we release Qwen-7B-Chat, a large-model-based AI assistant, which is trained with alignment techniques. Now we have updated both our pretrained and chat models with better performances. This repository is the one for Qwen-7B-Chat.\n\nFor more details about Qwen, please refer to the [GitHub](https://github.com/QwenLM/Qwen) code repository.\n<br>\n\n## Ë¶ÅÊ±ÇÔºàRequirementsÔºâ\n\n* python 3.8Âèä‰ª•‰∏äÁâàÊú¨\n* pytorch 1.12Âèä‰ª•‰∏äÁâàÊú¨ÔºåÊé®Ëçê2.0Âèä‰ª•‰∏äÁâàÊú¨\n* Âª∫ËÆÆ‰ΩøÁî®CUDA 11.4Âèä‰ª•‰∏äÔºàGPUÁî®Êà∑„ÄÅflash-attentionÁî®Êà∑Á≠âÈúÄËÄÉËôëÊ≠§ÈÄâÈ°πÔºâ\n* python 3.8 and above\n* pytorch 1.12 and above, 2.0 and above are recommended\n* CUDA 11.4 and above are recommended (this is for GPU users, flash-attention users, etc.)\n<br>\n\n## ‰æùËµñÈ°πÔºàDependencyÔºâ\n\nËøêË°åQwen-7B-ChatÔºåËØ∑Á°Æ‰øùÊª°Ë∂≥‰∏äËø∞Ë¶ÅÊ±ÇÔºåÂÜçÊâßË°å‰ª•‰∏ãpipÂëΩ‰ª§ÂÆâË£Ö‰æùËµñÂ∫ì\n\nTo run Qwen-7B-Chat, please make sure you meet the above requirements, and then execute the following pip commands to install the dependent libraries.\n\n```bash\npip install transformers==4.32.0 accelerate tiktoken einops scipy transformers_stream_generator==0.0.4 peft deepspeed\n```\n\nÂè¶Â§ñÔºåÊé®ËçêÂÆâË£Ö`flash-attention`Â∫ìÔºà**ÂΩìÂâçÂ∑≤ÊîØÊåÅflash attention 2**ÔºâÔºå‰ª•ÂÆûÁé∞Êõ¥È´òÁöÑÊïàÁéáÂíåÊõ¥‰ΩéÁöÑÊòæÂ≠òÂç†Áî®„ÄÇ\n\nIn addition, it is recommended to install the `flash-attention` library (**we support flash attention 2 now.**) for higher efficiency and lower memory usage.\n\n```bash\ngit clone https://github.com/Dao-AILab/flash-attention\ncd flash-attention && pip install .\n# ‰∏ãÊñπÂÆâË£ÖÂèØÈÄâÔºåÂÆâË£ÖÂèØËÉΩÊØîËæÉÁºìÊÖ¢„ÄÇ\n# pip install csrc/layer_norm\n# pip install csrc/rotary\n```\n<br>\n\n## Âø´ÈÄü‰ΩøÁî®ÔºàQuickstartÔºâ\n\n‰∏ãÈù¢Êàë‰ª¨Â±ïÁ§∫‰∫Ü‰∏Ä‰∏™‰ΩøÁî®Qwen-7B-ChatÊ®°ÂûãÔºåËøõË°åÂ§öËΩÆÂØπËØù‰∫§‰∫íÁöÑÊ†∑‰æãÔºö\n\nWe show an example of multi-turn interaction with Qwen-7B-Chat in the following code:\n\n```python\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.generation import GenerationConfig\n\n# Note: The default behavior now has injection attack prevention off.\ntokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)\n\n# use bf16\n# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()\n# use fp16\n# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()\n# use cpu only\n# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="cpu", trust_remote_code=True).eval()\n# use auto mode, automatically select precision based on the device.\nmodel = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True).eval()\n\n# Specify hyperparameters for generation. But if you use transformers>=4.32.0, there is no need to do this.\n# model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True) # ÂèØÊåáÂÆö‰∏çÂêåÁöÑÁîüÊàêÈïøÂ∫¶„ÄÅtop_pÁ≠âÁõ∏ÂÖ≥Ë∂ÖÂèÇ\n\n# Á¨¨‰∏ÄËΩÆÂØπËØù 1st dialogue turn\nresponse, history = model.chat(tokenizer, "‰Ω†Â•Ω", history=None)\nprint(response)\n# ‰Ω†Â•ΩÔºÅÂæàÈ´òÂÖ¥‰∏∫‰Ω†Êèê‰æõÂ∏ÆÂä©„ÄÇ\n\n# Á¨¨‰∫åËΩÆÂØπËØù 2nd dialogue turn\nresponse, history = model.chat(tokenizer, "ÁªôÊàëËÆ≤‰∏Ä‰∏™Âπ¥ËΩª‰∫∫Â•ãÊñóÂàõ‰∏öÊúÄÁªàÂèñÂæóÊàêÂäüÁöÑÊïÖ‰∫ã„ÄÇ", history=history)\nprint(response)\n# ËøôÊòØ‰∏Ä‰∏™ÂÖ≥‰∫é‰∏Ä‰∏™Âπ¥ËΩª‰∫∫Â•ãÊñóÂàõ‰∏öÊúÄÁªàÂèñÂæóÊàêÂäüÁöÑÊïÖ‰∫ã„ÄÇ\n# ÊïÖ‰∫ãÁöÑ‰∏ª‰∫∫ÂÖ¨Âè´ÊùéÊòéÔºå‰ªñÊù•Ëá™‰∏Ä‰∏™ÊôÆÈÄöÁöÑÂÆ∂Â∫≠ÔºåÁà∂ÊØçÈÉΩÊòØÊôÆÈÄöÁöÑÂ∑•‰∫∫„ÄÇ‰ªéÂ∞èÔºåÊùéÊòéÂ∞±Á´ã‰∏ã‰∫Ü‰∏Ä‰∏™ÁõÆÊ†áÔºöË¶ÅÊàê‰∏∫‰∏ÄÂêçÊàêÂäüÁöÑ‰ºÅ‰∏öÂÆ∂„ÄÇ\n# ‰∏∫‰∫ÜÂÆûÁé∞Ëøô‰∏™ÁõÆÊ†áÔºåÊùéÊòéÂã§Â•ãÂ≠¶‰π†ÔºåËÄÉ‰∏ä‰∫ÜÂ§ßÂ≠¶„ÄÇÂú®Â§ßÂ≠¶ÊúüÈó¥Ôºå‰ªñÁßØÊûÅÂèÇÂä†ÂêÑÁßçÂàõ‰∏öÊØîËµõÔºåËé∑Âæó‰∫Ü‰∏çÂ∞ëÂ•ñÈ°π„ÄÇ‰ªñËøòÂà©Áî®ËØæ‰ΩôÊó∂Èó¥ÂéªÂÆû‰π†ÔºåÁßØÁ¥Ø‰∫ÜÂÆùË¥µÁöÑÁªèÈ™å„ÄÇ\n# ÊØï‰∏öÂêéÔºåÊùéÊòéÂÜ≥ÂÆöÂºÄÂßãËá™Â∑±ÁöÑÂàõ‰∏ö‰πãË∑Ø„ÄÇ‰ªñÂºÄÂßãÂØªÊâæÊäïËµÑÊú∫‰ºöÔºå‰ΩÜÂ§öÊ¨°ÈÉΩË¢´ÊãíÁªù‰∫Ü„ÄÇÁÑ∂ËÄåÔºå‰ªñÂπ∂Ê≤°ÊúâÊîæÂºÉ„ÄÇ‰ªñÁªßÁª≠Âä™ÂäõÔºå‰∏çÊñ≠ÊîπËøõËá™Â∑±ÁöÑÂàõ‰∏öËÆ°ÂàíÔºåÂπ∂ÂØªÊâæÊñ∞ÁöÑÊäïËµÑÊú∫‰ºö„ÄÇ\n# ÊúÄÁªàÔºåÊùéÊòéÊàêÂäüÂú∞Ëé∑Âæó‰∫Ü‰∏ÄÁ¨îÊäïËµÑÔºåÂºÄÂßã‰∫ÜËá™Â∑±ÁöÑÂàõ‰∏ö‰πãË∑Ø„ÄÇ‰ªñÊàêÁ´ã‰∫Ü‰∏ÄÂÆ∂ÁßëÊäÄÂÖ¨Âè∏Ôºå‰∏ìÊ≥®‰∫éÂºÄÂèëÊñ∞ÂûãËΩØ‰ª∂„ÄÇÂú®‰ªñÁöÑÈ¢ÜÂØº‰∏ãÔºåÂÖ¨Âè∏ËøÖÈÄüÂèëÂ±ïËµ∑Êù•ÔºåÊàê‰∏∫‰∫Ü‰∏ÄÂÆ∂ÊàêÂäüÁöÑÁßëÊäÄ‰ºÅ‰∏ö„ÄÇ\n# ÊùéÊòéÁöÑÊàêÂäüÂπ∂‰∏çÊòØÂÅ∂ÁÑ∂ÁöÑ„ÄÇ‰ªñÂã§Â•ã„ÄÅÂùöÈüß„ÄÅÂãá‰∫éÂÜíÈô©Ôºå‰∏çÊñ≠Â≠¶‰π†ÂíåÊîπËøõËá™Â∑±„ÄÇ‰ªñÁöÑÊàêÂäü‰πüËØÅÊòé‰∫ÜÔºåÂè™Ë¶ÅÂä™ÂäõÂ•ãÊñóÔºå‰ªª‰Ωï‰∫∫ÈÉΩÊúâÂèØËÉΩÂèñÂæóÊàêÂäü„ÄÇ\n\n# Á¨¨‰∏âËΩÆÂØπËØù 3rd dialogue turn\nresponse, history = model.chat(tokenizer, "ÁªôËøô‰∏™ÊïÖ‰∫ãËµ∑‰∏Ä‰∏™Ê†áÈ¢ò", history=history)\nprint(response)\n# „ÄäÂ•ãÊñóÂàõ‰∏öÔºö‰∏Ä‰∏™Âπ¥ËΩª‰∫∫ÁöÑÊàêÂäü‰πãË∑Ø„Äã\n```\n\nÂÖ≥‰∫éÊõ¥Â§öÁöÑ‰ΩøÁî®ËØ¥ÊòéÔºåËØ∑ÂèÇËÄÉÊàë‰ª¨ÁöÑ[GitHub repo](https://github.com/QwenLM/Qwen)Ëé∑ÂèñÊõ¥Â§ö‰ø°ÊÅØ„ÄÇ\n\nFor more information, please refer to our [GitHub repo](https://github.com/QwenLM/Qwen) for more information.\n<br>\n\n## Tokenizer\n\n> Ê≥®Ôºö‰Ωú‰∏∫ÊúØËØ≠ÁöÑ‚Äútokenization‚ÄùÂú®‰∏≠Êñá‰∏≠Â∞öÊó†ÂÖ±ËØÜÁöÑÊ¶ÇÂøµÂØπÂ∫îÔºåÊú¨ÊñáÊ°£ÈááÁî®Ëã±ÊñáË°®Ëææ‰ª•Âà©ËØ¥Êòé„ÄÇ\n\nÂü∫‰∫étiktokenÁöÑÂàÜËØçÂô®ÊúâÂà´‰∫éÂÖ∂‰ªñÂàÜËØçÂô®ÔºåÊØîÂ¶ÇsentencepieceÂàÜËØçÂô®„ÄÇÂ∞§ÂÖ∂Âú®ÂæÆË∞ÉÈò∂ÊÆµÔºåÈúÄË¶ÅÁâπÂà´Ê≥®ÊÑèÁâπÊÆätokenÁöÑ‰ΩøÁî®„ÄÇÂÖ≥‰∫étokenizerÁöÑÊõ¥Â§ö‰ø°ÊÅØÔºå‰ª•ÂèäÂæÆË∞ÉÊó∂Ê∂âÂèäÁöÑÁõ∏ÂÖ≥‰ΩøÁî®ÔºåËØ∑ÂèÇÈòÖ[ÊñáÊ°£](https://github.com/QwenLM/Qwen/blob/main/tokenization_note_zh.md)„ÄÇ\n\nOur tokenizer based on tiktoken is different from other tokenizers, e.g., sentencepiece tokenizer. You need to pay attention to special tokens, especially in finetuning. For more detailed information on the tokenizer and related use in fine-tuning, please refer to the [documentation](https://github.com/QwenLM/Qwen/blob/main/tokenization_note.md).\n<br>\n\n## ÈáèÂåñ (Quantization)\n\n### Áî®Ê≥ï (Usage)\n\n**ËØ∑Ê≥®ÊÑèÔºöÊàë‰ª¨Êõ¥Êñ∞ÈáèÂåñÊñπÊ°à‰∏∫Âü∫‰∫é[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)ÁöÑÈáèÂåñÔºåÊèê‰æõQwen-7B-ChatÁöÑInt4ÈáèÂåñÊ®°Âûã[ÁÇπÂáªËøôÈáå](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4)„ÄÇÁõ∏ÊØîÊ≠§ÂâçÊñπÊ°àÔºåËØ•ÊñπÊ°àÂú®Ê®°ÂûãËØÑÊµãÊïàÊûúÂá†‰πéÊó†ÊçüÔºå‰∏îÂ≠òÂÇ®ÈúÄÊ±ÇÊõ¥‰ΩéÔºåÊé®ÁêÜÈÄüÂ∫¶Êõ¥‰ºò„ÄÇ**\n\n**Note: we provide a new solution based on [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ), and release an Int4 quantized model for Qwen-7B-Chat [Click here](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4), which achieves nearly lossless model effects but improved performance on both memory costs and inference speed, in comparison with the previous solution.**\n\n‰ª•‰∏ãÊàë‰ª¨Êèê‰æõÁ§∫‰æãËØ¥ÊòéÂ¶Ç‰Ωï‰ΩøÁî®Int4ÈáèÂåñÊ®°Âûã„ÄÇÂú®ÂºÄÂßã‰ΩøÁî®ÂâçÔºåËØ∑ÂÖà‰øùËØÅÊª°Ë∂≥Ë¶ÅÊ±ÇÔºàÂ¶Çtorch 2.0Âèä‰ª•‰∏äÔºåtransformersÁâàÊú¨‰∏∫4.32.0Âèä‰ª•‰∏äÔºåÁ≠âÁ≠âÔºâÔºåÂπ∂ÂÆâË£ÖÊâÄÈúÄÂÆâË£ÖÂåÖÔºö\n\nHere we demonstrate how to use our provided quantized models for inference. Before you start, make sure you meet the requirements of auto-gptq (e.g., torch 2.0 and above, transformers 4.32.0 and above, etc.) and install the required packages:\n\n```bash\npip install auto-gptq optimum\n```\n\nÂ¶ÇÂÆâË£Ö`auto-gptq`ÈÅáÂà∞ÈóÆÈ¢òÔºåÊàë‰ª¨Âª∫ËÆÆÊÇ®Âà∞ÂÆòÊñπ[repo](https://github.com/PanQiWei/AutoGPTQ)ÊêúÁ¥¢ÂêàÈÄÇÁöÑÈ¢ÑÁºñËØëwheel„ÄÇ\n\nÈöèÂêéÂç≥ÂèØ‰ΩøÁî®Âíå‰∏äËø∞‰∏ÄËá¥ÁöÑÁî®Ê≥ïË∞ÉÁî®ÈáèÂåñÊ®°ÂûãÔºö\n\nIf you meet problems installing `auto-gptq`, we advise you to check out the official [repo](https://github.com/PanQiWei/AutoGPTQ) to find a pre-build wheel.\n\nThen you can load the quantized model easily and run inference as same as usual:\n\n```python\nmodel = AutoModelForCausalLM.from_pretrained(\n    "Qwen/Qwen-7B-Chat-Int4",\n    device_map="auto",\n    trust_remote_code=True\n).eval()\nresponse, history = model.chat(tokenizer, "‰Ω†Â•Ω", history=None)\n```\n\n\n\n### ÊïàÊûúËØÑÊµã\n\nÊàë‰ª¨ÂØπBF16ÔºåInt8ÂíåInt4Ê®°ÂûãÂú®Âü∫ÂáÜËØÑÊµã‰∏äÂÅö‰∫ÜÊµãËØïÔºà‰ΩøÁî®zero-shotËÆæÁΩÆÔºâÔºåÂèëÁé∞ÈáèÂåñÊ®°ÂûãÊïàÊûúÊçüÂ§±ËæÉÂ∞èÔºåÁªìÊûúÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö\n\nWe illustrate the zero-shot performance of both BF16, Int8 and Int4 models on the benchmark, and we find that the quantized model does not suffer from significant performance degradation. Results are shown below:\n\n|  Quantization |   MMLU     |  CEval (val) |  GSM8K |  Humaneval |\n| ------------- | :--------: | :----------: | :----: | :--------: |\n| BF16          |    55.8    |     59.7     |  50.3  |    37.2    |\n| Int8          |    55.4    |     59.4     |  48.3  |    34.8    |\n| Int4          |    55.1    |     59.2     |  49.7  |    29.9    |\n\n### Êé®ÁêÜÈÄüÂ∫¶ (Inference Speed)\n\nÊàë‰ª¨ÊµãÁÆó‰∫Ü‰∏çÂêåÁ≤æÂ∫¶Ê®°Âûã‰ª•Âèä‰∏çÂêåFlashAttnÂ∫ìÁâàÊú¨‰∏ãÊ®°ÂûãÁîüÊàê2048Âíå8192‰∏™tokenÁöÑÂπ≥ÂùáÊé®ÁêÜÈÄüÂ∫¶„ÄÇÂ¶ÇÂõæÊâÄÁ§∫Ôºö\n\nWe measured the average inference speed of generating 2048 and 8192 tokens with different quantization levels and versions of flash-attention, respectively.\n\n|  Quantization | FlashAttn | Speed (2048 tokens) | Speed (8192 tokens) |\n| ------------- | :-------: | :------------------:| :------------------:|\n|      BF16     |   v2      | 40.93               | 36.14               |\n|      Int8     |   v2      | 37.47               | 32.54               |\n|      Int4     |   v2      | 50.09               | 38.61               |\n|      BF16     |   v1      | 40.75               | 35.34               |\n|      Int8     |   v1      | 37.51               | 32.39               |\n|      Int4     |   v1      | 45.98               | 36.47               |\n|      BF16     |  Disabled | 37.55               | 33.56               |\n|      Int8     |  Disabled | 37.84               | 32.65               |\n|      Int4     |  Disabled | 48.12               | 36.70               |\n\nÂÖ∑‰ΩìËÄåË®ÄÔºåÊàë‰ª¨ËÆ∞ÂΩïÂú®ÈïøÂ∫¶‰∏∫1ÁöÑ‰∏ä‰∏ãÊñáÁöÑÊù°‰ª∂‰∏ãÁîüÊàê8192‰∏™tokenÁöÑÊÄßËÉΩ„ÄÇËØÑÊµãËøêË°å‰∫éÂçïÂº†A100-SXM4-80G GPUÔºå‰ΩøÁî®PyTorch 2.0.1ÂíåCUDA 11.8„ÄÇÊé®ÁêÜÈÄüÂ∫¶ÊòØÁîüÊàê8192‰∏™tokenÁöÑÈÄüÂ∫¶ÂùáÂÄº„ÄÇ\n\nIn detail, the setting of profiling is generating 8192 new tokens with 1 context token. The profiling runs on a single A100-SXM4-80G GPU with PyTorch 2.0.1 and CUDA 11.8. The inference speed is averaged over the generated 8192 tokens.\n\nÊ≥®ÊÑèÔºö‰ª•‰∏äInt4/Int8Ê®°ÂûãÁîüÊàêÈÄüÂ∫¶‰ΩøÁî®autogptqÂ∫ìÁªôÂá∫ÔºåÂΩìÂâç``AutoModelForCausalLM.from_pretrained``ËΩΩÂÖ•ÁöÑÊ®°ÂûãÁîüÊàêÈÄüÂ∫¶‰ºöÊÖ¢Â§ßÁ∫¶20%„ÄÇÊàë‰ª¨Â∑≤ÁªèÂ∞ÜËØ•ÈóÆÈ¢òÊ±áÊä•ÁªôHuggingFaceÂõ¢ÈòüÔºåËã•ÊúâËß£ÂÜ≥ÊñπÊ°àÂ∞ÜÂç≥Êó∂Êõ¥Êñ∞„ÄÇ\n\nNote: The generation speed of the Int4/Int8 models mentioned above is provided by the autogptq library. The current speed of the model loaded using "AutoModelForCausalLM.from_pretrained" will be approximately 20% slower. We have reported this issue to the HuggingFace team and will update it promptly if a solution is available.\n\n### ÊòæÂ≠ò‰ΩøÁî® (GPU Memory Usage)\n\nÊàë‰ª¨ËøòÊµãÁÆó‰∫Ü‰∏çÂêåÊ®°ÂûãÁ≤æÂ∫¶ÁºñÁ†Å2048‰∏™tokenÂèäÁîüÊàê8192‰∏™tokenÁöÑÂ≥∞ÂÄºÊòæÂ≠òÂç†Áî®ÊÉÖÂÜµ„ÄÇÔºàÊòæÂ≠òÊ∂àËÄóÂú®ÊòØÂê¶‰ΩøÁî®FlashAttnÁöÑÊÉÖÂÜµ‰∏ãÂùáÁ±ª‰ºº„ÄÇÔºâÁªìÊûúÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö\n\nWe also profile the peak GPU memory usage for encoding 2048 tokens as context (and generating single token) and generating 8192 tokens (with single token as context) under different quantization levels, respectively. ÔºàThe GPU memory usage is similar when using flash-attention or not.ÔºâThe results are shown below.\n\n| Quantization Level | Peak Usage for Encoding 2048 Tokens | Peak Usage for Generating 8192 Tokens |\n| ------------------ | :---------------------------------: | :-----------------------------------: |\n| BF16               | 16.99GB                             | 22.53GB                               |\n| Int8               | 11.20GB                             | 16.62GB                               |\n| Int4               |  8.21GB                             | 13.63GB                               |\n\n‰∏äËø∞ÊÄßËÉΩÊµãÁÆó‰ΩøÁî®[Ê≠§ËÑöÊú¨](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py)ÂÆåÊàê„ÄÇ\n\nThe above speed and memory profiling are conducted using [this script](https://qianwen-res.oss-cn-beijing.aliyuncs.com/profile.py).\n<br>\n\n## Ê®°ÂûãÁªÜËäÇÔºàModelÔºâ\n\n‰∏éQwen-7BÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁõ∏ÂêåÔºåQwen-7B-ChatÊ®°ÂûãËßÑÊ®°Âü∫Êú¨ÊÉÖÂÜµÂ¶Ç‰∏ãÊâÄÁ§∫:\n\nThe details of the model architecture of Qwen-7B-Chat are listed as follows:\n\n| Hyperparameter  | Value  |\n|:----------------|:------:|\n| n_layers        |   32   |\n| n_heads         |   32   |\n| d_model         |  4096  |\n| vocab size      | 151851 |\n| sequence length |  8192  |\n\nÂú®‰ΩçÁΩÆÁºñÁ†Å„ÄÅFFNÊøÄÊ¥ªÂáΩÊï∞ÂíånormalizationÁöÑÂÆûÁé∞ÊñπÂºè‰∏äÔºåÊàë‰ª¨‰πüÈááÁî®‰∫ÜÁõÆÂâçÊúÄÊµÅË°åÁöÑÂÅöÊ≥ïÔºå\nÂç≥RoPEÁõ∏ÂØπ‰ΩçÁΩÆÁºñÁ†Å„ÄÅSwiGLUÊøÄÊ¥ªÂáΩÊï∞„ÄÅRMSNormÔºàÂèØÈÄâÂÆâË£Öflash-attentionÂä†ÈÄüÔºâ„ÄÇ\n\nÂú®ÂàÜËØçÂô®ÊñπÈù¢ÔºåÁõ∏ÊØîÁõÆÂâç‰∏ªÊµÅÂºÄÊ∫êÊ®°Âûã‰ª•‰∏≠Ëã±ËØçË°®‰∏∫‰∏ªÔºåQwen-7B-Chat‰ΩøÁî®‰∫ÜÁ∫¶15‰∏átokenÂ§ßÂ∞èÁöÑËØçË°®„ÄÇ\nËØ•ËØçË°®Âú®GPT-4‰ΩøÁî®ÁöÑBPEËØçË°®`cl100k_base`Âü∫Á°Ä‰∏äÔºåÂØπ‰∏≠Êñá„ÄÅÂ§öËØ≠Ë®ÄËøõË°å‰∫Ü‰ºòÂåñÔºåÂú®ÂØπ‰∏≠„ÄÅËã±„ÄÅ‰ª£Á†ÅÊï∞ÊçÆÁöÑÈ´òÊïàÁºñËß£Á†ÅÁöÑÂü∫Á°Ä‰∏äÔºåÂØπÈÉ®ÂàÜÂ§öËØ≠Ë®ÄÊõ¥Âä†ÂèãÂ•ΩÔºåÊñπ‰æøÁî®Êà∑Âú®‰∏çÊâ©Â±ïËØçË°®ÁöÑÊÉÖÂÜµ‰∏ãÂØπÈÉ®ÂàÜËØ≠ÁßçËøõË°åËÉΩÂäõÂ¢ûÂº∫„ÄÇ\nËØçË°®ÂØπÊï∞Â≠óÊåâÂçï‰∏™Êï∞Â≠ó‰ΩçÂàáÂàÜ„ÄÇË∞ÉÁî®ËæÉ‰∏∫È´òÊïàÁöÑ[tiktokenÂàÜËØçÂ∫ì](https://github.com/openai/tiktoken)ËøõË°åÂàÜËØç„ÄÇ\n\nFor position encoding, FFN activation function, and normalization calculation methods, we adopt the prevalent practices, i.e., RoPE relative position encoding, SwiGLU for activation function, and RMSNorm for normalization (optional installation of flash-attention for acceleration).\n\nFor tokenization, compared to the current mainstream open-source models based on Chinese and English vocabularies, Qwen-7B-Chat uses a vocabulary of over 150K tokens.\nIt first considers efficient encoding of Chinese, English, and code data, and is also more friendly to multilingual languages, enabling users to directly enhance the capability of some languages without expanding the vocabulary.\nIt segments numbers by single digit, and calls the [tiktoken](https://github.com/openai/tiktoken) tokenizer library for efficient tokenization.\n<br>\n\n## ËØÑÊµãÊïàÊûúÔºàEvaluationÔºâ\n\nÂØπ‰∫éQwen-7B-ChatÊ®°ÂûãÔºåÊàë‰ª¨ÂêåÊ†∑ËØÑÊµã‰∫ÜÂ∏∏ËßÑÁöÑ‰∏≠ÊñáÁêÜËß£ÔºàC-EvalÔºâ„ÄÅËã±ÊñáÁêÜËß£ÔºàMMLUÔºâ„ÄÅ‰ª£Á†ÅÔºàHumanEvalÔºâÂíåÊï∞Â≠¶ÔºàGSM8KÔºâÁ≠âÊùÉÂ®Å‰ªªÂä°ÔºåÂêåÊó∂ÂåÖÂê´‰∫ÜÈïøÂ∫èÂàó‰ªªÂä°ÁöÑËØÑÊµãÁªìÊûú„ÄÇÁî±‰∫éQwen-7B-ChatÊ®°ÂûãÁªèËøáÂØπÈΩêÂêéÔºåÊøÄÂèë‰∫ÜËæÉÂº∫ÁöÑÂ§ñÈÉ®Á≥ªÁªüË∞ÉÁî®ËÉΩÂäõÔºåÊàë‰ª¨ËøòËøõË°å‰∫ÜÂ∑•ÂÖ∑‰ΩøÁî®ËÉΩÂäõÊñπÈù¢ÁöÑËØÑÊµã„ÄÇ\n\nÊèêÁ§∫ÔºöÁî±‰∫éÁ°¨‰ª∂ÂíåÊ°ÜÊû∂ÈÄ†ÊàêÁöÑËàçÂÖ•ËØØÂ∑ÆÔºåÂ§çÁé∞ÁªìÊûúÂ¶ÇÊúâÊ≥¢Âä®Â±û‰∫éÊ≠£Â∏∏Áé∞Ë±°„ÄÇ\n\nFor Qwen-7B-Chat, we also evaluate the model on C-Eval, MMLU, HumanEval, GSM8K, etc., as well as the benchmark evaluation for long-context understanding, and tool usage.\n\nNote: Due to rounding errors caused by hardware and framework, differences in reproduced results are possible.\n\n### ‰∏≠ÊñáËØÑÊµãÔºàChinese EvaluationÔºâ\n\n#### C-Eval\n\nÂú®[C-Eval](https://arxiv.org/abs/2305.08322)È™åËØÅÈõÜ‰∏äÔºåÊàë‰ª¨ËØÑ‰ª∑‰∫ÜQwen-7B-ChatÊ®°ÂûãÁöÑ0-shot & 5-shotÂáÜÁ°ÆÁéá\n\nWe demonstrate the 0-shot & 5-shot accuracy of Qwen-7B-Chat on C-Eval validation set\n\n|              Model               | Avg. Acc. |\n|:--------------------------------:|:---------:|\n|          LLaMA2-7B-Chat          |   31.9    |\n|         LLaMA2-13B-Chat          |   36.2    |\n|         LLaMA2-70B-Chat          |   44.3    |\n|         ChatGLM2-6B-Chat         |   52.6    |\n|         InternLM-7B-Chat         |   53.6    |\n|        Baichuan2-7B-Chat         |   55.6    |\n|        Baichuan2-13B-Chat        |   56.7    |\n| Qwen-7B-Chat (original) (0-shot) |   54.2    |\n|    **Qwen-7B-Chat (0-shot)**     |   59.7    |\n|    **Qwen-7B-Chat (5-shot)**     |   59.3    |\n|    **Qwen-14B-Chat (0-shot)**    |   69.8    |\n|    **Qwen-14B-Chat (5-shot)**    | **71.7**  |\n\nC-EvalÊµãËØïÈõÜ‰∏äÔºåQwen-7B-ChatÊ®°ÂûãÁöÑzero-shotÂáÜÁ°ÆÁéáÁªìÊûúÂ¶Ç‰∏ãÔºö\n\nThe zero-shot accuracy of Qwen-7B-Chat on C-Eval testing set is provided below:\n\n| Model                   |   Avg.   | STEM | Social Sciences | Humanities | Others |\n| :---------------------- | :------: | :--: | :-------------: | :--------: | :----: |\n| Chinese-Alpaca-Plus-13B |   41.5   | 36.6 |      49.7       |    43.1    |  41.2  |\n| Chinese-Alpaca-2-7B     |   40.3   |  -   |        -        |     -      |   -    |\n| ChatGLM2-6B-Chat        |   50.1   | 46.4 |      60.4       |    50.6    |  46.9  |\n| Baichuan-13B-Chat       |   51.5   | 43.7 |      64.6       |    56.2    |  49.2  |\n| Qwen-7B-Chat (original)        |   54.6   | 47.8 |      67.6       |    59.3    |  50.6  |\n| **Qwen-7B-Chat**   |   58.6   | 53.3 |      72.1       |    62.8    |  52.0  |\n| **Qwen-14B-Chat**       | **69.1** | 65.1 |      80.9       |    71.2    |  63.4  |\n\nÂú®7BËßÑÊ®°Ê®°Âûã‰∏äÔºåÁªèËøá‰∫∫Á±ªÊåá‰ª§ÂØπÈΩêÁöÑQwen-7B-ChatÊ®°ÂûãÔºåÂáÜÁ°ÆÁéáÂú®ÂêåÁ±ªÁõ∏ËøëËßÑÊ®°Ê®°Âûã‰∏≠‰ªçÁÑ∂Â§Ñ‰∫éÂâçÂàó„ÄÇ\n\nCompared with other pretrained models with comparable model size, the human-aligned Qwen-7B-Chat performs well in C-Eval accuracy.\n\n### Ëã±ÊñáËØÑÊµãÔºàEnglish EvaluationÔºâ\n\n#### MMLU\n\n[MMLU](https://arxiv.org/abs/2009.03300)ËØÑÊµãÈõÜ‰∏äÔºåQwen-7B-ChatÊ®°ÂûãÁöÑ 0-shot & 5-shot ÂáÜÁ°ÆÁéáÂ¶Ç‰∏ãÔºåÊïàÊûúÂêåÊ†∑Âú®ÂêåÁ±ªÂØπÈΩêÊ®°Âûã‰∏≠ÂêåÊ†∑Ë°®Áé∞ËæÉ‰ºò„ÄÇ\n\nThe 0-shot & 5-shot accuracy of Qwen-7B-Chat on MMLU is provided below.\nThe performance of Qwen-7B-Chat still on the top between other human-aligned models with comparable size.\n\n|              Model               | Avg. Acc. |\n|:--------------------------------:|:---------:|\n|         ChatGLM2-6B-Chat         |   46.0    |\n|          LLaMA2-7B-Chat          |   46.2    |\n|         InternLM-7B-Chat         |   51.1    |\n|        Baichuan2-7B-Chat         |   52.9    |\n|         LLaMA2-13B-Chat          |   54.6    |\n|        Baichuan2-13B-Chat        |   57.3    |\n|         LLaMA2-70B-Chat          |   63.8    |\n| Qwen-7B-Chat (original) (0-shot) |   53.9    |\n|    **Qwen-7B-Chat (0-shot)**     |   55.8    |\n|    **Qwen-7B-Chat (5-shot)**     |   57.0    |\n|    **Qwen-14B-Chat (0-shot)**    |   64.6    |\n|    **Qwen-14B-Chat (5-shot)**    | **66.5**  |\n\n### ‰ª£Á†ÅËØÑÊµãÔºàCoding EvaluationÔºâ\n\nQwen-7B-ChatÂú®[HumanEval](https://github.com/openai/human-eval)ÁöÑzero-shot Pass@1ÊïàÊûúÂ¶Ç‰∏ã\n\nThe zero-shot Pass@1 of Qwen-7B-Chat on [HumanEval](https://github.com/openai/human-eval) is demonstrated below\n\n|          Model          |  Pass@1  |\n|:-----------------------:|:--------:|\n|    ChatGLM2-6B-Chat     |   11.0   |\n|     LLaMA2-7B-Chat      |   12.2   |\n|    Baichuan2-7B-Chat    |   13.4   |\n|    InternLM-7B-Chat     |   14.6   |\n|   Baichuan2-13B-Chat    |   17.7   |\n|     LLaMA2-13B-Chat     |   18.9   |\n|     LLaMA2-70B-Chat     |   32.3   |\n| Qwen-7B-Chat (original) |   24.4   |\n|    **Qwen-7B-Chat**     |   37.2   |\n|    **Qwen-14B-Chat**    | **43.9** |\n\n### Êï∞Â≠¶ËØÑÊµãÔºàMathematics EvaluationÔºâ\n\nÂú®ËØÑÊµãÊï∞Â≠¶ËÉΩÂäõÁöÑ[GSM8K](https://github.com/openai/grade-school-math)‰∏äÔºåQwen-7B-ChatÁöÑÂáÜÁ°ÆÁéáÁªìÊûúÂ¶Ç‰∏ã\n\nThe accuracy of Qwen-7B-Chat on GSM8K is shown below\n\n|                Model                 |   Acc.   |\n|:------------------------------------:|:--------:|\n|            LLaMA2-7B-Chat            |   26.3   |\n|           ChatGLM2-6B-Chat           |   28.8   |\n|          Baichuan2-7B-Chat           |   32.8   |\n|           InternLM-7B-Chat           |   33.0   |\n|           LLaMA2-13B-Chat            |   37.1   |\n|          Baichuan2-13B-Chat          |   55.3   |\n|           LLaMA2-70B-Chat            |   59.3   |\n| **Qwen-7B-Chat (original) (0-shot)** |   41.1   |\n|      **Qwen-7B-Chat (0-shot)**       |   50.3   |\n|      **Qwen-7B-Chat (8-shot)**       |   54.1   |\n|      **Qwen-14B-Chat (0-shot)**      | **60.1** |\n|      **Qwen-14B-Chat (8-shot)**      |   59.3   |\n\n### ÈïøÂ∫èÂàóËØÑÊµãÔºàLong-Context UnderstandingÔºâ\n\nÈÄöËøáNTKÊèíÂÄºÔºåLogNÊ≥®ÊÑèÂäõÁº©ÊîæÂèØ‰ª•Êâ©Â±ïQwen-7B-ChatÁöÑ‰∏ä‰∏ãÊñáÈïøÂ∫¶„ÄÇÂú®ÈïøÊñáÊú¨ÊëòË¶ÅÊï∞ÊçÆÈõÜ[VCSUM](https://arxiv.org/abs/2305.05280)‰∏äÔºàÊñáÊú¨Âπ≥ÂùáÈïøÂ∫¶Âú®15KÂ∑¶Âè≥ÔºâÔºåQwen-7B-ChatÁöÑRouge-LÁªìÊûúÂ¶Ç‰∏ãÔºö\n\n**(Ëã•Ë¶ÅÂêØÁî®Ëøô‰∫õÊäÄÂ∑ßÔºåËØ∑Â∞Üconfig.jsonÈáåÁöÑ`use_dynamic_ntk`Âíå`use_logn_attn`ËÆæÁΩÆ‰∏∫true)**\n\nWe introduce NTK-aware interpolation, LogN attention scaling to extend the context length of Qwen-7B-Chat. The Rouge-L results of Qwen-7B-Chat on long-text summarization dataset [VCSUM](https://arxiv.org/abs/2305.05280) (The average length of this dataset is around 15K) are shown below:\n\n**(To use these tricks, please set `use_dynamic_ntk` and `use_long_attn` to true in config.json.)**\n\n| Model             | VCSUM (zh) |\n|:------------------|:----------:|\n| GPT-3.5-Turbo-16k |    16.0    |\n| LLama2-7B-Chat    |    0.2     |\n| InternLM-7B-Chat  |    13.0    |\n| ChatGLM2-6B-Chat  |    16.3    |\n| **Qwen-7B-Chat**  |  **16.6**  |\n\n### Â∑•ÂÖ∑‰ΩøÁî®ËÉΩÂäõÁöÑËØÑÊµãÔºàTool UsageÔºâ\n\n#### ReAct Prompting\n\nÂçÉÈóÆÊîØÊåÅÈÄöËøá [ReAct Prompting](https://arxiv.org/abs/2210.03629) Ë∞ÉÁî®Êèí‰ª∂/Â∑•ÂÖ∑/API„ÄÇReAct ‰πüÊòØ [LangChain](https://python.langchain.com/) Ê°ÜÊû∂ÈááÁî®ÁöÑ‰∏ªË¶ÅÊñπÂºè‰πã‰∏Ä„ÄÇÂú®Êàë‰ª¨ÂºÄÊ∫êÁöÑ„ÄÅÁî®‰∫éËØÑ‰º∞Â∑•ÂÖ∑‰ΩøÁî®ËÉΩÂäõÁöÑËØÑÊµãÂü∫ÂáÜ‰∏äÔºåÂçÉÈóÆÁöÑË°®Áé∞Â¶Ç‰∏ãÔºö\n\nQwen-Chat supports calling plugins/tools/APIs through [ReAct Prompting](https://arxiv.org/abs/2210.03629). ReAct is also one of the main approaches used by the [LangChain](https://python.langchain.com/) framework. In our evaluation benchmark for assessing tool usage capabilities, Qwen-Chat''s performance is as follows:\n\n<table>\n    <tr>\n        <th colspan="4" align="center">Chinese Tool-Use Benchmark</th>\n    </tr>\n    <tr>\n        <th align="center">Model</th><th align="center">Tool Selection (Acc.‚Üë)</th><th align="center">Tool Input (Rouge-L‚Üë)</th><th align="center">False Positive Error‚Üì</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align="center">95%</td><td align="center">0.90</td><td align="center">15.0%</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align="center">85%</td><td align="center">0.88</td><td align="center">75.0%</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align="center">98%</td><td align="center">0.91</td><td align="center">7.3%</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align="center">98%</td><td align="center">0.93</td><td align="center">2.4%</td>\n    </tr>\n</table>\n\n> ËØÑÊµãÂü∫ÂáÜ‰∏≠Âá∫Áé∞ÁöÑÊèí‰ª∂ÂùáÊ≤°ÊúâÂá∫Áé∞Âú®ÂçÉÈóÆÁöÑËÆ≠ÁªÉÈõÜ‰∏≠„ÄÇËØ•Âü∫ÂáÜËØÑ‰º∞‰∫ÜÊ®°ÂûãÂú®Â§ö‰∏™ÂÄôÈÄâÊèí‰ª∂‰∏≠ÈÄâÊã©Ê≠£Á°ÆÊèí‰ª∂ÁöÑÂáÜÁ°ÆÁéá„ÄÅ‰º†ÂÖ•Êèí‰ª∂ÁöÑÂèÇÊï∞ÁöÑÂêàÁêÜÊÄß„ÄÅ‰ª•ÂèäÂÅáÈò≥Áéá„ÄÇÂÅáÈò≥ÁéáÔºàFalse PositiveÔºâÂÆö‰πâÔºöÂú®Â§ÑÁêÜ‰∏çËØ•Ë∞ÉÁî®Êèí‰ª∂ÁöÑËØ∑Ê±ÇÊó∂ÔºåÈîôËØØÂú∞Ë∞ÉÁî®‰∫ÜÊèí‰ª∂„ÄÇ\n\n> The plugins that appear in the evaluation set do not appear in the training set of Qwen. This benchmark evaluates the accuracy of the model in selecting the correct plugin from multiple candidate plugins, the rationality of the parameters passed into the plugin, and the false positive rate. False Positive: Incorrectly invoking a plugin when it should not have been called when responding to a query.\n\n![](assets/react_showcase_001.png)\n![](assets/react_showcase_002.png)\n\n#### Code Interpreter\n\n‰∏∫‰∫ÜËÄÉÂØüQwen‰ΩøÁî®Python Code InterpreterÂÆåÊàêÊï∞Â≠¶Ëß£È¢ò„ÄÅÊï∞ÊçÆÂèØËßÜÂåñ„ÄÅÂèäÊñá‰ª∂Â§ÑÁêÜ‰∏éÁà¨Ëô´Á≠â‰ªªÂä°ÁöÑËÉΩÂäõÔºåÊàë‰ª¨‰∏ìÈó®Âª∫ËÆæÂπ∂ÂºÄÊ∫ê‰∫Ü‰∏Ä‰∏™ËØÑÊµãËøôÊñπÈù¢ËÉΩÂäõÁöÑ[ËØÑÊµãÂü∫ÂáÜ](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark)„ÄÇ\n\nÊàë‰ª¨ÂèëÁé∞QwenÂú®ÁîüÊàê‰ª£Á†ÅÁöÑÂèØÊâßË°åÁéá„ÄÅÁªìÊûúÊ≠£Á°ÆÊÄß‰∏äÂùáË°®Áé∞ËæÉÂ•ΩÔºö\n\nTo assess Qwen''s ability to use the Python Code Interpreter for tasks such as mathematical problem solving, data visualization, and other general-purpose tasks such as file handling and web scraping, we have created and open-sourced a benchmark specifically designed for evaluating these capabilities. You can find the benchmark at this [link](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark).\n\nWe have observed that Qwen performs well in terms of code executability and result accuracy when generating code:\n\n<table>\n    <tr>\n        <th colspan="4" align="center">Executable Rate of Generated Code (%)</th>\n    </tr>\n    <tr>\n        <th align="center">Model</th><th align="center">Math‚Üë</th><th align="center">Visualization‚Üë</th><th align="center">General‚Üë</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align="center">91.9</td><td align="center">85.9</td><td align="center">82.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align="center">89.2</td><td align="center">65.0</td><td align="center">74.1</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-7B-Chat</td>\n        <td align="center">41.9</td>\n        <td align="center">33.1</td>\n        <td align="center">24.1 </td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align="center">50.0</td>\n        <td align="center">40.5</td>\n        <td align="center">48.3 </td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-7B-Instruct</td>\n        <td align="center">85.1</td>\n        <td align="center">54.0</td>\n        <td align="center">70.7 </td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align="center">93.2</td>\n        <td align="center">55.8</td>\n        <td align="center">74.1 </td>\n    </tr>\n    <tr>\n        <td>InternLM-7B-Chat-v1.1</td>\n        <td align="center">78.4</td>\n        <td align="center">44.2</td>\n        <td align="center">62.1 </td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align="center">70.3</td>\n        <td align="center">44.2</td>\n        <td align="center">65.5 </td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align="center">82.4</td>\n        <td align="center">64.4</td>\n        <td align="center">67.2 </td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align="center">89.2</td>\n        <td align="center">84.1</td>\n        <td align="center">65.5</td>\n    </tr>\n</table>\n\n<table>\n    <tr>\n        <th colspan="4" align="center">Accuracy of Code Execution Results (%)</th>\n    </tr>\n    <tr>\n        <th align="center">Model</th><th align="center">Math‚Üë</th><th align="center">Visualization-Hard‚Üë</th><th align="center">Visualization-Easy‚Üë</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align="center">82.8</td><td align="center">66.7</td><td align="center">60.8</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align="center">47.3</td><td align="center">33.3</td><td align="center">55.7</td>\n    </tr>\n    <tr>\n        <td>LLaMA2-7B-Chat</td>\n        <td align="center">3.9</td>\n        <td align="center">14.3</td>\n        <td align="center">39.2 </td>\n    </tr>\n    <tr>\n        <td>LLaMA2-13B-Chat</td>\n        <td align="center">8.3</td>\n        <td align="center">8.3</td>\n        <td align="center">40.5 </td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-7B-Instruct</td>\n        <td align="center">14.3</td>\n        <td align="center">26.2</td>\n        <td align="center">60.8 </td>\n    </tr>\n    <tr>\n        <td>CodeLLaMA-13B-Instruct</td>\n        <td align="center">28.2</td>\n        <td align="center">27.4</td>\n        <td align="center">62.0 </td>\n    </tr>\n    <tr>\n        <td>InternLM-7B-Chat-v1.1</td>\n        <td align="center">28.5</td>\n        <td align="center">4.8</td>\n        <td align="center">40.5 </td>\n    </tr>\n    <tr>\n        <td>InternLM-20B-Chat</td>\n        <td align="center">34.6</td>\n        <td align="center">21.4</td>\n        <td align="center">45.6 </td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td>\n        <td align="center">41.9</td>\n        <td align="center">40.5</td>\n        <td align="center">54.4 </td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td>\n        <td align="center">58.4</td>\n        <td align="center">53.6</td>\n        <td align="center">59.5</td>\n    </tr>\n</table>\n\n<p align="center">\n    <br>\n    <img src="assets/code_interpreter_showcase_001.jpg" />\n    <br>\n<p>\n\n#### Huggingface Agent\n\nÂçÉÈóÆËøòÂÖ∑Â§á‰Ωú‰∏∫ [HuggingFace Agent](https://huggingface.co/docs/transformers/transformers_agents) ÁöÑËÉΩÂäõ„ÄÇÂÆÉÂú® Huggingface Êèê‰æõÁöÑrunÊ®°ÂºèËØÑÊµãÂü∫ÂáÜ‰∏äÁöÑË°®Áé∞Â¶Ç‰∏ãÔºö\n\nQwen-Chat also has the capability to be used as a [HuggingFace Agent](https://huggingface.co/docs/transformers/transformers_agents). Its performance on the run-mode benchmark provided by HuggingFace is as follows:\n\n<table>\n    <tr>\n        <th colspan="4" align="center">HuggingFace Agent Benchmark- Run Mode</th>\n    </tr>\n    <tr>\n        <th align="center">Model</th><th align="center">Tool Selection‚Üë</th><th align="center">Tool Used‚Üë</th><th align="center">Code‚Üë</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align="center">100</td><td align="center">100</td><td align="center">97.4</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align="center">95.4</td><td align="center">96.3</td><td align="center">87.0</td>\n    </tr>\n    <tr>\n        <td>StarCoder-Base-15B</td><td align="center">86.1</td><td align="center">87.0</td><td align="center">68.9</td>\n    </tr>\n    <tr>\n        <td>StarCoder-15B</td><td align="center">87.0</td><td align="center">88.0</td><td align="center">68.9</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align="center">87.0</td><td align="center">87.0</td><td align="center">71.5</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align="center">93.5</td><td align="center">94.4</td><td align="center">87.0</td>\n    </tr>\n</table>\n\n<table>\n    <tr>\n        <th colspan="4" align="center">HuggingFace Agent Benchmark - Chat Mode</th>\n    </tr>\n    <tr>\n        <th align="center">Model</th><th align="center">Tool Selection‚Üë</th><th align="center">Tool Used‚Üë</th><th align="center">Code‚Üë</th>\n    </tr>\n    <tr>\n        <td>GPT-4</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">98.5</td>\n    </tr>\n    <tr>\n        <td>GPT-3.5</td><td align="center">97.3</td><td align="center">96.8</td><td align="center">89.6</td>\n    </tr>\n    <tr>\n        <td>StarCoder-Base-15B</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">91.1</td>\n    </tr>\n    <tr>\n        <td>StarCoder-15B</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">89.6</td>\n    </tr>\n    <tr>\n        <td>Qwen-7B-Chat</td><td align="center">94.7</td><td align="center">94.7</td><td align="center">85.1</td>\n    </tr>\n    <tr>\n        <td>Qwen-14B-Chat</td><td align="center">97.9</td><td align="center">97.9</td><td align="center">95.5</td>\n    </tr>\n</table>\n\n<br>\n\n## x86 Âπ≥Âè∞ (x86 Platforms)\nÂú® ÈÖ∑Áùø‚Ñ¢/Ëá≥Âº∫¬Æ ÂèØÊâ©Â±ïÂ§ÑÁêÜÂô®Êàñ Arc‚Ñ¢ GPU ‰∏äÈÉ®ÁΩ≤ÈáèÂåñÊ®°ÂûãÊó∂ÔºåÂª∫ËÆÆ‰ΩøÁî® [OpenVINO‚Ñ¢ Toolkit](https://docs.openvino.ai/2023.3/gen_ai_guide.html)‰ª•ÂÖÖÂàÜÂà©Áî®Á°¨‰ª∂ÔºåÂÆûÁé∞Êõ¥Â•ΩÁöÑÊé®ÁêÜÊÄßËÉΩ„ÄÇÊÇ®ÂèØ‰ª•ÂÆâË£ÖÂπ∂ËøêË°åÊ≠§ [example notebook](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot)„ÄÇÁõ∏ÂÖ≥ÈóÆÈ¢òÔºåÊÇ®ÂèØÂú®[OpenVINO repo](https://github.com/openvinotoolkit/openvino_notebooks/issues)‰∏≠Êèê‰∫§„ÄÇ\n\nWhen deploy on Core‚Ñ¢/Xeon¬Æ Scalable Processors or with Arc‚Ñ¢ GPU, [OpenVINO‚Ñ¢ Toolkit](https://docs.openvino.ai/2023.3/gen_ai_guide.html) is recommended. You can install and run this [example notebook](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/254-llm-chatbot). For related issues, you are welcome to file an issue at [OpenVINO repo](https://github.com/openvinotoolkit/openvino_notebooks/issues).\n\n## FAQ\n\nÂ¶ÇÈÅáÂà∞ÈóÆÈ¢òÔºåÊï¨ËØ∑Êü•ÈòÖ[FAQ](https://github.com/QwenLM/Qwen/blob/main/FAQ_zh.md)‰ª•ÂèäissueÂå∫ÔºåÂ¶Ç‰ªçÊó†Ê≥ïËß£ÂÜ≥ÂÜçÊèê‰∫§issue„ÄÇ\n\nIf you meet problems, please refer to [FAQ](https://github.com/QwenLM/Qwen/blob/main/FAQ.md) and the issues first to search a solution before you launch a new issue.\n<br>\n\n## ÂºïÁî® (Citation)\n\nÂ¶ÇÊûú‰Ω†ËßâÂæóÊàë‰ª¨ÁöÑÂ∑•‰ΩúÂØπ‰Ω†ÊúâÂ∏ÆÂä©ÔºåÊ¨¢ËøéÂºïÁî®ÔºÅ\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@article{qwen,\n  title={Qwen Technical Report},\n  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},\n  journal={arXiv preprint arXiv:2309.16609},\n  year={2023}\n}\n```\n<br>\n\n## ‰ΩøÁî®ÂçèËÆÆÔºàLicense AgreementÔºâ\n\nÊàë‰ª¨ÁöÑ‰ª£Á†ÅÂíåÊ®°ÂûãÊùÉÈáçÂØπÂ≠¶ÊúØÁ†îÁ©∂ÂÆåÂÖ®ÂºÄÊîæÔºåÂπ∂ÊîØÊåÅÂïÜÁî®„ÄÇËØ∑Êü•Áúã[LICENSE](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT)‰∫ÜËß£ÂÖ∑‰ΩìÁöÑÂºÄÊ∫êÂçèËÆÆÁªÜËäÇ„ÄÇÂ¶ÇÈúÄÂïÜÁî®ÔºåËØ∑Â°´ÂÜô[ÈóÆÂç∑](https://dashscope.console.aliyun.com/openModelApply/qianwen)Áî≥ËØ∑„ÄÇ\n\nOur code and checkpoints are open to research purpose, and they are allowed for commercial purposes. Check [LICENSE](https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT) for more details about the license. If you have requirements for commercial use, please fill out the [form](https://dashscope.console.aliyun.com/openModelApply/qianwen) to apply.\n<br>\n\n## ËÅîÁ≥ªÊàë‰ª¨ÔºàContact UsÔºâ\n\nÂ¶ÇÊûú‰Ω†ÊÉ≥ÁªôÊàë‰ª¨ÁöÑÁ†îÂèëÂõ¢ÈòüÂíå‰∫ßÂìÅÂõ¢ÈòüÁïôË®ÄÔºåÊ¨¢ËøéÂä†ÂÖ•Êàë‰ª¨ÁöÑÂæÆ‰ø°Áæ§„ÄÅÈíâÈíâÁæ§‰ª•ÂèäDiscordÔºÅÂêåÊó∂Ôºå‰πüÊ¨¢ËøéÈÄöËøáÈÇÆ‰ª∂Ôºàqianwen_opensource@alibabacloud.comÔºâËÅîÁ≥ªÊàë‰ª¨„ÄÇ\n\nIf you are interested to leave a message to either our research team or product team, join our Discord or WeChat groups! Also, feel free to send an email to qianwen_opensource@alibabacloud.com.\n\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7721324544,"storage_bytes":46329475248,"files_count":30,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["QWenLMHeadModel"],"auto_map":{"AutoConfig":"configuration_qwen.QWenConfig","AutoModelForCausalLM":"modeling_qwen.QWenLMHeadModel"},"model_type":"qwen","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:Dao-AILab:flash-attention","source_url":"https://github.com/Dao-AILab/flash-attention"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:PanQiWei:AutoGPTQ","source_url":"https://github.com/PanQiWei/AutoGPTQ"},{"type":"has_code","target_id":"github:PanQiWei:AutoGPTQ","source_url":"https://github.com/PanQiWei/AutoGPTQ"},{"type":"has_code","target_id":"github:PanQiWei:AutoGPTQ","source_url":"https://github.com/PanQiWei/AutoGPTQ"},{"type":"has_code","target_id":"github:PanQiWei:AutoGPTQ","source_url":"https://github.com/PanQiWei/AutoGPTQ"},{"type":"has_code","target_id":"github:openai:tiktoken","source_url":"https://github.com/openai/tiktoken"},{"type":"has_code","target_id":"github:openai:tiktoken","source_url":"https://github.com/openai/tiktoken"},{"type":"has_code","target_id":"github:openai:human-eval","source_url":"https://github.com/openai/human-eval"},{"type":"has_code","target_id":"github:openai:human-eval","source_url":"https://github.com/openai/human-eval"},{"type":"has_code","target_id":"github:openai:grade-school-math","source_url":"https://github.com/openai/grade-school-math"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Agent","source_url":"https://github.com/QwenLM/Qwen-Agent"},{"type":"has_code","target_id":"github:QwenLM:Qwen-Agent","source_url":"https://github.com/QwenLM/Qwen-Agent"},{"type":"has_code","target_id":"github:openvinotoolkit:openvino_notebooks","source_url":"https://github.com/openvinotoolkit/openvino_notebooks"},{"type":"has_code","target_id":"github:openvinotoolkit:openvino_notebooks","source_url":"https://github.com/openvinotoolkit/openvino_notebooks"},{"type":"has_code","target_id":"github:openvinotoolkit:openvino_notebooks","source_url":"https://github.com/openvinotoolkit/openvino_notebooks"},{"type":"has_code","target_id":"github:openvinotoolkit:openvino_notebooks","source_url":"https://github.com/openvinotoolkit/openvino_notebooks"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"has_code","target_id":"github:QwenLM:Qwen","source_url":"https://github.com/QwenLM/Qwen"},{"type":"based_on_paper","target_id":"arxiv:2309.16609","source_url":"https://arxiv.org/abs/2309.16609"},{"type":"based_on_paper","target_id":"arxiv:2305.08322","source_url":"https://arxiv.org/abs/2305.08322"},{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:2305.05280","source_url":"https://arxiv.org/abs/2305.05280"},{"type":"based_on_paper","target_id":"arxiv:2210.03629","source_url":"https://arxiv.org/abs/2210.03629"}]', NULL, 'Other', 'approved', 98.9, '3c218bdb581696643fdc5a28684cae60', NULL, 'https://huggingface.co/Qwen/Qwen-7B-Chat/resolve/main/assets/code_interpreter_showcase_001.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-Qwen-Qwen-7B-Chat from https://huggingface.co/Qwen/Qwen-7B-Chat/resolve/main/assets/code_interpreter_showcase_001.jpg
Image converted to WebP: data/images/huggingface-Qwen-Qwen-7B-Chat.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-medgemma-4b-it', 'huggingface--google--medgemma-4b-it', 'medgemma-4b-it', 'google', '', '["transformers","safetensors","gemma3","any-to-any","medical","radiology","clinical-reasoning","dermatology","pathology","ophthalmology","chest-x-ray","image-text-to-text","conversational","arxiv:2303.15343","arxiv:2507.05201","arxiv:2405.03162","arxiv:2106.14463","arxiv:2412.03555","arxiv:2501.19393","arxiv:2009.13081","arxiv:2102.09542","arxiv:2411.15640","arxiv:2404.05590","arxiv:2501.18362","base_model:google/medgemma-4b-pt","base_model:finetune:google/medgemma-4b-pt","license:other","text-generation-inference","endpoints_compatible","region:us"]', 'image-text-to-text', 784, 596852, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/medgemma-4b-it","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":4300079472,"storage_bytes":13599603276,"files_count":15,"spaces_count":100,"gated":"auto","private":false,"config":{"architectures":["Gemma3ForConditionalGeneration"],"model_type":"gemma3","tokenizer_config":{"bos_token":"<bos>","chat_template":"{{ bos_token }}\n{%- if messages[0][''role''] == ''system'' -%}\n    {%- if messages[0][''content''] is string -%}\n        {%- set first_user_prefix = messages[0][''content''] + ''\n\n'' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0][''content''][0][''text''] + ''\n\n'' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message[''role''] == ''assistant'') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message[''role''] -%}\n    {%- endif -%}\n    {{ ''<start_of_turn>'' + role + ''\n'' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message[''content''] is string -%}\n        {{ message[''content''] | trim }}\n    {%- elif message[''content''] is iterable -%}\n        {%- for item in message[''content''] -%}\n            {%- if item[''type''] == ''image'' -%}\n                {{ ''<start_of_image>'' }}\n            {%- elif item[''type''] == ''text'' -%}\n                {{ item[''text''] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ ''<end_of_turn>\n'' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{''<start_of_turn>model\n''}}\n{%- endif -%}\n","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false},"chat_template_jinja":"{{ bos_token }}\n{%- if messages[0][''role''] == ''system'' -%}\n    {%- if messages[0][''content''] is string -%}\n        {%- set first_user_prefix = messages[0][''content''] + ''\n\n'' -%}\n    {%- else -%}\n        {%- set first_user_prefix = messages[0][''content''][0][''text''] + ''\n\n'' -%}\n    {%- endif -%}\n    {%- set loop_messages = messages[1:] -%}\n{%- else -%}\n    {%- set first_user_prefix = \"\" -%}\n    {%- set loop_messages = messages -%}\n{%- endif -%}\n{%- for message in loop_messages -%}\n    {%- if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) -%}\n        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n    {%- endif -%}\n    {%- if (message[''role''] == ''assistant'') -%}\n        {%- set role = \"model\" -%}\n    {%- else -%}\n        {%- set role = message[''role''] -%}\n    {%- endif -%}\n    {{ ''<start_of_turn>'' + role + ''\n'' + (first_user_prefix if loop.first else \"\") }}\n    {%- if message[''content''] is string -%}\n        {{ message[''content''] | trim }}\n    {%- elif message[''content''] is iterable -%}\n        {%- for item in message[''content''] -%}\n            {%- if item[''type''] == ''image'' -%}\n                {{ ''<start_of_image>'' }}\n            {%- elif item[''type''] == ''text'' -%}\n                {{ item[''text''] | trim }}\n            {%- endif -%}\n        {%- endfor -%}\n    {%- else -%}\n        {{ raise_exception(\"Invalid content type\") }}\n    {%- endif -%}\n    {{ ''<end_of_turn>\n'' }}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{''<start_of_turn>model\n''}}\n{%- endif -%}\n"}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2303.15343","source_url":"https://arxiv.org/abs/2303.15343"},{"type":"based_on_paper","target_id":"arxiv:2507.05201","source_url":"https://arxiv.org/abs/2507.05201"},{"type":"based_on_paper","target_id":"arxiv:2405.03162","source_url":"https://arxiv.org/abs/2405.03162"},{"type":"based_on_paper","target_id":"arxiv:2106.14463","source_url":"https://arxiv.org/abs/2106.14463"},{"type":"based_on_paper","target_id":"arxiv:2412.03555","source_url":"https://arxiv.org/abs/2412.03555"},{"type":"based_on_paper","target_id":"arxiv:2501.19393","source_url":"https://arxiv.org/abs/2501.19393"},{"type":"based_on_paper","target_id":"arxiv:2009.13081","source_url":"https://arxiv.org/abs/2009.13081"},{"type":"based_on_paper","target_id":"arxiv:2102.09542","source_url":"https://arxiv.org/abs/2102.09542"},{"type":"based_on_paper","target_id":"arxiv:2411.15640","source_url":"https://arxiv.org/abs/2411.15640"},{"type":"based_on_paper","target_id":"arxiv:2404.05590","source_url":"https://arxiv.org/abs/2404.05590"},{"type":"based_on_paper","target_id":"arxiv:2501.18362","source_url":"https://arxiv.org/abs/2501.18362"}]', NULL, 'Other', 'approved', 38.9, '3da5a507dff9ebcdc3056b02857500c5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nitrosocke-Ghibli-Diffusion', 'huggingface--nitrosocke--ghibli-diffusion', 'Ghibli-Diffusion', 'nitrosocke', '--- language: - en license: creativeml-openrail-m thumbnail: "https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-thumbnail.jpg" tags: - stable-diffusion - text-to-image - image-to-image - diffusers --- This is the fine-tuned Stable Diffusion model trained on images from modern anime feature films from Studio Ghibli. Use the tokens **_ghibli style_** in your prompts for the effect. **If you enjoy my work and want to test new models before release, please co...', '["diffusers","safetensors","stable-diffusion","text-to-image","image-to-image","en","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 782, 2909, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nitrosocke/Ghibli-Diffusion","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlanguage:\n- en\nlicense: creativeml-openrail-m\nthumbnail: "https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-thumbnail.jpg"\ntags:\n- stable-diffusion\n- text-to-image\n- image-to-image\n- diffusers\n\n---\n### Ghibli Diffusion\n\nThis is the fine-tuned Stable Diffusion model trained on images from modern anime feature films from Studio Ghibli.\nUse the tokens **_ghibli style_** in your prompts for the effect.\n\n**If you enjoy my work and want to test new models before release, please consider supporting me**\n[![Become A Patreon](https://badgen.net/badge/become/a%20patron/F96854)](https://patreon.com/user?u=79196446)\n\n**Characters rendered with the model:**\n![Characters Samples](https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-samples-01s.jpg)\n**Cars and Animals rendered with the model:**\n![Misc. Samples](https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-samples-02s.jpg)\n**Landscapes rendered with the model:**\n![Landscape 1](https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-samples-03s.jpg)\n_ghibli style beautiful Caribbean beach tropical (sunset) - Negative prompt: soft blurry_\n![Landscape 2](https://huggingface.co/nitrosocke/Ghibli-Diffusion/resolve/main/images/ghibli-diffusion-samples-04s.jpg)\n_ghibli style ice field white mountains ((northern lights)) starry sky low horizon - Negative prompt: soft blurry_\n\n#### Prompt and settings for the Strom Trooper:\n**ghibli style (storm trooper) Negative prompt: (bad anatomy)**\n_Steps: 20, Sampler: DPM++ 2M Karras, CFG scale: 7, Seed: 3450349066, Size: 512x704_\n\n#### Prompt and settings for the VW Beetle:\n**ghibli style VW beetle Negative prompt: soft blurry**\n_Steps: 30, Sampler: Euler a, CFG scale: 7, Seed: 1529856912, Size: 704x512_\n\nThis model was trained using the diffusers based dreambooth training by ShivamShrirao using prior-preservation loss and the _train-text-encoder_ flag in 15.000 steps.\n\n<!-- ### Gradio\n\nWe support a [Gradio](https://github.com/gradio-app/gradio) Web UI run redshift-diffusion:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/nitrosocke/Ghibli-Diffusion-Demo)-->\n\n### üß® Diffusers\n\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\n\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\n\n```python\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = "nitrosocke/Ghibli-Diffusion"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to("cuda")\n\nprompt = "ghibli style magical princess with golden hair"\nimage = pipe(prompt).images[0]\n\nimage.save("./magical_princess.png")\n```\n\n## License\n\nThis model is open access and available to all, with a CreativeML OpenRAIL-M license further specifying rights and usage.\nThe CreativeML OpenRAIL License specifies: \n\n1. You can''t use the model to deliberately produce nor share illegal or harmful outputs or content \n2. The authors claims no rights on the outputs you generate, you are free to use them and are accountable for their use which must not go against the provisions set in the license\n3. You may re-distribute the weights and use the model commercially and/or as a service. If you do, please be aware you have to include the same use restrictions as the ones in the license and share a copy of the CreativeML OpenRAIL-M to all your users (please read the license entirely and carefully)\n[Please read the full license here](https://huggingface.co/spaces/CompVis/stable-diffusion-license)', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":20376154998,"files_count":28,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"}]', NULL, 'creativeml-openrail-m', 'approved', 63.9, 'a35bd9a701739533fc86b212e9ef495a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Alibaba-NLP-Tongyi-DeepResearch-30B-A3B', 'huggingface--alibaba-nlp--tongyi-deepresearch-30b-a3b', 'Tongyi-DeepResearch-30B-A3B', 'Alibaba-NLP', '--- license: apache-2.0 language: - en pipeline_tag: text-generation library_name: transformers --- We present **Tongyi DeepResearch**, an agentic large language model featuring 30 billion total parameters, with only 3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for **long-horizon, deep information-seeking** tasks. Tongyi-DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity''s Last Ex...', '["transformers","safetensors","qwen3_moe","text-generation","conversational","en","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'text-generation', 779, 13287, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\npipeline_tag: text-generation\nlibrary_name: transformers\n---\n\n# Introduction\n\nWe present  **Tongyi DeepResearch**, an agentic large language model featuring 30 billion total parameters, with only 3 billion activated per token. Developed by Tongyi Lab, the model is specifically designed for **long-horizon, deep information-seeking** tasks. Tongyi-DeepResearch demonstrates state-of-the-art performance across a range of agentic search benchmarks, including Humanity''s Last Exam, BrowserComp, BrowserComp-ZH, WebWalkerQA, GAIA, xbench-DeepSearch and FRAMES.\n\nMore details can be found in our üì∞ [Tech Blog](https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research).\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/63fc4c00a3c067e62899d32b/OhQCYYJu1LhrS446Qct5D.png)\n\n## Key Features\n\n- ‚öôÔ∏è **Fully automated synthetic data generation pipeline**: We design a highly scalable data synthesis pipeline, which is fully automatic and empowers agentic pre-training, supervised fine-tuning, and reinforcement learning.\n- üîÑ **Large-scale continual pre-training on agentic data**: Leveraging diverse, high-quality agentic interaction data to extend model capabilities, maintain freshness, and strengthen reasoning performance.\n- üîÅ **End-to-end reinforcement learning**: We employ a strictly on-policy RL approach based on a customized Group Relative Policy Optimization framework, with token-level policy gradients, leave-one-out advantage estimation, and selective filtering of negative samples to stabilize training in a non‚Äëstationary environment.\n- ü§ñ **Agent Inference Paradigm Compatibility**: At inference, Tongyi-DeepResearch is compatible with two inference paradigms: ReAct, for rigorously evaluating the model''s core intrinsic abilities, and an IterResearch-based ''Heavy'' mode, which uses a test-time scaling strategy to unlock the model''s maximum performance ceiling.\n\n## Download\n\nYou can download the model then run the inference scipts in https://github.com/Alibaba-NLP/DeepResearch.\n\n\n```bibtex\n@misc{tongyidr,\n  author={Tongyi DeepResearch Team},\n  title={Tongyi DeepResearch: A New Era of Open-Source AI Researchers},\n  year={2025},\n  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":30532122624,"storage_bytes":61077998390,"files_count":27,"spaces_count":3,"gated":false,"private":false,"config":{"architectures":["Qwen3MoeForCausalLM"],"model_type":"qwen3_moe","tokenizer_config":{"bos_token":null,"eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null},"chat_template_jinja":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(''<tool_response>'') and message.content.endswith(''</tool_response>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.reasoning_content is string %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if ''</think>'' in content %}\n                {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n                {%- set content = content.split(''</think>'')[-1].lstrip(''\\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- ''<|im_start|>'' + message.role + ''\\n<think>\\n'' + reasoning_content.strip(''\\n'') + ''\\n</think>\\n\\n'' + content.lstrip(''\\n'') }}\n            {%- else %}\n                {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- ''<think>\\n\\n</think>\\n\\n'' }}\n    {%- endif %}\n{%- endif %}"}}', '[]', '[{"type":"has_code","target_id":"github:Alibaba-NLP:DeepResearch.","source_url":"https://github.com/Alibaba-NLP/DeepResearch."},{"type":"has_code","target_id":"github:Alibaba-NLP:DeepResearch}}","source_url":"https://github.com/Alibaba-NLP/DeepResearch}}"}]', NULL, 'Apache-2.0', 'approved', 63.9, '58fc596c773384438f639bb6b5b7656c', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-timesfm-1.0-200m', 'huggingface--google--timesfm-1.0-200m', 'timesfm-1.0-200m', 'google', '--- license: apache-2.0 library_name: timesfm pipeline_tag: time-series-forecasting --- TimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting. **Resources and Technical Documentation**: * Paper: A decoder-only foundation model for time-series forecasting, to appear in ICML 2024. * Google Research blog * GitHub repo **Authors**: Google Research This is not an officially supported Google product. is the first...', '["timesfm","time-series-forecasting","arxiv:2310.10688","license:apache-2.0","region:us"]', 'time-series-forecasting', 776, 1959, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/timesfm-1.0-200m","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlibrary_name: timesfm\npipeline_tag: time-series-forecasting\n---\n\n# TimesFM\n\nTimesFM (Time Series Foundation Model) is a pretrained time-series foundation model developed by Google Research for time-series forecasting.\n\n**Resources and Technical Documentation**:\n\n* Paper: [A decoder-only foundation model for time-series forecasting](https://arxiv.org/abs/2310.10688), to appear in ICML 2024.\n* [Google Research blog](https://research.google/blog/a-decoder-only-foundation-model-for-time-series-forecasting/)\n* [GitHub repo](https://github.com/google-research/timesfm)\n\n**Authors**: Google Research\n\nThis is not an officially supported Google product.\n\n## Checkpoint timesfm-1.0-200m\n\n`timesfm-1.0-200m` is the first open model checkpoint:\n\n- It performs univariate time series forecasting for context lengths up to 512 time points and any horizon lengths, with an optional frequency indicator.\n- It focuses on point forecasts and does not support probabilistic forecasts. We experimentally offer quantile heads but they have not been calibrated after pretraining.\n- It requires the context to be contiguous (i.e. no "holes"), and the context and the horizon to be of the same frequency.\n\n## Benchmarks\n\nPlease refer to our result tables on the [extended benchmarks](https://github.com/google-research/timesfm/blob/master/experiments/extended_benchmarks/tfm_results.png) and the [long horizon benchmarks](https://github.com/google-research/timesfm/blob/master/experiments/long_horizon_benchmarks/tfm_long_horizon.png).\n\nPlease look into the README files in the respective benchmark directories within `experiments/` for instructions for running TimesFM on the respective benchmarks.\n\n## Installation\n\nThis HuggingFace repo hosts TimesFm checkpoints. Please visit our [GitHub repo](https://github.com/google-research/timesfm) and follow the instructions there to install the `timesfm` library for model inference.\n\nIn particular, the dependency `lingvo` does not support ARM architectures and the inference code is not working for machines with Apple silicon. We are aware of this issue and are working on a solution. Stay tuned.\n\n## Usage \n\n### Initialize the model and load a checkpoint.\nThen the base class can be loaded as,\n\n```python\nimport timesfm\n\ntfm = timesfm.TimesFm(\n    context_len=<context>,\n    horizon_len=<horizon>,\n    input_patch_len=32,\n    output_patch_len=128,\n    num_layers=20,\n    model_dims=1280,\n    backend=<backend>,\n)\ntfm.load_from_checkpoint(repo_id="google/timesfm-1.0-200m")\n```\n\nNote that the four parameters are fixed to load the 200m model\n\n```python\ninput_patch_len=32,\noutput_patch_len=128,\nnum_layers=20,\nmodel_dims=1280,\n```\n\n1. The context_len here can be set as the max context length **of the model**. You can provide a shorter series to the `tfm.forecast()` function and the model will handle it. Currently, the model handles a max context length of 512, which can be increased in later releases. The input time series can have **any context length**. Padding / truncation will be handled by the inference code if needed.\n\n2. The horizon length can be set to anything. We recommend setting it to the largest horizon length you would need in the forecasting tasks for your application. We generally recommend horizon length <= context length but it is not a requirement in the function call.\n\n### Perform inference\n\nWe provide APIs to forecast from either array inputs or `pandas` dataframe. Both forecast methods expect (1) the input time series contexts, (2) along with their frequencies. Please look at the documentation of the functions `tfm.forecast()` and `tfm.forecast_on_df()` for detailed instructions.\n\nIn particular, regarding the frequency, TimesFM expects a categorical indicator valued in {0, 1, 2}:\n\n- **0** (default): high frequency, long horizon time series. We recommend using this for time series up to daily granularity.\n- **1**: medium frequency time series. We recommend using this for weekly and monthly data.\n- **2**: low frequency, short horizon time series. We recommend using this for anything beyond monthly, e.g. quarterly or yearly.\n\nThis categorical value should be directly provided with the array inputs. For dataframe inputs, we convert the conventional letter coding of frequencies to our expected categories, that\n\n- **0**: T, MIN, H, D, B, U\n- **1**: W, M\n- **2**: Q, Y\n\nNotice you do **NOT** have to strictly follow our recommendation here. Although this is our setup during model training and we expect it to offer the best forecast result, you can also view the frequency input as a free parameter and modify it per your specific use case.\n\n\nExamples:\n\nArray inputs, with the frequencies set to low, medium, and high respectively.\n\n```python\nimport numpy as np\nforecast_input = [\n    np.sin(np.linspace(0, 20, 100))\n    np.sin(np.linspace(0, 20, 200)),\n    np.sin(np.linspace(0, 20, 400)),\n]\nfrequency_input = [0, 1, 2]\n\npoint_forecast, experimental_quantile_forecast = tfm.forecast(\n    forecast_input,\n    freq=frequency_input,\n)\n```\n\n`pandas` dataframe, with the frequency set to "M" monthly.\n\n```python\nimport pandas as pd\n\n# e.g. input_df is\n#       unique_id  ds          y\n# 0     T1         1975-12-31  697458.0\n# 1     T1         1976-01-31  1187650.0\n# 2     T1         1976-02-29  1069690.0\n# 3     T1         1976-03-31  1078430.0\n# 4     T1         1976-04-30  1059910.0\n# ...   ...        ...         ...\n# 8175  T99        1986-01-31  602.0\n# 8176  T99        1986-02-28  684.0\n# 8177  T99        1986-03-31  818.0\n# 8178  T99        1986-04-30  836.0\n# 8179  T99        1986-05-31  878.0\n\nforecast_df = tfm.forecast_on_df(\n    inputs=input_df,\n    freq="M",  # monthly\n    value_name="y",\n    num_jobs=-1,\n)\n```', '{"pipeline_tag":"time-series-forecasting","library_name":"timesfm","framework":"timesfm","params":null,"storage_bytes":814291152,"files_count":5,"spaces_count":4,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:google-research:timesfm","source_url":"https://github.com/google-research/timesfm"},{"type":"has_code","target_id":"github:google-research:timesfm","source_url":"https://github.com/google-research/timesfm"},{"type":"has_code","target_id":"github:google-research:timesfm","source_url":"https://github.com/google-research/timesfm"},{"type":"has_code","target_id":"github:google-research:timesfm","source_url":"https://github.com/google-research/timesfm"},{"type":"based_on_paper","target_id":"arxiv:2310.10688","source_url":"https://arxiv.org/abs/2310.10688"}]', NULL, 'Apache-2.0', 'approved', 63.9, '787c584aa80c6e00bc16f8ee24933613', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nvidia-NVLM-D-72B', 'huggingface--nvidia--nvlm-d-72b', 'NVLM-D-72B', 'nvidia', '--- license: cc-by-nc-4.0 language: - en pipeline_tag: image-text-to-text tags: - nvidia - NVLM - pytorch - multimodal - conversational library_name: transformers --- <p align="center"> <img src="nvlm-logo-light.png" alt="Image Description" width="300" > </p> This family of models performs vision-language and text-only tasks including optical character recognition, multimodal reasoning, localization, common sense reasoning, world knowledge utilization, and coding. This model is ready for non-...', '["transformers","safetensors","nvlm_d","nvidia","nvlm","pytorch","multimodal","conversational","image-text-to-text","custom_code","en","arxiv:2409.11402","license:cc-by-nc-4.0","endpoints_compatible","region:us"]', 'image-text-to-text', 775, 54834, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nvidia/NVLM-D-72B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: cc-by-nc-4.0\nlanguage:\n- en\npipeline_tag: image-text-to-text\ntags:\n- nvidia\n- NVLM\n- pytorch\n- multimodal\n- conversational\nlibrary_name: transformers\n---\n\n<p align="center">\n  <img src="nvlm-logo-light.png" alt="Image Description" width="300" >\n</p>\n\n\n# Model Overview\n\n## Description\nThis family of models performs vision-language and text-only tasks including optical character recognition, multimodal reasoning, localization, common sense reasoning, world knowledge utilization, and coding.\n\nThis model is ready for non-commercial use.\n\n## License/Terms of Use\n\nGoverning Terms: Deed - [Attribution-NonCommercial 4.0 International - Creative Commons](https://creativecommons.org/licenses/by-nc/4.0/deed.en).\n\nAdditional Information: [LICENSE ¬∑ Qwen/Qwen2-72B-Instruct at main](https://huggingface.co/Qwen/Qwen2-72B-Instruct/blob/main/LICENSE) for Qwen2-72B-Instruct and [The MIT License ‚Äì Open Source Initiative](https://opensource.org/license/mit) for InternViT-6B-448px-V1-2.\n\n# Model Details\n\nToday (September 17th, 2024), we introduce [NVLM 1.0](https://arxiv.org/abs/2409.11402), a family of frontier-class multimodal large language models (LLMs) that achieve state-of-the-art results on vision-language tasks, rivaling the leading proprietary models (e.g., GPT-4o) and open-access models (e.g., Llama 3-V 405B and InternVL 2). Remarkably, NVLM 1.0 shows improved text-only performance over its LLM backbone after multimodal training. \n\nIn this repo, we are open-sourcing NVLM-1.0-D-72B (decoder-only architecture), the decoder-only model weights and code for the community.\n\n\n\n## Reference(s)\n[Paper](https://arxiv.org/abs/2409.11402) &ensp; [Inference Code (HF)](https://huggingface.co/nvidia/NVLM-D-72B/tree/main) &ensp; [Training Code](https://github.com/NVIDIA/Megatron-LM/tree/NVLM-1.0/examples/multimodal/nvlm) &ensp; [Website](https://research.nvidia.com/labs/adlr/NVLM-1/) \n\n## Benchmark Results\nWe train our model with legacy [Megatron-LM](https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/legacy) and adapt the codebase to Huggingface for model hosting, reproducibility, and inference.\nWe observe numerical differences between the Megatron and Huggingface codebases, which are within the expected range of variation. \nWe provide the results from both the Huggingface codebase and the Megatron codebase for reproducibility and comparison with other models.\n\nResults (as of September 17th, 2024) in the multimodal benchmarks are as follows:\n\n### Vision-language Benchmarks \n\n| Benchmark                    | MMMU (val / test) | MathVista | OCRBench | AI2D | ChartQA | DocVQA | TextVQA | RealWorldQA | VQAv2 |\n|------------------------------|-------------------|-----------|----------|------|---------|--------|---------|-------------|-------|\n| NVLM-D 1.0 72B (Huggingface) | 58.7 / 54.9       | 65.2      | 852      | 94.2 | 86.0    | 92.6   | 82.6    | 69.5        | 85.4  |\n| NVLM-D 1.0 72B (Megatron)    | 59.7 / 54.6       | 65.2      | 853      | 94.2 | 86.0    | 92.6   | 82.1    | 69.7        | 85.4  |\n| Llama 3.2 90B                | 60.3 / -          | 57.3      | -        | 92.3 | 85.5    | 90.1   | -       | -           | 78.1  |\n| Llama 3-V 70B                | 60.6 / -          | -         | -        | 93.0 | 83.2    | 92.2   | 83.4    | -           | 79.1  |\n| Llama 3-V 405B               | 64.5 / -          | -         | -        | 94.1 | 85.8    | 92.6   | 84.8    | -           | 80.2  |\n| InternVL2-Llama3-76B         | 55.2 / -          | 65.5      | 839      | 94.8 | 88.4    | 94.1   | 84.4    | 72.2        | -     |\n| GPT-4V                       | 56.8 / 55.7       | 49.9      | 645      | 78.2 | 78.5    | 88.4   | 78.0    | 61.4        | 77.2  |\n| GPT-4o                       | 69.1 / -          | 63.8      | 736      | 94.2 | 85.7    | 92.8   | -       | -           | -     |\n| Claude 3.5 Sonnet            | 68.3 / -          | 67.7      | 788      | 94.7 | 90.8    | 95.2   | -       | -           | -     |\n| Gemini 1.5 Pro (Aug 2024)    | 62.2 / -          | 63.9      | 754      | 94.4 | 87.2    | 93.1   | 78.7    | 70.4        | 80.2  |\n\n### Text-only Benchmarks\n\n| Tasks                        | Backbone LLM | MMLU | GSM8K | MATH | HumanEval | Avg. Accuracy    |\n|------------------------------|--------------|------|-------|------|-----------|------------------|\n| **Proprietary**              |              |      |       |      |           |                  |\n| GPT-4.0                      | N/A          | 88.7 | -     | 76.6 | 90.2      | -                |\n| Gemini Pro 1.5 (Aug 2024)    | N/A          | 85.9 | 90.8  | 67.7 | 84.1      | 82.1             |\n| Claude 3.5 Sonnet            | N/A          | 88.7 | 96.4  | 71.1 | 92.0      | 87.0             |\n| **Open LLM**                 |              |      |       |      |           |                  |\n| (a) Nous-Hermes-2-Yi-34B     | N/A          | 75.5 | 78.6  | 21.8 | 43.3      | 54.8             |\n| (b) Qwen-72B-Instruct        | N/A          | 82.3 | 91.1  | 59.7 | 86.0      | 79.8             |\n| (c) Llama-3-70B-Instruct     | N/A          | 82.0 | 93.0  | 51.0 | 81.7      | 76.6             |\n| (d) Llama-3.1-70B-Instruct   | N/A          | 83.6 | 95.1  | 68.0 | 80.5      | 81.8             |\n| (e) Llama-3.1-405B-Instruct  | N/A          | 87.3 | 96.8  | 73.8 | 89.0      | 86.7             |\n| **Open Multimodal LLM**      |              |      |       |      |           |                  |\n| VILA-1.5 40B                 | (a)          | 73.3 | 67.5  | 16.8 | 34.1      | ü•∂ 47.9   (-6.9) |\n| LLaVA-OneVision 72B          | (b)          | 80.6 | 89.9  | 49.2 | 74.4      | ü•∂ 73.5   (-6.3) |\n| InternVL-2-Llama3-76B        | (c)          | 78.5 | 87.1  | 42.5 | 71.3      | ü•∂ 69.9   (-6.7) |\n| *Llama 3-V 70B               | (d)          | 83.6 | 95.1  | 68.0 | 80.5      | üôÇ 81.8   (0)    |\n| *Llama 3-V 405B              | (e)          | 87.3 | 96.8  | 73.8 | 89.0      | üôÇ 86.7   (0)    |\n| NVLM-D 1.0 72B (Megatron)    | (b)          | 82.0 | 92.9  | 73.1 | 88.4      | ü•≥ 84.1   (+4.3) |\n| NVLM-D 1.0 72B (Huggingface) | (b)          | 81.7 | 93.2  | 73.1 | 89.0      | ü•≥ 84.3   (+4.5) |\n\n\n## Model Architectures\n\n**Network Architecture:** Decoder-Only Transformer\n\n**Text-only LLM backbone:** [Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct)\n\n**Vision encoder:** [InternViT-6B](https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-2)\n\n### Robustness\n\nThe model trained on this dataset cannot regenerate its training data:\n\n1. The model has no image generation capability since its output is only text. Hence it cannot regenerate any image it would have seen during training.\n\n2. The model cannot regenerate training text data: during training, the model takes text and images as inputs, and the model output (text) is conditioned on both inputs. During inference, without training images as input, the models would not be able to reproduce any part of the training text data.\n\n\n### Input\n**Input Type(s):** Text, Image <br>\n**Input Format(s):** String, [Pillow Library-Supported Formats](https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html) <br>\n**Input Dimensions:** One-Dimensional (1D), Two Dimensional (2D) <br>\n**Other Properties Related to Input:** Maximum Token Length = 128K Tokens <br>\n\n### Output\n**Output Type(s):** Text <br>\n**Output Format:** String <br>\n**Model Output:** 1D <br>\n**Other Properties Related to Output:** None <br> \n\n## How to use\n\nWhen converting Megatron checkpoint to Huggingface, we adapt [InternVL codebase](https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B) to support model loading and multi-GPU inference in HF. \nWe also use the tokenizer from [Qwen2.5-72B-Instruct](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct/tree/main) when adapting the tokenizer to Huggingface, as it contains extra special tokens for vision tasks, e.g., `<|vision_pad|>`. \nWe train NVLM-1.0-D-72B based on the [Qwen2-72B-Instruct](https://huggingface.co/Qwen/Qwen2-72B-Instruct/tree/main) text-only model and [InternViT-6B-448px-V1-5](https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5) ViT model with our large-scale high-quality multimodal dataset. \nFor training code, please refer to [Megatron-Core](https://github.com/NVIDIA/Megatron-LM/tree/NVLM-1.0/examples/multimodal/nvlm).\n\n\n### Prepare the environment\n\nWe provide a docker build file in the [Dockerfile](Dockerfile) for reproduction. \n\nThe docker image is based on `nvcr.io/nvidia/pytorch:23.09-py3`. \n\n*Note: We observe that different transformer versions / CUDA versions / docker versions can lead to slight benchmark number differences. We recommend using the Dockerfile above for precise reproduction.*\n\n### Model loading\n\n```python\nimport torch\nfrom transformers import AutoModel\n\npath = "nvidia/NVLM-D-72B"\nmodel = AutoModel.from_pretrained(\n    path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    use_flash_attn=False,\n    trust_remote_code=True).eval()\n```\n\n### Multiple GPUs\n\nThe model can be loaded on multiple GPUs as follows:\n\n```python\nimport torch\nimport math\nfrom transformers import AutoModel\n\ndef split_model():\n    device_map = {}\n    world_size = torch.cuda.device_count()\n    num_layers = 80\n    # Since the first GPU will be used for ViT, treat it as half a GPU.\n    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n    layer_cnt = 0\n    for i, num_layer in enumerate(num_layers_per_gpu):\n        for j in range(num_layer):\n            device_map[f''language_model.model.layers.{layer_cnt}''] = i\n            layer_cnt += 1\n    device_map[''vision_model''] = 0\n    device_map[''mlp1''] = 0\n    device_map[''language_model.model.tok_embeddings''] = 0\n    device_map[''language_model.model.embed_tokens''] = 0\n    device_map[''language_model.output''] = 0\n    device_map[''language_model.model.norm''] = 0\n    device_map[''language_model.lm_head''] = 0\n    device_map[''language_model.model.rotary_emb''] = 0\n    device_map[f''language_model.model.layers.{num_layers - 1}''] = 0\n\n    return device_map\n\npath = "nvidia/NVLM-D-72B"\ndevice_map = split_model()\nmodel = AutoModel.from_pretrained(\n    path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    use_flash_attn=False,\n    trust_remote_code=True,\n    device_map=device_map).eval()\n```\n\n\n### Inference\n\n```python\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\nimport math\nfrom PIL import Image\nimport torchvision.transforms as T\nfrom torchvision.transforms.functional import InterpolationMode\n\n\ndef split_model():\n    device_map = {}\n    world_size = torch.cuda.device_count()\n    num_layers = 80\n    # Since the first GPU will be used for ViT, treat it as half a GPU.\n    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n    layer_cnt = 0\n    for i, num_layer in enumerate(num_layers_per_gpu):\n        for j in range(num_layer):\n            device_map[f''language_model.model.layers.{layer_cnt}''] = i\n            layer_cnt += 1\n    device_map[''vision_model''] = 0\n    device_map[''mlp1''] = 0\n    device_map[''language_model.model.tok_embeddings''] = 0\n    device_map[''language_model.model.embed_tokens''] = 0\n    device_map[''language_model.output''] = 0\n    device_map[''language_model.model.norm''] = 0\n    device_map[''language_model.lm_head''] = 0\n    device_map[''language_model.model.rotary_emb''] = 0\n    device_map[f''language_model.model.layers.{num_layers - 1}''] = 0\n\n    return device_map\n\n\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\n\n\ndef build_transform(input_size):\n    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n    transform = T.Compose([\n        T.Lambda(lambda img: img.convert(''RGB'') if img.mode != ''RGB'' else img),\n        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize(mean=MEAN, std=STD)\n    ])\n    return transform\n\n\ndef find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n    best_ratio_diff = float(''inf'')\n    best_ratio = (1, 1)\n    area = width * height\n    for ratio in target_ratios:\n        target_aspect_ratio = ratio[0] / ratio[1]\n        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n        if ratio_diff < best_ratio_diff:\n            best_ratio_diff = ratio_diff\n            best_ratio = ratio\n        elif ratio_diff == best_ratio_diff:\n            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n                best_ratio = ratio\n    return best_ratio\n\n\ndef dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n    orig_width, orig_height = image.size\n    aspect_ratio = orig_width / orig_height\n\n    # calculate the existing image aspect ratio\n    target_ratios = set(\n        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n        i * j <= max_num and i * j >= min_num)\n    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n\n    # find the closest aspect ratio to the target\n    target_aspect_ratio = find_closest_aspect_ratio(\n        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n\n    # calculate the target width and height\n    target_width = image_size * target_aspect_ratio[0]\n    target_height = image_size * target_aspect_ratio[1]\n    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n\n    # resize the image\n    resized_img = image.resize((target_width, target_height))\n    processed_images = []\n    for i in range(blocks):\n        box = (\n            (i % (target_width // image_size)) * image_size,\n            (i // (target_width // image_size)) * image_size,\n            ((i % (target_width // image_size)) + 1) * image_size,\n            ((i // (target_width // image_size)) + 1) * image_size\n        )\n        # split the image\n        split_img = resized_img.crop(box)\n        processed_images.append(split_img)\n    assert len(processed_images) == blocks\n    if use_thumbnail and len(processed_images) != 1:\n        thumbnail_img = image.resize((image_size, image_size))\n        processed_images.append(thumbnail_img)\n    return processed_images\n\n\ndef load_image(image_file, input_size=448, max_num=12):\n    image = Image.open(image_file).convert(''RGB'')\n    transform = build_transform(input_size=input_size)\n    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n    pixel_values = [transform(image) for image in images]\n    pixel_values = torch.stack(pixel_values)\n    return pixel_values\n\npath = "nvidia/NVLM-D-72B"\ndevice_map = split_model()\nmodel = AutoModel.from_pretrained(\n    path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    use_flash_attn=False,\n    trust_remote_code=True,\n    device_map=device_map).eval()\n\nprint(model)\n\ntokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\ngeneration_config = dict(max_new_tokens=1024, do_sample=False)\n\n# pure-text conversation\nquestion = ''Hello, who are you?''\nresponse, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\nprint(f''User: {question}\nAssistant: {response}'')\n\n# single-image single-round conversation\npixel_values = load_image(''path/to/your/example/image.jpg'', max_num=6).to(\n    torch.bfloat16)\nquestion = ''<image>\nPlease describe the image shortly.''\nresponse = model.chat(tokenizer, pixel_values, question, generation_config)\nprint(f''User: {question}\nAssistant: {response}'')\n```\n\n### Benchmark Evaluation\n\nTo test our NVLM-1.0 model on the benchmark datasets, you can use the following code:\n\n```bash\npython run_eval.py --config-path eval/full_eval.yaml \\n --result-save-path path/to/eval_results/ \\n --zero-shot-eval-tasks chartqa coco_caption flickr30k_caption vqav2 mmmu textvqa mathvista mmbench chartqa docvqa realworldqa ocrbench ai2diagram ai2diagram_nomask mmmu_pro docvqa_test\n```\n\nSpecifically,\n- `--config-path eval/full_eval.yaml` file contains the evaluation configurations, including  the evaluation prompt, the evaluation dataset paths, and generation hyper-parameters.\n- `--result-save-path path/to/eval_results/` specifies the path to save the evaluation results.\n- `--zero-shot-eval-tasks` specifies the tasks to evaluate on.\n\n\n## Software Integration\n**Runtime Engine(s)** \n* PyTorch <br>\n\n**Supported Hardware Microarchitecture Compatibility:** <br>\n* NVIDIA Hopper <br>\n\n**[Preferred/Supported] Operating System(s):** <br>\n* Linux <br>\n\n## Inference\n**Engine:** PyTorch <br>\n**Test Hardware:** <br>\n* H100 <br>\n\n## Model Version(s)\n* v1.0-D (NVLM-D)\n\n## Training, Testing, and Evaluation Datasets \n\n### Pre-Training Dataset\n\n**Link** <br>\n* [See Table 4](https://arxiv.org/abs/2409.11402) <br>\n\n**Data Collection Method by dataset** <br>\n* Hybrid: Automated, Human, Synthetic, Unknown <br>\n\n**Labeling Method by dataset** <br>\n* Hybrid: Automated, Human, Synthetic, Unknown <br>\n\n**Properties** \n* Trained on image captions, image-text pairs, natural images, charts, documents, scene descriptions, and mathematical reasoning. <br>\n\n### Supervised Fine-Tuning Dataset\n**Link** <br>\n* [See Table 6](https://arxiv.org/abs/2409.11402) <br>\n\n**Data Collection Method by dataset** <br>\n* Hybrid: Automated, Human, Synthetic, Unknown <br>\n\n**Labeling Method by dataset** <br>\n* Hybrid: Automated, Human, Synthetic, Unknown <br>\n\n**Properties** \n* Trained on image captions; general knowledge; image-text pairs; natural images; charts; diagrams; documents; scene descriptions; science diagrams, lessons, textbook data, and question-answer pairs; visual instruction tuning; and mathematical reasoning. <br>\n\n### Evaluation Dataset\n**Link** <br>\n* [See Section 6.1, "Benchmark"](https://arxiv.org/abs/2409.11402) <br>\n\n**Data collection method by dataset** <br>\n* Human <br>\n\n**Labeling method by dataset** <br>\n* Human <br>\n\n**Properties** <br>\n* Evaluated on general knowledge, visual answering, chart understanding, table, optical character recognition, and mathematical reasoning. <br> \n\n\n## Correspondence to\nWenliang Dai* (wdai@nvidia.com), Nayeon Lee* (nayeonl@nvidia.com), Boxin Wang* (boxinw@nvidia.com), Zhuolin Yang* (zhuoliny@nvidia.com), Wei Ping* (wping@nvidia.com)\n\n*Equal contribution\n\n## Citation\n<pre>\n@article{nvlm2024,\n  title={NVLM: Open Frontier-Class Multimodal LLMs},\n  author={Dai, Wenliang and Lee, Nayeon and Wang, Boxin and Yang, Zhuolin and Liu, Zihan and Barker, Jon and Rintamaki, Tuomas and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},\n  journal={arXiv preprint},\n  year={2024}}\n</pre>\n\n\n## Ethical Considerations\nNVIDIA believes Trustworthy AI is a shared responsibility and we have established policies and practices to enable development for a wide array of AI applications.  When downloaded or used in accordance with our terms of service, developers should work with their supporting model team to ensure this model meets requirements for the relevant industry and use case and addresses unforeseen product misuse. \n\nPlease report security vulnerabilities or NVIDIA AI Concerns [here](https://www.nvidia.com/en-us/support/submit-security-vulnerability/).\n ', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":79379593344,"storage_bytes":354049622570,"files_count":72,"spaces_count":6,"gated":false,"private":false,"config":{"architectures":["NVLM_D"],"auto_map":{"AutoConfig":"configuration_nvlm_d.NVLM_D_Config","AutoModel":"modeling_nvlm_d.NVLM_D_Model","AutoModelForCausalLM":"modeling_nvlm_d.NVLM_D_Model"},"model_type":"NVLM_D","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- messages[0][''content''] }}\n    {%- else %}\n        {{- ''You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0][''role''] == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0][''content''] + ''<|im_end|>\\n'' }}\n    {%- else %}\n        {{- ''<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- ''<|im_start|>'' + message.role }}\n        {%- if message.content %}\n            {{- ''\\n'' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- ''\\n<tool_call>\\n{\"name\": \"'' }}\n            {{- tool_call.name }}\n            {{- ''\", \"arguments\": '' }}\n            {{- tool_call.arguments | tojson }}\n            {{- ''}\\n</tool_call>'' }}\n        {%- endfor %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n{%- endif %}\n","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:NVIDIA:Megatron-LM","source_url":"https://github.com/NVIDIA/Megatron-LM"},{"type":"has_code","target_id":"github:NVIDIA:Megatron-LM","source_url":"https://github.com/NVIDIA/Megatron-LM"},{"type":"has_code","target_id":"github:NVIDIA:Megatron-LM","source_url":"https://github.com/NVIDIA/Megatron-LM"},{"type":"based_on_paper","target_id":"arxiv:2409.11402","source_url":"https://arxiv.org/abs/2409.11402"}]', NULL, 'CC-BY-NC-4.0', 'approved', 78.9, 'aeb4c72e78697ea94b86421ae136ba7f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-WizardLMTeam-WizardCoder-Python-34B-V1.0', 'huggingface--wizardlmteam--wizardcoder-python-34b-v1.0', 'WizardCoder-Python-34B-V1.0', 'WizardLMTeam', '--- license: llama2 metrics: - code_eval library_name: transformers tags: - code model-index: - name: WizardCoder-Python-34B-V1.0 results: - task: type: text-generation dataset: type: openai_humaneval name: HumanEval metrics: - name: pass@1 type: pass@1 value: 0.732 verified: false --- <p style="font-size:28px;" align="center"> üè† <a href="https://wizardlm.github.io/" target="_blank">Home Page</a> </p> <p align="center"> <p align="center"> ü§ó <a href="https://huggingface.co/WizardLM" target="...', '["transformers","pytorch","llama","text-generation","code","arxiv:2304.12244","arxiv:2306.08568","arxiv:2308.09583","license:llama2","model-index","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 771, 164, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/WizardLMTeam/WizardCoder-Python-34B-V1.0","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: llama2\nmetrics:\n- code_eval\nlibrary_name: transformers\ntags:\n- code\nmodel-index:\n- name: WizardCoder-Python-34B-V1.0\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      type: openai_humaneval\n      name: HumanEval\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 0.732\n      verified: false\n---\n\n## WizardCoder: Empowering Code Large Language Models with Evol-Instruct\n\n<p style="font-size:28px;" align="center">\nüè† <a href="https://wizardlm.github.io/" target="_blank">Home Page</a> </p>\n<p align="center">\n<p align="center">\nü§ó <a href="https://huggingface.co/WizardLM" target="_blank">HF Repo</a>  ‚Ä¢üê± <a href="https://github.com/nlpxucan/WizardLM" target="_blank">Github Repo</a> ‚Ä¢ üê¶ <a href="https://twitter.com/WizardLM_AI" target="_blank">Twitter</a> </p>\n<p align="center">\n üìÉ <a href="https://arxiv.org/abs/2304.12244" target="_blank">[WizardLM]</a>  ‚Ä¢ üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>   ‚Ä¢ üìÉ <a href="https://arxiv.org/abs/2308.09583" target="_blank">[WizardMath]</a>  <br>\n</p>\n<p align="center">\n    üëã Join our <a href="https://discord.gg/VZjjHtWrKs" target="_blank">Discord</a>\n</p>\n\n## News\n\n[2024/01/04] üî• We released **WizardCoder-33B-V1.1**  trained from deepseek-coder-33b-base, the **SOTA OSS Code LLM** on [EvalPlus Leaderboard](https://evalplus.github.io/leaderboard.html), achieves **79.9 pass@1** on HumanEval, **73.2 pass@1** on HumanEval-Plus, **78.9 pass@1** on MBPP, and **66.9 pass@1** on MBPP-Plus.\n\n[2024/01/04] üî• **WizardCoder-33B-V1.1** outperforms **ChatGPT 3.5**, **Gemini Pro**, and **DeepSeek-Coder-33B-instruct** on HumanEval and HumanEval-Plus pass@1.\n\n[2024/01/04] üî• **WizardCoder-33B-V1.1** is comparable with **ChatGPT 3.5**, and surpasses **Gemini Pro** on MBPP and MBPP-Plus pass@1.\n\n|  Model  |  Checkpoint  | Paper    | HumanEval  |   HumanEval+ | MBPP | MBPP+ | License |\n| ----- |------| ---- |------|-------| ----- |  ----- |----- | \n|  GPT-4-Turbo (Nov 2023)  | - | - | 85.4  | 81.7 | 83.0 | 70.7 |-|\n|  GPT-4 (May 2023)  | - | - | 88.4  | 76.8 | - | - |-|\n|  GPT-3.5-Turbo (Nov 2023)  | - | - | 72.6  | 65.9 | 81.7 | 69.4 |-|\n|  Gemini Pro  | - | - | 63.4  | 55.5 | 72.9 | 57.9 |-|\n|  DeepSeek-Coder-33B-instruct | - | - |  78.7 | 72.6 | 78.7 | 66.7 |-|\n|  **WizardCoder-33B-V1.1**  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-33B-V1.1" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  79.9  | 73.2 | 78.9 | 66.9 |  <a href="https://huggingface.co/WizardLM/WizardMath-7B-V1.1/resolve/main/LICENSE" target="_blank">MSFTResearch</a>  |\n|  WizardCoder-Python-34B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  73.2   | 64.6 | 73.2 | 59.9 |  <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama2</a>  |\n|  WizardCoder-15B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-15B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  59.8   | 52.4 | -- | -- |  <a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement" target="_blank">OpenRAIL-M</a>  |\n|  WizardCoder-Python-13B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-Python-13B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  64.0   | -- | -- | -- |  <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama2</a>  |\n|  WizardCoder-Python-7B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-Python-7B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  55.5   | -- | -- | -- |  <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama2</a>  |\n|  WizardCoder-3B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-3B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  34.8   | -- | -- | -- |  <a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement" target="_blank">OpenRAIL-M</a>  |\n|  WizardCoder-1B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-1B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  23.8   | -- | -- | -- |  <a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement" target="_blank">OpenRAIL-M</a>  |\n\n\n\n-  Our **WizardMath-70B-V1.0** model slightly outperforms some closed-source LLMs on the GSM8K, including **ChatGPT 3.5**, **Claude Instant 1** and **PaLM 2 540B**.\n-  Our **WizardMath-70B-V1.0** model achieves  **81.6 pass@1** on the [GSM8k Benchmarks](https://github.com/openai/grade-school-math), which is **24.8** points higher than the SOTA open-source LLM, and achieves  **22.7 pass@1** on the [MATH Benchmarks](https://github.com/hendrycks/math), which is **9.2** points higher than the SOTA open-source LLM.\n\n<font size=4>\n    \n| Model | Checkpoint | Paper  | GSM8k | MATH  |Online Demo| License|\n| ----- |------| ---- |------|-------| ----- | ----- |\n| WizardMath-70B-V1.0 | ü§ó <a href="https://huggingface.co/WizardLM/WizardMath-70B-V1.0" target="_blank">HF Link</a> |  üìÉ <a href="https://arxiv.org/abs/2308.09583" target="_blank">[WizardMath]</a>| **81.6**  |  **22.7**	|[Demo](http://47.103.63.15:50083/)| <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2  </a> |\n| WizardMath-13B-V1.0 | ü§ó <a href="https://huggingface.co/WizardLM/WizardMath-13B-V1.0" target="_blank">HF Link</a> |  üìÉ <a href="https://arxiv.org/abs/2308.09583" target="_blank">[WizardMath]</a>| **63.9**  |  **14.0** |[Demo](http://47.103.63.15:50082/)| <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2 </a> |\n| WizardMath-7B-V1.0 | ü§ó <a href="https://huggingface.co/WizardLM/WizardMath-7B-V1.0" target="_blank">HF Link</a>  |  üìÉ <a href="https://arxiv.org/abs/2308.09583" target="_blank">[WizardMath]</a>| 	 **54.9**  |  **10.7** | [Demo ](http://47.103.63.15:50080/)|  <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2  </a>|\n</font>\n\n\n- [08/09/2023] We released **WizardLM-70B-V1.0** model. Here is [Full Model Weight](https://huggingface.co/WizardLM/WizardLM-70B-V1.0). \n\n<font size=4>\n    \n   \n| <sup>Model</sup> | <sup>Checkpoint</sup> | <sup>Paper</sup> |<sup>MT-Bench</sup> | <sup>AlpacaEval</sup>  | <sup>GSM8k</sup> | <sup>HumanEval</sup>  | <sup>License</sup>|\n| ----- |------| ---- |------|-------| ----- | ----- | ----- | \n| <sup>**WizardLM-70B-V1.0**</sup> | <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-70B-V1.0" target="_blank">HF Link</a> </sup>|<sup>üìÉ**Coming Soon**</sup>| <sup>**7.78**</sup> | <sup>**92.91%**</sup>	 |<sup>**77.6%**</sup>	 | <sup>   **50.6**</sup>|<sup> <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2 License </a></sup> |\n| <sup>WizardLM-13B-V1.2</sup> | <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.2" target="_blank">HF Link</a> </sup>|  | <sup>7.06</sup> | <sup>89.17%</sup>	 |<sup>55.3%</sup>	 | <sup>36.6   </sup>|<sup> <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2 License </a></sup> |\n| <sup>WizardLM-13B-V1.1</sup> |<sup> ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.1" target="_blank">HF Link</a> </sup> |  | <sup>6.76</sup>  |<sup>86.32%</sup>	 | 	 | <sup>25.0   </sup>| <sup>Non-commercial</sup>|\n| <sup>WizardLM-30B-V1.0</sup> | <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-30B-V1.0" target="_blank">HF Link</a></sup>  | | <sup>7.01</sup> |                    | |  <sup>37.8  </sup>| <sup>Non-commercial</sup> |\n| <sup>WizardLM-13B-V1.0</sup> | <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.0" target="_blank">HF Link</a> </sup> |  | <sup>6.35</sup> | <sup>75.31%</sup> |  | <sup> 24.0   </sup> | <sup>Non-commercial</sup>|\n| <sup>WizardLM-7B-V1.0 </sup>|  <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-7B-V1.0" target="_blank">HF Link</a> </sup> |<sup> üìÉ <a href="https://arxiv.org/abs/2304.12244" target="_blank">[WizardLM]</a> </sup>|  |  |  |<sup>19.1 </sup>|<sup> Non-commercial</sup>|\n</font>\n\n\n## Comparing WizardCoder-Python-34B-V1.0 with Other LLMs.\n\nüî• The following figure shows that our **WizardCoder-Python-34B-V1.0 attains the second position in this benchmark**, surpassing GPT4 (2023/03/15, 73.2 vs. 67.0), ChatGPT-3.5 (73.2 vs. 72.5) and Claude2 (73.2 vs. 71.2).\n\n<p align="center" width="100%">\n<a ><img src="https://raw.githubusercontent.com/nlpxucan/WizardLM/main/WizardCoder/imgs/compare_sota.png" alt="WizardCoder" style="width: 96%; min-width: 300px; display: block; margin: auto;"></a>\n</p>\n\n## Prompt Format\n```\n"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:"\n```\n\n## Inference Demo Script\n\nWe provide the inference demo code [here](https://github.com/nlpxucan/WizardLM/tree/main/demo).\n\n## Citation\n\nPlease cite the repo if you use the data, method or code in this repo.\n\n```\n@article{luo2023wizardcoder,\n  title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},\n  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},\n  journal={arXiv preprint arXiv:2306.08568},\n  year={2023}\n}\n```', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":134976679658,"files_count":17,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["LlamaForCausalLM"],"model_type":"llama","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<s>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"</s>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":null,"unk_token":{"__type":"AddedToken","content":"<unk>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false}}}}', '[]', '[{"type":"has_code","target_id":"github:nlpxucan:WizardLM\"","source_url":"https://github.com/nlpxucan/WizardLM\""},{"type":"has_code","target_id":"github:openai:grade-school-math","source_url":"https://github.com/openai/grade-school-math"},{"type":"has_code","target_id":"github:hendrycks:math","source_url":"https://github.com/hendrycks/math"},{"type":"has_code","target_id":"github:nlpxucan:WizardLM","source_url":"https://github.com/nlpxucan/WizardLM"},{"type":"based_on_paper","target_id":"arxiv:2304.12244","source_url":"https://arxiv.org/abs/2304.12244"},{"type":"based_on_paper","target_id":"arxiv:2306.08568","source_url":"https://arxiv.org/abs/2306.08568"},{"type":"based_on_paper","target_id":"arxiv:2308.09583","source_url":"https://arxiv.org/abs/2308.09583"}]', NULL, 'LLaMA-2', 'approved', 63.9, '1167423df261871b914e9789912ee79a', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-FacebookAI-xlm-roberta-base', 'huggingface--facebookai--xlm-roberta-base', 'xlm-roberta-base', 'FacebookAI', '--- tags: - exbert language: - multilingual - af - am - ar - as - az - be - bg - bn - br - bs - ca - cs - cy - da - de - el - en - eo - es - et - eu - fa - fi - fr - fy - ga - gd - gl - gu - ha - he - hi - hr - hu - hy - id - is - it - ja - jv - ka - kk - km - kn - ko - ku - ky - la - lo - lt - lv - mg - mk - ml - mn - mr - ms - my - ne - nl - no - om - or - pa - pl - ps - pt - ro - ru - sa - sd - si - sk - sl - so - sq - sr - su - sv - sw - ta - te - th - tl - tr - ug - uk - ur - uz - vi - x...', '["transformers","pytorch","tf","jax","onnx","safetensors","xlm-roberta","fill-mask","exbert","multilingual","af","am","ar","as","az","be","bg","bn","br","bs","ca","cs","cy","da","de","el","en","eo","es","et","eu","fa","fi","fr","fy","ga","gd","gl","gu","ha","he","hi","hr","hu","hy","id","is","it","ja","jv","ka","kk","km","kn","ko","ku","ky","la","lo","lt","lv","mg","mk","ml","mn","mr","ms","my","ne","nl","no","om","or","pa","pl","ps","pt","ro","ru","sa","sd","si","sk","sl","so","sq","sr","su","sv","sw","ta","te","th","tl","tr","ug","uk","ur","uz","vi","xh","yi","zh","arxiv:1911.02116","license:mit","endpoints_compatible","deploy:azure","region:us"]', 'fill-mask', 770, 7108935, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/FacebookAI/xlm-roberta-base","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\ntags:\n- exbert\nlanguage: \n- multilingual\n- af\n- am\n- ar\n- as\n- az\n- be\n- bg\n- bn\n- br\n- bs\n- ca\n- cs\n- cy\n- da\n- de\n- el\n- en\n- eo\n- es\n- et\n- eu\n- fa\n- fi\n- fr\n- fy\n- ga\n- gd\n- gl\n- gu\n- ha\n- he\n- hi\n- hr\n- hu\n- hy\n- id\n- is\n- it\n- ja\n- jv\n- ka\n- kk\n- km\n- kn\n- ko\n- ku\n- ky\n- la\n- lo\n- lt\n- lv\n- mg\n- mk\n- ml\n- mn\n- mr\n- ms\n- my\n- ne\n- nl\n- no\n- om\n- or\n- pa\n- pl\n- ps\n- pt\n- ro\n- ru\n- sa\n- sd\n- si\n- sk\n- sl\n- so\n- sq\n- sr\n- su\n- sv\n- sw\n- ta\n- te\n- th\n- tl\n- tr\n- ug\n- uk\n- ur\n- uz\n- vi\n- xh\n- yi\n- zh\nlicense: mit\n---\n\n# XLM-RoBERTa (base-sized model) \n\nXLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper [Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116) by Conneau et al. and first released in [this repository](https://github.com/pytorch/fairseq/tree/master/examples/xlmr). \n\nDisclaimer: The team releasing XLM-RoBERTa did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nXLM-RoBERTa is a multilingual version of RoBERTa. It is pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. \n\nRoBERTa is a transformers model pretrained on a large corpus in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts.\n\nMore precisely, it was pretrained with the Masked language modeling (MLM) objective. Taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.\n\nThis way, the model learns an inner representation of 100 languages that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the XLM-RoBERTa model as inputs.\n\n## Intended uses & limitations\n\nYou can use the raw model for masked language modeling, but it''s mostly intended to be fine-tuned on a downstream task. See the [model hub](https://huggingface.co/models?search=xlm-roberta) to look for fine-tuned versions on a task that interests you.\n\nNote that this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation, you should look at models like GPT2.\n\n## Usage\n\nYou can use this model directly with a pipeline for masked language modeling:\n\n```python\n>>> from transformers import pipeline\n>>> unmasker = pipeline(''fill-mask'', model=''xlm-roberta-base'')\n>>> unmasker("Hello I''m a <mask> model.")\n\n[{''score'': 0.10563907772302628,\n  ''sequence'': "Hello I''m a fashion model.",\n  ''token'': 54543,\n  ''token_str'': ''fashion''},\n {''score'': 0.08015287667512894,\n  ''sequence'': "Hello I''m a new model.",\n  ''token'': 3525,\n  ''token_str'': ''new''},\n {''score'': 0.033413201570510864,\n  ''sequence'': "Hello I''m a model model.",\n  ''token'': 3299,\n  ''token_str'': ''model''},\n {''score'': 0.030217764899134636,\n  ''sequence'': "Hello I''m a French model.",\n  ''token'': 92265,\n  ''token_str'': ''French''},\n {''score'': 0.026436051353812218,\n  ''sequence'': "Hello I''m a sexy model.",\n  ''token'': 17473,\n  ''token_str'': ''sexy''}]\n```\n\nHere is how to use this model to get the features of a given text in PyTorch:\n\n```python\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained(''xlm-roberta-base'')\nmodel = AutoModelForMaskedLM.from_pretrained("xlm-roberta-base")\n\n# prepare input\ntext = "Replace me by any text you''d like."\nencoded_input = tokenizer(text, return_tensors=''pt'')\n\n# forward pass\noutput = model(**encoded_input)\n```\n\n### BibTeX entry and citation info\n\n```bibtex\n@article{DBLP:journals/corr/abs-1911-02116,\n  author    = {Alexis Conneau and\n               Kartikay Khandelwal and\n               Naman Goyal and\n               Vishrav Chaudhary and\n               Guillaume Wenzek and\n               Francisco Guzm{\''{a}}n and\n               Edouard Grave and\n               Myle Ott and\n               Luke Zettlemoyer and\n               Veselin Stoyanov},\n  title     = {Unsupervised Cross-lingual Representation Learning at Scale},\n  journal   = {CoRR},\n  volume    = {abs/1911.02116},\n  year      = {2019},\n  url       = {http://arxiv.org/abs/1911.02116},\n  eprinttype = {arXiv},\n  eprint    = {1911.02116},\n  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n<a href="https://huggingface.co/exbert/?model=xlm-roberta-base">\n	<img width="300px" src="https://cdn-media.huggingface.co/exbert/button.png">\n</a>\n', '{"pipeline_tag":"fill-mask","library_name":"transformers","framework":"transformers","params":278885778,"storage_bytes":6352424687,"files_count":11,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["XLMRobertaForMaskedLM"],"model_type":"xlm-roberta","tokenizer_config":{}}}', '[]', '[{"type":"has_code","target_id":"github:pytorch:fairseq","source_url":"https://github.com/pytorch/fairseq"},{"type":"based_on_paper","target_id":"arxiv:1911.02116","source_url":"https://arxiv.org/abs/1911.02116"}]', NULL, 'MIT', 'approved', 63.9, '1fba0749c413cab557edb8d815166b93', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-openbmb-VoxCPM-0.5B', 'huggingface--openbmb--voxcpm-0.5b', 'VoxCPM-0.5B', 'openbmb', '--- license: apache-2.0 language: - en - zh base_model: - openbmb/MiniCPM4-0.5B pipeline_tag: text-to-speech library_name: voxcpm tags: - text-to-speech - speech - speech generation - voice cloning --- <div align="center"> <img src="assets/voxcpm_logo.png" alt="VoxCPM Logo" width="40%"> </div> VoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization a...', '["voxcpm","pytorch","text-to-speech","speech","speech generation","voice cloning","en","zh","base_model:openbmb/minicpm4-0.5b","base_model:finetune:openbmb/minicpm4-0.5b","license:apache-2.0","region:us"]', 'text-to-speech', 770, 2428, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/openbmb/VoxCPM-0.5B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\n- zh\nbase_model:\n- openbmb/MiniCPM4-0.5B\npipeline_tag: text-to-speech\nlibrary_name: voxcpm\ntags:\n- text-to-speech\n- speech\n- speech generation\n- voice cloning\n---\n\n## üéôÔ∏è VoxCPM: Tokenizer-Free TTS for Context-Aware Speech Generation and True-to-Life Voice Cloning\n\n\n[![Project Page](https://img.shields.io/badge/Project%20Page-GitHub-blue)](https://github.com/OpenBMB/VoxCPM/) [![Hugging Face](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-OpenBMB-yellow)](https://huggingface.co/openbmb/VoxCPM-0.5B) [![Live Playground](https://img.shields.io/badge/Live%20PlayGround-Demo-orange)](https://huggingface.co/spaces/OpenBMB/VoxCPM-Demo) [![Samples](https://img.shields.io/badge/Page-Samples-red)](https://openbmb.github.io/VoxCPM-demopage/)\n\n\n<div align="center">\n  <img src="assets/voxcpm_logo.png" alt="VoxCPM Logo" width="40%">\n</div>\n\n## Overview\n\nVoxCPM is a novel tokenizer-free Text-to-Speech (TTS) system that redefines realism in speech synthesis. By modeling speech in a continuous space, it overcomes the limitations of discrete tokenization and enables two flagship capabilities: context-aware speech generation and true-to-life zero-shot voice cloning.\n\nUnlike mainstream approaches that convert speech to discrete tokens, VoxCPM uses an end-to-end diffusion autoregressive architecture that directly generates continuous speech representations from text. Built on [MiniCPM-4](https://huggingface.co/openbmb/MiniCPM4-0.5B) backbone, it achieves implicit semantic-acoustic decoupling through hierachical language modeling and FSQ constraints, greatly enhancing both expressiveness and generation stability.\n\n<div align="center">\n  <img src="assets/voxcpm_model.png" alt="VoxCPM Model Architecture" width="90%">\n</div>\n\n\n###  üöÄ Key Features\n- **Context-Aware, Expressive Speech Generation** - VoxCPM comprehends text to infer and generate appropriate prosody, delivering speech with remarkable expressiveness and natural flow. It spontaneously adapts speaking style based on content, producing highly fitting vocal expression trained on a massive 1.8 million-hour bilingual corpus.\n- **True-to-Life Voice Cloning** - With only a short reference audio clip, VoxCPM performs accurate zero-shot voice cloning, capturing not only the speaker‚Äôs timbre but also fine-grained characteristics such as accent, emotional tone, rhythm, and pacing to create a faithful and natural replica.\n- **High-Efficiency Synthesis** - VoxCPM supports streaming synthesis with a Real-Time Factor (RTF) as low as 0.17 on a consumer-grade NVIDIA RTX 4090 GPU, making it possible for real-time applications.\n\n\n##  Quick Start\n\n### üîß Install from PyPI\n``` sh\npip install voxcpm\n```\n### 1.  Model Download (Optional)\nBy default, when you first run the script, the model will be downloaded automatically, but you can also download the model in advance.\n- Download VoxCPM-0.5B\n    ```\n    from huggingface_hub import snapshot_download\n    snapshot_download("openbmb/VoxCPM-0.5B",local_files_only=local_files_only)\n    ```\n- Download ZipEnhancer and SenseVoice-Small. We use ZipEnhancer to enhance speech prompts and SenseVoice-Small for speech prompt ASR in the web demo.\n    ```\n    from modelscope import snapshot_download\n    snapshot_download(''iic/speech_zipenhancer_ans_multiloss_16k_base'')\n    snapshot_download(''iic/SenseVoiceSmall'')\n    ```\n\n### 2. Basic Usage\n```python\nimport soundfile as sf\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained("openbmb/VoxCPM-0.5B")\n\nwav = model.generate(\n    text="VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write("output.wav", wav, 16000)\nprint("saved: output.wav")\n```\n\n### 3. CLI Usage\n\nAfter installation, the entry point is `voxcpm` (or use `python -m voxcpm.cli`).\n\n```bash\n# 1) Direct synthesis (single text)\nvoxcpm --text "Hello VoxCPM" --output out.wav\n\n# 2) Voice cloning (reference audio + transcript)\nvoxcpm --text "Hello" \\n  --prompt-audio path/to/voice.wav \\n  --prompt-text "reference transcript" \\n  --output out.wav \\n  --denoise\n\n# 3) Batch processing (one text per line)\nvoxcpm --input examples/input.txt --output-dir outs\n# (optional) Batch + cloning\nvoxcpm --input examples/input.txt --output-dir outs \\n  --prompt-audio path/to/voice.wav \\n  --prompt-text "reference transcript" \\n  --denoise\n\n# 4) Inference parameters (quality/speed)\nvoxcpm --text "..." --output out.wav \\n  --cfg-value 2.0 --inference-timesteps 10 --normalize\n\n# 5) Model loading\n# Prefer local path\nvoxcpm --text "..." --output out.wav --model-path /path/to/VoxCPM_model_dir\n# Or from Hugging Face (auto download/cache)\nvoxcpm --text "..." --output out.wav \\n  --hf-model-id openbmb/VoxCPM-0.5B --cache-dir ~/.cache/huggingface --local-files-only\n\n# 6) Denoiser control\nvoxcpm --text "..." --output out.wav \\n  --no-denoiser --zipenhancer-path iic/speech_zipenhancer_ans_multiloss_16k_base\n\n# 7) Help\nvoxcpm --help\npython -m voxcpm.cli --help\n```\n\n### 4. Start web demo\n\nYou can start the UI interface by running `python app.py`, which allows you to perform Voice Cloning and Voice Creation.\n\n\n\n## üë©‚Äçüç≥ A Voice Chef''s Guide\nWelcome to the VoxCPM kitchen! Follow this recipe to cook up perfect generated speech. Let‚Äôs begin.\n\n---\n### ü•ö Step 1: Prepare Your Base Ingredients (Content)\n\nFirst, choose how you‚Äôd like to input your text:.\n1. Regular Text (Classic Mode)\n- ‚úÖ Keep "Text Normalization" ON. Type naturally (e.g., "Hello, world! 123"). The system will automatically process numbers, abbreviations, and punctuation using WeTextProcessing library.\n2. Phoneme Input (Native Mode)\n- ‚ùå Turn "Text Normalization" OFF. Enter phoneme text like {HH AH0 L OW1} (EN) or {ni3}{hao3} (ZH) for precise pronunciation  control. In this mode, VoxCPM also supports native understanding of other complex non-normalized text‚Äîtry it out!\n\n---\n### üç≥ Step 2: Choose Your Flavor Profile (Voice Style) \n\nThis is the secret sauce that gives your audio its unique sound.\n1. Cooking with a Prompt Speech (Following a Famous Recipe)\n  - A prompt speech provides the desired acoustic characteristics for VoxCPM. The speaker''s timbre, speaking style, and even the background sounds and ambiance will be replicated.\n  - For a Clean, Studio-Quality Voice:\n    - ‚úÖ Enable "Prompt Speech Enhancement". This acts like a noise filter, removing background hiss and rumble to give you a pure, clean voice clone.\n2. Cooking au Naturel (Letting the Model Improvise)\n  - If no reference is provided, VoxCPM becomes a creative chef! It will infer a fitting speaking style based on the text itself, thanks to the text-smartness of its foundation model, MiniCPM-4.\n  - Pro Tip: Challenge VoxCPM with any text‚Äîpoetry, song lyrics, dramatic monologues‚Äîit may deliver some interesting results!\n\n---\n### üßÇ Step 3: The Final Seasoning (Fine-Tuning Your Results)\nYou''re ready to serve! But for master chefs who want to tweak the flavor, here are two key spices.\n- CFG Value (How Closely to Follow the Recipe)\n  - Default: A great starting point.\n  - Voice sounds strained or weird? Lower this value. It tells the model to be more relaxed and improvisational, great for expressive prompts.\n  - Need maximum clarity and adherence to the text? Raise it slightly to keep the model on a tighter leash.\n- Inference Timesteps (Simmering Time: Quality vs. Speed)\n  - Need a quick snack? Use a lower number. Perfect for fast drafts and experiments.\n  - Cooking a gourmet meal? Use a higher number. This lets the model "simmer" longer, refining the audio for superior detail and naturalness.\n\n---\nHappy creating! üéâ Start with the default settings and tweak from there to suit your project. The kitchen is yours!\n\n\n---\n\n\n\n## üìä Performance Highlights\n\nVoxCPM achieves competitive results on public zero-shot TTS benchmarks:\n\n### Seed-TTS-eval Benchmark\n\n| Model | Parameters | Open-Source | test-EN | | test-ZH | | test-Hard | |\n|------|------|------|:------------:|:--:|:------------:|:--:|:-------------:|:--:|\n| | | | WER/%‚¨á | SIM/%‚¨Ü| CER/%‚¨á| SIM/%‚¨Ü | CER/%‚¨á | SIM/%‚¨Ü |\n| MegaTTS3 | 0.5B | ‚ùå | 2.79 | 77.1 | 1.52 | 79.0 | - | - |\n| DiTAR | 0.6B | ‚ùå | 1.69 | 73.5 | 1.02 | 75.3 | - | - |\n| CosyVoice3 | 0.5B | ‚ùå | 2.02 | 71.8 | 1.16 | 78.0 | 6.08 | 75.8 |\n| CosyVoice3 | 1.5B | ‚ùå | 2.22 | 72.0 | 1.12 | 78.1 | 5.83 | 75.8 |\n| Seed-TTS | - | ‚ùå | 2.25 | 76.2 | 1.12 | 79.6 | 7.59 | 77.6 |\n| MiniMax-Speech | - | ‚ùå | 1.65 | 69.2 | 0.83 | 78.3 | - | - |\n| CosyVoice | 0.3B | ‚úÖ | 4.29 | 60.9 | 3.63 | 72.3 | 11.75 | 70.9 |\n| CosyVoice2 | 0.5B | ‚úÖ | 3.09 | 65.9 | 1.38 | 75.7 | **6.83** | 72.4 |\n| F5-TTS | 0.3B | ‚úÖ | 2.00 | 67.0 | 1.53 | 76.0 | 8.67 | 71.3 |\n| SparkTTS | 0.5B | ‚úÖ | 3.14 | 57.3 | 1.54 | 66.0 | - | - |\n| FireRedTTS | 0.5B | ‚úÖ | 3.82 | 46.0 | 1.51 | 63.5 | 17.45 | 62.1 |\n| FireRedTTS-2 | 1.5B | ‚úÖ | 1.95 | 66.5 | 1.14 | 73.6 | - | - |\n| Qwen2.5-Omni | 7B | ‚úÖ | 2.72 | 63.2 | 1.70 | 75.2 | 7.97 | **74.7** |\n| OpenAudio-s1-mini | 0.5B | ‚úÖ | 1.94 | 55.0 | 1.18 | 68.5 | - | - |\n| IndexTTS2 | 1.5B | ‚úÖ | 2.23 | 70.6 | 1.03 | 76.5 | - | - |\n| VibeVoice | 1.5B | ‚úÖ | 3.04 | 68.9 | 1.16 | 74.4 | - | - |\n| HiggsAudio-v2 | 3B | ‚úÖ | 2.44 | 67.7 | 1.50 | 74.0 | - | - |\n| **VoxCPM** | 0.5B | ‚úÖ | **1.85** | **72.9** | **0.93** | **77.2** | 8.87 | 73.0 |\n\n\n###  CV3-eval Benchmark\n\n| Model | zh | en | hard-zh | | | hard-en | | |\n|-------|:--:|:--:|:-------:|:--:|:--:|:-------:|:--:|:--:|\n| | CER/%‚¨á | WER/%‚¨á | CER/%‚¨á | SIM/%‚¨Ü | DNSMOS‚¨Ü | WER/%‚¨á | SIM/%‚¨Ü | DNSMOS‚¨Ü |\n| F5-TTS | 5.47 | 8.90 | - | - | - | - | - | - |\n| SparkTTS | 5.15 | 11.0 | - | - | - | - | - | - |\n| GPT-SoVits | 7.34 | 12.5 | - | - | - | - | - | - |\n| CosyVoice2 | 4.08 | 6.32 | 12.58 | 72.6 | 3.81 | 11.96 | 66.7 | 3.95 |\n| OpenAudio-s1-mini | 4.00 | 5.54 | 18.1 | 58.2 | 3.77 | 12.4 | 55.7 | 3.89 |\n| IndexTTS2 | 3.58 | 4.45 | 12.8 | 74.6 | 3.65 | - | - | - |\n| HiggsAudio-v2 | 9.54 | 7.89 | 41.0 | 60.2 | 3.39 | 10.3 | 61.8 | 3.68 |\n| CosyVoice3-0.5B | 3.89 | 5.24 | 14.15 | 78.6 | 3.75 | 9.04 | 75.9 | 3.92 |\n| CosyVoice3-1.5B | 3.91 | 4.99 | 9.77 | 78.5 | 3.79 | 10.55 | 76.1 | 3.95 |\n| **VoxCPM** | **3.40** | **4.04** | 12.9 | 66.1 | 3.59 | **7.89** | 64.3 | 3.74 |\n\n\n## ‚ö†Ô∏è Risks and limitations\n- General Model Behavior: While VoxCPM has been trained on a large-scale dataset, it may still produce outputs that are unexpected, biased, or contain artifacts.\n- Potential for Misuse of Voice Cloning: VoxCPM''s powerful zero-shot voice cloning capability can generate highly realistic synthetic speech. This technology could be misused for creating convincing deepfakes for purposes of impersonation, fraud, or spreading disinformation. Users of this model must not use it to create content that infringes upon the rights of individuals. It is strictly forbidden to use VoxCPM for any illegal or unethical purposes. We strongly recommend that any publicly shared content generated with this model be clearly marked as AI-generated.\n- Current Technical Limitations: Although generally stable, the model may occasionally exhibit instability, especially with very long or expressive inputs. Furthermore, the current version offers limited direct control over specific speech attributes like emotion or speaking style.\n- Bilingual Model: VoxCPM is trained primarily on Chinese and English data. Performance on other languages is not guaranteed and may result in unpredictable or low-quality audio.\n- This model is released for research and development purposes only. We do not recommend its use in production or commercial applications without rigorous testing and safety evaluations. Please use VoxCPM responsibly.\n\n\n\n## üìÑ License\nThe VoxCPM model weights and code are open-sourced under the Apache-2.0 license.\n\n\n', '{"pipeline_tag":"text-to-speech","library_name":"voxcpm","framework":"voxcpm","params":null,"storage_bytes":2910957348,"files_count":12,"spaces_count":15,"gated":false,"private":false,"config":{"tokenizer_config":{"bos_token":"<s>","eos_token":"<|im_end|>","pad_token":null,"unk_token":"<unk>","use_default_system_prompt":false,"chat_template":"{% for message in messages %}{{''<|im_start|>'' + message[''role''] + ''\n'' + message[''content''] + ''<|im_end|>'' + ''\n''}}{% endfor %}{% if add_generation_prompt %}{{ ''<|im_start|>assistant\n'' }}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:OpenBMB:VoxCPM","source_url":"https://github.com/OpenBMB/VoxCPM"}]', NULL, 'Apache-2.0', 'approved', 98.9, '52f8d486d498dcd78e44592c0f4db47a', NULL, 'https://huggingface.co/openbmb/VoxCPM-0.5B/resolve/main/assets/modelbest_logo.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-openbmb-VoxCPM-0.5B from https://huggingface.co/openbmb/VoxCPM-0.5B/resolve/main/assets/modelbest_logo.png
Image converted to WebP: data/images/huggingface-openbmb-VoxCPM-0.5B.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-baidu-ERNIE-4.5-21B-A3B-Thinking', 'huggingface--baidu--ernie-4.5-21b-a3b-thinking', 'ERNIE-4.5-21B-A3B-Thinking', 'baidu', '--- license: apache-2.0 language: - en - zh pipeline_tag: text-generation tags: - ERNIE4.5 library_name: transformers --- <div align="center" style="line-height: 1;"> <a href="https://ernie.baidu.com/" target="_blank" style="margin: 2px;"> <img alt="Chat" src="https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue" style="display: inline-block; vertical-align: middle;"/> </a> <a href="https://huggingface.co/baidu" target="_blank" style="margin: 2px;"> <img alt="Hugging Face" src="https://img.shi...', '["transformers","safetensors","ernie4_5_moe","text-generation","ernie4.5","conversational","en","zh","license:apache-2.0","endpoints_compatible","region:us"]', 'text-generation', 768, 14525, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Thinking","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- en\n- zh\npipeline_tag: text-generation\ntags:\n- ERNIE4.5\nlibrary_name: transformers\n---\n\n<div align="center" style="line-height: 1;">\n  <a href="https://ernie.baidu.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ_Chat-ERNIE_Bot-blue" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/baidu" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Baidu-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/PaddlePaddle/ERNIE" target="_blank" style="margin: 2px;">\n    <img alt="Github" src="https://img.shields.io/badge/GitHub-ERNIE-000?logo=github&color=0000FF" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://ernie.baidu.com/blog/ernie4.5" target="_blank" style="margin: 2px;">\n    <img alt="Blog" src="https://img.shields.io/badge/üññ_Blog-ERNIE4.5-A020A0" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://discord.gg/JPmZXDsEEK" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-ERNIE-5865F2?logo=discord&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://x.com/PaddlePaddle" target="_blank" style="margin: 2px;">\n    <img alt="X" src="https://img.shields.io/badge/X-PaddlePaddle-6080F0"?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="#license" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-Apache2.0-A5de54" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n# ERNIE-4.5-21B-A3B-Thinking\n\n## Model Highlights\n\nOver the past three months, we have continued to scale the **thinking capability** of ERNIE-4.5-21B-A3B, improving both the **quality and depth** of reasoning, thereby advancing the competitiveness of ERNIE **lightweight models** in complex reasoning tasks. We are pleased to introduce **ERNIE-4.5-21B-A3B-Thinking**, featuring the following key enhancements:\n\n* **Significantly improved performance** on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.\n* **Efficient tool usage** capabilities.\n* **Enhanced 128K long-context understanding** capabilities.\n\n> [!NOTE]\n> Note: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.\n\n![benchmark](./benchmark.png)\n\n## Model Overview\n\nERNIE-4.5-21B-A3B-Thinking is a text MoE post-trained model, with 21B total parameters and 3B activated parameters for each token. The following are the model configuration details:\n\n|Key|Value|\n|-|-|\n|Modality|Text|\n|Training Stage|Posttraining|\n|Params(Total / Activated)|21B / 3B|\n|Layers|28|\n|Heads(Q/KV)|20 / 4|\n|Text Experts(Total / Activated)|64 / 6|\n|Shared Experts|2|\n|Context Length|131072|\n\n## Quickstart\n\n> [!NOTE]\n> To align with the wider community, this model releases Transformer-style weights. Both PyTorch and PaddlePaddle ecosystem tools, such as vLLM, transformers, and FastDeploy, are expected to be able to load and run this model.\n\n### FastDeploy Inference\n\nQuickly deploy services using FastDeploy as shown below. For more detailed usage, refer to the [FastDeploy GitHub Repository](https://github.com/PaddlePaddle/FastDeploy).\n\n**Note**: 80GB x 1 GPU resources are required. Deploying this model requires FastDeploy version 2.2.\n\n```bash\npython -m fastdeploy.entrypoints.openai.api_server \\n       --model baidu/ERNIE-4.5-21B-A3B-Thinking \\n       --port 8180 \\n       --metrics-port 8181 \\n       --engine-worker-queue-port 8182 \\n       --load-choices "default_v1" \\n       --tensor-parallel-size 1 \\n       --max-model-len 131072 \\n       --reasoning-parser ernie_x1 \\n       --tool-call-parser ernie_x1 \\n       --max-num-seqs 32\n```\n\nThe ERNIE-4.5-21B-A3B-Thinking model supports function call.\n\n```bash\ncurl -X POST "http://0.0.0.0:8180/v1/chat/completions" \\n-H "Content-Type: application/json" \\n-d $''{\n  "messages": [\n    {\n      "role": "user",\n      "content": "How \''s the weather in Beijing today?"\n    }\n  ],\n  "tools": [\n    {\n      "type": "function",\n      "function": {\n        "name": "get_weather",\n        "description": "Determine weather in my location",\n        "parameters": {\n          "type": "object",\n          "properties": {\n            "location": {\n              "type": "string",\n              "description": "The city and state e.g. San Francisco, CA"\n            },\n            "unit": {\n              "type": "string",\n              "enum": [\n                "c",\n                "f"\n              ]\n            }\n          },\n          "additionalProperties": false,\n          "required": [\n            "location",\n            "unit"\n          ]\n        },\n        "strict": true\n      }\n    }]\n}''\n```\n\n### vLLM inference\n\nVLLM>=0.10.2 (excluding 0.11.0)\n\n```bash\nvllm serve baidu/ERNIE-4.5-21B-A3B-Thinking\n```\n\nThe `reasoning-parser` and `tool-call-parser` for vLLM Ernie need install vllm main branch\n\n### Using `transformers` library\n\n**Note**: You''ll need the`transformers`library (version 4.54.0 or newer) installed to use this model.\n\nThe following contains a code snippet illustrating how to use the model generate content based on given inputs.\n\n```python\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = "baidu/ERNIE-4.5-21B-A3B-Thinking"\n\n# load the tokenizer and the model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map="auto",\n    torch_dtype=torch.bfloat16,\n)\n\n# prepare the model input\nprompt = "Give me a short introduction to large language model."\nmessages = [\n    {"role": "user", "content": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], add_special_tokens=False, return_tensors="pt").to(model.device)\n\n# conduct text completion\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=1024\n)\noutput_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n\n# decode the generated ids\ngenerate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\nprint("generate_text:", generate_text)\n```\n\n## License\n\nThe ERNIE 4.5 models are provided under the Apache License 2.0. This license permits commercial use, subject to its terms and conditions. Copyright (c) 2025 Baidu, Inc. All Rights Reserved.\n\n## Citation\n\nIf you find ERNIE 4.5 useful or wish to use it in your projects, please kindly cite our technical report:\n\n```text\n@misc{ernie2025technicalreport,\n      title={ERNIE 4.5 Technical Report},\n      author={Baidu-ERNIE-Team},\n      year={2025},\n      primaryClass={cs.CL},\n      howpublished={\url{https://ernie.baidu.com/blog/publication/ERNIE_Technical_Report.pdf}}\n}\n```\n\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":21825437888,"storage_bytes":43674659350,"files_count":21,"spaces_count":8,"gated":false,"private":false,"config":{"architectures":["Ernie4_5_MoeForCausalLM"],"model_type":"ernie4_5_moe","tokenizer_config":{"bos_token":"<s>","eos_token":"</s>","pad_token":"<unk>","unk_token":"<unk>","cls_token":"<|begin_of_sentence|>","sep_token":"<|end_of_sentence|>","mask_token":"<mask:1>","chat_template":"{{- ''<|im_start|>system\n'' }}{%- if messages[0].role != ''system'' and not system_settings %}{{- ''<global_setting>\nthink_mode=True\n</global_setting>'' }}{%- else%}{{- ''<system_setting>\n'' }}{{- system_settings + ''\n'' if system_settings else '''' }}{{- (messages[0].content + ''\n'' if messages[0].role == ''system'' else '''') + ''</system_setting>\n\n<global_setting>\nthink_mode=True\n</global_setting>'' }}{%- endif %}{%- if tools %}{{- \"\n\n<tool_list>\" }}{{- ''\n'' }}{{-''[''}}{% for tool in tools %}{{''{\"type\": \"function\", \"function\": ''}}{{-(tool.function | tojson)}}}{%-if not loop.last%},{%- endif %}{%endfor%}{{-'']''}}{{- \"\n</tool_list>\" }}{%- endif %}{{-''<|im_end|>\n\n'' }}{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(''<tool_output>'') and message.content.endswith(''</tool_output>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\n'' + content + ''<|im_end|>'' + ''\n\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.thoughts is string %}\n            {%- set reasoning_content = message.thoughts %}\n        {%- else %}\n            {%- if ''</think>'' in content %}\n                {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\n'').split(''<think>'')[-1].lstrip(''\n'') %}\n                {%- set content = content.split(''</think>'')[-1].lstrip(''\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index and  (loop.last or (not loop.last and reasoning_content)) %} {{- ''<|im_start|>'' + message.role + ''\n<think>\n'' + reasoning_content.strip(''\n'') + ''\n</think>\n'' }} {%- else %} {{- ''<|im_start|>'' + message.role + ''\n'' }} {%- endif %}  {%- if content|length > 0 %}  {{- ''<response>\n'' + content + ''\n</response>\n'' }}  {%- endif %} {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''\n<tool_call>\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\n</tool_call>\n'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\n\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>tool'' }}\n        {%- endif %}\n        {{- ''\n<tool_output>'' }}\n        {{- message.content|tojson }}\n        {{- ''</tool_output>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\n\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n {{- \"<|im_start|>assistant\n<think>\n\"}}"},"chat_template_jinja":"{{- ''<|im_start|>system\n'' }}{%- if messages[0].role != ''system'' and not system_settings %}{{- ''<global_setting>\nthink_mode=True\n</global_setting>'' }}{%- else%}{{- ''<system_setting>\n'' }}{{- system_settings + ''\n'' if system_settings else '''' }}{{- (messages[0].content + ''\n'' if messages[0].role == ''system'' else '''') + ''</system_setting>\n\n<global_setting>\nthink_mode=True\n</global_setting>'' }}{%- endif %}{%- if tools %}{{- \"\n\n<tool_list>\" }}{{- ''\n'' }}{{-''[''}}{% for tool in tools %}{{''{\"type\": \"function\", \"function\": ''}}{{-(tool.function | tojson)}}}{%-if not loop.last%},{%- endif %}{%endfor%}{{-'']''}}{{- \"\n</tool_list>\" }}{%- endif %}{{-''<|im_end|>\n\n'' }}{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith(''<tool_output>'') and message.content.endswith(''</tool_output>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if message.content is string %}\n        {%- set content = message.content %}\n    {%- else %}\n        {%- set content = '''' %}\n    {%- endif %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\n'' + content + ''<|im_end|>'' + ''\n\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.thoughts is string %}\n            {%- set reasoning_content = message.thoughts %}\n        {%- else %}\n            {%- if ''</think>'' in content %}\n                {%- set reasoning_content = content.split(''</think>'')[0].rstrip(''\n'').split(''<think>'')[-1].lstrip(''\n'') %}\n                {%- set content = content.split(''</think>'')[-1].lstrip(''\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index and  (loop.last or (not loop.last and reasoning_content)) %} {{- ''<|im_start|>'' + message.role + ''\n<think>\n'' + reasoning_content.strip(''\n'') + ''\n</think>\n'' }} {%- else %} {{- ''<|im_start|>'' + message.role + ''\n'' }} {%- endif %}  {%- if content|length > 0 %}  {{- ''<response>\n'' + content + ''\n</response>\n'' }}  {%- endif %} {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''\n<tool_call>\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\n</tool_call>\n'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\n\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>tool'' }}\n        {%- endif %}\n        {{- ''\n<tool_output>'' }}\n        {{- message.content|tojson }}\n        {{- ''</tool_output>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\n\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n {{- \"<|im_start|>assistant\n<think>\n\"}}"}}', '[]', '[{"type":"has_code","target_id":"github:PaddlePaddle:ERNIE\"","source_url":"https://github.com/PaddlePaddle/ERNIE\""},{"type":"has_code","target_id":"github:PaddlePaddle:FastDeploy","source_url":"https://github.com/PaddlePaddle/FastDeploy"}]', NULL, 'Apache-2.0', 'approved', 83.9, '5344f8a9cdfdc47a99ebceb2c13bf40d', NULL, 'https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Thinking/resolve/main/benchmark.png', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-baidu-ERNIE-4.5-21B-A3B-Thinking from https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Thinking/resolve/main/benchmark.png
Image converted to WebP: data/images/huggingface-baidu-ERNIE-4.5-21B-A3B-Thinking.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-Qwen-Qwen3-Embedding-0.6B', 'huggingface--qwen--qwen3-embedding-0.6b', 'Qwen3-Embedding-0.6B', 'Qwen', '--- license: apache-2.0 base_model: - Qwen/Qwen3-0.6B-Base tags: - transformers - sentence-transformers - sentence-similarity - feature-extraction - text-embeddings-inference --- <p align="center"> <img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/logo_qwen3.png" width="400"/> <p> The Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. Building upon the dense foundational models of the Qw...', '["sentence-transformers","safetensors","qwen3","text-generation","transformers","sentence-similarity","feature-extraction","text-embeddings-inference","arxiv:2506.05176","base_model:qwen/qwen3-0.6b-base","base_model:finetune:qwen/qwen3-0.6b-base","license:apache-2.0","text-generation-inference","endpoints_compatible","region:us"]', 'feature-extraction', 767, 4441025, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/Qwen/Qwen3-Embedding-0.6B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nbase_model:\n- Qwen/Qwen3-0.6B-Base\ntags:\n- transformers\n- sentence-transformers\n- sentence-similarity\n- feature-extraction\n- text-embeddings-inference\n---\n# Qwen3-Embedding-0.6B\n\n<p align="center">\n    <img src="https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/logo_qwen3.png" width="400"/>\n<p>\n\n## Highlights\n\nThe Qwen3 Embedding model series is the latest proprietary model of the Qwen family, specifically designed for text embedding and ranking tasks. Building upon the dense foundational models of the Qwen3 series, it provides a comprehensive range of text embeddings and reranking models in various sizes (0.6B, 4B, and 8B). This series inherits the exceptional multilingual capabilities, long-text understanding, and reasoning skills of its foundational model. The Qwen3 Embedding series represents significant advancements in multiple text embedding and ranking tasks, including text retrieval, code retrieval, text classification, text clustering, and bitext mining.\n\n**Exceptional Versatility**: The embedding model has achieved state-of-the-art performance across a wide range of downstream application evaluations. The 8B size embedding model ranks **No.1** in the MTEB multilingual leaderboard (as of June 5, 2025, score **70.58**), while the reranking model excels in various text retrieval scenarios.\n\n**Comprehensive Flexibility**: The Qwen3 Embedding series offers a full spectrum of sizes (from 0.6B to 8B) for both embedding and reranking models, catering to diverse use cases that prioritize efficiency and effectiveness. Developers can seamlessly combine these two modules. Additionally, the embedding model allows for flexible vector definitions across all dimensions, and both embedding and reranking models support user-defined instructions to enhance performance for specific tasks, languages, or scenarios.\n\n**Multilingual Capability**: The Qwen3 Embedding series offer support for over 100 languages, thanks to the multilingual capabilites of Qwen3 models. This includes various programming languages, and provides robust multilingual, cross-lingual, and code retrieval capabilities.\n\n## Model Overview\n\n**Qwen3-Embedding-0.6B** has the following features:\n\n- Model Type: Text Embedding\n- Supported Languages: 100+ Languages\n- Number of Paramaters: 0.6B\n- Context Length: 32k\n- Embedding Dimension: Up to 1024, supports user-defined output dimensions ranging from 32 to 1024\n\nFor more details, including benchmark evaluation, hardware requirements, and inference performance, please refer to our [blog](https://qwenlm.github.io/blog/qwen3-embedding/), [GitHub](https://github.com/QwenLM/Qwen3-Embedding).\n\n## Qwen3 Embedding Series Model list\n\n| Model Type       | Models               | Size | Layers | Sequence Length | Embedding Dimension | MRL Support | Instruction Aware |\n|------------------|----------------------|------|--------|-----------------|---------------------|-------------|----------------|\n| Text Embedding   | [Qwen3-Embedding-0.6B](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B) | 0.6B | 28     | 32K             | 1024                | Yes         | Yes            |\n| Text Embedding   | [Qwen3-Embedding-4B](https://huggingface.co/Qwen/Qwen3-Embedding-4B)   | 4B   | 36     | 32K             | 2560                | Yes         | Yes            |\n| Text Embedding   | [Qwen3-Embedding-8B](https://huggingface.co/Qwen/Qwen3-Embedding-8B)   | 8B   | 36     | 32K             | 4096                | Yes         | Yes            |\n| Text Reranking   | [Qwen3-Reranker-0.6B](https://huggingface.co/Qwen/Qwen3-Reranker-0.6B) | 0.6B | 28     | 32K             | -                   | -           | Yes            |\n| Text Reranking   | [Qwen3-Reranker-4B](https://huggingface.co/Qwen/Qwen3-Reranker-4B)   | 4B   | 36     | 32K             | -                   | -           | Yes            |\n| Text Reranking   | [Qwen3-Reranker-8B](https://huggingface.co/Qwen/Qwen3-Reranker-8B)   | 8B   | 36     | 32K             | -                   | -           | Yes            |\n\n> **Note**:\n> - `MRL Support` indicates whether the embedding model supports custom dimensions for the final embedding. \n> - `Instruction Aware` notes whether the embedding or reranking model supports customizing the input instruction according to different tasks.\n> - Our evaluation indicates that, for most downstream tasks, using instructions (instruct) typically yields an improvement of 1% to 5% compared to not using them. Therefore, we recommend that developers create tailored instructions specific to their tasks and scenarios. In multilingual contexts, we also advise users to write their instructions in English, as most instructions utilized during the model training process were originally written in English.\n\n## Usage\n\nWith Transformers versions earlier than 4.51.0, you may encounter the following error:\n```\nKeyError: ''qwen3''\n```\n\n### Sentence Transformers Usage\n\n```python\n# Requires transformers>=4.51.0\n# Requires sentence-transformers>=2.7.0\n\nfrom sentence_transformers import SentenceTransformer\n\n# Load the model\nmodel = SentenceTransformer("Qwen/Qwen3-Embedding-0.6B")\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving,\n# together with setting `padding_side` to "left":\n# model = SentenceTransformer(\n#     "Qwen/Qwen3-Embedding-0.6B",\n#     model_kwargs={"attn_implementation": "flash_attention_2", "device_map": "auto"},\n#     tokenizer_kwargs={"padding_side": "left"},\n# )\n\n# The queries and documents to embed\nqueries = [\n    "What is the capital of China?",\n    "Explain gravity",\n]\ndocuments = [\n    "The capital of China is Beijing.",\n    "Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.",\n]\n\n# Encode the queries and documents. Note that queries benefit from using a prompt\n# Here we use the prompt called "query" stored under `model.prompts`, but you can\n# also pass your own prompt via the `prompt` argument\nquery_embeddings = model.encode(queries, prompt_name="query")\ndocument_embeddings = model.encode(documents)\n\n# Compute the (cosine) similarity between the query and document embeddings\nsimilarity = model.similarity(query_embeddings, document_embeddings)\nprint(similarity)\n# tensor([[0.7646, 0.1414],\n#         [0.1355, 0.6000]])\n```\n\n### Transformers Usage\n\n```python\n# Requires transformers>=4.51.0\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch import Tensor\nfrom transformers import AutoTokenizer, AutoModel\n\n\ndef last_token_pool(last_hidden_states: Tensor,\n                 attention_mask: Tensor) -> Tensor:\n    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n    if left_padding:\n        return last_hidden_states[:, -1]\n    else:\n        sequence_lengths = attention_mask.sum(dim=1) - 1\n        batch_size = last_hidden_states.shape[0]\n        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n\n\ndef get_detailed_instruct(task_description: str, query: str) -> str:\n    return f''Instruct: {task_description}\nQuery:{query}''\n\n# Each query must come with a one-sentence instruction that describes the task\ntask = ''Given a web search query, retrieve relevant passages that answer the query''\n\nqueries = [\n    get_detailed_instruct(task, ''What is the capital of China?''),\n    get_detailed_instruct(task, ''Explain gravity'')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n    "The capital of China is Beijing.",\n    "Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun."\n]\ninput_texts = queries + documents\n\ntokenizer = AutoTokenizer.from_pretrained(''Qwen/Qwen3-Embedding-0.6B'', padding_side=''left'')\nmodel = AutoModel.from_pretrained(''Qwen/Qwen3-Embedding-0.6B'')\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n# model = AutoModel.from_pretrained(''Qwen/Qwen3-Embedding-0.6B'', attn_implementation="flash_attention_2", torch_dtype=torch.float16).cuda()\n\nmax_length = 8192\n\n# Tokenize the input texts\nbatch_dict = tokenizer(\n    input_texts,\n    padding=True,\n    truncation=True,\n    max_length=max_length,\n    return_tensors="pt",\n)\nbatch_dict.to(model.device)\noutputs = model(**batch_dict)\nembeddings = last_token_pool(outputs.last_hidden_state, batch_dict[''attention_mask''])\n\n# normalize embeddings\nembeddings = F.normalize(embeddings, p=2, dim=1)\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.7645568251609802, 0.14142508804798126], [0.13549736142158508, 0.5999549627304077]]\n```\n\n### vLLM Usage\n\n```python\n# Requires vllm>=0.8.5\nimport torch\nimport vllm\nfrom vllm import LLM\n\ndef get_detailed_instruct(task_description: str, query: str) -> str:\n    return f''Instruct: {task_description}\nQuery:{query}''\n\n# Each query must come with a one-sentence instruction that describes the task\ntask = ''Given a web search query, retrieve relevant passages that answer the query''\n\nqueries = [\n    get_detailed_instruct(task, ''What is the capital of China?''),\n    get_detailed_instruct(task, ''Explain gravity'')\n]\n# No need to add instruction for retrieval documents\ndocuments = [\n    "The capital of China is Beijing.",\n    "Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun."\n]\ninput_texts = queries + documents\n\nmodel = LLM(model="Qwen/Qwen3-Embedding-0.6B", task="embed")\n\noutputs = model.embed(input_texts)\nembeddings = torch.tensor([o.outputs.embedding for o in outputs])\nscores = (embeddings[:2] @ embeddings[2:].T)\nprint(scores.tolist())\n# [[0.7620252966880798, 0.14078938961029053], [0.1358368694782257, 0.6013815999031067]]\n```\n\nüìå **Tip**: We recommend that developers customize the `instruct` according to their specific scenarios, tasks, and languages. Our tests have shown that in most retrieval scenarios, not using an `instruct` on the query side can lead to a drop in retrieval performance by approximately 1% to 5%.\n\n### Text Embeddings Inference (TEI) Usage\n\nYou can either run / deploy TEI on NVIDIA GPUs as:\n\n```bash\ndocker run --gpus all -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:cpu-1.7.2 --model-id Qwen/Qwen3-Embedding-0.6B --dtype float16\n```\n\nOr on CPU devices as:\n\n```bash\ndocker run -p 8080:80 -v hf_cache:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7.2 --model-id Qwen/Qwen3-Embedding-0.6B\n```\n\nAnd then, generate the embeddings sending a HTTP POST request as:\n\n```bash\ncurl http://localhost:8080/embed \\n    -X POST \\n    -d ''{"inputs": ["Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery: What is the capital of China?", "Instruct: Given a web search query, retrieve relevant passages that answer the query\nQuery: Explain gravity"]}'' \\n    -H "Content-Type: application/json"\n```\n\n## Evaluation\n\n### MTEB (Multilingual)\n\n| Model                            |  Size   |  Mean (Task)  | Mean (Type) | Bitxt Mining | Class. | Clust. | Inst. Retri. | Multi. Class. | Pair. Class. | Rerank | Retri. | STS  |\n|----------------------------------|:-------:|:-------------:|:-------------:|:--------------:|:--------:|:--------:|:--------------:|:---------------:|:--------------:|:--------:|:--------:|:------:|\n| NV-Embed-v2                      |   7B    |     56.29     | 49.58       | 57.84        | 57.29  | 40.80  | 1.04         | 18.63         | 78.94        | 63.82  | 56.72  | 71.10|\n| GritLM-7B                        |   7B    |     60.92     | 53.74       | 70.53        | 61.83  | 49.75  | 3.45         | 22.77         | 79.94        | 63.78  | 58.31  | 73.33|\n| BGE-M3                           |  0.6B   |     59.56     | 52.18       | 79.11        | 60.35  | 40.88  | -3.11        | 20.1          | 80.76        | 62.79  | 54.60  | 74.12|\n| multilingual-e5-large-instruct   |  0.6B   |     63.22     | 55.08       | 80.13        | 64.94  | 50.75  | -0.40        | 22.91         | 80.86        | 62.61  | 57.12  | 76.81|\n| gte-Qwen2-1.5B-instruct          |  1.5B   |     59.45     | 52.69       | 62.51        | 58.32  | 52.05  | 0.74         | 24.02         | 81.58        | 62.58  | 60.78  | 71.61|\n| gte-Qwen2-7b-Instruct            |   7B    |     62.51     | 55.93       | 73.92        | 61.55  | 52.77  | 4.94         | 25.48         | 85.13        | 65.55  | 60.08  | 73.98|\n| text-embedding-3-large           |    -    |     58.93     | 51.41       | 62.17        | 60.27  | 46.89  | -2.68        | 22.03         | 79.17        | 63.89  | 59.27  | 71.68|\n| Cohere-embed-multilingual-v3.0   |    -    |     61.12     | 53.23       | 70.50        | 62.95  | 46.89  | -1.89        | 22.74         | 79.88        | 64.07  | 59.16  | 74.80|\n| Gemini Embedding                 |    -    |     68.37     | 59.59       | 79.28        | 71.82  | 54.59  | 5.18         | **29.16**     | 83.63        | 65.58  | 67.71  | 79.40|\n| **Qwen3-Embedding-0.6B**         |  0.6B   |     64.33     | 56.00       | 72.22        | 66.83  | 52.33  | 5.09         | 24.59         | 80.83        | 61.41  | 64.64  | 76.17|\n| **Qwen3-Embedding-4B**           |   4B    |     69.45     | 60.86       | 79.36        | 72.33  | 57.15  | **11.56**    | 26.77         | 85.05        | 65.08  | 69.60  | 80.86|\n| **Qwen3-Embedding-8B**           |   8B    |   **70.58**   | **61.69**   | **80.89**    | **74.00** | **57.65** | 10.06      | 28.66         | **86.40**    | **65.63** | **70.88** | **81.08** |\n\n> **Note**: For compared models, the scores are retrieved from MTEB online [leaderboard](https://huggingface.co/spaces/mteb/leaderboard) on May 24th, 2025.\n\n### MTEB (Eng v2)\n\n| MTEB English / Models          |  Param.  | Mean(Task) | Mean(Type) | Class. | Clust. | Pair Class. | Rerank. | Retri. | STS   | Summ. |\n|--------------------------------|:--------:|:------------:|:------------:|:--------:|:--------:|:-------------:|:---------:|:--------:|:-------:|:-------:|\n| multilingual-e5-large-instruct |   0.6B   | 65.53      | 61.21      | 75.54  | 49.89  | 86.24       | 48.74   | 53.47  | 84.72 | 29.89 |\n| NV-Embed-v2                    |   7.8B   | 69.81      | 65.00      | 87.19  | 47.66  | 88.69       | 49.61   | 62.84  | 83.82 | 35.21 |\n| GritLM-7B                      |   7.2B   | 67.07      | 63.22      | 81.25  | 50.82  | 87.29       | 49.59   | 54.95  | 83.03 | 35.65 |\n| gte-Qwen2-1.5B-instruct        |   1.5B   | 67.20      | 63.26      | 85.84  | 53.54  | 87.52       | 49.25   | 50.25  | 82.51 | 33.94 |\n| stella_en_1.5B_v5              |   1.5B   | 69.43      | 65.32      | 89.38  | 57.06  | 88.02       | 50.19   | 52.42  | 83.27 | 36.91 |\n| gte-Qwen2-7B-instruct          |   7.6B   | 70.72      | 65.77      | 88.52  | 58.97  | 85.9        | 50.47   | 58.09  | 82.69 | 35.74 |\n| gemini-embedding-exp-03-07     |    -     | 73.3       | 67.67      | 90.05  | 59.39  | 87.7        | 48.59   | 64.35  | 85.29 | 38.28 |\n| **Qwen3-Embedding-0.6B**       |   0.6B   | 70.70      | 64.88      | 85.76  | 54.05  | 84.37       | 48.18   | 61.83  | 86.57 | 33.43 |\n| **Qwen3-Embedding-4B**         |    4B    | 74.60      | 68.10      | 89.84  | 57.51  | 87.01       | 50.76   | 68.46  | 88.72 | 34.39 |\n| **Qwen3-Embedding-8B**         |    8B    | 75.22      | 68.71      | 90.43  | 58.57  | 87.52       | 51.56   | 69.44  | 88.58 | 34.83 |\n\n### C-MTEB (MTEB Chinese)\n\n| C-MTEB           | Param. | Mean(Task) | Mean(Type) | Class. | Clust. | Pair Class. | Rerank. | Retr. | STS   |\n|------------------|--------|------------|------------|--------|--------|-------------|---------|-------|-------|\n| multilingual-e5-large-instruct | 0.6B   | 58.08      | 58.24      | 69.80  | 48.23  | 64.52       | 57.45   | 63.65 | 45.81 |\n| bge-multilingual-gemma2 | 9B     | 67.64      | 75.31      | 59.30  | 86.67  | 68.28       | 73.73   | 55.19 | -     |\n| gte-Qwen2-1.5B-instruct  | 1.5B   | 67.12      | 67.79      | 72.53  | 54.61  | 79.5        | 68.21   | 71.86 | 60.05 |\n| gte-Qwen2-7B-instruct    | 7.6B   | 71.62      | 72.19      | 75.77  | 66.06  | 81.16       | 69.24   | 75.70 | 65.20 |\n| ritrieve_zh_v1          | 0.3B   | 72.71      | 73.85      | 76.88  | 66.5   | 85.98       | 72.86   | 76.97 | 63.92 |\n| **Qwen3-Embedding-0.6B** | 0.6B   | 66.33      | 67.45      | 71.40  | 68.74  | 76.42       | 62.58   | 71.03 | 54.52 |\n| **Qwen3-Embedding-4B**   | 4B     | 72.27      | 73.51      | 75.46  | 77.89  | 83.34       | 66.05   | 77.03 | 61.26 |\n| **Qwen3-Embedding-8B**   | 8B     | 73.84      | 75.00      | 76.97  | 80.08  | 84.23       | 66.99   | 78.21 | 63.53 |\n\n\n## Citation\n\nIf you find our work helpful, feel free to give us a cite.\n\n```\n@article{qwen3embedding,\n  title={Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models},\n  author={Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2506.05176},\n  year={2025}\n}\n```', '{"pipeline_tag":"feature-extraction","library_name":"sentence-transformers","framework":"sentence-transformers","params":595776512,"storage_bytes":5993775548,"files_count":12,"spaces_count":68,"gated":false,"private":false,"config":{"architectures":["Qwen3ForCausalLM"],"model_type":"qwen3","tokenizer_config":{"bos_token":null,"chat_template":"{%- if tools %}\n    {{- ''<|im_start|>system\\n'' }}\n    {%- if messages[0].role == ''system'' %}\n        {{- messages[0].content + ''\\n\\n'' }}\n    {%- endif %}\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0].role == ''system'' %}\n        {{- ''<|im_start|>system\\n'' + messages[0].content + ''<|im_end|>\\n'' }}\n    {%- endif %}\n{%- endif %}\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n{%- for message in messages[::-1] %}\n    {%- set index = (messages|length - 1) - loop.index0 %}\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(message.content.startswith(''<tool_response>'') and message.content.endswith(''</tool_response>'')) %}\n        {%- set ns.multi_step_tool = false %}\n        {%- set ns.last_query_index = index %}\n    {%- endif %}\n{%- endfor %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n        {{- ''<|im_start|>'' + message.role + ''\\n'' + message.content + ''<|im_end|>'' + ''\\n'' }}\n    {%- elif message.role == \"assistant\" %}\n        {%- set content = message.content %}\n        {%- set reasoning_content = '''' %}\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\n            {%- set reasoning_content = message.reasoning_content %}\n        {%- else %}\n            {%- if ''</think>'' in message.content %}\n                {%- set content = message.content.split(''</think>'')[-1].lstrip(''\\n'') %}\n                {%- set reasoning_content = message.content.split(''</think>'')[0].rstrip(''\\n'').split(''<think>'')[-1].lstrip(''\\n'') %}\n            {%- endif %}\n        {%- endif %}\n        {%- if loop.index0 > ns.last_query_index %}\n            {%- if loop.last or (not loop.last and reasoning_content) %}\n                {{- ''<|im_start|>'' + message.role + ''\\n<think>\\n'' + reasoning_content.strip(''\\n'') + ''\\n</think>\\n\\n'' + content.lstrip(''\\n'') }}\n            {%- else %}\n                {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n            {%- endif %}\n        {%- else %}\n            {{- ''<|im_start|>'' + message.role + ''\\n'' + content }}\n        {%- endif %}\n        {%- if message.tool_calls %}\n            {%- for tool_call in message.tool_calls %}\n                {%- if (loop.first and content) or (not loop.first) %}\n                    {{- ''\\n'' }}\n                {%- endif %}\n                {%- if tool_call.function %}\n                    {%- set tool_call = tool_call.function %}\n                {%- endif %}\n                {{- ''<tool_call>\\n{\"name\": \"'' }}\n                {{- tool_call.name }}\n                {{- ''\", \"arguments\": '' }}\n                {%- if tool_call.arguments is string %}\n                    {{- tool_call.arguments }}\n                {%- else %}\n                    {{- tool_call.arguments | tojson }}\n                {%- endif %}\n                {{- ''}\\n</tool_call>'' }}\n            {%- endfor %}\n        {%- endif %}\n        {{- ''<|im_end|>\\n'' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- ''<|im_start|>user'' }}\n        {%- endif %}\n        {{- ''\\n<tool_response>\\n'' }}\n        {{- message.content }}\n        {{- ''\\n</tool_response>'' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- ''<|im_end|>\\n'' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- ''<|im_start|>assistant\\n'' }}\n    {%- if enable_thinking is defined and enable_thinking is false %}\n        {{- ''<think>\\n\\n</think>\\n\\n'' }}\n    {%- endif %}\n{%- endif %}","eos_token":"<|im_end|>","pad_token":"<|endoftext|>","unk_token":null}}}', '[]', '[{"type":"has_code","target_id":"github:QwenLM:Qwen3-Embedding","source_url":"https://github.com/QwenLM/Qwen3-Embedding"},{"type":"based_on_paper","target_id":"arxiv:2506.05176","source_url":"https://arxiv.org/abs/2506.05176"}]', NULL, 'Apache-2.0', 'approved', 78.9, '8943aadb4e812643fdc48a7768fa5cd2', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-shibing624-text2vec-base-chinese', 'huggingface--shibing624--text2vec-base-chinese', 'text2vec-base-chinese', 'shibing624', '--- license: apache-2.0 pipeline_tag: sentence-similarity tags: - Sentence Transformers - sentence-similarity - sentence-transformers datasets: - shibing624/nli_zh language: - zh library_name: sentence-transformers --- This is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese. It maps sentences to a 768 dimensional dense vector space and can be used for tasks like sentence embeddings, text matching or semantic search. For an automated evaluation of this model, see the *Evaluat...', '["sentence-transformers","pytorch","onnx","safetensors","openvino","bert","sentence transformers","sentence-similarity","zh","dataset:shibing624/nli_zh","license:apache-2.0","endpoints_compatible","deploy:azure","region:us"]', 'sentence-similarity', 765, 479992, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/shibing624/text2vec-base-chinese","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\nlicense: apache-2.0\npipeline_tag: sentence-similarity\ntags:\n- Sentence Transformers\n- sentence-similarity\n- sentence-transformers\ndatasets:\n- shibing624/nli_zh\nlanguage:\n- zh\nlibrary_name: sentence-transformers\n---\n\n\n# shibing624/text2vec-base-chinese\nThis is a CoSENT(Cosine Sentence) model: shibing624/text2vec-base-chinese.\n\nIt maps sentences to a 768 dimensional dense vector space and can be used for tasks \nlike sentence embeddings, text matching or semantic search.\n\n\n## Evaluation\nFor an automated evaluation of this model, see the *Evaluation Benchmark*: [text2vec](https://github.com/shibing624/text2vec)\n\n- chinese text matching taskÔºö\n\n| Arch       | BaseModel                         | Model                                                                                                                                             | ATEC  |  BQ   | LCQMC | PAWSX | STS-B | SOHU-dd | SOHU-dc |    Avg    |  QPS  |\n|:-----------|:----------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------|:-----:|:-----:|:-----:|:-----:|:-----:|:-------:|:-------:|:---------:|:-----:|\n| Word2Vec   | word2vec                          | [w2v-light-tencent-chinese](https://ai.tencent.com/ailab/nlp/en/download.html)                                                                    | 20.00 | 31.49 | 59.46 | 2.57  | 55.78 |  55.04  |  20.70  |   35.03   | 23769 |\n| SBERT      | xlm-roberta-base                  | [sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2) | 18.42 | 38.52 | 63.96 | 10.14 | 78.90 |  63.01  |  52.28  |   46.46   | 3138  |\n| Instructor | hfl/chinese-roberta-wwm-ext       | [moka-ai/m3e-base](https://huggingface.co/moka-ai/m3e-base)                                                                                       | 41.27 | 63.81 | 74.87 | 12.20 | 76.96 |  75.83  |  60.55  |   57.93   | 2980  |\n| CoSENT     | hfl/chinese-macbert-base          | [shibing624/text2vec-base-chinese](https://huggingface.co/shibing624/text2vec-base-chinese)                                                       | 31.93 | 42.67 | 70.16 | 17.21 | 79.30 |  70.27  |  50.42  |   51.61   | 3008  |\n| CoSENT     | hfl/chinese-lert-large            | [GanymedeNil/text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese)                                                   | 32.61 | 44.59 | 69.30 | 14.51 | 79.44 |  73.01  |  59.04  |   53.12   | 2092  |\n| CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-sentence](https://huggingface.co/shibing624/text2vec-base-chinese-sentence)                                     | 43.37 | 61.43 | 73.48 | 38.90 | 78.25 |  70.60  |  53.08  |   59.87   | 3089  |\n| CoSENT     | nghuyong/ernie-3.0-base-zh        | [shibing624/text2vec-base-chinese-paraphrase](https://huggingface.co/shibing624/text2vec-base-chinese-paraphrase)                                 | 44.89 | 63.58 | 74.24 | 40.90 | 78.93 |  76.70  |  63.30  |    63.08  | 3066  |\n| CoSENT     | sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2  | [shibing624/text2vec-base-multilingual](https://huggingface.co/shibing624/text2vec-base-multilingual)                                             | 32.39 | 50.33 | 65.64 | 32.56 | 74.45 |  68.88  |  51.17  |   53.67   | 4004  |\n\n\nËØ¥ÊòéÔºö\n- ÁªìÊûúËØÑÊµãÊåáÊ†áÔºöspearmanÁ≥ªÊï∞\n- `shibing624/text2vec-base-chinese`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`hfl/chinese-macbert-base`Âú®‰∏≠ÊñáSTS-BÊï∞ÊçÆËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠ÊñáSTS-BÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞ËæÉÂ•ΩÊïàÊûúÔºåËøêË°å[examples/training_sup_text_matching_model.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠ÊñáÈÄöÁî®ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®\n- `shibing624/text2vec-base-chinese-sentence`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`nghuyong/ernie-3.0-base-zh`Áî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑ‰∏≠ÊñáSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)ËÆ≠ÁªÉÂæóÂà∞ÔºåÂπ∂Âú®‰∏≠ÊñáÂêÑNLIÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞ËæÉÂ•ΩÊïàÊûúÔºåËøêË°å[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠Êñás2s(Âè•Â≠êvsÂè•Â≠ê)ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®\n- `shibing624/text2vec-base-chinese-paraphrase`Ê®°ÂûãÔºåÊòØÁî®CoSENTÊñπÊ≥ïËÆ≠ÁªÉÔºåÂü∫‰∫é`nghuyong/ernie-3.0-base-zh`Áî®‰∫∫Â∑•ÊåëÈÄâÂêéÁöÑ‰∏≠ÊñáSTSÊï∞ÊçÆÈõÜ[shibing624/nli-zh-all/text2vec-base-chinese-paraphrase-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-paraphrase-dataset)ÔºåÊï∞ÊçÆÈõÜÁõ∏ÂØπ‰∫é[shibing624/nli-zh-all/text2vec-base-chinese-sentence-dataset](https://huggingface.co/datasets/shibing624/nli-zh-all/tree/main/text2vec-base-chinese-sentence-dataset)Âä†ÂÖ•‰∫Üs2p(sentence to paraphrase)Êï∞ÊçÆÔºåÂº∫Âåñ‰∫ÜÂÖ∂ÈïøÊñáÊú¨ÁöÑË°®ÂæÅËÉΩÂäõÔºåÂπ∂Âú®‰∏≠ÊñáÂêÑNLIÊµãËØïÈõÜËØÑ‰º∞ËææÂà∞SOTAÔºåËøêË°å[examples/training_sup_text_matching_model_jsonl_data.py](https://github.com/shibing624/text2vec/blob/master/examples/training_sup_text_matching_model_jsonl_data.py)‰ª£Á†ÅÂèØËÆ≠ÁªÉÊ®°ÂûãÔºåÊ®°ÂûãÊñá‰ª∂Â∑≤Áªè‰∏ä‰º†HF model hubÔºå‰∏≠Êñás2p(Âè•Â≠êvsÊÆµËêΩ)ËØ≠‰πâÂåπÈÖç‰ªªÂä°Êé®Ëçê‰ΩøÁî®\n- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`Ê®°ÂûãÊòØÁî®SBERTËÆ≠ÁªÉÔºåÊòØ`paraphrase-MiniLM-L12-v2`Ê®°ÂûãÁöÑÂ§öËØ≠Ë®ÄÁâàÊú¨ÔºåÊîØÊåÅ‰∏≠Êñá„ÄÅËã±ÊñáÁ≠â\n- `w2v-light-tencent-chinese`ÊòØËÖæËÆØËØçÂêëÈáèÁöÑWord2VecÊ®°ÂûãÔºåCPUÂä†ËΩΩ‰ΩøÁî®ÔºåÈÄÇÁî®‰∫é‰∏≠ÊñáÂ≠óÈù¢ÂåπÈÖç‰ªªÂä°ÂíåÁº∫Â∞ëÊï∞ÊçÆÁöÑÂÜ∑ÂêØÂä®ÊÉÖÂÜµ\n\n## Usage (text2vec)\nUsing this model becomes easy when you have [text2vec](https://github.com/shibing624/text2vec) installed:\n\n```\npip install -U text2vec\n```\n\nThen you can use the model like this:\n\n```python\nfrom text2vec import SentenceModel\nsentences = [''Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°'', ''Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°'']\n\nmodel = SentenceModel(''shibing624/text2vec-base-chinese'')\nembeddings = model.encode(sentences)\nprint(embeddings)\n```\n\n## Usage (HuggingFace Transformers)\nWithout [text2vec](https://github.com/shibing624/text2vec), you can use the model like this: \n\nFirst, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\nInstall transformers:\n```\npip install transformers\n```\n\nThen load model and predict:\n```python\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\n# Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n# Load model from HuggingFace Hub\ntokenizer = BertTokenizer.from_pretrained(''shibing624/text2vec-base-chinese'')\nmodel = BertModel.from_pretrained(''shibing624/text2vec-base-chinese'')\nsentences = [''Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°'', ''Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°'']\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors=''pt'')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n# Perform pooling. In this case, mean pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input[''attention_mask''])\nprint("Sentence embeddings:")\nprint(sentence_embeddings)\n```\n\n## Usage (sentence-transformers)\n[sentence-transformers](https://github.com/UKPLab/sentence-transformers) is a popular library to compute dense vector representations for sentences.\n\nInstall sentence-transformers:\n```\npip install -U sentence-transformers\n```\n\nThen load model and predict:\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\nm = SentenceTransformer("shibing624/text2vec-base-chinese")\nsentences = [''Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°'', ''Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°'']\n\nsentence_embeddings = m.encode(sentences)\nprint("Sentence embeddings:")\nprint(sentence_embeddings)\n```\n\n## Model speed up\n\n\n| Model                                                                                                                        | ATEC              | BQ                | LCQMC            | PAWSX            | STSB             |\n|------------------------------------------------------------------------------------------------------------------------------|-------------------|-------------------|------------------|------------------|------------------|\n| shibing624/text2vec-base-chinese (fp32, baseline)                                                                            | 0.31928           | 0.42672           | 0.70157          | 0.17214          | 0.79296          |\n| shibing624/text2vec-base-chinese (onnx-O4, [#29](https://huggingface.co/shibing624/text2vec-base-chinese/discussions/29))    | 0.31928           | 0.42672           | 0.70157          | 0.17214          | 0.79296          |\n| shibing624/text2vec-base-chinese (ov, [#27](https://huggingface.co/shibing624/text2vec-base-chinese/discussions/27))         | 0.31928           | 0.42672           | 0.70157          | 0.17214          | 0.79296          |\n| shibing624/text2vec-base-chinese (ov-qint8, [#30](https://huggingface.co/shibing624/text2vec-base-chinese/discussions/30))   | 0.30778 (-3.60%)  | 0.43474 (+1.88%)  | 0.69620 (-0.77%) | 0.16662 (-3.20%) | 0.79396 (+0.13%) |\n\nIn short:\n1. ‚úÖ shibing624/text2vec-base-chinese (onnx-O4), ONNX Optimized to [O4](https://huggingface.co/docs/optimum/en/onnxruntime/usage_guides/optimization) does not reduce performance, but gives a [~2x speedup](https://sbert.net/docs/sentence_transformer/usage/efficiency.html#benchmarks) on GPU.\n2. ‚úÖ shibing624/text2vec-base-chinese (ov), OpenVINO does not reduce performance, but gives a 1.12x speedup on CPU.\n3. üü° shibing624/text2vec-base-chinese (ov-qint8), int8 quantization with OV incurs a small performance hit on some tasks, and a tiny performance gain on others, when quantizing with [Chinese STSB](https://huggingface.co/datasets/PhilipMay/stsb_multi_mt). Additionally, it results in a [4.78x speedup](https://sbert.net/docs/sentence_transformer/usage/efficiency.html#benchmarks) on CPU.\n\n- usage: shibing624/text2vec-base-chinese (onnx-O4), for gpu\n```python\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    "shibing624/text2vec-base-chinese",\n    backend="onnx",\n    model_kwargs={"file_name": "model_O4.onnx"},\n)\nembeddings = model.encode(["Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°", "Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°", "‰Ω†ÊòØË∞Å"])\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n```\n\n\n- usage: shibing624/text2vec-base-chinese (ov), for cpu\n```python\n# pip install ''optimum[openvino]''\n\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    "shibing624/text2vec-base-chinese",\n    backend="openvino",\n)\n\nembeddings = model.encode(["Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°", "Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°", "‰Ω†ÊòØË∞Å"])\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n```\n\n- usage: shibing624/text2vec-base-chinese (ov-qint8), for cpu\n```python\n# pip install optimum\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    "shibing624/text2vec-base-chinese",\n    backend="onnx",\n    model_kwargs={"file_name": "model_qint8_avx512_vnni.onnx"},\n)\nembeddings = model.encode(["Â¶Ç‰ΩïÊõ¥Êç¢Ëä±ÂëóÁªëÂÆöÈì∂Ë°åÂç°", "Ëä±ÂëóÊõ¥ÊîπÁªëÂÆöÈì∂Ë°åÂç°", "‰Ω†ÊòØË∞Å"])\nprint(embeddings.shape)\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities)\n```\n\n\n## Full Model Architecture\n```\nCoSENT(\n  (0): Transformer({''max_seq_length'': 128, ''do_lower_case'': False}) with Transformer model: BertModel \n  (1): Pooling({''word_embedding_dimension'': 768, ''pooling_mode_mean_tokens'': True})\n)\n```\n\n## Intended uses\n\nOur model is intented to be used as a sentence and short paragraph encoder. Given an input text, it ouptuts a vector which captures \nthe semantic information. The sentence vector may be used for information retrieval, clustering or sentence similarity tasks.\n\nBy default, input text longer than 256 word pieces is truncated.\n\n\n## Training procedure\n\n### Pre-training \n\nWe use the pretrained [`hfl/chinese-macbert-base`](https://huggingface.co/hfl/chinese-macbert-base) model. \nPlease refer to the model card for more detailed information about the pre-training procedure.\n\n### Fine-tuning \n\nWe fine-tune the model using a contrastive objective. Formally, we compute the cosine similarity from each \npossible sentence pairs from the batch.\nWe then apply the rank loss by comparing with true pairs and false pairs.\n\n#### Hyper parameters\n\n- training dataset: https://huggingface.co/datasets/shibing624/nli_zh\n- max_seq_length: 128\n- best epoch: 5\n- sentence embedding dim: 768\n\n\n\n## Citing & Authors\nThis model was trained by [text2vec](https://github.com/shibing624/text2vec). \n        \nIf you find this model helpful, feel free to cite:\n```bibtex \n@software{text2vec,\n  author = {Xu Ming},\n  title = {text2vec: A Tool for Text to Vector},\n  year = {2022},\n  url = {https://github.com/shibing624/text2vec},\n}\n```', '{"pipeline_tag":"sentence-similarity","library_name":"sentence-transformers","framework":"sentence-transformers","params":102268160,"storage_bytes":2040942336,"files_count":22,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BertModel"],"model_type":"bert","tokenizer_config":{"unk_token":"[UNK]","sep_token":"[SEP]","pad_token":"[PAD]","cls_token":"[CLS]","mask_token":"[MASK]"}}}', '[]', '[{"type":"has_code","target_id":"github:shibing624:text2vec","source_url":"https://github.com/shibing624/text2vec"},{"type":"has_code","target_id":"github:shibing624:text2vec","source_url":"https://github.com/shibing624/text2vec"},{"type":"has_code","target_id":"github:shibing624:text2vec","source_url":"https://github.com/shibing624/text2vec"},{"type":"has_code","target_id":"github:shibing624:text2vec","source_url":"https://github.com/shibing624/text2vec"},{"type":"has_code","target_id":"github:shibing624:text2vec","source_url":"https://github.com/shibing624/text2vec"},{"type":"has_code","target_id":"github:shibing624:text2vec","source_url":"https://github.com/shibing624/text2vec"},{"type":"has_code","target_id":"github:UKPLab:sentence-transformers","source_url":"https://github.com/UKPLab/sentence-transformers"},{"type":"has_code","target_id":"github:shibing624:text2vec","source_url":"https://github.com/shibing624/text2vec"},{"type":"has_code","target_id":"github:shibing624:text2vec},","source_url":"https://github.com/shibing624/text2vec},"}]', NULL, 'Apache-2.0', 'approved', 78.8, '865e8389d8606a4d1fb1da99d3bdf79f', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-WizardLMTeam-WizardCoder-15B-V1.0', 'huggingface--wizardlmteam--wizardcoder-15b-v1.0', 'WizardCoder-15B-V1.0', 'WizardLMTeam', '--- license: bigscience-openrail-m metrics: - code_eval library_name: transformers tags: - code model-index: - name: WizardCoder results: - task: type: text-generation dataset: type: openai_humaneval name: HumanEval metrics: - name: pass@1 type: pass@1 value: 0.573 verified: false --- <p style="font-size:28px;" align="center"> üè† <a href="https://wizardlm.github.io/" target="_blank">Home Page</a> </p> <p align="center"> <p align="center"> ü§ó <a href="https://huggingface.co/WizardLM" target="_...', '["transformers","pytorch","gpt_bigcode","text-generation","code","arxiv:2304.12244","arxiv:2306.08568","arxiv:2308.09583","license:bigscience-openrail-m","model-index","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 762, 566, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/WizardLMTeam/WizardCoder-15B-V1.0","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: bigscience-openrail-m\nmetrics:\n- code_eval\nlibrary_name: transformers\ntags:\n- code\nmodel-index:\n- name: WizardCoder\n  results:\n  - task:\n      type: text-generation\n    dataset:\n      type: openai_humaneval\n      name: HumanEval\n    metrics:\n    - name: pass@1\n      type: pass@1\n      value: 0.573\n      verified: false\n---\n \n## WizardCoder: Empowering Code Large Language Models with Evol-Instruct\n\n<p style="font-size:28px;" align="center">\nüè† <a href="https://wizardlm.github.io/" target="_blank">Home Page</a> </p>\n<p align="center">\n<p align="center">\nü§ó <a href="https://huggingface.co/WizardLM" target="_blank">HF Repo</a>  ‚Ä¢üê± <a href="https://github.com/nlpxucan/WizardLM" target="_blank">Github Repo</a> ‚Ä¢ üê¶ <a href="https://twitter.com/WizardLM_AI" target="_blank">Twitter</a> </p>\n<p align="center">\n üìÉ <a href="https://arxiv.org/abs/2304.12244" target="_blank">[WizardLM]</a>  ‚Ä¢ üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>   ‚Ä¢ üìÉ <a href="https://arxiv.org/abs/2308.09583" target="_blank">[WizardMath]</a>  <br>\n</p>\n<p align="center">\n    üëã Join our <a href="https://discord.gg/VZjjHtWrKs" target="_blank">Discord</a>\n</p>\n\n## News\n\n[2024/01/04] üî• We released **WizardCoder-33B-V1.1**  trained from deepseek-coder-33b-base, the **SOTA OSS Code LLM** on [EvalPlus Leaderboard](https://evalplus.github.io/leaderboard.html), achieves **79.9 pass@1** on HumanEval, **73.2 pass@1** on HumanEval-Plus, **78.9 pass@1** on MBPP, and **66.9 pass@1** on MBPP-Plus.\n\n[2024/01/04] üî• **WizardCoder-33B-V1.1** outperforms **ChatGPT 3.5**, **Gemini Pro**, and **DeepSeek-Coder-33B-instruct** on HumanEval and HumanEval-Plus pass@1.\n\n[2024/01/04] üî• **WizardCoder-33B-V1.1** is comparable with **ChatGPT 3.5**, and surpasses **Gemini Pro** on MBPP and MBPP-Plus pass@1.\n\n|  Model  |  Checkpoint  | Paper    | HumanEval  |   HumanEval+ | MBPP | MBPP+ | License |\n| ----- |------| ---- |------|-------| ----- |  ----- |----- | \n|  GPT-4-Turbo (Nov 2023)  | - | - | 85.4  | 81.7 | 83.0 | 70.7 |-|\n|  GPT-4 (May 2023)  | - | - | 88.4  | 76.8 | - | - |-|\n|  GPT-3.5-Turbo (Nov 2023)  | - | - | 72.6  | 65.9 | 81.7 | 69.4 |-|\n|  Gemini Pro  | - | - | 63.4  | 55.5 | 72.9 | 57.9 |-|\n|  DeepSeek-Coder-33B-instruct | - | - |  78.7 | 72.6 | 78.7 | 66.7 |-|\n|  **WizardCoder-33B-V1.1**  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-33B-V1.1" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  79.9  | 73.2 | 78.9 | 66.9 |  <a href="https://huggingface.co/WizardLM/WizardMath-7B-V1.1/resolve/main/LICENSE" target="_blank">MSFTResearch</a>  |\n|  WizardCoder-Python-34B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  73.2   | 64.6 | 73.2 | 59.9 |  <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama2</a>  |\n|  WizardCoder-15B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-15B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  59.8   | 52.4 | -- | -- |  <a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement" target="_blank">OpenRAIL-M</a>  |\n|  WizardCoder-Python-13B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-Python-13B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  64.0   | -- | -- | -- |  <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama2</a>  |\n|  WizardCoder-Python-7B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-Python-7B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  55.5   | -- | -- | -- |  <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama2</a>  |\n|  WizardCoder-3B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-3B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  34.8   | -- | -- | -- |  <a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement" target="_blank">OpenRAIL-M</a>  |\n|  WizardCoder-1B-V1.0  |   ü§ó <a href="https://huggingface.co/WizardLM/WizardCoder-1B-V1.0" target="_blank">HF Link</a>   |  üìÉ <a href="https://arxiv.org/abs/2306.08568" target="_blank">[WizardCoder]</a>  |  23.8   | -- | -- | -- |  <a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement" target="_blank">OpenRAIL-M</a>  |\n\n<p align="center" width="100%">\n<a ><img src="https://raw.githubusercontent.com/nlpxucan/WizardLM/main/WizardCoder/imgs/compare_sota.png" alt="WizardCoder" style="width: 96%; min-width: 300px; display: block; margin: auto;"></a>\n</p>\n\n- üî• [08/11/2023] We release **WizardMath** Models.\n- üî• Our **WizardMath-70B-V1.0** model slightly outperforms some closed-source LLMs on the GSM8K, including **ChatGPT 3.5**, **Claude Instant 1** and **PaLM 2 540B**.\n- üî• Our **WizardMath-70B-V1.0** model achieves  **81.6 pass@1** on the [GSM8k Benchmarks](https://github.com/openai/grade-school-math), which is **24.8** points higher than the SOTA open-source LLM.\n- üî• Our **WizardMath-70B-V1.0** model achieves  **22.7 pass@1** on the [MATH Benchmarks](https://github.com/hendrycks/math), which is **9.2** points higher than the SOTA open-source LLM.\n\n| Model | Checkpoint | Paper  | GSM8k | MATH  |Online Demo| License|\n| ----- |------| ---- |------|-------| ----- | ----- |\n| WizardMath-70B-V1.0 | ü§ó <a href="https://huggingface.co/WizardLM/WizardMath-70B-V1.0" target="_blank">HF Link</a> |  üìÉ <a href="https://arxiv.org/abs/2308.09583" target="_blank">[WizardMath]</a>| **81.6**  |  **22.7**	|[Demo](http://47.103.63.15:50083/)| <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2  </a> |\n| WizardMath-13B-V1.0 | ü§ó <a href="https://huggingface.co/WizardLM/WizardMath-13B-V1.0" target="_blank">HF Link</a> |  üìÉ <a href="https://arxiv.org/abs/2308.09583" target="_blank">[WizardMath]</a>| **63.9**  |  **14.0** |[Demo](http://47.103.63.15:50082/)| <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2 </a> |\n| WizardMath-7B-V1.0 | ü§ó <a href="https://huggingface.co/WizardLM/WizardMath-7B-V1.0" target="_blank">HF Link</a>  |  üìÉ <a href="https://arxiv.org/abs/2308.09583" target="_blank">[WizardMath]</a>| 	 **54.9**  |  **10.7** | [Demo](http://47.103.63.15:50080/)|  <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2  </a>|    \n\n\n<font size=4>\n    \n| <sup>Model</sup> | <sup>Checkpoint</sup> | <sup>Paper</sup> |<sup>MT-Bench</sup> | <sup>AlpacaEval</sup> | <sup>WizardEval</sup> | <sup>HumanEval</sup>  | <sup>License</sup>|\n| ----- |------| ---- |------|-------| ----- | ----- | ----- |\n| <sup>WizardLM-13B-V1.2</sup> | <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.2" target="_blank">HF Link</a> </sup>|  | <sup>7.06</sup> | <sup>89.17%</sup>	 | <sup>101.4% </sup>|<sup>36.6  pass@1</sup>|<sup> <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" target="_blank">Llama 2 License </a></sup> |\n| <sup>WizardLM-13B-V1.1</sup> |<sup> ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.1" target="_blank">HF Link</a> </sup> |  | <sup>6.76</sup>  |<sup>86.32%</sup>	 | <sup>99.3% </sup> |<sup>25.0  pass@1</sup>| <sup>Non-commercial</sup>|\n| <sup>WizardLM-30B-V1.0</sup> | <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-30B-V1.0" target="_blank">HF Link</a></sup>  | | <sup>7.01</sup> |  |  <sup>97.8% </sup> | <sup>37.8  pass@1</sup>| <sup>Non-commercial</sup> |\n| <sup>WizardLM-13B-V1.0</sup> | <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-13B-V1.0" target="_blank">HF Link</a> </sup> |  | <sup>6.35</sup> | <sup>75.31%</sup> |  <sup>89.1% </sup> |<sup> 24.0 pass@1 </sup> | <sup>Non-commercial</sup>|\n| <sup>WizardLM-7B-V1.0 </sup>|  <sup>ü§ó <a href="https://huggingface.co/WizardLM/WizardLM-7B-V1.0" target="_blank">HF Link</a> </sup> |<sup> üìÉ <a href="https://arxiv.org/abs/2304.12244" target="_blank">[WizardLM]</a> </sup>|  |  |  <sup>78.0% </sup> |<sup>19.1 pass@1 </sup>|<sup> Non-commercial</sup>|\n</font>\n\n\n\n\n\n# WizardCoder: Empowering Code Large Language Models with Evol-Instruct\n\n\nTo develop our WizardCoder model, we begin by adapting the Evol-Instruct method specifically for coding tasks. This involves tailoring the prompt to the domain of code-related instructions. Subsequently, we fine-tune the Code LLM, StarCoder, utilizing the newly created instruction-following training set.\n\n## News\n\n- üî• Our **WizardCoder-15B-v1.0** model achieves the **57.3 pass@1** on the [HumanEval Benchmarks](https://github.com/openai/human-eval), which is **22.3** points higher than the SOTA open-source Code LLMs.\n- üî• We released **WizardCoder-15B-v1.0** trained with **78k** evolved code instructions. Please checkout the [Model Weights](https://huggingface.co/WizardLM/WizardCoder-15B-V1.0), and [Paper]().\n- &#x1F4E3; Please refer to our Twitter account https://twitter.com/WizardLM_AI and HuggingFace Repo https://huggingface.co/WizardLM . We will use them to announce any new release at the 1st time. \n\n\n## Comparing WizardCoder with the Closed-Source Models.\n\n\nüî• The following figure shows that our **WizardCoder attains the third position in this benchmark**, surpassing Claude-Plus (59.8 vs. 53.0) and Bard (59.8 vs. 44.5). Notably, our model exhibits a substantially smaller size compared to these models.\n\n<p align="center" width="100%">\n<a ><img src="https://raw.githubusercontent.com/nlpxucan/WizardLM/main/WizardCoder/imgs/pass1.png" alt="WizardCoder" style="width: 86%; min-width: 300px; display: block; margin: auto;"></a>\n</p>\n\n‚ùó**Note: In this study, we copy the scores for HumanEval and HumanEval+ from the [LLM-Humaneval-Benchmarks](https://github.com/my-other-github-account/llm-humaneval-benchmarks). Notably, all the mentioned models generate code solutions for each problem utilizing a **single attempt**, and the resulting pass rate percentage is reported. Our **WizardCoder** generates answers using greedy decoding and tests with the same [code](https://github.com/evalplus/evalplus).**\n\n## Comparing WizardCoder with the Open-Source Models.\n\nThe following table clearly demonstrates that our **WizardCoder** exhibits a substantial performance advantage over all the open-source models. ‚ùó**If you are confused with the different scores of our model (57.3 and 59.8), please check the Notes.**\n\n\n| Model            | HumanEval Pass@1 | MBPP Pass@1 |\n|------------------|------------------|-------------|\n| CodeGen-16B-Multi| 18.3             |20.9         |\n| CodeGeeX         | 22.9             |24.4         |\n| LLaMA-33B        | 21.7             |30.2         |\n| LLaMA-65B        | 23.7             |37.7         |\n| PaLM-540B        | 26.2             |36.8         |\n| PaLM-Coder-540B  | 36.0             |47.0         |\n| PaLM 2-S         | 37.6             |50.0         |\n| CodeGen-16B-Mono | 29.3             |35.3         |\n| Code-Cushman-001 | 33.5             |45.9         |\n| StarCoder-15B    | 33.6             |43.6*        |\n| InstructCodeT5+  | 35.0             |--           |\n| WizardLM-30B  1.0| 37.8             |--           |\n| WizardCoder-15B  1.0 | **57.3**     |**51.8**     |\n\n\n‚ùó**Note: The reproduced result of StarCoder on MBPP.**\n\n‚ùó**Note: The above table conducts a comprehensive comparison of our **WizardCoder** with other models on the HumanEval and MBPP benchmarks. We adhere to the approach outlined in previous studies by generating **20 samples** for each problem to estimate the pass@1 score and evaluate with the same [code](https://github.com/openai/human-eval/tree/master). The scores of GPT4 and GPT3.5 reported by [OpenAI](https://openai.com/research/gpt-4) are 67.0 and 48.1 (maybe these are the early version GPT4&3.5).**\n\n## Call for Feedbacks\nWe welcome everyone to use your professional and difficult instructions to evaluate WizardCoder, and show us examples of poor performance and your suggestions in the [issue discussion](https://github.com/nlpxucan/WizardLM/issues) area. We are focusing on improving the Evol-Instruct now and hope to relieve existing weaknesses and issues in the the next version of WizardCoder. After that, we will open the code and pipeline of up-to-date Evol-Instruct algorithm and work with you together to improve it.\n\n\n## Contents\n\n1. [Online Demo](#online-demo)\n\n2. [Fine-tuning](#fine-tuning)\n\n3. [Inference](#inference)\n\n4. [Evaluation](#evaluation)\n\n5. [Citation](#citation)\n\n6. [Disclaimer](#disclaimer)\n\n## Online Demo\n\nWe will provide our latest models for you to try for as long as possible. If you find a link is not working, please try another one. At the same time, please try as many **real-world** and **challenging** code-related problems that you encounter in your work and life as possible. We will continue to evolve our models with your feedbacks.\n\n\n\n## Fine-tuning\n\nWe fine-tune WizardCoder using the modified code `train.py` from [Llama-X](https://github.com/AetherCortex/Llama-X).\nWe fine-tune StarCoder-15B with the following hyperparameters:\n\n| Hyperparameter | StarCoder-15B |\n|----------------|---------------|\n| Batch size     | 512           |\n| Learning rate  | 2e-5          |\n| Epochs         | 3             |\n| Max length     | 2048          |\n| Warmup step    | 30            |\n| LR scheduler   | cosine        |\n\nTo reproduce our fine-tuning of WizardCoder, please follow the following steps:\n1. According to the instructions of [Llama-X](https://github.com/AetherCortex/Llama-X), install the environment, download the training code, and deploy. (Note: `deepspeed==0.9.2` and `transformers==4.29.2`)\n2. Replace the `train.py` with the `train_wizardcoder.py` in our repo (`src/train_wizardcoder.py`)\n3. Login Huggingface:\n```bash\nhuggingface-cli login\n```\n4. Execute the following training command:\n```bash\ndeepspeed train_wizardcoder.py \\n    --model_name_or_path "bigcode/starcoder" \\n    --data_path "/your/path/to/code_instruction_data.json" \\n    --output_dir "/your/path/to/ckpt" \\n    --num_train_epochs 3 \\n    --model_max_length 2048 \\n    --per_device_train_batch_size 16 \\n    --per_device_eval_batch_size 1 \\n    --gradient_accumulation_steps 4 \\n    --evaluation_strategy "no" \\n    --save_strategy "steps" \\n    --save_steps 50 \\n    --save_total_limit 2 \\n    --learning_rate 2e-5 \\n    --warmup_steps 30 \\n    --logging_steps 2 \\n    --lr_scheduler_type "cosine" \\n    --report_to "tensorboard" \\n    --gradient_checkpointing True \\n    --deepspeed configs/deepspeed_config.json \\n    --fp16 True\n```\n\n## Inference\n\nWe provide the decoding script for WizardCoder, which reads a input file and generates corresponding responses for each sample, and finally consolidates them into an output file.\n\nYou can specify `base_model`, `input_data_path` and `output_data_path` in `src\inference_wizardcoder.py` to set the decoding model, path of input file and path of output file.\n\n```bash\npip install jsonlines\n```\n\nThe decoding command is:\n```\npython src\inference_wizardcoder.py \\n    --base_model "/your/path/to/ckpt" \\n    --input_data_path "/your/path/to/input/data.jsonl" \\n    --output_data_path "/your/path/to/output/result.jsonl"\n```\n\nThe format of `data.jsonl` should be:\n```\n{"idx": 11, "Instruction": "Write a Python code to count 1 to 10."}\n{"idx": 12, "Instruction": "Write a Jave code to sum 1 to 10."}\n```\n\nThe prompt for our WizardCoder in `src\inference_wizardcoder.py` is:\n```\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n```\n\n## Evaluation\n\nWe provide the evaluation script on HumanEval for WizardCoder.\n\n1. According to the instructions of [HumanEval](https://github.com/openai/human-eval), install the environment.\n2. Run the following script to generate the answer.\n```bash\nmodel="/path/to/your/model"\ntemp=0.2\nmax_len=2048\npred_num=200\nnum_seqs_per_iter=2\n\noutput_path=preds/T${temp}_N${pred_num}\n\nmkdir -p ${output_path}\necho ''Output path: ''$output_path\necho ''Model to eval: ''$model\n\n# 164 problems, 21 per GPU if GPU=8\nindex=0\ngpu_num=8\nfor ((i = 0; i < $gpu_num; i++)); do\n  start_index=$((i * 21))\n  end_index=$(((i + 1) * 21))\n\n  gpu=$((i))\n  echo ''Running process #'' ${i} ''from'' $start_index ''to'' $end_index ''on GPU'' ${gpu}\n  ((index++))\n  (\n    CUDA_VISIBLE_DEVICES=$gpu python humaneval_gen.py --model ${model} \\n      --start_index ${start_index} --end_index ${end_index} --temperature ${temp} \\n      --num_seqs_per_iter ${num_seqs_per_iter} --N ${pred_num} --max_len ${max_len} --output_path ${output_path}\n  ) &\n  if (($index % $gpu_num == 0)); then wait; fi\ndone\n```\n3. Run the post processing code `src/process_humaneval.py` to collect the code completions from all answer files.\n```bash\noutput_path=preds/T${temp}_N${pred_num}\n\necho ''Output path: ''$output_path\npython process_humaneval.py --path ${output_path} --out_path ${output_path}.jsonl --add_prompt\n\nevaluate_functional_correctness ${output_path}.jsonl\n```\n\n## Citation\n\nPlease cite the repo if you use the data, method or code in this repo.\n\n```\n@article{luo2023wizardcoder,\n  title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct},\n  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},\n  journal={arXiv preprint arXiv:2306.08568},\n  year={2023}\n}\n```\n## Disclaimer\n\nWizardCoder model follows the same license as StarCoder. The content produced by any version of WizardCoder is influenced by uncontrollable variables such as randomness, and therefore, the accuracy of the output cannot be guaranteed by this project. This project does not accept any legal liability for the content of the model output, nor does it assume responsibility for any losses incurred due to the use of associated resources and output results.', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":62070055381,"files_count":11,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["GPTBigCodeForCausalLM"],"model_type":"gpt_bigcode","tokenizer_config":{"bos_token":"<|endoftext|>","eos_token":"<|endoftext|>","unk_token":"<|endoftext|>"}}}', '[]', '[{"type":"has_code","target_id":"github:nlpxucan:WizardLM\"","source_url":"https://github.com/nlpxucan/WizardLM\""},{"type":"has_code","target_id":"github:openai:grade-school-math","source_url":"https://github.com/openai/grade-school-math"},{"type":"has_code","target_id":"github:hendrycks:math","source_url":"https://github.com/hendrycks/math"},{"type":"has_code","target_id":"github:openai:human-eval","source_url":"https://github.com/openai/human-eval"},{"type":"has_code","target_id":"github:my-other-github-account:llm-humaneval-benchmarks","source_url":"https://github.com/my-other-github-account/llm-humaneval-benchmarks"},{"type":"has_code","target_id":"github:evalplus:evalplus","source_url":"https://github.com/evalplus/evalplus"},{"type":"has_code","target_id":"github:openai:human-eval","source_url":"https://github.com/openai/human-eval"},{"type":"has_code","target_id":"github:nlpxucan:WizardLM","source_url":"https://github.com/nlpxucan/WizardLM"},{"type":"has_code","target_id":"github:AetherCortex:Llama-X","source_url":"https://github.com/AetherCortex/Llama-X"},{"type":"has_code","target_id":"github:AetherCortex:Llama-X","source_url":"https://github.com/AetherCortex/Llama-X"},{"type":"has_code","target_id":"github:openai:human-eval","source_url":"https://github.com/openai/human-eval"},{"type":"based_on_paper","target_id":"arxiv:2304.12244","source_url":"https://arxiv.org/abs/2304.12244"},{"type":"based_on_paper","target_id":"arxiv:2306.08568","source_url":"https://arxiv.org/abs/2306.08568"},{"type":"based_on_paper","target_id":"arxiv:2308.09583","source_url":"https://arxiv.org/abs/2308.09583"}]', NULL, 'BigScience-OpenRAIL-M', 'approved', 78.8, '1a87111d92467fe6ff77067c5ba98e13', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-latent-consistency-lcm-lora-sdxl', 'huggingface--latent-consistency--lcm-lora-sdxl', 'lcm-lora-sdxl', 'latent-consistency', '--- library_name: diffusers base_model: stabilityai/stable-diffusion-xl-base-1.0 tags: - lora - text-to-image license: openrail++ inference: false --- Latent Consistency Model (LCM) LoRA was proposed in LCM-LoRA: A universal Stable-Diffusion Acceleration Module by *Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu et al.* It is a distilled consistency adapter for [](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) that allows to reduce the number of inference steps to only between...', '["diffusers","lora","text-to-image","arxiv:2311.05556","license:openrail++","region:us"]', 'text-to-image', 762, 57269, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/latent-consistency/lcm-lora-sdxl","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: diffusers\nbase_model: stabilityai/stable-diffusion-xl-base-1.0\ntags:\n- lora\n- text-to-image\nlicense: openrail++\ninference: false\n---\n\n# Latent Consistency Model (LCM) LoRA: SDXL\n\nLatent Consistency Model (LCM) LoRA was proposed in [LCM-LoRA: A universal Stable-Diffusion Acceleration Module](https://arxiv.org/abs/2311.05556) \nby *Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu et al.*\n\nIt is a distilled consistency adapter for [`stable-diffusion-xl-base-1.0`](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0) that allows\nto reduce the number of inference steps to only between **2 - 8 steps**.\n\n| Model                                                                      | Params / M | \n|----------------------------------------------------------------------------|------------|\n| [lcm-lora-sdv1-5](https://huggingface.co/latent-consistency/lcm-lora-sdv1-5)   | 67.5        |\n| [lcm-lora-ssd-1b](https://huggingface.co/latent-consistency/lcm-lora-ssd-1b)   | 105        |\n| [**lcm-lora-sdxl**](https://huggingface.co/latent-consistency/lcm-lora-sdxl) | **197M**  |\n\n## Usage\n\nLCM-LoRA is supported in ü§ó Hugging Face Diffusers library from version v0.23.0 onwards. To run the model, first \ninstall the latest version of the Diffusers library as well as `peft`, `accelerate` and `transformers`.\naudio dataset from the Hugging Face Hub:\n\n```bash\npip install --upgrade pip\npip install --upgrade diffusers transformers accelerate peft\n```\n\n***Note: For detailed usage examples we recommend you to check out our official [LCM-LoRA docs](https://huggingface.co/docs/diffusers/main/en/using-diffusers/inference_with_lcm_lora)***\n\n### Text-to-Image\n\nThe adapter can be loaded with it''s base model `stabilityai/stable-diffusion-xl-base-1.0`. Next, the scheduler needs to be changed to [`LCMScheduler`](https://huggingface.co/docs/diffusers/v0.22.3/en/api/schedulers/lcm#diffusers.LCMScheduler) and we can reduce the number of inference steps to just 2 to 8 steps.\nPlease make sure to either disable `guidance_scale` or use values between 1.0 and 2.0.\n\n```python\nimport torch\nfrom diffusers import LCMScheduler, AutoPipelineForText2Image\n\nmodel_id = "stabilityai/stable-diffusion-xl-base-1.0"\nadapter_id = "latent-consistency/lcm-lora-sdxl"\n\npipe = AutoPipelineForText2Image.from_pretrained(model_id, torch_dtype=torch.float16, variant="fp16")\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\npipe.to("cuda")\n\n# load and fuse lcm lora\npipe.load_lora_weights(adapter_id)\npipe.fuse_lora()\n\nprompt = "Self-portrait oil painting, a beautiful cyborg with golden hair, 8k"\n\n# disable guidance_scale by passing 0\nimage = pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0).images[0]\n```\n\n![](./image.png)\n\n### Inpainting\n\nLCM-LoRA can be used for inpainting as well. \n\n```python\nimport torch\nfrom diffusers import AutoPipelineForInpainting, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\npipe = AutoPipelineForInpainting.from_pretrained(\n    "diffusers/stable-diffusion-xl-1.0-inpainting-0.1",\n    torch_dtype=torch.float16,\n    variant="fp16",\n).to("cuda")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights("latent-consistency/lcm-lora-sdxl")\npipe.fuse_lora()\n\n# load base and mask image\ninit_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png").resize((1024, 1024))\nmask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png").resize((1024, 1024))\n\nprompt = "a castle on top of a mountain, highly detailed, 8k"\ngenerator = torch.manual_seed(42)\nimage = pipe(\n    prompt=prompt,\n    image=init_image,\n    mask_image=mask_image,\n    generator=generator,\n    num_inference_steps=5,\n    guidance_scale=4,\n).images[0]\nmake_image_grid([init_image, mask_image, image], rows=1, cols=3)\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_inpainting.png)\n\n\n## Combine with styled LoRAs\n\nLCM-LoRA can be combined with other LoRAs to generate styled-images in very few steps (4-8). In the following example, we''ll use the LCM-LoRA with the [papercut LoRA](TheLastBen/Papercut_SDXL). \nTo learn more about how to combine LoRAs, refer to [this guide](https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference#combine-multiple-adapters).\n\n```python\nimport torch\nfrom diffusers import DiffusionPipeline, LCMScheduler\n\npipe = DiffusionPipeline.from_pretrained(\n    "stabilityai/stable-diffusion-xl-base-1.0",\n    variant="fp16",\n    torch_dtype=torch.float16\n).to("cuda")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LoRAs\npipe.load_lora_weights("latent-consistency/lcm-lora-sdxl", adapter_name="lcm")\npipe.load_lora_weights("TheLastBen/Papercut_SDXL", weight_name="papercut.safetensors", adapter_name="papercut")\n\n# Combine LoRAs\npipe.set_adapters(["lcm", "papercut"], adapter_weights=[1.0, 0.8])\n\nprompt = "papercut, a cute fox"\ngenerator = torch.manual_seed(0)\nimage = pipe(prompt, num_inference_steps=4, guidance_scale=1, generator=generator).images[0]\nimage\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdx_lora_mix.png)\n\n### ControlNet\n\n```python\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel, LCMScheduler\nfrom diffusers.utils import load_image\n\nimage = load_image(\n    "https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png"\n).resize((1024, 1024))\n\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image)\n\ncontrolnet = ControlNetModel.from_pretrained("diffusers/controlnet-canny-sdxl-1.0-small", torch_dtype=torch.float16, variant="fp16")\npipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n    "stabilityai/stable-diffusion-xl-base-1.0",\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    safety_checker=None,\n    variant="fp16"\n).to("cuda")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights("latent-consistency/lcm-lora-sdxl")\npipe.fuse_lora()\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    "picture of the mona lisa",\n    image=canny_image,\n    num_inference_steps=5,\n    guidance_scale=1.5,\n    controlnet_conditioning_scale=0.5,\n    cross_attention_kwargs={"scale": 1},\n    generator=generator,\n).images[0]\nmake_image_grid([canny_image, image], rows=1, cols=2)\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_controlnet.png)\n\n\n<Tip>\nThe inference parameters in this example might not work for all examples, so we recommend you to try different values for `num_inference_steps`, `guidance_scale`, `controlnet_conditioning_scale` and `cross_attention_kwargs` parameters and choose the best one. \n</Tip>\n\n### T2I Adapter\n\nThis example shows how to use the LCM-LoRA with the [Canny T2I-Adapter](TencentARC/t2i-adapter-canny-sdxl-1.0) and SDXL.\n\n```python\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom diffusers import StableDiffusionXLAdapterPipeline, T2IAdapter, LCMScheduler\nfrom diffusers.utils import load_image, make_image_grid\n\n# Prepare image\n# Detect the canny map in low resolution to avoid high-frequency details\nimage = load_image(\n    "https://huggingface.co/Adapter/t2iadapter/resolve/main/figs_SDXLV1.0/org_canny.jpg"\n).resize((384, 384))\n\nimage = np.array(image)\n\nlow_threshold = 100\nhigh_threshold = 200\n\nimage = cv2.Canny(image, low_threshold, high_threshold)\nimage = image[:, :, None]\nimage = np.concatenate([image, image, image], axis=2)\ncanny_image = Image.fromarray(image).resize((1024, 1024))\n\n# load adapter\nadapter = T2IAdapter.from_pretrained("TencentARC/t2i-adapter-canny-sdxl-1.0", torch_dtype=torch.float16, varient="fp16").to("cuda")\n\npipe = StableDiffusionXLAdapterPipeline.from_pretrained(\n    "stabilityai/stable-diffusion-xl-base-1.0", \n    adapter=adapter,\n    torch_dtype=torch.float16,\n    variant="fp16", \n).to("cuda")\n\n# set scheduler\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n\n# load LCM-LoRA\npipe.load_lora_weights("latent-consistency/lcm-lora-sdxl")\n\nprompt = "Mystical fairy in real, magic, 4k picture, high quality"\nnegative_prompt = "extra digit, fewer digits, cropped, worst quality, low quality, glitch, deformed, mutated, ugly, disfigured"\n\ngenerator = torch.manual_seed(0)\nimage = pipe(\n    prompt=prompt,\n    negative_prompt=negative_prompt,\n    image=canny_image,\n    num_inference_steps=4,\n    guidance_scale=1.5, \n    adapter_conditioning_scale=0.8, \n    adapter_conditioning_factor=1,\n    generator=generator,\n).images[0]\nmake_image_grid([canny_image, image], rows=1, cols=2)\n```\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lcm/lcm_sdxl_t2iadapter.png)\n\n\n## Speed Benchmark\n\nTODO\n\n## Training\n\nTODO\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":2761540034,"files_count":4,"spaces_count":100,"gated":false,"private":false,"config":null}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2311.05556","source_url":"https://arxiv.org/abs/2311.05556"}]', NULL, 'OpenRAIL++', 'approved', 63.8, 'b42a74f884f906a10cedd69df8775b90', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-deepseek-ai-DeepSeek-R1-Distill-Qwen-7B', 'huggingface--deepseek-ai--deepseek-r1-distill-qwen-7b', 'DeepSeek-R1-Distill-Qwen-7B', 'deepseek-ai', '--- license: mit library_name: transformers --- <!-- markdownlint-disable first-line-h1 --> <!-- markdownlint-disable html --> <!-- markdownlint-disable no-duplicate-header --> <div align="center"> <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" /> </div> <hr> <div align="center" style="line-height: 1;"> <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;"> <img alt="Homepage" src="https://github.com/d...', '["transformers","safetensors","qwen2","text-generation","conversational","arxiv:2501.12948","license:mit","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 762, 794795, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlibrary_name: transformers\n---\n# DeepSeek-R1\n<!-- markdownlint-disable first-line-h1 -->\n<!-- markdownlint-disable html -->\n<!-- markdownlint-disable no-duplicate-header -->\n\n<div align="center">\n  <img src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/logo.svg?raw=true" width="60%" alt="DeepSeek-V3" />\n</div>\n<hr>\n<div align="center" style="line-height: 1;">\n  <a href="https://www.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Homepage" src="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/badge.svg?raw=true" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://chat.deepseek.com/" target="_blank" style="margin: 2px;">\n    <img alt="Chat" src="https://img.shields.io/badge/ü§ñ%20Chat-DeepSeek%20R1-536af5?color=536af5&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://huggingface.co/deepseek-ai" target="_blank" style="margin: 2px;">\n    <img alt="Hugging Face" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-DeepSeek%20AI-ffc107?color=ffc107&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://discord.gg/Tc7c45Zzu5" target="_blank" style="margin: 2px;">\n    <img alt="Discord" src="https://img.shields.io/badge/Discord-DeepSeek%20AI-7289da?logo=discord&logoColor=white&color=7289da" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://github.com/deepseek-ai/DeepSeek-V2/blob/main/figures/qr.jpeg?raw=true" target="_blank" style="margin: 2px;">\n    <img alt="Wechat" src="https://img.shields.io/badge/WeChat-DeepSeek%20AI-brightgreen?logo=wechat&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n  <a href="https://twitter.com/deepseek_ai" target="_blank" style="margin: 2px;">\n    <img alt="Twitter Follow" src="https://img.shields.io/badge/Twitter-deepseek_ai-white?logo=x&logoColor=white" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n<div align="center" style="line-height: 1;">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE" style="margin: 2px;">\n    <img alt="License" src="https://img.shields.io/badge/License-MIT-f5de53?&color=f5de53" style="display: inline-block; vertical-align: middle;"/>\n  </a>\n</div>\n\n\n<p align="center">\n  <a href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf"><b>Paper Link</b>üëÅÔ∏è</a>\n</p>\n\n\n## 1. Introduction\n\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\n**NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing the [Usage Recommendation](#usage-recommendations) section.**\n\n<p align="center">\n  <img width="80%" src="figures/benchmark.jpg">\n</p>\n\n## 2. Model Summary\n\n---\n\n**Post-Training: Large-Scale Reinforcement Learning on the Base Model**\n\n-  We directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.\n\n-   We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model''s reasoning and non-reasoning capabilities.\n    We believe the pipeline will benefit the industry by creating better models. \n\n---\n\n**Distillation: Smaller Models Can Be Powerful Too**\n\n-  We demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future. \n- Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n\n## 3. Model Downloads\n\n### DeepSeek-R1 Models\n\n<div align="center">\n\n| **Model** | **#Total Params** | **#Activated Params** | **Context Length** | **Download** |\n| :------------: | :------------: | :------------: | :------------: | :------------: |\n| DeepSeek-R1-Zero | 671B | 37B | 128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero)   |\n| DeepSeek-R1   | 671B | 37B |  128K   | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1)   |\n\n</div>\n\nDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer to [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repository.\n\n### DeepSeek-R1-Distill Models\n\n<div align="center">\n\n| **Model** | **Base Model** | **Download** |\n| :------------: | :------------: | :------------: |\n| DeepSeek-R1-Distill-Qwen-1.5B  | [Qwen2.5-Math-1.5B](https://huggingface.co/Qwen/Qwen2.5-Math-1.5B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)   |\n| DeepSeek-R1-Distill-Qwen-7B  | [Qwen2.5-Math-7B](https://huggingface.co/Qwen/Qwen2.5-Math-7B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)   |\n| DeepSeek-R1-Distill-Llama-8B  | [Llama-3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B)   |\n| DeepSeek-R1-Distill-Qwen-14B   | [Qwen2.5-14B](https://huggingface.co/Qwen/Qwen2.5-14B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B)   |\n|DeepSeek-R1-Distill-Qwen-32B  | [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)   |\n| DeepSeek-R1-Distill-Llama-70B  | [Llama-3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [ü§ó HuggingFace](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)   |\n\n</div>\n\nDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.\n\n## 4. Evaluation Results\n\n### DeepSeek-R1-Evaluation\n For all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.\n<div align="center">\n\n\n| Category | Benchmark (Metric) | Claude-3.5-Sonnet-1022 | GPT-4o 0513 | DeepSeek V3 | OpenAI o1-mini | OpenAI o1-1217 | DeepSeek R1 |\n|----------|-------------------|----------------------|------------|--------------|----------------|------------|--------------|\n| | Architecture | - | - | MoE | - | - | MoE |\n| | # Activated Params | - | - | 37B | - | - | 37B |\n| | # Total Params | - | - | 671B | - | - | 671B |\n| English | MMLU (Pass@1) | 88.3 | 87.2 | 88.5 | 85.2 | **91.8** | 90.8 |\n| | MMLU-Redux (EM) | 88.9 | 88.0 | 89.1 | 86.7 | - | **92.9** |\n| | MMLU-Pro (EM) | 78.0 | 72.6 | 75.9 | 80.3 | - | **84.0** |\n| | DROP (3-shot F1) | 88.3 | 83.7 | 91.6 | 83.9 | 90.2 | **92.2** |\n| | IF-Eval (Prompt Strict) | **86.5** | 84.3 | 86.1 | 84.8 | - | 83.3 |\n| | GPQA-Diamond (Pass@1) | 65.0 | 49.9 | 59.1 | 60.0 | **75.7** | 71.5 |\n| | SimpleQA (Correct) | 28.4 | 38.2 | 24.9 | 7.0 | **47.0** | 30.1 |\n| | FRAMES (Acc.) | 72.5 | 80.5 | 73.3 | 76.9 | - | **82.5** |\n| | AlpacaEval2.0 (LC-winrate) | 52.0 | 51.1 | 70.0 | 57.8 | - | **87.6** |\n| | ArenaHard (GPT-4-1106) | 85.2 | 80.4 | 85.5 | 92.0 | - | **92.3** |\n| Code | LiveCodeBench (Pass@1-COT) | 33.8 | 34.2 | - | 53.8 | 63.4 | **65.9** |\n| | Codeforces (Percentile) | 20.3 | 23.6 | 58.7 | 93.4 | **96.6** | 96.3 |\n| | Codeforces (Rating) | 717 | 759 | 1134 | 1820 | **2061** | 2029 |\n| | SWE Verified (Resolved) | **50.8** | 38.8 | 42.0 | 41.6 | 48.9 | 49.2 |\n| | Aider-Polyglot (Acc.) | 45.3 | 16.0 | 49.6 | 32.9 | **61.7** | 53.3 |\n| Math | AIME 2024 (Pass@1) | 16.0 | 9.3 | 39.2 | 63.6 | 79.2 | **79.8** |\n| | MATH-500 (Pass@1) | 78.3 | 74.6 | 90.2 | 90.0 | 96.4 | **97.3** |\n| | CNMO 2024 (Pass@1) | 13.1 | 10.8 | 43.2 | 67.6 | - | **78.8** |\n| Chinese | CLUEWSC (EM) | 85.4 | 87.9 | 90.9 | 89.9 | - | **92.8** |\n| | C-Eval (EM) | 76.7 | 76.0 | 86.5 | 68.9 | - | **91.8** |\n| | C-SimpleQA (Correct) | 55.4 | 58.7 | **68.0** | 40.3 | - | 63.7 |\n\n</div>\n\n\n### Distilled Model Evaluation\n\n\n<div align="center">\n\n| Model                                    | AIME 2024 pass@1 | AIME 2024 cons@64 | MATH-500 pass@1 | GPQA Diamond pass@1 | LiveCodeBench pass@1 | CodeForces rating |\n|------------------------------------------|------------------|-------------------|-----------------|----------------------|----------------------|-------------------|\n| GPT-4o-0513                          | 9.3              | 13.4              | 74.6            | 49.9                 | 32.9                 | 759               |\n| Claude-3.5-Sonnet-1022             | 16.0             | 26.7                 | 78.3            | 65.0                 | 38.9                 | 717               |\n| o1-mini                              | 63.6             | 80.0              | 90.0            | 60.0                 | 53.8                 | **1820**          |\n| QwQ-32B-Preview                              | 44.0             | 60.0                 | 90.6            | 54.5               | 41.9                 | 1316              |\n| DeepSeek-R1-Distill-Qwen-1.5B       | 28.9             | 52.7              | 83.9            | 33.8                 | 16.9                 | 954               |\n| DeepSeek-R1-Distill-Qwen-7B          | 55.5             | 83.3              | 92.8            | 49.1                 | 37.6                 | 1189              |\n| DeepSeek-R1-Distill-Qwen-14B         | 69.7             | 80.0              | 93.9            | 59.1                 | 53.1                 | 1481              |\n| DeepSeek-R1-Distill-Qwen-32B        | **72.6**         | 83.3              | 94.3            | 62.1                 | 57.2                 | 1691              |\n| DeepSeek-R1-Distill-Llama-8B         | 50.4             | 80.0              | 89.1            | 49.0                 | 39.6                 | 1205              |\n| DeepSeek-R1-Distill-Llama-70B        | 70.0             | **86.7**          | **94.5**        | **65.2**             | **57.5**             | 1633              |\n\n</div>\n\n\n## 5. Chat Website & API Platform\nYou can chat with DeepSeek-R1 on DeepSeek''s official website: [chat.deepseek.com](https://chat.deepseek.com), and switch on the button "DeepThink"\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform: [platform.deepseek.com](https://platform.deepseek.com/)\n\n## 6. How to Run Locally\n\n### DeepSeek-R1 Models\n\nPlease visit [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) repo for more information about running DeepSeek-R1 locally.\n\n**NOTE: Hugging Face''s Transformers has not been directly supported yet.**\n\n### DeepSeek-R1-Distill Models\n\nDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.\n\nFor instance, you can easily start a service using [vLLM](https://github.com/vllm-project/vllm):\n\n```shell\nvllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eager\n```\n\nYou can also easily start a service using [SGLang](https://github.com/sgl-project/sglang)\n\n```bash\npython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2\n```\n\n### Usage Recommendations\n\n**We recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:**\n\n1. Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.\n2. **Avoid adding a system prompt; all instructions should be contained within the user prompt.**\n3. For mathematical problems, it is advisable to include a directive in your prompt such as: "Please reason step by step, and put your final answer within \boxed{}."\n4. When evaluating model performance, it is recommended to conduct multiple tests and average the results.\n\nAdditionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting "\<think\>\n\n\</think\>") when responding to certain queries, which can adversely affect the model''s performance.\n**To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with "\<think\>\n" at the beginning of every output.**\n\n## 7. License\nThis code repository and the model weights are licensed under the [MIT License](https://github.com/deepseek-ai/DeepSeek-R1/blob/main/LICENSE).\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:\n- DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived from [Qwen-2.5 series](https://github.com/QwenLM/Qwen2.5), which are originally licensed under [Apache 2.0 License](https://huggingface.co/Qwen/Qwen2.5-1.5B/blob/main/LICENSE), and now finetuned with 800k samples curated with DeepSeek-R1.\n- DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed under [llama3.1 license](https://huggingface.co/meta-llama/Llama-3.1-8B/blob/main/LICENSE).\n- DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed under [llama3.3 license](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE).\n\n## 8. Citation\n```\n@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}\n\n```\n\n## 9. Contact\nIf you have any questions, please raise an issue or contact us at [service@deepseek.com](service@deepseek.com).\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":7615616512,"storage_bytes":15231404337,"files_count":11,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["Qwen2ForCausalLM"],"model_type":"qwen2","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"pad_token":{"__type":"AddedToken","content":"<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":null,"chat_template":"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='''') %}{%- for message in messages %}{%- if message[''role''] == ''system'' %}{% set ns.system_prompt = message[''content''] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message[''role''] == ''user'' %}{%- set ns.is_tool = false -%}{{''<ÔΩúUserÔΩú>'' + message[''content'']}}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is none %}{%- set ns.is_tool = false -%}{%- for tool in message[''tool_calls'']%}{%- if not ns.is_first %}{{''<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{%- set ns.is_first = true -%}{%- else %}{{''\\n'' + ''<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>'' + tool[''type''] + ''<ÔΩútool‚ñÅsepÔΩú>'' + tool[''function''][''name''] + ''\\n'' + ''```json'' + ''\\n'' + tool[''function''][''arguments''] + ''\\n'' + ''```'' + ''<ÔΩútool‚ñÅcall‚ñÅendÔΩú>''}}{{''<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endfor %}{%- endif %}{%- if message[''role''] == ''assistant'' and message[''content''] is not none %}{%- if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'' + message[''content''] + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message[''content''] %}{% if ''</think>'' in content %}{% set content = content.split(''</think>'')[-1] %}{% endif %}{{''<ÔΩúAssistantÔΩú>'' + content + ''<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>''}}{%- endif %}{%- endif %}{%- if message[''role''] == ''tool'' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{''<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- set ns.is_output_first = false %}{%- else %}{{''\\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>'' + message[''content''] + ''<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>''}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{''<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>''}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{''<ÔΩúAssistantÔΩú><think>\\n''}}{% endif %}"}}}', '[]', '[{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V2","source_url":"https://github.com/deepseek-ai/DeepSeek-V2"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-V3","source_url":"https://github.com/deepseek-ai/DeepSeek-V3"},{"type":"has_code","target_id":"github:vllm-project:vllm","source_url":"https://github.com/vllm-project/vllm"},{"type":"has_code","target_id":"github:sgl-project:sglang","source_url":"https://github.com/sgl-project/sglang"},{"type":"has_code","target_id":"github:deepseek-ai:DeepSeek-R1","source_url":"https://github.com/deepseek-ai/DeepSeek-R1"},{"type":"has_code","target_id":"github:QwenLM:Qwen2.5","source_url":"https://github.com/QwenLM/Qwen2.5"},{"type":"based_on_paper","target_id":"arxiv:2501.12948","source_url":"https://arxiv.org/abs/2501.12948"}]', NULL, 'MIT', 'approved', 98.8, 'a69d43aaaa6ca87c00d3b1c409ddb6b7', NULL, 'https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/resolve/main/figures/benchmark.jpg', CURRENT_TIMESTAMP);
/* LOGS:
Downloading image for huggingface-deepseek-ai-DeepSeek-R1-Distill-Qwen-7B from https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/resolve/main/figures/benchmark.jpg
Image converted to WebP: data/images/huggingface-deepseek-ai-DeepSeek-R1-Distill-Qwen-7B.webp

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-GanymedeNil-text2vec-large-chinese', 'huggingface--ganymedenil--text2vec-large-chinese', 'text2vec-large-chinese', 'GanymedeNil', '--- license: apache-2.0 language: - zh pipeline_tag: sentence-similarity tags: - text2vec - feature-extraction - sentence-similarity - transformers --- Based on the derivative model of https://huggingface.co/shibing624/text2vec-base-chinese, replace MacBERT with LERT, and keep other training conditions unchanged„ÄÇ News 2024-06-25 text2vec-large-chinese onnxruntime version. Talk to me: https://twitter.com/GanymedeNil', '["transformers","pytorch","safetensors","bert","feature-extraction","text2vec","sentence-similarity","zh","license:apache-2.0","text-embeddings-inference","endpoints_compatible","deploy:azure","region:us"]', 'sentence-similarity', 760, 3700, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/GanymedeNil/text2vec-large-chinese","fetched_at":"2025-12-08T10:30:37.943Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: apache-2.0\nlanguage:\n- zh\npipeline_tag: sentence-similarity\ntags:\n- text2vec\n- feature-extraction\n- sentence-similarity\n- transformers\n---\n\nBased on the derivative model of https://huggingface.co/shibing624/text2vec-base-chinese, replace MacBERT with LERT, and keep other training conditions unchanged„ÄÇ\n\nNews\n\n2024-06-25 [text2vec-large-chinese](https://huggingface.co/GanymedeNil/text2vec-large-chinese-onnx) onnxruntime version.\n\nTalk to me: https://twitter.com/GanymedeNil', '{"pipeline_tag":"sentence-similarity","library_name":"transformers","framework":"transformers","params":325522944,"storage_bytes":2604361841,"files_count":10,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["BertModel"],"model_type":"bert","tokenizer_config":{"cls_token":"[CLS]","mask_token":"[MASK]","pad_token":"[PAD]","sep_token":"[SEP]","unk_token":"[UNK]"}}}', '[]', '[]', NULL, 'Apache-2.0', 'approved', 38.8, '8ba399022290a540fc5577d5f9d66895', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-t5-t5-base', 'huggingface--google-t5--t5-base', 't5-base', 'google-t5', '--- pipeline_tag: translation language: - en - fr - ro - de datasets: - c4 tags: - summarization - translation license: apache-2.0 --- !model image 1. Model Details 2. Uses 3. Bias, Risks, and Limitations 4. Training Details 5. Evaluation 6. Environmental Impact 7. Citation 8. Model Card Authors 9. How To Get Started With the Model The developers of the Text-To-Text Transfer Transformer (T5) write: > With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the inpu...', '["transformers","pytorch","tf","jax","rust","safetensors","t5","text2text-generation","summarization","translation","en","fr","ro","de","dataset:c4","arxiv:1805.12471","arxiv:1708.00055","arxiv:1704.05426","arxiv:1606.05250","arxiv:1808.09121","arxiv:1810.12885","arxiv:1905.10044","arxiv:1910.09700","license:apache-2.0","text-generation-inference","endpoints_compatible","deploy:azure","region:us"]', 'translation', 757, 2240237, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google-t5/t5-base","fetched_at":"2025-12-08T10:30:37.944Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\npipeline_tag: translation\nlanguage:\n- en\n- fr\n- ro\n- de\ndatasets:\n- c4\ntags:\n- summarization\n- translation\nlicense: apache-2.0\n---\n\n# Model Card for T5 Base\n\n![model image](https://camo.githubusercontent.com/623b4dea0b653f2ad3f36c71ebfe749a677ac0a1/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f343030362f312a44304a31674e51663876727255704b657944387750412e706e67)\n\n#  Table of Contents\n\n1. [Model Details](#model-details)\n2. [Uses](#uses)\n3. [Bias, Risks, and Limitations](#bias-risks-and-limitations)\n4. [Training Details](#training-details)\n5. [Evaluation](#evaluation)\n6. [Environmental Impact](#environmental-impact)\n7. [Citation](#citation)\n8. [Model Card Authors](#model-card-authors)\n9. [How To Get Started With the Model](#how-to-get-started-with-the-model)\n\n# Model Details\n\n## Model Description\n\nThe developers of the Text-To-Text Transfer Transformer (T5) [write](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html): \n\n> With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.\n\nT5-Base is the checkpoint with 220 million parameters. \n\n- **Developed by:** Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. See [associated paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) and [GitHub repo](https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints)\n- **Model type:** Language model\n- **Language(s) (NLP):** English, French, Romanian, German\n- **License:** Apache 2.0\n- **Related Models:** [All T5 Checkpoints](https://huggingface.co/models?search=t5)\n- **Resources for more information:**\n  - [Research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf)\n  - [Google''s T5 Blog Post](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) \n  - [GitHub Repo](https://github.com/google-research/text-to-text-transfer-transformer)\n  - [Hugging Face T5 Docs](https://huggingface.co/docs/transformers/model_doc/t5)\n  \n# Uses\n\n## Direct Use and Downstream Use\n\nThe developers write in a [blog post](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) that the model: \n\n> Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task, including machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis). We can even apply T5 to regression tasks by training it to predict the string representation of a number instead of the number itself.\n\nSee the [blog post](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) and [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for further details.\n\n## Out-of-Scope Use\n\nMore information needed.\n\n# Bias, Risks, and Limitations\n\nMore information needed.\n\n## Recommendations\n\nMore information needed.\n\n# Training Details\n\n## Training Data\n\nThe model is pre-trained on the [Colossal Clean Crawled Corpus (C4)](https://www.tensorflow.org/datasets/catalog/c4), which was developed and released in the context of the same [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) as T5.\n\nThe model was pre-trained on a on a **multi-task mixture of unsupervised (1.) and supervised tasks (2.)**.\nThereby, the following datasets were being used for (1.) and (2.):\n\n1. **Datasets used for Unsupervised denoising objective**:\n\n- [C4](https://huggingface.co/datasets/c4)\n- [Wiki-DPR](https://huggingface.co/datasets/wiki_dpr)\n\n\n2. **Datasets used for Supervised text-to-text language modeling objective**\n\n- Sentence acceptability judgment\n  - CoLA [Warstadt et al., 2018](https://arxiv.org/abs/1805.12471)\n- Sentiment analysis \n  - SST-2 [Socher et al., 2013](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n- Paraphrasing/sentence similarity\n  - MRPC [Dolan and Brockett, 2005](https://aclanthology.org/I05-5002)\n  - STS-B [Ceret al., 2017](https://arxiv.org/abs/1708.00055)\n  - QQP [Iyer et al., 2017](https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs)\n- Natural language inference\n  - MNLI [Williams et al., 2017](https://arxiv.org/abs/1704.05426)\n  - QNLI [Rajpurkar et al.,2016](https://arxiv.org/abs/1606.05250)\n  - RTE [Dagan et al., 2005](https://link.springer.com/chapter/10.1007/11736790_9) \n  - CB [De Marneff et al., 2019](https://semanticsarchive.net/Archive/Tg3ZGI2M/Marneffe.pdf)\n- Sentence completion\n  - COPA [Roemmele et al., 2011](https://www.researchgate.net/publication/221251392_Choice_of_Plausible_Alternatives_An_Evaluation_of_Commonsense_Causal_Reasoning)\n- Word sense disambiguation\n  - WIC [Pilehvar and Camacho-Collados, 2018](https://arxiv.org/abs/1808.09121)\n- Question answering\n  - MultiRC [Khashabi et al., 2018](https://aclanthology.org/N18-1023)\n  - ReCoRD [Zhang et al., 2018](https://arxiv.org/abs/1810.12885)\n  - BoolQ [Clark et al., 2019](https://arxiv.org/abs/1905.10044)\n\n## Training Procedure\n\nIn their [abstract](https://jmlr.org/papers/volume21/20-074/20-074.pdf), the model developers write: \n\n> In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. \n\nThe framework introduced, the T5 framework, involves a training procedure that brings together the approaches studied in the paper. See the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for further details.\n\n# Evaluation\n\n## Testing Data, Factors & Metrics\n\nThe developers evaluated the model on 24 tasks, see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for full details.\n\n## Results \n\nFor full results for T5-Base, see the [research paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf), Table 14.\n\n# Environmental Impact\n\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\n\n- **Hardware Type:** Google Cloud TPU Pods\n- **Hours used:** More information needed\n- **Cloud Provider:** GCP\n- **Compute Region:** More information needed\n- **Carbon Emitted:** More information needed\n\n# Citation\n\n**BibTeX:**\n\n```bibtex\n@article{2020t5,\n  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},\n  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},\n  journal = {Journal of Machine Learning Research},\n  year    = {2020},\n  volume  = {21},\n  number  = {140},\n  pages   = {1-67},\n  url     = {http://jmlr.org/papers/v21/20-074.html}\n}\n```\n\n**APA:**\n- Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140), 1-67.\n\n# Model Card Authors\n\nThis model card was written by the team at Hugging Face.\n\n# How to Get Started with the Model\n\nUse the code below to get started with the model.\n\n<details>\n<summary> Click to expand </summary>\n\n```python\nfrom transformers import T5Tokenizer, T5Model\n\ntokenizer = T5Tokenizer.from_pretrained("t5-base")\nmodel = T5Model.from_pretrained("t5-base")\n\ninput_ids = tokenizer(\n    "Studies have been shown that owning a dog is good for you", return_tensors="pt"\n).input_ids  # Batch size 1\ndecoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1\n\n# forward pass\noutputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\nlast_hidden_states = outputs.last_hidden_state\n```\n\nSee the [Hugging Face T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Model) docs and a [Colab Notebook](https://colab.research.google.com/github/google-research/text-to-text-transfer-transformer/blob/main/notebooks/t5-trivia.ipynb) created by the model developers for more examples.\n</details>', '{"pipeline_tag":"translation","library_name":"transformers","framework":"transformers","params":222903936,"storage_bytes":6793551855,"files_count":11,"spaces_count":100,"gated":false,"private":false,"config":{"architectures":["T5ForConditionalGeneration"],"model_type":"t5"}}', '[]', '[{"type":"has_code","target_id":"github:google-research:text-to-text-transfer-transformer","source_url":"https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints"},{"type":"has_code","target_id":"github:google-research:text-to-text-transfer-transformer","source_url":"https://github.com/google-research/text-to-text-transfer-transformer"},{"type":"based_on_paper","target_id":"arxiv:1805.12471","source_url":"https://arxiv.org/abs/1805.12471"},{"type":"based_on_paper","target_id":"arxiv:1708.00055","source_url":"https://arxiv.org/abs/1708.00055"},{"type":"based_on_paper","target_id":"arxiv:1704.05426","source_url":"https://arxiv.org/abs/1704.05426"},{"type":"based_on_paper","target_id":"arxiv:1606.05250","source_url":"https://arxiv.org/abs/1606.05250"},{"type":"based_on_paper","target_id":"arxiv:1808.09121","source_url":"https://arxiv.org/abs/1808.09121"},{"type":"based_on_paper","target_id":"arxiv:1810.12885","source_url":"https://arxiv.org/abs/1810.12885"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1910.09700","source_url":"https://arxiv.org/abs/1910.09700"}]', NULL, 'Apache-2.0', 'approved', 63.8, '026b2f166c5be0d63c5c30cd2149d786', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-nitrosocke-Arcane-Diffusion', 'huggingface--nitrosocke--arcane-diffusion', 'Arcane-Diffusion', 'nitrosocke', '--- license: creativeml-openrail-m tags: - stable-diffusion - text-to-image --- This is the fine-tuned Stable Diffusion model trained on images from the TV Show Arcane. Use the tokens **_arcane style_** in your prompts for the effect. **If you enjoy my work, please consider supporting me** This model can be used just like any other Stable Diffusion model. For more information, please have a look at the Stable Diffusion. You can also export the model to ONNX, MPS and/or [FLAX/JAX](). We also s...', '["diffusers","stable-diffusion","text-to-image","license:creativeml-openrail-m","endpoints_compatible","diffusers:stablediffusionpipeline","region:us"]', 'text-to-image', 756, 908, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/nitrosocke/Arcane-Diffusion","fetched_at":"2025-12-08T10:30:37.944Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\ntags:\n- stable-diffusion\n- text-to-image\n---\n# Arcane Diffusion\nThis is the fine-tuned Stable Diffusion model trained on images from the TV Show Arcane.\nUse the tokens **_arcane style_** in your prompts for the effect.\n\n**If you enjoy my work, please consider supporting me** \n[![Become A Patreon](https://badgen.net/badge/become/a%20patron/F96854)](https://patreon.com/user?u=79196446)\n\n### üß® Diffusers\n\nThis model can be used just like any other Stable Diffusion model. For more information,\nplease have a look at the [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion).\n\nYou can also export the model to [ONNX](https://huggingface.co/docs/diffusers/optimization/onnx), [MPS](https://huggingface.co/docs/diffusers/optimization/mps) and/or [FLAX/JAX]().\n\n```python\n#!pip install diffusers transformers scipy torch\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = "nitrosocke/Arcane-Diffusion"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to("cuda")\n\nprompt = "arcane style, a magical princess with golden hair"\nimage = pipe(prompt).images[0]\n\nimage.save("./magical_princess.png")\n```\n\n# Gradio & Colab\n\nWe also support a [Gradio](https://github.com/gradio-app/gradio) Web UI and Colab with Diffusers to run fine-tuned Stable Diffusion models:\n[![Open In Spaces](https://camo.githubusercontent.com/00380c35e60d6b04be65d3d94a58332be5cc93779f630bcdfc18ab9a3a7d3388/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d5370616365732d626c7565)](https://huggingface.co/spaces/anzorq/finetuned_diffusion)\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1j5YvfMZoGdDGdj3O3xRU1m4ujKYsElZO?usp=sharing)\n\n![img](https://huggingface.co/nitrosocke/Arcane-Diffusion/resolve/main/magical_princess.png)\n\n### Sample images from v3:\n![output Samples v3](https://huggingface.co/nitrosocke/Arcane-Diffusion/resolve/main/arcane-v3-samples-01.jpg)\n![output Samples v3](https://huggingface.co/nitrosocke/Arcane-Diffusion/resolve/main/arcane-v3-samples-02.jpg)\n### Sample images from the model:\n![output Samples](https://huggingface.co/nitrosocke/Arcane-Diffusion/resolve/main/arcane-diffusion-output-images.jpg)\n### Sample images used for training:\n![Training Samples](https://huggingface.co/nitrosocke/Arcane-Diffusion/resolve/main/arcane-diffusion-training-images.jpg)\n\n**Version 3** (arcane-diffusion-v3): This version uses the new _train-text-encoder_ setting and improves the quality and edibility of the model immensely. Trained on 95 images from the show in 8000 steps.\n\n**Version 2** (arcane-diffusion-v2): This uses the diffusers based dreambooth training and prior-preservation loss is way more effective. The diffusers where then converted with a script to a ckpt file in order to work with automatics repo.\nTraining was done with 5k steps for a direct comparison to v1 and results show that it needs more steps for a more prominent result. Version 3 will be tested with 11k steps.\n\n**Version 1** (arcane-diffusion-5k): This model was trained using _Unfrozen Model Textual Inversion_ utilizing the _Training with prior-preservation loss_ methods. There is still a slight shift towards the style, while not using the arcane token.\n', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":41619195807,"files_count":25,"spaces_count":100,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionPipeline"}}}', '[]', '[{"type":"has_code","target_id":"github:gradio-app:gradio","source_url":"https://github.com/gradio-app/gradio"}]', NULL, 'creativeml-openrail-m', 'approved', 63.8, 'daa731ebde512f4fb1a37f5acc62e26d', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-zai-org-GLM-4.1V-9B-Thinking', 'huggingface--zai-org--glm-4.1v-9b-thinking', 'GLM-4.1V-9B-Thinking', 'zai-org', '--- license: mit language: - en - zh base_model: - zai-org/GLM-4-9B-0414 pipeline_tag: image-text-to-text library_name: transformers tags: - reasoning --- <div align="center"> <img src=https://raw.githubusercontent.com/zai-org/GLM-4.1V-Thinking/99c5eb6563236f0ff43605d91d107544da9863b2/resources/logo.svg width="40%"/> </div> <p align="center"> üìñ View the GLM-4.1V-9B-Thinking <a href="https://arxiv.org/abs/2507.01006" target="_blank">paper</a>. <br> üìç Using GLM-4.1V-9B-Thinking API at <a href...', '["transformers","safetensors","glm4v","any-to-any","reasoning","image-text-to-text","conversational","en","zh","arxiv:2507.01006","base_model:zai-org/glm-4-9b-0414","base_model:finetune:zai-org/glm-4-9b-0414","license:mit","endpoints_compatible","region:us"]', 'image-text-to-text', 755, 351286, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/zai-org/GLM-4.1V-9B-Thinking","fetched_at":"2025-12-08T10:30:37.944Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: mit\nlanguage:\n- en\n- zh\nbase_model:\n- zai-org/GLM-4-9B-0414\npipeline_tag: image-text-to-text\nlibrary_name: transformers\ntags:\n- reasoning\n---\n\n# GLM-4.1V-9B-Thinking\n\n<div align="center">\n<img src=https://raw.githubusercontent.com/zai-org/GLM-4.1V-Thinking/99c5eb6563236f0ff43605d91d107544da9863b2/resources/logo.svg width="40%"/>\n</div>\n<p align="center">\n    üìñ View the GLM-4.1V-9B-Thinking <a href="https://arxiv.org/abs/2507.01006" target="_blank">paper</a>.\n    <br>\n    üìç Using GLM-4.1V-9B-Thinking API at <a href="https://www.bigmodel.cn/dev/api/visual-reasoning-model/GLM-4.1V-Thinking">Zhipu Foundation Model Open Platform</a>\n</p>\n\n\n## Model Introduction\n\nVision-Language Models (VLMs) have become foundational components of intelligent systems. As real-world AI tasks grow\nincreasingly complex, VLMs must evolve beyond basic multimodal perception to enhance their reasoning capabilities in\ncomplex tasks. This involves improving accuracy, comprehensiveness, and intelligence, enabling applications such as\ncomplex problem solving, long-context understanding, and multimodal agents.\n\nBased on the [GLM-4-9B-0414](https://github.com/zai-org/GLM-4) foundation model, we present the new open-source VLM model\n**GLM-4.1V-9B-Thinking**, designed to explore the upper limits of reasoning in vision-language models. By introducing\na "thinking paradigm" and leveraging reinforcement learning, the model significantly enhances its capabilities. It\nachieves state-of-the-art performance among 10B-parameter VLMs, matching or even surpassing the 72B-parameter\nQwen-2.5-VL-72B on 18 benchmark tasks. We are also open-sourcing the base model GLM-4.1V-9B-Base to\nsupport further research into the boundaries of VLM capabilities.\n\n![rl](https://raw.githubusercontent.com/zai-org/GLM-4.1V-Thinking/refs/heads/main/resources/rl.jpeg)\n\nCompared to the previous generation models CogVLM2 and the GLM-4V series, **GLM-4.1V-Thinking** offers the\nfollowing improvements:\n\n1. The first reasoning-focused model in the series, achieving world-leading performance not only in mathematics but also\n   across various sub-domains.\n2. Supports **64k** context length.\n3. Handles **arbitrary aspect ratios** and up to **4K** image resolution.\n4. Provides an open-source version supporting both **Chinese and English bilingual** usage.\n\n## Benchmark Performance\n\nBy incorporating the Chain-of-Thought reasoning paradigm, GLM-4.1V-9B-Thinking significantly improves answer accuracy,\nrichness, and interpretability. It comprehensively surpasses traditional non-reasoning visual models.\nOut of 28 benchmark tasks, it achieved the best performance among 10B-level models on 23 tasks,\nand even outperformed the 72B-parameter Qwen-2.5-VL-72B on 18 tasks.\n\n![bench](https://raw.githubusercontent.com/zai-org/GLM-4.1V-Thinking/refs/heads/main/resources/bench.jpeg)\n\n## Quick Inference\n\nThis is a simple example of running single-image inference using the `transformers` library.  \nFirst, install the `transformers` library from source:\n\n```\npip install transformers>=4.57.1\n```\n\nThen, run the following code:\n\n```python\nfrom transformers import AutoProcessor, Glm4vForConditionalGeneration\nimport torch\n\nMODEL_PATH = "zai-org/GLM-4.1V-9B-Thinking"\nmessages = [\n    {\n        "role": "user",\n        "content": [\n            {\n                "type": "image",\n                "url": "https://upload.wikimedia.org/wikipedia/commons/f/fa/Grayscale_8bits_palette_sample_image.png"\n            },\n            {\n                "type": "text",\n                "text": "describe this image"\n            }\n        ],\n    }\n]\nprocessor = AutoProcessor.from_pretrained(MODEL_PATH, use_fast=True)\nmodel = Glm4vForConditionalGeneration.from_pretrained(\n    pretrained_model_name_or_path=MODEL_PATH,\n    torch_dtype=torch.bfloat16,\n    device_map="auto",\n)\ninputs = processor.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_dict=True,\n    return_tensors="pt"\n).to(model.device)\ngenerated_ids = model.generate(**inputs, max_new_tokens=8192)\noutput_text = processor.decode(generated_ids[0][inputs["input_ids"].shape[1]:], skip_special_tokens=False)\nprint(output_text)\n```\n\nFor video reasoning, web demo deployment, and more code, please check\nour [GitHub](https://github.com/zai-org/GLM-V).', '{"pipeline_tag":"image-text-to-text","library_name":"transformers","framework":"transformers","params":10292777472,"storage_bytes":20605610600,"files_count":15,"spaces_count":29,"gated":false,"private":false,"config":{"architectures":["Glm4vForConditionalGeneration"],"model_type":"glm4v","tokenizer_config":{"eos_token":"<|endoftext|>","pad_token":"<|endoftext|>"},"chat_template_jinja":"[gMASK]<sop>\n{%- for msg in messages %}\n    {%- if msg.role == ''system'' %}\n<|system|>\n{{ msg.content }}\n    {%- elif msg.role == ''user'' %}\n<|user|>{{ ''\\n'' }}\n\n        {%- if msg.content is string %}\n{{ msg.content }}\n        {%- else %}\n            {%- for item in msg.content %}\n                {%- if item.type == ''video'' or ''video'' in item %}\n<|begin_of_video|><|video|><|end_of_video|>\n                {%- elif item.type == ''image'' or ''image'' in item %}\n<|begin_of_image|><|image|><|end_of_image|>\n                {%- elif item.type == ''text'' %}\n{{ item.text }}\n                {%- endif %}\n            {%- endfor %}\n        {%- endif %}\n    {%- elif msg.role == ''assistant'' %}\n        {%- if msg.metadata %}\n<|assistant|>{{ msg.metadata }}\n{{ msg.content }}\n        {%- else %}\n<|assistant|>\n{{ msg.content }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{% if add_generation_prompt %}<|assistant|>\n{% endif %}"}}', '[]', '[{"type":"has_code","target_id":"github:zai-org:GLM-4","source_url":"https://github.com/zai-org/GLM-4"},{"type":"has_code","target_id":"github:zai-org:GLM-V","source_url":"https://github.com/zai-org/GLM-V"},{"type":"based_on_paper","target_id":"arxiv:2507.01006","source_url":"https://arxiv.org/abs/2507.01006"}]', NULL, 'MIT', 'approved', 63.8, '0ea38018b8e7114f7fd42a2e0dc26e77', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-PygmalionAI-pygmalion-6b', 'huggingface--pygmalionai--pygmalion-6b', 'pygmalion-6b', 'PygmalionAI', '--- license: creativeml-openrail-m language: - en thumbnail: tags: - text generation - conversational inference: false --- Pymalion 6B is a proof-of-concept dialogue model based on EleutherAI''s GPT-J-6B. **Warning:** This model is **NOT** suitable for use by minors. It **will** output X-rated content under certain circumstances. The fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real _and_ partially machine-generated conversations. M...', '["transformers","pytorch","tensorboard","gptj","text-generation","text generation","conversational","en","license:creativeml-openrail-m","region:us"]', 'text-generation', 752, 1479, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/PygmalionAI/pygmalion-6b","fetched_at":"2025-12-08T10:30:37.944Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: creativeml-openrail-m\nlanguage:\n- en\nthumbnail:\ntags:\n- text generation\n- conversational\ninference: false\n---\n\n# Pygmalion 6B\n\n## Model description\n\nPymalion 6B is a proof-of-concept dialogue model based on EleutherAI''s [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6B).\n\n**Warning:** This model is **NOT** suitable for use by minors. It **will** output X-rated content under certain circumstances.\n\n## Training data\n\nThe fine-tuning dataset consisted of 56MB of dialogue data gathered from multiple sources, which includes both real _and_ partially machine-generated conversations.\n\n## Training procedure\n\nModel weights were initialized from the `uft-6b` ConvoGPT model made available in [this commit](https://huggingface.co/hakurei/convogpt/tree/41b67bfddb6cd97070ffddf708e9720c9cb8d224/6b-uft).\n\nThe model was then further fine-tuned on ~48.5 million tokens for ~5k steps on 4 NVIDIA A40s using DeepSpeed.\n\n## Intended use\n\n### The easy way\n\nWe provide a notebook with a Gradio UI for playing around with the model without having to manually format inputs. This notebook can be found [here](https://github.com/PygmalionAI/gradio-ui/blob/master/notebooks/GPU.ipynb).\n\n### The manual way\n\nThe model can be used as a regular text generation model, but it''ll perform best if the input prompt adheres to the following format:\n\n```\n[CHARACTER]''s Persona: [A few sentences about the character you want the model to play]\n<START>\n[DIALOGUE HISTORY]\nYou: [Your input message here]\n[CHARACTER]:\n```\n\nWhere `[CHARACTER]` is, as you can probably guess, the name of the character you want the model to portray, `<START>` should be used verbatim as a delimiter token to separate persona and scenario data from the dialogue, and `[DIALOGUE HISTORY]` is chat history so the model can have some conversational context to draw from. Ideally it''ll be pairs of messages like:\n\n```\n[CHARACTER]: [some dialogue here]\nYou: [your response to the dialogue above]\n```\n\nApart from chat history, you can also just add example conversations in `[DIALOGUE HISTORY]` to show how the character should speak - ideally at the beginning, so it doesn''t get confused as to what''s conversation history vs. character definition.\n\n## Known issues\n\nWe haven''t played around with the model enough to enumerate them. Feel free to give us some feedback!\n', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":null,"storage_bytes":171187170634,"files_count":24,"spaces_count":56,"gated":false,"private":false,"config":{"architectures":["GPTJForCausalLM"],"model_type":"gptj","tokenizer_config":{"bos_token":{"__type":"AddedToken","content":"<|endoftext|>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"eos_token":{"__type":"AddedToken","content":"<|endoftext|>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false},"unk_token":{"__type":"AddedToken","content":"<|endoftext|>","lstrip":false,"normalized":true,"rstrip":false,"single_word":false}}}}', '[]', '[{"type":"has_code","target_id":"github:PygmalionAI:gradio-ui","source_url":"https://github.com/PygmalionAI/gradio-ui"}]', NULL, 'creativeml-openrail-m', 'approved', 63.8, '9c0f86f48f8068d1c6fd09e1711b21f6', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-playgroundai-playground-v2.5-1024px-aesthetic', 'huggingface--playgroundai--playground-v2.5-1024px-aesthetic', 'playground-v2.5-1024px-aesthetic', 'playgroundai', '--- license: other license_name: playground-v2dot5-community license_link: https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic/blob/main/LICENSE.md tags: - text-to-image - playground inference: parameters: guidance_scale: 3.0 --- This repository contains a model that generates highly aesthetic images of resolution 1024x1024, as well as portrait and landscape aspect ratios. You can use the model with Hugging Face üß® Diffusers. !image/png **Playground v2.5** is a diffusion-base...', '["diffusers","safetensors","text-to-image","playground","arxiv:2206.00364","arxiv:2402.17245","license:other","endpoints_compatible","diffusers:stablediffusionxlpipeline","region:us"]', 'text-to-image', 752, 266502, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic","fetched_at":"2025-12-08T10:30:37.944Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlicense: other\nlicense_name: playground-v2dot5-community\nlicense_link: https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic/blob/main/LICENSE.md\ntags:\n- text-to-image\n- playground\ninference:\n  parameters:\n    guidance_scale: 3.0\n---\n# Playground v2.5 ‚Äì 1024px Aesthetic Model\n\nThis repository contains a model that generates highly aesthetic images of resolution 1024x1024, as well as portrait and landscape aspect ratios. You can use the model with Hugging Face üß® Diffusers.\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/636c0c4eaae2da3c76b8a9a3/HYUUGfU6SOCHsvyeISQ5Y.png)\n\n**Playground v2.5** is a diffusion-based text-to-image generative model, and a successor to [Playground v2](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic).\n\nPlayground v2.5 is the state-of-the-art open-source model in aesthetic quality. Our user studies demonstrate that our model outperforms SDXL, Playground v2, PixArt-Œ±, DALL-E 3, and Midjourney 5.2.\n\nFor details on the development and training of our model, please refer to our [blog post](https://blog.playgroundai.com/playground-v2-5/) and [technical report](https://marketing-cdn.playground.com/research/pgv2.5_compressed.pdf).\n\n### Model Description\n- **Developed by:** [Playground](https://playground.com)\n- **Model type:** Diffusion-based text-to-image generative model\n- **License:** [Playground v2.5 Community License](https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic/blob/main/LICENSE.md)\n- **Summary:** This model generates images based on text prompts. It is a Latent Diffusion Model that uses two fixed, pre-trained text encoders (OpenCLIP-ViT/G and CLIP-ViT/L). It follows the same architecture as [Stable Diffusion XL](https://huggingface.co/docs/diffusers/en/using-diffusers/sdxl).\n\n### Using the model with üß® Diffusers\n\nInstall diffusers >= 0.27.0 and the relevant dependencies.\n\n```\npip install diffusers>=0.27.0\npip install transformers accelerate safetensors\n```\n\n**Notes:**\n- The pipeline uses the `EDMDPMSolverMultistepScheduler` scheduler by default, for crisper fine details. It''s an [EDM formulation](https://arxiv.org/abs/2206.00364) of the DPM++ 2M Karras scheduler. `guidance_scale=3.0` is a good default for this scheduler.\n- The pipeline also supports the `EDMEulerScheduler` scheduler. It''s an [EDM formulation](https://arxiv.org/abs/2206.00364) of the Euler scheduler. `guidance_scale=5.0` is a good default for this scheduler.\n\nThen, run the following snippet:\n\n```python\nfrom diffusers import DiffusionPipeline\nimport torch\n\npipe = DiffusionPipeline.from_pretrained(\n    "playgroundai/playground-v2.5-1024px-aesthetic",\n    torch_dtype=torch.float16,\n    variant="fp16",\n).to("cuda")\n\n# # Optional: Use DPM++ 2M Karras scheduler for crisper fine details\n# from diffusers import EDMDPMSolverMultistepScheduler\n# pipe.scheduler = EDMDPMSolverMultistepScheduler()\n\nprompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"\nimage = pipe(prompt=prompt, num_inference_steps=50, guidance_scale=3).images[0]\n```\n\n### Using the model with Automatic1111/ComfyUI\n\nSupport coming soon. We will update this model card with instructions when ready.\n\n### User Studies\n\nThis model card only provides a brief summary of our user study results. For extensive details on how we perform user studies, please check out our [technical report](https://marketing-cdn.playground.com/research/pgv2.5_compressed.pdf).\n\nWe conducted studies to measure overall aesthetic quality, as well as for the specific areas we aimed to improve with Playground v2.5, namely multi aspect ratios and human preference alignment.\n\n#### Comparison to State-of-the-Art\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/63855d851769b7c4b10e1f76/V7LFNzgoQJnL__ndU0CnE.png)\n\nThe aesthetic quality of Playground v2.5 dramatically outperforms the current state-of-the-art open source models SDXL and PIXART-Œ±, as well as Playground v2. Because the performance differential between Playground V2.5 and SDXL was so large, we also tested our aesthetic quality against world-class closed-source models like DALL-E 3 and Midjourney 5.2, and found that Playground v2.5 outperforms them as well.\n\n#### Multi Aspect Ratios\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/636c0c4eaae2da3c76b8a9a3/xMB0r-CmR3N6dABFlcV71.png)\n\nSimilarly, for multi aspect ratios, we outperform SDXL by a large margin.\n\n#### Human Preference Alignment on People-related images\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/636c0c4eaae2da3c76b8a9a3/7c-8Stw52OsNtUjse8Slv.png)\n\nNext, we benchmark Playground v2.5 specifically on people-related images, to test Human Preference Alignment. We compared Playground v2.5 against two commonly-used baseline models: SDXL and RealStock v2, a community fine-tune of SDXL that was trained on a realistic people dataset.\n\nPlayground v2.5 outperforms both baselines by a large margin.\n\n### MJHQ-30K Benchmark\n\n![image/png](https://cdn-uploads.huggingface.co/production/uploads/636c0c4eaae2da3c76b8a9a3/7tyYDPGUtokh-k18XDSte.png)\n\n| Model                                 | Overall FID   |\n| ------------------------------------- | ----- |\n| SDXL-1-0-refiner                      | 9.55  |\n| [playground-v2-1024px-aesthetic](https://huggingface.co/playgroundai/playground-v2-1024px-aesthetic)        | 7.07  |\n| [playground-v2.5-1024px-aesthetic](https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic) | **4.48** |\n\nLastly, we report metrics using our MJHQ-30K benchmark which we [open-sourced](https://huggingface.co/datasets/playgroundai/MJHQ-30K) with the v2 release. We report both the overall FID and per category FID. All FID metrics are computed at resolution 1024x1024. Our results show that Playground v2.5 outperforms both Playground v2 and SDXL in overall FID and all category FIDs, especially in the people and fashion categories. This is in line with the results of the user study, which indicates a correlation between human preferences and the FID score of the MJHQ-30K benchmark.\n\n### How to cite us\n\n```\n@misc{li2024playground,\n      title={Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation}, \n      author={Daiqing Li and Aleks Kamko and Ehsan Akhgari and Ali Sabet and Linmiao Xu and Suhail Doshi},\n      year={2024},\n      eprint={2402.17245},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```', '{"pipeline_tag":"text-to-image","library_name":"diffusers","framework":"diffusers","params":null,"storage_bytes":62442642030,"files_count":35,"spaces_count":84,"gated":false,"private":false,"config":{"diffusers":{"_class_name":"StableDiffusionXLPipeline"}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2206.00364","source_url":"https://arxiv.org/abs/2206.00364"},{"type":"based_on_paper","target_id":"arxiv:2402.17245","source_url":"https://arxiv.org/abs/2402.17245"}]', NULL, 'Other', 'approved', 63.8, '6fe8313c2c1b3f19714e97e6239175f3', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-stabilityai-stable-zero123', 'huggingface--stabilityai--stable-zero123', 'stable-zero123', 'stabilityai', '--- datasets: - allenai/objaverse tags: - 3d extra_gated_fields: Name: text Email: text Country: text Organization or Affiliation: text I ALLOW Stability AI to email me about new model releases: checkbox license: other license_name: sai-nc-community license_link_stable_zero123: https://huggingface.co/stabilityai/sdxl-turbo/blob/main/LICENSE_stable_zero123.md license_link_stable_zero123_c: https://huggingface.co/stabilityai/sdxl-turbo/blob/main/LICENSE_stable_zero123_c.md pipeline_tag: text-to...', '["3d","text-to-3d","dataset:allenai/objaverse","license:other","region:us"]', 'text-to-3d', 751, 0, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/stabilityai/stable-zero123","fetched_at":"2025-12-08T10:30:37.944Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'dataset', '---\ndatasets:\n- allenai/objaverse\ntags:\n- 3d\nextra_gated_fields:\n  Name: text\n  Email: text\n  Country: text\n  Organization or Affiliation: text\n  I ALLOW Stability AI to email me about new model releases: checkbox\nlicense: other\nlicense_name: sai-nc-community\nlicense_link_stable_zero123: https://huggingface.co/stabilityai/sdxl-turbo/blob/main/LICENSE_stable_zero123.md  \nlicense_link_stable_zero123_c: https://huggingface.co/stabilityai/sdxl-turbo/blob/main/LICENSE_stable_zero123_c.md  \npipeline_tag: text-to-3d\n---\n# Stable Zero123\n\nPlease note: For commercial use, please refer to https://stability.ai/license\n\n## Model Description\n\nStable Zero123 is a model for view-conditioned image generation based on [Zero123](https://github.com/cvlab-columbia/zero123). \n\nWith improved data rendering and model conditioning strategies, our model demonstrates improved performance when compared to the original Zero123 and its subsequent iteration, Zero123-XL.\n\n<img src=''img.png'' width=''700''>\n\n## Usage\n\nBy using Score Distillation Sampling (SDS) along with the Stable Zero123 model, we can produce high-quality 3D models from any input image. The process can also extend to text-to-3D generation by first generating a single image using SDXL and then using SDS on Stable Zero123 to generate the 3D object.\n\nTo enable open research in 3D object generation, we''ve improved the open-source code of threestudio by supporting Zero123 and Stable Zero123.\nTo use Stable Zero123 for object 3D mesh generation in [threestudio](https://github.com/threestudio-project/threestudio#stable-zero123), you can follow these steps:\n\n1. Install threestudio using their [instructions](https://github.com/threestudio-project/threestudio#installation)\n2. Download the Stable Zero123 checkpoint `stable_zero123.ckpt` into the `load/zero123/` directory\n2. Take an image of your choice, or generate it from text using your favourite AI image generator such as Stable Assistant (https://stability.ai/stable-assistant) E.g. "A simple 3D render of a friendly dog"\n3. Remove its background using Stable Assistant (https://stability.ai/stable-assistant)\n4. Save to `load/images/`, preferably with `_rgba.png` as the suffix\n5. Run Zero-1-to-3 with the Stable Zero123 ckpt:\n```sh\npython launch.py --config configs/stable-zero123.yaml --train --gpu 0 data.image_path=./load/images/hamburger_rgba.png\n```\n\n## Model Details\n\n* **Developed by**: [Stability AI](https://stability.ai/)\n* **Model type**: latent diffusion model.\n* **Finetuned from model**: [lambdalabs/sd-image-variations-diffusers](https://huggingface.co/lambdalabs/sd-image-variations-diffusers)\n* **License**: We released 2 versions of Stable Zero123.\n    * **Stable Zero123** included some CC-BY-NC 3D objects, so it cannot be used commercially, but can be used for research purposes. It is released under the [Stability AI Non-Commercial Research Community License](https://huggingface.co/stabilityai/zero123-sai/raw/main/LICENSE_stable_zero123.md).\n    * **Stable Zero123C** (‚ÄúC‚Äù for ‚ÄúCommercially-available‚Äù) was only trained on CC-BY and CC0 3D objects. It is released under [StabilityAI Community License](https://huggingface.co/stabilityai/zero123-sai/raw/main/LICENSE_stable_zero123_c.md). You can read more about the license [here](https://stability.ai/license). \nAccording to our internal tests, both models perform similarly in terms of prediction visual quality.\n\n### Training Dataset\n\nWe use renders from the [Objaverse](https://objaverse.allenai.org/objaverse-1.0) dataset, utilizing our enhanced rendering method\n\n### Training Infrastructure\n\n* **Hardware**: `Stable Zero123` was trained on the Stability AI cluster on a single node with 8 A100 80GBs GPUs.\n* **Code Base**: We use our modified version of [the original zero123 repository](https://github.com/cvlab-columbia/zero123).\n\n\n### Misuse, Malicious Use, and Out-of-Scope Use\n\nThe model should not be used to intentionally create or disseminate images that create hostile or alienating environments for people. This includes generating images that people would foreseeably find disturbing, distressing, or offensive; or content that propagates historical or current stereotypes.', '{"pipeline_tag":"text-to-3d","library_name":null,"framework":null,"params":null,"storage_bytes":34336215865,"files_count":7,"spaces_count":2,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:cvlab-columbia:zero123","source_url":"https://github.com/cvlab-columbia/zero123"},{"type":"has_code","target_id":"github:threestudio-project:threestudio","source_url":"https://github.com/threestudio-project/threestudio#stable-zero123"},{"type":"has_code","target_id":"github:threestudio-project:threestudio","source_url":"https://github.com/threestudio-project/threestudio#installation"},{"type":"has_code","target_id":"github:cvlab-columbia:zero123","source_url":"https://github.com/cvlab-columbia/zero123"}]', NULL, 'Other', 'approved', 63.8, '5a324e441553ebbc78b6936436a82901', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-tencent-Hunyuan3D-2.1', 'huggingface--tencent--hunyuan3d-2.1', 'Hunyuan3D-2.1', 'tencent', '--- library_name: hunyuan3d-2 license: other license_name: tencent-hunyuan-community license_link: https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1/blob/main/LICENSE language: - en - zh tags: - image-to-3d - text-to-3d pipeline_tag: image-to-3d extra_gated_eu_disallowed: true --- <p align="center"> <img src="https://raw.githubusercontent.com/Tencent-Hunyuan/Hunyuan3D-2.1/refs/heads/main/assets/images/teaser.jpg"> </p> <div align="center"> <a href=https://3d.hunyuan.tencent.com target="_blank"...', '["hunyuan3d-2","diffusers","safetensors","image-to-3d","text-to-3d","en","zh","arxiv:2506.15442","arxiv:2501.12202","arxiv:2411.02293","license:other","region:us"]', 'image-to-3d', 751, 28820, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/tencent/Hunyuan3D-2.1","fetched_at":"2025-12-08T10:30:37.944Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '---\nlibrary_name: hunyuan3d-2\nlicense: other\nlicense_name: tencent-hunyuan-community\nlicense_link: https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1/blob/main/LICENSE\nlanguage:\n  - en\n  - zh\ntags:\n  - image-to-3d\n  - text-to-3d\npipeline_tag: image-to-3d\nextra_gated_eu_disallowed: true\n---\n\n<p align="center">\n  <img src="https://raw.githubusercontent.com/Tencent-Hunyuan/Hunyuan3D-2.1/refs/heads/main/assets/images/teaser.jpg">\n</p>\n\n<div align="center">\n  <a href=https://3d.hunyuan.tencent.com target="_blank"><img src=https://img.shields.io/badge/Hunyuan3D-black.svg?logo=homepage height=22px></a>\n  <a href=https://huggingface.co/spaces/tencent/Hunyuan3D-2.1  target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Demo-276cb4.svg height=22px></a>\n  <a href=https://huggingface.co/tencent/Hunyuan3D-2.1 target="_blank"><img src=https://img.shields.io/badge/%F0%9F%A4%97%20Models-d96902.svg height=22px></a>\n  <a href=https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1 target="_blank"><img src= https://img.shields.io/badge/Page-bb8a2e.svg?logo=github height=22px></a>\n<a href=https://discord.gg/GuaWYwzKbX target="_blank"><img src= https://img.shields.io/badge/Discord-white.svg?logo=discord height=22px></a>\n    <a href=https://arxiv.org/abs/2506.15442 target="_blank"><img src=https://img.shields.io/badge/Report-b5212f.svg?logo=arxiv height=22px></a>\n</div>\n\n## üîó BibTeX\n\nIf you found this repository helpful, please cite our report:\n\n```bibtex\n@misc{hunyuan3d2025hunyuan3d,\n    title={Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material},\n    author={Team Hunyuan3D and Shuhui Yang and Mingxin Yang and Yifei Feng and Xin Huang and Sheng Zhang and Zebin He and Di Luo and Haolin Liu and Yunfei Zhao and Qingxiang Lin and Zeqiang Lai and Xianghui Yang and Huiwen Shi and Zibo Zhao and Bowen Zhang and Hongyu Yan and Lifu Wang and Sicong Liu and Jihong Zhang and Meng Chen and Liang Dong and Yiwen Jia and Yulin Cai and Jiaao Yu and Yixuan Tang and Dongyuan Guo and Junlin Yu and Hao Zhang and Zheng Ye and Peng He and Runzhou Wu and Shida Wei and Chao Zhang and Yonghao Tan and Yifu Sun and Lin Niu and Shirui Huang and Bojian Zheng and Shu Liu and Shilin Chen and Xiang Yuan and Xiaofeng Yang and Kai Liu and Jianchen Zhu and Peng Chen and Tian Liu and Di Wang and Yuhong Liu and Linus and Jie Jiang and Jingwei Huang and Chunchao Guo},\n    year={2025},\n    eprint={2506.15442},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n\n@misc{hunyuan3d22025tencent,\n    title={Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation},\n    author={Tencent Hunyuan3D Team},\n    year={2025},\n    eprint={2501.12202},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n\n@misc{yang2024tencent,\n    title={Tencent Hunyuan3D-1.0: A Unified Framework for Text-to-3D and Image-to-3D Generation},\n    author={Tencent Hunyuan3D Team},\n    year={2024},\n    eprint={2411.02293},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n```\n\n\n\n## Acknowledgements\n\nWe would like to thank the contributors to\nthe [TripoSG](https://github.com/VAST-AI-Research/TripoSG), [DINOv2](https://github.com/facebookresearch/dinov2), [Stable Diffusion](https://github.com/Stability-AI/stablediffusion), [FLUX](https://github.com/black-forest-labs/flux), [diffusers](https://github.com/huggingface/diffusers)\nand [HuggingFace](https://huggingface.co) repositories, for their open research and exploration.\n\n## Star History\n\n<a href="https://star-history.com/#Tencent-Hunyuan/Hunyuan3D-2.1&Date">\n <picture>\n   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=Tencent-Hunyuan/Hunyuan3D-2.1&type=Date&theme=dark" />\n   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=Tencent-Hunyuan/Hunyuan3D-2.1&type=Date" />\n   <img alt="Star History Chart" src="https://api.star-history.com/svg?repos=Tencent-Hunyuan/Hunyuan3D-2.1&type=Date" />\n </picture>\n</a>', '{"pipeline_tag":"image-to-3d","library_name":"hunyuan3d-2","framework":"hunyuan3d-2","params":null,"storage_bytes":14949350689,"files_count":30,"spaces_count":35,"gated":false,"private":false,"config":null}', '[]', '[{"type":"has_code","target_id":"github:Tencent-Hunyuan:Hunyuan3D-2.1","source_url":"https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1"},{"type":"has_code","target_id":"github:Tencent-Hunyuan:Hunyuan3D-2.1","source_url":"https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1"},{"type":"has_code","target_id":"github:VAST-AI-Research:TripoSG","source_url":"https://github.com/VAST-AI-Research/TripoSG"},{"type":"has_code","target_id":"github:facebookresearch:dinov2","source_url":"https://github.com/facebookresearch/dinov2"},{"type":"has_code","target_id":"github:Stability-AI:stablediffusion","source_url":"https://github.com/Stability-AI/stablediffusion"},{"type":"has_code","target_id":"github:black-forest-labs:flux","source_url":"https://github.com/black-forest-labs/flux"},{"type":"has_code","target_id":"github:huggingface:diffusers","source_url":"https://github.com/huggingface/diffusers"},{"type":"based_on_paper","target_id":"arxiv:2506.15442","source_url":"https://arxiv.org/abs/2506.15442"},{"type":"based_on_paper","target_id":"arxiv:2501.12202","source_url":"https://arxiv.org/abs/2501.12202"},{"type":"based_on_paper","target_id":"arxiv:2411.02293","source_url":"https://arxiv.org/abs/2411.02293"}]', NULL, 'Other', 'approved', 63.8, 'fc3da0276662e882ca91f75c7de9b1c5', NULL, NULL, CURRENT_TIMESTAMP);
/* LOGS:
No image URL available, skipping

*/
INSERT OR REPLACE INTO models (id, slug, name, author, description, tags, pipeline_tag, likes, downloads, cover_image_url, source_trail, commercial_slots, notebooklm_summary, velocity_score, last_commercial_at, type, body_content, meta_json, assets_json, relations_json, canonical_id, license_spdx, compliance_status, quality_score, content_hash, velocity, raw_image_url, last_updated) VALUES ('huggingface-google-gemma-2-9b-it', 'huggingface--google--gemma-2-9b-it', 'gemma-2-9b-it', 'google', '', '["transformers","safetensors","gemma2","text-generation","conversational","arxiv:2009.03300","arxiv:1905.07830","arxiv:1911.11641","arxiv:1904.09728","arxiv:1905.10044","arxiv:1907.10641","arxiv:1811.00937","arxiv:1809.02789","arxiv:1911.01547","arxiv:1705.03551","arxiv:2107.03374","arxiv:2108.07732","arxiv:2110.14168","arxiv:2009.11462","arxiv:2101.11718","arxiv:2110.08193","arxiv:1804.09301","arxiv:2109.07958","arxiv:1804.06876","arxiv:2103.03874","arxiv:2304.06364","arxiv:2206.04615","arxiv:2203.09509","base_model:google/gemma-2-9b","base_model:finetune:google/gemma-2-9b","license:gemma","text-generation-inference","endpoints_compatible","region:us"]', 'text-generation', 748, 150218, NULL, '[{"source_platform":"huggingface","source_url":"https://huggingface.co/google/gemma-2-9b-it","fetched_at":"2025-12-08T10:30:37.944Z","adapter_version":"3.2.0"}]', NULL, NULL, 0, NULL, 'model', '', '{"pipeline_tag":"text-generation","library_name":"transformers","framework":"transformers","params":9241705984,"storage_bytes":18550361413,"files_count":14,"spaces_count":100,"gated":"manual","private":false,"config":{"architectures":["Gemma2ForCausalLM"],"model_type":"gemma2","tokenizer_config":{"bos_token":"<bos>","chat_template":"{{ bos_token }}{% if messages[0][''role''] == ''system'' %}{{ raise_exception(''System role not supported'') }}{% endif %}{% for message in messages %}{% if (message[''role''] == ''user'') != (loop.index0 % 2 == 0) %}{{ raise_exception(''Conversation roles must alternate user/assistant/user/assistant/...'') }}{% endif %}{% if (message[''role''] == ''assistant'') %}{% set role = ''model'' %}{% else %}{% set role = message[''role''] %}{% endif %}{{ ''<start_of_turn>'' + role + ''\n'' + message[''content''] | trim + ''<end_of_turn>\n'' }}{% endfor %}{% if add_generation_prompt %}{{''<start_of_turn>model\n''}}{% endif %}","eos_token":"<eos>","pad_token":"<pad>","unk_token":"<unk>","use_default_system_prompt":false}}}', '[]', '[{"type":"based_on_paper","target_id":"arxiv:2009.03300","source_url":"https://arxiv.org/abs/2009.03300"},{"type":"based_on_paper","target_id":"arxiv:1905.07830","source_url":"https://arxiv.org/abs/1905.07830"},{"type":"based_on_paper","target_id":"arxiv:1911.11641","source_url":"https://arxiv.org/abs/1911.11641"},{"type":"based_on_paper","target_id":"arxiv:1904.09728","source_url":"https://arxiv.org/abs/1904.09728"},{"type":"based_on_paper","target_id":"arxiv:1905.10044","source_url":"https://arxiv.org/abs/1905.10044"},{"type":"based_on_paper","target_id":"arxiv:1907.10641","source_url":"https://arxiv.org/abs/1907.10641"},{"type":"based_on_paper","target_id":"arxiv:1811.00937","source_url":"https://arxiv.org/abs/1811.00937"},{"type":"based_on_paper","target_id":"arxiv:1809.02789","source_url":"https://arxiv.org/abs/1809.02789"},{"type":"based_on_paper","target_id":"arxiv:1911.01547","source_url":"https://arxiv.org/abs/1911.01547"},{"type":"based_on_paper","target_id":"arxiv:1705.03551","source_url":"https://arxiv.org/abs/1705.03551"},{"type":"based_on_paper","target_id":"arxiv:2107.03374","source_url":"https://arxiv.org/abs/2107.03374"},{"type":"based_on_paper","target_id":"arxiv:2108.07732","source_url":"https://arxiv.org/abs/2108.07732"},{"type":"based_on_paper","target_id":"arxiv:2110.14168","source_url":"https://arxiv.org/abs/2110.14168"},{"type":"based_on_paper","target_id":"arxiv:2009.11462","source_url":"https://arxiv.org/abs/2009.11462"},{"type":"based_on_paper","target_id":"arxiv:2101.11718","source_url":"https://arxiv.org/abs/2101.11718"},{"type":"based_on_paper","target_id":"arxiv:2110.08193","source_url":"https://arxiv.org/abs/2110.08193"},{"type":"based_on_paper","target_id":"arxiv:1804.09301","source_url":"https://arxiv.org/abs/1804.09301"},{"type":"based_on_paper","target_id":"arxiv:2109.07958","source_url":"https://arxiv.org/abs/2109.07958"},{"type":"based_on_paper","target_id":"arxiv:1804.06876","source_url":"https://arxiv.org/abs/1804.06876"},{"type":"based_on_paper","target_id":"arxiv:2103.03874","source_url":"https://arxiv.org/abs/2103.03874"},{"type":"based_on_paper","target_id":"arxiv:2304.06364","source_url":"https://arxiv.org/abs/2304.06364"},{"type":"based_on_paper","target_id":"arxiv:2206.04615","source_url":"https://arxiv.org/abs/2206.04615"},{"type":"based_on_paper","target_id":"arxiv:2203.09509","source_url":"https://arxiv.org/abs/2203.09509"}]', NULL, 'Gemma', 'approved', 38.7, '8417eea3455f488257ac0a2da6c2887a', NULL, NULL, CURRENT_TIMESTAMP);
