<!DOCTYPE html><html lang="en"> <head><!-- Google Tag Manager --><script type="module">(function(e,n,r,t,m){e[t]=e[t]||[],e[t].push({"gtm.start":new Date().getTime(),event:"gtm.js"});var g=n.getElementsByTagName(r)[0],a=n.createElement(r),s="";a.async=!0,a.src="https://www.googletagmanager.com/gtm.js?id="+m+s,g.parentNode.insertBefore(a,g)})(window,document,"script","dataLayer","GTM-58C2CQ8G");</script><!-- End Google Tag Manager --><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><!-- Open Graph Meta Tags --><meta property="og:type" content="website"><meta property="og:url" content="https://free2aitools.com/model/tomg-group-umd--huginn-0125/"><meta property="og:title" content="huginn-0125 - AI Model Details"><meta property="og:description" content="A model for text-generation."><meta property="og:image" content="https://free2aitools.com/og-image.jpg"><!-- Twitter Card Meta Tags --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://free2aitools.com/model/tomg-group-umd--huginn-0125/"><meta property="twitter:title" content="huginn-0125 - AI Model Details"><meta property="twitter:description" content="A model for text-generation."><meta property="twitter:image" content="https://free2aitools.com/og-image.jpg"><meta name="description" content="A model for text-generation."><title>huginn-0125 - AI Model Details</title><link rel="canonical" href="https://free2aitools.com/model/tomg-group-umd--huginn-0125/"><!-- Google AdSense --><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2292826803755214" crossorigin="anonymous"></script><link rel="stylesheet" href="/_astro/about.CAZw8hUu.css"></head> <body class="bg-gray-50 text-gray-800 font-sans antialiased flex flex-col min-h-screen"> <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-58C2CQ8G" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) --> <header class="sticky top-0 z-30 w-full backdrop-blur flex-none transition-colors duration-500 lg:border-b lg:border-gray-200 bg-white/80"> <nav class="container mx-auto px-4 sm:px-6 lg:px-8"> <div class="flex items-center justify-between h-16"> <!-- Site Logo --> <a href="/" class="flex items-center gap-2 text-xl sm:text-2xl font-bold text-gray-900"> <span>Free AI Tools</span> </a> <!-- Right-side items --> <div class="flex items-center gap-4"> <!-- Desktop Navigation --> <div class="hidden md:flex items-center gap-6"> <a href="/" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Explore Models </a><a href="/ranking" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Rankings </a><a href="/reports" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Reports </a><a href="/about" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> About </a> </div> <!-- Mobile Search & Menu Toggle --> <button data-collapse-toggle="navbar-default" type="button" class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200" aria-controls="navbar-default" aria-expanded="false"> <span class="sr-only">Open main menu</span> <svg class="w-5 h-5" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 17 14"><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 1h15M1 7h15M1 13h15"></path></svg> </button> </div> </div> <!-- Mobile Collapsible Menu --> <div class="hidden w-full md:hidden" id="navbar-default"> <ul class="font-medium flex flex-col p-4 md:p-0 mt-4 border border-gray-100 rounded-lg bg-gray-50"> <li> <a href="/" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Explore Models</a> </li><li> <a href="/ranking" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Rankings</a> </li><li> <a href="/reports" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Reports</a> </li><li> <a href="/about" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">About</a> </li> </ul> </div> </nav> </header> <main class="flex-grow">  <div class="container mx-auto px-4 py-12"> <div class="max-w-4xl mx-auto"> <!-- Header Section --> <header class="mb-8"> <div class="flex flex-col md:flex-row md:items-start md:justify-between gap-4"> <div> <h1 class="text-4xl md:text-5xl font-extrabold text-gray-900 dark:text-white mb-2">huginn-0125</h1> <p class="text-lg text-gray-500 dark:text-gray-400">by </p> </div>  </div> </header> <!-- Metadata Section --> <div class="mb-8 p-4 bg-gray-100 dark:bg-gray-800 rounded-lg"> <div class="grid grid-cols-2 md:grid-cols-4 gap-4 text-center"> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Likes</p> <p class="text-xl font-bold">2,880</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Downloads</p> <p class="text-xl font-bold">14,170</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Task</p> <p class="text-xl font-bold capitalize">text-generation</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Last Updated</p> <p class="text-xl font-bold">Invalid Date</p> </div> </div> </div> <h3 class="text-lg font-semibold mb-2">Sources</h3> <div class="flex flex-col gap-2"> <a href="https://huggingface.co/tomg-group-umd/huginn-0125" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a><a href="https://huggingface.co/tomg-group-umd/huginn-0125" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a><a href="https://huggingface.co/tomg-group-umd/huginn-0125" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a><a href="https://huggingface.co/tomg-group-umd/huginn-0125" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a><a href="https://huggingface.co/tomg-group-umd/huginn-0125" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a> </div> </div> </div>  <div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div><div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div><div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div><div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div><div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div> <article class="model-readme-content py-8 border-t border-gray-200 dark:border-gray-700"> <div class="prose prose-lg dark:prose-invert max-w-none"> <h1>Huginn-0125</h1>
<p>This is Huginn, version 01/25, a latent recurrent-depth model with 3.5B parameters, trained for 800B tokens on AMD MI250X machines. This is a proof-of-concept model, but surprisingly capable in reasoning and code given its training budget and size.
All details on this model can be found in the tech report: &quot;Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach.&quot; (<a href="https://www.arxiv.org/abs/2502.05171">https://www.arxiv.org/abs/2502.05171</a>)
For more information, see the paper page: <a href="https://huggingface.co/papers/2502.05171">https://huggingface.co/papers/2502.05171</a>.</p>
<p>8 intermediate checkpoints of the model can be found in its collection. Additional intermediate checkpoints are available upon request while we find a place to host all ~350 of them. The data used to train
this model is publicly available (entirely on Hugging Face), and scripts provided with the pretraining code at <a href="https://github.com/seal-rg/recurrent-pretraining">https://github.com/seal-rg/recurrent-pretraining</a> can be used to repeat our preprocessing and our entire training run. </p>
<img src="asset2.jpeg" width="60%">



<h2>Table of Contents</h2>
<ol>
<li><a href="#downloading-and-using-the-model">How to Use</a></li>
<li><a href="#advanced-features">Advanced Usage</a></li>
<li><a href="#model-summary">Model Summary</a></li>
<li><a href="#limitations">Limitations</a></li>
<li><a href="#training">Technical Details</a></li>
<li><a href="#license">License</a></li>
<li><a href="#citation">Citation</a></li>
</ol>
<h2>Downloading and Using the Model</h2>
<p>Load the model like this:</p>
<pre><code class="language-python">import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

model = AutoModelForCausalLM.from_pretrained(&quot;tomg-group-umd/huginn-0125&quot;, torch_dtype=torch.bfloat16, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(&quot;tomg-group-umd/huginn-0125&quot;)
</code></pre>
<h3>Modifying the Model&#39;s Depth at Test Time:</h3>
<p>By providing the argument <code>num_steps</code>, the model will execute a forward pass with that amount of compute: </p>
<pre><code class="language-python">input_ids = tokenizer.encode(&quot;The capital of Westphalia is&quot;, return_tensors=&quot;pt&quot;, add_special_tokens=True).to(device)
model.eval()
model.to(device)

model(input_ids, num_steps=32)
</code></pre>
<p>The model has about 1.5B parameters in its non-recurrent layers (prelude+coda), 0.5B parameters in the embedding, and 1.5B recurrent parameters, so, as a guideline, 
the number of materialized parameters is <code>num_steps * 1.5B + 2B</code>. Playing with this parameter is what makes this model interesting, and different from fixed-depth transformers!
The model is trained to accept an arbitrary number of steps. However, using fewer than 4 steps will result in very coarse answers. If given enough context to reason about, benchmarks show the model improving up to around <code>num_steps=64</code>. Beyond that, more steps generally do not hurt, but we see no further improvements.</p>
<p><em>Note</em>: Due to an upload issue the model is currently stored on HF with 2 copies of the tied embedding, instead of just one. This will be fixed in a future release.</p>
<h3>Inference</h3>
<p>The model was trained with bfloat16-mixed precision, so we recommend using <code>bfloat16</code> to run inference (or AMP bfloat16-mixed precision, if you really want). All benchmarks were evaluated in pure <code>bfloat16</code>.</p>
<h3>Sampling</h3>
<p>The model can be used like a normal HF model to generate text with KV-caching working as expected. You can provide <code>num_steps</code> directly to the <code>generate</code> call, for example:</p>
<pre><code>model.eval()
config = GenerationConfig(max_length=256, stop_strings=[&quot;&lt;|end_text|&gt;&quot;, &quot;&lt;|end_turn|&gt;&quot;], 
                          use_cache=True,
                          do_sample=False, temperature=None, top_k=None, top_p=None, min_p=None, 
                          return_dict_in_generate=True,
                          eos_token_id=65505,bos_token_id=65504,pad_token_id=65509)


input_ids = tokenizer.encode(&quot;The capital of Westphalia is&quot;, return_tensors=&quot;pt&quot;, add_special_tokens=True).to(device)
outputs = model.generate(input_ids, config, tokenizer=tokenizer, num_steps=16)
</code></pre>
<p><em>Note</em>: <code>num_steps</code> and other model arguments CANNOT be included in the <code>GenerationConfig</code>, they will shadow model args at runtime.</p>
<h3>Chat Templating</h3>
<p>The model was not finetuned or post-trained, but due to inclusion of instruction data during pretraining, natively understand its chat template. You can chat with the model like so</p>
<pre><code>messages = []
messages.append({&quot;role&quot;: &quot;system&quot;, &quot;content&quot; : &quot;You are a helpful assistant.&quot;})
messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot; : &quot;What do you think of Goethe&#39;s Faust?&quot;})
chat_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
print(chat_input)
input_ids = tokenizer.encode(chat_input, return_tensors=&quot;pt&quot;, add_special_tokens=False).to(device)

model.generate(input_ids, config, num_steps=64, tokenizer=tokenizer)
</code></pre>
<h3>KV-cache Details</h3>
<p>The model requires its own KV-cache implementation <code>HuginnDynamicCache</code>, otherwise the KV-caches of later calls to the recurrent block will overwrite the earlier ones.
The current implementation will always try to inject this Cache implementation, but that may break with huggingface updates. If you do not use generate, but implement your own generation, use a pattern like this:</p>
<pre><code class="language-python"># first step:
past_key_values = None
outputs = model(input_ids=input_ids, use_cache=True, past_key_values=past_key_values)
past_key_values = outputs.past_key_values # Should be an instance of HuginnDynamicCache
# next step
outputs = model(input_ids=input_ids, use_cache=True, past_key_values=past_key_values)
</code></pre>
<h2>Advanced Features</h2>
<h3>Per-Token Adaptive Compute</h3>
<p>When generating, you can use a variable amount of compute per-token. The model is not trained for this, so this is a proof-of-concept, that it can do this task zero-shot. 
You can pick between a few sane stopping rules, <code>entropy-diff</code>, <code>latent-diff</code>,<code>kl</code> and <code>argmax-stability</code>, via <code>criterion=...</code>. The exit threshold can be modified via <code>exit_threshold=5e-4</code>.
We suggest using <code>kl</code> for interesting exits and <code>argmax-stability</code> for conservative exits. Note that using these variables overrides the default generation function. Not all arguments that are valid for the normal <code>generate</code> call are valid here. To make this more explicit, you can also directly call <code>generate_with_adaptive_compute</code>:</p>
<pre><code class="language-python">from transformers import TextStreamer
streamer = TextStreamer(tokenizer)

model.generate_with_adaptive_compute(input_ids, config, num_steps=64, tokenizer=tokenizer, streamer=streamer,
                                     continuous_compute=False, criterion=&quot;kl&quot;, exit_threshold=5e-4, cache_kwargs={&quot;lookup_strategy&quot;: &quot;latest-m4&quot;})
</code></pre>
<p>Your cache strategy should be set to <code>&quot;latest-m4&quot;</code> if using adaptive compute.</p>
<h3>KV-cache Sharing</h3>
<p>To reduce KV cache memory requirements, the model can be run with fewer KV-caches, with later iterations in the recurrence overwriting earlier caches. To use this feature, set
the cache argument <code>lookup_strategy</code> to include <code>compress-s16</code> (where the last number determine the size of the cache).</p>
<pre><code>model.generate_with_adaptive_compute(input_ids, config, num_steps=64, tokenizer=tokenizer, streamer=streamer,
                                     continuous_compute=False, cache_kwargs={&quot;lookup_strategy&quot;: &quot;compress-s16&quot;})
</code></pre>
<p>You can combine this per-token adaptive compute. In that case your lookup strategy should be <code>latest-m4-compress-s16</code>.</p>
<h3>Warmstart / Continuous CoT</h3>
<p>At each generation step, the recurrence can be warmstarted with the final state from the previous token by setting <code>continuous_compute=True</code>, like so</p>
<pre><code>model.generate_with_adaptive_compute(input_ids, config, num_steps=64, tokenizer=tokenizer, streamer=streamer, continuous_compute=True)
</code></pre>
<h2>Model Summary</h2>
<p>The model is primarily structured around decoder-only transformer blocks. However these blocks are structured into three functional groups, the <strong>prelude</strong> \(P\), 
which embeds the input data into a latent space using multiple transformer layers, then the core <strong>recurrent block</strong> \(R\), which is the central unit of recurrent 
computation modifying states \(\mathbf{s} \in \mathbb{R}^{n \times h }\), and finally the <strong>coda</strong> \(C\), which un-embeds from latent space using several layers and
also contains the prediction head of the model. </p>
<p>Given a number of recurrent iterations \(r\), and a sequence of input tokens \(\mathbf{x} \in V^n\) these groups are used in the following way to produce output 
probabilities \(\mathbf{p} \in \mathbb{R}^{n \times |V|}\).</p>
<p>$$\mathbf{e} = P(\mathbf{x})$$</p>
<p>$$\mathbf{s}<em>0 \sim \mathcal{N}(\mathbf{0}, \sigma^2 I</em>{n\cdot h})$$</p>
<p>$$\mathbf{s}<em>i = R(\mathbf{e}, \mathbf{s}</em>{i-1}) ; \textnormal{for} ;  i \in \lbrace 1, \dots, r \rbrace$$</p>
<p>$$\mathbf{p} = C(\mathbf{s}_r)$$
where \(\sigma\) is the standard deviation of the initial random state. Given an init random state \(\mathbf{s}<em>0\), the model repeatedly applies the core recurrent 
block \(R\), which accepts the latent state \(\mathbf{s}</em>{i-1}\) and the embedded input \(\mathbf{e}\) and outputs a new latent state \(\mathbf{s}_i\). 
After finishing all iterations, the coda block processes the last state and produces the probabilities of the next token.</p>
<p>Please refer to the paper for benchmark performance on standard benchmarks.</p>
<h2>Limitations</h2>
<p>Our checkpoint is trained for only 47000 steps on a broadly untested data mixture with a constant learning rate. As an academic project, the model is trained only on publicly available data and the 800B token count, while large in comparison to older fully open-source models such as the Pythia series, is small in comparison to modern open-source efforts such as OLMo, and tiny in comparison to the datasets used to train industrial open-weight models.</p>
<h2>Technical Specifications</h2>
<p>This model was trained on 21 segments of 4096 AMD MI-250X GPUs on the OLCF Frontier Supercomputer in early December 2024. The model was trained using ROCM 6.2.0, and PyTorch 2.6 nightly pre-release 24/11/02. The code used to train the model can be found at <a href="https://github.com/seal-rg/recurrent-pretraining">https://github.com/seal-rg/recurrent-pretraining</a>.</p>
<h2>License</h2>
<p>This model is released under the <a href="https://choosealicense.com/licenses/apache-2.0/">apache-2.0</a> licence.</p>
<h2>Citation</h2>
<pre><code>@article{geiping_scaling_2025,
  title = {Scaling up {{Test-Time Compute}} with {{Latent Reasoning}}: {{A Recurrent Depth Approach}}},
  shorttitle = {Scaling up {{Test-Time Compute}} with {{Latent Reasoning}}},
  author = {Geiping, Jonas and McLeish, Sean and Jain, Neel and Kirchenbauer, John and Singh, Siddharth and Bartoldson, Brian R. and Kailkhura, Bhavya and Bhatele, Abhinav and Goldstein, Tom},
  year = {2025},
  month = feb,
  eprint = {2502.05171},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.05171},
  url = {http://arxiv.org/abs/2502.05171},
  urldate = {2025-02-10},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  journal = {arxiv:2502.05171[cs]}
}
</code></pre>
<h2>Contact</h2>
<p>Please, feel free to contact us with any questions, or open a discussion thread on Hugging Face.</p>
 </div> </article>  <section class="mt-16 pt-8 border-t border-gray-200 dark:border-gray-700"> <h2 class="text-3xl font-bold text-center mb-8">Related Models</h2> <div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-6"> <a href="/model/OrionStarAI--Orion-14B-Chat" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full">  <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="Orion-14B-Chat"> Orion-14B-Chat </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by </p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> A model for text-generation....
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="670 likes">‚ù§Ô∏è <span>670</span></div> <div class="flex items-center gap-1" title="85,840 downloads">üì• <span>85.8K</span></div> </div> </div> </a><a href="/model/dnotitia--DNA-R1" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full">  <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="DNA-R1"> DNA-R1 </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by </p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> A model for text-generation....
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="390 likes">‚ù§Ô∏è <span>390</span></div> <div class="flex items-center gap-1" title="150 downloads">üì• <span>150</span></div> </div> </div> </a><a href="/model/dnotitia--Smoothie-Qwen3-8B" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full">  <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="Smoothie-Qwen3-8B"> Smoothie-Qwen3-8B </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by </p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> A model for text-generation....
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="120 likes">‚ù§Ô∏è <span>120</span></div> <div class="flex items-center gap-1" title="100 downloads">üì• <span>100</span></div> </div> </div> </a> </div> </section> </main> <footer class="bg-gray-800 text-gray-300 py-8 mt-16"> <div class="container mx-auto px-4 text-center"> <div class="flex justify-center gap-4 mb-4"> <a href="/about" class="hover:underline">About</a> <a href="/compliance" class="hover:underline">Compliance</a> </div> <p class="text-sm mt-2">
&copy; 2025 Free AI Tools. An open-source project to index the world of AI.
</p> <a href="mailto:compliance@free2aitools.com" class="text-sm text-gray-400 hover:underline mt-1 block">compliance@free2aitools.com</a> </div> </footer> </body></html>