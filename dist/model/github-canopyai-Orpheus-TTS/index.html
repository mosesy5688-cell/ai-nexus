<!DOCTYPE html><html lang="en"> <head><!-- Google Tag Manager --><script type="module">(function(e,n,r,t,m){e[t]=e[t]||[],e[t].push({"gtm.start":new Date().getTime(),event:"gtm.js"});var g=n.getElementsByTagName(r)[0],a=n.createElement(r),s="";a.async=!0,a.src="https://www.googletagmanager.com/gtm.js?id="+m+s,g.parentNode.insertBefore(a,g)})(window,document,"script","dataLayer","GTM-58C2CQ8G");</script><!-- End Google Tag Manager --><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><!-- Open Graph Meta Tags --><meta property="og:type" content="website"><meta property="og:url" content="https://free2aitools.com/model/github-canopyai-Orpheus-TTS/"><meta property="og:title" content="Orpheus-TTS - AI Model Details"><meta property="og:description" content="Towards Human-Sounding Speech"><meta property="og:image" content="https://free2aitools.com/og-image.jpg"><!-- Twitter Card Meta Tags --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://free2aitools.com/model/github-canopyai-Orpheus-TTS/"><meta property="twitter:title" content="Orpheus-TTS - AI Model Details"><meta property="twitter:description" content="Towards Human-Sounding Speech"><meta property="twitter:image" content="https://free2aitools.com/og-image.jpg"><meta name="description" content="Towards Human-Sounding Speech"><title>Orpheus-TTS - AI Model Details</title><link rel="canonical" href="https://free2aitools.com/model/github-canopyai-Orpheus-TTS/"><!-- Google AdSense --><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2292826803755214" crossorigin="anonymous"></script><link rel="stylesheet" href="/_astro/about.CAZw8hUu.css"></head> <body class="bg-gray-50 text-gray-800 font-sans antialiased flex flex-col min-h-screen"> <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-58C2CQ8G" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) --> <header class="sticky top-0 z-30 w-full backdrop-blur flex-none transition-colors duration-500 lg:border-b lg:border-gray-200 bg-white/80"> <nav class="container mx-auto px-4 sm:px-6 lg:px-8"> <div class="flex items-center justify-between h-16"> <!-- Site Logo --> <a href="/" class="flex items-center gap-2 text-xl sm:text-2xl font-bold text-gray-900"> <span>Free AI Tools</span> </a> <!-- Right-side items --> <div class="flex items-center gap-4"> <!-- Desktop Navigation --> <div class="hidden md:flex items-center gap-6"> <a href="/" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Explore Models </a><a href="/ranking" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Rankings </a><a href="/reports" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Reports </a><a href="/about" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> About </a> </div> <!-- Mobile Search & Menu Toggle --> <button data-collapse-toggle="navbar-default" type="button" class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200" aria-controls="navbar-default" aria-expanded="false"> <span class="sr-only">Open main menu</span> <svg class="w-5 h-5" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 17 14"><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 1h15M1 7h15M1 13h15"></path></svg> </button> </div> </div> <!-- Mobile Collapsible Menu --> <div class="hidden w-full md:hidden" id="navbar-default"> <ul class="font-medium flex flex-col p-4 md:p-0 mt-4 border border-gray-100 rounded-lg bg-gray-50"> <li> <a href="/" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Explore Models</a> </li><li> <a href="/ranking" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Rankings</a> </li><li> <a href="/reports" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Reports</a> </li><li> <a href="/about" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">About</a> </li> </ul> </div> </nav> </header> <main class="flex-grow">  <div class="container mx-auto px-4 py-12"> <div class="max-w-4xl mx-auto"> <!-- Header Section --> <header class="mb-8"> <div class="flex flex-col md:flex-row md:items-start md:justify-between gap-4"> <div> <h1 class="text-4xl md:text-5xl font-extrabold text-gray-900 dark:text-white mb-2">Orpheus-TTS</h1> <p class="text-lg text-gray-500 dark:text-gray-400">by canopyai</p> </div>  </div> </header> <!-- Metadata Section --> <div class="mb-8 p-4 bg-gray-100 dark:bg-gray-800 rounded-lg"> <div class="grid grid-cols-2 md:grid-cols-4 gap-4 text-center"> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Likes</p> <p class="text-xl font-bold">11,482</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Downloads</p> <p class="text-xl font-bold">11,482</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Task</p> <p class="text-xl font-bold capitalize">tool</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Last Updated</p> <p class="text-xl font-bold">2025/11/20</p> </div> </div> </div> <h3 class="text-lg font-semibold mb-2">Sources</h3> <div class="flex flex-col gap-2"> <a href="https://github.com/canopyai/Orpheus-TTS" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> üì¶ GitHub </a> </div> </div> </div>  <div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> üì¶ GitHub Details
</h3> <div class="grid grid-cols-2 md:grid-cols-3 gap-4 text-sm"> <div><strong>Language:</strong> Python</div> <div><strong>Stars:</strong> 11,482</div> <div><strong>Forks:</strong> 492</div> <div><strong>Open Issues:</strong> 118</div> <div><strong>License:</strong> Apache License 2.0</div> <div><a href="https://canopylabs.ai" target="_blank" class="text-blue-600 dark:text-blue-400 hover:underline"><strong>Project Homepage ‚Üó</strong></a></div> </div>  </div> <article class="model-readme-content py-8 border-t border-gray-200 dark:border-gray-700"> <div class="prose prose-lg dark:prose-invert max-w-none"> <h1>Orpheus TTS</h1>
<h4>Updates üî•</h4>
<ul>
<li><p>[5/2025] We&#39;ve partnered with <a href="https://www.baseten.co/blog/canopy-labs-selects-baseten-as-preferred-inference-provider-for-orpheus-tts-model">Baseten</a> to bring highly optimized inference to Orpheus at fp8 (more performant) and fp16 (full fidelity) inference. See code and docs <a href="/additional_inference_options/baseten_inference_example/README.md">here</a>.</p>
</li>
<li><p>[4/2025] We release a <a href="https://huggingface.co/collections/canopylabs/orpheus-multilingual-research-release-67f5894cd16794db163786ba">family of multilingual models</a> in a research preview. We release a <a href="https://canopylabs.ai/releases/orpheus_can_speak_any_language#training">training guide</a> that explains how we created these models in the hopes that even better versions in both the languages released and new languages are created. We welcome feedback and criticism as well as invite questions in this <a href="https://github.com/canopyai/Orpheus-TTS/discussions/123">discussion</a> for feedback and questions.</p>
</li>
</ul>
<h2>Overview</h2>
<p>Orpheus TTS is a SOTA open-source text-to-speech system built on the Llama-3b backbone. Orpheus demonstrates the emergent capabilities of using LLMs for speech synthesis.</p>
<p><a href="https://canopylabs.ai/model-releases">Check out our original blog post</a></p>
<p><a href="https://github.com/user-attachments/assets/ce17dd3a-f866-4e67-86e4-0025e6e87b8a">https://github.com/user-attachments/assets/ce17dd3a-f866-4e67-86e4-0025e6e87b8a</a></p>
<h2>Abilities</h2>
<ul>
<li><strong>Human-Like Speech</strong>: Natural intonation, emotion, and rhythm that is superior to SOTA closed source models</li>
<li><strong>Zero-Shot Voice Cloning</strong>: Clone voices without prior fine-tuning</li>
<li><strong>Guided Emotion and Intonation</strong>: Control speech and emotion characteristics with simple tags</li>
<li><strong>Low Latency</strong>: ~200ms streaming latency for realtime applications, reducible to ~100ms with input streaming</li>
</ul>
<h2>Models</h2>
<p>We provide 2 English models, and additionally we offer the data processing scripts and sample datasets to make it very straightforward to create your own finetune.</p>
<ol>
<li><p><a href="https://huggingface.co/canopylabs/orpheus-tts-0.1-finetune-prod"><strong>Finetuned Prod</strong></a> ‚Äì A finetuned model for everyday TTS applications</p>
</li>
<li><p><a href="https://huggingface.co/canopylabs/orpheus-tts-0.1-pretrained"><strong>Pretrained</strong></a> ‚Äì Our base model trained on 100k+ hours of English speech data</p>
</li>
</ol>
<p>We also offer a family of multilingual models in a research release.</p>
<ol>
<li><a href="https://huggingface.co/collections/canopylabs/orpheus-multilingual-research-release-67f5894cd16794db163786ba"><strong>Multlingual Family</strong></a> - 7 pairs of pretrained and finetuned models.</li>
</ol>
<h3>Inference</h3>
<h4>Simple setup on Colab</h4>
<p>We offer a standardised prompt format across languages, and these notebooks illustrate how to use our models in English.</p>
<ol>
<li><a href="https://colab.research.google.com/drive/1KhXT56UePPUHhqitJNUxq63k-pQomz3N?usp=sharing">Colab For Tuned Model</a> (not streaming, see below for realtime streaming) ‚Äì A finetuned model for everyday TTS applications.</li>
<li><a href="https://colab.research.google.com/drive/10v9MIEbZOr_3V8ZcPAIh8MN7q2LjcstS?usp=sharing">Colab For Pretrained Model</a> ‚Äì This notebook is set up for conditioned generation but can be extended to a range of tasks.</li>
</ol>
<h4>One-click deployment on Baseten</h4>
<p>Baseten is our <a href="https://www.baseten.co/blog/canopy-labs-selects-baseten-as-preferred-inference-provider-for-orpheus-tts-model">preferred inference partner</a> for Orpheus. Get a dedicated deployment with real-time streaming on production-grade infrastructure <a href="https://www.baseten.co/library/orpheus-tts/">in one click on Baseten</a>.</p>
<h4>Streaming Inference Example</h4>
<ol>
<li>Clone this repo<pre><code class="language-bash">git clone https://github.com/canopyai/Orpheus-TTS.git
</code></pre>
</li>
<li>Navigate and install packages<pre><code class="language-bash">cd Orpheus-TTS &amp;&amp; pip install orpheus-speech # uses vllm under the hood for fast inference
</code></pre>
vllm pushed a slightly buggy version on March 18th so some bugs are being resolved by reverting to <code>pip install vllm==0.7.3</code> after <code>pip install orpheus-speech</code></li>
<li>Run the example below:<pre><code class="language-python">from orpheus_tts import OrpheusModel
import wave
import time

model = OrpheusModel(model_name =&quot;canopylabs/orpheus-tts-0.1-finetune-prod&quot;, max_model_len=2048)
prompt = &#39;&#39;&#39;Man, the way social media has, um, completely changed how we interact is just wild, right? Like, we&#39;re all connected 24/7 but somehow people feel more alone than ever. And don&#39;t even get me started on how it&#39;s messing with kids&#39; self-esteem and mental health and whatnot.&#39;&#39;&#39;

start_time = time.monotonic()
syn_tokens = model.generate_speech(
   prompt=prompt,
   voice=&quot;tara&quot;,
   )

with wave.open(&quot;output.wav&quot;, &quot;wb&quot;) as wf:
   wf.setnchannels(1)
   wf.setsampwidth(2)
   wf.setframerate(24000)

   total_frames = 0
   chunk_counter = 0
   for audio_chunk in syn_tokens: # output streaming
      chunk_counter += 1
      frame_count = len(audio_chunk) // (wf.getsampwidth() * wf.getnchannels())
      total_frames += frame_count
      wf.writeframes(audio_chunk)
   duration = total_frames / wf.getframerate()

   end_time = time.monotonic()
   print(f&quot;It took {end_time - start_time} seconds to generate {duration:.2f} seconds of audio&quot;)
</code></pre>
</li>
</ol>
<h4>Additional Functionality</h4>
<ol>
<li><p>Watermark your audio: Use Silent Cipher to watermark your audio generation; see <a href="additional_inference_options/watermark_audio">Watermark Audio Implementation</a> for implementation.</p>
</li>
<li><p>For No GPU inference using Llama cpp see implementation <a href="additional_inference_options/no_gpu/README.md">documentation</a> for implementation example</p>
</li>
</ol>
<h4>Prompting</h4>
<ol>
<li><p>The <code>finetune-prod</code> models: for the primary model, your text prompt is formatted as <code>{name}: I went to the ...</code>. The options for name in order of conversational realism (subjective benchmarks) are &quot;tara&quot;, &quot;leah&quot;, &quot;jess&quot;, &quot;leo&quot;, &quot;dan&quot;, &quot;mia&quot;, &quot;zac&quot;, &quot;zoe&quot; for English - each language has different voices [see voices here] (<a href="https://canopylabs.ai/releases/orpheus_can_speak_any_language#info">https://canopylabs.ai/releases/orpheus_can_speak_any_language#info</a>)). Our python package does this formatting for you, and the notebook also prepends the appropriate string. You can additionally add the following emotive tags: <code>&lt;laugh&gt;</code>, <code>&lt;chuckle&gt;</code>, <code>&lt;sigh&gt;</code>, <code>&lt;cough&gt;</code>, <code>&lt;sniffle&gt;</code>, <code>&lt;groan&gt;</code>, <code>&lt;yawn&gt;</code>, <code>&lt;gasp&gt;</code>. For multilingual, see this <a href="https://huggingface.co/collections/canopylabs/orpheus-multilingual-research-release-67f5894cd16794db163786ba">post</a> for supported tags.</p>
</li>
<li><p>The pretrained model: you can either generate speech just conditioned on text, or generate speech conditioned on one or more existing text-speech pairs in the prompt. Since this model hasn&#39;t been explicitly trained on the zero-shot voice cloning objective, the more text-speech pairs you pass in the prompt, the more reliably it will generate in the correct voice.</p>
</li>
</ol>
<p>Additionally, use regular LLM generation args like <code>temperature</code>, <code>top_p</code>, etc. as you expect for a regular LLM. <code>repetition_penalty&gt;=1.1</code>is required for stable generations. Increasing <code>repetition_penalty</code> and <code>temperature</code> makes the model speak faster.</p>
<h2>Finetune Model</h2>
<p>Here is an overview of how to finetune your model on any text and speech.
This is a very simple process analogous to tuning an LLM using Trainer and Transformers.</p>
<p>You should start to see high quality results after ~50 examples but for best results, aim for 300 examples/speaker.</p>
<ol>
<li>Your dataset should be a huggingface dataset in <a href="https://huggingface.co/datasets/canopylabs/zac-sample-dataset">this format</a></li>
<li>We prepare the data using <a href="https://colab.research.google.com/drive/1wg_CPCA-MzsWtsujwy-1Ovhv-tn8Q1nD?usp=sharing">this notebook</a>. This pushes an intermediate dataset to your Hugging Face account which you can can feed to the training script in finetune/train.py. Preprocessing should take less than 1 minute/thousand rows.</li>
<li>Modify the <code>finetune/config.yaml</code> file to include your dataset and training properties, and run the training script. You can additionally run any kind of huggingface compatible process like Lora to tune the model.<pre><code class="language-bash"> pip install transformers datasets wandb trl flash_attn torch
 huggingface-cli login &lt;enter your HF token&gt;
 wandb login &lt;wandb token&gt;
 accelerate launch train.py
</code></pre>
</li>
</ol>
<h3>Additional Resources</h3>
<ol>
<li><a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Orpheus_(3B)-TTS.ipynb">Finetuning with unsloth</a></li>
</ol>
<h2>Pretrain Model</h2>
<p>This is a very simple process analogous to training an LLM using Trainer and Transformers.</p>
<p>The base model provided is trained over 100k hours. I recommend not using synthetic data for training as it produces worse results when you try to finetune specific voices, probably because synthetic voices lack diversity and map to the same set of tokens when tokenised (i.e. lead to poor codebook utilisation).</p>
<p>We train the 3b model on sequences of length 8192 - we use the same dataset format for TTS finetuning for the <TTS-dataset> pretraining. We chain input_ids sequences together for more efficient training. The text dataset required is in the form described in this issue <a href="https://github.com/canopyai/Orpheus-TTS/issues/37">#37 </a>. </p>
<p>If you are doing extended training this model, i.e. for another language or style we recommend starting with finetuning only (no text dataset). The main idea behind the text dataset is discussed in the blog post. (tldr; doesn&#39;t forget too much semantic/reasoning ability so its able to better understand how to intone/express phrases when spoken, however most of the forgetting would happen very early on in the training i.e. &lt;100000 rows), so unless you are doing very extended finetuning it may not make too much of a difference.</p>
<h2>Also Check out</h2>
<p>While we can&#39;t verify these implementations are completely accurate/bug free, they have been recommended on a couple of forums, so we include them here:</p>
<ol>
<li><a href="https://github.com/isaiahbjork/orpheus-tts-local">A lightweight client for running Orpheus TTS locally using LM Studio API</a></li>
<li><a href="https://github.com/Lex-au/Orpheus-FastAPI">Open AI compatible Fast-API implementation</a></li>
<li><a href="https://huggingface.co/spaces/MohamedRashad/Orpheus-TTS">HuggingFace Space kindly set up by MohamedRashad</a></li>
<li><a href="https://github.com/Saganaki22/OrpheusTTS-WebUI">Gradio WebUI that runs smoothly on WSL and CUDA</a></li>
</ol>
<h1>Checklist</h1>
<ul>
<li><input checked="" disabled="" type="checkbox"> Release 3b pretrained model and finetuned models</li>
<li><input disabled="" type="checkbox"> Release pretrained and finetuned models in sizes: 1b, 400m, 150m parameters</li>
<li><input disabled="" type="checkbox"> Fix glitch in realtime streaming package that occasionally skips frames.</li>
<li><input disabled="" type="checkbox"> Fix voice cloning Colab notebook implementation</li>
</ul>
 </div> </article>  <section class="mt-16 pt-8 border-t border-gray-200 dark:border-gray-700"> <h2 class="text-3xl font-bold text-center mb-8">Related Models</h2> <div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-6"> <a href="/model/github-unslothai-unsloth" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full"> <div class="absolute top-2 right-2 bg-yellow-400 text-yellow-900 text-xs font-bold px-2 py-1 rounded-full animate-pulse" title="Rising Star">
üî•
</div> <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="unsloth"> unsloth </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by unslothai</p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> Fine-tuning &amp; Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, DeepSeek-R1, Qwen3, Gemma 3, TTS 2x faster with ...
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="96,990 likes">‚ù§Ô∏è <span>97.0K</span></div> <div class="flex items-center gap-1" title="96,990 downloads">üì• <span>97.0K</span></div> </div> </div> </a><a href="/model/github-mudler-LocalAI" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full"> <div class="absolute top-2 right-2 bg-yellow-400 text-yellow-900 text-xs font-bold px-2 py-1 rounded-full animate-pulse" title="Rising Star">
üî•
</div> <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="LocalAI"> LocalAI </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by mudler</p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> :robot: The free, Open Source alternative to OpenAI, Claude and others. Self-hosted and local-first. Drop-in replacement...
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="77,768 likes">‚ù§Ô∏è <span>77.8K</span></div> <div class="flex items-center gap-1" title="77,768 downloads">üì• <span>77.8K</span></div> </div> </div> </a><a href="/model/github-2noise-ChatTTS" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full"> <div class="absolute top-2 right-2 bg-yellow-400 text-yellow-900 text-xs font-bold px-2 py-1 rounded-full animate-pulse" title="Rising Star">
üî•
</div> <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="ChatTTS"> ChatTTS </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by 2noise</p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> A generative speech model for daily dialogue....
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="76,366 likes">‚ù§Ô∏è <span>76.4K</span></div> <div class="flex items-center gap-1" title="76,366 downloads">üì• <span>76.4K</span></div> </div> </div> </a> </div> </section> </main> <footer class="bg-gray-800 text-gray-300 py-8 mt-16"> <div class="container mx-auto px-4 text-center"> <div class="flex justify-center gap-4 mb-4"> <a href="/about" class="hover:underline">About</a> <a href="/compliance" class="hover:underline">Compliance</a> </div> <p class="text-sm mt-2">
&copy; 2025 Free AI Tools. An open-source project to index the world of AI.
</p> <a href="mailto:compliance@free2aitools.com" class="text-sm text-gray-400 hover:underline mt-1 block">compliance@free2aitools.com</a> </div> </footer> </body></html>