<!DOCTYPE html><html lang="en" data-astro-cid-37fxchfa> <head><meta charset="UTF-8"><meta name="description" content="A model for image-to-video."><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v5.15.3"><title>LTX-Video</title><link rel="stylesheet" href="/_astro/about.DnK1pLJZ.css">
<style>body{background-color:var(--color-background);color:var(--color-text-primary)}:root{--color-background: #F9FAFB;--color-text-primary: #111827}.dark{--color-background: #1F2937;--color-text-primary: #D1D5DB}
</style></head> <body class="flex flex-col min-h-screen" data-astro-cid-37fxchfa> <header class="bg-white dark:bg-gray-800 shadow-md sticky top-0 z-50" data-astro-cid-37fxchfa> <nav class="container mx-auto px-4 py-4 flex justify-between items-center" data-astro-cid-37fxchfa> <a href="/" class="text-2xl font-bold text-gray-900 dark:text-white shrink-0" data-astro-cid-37fxchfa>Free AI Tools</a> <div class="w-full max-w-md mx-4" data-astro-cid-37fxchfa> <input type="search" id="header-search" placeholder="Search models..." class="w-full px-4 py-2 text-base bg-gray-100 dark:bg-gray-700 border border-gray-300 dark:border-gray-600 rounded-full focus:ring-2 focus:ring-blue-500 focus:border-blue-500 outline-none transition-colors" data-astro-cid-37fxchfa> </div> <div class="flex items-center space-x-4 shrink-0" data-astro-cid-37fxchfa> <button id="theme-toggle" class="p-2 rounded-full hover:bg-gray-200 dark:hover:bg-gray-700 transition-colors" aria-label="Toggle theme"> <svg id="theme-icon-light" xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"></path></svg> <svg id="theme-icon-dark" xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 hidden" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z"></path></svg> </button> <script>
  const theme = (() => {
    if (typeof localStorage !== 'undefined' && localStorage.getItem('theme')) {
      return localStorage.getItem('theme');
    }
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
      return 'dark';
    }
    return 'light';
  })();

  if (theme === 'dark') {
    document.documentElement.classList.add('dark');
  } else {
    document.documentElement.classList.remove('dark');
  }

  const handleToggleClick = () => {
    const element = document.documentElement;
    element.classList.toggle("dark");

    const isDark = element.classList.contains("dark");
    localStorage.setItem("theme", isDark ? "dark" : "light");

    document.getElementById('theme-icon-light').classList.toggle('hidden', isDark);
    document.getElementById('theme-icon-dark').classList.toggle('hidden', !isDark);
  }

  document.getElementById('theme-toggle').addEventListener('click', handleToggleClick);

  // Set initial icon state
  document.getElementById('theme-icon-light').classList.toggle('hidden', theme === 'dark');
  document.getElementById('theme-icon-dark').classList.toggle('hidden', theme !== 'dark');
</script> <a href="/explore" class="text-gray-600 dark:text-gray-300 hover:text-blue-500" data-astro-cid-37fxchfa>Explore</a> <a href="/about" class="text-gray-600 dark:text-gray-300 hover:text-blue-500" data-astro-cid-37fxchfa>About</a> </div> </nav> </header> <main class="flex-grow" data-astro-cid-37fxchfa>  <div class="container mx-auto px-4 py-12"> <a href="javascript:history.back()" class="text-blue-500 hover:underline mb-8 block font-medium transition-colors">&larr; Back to previous page</a> <article class="bg-white dark:bg-gray-800 p-6 rounded-xl shadow-xl"> <header class="border-b pb-4 mb-6"> <h1 class="text-4xl font-extrabold text-gray-900 dark:text-white leading-tight">LTX-Video</h1>  </header> <div class="grid grid-cols-1 lg:grid-cols-4 gap-12 relative"> <div class="lg:col-span-3"> <section class="mb-8"> <h3 class="text-lg font-semibold mb-2 text-gray-800 dark:text-gray-200">Tags</h3> <div id="tags-container" class="flex flex-wrap gap-2"> <span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">diffusers</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">safetensors</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">ltx-video</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">image-to-video</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">en</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">license:other</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">diffusers:LTXPipeline</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">region:us</span>  </div> </section> <section id="details-usage" class="mt-8 mb-10"> <h2 class="text-3xl font-bold mb-4 text-gray-800 dark:text-gray-200 border-b pb-2">Details & Usage</h2> <div class="prose dark:prose-invert prose-lg max-w-none bg-gray-50 dark:bg-gray-900 p-6 rounded-lg prose-pre:bg-gray-200 dark:prose-pre:bg-gray-800 prose-pre:text-gray-800 dark:prose-pre:text-gray-200 border border-gray-100 dark:border-gray-700 shadow-inner"> <div>---
tags:
- ltx-video
- image-to-video
pinned: true
language:
- en
license: other
library_name: diffusers
---

# LTX-Video Model Card
This model card focuses on the model associated with the LTX-Video model, codebase available [here](https://github.com/Lightricks/LTX-Video).

LTX-Video is the first DiT-based video generation model capable of generating high-quality videos in real-time. It produces 30 FPS videos at a 1216√ó704 resolution faster than they can be watched. Trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content.

<img src="./media/trailer.gif" alt="trailer" width="512">

### Image-to-video examples
| | | |
|:---:|:---:|:---:|
| ![example1](./media/ltx-video_i2v_example_00001.gif) | ![example2](./media/ltx-video_i2v_example_00002.gif) | ![example3](./media/ltx-video_i2v_example_00003.gif) |
| ![example4](./media/ltx-video_i2v_example_00004.gif) | ![example5](./media/ltx-video_i2v_example_00005.gif) |  ![example6](./media/ltx-video_i2v_example_00006.gif) |
| ![example7](./media/ltx-video_i2v_example_00007.gif) |  ![example8](./media/ltx-video_i2v_example_00008.gif) | ![example9](./media/ltx-video_i2v_example_00009.gif) |

# Models & Workflows

| Name                                                                                                                                   | Notes                                                                                                         | inference.py config                                                                                                              | ComfyUI workflow (Recommended)                                                                                                                                |
|----------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ltxv-13b-0.9.8-dev                                                                                                                     | Highest quality, requires more VRAM                                                                           | [ltxv-13b-0.9.8-dev.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-dev.yaml)                     | [ltxv-13b-i2v-base.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-base.json)                                 |
| [ltxv-13b-0.9.8-mix](https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b)                                                      | Mix ltxv-13b-dev and ltxv-13b-distilled in the same multi-scale rendering workflow for balanced speed-quality | N/A                                                                                                                              | [ltxv-13b-i2v-mixed-multiscale.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-mixed-multiscale.json)         |
| [ltxv-13b-0.9.8-distilled](https://app.ltx.studio/motion-workspace?videoModel=ltxv)                                                    | Faster, less VRAM usage, slight quality reduction compared to 13b. Ideal for rapid iterations                 | [ltxv-13b-0.9.8-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-dev.yaml)               | [ltxv-13b-dist-i2v-base.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base.json)         |
| ltxv-2b-0.9.8-distilled                                                                                                                | Smaller model, slight quality reduction compared to 13b distilled. Ideal for light VRAM usage                 | [ltxv-2b-0.9.8-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.8-dev.yaml)                 | N/A                                                                                                                                                           |
| ltxv-13b-0.9.8-fp8                                                                                                                     | Quantized version of ltxv-13b                                                                                 | [ltxv-13b-0.9.8-dev-fp8.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-dev-fp8.yaml)             | [ltxv-13b-i2v-base-fp8.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/ltxv-13b-i2v-base-fp8.json)                         |
| ltxv-13b-0.9.8-distilled-fp8                                                                                                           | Quantized version of ltxv-13b-distilled                                                                       | [ltxv-13b-0.9.8-distilled-fp8.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-13b-0.9.8-distilled-fp8.yaml) | [ltxv-13b-dist-i2v-base-fp8.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/13b-distilled/ltxv-13b-dist-i2v-base-fp8.json) |
| ltxv-2b-0.9.8-distilled-fp8                                                                                                            | Quantized version of ltxv-2b-distilled                                                                        | [ltxv-2b-0.9.8-distilled-fp8.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.8-distilled-fp8.yaml)   | N/A                                                                                                                                                           |
| ltxv-2b-0.9.6                                                                                                                          | Good quality, lower VRAM requirement than ltxv-13b                                                            | [ltxv-2b-0.9.6-dev.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.6-dev.yaml)                       | [ltxvideo-i2v.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/low_level/ltxvideo-i2v.json)                                 |
| ltxv-2b-0.9.6-distilled                                                                                                                | 15√ó faster, real-time capable, fewer steps needed, no STG/CFG required                                        | [ltxv-2b-0.9.6-distilled.yaml](https://github.com/Lightricks/LTX-Video/blob/main/configs/ltxv-2b-0.9.6-distilled.yaml)           | [ltxvideo-i2v-distilled.json](https://github.com/Lightricks/ComfyUI-LTXVideo/blob/master/example_workflows/low_level/ltxvideo-i2v-distilled.json)             |


## Model Details
- **Developed by:** Lightricks
- **Model type:** Diffusion-based image-to-video generation model
- **Language(s):** English


## Usage

### Direct use
You can use the model for purposes under the license:
- 2B version 0.9: [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltx-video-2b-v0.9.license.txt)
- 2B version 0.9.1 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltx-video-2b-v0.9.1.license.txt)
- 2B version 0.9.5 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/ltx-video-2b-v0.9.5.license.txt)
- 2B version 0.9.6-dev [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- 2B version 0.9.6-distilled [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- 13B version 0.9.7-dev [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- 13B version 0.9.7-dev-fp8 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- 13B version 0.9.7-distilled [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- 13B version 0.9.7-distilled-fp8 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- 13B version 0.9.7-distilled-lora128 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- 13B version 0.9.7-ICLoRA Depth [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- 13B version 0.9.7-ICLoRA Pose [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- 13B version 0.9.7-ICLoRA Canny [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- Temporal upscaler version 0.9.7 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- Spatial upscaler version 0.9.7 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- 13B version 0.9.8-dev [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- 13B version 0.9.8-dev-fp8 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- 13B version 0.9.8-distilled [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- 13B version 0.9.8-distilled-fp8 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- 2B version 0.9.8-distilled [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- 2B version 0.9.8-distilled-fp8 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- 13B version 0.9.8-ICLoRA detailer [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- Temporal upscaler version 0.9.8 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)
- Spatial upscaler version 0.9.8 [license](https://huggingface.co/Lightricks/LTX-Video/blob/main/LTX-Video-Open-Weights-License-0.X.txt)

### General tips:
* The model works on resolutions that are divisible by 32 and number of frames that are divisible by 8 + 1 (e.g. 257). In case the resolution or number of frames are not divisible by 32 or 8 + 1, the input will be padded with -1 and then cropped to the desired resolution and number of frames.
* The model works best on resolutions under 720 x 1280 and number of frames below 257.
* Prompts should be in English. The more elaborate the better. Good prompt looks like `The turquoise waves crash against the dark, jagged rocks of the shore, sending white foam spraying into the air. The scene is dominated by the stark contrast between the bright blue water and the dark, almost black rocks. The water is a clear, turquoise color, and the waves are capped with white foam. The rocks are dark and jagged, and they are covered in patches of green moss. The shore is lined with lush green vegetation, including trees and bushes. In the background, there are rolling hills covered in dense forest. The sky is cloudy, and the light is dim.`

### Online demo
The model is accessible right away via the following links:
- [LTX-Studio image-to-video (13B-mix)](https://app.ltx.studio/motion-workspace?videoModel=ltxv-13b)
- [LTX-Studio image-to-video (13B distilled)](https://app.ltx.studio/motion-workspace?videoModel=ltxv)
- [Fal.ai image-to-video (13B full)](https://fal.ai/models/fal-ai/ltx-video-13b-dev/image-to-video)
- [Fal.ai image-to-video (13B distilled)](https://fal.ai/models/fal-ai/ltx-video-13b-distilled/image-to-video)
- [Replicate image-to-video](https://replicate.com/lightricks/ltx-video)

### ComfyUI
To use our model with ComfyUI, please follow the instructions at a dedicated [ComfyUI repo](https://github.com/Lightricks/ComfyUI-LTXVideo/).

### Run locally

#### Installation

The codebase was tested with Python 3.10.5, CUDA version 12.2, and supports PyTorch >= 2.1.2.

```bash
git clone https://github.com/Lightricks/LTX-Video.git
cd LTX-Video

# create env
python -m venv env
source env/bin/activate
python -m pip install -e .\[inference-script\]
```

#### Inference

To use our model, please follow the inference code in [inference.py](https://github.com/Lightricks/LTX-Video/blob/main/inference.py):


#### For image-to-video generation:

```bash
python inference.py --prompt "PROMPT" --input_image_path IMAGE_PATH --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
```

#### For video generation with multiple conditions:

You can now generate a video conditioned on a set of images and/or short video segments.
Simply provide a list of paths to the images or video segments you want to condition on, along with their target frame numbers in the generated video. You can also specify the conditioning strength for each item (default: 1.0).

```bash
python inference.py --prompt "PROMPT" --conditioning_media_paths IMAGE_OR_VIDEO_PATH_1 IMAGE_OR_VIDEO_PATH_2 --conditioning_start_frames TARGET_FRAME_1 TARGET_FRAME_2 --height HEIGHT --width WIDTH --num_frames NUM_FRAMES --seed SEED --pipeline_config configs/ltxv-13b-0.9.8-distilled.yaml
```

### Diffusers üß®

LTX Video is compatible with the [Diffusers Python library](https://huggingface.co/docs/diffusers/main/en/index) for image-to-video generation.

Make sure you install `diffusers` before trying out the examples below.

```bash
pip install -U git+https://github.com/huggingface/diffusers
```

Now, you can run the examples below (note that the upsampling stage is optional but reccomeneded):


### For image-to-video:

```py
import torch
from diffusers import LTXConditionPipeline, LTXLatentUpsamplePipeline
from diffusers.pipelines.ltx.pipeline_ltx_condition import LTXVideoCondition
from diffusers.utils import export_to_video, load_image, load_video

pipe = LTXConditionPipeline.from_pretrained("Lightricks/LTX-Video-0.9.8-dev", torch_dtype=torch.bfloat16)
pipe_upsample = LTXLatentUpsamplePipeline.from_pretrained("Lightricks/ltxv-spatial-upscaler-0.9.8", vae=pipe.vae, torch_dtype=torch.bfloat16)
pipe.to("cuda")
pipe_upsample.to("cuda")
pipe.vae.enable_tiling()

def round_to_nearest_resolution_acceptable_by_vae(height, width):
    height = height - (height % pipe.vae_spatial_compression_ratio)
    width = width - (width % pipe.vae_spatial_compression_ratio)
    return height, width

image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/penguin.png")
video = load_video(export_to_video([image])) # compress the image using video compression as the model was trained on videos
condition1 = LTXVideoCondition(video=video, frame_index=0)

prompt = "A cute little penguin takes out a book and starts reading it"
negative_prompt = "worst quality, inconsistent motion, blurry, jittery, distorted"
expected_height, expected_width = 480, 832
downscale_factor = 2 / 3
num_frames = 96

# Part 1. Generate video at smaller resolution
downscaled_height, downscaled_width = int(expected_height * downscale_factor), int(expected_width * downscale_factor)
downscaled_height, downscaled_width = round_to_nearest_resolution_acceptable_by_vae(downscaled_height, downscaled_width)
latents = pipe(
    conditions=[condition1],
    prompt=prompt,
    negative_prompt=negative_prompt,
    width=downscaled_width,
    height=downscaled_height,
    num_frames=num_frames,
    num_inference_steps=30,
    generator=torch.Generator().manual_seed(0),
    output_type="latent",
).frames

# Part 2. Upscale generated video using latent upsampler with fewer inference steps
# The available latent upsampler upscales the height/width by 2x
upscaled_height, upscaled_width = downscaled_height * 2, downscaled_width * 2
upscaled_latents = pipe_upsample(
    latents=latents,
    output_type="latent"
).frames

# Part 3. Denoise the upscaled video with few steps to improve texture (optional, but recommended)
video = pipe(
    conditions=[condition1],
    prompt=prompt,
    negative_prompt=negative_prompt,
    width=upscaled_width,
    height=upscaled_height,
    num_frames=num_frames,
    denoise_strength=0.4,  # Effectively, 4 inference steps out of 10
    num_inference_steps=10,
    latents=upscaled_latents,
    decode_timestep=0.05,
    image_cond_noise_scale=0.025,
    generator=torch.Generator().manual_seed(0),
    output_type="pil",
).frames[0]

# Part 4. Downscale the video to the expected resolution
video = [frame.resize((expected_width, expected_height)) for frame in video]

export_to_video(video, "output.mp4", fps=24)
```


### For video-to-video: 

```py
import torch
from diffusers import LTXConditionPipeline, LTXLatentUpsamplePipeline
from diffusers.pipelines.ltx.pipeline_ltx_condition import LTXVideoCondition
from diffusers.utils import export_to_video, load_video

pipe = LTXConditionPipeline.from_pretrained("Lightricks/LTX-Video-0.9.8-dev", torch_dtype=torch.bfloat16)
pipe_upsample = LTXLatentUpsamplePipeline.from_pretrained("Lightricks/ltxv-spatial-upscaler-0.9.8", vae=pipe.vae, torch_dtype=torch.bfloat16)
pipe.to("cuda")
pipe_upsample.to("cuda")
pipe.vae.enable_tiling()

def round_to_nearest_resolution_acceptable_by_vae(height, width):
    height = height - (height % pipe.vae_spatial_compression_ratio)
    width = width - (width % pipe.vae_spatial_compression_ratio)
    return height, width

video = load_video(
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cosmos/cosmos-video2world-input-vid.mp4"
)[:21]  # Use only the first 21 frames as conditioning
condition1 = LTXVideoCondition(video=video, frame_index=0)

prompt = "The video depicts a winding mountain road covered in snow, with a single vehicle traveling along it. The road is flanked by steep, rocky cliffs and sparse vegetation. The landscape is characterized by rugged terrain and a river visible in the distance. The scene captures the solitude and beauty of a winter drive through a mountainous region."
negative_prompt = "worst quality, inconsistent motion, blurry, jittery, distorted"
expected_height, expected_width = 768, 1152
downscale_factor = 2 / 3
num_frames = 161

# Part 1. Generate video at smaller resolution
downscaled_height, downscaled_width = int(expected_height * downscale_factor), int(expected_width * downscale_factor)
downscaled_height, downscaled_width = round_to_nearest_resolution_acceptable_by_vae(downscaled_height, downscaled_width)
latents = pipe(
    conditions=[condition1],
    prompt=prompt,
    negative_prompt=negative_prompt,
    width=downscaled_width,
    height=downscaled_height,
    num_frames=num_frames,
    num_inference_steps=30,
    generator=torch.Generator().manual_seed(0),
    output_type="latent",
).frames

# Part 2. Upscale generated video using latent upsampler with fewer inference steps
# The available latent upsampler upscales the height/width by 2x
upscaled_height, upscaled_width = downscaled_height * 2, downscaled_width * 2
upscaled_latents = pipe_upsample(
    latents=latents,
    output_type="latent"
).frames

# Part 3. Denoise the upscaled video with few steps to improve texture (optional, but recommended)
video = pipe(
    conditions=[condition1],
    prompt=prompt,
    negative_prompt=negative_prompt,
    width=upscaled_width,
    height=upscaled_height,
    num_frames=num_frames,
    denoise_strength=0.4,  # Effectively, 4 inference steps out of 10
    num_inference_steps=10,
    latents=upscaled_latents,
    decode_timestep=0.05,
    image_cond_noise_scale=0.025,
    generator=torch.Generator().manual_seed(0),
    output_type="pil",
).frames[0]

# Part 4. Downscale the video to the expected resolution
video = [frame.resize((expected_width, expected_height)) for frame in video]

export_to_video(video, "output.mp4", fps=24)
```

To learn more, check out the [official documentation](https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video). 

Diffusers also supports directly loading from the original LTX checkpoints using the `from_single_file()` method. Check out [this section](https://huggingface.co/docs/diffusers/main/en/api/pipelines/ltx_video#loading-single-files) to learn more.

## Limitations
- This model is not intended or able to provide factual information.
- As a statistical model this checkpoint might amplify existing societal biases.
- The model may fail to generate videos that matches the prompts perfectly.
- Prompt following is heavily influenced by the prompting-style.</div> </div> </section> <div class="border-b border-gray-200 dark:border-gray-700 mb-6"> <nav class="-mb-px flex space-x-8" aria-label="Secondary Tabs"> <button id="tab-btn-files" class="tab-button whitespace-nowrap py-3 px-1 border-b-2 font-semibold text-xl focus:outline-none active-tab">
Files
</button> <button id="tab-btn-community" class="tab-button whitespace-nowrap py-3 px-1 border-b-2 font-semibold text-xl focus:outline-none inactive-tab">
Community
</button> </nav> </div> <div id="tab-content" class="mt-6"> <div id="tab-files" class="tab-panel"> <h2 class="text-2xl font-semibold mb-4 text-gray-800 dark:text-gray-200">Files and Versions</h2> <p class="text-gray-600 dark:text-gray-400">This section will list the model files. (Functionality to be implemented)</p> <div class="ad-container my-4 mt-10">  <div id="amazon-ad-YOUR_AMAZON_DETAIL_PAGE_AD_ID" style="width:100%;height:250px">  </div>  </div> </div> <div id="tab-community" class="tab-panel hidden"> <h2 class="text-2xl font-semibold mb-4 text-gray-800 dark:text-gray-200">Community Discussions</h2> <p class="text-gray-600 dark:text-gray-400">This section will display community discussions and showcase user-created content. (Functionality to be implemented)</p> </div> </div> </div> <aside class="lg:col-span-1"> <div class="bg-gray-50 dark:bg-gray-700 p-6 rounded-lg sticky top-6 shadow-md border border-gray-100 dark:border-gray-600"> <h3 class="text-2xl font-bold mb-4 text-gray-900 dark:text-white border-b pb-2">Model Details</h3> <div class="space-y-3 text-gray-700 dark:text-gray-300"> <div class="flex items-center gap-2"> <span class="text-xl">‚öôÔ∏è</span> <span class="font-semibold">Task:</span> <span class="break-all">image-to-video</span> </div> <div class="flex items-center gap-2"> <span class="text-xl">‚ù§Ô∏è</span> <span class="font-semibold">Likes:</span> <span>2,040</span> </div> <div class="flex items-center gap-2"> <span class="text-xl">‚¨áÔ∏è</span> <span class="font-semibold">Downloads:</span> <span>242,182</span> </div> <div class="flex items-center gap-2"> <span class="text-xl">üìÖ</span> <span class="font-semibold">Updated:</span> <span>N/A</span> </div> </div> <div class="mt-6 border-t border-gray-200 dark:border-gray-600 pt-6"> <h4 class="font-bold text-lg mb-3 text-gray-900 dark:text-white">Sources & Repositories</h4> <ul class="space-y-2"> <li> <a href="https://huggingface.co/Lightricks/LTX-Video" target="_blank" rel="noopener noreferrer" class="text-blue-500 hover:text-blue-600 dark:hover:text-blue-400 hover:underline transition-colors break-all flex items-center gap-1"> <span>Hugging Face</span> <span class="ml-1 text-sm">&rarr;</span> </a> </li> </ul> </div> </div> </aside> </div> </article> </div> <script>(function(){const allTags = ["diffusers","safetensors","ltx-video","image-to-video","en","license:other","diffusers:LTXPipeline","region:us"];
const hasMoreTags = false;

¬† ¬† // Tab switching logic
¬† ¬† const tabButtons = document.querySelectorAll('.tab-button');
¬† ¬† const tabPanels = document.querySelectorAll('.tab-panel');
¬† ¬† const tabIdMap = {
¬† ¬† ¬† 'tab-btn-files': 'tab-files',
¬† ¬† ¬† 'tab-btn-community': 'tab-community',
¬† ¬† };

¬† ¬† tabButtons.forEach(button => {
¬† ¬† ¬† button.addEventListener('click', () => {
¬† ¬† ¬† ¬† // Update active tab
¬† ¬† ¬† ¬† tabButtons.forEach(t => t.classList.replace('active-tab', 'inactive-tab'));
¬† ¬† ¬† ¬† button.classList.replace('inactive-tab', 'active-tab');

¬† ¬† ¬† ¬† // Get the target panel ID
¬† ¬† ¬† ¬† const targetPanelId = tabIdMap[button.id];

¬† ¬† ¬† ¬† // Update panel visibility
¬† ¬† ¬† ¬† tabPanels.forEach(p => p.classList.add('hidden'));
¬† ¬† ¬† ¬† if (targetPanelId) {
¬† ¬† ¬† ¬† ¬† document.getElementById(targetPanelId)?.classList.remove('hidden');
¬† ¬† ¬† ¬† }
¬† ¬† ¬† });
¬† ¬† });


¬† ¬† // "Show all tags" button logic
¬† ¬† if (hasMoreTags) {
¬† ¬† ¬† const showAllBtn = document.getElementById('show-all-tags');
¬† ¬† ¬† const tagsContainer = document.getElementById('tags-container');

¬† ¬† ¬† if (showAllBtn && tagsContainer) {
¬† ¬† ¬† ¬† showAllBtn.addEventListener('click', () => {
¬† ¬† ¬† ¬† ¬† const remainingTags = allTags.slice(15);
¬† ¬† ¬† ¬† ¬† const tagsHtml = remainingTags.map(tag =>¬†
¬† ¬† ¬† ¬† ¬† ¬† `<span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">${tag}</span>`
¬† ¬† ¬† ¬† ¬† ).join('');
¬† ¬† ¬† ¬† ¬†¬†
¬† ¬† ¬† ¬† ¬† const tempDiv = document.createElement('div');
¬† ¬† ¬† ¬† ¬† tempDiv.innerHTML = tagsHtml;

¬† ¬† ¬† ¬† ¬† tagsContainer.append(...tempDiv.childNodes);
¬† ¬† ¬† ¬† ¬† showAllBtn.remove();
¬† ¬† ¬† ¬† });
¬† ¬† ¬† }
¬† ¬† }
¬† })();</script>  </main> <footer class="bg-gray-800 text-gray-300 py-8 mt-16" data-astro-cid-37fxchfa> <div class="container mx-auto px-4 text-center" data-astro-cid-37fxchfa> <div class="flex justify-center gap-4 mb-4" data-astro-cid-37fxchfa> <a href="/about" class="hover:underline" data-astro-cid-37fxchfa>About</a> <a href="/compliance" class="hover:underline" data-astro-cid-37fxchfa>Compliance</a> </div> <p class="text-sm mt-2" data-astro-cid-37fxchfa>&copy; 2025 Free AI Tools. An open-source project to index the world of AI.</p> <a href="mailto:compliance@free2aitools.com" class="text-sm text-gray-400 hover:underline mt-1 block" data-astro-cid-37fxchfa>compliance@free2aitools.com</a> </div> </footer> <script type="module">const e=document.getElementById("header-search");e.addEventListener("keydown",n=>{if(n.key==="Enter"){const o=e.value;window.location.href=`/explore?q=${encodeURIComponent(o)}`}});</script> </body> </html>