<!DOCTYPE html><html lang="en"> <head><!-- Google Tag Manager --><script type="module">(function(e,n,r,t,m){e[t]=e[t]||[],e[t].push({"gtm.start":new Date().getTime(),event:"gtm.js"});var g=n.getElementsByTagName(r)[0],a=n.createElement(r),s="";a.async=!0,a.src="https://www.googletagmanager.com/gtm.js?id="+m+s,g.parentNode.insertBefore(a,g)})(window,document,"script","dataLayer","GTM-58C2CQ8G");</script><!-- End Google Tag Manager --><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><!-- Open Graph Meta Tags --><meta property="og:type" content="website"><meta property="og:url" content="https://free2aitools.com/model/github-ByteDance-Seed-Depth-Anything-3/"><meta property="og:title" content="Depth-Anything-3 - AI Model Details"><meta property="og:description" content="<div align=&#34;center&#34;> <h1 style=&#34;border-bottom: none; margin-bottom: 0px &#34;>Depth Anything 3: Recovering the Visual Space from Any Views</h1>..."><meta property="og:image" content="https://free2aitools.com/og-image.jpg"><!-- Twitter Card Meta Tags --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://free2aitools.com/model/github-ByteDance-Seed-Depth-Anything-3/"><meta property="twitter:title" content="Depth-Anything-3 - AI Model Details"><meta property="twitter:description" content="<div align=&#34;center&#34;> <h1 style=&#34;border-bottom: none; margin-bottom: 0px &#34;>Depth Anything 3: Recovering the Visual Space from Any Views</h1>..."><meta property="twitter:image" content="https://free2aitools.com/og-image.jpg"><meta name="description" content="<div align=&#34;center&#34;> <h1 style=&#34;border-bottom: none; margin-bottom: 0px &#34;>Depth Anything 3: Recovering the Visual Space from Any Views</h1>..."><title>Depth-Anything-3 - AI Model Details</title><link rel="canonical" href="https://free2aitools.com/model/github-ByteDance-Seed-Depth-Anything-3/"><!-- Google AdSense --><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2292826803755214" crossorigin="anonymous"></script><link rel="stylesheet" href="/_astro/about.C4wyk6Sp.css"></head> <body class="bg-gray-50 text-gray-800 font-sans antialiased flex flex-col min-h-screen"> <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-58C2CQ8G" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) --> <header class="sticky top-0 z-30 w-full backdrop-blur flex-none transition-colors duration-500 lg:border-b lg:border-gray-200 bg-white/80"> <nav class="container mx-auto px-4 sm:px-6 lg:px-8"> <div class="flex items-center justify-between h-16"> <!-- Site Logo --> <a href="/" class="flex items-center gap-2 text-xl sm:text-2xl font-bold text-gray-900"> <span>Free AI Tools</span> </a> <!-- Right-side items --> <div class="flex items-center gap-4"> <!-- Desktop Navigation --> <div class="hidden md:flex items-center gap-6"> <a href="/" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Explore Models </a><a href="/ranking" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Rankings </a><a href="/reports" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Reports </a><a href="/about" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> About </a> </div> <!-- Mobile Search & Menu Toggle --> <button data-collapse-toggle="navbar-default" type="button" class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200" aria-controls="navbar-default" aria-expanded="false"> <span class="sr-only">Open main menu</span> <svg class="w-5 h-5" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 17 14"><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 1h15M1 7h15M1 13h15"></path></svg> </button> </div> </div> <!-- Mobile Collapsible Menu --> <div class="hidden w-full md:hidden" id="navbar-default"> <ul class="font-medium flex flex-col p-4 md:p-0 mt-4 border border-gray-100 rounded-lg bg-gray-50"> <li> <a href="/" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Explore Models</a> </li><li> <a href="/ranking" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Rankings</a> </li><li> <a href="/reports" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Reports</a> </li><li> <a href="/about" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">About</a> </li> </ul> </div> </nav> </header> <main class="flex-grow">  <div class="container mx-auto px-4 py-12"> <div class="max-w-4xl mx-auto"> <!-- Header Section --> <header class="mb-8"> <div class="flex flex-col md:flex-row md:items-start md:justify-between gap-4"> <div> <h1 class="text-4xl md:text-5xl font-extrabold text-gray-900 dark:text-white mb-2">Depth-Anything-3</h1> <p class="text-lg text-gray-500 dark:text-gray-400">by ByteDance-Seed</p> </div>  </div> </header> <!-- Metadata Section --> <div class="mb-8 p-4 bg-gray-100 dark:bg-gray-800 rounded-lg"> <div class="grid grid-cols-2 md:grid-cols-4 gap-4 text-center"> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Likes</p> <p class="text-xl font-bold">2,395</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Downloads</p> <p class="text-xl font-bold">2,395</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Task</p> <p class="text-xl font-bold capitalize">tool</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Last Updated</p> <p class="text-xl font-bold">2025/11/20</p> </div> </div> </div> <!-- Tags and Sources --> <div class="mb-8 flex flex-wrap items-start gap-4"> <div class="flex-1"> <h3 class="text-lg font-semibold mb-2">Tags</h3> <div class="flex flex-wrap gap-2">  </div> </div> <div class="flex-shrink-0"> <h3 class="text-lg font-semibold mb-2">Sources</h3> <div class="flex flex-col gap-2"> <a href="https://github.com/ByteDance-Seed/Depth-Anything-3" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ğŸ“¦ GitHub </a> </div> </div> </div> <!-- Source Specific Details --> <div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ğŸ“¦ GitHub Details
</h3> <div class="grid grid-cols-2 md:grid-cols-3 gap-4 text-sm"> <div><strong>Language:</strong> Jupyter Notebook</div> <div><strong>Stars:</strong> 2,395</div> <div><strong>Forks:</strong> 154</div> <div><strong>Open Issues:</strong> 47</div> <div><strong>License:</strong> Apache License 2.0</div> <div><a href="https://depth-anything-3.github.io/" target="_blank" class="text-blue-600 dark:text-blue-400 hover:underline"><strong>Project Homepage â†—</strong></a></div> </div>  </div> <!-- README Content --> <article class="model-readme-content py-8 border-t border-gray-200 dark:border-gray-700"> <div class="prose prose-lg dark:prose-invert max-w-none"> <div align="center">
<h1 style="border-bottom: none; margin-bottom: 0px ">Depth Anything 3: Recovering the Visual Space from Any Views</h1>
<!-- <h2 style="border-top: none; margin-top: 3px;">Recovering the Visual Space from Any Views</h2> -->


<p><a href="https://haotongl.github.io/"><strong>Haotong Lin</strong></a><sup>&ast;</sup> Â· <a href="https://github.com/SiliChen321"><strong>Sili Chen</strong></a><sup>&ast;</sup> Â· <a href="https://liewjunhao.github.io/"><strong>Jun Hao Liew</strong></a><sup>&ast;</sup> Â· <a href="https://donydchen.github.io"><strong>Donny Y. Chen</strong></a><sup>&ast;</sup> Â· <a href="https://zhyever.github.io/"><strong>Zhenyu Li</strong></a> Â· <a href="https://scholar.google.com/citations?user=MjXxWbUAAAAJ&hl=en"><strong>Guang Shi</strong></a> Â· <a href="https://scholar.google.com.sg/citations?user=Q8iay0gAAAAJ&hl=en"><strong>Jiashi Feng</strong></a>
<br>
<a href="https://bingykang.github.io/"><strong>Bingyi Kang</strong></a><sup>&ast;&dagger;</sup></p>
<p>&dagger;project lead&emsp;&ast;Equal Contribution</p>
<p><a href="https://arxiv.org/abs/2511.10647"><img src='https://img.shields.io/badge/arXiv-Depth Anything 3-red' alt='Paper PDF'></a>
<a href='https://depth-anything-3.github.io'><img src='https://img.shields.io/badge/Project_Page-Depth Anything 3-green' alt='Project Page'></a>
<a href='https://huggingface.co/spaces/depth-anything/Depth-Anything-3'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Demo-blue'></a></p>
<!-- <a href='https://huggingface.co/datasets/depth-anything/VGB'><img src='https://img.shields.io/badge/Benchmark-VisGeo-yellow' alt='Benchmark'></a> -->
<!-- <a href='https://huggingface.co/datasets/depth-anything/data'><img src='https://img.shields.io/badge/Benchmark-xxx-yellow' alt='Data'></a> -->

</div>

<p>This work presents <strong>Depth Anything 3 (DA3)</strong>, a model that predicts spatially consistent geometry from
arbitrary visual inputs, with or without known camera poses.
In pursuit of minimal modeling, DA3 yields two key insights:</p>
<ul>
<li>ğŸ’ A <strong>single plain transformer</strong> (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization,</li>
<li>âœ¨ A singular <strong>depth-ray representation</strong> obviates the need for complex multi-task learning.</li>
</ul>
<p>ğŸ† DA3 significantly outperforms
<a href="https://github.com/DepthAnything/Depth-Anything-V2">DA2</a> for monocular depth estimation,
and <a href="https://github.com/facebookresearch/vggt">VGGT</a> for multi-view depth estimation and pose estimation.
All models are trained exclusively on <strong>public academic datasets</strong>.</p>
<!-- <p align="center">
  <img src="assets/images/da3_teaser.png" alt="Depth Anything 3" width="100%">
</p> -->
<p align="center">
  <img src="assets/images/demo320-2.gif" alt="Depth Anything 3 - Left" width="70%">
</p>
<p align="center">
  <img src="assets/images/da3_radar.png" alt="Depth Anything 3" width="100%">
</p>


<h2>ğŸ“° News</h2>
<ul>
<li><strong>2025-11-14:</strong> ğŸ‰ Paper, project page, code and models are all released.</li>
</ul>
<h2>âœ¨ Highlights</h2>
<h3>ğŸ† Model Zoo</h3>
<p>We release three series of models, each tailored for specific use cases in visual geometry.</p>
<ul>
<li><p>ğŸŒŸ <strong>DA3 Main Series</strong> (<code>DA3-Giant</code>, <code>DA3-Large</code>, <code>DA3-Base</code>, <code>DA3-Small</code>) These are our flagship foundation models, trained with a unified depth-ray representation. By varying the input configuration, a single model can perform a wide range of tasks:</p>
<ul>
<li>ğŸŒŠ <strong>Monocular Depth Estimation</strong>: Predicts a depth map from a single RGB image.</li>
<li>ğŸŒŠ <strong>Multi-View Depth Estimation</strong>: Generates consistent depth maps from multiple images for high-quality fusion.</li>
<li>ğŸ¯ <strong>Pose-Conditioned Depth Estimation</strong>: Achieves superior depth consistency when camera poses are provided as input.</li>
<li>ğŸ“· <strong>Camera Pose Estimation</strong>:  Estimates camera extrinsics and intrinsics from one or more images.</li>
<li>ğŸŸ¡ <strong>3D Gaussian Estimation</strong>: Directly predicts 3D Gaussians, enabling high-fidelity novel view synthesis.</li>
</ul>
</li>
<li><p>ğŸ“ <strong>DA3 Metric Series</strong> (<code>DA3Metric-Large</code>) A specialized model fine-tuned for metric depth estimation in monocular settings, ideal for applications requiring real-world scale.</p>
</li>
<li><p>ğŸ” <strong>DA3 Monocular Series</strong> (<code>DA3Mono-Large</code>). A dedicated model for high-quality relative monocular depth estimation. Unlike disparity-based models (e.g.,  <a href="https://github.com/DepthAnything/Depth-Anything-V2">Depth Anything 2</a>), it directly predicts depth, resulting in superior geometric accuracy.</p>
</li>
</ul>
<p>ğŸ”— Leveraging these available models, we developed a <strong>nested series</strong> (<code>DA3Nested-Giant-Large</code>). This series combines a any-view giant model with a metric model to reconstruct visual geometry at a real-world metric scale.</p>
<h3>ğŸ› ï¸ Codebase Features</h3>
<p>Our repository is designed to be a powerful and user-friendly toolkit for both practical application and future research.</p>
<ul>
<li>ğŸ¨ <strong>Interactive Web UI &amp; Gallery</strong>: Visualize model outputs and compare results with an easy-to-use Gradio-based web interface.</li>
<li>âš¡ <strong>Flexible Command-Line Interface (CLI)</strong>: Powerful and scriptable CLI for batch processing and integration into custom workflows.</li>
<li>ğŸ’¾ <strong>Multiple Export Formats</strong>: Save your results in various formats, including <code>glb</code>, <code>npz</code>, depth images, <code>ply</code>, 3DGS videos, etc, to seamlessly connect with other tools.</li>
<li>ğŸ”§ <strong>Extensible and Modular Design</strong>: The codebase is structured to facilitate future research and the integration of new models or functionalities.</li>
</ul>
<!-- ### ğŸ¯ Visual Geometry Benchmark
We introduce a new benchmark to rigorously evaluate geometry prediction models on three key tasks: pose estimation, 3D reconstruction, and visual rendering (novel view synthesis) quality.

- ğŸ”„ **Broad Model Compatibility**: Our benchmark is designed to be versatile, supporting the evaluation of various models, including both monocular and multi-view depth estimation approaches.
- ğŸ”¬ **Robust Evaluation Pipeline**: We provide a standardized pipeline featuring RANSAC-based pose alignment, TSDF fusion for dense reconstruction, and a principled view selection strategy for novel view synthesis.
- ğŸ“Š **Standardized Metrics**: Performance is measured using established metrics: AUC for pose accuracy, F1-score and Chamfer Distance for reconstruction, and PSNR/SSIM/LPIPS for rendering quality.
- ğŸŒ **Diverse and Challenging Datasets**: The benchmark spans a wide range of scenes from datasets like HiRoom, ETH3D, DTU, 7Scenes, ScanNet++, DL3DV, Tanks and Temples, and MegaDepth. -->


<h2>ğŸš€ Quick Start</h2>
<h3>ğŸ“¦ Installation</h3>
<pre><code class="language-bash">pip install torch\&gt;=2 torchvision
pip install -e . # Basic
pip install -e &quot;.[gs]&quot; # Gaussians Estimation and Rendering
pip install -e &quot;.[app]&quot; # Gradio, python&gt;=3.10
pip install -e &quot;.[all]&quot; # ALL
</code></pre>
<p>For detailed model information, please refer to the <a href="#-model-cards">Model Cards</a> section below.</p>
<h3>ğŸ’» Basic Usage</h3>
<pre><code class="language-python">import glob, os, torch
from depth_anything_3.api import DepthAnything3
device = torch.device(&quot;cuda&quot;)
model = DepthAnything3.from_pretrained(&quot;depth-anything/DA3NESTED-GIANT-LARGE&quot;)
model = model.to(device=device)
example_path = &quot;assets/examples/SOH&quot;
images = sorted(glob.glob(os.path.join(example_path, &quot;*.png&quot;)))
prediction = model.inference(
    images,
)
# prediction.processed_images : [N, H, W, 3] uint8   array
print(prediction.processed_images.shape)
# prediction.depth            : [N, H, W]    float32 array
print(prediction.depth.shape)  
# prediction.conf             : [N, H, W]    float32 array
print(prediction.conf.shape)  
# prediction.extrinsics       : [N, 3, 4]    float32 array # opencv w2c or colmap format
print(prediction.extrinsics.shape)
# prediction.intrinsics       : [N, 3, 3]    float32 array
print(prediction.intrinsics.shape)
</code></pre>
<pre><code class="language-bash">
export MODEL_DIR=depth-anything/DA3NESTED-GIANT-LARGE
# This can be a Hugging Face repository or a local directory
# If you encounter network issues, consider using the following mirror: export HF_ENDPOINT=https://hf-mirror.com
# Alternatively, you can download the model directly from Hugging Face
export GALLERY_DIR=workspace/gallery
mkdir -p $GALLERY_DIR

# CLI auto mode with backend reuse
da3 backend --model-dir ${MODEL_DIR} --gallery-dir ${GALLERY_DIR} # Cache model to gpu
da3 auto assets/examples/SOH \
    --export-format glb \
    --export-dir ${GALLERY_DIR}/TEST_BACKEND/SOH \
    --use-backend

# CLI video processing with feature visualization
da3 video assets/examples/robot_unitree.mp4 \
    --fps 15 \
    --use-backend \
    --export-dir ${GALLERY_DIR}/TEST_BACKEND/robo \
    --export-format glb-feat_vis \
    --feat-vis-fps 15 \
    --process-res-method lower_bound_resize \
    --export-feat &quot;11,21,31&quot;

# CLI auto mode without backend reuse
da3 auto assets/examples/SOH \
    --export-format glb \
    --export-dir ${GALLERY_DIR}/TEST_CLI/SOH \
    --model-dir ${MODEL_DIR}
</code></pre>
<p>The model architecture is defined in <a href="src/depth_anything_3/model/da3.py"><code>DepthAnything3Net</code></a>, and specified with a Yaml config file located at <a href="src/depth_anything_3/configs"><code>src/depth_anything_3/configs</code></a>. The input and output processing are handled by <a href="src/depth_anything_3/api.py"><code>DepthAnything3</code></a>. To customize the model architecture, simply create a new config file (<em>e.g.</em>, <code>path/to/new/config</code>) as:</p>
<pre><code class="language-yaml">__object__:
  path: depth_anything_3.model.da3
  name: DepthAnything3Net
  args: as_params

net:
  __object__:
    path: depth_anything_3.model.dinov2.dinov2
    name: DinoV2
    args: as_params

  name: vitb
  out_layers: [5, 7, 9, 11]
  alt_start: 4
  qknorm_start: 4
  rope_start: 4
  cat_token: True

head:
  __object__:
    path: depth_anything_3.model.dualdpt
    name: DualDPT
    args: as_params

  dim_in: &amp;head_dim_in 1536
  output_dim: 2
  features: &amp;head_features 128
  out_channels: &amp;head_out_channels [96, 192, 384, 768]
</code></pre>
<p>Then, the model can be created with the following code snippet.</p>
<pre><code class="language-python">from depth_anything_3.cfg import create_object, load_config

Model = create_object(load_config(&quot;path/to/new/config&quot;))
</code></pre>
<h2>ğŸ“š Useful Documentation</h2>
<ul>
<li>ğŸ–¥ï¸ <a href="docs/CLI.md">Command Line Interface</a></li>
<li>ğŸ“‘ <a href="docs/API.md">Python API</a><!-- - ğŸ [Visual Geometry Benchmark](docs/BENCHMARK.md) --></li>
</ul>
<h2>ğŸ—‚ï¸ Model Cards</h2>
<p>Generally, you should observe that DA3-LARGE achieves comparable results to VGGT.</p>
<table>
<thead>
<tr>
<th>ğŸ—ƒï¸ Model Name</th>
<th>ğŸ“ Params</th>
<th>ğŸ“Š Rel. Depth</th>
<th>ğŸ“· Pose Est.</th>
<th>ğŸ§­ Pose Cond.</th>
<th>ğŸ¨ GS</th>
<th>ğŸ“ Met. Depth</th>
<th>â˜ï¸ Sky Seg</th>
<th>ğŸ“„ License</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Nested</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://huggingface.co/depth-anything/DA3NESTED-GIANT-LARGE">DA3NESTED-GIANT-LARGE</a></td>
<td>1.40B</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>CC BY-NC 4.0</td>
</tr>
<tr>
<td><strong>Any-view Model</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://huggingface.co/depth-anything/DA3-GIANT">DA3-GIANT</a></td>
<td>1.15B</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td></td>
<td></td>
<td>CC BY-NC 4.0</td>
</tr>
<tr>
<td><a href="https://huggingface.co/depth-anything/DA3-LARGE">DA3-LARGE</a></td>
<td>0.35B</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td></td>
<td></td>
<td></td>
<td>CC BY-NC 4.0</td>
</tr>
<tr>
<td><a href="https://huggingface.co/depth-anything/DA3-BASE">DA3-BASE</a></td>
<td>0.12B</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td></td>
<td></td>
<td></td>
<td>Apache 2.0</td>
</tr>
<tr>
<td><a href="https://huggingface.co/depth-anything/DA3-SMALL">DA3-SMALL</a></td>
<td>0.08B</td>
<td>âœ…</td>
<td>âœ…</td>
<td>âœ…</td>
<td></td>
<td></td>
<td></td>
<td>Apache 2.0</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Monocular Metric Depth</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://huggingface.co/depth-anything/DA3METRIC-LARGE">DA3METRIC-LARGE</a></td>
<td>0.35B</td>
<td>âœ…</td>
<td></td>
<td></td>
<td></td>
<td>âœ…</td>
<td>âœ…</td>
<td>Apache 2.0</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Monocular Depth</strong></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://huggingface.co/depth-anything/DA3MONO-LARGE">DA3MONO-LARGE</a></td>
<td>0.35B</td>
<td>âœ…</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>âœ…</td>
<td>Apache 2.0</td>
</tr>
</tbody></table>
<h2>â“ FAQ</h2>
<ul>
<li><strong>Older GPUs without XFormers support</strong>: See <a href="https://github.com/ByteDance-Seed/Depth-Anything-3/issues/11">Issue #11</a>. Thanks to <a href="https://github.com/S-Mahoney">@S-Mahoney</a> for the solution!</li>
</ul>
<h2>ğŸ“ Citations</h2>
<p>If you find Depth Anything 3 useful in your research or projects, please cite our work:</p>
<pre><code>@article{depthanything3,
  title={Depth Anything 3: Recovering the visual space from any views},
  author={Haotong Lin and Sili Chen and Jun Hao Liew and Donny Y. Chen and Zhenyu Li and Guang Shi and Jiashi Feng and Bingyi Kang},
  journal={arXiv preprint arXiv:2511.10647},
  year={2025}
}
</code></pre>
 </div> </article> <!-- Related Models Section -->  </div> </div>  </main> <footer class="bg-gray-800 text-gray-300 py-8 mt-16"> <div class="container mx-auto px-4 text-center"> <div class="flex justify-center gap-4 mb-4"> <a href="/about" class="hover:underline">About</a> <a href="/compliance" class="hover:underline">Compliance</a> </div> <p class="text-sm mt-2">
&copy; 2025 Free AI Tools. An open-source project to index the world of AI.
</p> <a href="mailto:compliance@free2aitools.com" class="text-sm text-gray-400 hover:underline mt-1 block">compliance@free2aitools.com</a> </div> </footer> </body></html>