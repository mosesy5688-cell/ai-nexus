<!DOCTYPE html><html lang="en"> <head><!-- Google Tag Manager --><script type="module">(function(e,n,r,t,m){e[t]=e[t]||[],e[t].push({"gtm.start":new Date().getTime(),event:"gtm.js"});var g=n.getElementsByTagName(r)[0],a=n.createElement(r),s="";a.async=!0,a.src="https://www.googletagmanager.com/gtm.js?id="+m+s,g.parentNode.insertBefore(a,g)})(window,document,"script","dataLayer","GTM-58C2CQ8G");</script><!-- End Google Tag Manager --><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><!-- Open Graph Meta Tags --><meta property="og:type" content="website"><meta property="og:url" content="https://free2aitools.com/model/llm-blender--PairRM/"><meta property="og:title" content="PairRM - AI Model Details"><meta property="og:description" content="A model for text-generation."><meta property="og:image" content="https://free2aitools.com/og-image.jpg"><!-- Twitter Card Meta Tags --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://free2aitools.com/model/llm-blender--PairRM/"><meta property="twitter:title" content="PairRM - AI Model Details"><meta property="twitter:description" content="A model for text-generation."><meta property="twitter:image" content="https://free2aitools.com/og-image.jpg"><meta name="description" content="A model for text-generation."><title>PairRM - AI Model Details</title><link rel="canonical" href="https://free2aitools.com/model/llm-blender--PairRM/"><!-- Google AdSense --><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2292826803755214" crossorigin="anonymous"></script><link rel="stylesheet" href="/_astro/about.CAZw8hUu.css"></head> <body class="bg-gray-50 text-gray-800 font-sans antialiased flex flex-col min-h-screen"> <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-58C2CQ8G" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) --> <header class="sticky top-0 z-30 w-full backdrop-blur flex-none transition-colors duration-500 lg:border-b lg:border-gray-200 bg-white/80"> <nav class="container mx-auto px-4 sm:px-6 lg:px-8"> <div class="flex items-center justify-between h-16"> <!-- Site Logo --> <a href="/" class="flex items-center gap-2 text-xl sm:text-2xl font-bold text-gray-900"> <span>Free AI Tools</span> </a> <!-- Right-side items --> <div class="flex items-center gap-4"> <!-- Desktop Navigation --> <div class="hidden md:flex items-center gap-6"> <a href="/" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Explore Models </a><a href="/ranking" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Rankings </a><a href="/reports" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Reports </a><a href="/about" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> About </a> </div> <!-- Mobile Search & Menu Toggle --> <button data-collapse-toggle="navbar-default" type="button" class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200" aria-controls="navbar-default" aria-expanded="false"> <span class="sr-only">Open main menu</span> <svg class="w-5 h-5" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 17 14"><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 1h15M1 7h15M1 13h15"></path></svg> </button> </div> </div> <!-- Mobile Collapsible Menu --> <div class="hidden w-full md:hidden" id="navbar-default"> <ul class="font-medium flex flex-col p-4 md:p-0 mt-4 border border-gray-100 rounded-lg bg-gray-50"> <li> <a href="/" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Explore Models</a> </li><li> <a href="/ranking" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Rankings</a> </li><li> <a href="/reports" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Reports</a> </li><li> <a href="/about" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">About</a> </li> </ul> </div> </nav> </header> <main class="flex-grow">  <div class="container mx-auto px-4 py-12"> <div class="max-w-4xl mx-auto"> <!-- Header Section --> <header class="mb-8"> <div class="flex flex-col md:flex-row md:items-start md:justify-between gap-4"> <div> <h1 class="text-4xl md:text-5xl font-extrabold text-gray-900 dark:text-white mb-2">PairRM</h1> <p class="text-lg text-gray-500 dark:text-gray-400">by </p> </div>  </div> </header> <!-- Metadata Section --> <div class="mb-8 p-4 bg-gray-100 dark:bg-gray-800 rounded-lg"> <div class="grid grid-cols-2 md:grid-cols-4 gap-4 text-center"> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Likes</p> <p class="text-xl font-bold">3,075</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Downloads</p> <p class="text-xl font-bold">34,455</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Task</p> <p class="text-xl font-bold capitalize">text-generation</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Last Updated</p> <p class="text-xl font-bold">Invalid Date</p> </div> </div> </div> <h3 class="text-lg font-semibold mb-2">Sources</h3> <div class="flex flex-col gap-2"> <a href="https://huggingface.co/llm-blender/PairRM" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a><a href="https://huggingface.co/llm-blender/PairRM" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a><a href="https://huggingface.co/llm-blender/PairRM" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a><a href="https://huggingface.co/llm-blender/PairRM" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a><a href="https://huggingface.co/llm-blender/PairRM" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a> </div> </div> </div>  <div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div><div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div><div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div><div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div><div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div> <article class="model-readme-content py-8 border-t border-gray-200 dark:border-gray-700"> <div class="prose prose-lg dark:prose-invert max-w-none"> <h1>Pairwise Reward Model for LLMs (PairRM) from LLM-Blender</h1>
<ul>
<li>Github: <a href="https://github.com/yuchenlin/LLM-Blender">https://github.com/yuchenlin/LLM-Blender</a></li>
<li>Paper: <a href="https://arxiv.org/abs/2306.02561">https://arxiv.org/abs/2306.02561</a></li>
<li>Space Demo: <a href="https://huggingface.co/spaces/llm-blender/LLM-Blender">https://huggingface.co/spaces/llm-blender/LLM-Blender</a></li>
</ul>
<h2>News</h2>
<ul>
<li>Check out our results on AlpacaEval leaderboard: <a href="https://x.com/billyuchenlin/status/1732198787354067380?s=20">Twitter</a> <a href="https://tatsu-lab.github.io/alpaca_eval/">Leaderboard</a></li>
</ul>
<h2>Introduction</h2>
<p>Pairwise Reward Model (PairRM) takes an instruction and a <strong>pair</strong> of output candidates as the input, 
and output a score for each candidate to measure their <strong>relative</strong> quality. 
PairRM can be used to (re-)rank a list of candidate outputs and thus can be used an LLM evaluator to efficiently assess the quality of LLMs in local environment.
PairRM can also be used to enhance the decoding by <code>best-of-n sampling</code> (i.e., reranking N sampled outputs). 
Apart from that, one can also use PairRM to further align instruction-tuned LLMs with RLHF methods. </p>
<p>Unlike the other RMs that encode and score each candidate respectively, 
PairRM takes a pair of candidates and compares them side-by-side to indentify the subtle differences between them.
Also, PairRM is based on <a href="https://huggingface.co/microsoft/deberta-v3-large"><code>microsoft/deberta-v3-large</code></a>, and thus it is super efficient: <strong>0.4B</strong>.
We trained PairRM on a diverse collection of six human-preference datasets (see more <a href="https://huggingface.co/llm-blender/PairRM#training-datasets">here</a>).</p>
<p>PairRM is part of the LLM-Blender project (ACL 2023). Please see our <a href="https://arxiv.org/abs/2306.02561">paper</a> above to know more.</p>
<h2>Installation</h2>
<ul>
<li>First install <code>llm-blender</code></li>
</ul>
<pre><code class="language-bash">pip install git+https://github.com/yuchenlin/LLM-Blender.git
</code></pre>
<ul>
<li>Then load PairRM:</li>
</ul>
<pre><code class="language-python">import llm_blender
blender = llm_blender.Blender()
blender.loadranker(&quot;llm-blender/PairRM&quot;) # load PairRM
</code></pre>
<h2>Usage</h2>
<h3>Use Case 1: Comparing/Ranking output candidates given an instruction</h3>
<ul>
<li>Ranking a list candidate responses</li>
</ul>
<pre><code class="language-python">inputs = [&quot;hello, how are you!&quot;, &quot;I love you!&quot;]
candidates_texts = [[&quot;get out!&quot;, &quot;hi! I am fine, thanks!&quot;, &quot;bye!&quot;], 
                    [&quot;I love you too!&quot;, &quot;I hate you!&quot;, &quot;Thanks! You&#39;re a good guy!&quot;]]
ranks = blender.rank(inputs, candidates_texts, return_scores=False, batch_size=1)
# ranks is a list of ranks
# ranks[i][j] represents the ranks of candidate-j for input-i
&quot;&quot;&quot;
ranks --&gt;
array([[3, 1, 2], # it means &quot;hi! I am fine, thanks!&quot; ranks the 1st, &quot;bye&quot; ranks the 2nd, and &quot;get out!&quot; ranks the 3rd. 
       [1, 3, 2]], # it means &quot;I love you too&quot;! ranks the the 1st, and &quot;I hate you!&quot; ranks the 3rd.
       dtype=int32) 

&quot;&quot;&quot;
</code></pre>
<ul>
<li>Directly comparing two candidate responses</li>
</ul>
<pre><code class="language-python">inputs = [&quot;hello!&quot;, &quot;I love you!&quot;]
candidates_A = [&quot;hi!&quot;, &quot;I hate you!&quot;]
candidates_B = [&quot;f**k off!&quot;, &quot;I love you, too!&quot;]
comparison_results = blender.compare(inputs, candidates_A, candidates_B)
# comparison_results is a list of bool, where comparison_results[i] denotes
       # whether candidates_A[i] is better than candidates_B[i] for inputs[i]
# Example: comparison_results[0]--&gt; True 
</code></pre>
<details><summary> Comparing two multi-turn conversations. </summary>

<pre><code class="language-python">conv1 = [
    {
        &quot;content&quot;: &quot;hello&quot;,
        &quot;role&quot;: &quot;USER&quot;
    },
    {
        &quot;content&quot;: &quot;[assistant1‚Äòs response 1]&quot;,
        &quot;role&quot;: &quot;ASSISTANT&quot;
    },
    ...
]
conv2 = [
    {
        &quot;content&quot;: &quot;hello&quot;,
        &quot;role&quot;: &quot;USER&quot;
    },
    {
        &quot;content&quot;: &quot;[assistant2&#39;s response 1]&quot;,
        &quot;role&quot;: &quot;ASSISTANT&quot;
    },
    ...
]
comparison_results = blender.compare_conversations([conv1], [conv2])
# comparison_results is a list of bool, where each element denotes whether all the responses in conv1 together is better than that of conv2
</code></pre>
</details>

          
<h3>Use Case 2: Best-of-n Sampling (Decoding Enhancment)</h3>
<p><strong>Best-of-n Sampling</strong>, aka, rejection sampling, is a strategy to enhance the response quality by selecting the one that was ranked highest by the reward model 
(see more in <a href="https://arxiv.org/pdf/2112.09332.pdf">OpenAI WebGPT section 3.2</a> and <a href="https://openai.com/research/measuring-goodharts-law">OpenAI Blog</a>). 
Best-of-n sampling with PairRM is a very easy way to imporve your LLMs with only a few changes of your inference code: </p>
<pre><code class="language-python"># loading models 
import llm_blender
from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained(&quot;HuggingFaceH4/zephyr-7b-beta&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;HuggingFaceH4/zephyr-7b-beta&quot;, device_map=&quot;auto&quot;)
system_message = {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a friendly chatbot.&quot;}

# formatting your inputs 
inputs = [&quot;can you tell me a joke about OpenAI?&quot;]
messages = [[system_message, {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: _input}] for _input in inputs]
prompts = [tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True) for m in messages]

# Conventional generation method 
input_ids = tokenizer(prompts[0], return_tensors=&quot;pt&quot;).input_ids
sampled_outputs = model.generate(input_ids, do_sample=True, top_k=50, top_p=0.95, num_return_sequences=1)
print(tokenizer.decode(sampled_outputs[0][len(input_ids[0]):], skip_special_tokens=False))
# --&gt; The output could be a bad case such as a very short one, e.g., `Sure` 

# PairRM for best-of-n sampling 
blender = llm_blender.Blender()
blender.loadranker(&quot;llm-blender/PairRM&quot;) # load ranker checkpoint
outputs = blender.best_of_n_generate(model, tokenizer, prompts, n=10)

print(&quot;### Prompt:\n&quot;, prompts[0])
print(&quot;### best-of-n generations:\n&quot;, outputs[0])
# --&gt; The output will be much more stable and consistently better than single sampling, for example: 
&quot;&quot;&quot; 
Sure, here&#39;s a joke about OpenAI:

Why did OpenAI decide to hire a mime as their new AI researcher?

Because they wanted someone who could communicate complex ideas without making a sound!

(Note: This is a joke, not a reflection of OpenAI&#39;s actual hiring practices.)
&quot;&quot;&quot;
</code></pre>
<h3>Use case 3: RLHF</h3>
<p>PairRM has been trained on various high-quality and large-scale datasets with human preference annotations 
and shown great correlation with human preferences with an extremely small model size (0.4B), 
approching the performance of GPT-4. 
PairRM will better help the future alignment of LLMs in a more efficient and effective way.
With a <code>blender.compare()</code> function, you can apply PairRM to popular RLHF toolkits such as <a href="https://huggingface.co/docs/trl/index">trl</a>. </p>
<p><strong>üî• Check more details on our example jupyter notebook usage: <a href="https://github.com/yuchenlin/LLM-Blender/blob/main/blender_usage.ipynb"><code>blender_usage.ipynb</code></a></strong></p>
<p>Learn more in our LLM-Blender Github <a href="https://github.com/yuchenlin/LLM-Blender#rank-and-fusion">README.md</a></p>
<h2>Statistics</h2>
<h3>Context length</h3>
<table>
<thead>
<tr>
<th align="center">PairRanker type</th>
<th align="center">Source max length</th>
<th>Candidate max length</th>
<th>Total max length</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><a href="https://huggingface.co/llm-blender/pair-ranker">pair-ranker</a>  (our previous version)</td>
<td align="center">128</td>
<td>128</td>
<td>384</td>
</tr>
<tr>
<td align="center"><a href="https://huggingface.co/llm-blender/pair-reward-model/">PairRM</a> (This model)</td>
<td align="center">1224</td>
<td>412</td>
<td>2048</td>
</tr>
</tbody></table>
<h3>Training Datasets</h3>
<ul>
<li><a href="https://huggingface.co/datasets/openai/summarize_from_feedback">openai/summarize_from_feedback</a></li>
<li><a href="https://huggingface.co/datasets/openai/webgpt_comparisons">openai/webgpt_comparisons</a></li>
<li><a href="https://huggingface.co/datasets/Dahoas/synthetic-instruct-gptj-pairwise">Dahoas/synthetic-instruct-gptj-pairwise</a></li>
<li><a href="https://huggingface.co/datasets/Anthropic/hh-rlhf">Anthropic/hh-rlhf</a></li>
<li><a href="https://huggingface.co/datasets/lmsys/chatbot_arena_conversations">lmsys/chatbot_arena_conversations</a></li>
<li><a href="https://huggingface.co/datasets/openbmb/UltraFeedback">openbmb/UltraFeedback</a></li>
</ul>
<h3>Performance</h3>
<p>PairRM has been trained on various high-quality and large-scale dataset with human preference annotations and exhibits great correlation with human preferences 
with an extremly small model size (0.4B), approching the performance of GPT-4.</p>
<p>We test the pairwise comparison on </p>
<ul>
<li><a href="https://github.com/GAIR-NLP/auto-j#pairwise-response-comparison">Auto-J pairwise testdata</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceH4/hhh_alignment">HHH-alignment</a></li>
<li><a href="https://huggingface.co/datasets/lmsys/mt_bench_human_judgments">MT-bench-human-judgements</a></li>
</ul>
<p>All following results are reported as pairwise comparison accuracies (agreements).</p>
<h4>Auto-J Pairwise test data performance</h4>
<table>
<thead>
<tr>
<th align="center">Model</th>
<th align="center">Summ</th>
<th align="center">Exam</th>
<th align="center">Code</th>
<th align="center">Rewriting</th>
<th align="center">Crea W</th>
<th align="center">Func W</th>
<th align="center">Comm</th>
<th align="center">NLP</th>
<th align="center">Overall</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Closed -source Models</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">ChatGPT</td>
<td align="center">33.3</td>
<td align="center">40.3</td>
<td align="center">36.6</td>
<td align="center">31.6</td>
<td align="center">48.2</td>
<td align="center">40.4</td>
<td align="center">47.6</td>
<td align="center">45.8</td>
<td align="center">42.7</td>
</tr>
<tr>
<td align="center">Claude -2</td>
<td align="center">30.6</td>
<td align="center">36.1</td>
<td align="center">41.7</td>
<td align="center">34.2</td>
<td align="center">48.1</td>
<td align="center">42.5</td>
<td align="center">40.6</td>
<td align="center">48.5</td>
<td align="center">42.4</td>
</tr>
<tr>
<td align="center">GPT -4</td>
<td align="center">59.7</td>
<td align="center">51.4</td>
<td align="center">69.2</td>
<td align="center">58.3</td>
<td align="center">66.7</td>
<td align="center">60.4</td>
<td align="center">58.3</td>
<td align="center">65.2</td>
<td align="center">61.9</td>
</tr>
<tr>
<td align="center">Open -source Models</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">SteamSHP</td>
<td align="center">33.3</td>
<td align="center">29.2</td>
<td align="center">26.7</td>
<td align="center">33.3</td>
<td align="center">40.7</td>
<td align="center">31.3</td>
<td align="center">51.4</td>
<td align="center">51.9</td>
<td align="center">40.6</td>
</tr>
<tr>
<td align="center">PandaLM</td>
<td align="center">29.2</td>
<td align="center">33.3</td>
<td align="center">31.7</td>
<td align="center">23.3</td>
<td align="center">43.5</td>
<td align="center">32.9</td>
<td align="center">44.8</td>
<td align="center">48.9</td>
<td align="center">38.9</td>
</tr>
<tr>
<td align="center">LLaMA -2-Chat -13B</td>
<td align="center">20.8</td>
<td align="center">27.8</td>
<td align="center">19.2</td>
<td align="center">20</td>
<td align="center">31.5</td>
<td align="center">27.5</td>
<td align="center">35.8</td>
<td align="center">31.8</td>
<td align="center">29</td>
</tr>
<tr>
<td align="center">Vicuna -13B-v1.5</td>
<td align="center">30.6</td>
<td align="center">23.6</td>
<td align="center">35</td>
<td align="center">28.3</td>
<td align="center">36.1</td>
<td align="center">37.5</td>
<td align="center">45.5</td>
<td align="center">39.8</td>
<td align="center">37.3</td>
</tr>
<tr>
<td align="center">WizardLM -13B-v1.2</td>
<td align="center">22.2</td>
<td align="center">20.8</td>
<td align="center">32.5</td>
<td align="center">19.2</td>
<td align="center">28.7</td>
<td align="center">25.4</td>
<td align="center">29.2</td>
<td align="center">33</td>
<td align="center">27.8</td>
</tr>
<tr>
<td align="center">LLAMA -2-chat -70B</td>
<td align="center">34.7</td>
<td align="center">33.3</td>
<td align="center">36.7</td>
<td align="center">35.8</td>
<td align="center">51.4</td>
<td align="center">54.2</td>
<td align="center">47.2</td>
<td align="center">47.7</td>
<td align="center">45.9</td>
</tr>
<tr>
<td align="center">AUTO -J (13b)</td>
<td align="center">45.8</td>
<td align="center">38.9</td>
<td align="center"><strong>59.2</strong></td>
<td align="center">47.5</td>
<td align="center">54.6</td>
<td align="center">57.1</td>
<td align="center"><strong>58</strong></td>
<td align="center">57.6</td>
<td align="center">54.8</td>
</tr>
<tr>
<td align="center">UltraRM (13b)</td>
<td align="center">56.94</td>
<td align="center">43.06</td>
<td align="center">55.0</td>
<td align="center">53.33</td>
<td align="center"><strong>67.13</strong></td>
<td align="center"><strong>64.17</strong></td>
<td align="center">56.25</td>
<td align="center">59.85</td>
<td align="center"><strong>59.85</strong></td>
</tr>
<tr>
<td align="center"><strong>PairRM (0.4b)</strong></td>
<td align="center"><strong>56.94</strong></td>
<td align="center"><strong>52.78</strong></td>
<td align="center">58.33</td>
<td align="center"><strong>55.83</strong></td>
<td align="center">61.57</td>
<td align="center">59.17</td>
<td align="center">57.64</td>
<td align="center"><strong>62.5</strong></td>
<td align="center">59.05</td>
</tr>
</tbody></table>
<h4>HHH-Alignment and MT-bench human judgements</h4>
<table>
<thead>
<tr>
<th align="center">Evaluator LM</th>
<th align="center">HHH ALIGNMENT</th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"></th>
<th align="center">MT BENCH HUMAN JUDG .</th>
</tr>
</thead>
<tbody><tr>
<td align="center"></td>
<td align="center">Help .</td>
<td align="center">Harm .</td>
<td align="center">Hon .</td>
<td align="center">Other</td>
<td align="center">Total Avg .</td>
<td align="center">Human Preference</td>
</tr>
<tr>
<td align="center">RANDOM</td>
<td align="center">50</td>
<td align="center">50</td>
<td align="center">50</td>
<td align="center">50</td>
<td align="center">50</td>
<td align="center">34.26</td>
</tr>
<tr>
<td align="center">STANFORDNLP REWARD MODEL</td>
<td align="center">69.49</td>
<td align="center">60.34</td>
<td align="center">52.46</td>
<td align="center">51.16</td>
<td align="center">58.82</td>
<td align="center">44.79</td>
</tr>
<tr>
<td align="center">ALMOST REWARD MODEL</td>
<td align="center">74.58</td>
<td align="center">67.24</td>
<td align="center">78.69</td>
<td align="center">86.05</td>
<td align="center">76.02</td>
<td align="center">49.9</td>
</tr>
<tr>
<td align="center">LLAMA2 -CHAT 7B</td>
<td align="center">66.1</td>
<td align="center">81.03</td>
<td align="center">70.49</td>
<td align="center">74.42</td>
<td align="center">72.85</td>
<td align="center">51.78</td>
</tr>
<tr>
<td align="center">LLAMA2 -CHAT 13B</td>
<td align="center">74.58</td>
<td align="center">87.93</td>
<td align="center">55.74</td>
<td align="center">79.07</td>
<td align="center">73.76</td>
<td align="center">52.34</td>
</tr>
<tr>
<td align="center">LLAMA2 -CHAT 70B</td>
<td align="center">66.1</td>
<td align="center"><strong>89.66</strong></td>
<td align="center">67.21</td>
<td align="center">74.42</td>
<td align="center">74.21</td>
<td align="center">53.67</td>
</tr>
<tr>
<td align="center">LLAMA2 -CHAT 13B+COARSE .</td>
<td align="center">68.74</td>
<td align="center">68.97</td>
<td align="center">65.57</td>
<td align="center">67.44</td>
<td align="center">67.42</td>
<td align="center">46.89</td>
</tr>
<tr>
<td align="center">GPT -3.5-TURBO -0613</td>
<td align="center">76.27</td>
<td align="center">87.93</td>
<td align="center">67.21</td>
<td align="center">86.05</td>
<td align="center">78.73</td>
<td align="center">57.12</td>
</tr>
<tr>
<td align="center">PROMETHEUS 7B</td>
<td align="center">69.49</td>
<td align="center">84.48</td>
<td align="center">78.69</td>
<td align="center">90.7</td>
<td align="center">80.09</td>
<td align="center">55.14</td>
</tr>
<tr>
<td align="center">PROMETHEUS 13B</td>
<td align="center">81.36</td>
<td align="center">82.76</td>
<td align="center">75.41</td>
<td align="center">76.74</td>
<td align="center">79.19</td>
<td align="center">57.72</td>
</tr>
<tr>
<td align="center">UltraRM (13B)</td>
<td align="center"><strong>86.44</strong></td>
<td align="center">79.31</td>
<td align="center"><strong>81.97</strong></td>
<td align="center">88.37</td>
<td align="center">83.71</td>
<td align="center">56</td>
</tr>
<tr>
<td align="center"><strong>PairRM (0.4B)</strong></td>
<td align="center">84.75</td>
<td align="center">84.48</td>
<td align="center">80.33</td>
<td align="center"><strong>90.7</strong></td>
<td align="center"><strong>84.62</strong></td>
<td align="center"><strong>59</strong></td>
</tr>
<tr>
<td align="center">GPT -4-0613</td>
<td align="center">91.53</td>
<td align="center">93.1</td>
<td align="center">85.25</td>
<td align="center">83.72</td>
<td align="center">88.69</td>
<td align="center">63.87</td>
</tr>
</tbody></table>
<p><strong>While PairRM is a extremely small model (0.4B) based on deberta, the pairwise comparison aggrement performance approches GPT-4&#39;s performance!</strong></p>
<p>Two reasons to attribute:</p>
<ul>
<li>Our PairRM specically designed model arch for pairwise comparison through bidirectional attention (See LLM-blender paper for more details)</li>
<li>The high-quality and large-scale human preference annotation data it was train on (see training dataset list on this hugging face page)</li>
</ul>
<h2>Citation &amp; Credits</h2>
<p>If you are using PairRM in your research, please cite LLM-blender.</p>
<pre><code class="language-bibtex">@inproceedings{llm-blender-2023,
    title = &quot;LLM-Blender: Ensembling Large Language Models with Pairwise Comparison and Generative Fusion&quot;,
    author = &quot;Jiang, Dongfu and Ren, Xiang and Lin, Bill Yuchen&quot;,
    booktitle = &quot;Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (ACL 2023)&quot;,
    year = &quot;2023&quot;
}
</code></pre>
 </div> </article>  <section class="mt-16 pt-8 border-t border-gray-200 dark:border-gray-700"> <h2 class="text-3xl font-bold text-center mb-8">Related Models</h2> <div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-6"> <a href="/model/llm-blender--PairRM-hf" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full">  <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="PairRM-hf"> PairRM-hf </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by </p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> A model for text-generation....
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="240 likes">‚ù§Ô∏è <span>240</span></div> <div class="flex items-center gap-1" title="4,935 downloads">üì• <span>4.9K</span></div> </div> </div> </a><a href="/model/Lamapi--next-12b" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full">  <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="next-12b"> next-12b </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by </p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> A model for image-text-to-text....
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="165 likes">‚ù§Ô∏è <span>165</span></div> <div class="flex items-center gap-1" title="27,960 downloads">üì• <span>28.0K</span></div> </div> </div> </a><a href="/model/Lamapi--next-1b" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full">  <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="next-1b"> next-1b </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by </p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> A model for text-generation....
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="135 likes">‚ù§Ô∏è <span>135</span></div> <div class="flex items-center gap-1" title="53,385 downloads">üì• <span>53.4K</span></div> </div> </div> </a> </div> </section> </main> <footer class="bg-gray-800 text-gray-300 py-8 mt-16"> <div class="container mx-auto px-4 text-center"> <div class="flex justify-center gap-4 mb-4"> <a href="/about" class="hover:underline">About</a> <a href="/compliance" class="hover:underline">Compliance</a> </div> <p class="text-sm mt-2">
&copy; 2025 Free AI Tools. An open-source project to index the world of AI.
</p> <a href="mailto:compliance@free2aitools.com" class="text-sm text-gray-400 hover:underline mt-1 block">compliance@free2aitools.com</a> </div> </footer> </body></html>