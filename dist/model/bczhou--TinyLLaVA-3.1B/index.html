<!DOCTYPE html><html lang="en"> <head><!-- Google Tag Manager --><script type="module">(function(e,n,r,t,m){e[t]=e[t]||[],e[t].push({"gtm.start":new Date().getTime(),event:"gtm.js"});var g=n.getElementsByTagName(r)[0],a=n.createElement(r),s="";a.async=!0,a.src="https://www.googletagmanager.com/gtm.js?id="+m+s,g.parentNode.insertBefore(a,g)})(window,document,"script","dataLayer","GTM-58C2CQ8G");</script><!-- End Google Tag Manager --><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><!-- Open Graph Meta Tags --><meta property="og:type" content="website"><meta property="og:url" content="https://free2aitools.com/model/bczhou--TinyLLaVA-3.1B/"><meta property="og:title" content="TinyLLaVA-3.1B - AI Model Details"><meta property="og:description" content="A model for text-generation."><meta property="og:image" content="https://free2aitools.com/og-image.jpg"><!-- Twitter Card Meta Tags --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://free2aitools.com/model/bczhou--TinyLLaVA-3.1B/"><meta property="twitter:title" content="TinyLLaVA-3.1B - AI Model Details"><meta property="twitter:description" content="A model for text-generation."><meta property="twitter:image" content="https://free2aitools.com/og-image.jpg"><meta name="description" content="A model for text-generation."><title>TinyLLaVA-3.1B - AI Model Details</title><link rel="canonical" href="https://free2aitools.com/model/bczhou--TinyLLaVA-3.1B/"><!-- Google AdSense --><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2292826803755214" crossorigin="anonymous"></script><link rel="stylesheet" href="/_astro/about.C4wyk6Sp.css"></head> <body class="bg-gray-50 text-gray-800 font-sans antialiased flex flex-col min-h-screen"> <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-58C2CQ8G" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) --> <header class="sticky top-0 z-30 w-full backdrop-blur flex-none transition-colors duration-500 lg:border-b lg:border-gray-200 bg-white/80"> <nav class="container mx-auto px-4 sm:px-6 lg:px-8"> <div class="flex items-center justify-between h-16"> <!-- Site Logo --> <a href="/" class="flex items-center gap-2 text-xl sm:text-2xl font-bold text-gray-900"> <span>Free AI Tools</span> </a> <!-- Right-side items --> <div class="flex items-center gap-4"> <!-- Desktop Navigation --> <div class="hidden md:flex items-center gap-6"> <a href="/" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Explore Models </a><a href="/ranking" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Rankings </a><a href="/reports" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Reports </a><a href="/about" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> About </a> </div> <!-- Mobile Search & Menu Toggle --> <button data-collapse-toggle="navbar-default" type="button" class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200" aria-controls="navbar-default" aria-expanded="false"> <span class="sr-only">Open main menu</span> <svg class="w-5 h-5" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 17 14"><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 1h15M1 7h15M1 13h15"></path></svg> </button> </div> </div> <!-- Mobile Collapsible Menu --> <div class="hidden w-full md:hidden" id="navbar-default"> <ul class="font-medium flex flex-col p-4 md:p-0 mt-4 border border-gray-100 rounded-lg bg-gray-50"> <li> <a href="/" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Explore Models</a> </li><li> <a href="/ranking" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Rankings</a> </li><li> <a href="/reports" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Reports</a> </li><li> <a href="/about" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">About</a> </li> </ul> </div> </nav> </header> <main class="flex-grow">  <div class="container mx-auto px-4 py-12"> <div class="max-w-4xl mx-auto"> <!-- Header Section --> <header class="mb-8"> <div class="flex flex-col md:flex-row md:items-start md:justify-between gap-4"> <div> <h1 class="text-4xl md:text-5xl font-extrabold text-gray-900 dark:text-white mb-2">TinyLLaVA-3.1B</h1> <p class="text-lg text-gray-500 dark:text-gray-400">by </p> </div>  </div> </header> <!-- Metadata Section --> <div class="mb-8 p-4 bg-gray-100 dark:bg-gray-800 rounded-lg"> <div class="grid grid-cols-2 md:grid-cols-4 gap-4 text-center"> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Likes</p> <p class="text-xl font-bold">135</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Downloads</p> <p class="text-xl font-bold">755</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Task</p> <p class="text-xl font-bold capitalize">text-generation</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Last Updated</p> <p class="text-xl font-bold">Invalid Date</p> </div> </div> </div> <!-- Tags and Sources --> <div class="mb-8 flex flex-wrap items-start gap-4"> <div class="flex-1"> <h3 class="text-lg font-semibold mb-2">Tags</h3> <div class="flex flex-wrap gap-2"> <a href="/?tag=transformers" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> transformers </a><a href="/?tag=safetensors" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> safetensors </a><a href="/?tag=tiny_llava_phi" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> tiny_llava_phi </a><a href="/?tag=text-generation" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> text-generation </a><a href="/?tag=llava" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> llava </a><a href="/?tag=vision-language" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> vision-language </a><a href="/?tag=llm" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> llm </a><a href="/?tag=lmm" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> lmm </a><a href="/?tag=custom_code" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> custom_code </a><a href="/?tag=en" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> en </a> </div> </div> <div class="flex-shrink-0"> <h3 class="text-lg font-semibold mb-2">Sources</h3> <div class="flex flex-col gap-2"> <a href="https://huggingface.co/bczhou/TinyLLaVA-3.1B" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a><a href="https://huggingface.co/bczhou/TinyLLaVA-3.1B" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a><a href="https://huggingface.co/bczhou/TinyLLaVA-3.1B" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a><a href="https://huggingface.co/bczhou/TinyLLaVA-3.1B" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a><a href="https://huggingface.co/bczhou/TinyLLaVA-3.1B" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a> </div> </div> </div> <!-- Source Specific Details --> <div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div><div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div><div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div><div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div><div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div> <!-- README Content --> <article class="model-readme-content py-8 border-t border-gray-200 dark:border-gray-700"> <div class="prose prose-lg dark:prose-invert max-w-none"> <h2 align="center"> <a href="https://arxiv.org/abs/2402.14289">TinyLLaVA: A Framework of Small-scale Large Multimodal Models</a>

<h5 align="center">

<p><a href="https://github.com/DLCV-BUAA/TinyLLaVABench"><img src="https://img.shields.io/badge/GitHub-TinyLLaVA-blue" alt="github"></a> <a href="https://arxiv.org/abs/2402.14289"><img src="https://img.shields.io/badge/Arxiv-2402.14289-b31b1b.svg?logo=arXiv" alt="arXiv"></a> <a href="https://github.com/PKU-YuanGroup/MoE-LLaVA/blob/main/LICENSE"><img src="https://img.shields.io/badge/License-Apache%202.0-yellow" alt="License"></a> </p>
<h2>&#x1F389; News</h2>
<ul>
<li><strong>[2024.03.10]</strong>  base recipe out!</li>
<li><strong>[2024.03.10]</strong>  Finetune scripts out!</li>
<li><strong>[2024.02.25]</strong>  Update evaluation scripts and docs!</li>
<li><strong>[2024.02.25]</strong>  Data descriptions out. Release TinyLLaVA-1.5B and TinyLLaVA-2.0B!</li>
<li><strong>[2024.02.24]</strong>  Example code on inference and model loading added!</li>
<li><strong>[2024.02.23]</strong>  Evaluation code and scripts released!</li>
<li><strong>[2024.02.21]</strong>  Creating the <a href="https://github.com/DLCV-BUAA/TinyLLavaBench">TinyLLaVABench</a> repository on GitHub!</li>
<li><strong>[2024.02.21]</strong>  Our paper: <a href="https://arxiv.org/abs/2402.14289">TinyLLaVA: A Framework of Small-scale Large Multimodal Models</a> is out!</li>
<li><strong>[2024.01.11]</strong>  Our fist model <a href="https://huggingface.co/bczhou/tiny-llava-v1-hf">TinyLLaVA-1.4B</a> is out!</li>
</ul>
<h2>&#x231B; TODO</h2>
<ul>
<li><input disabled="" type="checkbox"> Add support for Ollama and llama.cpp.</li>
<li><input checked="" disabled="" type="checkbox"> Developers&#39; guide / How to build demo locally.</li>
<li><input checked="" disabled="" type="checkbox"> Training and custom finetuning docs.</li>
<li><input checked="" disabled="" type="checkbox"> Model Zoo descriptions.</li>
<li><input checked="" disabled="" type="checkbox"> Examples and inference.</li>
<li><input checked="" disabled="" type="checkbox"> Release code for training.</li>
<li><input checked="" disabled="" type="checkbox"> Add descriptions for evaluation.</li>
<li><input checked="" disabled="" type="checkbox"> Add descriptions for data preparation.</li>
<li><input checked="" disabled="" type="checkbox"> Release TinyLLaVA-1.5B and TinyLLaVA-2.0B.</li>
<li><input checked="" disabled="" type="checkbox"> Release TinyLLaVA-3.1B.</li>
<li><input checked="" disabled="" type="checkbox"> Release the evaluation code and weights today(2024.2.23).</li>
</ul>
<h3>&#x1F525; High performance, but with fewer parameters</h3>
<ul>
<li>Our best model, TinyLLaVA-3.1B, achieves better overall performance against existing 7B models such as LLaVA-1.5 and Qwen-VL.</li>
</ul>
<h2>Contents</h2>
<ul>
<li><a href="#x1f527-requirements-and-installation">Install</a></li>
<li><a href="#x1f433-model-zoo">Model Zoo</a></li>
<li><a href="#Demo">Demo</a></li>
<li><a href="#x1f527-quick-start">Quick Start</a></li>
<li><a href="#x1f527-run-inference">Run Inference</a></li>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#data-preparation">Data</a></li>
<li><a href="#train">Train</a></li>
<li><a href="#custom-finetune">Custom Finetune</a></li>
</ul>
<h2>&#x1F527; Requirements and Installation</h2>
<p>We recommend the requirements as follows.</p>
<ol>
<li>Clone this repository and navigate to LLaVA folder</li>
</ol>
<pre><code class="language-bash">git clone https://github.com/DLCV-BUAA/TinyLLaVABench.git
cd TinyLLaVABench
</code></pre>
<ol start="2">
<li>Install Package</li>
</ol>
<pre><code class="language-Shell">conda create -n tinyllava python=3.10 -y
conda activate tinyllava
pip install --upgrade pip  # enable PEP 660 support
pip install -e .
</code></pre>
<ol start="3">
<li>Install additional packages for training cases</li>
</ol>
<pre><code class="language-Shell">pip install -e &quot;.[train]&quot;
pip install flash-attn --no-build-isolation
</code></pre>
<h3>Upgrade to the latest code base</h3>
<pre><code class="language-Shell">git pull
pip install -e .

# if you see some import errors when you upgrade, please try running the command below (without #)
# pip install flash-attn --no-build-isolation --no-cache-dir
</code></pre>
<h2>&#x1F433; Model Zoo</h2>
<h3>Legacy Model</h3>
<ul>
<li><a href="https://huggingface.co/bczhou/tiny-llava-v1-hf">tiny-llava-hf</a></li>
</ul>
<h3>Pretrained Models</h3>
<ul>
<li><a href="https://huggingface.co/bczhou/TinyLLaVA-3.1B">TinyLLaVA-3.1B</a></li>
<li><a href="https://huggingface.co/bczhou/TinyLLaVA-2.0B">TinyLLaVA-2.0B</a></li>
<li><a href="https://huggingface.co/bczhou/TinyLLaVA-1.5B">TinyLLaVA-1.5B</a></li>
</ul>
<h3>Model Details</h3>
<table>
<thead>
<tr>
<th>Name</th>
<th>LLM</th>
<th>Checkpoint</th>
<th>LLaVA-Bench-Wild</th>
<th>MME</th>
<th>MMBench</th>
<th>MM-Vet</th>
<th>SQA-image</th>
<th>VQA-v2</th>
<th>GQA</th>
<th>TextVQA</th>
</tr>
</thead>
<tbody><tr>
<td>TinyLLaVA-3.1B</td>
<td>Phi-2</td>
<td><a href="https://huggingface.co/bczhou/TinyLLaVA-3.1B">TinyLLaVA-3.1B</a></td>
<td>75.8</td>
<td>1464.9</td>
<td>66.9</td>
<td>32.0</td>
<td>69.1</td>
<td>79.9</td>
<td>62.0</td>
<td>59.1</td>
</tr>
<tr>
<td>TinyLLaVA-2.0B</td>
<td>StableLM-2-1.6B</td>
<td><a href="https://huggingface.co/bczhou/TinyLLaVA-2.0B">TinyLLaVA-2.0B</a></td>
<td>66.4</td>
<td>1433.8</td>
<td>63.3</td>
<td>32.6</td>
<td>64.7</td>
<td>78.9</td>
<td>61.9</td>
<td>56.4</td>
</tr>
<tr>
<td>TinyLLaVA-1.5B</td>
<td>TinyLlama</td>
<td><a href="https://huggingface.co/bczhou/TinyLLaVA-1.5B">TinyLLaVA-1.5B</a></td>
<td>60.8</td>
<td>1276.5</td>
<td>55.2</td>
<td>25.8</td>
<td>60.3</td>
<td>76.9</td>
<td>60.3</td>
<td>51.7</td>
</tr>
</tbody></table>
<h2>Demo</h2>
<h3>Gradio Web Demo</h3>
<p>Launch a local web demo by running:</p>
<pre><code class="language-shell">python tinyllava/serve/app.py --model-path bczhou/TinyLLaVA-3.1B --model-name TinyLLaVA-3.1B
</code></pre>
<h3>CLI Inference</h3>
<p>We also support running inference with CLI. To use our model, run:</p>
<pre><code class="language-shell">python -m tinyllava.serve.cli \
    --model-path bczhou/TinyLLaVA-3.1B \
    --image-file &quot;./tinyllava/serve/examples/extreme_ironing.jpg&quot; 
</code></pre>
<h2>&#x1F527; Quick Start</h2>
<details>
<summary>Load model</summary>
    
<pre><code class="language-Python">from tinyllava.model.builder import load_pretrained_model
from tinyllava.mm_utils import get_model_name_from_path
from tinyllava.eval.run_tiny_llava import eval_model

model_path = &quot;bczhou/TinyLLaVA-3.1B&quot;

tokenizer, model, image_processor, context_len = load_pretrained_model(
    model_path=model_path,
    model_base=None,
    model_name=get_model_name_from_path(model_path)
)
</code></pre>
</details>

<h2>&#x1F527; Run Inference</h2>
<p>Here&#39;s an example of running inference with <a href="https://huggingface.co/bczhou/TinyLLaVA-3.1B">TinyLLaVA-3.1B</a></p>
<details>
<summary>Run Inference</summary>
    
<pre><code class="language-Python">from tinyllava.model.builder import load_pretrained_model
from tinyllava.mm_utils import get_model_name_from_path
from tinyllava.eval.run_tiny_llava import eval_model

model_path = &quot;bczhou/TinyLLaVA-3.1B&quot;
prompt = &quot;What are the things I should be cautious about when I visit here?&quot;
image_file = &quot;https://llava-vl.github.io/static/images/view.jpg&quot;

args = type(&#39;Args&#39;, (), {
    &quot;model_path&quot;: model_path,
    &quot;model_base&quot;: None,
    &quot;model_name&quot;: get_model_name_from_path(model_path),
    &quot;query&quot;: prompt,
    &quot;conv_mode&quot;: &quot;phi&quot;,
    &quot;image_file&quot;: image_file,
    &quot;sep&quot;: &quot;,&quot;,
    &quot;temperature&quot;: 0,
    &quot;top_p&quot;: None,
    &quot;num_beams&quot;: 1,
    &quot;max_new_tokens&quot;: 512
})()

eval_model(args)
</code></pre>
</details>

<h3>Important</h3>
<p>We use different <code>conv_mode</code> for different models. Replace the <code>conv_mode</code> in <code>args</code> according to this table:
| model          	| conv_mode 	|
|----------------	|-----------	|
| TinyLLaVA-3.1B 	| phi       	|
| TinyLLaVA-2.0B 	| phi       	|
| TinyLLaVA-1.5B 	| v1        	|</p>
<h2>Evaluation</h2>
<p>To ensure the reproducibility, we evaluate the models with greedy decoding.</p>
<p>See <a href="https://github.com/DLCV-BUAA/TinyLLaVABench/blob/main/docs/Evaluation.md">Evaluation.md</a></p>
<h2>Data Preparation</h2>
<p>In our paper, we used two different datasets: the <a href="https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#pretrain-feature-alignment">LLaVA dataset</a> and the <a href="https://github.com/InternLM/InternLM-XComposer/blob/main/projects/ShareGPT4V/docs/Data.md">ShareGPT4V dataset</a>, and compared their differences. In this section, we provide information on data preparation.</p>
<h3>Pretraining Images</h3>
<ul>
<li>LLaVA: The pretraining images of LLaVA is from the 558K subset of the LAION-CC-SBU dataset.</li>
<li>ShareGPT4V: The pretraining images of ShareGPT4V is a mixture of 558K LAION-CC-SBU subset, SAM dataset, and COCO dataset.</li>
</ul>
<h3>Pretraining Annotations</h3>
<ul>
<li>LLaVA: The pretraining annotations of LLaVA are <a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain">here</a>.</li>
<li>ShareGPT4V: The pretraining annotations of ShareGPT4V are <a href="https://huggingface.co/datasets/Lin-Chen/ShareGPT4V/blob/main/share-captioner_coco_lcs_sam_1246k_1107.json">here</a>.</li>
</ul>
<h3>SFT Images &amp; Annotations</h3>
<p>The majority of the two SFT datasets are the same, with the exception that the 23K detailed description data in LLaVA-1.5-SFT being replaced with detailed captions randomly sampled from the <a href="https://huggingface.co/datasets/Lin-Chen/ShareGPT4V/blob/main/sharegpt4v_instruct_gpt4-vision_cap100k.json">100K ShareGPT4V data</a>.</p>
<h3>Download data</h3>
<ol>
<li>Download relevant images</li>
</ol>
<ul>
<li>LAION-CC-SBU-558K: <a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain/blob/main/images.zip">images.zip</a></li>
<li>COCO: This dataset is from the <a href="https://cocodataset.org/">COCO2017 challenge</a>. Download: <a href="http://images.cocodataset.org/zips/train2017.zip">train2017</a></li>
<li>WebData: This dataset is curated by the <a href="https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V">ShareGPT4V project</a>. Download: <a href="https://drive.google.com/drive/folders/1tCUQ-sq6vdshZVkF0ZeF3K4eztkXJgax?usp=sharing">images</a>. Only for academic usage.</li>
<li>SAM: This dataset is collected by <a href="https://ai.meta.com/datasets/segment-anything-downloads/">Meta</a>. Download: <a href="https://ai.meta.com/datasets/segment-anything-downloads/">images</a>. We only use 000000~000050.tar for now. If you just want to use ShareGPT4V for SFT, you can quickly download 9K images from <a href="https://drive.google.com/file/d/1dKumdOKSXtV7lIXdrG7jsIK_z2vZv2gs/view?usp=drive_link">here</a>.</li>
<li>GQA: <a href="https://cs.stanford.edu/people/dorarad/gqa/about.html">GQA project page</a>. Download: <a href="https://downloads.cs.stanford.edu/nlp/data/gqa/images.zip">images</a></li>
<li>OCR-VQA: <a href="https://ocr-vqa.github.io/">OCR-VQA project page</a>. Download: <a href="https://drive.google.com/drive/folders/1_GYPY5UkUy7HIcR0zq3ZCFgeZN7BAfm_?usp=sharing">download script</a>. We save all files as <code>.jpg</code></li>
<li>TextVQA: <a href="https://textvqa.org/">TextVQA project page</a>. Download: <a href="https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip">trainvalimages</a></li>
<li>VisualGenome: <a href="https://homes.cs.washington.edu/~ranjay/visualgenome/index.html">VisualGenome project page</a>. Download: <a href="https://cs.stanford.edu/people/rak248/VG_100K_2/images.zip">part1</a>, <a href="https://cs.stanford.edu/people/rak248/VG_100K_2/images2.zip">part2</a></li>
</ul>
<ol start="2">
<li>Download relevant annotations</li>
</ol>
<ul>
<li>LLaVA&#39;s pretraining annotations: <a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain">blip_laion_cc_sbu_558k.json</a></li>
<li>LLaVA&#39;s SFT annotations: <a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/blob/main/llava_v1_5_mix665k.json">llava_v1_5_mix665k.json</a></li>
<li>ShareGPT4V&#39;s pretraining annotations: <a href="https://huggingface.co/datasets/Lin-Chen/ShareGPT4V/blob/main/share-captioner_coco_lcs_sam_1246k_1107.json">share-captioner_coco_lcs_sam_1246k_1107.json</a></li>
<li>ShareGPT4V&#39;s SFT annotations: <a href="https://huggingface.co/datasets/Lin-Chen/ShareGPT4V/blob/main/sharegpt4v_mix665k_cap23k_coco-ap9k_lcs3k_sam9k_div2k.json">sharegpt4v_mix665k_cap23k_coco-ap9k_lcs3k_sam9k_div2k.json</a></li>
</ul>
<h3>Organize Data</h3>
<p>Organize the image files and annotation files as follows in <code>path/to/your/data</code>:</p>
<pre><code class="language-none">data
‚îú‚îÄ‚îÄ llava
‚îÇ   ‚îú‚îÄ‚îÄ llava_pretrain
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ images
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ blip_laion_cc_sbu_558k.json
‚îú‚îÄ‚îÄ coco
‚îÇ   ‚îú‚îÄ‚îÄ train2017
‚îú‚îÄ‚îÄ sam
‚îÇ   ‚îú‚îÄ‚îÄ images
‚îú‚îÄ‚îÄ gqa
‚îÇ   ‚îú‚îÄ‚îÄ images
‚îú‚îÄ‚îÄ ocr_vqa
‚îÇ   ‚îú‚îÄ‚îÄ images
‚îú‚îÄ‚îÄ textvqa
‚îÇ   ‚îú‚îÄ‚îÄ train_images
‚îú‚îÄ‚îÄ vg
‚îÇ   ‚îú‚îÄ‚îÄ VG_100K
‚îÇ   ‚îú‚îÄ‚îÄ VG_100K_2
‚îú‚îÄ‚îÄ share_textvqa
‚îÇ   ‚îú‚îÄ‚îÄ images
‚îú‚îÄ‚îÄ web-celebrity
‚îÇ   ‚îú‚îÄ‚îÄ images
‚îú‚îÄ‚îÄ web-landmark
‚îÇ   ‚îú‚îÄ‚îÄ images
‚îú‚îÄ‚îÄ wikiart
‚îÇ   ‚îú‚îÄ‚îÄ images
‚îú‚îÄ‚îÄ text_files
‚îÇ   ‚îú‚îÄ‚îÄ llava_v1_5_mix665k.json
‚îÇ   ‚îú‚îÄ‚îÄ share-captioner_coco_lcs_sam_1246k_1107.json
‚îÇ   ‚îú‚îÄ‚îÄ sharegpt4v_mix665k_cap23k_coco-ap9k_lcs3k_sam9k_div2k.json
</code></pre>
<h2>Train</h2>
<p><strong>This section we describe the base recipe.</strong></p>
<h3>Hyperparameters</h3>
<p>Both hyperparameters used in pretraining and finetuning are provided below.</p>
<ol>
<li>Pretraining</li>
</ol>
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th align="right">Global Batch Size</th>
<th align="right">Learning rate</th>
<th align="right">Epochs</th>
<th align="right">Max length</th>
<th align="right">Weight decay</th>
</tr>
</thead>
<tbody><tr>
<td>TinyLLaVA-3.1B</td>
<td align="right">256</td>
<td align="right">1e-3</td>
<td align="right">1</td>
<td align="right">3072</td>
<td align="right">0</td>
</tr>
</tbody></table>
<ol start="2">
<li>Finetuning</li>
</ol>
<table>
<thead>
<tr>
<th>Hyperparameter</th>
<th align="right">Global Batch Size</th>
<th align="right">Learning rate</th>
<th align="right">Epochs</th>
<th align="right">Max length</th>
<th align="right">Weight decay</th>
</tr>
</thead>
<tbody><tr>
<td>TinyLLaVA-3.1B</td>
<td align="right">128</td>
<td align="right">2e-5</td>
<td align="right">1</td>
<td align="right">3072</td>
<td align="right">0</td>
</tr>
</tbody></table>
<h3>Pretrain</h3>
<p><strong>Replace paths to your paths</strong></p>
<p>Training script with DeepSpeed ZeRO-2: <a href="https://github.com/DLCV-BUAA/TinyLLaVABench/blob/main/scripts/tiny_llava/pretrain.sh"><code>pretrain.sh</code></a>.</p>
<h3>Finetune</h3>
<p><strong>Replace paths to your paths</strong></p>
<p>Training script with DeepSpeed ZeRO-3: <a href="https://github.com/DLCV-BUAA/TinyLLaVABench/blob/main/scripts/tiny_llava/finetune.sh"><code>finetune.sh</code></a>.</p>
<h2>Custom-Finetune</h2>
<p>Check out our custom finetune using LoRA <a href="https://github.com/DLCV-BUAA/TinyLLaVABench/blob/dev/docs/CUTOM_FINETUNE.md">here</a>.</p>
<h2>&#x270F; Citation</h2>
<p>If you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil:.</p>
<pre><code class="language-BibTeX">@misc{zhou2024tinyllava,
      title={TinyLLaVA: A Framework of Small-scale Large Multimodal Models}, 
      author={Baichuan Zhou and Ying Hu and Xi Weng and Junlong Jia and Jie Luo and Xien Liu and Ji Wu and Lei Huang},
      year={2024},
      eprint={2402.14289},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
</code></pre>
<h2>‚ù§Ô∏è Community efforts</h2>
<ul>
<li>Our codebase is built upon the <a href="https://github.com/haotian-liu/LLaVA">LLaVA</a> project. Great work!</li>
<li>Our project uses data from the <a href="https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V">ShareGPT4V</a> project. Great work!</li>
</ul>
 </div> </article> <!-- Related Models Section --> <section class="mt-16 pt-8 border-t border-gray-200 dark:border-gray-700"> <h2 class="text-3xl font-bold text-center mb-8">Related Models</h2> <div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-6"> <a href="/model/bczhou--TinyLLaVA-1.5B" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full">  <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="TinyLLaVA-1.5B"> TinyLLaVA-1.5B </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by </p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> A model for image-text-to-text....
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="95 likes">‚ù§Ô∏è <span>95</span></div> <div class="flex items-center gap-1" title="2,085 downloads">üì• <span>2.1K</span></div> </div> </div> </a><a href="/model/Cylingo--Xinyuan-LLM-14B-0428" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full">  <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="Xinyuan-LLM-14B-0428"> Xinyuan-LLM-14B-0428 </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by </p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> A model for text-generation....
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="65 likes">‚ù§Ô∏è <span>65</span></div> <div class="flex items-center gap-1" title="25 downloads">üì• <span>25</span></div> </div> </div> </a><a href="/model/tomg-group-umd--huginn-0125" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full">  <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="huginn-0125"> huginn-0125 </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by </p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> A model for text-generation....
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="1,440 likes">‚ù§Ô∏è <span>1.4K</span></div> <div class="flex items-center gap-1" title="7,085 downloads">üì• <span>7.1K</span></div> </div> </div> </a> </div> </section> </div> </div>  </main> <footer class="bg-gray-800 text-gray-300 py-8 mt-16"> <div class="container mx-auto px-4 text-center"> <div class="flex justify-center gap-4 mb-4"> <a href="/about" class="hover:underline">About</a> <a href="/compliance" class="hover:underline">Compliance</a> </div> <p class="text-sm mt-2">
&copy; 2025 Free AI Tools. An open-source project to index the world of AI.
</p> <a href="mailto:compliance@free2aitools.com" class="text-sm text-gray-400 hover:underline mt-1 block">compliance@free2aitools.com</a> </div> </footer> </body></html>