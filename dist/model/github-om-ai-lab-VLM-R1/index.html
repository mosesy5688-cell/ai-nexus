<!DOCTYPE html><html lang="en"> <head><!-- Google Tag Manager --><script type="module">(function(e,n,r,t,m){e[t]=e[t]||[],e[t].push({"gtm.start":new Date().getTime(),event:"gtm.js"});var g=n.getElementsByTagName(r)[0],a=n.createElement(r),s="";a.async=!0,a.src="https://www.googletagmanager.com/gtm.js?id="+m+s,g.parentNode.insertBefore(a,g)})(window,document,"script","dataLayer","GTM-58C2CQ8G");</script><!-- End Google Tag Manager --><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><!-- Open Graph Meta Tags --><meta property="og:type" content="website"><meta property="og:url" content="https://free2aitools.com/model/github-om-ai-lab-VLM-R1/"><meta property="og:title" content="VLM-R1 - AI Model Details"><meta property="og:description" content="Solve Visual Understanding with Reinforced VLMs"><meta property="og:image" content="https://free2aitools.com/og-image.jpg"><!-- Twitter Card Meta Tags --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://free2aitools.com/model/github-om-ai-lab-VLM-R1/"><meta property="twitter:title" content="VLM-R1 - AI Model Details"><meta property="twitter:description" content="Solve Visual Understanding with Reinforced VLMs"><meta property="twitter:image" content="https://free2aitools.com/og-image.jpg"><meta name="description" content="Solve Visual Understanding with Reinforced VLMs"><title>VLM-R1 - AI Model Details</title><link rel="canonical" href="https://free2aitools.com/model/github-om-ai-lab-VLM-R1/"><!-- Google AdSense --><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2292826803755214" crossorigin="anonymous"></script><link rel="stylesheet" href="/_astro/about.CAZw8hUu.css"></head> <body class="bg-gray-50 text-gray-800 font-sans antialiased flex flex-col min-h-screen"> <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-58C2CQ8G" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) --> <header class="sticky top-0 z-30 w-full backdrop-blur flex-none transition-colors duration-500 lg:border-b lg:border-gray-200 bg-white/80"> <nav class="container mx-auto px-4 sm:px-6 lg:px-8"> <div class="flex items-center justify-between h-16"> <!-- Site Logo --> <a href="/" class="flex items-center gap-2 text-xl sm:text-2xl font-bold text-gray-900"> <span>Free AI Tools</span> </a> <!-- Right-side items --> <div class="flex items-center gap-4"> <!-- Desktop Navigation --> <div class="hidden md:flex items-center gap-6"> <a href="/" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Explore Models </a><a href="/ranking" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Rankings </a><a href="/reports" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Reports </a><a href="/about" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> About </a> </div> <!-- Mobile Search & Menu Toggle --> <button data-collapse-toggle="navbar-default" type="button" class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200" aria-controls="navbar-default" aria-expanded="false"> <span class="sr-only">Open main menu</span> <svg class="w-5 h-5" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 17 14"><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 1h15M1 7h15M1 13h15"></path></svg> </button> </div> </div> <!-- Mobile Collapsible Menu --> <div class="hidden w-full md:hidden" id="navbar-default"> <ul class="font-medium flex flex-col p-4 md:p-0 mt-4 border border-gray-100 rounded-lg bg-gray-50"> <li> <a href="/" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Explore Models</a> </li><li> <a href="/ranking" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Rankings</a> </li><li> <a href="/reports" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Reports</a> </li><li> <a href="/about" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">About</a> </li> </ul> </div> </nav> </header> <main class="flex-grow">  <div class="container mx-auto px-4 py-12"> <div class="max-w-4xl mx-auto"> <!-- Header Section --> <header class="mb-8"> <div class="flex flex-col md:flex-row md:items-start md:justify-between gap-4"> <div> <h1 class="text-4xl md:text-5xl font-extrabold text-gray-900 dark:text-white mb-2">VLM-R1</h1> <p class="text-lg text-gray-500 dark:text-gray-400">by om-ai-lab</p> </div>  </div> </header> <!-- Metadata Section --> <div class="mb-8 p-4 bg-gray-100 dark:bg-gray-800 rounded-lg"> <div class="grid grid-cols-2 md:grid-cols-4 gap-4 text-center"> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Likes</p> <p class="text-xl font-bold">11,406</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Downloads</p> <p class="text-xl font-bold">11,406</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Task</p> <p class="text-xl font-bold capitalize">tool</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Last Updated</p> <p class="text-xl font-bold">2025/11/20</p> </div> </div> </div> <h3 class="text-lg font-semibold mb-2">Sources</h3> <div class="flex flex-col gap-2"> <a href="https://github.com/om-ai-lab/VLM-R1" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> üì¶ GitHub </a> </div> </div> </div>  <div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> üì¶ GitHub Details
</h3> <div class="grid grid-cols-2 md:grid-cols-3 gap-4 text-sm"> <div><strong>Language:</strong> Python</div> <div><strong>Stars:</strong> 11,406</div> <div><strong>Forks:</strong> 370</div> <div><strong>Open Issues:</strong> 161</div> <div><strong>License:</strong> Apache License 2.0</div>  </div>  </div> <article class="model-readme-content py-8 border-t border-gray-200 dark:border-gray-700"> <div class="prose prose-lg dark:prose-invert max-w-none"> <h1>VLM-R1: A stable and generalizable R1-style Large Vision-Language Model</h1>
<p><font size=4><div align='center' > [<a href="https://huggingface.co/spaces/omlab/VLM-R1-Referral-Expression">ü§ó REC Demo</a>] [<a href="https://huggingface.co/spaces/omlab/VLM-R1-OVD">ü§ó OVD Demo</a>] [<a href="https://huggingface.co/datasets/omlab/VLM-R1">ü§ó REC Data</a>] [<a href="https://huggingface.co/collections/omlab/vlm-r1-models-67b7352db15c19d57157c348">ü§ó Checkpoints</a>] </div></font></p>
<p><font size=4><div align='center'>[<a href="https://arxiv.org/abs/2504.07615">üìÑ Tech Report</a>] [<a href="https://om-ai-lab.github.io/index.html">üìù Blog</a>]</div></font></p>
<div align="center">
<img src="./assets/performance4.png" width="900"/>
<div>
  <font size=4>
    <p>üéâ  <b>Our VLM-R1 Math model reaches the top of the Open-Compass Math Leaderboard (under 4B parameters) and OVD model achieves the state-of-the-art performance on OVDEval.</b></p>
  </font>
</div>
</div>

<p>Since the introduction of <a href="https://github.com/deepseek-ai/DeepSeek-R1">Deepseek-R1</a>, numerous works have emerged focusing on reproducing and improving upon it. In this project, we propose VLM-R1, a stable and generalizable R1-style Large Vision-Language Model.</p>
<p>Specifically, for the task of Referring Expression Comprehension (REC), we trained <a href="https://github.com/QwenLM/Qwen2.5-VL">Qwen2.5-VL</a> using both R1 and SFT approaches. The results reveal that, on the in-domain test data, the performance of the SFT model shows little change compared to that of the R1 model base model when the number of training steps is relatively small (100‚Äì600 steps), while the R1 model shows a steady improvement (as shown at the left of the figure below). More importantly, on the out-of-domain test data, the SFT model&#39;s performance deteriorates slightly as the number of steps increases. Nevertheless, the RL model generalizes its reasoning ability to the out-of-domain data (as shown at the right of the figure below).</p>
<p><img src="./assets/performance3.png" alt="image">
* <em>We found previous REC SFT exps used a mismatch pixel config. Therefore, we re-run the study with the correct config on a more complex out-of-domain data. See our <a href="https://om-ai-lab.github.io/2025_03_24.html">findings</a> for details.</em></p>
<h2>üöÄ Features</h2>
<p>This repository supports:</p>
<ul>
<li><strong><code>Full Fine-tuning for GRPO</code></strong>: see <a href="run_scripts/run_grpo_rec.sh">run_grpo_rec.sh</a></li>
<li><strong><code>Freeze Vision Modules</code></strong>: set <code>freeze_vision_modules</code> as <code>true</code> in the script.</li>
<li><strong><code>LoRA Fine-tuning for GRPO</code></strong>: see <a href="run_scripts/run_grpo_rec_lora.sh">run_grpo_rec_lora.sh</a></li>
<li><strong><code>Multi-node Training</code></strong>: see <a href="run_scripts/multinode_training_demo.sh">multinode_training_demo.sh</a></li>
<li><strong><code>Multi-image Input Training</code></strong>: see <a href="run_scripts/run_grpo_gui.sh">run_grpo_gui.sh</a></li>
<li><strong><code>For your own data</code></strong>: see <a href="#for-your-own-data">here</a></li>
<li><strong><code>Various VLMs</code></strong>: see <a href="assets/add_new_model.md">How to add a new model</a>, now we support QwenVL and InternVL</li>
</ul>
<h2>üóûÔ∏è Update</h2>
<ul>
<li><p><strong><code>2025-08-29</code></strong>: üî•üî•üî• We have further optimized the VLM-R1 series models based on JD&#39;s latest open-source inference framework <code>xllm</code> (github is <a href="https://github.com/jd-opensource/xllm">here</a>). The TTFT (Time to First Token) has been reduced by 50% compared to <code>vllm-ascend</code>, and the overall throughput has increased by 127% compared to <code>vllm-ascend</code>. Please refer to <a href="ascend_inference/910B/xllm/README.md">ascend_inference/910B/xllm/README.md</a> for more details.</p>
</li>
<li><p><strong><code>2025-08-22</code></strong>: We have adapted the VLM-R1 series models to Huawei Ascend Atlas 800T A2 and Atlas 300I Duo series using the vllm-ascend framework, further expanding the deployment scenarios and hardware compatibility of the model series. Please refer to <a href="ascend_inference/910B/vllm_ascend/README.md">ascend_inference/910B/vllm_ascend/README.md</a> and <a href="ascend_inference/300IDuo/README.md">ascend_inference/300IDuo/README.md</a> for more details.</p>
</li>
<li><p><strong><code>2025-06-26</code></strong>: We introduce a post-resize operation for the bounding box for QwenVL (both <a href="src/open-r1-multimodal/src/open_r1/vlm_modules/qwen_module.py#L124-L129">training</a> and <a href="src/eval/test_rec_r1.py#L92-L97">evaluation</a>) and the results are improved slightly.</p>
</li>
<li><p><strong><code>2025-04-16</code></strong>: We have updated the codebase to improve functionality and maintain unified implementation. Specifically, the REC process is now integrated into <a href="src/open-r1-multimodal/src/open_r1/grpo_jsonl.py">grpo_jsonl.py</a> for consistency across tasks. Additionally, we introduce a new parameter, <code>is_reward_customized_from_vlm_module</code>, which enables the use of customized reward functions defined within the VLM module. When set to <code>true</code>, the reward logic is handled in either <a href="src/open-r1-multimodal/src/open_r1/vlm_modules/qwen_module.py">QwenVL2Module</a> or <a href="src/open-r1-multimodal/src/open_r1/vlm_modules/internvl_module.py">InternVLModule</a>, depending on the selected model. Furthermore, the training log has been enhanced to provide more detailed output for easier monitoring and debugging.</p>
</li>
<li><p><strong><code>2025-04-11</code></strong>: üî•üî•üî• We release the <a href="https://arxiv.org/abs/2504.07615">technical report</a> of VLM-R1, summarizing our main results and insights.</p>
</li>
<li><p><strong><code>2025-04-03</code></strong>: We add the <code>odLength</code>, <code>weighted_sum</code>, and <code>cosine</code> reward used in OVD task, please refer our <a href="https://om-ai-lab.github.io/2025_03_20.html">blog post</a> and <a href="https://om-ai-lab.github.io/2025_03_24.html">findings</a> to the details of the reward usage and see <a href="src/open-r1-multimodal/src/open_r1/grpo_jsonl.py">grpo_jsonl.py</a> for code implementation.</p>
</li>
<li><p><strong><code>2025-03-24</code></strong>: üî• We release the <a href="https://om-ai-lab.github.io/2025_03_24.html">findings</a> of VLM-R1-OVD.</p>
</li>
<li><p><strong><code>2025-03-23</code></strong>: üî• We release the VLM-R1-OVD <a href="https://huggingface.co/omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321">model weights</a> and <a href="https://huggingface.co/spaces/omlab/VLM-R1-OVD">demo</a>, which shows the state-of-the-art performance on OVDEval. Welcome to use it.</p>
</li>
<li><p><strong><code>2025-03-20</code></strong>: üî• We achieved SOTA results on <a href="https://github.com/om-ai-lab/OVDEval">OVDEval</a> with our RL-based model, outperforming SFT baselines and specialized object detection models. Read our <a href="https://om-ai-lab.github.io/2025_03_20.html">blog post</a> for details on how reinforcement learning enhances object detection performance.</p>
</li>
<li><p><strong><code>2025-03-17</code></strong>: Our VLM-R1 Math model reaches the top of the <a href="https://rank.opencompass.org.cn/leaderboard-multimodal-reasoning/?m=REALTIME">Open-Compass Math Leaderboard</a> (under 4B parameters). We have released the <a href="https://huggingface.co/omlab/VLM-R1-Qwen2.5VL-3B-Math-0305">checkpoint</a>.</p>
</li>
<li><p><strong><code>2025-03-15</code></strong>: We support multi-image input data. Check the format of multi-image input <a href="#for-your-own-data">here</a>. We also provide an example of multi-image script <a href="run_scripts/run_grpo_gui.sh">run_grpo_gui.sh</a>, see <a href="#for-your-own-data">here</a> for details.</p>
</li>
<li><p><strong><code>2025-03-13</code></strong>: We support InternVL for GRPO. See <a href="run_scripts/run_grpo_rec_internvl.sh">run_grpo_rec_internvl.sh</a> for details. The annotation json files used in InternVL are <a href="https://huggingface.co/datasets/omlab/VLM-R1/resolve/main/rec_jsons_internvl.zip">here</a>. If you want to add your new model, please refer to <a href="assets/add_new_model.md">How to add a new model</a>.</p>
</li>
<li><p><strong><code>2025-03-02</code></strong>: We support LoRA Fine-tuning for GRPO. See <a href="run_scripts/run_grpo_rec_lora.sh">run_grpo_rec_lora.sh</a> for details.</p>
</li>
<li><p><strong><code>2025-02-27</code></strong>: We support the <code>number of iterations per batch</code> and <code>epsilon value for clipping</code> in the original GRPO algorithm with args: <code>--num_iterations</code> and <code>--epsilon</code>.</p>
</li>
<li><p><strong><code>2025-02-25</code></strong>: We support multi-node training for GRPO. See <a href="run_scripts/multinode_training_demo.sh">multinode_training_demo.sh</a> for details.</p>
</li>
<li><p><strong><code>2025-02-21</code></strong>: We release the <a href="https://huggingface.co/omlab/Qwen2.5VL-3B-VLM-R1-REC-500steps">checkpoint</a> of the VLM-R1 REC model.</p>
</li>
<li><p><strong><code>2025-02-20</code></strong>: We release the script for <a href="#for-your-own-data">general data loading</a>.</p>
</li>
<li><p><strong><code>2025-02-19</code></strong>: We incorporate an explanation of the <a href="#sft">SFT</a> method.</p>
</li>
<li><p><strong><code>2025-02-17</code></strong>: We release the VLM-R1 REC <a href="https://huggingface.co/spaces/omlab/VLM-R1-Referral-Expression">Demo</a> on Hugging Face Spaces.</p>
</li>
<li><p><strong><code>2025-02-15</code></strong>: We release the VLM-R1 repository and <a href="#grpo">GRPO</a> training script.</p>
</li>
</ul>
<h2>ü§ñ Models</h2>
<ul>
<li><strong><a href="https://huggingface.co/omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321"><code>OVD</code></a></strong>: Trained with VLM-R1, our Open-Vocabulary Detection (OVD) model achieves the state-of-the-art performance on OVDEval.</li>
<li><strong><a href="https://huggingface.co/omlab/VLM-R1-Qwen2.5VL-3B-Math-0305"><code>Math</code></a></strong>: Through VLM-R1 training, our math model focuses on multimodal reasoning tasks and has achieved Top1 on the OpenCompass Multi-modal Reasoning Leaderboard among models &lt; 4B.</li>
<li><strong><a href="https://huggingface.co/omlab/Qwen2.5VL-3B-VLM-R1-REC-500steps"><code>REC</code></a></strong>: Trained with VLM-R1, our Referring Expression Comprehension (REC) model showcases the superior performance on out-of-domain data and a series of reasoning-grounding tasks.</li>
<li><strong><a href="https://huggingface.co/konkazzz/GT-r1"><code>GUI</code></a></strong>: Trained with VLM-R1, our GUI Defect Detection model outperforms both base and SFT models by achieving the best accuracy and improved generalization across both defective and clean screens.</li>
</ul>
<table>
<thead>
<tr>
<th>Version</th>
<th>Base VLM</th>
<th>Checkpoint</th>
<th>Task Type</th>
</tr>
</thead>
<tbody><tr>
<td>VLM-R1-Qwen2.5VL-3B-OVD-0321</td>
<td>Qwen2.5VL-3B</td>
<td><a href="https://huggingface.co/omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321">omlab/VLM-R1-Qwen2.5VL-3B-OVD-0321</a></td>
<td>Open-Vocabulary Detection</td>
</tr>
<tr>
<td>VLM-R1-Qwen2.5VL-3B-Math-0305</td>
<td>Qwen2.5VL-3B</td>
<td><a href="https://huggingface.co/omlab/VLM-R1-Qwen2.5VL-3B-Math-0305">omlab/VLM-R1-Qwen2.5VL-3B-Math-0305</a></td>
<td>Multi-Modal Math</td>
</tr>
<tr>
<td>VLM-R1-Qwen2.5VL-3B-REC-500steps</td>
<td>Qwen2.5VL-3B</td>
<td><a href="https://huggingface.co/omlab/Qwen2.5VL-3B-VLM-R1-REC-500steps">omlab/Qwen2.5VL-3B-VLM-R1-REC-500steps</a></td>
<td>REC/Reasoning-Grounding</td>
</tr>
</tbody></table>
<h2>üéØ ToDo</h2>
<ul>
<li><input checked="" disabled="" type="checkbox"> Implement multi-node training.</li>
<li><input checked="" disabled="" type="checkbox"> Implement LoRA Fine-tuning.</li>
<li><input checked="" disabled="" type="checkbox"> Support more Multimodal LLMs.</li>
<li><input checked="" disabled="" type="checkbox"> Support multi-image input.</li>
<li><input checked="" disabled="" type="checkbox"> Release the VLM-R1 Math model.</li>
<li><input checked="" disabled="" type="checkbox"> Release the blog of VLM-R1.</li>
<li><input checked="" disabled="" type="checkbox"> Release the VLM-R1-OVD model.</li>
<li><input checked="" disabled="" type="checkbox"> Release the technical report of VLM-R1.</li>
<li><input checked="" disabled="" type="checkbox"> Adapt to Huawei Ascend Atlas 800T A2 and Atlas 300I Duo series using the vllm-ascend framework.</li>
<li><input checked="" disabled="" type="checkbox"> Adapt to Huawei Ascend Atlas 800T A2 series using the xllm framework.</li>
<li><input disabled="" type="checkbox"> Study cross task generalization.</li>
<li><input disabled="" type="checkbox"> Enhance VLM for other tasks [welcome issue].</li>
</ul>
<h2>üõ†Ô∏è Setup</h2>
<pre><code class="language-bash">conda create -n vlm-r1 python=3.10
conda activate vlm-r1
bash setup.sh
</code></pre>
<h2>üí™üèª Training</h2>
<h3>Referring Expression Comprehension (REC)</h3>
<h4>üìö GRPO</h4>
<ol>
<li>Download the <a href="https://huggingface.co/datasets/omlab/VLM-R1/resolve/main/train2014.zip">COCO Train2014 image</a> and unzip it, and we refer to the image dir as <code>&lt;your_image_root&gt;</code>.</li>
<li>Download the <a href="https://huggingface.co/datasets/omlab/VLM-R1/resolve/main/rec_jsons_processed.zip">RefCOCO/+/g and LISA-Grounding Annotation files</a> and unzip it (LISA-Grounding is used for out-of-domain evaluation).</li>
<li>Change the <code>data_paths</code> and <code>image_folders</code> in the <a href="run_scripts/run_grpo_rec.sh">run_scripts/run_grpo_rec.sh</a> file.</li>
</ol>
<pre><code class="language-bash"># These jsonl files are included in the annotation files at step 2.
# Note: please use jsonl files instead of json files.
data_paths=&quot;path/to/refcoco_train.jsonl:path/to/refcocop_train.jsonl:path/to/refcocog_train.jsonl&quot;
image_folders=&quot;path/to/coco:path/to/coco:path/to/coco&quot;
</code></pre>
<ol start="4">
<li><code>bash run_scripts/run_grpo_rec.sh</code></li>
</ol>
<blockquote>
<p>[!NOTE]
If you encounter &#39;CUDA out of memory&#39; error, you can try to reduce the <code>per_device_train_batch_size</code>.</p>
</blockquote>
<div align="center">
<img src="./assets/iou.jpg" width="750"/>
</div>
<!-- ![image](./assets/wandb.jpg) -->

<h4>üìö Multi-Node GRPO</h4>
<p>For multi-node training, please refers to <a href="src/open-r1-multimodal/multinode_training_demo.sh">multinode_training_demo.sh</a>.</p>
<h4>üìö SFT</h4>
<p>We use <a href="https://github.com/hiyouga/LLaMA-Factory">LLaMA-Factory</a> to train the SFT model.</p>
<ol>
<li>Clone the <a href="https://github.com/hiyouga/LLaMA-Factory">LLaMA-Factory</a> repository and install the dependencies.</li>
</ol>
<pre><code class="language-bash">git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e &quot;.[torch,metrics]&quot;
</code></pre>
<ol start="2">
<li>Download the dataset_info.json, mllm_rec_json.json, and qwen2_5_vl_full_sft.yaml we provided <a href="https://huggingface.co/datasets/omlab/VLM-R1/tree/main/sft_related">here</a>. Put the json files in the <code>LLaMA-Factory/data</code> directory and the yaml file in the <code>LLaMA-Factory/examples/train_full</code> directory.</li>
<li>Run the following command to train the SFT model.</li>
</ol>
<pre><code class="language-bash">llamafactory-cli train examples/train_full/qwen2_5_vl_full_sft.yaml
</code></pre>
<h3>For your own data</h3>
<div style="text-align: justify;">

<p>We support data loading the jsonl data of this format in <a href="src/open-r1-multimodal/src/open_r1/grpo_jsonl.py"><code>src/open-r1-multimodal/src/open_r1/grpo_jsonl.py</code></a>. Please note that you may need to use different reward functions for your specialized tasks. Welcome to PR to add your own reward functions or share any other interesting findings!</p>
</div>

<p>The jsonl has the format as follows:</p>
<pre><code class="language-json">{
  &quot;id&quot;: 1,
  &quot;image&quot;: &quot;Clevr_CoGenT_TrainA_R1/data/images/CLEVR_trainA_000001_16885.png&quot;,
  &quot;conversations&quot;: [
    {&quot;from&quot;: &quot;human&quot;, &quot;value&quot;: &quot;&lt;image&gt;What number of purple metallic balls are there?&quot;},
    {&quot;from&quot;: &quot;gpt&quot;, &quot;value&quot;: &quot;0&quot;}
  ]
}
</code></pre>
<p>If you want to use multi-image input, you can use the following format:</p>
<pre><code class="language-json">{
  &quot;id&quot;: 1,
  &quot;image&quot;: [&quot;Clevr_CoGenT_TrainA_R1/data/images/CLEVR_trainA_000001_16885.png&quot;, &quot;Clevr_CoGenT_TrainA_R1/data/images/CLEVR_trainA_000001_16886.png&quot;],
  &quot;conversations&quot;: [
    {&quot;from&quot;: &quot;human&quot;, &quot;value&quot;: &quot;&lt;image&gt;&lt;image&gt;What number of purple metallic balls in total within the two images?&quot;},
    {&quot;from&quot;: &quot;gpt&quot;, &quot;value&quot;: &quot;3&quot;}
  ]
}
</code></pre>
<blockquote>
<p>[!NOTE]
The image path in the jsonl file should be relative to the image folder specified in <code>--image_folders</code>. The absolute path of the input image is constructed as <code>os.path.join(image_folder, data[&#39;image&#39;])</code>. For example:</p>
</blockquote>
<ul>
<li>If your jsonl has <code>&quot;image&quot;: &quot;folder1/image1.jpg&quot;</code></li>
<li>And you specify <code>--image_folders &quot;/path/to/images/&quot;</code></li>
<li>The full image path will be <code>/path/to/images/folder1/image1.jpg</code></li>
</ul>
<p>Multiple data files and image folders can be specified using &quot;:&quot; as a separator:</p>
<pre><code class="language-bash">--data_file_paths /path/to/data1.jsonl:/path/to/data2.jsonl \
--image_folders /path/to/images1/:/path/to/images2/
</code></pre>
<p>The script can be run like this:</p>
<pre><code class="language-bash"># You could refer to the run_grpo_rec.sh for the example
torchrun --nproc_per_node=&quot;8&quot; \
    --nnodes=&quot;1&quot; \
    --node_rank=&quot;0&quot; \
    --master_addr=&quot;127.0.0.1&quot; \
    --master_port=&quot;12345&quot; \
  src/open_r1/grpo_jsonl.py \
    --output_dir output/$RUN_NAME \
    --model_name_or_path Qwen/Qwen2.5-VL-3B-Instruct \
    --deepspeed ${REPO_HOME}/src/open-r1-multimodal/local_scripts/zero3.json \
    --data_file_paths /path/to/your/data.jsonl \ # can be multiple, separated by &quot;:&quot;
    --image_folders /path/to/your/image/folder \ # can be multiple, separated by &quot;:&quot;
    ...
</code></pre>
<div style="text-align: justify;">

<h3>Multi-image Input</h3>
<p>We provide an example of multi-image script <a href="src/open-r1-multimodal/run_scripts/run_grpo_gui.sh">run_grpo_gui.sh</a>. This task requires the model to analyze two GUI screenshots, taken before and after a user action, to determine if any UI interaction defects are present, which is from <a href="https://huggingface.co/datasets/songjah/GTArena-UI-Defects">GUI-Testing-Arena</a>. Download the <a href="https://huggingface.co/datasets/omlab/VLM-R1/resolve/main/gui_multi-image.zip">image</a> and unzip it into the <code>/path/to/images/</code>. Then modify the <code>image_folders</code> parameter in the script and run it.</p>
<pre><code class="language-bash">bash run_scripts/run_grpo_gui.sh
</code></pre>
</div>

<h2>üìä Evaluation</h2>
<p><img src="./assets/data2.png" alt="image"></p>
<ol>
<li>Download the provided <a href="https://huggingface.co/datasets/omlab/VLM-R1/resolve/main/lisa-test.zip">LISA-Grounding images</a>.</li>
</ol>
<pre><code class="language-bash">cd ./src/eval

# Remember to change the model path, image root, and annotation path in the script
torchrun --nproc_per_node=X test_rec_r1.py # for GRPO. &#39;X&#39; is the number of GPUs you have.
torchrun --nproc_per_node=X test_rec_baseline.py # for SFT.
</code></pre>
<h2>üîç Ascend Inference</h2>
<p>We have adapted the VLM-R1 series models to Huawei Ascend Atlas 800T A2 and Atlas 300I Duo series using the vllm-ascend framework. The specific adaptation and inference are as follows:</p>
<ul>
<li><strong>Atlas 800T A2</strong>: Please refer to <a href="ascend_inference/910B/vllm_ascend/README.md">ascend_inference/910B/vllm_ascend/README.md</a></li>
<li><strong>Atlas 300I Duo</strong>: Please refer to <a href="ascend_inference/300IDuo/README.md">ascend_inference/300IDuo/README.md</a></li>
</ul>
<h2>ü§ù Acknowledgements</h2>
<p>We would like to express our sincere gratitude to <a href="https://github.com/deepseek-ai/DeepSeek-R1">DeepSeek</a>, <a href="https://github.com/huggingface/open-r1">Open-R1</a>, <a href="https://github.com/QwenLM/Qwen2.5-VL">QwenVL</a>, <a href="https://github.com/EvolvingLMMs-Lab/open-r1-multimodal">Open-R1-Multimodal</a>, <a href="https://github.com/Deep-Agent/R1-V">R1-V</a>, <a href="https://github.com/lichengunc/refer">RefCOCO</a>, <a href="https://github.com/mikittt/easy-to-understand-REG/tree/master/pyutils/refer2">RefGTA</a>, <a href="https://github.com/hiyouga/LLaMA-Factory">LLaMA-Factory</a>, <a href="https://github.com/om-ai-lab/OVDEval">OVDEval</a>, <a href="https://huggingface.co/datasets/songjah/GTArena-UI-Defects">GUI-Testing-Arena</a>, and <a href="https://github.com/dvlab-research/LISA">LISA</a> for providing open-source resources that contributed to the development of this project.</p>
<h2>‚≠êÔ∏è Citation</h2>
<p>If you find this project useful, welcome to cite us.</p>
<pre><code class="language-bib">@article{shen2025vlm,
  title={Vlm-r1: A stable and generalizable r1-style large vision-language model},
  author={Shen, Haozhan and Liu, Peng and Li, Jingcheng and Fang, Chunxin and Ma, Yibo and Liao, Jiajia and Shen, Qiaoli and Zhang, Zilun and Zhao, Kangjia and Zhang, Qianqian and Xu, Ruochen and Zhao, Tiancheng },
  journal={arXiv preprint arXiv:2504.07615},
  year={2025}
}
</code></pre>
 </div> </article>  <section class="mt-16 pt-8 border-t border-gray-200 dark:border-gray-700"> <h2 class="text-3xl font-bold text-center mb-8">Related Models</h2> <div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-6"> <a href="/model/github-unslothai-unsloth" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full"> <div class="absolute top-2 right-2 bg-yellow-400 text-yellow-900 text-xs font-bold px-2 py-1 rounded-full animate-pulse" title="Rising Star">
üî•
</div> <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="unsloth"> unsloth </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by unslothai</p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> Fine-tuning &amp; Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, DeepSeek-R1, Qwen3, Gemma 3, TTS 2x faster with ...
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="96,990 likes">‚ù§Ô∏è <span>97.0K</span></div> <div class="flex items-center gap-1" title="96,990 downloads">üì• <span>97.0K</span></div> </div> </div> </a><a href="/model/github-modelscope-ms-swift" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full"> <div class="absolute top-2 right-2 bg-yellow-400 text-yellow-900 text-xs font-bold px-2 py-1 rounded-full animate-pulse" title="Rising Star">
üî•
</div> <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="ms-swift"> ms-swift </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by modelscope</p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 500+ LLMs (Qwen3, Qwen3-MoE, Llama4, GLM4.5, InternLM3, DeepSeek-R1, ...)...
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="22,310 likes">‚ù§Ô∏è <span>22.3K</span></div> <div class="flex items-center gap-1" title="22,310 downloads">üì• <span>22.3K</span></div> </div> </div> </a><a href="/model/github-huggingface-transformers" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full"> <div class="absolute top-2 right-2 bg-yellow-400 text-yellow-900 text-xs font-bold px-2 py-1 rounded-full animate-pulse" title="Rising Star">
üî•
</div> <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="transformers"> transformers </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by huggingface</p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> ü§ó Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and...
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="305,558 likes">‚ù§Ô∏è <span>305.6K</span></div> <div class="flex items-center gap-1" title="305,558 downloads">üì• <span>305.6K</span></div> </div> </div> </a> </div> </section> </main> <footer class="bg-gray-800 text-gray-300 py-8 mt-16"> <div class="container mx-auto px-4 text-center"> <div class="flex justify-center gap-4 mb-4"> <a href="/about" class="hover:underline">About</a> <a href="/compliance" class="hover:underline">Compliance</a> </div> <p class="text-sm mt-2">
&copy; 2025 Free AI Tools. An open-source project to index the world of AI.
</p> <a href="mailto:compliance@free2aitools.com" class="text-sm text-gray-400 hover:underline mt-1 block">compliance@free2aitools.com</a> </div> </footer> </body></html>