<!DOCTYPE html><html lang="en"> <head><!-- Google Tag Manager --><script type="module">(function(e,n,r,t,m){e[t]=e[t]||[],e[t].push({"gtm.start":new Date().getTime(),event:"gtm.js"});var g=n.getElementsByTagName(r)[0],a=n.createElement(r),s="";a.async=!0,a.src="https://www.googletagmanager.com/gtm.js?id="+m+s,g.parentNode.insertBefore(a,g)})(window,document,"script","dataLayer","GTM-58C2CQ8G");</script><!-- End Google Tag Manager --><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><!-- Open Graph Meta Tags --><meta property="og:type" content="website"><meta property="og:url" content="https://free2aitools.com/model/github-timescale-pgai/"><meta property="og:title" content="pgai - AI Model Details"><meta property="og:description" content="A suite of tools to develop RAG, semantic search, and other AI applications more easily with PostgreSQL"><meta property="og:image" content="https://free2aitools.com/og-image.jpg"><!-- Twitter Card Meta Tags --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://free2aitools.com/model/github-timescale-pgai/"><meta property="twitter:title" content="pgai - AI Model Details"><meta property="twitter:description" content="A suite of tools to develop RAG, semantic search, and other AI applications more easily with PostgreSQL"><meta property="twitter:image" content="https://free2aitools.com/og-image.jpg"><meta name="description" content="A suite of tools to develop RAG, semantic search, and other AI applications more easily with PostgreSQL"><title>pgai - AI Model Details</title><link rel="canonical" href="https://free2aitools.com/model/github-timescale-pgai/"><!-- Google AdSense --><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2292826803755214" crossorigin="anonymous"></script><link rel="stylesheet" href="/_astro/about.CAZw8hUu.css"></head> <body class="bg-gray-50 text-gray-800 font-sans antialiased flex flex-col min-h-screen"> <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-58C2CQ8G" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) --> <header class="sticky top-0 z-30 w-full backdrop-blur flex-none transition-colors duration-500 lg:border-b lg:border-gray-200 bg-white/80"> <nav class="container mx-auto px-4 sm:px-6 lg:px-8"> <div class="flex items-center justify-between h-16"> <!-- Site Logo --> <a href="/" class="flex items-center gap-2 text-xl sm:text-2xl font-bold text-gray-900"> <span>Free AI Tools</span> </a> <!-- Right-side items --> <div class="flex items-center gap-4"> <!-- Desktop Navigation --> <div class="hidden md:flex items-center gap-6"> <a href="/" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Explore Models </a><a href="/ranking" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Rankings </a><a href="/reports" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Reports </a><a href="/about" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> About </a> </div> <!-- Mobile Search & Menu Toggle --> <button data-collapse-toggle="navbar-default" type="button" class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200" aria-controls="navbar-default" aria-expanded="false"> <span class="sr-only">Open main menu</span> <svg class="w-5 h-5" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 17 14"><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 1h15M1 7h15M1 13h15"></path></svg> </button> </div> </div> <!-- Mobile Collapsible Menu --> <div class="hidden w-full md:hidden" id="navbar-default"> <ul class="font-medium flex flex-col p-4 md:p-0 mt-4 border border-gray-100 rounded-lg bg-gray-50"> <li> <a href="/" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Explore Models</a> </li><li> <a href="/ranking" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Rankings</a> </li><li> <a href="/reports" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Reports</a> </li><li> <a href="/about" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">About</a> </li> </ul> </div> </nav> </header> <main class="flex-grow">  <div class="container mx-auto px-4 py-12"> <div class="max-w-4xl mx-auto"> <!-- Header Section --> <header class="mb-8"> <div class="flex flex-col md:flex-row md:items-start md:justify-between gap-4"> <div> <h1 class="text-4xl md:text-5xl font-extrabold text-gray-900 dark:text-white mb-2">pgai</h1> <p class="text-lg text-gray-500 dark:text-gray-400">by timescale</p> </div>  </div> </header> <!-- Metadata Section --> <div class="mb-8 p-4 bg-gray-100 dark:bg-gray-800 rounded-lg"> <div class="grid grid-cols-2 md:grid-cols-4 gap-4 text-center"> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Likes</p> <p class="text-xl font-bold">16,542</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Downloads</p> <p class="text-xl font-bold">16,542</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Task</p> <p class="text-xl font-bold capitalize">tool</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Last Updated</p> <p class="text-xl font-bold">2025/11/20</p> </div> </div> </div> <h3 class="text-lg font-semibold mb-2">Sources</h3> <div class="flex flex-col gap-2"> <a href="https://github.com/timescale/pgai" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> üì¶ GitHub </a> </div> </div> </div>  <div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> üì¶ GitHub Details
</h3> <div class="grid grid-cols-2 md:grid-cols-3 gap-4 text-sm"> <div><strong>Language:</strong> PLpgSQL</div> <div><strong>Stars:</strong> 16,542</div> <div><strong>Forks:</strong> 285</div> <div><strong>Open Issues:</strong> 43</div> <div><strong>License:</strong> PostgreSQL License</div>  </div>  </div> <article class="model-readme-content py-8 border-t border-gray-200 dark:border-gray-700"> <div class="prose prose-lg dark:prose-invert max-w-none"> <p align="center">
    <img height="200" src="docs/images/pgai_logo.png#gh-dark-mode-only" alt="pgai"/>
    <img height="200" src="docs/images/pgai_white.png#gh-light-mode-only" alt="pgai"/>
</p>

<div align=center>

<h3>Power your RAG and Agentic applications with PostgreSQL</h3>

<div>
  <a href="https://github.com/timescale/pgai/tree/main/docs"><strong>Docs</strong></a> ¬∑
  <a href="https://discord.gg/KRdHVXAmkp"><strong>Join the pgai Discord!</strong></a> ¬∑
  <a href="https://tsdb.co/gh-pgai-signup"><strong>Try timescale for free!</strong></a> ¬∑
  <a href="https://github.com/timescale/pgai/releases"><strong>Changelog</strong></a>
</div>
</div>
<br/>

<p>A Python library that transforms PostgreSQL into a robust, production-ready retrieval engine for RAG and Agentic applications.</p>
<ul>
<li><p>üîÑ <strong>Automatically create and synchronize vector embeddings</strong> from PostgreSQL data and S3 documents. Embeddings update automatically as data changes.</p>
</li>
<li><p>ü§ñ <strong><a href="/docs/semantic_catalog/README.md">Semantic Catalog</a>: Enable natural language to SQL with AI</strong>. Automatically generate database descriptions and power text-to-SQL for agentic applications. </p>
</li>
<li><p>üîç Powerful vector and semantic search with pgvector and pgvectorscale.</p>
</li>
<li><p>üõ°Ô∏è Production-ready out-of-the-box: Supports batch processing for efficient embedding generation, with built-in handling for model failures, rate limits, and latency spikes.</p>
</li>
<li><p>üêò Works with any PostgreSQL database, including Timescale Cloud, Amazon RDS, Supabase and more.</p>
</li>
</ul>
<p><strong>Basic Architecture</strong>:
The system consists of an application you write, a PostgreSQL database, and stateless vectorizer workers. The application defines a vectorizer configuration to embed data from sources like PostgreSQL or S3. The workers read this configuration, processes the data queue into embeddings and chunked text, and writes the results back. The application then queries this data to power RAG and semantic search.</p>
<p>The key strength of this architecture lies in its resilience: data modifications made by the application are decoupled from the embedding process, ensuring that failures in the embedding service do not affect the core data operations.</p>
<div align=center>
<img height="400" src="docs/images/pgai_architecture.png" alt="Pgai Architecture: application, database, vectorizer worker">


</div>

<h3>install</h3>
<p>First, install the pgai package.</p>
<pre><code class="language-bash">pip install pgai
</code></pre>
<p>Then, install the pgai database components. You can do this from the terminal using the CLI or in your Python application code using the pgai python package.</p>
<pre><code># from the cli
pgai install -d &lt;database-url&gt;

# or from the python package, often done as part of your application setup
import pgai
pgai.install(DB_URL)
</code></pre>
<p>If you are not on Timescale Cloud you will also need to run the pgai vectorizer worker. Install the dependencies for it via:</p>
<pre><code class="language-bash">pip install &quot;pgai[vectorizer-worker]&quot;
</code></pre>
<p>If you are using the <a href="/docs/semantic_catalog/README.md">semantic catalog</a>, you will need to run:</p>
<pre><code class="language-bash">pip install &quot;pgai[semantic-catalog]&quot;
</code></pre>
<h1>Quick Start</h1>
<p>This quickstart demonstrates how pgai Vectorizer enables semantic search and RAG over PostgreSQL data by automatically creating and synchronizing embeddings as data changes.</p>
<p><strong>Looking for text-to-SQL?</strong> Check out the <a href="/docs/semantic_catalog/README.md">Semantic Catalog quickstart</a> to transform natural language questions into SQL queries.</p>
<p>The key &quot;secret sauce&quot; of pgai Vectorizer is its declarative approach to
embedding generation. Simply define your pipeline and let Vectorizer handle the
operational complexity of keeping embeddings in sync, even when embedding
endpoints are unreliable. You can define a simple version of the pipeline as
follows:</p>
<pre><code class="language-sql">CREATE TABLE IF NOT EXISTS wiki (
    id INTEGER PRIMARY KEY GENERATED ALWAYS AS IDENTITY,
    url TEXT NOT NULL,
    title TEXT NOT NULL,
    text TEXT NOT NULL
)

SELECT ai.create_vectorizer(
     &#39;wiki&#39;::regclass,
     loading =&gt; ai.loading_column(column_name=&gt;&#39;text&#39;),
     destination =&gt; ai.destination_table(target_table=&gt;&#39;wiki_embedding_storage&#39;),
     embedding =&gt; ai.embedding_openai(model=&gt;&#39;text-embedding-ada-002&#39;, dimensions=&gt;&#39;1536&#39;)
    )
</code></pre>
<p>The vectorizer will automatically create embeddings for all the rows in the
<code>wiki</code> table, and, more importantly, will keep the embeddings synced with the
underlying data as it changes.  <strong>Think of it almost like declaring an index</strong> on
the <code>wiki</code> table, but instead of the database managing the index datastructure
for you, the Vectorizer is managing the embeddings. </p>
<h2>Running the quick start</h2>
<p><strong>Prerequisites:</strong></p>
<ul>
<li>A PostgreSQL database (<a href="https://docs.timescale.com/self-hosted/latest/install/installation-docker/">docker instructions</a>).</li>
<li>An OpenAI API key (we use openai for embedding in the quick start, but you can use <a href="#supported-embedding-models">multiple providers</a>).</li>
</ul>
<p>Create a <code>.env</code> file with the following:</p>
<pre><code>OPENAI_API_KEY=&lt;your-openai-api-key&gt;
DB_URL=&lt;your-database-url&gt;
</code></pre>
<p>You can download the full <a href="examples/quickstart/main.py">python code</a> and <a href="examples/quickstart/requirements.txt">requirements.txt</a> from the quickstart example and run it in the same directory as the <code>.env</code> file.</p>
<details>
<summary>Click here for a bash script to run the quickstart</summary>

<pre><code class="language-bash">curl -O https://raw.githubusercontent.com/timescale/pgai/main/examples/quickstart/main.py
curl -O https://raw.githubusercontent.com/timescale/pgai/main/examples/quickstart/requirements.txt
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python main.py
</code></pre>
</details>
Sample output:

<details>
<summary>Click to expand sample output</summary>

<pre><code>Search results 1:
[WikiSearchResult(id=7,
                  url=&#39;https://en.wikipedia.org/wiki/Aristotle&#39;,
                  title=&#39;Aristotle&#39;,
                  text=&#39;Aristotle (;  Aristot√©lƒìs, ; 384‚Äì322\xa0BC) was an &#39;
                       &#39;Ancient Greek philosopher and polymath. His writings &#39;
                       &#39;cover a broad range of subjects spanning the natural &#39;
                       &#39;sciences, philosophy, linguistics, economics, &#39;
                       &#39;politics, psychology and the arts. As the founder of &#39;
                       &#39;the Peripatetic school of philosophy in the Lyceum in &#39;
                       &#39;Athens, he began the wider Aristotelian tradition that &#39;
                       &#39;followed, which set the groundwork for the development &#39;
                       &#39;of modern science.\n&#39;
                       &#39;\n&#39;
                       &quot;Little is known about Aristotle&#39;s life. He was born in &quot;
                       &#39;the city of Stagira in northern Greece during the &#39;
                       &#39;Classical period. His father, Nicomachus, died when &#39;
                       &#39;Aristotle was a child, and he was brought up by a &#39;
                       &quot;guardian. At 17 or 18 he joined Plato&#39;s Academy in &quot;
                       &#39;Athens and remained there till the age of 37 (). &#39;
                       &#39;Shortly after Plato died, Aristotle left Athens and, &#39;
                       &#39;at the request of Philip II of Macedon, tutored his &#39;
                       &#39;son Alexander the Great beginning in 343 BC. He &#39;
                       &#39;established a library in the Lyceum which helped him &#39;
                       &#39;to produce many of his hundreds of books on papyru&#39;,
                  chunk=&#39;Aristotle (;  Aristot√©lƒìs, ; 384‚Äì322\xa0BC) was an &#39;
                        &#39;Ancient Greek philosopher and polymath. His writings &#39;
                        &#39;cover a broad range of subjects spanning the natural &#39;
                        &#39;sciences, philosophy, linguistics, economics, &#39;
                        &#39;politics, psychology and the arts. As the founder of &#39;
                        &#39;the Peripatetic school of philosophy in the Lyceum in &#39;
                        &#39;Athens, he began the wider Aristotelian tradition &#39;
                        &#39;that followed, which set the groundwork for the &#39;
                        &#39;development of modern science.&#39;,
                  distance=0.22242502364217387)]
Search results 2:
[WikiSearchResult(id=41,
                  url=&#39;https://en.wikipedia.org/wiki/pgai&#39;,
                  title=&#39;pgai&#39;,
                  text=&#39;pgai is a Python library that turns PostgreSQL into &#39;
                       &#39;the retrieval engine behind robust, production-ready &#39;
                       &#39;RAG and Agentic applications. It does this by &#39;
                       &#39;automatically creating vector embeddings for your data &#39;
                       &#39;based on the vectorizer you define.&#39;,
                  chunk=&#39;pgai is a Python library that turns PostgreSQL into &#39;
                        &#39;the retrieval engine behind robust, production-ready &#39;
                        &#39;RAG and Agentic applications. It does this by &#39;
                        &#39;automatically creating vector embeddings for your &#39;
                        &#39;data based on the vectorizer you define.&#39;,
                  distance=0.13639101792546204)]
RAG response:
The main thing pgai does right now is generating vector embeddings for data in PostgreSQL databases based on the vectorizer defined by the user, enabling the creation of robust RAG and Agentic applications.
</code></pre>
</details>


<h2>Code walkthrough</h2>
<h3>Install the pgai database components</h3>
<p>Pgai requires a few catalog tables and functions to be installed into the database. This is done using the <code>pgai.install</code> function, which will install the necessary components into the <code>ai</code> schema of the database.</p>
<pre><code class="language-python">pgai.install(DB_URL)
</code></pre>
<h3>Create the vectorizer</h3>
<p>This defines the vectorizer, which tells the system how to create the embeddings from the <code>text</code> column in the <code>wiki</code> table. The vectorizer creates a view <code>wiki_embedding</code> that we can query for the embeddings (as we&#39;ll see below).</p>
<pre><code class="language-python">async def create_vectorizer(conn: psycopg.AsyncConnection):
    async with conn.cursor() as cur:    
        await cur.execute(&quot;&quot;&quot;
            SELECT ai.create_vectorizer(
                &#39;wiki&#39;::regclass,
                if_not_exists =&gt; true,
                loading =&gt; ai.loading_column(column_name=&gt;&#39;text&#39;),
                embedding =&gt; ai.embedding_openai(model=&gt;&#39;text-embedding-ada-002&#39;, dimensions=&gt;&#39;1536&#39;),
                destination =&gt; ai.destination_table(view_name=&gt;&#39;wiki_embedding&#39;)
            )
        &quot;&quot;&quot;)   
    await conn.commit()
</code></pre>
<h3>Run the vectorizer worker</h3>
<p>In this example, we run the vectorizer worker once to create the embeddings for the existing data.</p>
<pre><code class="language-python">worker = Worker(DB_URL, once=True)
worker.run()
</code></pre>
<p>In a real application, we would not call the worker manually like this every time we want to create the embeddings. Instead, we would run the worker in the background and it would run continuously, polling for work from the vectorizer. </p>
<p>You can run the worker in the background from the application, the cli, or docker. See the <a href="/docs/vectorizer/worker.md">vectorizer worker</a> documentation for more details.</p>
<h3>Search the wiki articles using semantic search</h3>
<p>This is standard pgvector semantic search in PostgreSQL. The search is performed against the <code>wiki_embedding</code> view, which is created by the vectorizer and includes all the columns from the <code>wiki</code> table plus the <code>embedding</code> column and the chunk text. This function returns both the entire <code>text</code> column from the <code>wiki</code> table and smaller chunks of the text that are most relevant to the query.</p>
<pre><code class="language-python">@dataclass
class WikiSearchResult:
    id: int
    url: str
    title: str
    text: str
    chunk: str
    distance: float

async def _find_relevant_chunks(client: AsyncOpenAI, query: str, limit: int = 1) -&gt; List[WikiSearchResult]:
    # Generate embedding for the query using OpenAI&#39;s API
    response = await client.embeddings.create(
        model=&quot;text-embedding-ada-002&quot;,
        input=query,
        encoding_format=&quot;float&quot;,
    )
    
    embedding = np.array(response.data[0].embedding)
    
    # Query the database for the most similar chunks using pgvector&#39;s cosine distance operator (&lt;=&gt;)
    async with pool.connection() as conn:
        async with conn.cursor(row_factory=class_row(WikiSearchResult)) as cur:
            await cur.execute(&quot;&quot;&quot;
                SELECT w.id, w.url, w.title, w.text, w.chunk, w.embedding &lt;=&gt; %s as distance
                FROM wiki_embedding w
                ORDER BY distance
                LIMIT %s
            &quot;&quot;&quot;, (embedding, limit))
            
            return await cur.fetchall()
</code></pre>
<h3>Insert a new article into the wiki table</h3>
<p>This code is notable for what it is not doing. This is a simple insert of a new article into the <code>wiki</code> table. We did not need to do anything different to create the embeddings, the vectorizer worker will take care of updating the embeddings as the data changes.</p>
<pre><code class="language-python">def insert_article_about_pgai(conn: psycopg.AsyncConnection):
    async with conn.cursor(row_factory=class_row(WikiSearchResult)) as cur:
        await cur.execute(&quot;&quot;&quot;
            INSERT INTO wiki (url, title, text) VALUES
            (&#39;https://en.wikipedia.org/wiki/pgai&#39;, &#39;pgai&#39;, &#39;pgai is a Python library that turns PostgreSQL into the retrieval engine behind robust, production-ready RAG and Agentic applications. It does this by automatically creating vector embeddings for your data based on the vectorizer you define.&#39;)
        &quot;&quot;&quot;)
    await conn.commit() 
</code></pre>
<h3>Perform RAG with the LLM</h3>
<p>This code performs RAG with the LLM. It uses the <code>_find_relevant_chunks</code> function defined above to find the most relevant chunks of text from the <code>wiki</code> table and then uses the LLM to generate a response.</p>
<pre><code class="language-python">    query = &quot;What is the main thing pgai does right now?&quot;
    relevant_chunks = await _find_relevant_chunks(client, query)
    context = &quot;\n\n&quot;.join(
        f&quot;{chunk.title}:\n{chunk.text}&quot; 
        for chunk in relevant_chunks
    )
    prompt = f&quot;&quot;&quot;Question: {query}

Please use the following context to provide an accurate response:   

{context}

Answer:&quot;&quot;&quot;

    response = await client.chat.completions.create({
        model: &quot;gpt-4o-mini&quot;,
        messages: [{ role: &quot;user&quot;, content: prompt }],
    })
    print(&quot;RAG response:&quot;)
    print(response.choices[0].message.content)
</code></pre>
<h2>Next steps</h2>
<h3>More RAG and Vectorization Examples</h3>
<ul>
<li><a href="/examples/simple_fastapi_app/README.md">FastAPI + psycopg quickstart</a></li>
<li><a href="/docs/vectorizer/overview.md">Vectorizer overview</a> and <a href="/docs/vectorizer/worker.md">worker documentation</a></li>
<li><a href="/docs/vectorizer/api-reference.md">Vectorizer API reference</a></li>
</ul>
<h3>Text-to-SQL with Semantic Catalog</h3>
<ul>
<li><strong><a href="/docs/semantic_catalog/README.md">Semantic Catalog Quickstart</a></strong> - Learn how to use the semantic catalog to translate natural language to SQL for agentic applications.</li>
</ul>
<h1>Features</h1>
<p>Our pgai Python library lets you work with embeddings generated from your data:</p>
<ul>
<li>Automatically create and sync vector embeddings for your data using the <a href="/docs/vectorizer/overview.md">vectorizer</a>.</li>
<li><a href="/docs/vectorizer/api-reference.md#loading-configuration">Load data</a> from a column in your table or from a file, s3 bucket, etc.</li>
<li>Create multiple embeddings for the same data with different models and parameters for testing and experimentation.</li>
<li><a href="#a-configurable-vectorizer-pipeline">Customize</a> how your embedding pipeline parses, chunks, formats, and embeds your data.</li>
</ul>
<p>You can use the vector embeddings to:</p>
<ul>
<li><a href="/docs/vectorizer/overview.md#query-an-embedding">Perform semantic search</a> using pgvector.</li>
<li>Implement Retrieval Augmented Generation (RAG)</li>
<li>Perform high-performance, cost-efficient ANN search on large vector workloads with <a href="https://github.com/timescale/pgvectorscale">pgvectorscale</a>, which complements pgvector.</li>
</ul>
<p><strong>Text-to-SQL with Semantic Catalog:</strong> Transform natural language into accurate SQL queries. The semantic catalog generates database descriptions automatically, lets a human in the loop review and improve the descriptions and stores SQL examples and business facts. This enables LLMs to understand your schema and data context. See the <a href="/docs/semantic_catalog/README.md">semantic catalog</a> for more details.</p>
<p>We also offer a <a href="/projects/extension/README.md">PostgreSQL extension</a> that can perform LLM model calling directly from SQL. This is often useful for use cases like classification, summarization, and data enrichment on your existing data.</p>
<h2>A configurable vectorizer pipeline</h2>
<p>The vectorizer is designed to be flexible and customizable. Each vectorizer defines a pipeline for creating embeddings from your data. The pipeline is defined by a series of components that are applied in sequence to the data:</p>
<ul>
<li><strong><a href="/docs/vectorizer/api-reference.md#loading-configuration">Loading</a>:</strong> First, you define the source of the data to embed. It can be the data stored directly in a column of the source table or a URI referenced in a column of the source table that points to a file, s3 bucket, etc.</li>
<li><strong><a href="/docs/vectorizer/api-reference.md#parsing-configuration">Parsing</a>:</strong> Then, you define the way the data is parsed if it is a non-text document such as a PDF, HTML, or markdown file.</li>
<li><strong><a href="/docs/vectorizer/api-reference.md#chunking-configuration">Chunking</a>:</strong> Next, you define the way text data is split into chunks.</li>
<li><strong><a href="/docs/vectorizer/api-reference.md#formatting-configuration">Formatting</a>:</strong> Then, for each chunk, you define the way the data is formatted before it is sent for embedding. For example, you can add the title of the document as the first line of the chunk.</li>
<li><strong><a href="/docs/vectorizer/api-reference.md#embedding-configuration">Embedding</a>:</strong> Finally, you specify the LLM provider, model, and the parameters to be used when generating the embeddings.</li>
</ul>
<h2>Supported embedding models</h2>
<p>The following models are supported for embedding:</p>
<ul>
<li><a href="/docs/vectorizer/api-reference.md#aiembedding_ollama">Ollama</a></li>
<li><a href="/docs/vectorizer/api-reference.md#aiembedding_openai">OpenAI</a></li>
<li><a href="/docs/vectorizer/api-reference.md#aiembedding_voyageai">Voyage AI</a></li>
<li><a href="/docs/vectorizer/api-reference.md#aiembedding_litellm">Cohere</a></li>
<li><a href="/docs/vectorizer/api-reference.md#aiembedding_litellm">Huggingface</a></li>
<li><a href="/docs/vectorizer/api-reference.md#aiembedding_litellm">Mistral</a></li>
<li><a href="/docs/vectorizer/api-reference.md#aiembedding_litellm">Azure OpenAI</a></li>
<li><a href="/docs/vectorizer/api-reference.md#aiembedding_litellm">AWS Bedrock</a></li>
<li><a href="/docs/vectorizer/api-reference.md#aiembedding_litellm">Vertex AI</a></li>
</ul>
<h2>The devil is in the error handling</h2>
<p>Simply creating vector embeddings is easy and straightforward. The challenge is
that LLMs are somewhat unreliable and the endpoints exhibit intermittent
failures and/or degraded performance. A critical part of properly handling
failures is that your primary data-modification operations (INSERT, UPDATE,
DELETE) should not be dependent on the embedding operation. Otherwise, your
application will be down every time the endpoint is slow or fails and your user
experience will suffer.</p>
<p>Normally, you would need to implement a custom MLops pipeline to properly handle
endpoint failures. This commonly involves queuing system like Kafka, specialized
workers, and other infrastructure for handling the queue and retrying failed
requests. This is a lot of work and it is easy to get wrong.</p>
<p>With pgai, you can skip all that and focus on building your application because
the vectorizer is managing the embeddings for you. We have built in queueing and
retry logic to handle the various failure modes you can encounter. Because we do
this work in the background, the primary data modification operations are not
dependent on the embedding operation. This is why pgai is production-ready out of the box.</p>
<p>Many specialized vector databases create embeddings for you. However, they typically fail when embedding endpoints are down or degraded, placing the burden of error handling and retries back on you.</p>
<h1>Resources</h1>
<h2>Why we built it</h2>
<ul>
<li><a href="https://www.timescale.com/blog/vector-databases-are-the-wrong-abstraction/">Vector Databases Are the Wrong Abstraction</a></li>
<li><a href="http://www.timescale.com/blog/pgai-giving-postgresql-developers-ai-engineering-superpowers">pgai: Giving PostgreSQL Developers AI Engineering Superpowers</a></li>
</ul>
<h2>Quick start guides</h2>
<ul>
<li><a href="/docs/semantic_catalog/README.md">Semantic Catalog (Text-to-SQL)</a> - Learn how to use the semantic catalog to improve the translation of natural language to SQL for agentic applications.</li>
<li><a href="#quick-start">The vectorizer quick start above</a></li>
<li><a href="/docs/vectorizer/quick-start-openai.md">Quick start with OpenAI</a></li>
<li><a href="/docs/vectorizer/quick-start-voyage.md">Quick start with VoyageAI</a></li>
</ul>
<h2>Tutorials about pgai vectorizer</h2>
<ul>
<li><a href="https://www.timescale.com/blog/how-to-automatically-create-update-embeddings-in-postgresql/">How to Automatically Create &amp; Update Embeddings in PostgreSQL‚ÄîWith One SQL Query</a></li>
<li>[video] <a href="https://www.youtube.com/watch?v=ZoC2XYol6Zk">Auto Create and Sync Vector Embeddings in 1 Line of SQL</a></li>
<li><a href="https://www.timescale.com/blog/which-openai-embedding-model-is-best/">Which OpenAI Embedding Model Is Best for Your RAG App With Pgvector?</a></li>
<li><a href="https://www.timescale.com/blog/which-rag-chunking-and-formatting-strategy-is-best/">Which RAG Chunking and Formatting Strategy Is Best for Your App With Pgvector</a></li>
<li><a href="https://www.timescale.com/blog/parsing-all-the-data-with-open-source-tools-unstructured-and-pgai/">Parsing All the Data With Open-Source Tools: Unstructured and Pgai</a></li>
</ul>
<h2>Contributing</h2>
<p>We welcome contributions to pgai! See the <a href="/CONTRIBUTING.md">Contributing</a> page for more information.</p>
<h2>Get involved</h2>
<p>pgai is still at an early stage. Now is a great time to help shape the direction of this project;
we are currently deciding priorities. Have a look at the <a href="https://github.com/timescale/pgai/issues">list of features</a> we&#39;re thinking of working on.
Feel free to comment, expand the list, or hop on the Discussions forum.</p>
<p>To get started, take a look at <a href="./CONTRIBUTING.md">how to contribute</a>
and <a href="./DEVELOPMENT.md">how to set up a dev/test environment</a>.</p>
<h2>About Timescale</h2>
<p>Timescale is a PostgreSQL database company. To learn more visit the <a href="https://www.timescale.com">timescale.com</a>.</p>
<p>Timescale Cloud is a high-performance, developer focused, cloud platform that provides PostgreSQL services
for the most demanding AI, time-series, analytics, and event workloads. Timescale Cloud is ideal for production applications and provides high availability, streaming backups, upgrades over time, roles and permissions, and great security.</p>
 </div> </article>  <section class="mt-16 pt-8 border-t border-gray-200 dark:border-gray-700"> <h2 class="text-3xl font-bold text-center mb-8">Related Models</h2> <div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-6"> <a href="/model/github-QuivrHQ-quivr" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full"> <div class="absolute top-2 right-2 bg-yellow-400 text-yellow-900 text-xs font-bold px-2 py-1 rounded-full animate-pulse" title="Rising Star">
üî•
</div> <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="quivr"> quivr </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by QuivrHQ</p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> Opiniated RAG for integrating GenAI in your apps üß†   Focus on your product rather than the RAG. Easy integration in exi...
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="115,896 likes">‚ù§Ô∏è <span>115.9K</span></div> <div class="flex items-center gap-1" title="115,896 downloads">üì• <span>115.9K</span></div> </div> </div> </a><a href="/model/github-vanna-ai-vanna" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full"> <div class="absolute top-2 right-2 bg-yellow-400 text-yellow-900 text-xs font-bold px-2 py-1 rounded-full animate-pulse" title="Rising Star">
üî•
</div> <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="vanna"> vanna </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by vanna-ai</p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> ü§ñ Chat with your SQL database üìä. Accurate Text-to-SQL Generation via LLMs using Agentic Retrieval üîÑ....
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="65,019 likes">‚ù§Ô∏è <span>65.0K</span></div> <div class="flex items-center gap-1" title="65,019 downloads">üì• <span>65.0K</span></div> </div> </div> </a><a href="/model/github-Canner-WrenAI" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full"> <div class="absolute top-2 right-2 bg-yellow-400 text-yellow-900 text-xs font-bold px-2 py-1 rounded-full animate-pulse" title="Rising Star">
üî•
</div> <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="WrenAI"> WrenAI </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by Canner</p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> ‚ö°Ô∏è GenBI (Generative BI) queries any database in natural language, generates accurate SQL (Text-to-SQL), charts (Text-to...
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="39,063 likes">‚ù§Ô∏è <span>39.1K</span></div> <div class="flex items-center gap-1" title="39,063 downloads">üì• <span>39.1K</span></div> </div> </div> </a> </div> </section> </main> <footer class="bg-gray-800 text-gray-300 py-8 mt-16"> <div class="container mx-auto px-4 text-center"> <div class="flex justify-center gap-4 mb-4"> <a href="/about" class="hover:underline">About</a> <a href="/compliance" class="hover:underline">Compliance</a> </div> <p class="text-sm mt-2">
&copy; 2025 Free AI Tools. An open-source project to index the world of AI.
</p> <a href="mailto:compliance@free2aitools.com" class="text-sm text-gray-400 hover:underline mt-1 block">compliance@free2aitools.com</a> </div> </footer> </body></html>