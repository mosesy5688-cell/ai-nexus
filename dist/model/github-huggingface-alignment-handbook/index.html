<!DOCTYPE html><html lang="en"> <head><!-- Google Tag Manager --><script type="module">(function(e,n,r,t,m){e[t]=e[t]||[],e[t].push({"gtm.start":new Date().getTime(),event:"gtm.js"});var g=n.getElementsByTagName(r)[0],a=n.createElement(r),s="";a.async=!0,a.src="https://www.googletagmanager.com/gtm.js?id="+m+s,g.parentNode.insertBefore(a,g)})(window,document,"script","dataLayer","GTM-58C2CQ8G");</script><!-- End Google Tag Manager --><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><!-- Open Graph Meta Tags --><meta property="og:type" content="website"><meta property="og:url" content="https://free2aitools.com/model/github-huggingface-alignment-handbook/"><meta property="og:title" content="alignment-handbook - AI Model Details"><meta property="og:description" content="Robust recipes to align language models with human and AI preferences"><meta property="og:image" content="https://free2aitools.com/og-image.jpg"><!-- Twitter Card Meta Tags --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://free2aitools.com/model/github-huggingface-alignment-handbook/"><meta property="twitter:title" content="alignment-handbook - AI Model Details"><meta property="twitter:description" content="Robust recipes to align language models with human and AI preferences"><meta property="twitter:image" content="https://free2aitools.com/og-image.jpg"><meta name="description" content="Robust recipes to align language models with human and AI preferences"><title>alignment-handbook - AI Model Details</title><link rel="canonical" href="https://free2aitools.com/model/github-huggingface-alignment-handbook/"><!-- Google AdSense --><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2292826803755214" crossorigin="anonymous"></script><link rel="stylesheet" href="/_astro/about.C4wyk6Sp.css"></head> <body class="bg-gray-50 text-gray-800 font-sans antialiased flex flex-col min-h-screen"> <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-58C2CQ8G" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) --> <header class="sticky top-0 z-30 w-full backdrop-blur flex-none transition-colors duration-500 lg:border-b lg:border-gray-200 bg-white/80"> <nav class="container mx-auto px-4 sm:px-6 lg:px-8"> <div class="flex items-center justify-between h-16"> <!-- Site Logo --> <a href="/" class="flex items-center gap-2 text-xl sm:text-2xl font-bold text-gray-900"> <span>Free AI Tools</span> </a> <!-- Right-side items --> <div class="flex items-center gap-4"> <!-- Desktop Navigation --> <div class="hidden md:flex items-center gap-6"> <a href="/" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Explore Models </a><a href="/ranking" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Rankings </a><a href="/reports" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Reports </a><a href="/about" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> About </a> </div> <!-- Mobile Search & Menu Toggle --> <button data-collapse-toggle="navbar-default" type="button" class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200" aria-controls="navbar-default" aria-expanded="false"> <span class="sr-only">Open main menu</span> <svg class="w-5 h-5" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 17 14"><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 1h15M1 7h15M1 13h15"></path></svg> </button> </div> </div> <!-- Mobile Collapsible Menu --> <div class="hidden w-full md:hidden" id="navbar-default"> <ul class="font-medium flex flex-col p-4 md:p-0 mt-4 border border-gray-100 rounded-lg bg-gray-50"> <li> <a href="/" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Explore Models</a> </li><li> <a href="/ranking" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Rankings</a> </li><li> <a href="/reports" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Reports</a> </li><li> <a href="/about" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">About</a> </li> </ul> </div> </nav> </header> <main class="flex-grow">  <div class="container mx-auto px-4 py-12"> <div class="max-w-4xl mx-auto"> <!-- Header Section --> <header class="mb-8"> <div class="flex flex-col md:flex-row md:items-start md:justify-between gap-4"> <div> <h1 class="text-4xl md:text-5xl font-extrabold text-gray-900 dark:text-white mb-2">alignment-handbook</h1> <p class="text-lg text-gray-500 dark:text-gray-400">by huggingface</p> </div>  </div> </header> <!-- Metadata Section --> <div class="mb-8 p-4 bg-gray-100 dark:bg-gray-800 rounded-lg"> <div class="grid grid-cols-2 md:grid-cols-4 gap-4 text-center"> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Likes</p> <p class="text-xl font-bold">5,427</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Downloads</p> <p class="text-xl font-bold">5,427</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Task</p> <p class="text-xl font-bold capitalize">tool</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Last Updated</p> <p class="text-xl font-bold">2025/11/20</p> </div> </div> </div> <!-- Tags and Sources --> <div class="mb-8 flex flex-wrap items-start gap-4"> <div class="flex-1"> <h3 class="text-lg font-semibold mb-2">Tags</h3> <div class="flex flex-wrap gap-2"> <a href="/?tag=llm" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> llm </a><a href="/?tag=rlhf" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> rlhf </a><a href="/?tag=transformers" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> transformers </a> </div> </div> <div class="flex-shrink-0"> <h3 class="text-lg font-semibold mb-2">Sources</h3> <div class="flex flex-col gap-2"> <a href="https://github.com/huggingface/alignment-handbook" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ğŸ“¦ GitHub </a> </div> </div> </div> <!-- Source Specific Details --> <div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ğŸ“¦ GitHub Details
</h3> <div class="grid grid-cols-2 md:grid-cols-3 gap-4 text-sm"> <div><strong>Language:</strong> Python</div> <div><strong>Stars:</strong> 5,427</div> <div><strong>Forks:</strong> 465</div> <div><strong>Open Issues:</strong> 95</div> <div><strong>License:</strong> Apache License 2.0</div> <div><a href="https://huggingface.co/HuggingFaceH4" target="_blank" class="text-blue-600 dark:text-blue-400 hover:underline"><strong>Project Homepage â†—</strong></a></div> </div>  </div> <!-- README Content --> <article class="model-readme-content py-8 border-t border-gray-200 dark:border-gray-700"> <div class="prose prose-lg dark:prose-invert max-w-none"> <p align="center">
  <img src="https://raw.githubusercontent.com/huggingface/alignment-handbook/main/assets/handbook.png">
</p>

<p align="center">
    ğŸ¤— <a href="https://huggingface.co/collections/alignment-handbook/handbook-v01-models-and-datasets-654e424d22e6880da5ebc015" target="_blank">Models & Datasets</a> | ğŸ“ƒ <a href="https://arxiv.org/abs/2310.16944" target="_blank">Technical Report</a>
</p>

<h1>The Alignment Handbook</h1>
<p>Robust recipes to continue pretraining and to align language models with human and AI preferences.</p>
<h2>What is this?</h2>
<p>Just one year ago, chatbots were out of fashion and most people hadn&#39;t heard about techniques like Reinforcement Learning from Human Feedback (RLHF) to align language models with human preferences. Then, OpenAI broke the internet with ChatGPT and Meta followed suit by releasing the Llama series of language models which enabled the ML community to build their very own capable chatbots. This has led to a rich ecosystem of datasets and models that have mostly focused on teaching language models to follow instructions through supervised fine-tuning (SFT).</p>
<p>However, we know from the <a href="https://huggingface.co/papers/2203.02155">InstructGPT</a> and <a href="https://huggingface.co/papers/2307.09288">Llama2</a> papers that significant gains in helpfulness and safety can be had by augmenting SFT with human (or AI) preferences. At the same time, aligning language models to a set of preferences is a fairly novel idea and there are few public resources available on how to train these models, what data to collect, and what metrics to measure for best downstream performance.</p>
<p>The Alignment Handbook aims to fill that gap by providing the community with a series of robust training recipes that span the whole pipeline.</p>
<h2>News ğŸ—ï¸</h2>
<ul>
<li><strong>July 24, 2025</strong>: We release the full <a href="recipes/smollm3/README.md">post-training recipe</a> behind SmolLM3-3B: a state-of-the-art hybrid reasoning model ğŸ’­</li>
<li><strong>November 21, 2024</strong>: We release the <a href="recipes/smollm2/README.md">recipe</a> for fine-tuning SmolLM2-Instruct.</li>
<li><strong>August 18, 2024</strong>: We release SmolLM-Instruct v0.2, along with the <a href="recipes/smollm/README.md">recipe</a>  to fine-tuning small LLMs ğŸ’»</li>
<li><strong>April 12, 2024</strong>: We release Zephyr 141B (A35B), in collaboration with Argilla and Kaist AI, along with the recipe to fine-tune Mixtral 8x22B with ORPO ğŸª</li>
<li><strong>March 12, 2024:</strong> We release StarChat2 15B, along with the recipe to train capable coding assistants ğŸŒŸ</li>
<li><strong>March 1, 2024:</strong> We release Zephyr 7B Gemma, which is a new recipe to align Gemma 7B with RLAIF ğŸ”¥</li>
<li><strong>February 1, 2024:</strong> We release a recipe to align open LLMs with Constitutional AI ğŸ“œ! See the <a href="https://github.com/huggingface/alignment-handbook/tree/main/recipes/constitutional-ai">recipe</a> and the <a href="https://huggingface.co/blog/constitutional_ai">blog post</a> for details. </li>
<li><strong>January 18, 2024:</strong> We release a suite of evaluations of DPO vs KTO vs IPO, see the <a href="recipes/pref_align_scan/README.md">recipe</a> and the <a href="https://huggingface.co/blog/pref-tuning">blog post</a> for details.</li>
<li><strong>November 10, 2023:</strong> We release all the training code to replicate Zephyr-7b-Î² ğŸª! We also release <a href="https://huggingface.co/datasets/HuggingFaceH4/no_robots">No Robots</a>, a brand new dataset of 10,000 instructions and demonstrations written entirely by skilled human annotators.</li>
</ul>
<h2>Links ğŸ”—</h2>
<ul>
<li><a href="https://huggingface.co/collections/HuggingFaceH4/zephyr-7b-6538c6d6d5ddd1cbb1744a66">Zephyr 7B models, datasets, and demos</a></li>
</ul>
<h2>How to navigate this project ğŸ§­</h2>
<p>This project is simple by design and mostly consists of:</p>
<ul>
<li><a href="./scripts/"><code>scripts</code></a> to train and evaluate models. Four steps are included: continued pretraining, supervised-finetuning (SFT) for chat, preference alignment with DPO, and supervised-finetuning with preference alignment with ORPO. Each script supports distributed training of the full model weights with DeepSpeed ZeRO-3, or LoRA/QLoRA for parameter-efficient fine-tuning.</li>
<li><a href="./recipes/"><code>recipes</code></a> to reproduce models like Zephyr 7B. Each recipe takes the form of a YAML file which contains all the parameters associated with a single training run. A <code>gpt2-nl</code> recipe is also given to illustrate how this handbook can be used for language or domain adaptation, e.g. by continuing to pretrain on a different language, and then SFT and DPO tuning the result.</li>
</ul>
<p>We are also working on a series of guides to explain how methods like direct preference optimization (DPO) work, along with lessons learned from gathering human preferences in practice. To get started, we recommend the following:</p>
<ol>
<li>Follow the <a href="#installation-instructions">installation instructions</a> to set up your environment etc.</li>
<li>Replicate Zephyr-7b-Î² by following the <a href="./recipes/zephyr-7b-beta/README.md">recipe instructions</a>.</li>
</ol>
<p>If you would like to train chat models on your own datasets, we recommend following the dataset formatting instructions <a href="./scripts/README.md#fine-tuning-on-your-datasets">here</a>.</p>
<h2>Contents</h2>
<p>The initial release of the handbook will focus on the following techniques:</p>
<ul>
<li><strong>Continued pretraining:</strong> adapt language models to a new language or domain, or simply improve it by continued pretraining (causal language modeling) on a new dataset.</li>
<li><strong>Supervised fine-tuning:</strong> teach language models to follow instructions and tips on how to collect and curate your training dataset.</li>
<li><strong>Reward modeling:</strong> teach language models to distinguish model responses according to human or AI preferences.</li>
<li><strong>Rejection sampling:</strong> a simple, but powerful technique to boost the performance of your SFT model.</li>
<li><strong>Direct preference optimisation (DPO):</strong> a powerful and promising alternative to PPO.</li>
<li><strong>Odds Ratio Preference Optimisation (ORPO)</strong>: a technique to fine-tune language models with human preferences, combining SFT and DPO in a single stage.</li>
</ul>
<h2>Installation instructions</h2>
<p>To run the code in this project, first, create a Python virtual environment using e.g. <code>uv</code>:</p>
<pre><code class="language-shell">uv venv handbook --python 3.11 &amp;&amp; source handbook/bin/activate &amp;&amp; uv pip install --upgrade pip
</code></pre>
<blockquote>
<p>[!TIP]
To install <code>uv</code>, follow the <a href="https://docs.astral.sh/uv/getting-started/installation/">UV Installation Guide</a>.</p>
</blockquote>
<p>Next, install PyTorch <code>v2.6.0</code> </p>
<pre><code class="language-shell">uv pip install torch==2.6.0 --index-url https://download.pytorch.org/whl/cu126
</code></pre>
<p>Note that the precise version is important for reproducibility! Since this is hardware-dependent, we also direct you to the <a href="https://pytorch.org/get-started/locally/">PyTorch Installation Page</a>.</p>
<p>You can then install the remaining package dependencies as follows:</p>
<pre><code class="language-shell">uv pip install .
</code></pre>
<p>You will also need Flash Attention 2 installed, which can be done by running:</p>
<pre><code class="language-shell">uv pip install &quot;flash-attn==2.7.4.post1&quot; --no-build-isolation
</code></pre>
<p>Next, log into your Hugging Face account as follows:</p>
<pre><code class="language-shell">huggingface-cli login
</code></pre>
<p>Finally, install Git LFS so that you can push models to the Hugging Face Hub:</p>
<pre><code class="language-shell">sudo apt-get install git-lfs
</code></pre>
<p>You can now check out the <code>scripts</code> and <code>recipes</code> directories for instructions on how to train some models ğŸª!</p>
<h2>Project structure</h2>
<pre><code>â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile                    &lt;- Makefile with commands like `make style`
â”œâ”€â”€ README.md                   &lt;- The top-level README for developers using this project
â”œâ”€â”€ recipes                     &lt;- Recipe configs, accelerate configs, slurm scripts
â”œâ”€â”€ scripts                     &lt;- Scripts to train and evaluate chat models
â”œâ”€â”€ setup.cfg                   &lt;- Installation config (mostly used for configuring code quality &amp; tests)
â”œâ”€â”€ setup.py                    &lt;- Makes project pip installable (pip install -e .) so `alignment` can be imported
â”œâ”€â”€ src                         &lt;- Source code for use in this project
â””â”€â”€ tests                       &lt;- Unit tests
</code></pre>
<h2>Citation</h2>
<p>If you find the content of this repo useful in your work, please cite it as follows via <code>\usepackage{biblatex}</code>:</p>
<pre><code class="language-bibtex">@software{Tunstall_The_Alignment_Handbook,
  author = {Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Huang, Shengyi and Rasul, Kashif and Bartolome, Alvaro, and M. PatiÃ±o, Carlos and M. Rush, Alexander and Wolf, Thomas},
  license = {Apache-2.0},
  title = {{The Alignment Handbook}},
  url = {https://github.com/huggingface/alignment-handbook},
  version = {0.4.0.dev0}
}
</code></pre>
 </div> </article> <!-- Related Models Section --> <section class="mt-16 pt-8 border-t border-gray-200 dark:border-gray-700"> <h2 class="text-3xl font-bold text-center mb-8">Related Models</h2> <div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-6"> <a href="/model/github-hiyouga-LLaMA-Factory" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full"> <div class="absolute top-2 right-2 bg-yellow-400 text-yellow-900 text-xs font-bold px-2 py-1 rounded-full animate-pulse" title="Rising Star">
ğŸ”¥
</div> <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="LLaMA-Factory"> LLaMA-Factory </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by hiyouga</p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> Unified Efficient Fine-Tuning of 100+ LLMs &amp; VLMs (ACL 2024)...
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="62,802 likes">â¤ï¸ <span>62.8K</span></div> <div class="flex items-center gap-1" title="62,802 downloads">ğŸ“¥ <span>62.8K</span></div> </div> </div> </a><a href="/model/github-rasbt-LLMs-from-scratch" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full"> <div class="absolute top-2 right-2 bg-yellow-400 text-yellow-900 text-xs font-bold px-2 py-1 rounded-full animate-pulse" title="Rising Star">
ğŸ”¥
</div> <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="LLMs-from-scratch"> LLMs-from-scratch </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by rasbt</p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> Implement a ChatGPT-like LLM in PyTorch from scratch, step by step...
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="79,060 likes">â¤ï¸ <span>79.1K</span></div> <div class="flex items-center gap-1" title="79,060 downloads">ğŸ“¥ <span>79.1K</span></div> </div> </div> </a><a href="/model/github-deepset-ai-haystack" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full"> <div class="absolute top-2 right-2 bg-yellow-400 text-yellow-900 text-xs font-bold px-2 py-1 rounded-full animate-pulse" title="Rising Star">
ğŸ”¥
</div> <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="haystack"> haystack </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by deepset-ai</p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> AI orchestration framework to build customizable, production-ready LLM applications. Connect components (models, vector ...
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="23,437 likes">â¤ï¸ <span>23.4K</span></div> <div class="flex items-center gap-1" title="23,437 downloads">ğŸ“¥ <span>23.4K</span></div> </div> </div> </a> </div> </section> </div> </div>  </main> <footer class="bg-gray-800 text-gray-300 py-8 mt-16"> <div class="container mx-auto px-4 text-center"> <div class="flex justify-center gap-4 mb-4"> <a href="/about" class="hover:underline">About</a> <a href="/compliance" class="hover:underline">Compliance</a> </div> <p class="text-sm mt-2">
&copy; 2025 Free AI Tools. An open-source project to index the world of AI.
</p> <a href="mailto:compliance@free2aitools.com" class="text-sm text-gray-400 hover:underline mt-1 block">compliance@free2aitools.com</a> </div> </footer> </body></html>