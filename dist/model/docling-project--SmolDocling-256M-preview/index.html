<!DOCTYPE html><html lang="en" data-astro-cid-37fxchfa> <head><meta charset="UTF-8"><meta name="description" content="A model for image-text-to-text."><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v5.15.3"><title>SmolDocling-256M-preview</title><link rel="stylesheet" href="/_astro/about.DnK1pLJZ.css">
<style>body{background-color:var(--color-background);color:var(--color-text-primary)}:root{--color-background: #F9FAFB;--color-text-primary: #111827}.dark{--color-background: #1F2937;--color-text-primary: #D1D5DB}
</style></head> <body class="flex flex-col min-h-screen" data-astro-cid-37fxchfa> <header class="bg-white dark:bg-gray-800 shadow-md sticky top-0 z-50" data-astro-cid-37fxchfa> <nav class="container mx-auto px-4 py-4 flex justify-between items-center" data-astro-cid-37fxchfa> <a href="/" class="text-2xl font-bold text-gray-900 dark:text-white shrink-0" data-astro-cid-37fxchfa>Free AI Tools</a> <div class="w-full max-w-md mx-4" data-astro-cid-37fxchfa> <input type="search" id="header-search" placeholder="Search models..." class="w-full px-4 py-2 text-base bg-gray-100 dark:bg-gray-700 border border-gray-300 dark:border-gray-600 rounded-full focus:ring-2 focus:ring-blue-500 focus:border-blue-500 outline-none transition-colors" data-astro-cid-37fxchfa> </div> <div class="flex items-center space-x-4 shrink-0" data-astro-cid-37fxchfa> <button id="theme-toggle" class="p-2 rounded-full hover:bg-gray-200 dark:hover:bg-gray-700 transition-colors" aria-label="Toggle theme"> <svg id="theme-icon-light" xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 3v1m0 16v1m9-9h-1M4 12H3m15.364 6.364l-.707-.707M6.343 6.343l-.707-.707m12.728 0l-.707.707M6.343 17.657l-.707.707M16 12a4 4 0 11-8 0 4 4 0 018 0z"></path></svg> <svg id="theme-icon-dark" xmlns="http://www.w3.org/2000/svg" class="h-6 w-6 hidden" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M20.354 15.354A9 9 0 018.646 3.646 9.003 9.003 0 0012 21a9.003 9.003 0 008.354-5.646z"></path></svg> </button> <script>
  const theme = (() => {
    if (typeof localStorage !== 'undefined' && localStorage.getItem('theme')) {
      return localStorage.getItem('theme');
    }
    if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
      return 'dark';
    }
    return 'light';
  })();

  if (theme === 'dark') {
    document.documentElement.classList.add('dark');
  } else {
    document.documentElement.classList.remove('dark');
  }

  const handleToggleClick = () => {
    const element = document.documentElement;
    element.classList.toggle("dark");

    const isDark = element.classList.contains("dark");
    localStorage.setItem("theme", isDark ? "dark" : "light");

    document.getElementById('theme-icon-light').classList.toggle('hidden', isDark);
    document.getElementById('theme-icon-dark').classList.toggle('hidden', !isDark);
  }

  document.getElementById('theme-toggle').addEventListener('click', handleToggleClick);

  // Set initial icon state
  document.getElementById('theme-icon-light').classList.toggle('hidden', theme === 'dark');
  document.getElementById('theme-icon-dark').classList.toggle('hidden', theme !== 'dark');
</script> <a href="/explore" class="text-gray-600 dark:text-gray-300 hover:text-blue-500" data-astro-cid-37fxchfa>Explore</a> <a href="/about" class="text-gray-600 dark:text-gray-300 hover:text-blue-500" data-astro-cid-37fxchfa>About</a> </div> </nav> </header> <main class="flex-grow" data-astro-cid-37fxchfa>  <div class="container mx-auto px-4 py-12"> <a href="javascript:history.back()" class="text-blue-500 hover:underline mb-8 block font-medium transition-colors">&larr; Back to previous page</a> <article class="bg-white dark:bg-gray-800 p-6 rounded-xl shadow-xl"> <header class="border-b pb-4 mb-6"> <h1 class="text-4xl font-extrabold text-gray-900 dark:text-white leading-tight">SmolDocling-256M-preview</h1>  </header> <div class="grid grid-cols-1 lg:grid-cols-4 gap-12 relative"> <div class="lg:col-span-3"> <section class="mb-8"> <h3 class="text-lg font-semibold mb-2 text-gray-800 dark:text-gray-200">Tags</h3> <div id="tags-container" class="flex flex-wrap gap-2"> <span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">transformers</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">onnx</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">safetensors</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">idefics3</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">image-to-text</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">image-text-to-text</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">conversational</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">en</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">dataset:ds4sd/SynthCodeNet</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">dataset:ds4sd/SynthFormulaNet</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">dataset:ds4sd/SynthChartNet</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">dataset:HuggingFaceM4/DoclingMatix</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">arxiv:2503.11576</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">arxiv:2305.03393</span><span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">base_model:HuggingFaceTB/SmolVLM-256M-Instruct</span> <button id="show-all-tags" class="bg-gray-200 text-gray-700 text-sm font-medium px-3 py-1 rounded-full dark:bg-gray-700 dark:text-gray-300 hover:bg-gray-300 dark:hover:bg-gray-600 transition-colors">
Show all (19)
</button> </div> </section> <section id="details-usage" class="mt-8 mb-10"> <h2 class="text-3xl font-bold mb-4 text-gray-800 dark:text-gray-200 border-b pb-2">Details & Usage</h2> <div class="prose dark:prose-invert prose-lg max-w-none bg-gray-50 dark:bg-gray-900 p-6 rounded-lg prose-pre:bg-gray-200 dark:prose-pre:bg-gray-800 prose-pre:text-gray-800 dark:prose-pre:text-gray-200 border border-gray-100 dark:border-gray-700 shadow-inner"> <div>---
base_model:
- HuggingFaceTB/SmolVLM-256M-Instruct
language:
- en
library_name: transformers
license: cdla-permissive-2.0
pipeline_tag: image-text-to-text
datasets:
- ds4sd/SynthCodeNet
- ds4sd/SynthFormulaNet
- ds4sd/SynthChartNet
- HuggingFaceM4/DoclingMatix
---

<div style="
  background-color: #f0f9ff;
  border: 1px solid #bae6fd;
  color: #0369a1;
  padding: 12px 16px;
  border-radius: 12px;
  margin-bottom: 16px;
  font-family: sans-serif;
">
  <strong>ğŸ“¢ New Release:</strong>  
  Weâ€™ve released <a href="https://huggingface.co/ibm-granite/granite-docling-258M" target="_blank" style="color:#0284c7; font-weight:bold; text-decoration:underline;">
    granite-docling-258M</a>, the successor to <b>SmolDocling</b>. It will now receive updates and support, check it out!
</div>

<div style="display: flex; align-items: center;">
    <img src="https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/assets/SmolDocling_doctags1.png" alt="SmolDocling" style="width: 200px; height: auto; margin-right: 20px;">
    <div>
        <h3>SmolDocling-256M-preview</h3>
        <p>SmolDocling is a multimodal Image-Text-to-Text model designed for efficient document conversion. It retains Docling's most popular features while ensuring full compatibility with Docling through seamless support for <strong>DoclingDocuments</strong>.</p>
    </div>
</div>

This model was presented in the paper [SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion](https://huggingface.co/papers/2503.11576).

### ğŸš€ Features:  
- ğŸ·ï¸ **DocTags for Efficient Tokenization** â€“ Introduces DocTags an efficient and minimal representation for documents that is fully compatible with **DoclingDocuments**.  
- ğŸ” **OCR (Optical Character Recognition)** â€“ Extracts text accurately from images.  
- ğŸ“ **Layout and Localization** â€“ Preserves document structure and document element **bounding boxes**.  
- ğŸ’» **Code Recognition** â€“ Detects and formats code blocks including identation.  
- ğŸ”¢ **Formula Recognition** â€“ Identifies and processes mathematical expressions.  
- ğŸ“Š **Chart Recognition** â€“ Extracts and interprets chart data.  
- ğŸ“‘ **Table Recognition** â€“ Supports column and row headers for structured table extraction.  
- ğŸ–¼ï¸ **Figure Classification** â€“ Differentiates figures and graphical elements.  
- ğŸ“ **Caption Correspondence** â€“ Links captions to relevant images and figures.  
- ğŸ“œ **List Grouping** â€“ Organizes and structures list elements correctly.  
- ğŸ“„ **Full-Page Conversion** â€“ Processes entire pages for comprehensive document conversion including all page elements (code, equations, tables, charts etc.) 
- ğŸ”² **OCR with Bounding Boxes** â€“ OCR regions using a bounding box.
- ğŸ“‚ **General Document Processing** â€“ Trained for both scientific and non-scientific documents.  
- ğŸ”„ **Seamless Docling Integration** â€“ Import into **Docling** and export in multiple formats.
- ğŸ’¨ **Fast inference using VLLM** â€“ Avg of 0.35 secs per page on A100 GPU.

### ğŸš§ *Coming soon!*
- ğŸ“Š **Better chart recognition ğŸ› ï¸**
- ğŸ“š **One shot multi-page inference â±ï¸**
- ğŸ§ª **Chemical Recognition**
- ğŸ“™ **Datasets**

## âŒ¨ï¸ Get started (code examples)

You can use **transformers**, **vllm**, or **onnx** to perform inference, and [Docling](https://github.com/docling-project/docling) to convert results to variety of output formats (md, html, etc.):

<details>
<summary>ğŸ“„ Single page image inference using Tranformers ğŸ¤–</summary>

```python
# Prerequisites:
# pip install torch
# pip install docling_core
# pip install transformers

import torch
from docling_core.types.doc import DoclingDocument
from docling_core.types.doc.document import DocTagsDocument
from transformers import AutoProcessor, AutoModelForVision2Seq
from transformers.image_utils import load_image
from pathlib import Path

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Load images
image = load_image("https://upload.wikimedia.org/wikipedia/commons/7/76/GazettedeFrance.jpg")

# Initialize processor and model
processor = AutoProcessor.from_pretrained("ds4sd/SmolDocling-256M-preview")
model = AutoModelForVision2Seq.from_pretrained(
    "ds4sd/SmolDocling-256M-preview",
    torch_dtype=torch.bfloat16,
    _attn_implementation="flash_attention_2" if DEVICE == "cuda" else "eager",
).to(DEVICE)

# Create input messages
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "Convert this page to docling."}
        ]
    },
]

# Prepare inputs
prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(text=prompt, images=[image], return_tensors="pt")
inputs = inputs.to(DEVICE)

# Generate outputs
generated_ids = model.generate(**inputs, max_new_tokens=8192)
prompt_length = inputs.input_ids.shape[1]
trimmed_generated_ids = generated_ids[:, prompt_length:]
doctags = processor.batch_decode(
    trimmed_generated_ids,
    skip_special_tokens=False,
)[0].lstrip()

# Populate document
doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])
print(doctags)
# create a docling document
doc = DoclingDocument.load_from_doctags(doctags_doc, document_name="Document")

# export as any format
# HTML
# Path("Out/").mkdir(parents=True, exist_ok=True)
# output_path_html = Path("Out/") / "example.html"
# doc.save_as_html(output_path_html)
# MD
print(doc.export_to_markdown())
```
</details>


<details>
<summary> ğŸš€ Fast Batch Inference Using VLLM</summary>

```python
# Prerequisites:
# pip install vllm
# pip install docling_core
# place page images you want to convert into "img/" dir

import time
import os
from vllm import LLM, SamplingParams
from PIL import Image
from docling_core.types.doc import DoclingDocument
from docling_core.types.doc.document import DocTagsDocument
from pathlib import Path

# Configuration
MODEL_PATH = "ds4sd/SmolDocling-256M-preview"
IMAGE_DIR = "img/"  # Place your page images here
OUTPUT_DIR = "out/"
PROMPT_TEXT = "Convert page to Docling."

# Ensure output directory exists
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Initialize LLM
llm = LLM(model=MODEL_PATH, limit_mm_per_prompt={"image": 1})

sampling_params = SamplingParams(
    temperature=0.0,
    max_tokens=8192)

chat_template = f"<|im_start|>User:<image>{PROMPT_TEXT}<end_of_utterance>
Assistant:"

image_files = sorted([f for f in os.listdir(IMAGE_DIR) if f.lower().endswith((".png", ".jpg", ".jpeg"))])

start_time = time.time()
total_tokens = 0

for idx, img_file in enumerate(image_files, 1):
    img_path = os.path.join(IMAGE_DIR, img_file)
    image = Image.open(img_path).convert("RGB")

    llm_input = {"prompt": chat_template, "multi_modal_data": {"image": image}}
    output = llm.generate([llm_input], sampling_params=sampling_params)[0]
    
    doctags = output.outputs[0].text
    img_fn = os.path.splitext(img_file)[0]
    output_filename = img_fn + ".dt"
    output_path = os.path.join(OUTPUT_DIR, output_filename)

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(doctags)

    # To convert to Docling Document, MD, HTML, etc.:
    doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])
    doc = DoclingDocument.load_from_doctags(doctags_doc, document_name="Document")
    # export as any format
    # HTML
    # output_path_html = Path(OUTPUT_DIR) / f"{img_fn}.html"
    # doc.save_as_html(output_path_html)
    # MD
    output_path_md = Path(OUTPUT_DIR) / f"{img_fn}.md"
    doc.save_as_markdown(output_path_md)
print(f"Total time: {time.time() - start_time:.2f} sec")
```
</details>
<details>
<summary> ONNX Inference</summary>

```python
# Prerequisites:
# pip install onnxruntime
# pip install onnxruntime-gpu
from transformers import AutoConfig, AutoProcessor
from transformers.image_utils import load_image
import onnxruntime
import numpy as np
import os
from docling_core.types.doc import DoclingDocument
from docling_core.types.doc.document import DocTagsDocument

os.environ["OMP_NUM_THREADS"] = "1"
# cuda
os.environ["ORT_CUDA_USE_MAX_WORKSPACE"] = "1"

# 1. Load models
## Load config and processor
model_id = "ds4sd/SmolDocling-256M-preview"
config = AutoConfig.from_pretrained(model_id)
processor = AutoProcessor.from_pretrained(model_id)

## Load sessions
# !wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/vision_encoder.onnx
# !wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/embed_tokens.onnx
# !wget https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/onnx/decoder_model_merged.onnx
# cpu
# vision_session = onnxruntime.InferenceSession("vision_encoder.onnx")
# embed_session = onnxruntime.InferenceSession("embed_tokens.onnx")
# decoder_session = onnxruntime.InferenceSession("decoder_model_merged.onnx"

# cuda
vision_session = onnxruntime.InferenceSession("vision_encoder.onnx", providers=["CUDAExecutionProvider"])
embed_session = onnxruntime.InferenceSession("embed_tokens.onnx", providers=["CUDAExecutionProvider"])
decoder_session = onnxruntime.InferenceSession("decoder_model_merged.onnx", providers=["CUDAExecutionProvider"])

## Set config values
num_key_value_heads = config.text_config.num_key_value_heads
head_dim = config.text_config.head_dim
num_hidden_layers = config.text_config.num_hidden_layers
eos_token_id = config.text_config.eos_token_id
image_token_id = config.image_token_id
end_of_utterance_id = processor.tokenizer.convert_tokens_to_ids("<end_of_utterance>")

# 2. Prepare inputs
## Create input messages
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image"},
            {"type": "text", "text": "Convert this page to docling."}
        ]
    },
]

## Load image and apply processor
image = load_image("https://ibm.biz/docling-page-with-table")
prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
inputs = processor(text=prompt, images=[image], return_tensors="np")

## Prepare decoder inputs
batch_size = inputs['input_ids'].shape[0]
past_key_values = {
    f'past_key_values.{layer}.{kv}': np.zeros([batch_size, num_key_value_heads, 0, head_dim], dtype=np.float32)
    for layer in range(num_hidden_layers)
    for kv in ('key', 'value')
}
image_features = None
input_ids = inputs['input_ids']
attention_mask = inputs['attention_mask']
position_ids = np.cumsum(inputs['attention_mask'], axis=-1)


# 3. Generation loop
max_new_tokens = 8192
generated_tokens = np.array([[]], dtype=np.int64)
for i in range(max_new_tokens):
  inputs_embeds = embed_session.run(None, {'input_ids': input_ids})[0]

  if image_features is None:
    ## Only compute vision features if not already computed
    image_features = vision_session.run(
        ['image_features'],  # List of output names or indices
        {
            'pixel_values': inputs['pixel_values'],
            'pixel_attention_mask': inputs['pixel_attention_mask'].astype(np.bool_)
        }
    )[0]
    
    ## Merge text and vision embeddings
    inputs_embeds[inputs['input_ids'] == image_token_id] = image_features.reshape(-1, image_features.shape[-1])

  logits, *present_key_values = decoder_session.run(None, dict(
      inputs_embeds=inputs_embeds,
      attention_mask=attention_mask,
      position_ids=position_ids,
      **past_key_values,
  ))

  ## Update values for next generation loop
  input_ids = logits[:, -1].argmax(-1, keepdims=True)
  attention_mask = np.ones_like(input_ids)
  position_ids = position_ids[:, -1:] + 1
  for j, key in enumerate(past_key_values):
    past_key_values[key] = present_key_values[j]

  generated_tokens = np.concatenate([generated_tokens, input_ids], axis=-1)
  if (input_ids == eos_token_id).all() or (input_ids == end_of_utterance_id).all():
    break  # Stop predicting

doctags = processor.batch_decode(
    generated_tokens,
    skip_special_tokens=False,
)[0].lstrip()

print(doctags)

doctags_doc = DocTagsDocument.from_doctags_and_image_pairs([doctags], [image])
print(doctags)
# create a docling document
doc = DoclingDocument.load_from_doctags(doctags_doc, document_name="Document")

print(doc.export_to_markdown())
```
</details>


ğŸ’» Local inference on Apple Silicon with MLX: [see here](https://huggingface.co/ds4sd/SmolDocling-256M-preview-mlx-bf16)

## DocTags

<img src="https://huggingface.co/ds4sd/SmolDocling-256M-preview/resolve/main/assets/doctags_v2.png" width="800" height="auto" alt="Image description">
DocTags create a clear and structured system of tags and rules that separate text from the document's structure. This makes things easier for Image-to-Sequence models by reducing confusion. On the other hand, converting directly to formats like HTML or Markdown can be messyâ€”it often loses details, doesnâ€™t clearly show the documentâ€™s layout, and increases the number of tokens, making processing less efficient.
DocTags are integrated with Docling, which allows export to HTML, Markdown, and JSON. These exports can be offloaded to the CPU, reducing token generation overhead and improving efficiency.

## Supported Instructions

<table>
  <tr>
    <td><b>Description</b></td>
    <td><b>Instruction</b></td>
    <td><b>Comment</b></td>
  </tr>
  <tr>
    <td><b>Full conversion</b></td>
    <td>Convert this page to docling.</td>
    <td>DocTags represetation</td>
  </tr>
  <tr>
    <td><b>Chart</b></td>
    <td>Convert chart to table.</td>
    <td>(e.g., &lt;chart&gt;)</td>
  </tr>
  <tr>
    <td><b>Formula</b></td>
    <td>Convert formula to LaTeX.</td>
    <td>(e.g., &lt;formula&gt;)</td>
  </tr>
  <tr>
    <td><b>Code</b></td>
    <td>Convert code to text.</td>
    <td>(e.g., &lt;code&gt;)</td>
  </tr>
  <tr>
    <td><b>Table</b></td>
    <td>Convert table to OTSL.</td>
    <td>(e.g., &lt;otsl&gt;) OTSL: <a href="https://arxiv.org/pdf/2305.03393">Lysak et al., 2023</a></td>
  </tr>
  <tr>
    <td rowspan=4><b>Actions and Pipelines</b></td>
    <td>OCR the text in a specific location: &lt;loc_155&gt;&lt;loc_233&gt;&lt;loc_206&gt;&lt;loc_237&gt;</td>
    <td></td>
  </tr>
  <tr>
    <td>Identify element at: &lt;loc_247&gt;&lt;loc_482&gt;&lt;10c_252&gt;&lt;loc_486&gt;</td>
    <td></td>
  </tr>
  <tr>
    <td>Find all 'text' elements on the page, retrieve all section headers.</td>
    <td></td>
  </tr>
  <tr>
    <td>Detect footer elements on the page.</td>
    <td></td>
  </tr>
</table>


#### ğŸ“Š Datasets
- [SynthCodeNet](https://huggingface.co/datasets/ds4sd/SynthCodeNet)
- [SynthFormulaNet](https://huggingface.co/datasets/ds4sd/SynthFormulaNet)
- [SynthChartNet](https://huggingface.co/datasets/ds4sd/SynthChartNet)
- [DoclingMatix](https://huggingface.co/datasets/HuggingFaceM4/DoclingMatix)

#### Model Summary

- **Developed by:** Docling Team, IBM Research
- **Model type:** Multi-modal model (image+text)
- **Language(s) (NLP):** English
- **License:** Apache 2.0
- **Architecture:** Based on [Idefics3](https://huggingface.co/HuggingFaceM4/Idefics3-8B-Llama3) (see technical summary)
- **Finetuned from model:** Based on [SmolVLM-256M-Instruct](https://huggingface.co/HuggingFaceTB/SmolVLM-256M-Instruct)

**Repository:** [Docling](https://github.com/docling-project/docling)

**Paper:** [arXiv](https://arxiv.org/abs/2503.11576)

**Project Page:** [Hugging Face](https://huggingface.co/ds4sd/SmolDocling-256M-preview)

**Citation:**
```
@misc{nassar2025smoldoclingultracompactvisionlanguagemodel,
      title={SmolDocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion}, 
      author={Ahmed Nassar and Andres Marafioti and Matteo Omenetti and Maksym Lysak and Nikolaos Livathinos and Christoph Auer and Lucas Morin and Rafael Teixeira de Lima and Yusik Kim and A. Said Gurbuz and Michele Dolfi and Miquel FarrÃ© and Peter W. J. Staar},
      year={2025},
      eprint={2503.11576},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2503.11576}, 
}
```
**Demo:** [HF Space](https://huggingface.co/spaces/ds4sd/SmolDocling-256M-Demo)</div> </div> </section> <div class="border-b border-gray-200 dark:border-gray-700 mb-6"> <nav class="-mb-px flex space-x-8" aria-label="Secondary Tabs"> <button id="tab-btn-files" class="tab-button whitespace-nowrap py-3 px-1 border-b-2 font-semibold text-xl focus:outline-none active-tab">
Files
</button> <button id="tab-btn-community" class="tab-button whitespace-nowrap py-3 px-1 border-b-2 font-semibold text-xl focus:outline-none inactive-tab">
Community
</button> </nav> </div> <div id="tab-content" class="mt-6"> <div id="tab-files" class="tab-panel"> <h2 class="text-2xl font-semibold mb-4 text-gray-800 dark:text-gray-200">Files and Versions</h2> <p class="text-gray-600 dark:text-gray-400">This section will list the model files. (Functionality to be implemented)</p> <div class="ad-container my-4 mt-10">  <div id="amazon-ad-YOUR_AMAZON_DETAIL_PAGE_AD_ID" style="width:100%;height:250px">  </div>  </div> </div> <div id="tab-community" class="tab-panel hidden"> <h2 class="text-2xl font-semibold mb-4 text-gray-800 dark:text-gray-200">Community Discussions</h2> <p class="text-gray-600 dark:text-gray-400">This section will display community discussions and showcase user-created content. (Functionality to be implemented)</p> </div> </div> </div> <aside class="lg:col-span-1"> <div class="bg-gray-50 dark:bg-gray-700 p-6 rounded-lg sticky top-6 shadow-md border border-gray-100 dark:border-gray-600"> <h3 class="text-2xl font-bold mb-4 text-gray-900 dark:text-white border-b pb-2">Model Details</h3> <div class="space-y-3 text-gray-700 dark:text-gray-300"> <div class="flex items-center gap-2"> <span class="text-xl">âš™ï¸</span> <span class="font-semibold">Task:</span> <span class="break-all">image-text-to-text</span> </div> <div class="flex items-center gap-2"> <span class="text-xl">â¤ï¸</span> <span class="font-semibold">Likes:</span> <span>1,593</span> </div> <div class="flex items-center gap-2"> <span class="text-xl">â¬‡ï¸</span> <span class="font-semibold">Downloads:</span> <span>364,646</span> </div> <div class="flex items-center gap-2"> <span class="text-xl">ğŸ“…</span> <span class="font-semibold">Updated:</span> <span>N/A</span> </div> </div> <div class="mt-6 border-t border-gray-200 dark:border-gray-600 pt-6"> <h4 class="font-bold text-lg mb-3 text-gray-900 dark:text-white">Sources & Repositories</h4> <ul class="space-y-2"> <li> <a href="https://huggingface.co/docling-project/SmolDocling-256M-preview" target="_blank" rel="noopener noreferrer" class="text-blue-500 hover:text-blue-600 dark:hover:text-blue-400 hover:underline transition-colors break-all flex items-center gap-1"> <span>Hugging Face</span> <span class="ml-1 text-sm">&rarr;</span> </a> </li> </ul> </div> </div> </aside> </div> </article> </div> <script>(function(){const allTags = ["transformers","onnx","safetensors","idefics3","image-to-text","image-text-to-text","conversational","en","dataset:ds4sd/SynthCodeNet","dataset:ds4sd/SynthFormulaNet","dataset:ds4sd/SynthChartNet","dataset:HuggingFaceM4/DoclingMatix","arxiv:2503.11576","arxiv:2305.03393","base_model:HuggingFaceTB/SmolVLM-256M-Instruct","base_model:quantized:HuggingFaceTB/SmolVLM-256M-Instruct","license:cdla-permissive-2.0","endpoints_compatible","region:us"];
const hasMoreTags = true;

Â  Â  // Tab switching logic
Â  Â  const tabButtons = document.querySelectorAll('.tab-button');
Â  Â  const tabPanels = document.querySelectorAll('.tab-panel');
Â  Â  const tabIdMap = {
Â  Â  Â  'tab-btn-files': 'tab-files',
Â  Â  Â  'tab-btn-community': 'tab-community',
Â  Â  };

Â  Â  tabButtons.forEach(button => {
Â  Â  Â  button.addEventListener('click', () => {
Â  Â  Â  Â  // Update active tab
Â  Â  Â  Â  tabButtons.forEach(t => t.classList.replace('active-tab', 'inactive-tab'));
Â  Â  Â  Â  button.classList.replace('inactive-tab', 'active-tab');

Â  Â  Â  Â  // Get the target panel ID
Â  Â  Â  Â  const targetPanelId = tabIdMap[button.id];

Â  Â  Â  Â  // Update panel visibility
Â  Â  Â  Â  tabPanels.forEach(p => p.classList.add('hidden'));
Â  Â  Â  Â  if (targetPanelId) {
Â  Â  Â  Â  Â  document.getElementById(targetPanelId)?.classList.remove('hidden');
Â  Â  Â  Â  }
Â  Â  Â  });
Â  Â  });


Â  Â  // "Show all tags" button logic
Â  Â  if (hasMoreTags) {
Â  Â  Â  const showAllBtn = document.getElementById('show-all-tags');
Â  Â  Â  const tagsContainer = document.getElementById('tags-container');

Â  Â  Â  if (showAllBtn && tagsContainer) {
Â  Â  Â  Â  showAllBtn.addEventListener('click', () => {
Â  Â  Â  Â  Â  const remainingTags = allTags.slice(15);
Â  Â  Â  Â  Â  const tagsHtml = remainingTags.map(tag =>Â 
Â  Â  Â  Â  Â  Â  `<span class="bg-blue-100 text-blue-800 text-sm font-medium px-3 py-1 rounded-full dark:bg-blue-900 dark:text-blue-300 transition-colors">${tag}</span>`
Â  Â  Â  Â  Â  ).join('');
Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  const tempDiv = document.createElement('div');
Â  Â  Â  Â  Â  tempDiv.innerHTML = tagsHtml;

Â  Â  Â  Â  Â  tagsContainer.append(...tempDiv.childNodes);
Â  Â  Â  Â  Â  showAllBtn.remove();
Â  Â  Â  Â  });
Â  Â  Â  }
Â  Â  }
Â  })();</script>  </main> <footer class="bg-gray-800 text-gray-300 py-8 mt-16" data-astro-cid-37fxchfa> <div class="container mx-auto px-4 text-center" data-astro-cid-37fxchfa> <div class="flex justify-center gap-4 mb-4" data-astro-cid-37fxchfa> <a href="/about" class="hover:underline" data-astro-cid-37fxchfa>About</a> <a href="/compliance" class="hover:underline" data-astro-cid-37fxchfa>Compliance</a> </div> <p class="text-sm mt-2" data-astro-cid-37fxchfa>&copy; 2025 Free AI Tools. An open-source project to index the world of AI.</p> <a href="mailto:compliance@free2aitools.com" class="text-sm text-gray-400 hover:underline mt-1 block" data-astro-cid-37fxchfa>compliance@free2aitools.com</a> </div> </footer> <script type="module">const e=document.getElementById("header-search");e.addEventListener("keydown",n=>{if(n.key==="Enter"){const o=e.value;window.location.href=`/explore?q=${encodeURIComponent(o)}`}});</script> </body> </html>