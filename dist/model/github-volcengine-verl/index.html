<!DOCTYPE html><html lang="en"> <head><!-- Google Tag Manager --><script type="module">(function(e,n,r,t,m){e[t]=e[t]||[],e[t].push({"gtm.start":new Date().getTime(),event:"gtm.js"});var g=n.getElementsByTagName(r)[0],a=n.createElement(r),s="";a.async=!0,a.src="https://www.googletagmanager.com/gtm.js?id="+m+s,g.parentNode.insertBefore(a,g)})(window,document,"script","dataLayer","GTM-58C2CQ8G");</script><!-- End Google Tag Manager --><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><!-- Open Graph Meta Tags --><meta property="og:type" content="website"><meta property="og:url" content="https://free2aitools.com/model/github-volcengine-verl/"><meta property="og:title" content="verl - AI Model Details"><meta property="og:description" content="verl: Volcano Engine Reinforcement Learning for LLMs"><meta property="og:image" content="https://free2aitools.com/og-image.jpg"><!-- Twitter Card Meta Tags --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://free2aitools.com/model/github-volcengine-verl/"><meta property="twitter:title" content="verl - AI Model Details"><meta property="twitter:description" content="verl: Volcano Engine Reinforcement Learning for LLMs"><meta property="twitter:image" content="https://free2aitools.com/og-image.jpg"><meta name="description" content="verl: Volcano Engine Reinforcement Learning for LLMs"><title>verl - AI Model Details</title><link rel="canonical" href="https://free2aitools.com/model/github-volcengine-verl/"><!-- Google AdSense --><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2292826803755214" crossorigin="anonymous"></script><link rel="stylesheet" href="/_astro/about.C4wyk6Sp.css"></head> <body class="bg-gray-50 text-gray-800 font-sans antialiased flex flex-col min-h-screen"> <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-58C2CQ8G" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) --> <header class="sticky top-0 z-30 w-full backdrop-blur flex-none transition-colors duration-500 lg:border-b lg:border-gray-200 bg-white/80"> <nav class="container mx-auto px-4 sm:px-6 lg:px-8"> <div class="flex items-center justify-between h-16"> <!-- Site Logo --> <a href="/" class="flex items-center gap-2 text-xl sm:text-2xl font-bold text-gray-900"> <span>Free AI Tools</span> </a> <!-- Right-side items --> <div class="flex items-center gap-4"> <!-- Desktop Navigation --> <div class="hidden md:flex items-center gap-6"> <a href="/" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Explore Models </a><a href="/ranking" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Rankings </a><a href="/reports" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Reports </a><a href="/about" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> About </a> </div> <!-- Mobile Search & Menu Toggle --> <button data-collapse-toggle="navbar-default" type="button" class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200" aria-controls="navbar-default" aria-expanded="false"> <span class="sr-only">Open main menu</span> <svg class="w-5 h-5" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 17 14"><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 1h15M1 7h15M1 13h15"></path></svg> </button> </div> </div> <!-- Mobile Collapsible Menu --> <div class="hidden w-full md:hidden" id="navbar-default"> <ul class="font-medium flex flex-col p-4 md:p-0 mt-4 border border-gray-100 rounded-lg bg-gray-50"> <li> <a href="/" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Explore Models</a> </li><li> <a href="/ranking" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Rankings</a> </li><li> <a href="/reports" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Reports</a> </li><li> <a href="/about" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">About</a> </li> </ul> </div> </nav> </header> <main class="flex-grow">  <div class="container mx-auto px-4 py-12"> <div class="max-w-4xl mx-auto"> <!-- Header Section --> <header class="mb-8"> <div class="flex flex-col md:flex-row md:items-start md:justify-between gap-4"> <div> <h1 class="text-4xl md:text-5xl font-extrabold text-gray-900 dark:text-white mb-2">verl</h1> <p class="text-lg text-gray-500 dark:text-gray-400">by volcengine</p> </div>  </div> </header> <!-- Metadata Section --> <div class="mb-8 p-4 bg-gray-100 dark:bg-gray-800 rounded-lg"> <div class="grid grid-cols-2 md:grid-cols-4 gap-4 text-center"> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Likes</p> <p class="text-xl font-bold">16,182</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Downloads</p> <p class="text-xl font-bold">16,182</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Task</p> <p class="text-xl font-bold capitalize">tool</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Last Updated</p> <p class="text-xl font-bold">2025/11/20</p> </div> </div> </div> <!-- Tags and Sources --> <div class="mb-8 flex flex-wrap items-start gap-4"> <div class="flex-1"> <h3 class="text-lg font-semibold mb-2">Tags</h3> <div class="flex flex-wrap gap-2">  </div> </div> <div class="flex-shrink-0"> <h3 class="text-lg font-semibold mb-2">Sources</h3> <div class="flex flex-col gap-2"> <a href="https://github.com/volcengine/verl" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> üì¶ GitHub </a> </div> </div> </div> <!-- Source Specific Details --> <div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> üì¶ GitHub Details
</h3> <div class="grid grid-cols-2 md:grid-cols-3 gap-4 text-sm"> <div><strong>Language:</strong> Python</div> <div><strong>Stars:</strong> 16,182</div> <div><strong>Forks:</strong> 2,600</div> <div><strong>Open Issues:</strong> 1,450</div> <div><strong>License:</strong> Apache License 2.0</div> <div><a href="https://verl.readthedocs.io/en/latest/index.html" target="_blank" class="text-blue-600 dark:text-blue-400 hover:underline"><strong>Project Homepage ‚Üó</strong></a></div> </div>  </div> <!-- README Content --> <article class="model-readme-content py-8 border-t border-gray-200 dark:border-gray-700"> <div class="prose prose-lg dark:prose-invert max-w-none"> <div align="center">
 üëã Hi, everyone!
    verl is a RL training library initiated by <b>ByteDance Seed team</b> and maintained by the verl community.
    <br>
    <br>
</div>

<div align="center">

<p><a href="https://deepwiki.com/volcengine/verl"><img src="https://devin.ai/assets/deepwiki-badge.png" alt="Ask DeepWiki.com" style="height:20px;"></a>
<a href="https://github.com/volcengine/verl/stargazers"><img src="https://img.shields.io/github/stars/volcengine/verl" alt="GitHub Repo stars"></a>
<a href="https://twitter.com/verl_project"><img src="https://img.shields.io/twitter/follow/verl_project" alt="Twitter"></a>
<a href="https://join.slack.com/t/verl-project/shared_invite/zt-3c6mc2khw-v0lo6NfDPuFP6OnkrZwfqw"><img src="https://img.shields.io/badge/Slack-verl-blueviolet?logo=slack&amp"></a>
<a href="https://arxiv.org/pdf/2409.19256"><img src="https://img.shields.io/static/v1?label=EuroSys&message=Paper&color=red"></a>
<a href="https://verl.readthedocs.io/en/latest/"><img src="https://img.shields.io/badge/documentation-blue" alt="Documentation"></a>
<a href="https://raw.githubusercontent.com/eric-haibin-lin/verl-community/refs/heads/main/WeChat.JPG"><img src="https://img.shields.io/badge/ÂæÆ‰ø°-green?logo=wechat&amp"></a></p>
</div>

<p><img src="https://github.com/user-attachments/assets/c42e675e-497c-4508-8bb9-093ad4d1f216" alt="seed logo"></p>
<h1 style="text-align: center;">verl: Volcano Engine Reinforcement Learning for LLMs</h1>

<p>verl is a flexible, efficient and production-ready RL training library for large language models (LLMs).</p>
<p>verl is the open-source version of <strong><a href="https://arxiv.org/abs/2409.19256v2">HybridFlow: A Flexible and Efficient RLHF Framework</a></strong> paper.</p>
<p>verl is flexible and easy to use with:</p>
<ul>
<li><p><strong>Easy extension of diverse RL algorithms</strong>: The hybrid-controller programming model enables flexible representation and efficient execution of complex post-training dataflows. Build RL dataflows such as GRPO, PPO in a few lines of code.</p>
</li>
<li><p><strong>Seamless integration of existing LLM infra with modular APIs</strong>: Decouples computation and data dependencies, enabling seamless integration with existing LLM frameworks, such as FSDP, Megatron-LM, vLLM, SGLang, etc</p>
</li>
<li><p><strong>Flexible device mapping</strong>: Supports various placement of models onto different sets of GPUs for efficient resource utilization and scalability across different cluster sizes.</p>
</li>
<li><p>Ready integration with popular HuggingFace models</p>
</li>
</ul>
<p>verl is fast with:</p>
<ul>
<li><p><strong>State-of-the-art throughput</strong>: SOTA LLM training and inference engine integrations and SOTA RL throughput.</p>
</li>
<li><p><strong>Efficient actor model resharding with 3D-HybridEngine</strong>: Eliminates memory redundancy and significantly reduces communication overhead during transitions between training and generation phases.</p>
</li>
</ul>
</p>

<h2>News</h2>
<ul>
<li>[2025/10] verl is presented in the <a href="https://pytorch.org/event/pytorch-conference-2025/">PyTorch Conference 2025</a>.</li>
<li>[2025/08] verl is presented in the <a href="https://www.youtube.com/watch?v=Vd79NmmqY3Q&t=2s">PyTorch Expert Exchange Webinar</a>. <a href="https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl_talk_pytorch_2025_08.pdf">Slides</a> available.</li>
<li>[2025/07] The <a href="https://arxiv.org/pdf/2504.11536">ReTool</a> recipe is fully open sourced. <a href="https://www.notion.so/verl-reTool-recipe-Using-multi-round-conversations-and-code-sandboxing-to-improve-the-math-of-large-23a8b5b7feba80b386b2e5b5e3c1cde0">Blog</a></li>
<li>[2025/07] The first verl meetup will be held at ICML Vancouver on July 16th! Please <a href="https://lu.ma/0ek2nyao">join us</a> if you are at ICML! (onsite only)</li>
<li>[2025/06] verl with Megatron backend enables large MoE models such as <a href="https://verl.readthedocs.io/en/latest/perf/dpsk.html">DeepSeek-671B and Qwen3-235B</a>.</li>
<li>[2025/03] <a href="https://dapo-sia.github.io/">DAPO</a> is the open-sourced SOTA RL algorithm that achieves 50 points on AIME 2024 based on the Qwen2.5-32B pre-trained model, surpassing the previous SOTA achieved by DeepSeek&#39;s GRPO (DeepSeek-R1-Zero-Qwen-32B). DAPO&#39;s training is fully powered by verl and the reproduction code is available in <code>recipe/dapo</code> now.<details><summary> more... </summary>
<ul>
<li>[2025/04] [Seed-Thinking-v1.5](https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf) tech report is released! Trained with verl, Seed-Thinking-v1.5 achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond reasoning tasks, the method demonstrates notable generalization across diverse domains.</li>
<li>[2025/07] verl keynote at [AWS AI Hours Singapore](https://pages.awscloud.com/aws-ai-hours-sg.html#agenda) on 7/8, verl & verl-agent project updates at [Agent for SWE meetup](https://lu.ma/e498qhsi) by LF AI & Data Singapore on 7/11.</li>
<li>[2025/06] verl team will provide latest project updates at [PyTorch Day China](https://www.lfasiallc.com/pytorch-day-china/) on June 7th. Meet our dev team in Beijing!</li>
<li> [2025/04] [VAPO](https://arxiv.org/pdf/2504.05118) (value-based augmented PPO) paper covers our latest RL method for reasoning models. Trained from Qwen-32B-base model, VAPO achieves 60.4 on AIME 2024, outperforming DAPO-32B.</li>
<li>[2025/05] [PF-PPO](https://arxiv.org/abs/2409.06957), accepted to ICML 2025, is now supported in verl! PF-PPO enhances policy learning efficiency and robustness by filtering potentially noisy reward signals and reusing high-quality experiences via a replay buffer.</li>
<li>[2025/04] We will give a tutorial about latest post-training techniques and programming guide for verl at [ICLR 2025 Expo](https://iclr.cc/virtual/2025/calendar?filter_events=Expo+Talk+Panel&filter_rooms=), [SCI-FM workshop](https://open-foundation-model.github.io/) and [LMSys afterparty](https://lu.ma/d23nyynm). Talk materials available [here](https://github.com/eric-haibin-lin/verl-community/tree/main/iclr25). </li>
<li>[2025/03] verl v0.3.0.post1 is released! See [release note](https://github.com/volcengine/verl/releases/) for details. It achieves [~1.4x speedup](https://tongyx361.github.io/blogs/posts/verl-intro/#/verl-flexible-and-efficient-rl-for-llms) compared to prev versions.</li>
<li>[2025/05] verl will be presented at [A2M Shanghai](https://a2m.msup.com.cn/home/?aid=4488&city=shanghai) on 5/16 - 5/17.</li>
<li>[2025/05] verl will be presented at [GOSIM x PyTorch Day 2025](https://paris2025.gosim.org/). See you in Paris! </li>
<li>[2025/03] We introduced the programming model of verl at the [vLLM Beijing Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg) and [verl intro and updates](https://github.com/eric-haibin-lin/verl-community/blob/main/slides/verl-lmsys-meetup.pdf) at the [SGLang-LMSYS Org Meetup](https://lu.ma/ntjrr7ig) in Sunnyvale mid-March.</li>
<li>[2025/03] We will present verl(HybridFlow) at EuroSys 2025. See you in Rotterdam!</li>
<li>[2025/02] verl v0.2.0.post2 is released!</li>
<li>[2025/02] We presented verl in the <a href="https://lu.ma/ji7atxux">Bytedance/NVIDIA/Anyscale Ray Meetup</a>. See you in San Jose!</li>
<li>[2025/01] [Doubao-1.5-pro](https://team.doubao.com/zh/special/doubao_1_5_pro) is released with SOTA-level performance on LLM & VLM. The RL scaling preview model is trained using verl, reaching OpenAI O1-level performance on math benchmarks (70.0 pass@1 on AIME).</li>
<li>[2024/12] verl is presented at Ray Forward 2024. Slides available <a href="https://github.com/eric-haibin-lin/verl-community/blob/main/slides/Ray_Forward_2024_%E5%B7%AB%E9%94%A1%E6%96%8C.pdf">here</a></li>
<li>[2024/12] The team presented <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">Post-training LLMs: From Algorithms to Infrastructure</a> at NeurIPS 2024. <a href="https://github.com/eric-haibin-lin/verl-data/tree/neurips">Slides</a> and <a href="https://neurips.cc/Expo/Conferences/2024/workshop/100677">video</a> available.</li>
<li>[2024/10] verl is presented at Ray Summit. <a href="https://www.youtube.com/watch?v=MrhMcXkXvJU&list=PLzTswPQNepXntmT8jr9WaNfqQ60QwW7-U&index=37">Youtube video</a> available.</li>
<li>[2024/08] HybridFlow (verl) is accepted to EuroSys 2025.</li>
</ul>
</details></li>
</ul>
<h2>Key Features</h2>
<ul>
<li><strong>FSDP</strong>, <strong>FSDP2</strong> and <strong>Megatron-LM</strong> for training.</li>
<li><strong>vLLM</strong>, <strong>SGLang</strong> and <strong>HF Transformers</strong> for rollout generation.</li>
<li>Compatible with Hugging Face Transformers and Modelscope Hub: <a href="https://github.com/volcengine/verl/blob/main/examples/grpo_trainer/run_qwen3-8b.sh">Qwen-3</a>, Qwen-2.5, Llama3.1, Gemma2, DeepSeek-LLM, etc</li>
<li>Supervised fine-tuning.</li>
<li>Reinforcement learning with <a href="examples/ppo_trainer/">PPO</a>, <a href="examples/grpo_trainer/">GRPO</a>, <a href="recipe/gspo/">GSPO</a>, <a href="examples/remax_trainer/">ReMax</a>, <a href="https://verl.readthedocs.io/en/latest/examples/config.html#algorithm">REINFORCE++</a>, <a href="examples/rloo_trainer/">RLOO</a>, <a href="recipe/prime/">PRIME</a>, <a href="recipe/dapo/">DAPO</a>, <a href="recipe/drgrpo">DrGRPO</a>, <a href="recipe/entropy">KL_Cov &amp; Clip_Cov</a> etc.<ul>
<li>Support model-based reward and function-based reward (verifiable reward) for math, <a href="https://github.com/volcengine/verl/tree/main/recipe/dapo">coding</a>, etc</li>
<li>Support vision-language models (VLMs) and <a href="examples/grpo_trainer/run_qwen2_5_vl-7b.sh">multi-modal RL</a> with Qwen2.5-vl, Kimi-VL</li>
<li><a href="https://github.com/volcengine/verl/tree/main/examples/sglang_multiturn">Multi-turn with tool calling</a></li>
</ul>
</li>
<li>LLM alignment recipes such as <a href="https://github.com/volcengine/verl/tree/main/recipe/sppo">Self-play preference optimization (SPPO)</a></li>
<li>Flash attention 2, <a href="examples/ppo_trainer/run_qwen2-7b_seq_balance.sh">sequence packing</a>, <a href="examples/ppo_trainer/run_deepseek7b_llm_sp2.sh">sequence parallelism</a> support via DeepSpeed Ulysses, <a href="examples/sft/gsm8k/run_qwen_05_peft.sh">LoRA</a>, <a href="examples/sft/gsm8k/run_qwen_05_sp2_liger.sh">Liger-kernel</a>.</li>
<li>Scales up to 671B models and hundreds of GPUs with <a href="https://github.com/volcengine/verl/pull/1467">expert parallelism</a></li>
<li>Multi-gpu <a href="https://verl.readthedocs.io/en/latest/advance/ppo_lora.html">LoRA RL</a> support to save memory.</li>
<li>Experiment tracking with wandb, swanlab, mlflow and tensorboard.</li>
</ul>
<h2>Upcoming Features and Changes</h2>
<ul>
<li>Q3 Roadmap <a href="https://github.com/volcengine/verl/issues/2388">https://github.com/volcengine/verl/issues/2388</a></li>
<li>DeepSeek 671b optimizations with Megatron <a href="https://github.com/volcengine/verl/issues/1033">https://github.com/volcengine/verl/issues/1033</a></li>
<li>Multi-turn rollout and tools using optimizations <a href="https://github.com/volcengine/verl/issues/1882">https://github.com/volcengine/verl/issues/1882</a></li>
<li><a href="https://github.com/volcengine/verl/tree/main/verl/experimental/agent_loop">Agent integration</a></li>
<li>Async and off-policy architecture <a href="https://github.com/volcengine/verl/pull/2231">https://github.com/volcengine/verl/pull/2231</a></li>
<li>List of breaking changes since v0.4 <a href="https://github.com/volcengine/verl/discussions/2270">https://github.com/volcengine/verl/discussions/2270</a></li>
</ul>
<h2>Getting Started</h2>
<p><a href="https://verl.readthedocs.io/en/latest/index.html"><b>Documentation</b></a></p>
<p><strong>Quickstart:</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/start/install.html">Installation</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/start/quickstart.html">Quickstart</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/hybrid_flow.html">Programming Guide</a> &amp; <a href="https://hcqnc.xetlk.com/sl/3vACOK">Tech Talk</a> (in Chinese)</li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/ppo.html">PPO in verl</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/algo/grpo.html">GRPO in verl</a></li>
</ul>
<p><strong>Running a PPO example step-by-step:</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/preparation/prepare_data.html">Prepare Data for Post-Training</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/preparation/reward_function.html">Implement Reward Function for Dataset</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/ppo_code_architecture.html">PPO Example Architecture</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/config.html">Config Explanation</a></li>
</ul>
<p><strong>Reproducible algorithm baselines:</strong></p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/algo/baseline.html">RL performance on coding, math</a></li>
</ul>
<p><strong>For code explanation and advance usage (extension):</strong></p>
<ul>
<li><p>PPO Trainer and Workers</p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/workers/ray_trainer.html">PPO Ray Trainer</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/workers/fsdp_workers.html">PyTorch FSDP Backend</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/index.html">Megatron-LM Backend</a></li>
</ul>
</li>
<li><p>Advanced Usage and Extension</p>
<ul>
<li><a href="https://verl.readthedocs.io/en/latest/advance/fsdp_extension.html">Add Models with the FSDP Backend</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/megatron_extension.html">Add Models with the Megatron-LM Backend</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/multiturn.html">Multi-turn Rollout Support</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/sglang_multiturn/search_tool_example.html">Search Tool Integration</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/examples/sandbox_fusion_example.html">Sandbox Fusion Integration</a></li>
<li><a href="https://github.com/volcengine/verl/tree/main/examples/split_placement">Deployment using Separate GPU Resources</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/dpo_extension.html">Extend to Other RL(HF) algorithms</a></li>
<li><a href="https://verl.readthedocs.io/en/latest/advance/placement.html">Ray API design tutorial</a></li>
</ul>
</li>
</ul>
<p><strong>Blogs from the community</strong></p>
<ul>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/fast_tokenization/multiturn_tokenization_and_masking.md">When Reasoning Models Break Tokenization: The Hidden Complexity of Multiturn Training</a></li>
<li><a href="https://medium.com/@kaige.yang0110/run-verl-on-sagemaker-using-4x8-l40s-gpus-8e6d5c3c61d3">verl deployment on AWS SageMaker</a></li>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/code-walk-through/readme_EN.md">verl x SGLang Multi-turn Code Walkthrough</a></li>
<li><a href="https://hebiao064.github.io/rl-memory-management">Optimizing SGLang Memory Usage in verl</a></li>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/multi-turn/verl-multiturn-rollout-Release.md">SGLang, verl, OpenBMB and Tsinghua University: Pioneering End-to-End Multi-Turn RLHF</a></li>
<li><a href="https://rocm.blogs.amd.com/artificial-intelligence/verl-large-scale/README.html">Reinforcement Learning from Human Feedback on AMD GPUs with verl and ROCm Integration</a></li>
<li><a href="https://mp.weixin.qq.com/s/7nbqxk4knMGd-hQE9ls2tA">veMLP x verl ÔºöÁé©ËΩ¨Âº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉ</a></li>
<li><a href="https://www.volcengine.com/docs/6459/1463942">‰ΩøÁî® verl ËøõË°å GRPO ÂàÜÂ∏ÉÂºèÂº∫ÂåñÂ≠¶‰π†ËÆ≠ÁªÉÊúÄ‰Ω≥ÂÆûË∑µ</a></li>
<li><a href="https://github.com/zhaochenyang20/Awesome-ML-SYS-Tutorial/blob/main/rlhf/verl/readme.md">HybridFlow verl ÂéüÊñáÊµÖÊûê</a></li>
<li><a href="https://team.doubao.com/en/blog/%E6%9C%80%E9%AB%98%E6%8F%90%E5%8D%8720%E5%80%8D%E5%90%9E%E5%90%90%E9%87%8F-%E8%B1%86%E5%8C%85%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9B%A2%E9%98%9F%E5%8F%91%E5%B8%83%E5%85%A8%E6%96%B0-rlhf-%E6%A1%86%E6%9E%B6-%E7%8E%B0%E5%B7%B2%E5%BC%80%E6%BA%90">ÊúÄÈ´òÊèêÂçá 20 ÂÄçÂêûÂêêÈáèÔºÅË±ÜÂåÖÂ§ßÊ®°ÂûãÂõ¢ÈòüÂèëÂ∏ÉÂÖ®Êñ∞ RLHF Ê°ÜÊû∂ÔºåÁé∞Â∑≤ÂºÄÊ∫êÔºÅ</a></li>
</ul>
<h2>Performance Tuning Guide</h2>
<p>The performance is essential for on-policy RL algorithm. We have written a detailed <a href="https://verl.readthedocs.io/en/latest/perf/perf_tuning.html">performance tuning guide</a> to help you optimize performance.</p>
<h2>Upgrade to vLLM &gt;= v0.8.2</h2>
<p>verl now supports vLLM&gt;=0.8.2 when using FSDP as the training backend. Please refer to <a href="https://github.com/volcengine/verl/blob/main/docs/README_vllm0.8.md">this document</a> for the installation guide and more information. Please avoid vllm 0.7.x, which contains bugs that may lead to OOMs and unexpected errors.</p>
<h2>Use Latest SGLang</h2>
<p>SGLang is fully supported with verl, and SGLang RL Group is working extensively on building unique features, including multi-turn agentic RL, VLM RLHF, server-based RL, and partial rollout. Please refer to <a href="https://verl.readthedocs.io/en/latest/workers/sglang_worker.html">this document</a> for the installation guide and more information.</p>
<h2>Upgrade to FSDP2</h2>
<p>verl is fully embracing FSDP2! FSDP2 is recommended by torch distributed team, providing better throughput and memory usage, and is composible with other features (e.g. torch.compile). To enable FSDP2, simply use verl main and set the following options:</p>
<pre><code>actor_rollout_ref.ref.strategy=fsdp2
actor_rollout_ref.actor.strategy=fsdp2
critic.strategy=fsdp2
reward_model.strategy=fsdp2
</code></pre>
<p>Furthermore, FSDP2 cpu offloading is compatible with gradient accumulation. You can turn it on to save memory with <code>actor_rollout_ref.actor.fsdp_config.offload_policy=True</code>. For more details, see <a href="https://github.com/volcengine/verl/pull/1026">https://github.com/volcengine/verl/pull/1026</a></p>
<h2>AMD Support (ROCm Kernel)</h2>
<p>verl now supports FSDP as the training engine (Megatron support coming soon) and both integrates with vLLM and SGLang as inference engines. Please refer to <a href="https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_build_dockerfile_page.rst">this document</a> for the installation guide and more information, and <a href="https://github.com/volcengine/verl/blob/main/docs/amd_tutorial/amd_vllm_page.rst">this document</a> for the vLLM performance tuning for ROCm.</p>
<h2>Citation and acknowledgement</h2>
<p>If you find the project helpful, please cite:</p>
<ul>
<li><a href="https://arxiv.org/abs/2409.19256v2">HybridFlow: A Flexible and Efficient RLHF Framework</a></li>
<li><a href="https://i.cs.hku.hk/~cwu/papers/gmsheng-NL2Code24.pdf">A Framework for Training Large Language Models for Code Generation via Proximal Policy Optimization</a></li>
</ul>
<pre><code class="language-bibtex">@article{sheng2024hybridflow,
  title   = {HybridFlow: A Flexible and Efficient RLHF Framework},
  author  = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2409.19256}
}
</code></pre>
<p>verl is inspired by the design of Nemo-Aligner, Deepspeed-chat and OpenRLHF. The project is adopted and contributed by Bytedance, Anyscale, LMSys.org, <a href="https://github.com/QwenLM/">Alibaba Qwen team</a>, Shanghai AI Lab, Tsinghua University, UC Berkeley, UCLA, UIUC, University of Hong Kong, ke.com, <a href="https://www.all-hands.dev/">All Hands AI</a>, <a href="http://modelbest.cn/">ModelBest</a>, JD AI Lab, Microsoft Research, <a href="https://www.stepfun.com/">StepFun</a>, Amazon, LinkedIn, Meituan, <a href="https://www.camel-ai.org/">Camel-AI</a>, <a href="https://github.com/OpenManus">OpenManus</a>, Xiaomi, NVIDIA research, <a href="https://www.baichuan-ai.com/home">Baichuan</a>, <a href="https://www.xiaohongshu.com/">RedNote</a>, <a href="https://www.swiss-ai.org/">SwissAI</a>, <a href="https://www.moonshot-ai.com/">Moonshot AI (Kimi)</a>, Baidu, Snowflake, Skywork.ai, JetBrains, <a href="https://www.iceswordlab.com">IceSword Lab</a>, and many more.</p>
<h2>Awesome work using verl</h2>
<ul>
<li><a href="https://github.com/Jiayi-Pan/TinyZero">TinyZero</a>: a reproduction of <strong>DeepSeek R1 Zero</strong> recipe for reasoning tasks <img src="https://img.shields.io/github/stars/Jiayi-Pan/TinyZero" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/NovaSky-AI/SkyThought">SkyThought</a>: RL training for Sky-T1-7B by NovaSky AI team. <img src="https://img.shields.io/github/stars/NovaSky-AI/SkyThought" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/hkust-nlp/simpleRL-reason">simpleRL-reason</a>: SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild <img src="https://img.shields.io/github/stars/hkust-nlp/simpleRL-reason" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/hiyouga/EasyR1">Easy-R1</a>: <strong>Multi-modal</strong> RL training framework <img src="https://img.shields.io/github/stars/hiyouga/EasyR1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/OpenManus/OpenManus-RL">OpenManus-RL</a>: LLM Agents RL tuning framework for multiple agent environments. <img src="https://img.shields.io/github/stars/OpenManus/OpenManus-RL" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/agentica-project/rllm">rllm</a>: async RL training with <a href="https://github.com/agentica-project/verl-pipeline">verl-pipeline</a> <img src="https://img.shields.io/github/stars/agentica-project/rllm" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/ZihanWang314/ragen">RAGEN</a>: a general-purpose reasoning <strong>agent</strong> training framework <img src="https://img.shields.io/github/stars/ZihanWang314/ragen" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/PeterGriffinJin/Search-R1">Search-R1</a>: RL with reasoning and <strong>searching (tool-call)</strong> interleaved LLMs <img src="https://img.shields.io/github/stars/PeterGriffinJin/Search-R1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/Agent-RL/ReSearch">ReSearch</a>: Learning to <strong>Re</strong>ason with <strong>Search</strong> for LLMs via Reinforcement Learning <img src="https://img.shields.io/github/stars/Agent-RL/ReSearch" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/SkyworkAI/Skywork-OR1">Skywork-OR1</a>: Skywork open reaonser series <img src="https://img.shields.io/github/stars/SkyworkAI/Skywork-OR1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/GAIR-NLP/ToRL">ToRL</a>: Scaling tool-integrated RL <img src="https://img.shields.io/github/stars/GAIR-NLP/ToRL" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/LeapLabTHU/Absolute-Zero-Reasoner">Absolute Zero Reasoner</a>: <a href="https://arxiv.org/abs/2505.03335">A no human curated data self-play framework for reasoning</a> <img src="https://img.shields.io/github/stars/LeapLabTHU/Absolute-Zero-Reasoner" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/langfengQ/verl-agent">verl-agent</a>: A scalable training framework for <strong>long-horizon LLM/VLM agents</strong>, along with a new algorithm <strong>GiGPO</strong> <img src="https://img.shields.io/github/stars/langfengQ/verl-agent" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/Simple-Efficient/RL-Factory">RL-Factory</a>: An easy and efficient RL post-training framework for Agentic Learning <img src="https://img.shields.io/github/stars/Simple-Efficient/RL-Factory" alt="GitHub Repo stars"></li>
<li><a href="https://retool-rl.github.io/">ReTool</a>: ReTool: reinforcement learning for strategic tool use in LLMs. Code release is in progress...</li>
<li><a href="https://github.com/TIGER-AI-Lab/verl-tool">verl-tool</a>: An unified and easy-to-extend tool-agent training framework based on verl<img src="https://img.shields.io/github/stars/TIGER-AI-Lab/verl-tool" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/PRIME-RL/PRIME">PRIME</a>: Process reinforcement through implicit rewards <img src="https://img.shields.io/github/stars/PRIME-RL/PRIME" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/BytedTsinghua-SIA/MemAgent">MemAgent</a>: MemAgent: Reshaping Long-Context LLM with Multi-Conv RL based Memory Agent <img src="https://img.shields.io/github/stars/BytedTsinghua-SIA/MemAgent" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/ChenxinAn-fdu/POLARIS">POLARIS</a>: A Post-training recipe for scaling RL on Advanced Reasoning models <img src="https://img.shields.io/github/stars/ChenxinAn-fdu/POLARIS" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/ritzz-ai/GUI-R1">GUI-R1</a>: <strong>GUI-R1</strong>: A Generalist R1-style Vision-Language Action Model For <strong>GUI Agents</strong> <img src="https://img.shields.io/github/stars/ritzz-ai/GUI-R1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/pat-jj/DeepRetrieval">DeepRetrieval</a>: RL Training of <strong>Search Agent</strong> with <strong>Search/Retrieval Outcome</strong> <img src="https://img.shields.io/github/stars/pat-jj/DeepRetrieval" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/ganler/code-r1">Code-R1</a>: Reproducing R1 for <strong>Code</strong> with Reliable Rewards <img src="https://img.shields.io/github/stars/ganler/code-r1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/GAIR-NLP/DeepResearcher">DeepResearcher</a>: Scaling deep research via reinforcement learning in real-world environments <img src="https://img.shields.io/github/stars/GAIR-NLP/DeepResearcher" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/RAGEN-AI/VAGEN">VAGEN</a>: Training VLM agents with multi-turn reinforcement learning <img src="https://img.shields.io/github/stars/RAGEN-AI/VAGEN" alt="GitHub Repo stars"></li>
<li><a href="https://arxiv.org/abs/2505.02387">RM-R1</a>: RL training of reasoning reward models <img src="https://img.shields.io/github/stars/RM-R1-UIUC/RM-R1" alt="GitHub Repo stars"></li>
<li><a href="https://arxiv.org/pdf/2504.14945">LUFFY</a>: Learning to Reason under Off-Policy Guidance<img src="https://img.shields.io/github/stars/ElliottYan/LUFFY" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/zwhe99/DeepMath">DeepMath</a>: DeepMath-103K data and series models for math reasoning<img src="https://img.shields.io/github/stars/zwhe99/DeepMath" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/ritzz-ai/PACS">PACS</a>: Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR <img src="https://img.shields.io/github/stars/ritzz-ai/PACS" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/PRIME-RL/Entropy-Mechanism-of-RL">Entropy Mechanism of RL</a>: The Entropy Mechanism of Reinforcement Learning for Large Language Model Reasoning<img src="https://img.shields.io/github/stars/PRIME-RL/Entropy-Mechanism-of-RL" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/channel-io/ch-tts-llasa-rl-grpo">LLaSA-TTS-GRPO</a>: TTS fine-tuning with GRPO optimization based on LLASA models <img src="https://img.shields.io/github/stars/channel-io/ch-tts-llasa-rl-grpo" alt="GitHub Repo stars"></li>
<li><a href="https://arxiv.org/abs/2409.06957">PF-PPO</a>: Policy Filtration for PPO based on the reliability of reward signals for more efficient and robust RLHF.</li>
<li><a href="https://github.com/gyhdog99/RACRO2">RACRO</a>: Build multi-modal reasoning models via decoupling it into query-conditioned captioning and text-only reasoning <img src="https://img.shields.io/github/stars/gyhdog99/RACRO2" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/microsoft/agent-lightning">Agent Lightning</a>: A flexible and extensible framework that enables seamless agent optimization for any existing agent framework. <img src="https://img.shields.io/github/stars/microsoft/agent-lightning" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/VTOOL-R1/vtool-r1">VTool-R1</a>: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use. <img src="https://img.shields.io/github/stars/VTOOL-R1/vtool-r1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/project-numina/kimina-prover-rl/tree/main/recipe/kimina_prover_rl">Kimina-Prover-RL</a>: Training pipeline for formal theorem proving, based on a paradigm inspired by DeepSeek-R1.</li>
<li><a href="https://github.com/YihongDong/RL-PLUS">RL-PLUS</a>: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization.</li>
<li><a href="https://github.com/microsoft/rStar">rStar2-Agent</a>: Using reinforcement learning with multi-step tool-calling for math tasks, rStar2-Agent-14B reaches frontier-level math reasoning in just 510 RL training steps <img src="https://img.shields.io/github/stars/microsoft/rStar" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/zli12321/Vision-SR1">Vision-SR1</a>: Self-Rewarding Vision-Language Model via Reasoning Decomposition <img src="https://img.shields.io/github/stars/zli12321/Vision-SR1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/PRIME-RL/SimpleVLA-RL">SimpleVLA-RL</a>: SimpleVLA-RL: A Simple yet Effective Vision-Language Action Model for Reinforcement Learning <img src="https://img.shields.io/github/stars/PRIME-RL/SimpleVLA-RL" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/Table-R1/Table-R1">Table-R1</a>: Table-R1: Inference-Time Scaling for Table Reasoning <img src="https://img.shields.io/github/stars/Table-R1/Table-R1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/CSfufu/Revisual-R1">Revisual-R1</a>: Revisual-R1: Advancing Multimodal Reasoning From Optimized Cold Start to Staged Reinforcement Learning <img src="https://img.shields.io/github/stars/CSfufu/Revisual-R1" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/shawn0728/ARES">ARES</a>: ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping <img src="https://img.shields.io/github/stars/shawn0728/ARES" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/sanxing-chen/meta-bandit-llm">Meta-Bandit-LLM</a>: Meta-Bandit-LLM: Long-horizon multiturn interactive training for meta-bandit agents <img src="https://img.shields.io/github/stars/sanxing-chen/meta-bandit-llm" alt="GitHub Repo stars"></li>
<li><a href="https://github.com/Pokee-AI/PokeeResearchOSS">PokeeResearch</a>: PokeeResearch: State-of-the-art 7B DeepResearch Agent that leverages web search and content reading capabilities to answer complex questions using the most up-to-date information available online. <img src="https://img.shields.io/github/stars/Pokee-AI/PokeeResearchOSS" alt="Github Repo Stars"></li>
</ul>
<p>and many more awesome work listed in <a href="recipe/README.md">recipe</a>.</p>
<h2>Contribution Guide</h2>
<p>See <a href="CONTRIBUTING.md">contributions guide</a></p>
<h2>About <a href="https://team.doubao.com/">ByteDance Seed Team</a></h2>
<p>Founded in 2023, ByteDance Seed Team is dedicated to crafting the industry&#39;s most advanced AI foundation models. The team aspires to become a world-class research team and make significant contributions to the advancement of science and society. You can get to know Bytedance Seed better through the following channelsüëá</p>
<div>
  <a href="https://team.doubao.com/">
    <img src="https://img.shields.io/badge/Website-%231e37ff?style=for-the-badge&logo=bytedance&logoColor=white"></a>
  <a href="https://github.com/user-attachments/assets/469535a8-42f2-4797-acdf-4f7a1d4a0c3e">
    <img src="https://img.shields.io/badge/WeChat-07C160?style=for-the-badge&logo=wechat&logoColor=white"></a>
 <a href="https://www.xiaohongshu.com/user/profile/668e7e15000000000303157d?xsec_token=ABl2-aqekpytY6A8TuxjrwnZskU-6BsMRE_ufQQaSAvjc%3D&xsec_source=pc_search">
    <img src="https://img.shields.io/badge/Xiaohongshu-%23FF2442?style=for-the-badge&logo=xiaohongshu&logoColor=white"></a>
  <a href="https://www.zhihu.com/org/dou-bao-da-mo-xing-tuan-dui/">
    <img src="https://img.shields.io/badge/zhihu-%230084FF?style=for-the-badge&logo=zhihu&logoColor=white"></a>

</div>
---

<p>We are HIRING! Send us an <a href="mailto:the.verl.project@gmail.com">email</a> if you are interested in internship/FTE opportunities in RL for agents.</p>
 </div> </article> <!-- Related Models Section -->  </div> </div>  </main> <footer class="bg-gray-800 text-gray-300 py-8 mt-16"> <div class="container mx-auto px-4 text-center"> <div class="flex justify-center gap-4 mb-4"> <a href="/about" class="hover:underline">About</a> <a href="/compliance" class="hover:underline">Compliance</a> </div> <p class="text-sm mt-2">
&copy; 2025 Free AI Tools. An open-source project to index the world of AI.
</p> <a href="mailto:compliance@free2aitools.com" class="text-sm text-gray-400 hover:underline mt-1 block">compliance@free2aitools.com</a> </div> </footer> </body></html>