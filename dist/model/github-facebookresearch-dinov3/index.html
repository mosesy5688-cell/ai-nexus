<!DOCTYPE html><html lang="en"> <head><!-- Google Tag Manager --><script type="module">(function(e,n,r,t,m){e[t]=e[t]||[],e[t].push({"gtm.start":new Date().getTime(),event:"gtm.js"});var g=n.getElementsByTagName(r)[0],a=n.createElement(r),s="";a.async=!0,a.src="https://www.googletagmanager.com/gtm.js?id="+m+s,g.parentNode.insertBefore(a,g)})(window,document,"script","dataLayer","GTM-58C2CQ8G");</script><!-- End Google Tag Manager --><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><!-- Open Graph Meta Tags --><meta property="og:type" content="website"><meta property="og:url" content="https://free2aitools.com/model/github-facebookresearch-dinov3/"><meta property="og:title" content="dinov3 - AI Model Details"><meta property="og:description" content="Reference PyTorch implementation and models for DINOv3"><meta property="og:image" content="https://free2aitools.com/og-image.jpg"><!-- Twitter Card Meta Tags --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://free2aitools.com/model/github-facebookresearch-dinov3/"><meta property="twitter:title" content="dinov3 - AI Model Details"><meta property="twitter:description" content="Reference PyTorch implementation and models for DINOv3"><meta property="twitter:image" content="https://free2aitools.com/og-image.jpg"><meta name="description" content="Reference PyTorch implementation and models for DINOv3"><title>dinov3 - AI Model Details</title><link rel="canonical" href="https://free2aitools.com/model/github-facebookresearch-dinov3/"><!-- Google AdSense --><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2292826803755214" crossorigin="anonymous"></script><link rel="stylesheet" href="/_astro/about.C4wyk6Sp.css"></head> <body class="bg-gray-50 text-gray-800 font-sans antialiased flex flex-col min-h-screen"> <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-58C2CQ8G" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) --> <header class="sticky top-0 z-30 w-full backdrop-blur flex-none transition-colors duration-500 lg:border-b lg:border-gray-200 bg-white/80"> <nav class="container mx-auto px-4 sm:px-6 lg:px-8"> <div class="flex items-center justify-between h-16"> <!-- Site Logo --> <a href="/" class="flex items-center gap-2 text-xl sm:text-2xl font-bold text-gray-900"> <span>Free AI Tools</span> </a> <!-- Right-side items --> <div class="flex items-center gap-4"> <!-- Desktop Navigation --> <div class="hidden md:flex items-center gap-6"> <a href="/" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Explore Models </a><a href="/ranking" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Rankings </a><a href="/reports" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Reports </a><a href="/about" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> About </a> </div> <!-- Mobile Search & Menu Toggle --> <button data-collapse-toggle="navbar-default" type="button" class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200" aria-controls="navbar-default" aria-expanded="false"> <span class="sr-only">Open main menu</span> <svg class="w-5 h-5" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 17 14"><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 1h15M1 7h15M1 13h15"></path></svg> </button> </div> </div> <!-- Mobile Collapsible Menu --> <div class="hidden w-full md:hidden" id="navbar-default"> <ul class="font-medium flex flex-col p-4 md:p-0 mt-4 border border-gray-100 rounded-lg bg-gray-50"> <li> <a href="/" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Explore Models</a> </li><li> <a href="/ranking" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Rankings</a> </li><li> <a href="/reports" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Reports</a> </li><li> <a href="/about" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">About</a> </li> </ul> </div> </nav> </header> <main class="flex-grow">  <div class="container mx-auto px-4 py-12"> <div class="max-w-4xl mx-auto"> <!-- Header Section --> <header class="mb-8"> <div class="flex flex-col md:flex-row md:items-start md:justify-between gap-4"> <div> <h1 class="text-4xl md:text-5xl font-extrabold text-gray-900 dark:text-white mb-2">dinov3</h1> <p class="text-lg text-gray-500 dark:text-gray-400">by facebookresearch</p> </div>  </div> </header> <!-- Metadata Section --> <div class="mb-8 p-4 bg-gray-100 dark:bg-gray-800 rounded-lg"> <div class="grid grid-cols-2 md:grid-cols-4 gap-4 text-center"> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Likes</p> <p class="text-xl font-bold">8,411</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Downloads</p> <p class="text-xl font-bold">8,411</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Task</p> <p class="text-xl font-bold capitalize">tool</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Last Updated</p> <p class="text-xl font-bold">2025/11/20</p> </div> </div> </div> <!-- Tags and Sources --> <div class="mb-8 flex flex-wrap items-start gap-4"> <div class="flex-1"> <h3 class="text-lg font-semibold mb-2">Tags</h3> <div class="flex flex-wrap gap-2">  </div> </div> <div class="flex-shrink-0"> <h3 class="text-lg font-semibold mb-2">Sources</h3> <div class="flex flex-col gap-2"> <a href="https://github.com/facebookresearch/dinov3" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ðŸ“¦ GitHub </a> </div> </div> </div> <!-- Source Specific Details --> <div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ðŸ“¦ GitHub Details
</h3> <div class="grid grid-cols-2 md:grid-cols-3 gap-4 text-sm"> <div><strong>Language:</strong> Jupyter Notebook</div> <div><strong>Stars:</strong> 8,411</div> <div><strong>Forks:</strong> 589</div> <div><strong>Open Issues:</strong> 130</div> <div><strong>License:</strong> Other</div>  </div>  </div> <!-- README Content --> <article class="model-readme-content py-8 border-t border-gray-200 dark:border-gray-700"> <div class="prose prose-lg dark:prose-invert max-w-none"> <p>ðŸ†• [2025-09-17] :fire: DINOv3 backbones are now supported by the <a href="https://github.com/huggingface/pytorch-image-models/">PyTorch Image Models / timm</a> library starting with version <a href="https://github.com/huggingface/pytorch-image-models/releases/tag/v1.0.20">1.0.20</a></p>
<p>[2025-08-29] DINOv3 backbones are <a href="https://huggingface.co/docs/transformers/model_doc/dinov3">supported</a> by released versions of the Hugging Face <a href="https://huggingface.co/docs/transformers/index">Transformers</a> library starting with version <a href="https://github.com/huggingface/transformers/releases/tag/v4.56.0">4.56.0</a></p>
<p>[2025-08-14] DINOv3 backbones are now available in <a href="https://huggingface.co/collections/facebook/dinov3-68924841bd6b561778e31009">Hugging Face Hub</a> and <a href="https://huggingface.co/docs/transformers/model_doc/dinov3">supported</a> by the <a href="https://github.com/huggingface/transformers/">development</a> version of the Hugging Face <a href="https://huggingface.co/docs/transformers/index">Transformers</a> library</p>
<h1>DINOv3 ðŸ¦–ðŸ¦–ðŸ¦–</h1>
<p><strong><a href="https://ai.meta.com/research/">Meta AI Research, FAIR</a></strong></p>
<p>Oriane SimÃ©oni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, <br/>
Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, MichaÃ«l Ramamonjisoa, <br/>
Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, <br/>
TimothÃ©e Darcet, ThÃ©o Moutakanni, Leonel Sentana, Claire Roberts, <br/>
Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, <br/>
Julien Mairal, HervÃ© JÃ©gou, Patrick Labatut, Piotr Bojanowski</p>
<p>[ :scroll: <a href="https://arxiv.org/abs/2508.10104"><code>Paper</code></a>] [ :newspaper: <a href="https://ai.meta.com/blog/dinov3-self-supervised-vision-model/"><code>Blog</code></a>] [ :globe_with_meridians: <a href="https://ai.meta.com/dinov3/"><code>Website</code></a>] [ :book: <a href="#citing-dinov3"><code>BibTeX</code></a>]</p>
<p>Reference PyTorch implementation and models for DINOv3. For details, see the <strong><a href="https://arxiv.org/abs/2508.10104">DINOv3</a></strong> paper.</p>
<h2>Overview</h2>
<div align="center">
  <img width="1364" height="1024" alt="market" src="https://github.com/user-attachments/assets/1411f491-988e-49cb-95ae-d03fe6e3c268" />

<p>  <i></em><b>High-resolution dense features.</b><br/>We visualize the cosine similarity maps obtained with DINOv3 output features<br/> between the patches marked with a red cross and all other patches.</i></p>
</div>

<br/>

<p>An extended family of versatile vision foundation models producing high-quality dense features and achieving outstanding performance on various vision tasks including outperforming the specialized state of the art across a broad range of settings, without fine-tuning</p>
<h2>Pretrained models</h2>
<p>:information_source: Please follow the link provided below to get access to all the model weights: once accepted, an e-mail will be sent with the complete list of URLs pointing to all the available model weights (both backbones and adapters). These URLs can then be used to either:</p>
<ul>
<li>download the model or adapter weights to a local filesystem and point <code>torch.hub.load()</code> to these local weights via the <code>weights</code> or <code>backbone_weights</code> parameters, or</li>
<li>directly invoke <code>torch.hub.load()</code> to download and load a backbone or an adapter from its URL via also the <code>weights</code> or <code>backbone_weights</code> parameters.</li>
</ul>
<p>See the example code snippets below.</p>
<p>:warning: Please use <code>wget</code> instead of a web browser to download the weights.</p>
<p>ViT models pretrained on web dataset (LVD-1689M):</p>
<table style="margin: auto">
  <thead>
    <tr>
      <th>Model</th>
      <th>Parameters</th>
      <th>Pretraining<br/>Dataset</th>
      <th>Download</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ViT-S/16 distilled </td>
      <td align="right">21M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a></td>
    </tr>
    <tr>
      <td>ViT-S+/16 distilled</td>
      <td align="right">29M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a></td>
    </tr>
    <tr>
      <td>ViT-B/16 distilled</td>
      <td align="right">86M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a></td>
    </tr>
    <tr>
      <td>ViT-L/16 distilled</td>
      <td align="right">300M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a></td>
    </tr>
    <tr>
      <td>ViT-H+/16 distilled</td>
      <td align="right">840M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a></td>
    </tr>
    <tr>
      <td>ViT-7B/16</td>
      <td align="right">6,716M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a></td>
    </tr>
  </tbody>
</table>

<p>ConvNeXt models pretrained on web dataset (LVD-1689M):</p>
<table style="margin: auto">
  <thead>
    <tr>
      <th>Model</th>
      <th>Parameters</th>
      <th>Pretraining<br/>Dataset</th>
      <th>Download</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ConvNeXt Tiny</td>
      <td align="right">29M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a></td>
    </tr>
    <tr>
      <td>ConvNeXt Small</td>
      <td align="right">50M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a></td>
    </tr>
    <tr>
      <td>ConvNeXt Base</td>
      <td align="right">89M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a></td>
    </tr>
    <tr>
      <td>ConvNeXt Large</td>
      <td align="right">198M</td>
      <td align="center">LVD-1689M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a></td>
    </tr>
  </tbody>
</table>

<p>ViT models pretrained on satellite dataset (SAT-493M):</p>
<table style="margin: auto">
  <thead>
    <tr>
      <th>Model</th>
      <th>Parameters</th>
      <th>Pretraining<br/>Dataset</th>
      <th>Download</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ViT-L/16 distilled</td>
      <td align="right">300M</td>
      <td align="center">SAT-493M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a></td>
    </tr>
    <tr>
      <td>ViT-7B/16</td>
      <td align="right">6,716M</td>
      <td align="center">SAT-493M</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a></td>
    </tr>
  </tbody>
</table>


<h3>Pretrained backbones (via PyTorch <a href="https://docs.pytorch.org/docs/stable/hub.html">Hub</a>)</h3>
<p>Please follow the instructions <a href="https://pytorch.org/get-started/locally/">here</a> to install PyTorch (the only required dependency for loading the model). Installing PyTorch with CUDA support is strongly recommended.</p>
<pre><code class="language-python">import torch

REPO_DIR = &lt;PATH/TO/A/LOCAL/DIRECTORY/WHERE/THE/DINOV3/REPO/WAS/CLONED&gt;

# DINOv3 ViT models pretrained on web images
dinov3_vits16 = torch.hub.load(REPO_DIR, &#39;dinov3_vits16&#39;, source=&#39;local&#39;, weights=&lt;CHECKPOINT/URL/OR/PATH&gt;)
dinov3_vits16plus = torch.hub.load(REPO_DIR, &#39;dinov3_vits16plus&#39;, source=&#39;local&#39;, weights=&lt;CHECKPOINT/URL/OR/PATH&gt;)
dinov3_vitb16 = torch.hub.load(REPO_DIR, &#39;dinov3_vitb16&#39;, source=&#39;local&#39;, weights=&lt;CHECKPOINT/URL/OR/PATH&gt;)
dinov3_vitl16 = torch.hub.load(REPO_DIR, &#39;dinov3_vitl16&#39;, source=&#39;local&#39;, weights=&lt;CHECKPOINT/URL/OR/PATH&gt;)
dinov3_vith16plus = torch.hub.load(REPO_DIR, &#39;dinov3_vith16plus&#39;, source=&#39;local&#39;, weights=&lt;CHECKPOINT/URL/OR/PATH&gt;)
dinov3_vit7b16 = torch.hub.load(REPO_DIR, &#39;dinov3_vit7b16&#39;, source=&#39;local&#39;, weights=&lt;CHECKPOINT/URL/OR/PATH&gt;)

# DINOv3 ConvNeXt models pretrained on web images
dinov3_convnext_tiny = torch.hub.load(REPO_DIR, &#39;dinov3_convnext_tiny&#39;, source=&#39;local&#39;, weights=&lt;CHECKPOINT/URL/OR/PATH&gt;)
dinov3_convnext_small = torch.hub.load(REPO_DIR, &#39;dinov3_convnext_small&#39;, source=&#39;local&#39;, weights=&lt;CHECKPOINT/URL/OR/PATH&gt;)
dinov3_convnext_base = torch.hub.load(REPO_DIR, &#39;dinov3_convnext_base&#39;, source=&#39;local&#39;, weights=&lt;CHECKPOINT/URL/OR/PATH&gt;)
dinov3_convnext_large = torch.hub.load(REPO_DIR, &#39;dinov3_convnext_large&#39;, source=&#39;local&#39;, weights=&lt;CHECKPOINT/URL/OR/PATH&gt;)

# DINOv3 ViT models pretrained on satellite imagery
dinov3_vitl16 = torch.hub.load(REPO_DIR, &#39;dinov3_vitl16&#39;, source=&#39;local&#39;, weights=&lt;CHECKPOINT/URL/OR/PATH&gt;)
dinov3_vit7b16 = torch.hub.load(REPO_DIR, &#39;dinov3_vit7b16&#39;, source=&#39;local&#39;, weights=&lt;CHECKPOINT/URL/OR/PATH&gt;)
</code></pre>
<h3>Pretrained backbones (via Hugging Face <a href="https://huggingface.co/docs/transformers/">Transformers</a>)</h3>
<p>All the backbones are available in the <a href="https://huggingface.co/collections/facebook/dinov3-68924841bd6b561778e31009">DINOv3</a> collection on Hugging Face Hub and supported via the Hugging Face <a href="https://huggingface.co/docs/transformers/index">Transformers</a> library (with released packages from version 4.56.0). Please refer to the corresponding documentation for usage, but below is a short example that demonstrates how to obtain an image embedding with either [Pipeline] or the [AutoModel] class.</p>
<pre><code class="language-python">from transformers import pipeline
from transformers.image_utils import load_image

url = &quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg&quot;
image = load_image(url)

feature_extractor = pipeline(
    model=&quot;facebook/dinov3-convnext-tiny-pretrain-lvd1689m&quot;,
    task=&quot;image-feature-extraction&quot;, 
)
features = feature_extractor(image)
</code></pre>
<pre><code class="language-python">import torch
from transformers import AutoImageProcessor, AutoModel
from transformers.image_utils import load_image

url = &quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;
image = load_image(url)

pretrained_model_name = &quot;facebook/dinov3-convnext-tiny-pretrain-lvd1689m&quot;
processor = AutoImageProcessor.from_pretrained(pretrained_model_name)
model = AutoModel.from_pretrained(
    pretrained_model_name, 
    device_map=&quot;auto&quot;, 
)

inputs = processor(images=image, return_tensors=&quot;pt&quot;).to(model.device)
with torch.inference_mode():
    outputs = model(**inputs)

pooled_output = outputs.pooler_output
print(&quot;Pooled output shape:&quot;, pooled_output.shape)
</code></pre>
<p>where <code>model</code> and <code>pretrained_model_name</code> above can be one of:</p>
<ul>
<li><code>facebook/dinov3-vits16-pretrain-lvd1689m</code></li>
<li><code>facebook/dinov3-vits16plus-pretrain-lvd1689m</code></li>
<li><code>facebook/dinov3-vitb16-pretrain-lvd1689m</code></li>
<li><code>facebook/dinov3-vitl16-pretrain-lvd1689m</code></li>
<li><code>facebook/dinov3-vith16plus-pretrain-lvd1689m</code></li>
<li><code>facebook/dinov3-vit7b16-pretrain-lvd1689m</code></li>
<li><code>facebook/dinov3-convnext-base-pretrain-lvd1689m</code></li>
<li><code>facebook/dinov3-convnext-large-pretrain-lvd1689m</code></li>
<li><code>facebook/dinov3-convnext-small-pretrain-lvd1689m</code></li>
<li><code>facebook/dinov3-convnext-tiny-pretrain-lvd1689m</code></li>
<li><code>facebook/dinov3-vitl16-pretrain-sat493m</code></li>
<li><code>facebook/dinov3-vit7b16-pretrain-sat493m</code></li>
</ul>
<h3>Image transforms</h3>
<p>For models using the LVD-1689M weights (pretrained on web images), please use the following transform (standard ImageNet evaluation transform):</p>
<pre><code class="language-python">import torchvision
from torchvision.transforms import v2

def make_transform(resize_size: int = 256):
    to_tensor = v2.ToImage()
    resize = v2.Resize((resize_size, resize_size), antialias=True)
    to_float = v2.ToDtype(torch.float32, scale=True)
    normalize = v2.Normalize(
        mean=(0.485, 0.456, 0.406),
        std=(0.229, 0.224, 0.225),
    )
    return v2.Compose([to_tensor, resize, to_float, normalize])
</code></pre>
<p>For models using the SAT-493M weights (pretrained on satellite imagery), please use the following transform:</p>
<pre><code class="language-python">import torchvision
from torchvision.transforms import v2

def make_transform(resize_size: int = 256):
    to_tensor = v2.ToImage()
    resize = v2.Resize((resize_size, resize_size), antialias=True)
    to_float = v2.ToDtype(torch.float32, scale=True)
    normalize = v2.Normalize(
        mean=(0.430, 0.411, 0.296),
        std=(0.213, 0.156, 0.143),
    )
    return v2.Compose([to_tensor, resize, to_float, normalize])
</code></pre>
<h3>Pretrained heads - Image classification</h3>
<table style="margin: auto">
  <thead>
    <tr>
      <th>Backbone</th>
      <th>Pretraining<br/>Dataset</th>
      <th>Head<br/>Dataset</th>
      <th>Download</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ViT-7B/16</td>
      <td align="center">LVD-1689M</td>
      <td align="center">ImageNet</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a></td>
    </tr>
  </tbody>
</table>


<p>The (full) classifier models can be loaded via PyTorch Hub:</p>
<pre><code class="language-python">import torch

# DINOv3
dinov3_vit7b16_lc = torch.hub.load(REPO_DIR, &#39;dinov3_vit7b16_lc&#39;, source=&quot;local&quot;, weights=&lt;DEPTHER/CHECKPOINT/URL/OR/PATH&gt;, backbone_weights=&lt;BACKBONE/CHECKPOINT/URL/OR/PATH&gt;)
</code></pre>
<h3>Pretrained heads - Depther trained on SYNTHMIX dataset</h3>
<table style="margin: auto">
  <thead>
    <tr>
      <th>Backbone</th>
      <th>Pretraining<br/>Dataset</th>
      <th>Head<br/>Dataset</th>
      <th>Download</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ViT-7B/16</td>
      <td align="center">LVD-1689M</td>
      <td align="center">SYNTHMIX</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a></td>
    </tr>
  </tbody>
</table>


<pre><code class="language-python">depther = torch.hub.load(REPO_DIR, &#39;dinov3_vit7b16_dd&#39;, source=&quot;local&quot;, weights=&lt;DEPTHER/CHECKPOINT/URL/OR/PATH&gt;, backbone_weights=&lt;BACKBONE/CHECKPOINT/URL/OR/PATH&gt;)
</code></pre>
<p>Full example code of depther on an image</p>
<pre><code class="language-python">from PIL import Image
import torch
from torchvision.transforms import v2
import matplotlib.pyplot as plt
from matplotlib import colormaps

def get_img():
    import requests
    url = &quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;
    image = Image.open(requests.get(url, stream=True).raw).convert(&quot;RGB&quot;)
    return image

def make_transform(resize_size: int | list[int] = 768):
    to_tensor = v2.ToImage()
    resize = v2.Resize((resize_size, resize_size), antialias=True)
    to_float = v2.ToDtype(torch.float32, scale=True)
    normalize = v2.Normalize(
        mean=(0.485, 0.456, 0.406),
        std=(0.229, 0.224, 0.225),
    )
    return v2.Compose([to_tensor, resize, to_float, normalize])

depther = torch.hub.load(REPO_DIR, &#39;dinov3_vit7b16_dd&#39;, source=&quot;local&quot;, weights=&lt;DEPTHER/CHECKPOINT/URL/OR/PATH&gt;, backbone_weights=&lt;BACKBONE/CHECKPOINT/URL/OR/PATH&gt;)

img_size = 1024
img = get_img()
transform = make_transform(img_size)
with torch.inference_mode():
    with torch.autocast(&#39;cuda&#39;, dtype=torch.bfloat16):
        batch_img = transform(img)[None]
        batch_img = batch_img
        depths = depther(batch_img)

plt.figure(figsize=(12, 6))
plt.subplot(121)
plt.imshow(img)
plt.axis(&quot;off&quot;)
plt.subplot(122)
plt.imshow(depths[0,0].cpu(), cmap=colormaps[&quot;Spectral&quot;])
plt.axis(&quot;off&quot;)
</code></pre>
<h4>Reproduce paper results</h4>
<p>Make sure the NYU dataset is setup following <a href="DATASETS.md#depth-estimation-on-nyu">this</a>.</p>
<p>Launch the following to reproduce our paper&#39;s depth estimation results on NYUv2 with the pretrained Depther trained on SYNTHMIX:</p>
<pre><code class="language-shell">PYTHONPATH=. python -m dinov3.run.submit dinov3/eval/depth/run.py \
config=dinov3/eval/depth/configs/config-nyu-synthmix-dpt-inference.yaml \
datasets.root=&lt;PATH/TO/DATASET&gt; \
load_from=dinov3_vit7b16_dd \
--output-dir &lt;PATH/TO/OUTPUT/DIR&gt;
</code></pre>
<p>Notes:</p>
<ul>
<li>if you want to launch the code without dinov3.run.submit, you can do so using python directly or torchrun:</li>
</ul>
<pre><code class="language-shell">PYTHONPATH=. python dinov3/eval/depth/run.py \
config=dinov3/eval/depth/configs/config-nyu-synthmix-dpt-inference.yaml \
datasets.root=&lt;PATH/TO/DATASET&gt; \
load_from=dinov3_vit7b16_dd \
output_dir=&lt;PATH/TO/OUTPUT/DIR&gt;
</code></pre>
<ul>
<li>One can also save prediction results using <code>result_config.save_results=true</code>.</li>
</ul>
<h4>Linear depth estimation on NYUv2 Depth</h4>
<pre><code class="language-shell">PYTHONPATH=. python -m dinov3.run.submit dinov3/eval/depth/run.py \
    model.dino_hub=dinov3_vit7b16 \
    config=dinov3/eval/depth/configs/config-nyu.yaml \
    datasets.root=&lt;PATH/TO/DATASET&gt; \
    --output-dir &lt;PATH/TO/OUTPUT/DIR&gt;
</code></pre>
<p>After the job completes, you will find in the output path directory you specified</p>
<ul>
<li><code>depth_config.yaml</code> that contains the config you trained the model with;</li>
<li><code>model_final.pth</code>, the final linear head checkpoint at the end of training; and</li>
<li><code>results-depth.csv</code> with the final metrics.</li>
</ul>
<h3>Pretrained heads - Detector trained on COCO2017 dataset</h3>
<table style="margin: auto">
  <thead>
    <tr>
      <th>Backbone</th>
      <th>Pretraining<br/>Dataset</th>
      <th>Head<br/>Dataset</th>
      <th>Download</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ViT-7B/16</td>
      <td align="center">LVD-1689M</td>
      <td align="center">COCO2017</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a></td>
    </tr>
  </tbody>
</table>


<pre><code class="language-python">detector = torch.hub.load(REPO_DIR, &#39;dinov3_vit7b16_de&#39;, source=&quot;local&quot;, weights=&lt;DETECTOR/CHECKPOINT/URL/OR/PATH&gt;, backbone_weights=&lt;BACKBONE/CHECKPOINT/URL/OR/PATH&gt;)
</code></pre>
<h3>Pretrained heads - Segmentor trained on ADE20K dataset</h3>
<table style="margin: auto">
  <thead>
    <tr>
      <th>Backbone</th>
      <th>Pretraining<br/>Dataset</th>
      <th>Head<br/>Dataset</th>
      <th>Download</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ViT-7B/16</td>
      <td align="center">LVD-1689M</td>
      <td align="center">ADE20K</td>
      <td align="center"><a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a></td>
    </tr>
  </tbody>
</table>

<pre><code class="language-python">segmentor = torch.hub.load(REPO_DIR, &#39;dinov3_vit7b16_ms&#39;, source=&quot;local&quot;, weights=&lt;SEGMENTOR/CHECKPOINT/URL/OR/PATH&gt;, backbone_weights=&lt;BACKBONE/CHECKPOINT/URL/OR/PATH&gt;)
</code></pre>
<p>Example command to run a full inference on ADE20K with the provided segmentor (ViT-7B + M2F):</p>
<pre><code class="language-shell">PYTHONPATH=. python -m dinov3.run.submit dinov3/eval/segmentation/run.py \
config=dinov3/eval/segmentation/configs/config-ade20k-m2f-inference.yaml  \
datasets.root=&lt;PATH/TO/DATASET&gt; \
load_from=dinov3_vit7b16_ms \
--output-dir &lt;PATH/TO/OUTPUT/DIR&gt;
</code></pre>
<p>Full example code of segmentator on an image</p>
<pre><code class="language-python">import sys
sys.path.append(REPO_DIR)

from PIL import Image
import torch
from torchvision import transforms
import matplotlib.pyplot as plt
from matplotlib import colormaps
from functools import partial
from dinov3.eval.segmentation.inference import make_inference


def get_img():
    import requests
    url = &quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;
    image = Image.open(requests.get(url, stream=True).raw).convert(&quot;RGB&quot;)
    return image

def make_transform(resize_size: int | list[int] = 768):
    to_tensor = v2.ToImage()
    resize = v2.Resize((resize_size, resize_size), antialias=True)
    to_float = v2.ToDtype(torch.float32, scale=True)
    normalize = v2.Normalize(
        mean=(0.485, 0.456, 0.406),
        std=(0.229, 0.224, 0.225),
    )
    return v2.Compose([to_tensor, resize, to_float, normalize])

segmentor = torch.hub.load(REPO_DIR, &#39;dinov3_vit7b16_ms&#39;, source=&quot;local&quot;, weights=&lt;SEGMENTOR/CHECKPOINT/URL/OR/PATH&gt;, backbone_weights=&lt;BACKBONE/CHECKPOINT/URL/OR/PATH&gt;)

img_size = 896
img  = get_img()
transform = make_transform(img_size)
with torch.inference_mode():
    with torch.autocast(&#39;cuda&#39;, dtype=torch.bfloat16):
        batch_img = transform(img)[None]
        pred_vit7b = segmentor(batch_img)  # raw predictions  
        # actual segmentation map
        segmentation_map_vit7b = make_inference(
            batch_img,
            segmentor,
            inference_mode=&quot;slide&quot;,
            decoder_head_type=&quot;m2f&quot;,
            rescale_to=(img.size[-1], img.size[-2]),
            n_output_channels=150,
            crop_size=(img_size, img_size),
            stride=(img_size, img_size),
            output_activation=partial(torch.nn.functional.softmax, dim=1),
        ).argmax(dim=1, keepdim=True)
plt.figure(figsize=(12, 6))
plt.subplot(121)
plt.imshow(img)
plt.axis(&quot;off&quot;)
plt.subplot(122)
plt.imshow(segmentation_map_vit7b[0,0].cpu(), cmap=colormaps[&quot;Spectral&quot;])
plt.axis(&quot;off&quot;)
</code></pre>
<h3>Pretrained heads - Zero-shot tasks with <code>dino.txt</code></h3>
<table style="margin: auto">
  <thead>
    <tr>
      <th rowspan="2">Backbone</th>
      <th>Download</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ViT-L/16 distilled</td>
      <td align="center">
        <a href="https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/">[link]</a>,
        <a href="https://dl.fbaipublicfiles.com/dinov3/thirdparty/bpe_simple_vocab_16e6.txt.gz">vocabulary</a>,
        <a href="https://dl.fbaipublicfiles.com/dinov2/thirdparty/LICENSE">vocabulary license</a>
      </td>
    </tr>
  </tbody>
</table>

<p>The (full) dino.txt model can be loaded via PyTorch Hub:</p>
<pre><code class="language-python">import torch
# DINOv3
dinov3_vitl16_dinotxt_tet1280d20h24l, tokenizer = torch.hub.load(REPO_DIR, &#39;dinov3_vitl16_dinotxt_tet1280d20h24l&#39;, weights=&lt;SEGMENTOR/CHECKPOINT/URL/OR/PATH&gt;, backbone_weights=&lt;BACKBONE/CHECKPOINT/URL/OR/PATH&gt;)
</code></pre>
<h2>Installation</h2>
<p>The training and evaluation code requires PyTorch version &gt;= 2.7.1 as well as a few other 3rd party packages. Note that the code has only been tested with the specified versions and also expects a Linux environment. To setup all the required dependencies for training and evaluation, please follow the instructions below:</p>
<p><em><a href="https://mamba.readthedocs.io/en/latest/user_guide/micromamba.html">micromamba</a></em> <strong>(Recommended)</strong> - Clone the repository and then create and activate a <code>dinov3</code> conda environment using the provided environment definition:</p>
<pre><code class="language-shell">micromamba env create -f conda.yaml
micromamba activate dinov3
</code></pre>
<h2>Getting started</h2>
<p>Several notebooks are provided to get started applying DINOv3:</p>
<ul>
<li><a href="notebooks/pca.ipynb">PCA of patch features</a>: display the PCA of DINOv3 patch features on a foreground object (rainbow visualizations from the paper) <a href="https://colab.research.google.com/github/facebookresearch/dinov3/blob/main/notebooks/pca.ipynb">[Run in Google Colab]</a></li>
<li><a href="notebooks/foreground_segmentation.ipynb">Foreground segmentation</a>: train a linear foreground segmentation model based on DINOv3 features <a href="https://colab.research.google.com/github/facebookresearch/dinov3/blob/main/notebooks/foreground_segmentation.ipynb">[Run in Google Colab]</a></li>
<li><a href="notebooks/dense_sparse_matching.ipynb">Dense and sparse matching</a>: match patches from objects on two different images based on DINOv3 features <a href="https://colab.research.google.com/github/facebookresearch/dinov3/blob/main/notebooks/dense_sparse_matching.ipynb">[Run in Google Colab]</a></li>
<li><a href="notebooks/segmentation_tracking.ipynb">Segmentation tracking</a>: video segmentation tracking using a non-parametric method based on DINOv3 features <a href="https://colab.research.google.com/github/facebookresearch/dinov3/blob/main/notebooks/segmentation_tracking.ipynb">[Run in Google Colab]</a></li>
<li><a href="notebooks/dinotxt_segmentation_inference.ipynb">Zero-shot segmentation with DINOv3-based dino.txt</a>: compute the open-vocabulary segmentation results with dino.txt strategy.</li>
</ul>
<h2>Data preparation</h2>
<h3>ImageNet-1k</h3>
<p>The root directory of the dataset should hold the following contents:</p>
<ul>
<li><code>&lt;ROOT&gt;/test/ILSVRC2012_test_00000001.JPEG</code></li>
<li><code>&lt;ROOT&gt;/test/[..]</code></li>
<li><code>&lt;ROOT&gt;/test/ILSVRC2012_test_00100000.JPEG</code></li>
<li><code>&lt;ROOT&gt;/train/n01440764/n01440764_10026.JPEG</code></li>
<li><code>&lt;ROOT&gt;/train/[...]</code></li>
<li><code>&lt;ROOT&gt;/train/n15075141/n15075141_9993.JPEG</code></li>
<li><code>&lt;ROOT&gt;/val/n01440764/ILSVRC2012_val_00000293.JPEG</code></li>
<li><code>&lt;ROOT&gt;/val/[...]</code></li>
<li><code>&lt;ROOT&gt;/val/n15075141/ILSVRC2012_val_00049174.JPEG</code></li>
<li><code>&lt;ROOT&gt;/labels.txt</code></li>
</ul>
<p>The provided dataset implementation expects a few additional metadata files to be present under the extra directory:</p>
<ul>
<li><code>&lt;EXTRA&gt;/class-ids-TRAIN.npy</code></li>
<li><code>&lt;EXTRA&gt;/class-ids-VAL.npy</code></li>
<li><code>&lt;EXTRA&gt;/class-names-TRAIN.npy</code></li>
<li><code>&lt;EXTRA&gt;/class-names-VAL.npy</code></li>
<li><code>&lt;EXTRA&gt;/entries-TEST.npy</code></li>
<li><code>&lt;EXTRA&gt;/entries-TRAIN.npy</code></li>
<li><code>&lt;EXTRA&gt;/entries-VAL.npy</code></li>
</ul>
<p>These metadata files can be generated (once) with the following lines of Python code:</p>
<pre><code class="language-python">from dinov3.data.datasets import ImageNet

for split in ImageNet.Split:
    dataset = ImageNet(split=split, root=&quot;&lt;ROOT&gt;&quot;, extra=&quot;&lt;EXTRA&gt;&quot;)
    dataset.dump_extra()
</code></pre>
<p>Note that the root and extra directories do not have to be distinct directories.</p>
<h3>ImageNet-22k</h3>
<p>Please adapt the <a href="dinov3/data/datasets/image_net_22k.py">dataset class</a> to match your local setup.</p>
<br />

<p>:warning: To execute the commands provided in the next sections for training and evaluation, the <code>dinov3</code> package should be included in the Python module search path, i.e. simply prefix the command to run with <code>PYTHONPATH=.</code>.</p>
<h2>Training</h2>
<h3>Fast setup: training DINOv3 ViT-L/16 on ImageNet-1k</h3>
<p>Run DINOv3 pre-training on 4 H100-80GB nodes (32 GPUs) in a SLURM cluster environment with submitit:</p>
<pre><code class="language-shell"> PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/train/train.py \
  --nodes 4 \
  --config-file dinov3/configs/train/vitl_im1k_lin834.yaml \
  --output-dir &lt;PATH/TO/OUTPUT/DIR&gt; \
  train.dataset_path=ImageNet22k:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt;
</code></pre>
<p>Training time is approximately 14 hours and the resulting checkpoint should reach 82.0% on k-NN eval and 83.5% on linear eval.</p>
<p>The training code saves the weights of the teacher in the eval folder every 12500 iterations for evaluation.</p>
<h3>Exact DINOv3 setup: training DINOv3 ViT-7B/16</h3>
<p>DINOv3 ViT-7B/16 is trained on a private dataset. The training involves 3 stages:</p>
<ul>
<li>Pretraining</li>
<li>Gram anchoring</li>
<li>High resolution adaptation</li>
</ul>
<h4>Pretraining</h4>
<p>Launch DINOV3 ViT-7B/16 pretraining on 32 nodes (256 GPUs) in a SLURM cluster environment with submitit.</p>
<pre><code class="language-shell">PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/train/train.py \
  --nodes 32 \
  --config-file dinov3/configs/train/dinov3_vit7b16_pretrain.yaml \
  --output-dir &lt;PATH/TO/OUTPUT/DIR&gt; \
  train.dataset_path=&lt;DATASET&gt;:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt;
</code></pre>
<h4>Gram anchoring</h4>
<pre><code class="language-shell">PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/train/train.py \
  --nodes 32 \
  --config-file dinov3/configs/train/dinov3_vit7b16_gram_anchor.yaml \
  --output-dir &lt;PATH/TO/OUTPUT/DIR&gt; \
  train.dataset_path=&lt;DATASET&gt;:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt; \
  gram.ckpt=&lt;PATH/TO/GRAM_TEACHER_FROM_PREVIOUS_STEP&gt;   
</code></pre>
<h4>High-resolution adaptation</h4>
<pre><code class="language-shell">PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/train/train.py \
  --nodes 32 \
  --config-file dinov3/configs/train/dinov3_vit7b16_high_res_adapt.yaml \
  --output-dir &lt;PATH/TO/OUTPUT/DIR&gt; \
  train.dataset_path=&lt;DATASET&gt;:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt; \
  gram.ckpt=&lt;PATH/TO/TEACHER_FROM_GRAM&gt; \
  student.resume_from_teacher_chkpt=&lt;PATH/TO/TEACHER_FROM_GRAM&gt;
</code></pre>
<h2>Multi-distillation</h2>
<h3>Test setup:</h3>
<pre><code class="language-shell">PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/train/train.py \
  --nodes 1 \
  --config-file dinov3/configs/train/multi_distillation_test.yaml \
  --output-dir &lt;PATH/TO/OUTPUT/DIR&gt; \
  --multi-distillation \
  train.dataset_path=&lt;DATASET&gt;:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt;
</code></pre>
<h2>Evaluation</h2>
<p>The training code regularly saves the teacher weights. In order to evaluate the model, run the following evaluation on a single node:</p>
<h3>Logistic regression classification on ImageNet-1k</h3>
<pre><code class="language-shell">PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/eval/log_regression.py \
  model.config_file=&lt;PATH/TO/OUTPUT/DIR&gt;/config.yaml \
  model.pretrained_weights=&lt;PATH/TO/OUTPUT/DIR&gt;/teacher_checkpoint.pth \
  output_dir=&lt;PATH/TO/OUTPUT/DIR&gt; \
  train.dataset=ImageNet:split=TRAIN:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt; \
  eval.test_dataset=ImageNet:split=VAL:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt;
</code></pre>
<h3>k-NN classification on ImageNet-1k</h3>
<pre><code class="language-shell">PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/eval/knn.py \
  model.config_file=&lt;PATH/TO/OUTPUT/DIR&gt;/config.yaml \
  model.pretrained_weights=&lt;PATH/TO/OUTPUT/DIR&gt;/teacher_checkpoint.pth \
  output_dir=&lt;PATH/TO/OUTPUT/DIR&gt; \
  train.dataset=ImageNet:split=TRAIN:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt; \
  eval.test_dataset=ImageNet:split=VAL:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt;
</code></pre>
<h3>Linear classification with data augmentation on ImageNet-1k</h3>
<pre><code class="language-shell">PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/eval/linear.py \
  model.config_file=&lt;PATH/TO/OUTPUT/DIR&gt;/config.yaml \
  model.pretrained_weights=&lt;PATH/TO/OUTPUT/DIR&gt;/teacher_checkpoint.pth \
  output_dir=&lt;PATH/TO/OUTPUT/DIR&gt; \
  train.dataset=ImageNet:split=TRAIN:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt; \
  train.val_dataset=ImageNet:split=VAL:root=&lt;PATH/TO/DATASET&gt;:extra=&lt;PATH/TO/DATASET&gt;
</code></pre>
<h3>Linear segmentation with data augmentation on ADE20K</h3>
<pre><code class="language-shell">PYTHONPATH=. python -m dinov3.run.submit dinov3/eval/segmentation/run.py \
model.dino_hub=dinov3_vit7b16 \
config=dinov3/eval/segmentation/configs/config-ade20k-linear-training.yaml \
datasets.root=&lt;PATH/TO/DATASET&gt; \
--output-dir &lt;PATH/TO/OUTPUT/DIR&gt;
</code></pre>
<p>After the job completes, you will find in the output path directory you specified</p>
<ul>
<li><code>segmentation_config.yaml</code> that contains the config you trained the model with;</li>
<li><code>model_final.pth</code>, the final linear head checkpoint at the end of training; and</li>
<li><code>results-semantic-segmentation.csv</code> with the final metrics.</li>
</ul>
<h3>Text alignment on DINOv3 using dino.txt</h3>
<p>Text alignment can be done following the method from <code>dino.txt</code> aka <a href="https://arxiv.org/abs/2412.16334">DINOv2 Meets Text</a>.</p>
<pre><code class="language-shell">PYTHONPATH=${PWD} python -m dinov3.run.submit dinov3/eval/text/train_dinotxt.py \
   --nodes 4 \
  # An example config for text alignment is here: dinov3/eval/text/configs/dinov3_vitl_text.yaml \ 
  trainer_config_file=&quot;&lt;PATH/TO/DINOv3/TEXT/CONFIG&gt;&quot; \
  output-dir=&lt;PATH/TO/OUTPUT/DIR&gt;
</code></pre>
<p>Launching the above trains text alignment on 4 nodes with 8 gpus each (32 gpus in total).
Please note that the text alignment model in the DINOv3 paper was trained on a private dataset and here we have given an example config in <code>dinov3/eval/text/configs/dinov3_vitl_text.yaml</code> using <code>CocoCaptions</code> dataset for illustration purposes.
Please adapt the provided <code>CocoCaptions</code> dataset class, the dataset can be found <a href="https://www.kaggle.com/datasets/nikhil7280/coco-image-caption">here</a>  </p>
<h2>License</h2>
<p>DINOv3 code and model weights are released under the DINOv3 License. See <a href="LICENSE.md">LICENSE.md</a> for additional details.</p>
<h2>Contributing</h2>
<p>See <a href="CONTRIBUTING.md">contributing</a> and the <a href="CODE_OF_CONDUCT.md">code of conduct</a>.</p>
<h2>Citing DINOv3</h2>
<p>If you find this repository useful, please consider giving a star :star: and citation :t-rex::</p>
<pre><code>@misc{simeoni2025dinov3,
  title={{DINOv3}},
  author={Sim{\&#39;e}oni, Oriane and Vo, Huy V. and Seitzer, Maximilian and Baldassarre, Federico and Oquab, Maxime and Jose, Cijo and Khalidov, Vasil and Szafraniec, Marc and Yi, Seungeun and Ramamonjisoa, Micha{\&quot;e}l and Massa, Francisco and Haziza, Daniel and Wehrstedt, Luca and Wang, Jianyuan and Darcet, Timoth{\&#39;e}e and Moutakanni, Th{\&#39;e}o and Sentana, Leonel and Roberts, Claire and Vedaldi, Andrea and Tolan, Jamie and Brandt, John and Couprie, Camille and Mairal, Julien and J{\&#39;e}gou, Herv{\&#39;e} and Labatut, Patrick and Bojanowski, Piotr},
  year={2025},
  eprint={2508.10104},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2508.10104},
}
</code></pre>
 </div> </article> <!-- Related Models Section -->  </div> </div>  </main> <footer class="bg-gray-800 text-gray-300 py-8 mt-16"> <div class="container mx-auto px-4 text-center"> <div class="flex justify-center gap-4 mb-4"> <a href="/about" class="hover:underline">About</a> <a href="/compliance" class="hover:underline">Compliance</a> </div> <p class="text-sm mt-2">
&copy; 2025 Free AI Tools. An open-source project to index the world of AI.
</p> <a href="mailto:compliance@free2aitools.com" class="text-sm text-gray-400 hover:underline mt-1 block">compliance@free2aitools.com</a> </div> </footer> </body></html>