<!DOCTYPE html><html lang="en"> <head><!-- Google Tag Manager --><script type="module">(function(e,n,r,t,m){e[t]=e[t]||[],e[t].push({"gtm.start":new Date().getTime(),event:"gtm.js"});var g=n.getElementsByTagName(r)[0],a=n.createElement(r),s="";a.async=!0,a.src="https://www.googletagmanager.com/gtm.js?id="+m+s,g.parentNode.insertBefore(a,g)})(window,document,"script","dataLayer","GTM-58C2CQ8G");</script><!-- End Google Tag Manager --><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><!-- Open Graph Meta Tags --><meta property="og:type" content="website"><meta property="og:url" content="https://free2aitools.com/model/github-hiyouga-LLaMA-Factory/"><meta property="og:title" content="LLaMA-Factory - AI Model Details"><meta property="og:description" content="Unified Efficient Fine-Tuning of 100+ LLMs &#38; VLMs (ACL 2024)"><meta property="og:image" content="https://free2aitools.com/og-image.jpg"><!-- Twitter Card Meta Tags --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://free2aitools.com/model/github-hiyouga-LLaMA-Factory/"><meta property="twitter:title" content="LLaMA-Factory - AI Model Details"><meta property="twitter:description" content="Unified Efficient Fine-Tuning of 100+ LLMs &#38; VLMs (ACL 2024)"><meta property="twitter:image" content="https://free2aitools.com/og-image.jpg"><meta name="description" content="Unified Efficient Fine-Tuning of 100+ LLMs &#38; VLMs (ACL 2024)"><title>LLaMA-Factory - AI Model Details</title><link rel="canonical" href="https://free2aitools.com/model/github-hiyouga-LLaMA-Factory/"><!-- Google AdSense --><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2292826803755214" crossorigin="anonymous"></script><link rel="stylesheet" href="/_astro/about.C4wyk6Sp.css"></head> <body class="bg-gray-50 text-gray-800 font-sans antialiased flex flex-col min-h-screen"> <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-58C2CQ8G" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) --> <header class="sticky top-0 z-30 w-full backdrop-blur flex-none transition-colors duration-500 lg:border-b lg:border-gray-200 bg-white/80"> <nav class="container mx-auto px-4 sm:px-6 lg:px-8"> <div class="flex items-center justify-between h-16"> <!-- Site Logo --> <a href="/" class="flex items-center gap-2 text-xl sm:text-2xl font-bold text-gray-900"> <span>Free AI Tools</span> </a> <!-- Right-side items --> <div class="flex items-center gap-4"> <!-- Desktop Navigation --> <div class="hidden md:flex items-center gap-6"> <a href="/" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Explore Models </a><a href="/ranking" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Rankings </a><a href="/reports" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Reports </a><a href="/about" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> About </a> </div> <!-- Mobile Search & Menu Toggle --> <button data-collapse-toggle="navbar-default" type="button" class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200" aria-controls="navbar-default" aria-expanded="false"> <span class="sr-only">Open main menu</span> <svg class="w-5 h-5" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 17 14"><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 1h15M1 7h15M1 13h15"></path></svg> </button> </div> </div> <!-- Mobile Collapsible Menu --> <div class="hidden w-full md:hidden" id="navbar-default"> <ul class="font-medium flex flex-col p-4 md:p-0 mt-4 border border-gray-100 rounded-lg bg-gray-50"> <li> <a href="/" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Explore Models</a> </li><li> <a href="/ranking" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Rankings</a> </li><li> <a href="/reports" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Reports</a> </li><li> <a href="/about" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">About</a> </li> </ul> </div> </nav> </header> <main class="flex-grow">  <div class="container mx-auto px-4 py-12"> <div class="max-w-4xl mx-auto"> <!-- Header Section --> <header class="mb-8"> <div class="flex flex-col md:flex-row md:items-start md:justify-between gap-4"> <div> <h1 class="text-4xl md:text-5xl font-extrabold text-gray-900 dark:text-white mb-2">LLaMA-Factory</h1> <p class="text-lg text-gray-500 dark:text-gray-400">by hiyouga</p> </div>  </div> </header> <!-- Metadata Section --> <div class="mb-8 p-4 bg-gray-100 dark:bg-gray-800 rounded-lg"> <div class="grid grid-cols-2 md:grid-cols-4 gap-4 text-center"> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Likes</p> <p class="text-xl font-bold">62,802</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Downloads</p> <p class="text-xl font-bold">62,802</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Task</p> <p class="text-xl font-bold capitalize">tool</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Last Updated</p> <p class="text-xl font-bold">2025/11/20</p> </div> </div> </div> <!-- Tags and Sources --> <div class="mb-8 flex flex-wrap items-start gap-4"> <div class="flex-1"> <h3 class="text-lg font-semibold mb-2">Tags</h3> <div class="flex flex-wrap gap-2"> <a href="/?tag=agent" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> agent </a><a href="/?tag=ai" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> ai </a><a href="/?tag=deepseek" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> deepseek </a><a href="/?tag=fine-tuning" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> fine-tuning </a><a href="/?tag=gemma" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> gemma </a><a href="/?tag=gpt" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> gpt </a><a href="/?tag=instruction-tuning" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> instruction-tuning </a><a href="/?tag=large-language-models" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> large-language-models </a><a href="/?tag=llama" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> llama </a><a href="/?tag=llama3" class="px-3 py-1 text-xs font-medium bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200 rounded-full hover:bg-blue-200 dark:hover:bg-blue-800"> llama3 </a> </div> </div> <div class="flex-shrink-0"> <h3 class="text-lg font-semibold mb-2">Sources</h3> <div class="flex flex-col gap-2"> <a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> üì¶ GitHub </a> </div> </div> </div> <!-- Source Specific Details --> <div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> üì¶ GitHub Details
</h3> <div class="grid grid-cols-2 md:grid-cols-3 gap-4 text-sm"> <div><strong>Language:</strong> Python</div> <div><strong>Stars:</strong> 62,802</div> <div><strong>Forks:</strong> 7,601</div> <div><strong>Open Issues:</strong> 783</div> <div><strong>License:</strong> Apache License 2.0</div> <div><a href="https://llamafactory.readthedocs.io" target="_blank" class="text-blue-600 dark:text-blue-400 hover:underline"><strong>Project Homepage ‚Üó</strong></a></div> </div>  </div> <!-- README Content --> <article class="model-readme-content py-8 border-t border-gray-200 dark:border-gray-700"> <div class="prose prose-lg dark:prose-invert max-w-none"> <p><img src="assets/logo.png" alt="# LLaMA Factory"></p>
<p><a href="https://github.com/hiyouga/LLaMA-Factory/stargazers"><img src="https://img.shields.io/github/stars/hiyouga/LLaMA-Factory?style=social" alt="GitHub Repo stars"></a>
<a href="https://github.com/hiyouga/LLaMA-Factory/commits/main"><img src="https://img.shields.io/github/last-commit/hiyouga/LLaMA-Factory" alt="GitHub last commit"></a>
<a href="https://github.com/hiyouga/LLaMA-Factory/graphs/contributors"><img src="https://img.shields.io/github/contributors/hiyouga/LLaMA-Factory?color=orange" alt="GitHub contributors"></a>
<a href="https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml"><img src="https://github.com/hiyouga/LLaMA-Factory/actions/workflows/tests.yml/badge.svg" alt="GitHub workflow"></a>
<a href="https://pypi.org/project/llamafactory/"><img src="https://img.shields.io/pypi/v/llamafactory" alt="PyPI"></a>
<a href="https://scholar.google.com/scholar?cites=12620864006390196564"><img src="https://img.shields.io/badge/citation-1000+-green" alt="Citation"></a>
<a href="https://hub.docker.com/r/hiyouga/llamafactory/tags"><img src="https://img.shields.io/docker/pulls/hiyouga/llamafactory" alt="Docker Pulls"></a></p>
<p><a href="https://twitter.com/llamafactory_ai"><img src="https://img.shields.io/twitter/follow/llamafactory_ai" alt="Twitter"></a>
<a href="https://discord.gg/rKfvV9r9FK"><img src="assets/thirdparty/discord.svg" alt="Discord"></a>
<a href="https://github.com/hiyouga/llamafactory-community"><img src="https://img.shields.io/badge/WeChat-User%20Group-blue?logo=wechat" alt="WeChat"></a>
<a href="https://blog.llamafactory.net/en/"><img src="https://img.shields.io/badge/Hugo-Official%20Blog-blue?logo=hugo" alt="Blog"></a></p>
<p><a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing"><img src="assets/thirdparty/colab.svg" alt="Open in Colab"></a>
<a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory"><img src="assets/thirdparty/dsw.svg" alt="Open in DSW"></a>
<a href="https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&utm_source=LLaMA-Factory"><img src="assets/thirdparty/lab4ai.svg" alt="Open in Lab4ai"></a>
<a href="https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory"><img src="assets/thirdparty/online.svg" alt="Open in Online"></a>
<a href="https://huggingface.co/spaces/hiyouga/LLaMA-Board"><img src="https://img.shields.io/badge/%F0%9F%A4%97-Open%20in%20Spaces-blue" alt="Open in Spaces"></a>
<a href="https://modelscope.cn/studios/hiyouga/LLaMA-Board"><img src="https://img.shields.io/badge/ModelScope-Open%20in%20Studios-blue" alt="Open in Studios"></a>
<a href="https://novita.ai/templates-library/105981?sharer=88115474-394e-4bda-968e-b88e123d0c47"><img src="https://img.shields.io/badge/Novita-Deploy%20Template-blue" alt="Open in Novita"></a></p>
<h3>Used by <a href="https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/">Amazon</a>, <a href="https://developer.nvidia.com/rtx/ai-toolkit">NVIDIA</a>, <a href="https://help.aliyun.com/zh/pai/use-cases/fine-tune-a-llama-3-model-with-llama-factory">Aliyun</a>, etc.</h3>
<div align="center" markdown="1">

<h3>Supporters ‚ù§Ô∏è</h3>
<table>
<thead>
<tr>
<th><div style="text-align: center;"><a href="https://warp.dev/llama-factory"><img alt="Warp sponsorship" width="400" src="assets/sponsors/warp.jpg"></a><br><a href="https://warp.dev/llama-factory" style="font-size:larger;">Warp, the agentic terminal for developers</a><br><a href="https://warp.dev/llama-factory">Available for MacOS, Linux, &amp; Windows</a></th>
<th><a href="https://serpapi.com"><img alt="SerpAPI sponsorship" width="250" src="assets/sponsors/serpapi.svg"> </a></th>
</tr>
</thead>
</table>
<hr>
<h3>Easily fine-tune 100+ large language models with zero-code <a href="#quickstart">CLI</a> and <a href="#fine-tuning-with-llama-board-gui-powered-by-gradio">Web UI</a></h3>
<p><img src="https://trendshift.io/api/badge/repositories/4535" alt="GitHub Trend"></p>
</div>

<p>üëã Join our <a href="https://github.com/hiyouga/llamafactory-community/blob/main/wechat/main.jpg">WeChat</a>, <a href="https://github.com/hiyouga/llamafactory-community/blob/main/wechat/npu.jpg">NPU</a>, <a href="https://github.com/hiyouga/llamafactory-community/blob/main/wechat/lab4ai.jpg">Lab4AI</a>, <a href="https://github.com/hiyouga/llamafactory-community/blob/main/wechat/online.jpg">LLaMA Factory Online</a> user group.</p>
<p>[ English | <a href="README_zh.md">‰∏≠Êñá</a> ]</p>
<p><strong>Fine-tuning a large language model can be easy as...</strong></p>
<p><a href="https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e">https://github.com/user-attachments/assets/3991a3a8-4276-4d30-9cab-4cb0c4b9b99e</a></p>
<p>Start local training:</p>
<ul>
<li>Please refer to <a href="#getting-started">usage</a></li>
</ul>
<p>Start cloud training:</p>
<ul>
<li><strong>Colab (free)</strong>: <a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing">https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing</a></li>
<li><strong>PAI-DSW (free trial)</strong>: <a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory">https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory</a></li>
<li><strong>LLaMA Factory Online</strong>: <a href="https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory">https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory</a></li>
<li><strong>Alaya NeW (cloud GPU deal)</strong>: <a href="https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory">https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory</a></li>
</ul>
<p>Read technical notes:</p>
<ul>
<li><strong>Documentation (WIP)</strong>: <a href="https://llamafactory.readthedocs.io/en/latest/">https://llamafactory.readthedocs.io/en/latest/</a></li>
<li><strong>Documentation (AMD GPU)</strong>: <a href="https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html">https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html</a></li>
<li><strong>Official Blog</strong>: <a href="https://blog.llamafactory.net/en/">https://blog.llamafactory.net/en/</a></li>
<li><strong>Official Course</strong>: <a href="https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&utm_source=LLaMA-Factory">https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;utm_source=LLaMA-Factory</a></li>
</ul>
<blockquote>
<p>[!NOTE]
Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.</p>
</blockquote>
<h2>Table of Contents</h2>
<ul>
<li><a href="#features">Features</a></li>
<li><a href="#blogs">Blogs</a></li>
<li><a href="#changelog">Changelog</a></li>
<li><a href="#supported-models">Supported Models</a></li>
<li><a href="#supported-training-approaches">Supported Training Approaches</a></li>
<li><a href="#provided-datasets">Provided Datasets</a></li>
<li><a href="#requirement">Requirement</a></li>
<li><a href="#getting-started">Getting Started</a><ul>
<li><a href="#installation">Installation</a></li>
<li><a href="#data-preparation">Data Preparation</a></li>
<li><a href="#quickstart">Quickstart</a></li>
<li><a href="#fine-tuning-with-llama-board-gui-powered-by-gradio">Fine-Tuning with LLaMA Board GUI</a></li>
<li><a href="#llama-factory-online">LLaMA Factory Online</a></li>
<li><a href="#build-docker">Build Docker</a></li>
<li><a href="#deploy-with-openai-style-api-and-vllm">Deploy with OpenAI-style API and vLLM</a></li>
<li><a href="#download-from-modelscope-hub">Download from ModelScope Hub</a></li>
<li><a href="#download-from-modelers-hub">Download from Modelers Hub</a></li>
<li><a href="#use-wb-logger">Use W&amp;B Logger</a></li>
<li><a href="#use-swanlab-logger">Use SwanLab Logger</a></li>
</ul>
</li>
<li><a href="#projects-using-llama-factory">Projects using LLaMA Factory</a></li>
<li><a href="#license">License</a></li>
<li><a href="#citation">Citation</a></li>
<li><a href="#acknowledgement">Acknowledgement</a></li>
</ul>
<h2>Features</h2>
<ul>
<li><strong>Various models</strong>: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.</li>
<li><strong>Integrated methods</strong>: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.</li>
<li><strong>Scalable resources</strong>: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.</li>
<li><strong>Advanced algorithms</strong>: <a href="https://github.com/jiaweizzhao/GaLore">GaLore</a>, <a href="https://github.com/Ledzy/BAdam">BAdam</a>, <a href="https://github.com/zhuhanqing/APOLLO">APOLLO</a>, <a href="https://github.com/zyushun/Adam-mini">Adam-mini</a>, <a href="https://github.com/KellerJordan/Muon">Muon</a>, <a href="https://github.com/huggingface/peft/tree/main/src/peft/tuners/oft">OFT</a>, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.</li>
<li><strong>Practical tricks</strong>: <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention-2</a>, <a href="https://github.com/unslothai/unsloth">Unsloth</a>, <a href="https://github.com/linkedin/Liger-Kernel">Liger Kernel</a>, <a href="https://github.com/kvcache-ai/ktransformers/">KTransformers</a>, RoPE scaling, NEFTune and rsLoRA.</li>
<li><strong>Wide tasks</strong>: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.</li>
<li><strong>Experiment monitors</strong>: LlamaBoard, TensorBoard, Wandb, MLflow, <a href="https://github.com/SwanHubX/SwanLab">SwanLab</a>, etc.</li>
<li><strong>Faster inference</strong>: OpenAI-style API, Gradio UI and CLI with <a href="https://github.com/vllm-project/vllm">vLLM worker</a> or <a href="https://github.com/sgl-project/sglang">SGLang worker</a>.</li>
</ul>
<h3>Day-N Support for Fine-Tuning Cutting-Edge Models</h3>
<table>
<thead>
<tr>
<th>Support Date</th>
<th>Model Name</th>
</tr>
</thead>
<tbody><tr>
<td>Day 0</td>
<td>Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6</td>
</tr>
<tr>
<td>Day 1</td>
<td>Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4</td>
</tr>
</tbody></table>
<h2>Blogs</h2>
<blockquote>
<p>[!TIP]
Now we have a dedicated blog for LLaMA Factory!</p>
<p>Website: <a href="https://blog.llamafactory.net/en/">https://blog.llamafactory.net/en/</a></p>
</blockquote>
<ul>
<li>üí° <a href="https://blog.llamafactory.net/en/posts/ktransformers/">KTransformers Fine-Tuning √ó LLaMA Factory: Fine-tuning 1000 Billion models with 2 4090-GPU + CPU</a> (English)</li>
<li>üí° <a href="https://buaa-act.feishu.cn/wiki/GVzlwYcRFiR8OLkHbL6cQpYin7g">Easy Dataset √ó LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge</a> (English)</li>
<li><a href="https://www.lab4ai.cn/project/detail?id=25cce32ec131497b9e06a93336a0817f&type=project&utm_source=LLaMA-Factory">Fine-tune a mental health LLM using LLaMA-Factory</a> (Chinese)</li>
<li><a href="https://docs.llamafactory.com.cn/docs/documents/best-practice/gptroleplay/?utm_source=LLaMA-Factory">Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory</a> (Chinese)</li>
<li><a href="https://aws.amazon.com/cn/blogs/china/building-llm-model-hub-based-on-llamafactory-and-easyr1/">A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1</a> (Chinese)</li>
<li><a href="https://aws.amazon.com/cn/blogs/machine-learning/how-apoidea-group-enhances-visual-information-extraction-from-banking-documents-with-multimodal-models-using-llama-factory-on-amazon-sagemaker-hyperpod/">How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod</a> (English)</li>
</ul>
<details><summary>All Blogs</summary>

<ul>
<li><a href="https://docs.alayanew.com/docs/documents/bestPractice/bigModel/llama70B/?utm_source=LLaMA-Factory">Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory</a> (Chinese)</li>
<li><a href="https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory">Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory</a> (Chinese)</li>
<li><a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_deepseek_r1_distill_7b">LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier</a> (Chinese)</li>
<li><a href="https://aws.amazon.com/cn/blogs/china/a-one-stop-code-free-model-fine-tuning-deployment-platform-based-on-sagemaker-and-llama-factory/">A One-Stop Code-Free Model Fine-Tuning &amp; Deployment Platform based on SageMaker and LLaMA-Factory</a> (Chinese)</li>
<li><a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory_qwen2vl">LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide</a> (Chinese)</li>
<li><a href="https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory">LLaMA Factory: Fine-tuning Llama3 for Role-Playing</a> (Chinese)</li>
</ul>
</details>

<h2>Changelog</h2>
<p>[25/10/26] We support Megatron-core training backend with <a href="https://github.com/alibaba/ROLL/tree/main/mcore_adapter"><strong>mcore_adapter</strong></a>. See <a href="https://github.com/hiyouga/LLaMA-Factory/pull/9237">PR #9237</a> to get started.</p>
<p>[25/08/22] We supported <strong><a href="https://arxiv.org/abs/2306.07280">OFT</a></strong> and <strong><a href="https://arxiv.org/abs/2506.19847">OFTv2</a></strong>. See <a href="examples/README.md">examples</a> for usage.</p>
<p>[25/08/20] We supported fine-tuning the <strong><a href="https://huggingface.co/internlm/Intern-S1-mini">Intern-S1-mini</a></strong> models. See <a href="https://github.com/hiyouga/LLaMA-Factory/pull/8976">PR #8976</a> to get started.</p>
<p>[25/08/06] We supported fine-tuning the <strong><a href="https://github.com/openai/gpt-oss">GPT-OSS</a></strong> models. See <a href="https://github.com/hiyouga/LLaMA-Factory/pull/8826">PR #8826</a> to get started.</p>
<details><summary>Full Changelog</summary>

<p>[25/07/02] We supported fine-tuning the <strong><a href="https://github.com/THUDM/GLM-4.1V-Thinking">GLM-4.1V-9B-Thinking</a></strong> model.</p>
<p>[25/04/28] We supported fine-tuning the <strong><a href="https://qwenlm.github.io/blog/qwen3/">Qwen3</a></strong> model family.</p>
<p>[25/04/21] We supported the <strong><a href="https://github.com/KellerJordan/Muon">Muon</a></strong> optimizer. See <a href="examples/README.md">examples</a> for usage. Thank <a href="https://github.com/tianshijing">@tianshijing</a>&#39;s PR.</p>
<p>[25/04/16] We supported fine-tuning the <strong><a href="https://huggingface.co/OpenGVLab/InternVL3-8B">InternVL3</a></strong> model. See <a href="https://github.com/hiyouga/LLaMA-Factory/pull/7258">PR #7258</a> to get started.</p>
<p>[25/04/14] We supported fine-tuning the <strong><a href="https://huggingface.co/THUDM/GLM-Z1-9B-0414">GLM-Z1</a></strong> and <strong><a href="https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct">Kimi-VL</a></strong> models.</p>
<p>[25/04/06] We supported fine-tuning the <strong><a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/">Llama 4</a></strong> model. See <a href="https://github.com/hiyouga/LLaMA-Factory/pull/7611">PR #7611</a> to get started.</p>
<p>[25/03/31] We supported fine-tuning the <strong><a href="https://qwenlm.github.io/blog/qwen2.5-omni/">Qwen2.5 Omni</a></strong> model. See <a href="https://github.com/hiyouga/LLaMA-Factory/pull/7537">PR #7537</a> to get started.</p>
<p>[25/03/15] We supported <strong><a href="https://github.com/sgl-project/sglang">SGLang</a></strong> as inference backend. Try <code>infer_backend: sglang</code> to accelerate inference.</p>
<p>[25/03/12] We supported fine-tuning the <strong><a href="https://huggingface.co/blog/gemma3">Gemma 3</a></strong> model.</p>
<p>[25/02/24] Announcing <strong><a href="https://github.com/hiyouga/EasyR1">EasyR1</a></strong>, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.</p>
<p>[25/02/11] We supported saving the <strong><a href="https://github.com/ollama/ollama">Ollama</a></strong> modelfile when exporting the model checkpoints. See <a href="examples/README.md">examples</a> for usage.</p>
<p>[25/02/05] We supported fine-tuning the <strong><a href="Qwen/Qwen2-Audio-7B-Instruct">Qwen2-Audio</a></strong> and <strong><a href="https://huggingface.co/openbmb/MiniCPM-o-2_6">MiniCPM-o-2.6</a></strong> on audio understanding tasks.</p>
<p>[25/01/31] We supported fine-tuning the <strong><a href="https://huggingface.co/deepseek-ai/DeepSeek-R1">DeepSeek-R1</a></strong> and <strong><a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct">Qwen2.5-VL</a></strong> models.</p>
<p>[25/01/15] We supported <strong><a href="https://arxiv.org/abs/2412.05270">APOLLO</a></strong> optimizer. See <a href="examples/README.md">examples</a> for usage.</p>
<p>[25/01/14] We supported fine-tuning the <strong><a href="https://huggingface.co/openbmb/MiniCPM-o-2_6">MiniCPM-o-2.6</a></strong> and <strong><a href="https://huggingface.co/openbmb/MiniCPM-V-2_6">MiniCPM-V-2.6</a></strong> models. Thank <a href="https://github.com/BUAADreamer">@BUAADreamer</a>&#39;s PR.</p>
<p>[25/01/14] We supported fine-tuning the <strong><a href="https://huggingface.co/collections/internlm/">InternLM 3</a></strong> models. Thank <a href="https://github.com/hhaAndroid">@hhaAndroid</a>&#39;s PR.</p>
<p>[25/01/10] We supported fine-tuning the <strong><a href="https://huggingface.co/microsoft/phi-4">Phi-4</a></strong> model.</p>
<p>[24/12/21] We supported using <strong><a href="https://github.com/SwanHubX/SwanLab">SwanLab</a></strong> for experiment tracking and visualization. See <a href="#use-swanlab-logger">this section</a> for details.</p>
<p>[24/11/27] We supported fine-tuning the <strong><a href="https://huggingface.co/Skywork/Skywork-o1-Open-Llama-3.1-8B">Skywork-o1</a></strong> model and the <strong><a href="https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT">OpenO1</a></strong> dataset.</p>
<p>[24/10/09] We supported downloading pre-trained models and datasets from the <strong><a href="https://modelers.cn/models">Modelers Hub</a></strong>. See <a href="#download-from-modelers-hub">this tutorial</a> for usage.</p>
<p>[24/09/19] We supported fine-tuning the <strong><a href="https://qwenlm.github.io/blog/qwen2.5/">Qwen2.5</a></strong> models.</p>
<p>[24/08/30] We supported fine-tuning the <strong><a href="https://qwenlm.github.io/blog/qwen2-vl/">Qwen2-VL</a></strong> models. Thank <a href="https://github.com/simonJJJ">@simonJJJ</a>&#39;s PR.</p>
<p>[24/08/27] We supported <strong><a href="https://github.com/linkedin/Liger-Kernel">Liger Kernel</a></strong>. Try <code>enable_liger_kernel: true</code> for efficient training.</p>
<p>[24/08/09] We supported <strong><a href="https://github.com/zyushun/Adam-mini">Adam-mini</a></strong> optimizer. See <a href="examples/README.md">examples</a> for usage. Thank <a href="https://github.com/relic-yuexi">@relic-yuexi</a>&#39;s PR.</p>
<p>[24/07/04] We supported <a href="https://github.com/MeetKai/functionary/tree/main/functionary/train/packing">contamination-free packed training</a>. Use <code>neat_packing: true</code> to activate it. Thank <a href="https://github.com/chuan298">@chuan298</a>&#39;s PR.</p>
<p>[24/06/16] We supported <strong><a href="https://arxiv.org/abs/2404.02948">PiSSA</a></strong> algorithm. See <a href="examples/README.md">examples</a> for usage.</p>
<p>[24/06/07] We supported fine-tuning the <strong><a href="https://qwenlm.github.io/blog/qwen2/">Qwen2</a></strong> and <strong><a href="https://github.com/THUDM/GLM-4">GLM-4</a></strong> models.</p>
<p>[24/05/26] We supported <strong><a href="https://arxiv.org/abs/2405.14734">SimPO</a></strong> algorithm for preference learning. See <a href="examples/README.md">examples</a> for usage.</p>
<p>[24/05/20] We supported fine-tuning the <strong>PaliGemma</strong> series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with <code>paligemma</code> template for chat completion.</p>
<p>[24/05/18] We supported <strong><a href="https://arxiv.org/abs/2402.01306">KTO</a></strong> algorithm for preference learning. See <a href="examples/README.md">examples</a> for usage.</p>
<p>[24/05/14] We supported training and inference on the Ascend NPU devices. Check <a href="#installation">installation</a> section for details.</p>
<p>[24/04/26] We supported fine-tuning the <strong>LLaVA-1.5</strong> multimodal LLMs. See <a href="examples/README.md">examples</a> for usage.</p>
<p>[24/04/22] We provided a <strong><a href="https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing">Colab notebook</a></strong> for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check <a href="https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat">Llama3-8B-Chinese-Chat</a> and <a href="https://huggingface.co/zhichen/Llama3-Chinese">Llama3-Chinese</a> for details.</p>
<p>[24/04/21] We supported <strong><a href="https://arxiv.org/abs/2404.02258">Mixture-of-Depths</a></strong> according to <a href="https://github.com/astramind-ai/Mixture-of-depths">AstraMindAI&#39;s implementation</a>. See <a href="examples/README.md">examples</a> for usage.</p>
<p>[24/04/16] We supported <strong><a href="https://arxiv.org/abs/2404.02827">BAdam</a></strong> optimizer. See <a href="examples/README.md">examples</a> for usage.</p>
<p>[24/04/16] We supported <strong><a href="https://github.com/unslothai/unsloth">unsloth</a></strong>&#39;s long-sequence training (Llama-2-7B-56k within 24GB). It achieves <strong>117%</strong> speed and <strong>50%</strong> memory compared with FlashAttention-2, more benchmarks can be found in <a href="https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison">this page</a>.</p>
<p>[24/03/31] We supported <strong><a href="https://arxiv.org/abs/2403.07691">ORPO</a></strong>. See <a href="examples/README.md">examples</a> for usage.</p>
<p>[24/03/21] Our paper &quot;<a href="https://arxiv.org/abs/2403.13372">LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models</a>&quot; is available at arXiv!</p>
<p>[24/03/20] We supported <strong>FSDP+QLoRA</strong> that fine-tunes a 70B model on 2x24GB GPUs. See <a href="examples/README.md">examples</a> for usage.</p>
<p>[24/03/13] We supported <strong><a href="https://arxiv.org/abs/2402.12354">LoRA+</a></strong>. See <a href="examples/README.md">examples</a> for usage.</p>
<p>[24/03/07] We supported <strong><a href="https://arxiv.org/abs/2403.03507">GaLore</a></strong> optimizer. See <a href="examples/README.md">examples</a> for usage.</p>
<p>[24/03/07] We integrated <strong><a href="https://github.com/vllm-project/vllm">vLLM</a></strong> for faster and concurrent inference. Try <code>infer_backend: vllm</code> to enjoy <strong>270%</strong> inference speed.</p>
<p>[24/02/28] We supported weight-decomposed LoRA (<strong><a href="https://arxiv.org/abs/2402.09353">DoRA</a></strong>). Try <code>use_dora: true</code> to activate DoRA training.</p>
<p>[24/02/15] We supported <strong>block expansion</strong> proposed by <a href="https://github.com/TencentARC/LLaMA-Pro">LLaMA Pro</a>. See <a href="examples/README.md">examples</a> for usage.</p>
<p>[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this <a href="https://qwenlm.github.io/blog/qwen1.5/">blog post</a> for details.</p>
<p>[24/01/18] We supported <strong>agent tuning</strong> for most models, equipping model with tool using abilities by fine-tuning with <code>dataset: glaive_toolcall_en</code>.</p>
<p>[23/12/23] We supported <strong><a href="https://github.com/unslothai/unsloth">unsloth</a></strong>&#39;s implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try <code>use_unsloth: true</code> argument to activate unsloth patch. It achieves <strong>170%</strong> speed in our benchmark, check <a href="https://github.com/hiyouga/LLaMA-Factory/wiki/Performance-comparison">this page</a> for details.</p>
<p>[23/12/12] We supported fine-tuning the latest MoE model <strong><a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1">Mixtral 8x7B</a></strong> in our framework. See hardware requirement <a href="#hardware-requirement">here</a>.</p>
<p>[23/12/01] We supported downloading pre-trained models and datasets from the <strong><a href="https://modelscope.cn/models">ModelScope Hub</a></strong>. See <a href="#download-from-modelscope-hub">this tutorial</a> for usage.</p>
<p>[23/10/21] We supported <strong><a href="https://arxiv.org/abs/2310.05914">NEFTune</a></strong> trick for fine-tuning. Try <code>neftune_noise_alpha: 5</code> argument to activate NEFTune.</p>
<p>[23/09/27] We supported <strong>$S^2$-Attn</strong> proposed by <a href="https://github.com/dvlab-research/LongLoRA">LongLoRA</a> for the LLaMA models. Try <code>shift_attn: true</code> argument to enable shift short attention.</p>
<p>[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See <a href="examples/README.md">examples</a> for usage.</p>
<p>[23/09/10] We supported <strong><a href="https://github.com/Dao-AILab/flash-attention">FlashAttention-2</a></strong>. Try <code>flash_attn: fa2</code> argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.</p>
<p>[23/08/12] We supported <strong>RoPE scaling</strong> to extend the context length of the LLaMA models. Try <code>rope_scaling: linear</code> argument in training and <code>rope_scaling: dynamic</code> argument at inference to extrapolate the position embeddings.</p>
<p>[23/08/11] We supported <strong><a href="https://arxiv.org/abs/2305.18290">DPO training</a></strong> for instruction-tuned models. See <a href="examples/README.md">examples</a> for usage.</p>
<p>[23/07/31] We supported <strong>dataset streaming</strong>. Try <code>streaming: true</code> and <code>max_steps: 10000</code> arguments to load your dataset in streaming mode.</p>
<p>[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos (<a href="https://huggingface.co/hiyouga/Llama-2-Chinese-13b-chat">LLaMA-2</a> / <a href="https://huggingface.co/hiyouga/Baichuan-13B-sft">Baichuan</a>) for details.</p>
<p>[23/07/18] We developed an <strong>all-in-one Web UI</strong> for training, evaluation and inference. Try <code>train_web.py</code> to fine-tune models in your Web browser. Thank <a href="https://github.com/KanadeSiina">@KanadeSiina</a> and <a href="https://github.com/codemayq">@codemayq</a> for their efforts in the development.</p>
<p>[23/07/09] We released <strong><a href="https://github.com/hiyouga/FastEdit">FastEdit</a></strong> ‚ö°ü©π, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow <a href="https://github.com/hiyouga/FastEdit">FastEdit</a> if you are interested.</p>
<p>[23/06/29] We provided a <strong>reproducible example</strong> of training a chat model using instruction-following datasets, see <a href="https://huggingface.co/hiyouga/Baichuan-7B-sft">Baichuan-7B-sft</a> for details.</p>
<p>[23/06/22] We aligned the <a href="src/api_demo.py">demo API</a> with the <a href="https://platform.openai.com/docs/api-reference/chat">OpenAI&#39;s</a> format where you can insert the fine-tuned model in <strong>arbitrary ChatGPT-based applications</strong>.</p>
<p>[23/06/03] We supported quantized training and inference (aka <strong><a href="https://github.com/artidoro/qlora">QLoRA</a></strong>). See <a href="examples/README.md">examples</a> for usage.</p>
</details>

<blockquote>
<p>[!TIP]
If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.</p>
</blockquote>
<h2>Supported Models</h2>
<table>
<thead>
<tr>
<th>Model</th>
<th>Model size</th>
<th>Template</th>
</tr>
</thead>
<tbody><tr>
<td><a href="https://huggingface.co/baichuan-inc">Baichuan 2</a></td>
<td>7B/13B</td>
<td>baichuan2</td>
</tr>
<tr>
<td><a href="https://huggingface.co/bigscience">BLOOM/BLOOMZ</a></td>
<td>560M/1.1B/1.7B/3B/7.1B/176B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/THUDM">ChatGLM3</a></td>
<td>6B</td>
<td>chatglm3</td>
</tr>
<tr>
<td><a href="https://huggingface.co/CohereForAI">Command R</a></td>
<td>35B/104B</td>
<td>cohere</td>
</tr>
<tr>
<td><a href="https://huggingface.co/deepseek-ai">DeepSeek (Code/MoE)</a></td>
<td>7B/16B/67B/236B</td>
<td>deepseek</td>
</tr>
<tr>
<td><a href="https://huggingface.co/deepseek-ai">DeepSeek 2.5/3</a></td>
<td>236B/671B</td>
<td>deepseek3</td>
</tr>
<tr>
<td><a href="https://huggingface.co/deepseek-ai">DeepSeek R1 (Distill)</a></td>
<td>1.5B/7B/8B/14B/32B/70B/671B</td>
<td>deepseekr1</td>
</tr>
<tr>
<td><a href="https://huggingface.co/baidu">ERNIE-4.5</a></td>
<td>0.3B/21B/300B</td>
<td>ernie/ernie_nothink</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tiiuae">Falcon</a></td>
<td>7B/11B/40B/180B</td>
<td>falcon</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tiiuae">Falcon-H1</a></td>
<td>0.5B/1.5B/3B/7B/34B</td>
<td>falcon_h1</td>
</tr>
<tr>
<td><a href="https://huggingface.co/google">Gemma/Gemma 2/CodeGemma</a></td>
<td>2B/7B/9B/27B</td>
<td>gemma/gemma2</td>
</tr>
<tr>
<td><a href="https://huggingface.co/google">Gemma 3/Gemma 3n</a></td>
<td>270M/1B/4B/6B/8B/12B/27B</td>
<td>gemma3/gemma3n</td>
</tr>
<tr>
<td><a href="https://huggingface.co/zai-org">GLM-4/GLM-4-0414/GLM-Z1</a></td>
<td>9B/32B</td>
<td>glm4/glmz1</td>
</tr>
<tr>
<td><a href="https://huggingface.co/zai-org">GLM-4.1V</a></td>
<td>9B</td>
<td>glm4v</td>
</tr>
<tr>
<td><a href="https://huggingface.co/zai-org">GLM-4.5/GLM-4.5V</a></td>
<td>106B/355B</td>
<td>glm4_moe/glm4v_moe</td>
</tr>
<tr>
<td><a href="https://huggingface.co/openai-community">GPT-2</a></td>
<td>0.1B/0.4B/0.8B/1.5B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/openai">GPT-OSS</a></td>
<td>20B/120B</td>
<td>gpt</td>
</tr>
<tr>
<td><a href="https://huggingface.co/ibm-granite">Granite 3.0-3.3</a></td>
<td>1B/2B/3B/8B</td>
<td>granite3</td>
</tr>
<tr>
<td><a href="https://huggingface.co/ibm-granite">Granite 4</a></td>
<td>7B</td>
<td>granite4</td>
</tr>
<tr>
<td><a href="https://huggingface.co/tencent/">Hunyuan (MT)</a></td>
<td>7B</td>
<td>hunyuan</td>
</tr>
<tr>
<td><a href="https://huggingface.co/IndexTeam">Index</a></td>
<td>1.9B</td>
<td>index</td>
</tr>
<tr>
<td><a href="https://huggingface.co/internlm">InternLM 2-3</a></td>
<td>7B/8B/20B</td>
<td>intern2</td>
</tr>
<tr>
<td><a href="https://huggingface.co/OpenGVLab">InternVL 2.5-3.5</a></td>
<td>1B/2B/4B/8B/14B/30B/38B/78B/241B</td>
<td>intern_vl</td>
</tr>
<tr>
<td><a href="https://huggingface.co/internlm/">InternLM/Intern-S1-mini</a></td>
<td>8B</td>
<td>intern_s1</td>
</tr>
<tr>
<td><a href="https://huggingface.co/moonshotai">Kimi-VL</a></td>
<td>16B</td>
<td>kimi_vl</td>
</tr>
<tr>
<td><a href="https://huggingface.co/inclusionAI">Ling 2.0 (mini/flash)</a></td>
<td>16B/100B</td>
<td>bailing_v2</td>
</tr>
<tr>
<td><a href="https://github.com/facebookresearch/llama">Llama</a></td>
<td>7B/13B/33B/65B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama">Llama 2</a></td>
<td>7B/13B/70B</td>
<td>llama2</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama">Llama 3-3.3</a></td>
<td>1B/3B/8B/70B</td>
<td>llama3</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama">Llama 4</a></td>
<td>109B/402B</td>
<td>llama4</td>
</tr>
<tr>
<td><a href="https://huggingface.co/meta-llama">Llama 3.2 Vision</a></td>
<td>11B/90B</td>
<td>mllama</td>
</tr>
<tr>
<td><a href="https://huggingface.co/llava-hf">LLaVA-1.5</a></td>
<td>7B/13B</td>
<td>llava</td>
</tr>
<tr>
<td><a href="https://huggingface.co/llava-hf">LLaVA-NeXT</a></td>
<td>7B/8B/13B/34B/72B/110B</td>
<td>llava_next</td>
</tr>
<tr>
<td><a href="https://huggingface.co/llava-hf">LLaVA-NeXT-Video</a></td>
<td>7B/34B</td>
<td>llava_next_video</td>
</tr>
<tr>
<td><a href="https://huggingface.co/XiaomiMiMo">MiMo</a></td>
<td>7B</td>
<td>mimo</td>
</tr>
<tr>
<td><a href="https://huggingface.co/openbmb">MiniCPM 1-4.1</a></td>
<td>0.5B/1B/2B/4B/8B</td>
<td>cpm/cpm3/cpm4</td>
</tr>
<tr>
<td><a href="https://huggingface.co/openbmb">MiniCPM-o-2.6/MiniCPM-V-2.6</a></td>
<td>8B</td>
<td>minicpm_o/minicpm_v</td>
</tr>
<tr>
<td><a href="https://huggingface.co/mistralai">Ministral/Mistral-Nemo</a></td>
<td>8B/12B</td>
<td>ministral</td>
</tr>
<tr>
<td><a href="https://huggingface.co/mistralai">Mistral/Mixtral</a></td>
<td>7B/8x7B/8x22B</td>
<td>mistral</td>
</tr>
<tr>
<td><a href="https://huggingface.co/mistralai">Mistral Small</a></td>
<td>24B</td>
<td>mistral_small</td>
</tr>
<tr>
<td><a href="https://huggingface.co/allenai">OLMo</a></td>
<td>1B/7B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/google">PaliGemma/PaliGemma2</a></td>
<td>3B/10B/28B</td>
<td>paligemma</td>
</tr>
<tr>
<td><a href="https://huggingface.co/microsoft">Phi-1.5/Phi-2</a></td>
<td>1.3B/2.7B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/microsoft">Phi-3/Phi-3.5</a></td>
<td>4B/14B</td>
<td>phi</td>
</tr>
<tr>
<td><a href="https://huggingface.co/microsoft">Phi-3-small</a></td>
<td>7B</td>
<td>phi_small</td>
</tr>
<tr>
<td><a href="https://huggingface.co/microsoft">Phi-4</a></td>
<td>14B</td>
<td>phi4</td>
</tr>
<tr>
<td><a href="https://huggingface.co/mistralai">Pixtral</a></td>
<td>12B</td>
<td>pixtral</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen">Qwen (1-2.5) (Code/Math/MoE/QwQ)</a></td>
<td>0.5B/1.5B/3B/7B/14B/32B/72B/110B</td>
<td>qwen</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen">Qwen3 (MoE/Instruct/Thinking/Next)</a></td>
<td>0.6B/1.7B/4B/8B/14B/32B/80B/235B</td>
<td>qwen3/qwen3_nothink</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen">Qwen2-Audio</a></td>
<td>7B</td>
<td>qwen2_audio</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen">Qwen2.5-Omni</a></td>
<td>3B/7B</td>
<td>qwen2_omni</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen">Qwen3-Omni</a></td>
<td>30B</td>
<td>qwen3_omni</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen">Qwen2-VL/Qwen2.5-VL/QVQ</a></td>
<td>2B/3B/7B/32B/72B</td>
<td>qwen2_vl</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Qwen">Qwen3-VL</a></td>
<td>2B/4B/8B/30B/32B/235B</td>
<td>qwen3_vl</td>
</tr>
<tr>
<td><a href="https://huggingface.co/ByteDance-Seed">Seed (OSS/Coder)</a></td>
<td>8B/36B</td>
<td>seed_oss/seed_coder</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Skywork">Skywork o1</a></td>
<td>8B</td>
<td>skywork_o1</td>
</tr>
<tr>
<td><a href="https://huggingface.co/bigcode">StarCoder 2</a></td>
<td>3B/7B/15B</td>
<td>-</td>
</tr>
<tr>
<td><a href="https://huggingface.co/Tele-AI">TeleChat2</a></td>
<td>3B/7B/35B/115B</td>
<td>telechat2</td>
</tr>
<tr>
<td><a href="https://huggingface.co/xverse">XVERSE</a></td>
<td>7B/13B/65B</td>
<td>xverse</td>
</tr>
<tr>
<td><a href="https://huggingface.co/01-ai">Yi/Yi-1.5 (Code)</a></td>
<td>1.5B/6B/9B/34B</td>
<td>yi</td>
</tr>
<tr>
<td><a href="https://huggingface.co/01-ai">Yi-VL</a></td>
<td>6B/34B</td>
<td>yi_vl</td>
</tr>
<tr>
<td><a href="https://huggingface.co/IEITYuan">Yuan 2</a></td>
<td>2B/51B/102B</td>
<td>yuan</td>
</tr>
</tbody></table>
<blockquote>
<p>[!NOTE]
For the &quot;base&quot; models, the <code>template</code> argument can be chosen from <code>default</code>, <code>alpaca</code>, <code>vicuna</code> etc. But make sure to use the <strong>corresponding template</strong> for the &quot;instruct/chat&quot; models.</p>
<p>If the model has both reasoning and non-reasoning versions, please use the <code>_nothink</code> suffix to distinguish between them. For example, <code>qwen3</code> and <code>qwen3_nothink</code>.</p>
<p>Remember to use the <strong>SAME</strong> template in training and inference.</p>
<p>*: You should install the <code>transformers</code> from main branch and use <code>DISABLE_VERSION_CHECK=1</code> to skip version check.</p>
<p>**: You need to install a specific version of <code>transformers</code> to use the corresponding model.</p>
</blockquote>
<p>Please refer to <a href="src/llamafactory/extras/constants.py">constants.py</a> for a full list of models we supported.</p>
<p>You also can add a custom chat template to <a href="src/llamafactory/data/template.py">template.py</a>.</p>
<h2>Supported Training Approaches</h2>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Full-tuning</th>
<th>Freeze-tuning</th>
<th>LoRA</th>
<th>QLoRA</th>
<th>OFT</th>
<th>QOFT</th>
</tr>
</thead>
<tbody><tr>
<td>Pre-Training</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
</tr>
<tr>
<td>Supervised Fine-Tuning</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
</tr>
<tr>
<td>Reward Modeling</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
</tr>
<tr>
<td>PPO Training</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
</tr>
<tr>
<td>DPO Training</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
</tr>
<tr>
<td>KTO Training</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
</tr>
<tr>
<td>ORPO Training</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
</tr>
<tr>
<td>SimPO Training</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
<td>:white_check_mark:</td>
</tr>
</tbody></table>
<blockquote>
<p>[!TIP]
The implementation details of PPO can be found in <a href="https://newfacade.github.io/notes-on-reinforcement-learning/17-ppo-trl.html">this blog</a>.</p>
</blockquote>
<h2>Provided Datasets</h2>
<details><summary>Pre-training datasets</summary>

<ul>
<li><a href="data/wiki_demo.txt">Wiki Demo (en)</a></li>
<li><a href="https://huggingface.co/datasets/tiiuae/falcon-refinedweb">RefinedWeb (en)</a></li>
<li><a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2">RedPajama V2 (en)</a></li>
<li><a href="https://huggingface.co/datasets/olm/olm-wikipedia-20221220">Wikipedia (en)</a></li>
<li><a href="https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered">Wikipedia (zh)</a></li>
<li><a href="https://huggingface.co/datasets/EleutherAI/pile">Pile (en)</a></li>
<li><a href="https://huggingface.co/datasets/Skywork/SkyPile-150B">SkyPile (zh)</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb">FineWeb (en)</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu">FineWeb-Edu (en)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/CCI3-HQ">CCI3-HQ (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/CCI3-Data">CCI3-Data (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/CCI4.0-M2-Base-v1">CCI4.0-M2-Base-v1 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/CCI4.0-M2-CoT-v1">CCI4.0-M2-CoT-v1 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/CCI4.0-M2-Extra-v1">CCI4.0-M2-Extra-v1 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/bigcode/the-stack">The Stack (en)</a></li>
<li><a href="https://huggingface.co/datasets/bigcode/starcoderdata">StarCoder (en)</a></li>
</ul>
</details>

<details><summary>Supervised fine-tuning datasets</summary>

<ul>
<li><a href="data/identity.json">Identity (en&amp;zh)</a></li>
<li><a href="https://github.com/tatsu-lab/stanford_alpaca">Stanford Alpaca (en)</a></li>
<li><a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca-3">Stanford Alpaca (zh)</a></li>
<li><a href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM">Alpaca GPT4 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/glaiveai/glaive-function-calling-v2">Glaive Function Calling V2 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/GAIR/lima">LIMA (en)</a></li>
<li><a href="https://huggingface.co/datasets/JosephusCheung/GuanacoDataset">Guanaco Dataset (multilingual)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/train_2M_CN">BELLE 2M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/train_1M_CN">BELLE 1M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/train_0.5M_CN">BELLE 0.5M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M">BELLE Dialogue 0.4M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/school_math_0.25M">BELLE School Math 0.25M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M">BELLE Multiturn Chat 0.8M (zh)</a></li>
<li><a href="https://github.com/thunlp/UltraChat">UltraChat (en)</a></li>
<li><a href="https://huggingface.co/datasets/garage-bAInd/Open-Platypus">OpenPlatypus (en)</a></li>
<li><a href="https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k">CodeAlpaca 20k (en)</a></li>
<li><a href="https://huggingface.co/datasets/QingyiSi/Alpaca-CoT">Alpaca CoT (multilingual)</a></li>
<li><a href="https://huggingface.co/datasets/Open-Orca/OpenOrca">OpenOrca (en)</a></li>
<li><a href="https://huggingface.co/datasets/Open-Orca/SlimOrca">SlimOrca (en)</a></li>
<li><a href="https://huggingface.co/datasets/TIGER-Lab/MathInstruct">MathInstruct (en)</a></li>
<li><a href="https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M">Firefly 1.1M (zh)</a></li>
<li><a href="https://huggingface.co/datasets/wiki_qa">Wiki QA (en)</a></li>
<li><a href="https://huggingface.co/datasets/suolyer/webqa">Web QA (zh)</a></li>
<li><a href="https://huggingface.co/datasets/zxbsmk/webnovel_cn">WebNovel (zh)</a></li>
<li><a href="https://huggingface.co/datasets/berkeley-nest/Nectar">Nectar (en)</a></li>
<li><a href="https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data">deepctrl (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/HasturOfficial/adgen">Advertise Generating (zh)</a></li>
<li><a href="https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k">ShareGPT Hyperfiltered (en)</a></li>
<li><a href="https://huggingface.co/datasets/shibing624/sharegpt_gpt4">ShareGPT4 (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k">UltraChat 200k (en)</a></li>
<li><a href="https://huggingface.co/datasets/BAAI/Infinity-Instruct">Infinity Instruct (zh)</a></li>
<li><a href="https://huggingface.co/datasets/THUDM/AgentInstruct">AgentInstruct (en)</a></li>
<li><a href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m">LMSYS Chat 1M (en)</a></li>
<li><a href="https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k">Evol Instruct V2 (en)</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceTB/cosmopedia">Cosmopedia (en)</a></li>
<li><a href="https://huggingface.co/datasets/hfl/stem_zh_instruction">STEM (zh)</a></li>
<li><a href="https://huggingface.co/datasets/hfl/ruozhiba_gpt4_turbo">Ruozhiba (zh)</a></li>
<li><a href="https://huggingface.co/datasets/m-a-p/neo_sft_phase2">Neo-sft (zh)</a></li>
<li><a href="https://huggingface.co/datasets/Magpie-Align/Magpie-Pro-300K-Filtered">Magpie-Pro-300K-Filtered (en)</a></li>
<li><a href="https://huggingface.co/datasets/argilla/magpie-ultra-v0.1">Magpie-ultra-v0.1 (en)</a></li>
<li><a href="https://huggingface.co/datasets/TIGER-Lab/WebInstructSub">WebInstructSub (en)</a></li>
<li><a href="https://huggingface.co/datasets/O1-OPEN/OpenO1-SFT">OpenO1-SFT (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k">Open-Thoughts (en)</a></li>
<li><a href="https://huggingface.co/datasets/open-r1/OpenR1-Math-220k">Open-R1-Math (en)</a></li>
<li><a href="https://huggingface.co/datasets/Congliu/Chinese-DeepSeek-R1-Distill-data-110k-SFT">Chinese-DeepSeek-R1-Distill (zh)</a></li>
<li><a href="https://huggingface.co/datasets/BUAADreamer/llava-en-zh-300k">LLaVA mixed (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/jugg1024/pokemon-gpt4o-captions">Pokemon-gpt4o-captions (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/oasst_de">Open Assistant (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/dolly-15k_de">Dolly 15k (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/alpaca-gpt4_de">Alpaca GPT4 (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/openschnabeltier_de">OpenSchnabeltier (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/evol-instruct_de">Evol Instruct (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/dolphin_de">Dolphin (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/booksum_de">Booksum (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/airoboros-3.0_de">Airoboros (de)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/ultra-chat_de">Ultrachat (de)</a></li>
</ul>
</details>

<details><summary>Preference datasets</summary>

<ul>
<li><a href="https://huggingface.co/datasets/hiyouga/DPO-En-Zh-20k">DPO mixed (en&amp;zh)</a></li>
<li><a href="https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized">UltraFeedback (en)</a></li>
<li><a href="https://huggingface.co/datasets/m-a-p/COIG-P">COIG-P (zh)</a></li>
<li><a href="https://huggingface.co/datasets/openbmb/RLHF-V-Dataset">RLHF-V (en)</a></li>
<li><a href="https://huggingface.co/datasets/Zhihui/VLFeedback">VLFeedback (en)</a></li>
<li><a href="https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset">RLAIF-V (en)</a></li>
<li><a href="https://huggingface.co/datasets/Intel/orca_dpo_pairs">Orca DPO Pairs (en)</a></li>
<li><a href="https://huggingface.co/datasets/Anthropic/hh-rlhf">HH-RLHF (en)</a></li>
<li><a href="https://huggingface.co/datasets/berkeley-nest/Nectar">Nectar (en)</a></li>
<li><a href="https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de">Orca DPO (de)</a></li>
<li><a href="https://huggingface.co/datasets/argilla/kto-mix-15k">KTO mixed (en)</a></li>
</ul>
</details>

<p>Some datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.</p>
<pre><code class="language-bash">pip install &quot;huggingface_hub&lt;1.0.0&quot;
huggingface-cli login
</code></pre>
<h2>Requirement</h2>
<table>
<thead>
<tr>
<th>Mandatory</th>
<th>Minimum</th>
<th>Recommend</th>
</tr>
</thead>
<tbody><tr>
<td>python</td>
<td>3.9</td>
<td>3.10</td>
</tr>
<tr>
<td>torch</td>
<td>2.0.0</td>
<td>2.6.0</td>
</tr>
<tr>
<td>torchvision</td>
<td>0.15.0</td>
<td>0.21.0</td>
</tr>
<tr>
<td>transformers</td>
<td>4.49.0</td>
<td>4.50.0</td>
</tr>
<tr>
<td>datasets</td>
<td>2.16.0</td>
<td>3.2.0</td>
</tr>
<tr>
<td>accelerate</td>
<td>0.34.0</td>
<td>1.2.1</td>
</tr>
<tr>
<td>peft</td>
<td>0.14.0</td>
<td>0.15.1</td>
</tr>
<tr>
<td>trl</td>
<td>0.8.6</td>
<td>0.9.6</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>Optional</th>
<th>Minimum</th>
<th>Recommend</th>
</tr>
</thead>
<tbody><tr>
<td>CUDA</td>
<td>11.6</td>
<td>12.2</td>
</tr>
<tr>
<td>deepspeed</td>
<td>0.10.0</td>
<td>0.16.4</td>
</tr>
<tr>
<td>bitsandbytes</td>
<td>0.39.0</td>
<td>0.43.1</td>
</tr>
<tr>
<td>vllm</td>
<td>0.4.3</td>
<td>0.8.2</td>
</tr>
<tr>
<td>flash-attn</td>
<td>2.5.6</td>
<td>2.7.2</td>
</tr>
</tbody></table>
<h3>Hardware Requirement</h3>
<p>* <em>estimated</em></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Bits</th>
<th>7B</th>
<th>14B</th>
<th>30B</th>
<th>70B</th>
<th><code>x</code>B</th>
</tr>
</thead>
<tbody><tr>
<td>Full (<code>bf16</code> or <code>fp16</code>)</td>
<td>32</td>
<td>120GB</td>
<td>240GB</td>
<td>600GB</td>
<td>1200GB</td>
<td><code>18x</code>GB</td>
</tr>
<tr>
<td>Full (<code>pure_bf16</code>)</td>
<td>16</td>
<td>60GB</td>
<td>120GB</td>
<td>300GB</td>
<td>600GB</td>
<td><code>8x</code>GB</td>
</tr>
<tr>
<td>Freeze/LoRA/GaLore/APOLLO/BAdam/OFT</td>
<td>16</td>
<td>16GB</td>
<td>32GB</td>
<td>64GB</td>
<td>160GB</td>
<td><code>2x</code>GB</td>
</tr>
<tr>
<td>QLoRA / QOFT</td>
<td>8</td>
<td>10GB</td>
<td>20GB</td>
<td>40GB</td>
<td>80GB</td>
<td><code>x</code>GB</td>
</tr>
<tr>
<td>QLoRA / QOFT</td>
<td>4</td>
<td>6GB</td>
<td>12GB</td>
<td>24GB</td>
<td>48GB</td>
<td><code>x/2</code>GB</td>
</tr>
<tr>
<td>QLoRA / QOFT</td>
<td>2</td>
<td>4GB</td>
<td>8GB</td>
<td>16GB</td>
<td>24GB</td>
<td><code>x/4</code>GB</td>
</tr>
</tbody></table>
<h2>Getting Started</h2>
<h3>Installation</h3>
<blockquote>
<p>[!IMPORTANT]
Installation is mandatory.</p>
</blockquote>
<h4>Install from Source</h4>
<pre><code class="language-bash">git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e &quot;.[torch,metrics]&quot; --no-build-isolation
</code></pre>
<p>Extra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, openmind, swanlab, dev</p>
<h4>Install from Docker Image</h4>
<pre><code class="language-bash">docker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest
</code></pre>
<p>This image is built on Ubuntu 22.04 (x86_64), CUDA 12.4, Python 3.11, PyTorch 2.6.0, and Flash-attn 2.7.4.</p>
<p>Find the pre-built images: <a href="https://hub.docker.com/r/hiyouga/llamafactory/tags">https://hub.docker.com/r/hiyouga/llamafactory/tags</a></p>
<p>Please refer to <a href="#build-docker">build docker</a> to build the image yourself.</p>
<details><summary>Setting up a virtual environment with <b>uv</b></summary>

<p>Create an isolated Python environment with <a href="https://github.com/astral-sh/uv">uv</a>:</p>
<pre><code class="language-bash">uv sync --extra torch --extra metrics --prerelease=allow
</code></pre>
<p>Run LLaMA-Factory in the isolated environment:</p>
<pre><code class="language-bash">uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml
</code></pre>
</details>

<details><summary>For Windows users</summary>

<h4>Install PyTorch</h4>
<p>You need to manually install the GPU version of PyTorch on the Windows platform. Please refer to the <a href="https://pytorch.org/get-started/locally/">official website</a> and the following command to install PyTorch with CUDA support:</p>
<pre><code class="language-bash">pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
python -c &quot;import torch; print(torch.cuda.is_available())&quot;
</code></pre>
<p>If you see <code>True</code> then you have successfully installed PyTorch with CUDA support.</p>
<p>Try <code>dataloader_num_workers: 0</code> if you encounter <code>Can&#39;t pickle local object</code> error.</p>
<h4>Install BitsAndBytes</h4>
<p>If you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of <code>bitsandbytes</code> library, which supports CUDA 11.1 to 12.2, please select the appropriate <a href="https://github.com/jllllll/bitsandbytes-windows-webui/releases/tag/wheels">release version</a> based on your CUDA version.</p>
<pre><code class="language-bash">pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl
</code></pre>
<h4>Install Flash Attention-2</h4>
<p>To enable FlashAttention-2 on the Windows platform, please use the script from <a href="https://huggingface.co/lldacing/flash-attention-windows-wheel">flash-attention-windows-wheel</a> to compile and install it by yourself.</p>
</details>

<details><summary>For Ascend NPU users</summary>

<p>To install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: <code>pip install -e &quot;.[torch-npu,metrics]&quot;</code>. Additionally, you need to install the <strong><a href="https://www.hiascend.com/developer/download/community/result?module=cann">Ascend CANN Toolkit and Kernels</a></strong>. Please follow the <a href="https://www.hiascend.com/document/detail/en/CANNCommunityEdition/600alphaX/softwareinstall/instg/atlasdeploy_03_0031.html">installation tutorial</a> or use the following commands:</p>
<pre><code class="language-bash"># replace the url according to your CANN version and devices
# install CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-&quot;$(uname -i)&quot;.run
bash Ascend-cann-toolkit_8.0.0.alpha002_linux-&quot;$(uname -i)&quot;.run --install

# install CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-&quot;$(uname -i)&quot;.run
bash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-&quot;$(uname -i)&quot;.run --install

# set env variables
source /usr/local/Ascend/ascend-toolkit/set_env.sh
</code></pre>
<table>
<thead>
<tr>
<th>Requirement</th>
<th>Minimum</th>
<th>Recommend</th>
</tr>
</thead>
<tbody><tr>
<td>CANN</td>
<td>8.0.RC1</td>
<td>8.0.0.alpha002</td>
</tr>
<tr>
<td>torch</td>
<td>2.1.0</td>
<td>2.4.0</td>
</tr>
<tr>
<td>torch-npu</td>
<td>2.1.0</td>
<td>2.4.0.post2</td>
</tr>
<tr>
<td>deepspeed</td>
<td>0.13.2</td>
<td>0.13.2</td>
</tr>
<tr>
<td>vllm-ascend</td>
<td>-</td>
<td>0.7.3</td>
</tr>
</tbody></table>
<p>Remember to use <code>ASCEND_RT_VISIBLE_DEVICES</code> instead of <code>CUDA_VISIBLE_DEVICES</code> to specify the device to use.</p>
<p>If you cannot infer model on NPU devices, try setting <code>do_sample: false</code> in the configurations.</p>
<p>Download the pre-built Docker images: <a href="http://mirrors.cn-central-221.ovaijisuan.com/detail/130.html">32GB</a> | <a href="http://mirrors.cn-central-221.ovaijisuan.com/detail/131.html">64GB</a></p>
<h4>Install BitsAndBytes</h4>
<p>To use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:</p>
<ol>
<li>Manually compile bitsandbytes: Refer to <a href="https://huggingface.co/docs/bitsandbytes/installation?backend=Ascend+NPU&platform=Ascend+NPU">the installation documentation</a> for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.</li>
</ol>
<pre><code class="language-bash"># Install bitsandbytes from source
# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch
git clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git
cd bitsandbytes/

# Install dependencies
pip install -r requirements-dev.txt

# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference
apt-get install -y build-essential cmake

# Compile &amp; install  
cmake -DCOMPUTE_BACKEND=npu -S .
make
pip install .
</code></pre>
<ol start="2">
<li>Install transformers from the main branch.</li>
</ol>
<pre><code class="language-bash">git clone -b main https://github.com/huggingface/transformers.git
cd transformers
pip install .
</code></pre>
<ol start="3">
<li>Set <code>double_quantization: false</code> in the configuration. You can refer to the <a href="examples/train_qlora/llama3_lora_sft_bnb_npu.yaml">example</a>.</li>
</ol>
</details>

<h3>Data Preparation</h3>
<p>Please refer to <a href="data/README.md">data/README.md</a> for checking the details about the format of dataset files. You can use datasets on HuggingFace / ModelScope / Modelers hub, load the dataset in local disk, or specify a path to s3/gcs cloud storage.</p>
<blockquote>
<p>[!NOTE]
Please update <code>data/dataset_info.json</code> to use your custom dataset.</p>
</blockquote>
<p>You can also use <strong><a href="https://github.com/ConardLi/easy-dataset">Easy Dataset</a></strong>, <strong><a href="https://github.com/OpenDCAI/DataFlow">DataFlow</a></strong> and <strong><a href="https://github.com/open-sciencelab/GraphGen">GraphGen</a></strong> to create synthetic data for fine-tuning.</p>
<h3>Quickstart</h3>
<p>Use the following 3 commands to run LoRA <strong>fine-tuning</strong>, <strong>inference</strong> and <strong>merging</strong> of the Llama3-8B-Instruct model, respectively.</p>
<pre><code class="language-bash">llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml
</code></pre>
<p>See <a href="examples/README.md">examples/README.md</a> for advanced usage (including distributed training).</p>
<blockquote>
<p>[!TIP]
Use <code>llamafactory-cli help</code> to show help information.</p>
<p>Read <a href="https://github.com/hiyouga/LLaMA-Factory/issues/4614">FAQs</a> first if you encounter any problems.</p>
</blockquote>
<h3>Fine-Tuning with LLaMA Board GUI (powered by <a href="https://github.com/gradio-app/gradio">Gradio</a>)</h3>
<pre><code class="language-bash">llamafactory-cli webui
</code></pre>
<h3>LLaMA Factory Online</h3>
<p>Read our <a href="https://docs.llamafactory.com.cn/docs/documents/quickstart/getstarted/?utm_source=LLaMA-Factory">documentation</a>.</p>
<h3>Build Docker</h3>
<p>For CUDA users:</p>
<pre><code class="language-bash">cd docker/docker-cuda/
docker compose up -d
docker compose exec llamafactory bash
</code></pre>
<p>For Ascend NPU users:</p>
<pre><code class="language-bash">cd docker/docker-npu/
docker compose up -d
docker compose exec llamafactory bash
</code></pre>
<p>For AMD ROCm users:</p>
<pre><code class="language-bash">cd docker/docker-rocm/
docker compose up -d
docker compose exec llamafactory bash
</code></pre>
<details><summary>Build without Docker Compose</summary>

<p>For CUDA users:</p>
<pre><code class="language-bash">docker build -f ./docker/docker-cuda/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host --gpus=all \
    -p 7860:7860 \
    -p 8000:8000 \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
</code></pre>
<p>For Ascend NPU users:</p>
<pre><code class="language-bash">docker build -f ./docker/docker-npu/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=torch-npu,metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host \
    -v /usr/local/dcmi:/usr/local/dcmi \
    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
    -v /etc/ascend_install.info:/etc/ascend_install.info \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/davinci0 \
    --device /dev/davinci_manager \
    --device /dev/devmm_svm \
    --device /dev/hisi_hdc \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
</code></pre>
<p>For AMD ROCm users:</p>
<pre><code class="language-bash">docker build -f ./docker/docker-rocm/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/kfd \
    --device /dev/dri \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash
</code></pre>
</details>

<details><summary>Use Docker volumes</summary>

<p>You can uncomment <code>VOLUME [ &quot;/root/.cache/huggingface&quot;, &quot;/app/shared_data&quot;, &quot;/app/output&quot; ]</code> in the Dockerfile to use data volumes.</p>
<p>When building the Docker image, use <code>-v ./hf_cache:/root/.cache/huggingface</code> argument to mount the local directory to the container. The following data volumes are available.</p>
<ul>
<li><code>hf_cache</code>: Utilize Hugging Face cache on the host machine.</li>
<li><code>shared_data</code>: The directionary to store datasets on the host machine.</li>
<li><code>output</code>: Set export dir to this location so that the merged result can be accessed directly on the host machine.</li>
</ul>
</details>

<h3>Deploy with OpenAI-style API and vLLM</h3>
<pre><code class="language-bash">API_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true
</code></pre>
<blockquote>
<p>[!TIP]
Visit <a href="https://platform.openai.com/docs/api-reference/chat/create">this page</a> for API document.</p>
<p>Examples: <a href="scripts/api_example/test_image.py">Image understanding</a> | <a href="scripts/api_example/test_toolcall.py">Function calling</a></p>
</blockquote>
<h3>Download from ModelScope Hub</h3>
<p>If you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.</p>
<pre><code class="language-bash">export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows
</code></pre>
<p>Train the model by specifying a model ID of the ModelScope Hub as the <code>model_name_or_path</code>. You can find a full list of model IDs at <a href="https://modelscope.cn/models">ModelScope Hub</a>, e.g., <code>LLM-Research/Meta-Llama-3-8B-Instruct</code>.</p>
<h3>Download from Modelers Hub</h3>
<p>You can also use Modelers Hub to download models and datasets.</p>
<pre><code class="language-bash">export USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows
</code></pre>
<p>Train the model by specifying a model ID of the Modelers Hub as the <code>model_name_or_path</code>. You can find a full list of model IDs at <a href="https://modelers.cn/models">Modelers Hub</a>, e.g., <code>TeleAI/TeleChat-7B-pt</code>.</p>
<h3>Use W&amp;B Logger</h3>
<p>To use <a href="https://wandb.ai">Weights &amp; Biases</a> for logging experimental results, you need to add the following arguments to yaml files.</p>
<pre><code class="language-yaml">report_to: wandb
run_name: test_run # optional
</code></pre>
<p>Set <code>WANDB_API_KEY</code> to <a href="https://wandb.ai/authorize">your key</a> when launching training tasks to log in with your W&amp;B account.</p>
<h3>Use SwanLab Logger</h3>
<p>To use <a href="https://github.com/SwanHubX/SwanLab">SwanLab</a> for logging experimental results, you need to add the following arguments to yaml files.</p>
<pre><code class="language-yaml">use_swanlab: true
swanlab_run_name: test_run # optional
</code></pre>
<p>When launching training tasks, you can log in to SwanLab in three ways:</p>
<ol>
<li>Add <code>swanlab_api_key=&lt;your_api_key&gt;</code> to the yaml file, and set it to your <a href="https://swanlab.cn/settings">API key</a>.</li>
<li>Set the environment variable <code>SWANLAB_API_KEY</code> to your <a href="https://swanlab.cn/settings">API key</a>.</li>
<li>Use the <code>swanlab login</code> command to complete the login.</li>
</ol>
<h2>Projects using LLaMA Factory</h2>
<p>If you have a project that should be incorporated, please contact via email or create a pull request.</p>
<details><summary>Click to show</summary>

<ol>
<li>Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. <a href="https://arxiv.org/abs/2308.02223">[arxiv]</a></li>
<li>Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. <a href="https://arxiv.org/abs/2308.10092">[arxiv]</a></li>
<li>Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. <a href="https://arxiv.org/abs/2308.10526">[arxiv]</a></li>
<li>Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. <a href="https://arxiv.org/abs/2311.07816">[arxiv]</a></li>
<li>Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. <a href="https://arxiv.org/abs/2312.15710">[arxiv]</a></li>
<li>Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. <a href="https://arxiv.org/abs/2401.04319">[arxiv]</a></li>
<li>Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. <a href="https://arxiv.org/abs/2401.07286">[arxiv]</a></li>
<li>Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. <a href="https://arxiv.org/abs/2402.05904">[arxiv]</a></li>
<li>Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. <a href="https://arxiv.org/abs/2402.07625">[arxiv]</a></li>
<li>Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. <a href="https://arxiv.org/abs/2402.11176">[arxiv]</a></li>
<li>Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. <a href="https://arxiv.org/abs/2402.11187">[arxiv]</a></li>
<li>Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. <a href="https://arxiv.org/abs/2402.11746">[arxiv]</a></li>
<li>Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. <a href="https://arxiv.org/abs/2402.11801">[arxiv]</a></li>
<li>Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. <a href="https://arxiv.org/abs/2402.11809">[arxiv]</a></li>
<li>Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. <a href="https://arxiv.org/abs/2402.11819">[arxiv]</a></li>
<li>Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. <a href="https://arxiv.org/abs/2402.12204">[arxiv]</a></li>
<li>Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. <a href="https://arxiv.org/abs/2402.14714">[arxiv]</a></li>
<li>Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. <a href="https://arxiv.org/abs/2402.15043">[arxiv]</a></li>
<li>Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. <a href="https://arxiv.org/abs/2403.02333">[arxiv]</a></li>
<li>Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. <a href="https://arxiv.org/abs/2403.03419">[arxiv]</a></li>
<li>Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. <a href="https://arxiv.org/abs/2403.08228">[arxiv]</a></li>
<li>Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. <a href="https://arxiv.org/abs/2403.09073">[arxiv]</a></li>
<li>Zhang et al. EDT: Improving Large Language Models&#39; Generation by Entropy-based Dynamic Temperature Sampling. 2024. <a href="https://arxiv.org/abs/2403.14541">[arxiv]</a></li>
<li>Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. <a href="https://arxiv.org/abs/2403.15246">[arxiv]</a></li>
<li>Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. <a href="https://arxiv.org/abs/2403.16008">[arxiv]</a></li>
<li>Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. <a href="https://arxiv.org/abs/2403.16443">[arxiv]</a></li>
<li>Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. <a href="https://arxiv.org/abs/2404.00604">[arxiv]</a></li>
<li>Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. <a href="https://arxiv.org/abs/2404.02827">[arxiv]</a></li>
<li>Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. <a href="https://arxiv.org/abs/2404.04167">[arxiv]</a></li>
<li>Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. <a href="https://arxiv.org/abs/2404.04316">[arxiv]</a></li>
<li>Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. <a href="https://arxiv.org/abs/2404.07084">[arxiv]</a></li>
<li>Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. <a href="https://arxiv.org/abs/2404.09836">[arxiv]</a></li>
<li>Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. <a href="https://arxiv.org/abs/2404.11581">[arxiv]</a></li>
<li>Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. <a href="https://arxiv.org/abs/2404.14215">[arxiv]</a></li>
<li>Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. <a href="https://arxiv.org/abs/2404.16621">[arxiv]</a></li>
<li>Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. <a href="https://arxiv.org/abs/2404.17140">[arxiv]</a></li>
<li>Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. <a href="https://arxiv.org/abs/2404.18585">[arxiv]</a></li>
<li>Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. <a href="https://arxiv.org/abs/2405.04760">[arxiv]</a></li>
<li>Dammu et al. &quot;They are uncultured&quot;: Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. <a href="https://arxiv.org/abs/2405.05378">[arxiv]</a></li>
<li>Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. <a href="https://arxiv.org/abs/2405.09055">[arxiv]</a></li>
<li>Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. <a href="https://arxiv.org/abs/2405.12739">[arxiv]</a></li>
<li>Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. <a href="https://arxiv.org/abs/2405.13816">[arxiv]</a></li>
<li>Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. <a href="https://arxiv.org/abs/2405.20215">[arxiv]</a></li>
<li>Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. <a href="https://aclanthology.org/2024.lt4hala-1.30">[paper]</a></li>
<li>Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. <a href="https://arxiv.org/abs/2406.00380">[arxiv]</a></li>
<li>Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. <a href="https://arxiv.org/abs/2406.02106">[arxiv]</a></li>
<li>Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. <a href="https://arxiv.org/abs/2406.03136">[arxiv]</a></li>
<li>Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. <a href="https://arxiv.org/abs/2406.04496">[arxiv]</a></li>
<li>Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. <a href="https://arxiv.org/abs/2406.05688">[arxiv]</a></li>
<li>Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. <a href="https://arxiv.org/abs/2406.05955">[arxiv]</a></li>
<li>Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. <a href="https://arxiv.org/abs/2406.06973">[arxiv]</a></li>
<li>Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. <a href="https://arxiv.org/abs/2406.07115">[arxiv]</a></li>
<li>Zhu et al. Are Large Language Models Good Statisticians?. 2024. <a href="https://arxiv.org/abs/2406.07815">[arxiv]</a></li>
<li>Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. <a href="https://arxiv.org/abs/2406.10099">[arxiv]</a></li>
<li>Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. <a href="https://arxiv.org/abs/2406.10173">[arxiv]</a></li>
<li>He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. <a href="https://arxiv.org/abs/2406.12074">[arxiv]</a></li>
<li>Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. <a href="https://arxiv.org/abs/2406.14408">[arxiv]</a></li>
<li>Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. <a href="https://arxiv.org/abs/2406.14546">[arxiv]</a></li>
<li>Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. <a href="https://arxiv.org/abs/2406.15695">[arxiv]</a></li>
<li>Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. <a href="https://arxiv.org/abs/2406.17233">[arxiv]</a></li>
<li>Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. <a href="https://arxiv.org/abs/2406.18069">[arxiv]</a></li>
<li>Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh&#39;s Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. <a href="https://aclanthology.org/2024.americasnlp-1.25">[paper]</a></li>
<li>Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. <a href="https://arxiv.org/abs/2406.19949">[arxiv]</a></li>
<li>Yang et al. Financial Knowledge Large Language Model. 2024. <a href="https://arxiv.org/abs/2407.00365">[arxiv]</a></li>
<li>Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. <a href="https://arxiv.org/abs/2407.01470">[arxiv]</a></li>
<li>Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. <a href="https://arxiv.org/abs/2407.06129">[arxiv]</a></li>
<li>Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. <a href="https://arxiv.org/abs/2407.08044">[arxiv]</a></li>
<li>Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. <a href="https://arxiv.org/abs/2407.09756">[arxiv]</a></li>
<li>Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. <a href="https://scholarcommons.scu.edu/cseng_senior/272/">[paper]</a></li>
<li>Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. <a href="https://arxiv.org/abs/2407.13561">[arxiv]</a></li>
<li>Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. <a href="https://arxiv.org/abs/2407.16637">[arxiv]</a></li>
<li>Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. <a href="https://arxiv.org/abs/2407.17535">[arxiv]</a></li>
<li>Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. <a href="https://arxiv.org/abs/2407.19705">[arxiv]</a></li>
<li>Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. <a href="https://arxiv.org/abs/2408.00137">[arxiv]</a></li>
<li>Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. <a href="https://www.asianlp.sg/conferences/ialp2024/proceedings/papers/IALP2024_P055.pdf">[paper]</a></li>
<li>Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. <a href="https://link.springer.com/chapter/10.1007/978-981-97-5669-8_11">[paper]</a></li>
<li>Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. <a href="https://link.springer.com/chapter/10.1007/978-981-97-5669-8_23">[paper]</a></li>
<li>Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. <a href="https://arxiv.org/abs/2408.04693">[arxiv]</a></li>
<li>Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. <a href="https://arxiv.org/abs/2408.04168">[arxiv]</a></li>
<li>Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. <a href="https://aclanthology.org/2024.finnlp-2.1/">[paper]</a></li>
<li>Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. <a href="https://arxiv.org/abs/2408.08072">[arxiv]</a></li>
<li>Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. <a href="https://dl.acm.org/doi/10.1145/3627673.3679611">[paper]</a></li>
<li>Zhang et al. CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling. ACL 2024. <a href="https://aclanthology.org/2024.findings-acl.830.pdf">[paper]</a></li>
<li><strong><a href="https://github.com/Yu-Yang-Li/StarWhisper">StarWhisper</a></strong>: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.</li>
<li><strong><a href="https://github.com/FudanDISC/DISC-LawLLM">DISC-LawLLM</a></strong>: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.</li>
<li><strong><a href="https://github.com/X-D-Lab/Sunsimiao">Sunsimiao</a></strong>: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.</li>
<li><strong><a href="https://github.com/WangRongsheng/CareGPT">CareGPT</a></strong>: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.</li>
<li><strong><a href="https://github.com/PKU-YuanGroup/Machine-Mindset/">MachineMindset</a></strong>: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.</li>
<li><strong><a href="https://huggingface.co/Nekochu/Luminia-13B-v3">Luminia-13B-v3</a></strong>: A large language model specialized in generate metadata for stable diffusion. <a href="https://huggingface.co/spaces/Nekochu/Luminia-13B_SD_Prompt">[demo]</a></li>
<li><strong><a href="https://github.com/BUAADreamer/Chinese-LLaVA-Med">Chinese-LLaVA-Med</a></strong>: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.</li>
<li><strong><a href="https://github.com/THUDM/AutoRE">AutoRE</a></strong>: A document-level relation extraction system based on large language models.</li>
<li><strong><a href="https://github.com/NVIDIA/RTX-AI-Toolkit">NVIDIA RTX AI Toolkit</a></strong>: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.</li>
<li><strong><a href="https://github.com/LazyAGI/LazyLLM">LazyLLM</a></strong>: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.</li>
<li><strong><a href="https://github.com/NLPJCL/RAG-Retrieval">RAG-Retrieval</a></strong>: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. <a href="https://zhuanlan.zhihu.com/p/987727357">[blog]</a></li>
<li><strong><a href="https://github.com/Qihoo360/360-LLaMA-Factory">360-LLaMA-Factory</a></strong>: A modified library that supports long sequence SFT &amp; DPO using ring attention.</li>
<li><strong><a href="https://novasky-ai.github.io/posts/sky-t1/">Sky-T1</a></strong>: An o1-like model fine-tuned by NovaSky AI with very small cost.</li>
<li><strong><a href="https://github.com/xming521/WeClone">WeClone</a></strong>: One-stop solution for creating your digital avatar from chat logs.</li>
<li><strong><a href="https://github.com/SmartFlowAI/EmoLLM">EmoLLM</a></strong>: A project about large language models (LLMs) and mental health.</details></li>
</ol>
<h2>License</h2>
<p>This repository is licensed under the <a href="LICENSE">Apache-2.0 License</a>.</p>
<p>Please follow the model licenses to use the corresponding model weights: <a href="https://huggingface.co/baichuan-inc/Baichuan2-7B-Base/blob/main/Community%20License%20for%20Baichuan%202%20Model.pdf">Baichuan 2</a> / <a href="https://huggingface.co/spaces/bigscience/license">BLOOM</a> / <a href="https://github.com/THUDM/ChatGLM3/blob/main/MODEL_LICENSE">ChatGLM3</a> / <a href="https://cohere.com/c4ai-cc-by-nc-license">Command R</a> / <a href="https://github.com/deepseek-ai/DeepSeek-LLM/blob/main/LICENSE-MODEL">DeepSeek</a> / <a href="https://huggingface.co/tiiuae/falcon-180B/blob/main/LICENSE.txt">Falcon</a> / <a href="https://ai.google.dev/gemma/terms">Gemma</a> / <a href="https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE">GLM-4</a> / <a href="https://github.com/openai/gpt-2/blob/master/LICENSE">GPT-2</a> / <a href="LICENSE">Granite</a> / <a href="https://huggingface.co/IndexTeam/Index-1.9B/blob/main/LICENSE">Index</a> / <a href="https://github.com/InternLM/InternLM#license">InternLM</a> / <a href="https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md">Llama</a> / <a href="https://ai.meta.com/llama/license/">Llama 2</a> / <a href="https://llama.meta.com/llama3/license/">Llama 3</a> / <a href="https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE">Llama 4</a> / <a href="https://github.com/OpenBMB/MiniCPM/blob/main/MiniCPM%20Model%20License.md">MiniCPM</a> / <a href="LICENSE">Mistral/Mixtral/Pixtral</a> / <a href="LICENSE">OLMo</a> / <a href="https://huggingface.co/microsoft/phi-1_5/resolve/main/Research%20License.docx">Phi-1.5/Phi-2</a> / <a href="https://huggingface.co/microsoft/Phi-3-mini-4k-instruct/blob/main/LICENSE">Phi-3/Phi-4</a> / <a href="https://github.com/QwenLM/Qwen/blob/main/Tongyi%20Qianwen%20LICENSE%20AGREEMENT">Qwen</a> / <a href="https://huggingface.co/Skywork/Skywork-13B-base/blob/main/Skywork%20Community%20License.pdf">Skywork</a> / <a href="https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement">StarCoder 2</a> / <a href="https://huggingface.co/Tele-AI/telechat-7B/blob/main/TeleChat%E6%A8%A1%E5%9E%8B%E7%A4%BE%E5%8C%BA%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE.pdf">TeleChat2</a> / <a href="https://github.com/xverse-ai/XVERSE-13B/blob/main/MODEL_LICENSE.pdf">XVERSE</a> / <a href="https://huggingface.co/01-ai/Yi-6B/blob/main/LICENSE">Yi</a> / <a href="LICENSE">Yi-1.5</a> / <a href="https://github.com/IEIT-Yuan/Yuan-2.0/blob/main/LICENSE-Yuan">Yuan 2</a></p>
<h2>Citation</h2>
<p>If this work is helpful, please kindly cite as:</p>
<pre><code class="language-bibtex">@inproceedings{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  address={Bangkok, Thailand},
  publisher={Association for Computational Linguistics},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}
</code></pre>
<h2>Acknowledgement</h2>
<p>This repo benefits from <a href="https://github.com/huggingface/peft">PEFT</a>, <a href="https://github.com/huggingface/trl">TRL</a>, <a href="https://github.com/artidoro/qlora">QLoRA</a> and <a href="https://github.com/lm-sys/FastChat">FastChat</a>. Thanks for their wonderful works.</p>
<h2>Star History</h2>
<p><img src="https://api.star-history.com/svg?repos=hiyouga/LLaMA-Factory&type=Date" alt="Star History Chart"></p>
 </div> </article> <!-- Related Models Section --> <section class="mt-16 pt-8 border-t border-gray-200 dark:border-gray-700"> <h2 class="text-3xl font-bold text-center mb-8">Related Models</h2> <div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-6"> <a href="/model/github-yangjianxin1-Firefly" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full"> <div class="absolute top-2 right-2 bg-yellow-400 text-yellow-900 text-xs font-bold px-2 py-1 rounded-full animate-pulse" title="Rising Star">
üî•
</div> <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="Firefly"> Firefly </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by yangjianxin1</p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> Firefly: Â§ßÊ®°ÂûãËÆ≠ÁªÉÂ∑•ÂÖ∑ÔºåÊîØÊåÅËÆ≠ÁªÉQwen2.5„ÄÅQwen2„ÄÅYi1.5„ÄÅPhi-3„ÄÅLlama3„ÄÅGemma„ÄÅMiniCPM„ÄÅYi„ÄÅDeepseek„ÄÅOrion„ÄÅXverse„ÄÅMixtral-8x7B„ÄÅZephyr„ÄÅMistral...
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="6,592 likes">‚ù§Ô∏è <span>6.6K</span></div> <div class="flex items-center gap-1" title="6,592 downloads">üì• <span>6.6K</span></div> </div> </div> </a><a href="/model/github-unslothai-unsloth" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full"> <div class="absolute top-2 right-2 bg-yellow-400 text-yellow-900 text-xs font-bold px-2 py-1 rounded-full animate-pulse" title="Rising Star">
üî•
</div> <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="unsloth"> unsloth </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by unslothai</p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> Fine-tuning &amp; Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, DeepSeek-R1, Qwen3, Gemma 3, TTS 2x faster with ...
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="48,493 likes">‚ù§Ô∏è <span>48.5K</span></div> <div class="flex items-center gap-1" title="48,493 downloads">üì• <span>48.5K</span></div> </div> </div> </a><a href="/model/github-ollama-ollama" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full"> <div class="absolute top-2 right-2 bg-yellow-400 text-yellow-900 text-xs font-bold px-2 py-1 rounded-full animate-pulse" title="Rising Star">
üî•
</div> <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="ollama"> ollama </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by ollama</p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> Get up and running with OpenAI gpt-oss, DeepSeek-R1, Gemma 3 and other models....
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="156,286 likes">‚ù§Ô∏è <span>156.3K</span></div> <div class="flex items-center gap-1" title="156,286 downloads">üì• <span>156.3K</span></div> </div> </div> </a> </div> </section> </div> </div>  </main> <footer class="bg-gray-800 text-gray-300 py-8 mt-16"> <div class="container mx-auto px-4 text-center"> <div class="flex justify-center gap-4 mb-4"> <a href="/about" class="hover:underline">About</a> <a href="/compliance" class="hover:underline">Compliance</a> </div> <p class="text-sm mt-2">
&copy; 2025 Free AI Tools. An open-source project to index the world of AI.
</p> <a href="mailto:compliance@free2aitools.com" class="text-sm text-gray-400 hover:underline mt-1 block">compliance@free2aitools.com</a> </div> </footer> </body></html>