<!DOCTYPE html><html lang="en"> <head><!-- Google Tag Manager --><script type="module">(function(e,n,r,t,m){e[t]=e[t]||[],e[t].push({"gtm.start":new Date().getTime(),event:"gtm.js"});var g=n.getElementsByTagName(r)[0],a=n.createElement(r),s="";a.async=!0,a.src="https://www.googletagmanager.com/gtm.js?id="+m+s,g.parentNode.insertBefore(a,g)})(window,document,"script","dataLayer","GTM-58C2CQ8G");</script><!-- End Google Tag Manager --><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="/favicon.ico"><!-- Open Graph Meta Tags --><meta property="og:type" content="website"><meta property="og:url" content="https://free2aitools.com/model/Gen2B--HyGPT-10b-it/"><meta property="og:title" content="HyGPT-10b-it - AI Model Details"><meta property="og:description" content="A model for text-generation."><meta property="og:image" content="https://free2aitools.com/og-image.jpg"><!-- Twitter Card Meta Tags --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://free2aitools.com/model/Gen2B--HyGPT-10b-it/"><meta property="twitter:title" content="HyGPT-10b-it - AI Model Details"><meta property="twitter:description" content="A model for text-generation."><meta property="twitter:image" content="https://free2aitools.com/og-image.jpg"><meta name="description" content="A model for text-generation."><title>HyGPT-10b-it - AI Model Details</title><link rel="canonical" href="https://free2aitools.com/model/Gen2B--HyGPT-10b-it/"><!-- Google AdSense --><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2292826803755214" crossorigin="anonymous"></script><link rel="stylesheet" href="/_astro/about.CAZw8hUu.css"></head> <body class="bg-gray-50 text-gray-800 font-sans antialiased flex flex-col min-h-screen"> <!-- Google Tag Manager (noscript) --> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-58C2CQ8G" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <!-- End Google Tag Manager (noscript) --> <header class="sticky top-0 z-30 w-full backdrop-blur flex-none transition-colors duration-500 lg:border-b lg:border-gray-200 bg-white/80"> <nav class="container mx-auto px-4 sm:px-6 lg:px-8"> <div class="flex items-center justify-between h-16"> <!-- Site Logo --> <a href="/" class="flex items-center gap-2 text-xl sm:text-2xl font-bold text-gray-900"> <span>Free AI Tools</span> </a> <!-- Right-side items --> <div class="flex items-center gap-4"> <!-- Desktop Navigation --> <div class="hidden md:flex items-center gap-6"> <a href="/" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Explore Models </a><a href="/ranking" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Rankings </a><a href="/reports" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> Reports </a><a href="/about" class="text-base font-medium text-gray-600 hover:text-blue-600 transition-colors"> About </a> </div> <!-- Mobile Search & Menu Toggle --> <button data-collapse-toggle="navbar-default" type="button" class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200" aria-controls="navbar-default" aria-expanded="false"> <span class="sr-only">Open main menu</span> <svg class="w-5 h-5" aria-hidden="true" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 17 14"><path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M1 1h15M1 7h15M1 13h15"></path></svg> </button> </div> </div> <!-- Mobile Collapsible Menu --> <div class="hidden w-full md:hidden" id="navbar-default"> <ul class="font-medium flex flex-col p-4 md:p-0 mt-4 border border-gray-100 rounded-lg bg-gray-50"> <li> <a href="/" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Explore Models</a> </li><li> <a href="/ranking" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Rankings</a> </li><li> <a href="/reports" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">Reports</a> </li><li> <a href="/about" class="block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0">About</a> </li> </ul> </div> </nav> </header> <main class="flex-grow">  <div class="container mx-auto px-4 py-12"> <div class="max-w-4xl mx-auto"> <!-- Header Section --> <header class="mb-8"> <div class="flex flex-col md:flex-row md:items-start md:justify-between gap-4"> <div> <h1 class="text-4xl md:text-5xl font-extrabold text-gray-900 dark:text-white mb-2">HyGPT-10b-it</h1> <p class="text-lg text-gray-500 dark:text-gray-400">by </p> </div>  </div> </header> <!-- Metadata Section --> <div class="mb-8 p-4 bg-gray-100 dark:bg-gray-800 rounded-lg"> <div class="grid grid-cols-2 md:grid-cols-4 gap-4 text-center"> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Likes</p> <p class="text-xl font-bold">80</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Downloads</p> <p class="text-xl font-bold">230</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Task</p> <p class="text-xl font-bold capitalize">text-generation</p> </div> <div> <p class="text-sm text-gray-500 dark:text-gray-400">Last Updated</p> <p class="text-xl font-bold">Invalid Date</p> </div> </div> </div> <h3 class="text-lg font-semibold mb-2">Sources</h3> <div class="flex flex-col gap-2"> <a href="https://huggingface.co/Gen2B/HyGPT-10b-it" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a><a href="https://huggingface.co/Gen2B/HyGPT-10b-it" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a><a href="https://huggingface.co/Gen2B/HyGPT-10b-it" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a><a href="https://huggingface.co/Gen2B/HyGPT-10b-it" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a><a href="https://huggingface.co/Gen2B/HyGPT-10b-it" target="_blank" rel="noopener noreferrer" class="text-blue-600 dark:text-blue-400 hover:underline flex items-center gap-2"> ü§ó Hugging Face </a> </div> </div> </div>  <div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div><div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div><div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div><div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div><div class="mb-8 p-4 border border-gray-200 dark:border-gray-700 rounded-lg"> <h3 class="text-2xl font-bold mb-4 flex items-center gap-2"> ü§ó Hugging Face Details
</h3>   </div> <article class="model-readme-content py-8 border-t border-gray-200 dark:border-gray-700"> <div class="prose prose-lg dark:prose-invert max-w-none"> <h1>HyGPT-10b-it</h1>
<p>HyGPT-10b-it is an instruction-tuned version of HyGPT-10b, the first Armenian large language model that was pretrained on a corpus of Armenian text data. This model has been fine-tuned on a diverse instruction dataset to enhance its ability to follow instructions, engage in multi-turn conversations, and perform various language tasks in Armenian, Russian, and English.</p>
<h2>Model Details</h2>
<h3>Model Description</h3>
<p>HyGPT-10b-it is a decoder-only language model based on the HyGPT-10b base model that was first pretrained on 10B tokens of Armenian text and then instruction-tuned (SFT) on a diverse dataset of 50,000 instruction samples.</p>
<ul>
<li><strong>Developed by:</strong> <a href="https://gen2b.ai/">Gen2B</a> &amp; <a href="http://arm.ican24.net/">NCCAIT</a></li>
<li><strong>Model type:</strong> Instruction-tuned decoder-only language model</li>
<li><strong>Language(s) (NLP):</strong> Armenian, English, Russian</li>
<li><strong>Technical Report:</strong> <a href="https://gen2b.ai/hygpt-release-1-0">link</a></li>
<li><strong>License:</strong> <a href="https://huggingface.co/Gen2B/HyGPT-10b/raw/main/LICENSE">HyGPT Permissive Use License</a></li>
</ul>
<h2>Uses</h2>
<p>First, install the Transformers library with:</p>
<pre><code class="language-sh">pip install -U transformers
</code></pre>
<p>Then, run this example:</p>
<pre><code class="language-python">from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig
import torch

model_path = &quot;Gen2B/HyGPT-10b-it&quot;
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map=&quot;auto&quot;,
)

# Example of a single-turn conversation
chat = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;‘ª’∂’π’∏÷Ç ’ß ’≠’∏’ø’® ‘ø’°’∂’°’π:&quot;}
]

# Example of a multi-turn conversation
# chat = [
#     {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;‘≤’°÷Ä÷á, ’´’∂’π’∫’•’û’Ω ’•’Ω:&quot;},
#     {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;‘≤’°÷Ä÷á, ’•’Ω ’¨’°’æ ’•’¥: ‘ª’∂’π’∏’æ ’Ø’°÷Ä’∏’≤ ’•’¥ ÷Ö’£’∂’•’¨ ÷Ñ’•’¶ ’°’µ’Ω÷Ö÷Ä:&quot;},
#     {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;‘ª’∂’π’∏÷Ç ’ß ’≠’∏’ø’® ‘ø’°’∂’°’π:&quot;}
# ]

PROMPT = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)

inputs = tokenizer(
    PROMPT,
    return_tensors=&quot;pt&quot;,
)

print(&quot;Generating...&quot;)
generation_output = model.generate(
    input_ids=inputs[&quot;input_ids&quot;].cuda(),
    generation_config=GenerationConfig(
        temperature=0.0001,
        repetition_penalty=1.1,
        do_sample=True
    ),
    return_dict_in_generate=True,
    output_scores=True,
    max_new_tokens=1024,
)
for s in generation_output.sequences:
    print(tokenizer.decode(s))

# ‘Ω’∏’ø’´ ’¥’•’ª ’Ø’°’∂ ’∫’´’£’¥’•’∂’ø’∂’•÷Ä, ’∏÷Ä’∏’∂÷Ñ ’Ø’¨’°’∂’∏÷Ç’¥ ’•’∂ ’¨’∏÷Ç’µ’Ω’´ ’¥’´’°’µ’∂ ’Ø’°÷Ä’≥ ’°’¨’´÷Ñ’∂’•÷Ä’® ÷á ’°’∂’§÷Ä’°’§’°÷Ä’±’∂’∏÷Ç’¥ ’•÷Ä’Ø’°÷Ä ’°’¨’´÷Ñ’∂’•÷Ä’®÷â ‘¥÷Ä’°’∂÷Ñ ’∂’°÷á ’¢’°÷Å ’•’∂ ’©’∏’≤’∂’∏÷Ç’¥ ’∏÷Ç’¨’ø÷Ä’°’¥’°’∂’∏÷Ç’∑’°’Ø’°’£’∏÷Ç’µ’∂ ÷á ’´’∂÷Ü÷Ä’°’Ø’°÷Ä’¥’´÷Ä ’°’¨’´÷Ñ’∂’•÷Ä’®÷â ’Ñ’°÷Ä’§’∏÷Ç ’°’π÷Ñ’•÷Ä’® ’¶’£’°’µ’∏÷Ç’∂ ’π’•’∂ ’°’µ’Ω ’°’¨’´÷Ñ’∂’•÷Ä’´ ’∂’Ø’°’ø’¥’°’¥’¢, ’∏÷Ç’Ω’ø’´ ’§÷Ä’°’∂÷Ñ ’ø’•’Ω’°’∂’•’¨’´ ’π’•’∂÷â ‘±’µ’Ω’∫’´’Ω’∏’æ, ’•÷Ä’¢ ’°÷Ä÷á’´ ’¨’∏÷Ç’µ’Ω’® ’∞’°÷Ä’æ’°’Æ’∏÷Ç’¥ ’ß ’≠’∏’ø’´’∂, ’°’µ’∂ ’°’∂’§÷Ä’°’§’°÷Ä’±’∂’∏÷Ç’¥ ’ß ’•÷Ä’Ø’°÷Ä ’°’¨’´÷Ñ’∂’•÷Ä’®’ù ’°’º’°’ª’°÷Å’∂’•’¨’∏’æ ’Ø’°’∂’°’π ’£’∏÷Ç’µ’∂’®, ’∏÷Ä’® ’¥’•’∂÷Ñ ’ø’•’Ω’∂’∏÷Ç’¥ ’•’∂÷Ñ:
</code></pre>
<h3>Direct Use</h3>
<p>HyGPT-10b-it can be used directly for:</p>
<ul>
<li>Multi-turn conversations in Armenian</li>
<li>Rephrasing and paraphrasing Armenian text</li>
<li>Question answering in Armenian</li>
<li>Text summarization and paraphrasing</li>
<li>Translation between Armenian, Russian, and English</li>
<li>Mathematical problem solving</li>
<li>General knowledge queries</li>
<li>Educational content assistance</li>
</ul>
<h2>Bias, Risks, and Limitations</h2>
<ul>
<li>The model may reflect biases present in both the pretraining and instruction-tuning datasets</li>
<li>Accuracy may vary across different Armenian dialects and regional variations</li>
<li>The model may not have up-to-date knowledge beyond its training data</li>
<li>Like all language models, it may occasionally generate incorrect or nonsensical responses</li>
<li>The model&#39;s understanding of specialized Armenian terminology may be limited in certain domains</li>
<li>Performance on complex reasoning tasks may be inconsistent</li>
</ul>
<h2>Training Details</h2>
<h3>Base Model</h3>
<p>The base model (HyGPT-10b) was pretrained on a diverse corpus of Armenian text data comprising approximately 10 billion tokens, including:</p>
<ul>
<li>Armenian web content</li>
<li>Armenian literature and publications</li>
<li>Armenian news articles</li>
<li>Armenian Wikipedia</li>
<li>Other publicly available Armenian text sources</li>
</ul>
<h3>Instruction Tuning Dataset</h3>
<p>The model was fine-tuned on a diverse instruction dataset consisting of 50,000 samples with the following characteristics:</p>
<ul>
<li><p><strong>Dataset Composition:</strong></p>
<ul>
<li>Single-turn instruction-response pairs</li>
<li>Multi-turn conversations (dialogues with multiple exchanges)</li>
<li>Approximately 50% synthetic data generated with Gemini Flash 2.0</li>
</ul>
</li>
<li><p><strong>Task Types:</strong></p>
<ul>
<li>Summarization tasks</li>
<li>Paraphrasing exercises</li>
<li>Translation between Armenian, Russian, and English</li>
<li>Everyday conversational dialogues</li>
<li>Wikipedia-based knowledge questions</li>
<li>Mathematical and educational problems</li>
<li>General knowledge queries</li>
</ul>
</li>
</ul>
<h3>Preprocessing</h3>
<p>The instruction tuning data underwent several preprocessing steps:</p>
<ul>
<li>Formatting into consistent instruction-response pairs</li>
<li>Translation of some samples into Armenian language</li>
<li>Quality filtering</li>
<li>Conversion to chat format with appropriate role assignments</li>
<li>Tokenization using the base model&#39;s tokenizer</li>
</ul>
<h3>Training Procedure</h3>
<p>The model was fine-tuned from the HyGPT-10b base model using supervised fine-tuning (SFT) techniques. The training focused on teaching the model to:</p>
<ol>
<li>Follow instructions accurately</li>
<li>Maintain context across multi-turn conversations (context is better provided after the question)</li>
<li>Generate helpful, accurate, and contextually appropriate responses</li>
<li>Handle a variety of task types including translation, summarization, and question answering</li>
</ol>
<h3>Benchmarks</h3>
<p>The model was evaluated on several standard benchmarks that were translated into Armenian to accurately assess its performance in the target language. The benchmarks include:</p>
<ul>
<li><strong>Flores</strong>: Tests the model&#39;s ability to translate text between Armenian, Russian, and English languages</li>
<li><strong>ARC</strong>: A multiple-choice question benchmark that evaluates reasoning capabilities</li>
<li><strong>Truthful QA</strong>: Another multiple-choice benchmark that assesses the model&#39;s ability to provide truthful answers</li>
<li><strong>GSM8K</strong>: Evaluates the model&#39;s mathematical reasoning skills with school-level math problems</li>
</ul>
<p>Below is a table of accuracy of different models on 4 benchmarks. The results demonstrate significant improvements over the base model across these tasks:</p>
<table>
<thead>
<tr>
<th></th>
<th><strong>Gen2B/HyGPT-10b-it</strong></th>
<th><em>google/gemma-3-12b-it</em></th>
<th><em>mistralai/Mistral-Small-3.1-24B-Instruct-2503</em></th>
<th><em>google/gemma-2-9b-it</em></th>
<th><em>mistralai/Mistral-Nemo-Instruct-2407</em></th>
<th><em>meta-llama/Llama-3.1-8B-Instruct</em></th>
</tr>
</thead>
<tbody><tr>
<td>Flores</td>
<td><em>79.33</em></td>
<td><em>80.59</em></td>
<td><strong>80.62</strong></td>
<td><em>78.61</em></td>
<td><em>79.1</em></td>
<td><em>77.67</em></td>
</tr>
<tr>
<td>ARC</td>
<td><em>76.1</em></td>
<td><em>79.42</em></td>
<td><strong>81.76</strong></td>
<td><em>72.54</em></td>
<td><em>73.2</em></td>
<td><em>58.91</em></td>
</tr>
<tr>
<td>Truthful QA</td>
<td><strong>72.83</strong></td>
<td><em>65.52</em></td>
<td><em>67.98</em></td>
<td><em>67.49</em></td>
<td><em>39.9</em></td>
<td><em>39.41</em></td>
</tr>
<tr>
<td>GSM8K</td>
<td><strong>68.0</strong></td>
<td><em>65.8</em></td>
<td><em>41.07</em></td>
<td><em>38.0</em></td>
<td><em>44.19</em></td>
<td><em>17.22</em></td>
</tr>
<tr>
<td>avg</td>
<td><strong>74.06</strong></td>
<td><em>72.83</em></td>
<td><em>67.86</em></td>
<td><em>64.16</em></td>
<td><em>59.1</em></td>
<td><em>48.3</em></td>
</tr>
</tbody></table>
<h3>Results</h3>
<p>The instruction-tuned model demonstrates significantly improved capabilities in following instructions and engaging in conversations compared to the base model. It shows enhanced abilities in:</p>
<ul>
<li>Understanding and responding to complex instructions</li>
<li>Maintaining context across multi-turn dialogues</li>
<li>Generating more natural and helpful responses</li>
<li>Performing specific tasks like translation and summarization</li>
</ul>
<h4>Summary</h4>
<p>HyGPT-10b-it builds upon the strong foundation of HyGPT-10b to provide a more interactive and instruction-following Armenian language model. It is particularly well-suited for conversational applications, educational tools, and multilingual assistance systems that require Armenian language support.</p>
<hr>
<h2>License and Terms of Use</h2>
<p>This model is based on Gemma and is distributed according to the <a href="https://ai.google.dev/gemma/terms">Gemma Terms of Use</a>.</p>
<p><strong>Notice</strong>: Gemma is provided under and subject to the Gemma Terms of Use found at <a href="https://ai.google.dev/gemma/terms">ai.google.dev/gemma/terms</a>.</p>
<h3>Modifications Notice</h3>
<p>This model is a modified version of the original Gemma-2-9b model. The modifications include:</p>
<ol>
<li>Further pretraining on 10 billion tokens of Armenian text data</li>
<li>Decoupling of the embedding and LM head layers to allow independent training of the output layer</li>
<li>Instruction tuning (SFT) on a dataset of 50,000 instruction samples</li>
</ol>
<h3>Use Restrictions</h3>
<p>According to the Gemma Terms of Use, the model should not be used:</p>
<ol>
<li>For purposes outlined in the <a href="https://ai.google.dev/gemma/prohibited_use_policy">Gemma Prohibited Use Policy</a></li>
<li>In violation of applicable laws and regulations</li>
</ol>
<h2>Disclaimer of Warranty</h2>
<p>UNLESS REQUIRED BY APPLICABLE LAW, THE GEMMA SERVICES, AND OUTPUTS, ARE PROVIDED ON AN &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING, REPRODUCING, MODIFYING, PERFORMING, DISPLAYING OR DISTRIBUTING ANY OF THE GEMMA SERVICES OR OUTPUTS AND ASSUME ANY AND ALL RISKS ASSOCIATED WITH YOUR USE OR DISTRIBUTION OF ANY OF THE GEMMA SERVICES OR OUTPUTS AND YOUR EXERCISE OF RIGHTS AND PERMISSIONS UNDER THIS AGREEMENT.</p>
 </div> </article>  <section class="mt-16 pt-8 border-t border-gray-200 dark:border-gray-700"> <h2 class="text-3xl font-bold text-center mb-8">Related Models</h2> <div class="grid grid-cols-1 sm:grid-cols-2 lg:grid-cols-3 gap-6"> <a href="/model/Lamapi--next-1b" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full">  <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="next-1b"> next-1b </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by </p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> A model for text-generation....
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="90 likes">‚ù§Ô∏è <span>90</span></div> <div class="flex items-center gap-1" title="35,590 downloads">üì• <span>35.6K</span></div> </div> </div> </a><a href="/model/Lamapi--next-12b" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full">  <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="next-12b"> next-12b </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by </p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> A model for image-text-to-text....
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="110 likes">‚ù§Ô∏è <span>110</span></div> <div class="flex items-center gap-1" title="18,640 downloads">üì• <span>18.6K</span></div> </div> </div> </a><a href="/model/katanemo--Arch-Router-1.5B" class="group relative block bg-white dark:bg-gray-800 rounded-lg shadow-md hover:shadow-xl transition-shadow duration-300 overflow-hidden h-full">  <div class="p-5 flex flex-col h-full justify-between"> <div> <h3 class="text-lg font-bold text-gray-900 dark:text-white truncate group-hover:text-blue-600 dark:group-hover:text-blue-400 transition-colors" title="Arch-Router-1.5B"> Arch-Router-1.5B </h3> <p class="text-gray-500 dark:text-gray-400 text-xs mb-3">by </p> <p class="text-gray-600 dark:text-gray-300 text-sm h-20 overflow-hidden text-ellipsis leading-relaxed"> A model for text-generation....
</p> </div> <div class="mt-4 flex items-center justify-end gap-4 text-xs text-gray-500 dark:text-gray-400"> <div class="flex items-center gap-1" title="2,230 likes">‚ù§Ô∏è <span>2.2K</span></div> <div class="flex items-center gap-1" title="47,230 downloads">üì• <span>47.2K</span></div> </div> </div> </a> </div> </section> </main> <footer class="bg-gray-800 text-gray-300 py-8 mt-16"> <div class="container mx-auto px-4 text-center"> <div class="flex justify-center gap-4 mb-4"> <a href="/about" class="hover:underline">About</a> <a href="/compliance" class="hover:underline">Compliance</a> </div> <p class="text-sm mt-2">
&copy; 2025 Free AI Tools. An open-source project to index the world of AI.
</p> <a href="mailto:compliance@free2aitools.com" class="text-sm text-gray-400 hover:underline mt-1 block">compliance@free2aitools.com</a> </div> </footer> </body></html>